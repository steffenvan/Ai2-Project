<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.875563">
Computationally Efficient M-Estimation of Log-Linear Structure Models*
</title>
<author confidence="0.898837">
Noah A. Smith and Douglas L. Vail and John D. Lafferty
</author>
<affiliation confidence="0.827968333333333">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213 USA
</affiliation>
<email confidence="0.999463">
{nasmith,dvail2,lafferty}@cs.cmu.edu
</email>
<sectionHeader confidence="0.994814" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999993529411765">
We describe a new loss function, due to Jeon
and Lin (2006), for estimating structured
log-linear models on arbitrary features. The
loss function can be seen as a (generative) al-
ternative to maximum likelihood estimation
with an interesting information-theoretic in-
terpretation, and it is statistically consis-
tent. It is substantially faster than maximum
(conditional) likelihood estimation of condi-
tional random fields (Lafferty et al., 2001;
an order of magnitude or more). We com-
pare its performance and training time to an
HMM, a CRF, an MEMM, and pseudolike-
lihood on a shallow parsing task. These ex-
periments help tease apart the contributions
of rich features and discriminative training,
which are shown to be more than additive.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996263761904762">
Log-linear models are a very popular tool in natural
language processing, and are often lauded for per-
mitting the use of “arbitrary” and “correlated” fea-
tures of the data by a model. Users of log-linear
models know, however, that this claim requires some
qualification: any feature is permitted in principle,
but training log-linear models (and decoding under
them) is tractable only when the model’s indepen-
dence assumptions permit efficient inference proce-
dures. For example, in the original conditional ran-
dom fields (Lafferty et al., 2001), features were con-
∗This work was supported by NSF grant IIS-0427206 and
the DARPA CALO project. The authors are grateful for feed-
back from David Smith and from three anonymous ACL re-
viewers, and helpful discussions with Charles Sutton.
fined to locally-factored indicators on label bigrams
and label unigrams (with any of the observation).
Even in cases where inference in log-linear mod-
els is tractable, it requires the computation of a parti-
tion function. More formally, a log-linear model for
random variables X and Y over X, Y defines:
</bodyText>
<equation confidence="0.9928175">
ew&gt;f(x,y)
=
Z(w)
(1)
</equation>
<bodyText confidence="0.99991395">
where f : XxY � Rm is the feature vector-function
and w E Rm is a weight vector that parameterizes
the model. In NLP, we rarely train this model by
maximizing likelihood, because the partition func-
tion Z(w) is expensive to compute exactly. Z(w)
can be approximated (e.g., using Gibbs sampling;
Rosenfeld, 1997).
In this paper, we propose the use of a new loss
function that is computationally efficient and statis-
tically consistent (§2). Notably, repeated inference
is not required during estimation. This loss func-
tion can be seen as a case of M-estimation1 that
was originally developed by Jeon and Lin (2006) for
nonparametric density estimation. This paper gives
an information-theoretic motivation that helps eluci-
date the objective function (§3), shows how to ap-
ply the new estimator to structured models used in
NLP (§4), and compares it to a state-of-the-art noun
phrase chunker (§5). We discuss implications and
future directions in §6.
</bodyText>
<sectionHeader confidence="0.977762" genericHeader="method">
2 Loss Function
</sectionHeader>
<bodyText confidence="0.996116">
As before, let X be a random variable over a high-
dimensional space X, and similarly Y over Y. X
</bodyText>
<footnote confidence="0.909356">
1“M-estimation” is a generalization of MLE (van der Vaart,
1998); space does not permit a full discussion.
</footnote>
<equation confidence="0.9841325">
pw(x, y) = Ex0,y0EXx �
ew&gt;f(x,y)
</equation>
<page confidence="0.973651">
752
</page>
<note confidence="0.9249475">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 752–759,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99951925">
might be the set of all sentences in a language, and
� the set of all POS tag sequences or the set of all
parse trees. Let q0 be a “base” distribution that is
our first approximation to the true distribution over
X x �. HMMs and PCFGs, while less accurate as
predictors than the rich-featured log-linear models
we desire, might be used to define q0.
The model we estimate will have the form
</bodyText>
<equation confidence="0.894884">
pw(x, y) a q0(x,y)ew&gt;f(x,y) (2)
</equation>
<bodyText confidence="0.9811803">
Notice that pw(x, y) = 0 whenever q0(x, y) = 0.
It is therefore important for q0 to be smooth, since
the support of pw is a subset of the support of q0.
Notice that we have not written the partition function
explicitly in Eq. 2; it will never need to be computed
during estimation or inference. The unnormalized
distribution will suffice for all computation.
Suppose we have observations (x1, x2, ..., xn)
with annotations (y1, ..., yn). The (unregularized)
loss function, due to Jeon and Lin (2006), is2
</bodyText>
<equation confidence="0.9986258">
1 En e−w&gt;f(xi,yi)
`(w) = n i=1
E ( )
+ q0(x, y) w&gt;f(x, y) (3)
x,y
= 1 En e−w&gt;f(xi,yi) + w&gt; E q0(x, y)f(x, y)
n i=1 x,y
e−w&gt;f(xi,yi) + w&gt; Eq0(X,Y )If(X, Y )]
� �� �
constant(w)
</equation>
<bodyText confidence="0.99979">
Before explaining this objective, we point out
some attractive computational properties. Notice
that f(xi, yi) (for all i) and the expectations of the
feature vectors under q0 are constant with respect
to w. Computing the function in Eq. 3, then, re-
quires no inference and no dynamic programming,
only O(nm) floating-point operations.
</bodyText>
<sectionHeader confidence="0.984992" genericHeader="method">
3 An Interpretation
</sectionHeader>
<bodyText confidence="0.997682">
Here we give an account of the loss function as a
way of “cleaning up” a mediocre model (q0). We
</bodyText>
<footnote confidence="0.98314925">
2We give only the discrete version here, because it is most
relevant for an ACL audience. Also, our linear function
w&gt;f(xi, yi) is a simple case; another kernel (for example)
could be used.
</footnote>
<bodyText confidence="0.997123363636364">
show that this estimate aims to model a presumed
perturbation that created q0, by minimizing the KL
divergence between q0 and a perturbed version of the
sample distribution p.
Consider Eq. 2. Given a training dataset, maxi-
mizing likelihood under this model means assuming
that there is some w∗ for which the true distribu-
tion p∗(x, y) = pw∗(x, y). Carrying out MLE, how-
ever, would require computing the partition function
Ex0,y0 q0(x0, y0)ew&gt;f(x0,y0), which is in general in-
tractable. Rearranging Eq. 2 slightly, we have
</bodyText>
<equation confidence="0.952292">
q0(x,y) a p∗(x, y)e−w&gt;f(x,y) (4)
</equation>
<bodyText confidence="0.9948915">
If q0 is close to the true model, e−w&gt;f(x,y) should
be close to 1 and w close to zero. In the sequence
model setting, for example, if q0 is an HMM that ex-
plains the data well, then the additional features are
not necessary (equivalently, their weights should be
0). If q0 is imperfect, we might wish to make it more
powerful by adding features (e.g., f), but q0 nonethe-
less provides a reasonable “starting point” for defin-
ing our model.
So instead of maximizing likelihood, we will min-
imize the KL divergence between the two sides of
Eq. 4.3
</bodyText>
<equation confidence="0.980649647058824">
DKL(q0(x,y)IIp∗(x,y)e−w&gt;f(x,y)) (5)
E= q0(x, y)
x,y q0(x, y) log p∗(x, y)e−w&gt;f(x,y) (6)
E Ep∗(x, y)e−w&gt;f(x,y) − q0(x, y)
+ x,y
x,y
= −H(q0) + E p∗(x,y)e−w&gt;f(x,y) − 1
x,y
E− (q0(x, y) log p∗(x, y)e−w&gt;f(x,y))
x,y
E= constant(w) + p∗(x, y)e−w&gt;f(x,y)
x,y
( )
q0(x, y) w&gt;f(x, y)
3The KL divergence here is generalized for unnormalized
distributions, following O’Sullivan (1998): \\II
DKL(uIIv) = Ej (uj log v� − uj + vj/
</equation>
<bodyText confidence="0.9942685">
where u and v are nonnegative vectors defining unnormal-
ized distributions over the same event space. Note that when
Ej uj = Ej vj = 1, this formula takes on the more familiar
form, as − Ej uj and Ej vj cancel.
</bodyText>
<equation confidence="0.972750285714286">
1
n
=
En
i=1
+E
x,y
</equation>
<page confidence="0.987796">
753
</page>
<bodyText confidence="0.999983333333333">
If we replace p* with the empirical (sampled) dis-
tribution ˜p, minimizing the above KL divergence is
equivalent to minimizing `(w) (Eq. 3). It may be
helpful to think of —w as the parameters of a process
that “damage” the true model p*, producing q0, and
the estimation of w as learning to undo that damage.
In the remainder of the paper, we use the general
term “M-estimation” to refer to the minimization of
`(w) as a way of training a log-linear model.
</bodyText>
<sectionHeader confidence="0.8816655" genericHeader="method">
4 Algorithms for Models of Sequences and
Trees
</sectionHeader>
<bodyText confidence="0.9992505">
We discuss here some implementation aspects of the
application of M-estimation to NLP models.
</bodyText>
<subsectionHeader confidence="0.987089">
4.1 Expectations under q0
</subsectionHeader>
<bodyText confidence="0.999951222222222">
The base distribution q0 enters into implementation
in two places: Eq0(X,Y )[f(X, Y )] must be computed
for training, and q0(x, y) is a factor in the model
used in decoding.
If q0 is a familiar stochastic grammar, such as an
HMM or a PCFG, or any generative model from
which sampling is straightforward, it is possible to
estimate the feature expectations by sampling from
the model directly; for sample ((˜xi, ˜yi))si=1 let:
</bodyText>
<equation confidence="0.692218">
1 s
</equation>
<bodyText confidence="0.965460222222222">
Eq0(X,Y ) [fj (X, Y )] — s X
If the feature space is sparse under q0 (likely in most
settings), then smoothing may be required.
If q0 is an HMM or a PCFG, the expectation vec-
tor can be computed exactly by solving a system of
equations. We will see that for the common cases
where features are local substructures, inference is
straightforward. We briefly describe how this can be
done for a bigram HMM and a PCFG.
</bodyText>
<subsectionHeader confidence="0.926751">
4.1.1 Expectations under an HMM
</subsectionHeader>
<bodyText confidence="0.998982">
Let S be the state space of a first-order HMM.
If s = (s1, ..., sk) is a state sequence and x =
(x1, ..., xk) is an observed sequence of emissions,
then:
</bodyText>
<equation confidence="0.969906">
!tsi−1(si)esi(xi) tsk(stop) (8)
</equation>
<bodyText confidence="0.99062275">
(Assume s0 = start is the single, silent, initial state,
and stop is the only stop state, also silent. We as-
sume no other states are silent.)
The first step is to compute path-sums into and out
of each state, under the HMM q0. To do this, define
is as the total weight of state-prefixes (beginning in
start) ending in s and os as the total weight of state-
suffixes beginning in s (and ending in stop):4
</bodyText>
<equation confidence="0.998672">
istart = ostop = 1 (9)
Vs E S \ {start, stop} :
is = X00 X Yn !tsi−1(si) tsn(s)
n=1 �s1,...,sn�ESn i=1
X= is/ts/(s) (10)
s/ES
os = X00 X ts(s1) n !tsi−1(si)
n=1 �s1,...,sn�ESn Y
i=2
X= ts(s1)os/ (11)
s/ES
</equation>
<bodyText confidence="0.998296166666667">
This amounts to two linear systems given the tran-
sition probabilities t, where the variables are i• and
o•, respectively. In each system there are |S |vari-
ables and |S |equations. Once solved, expected
counts of transition and emission features under q0
are straightforward:
</bodyText>
<equation confidence="0.970280333333333">
Eq0 [IS transit 1
s * s ] = ists(s1)os/
Eq0[s emit * x] = ises(x)os
</equation>
<bodyText confidence="0.9870886">
Given i and o, Eq0 can be computed for other fea-
tures in the model in a similar way, provided they
correspond to contiguous substructures. For exam-
ple, a feature f627 that counts occurrences of “Si =
s and Xi+3 = x” has expected value Eq0[f627] =
</bodyText>
<equation confidence="0.995901">
X ists(s1)ts/(s11)ts//(s111)es///(x)os/// (12)
s/,s//,s///ES
</equation>
<bodyText confidence="0.9150136">
Non-contiguous substructure features with “gaps”
require summing over paths between any pair of
states. This is straightforward (we omit it for space),
but of course using such features (while interesting)
would complicate inference in decoding.
4It may be helpful to think of i as forward probabilities, but
for the observation set �* rather than a particular observation
y. o are like backward probabilities. Note that, because some
counted prefixes are prefixes of others, i can be &gt; 1; similarly
for o.
</bodyText>
<equation confidence="0.98036575">
fj(˜xi,˜yi) (7)
i=1
q0(s, x) = Yk
i=1
</equation>
<page confidence="0.981896">
754
</page>
<subsectionHeader confidence="0.714089">
4.1.2 Expectations under a PCFG
</subsectionHeader>
<bodyText confidence="0.999810333333333">
In general, the expectations for a PCFG require
solving a quadratic system of equations. The anal-
ogy this time is to inside and outside probabilities.
Let the PCFG have nonterminal set N, start symbol
S E N, terminal alphabet E, and rules of the form
A —* B C and A —* x. (We assume Chomsky nor-
mal form for clarity; the generalization is straight-
forward.) Let rA(B C) and rA(x) denote the proba-
bilities of nonterminal A rewriting to child sequence
</bodyText>
<equation confidence="0.9988891">
B C or x, respectively. Then VA E N:
�oA = � oBiC[rB(A C) + rB(C A)]
BEN CEN
( 1 if A = S
+ 1 0 otherwise
�iA = � rA(B C)iBiC + � rA(x)ix
BEN CEN x
�ox = oArA(x), Vx E E
AEN
ix = 1, Vx E E
</equation>
<bodyText confidence="0.999987666666667">
In most practical applications, the PCFG will be
“tight” (Booth and Thompson, 1973; Chi and Ge-
man, 1998). Informally, this means that the proba-
bility of a derivation rooted in S failing to terminate
is zero. If that is the case, then iA = 1 for all A E N,
and the system becomes linear (see also Corazza
and Satta, 2006).5 If tightness is not guaranteed,
iterative propagation of weights, following Stolcke
(1995), works well in our experience for solving the
quadratic system, and converges quickly.
As in the HMM case, expected counts of arbitrary
contiguous tree substructures can be computed as
products of probabilities of rules appearing within
the structure, factoring in the o value of the struc-
ture’s root and the i values of the structure’s leaves.
</bodyText>
<subsectionHeader confidence="0.964141">
4.2 Optimization
</subsectionHeader>
<bodyText confidence="0.948819">
To carry out M-estimation, we minimize the func-
tion `(w) in Eq. 3. To apply gradient de-
scent or a quasi-Newton numerical optimization
method,6 it suffices to specify the fixed quantities
</bodyText>
<footnote confidence="0.99942075">
5The same is true for HMMs: if the probability of non-
termination is zero, then for all s E S, o3 = 1.
6We use L-BFGS (Liu and Nocedal, 1989) as implemented
in the R language’s optim function.
</footnote>
<equation confidence="0.81465125">
f(xi, yi) (for all i E {1, 2, ..., n}) and the vector
Eq0(X,Y )[f(X, Y )]. The gradient is:7
e−wTf(xi,yi)fj(xi, yi) + Eq0[fj]
(13)
</equation>
<bodyText confidence="0.999867333333333">
The Hessian (matrix of second derivatives) can also
be computed with relative ease, though the space re-
quirement could become prohibitive. For problems
where m is relatively small, this would allow the use
of second-order optimization methods that are likely
to converge in fewer iterations.
It is easy to see that Eq. 3 is convex in w. There-
fore, convergence to a global optimum is guaranteed
and does not depend on the initializing value of w.
</bodyText>
<subsectionHeader confidence="0.991925">
4.3 Regularization
</subsectionHeader>
<bodyText confidence="0.99996664">
Regularization is a technique from pattern recogni-
tion that aims to keep parameters (like w) from over-
fitting the training data. It is crucial to the perfor-
mance of most statistical learning algorithms, and
our experiments show it has a major effect on the
success of the M-estimator. Here we use a quadratic
regularizer, minimizing `(w) + (wTw)/2c. Note
that this is also convex and differentiable if c &gt; 0.
The value of c can be chosen using a tuning dataset.
This regularizer aims to keep each coordinate of w
close to zero.
In the M-estimator, regularization is particularly
important when the expectation of some feature fj,
Eq0(X,Y )[fj(X, Y )] is equal to zero. This can hap-
pen either due to sampling error (fj simply failed
to appear with a positive value in the finite sample)
or because q0 assigns zero probability mass to any
x E X, y E � where fj(x, y) =� 0. Without regular-
ization, the weight wj will tend toward +oc, but the
quadratic penalty term will prevent that undesirable
tendency. Just as the addition of a quadratic regular-
izer to likelihood can be interpreted as a zero-mean
Gaussian prior on w (Chen and Rosenfeld, 2000), it
can be so-interpreted here. The regularized objective
is analogous to maximum a posteriori estimation.
</bodyText>
<sectionHeader confidence="0.982813" genericHeader="method">
5 Shallow Parsing
</sectionHeader>
<bodyText confidence="0.99783">
We compared M-estimation to a hidden Markov
model and other training methods on English noun
</bodyText>
<footnote confidence="0.913429">
7Taking the limit as n --+ oo and setting equal to zero, we
have the basis for a proof that f(w) is statistically consistent.
</footnote>
<equation confidence="0.867418">
∂` n
∂wj = � i=1
</equation>
<page confidence="0.93235">
755
</page>
<figure confidence="0.618475">
HMM CRF MEMM PL M-est.
</figure>
<figureCaption confidence="0.993868">
Figure 1: Wall time (hours:minutes) of training the
</figureCaption>
<bodyText confidence="0.990714641025641">
HMM and 100 L-BFGS iterations for each of the
extended-feature models on a 2.2 GHz Sun Opteron
with 8GB RAM. See discussion in text for details.
phrase (NP) chunking. The dataset comes from
the Conference on Natural Language Learning
(CoNLL) 2000 shallow parsing shared task (Tjong
Kim Sang and Buchholz, 2000); we apply the model
to NP chunking only. About 900 sentences were re-
served for tuning regularization parameters.
Baseline/q0 In this experiment, the simple base-
line is a second-order HMM. The states correspond
to {B, I, O} labels, denoting the beginning, inside,
and outside of noun phrases. Each state emits a
tag and a word (independent of each other given the
state). We replaced the first occurrence of every tag
and of every word in the training data with an OOV
symbol, giving a fixed tag vocabulary of 46 and a
fixed word vocabulary of 9,014. Transition distribu-
tions were estimated using MLE, and tag- and word-
emission distributions were estimated using add-1
smoothing. The HMM had 27,213 parameters. This
HMM achieves 86.3% F1-measure on the develop-
ment dataset (slightly better than the lowest-scoring
of the CoNLL-2000 systems). Heavier or weaker
smoothing (an order of magnitude difference in add-
A) of the emission distributions had very little effect.
Note that HMM training time is negligible (roughly
2 seconds); it requires counting events, smoothing
the counts, and normalizing.
Extended Feature Set Sha and Pereira (2003) ap-
plied a conditional random field to the NP chunk-
ing task, achieving excellent results. To improve the
performance of the HMM and test different estima-
tion methods, we use Sha and Pereira’s feature tem-
plates, which include subsequences of labels, tags,
and words of different lengths and offsets. Here,
we use only features observed to occur at least once
in the training data, accounting (in addition to our
OOV treatment) for the slight drop in performance
</bodyText>
<table confidence="0.999430416666667">
prec. recall F1
HMM features:
HMM 85.60 88.68 87.11
CRF 90.40 89.56 89.98
PL 80.31 81.37 80.84
MEMM 86.03 88.62 87.31
M-est. 85.57 88.65 87.08
extended features:
CRF 94.04 93.68 93.86
PL 91.88 91.79 91.83
MEMM 90.89 92.15 91.51
M-est. 88.88 90.42 89.64
</table>
<tableCaption confidence="0.997741">
Table 1: NP chunking accuracy on test data us-
</tableCaption>
<bodyText confidence="0.977500757575758">
ing different training methods. The effects of dis-
criminative training (CRF) and extended feature sets
(lower section) are more than additive.
compared to what Sha and Pereira report. There are
630,862 such features.
Using the original HMM feature set and the ex-
tended feature set, we trained four models that can
use arbitrary features: conditional random fields
(a near-replication of Sha and Pereira, 2003), maxi-
mum entropy Markov models (MEMMs; McCal-
lum et al., 2000), pseudolikelihood (Besag, 1975;
see Toutanova et al., 2003, for a tagging applica-
tion), and our M-estimator with the HMM as q0.
CRFs and MEMMs are discriminatively-trained to
maximize conditional likelihood (the former is pa-
rameterized using a sequence-normalized log-linear
model, the latter using a locally-normalized log-
linear model). Pseudolikelihood is a consistent esti-
mator for the joint likelihood, like our M-estimator;
its objective function is a sum of log probabilities.
In each case, we trained seven models for
each feature set with quadratic regularizers c E
[10−1,10], spaced at equal intervals in the log-scale,
plus an unregularized model (c = oc). As discussed
in §4.2, we trained using L-BFGS; training contin-
ued until relative improvement fell within machine
precision or 100 iterations, whichever came first.
After training, the value of c is chosen that maxi-
mizes F1 accuracy on the tuning set.
Runtime Fig. 1 compares the wall time of
carefully-timed training runs on a dedicated server.
Note that Dyna, a high-level programming language,
was used for dynamic programming (in the CRF)
</bodyText>
<figure confidence="0.799556">
2 sec.
64:18
9:35
3:40 1:04
</figure>
<page confidence="0.996721">
756
</page>
<bodyText confidence="0.999975875">
and summations (MEMM and pseudolikelihood).
The runtime overhead incurred by using Dyna is es-
timated as a slow-down factor of 3–5 against a hand-
tuned implementation (Eisner et al., 2005), though
the slow-down factor is almost certainly less for the
MEMM and pseudolikelihood. All training (except
the HMM, of course) was done using the R language
implementation of L-BFGS. In our implementation,
the M-estimator trained substantially faster than the
other methods. Of the 64 minutes required to train
the M-estimator, 6 minutes were spent precomput-
ing Ee0(X Y )[f(X, Y )] (this need not be repeated if
the regularization settings are altered).
Accuracy Tab. 1 shows how NP chunking accu-
racy compares among the models. With HMM
features, the M-estimator is about the same as the
HMM and MEMM (better than PL and worse than
the CRF). With extended features, the M-estimator
lags behind the slower methods, but performs about
the same as the HMM-featured CRF (2.5–3 points
over the HMM). The full-featured CRF improves
performance by another 4 points. Performance as
a function of training set size is plotted in Fig. 2;
the different methods behave relatively similarly as
the training data are reduced. Fig. 3 plots accuracy
(on tuning data) against training time, for a vari-
ety of training dataset sizes and regularizaton set-
tings, under different training methods. This illus-
trates the training-time/accuracy tradeoff: the M-
estimator, when well-regularized, is considerably
faster than the other methods, at the expense of ac-
curacy. This experiment gives some insight into the
relative importance of extended features versus es-
timation methods. The M-estimated model is, like
the maximum likelihood-estimated HMM, a gener-
ative model. Unlike the HMM, it uses a much larger
set of features–the same features that the discrimina-
tive models use. Our result supports the claim that
good features are necessary for state-of-the-art per-
formance, but so is good training.
</bodyText>
<subsectionHeader confidence="0.998606">
5.1 Effect of the Base Distribution
</subsectionHeader>
<bodyText confidence="0.9463342">
We now turn to the question of the base distribution
q0: how accurate does it need to be? Given that the
M-estimator is consistent, it should be clear that, in
the limit and assuming that our model family p is
correct, q0 should not matter (except in its support).
</bodyText>
<table confidence="0.993917">
q0 selection prec. recall F1
HMM F1, prec. 88.88 90.42 89.64
l.u. F1 72.91 57.56 64.33
prec. 84.40 37.68 52.10
emp. F1 84.38 89.43 86.83
</table>
<tableCaption confidence="0.965907">
Table 2: NP chunking accuracy on test data using
</tableCaption>
<bodyText confidence="0.9748842">
different base models for the M-estimator. The “se-
lection” column shows which accuracy measure was
optimized when selecting the hyperparameter c.
In NLP, we deal with finite datasets and imperfect
models, so q0 may have practical importance.
We next consider an alternative q0 that is far less
powerful; in fact, it is uninformative about the vari-
able to be predicted. Let x be a sequence of words,
t be a sequence of part-of-speech tags, and y be a
sequence of {B, I, O}-labels. The model is:
</bodyText>
<equation confidence="0.949268571428571">
� �
X
.u. def 1 1
q0l (x, t, y) = n puni (xi)puni (ti)
Ny�−1 Ny|X|
i=1
(14)
</equation>
<bodyText confidence="0.98275276">
where Ny is the number of labels (including stop)
that can follow y (3 for O and y0 = start, 4 for
B and I). puni are the tag and word unigram distri-
butions, estimated using MLE with add-1 smooth-
ing. This model ignores temporal effects. On its
own, this model achieves 0% precision and recall,
because it labels every word O (the most likely label
sequence is OX). We call this model l.u. (“locally
uniform”).
Tab. 2 shows that, while an M-estimate that uses
ql�u�
0 is not nearly as accurate as the one based on
an HMM, the M-estimator did manage to improve
considerably over ql�u�
0 . So the M-estimator is far
better than nothing, and in this case, tuning c to
maximize precision (rather than F1) led to an M-
estimated model with precision competitive with the
HMM. We point this out because, in applications in-
volving very large corpora, a model with good preci-
sion may be useful even if its coverage is mediocre.
Another question about q0 is whether it should
take into account all possible values of the input
variables (here, x and t), or only those seen in train-
ing. Consider the following model:
</bodyText>
<equation confidence="0.572661">
qemp
0 (x, t, y) def = q0(y  |x, t)�p(x, t) (15)
</equation>
<bodyText confidence="0.751063">
Here we use the empirical distribution over tag/word
</bodyText>
<page confidence="0.979851">
757
</page>
<figure confidence="0.975595666666667">
Fi
0 2000 4000 6000 8000 10000
training set size
</figure>
<figureCaption confidence="0.896232666666667">
Figure 2: Learning curves for different estimators;
all of these estimators except the HMM use the ex-
tended feature set.
</figureCaption>
<subsectionHeader confidence="0.949374">
5.2 Input-Only Features
</subsectionHeader>
<bodyText confidence="0.99957752">
We present briefly one negative result. Noting that
the M-estimator is a modeling technique that esti-
mates a distribution over both input and output vari-
ables (i.e., a generative model), we wanted a way
to make the objective more discriminative while still
maintaining the computational property that infer-
ence (of any kind) not be required during the inner
loop of iterative training.
The idea is to reduce the predictive burden on
the feature weights for f. When designing a CRF,
features that do not depend on the output variable
(here, y) are unnecessary. They cannot distinguish
between competing labelings for an input, and so
their weights will be set to zero during conditional
estimation. The feature vector function in Sha and
Pereira’s chunking model does not include such
features. In M-estimation, however, adding such
“input-only” features might permit better modeling
of the data and, more importantly, use the origi-
nal features primarily for the discriminative task of
modeling y given the input.
Adding unigram, bigram, and trigram features
to f for M-estimation resulted in a very small de-
crease in performance: selecting for F1, this model
achieves 89.33 F1 on test data.
</bodyText>
<figure confidence="0.996730678571428">
100
95
90
85
80
75
70
CRF
PL
MEMM
M-est.
HMM
Fl
100
95
90
75
70
65
85
80
HMM
M-est.
MEMM
PL
CRF
0 1 10 100 1000 10000 100000 1000000
training time (seconds)
</figure>
<figureCaption confidence="0.999734">
Figure 3: Accuracy (tuning data) vs. training time.
</figureCaption>
<bodyText confidence="0.964761105263158">
The M-estimator trains notably faster. The points
in a given curve correspond to different regulariza-
tion strengths (c); M-estimation is more damaged by
weak than strong regularization.
sequences, and the HMM to define the distri-
bution over label sequences. The expectations
Eqemp
0 (X)[f(X)] can be computed using dynamic
programming over the training data (recall that this
only needs to be done once, cf. the CRF). Strictly
speaking, q�mp
0 assigns probability zero to any se-
quence not seen in training, but we can ignore the
p marginal at decoding time. As shown in Tab. 2,
this model slightly improves recall over the HMM,
but damages precision; the gains of M-estimation
seen with the HMM as q0, are not reproduced. From
these experiments, we conclude that the M-estimator
might perform considerably better, given a better q0.
</bodyText>
<sectionHeader confidence="0.999843" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999980210526316">
M-estimation fills a gap in the plethora of train-
ing techniques that are available for NLP mod-
els today: it permits arbitrary features (like so-
called conditional “maximum entropy” models such
as CRFs) but estimates a generative model (permit-
ting, among other things, classification on input vari-
ables and meaningful combination with other mod-
els). It is similar in spirit to pseudolikelihood (Be-
sag, 1975), to which it compares favorably on train-
ing runtime and unfavorably on accuracy.
Further, since no inference is required during
training, any features really are permitted, so long
as their expected values can be estimated under the
base model q0. Indeed, M-estimation is consider-
ably easier to implement than conditional estima-
tion. Both require feature counts from the train-
ing data; M-estimation replaces repeated calculation
and differentiation of normalizing constants with in-
ference or sampling (once) under a base model. So
</bodyText>
<page confidence="0.929249">
758
</page>
<bodyText confidence="0.788848830769231">
the M-estimator is much faster to train. References
Generative and discriminative models have been J. E. Besag. 1975. Statistical analysis of non-lattice data. The
compared and discussed a great deal (Ng and Jordan, Statistician, 24:179–195.
2002), including for NLP models (Johnson, 2001; T. L. Booth and R. A. Thompson. 1973. Applying probabil-
Klein and Manning, 2002). Sutton and McCallum ity measures to abstract languages. IEEE Transactions on
(2005) present approximate methods that keep a dis- Computers, 22(5):442–450.
criminative objective while avoiding full inference. S. Chen and R. Rosenfeld. 2000. A survey of smoothing tech-
We see M-estimation as a particularly promising niques for ME models. IEEE Transactions on Speech and
method in settings where performance depends on Audio Processing, 8(1):37–50.
high-dimensional, highly-correlated feature spaces, Z. Chi and S. Geman. 1998. Estimation of probabilis-
where the desired features “large,” making discrimi- tic context-free grammars. Computational Linguistics,
native training too time-consuming—a compelling 24(2):299–305.
example is machine translation. Further, in some A. Corazza and G. Satta. 2006. Cross-entropy and estimation
settings a locally-normalized conditional log-linear of probabilistic context-free grammars. In Proc. of HLT-
model (like an MEMM) may be difficult to design; NAACL.
our estimator avoids normalization altogether.8 The A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likeli-
M-estimator may also be useful as a tool in design- hood estimation from incomplete data via the EM algorithm.
ing and selecting feature combinations, since more Journal of the Royal Statistical Society B, 39:1–38.
trials can be run in less time. After selecting a fea- J. Eisner, E. Goldlust, and N. A. Smith. 2005. Compiling
ture set under M-estimation, discriminative training Comp Ling: Practical weighted dynamic programming and
can be applied on that set. The M-estimator might the Dyna language. In Proc. of HLT-EMNLP.
also serve as an initializer to discriminative mod- Y. Jeon and Y. Lin. 2006. An effective method for high-
els, perhaps reducing the number of times inference dimensional log-density ANOVA estimation, with applica-
must be performed—this could be particularly use- tion to nonparametric graphical model building. Statistical
ful in very-large data scenarios. In future work we Sinica, 16:353–374.
hope to explore the use of the M-estimator within M. Johnson. 2001. Joint and conditional estimation of tagging
hidden variable learning, such as the Expectation- and parsing models. In Proc. of ACL.
Maximization algorithm (Dempster et al., 1977). D. Klein and C. D. Manning. 2002. Conditional structure vs.
7 Conclusions conditional estimation in NLP models. In Proc. of EMNLP.
We have presented a new loss function for genera- J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
tively estimating the parameters of log-linear mod- random fields: Probabilistic models for segmenting and la-
els. The M-estimator is fast to train, requiring beling sequence data. In Proc. of ICML.
no repeated, expensive calculation of normalization D. C. Liu and J. Nocedal. 1989. On the limited memory BFGS
terms. It was shown to improve performance on method for large scale optimization. Math. Programming,
a shallow parsing task over a baseline (generative) 45:503–528.
HMM, but it is not competitive with the state-of- A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum
the-art. Our sequence modeling experiments support entropy Markov models for information extraction and seg-
the widely accepted claim that discriminative, rich- mentation. In Proc. of ICML.
feature modeling works as well as it does not just A. Ng and M. Jordan. 2002. On discriminative vs. generative
because of rich features in the model, but also be- classifiers: A comparison of logistic regression and naive
cause of discriminative training. Our technique fills Bayes. In NIPS 14.
an important gap in the spectrum of learning meth- J. A. O’Sullivan. 1998. Alternating minimization algo-
ods for NLP models and shows promise for applica- rithms: from Blahut-Armijo to Expectation-Maximization.
tion when discriminative methods are too expensive. In A. Vardy, editor, Codes, Curves, and Signals: Common
Threads in Communications, pages 173–192. Kluwer.
R. Rosenfeld. 1997. A whole sentence maximum entropy lan-
guage model. In Proc. of ASRU.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of HLT-NAACL.
A. Stolcke. 1995. An efficient probabilistic context-free pars-
ing algorithm that computes prefix probabilities. Computa-
tional Linguistics, 21(2):165–201.
C. Sutton and A. McCallum. 2005. Piecewise training of undi-
rected models. In Proc. of UAI.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduction
to the CoNLL-2000 shared task: Chunking. In Proc. of
CoNLL.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic depen-
dency network. In Proc. of HLT-NAACL.
A. W. van der Vaart. 1998. Asymptotic Statistics. Cambridge
University Press.
8Note that MEMMs also require local partition functions—
which may be expensive—to be computed at decoding time.
759
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.879275">
<title confidence="0.999923">Efficient M-Estimation of Log-Linear Structure</title>
<author confidence="0.999873">A Smith L Vail D Lafferty</author>
<affiliation confidence="0.9999405">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.896447">Pittsburgh, PA 15213 USA</address>
<abstract confidence="0.9989135">We describe a new loss function, due to Jeon and Lin (2006), for estimating structured log-linear models on arbitrary features. The loss function can be seen as a (generative) alternative to maximum likelihood estimation with an interesting information-theoretic interpretation, and it is statistically consistent. It is substantially faster than maximum (conditional) likelihood estimation of conditional random fields (Lafferty et al., 2001; an order of magnitude or more). We compare its performance and training time to an HMM, a CRF, an MEMM, and pseudolikelihood on a shallow parsing task. These experiments help tease apart the contributions of rich features and discriminative training, which are shown to be more than additive.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>