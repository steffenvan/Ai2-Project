<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.082563">
<title confidence="0.981917">
Multilingual Deep Lexical Acquisition for HPSGs via Supertagging
</title>
<author confidence="0.992902">
Phil Blunsom and Timothy Baldwin
</author>
<affiliation confidence="0.998361">
Computer Science and Software Engineering
University of Melbourne, Victoria 3010 Australia
</affiliation>
<email confidence="0.994739">
{pcbl,tim}@csse.unimelb.edu.au
</email>
<sectionHeader confidence="0.993789" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999770333333333">
We propose a conditional random field-
based method for supertagging, and ap-
ply it to the task of learning new lexi-
cal items for HPSG-based precision gram-
mars of English and Japanese. Us-
ing a pseudo-likelihood approximation we
are able to scale our model to hun-
dreds of supertags and tens-of-thousands
of training sentences. We show that
it is possible to achieve start-of-the-art
results for both languages using maxi-
mally language-independent lexical fea-
tures. Further, we explore the performance
of the models at the type- and token-level,
demonstrating their superior performance
when compared to a unigram-based base-
line and a transformation-based learning
approach.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999155">
Over recent years, there has been a resurgence of
interest in the use of precision grammars in NLP
tasks, due to advances in parsing algorithm de-
velopment, grammar development tools and raw
computational power (Oepen et al., 2002b). Pre-
cision grammars are defined as implemented
grammars of natural language which capture fine-
grained linguistic distinctions, and are generative
in the sense of distinguishing between grammat-
ical and ungrammatical inputs (or at least have
some in-built notion of linguistic “markedness”).
Additional characteristics of precision grammars
are that they are frequently bidirectional, and out-
put a rich semantic abstraction for each span-
ning parse of the input string. Examples include
DELPH-IN grammars such as the English Resource
Grammar (Flickinger, 2002; Uszkoreit, 2002), the
various PARGRAM grammars (Butt et al., 1999),
and the Edinburgh CCG parser (Bos et al., 2004).
Due to their linguistic complexity, precision
grammars are generally hand-constructed and thus
restricted in size and coverage. Attempts to
(semi-)automate the process of expanding the cov-
erage of precision grammars have focused on ei-
ther: (a) constructional coverage, e.g. in the form
of error mining for constructional expansion (van
Noord, 2004; Zhang and Kordoni, 2006), or relax-
ation of lexico-grammatical constraints to support
partial and/or robust parsing (Riezler et al., 2002);
or (b) lexical coverage, e.g. in bootstrapping from
a pre-existing grammar and lexicon to learn new
lexical items (Baldwin, 2005a). Our particular in-
terest in this paper is in the latter of these two,
that is the development of methods for automati-
cally expanding the lexical coverage of an existing
precision grammar, or more broadly deep lexical
acquisition (DLA hereafter). In this, we follow
Baldwin (2005a) in assuming a semi-mature pre-
cision grammar with a fixed inventory of lexical
types, based on which we learn new lexical items.
For the purposes of this paper, we focus specif-
ically on supertagging as the mechanism for hy-
pothesising new lexical items.
Supertagging can be defined as the process of
applying a sequential tagger to the task of predict-
ing the lexical type(s) associated with each word
in an input string, relative to a given grammar. It
was first introduced as a means of reducing parser
ambiguity by Bangalore and Joshi (1999) in the
context of the LTAG formalism, and has since been
applied in a similar context within the CCG for-
malism (Clark and Curran, 2004). In both of these
cases, supertagging provides the means to perform
a beam search over the plausible lexical items for
a given string context, and ideally reduces pars-
ing complexity without sacrificing parser accu-
racy. An alternate application of supertagging is
in DLA, in postulating novel lexical items with
which to populate the lexicon of a given gram-
mar to boost parser coverage. This can take place
</bodyText>
<page confidence="0.978121">
164
</page>
<note confidence="0.8578525">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 164–171,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999168583333333">
either: (a) off-line for the purposes of rounding
out the coverage of a static lexicon, in which case
we are generally interested in globally maximising
precision over a given corpus and hence predict-
ing the single most plausible lexical type for each
word token (off-line DLA: Baldwin (2005b)); or
(b) on the fly for a given input string to temporar-
ily expand lexical coverage and achieve a spanning
parse, in which case we are interested in maximis-
ing recall by producing a (possibly weighted) list
of lexical item hypotheses to run past the grammar
(on-line DLA: Zhang and Kordoni (2005)). Our
immediate interest in this paper is in the first of
these tasks, although we would ideally like to de-
velop an off-line method which is trivially portable
to the second task of on-line DLA.
In this research, we focus particularly on
the Grammar Matrix-based DELPH-IN family of
grammars (Bender et al., 2002), which includes
grammars of English, Japanese, Norwegian, Mod-
ern Greek, Portuguese and Korean. The Gram-
mar Matrix is a framework for streamlining and
standardising HPSG-based multilingual grammar
development. One property of Grammar Matrix-
based grammars is that they are strongly lexical-
ist and adhere to a highly constrained lexicon-
grammar interface via a unique (terminal) lexi-
cal type for each lexical item. As such, lexical
item creation in any of the Grammar Matrix-based
grammars, irrespective of language, consists pre-
dominantly of predicting the appropriate lexical
type for each lexical item, relative to the lexical
hierarchy for the corresponding grammar. In this
same spirit of standardisation and multilingual-
ity, the aim of this research is to develop max-
imally language-independent supertagging meth-
ods which can be applied to any Grammar Matrix-
based grammar with the minimum of effort. Es-
sentially, we hope to provide the grammar engi-
neer with the means to semi-automatically popu-
late the lexicon of a semi-mature grammar, hence
accelerating the pace of lexicon development and
producing a resource of sufficient coverage to be
practically useful in NLP tasks.
The contributions of this paper are the devel-
opment of a pseudo-likelihood conditional ran-
dom field-based method of supertagging, which
we then apply to the task of off-line DLA for
grammars of both English and Japanese with only
minor language-specific adaptation. We show the
supertagger to outperform previously-proposed
supertagger-based DLA methods.
The remainder of this paper is structured as
follows. Section 2 outlines past work relative
to this research, and Section 3 reviews the re-
sources used in our supertagging experiments.
Section 4 outlines the proposed supertagger model
and reviews previous research on supertagger-
based DLA. Section 5 then outlines the set-up and
results of our evaluation.
</bodyText>
<sectionHeader confidence="0.910244" genericHeader="method">
2 Past Research
</sectionHeader>
<bodyText confidence="0.99993025">
According to Baldwin (2005b), research on DLA
falls into the two categories of in vitro methods,
where we leverage a secondary language resource
to generate an abstraction of the words we hope to
learn lexical items for, and in vivo methods, where
the target resource that we are hoping to perform
DLA relative to is used directly to perform DLA.
Supertagging is an instance of in vivo DLA, as it
operates directly over data tagged with the lexical
type system for the precision grammar of interest.
Research on supertagging which is relevant to
this paper includes the work of Baldwin (2005b) in
training a transformation-based learner over data
tagged with ERG lexical types. We discuss this
method in detail in Section 5.2 and replicate this
method over our English data set for direct com-
parability with this previous research.
As mentioned above, other work on supertag-
ging has tended to view it as a means of driving
a beam search to prune the parser search space
(Bangalore and Joshi, 1999; Clark and Curran,
2004). In supertagging, token-level annotations
(gold-standard, automatically-generated or other-
wise) for a given DLR are used to train a se-
quential tagger, akin to training a POS tagger over
POS-tagged data taken from the Penn Treebank.
One related in vivo approach to DLA targeted
specifically at precision grammars is that of Fou-
vry (2003). Fouvry uses the grammar to guide
the process of learning lexical items for unknown
words, by generating underspecified lexical items
for all unknown words and parsing with them.
Syntactico-semantic interaction between unknown
words and pre-existing lexical items during pars-
ing provides insight into the nature of each un-
known word. By combining such fragments of in-
formation, it is possible to incrementally arrive at
a consolidated lexical entry for that word. That is,
the precision grammar itself drives the incremen-
tal learning process within a parsing context.
</bodyText>
<page confidence="0.998027">
165
</page>
<bodyText confidence="0.99999648">
An alternate approach is to compile out a set of
word templates for each lexical type (with the im-
portant qualification that they do not rely on pre-
processing of any form), and check for corpus oc-
currences of an unknown word in such contexts.
That is, the morphological, syntactic and/or se-
mantic predictions implicit in each lexical type are
made explicit in the form of templates which rep-
resent distinguishing lexical contexts of that lexi-
cal type. This approach has been shown to be par-
ticularly effective over web data, where the sheer
size of the data precludes the possibility of linguis-
tic preprocessing but at the same time ameliorates
the effects of data sparseness inherent in any lexi-
calised DLA approach (Lapata and Keller, 2004).
Other work on DLA (e.g. Korhonen (2002),
Joanis and Stevenson (2003), Baldwin (2005a))
has tended to take an in vitro DLA approach, in
extrapolating away from a DLR to corpus or web
data, and analysing occurrences of words through
the conduit of an external resource (e.g. a sec-
ondary parser or POS tagger). In vitro DLA can
also take the form of resource translation, in map-
ping one DLR onto another to arrive at the lexical
information in the desired format.
</bodyText>
<sectionHeader confidence="0.963863" genericHeader="method">
3 Task and Resources
</sectionHeader>
<bodyText confidence="0.999930842105263">
In this section, we outline the resources targeted
in this research, namely the English Resource
Grammar (ERG: Flickinger (2002), Copestake
and Flickinger (2000)) and the JACY grammar of
Japanese (Siegel and Bender, 2002). Note that our
choice of the ERG and JACY as testbeds for exper-
imentation in this paper is somewhat arbitrary, and
that we could equally run experiments over any
Grammar Matrix-based grammar for which there
is treebank data.
Both the ERG and JACY are implemented
open-source broad-coverage precision Head-
driven Phrase Structure Grammars (HPSGs:
Pollard and Sag (1994)). A lexical item in each
of the grammars consists of a unique identifier,
a lexical type (a leaf type of a type hierarchy),
an orthography, and a semantic relation. For
example, in the English grammar, the lexical item
for the noun dog is simply:
</bodyText>
<equation confidence="0.979654">
dog_n1 := n_-_c_le &amp;
[ STEM &lt; &amp;quot;dog&amp;quot; &gt;,
SYNSEM [ LKEYS.KEYREL.PRED &amp;quot;_dog_n_1_rel&amp;quot; ] ].
</equation>
<bodyText confidence="0.999976319148936">
in which the lexical type of n - c le encodes
the fact that dog is a noun which does not sub-
categorise for any other constituents and which is
countable, &amp;quot;dog&amp;quot; specifies the lexical stem, and
&amp;quot; dog n 1 rel&amp;quot; introduces an ad hoc predicate
name for the lexical item to use in constructing a
semantic representation. In the context of the ERG
and JACY, DLA equates to learning the range of
lexical types a given lexeme occurs with, and gen-
erating a single lexical item for each.
Recent development of the ERG and JACY has
been tightly coupled with treebank annotation, and
all major versions of both grammars are deployed
over a common set of dynamically-updateable
treebank data to help empirically trace the evo-
lution of the grammar and retrain parse selection
models (Oepen et al., 2002a; Bond et al., 2004).
This serves as a source of training and test data for
building our supertaggers, as detailed in Table 1.
In translating our treebank data into a form that
can be understood by a supertagger, multiword ex-
pressions (MWEs) pose a slight problem. Both the
ERG and JACY include multiword lexical items,
which can either be strictly continuous (e.g. hot
line) or optionally discontinuous (e.g. transitive
English verb particle constructions, such as pick
up as in Kim picked the book up).
Strictly continuous lexical items are described
by way of a single whitespace-delimited lexical
stem (e.g. STEM &lt; &amp;quot;hot line&amp;quot; &gt;). When
faced with instances of this lexical item, the su-
pertagger must perform two roles: (1) predict that
the words hot and line combine together to form
a single lexeme, and (2) predict the lexical type
associated with the lexeme. This is performed
in a single step through the introduction of the
ditto lexical type, which indicates that the cur-
rent word combines (possibly recursively) with the
left-adjacent word to form a single lexeme, and
shares the same lexical type. This tagging conven-
tion is based on that used, e.g., in the CLAWS7
part-of-speech tagset.
Optionally discontinuous lexical items are less
of a concern, as selection of each of the discontin-
uous “components” is done via lexical types. E.g.
in the case of pick up, the lexical entry looks as
follows:
</bodyText>
<equation confidence="0.99138075">
pick_up_v1 := v_p-np_le &amp;
[ STEM &lt; &amp;quot;pick&amp;quot; &gt;,
SYNSEM [ LKEYS [ --COMPKEY _up_p_sel_rel,
KEYREL.PRED &amp;quot;_pick_v_up_rel&amp;quot; ] ] ].
</equation>
<bodyText confidence="0.99971025">
in which &amp;quot;pick&amp;quot; selects for the up p sel rel
predicate, which in turn is associated with the stem
&amp;quot;up&amp;quot; and lexical type p prtcl le. In terms of
lexical tag mark-up, we can treat these as separate
</bodyText>
<page confidence="0.986139">
166
</page>
<table confidence="0.999749266666667">
ERG JACY
GRAMMAR
Language English Japanese
Lexemes 16,498 41,559
Lexical items 26,297 47,997
Lexical types 915 484
Strictly continuous MWEs 2,581 422
Optionally discontinuous MWEs 699 0
Proportion of lexemes with more than one lexical item 0.29 0.14
Average lexical items per lexeme 1.59 1.16
TREEBANK
Training sentences 20,000 40,000
Training words 215,015 393,668
Test sentences 1,013 1,095
Test words 10,781 10,669
</table>
<tableCaption confidence="0.999939">
Table 1. Make-up of the English Resource Grammar (ERG) and JACY grammars and treebanks
</tableCaption>
<bodyText confidence="0.9995414375">
tags and leave the supertagger to model the mutual
inter-dependence between these lexical types.
For detailed statistics of the composition of the
two grammars, see Table 1.
For morphological processing (including to-
kenisation and lemmatisation), we use the pre-
existing machinery provided with each of the
grammars. In the case of the ERG, this consists
of a finite state machine which feeds into lexical
rules; in the case of JACY, segmentation and lem-
matisation is based on a combination of ChaSen
(Matsumoto et al., 2003) and lexical rules. That
is, we are able to assume that the Japanese data
has been pre-segmented in a form compatible with
JACY, as we are able to replicate the automatic
pre-processing that it uses.
</bodyText>
<sectionHeader confidence="0.996587" genericHeader="method">
4 Suppertagging
</sectionHeader>
<bodyText confidence="0.999974375">
The DLA strategy we adopt in this research is
based on supertagging, which is a simple in-
stance of sequential tagging with a larger, more
linguistically-diverse tag set than is conventionally
the case, e.g., with part-of-speech tagging. Below,
we describe the pseudo-likelihood CRF model we
base our supertagger on and outline the feature
space for the two grammars.
</bodyText>
<subsectionHeader confidence="0.9838">
4.1 Pseudo-likelihood CRF-based
Supertagging
</subsectionHeader>
<bodyText confidence="0.999879875">
CRFs are undirected graphical models which de-
fine a conditional distribution over a label se-
quence given an observation sequence. Here we
use CRFs to model sequences of lexical types,
where each input word in a sentence is assigned
a single tag.
The joint probability density of a sequence la-
belling, (a vector of lexical types), given the in-
</bodyText>
<equation confidence="0.953424">
put sentence,, is given by:
(1)
</equation>
<bodyText confidence="0.999693448275862">
where we make a first order Markov assumption
over the label sequence. Here ranges over the
word indices of the input sentence (), ranges
over the model’s features, and are the
model parameters (weights for their correspond-
ing features). The feature functions are pre-
defined real-valued functions over the input sen-
tence coupled with the lexical type labels over ad-
jacent “times” (= sentence locations). These fea-
ture functions are unconstrained, and may repre-
sent overlapping and non-independent features of
the data. The distribution is globally normalised
by the partition function, , which sums out
the numerator in (1) for every possible labelling:
We use a linear chain CRF, which is encoded in
the feature functions of (1).
The parameters of the CRF are usually esti-
mated from a fully observed training sample, by
maximising the likelihood of these data. I.e.
, where
is the complete set of training data.
However, as calculating has complexity
quadratic in the number of labels, we need to ap-
proximate in order to scale our model to
hundreds of lexical types and tens-of-thousands
of training sentences. Here we use the pseudo-
likelihood approximation (Li, 1994) in which
the marginals for a node at time are calculated
with its neighbour nodes’ labels fixed to those ob-
</bodyText>
<page confidence="0.987507">
167
</page>
<table confidence="0.948725769230769">
FEATURE DESCRIPTION
WORD CONTEXT FEATURES
&amp; lexeme + label
&amp; word unigram + label
&amp; previous word unigram + label
&amp; next word unigram + label
&amp; &amp; previous word bigram + label
&amp; &amp; next word bigram + label
&amp; clique label pair
LEXICAL FEATURES
&amp; -gram prefix + label
&amp; -gram suffix + label
&amp; word contains element of character set + label
</table>
<tableCaption confidence="0.999696">
Table 2. Extracted feature types for the CRF model
</tableCaption>
<bodyText confidence="0.995987125">
served in the training data:
where is the lexical type label observed in the
training data andranges over the label set. This
approximation removes the need to calculate the
partition function, thus reducing the complexity to
be linear in the number of labels and training in-
stances.
Because maximum likelihood estimators for
log-linear models have a tendency to overfit the
training sample (Chen and Rosenfeld, 1999), we
define a prior distribution over the model param-
eters and derive a maximum a posteriori (MAP)
estimate, .
We use a zero-mean Gaussian prior, with the prob-
ability density function .
This yields a log-pseudo-likelihood objective
function of:
In order to train the model, we maximize (4).
While the log-pseudo-likelihood cannot be max-
imised for the parameters, , in closed form, it is
a convex function, and thus we resort to numerical
optimisation to find the globally optimal parame-
ters. We use L-BFGS, an iterative quasi-Newton
optimisation method, which performs well for
training log-linear models (Malouf, 2002; Sha and
Pereira, 2003). Each L-BFGS iteration requires
the objective value and its gradient with respect to
the model parameters.
As we cannot observe label values for the test
data we must use when decoding. The
Viterbi algorithm is used to find the maximum
posterior probability alignment for test sentences,
</bodyText>
<subsectionHeader confidence="0.969482">
4.2 CRF features
</subsectionHeader>
<bodyText confidence="0.999740206896552">
One of the strengths of the CRF model is that
it supports the use of a large number of non-
independent and overlapping features of the input
sentence. Table 2 lists the word context and lexi-
cal features used by the CRF model (shared across
both grammars).
Word context features were extracted from the
words and lexemes of the sentence to be labelled
combined with a proposed label. A clique label
pair feature was also used to model sequences of
lexical types.
For the lexical features, we generate a feature
for the unigram, bigram and trigram prefixes and
suffixes of each word (e.g. for bottles, we would
generate the prefixes b, bo and bot, and the suf-
fixes s, es and les); for words in the test data, we
generate a feature only if that feature-value is at-
tested in the training data. We additionally test
each word for the existence of one or more ele-
ments of a range of character sets . In the case
of English, we focus on five character sets: upper
case letters, lower case letters, numbers, punctua-
tion and hyphens. For the Japanese data, we em-
ploy six character sets: Roman letters, hiragana,
katakana, kanji, (Arabic) numerals and punctua-
tion. For example, “mouldy” would be
flagged as containing katakana character(s), kanji
character(s) and hiragana character(s) only. Note
that the only language-dependent component of
</bodyText>
<equation confidence="0.543876">
.
</equation>
<page confidence="0.977155">
168
</page>
<table confidence="0.9796474">
ERG JACY
ACC ACC PREC REC F-SCORE ACC ACC PREC REC F-SCORE
0.802 0.053 0.184 0.019 0.034 0.866 0.592 0.680 0.323 0.438
0.915 0.236 0.370 0.038 0.068 — — — — —
0.911 0.427 0.339 0.053 0.092 0.920 0.816 0.548 0.414 0.471
0.917 0.489 0.509 0.059 0.105 0.932 0.827 0.696 0.424 0.527
Baseline
FNTBL
CRF
CRF
</table>
<tableCaption confidence="0.999796">
Table 3. Results of supertagging for the ERG and JACY (best result in each column in bold)
</tableCaption>
<bodyText confidence="0.996923888888889">
the lexical features is the character sets, which
requires little or no specialist knowledge of the
language. Note also that for languages with in-
fixing, such as Tagalog, we may want to include
-gram infixes in addition to -gram prefixes and
suffixes. Here again, however, the decision about
what range of affixes is appropriate for a given lan-
guage requires only superficial knowledge of its
morphology.
</bodyText>
<sectionHeader confidence="0.997077" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999814481481482">
Evaluation is based on the treebank data associ-
ated with each grammar, and a random training–
test split of 20,000 training sentences and 1,013
test sentences in the case of the ERG, and 40,000
training sentences and 1,095 test sentences in the
case of the JACY. This split is fixed for all models
tested.
Given that the goal of this research is to ac-
quire novel lexical items, our primary focus is on
the performance of the different models at pre-
dicting the lexical type of any lexical items which
occur only in the test data (which may be either
novel lexemes or previously-seen lexemes occur-
ring with a novel lexical type). As such, we iden-
tify all unknown lexical items in the test data and
evaluate according to: token accuracy (the pro-
portion of unknown lexical items which are cor-
rectly tagged: ACC ); type precision (the propor-
tion of correctly hypothesised unknown lexical en-
tries: PREC); type recall (the proportion of gold-
standard unknown lexical entries for which we get
a correct prediction: REC); and type F-score (the
harmonic mean of type precision and type recall:
F-SCORE). We also measure the overall token ac-
curacy (ACC) across all words in the test data, ir-
respective of whether they represent known or un-
known lexical items.
</bodyText>
<subsectionHeader confidence="0.994341">
5.1 Baseline: Unigram Supertagger
</subsectionHeader>
<bodyText confidence="0.997524125">
As a baseline model, we use a simple unigram su-
pertagger trained based on maximum likelihood
estimation over the relevant training data, i.e. the
tag for each token instance of a given word
is predicted by:
In the instance that was not observed in the
training data, we back off to the majority lexical
type in the training data.
</bodyText>
<subsectionHeader confidence="0.997271">
5.2 Benchmark: fnTBL
</subsectionHeader>
<bodyText confidence="0.999980807692308">
In order to benchmark our results with the CRF
models, we reimplemented the supertagger model
proposed by Baldwin (2005b) which simply takes
FNTBL 1.1 (Ngai and Florian, 2001) off the
shelf and trains it over our particular training set.
FNTBL is a transformation-based learner that is
distributed with pre-optimised POS tagging mod-
ules for English and other European languages that
can be redeployed over the task of supertagging.
Following Baldwin (2005b), the only modifica-
tions we make to the default English POS tag-
ging methodology are: (1) to set the default lexical
types for singular common and proper nouns to
n - c le and n - pn le, respectively; and (2)
reduce the threshold score for lexical and context
transformation rules to 1. It is important to realise
that, unlike our proposed method, the English POS
tagger implementation in FNTBL has been fine-
tuned to the English POS task, and includes a rich
set of lexical templates specific to English.
Note that were only able to run FNTBL over the
English data, as encoding issues with the Japanese
proved insurmountable. We are thus only able to
compare results over the English, although this is
expected to be representative of the relative per-
formance of the methods.
</bodyText>
<subsectionHeader confidence="0.803394">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999797">
The results for the baseline, benchmark FNTBL
method for English and our proposed CRF-based
supertagger are presented in Table 3, for each of
the ERG and JACY. In order to gauge the impact
of the lexical features on the performance of our
CRF-based supertagger, we ran the supertagger
first without lexical features (CRF ) and then
with the lexical features (CRF ).
</bodyText>
<page confidence="0.997454">
169
</page>
<bodyText confidence="0.999972290322581">
The first finding of note is that the proposed
model surpasses both the baseline and FNTBL in
all cases. If we look to token accuracy for un-
known lexical types, the CRF is far and away the
superior method, a result which is somewhat di-
minished but still marked for type-level precision,
recall and F-score. Recall that for the purposes of
this paper, our primary interest is in how success-
fully we are able to learn new lexical items, and
in this sense the CRF appears to have a clear edge
over the other models. It is also important to re-
call that our results over both English and Japanese
have been achieved with only the bare minimum
of lexical feature engineering, whereas those of
FNTBL are highly optimised.
Comparing the results for the CRF with and
without lexical features (CRF ), the lexical
features appear to have a strong bearing on type
precision in particular, for both the ERG and
JACY.
Looking to the raw numbers, the type-level per-
formance for all methods is far from flattering.
However, it is entirely predictable that the over-
all token accuracy should be considerably higher
than the token accuracy for unknown lexical items.
A breakdown of type precision and recall for un-
known words across the major word classes for
English suggests that the CRF supertagger is
most adept at learning nominal and adjectival lex-
ical items (with an F-score of 0.671 and 0.628, re-
spectively), and has the greatest difficulties with
verbs and adverbs (with an F-score of 0.333 and
0.395, respectively). In the case of Japanese, con-
jugating adjectives and verbs present the least dif-
ficulty (with an F-score of 0.933 and 0.886, re-
spectively), and non-conjugating adjectives and
adverbs are considerably harder (with an F-score
of 0.396 and 0.474, respectively).
It is encouraging to note that type precision is
higher than type recall in all cases (a phenomenon
that is especially noticeable for the ERG), as this
means that while we are not producing the full in-
ventory of lexical items for a given lexeme, over
half of the lexical items that we produce are gen-
uine (with CRF ). This suggests that it should
be possible to present the grammar developer with
a relatively low-noise set of automatically learned
lexical items for them to manually curate and feed
into the lexicon proper.
One final point of interest is the ability of the
CRF to identify multiword expressions (MWEs).
There were no unknown multiword expressions
in either the English or Japanese data, such that
we can only evaluate the performance of the su-
pertagger at identifying known MWEs. In the case
of English, CRF identified strictly continuous
MWEs with an accuracy of 0.758, and optionally
discontinuous MWEs (i.e. verb particle construc-
tions) with an accuracy of 0.625. For Japanese, the
accuracy is considerably lower, at 0.536 for con-
tinuous MWEs (recalling that there were no op-
tionally discontinuous MWEs in JACY).
</bodyText>
<sectionHeader confidence="0.999176" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9995794">
In this paper we have explored a method for
learning new lexical items for HPSG-based pre-
cision grammars through supertagging. Our
pseudo-likelihood conditional random field-based
approach provides a principled way of learning
a supertagger from tens-of-thousands of training
sentences and with hundreds of possible tags.
We achieve start-of-the-art results for both
English and Japanese data sets with a largely
language-independent feature set. Our model also
achieves performance at the type- and token-level,
over different word classes and at multiword ex-
pression identification, superior to a probabilistic
baseline and a transformation based learning ap-
proach.
</bodyText>
<sectionHeader confidence="0.993803" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9992349">
We would like to thank Dan Flickinger and
Francis Bond for support and expert assistance
with the ERG and JACY, respectively, and the
three anonymous reviewers for their valuable in-
put on this research. The research in this paper
has been supported by the Australian Research
Council through Discovery Project grant number
DP0663879, and also NTT Communication Sci-
ence Laboratories, Nippon Telegraph and Tele-
phone Corporation
</bodyText>
<sectionHeader confidence="0.998644" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.937936166666667">
Timothy Baldwin. 2005a. Bootstrapping deep lexical re-
sources: Resources for courses. In Proc. of the ACL-
SIGLEX 2005 Workshop on Deep Lexical Acquisition,
pages 67–76, Ann Arbor, USA.
Timothy Baldwin. 2005b. General-purpose lexical acquisi-
tion: Procedures, questions and results. In Proc. of the
</reference>
<footnote confidence="0.948944666666667">
6th Meeting of the Pacific Association for Computational
Linguistics (PACLING 2005), pages 23–32, Tokyo, Japan.
(Invited Paper).
</footnote>
<page confidence="0.991111">
170
</page>
<reference confidence="0.995823418181818">
Srinivas Bangalore and Aravind K. Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational Lin-
guistics, 25(2):237–65.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar Matrix. An open-source starter-kit
for the rapid development of cross-linguistically consis-
tent broad-coverage precision grammar. In Proc. of the
Workshop on Grammar Engineering and Evaluation at the
19th International Conference on Computational Linguis-
tics, Taipei, Taiwan.
Francis Bond, Sanae Fujita, Chikara Hashimoto, Kaname
Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani,
Takaaki Tanaka, and Shigeaki Amano. 2004. The Hinoki
treebank: A treebank for text understanding. In Proc. of
the First International Joint Conference on Natural Lan-
guage Processing (IJCNLP-04), pages 554–9, Hainan Is-
land, China.
Johan Bos, Stephen Clark, Mark Steedman, James R. Cur-
ran, and Julia Hockenmaier. 2004. Wide-coverage se-
mantic representations from a CCG parser. In Proc. of the
20th International Conference on Computational Linguis-
tics (COLING 2004), pages 1240–7, Geneva, Switzerland.
Miriam Butt, Tracy Holloway King, Maria-Eugenia Nino,
and Frederique Segond. 1999. A Grammar Writer’s
Cookbook. CSLI Publications, Stanford, USA.
Stanley F. Chen and Ronald Rosenfeld. 1999. A sur-
vey of smoothing techniques for maximum entropy mod-
els. IEEE Transactions on Speech and Audio Processing,
8(1):37–50.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG parsing. In
Proc. of the 20th International Conference on Computa-
tional Linguistics (COLING 2004), pages 282–8, Geneva,
Switzerland.
Ann Copestake and Dan Flickinger. 2000. An open-source
grammar development environment and broad-coverage
English grammar using HPSG. In Proc. of the 2nd In-
ternational Conference on Language Resources and Eval-
uation (LREC 2000), Athens, Greece.
Dan Flickinger. 2002. On building a more efficient grammar
by exploiting types. In Oepen et al. (Oepen et al., 2002b).
Frederik Fouvry. 2003. Robust Processing for Constraint-
based Grammar Formalisms. Ph.D. thesis, University of
Essex.
Eric Joanis and Suzanne Stevenson. 2003. A general feature
space for automatic verb classification. pages 163–70, Bu-
dapest, Hungary.
Anna Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge.
Mirella Lapata and Frank Keller. 2004. The web as a base-
line: Evaluating the performance of unsupervised web-
based models for a range of NLP tasks. pages 121–8,
Boston, USA.
Stan Z. Li. 1994. Markov random field models in computer
vision. In ECCV (2), pages 361–370.
Rob Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Proc. of the
6th Conference on Natural Language Learning (CoNLL-
2002), pages 49–55, Taipei, Taiwan.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshi-
taka Hirano, Hiroshi Matsuda, Kazuma Takaoka, and
Masayuki Asahara. 2003. Japanese Morphological Anal-
ysis System ChaSen Version 2.3.3 Manual. Technical re-
port, NAIST.
Grace Ngai and Radu Florian. 2001. Transformation-based
learning in the fast lane. In Proc. of the 2nd Annual
Meeting of the North American Chapter of Association
for Computational Linguistics (NAACL2001), pages 40–
7, Pittsburgh, USA.
Stephan Oepen, Dan Flickinger, Kristina Toutanova, and
Christoper D. Manning. 2002a. LinGO Redwoods: A
rich and dynamic treebank for HPSG. In Proc. of The
First Workshop on Treebanks and Linguistic Theories
(TLT2002), Sozopol, Bulgaria.
Stephan Oepen, Dan Flickinger, Jun’ichi Tsujii, and Hans
Uszkoreit, editors. 2002b. Collaborative Language En-
gineering. CSLI Publications, Stanford, USA.
Carl Pollard and Ivan A. Sag. 1994. Head-driven Phrase
Structure Grammar. The University of Chicago Press,
Chicago, USA.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell III, and Mark Johnson. 2002.
Parsing the Wall Street Journal using a Lexical-Functional
Grammar and discriminative estimation techniques. In
Proc. of the 40th Annual Meeting of the ACL and 3rd An-
nual Meeting of the NAACL (ACL-02), Philadelphia, USA.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proc. of the 3rd In-
ternational Conference on Human Language Technology
Research and 4th Annual Meeting of the NAACL (HLT-
NAACL 2003), pages 213–20, Edmonton, Canada.
Melanie Siegel and Emily M. Bender. 2002. Efficient deep
processing of Japanese. In Proc. of the 3rd Workshop on
Asian Language Resources and International Standard-
ization, Taipei, Taiwan.
Hans Uszkoreit. 2002. New chances for deep linguistic pro-
cessing. In Proc. of the 19th International Conference on
Computational Linguistics (COLING 2002), Taipei, Tai-
wan.
Gertjan van Noord. 2004. Error mining for wide-coverage
grammar engineering. In Proc. of the 42nd Annual Meet-
ing of the ACL, Barcelona, Spain.
Yi Zhang and Valia Kordoni. 2005. A statistical approach to-
wards unknown word type prediction for deep grammars.
In Proc. of the Australasian Language Technology Work-
shop 2005, pages 24–31, Sydney, Australia.
Yi Zhang and Valia Kordoni. 2006. Automated deep lexical
acquisition for robust open texts processing. In Proceed-
ings of the Fifth International Conference on Language
Resources and Evaluation (LREC 2006), Genoa, Italy.
</reference>
<page confidence="0.998149">
171
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.791084">
<title confidence="0.999325">Multilingual Deep Lexical Acquisition for HPSGs via Supertagging</title>
<author confidence="0.997907">Phil Blunsom</author>
<author confidence="0.997907">Timothy</author>
<affiliation confidence="0.9211375">Computer Science and Software University of Melbourne, Victoria 3010 Australia</affiliation>
<email confidence="0.996804">pcbl@csse.unimelb.edu.au</email>
<email confidence="0.996804">tim@csse.unimelb.edu.au</email>
<abstract confidence="0.997006947368421">We propose a conditional random fieldbased method for supertagging, and apply it to the task of learning new lexical items for HPSG-based precision grammars of English and Japanese. Using a pseudo-likelihood approximation we are able to scale our model to hundreds of supertags and tens-of-thousands of training sentences. We show that it is possible to achieve start-of-the-art results for both languages using maximally language-independent lexical features. Further, we explore the performance of the models at the typeand token-level, demonstrating their superior performance when compared to a unigram-based baseline and a transformation-based learning approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
</authors>
<title>Bootstrapping deep lexical resources: Resources for courses.</title>
<date>2005</date>
<booktitle>In Proc. of the ACLSIGLEX 2005 Workshop on Deep Lexical Acquisition,</booktitle>
<pages>67--76</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="2437" citStr="Baldwin, 2005" startWordPosition="357" endWordPosition="358"> to their linguistic complexity, precision grammars are generally hand-constructed and thus restricted in size and coverage. Attempts to (semi-)automate the process of expanding the coverage of precision grammars have focused on either: (a) constructional coverage, e.g. in the form of error mining for constructional expansion (van Noord, 2004; Zhang and Kordoni, 2006), or relaxation of lexico-grammatical constraints to support partial and/or robust parsing (Riezler et al., 2002); or (b) lexical coverage, e.g. in bootstrapping from a pre-existing grammar and lexicon to learn new lexical items (Baldwin, 2005a). Our particular interest in this paper is in the latter of these two, that is the development of methods for automatically expanding the lexical coverage of an existing precision grammar, or more broadly deep lexical acquisition (DLA hereafter). In this, we follow Baldwin (2005a) in assuming a semi-mature precision grammar with a fixed inventory of lexical types, based on which we learn new lexical items. For the purposes of this paper, we focus specifically on supertagging as the mechanism for hypothesising new lexical items. Supertagging can be defined as the process of applying a sequent</context>
<context position="4284" citStr="Baldwin (2005" startWordPosition="661" endWordPosition="662">n postulating novel lexical items with which to populate the lexicon of a given grammar to boost parser coverage. This can take place 164 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 164–171, Sydney, July 2006. c�2006 Association for Computational Linguistics either: (a) off-line for the purposes of rounding out the coverage of a static lexicon, in which case we are generally interested in globally maximising precision over a given corpus and hence predicting the single most plausible lexical type for each word token (off-line DLA: Baldwin (2005b)); or (b) on the fly for a given input string to temporarily expand lexical coverage and achieve a spanning parse, in which case we are interested in maximising recall by producing a (possibly weighted) list of lexical item hypotheses to run past the grammar (on-line DLA: Zhang and Kordoni (2005)). Our immediate interest in this paper is in the first of these tasks, although we would ideally like to develop an off-line method which is trivially portable to the second task of on-line DLA. In this research, we focus particularly on the Grammar Matrix-based DELPH-IN family of grammars (Bender e</context>
<context position="6826" citStr="Baldwin (2005" startWordPosition="1061" endWordPosition="1062">then apply to the task of off-line DLA for grammars of both English and Japanese with only minor language-specific adaptation. We show the supertagger to outperform previously-proposed supertagger-based DLA methods. The remainder of this paper is structured as follows. Section 2 outlines past work relative to this research, and Section 3 reviews the resources used in our supertagging experiments. Section 4 outlines the proposed supertagger model and reviews previous research on supertaggerbased DLA. Section 5 then outlines the set-up and results of our evaluation. 2 Past Research According to Baldwin (2005b), research on DLA falls into the two categories of in vitro methods, where we leverage a secondary language resource to generate an abstraction of the words we hope to learn lexical items for, and in vivo methods, where the target resource that we are hoping to perform DLA relative to is used directly to perform DLA. Supertagging is an instance of in vivo DLA, as it operates directly over data tagged with the lexical type system for the precision grammar of interest. Research on supertagging which is relevant to this paper includes the work of Baldwin (2005b) in training a transformation-bas</context>
<context position="9558" citStr="Baldwin (2005" startWordPosition="1512" endWordPosition="1513">known word in such contexts. That is, the morphological, syntactic and/or semantic predictions implicit in each lexical type are made explicit in the form of templates which represent distinguishing lexical contexts of that lexical type. This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). Other work on DLA (e.g. Korhonen (2002), Joanis and Stevenson (2003), Baldwin (2005a)) has tended to take an in vitro DLA approach, in extrapolating away from a DLR to corpus or web data, and analysing occurrences of words through the conduit of an external resource (e.g. a secondary parser or POS tagger). In vitro DLA can also take the form of resource translation, in mapping one DLR onto another to arrive at the lexical information in the desired format. 3 Task and Resources In this section, we outline the resources targeted in this research, namely the English Resource Grammar (ERG: Flickinger (2002), Copestake and Flickinger (2000)) and the JACY grammar of Japanese (Sieg</context>
<context position="22363" citStr="Baldwin (2005" startWordPosition="3670" endWordPosition="3671">C) across all words in the test data, irrespective of whether they represent known or unknown lexical items. 5.1 Baseline: Unigram Supertagger As a baseline model, we use a simple unigram supertagger trained based on maximum likelihood estimation over the relevant training data, i.e. the tag for each token instance of a given word is predicted by: In the instance that was not observed in the training data, we back off to the majority lexical type in the training data. 5.2 Benchmark: fnTBL In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FNTBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set. FNTBL is a transformation-based learner that is distributed with pre-optimised POS tagging modules for English and other European languages that can be redeployed over the task of supertagging. Following Baldwin (2005b), the only modifications we make to the default English POS tagging methodology are: (1) to set the default lexical types for singular common and proper nouns to n - c le and n - pn le, respectively; and (2) reduce the threshold score for lexical and context tra</context>
</contexts>
<marker>Baldwin, 2005</marker>
<rawString>Timothy Baldwin. 2005a. Bootstrapping deep lexical resources: Resources for courses. In Proc. of the ACLSIGLEX 2005 Workshop on Deep Lexical Acquisition, pages 67–76, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
</authors>
<title>General-purpose lexical acquisition: Procedures, questions and results.</title>
<date>2005</date>
<booktitle>In Proc. of the</booktitle>
<contexts>
<context position="2437" citStr="Baldwin, 2005" startWordPosition="357" endWordPosition="358"> to their linguistic complexity, precision grammars are generally hand-constructed and thus restricted in size and coverage. Attempts to (semi-)automate the process of expanding the coverage of precision grammars have focused on either: (a) constructional coverage, e.g. in the form of error mining for constructional expansion (van Noord, 2004; Zhang and Kordoni, 2006), or relaxation of lexico-grammatical constraints to support partial and/or robust parsing (Riezler et al., 2002); or (b) lexical coverage, e.g. in bootstrapping from a pre-existing grammar and lexicon to learn new lexical items (Baldwin, 2005a). Our particular interest in this paper is in the latter of these two, that is the development of methods for automatically expanding the lexical coverage of an existing precision grammar, or more broadly deep lexical acquisition (DLA hereafter). In this, we follow Baldwin (2005a) in assuming a semi-mature precision grammar with a fixed inventory of lexical types, based on which we learn new lexical items. For the purposes of this paper, we focus specifically on supertagging as the mechanism for hypothesising new lexical items. Supertagging can be defined as the process of applying a sequent</context>
<context position="4284" citStr="Baldwin (2005" startWordPosition="661" endWordPosition="662">n postulating novel lexical items with which to populate the lexicon of a given grammar to boost parser coverage. This can take place 164 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 164–171, Sydney, July 2006. c�2006 Association for Computational Linguistics either: (a) off-line for the purposes of rounding out the coverage of a static lexicon, in which case we are generally interested in globally maximising precision over a given corpus and hence predicting the single most plausible lexical type for each word token (off-line DLA: Baldwin (2005b)); or (b) on the fly for a given input string to temporarily expand lexical coverage and achieve a spanning parse, in which case we are interested in maximising recall by producing a (possibly weighted) list of lexical item hypotheses to run past the grammar (on-line DLA: Zhang and Kordoni (2005)). Our immediate interest in this paper is in the first of these tasks, although we would ideally like to develop an off-line method which is trivially portable to the second task of on-line DLA. In this research, we focus particularly on the Grammar Matrix-based DELPH-IN family of grammars (Bender e</context>
<context position="6826" citStr="Baldwin (2005" startWordPosition="1061" endWordPosition="1062">then apply to the task of off-line DLA for grammars of both English and Japanese with only minor language-specific adaptation. We show the supertagger to outperform previously-proposed supertagger-based DLA methods. The remainder of this paper is structured as follows. Section 2 outlines past work relative to this research, and Section 3 reviews the resources used in our supertagging experiments. Section 4 outlines the proposed supertagger model and reviews previous research on supertaggerbased DLA. Section 5 then outlines the set-up and results of our evaluation. 2 Past Research According to Baldwin (2005b), research on DLA falls into the two categories of in vitro methods, where we leverage a secondary language resource to generate an abstraction of the words we hope to learn lexical items for, and in vivo methods, where the target resource that we are hoping to perform DLA relative to is used directly to perform DLA. Supertagging is an instance of in vivo DLA, as it operates directly over data tagged with the lexical type system for the precision grammar of interest. Research on supertagging which is relevant to this paper includes the work of Baldwin (2005b) in training a transformation-bas</context>
<context position="9558" citStr="Baldwin (2005" startWordPosition="1512" endWordPosition="1513">known word in such contexts. That is, the morphological, syntactic and/or semantic predictions implicit in each lexical type are made explicit in the form of templates which represent distinguishing lexical contexts of that lexical type. This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). Other work on DLA (e.g. Korhonen (2002), Joanis and Stevenson (2003), Baldwin (2005a)) has tended to take an in vitro DLA approach, in extrapolating away from a DLR to corpus or web data, and analysing occurrences of words through the conduit of an external resource (e.g. a secondary parser or POS tagger). In vitro DLA can also take the form of resource translation, in mapping one DLR onto another to arrive at the lexical information in the desired format. 3 Task and Resources In this section, we outline the resources targeted in this research, namely the English Resource Grammar (ERG: Flickinger (2002), Copestake and Flickinger (2000)) and the JACY grammar of Japanese (Sieg</context>
<context position="22363" citStr="Baldwin (2005" startWordPosition="3670" endWordPosition="3671">C) across all words in the test data, irrespective of whether they represent known or unknown lexical items. 5.1 Baseline: Unigram Supertagger As a baseline model, we use a simple unigram supertagger trained based on maximum likelihood estimation over the relevant training data, i.e. the tag for each token instance of a given word is predicted by: In the instance that was not observed in the training data, we back off to the majority lexical type in the training data. 5.2 Benchmark: fnTBL In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FNTBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set. FNTBL is a transformation-based learner that is distributed with pre-optimised POS tagging modules for English and other European languages that can be redeployed over the task of supertagging. Following Baldwin (2005b), the only modifications we make to the default English POS tagging methodology are: (1) to set the default lexical types for singular common and proper nouns to n - c le and n - pn le, respectively; and (2) reduce the threshold score for lexical and context tra</context>
</contexts>
<marker>Baldwin, 2005</marker>
<rawString>Timothy Baldwin. 2005b. General-purpose lexical acquisition: Procedures, questions and results. In Proc. of the</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="3262" citStr="Bangalore and Joshi (1999)" startWordPosition="494" endWordPosition="497">broadly deep lexical acquisition (DLA hereafter). In this, we follow Baldwin (2005a) in assuming a semi-mature precision grammar with a fixed inventory of lexical types, based on which we learn new lexical items. For the purposes of this paper, we focus specifically on supertagging as the mechanism for hypothesising new lexical items. Supertagging can be defined as the process of applying a sequential tagger to the task of predicting the lexical type(s) associated with each word in an input string, relative to a given grammar. It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism (Clark and Curran, 2004). In both of these cases, supertagging provides the means to perform a beam search over the plausible lexical items for a given string context, and ideally reduces parsing complexity without sacrificing parser accuracy. An alternate application of supertagging is in DLA, in postulating novel lexical items with which to populate the lexicon of a given grammar to boost parser coverage. This can take place 164 Proceedings of the 2006 Conference on Empirical Metho</context>
<context position="7796" citStr="Bangalore and Joshi, 1999" startWordPosition="1225" endWordPosition="1228">nce of in vivo DLA, as it operates directly over data tagged with the lexical type system for the precision grammar of interest. Research on supertagging which is relevant to this paper includes the work of Baldwin (2005b) in training a transformation-based learner over data tagged with ERG lexical types. We discuss this method in detail in Section 5.2 and replicate this method over our English data set for direct comparability with this previous research. As mentioned above, other work on supertagging has tended to view it as a means of driving a beam search to prune the parser search space (Bangalore and Joshi, 1999; Clark and Curran, 2004). In supertagging, token-level annotations (gold-standard, automatically-generated or otherwise) for a given DLR are used to train a sequential tagger, akin to training a POS tagger over POS-tagged data taken from the Penn Treebank. One related in vivo approach to DLA targeted specifically at precision grammars is that of Fouvry (2003). Fouvry uses the grammar to guide the process of learning lexical items for unknown words, by generating underspecified lexical items for all unknown words and parsing with them. Syntactico-semantic interaction between unknown words and </context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>The grammar Matrix. An open-source starter-kit for the rapid development of cross-linguistically consistent broad-coverage precision grammar.</title>
<date>2002</date>
<booktitle>In Proc. of the Workshop on Grammar Engineering and Evaluation at the 19th International Conference on Computational Linguistics,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="4896" citStr="Bender et al., 2002" startWordPosition="764" endWordPosition="767">in (2005b)); or (b) on the fly for a given input string to temporarily expand lexical coverage and achieve a spanning parse, in which case we are interested in maximising recall by producing a (possibly weighted) list of lexical item hypotheses to run past the grammar (on-line DLA: Zhang and Kordoni (2005)). Our immediate interest in this paper is in the first of these tasks, although we would ideally like to develop an off-line method which is trivially portable to the second task of on-line DLA. In this research, we focus particularly on the Grammar Matrix-based DELPH-IN family of grammars (Bender et al., 2002), which includes grammars of English, Japanese, Norwegian, Modern Greek, Portuguese and Korean. The Grammar Matrix is a framework for streamlining and standardising HPSG-based multilingual grammar development. One property of Grammar Matrixbased grammars is that they are strongly lexicalist and adhere to a highly constrained lexicongrammar interface via a unique (terminal) lexical type for each lexical item. As such, lexical item creation in any of the Grammar Matrix-based grammars, irrespective of language, consists predominantly of predicting the appropriate lexical type for each lexical ite</context>
</contexts>
<marker>Bender, Flickinger, Oepen, 2002</marker>
<rawString>Emily M. Bender, Dan Flickinger, and Stephan Oepen. 2002. The grammar Matrix. An open-source starter-kit for the rapid development of cross-linguistically consistent broad-coverage precision grammar. In Proc. of the Workshop on Grammar Engineering and Evaluation at the 19th International Conference on Computational Linguistics, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Sanae Fujita</author>
</authors>
<title>Chikara Hashimoto, Kaname Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani, Takaaki Tanaka, and Shigeaki Amano.</title>
<date>2004</date>
<booktitle>In Proc. of the First International Joint Conference on Natural Language Processing (IJCNLP-04),</booktitle>
<pages>554--9</pages>
<location>Hainan Island, China.</location>
<marker>Bond, Fujita, 2004</marker>
<rawString>Francis Bond, Sanae Fujita, Chikara Hashimoto, Kaname Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani, Takaaki Tanaka, and Shigeaki Amano. 2004. The Hinoki treebank: A treebank for text understanding. In Proc. of the First International Joint Conference on Natural Language Processing (IJCNLP-04), pages 554–9, Hainan Island, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James R Curran</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Wide-coverage semantic representations from a CCG parser.</title>
<date>2004</date>
<booktitle>In Proc. of the 20th International Conference on Computational Linguistics (COLING 2004),</booktitle>
<pages>1240--7</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1819" citStr="Bos et al., 2004" startWordPosition="264" endWordPosition="267">of natural language which capture finegrained linguistic distinctions, and are generative in the sense of distinguishing between grammatical and ungrammatical inputs (or at least have some in-built notion of linguistic “markedness”). Additional characteristics of precision grammars are that they are frequently bidirectional, and output a rich semantic abstraction for each spanning parse of the input string. Examples include DELPH-IN grammars such as the English Resource Grammar (Flickinger, 2002; Uszkoreit, 2002), the various PARGRAM grammars (Butt et al., 1999), and the Edinburgh CCG parser (Bos et al., 2004). Due to their linguistic complexity, precision grammars are generally hand-constructed and thus restricted in size and coverage. Attempts to (semi-)automate the process of expanding the coverage of precision grammars have focused on either: (a) constructional coverage, e.g. in the form of error mining for constructional expansion (van Noord, 2004; Zhang and Kordoni, 2006), or relaxation of lexico-grammatical constraints to support partial and/or robust parsing (Riezler et al., 2002); or (b) lexical coverage, e.g. in bootstrapping from a pre-existing grammar and lexicon to learn new lexical it</context>
</contexts>
<marker>Bos, Clark, Steedman, Curran, Hockenmaier, 2004</marker>
<rawString>Johan Bos, Stephen Clark, Mark Steedman, James R. Curran, and Julia Hockenmaier. 2004. Wide-coverage semantic representations from a CCG parser. In Proc. of the 20th International Conference on Computational Linguistics (COLING 2004), pages 1240–7, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Butt</author>
<author>Tracy Holloway King</author>
<author>Maria-Eugenia Nino</author>
<author>Frederique Segond</author>
</authors>
<title>A Grammar Writer’s Cookbook.</title>
<date>1999</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, USA.</location>
<contexts>
<context position="1770" citStr="Butt et al., 1999" startWordPosition="255" endWordPosition="258">sion grammars are defined as implemented grammars of natural language which capture finegrained linguistic distinctions, and are generative in the sense of distinguishing between grammatical and ungrammatical inputs (or at least have some in-built notion of linguistic “markedness”). Additional characteristics of precision grammars are that they are frequently bidirectional, and output a rich semantic abstraction for each spanning parse of the input string. Examples include DELPH-IN grammars such as the English Resource Grammar (Flickinger, 2002; Uszkoreit, 2002), the various PARGRAM grammars (Butt et al., 1999), and the Edinburgh CCG parser (Bos et al., 2004). Due to their linguistic complexity, precision grammars are generally hand-constructed and thus restricted in size and coverage. Attempts to (semi-)automate the process of expanding the coverage of precision grammars have focused on either: (a) constructional coverage, e.g. in the form of error mining for constructional expansion (van Noord, 2004; Zhang and Kordoni, 2006), or relaxation of lexico-grammatical constraints to support partial and/or robust parsing (Riezler et al., 2002); or (b) lexical coverage, e.g. in bootstrapping from a pre-exi</context>
</contexts>
<marker>Butt, King, Nino, Segond, 1999</marker>
<rawString>Miriam Butt, Tracy Holloway King, Maria-Eugenia Nino, and Frederique Segond. 1999. A Grammar Writer’s Cookbook. CSLI Publications, Stanford, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A survey of smoothing techniques for maximum entropy models.</title>
<date>1999</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="17514" citStr="Chen and Rosenfeld, 1999" startWordPosition="2843" endWordPosition="2846">next word bigram + label &amp; clique label pair LEXICAL FEATURES &amp; -gram prefix + label &amp; -gram suffix + label &amp; word contains element of character set + label Table 2. Extracted feature types for the CRF model served in the training data: where is the lexical type label observed in the training data andranges over the label set. This approximation removes the need to calculate the partition function, thus reducing the complexity to be linear in the number of labels and training instances. Because maximum likelihood estimators for log-linear models have a tendency to overfit the training sample (Chen and Rosenfeld, 1999), we define a prior distribution over the model parameters and derive a maximum a posteriori (MAP) estimate, . We use a zero-mean Gaussian prior, with the probability density function . This yields a log-pseudo-likelihood objective function of: In order to train the model, we maximize (4). While the log-pseudo-likelihood cannot be maximised for the parameters, , in closed form, it is a convex function, and thus we resort to numerical optimisation to find the globally optimal parameters. We use L-BFGS, an iterative quasi-Newton optimisation method, which performs well for training log-linear mo</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999. A survey of smoothing techniques for maximum entropy models. IEEE Transactions on Speech and Audio Processing, 8(1):37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In Proc. of the 20th International Conference on Computational Linguistics (COLING 2004),</booktitle>
<pages>282--8</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="3398" citStr="Clark and Curran, 2004" startWordPosition="519" endWordPosition="522">ed inventory of lexical types, based on which we learn new lexical items. For the purposes of this paper, we focus specifically on supertagging as the mechanism for hypothesising new lexical items. Supertagging can be defined as the process of applying a sequential tagger to the task of predicting the lexical type(s) associated with each word in an input string, relative to a given grammar. It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism (Clark and Curran, 2004). In both of these cases, supertagging provides the means to perform a beam search over the plausible lexical items for a given string context, and ideally reduces parsing complexity without sacrificing parser accuracy. An alternate application of supertagging is in DLA, in postulating novel lexical items with which to populate the lexicon of a given grammar to boost parser coverage. This can take place 164 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 164–171, Sydney, July 2006. c�2006 Association for Computational Linguistics eithe</context>
<context position="7821" citStr="Clark and Curran, 2004" startWordPosition="1229" endWordPosition="1232">perates directly over data tagged with the lexical type system for the precision grammar of interest. Research on supertagging which is relevant to this paper includes the work of Baldwin (2005b) in training a transformation-based learner over data tagged with ERG lexical types. We discuss this method in detail in Section 5.2 and replicate this method over our English data set for direct comparability with this previous research. As mentioned above, other work on supertagging has tended to view it as a means of driving a beam search to prune the parser search space (Bangalore and Joshi, 1999; Clark and Curran, 2004). In supertagging, token-level annotations (gold-standard, automatically-generated or otherwise) for a given DLR are used to train a sequential tagger, akin to training a POS tagger over POS-tagged data taken from the Penn Treebank. One related in vivo approach to DLA targeted specifically at precision grammars is that of Fouvry (2003). Fouvry uses the grammar to guide the process of learning lexical items for unknown words, by generating underspecified lexical items for all unknown words and parsing with them. Syntactico-semantic interaction between unknown words and pre-existing lexical item</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. The importance of supertagging for wide-coverage CCG parsing. In Proc. of the 20th International Conference on Computational Linguistics (COLING 2004), pages 282–8, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>An open-source grammar development environment and broad-coverage English grammar using HPSG.</title>
<date>2000</date>
<booktitle>In Proc. of the 2nd International Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="10118" citStr="Copestake and Flickinger (2000)" startWordPosition="1605" endWordPosition="1608">n DLA (e.g. Korhonen (2002), Joanis and Stevenson (2003), Baldwin (2005a)) has tended to take an in vitro DLA approach, in extrapolating away from a DLR to corpus or web data, and analysing occurrences of words through the conduit of an external resource (e.g. a secondary parser or POS tagger). In vitro DLA can also take the form of resource translation, in mapping one DLR onto another to arrive at the lexical information in the desired format. 3 Task and Resources In this section, we outline the resources targeted in this research, namely the English Resource Grammar (ERG: Flickinger (2002), Copestake and Flickinger (2000)) and the JACY grammar of Japanese (Siegel and Bender, 2002). Note that our choice of the ERG and JACY as testbeds for experimentation in this paper is somewhat arbitrary, and that we could equally run experiments over any Grammar Matrix-based grammar for which there is treebank data. Both the ERG and JACY are implemented open-source broad-coverage precision Headdriven Phrase Structure Grammars (HPSGs: Pollard and Sag (1994)). A lexical item in each of the grammars consists of a unique identifier, a lexical type (a leaf type of a type hierarchy), an orthography, and a semantic relation. For ex</context>
</contexts>
<marker>Copestake, Flickinger, 2000</marker>
<rawString>Ann Copestake and Dan Flickinger. 2000. An open-source grammar development environment and broad-coverage English grammar using HPSG. In Proc. of the 2nd International Conference on Language Resources and Evaluation (LREC 2000), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2002</date>
<booktitle>In Oepen et</booktitle>
<contexts>
<context position="1702" citStr="Flickinger, 2002" startWordPosition="247" endWordPosition="248">ent tools and raw computational power (Oepen et al., 2002b). Precision grammars are defined as implemented grammars of natural language which capture finegrained linguistic distinctions, and are generative in the sense of distinguishing between grammatical and ungrammatical inputs (or at least have some in-built notion of linguistic “markedness”). Additional characteristics of precision grammars are that they are frequently bidirectional, and output a rich semantic abstraction for each spanning parse of the input string. Examples include DELPH-IN grammars such as the English Resource Grammar (Flickinger, 2002; Uszkoreit, 2002), the various PARGRAM grammars (Butt et al., 1999), and the Edinburgh CCG parser (Bos et al., 2004). Due to their linguistic complexity, precision grammars are generally hand-constructed and thus restricted in size and coverage. Attempts to (semi-)automate the process of expanding the coverage of precision grammars have focused on either: (a) constructional coverage, e.g. in the form of error mining for constructional expansion (van Noord, 2004; Zhang and Kordoni, 2006), or relaxation of lexico-grammatical constraints to support partial and/or robust parsing (Riezler et al., </context>
<context position="10085" citStr="Flickinger (2002)" startWordPosition="1603" endWordPosition="1604">2004). Other work on DLA (e.g. Korhonen (2002), Joanis and Stevenson (2003), Baldwin (2005a)) has tended to take an in vitro DLA approach, in extrapolating away from a DLR to corpus or web data, and analysing occurrences of words through the conduit of an external resource (e.g. a secondary parser or POS tagger). In vitro DLA can also take the form of resource translation, in mapping one DLR onto another to arrive at the lexical information in the desired format. 3 Task and Resources In this section, we outline the resources targeted in this research, namely the English Resource Grammar (ERG: Flickinger (2002), Copestake and Flickinger (2000)) and the JACY grammar of Japanese (Siegel and Bender, 2002). Note that our choice of the ERG and JACY as testbeds for experimentation in this paper is somewhat arbitrary, and that we could equally run experiments over any Grammar Matrix-based grammar for which there is treebank data. Both the ERG and JACY are implemented open-source broad-coverage precision Headdriven Phrase Structure Grammars (HPSGs: Pollard and Sag (1994)). A lexical item in each of the grammars consists of a unique identifier, a lexical type (a leaf type of a type hierarchy), an orthography</context>
</contexts>
<marker>Flickinger, 2002</marker>
<rawString>Dan Flickinger. 2002. On building a more efficient grammar by exploiting types. In Oepen et al. (Oepen et al., 2002b).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederik Fouvry</author>
</authors>
<title>Robust Processing for Constraintbased Grammar Formalisms.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Essex.</institution>
<contexts>
<context position="8158" citStr="Fouvry (2003)" startWordPosition="1283" endWordPosition="1285">method over our English data set for direct comparability with this previous research. As mentioned above, other work on supertagging has tended to view it as a means of driving a beam search to prune the parser search space (Bangalore and Joshi, 1999; Clark and Curran, 2004). In supertagging, token-level annotations (gold-standard, automatically-generated or otherwise) for a given DLR are used to train a sequential tagger, akin to training a POS tagger over POS-tagged data taken from the Penn Treebank. One related in vivo approach to DLA targeted specifically at precision grammars is that of Fouvry (2003). Fouvry uses the grammar to guide the process of learning lexical items for unknown words, by generating underspecified lexical items for all unknown words and parsing with them. Syntactico-semantic interaction between unknown words and pre-existing lexical items during parsing provides insight into the nature of each unknown word. By combining such fragments of information, it is possible to incrementally arrive at a consolidated lexical entry for that word. That is, the precision grammar itself drives the incremental learning process within a parsing context. 165 An alternate approach is to</context>
</contexts>
<marker>Fouvry, 2003</marker>
<rawString>Frederik Fouvry. 2003. Robust Processing for Constraintbased Grammar Formalisms. Ph.D. thesis, University of Essex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Joanis</author>
<author>Suzanne Stevenson</author>
</authors>
<title>A general feature space for automatic verb classification.</title>
<date>2003</date>
<pages>163--70</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="9543" citStr="Joanis and Stevenson (2003)" startWordPosition="1508" endWordPosition="1511">r corpus occurrences of an unknown word in such contexts. That is, the morphological, syntactic and/or semantic predictions implicit in each lexical type are made explicit in the form of templates which represent distinguishing lexical contexts of that lexical type. This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). Other work on DLA (e.g. Korhonen (2002), Joanis and Stevenson (2003), Baldwin (2005a)) has tended to take an in vitro DLA approach, in extrapolating away from a DLR to corpus or web data, and analysing occurrences of words through the conduit of an external resource (e.g. a secondary parser or POS tagger). In vitro DLA can also take the form of resource translation, in mapping one DLR onto another to arrive at the lexical information in the desired format. 3 Task and Resources In this section, we outline the resources targeted in this research, namely the English Resource Grammar (ERG: Flickinger (2002), Copestake and Flickinger (2000)) and the JACY grammar of</context>
</contexts>
<marker>Joanis, Stevenson, 2003</marker>
<rawString>Eric Joanis and Suzanne Stevenson. 2003. A general feature space for automatic verb classification. pages 163–70, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Subcategorization Acquisition.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge.</institution>
<contexts>
<context position="9514" citStr="Korhonen (2002)" startWordPosition="1506" endWordPosition="1507">rm), and check for corpus occurrences of an unknown word in such contexts. That is, the morphological, syntactic and/or semantic predictions implicit in each lexical type are made explicit in the form of templates which represent distinguishing lexical contexts of that lexical type. This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). Other work on DLA (e.g. Korhonen (2002), Joanis and Stevenson (2003), Baldwin (2005a)) has tended to take an in vitro DLA approach, in extrapolating away from a DLR to corpus or web data, and analysing occurrences of words through the conduit of an external resource (e.g. a secondary parser or POS tagger). In vitro DLA can also take the form of resource translation, in mapping one DLR onto another to arrive at the lexical information in the desired format. 3 Task and Resources In this section, we outline the resources targeted in this research, namely the English Resource Grammar (ERG: Flickinger (2002), Copestake and Flickinger (2</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>Anna Korhonen. 2002. Subcategorization Acquisition. Ph.D. thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>The web as a baseline: Evaluating the performance of unsupervised webbased models for a range of NLP tasks.</title>
<date>2004</date>
<pages>121--8</pages>
<location>Boston, USA.</location>
<contexts>
<context position="9473" citStr="Lapata and Keller, 2004" startWordPosition="1497" endWordPosition="1500">n that they do not rely on preprocessing of any form), and check for corpus occurrences of an unknown word in such contexts. That is, the morphological, syntactic and/or semantic predictions implicit in each lexical type are made explicit in the form of templates which represent distinguishing lexical contexts of that lexical type. This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). Other work on DLA (e.g. Korhonen (2002), Joanis and Stevenson (2003), Baldwin (2005a)) has tended to take an in vitro DLA approach, in extrapolating away from a DLR to corpus or web data, and analysing occurrences of words through the conduit of an external resource (e.g. a secondary parser or POS tagger). In vitro DLA can also take the form of resource translation, in mapping one DLR onto another to arrive at the lexical information in the desired format. 3 Task and Resources In this section, we outline the resources targeted in this research, namely the English Resource Grammar (ERG: Flick</context>
</contexts>
<marker>Lapata, Keller, 2004</marker>
<rawString>Mirella Lapata and Frank Keller. 2004. The web as a baseline: Evaluating the performance of unsupervised webbased models for a range of NLP tasks. pages 121–8, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stan Z Li</author>
</authors>
<title>Markov random field models in computer vision.</title>
<date>1994</date>
<booktitle>In ECCV (2),</booktitle>
<pages>361--370</pages>
<contexts>
<context position="16598" citStr="Li, 1994" startWordPosition="2684" endWordPosition="2685">y the partition function, , which sums out the numerator in (1) for every possible labelling: We use a linear chain CRF, which is encoded in the feature functions of (1). The parameters of the CRF are usually estimated from a fully observed training sample, by maximising the likelihood of these data. I.e. , where is the complete set of training data. However, as calculating has complexity quadratic in the number of labels, we need to approximate in order to scale our model to hundreds of lexical types and tens-of-thousands of training sentences. Here we use the pseudolikelihood approximation (Li, 1994) in which the marginals for a node at time are calculated with its neighbour nodes’ labels fixed to those ob167 FEATURE DESCRIPTION WORD CONTEXT FEATURES &amp; lexeme + label &amp; word unigram + label &amp; previous word unigram + label &amp; next word unigram + label &amp; &amp; previous word bigram + label &amp; &amp; next word bigram + label &amp; clique label pair LEXICAL FEATURES &amp; -gram prefix + label &amp; -gram suffix + label &amp; word contains element of character set + label Table 2. Extracted feature types for the CRF model served in the training data: where is the lexical type label observed in the training data andranges </context>
</contexts>
<marker>Li, 1994</marker>
<rawString>Stan Z. Li. 1994. Markov random field models in computer vision. In ECCV (2), pages 361–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proc. of the 6th Conference on Natural Language Learning (CoNLL2002),</booktitle>
<pages>49--55</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="18132" citStr="Malouf, 2002" startWordPosition="2943" endWordPosition="2944">efine a prior distribution over the model parameters and derive a maximum a posteriori (MAP) estimate, . We use a zero-mean Gaussian prior, with the probability density function . This yields a log-pseudo-likelihood objective function of: In order to train the model, we maximize (4). While the log-pseudo-likelihood cannot be maximised for the parameters, , in closed form, it is a convex function, and thus we resort to numerical optimisation to find the globally optimal parameters. We use L-BFGS, an iterative quasi-Newton optimisation method, which performs well for training log-linear models (Malouf, 2002; Sha and Pereira, 2003). Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. As we cannot observe label values for the test data we must use when decoding. The Viterbi algorithm is used to find the maximum posterior probability alignment for test sentences, 4.2 CRF features One of the strengths of the CRF model is that it supports the use of a large number of nonindependent and overlapping features of the input sentence. Table 2 lists the word context and lexical features used by the CRF model (shared across both grammars). Word context fe</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Rob Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proc. of the 6th Conference on Natural Language Learning (CoNLL2002), pages 49–55, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
<author>Akira Kitauchi</author>
<author>Tatsuo Yamashita</author>
<author>Yoshitaka Hirano</author>
<author>Hiroshi Matsuda</author>
<author>Kazuma Takaoka</author>
<author>Masayuki Asahara</author>
</authors>
<title>Japanese Morphological Analysis System ChaSen Version 2.3.3 Manual.</title>
<date>2003</date>
<tech>Technical report, NAIST.</tech>
<contexts>
<context position="14427" citStr="Matsumoto et al., 2003" startWordPosition="2327" endWordPosition="2330">,781 10,669 Table 1. Make-up of the English Resource Grammar (ERG) and JACY grammars and treebanks tags and leave the supertagger to model the mutual inter-dependence between these lexical types. For detailed statistics of the composition of the two grammars, see Table 1. For morphological processing (including tokenisation and lemmatisation), we use the preexisting machinery provided with each of the grammars. In the case of the ERG, this consists of a finite state machine which feeds into lexical rules; in the case of JACY, segmentation and lemmatisation is based on a combination of ChaSen (Matsumoto et al., 2003) and lexical rules. That is, we are able to assume that the Japanese data has been pre-segmented in a form compatible with JACY, as we are able to replicate the automatic pre-processing that it uses. 4 Suppertagging The DLA strategy we adopt in this research is based on supertagging, which is a simple instance of sequential tagging with a larger, more linguistically-diverse tag set than is conventionally the case, e.g., with part-of-speech tagging. Below, we describe the pseudo-likelihood CRF model we base our supertagger on and outline the feature space for the two grammars. 4.1 Pseudo-likeli</context>
</contexts>
<marker>Matsumoto, Kitauchi, Yamashita, Hirano, Matsuda, Takaoka, Asahara, 2003</marker>
<rawString>Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshitaka Hirano, Hiroshi Matsuda, Kazuma Takaoka, and Masayuki Asahara. 2003. Japanese Morphological Analysis System ChaSen Version 2.3.3 Manual. Technical report, NAIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>Radu Florian</author>
</authors>
<title>Transformation-based learning in the fast lane.</title>
<date>2001</date>
<booktitle>In Proc. of the 2nd Annual Meeting of the North American Chapter of Association for Computational Linguistics (NAACL2001),</booktitle>
<volume>40</volume>
<pages>pages</pages>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="22419" citStr="Ngai and Florian, 2001" startWordPosition="3677" endWordPosition="3680">tive of whether they represent known or unknown lexical items. 5.1 Baseline: Unigram Supertagger As a baseline model, we use a simple unigram supertagger trained based on maximum likelihood estimation over the relevant training data, i.e. the tag for each token instance of a given word is predicted by: In the instance that was not observed in the training data, we back off to the majority lexical type in the training data. 5.2 Benchmark: fnTBL In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FNTBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set. FNTBL is a transformation-based learner that is distributed with pre-optimised POS tagging modules for English and other European languages that can be redeployed over the task of supertagging. Following Baldwin (2005b), the only modifications we make to the default English POS tagging methodology are: (1) to set the default lexical types for singular common and proper nouns to n - c le and n - pn le, respectively; and (2) reduce the threshold score for lexical and context transformation rules to 1. It is important to realise that,</context>
</contexts>
<marker>Ngai, Florian, 2001</marker>
<rawString>Grace Ngai and Radu Florian. 2001. Transformation-based learning in the fast lane. In Proc. of the 2nd Annual Meeting of the North American Chapter of Association for Computational Linguistics (NAACL2001), pages 40– 7, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Dan Flickinger</author>
<author>Kristina Toutanova</author>
<author>Christoper D Manning</author>
</authors>
<title>LinGO Redwoods: A rich and dynamic treebank for HPSG.</title>
<date>2002</date>
<booktitle>In Proc. of The First Workshop on Treebanks and Linguistic Theories (TLT2002), Sozopol,</booktitle>
<contexts>
<context position="1143" citStr="Oepen et al., 2002" startWordPosition="165" endWordPosition="168">nds of training sentences. We show that it is possible to achieve start-of-the-art results for both languages using maximally language-independent lexical features. Further, we explore the performance of the models at the type- and token-level, demonstrating their superior performance when compared to a unigram-based baseline and a transformation-based learning approach. 1 Introduction Over recent years, there has been a resurgence of interest in the use of precision grammars in NLP tasks, due to advances in parsing algorithm development, grammar development tools and raw computational power (Oepen et al., 2002b). Precision grammars are defined as implemented grammars of natural language which capture finegrained linguistic distinctions, and are generative in the sense of distinguishing between grammatical and ungrammatical inputs (or at least have some in-built notion of linguistic “markedness”). Additional characteristics of precision grammars are that they are frequently bidirectional, and output a rich semantic abstraction for each spanning parse of the input string. Examples include DELPH-IN grammars such as the English Resource Grammar (Flickinger, 2002; Uszkoreit, 2002), the various PARGRAM g</context>
<context position="11666" citStr="Oepen et al., 2002" startWordPosition="1872" endWordPosition="1875">s the lexical stem, and &amp;quot; dog n 1 rel&amp;quot; introduces an ad hoc predicate name for the lexical item to use in constructing a semantic representation. In the context of the ERG and JACY, DLA equates to learning the range of lexical types a given lexeme occurs with, and generating a single lexical item for each. Recent development of the ERG and JACY has been tightly coupled with treebank annotation, and all major versions of both grammars are deployed over a common set of dynamically-updateable treebank data to help empirically trace the evolution of the grammar and retrain parse selection models (Oepen et al., 2002a; Bond et al., 2004). This serves as a source of training and test data for building our supertaggers, as detailed in Table 1. In translating our treebank data into a form that can be understood by a supertagger, multiword expressions (MWEs) pose a slight problem. Both the ERG and JACY include multiword lexical items, which can either be strictly continuous (e.g. hot line) or optionally discontinuous (e.g. transitive English verb particle constructions, such as pick up as in Kim picked the book up). Strictly continuous lexical items are described by way of a single whitespace-delimited lexica</context>
</contexts>
<marker>Oepen, Flickinger, Toutanova, Manning, 2002</marker>
<rawString>Stephan Oepen, Dan Flickinger, Kristina Toutanova, and Christoper D. Manning. 2002a. LinGO Redwoods: A rich and dynamic treebank for HPSG. In Proc. of The First Workshop on Treebanks and Linguistic Theories (TLT2002), Sozopol, Bulgaria.</rawString>
</citation>
<citation valid="false">
<booktitle>2002b. Collaborative Language Engineering.</booktitle>
<editor>Stephan Oepen, Dan Flickinger, Jun’ichi Tsujii, and Hans Uszkoreit, editors.</editor>
<publisher>CSLI Publications,</publisher>
<location>Stanford, USA.</location>
<marker></marker>
<rawString>Stephan Oepen, Dan Flickinger, Jun’ichi Tsujii, and Hans Uszkoreit, editors. 2002b. Collaborative Language Engineering. CSLI Publications, Stanford, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>The University of Chicago Press,</publisher>
<location>Chicago, USA.</location>
<contexts>
<context position="10546" citStr="Pollard and Sag (1994)" startWordPosition="1673" endWordPosition="1676">red format. 3 Task and Resources In this section, we outline the resources targeted in this research, namely the English Resource Grammar (ERG: Flickinger (2002), Copestake and Flickinger (2000)) and the JACY grammar of Japanese (Siegel and Bender, 2002). Note that our choice of the ERG and JACY as testbeds for experimentation in this paper is somewhat arbitrary, and that we could equally run experiments over any Grammar Matrix-based grammar for which there is treebank data. Both the ERG and JACY are implemented open-source broad-coverage precision Headdriven Phrase Structure Grammars (HPSGs: Pollard and Sag (1994)). A lexical item in each of the grammars consists of a unique identifier, a lexical type (a leaf type of a type hierarchy), an orthography, and a semantic relation. For example, in the English grammar, the lexical item for the noun dog is simply: dog_n1 := n_-_c_le &amp; [ STEM &lt; &amp;quot;dog&amp;quot; &gt;, SYNSEM [ LKEYS.KEYREL.PRED &amp;quot;_dog_n_1_rel&amp;quot; ] ]. in which the lexical type of n - c le encodes the fact that dog is a noun which does not subcategorise for any other constituents and which is countable, &amp;quot;dog&amp;quot; specifies the lexical stem, and &amp;quot; dog n 1 rel&amp;quot; introduces an ad hoc predicate name for the lexical item to</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-driven Phrase Structure Grammar. The University of Chicago Press, Chicago, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the ACL and 3rd Annual Meeting of the NAACL (ACL-02),</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="2307" citStr="Riezler et al., 2002" startWordPosition="335" endWordPosition="338">Flickinger, 2002; Uszkoreit, 2002), the various PARGRAM grammars (Butt et al., 1999), and the Edinburgh CCG parser (Bos et al., 2004). Due to their linguistic complexity, precision grammars are generally hand-constructed and thus restricted in size and coverage. Attempts to (semi-)automate the process of expanding the coverage of precision grammars have focused on either: (a) constructional coverage, e.g. in the form of error mining for constructional expansion (van Noord, 2004; Zhang and Kordoni, 2006), or relaxation of lexico-grammatical constraints to support partial and/or robust parsing (Riezler et al., 2002); or (b) lexical coverage, e.g. in bootstrapping from a pre-existing grammar and lexicon to learn new lexical items (Baldwin, 2005a). Our particular interest in this paper is in the latter of these two, that is the development of methods for automatically expanding the lexical coverage of an existing precision grammar, or more broadly deep lexical acquisition (DLA hereafter). In this, we follow Baldwin (2005a) in assuming a semi-mature precision grammar with a fixed inventory of lexical types, based on which we learn new lexical items. For the purposes of this paper, we focus specifically on s</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell III, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques. In Proc. of the 40th Annual Meeting of the ACL and 3rd Annual Meeting of the NAACL (ACL-02), Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. of the 3rd International Conference on Human Language Technology Research and 4th Annual Meeting of the NAACL (HLTNAACL</booktitle>
<pages>213--20</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="18156" citStr="Sha and Pereira, 2003" startWordPosition="2945" endWordPosition="2948">distribution over the model parameters and derive a maximum a posteriori (MAP) estimate, . We use a zero-mean Gaussian prior, with the probability density function . This yields a log-pseudo-likelihood objective function of: In order to train the model, we maximize (4). While the log-pseudo-likelihood cannot be maximised for the parameters, , in closed form, it is a convex function, and thus we resort to numerical optimisation to find the globally optimal parameters. We use L-BFGS, an iterative quasi-Newton optimisation method, which performs well for training log-linear models (Malouf, 2002; Sha and Pereira, 2003). Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. As we cannot observe label values for the test data we must use when decoding. The Viterbi algorithm is used to find the maximum posterior probability alignment for test sentences, 4.2 CRF features One of the strengths of the CRF model is that it supports the use of a large number of nonindependent and overlapping features of the input sentence. Table 2 lists the word context and lexical features used by the CRF model (shared across both grammars). Word context features were extracted fr</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proc. of the 3rd International Conference on Human Language Technology Research and 4th Annual Meeting of the NAACL (HLTNAACL 2003), pages 213–20, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie Siegel</author>
<author>Emily M Bender</author>
</authors>
<title>Efficient deep processing of Japanese.</title>
<date>2002</date>
<booktitle>In Proc. of the 3rd Workshop on Asian Language Resources and International Standardization,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="10178" citStr="Siegel and Bender, 2002" startWordPosition="1615" endWordPosition="1618">2005a)) has tended to take an in vitro DLA approach, in extrapolating away from a DLR to corpus or web data, and analysing occurrences of words through the conduit of an external resource (e.g. a secondary parser or POS tagger). In vitro DLA can also take the form of resource translation, in mapping one DLR onto another to arrive at the lexical information in the desired format. 3 Task and Resources In this section, we outline the resources targeted in this research, namely the English Resource Grammar (ERG: Flickinger (2002), Copestake and Flickinger (2000)) and the JACY grammar of Japanese (Siegel and Bender, 2002). Note that our choice of the ERG and JACY as testbeds for experimentation in this paper is somewhat arbitrary, and that we could equally run experiments over any Grammar Matrix-based grammar for which there is treebank data. Both the ERG and JACY are implemented open-source broad-coverage precision Headdriven Phrase Structure Grammars (HPSGs: Pollard and Sag (1994)). A lexical item in each of the grammars consists of a unique identifier, a lexical type (a leaf type of a type hierarchy), an orthography, and a semantic relation. For example, in the English grammar, the lexical item for the noun</context>
</contexts>
<marker>Siegel, Bender, 2002</marker>
<rawString>Melanie Siegel and Emily M. Bender. 2002. Efficient deep processing of Japanese. In Proc. of the 3rd Workshop on Asian Language Resources and International Standardization, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Uszkoreit</author>
</authors>
<title>New chances for deep linguistic processing.</title>
<date>2002</date>
<booktitle>In Proc. of the 19th International Conference on Computational Linguistics (COLING</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="1720" citStr="Uszkoreit, 2002" startWordPosition="249" endWordPosition="250">computational power (Oepen et al., 2002b). Precision grammars are defined as implemented grammars of natural language which capture finegrained linguistic distinctions, and are generative in the sense of distinguishing between grammatical and ungrammatical inputs (or at least have some in-built notion of linguistic “markedness”). Additional characteristics of precision grammars are that they are frequently bidirectional, and output a rich semantic abstraction for each spanning parse of the input string. Examples include DELPH-IN grammars such as the English Resource Grammar (Flickinger, 2002; Uszkoreit, 2002), the various PARGRAM grammars (Butt et al., 1999), and the Edinburgh CCG parser (Bos et al., 2004). Due to their linguistic complexity, precision grammars are generally hand-constructed and thus restricted in size and coverage. Attempts to (semi-)automate the process of expanding the coverage of precision grammars have focused on either: (a) constructional coverage, e.g. in the form of error mining for constructional expansion (van Noord, 2004; Zhang and Kordoni, 2006), or relaxation of lexico-grammatical constraints to support partial and/or robust parsing (Riezler et al., 2002); or (b) lexi</context>
</contexts>
<marker>Uszkoreit, 2002</marker>
<rawString>Hans Uszkoreit. 2002. New chances for deep linguistic processing. In Proc. of the 19th International Conference on Computational Linguistics (COLING 2002), Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Error mining for wide-coverage grammar engineering.</title>
<date>2004</date>
<booktitle>In Proc. of the 42nd Annual Meeting of the ACL,</booktitle>
<location>Barcelona,</location>
<marker>van Noord, 2004</marker>
<rawString>Gertjan van Noord. 2004. Error mining for wide-coverage grammar engineering. In Proc. of the 42nd Annual Meeting of the ACL, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Zhang</author>
<author>Valia Kordoni</author>
</authors>
<title>A statistical approach towards unknown word type prediction for deep grammars.</title>
<date>2005</date>
<booktitle>In Proc. of the Australasian Language Technology Workshop</booktitle>
<pages>24--31</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="4583" citStr="Zhang and Kordoni (2005)" startWordPosition="711" endWordPosition="714">ion for Computational Linguistics either: (a) off-line for the purposes of rounding out the coverage of a static lexicon, in which case we are generally interested in globally maximising precision over a given corpus and hence predicting the single most plausible lexical type for each word token (off-line DLA: Baldwin (2005b)); or (b) on the fly for a given input string to temporarily expand lexical coverage and achieve a spanning parse, in which case we are interested in maximising recall by producing a (possibly weighted) list of lexical item hypotheses to run past the grammar (on-line DLA: Zhang and Kordoni (2005)). Our immediate interest in this paper is in the first of these tasks, although we would ideally like to develop an off-line method which is trivially portable to the second task of on-line DLA. In this research, we focus particularly on the Grammar Matrix-based DELPH-IN family of grammars (Bender et al., 2002), which includes grammars of English, Japanese, Norwegian, Modern Greek, Portuguese and Korean. The Grammar Matrix is a framework for streamlining and standardising HPSG-based multilingual grammar development. One property of Grammar Matrixbased grammars is that they are strongly lexica</context>
</contexts>
<marker>Zhang, Kordoni, 2005</marker>
<rawString>Yi Zhang and Valia Kordoni. 2005. A statistical approach towards unknown word type prediction for deep grammars. In Proc. of the Australasian Language Technology Workshop 2005, pages 24–31, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Zhang</author>
<author>Valia Kordoni</author>
</authors>
<title>Automated deep lexical acquisition for robust open texts processing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="2194" citStr="Zhang and Kordoni, 2006" startWordPosition="319" endWordPosition="322">r each spanning parse of the input string. Examples include DELPH-IN grammars such as the English Resource Grammar (Flickinger, 2002; Uszkoreit, 2002), the various PARGRAM grammars (Butt et al., 1999), and the Edinburgh CCG parser (Bos et al., 2004). Due to their linguistic complexity, precision grammars are generally hand-constructed and thus restricted in size and coverage. Attempts to (semi-)automate the process of expanding the coverage of precision grammars have focused on either: (a) constructional coverage, e.g. in the form of error mining for constructional expansion (van Noord, 2004; Zhang and Kordoni, 2006), or relaxation of lexico-grammatical constraints to support partial and/or robust parsing (Riezler et al., 2002); or (b) lexical coverage, e.g. in bootstrapping from a pre-existing grammar and lexicon to learn new lexical items (Baldwin, 2005a). Our particular interest in this paper is in the latter of these two, that is the development of methods for automatically expanding the lexical coverage of an existing precision grammar, or more broadly deep lexical acquisition (DLA hereafter). In this, we follow Baldwin (2005a) in assuming a semi-mature precision grammar with a fixed inventory of lex</context>
</contexts>
<marker>Zhang, Kordoni, 2006</marker>
<rawString>Yi Zhang and Valia Kordoni. 2006. Automated deep lexical acquisition for robust open texts processing. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC 2006), Genoa, Italy.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>