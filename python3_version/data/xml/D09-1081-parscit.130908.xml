<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004590">
<title confidence="0.997237">
Estimating Semantic Distance Using Soft Semantic Constraints
in Knowledge-Source–Corpus Hybrid Models
</title>
<author confidence="0.980585">
Yuval Marton*†, Saif Mohammad†, and Philip Resnik*†
</author>
<affiliation confidence="0.99883475">
*Department of Linguistics and
†Laboratory for Computational Linguistics and Information Processing,
Institute for Advanced Computer Studies.
University of Maryland, College Park, MD 20742-7505, USA.
</affiliation>
<email confidence="0.999395">
{ymarton,saif,resnik}@umiacs.umd.edu
</email>
<sectionHeader confidence="0.997396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999912086956522">
Strictly corpus-based measures of seman-
tic distance conflate co-occurrence infor-
mation pertaining to the many possible
senses of target words. We propose a
corpus–thesaurus hybrid method that uses
soft constraints to generate word-sense-
aware distributional profiles (DPs) from
coarser “concept DPs” (derived from a
Roget-like thesaurus) and sense-unaware
traditional word DPs (derived from raw
text). Although it uses a knowledge
source, the method is not vocabulary-
limited: if the target word is not in the
thesaurus, the method falls back grace-
fully on the word’s co-occurrence infor-
mation. This allows the method to access
valuable information encoded in a lexical
resource, such as a thesaurus, while still
being able to effectively handle domain-
specific terms and named entities. Exper-
iments on word-pair ranking by semantic
distance show the new hybrid method to
be superior to others.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992398">
Semantic distance is a measure of the closeness
in meaning of two concepts. People are consis-
tent judges of semantic distance. For example, we
can easily tell that the concepts of “exercise” and
“jog” are closer in meaning than “exercise” and
“theater”. Studies asking native speakers of a lan-
guage to rank word pairs in order of semantic dis-
tance confirm this—average inter-annotator corre-
lation on ranking word pairs in order of semantic
distance has been repeatedly shown to be around
0.9 (Rubenstein and Goodenough, 1965; Resnik,
1999).
A number of natural language tasks such as ma-
chine translation (Lopez, 2008) and word sense
disambiguation (Banerjee and Pedersen, 2003;
McCarthy, 2006), can be framed as semantic
distance problems. Thus, developing automatic
measures that are in-line with human notions of
semantic distance has received much attention.
These automatic approaches to semantic distance
rely on manually created lexical resources such as
WordNet, large amounts of text corpora, or both.
WordNet-based information content measures
have been successful (Hirst and Budanitsky,
2005), but there are significant limitations on their
applicability. They can be applied only if a Word-
Net exists for the language of interest (which is
not the case for the “low-density” languages); and
even if there is a WordNet, a number of domain-
specific terms may not be encoded in it. On the
other hand, corpus-based distributional measures
of semantic distance, such as cosine and α-skew
divergence (Dagan et al., 1999), rely on raw text
alone (Weeds et al., 2004; Mohammad, 2008).
However, when used to rank word pairs in order
of semantic distance or correct real-word spelling
errors, they have been shown to perform poorly
(Weeds et al., 2004; Mohammad and Hirst, 2006).
Mohammad and Hirst (2006) and Patwardhan
and Pedersen (2006) argued that word sense ambi-
guity is a key reason for the poor performance of
traditional distributional measures, and they pro-
posed hybrid approaches that are distributional in
nature, but also make use of information in lexical
resources such as published thesauri and WordNet.
However, both these approaches can be applied to
estimate the semantic distance between two terms
only if both terms exist in the lexical resource they
rely on. We know lexical resources tend to have
limited vocabulary and a large number of domain-
</bodyText>
<page confidence="0.976325">
775
</page>
<note confidence="0.9966195">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 775–783,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999436955555555">
specific terms are usually not included.
It should also be noted that similarity values
from different distance measures are not compa-
rable (even after normalization to the same scale),
that is, a similarity score of .75 as per one distance
measure does not correspond to the same seman-
tic distance as a similarity score of .75 from an-
other distance measure.1 Thus if one uses two
independent distance measures, in this case: one
resource-reliant and one only corpus-dependent,
then these two measures are not comparable (and
hence cannot be used in tandem), even if both
rely—partially or entirely—on distributional cor-
pus statistics.
We propose a hybrid semantic distance method
that inherently combines the elements of a
resource-reliant measure and a strictly corpus-
dependent measure by imposing resource-reliant
soft constraints on the corpus-dependent model.
We choose the Mohammad and Hirst (2006)
method as the resource-reliant method and not
one of the WordNet-based measures because, un-
like the WordNet-based measures, the Moham-
mad and Hirst method is distributional in nature
and so lends itself immediately for combination
with traditional distributional similarity measures.
Our new hybrid method combines concept–word
co-occurrence information (the Mohammad and
Hirst distributional profiles of thesaurus concepts
(DPC)) with word–word co-occurrence informa-
tion, to generate word-sense-biased distributional
profiles. The “pure” corpus-based distributional
profile (a.k.a. co-occurrence vector, or word asso-
ciation vector), for some target word u, is biased
with soft constraints towards each of the concepts
c that list u in the thesaurus, to create a distribu-
tional profile that is specific to u in the sense that
is most related to the other words listed under c.
Thus, this method can make more fine-
grained distinctions than the Mohammad and Hirst
method, and yet uses word sense information.2
Our proposed method falls back gracefully to rely
only on word-word co-occurrence information if
any of the target terms is not listed in the lexical re-
source. Experiments on the word-pair ranking task
</bodyText>
<footnote confidence="0.964949625">
1All we can infer is that if w1 and w2 have a similarity
score of .75 and w3 and w4 have a score of .5 by the same
distance measure, then w1–w2 are closer in meaning than
w3–w4.
2Even though Mohammad and Hirst (2006) use thesaurus
categories as coarse concepts, their algorithm can be applied
using more finer-grained thesaurus word groupings (para-
graphs and semicolon units), as well.
</footnote>
<bodyText confidence="0.9994925625">
on three different datasets show that the our pro-
posed hybrid measure outperforms all other com-
parable distance measures.
Mohammad and Hirst (2007) show that their
method can be used to compute semantic dis-
tance in a resource poor language L1 by com-
bining its text with a thesaurus in a resource-rich
language L2 using an L1–L2 bilingual lexicon to
create cross-lingual distributional profiles of con-
cepts, that is, L2 word co-occurrence profiles of
L1 thesaurus concepts. Since our method makes
use of the Mohammad and Hirst DPCs, it can just
as well make use of their cross-lingual DPCs, to
compute semantic distance in a resource-poor lan-
guage, just as they did. We leave that for future
work.
</bodyText>
<sectionHeader confidence="0.962046" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.999636666666667">
Strictly speaking, semantic distance/closeness is
a property of lexical units—a combination of the
surface form and word sense.3 Two terms are con-
sidered to be semantically close if there is a lex-
ical semantic relation between them. Such a re-
lation may be a classical relation such as hyper-
nymy, troponymy, meronymy, and antonymy, or
it may be what have been called an ad-hoc non-
classical relation, such as cause-and-effect (Mor-
ris and Hirst, 2004). If the closeness in meaning
is due to certain specific classical relations such as
hypernymy and troponymy, then the terms are said
to be semantically similar. Semantic relatedness
is the term used to describe the more general form
of semantic closeness caused by any semantic re-
lation (Hirst and Budanitsky, 2005). So the nouns
liquid and water are both semantically similar and
semantically related, whereas the nouns boat and
rudder are semantically related, but not similar.
The next three sub-sections describe three kinds
of automatic distance measures: (1) lexical-
resource-based measures that rely on a manually
created resource such as WordNet; (2) corpus-
based measures that rely only on co-occurrence
statistics from large corpora; and (3) hybrid mea-
sures that are distributional in nature, and that also
exploit the information in a lexical resource.
</bodyText>
<subsectionHeader confidence="0.969411">
2.1 Lexical-resource-based measures
</subsectionHeader>
<bodyText confidence="0.995099">
WordNet is a manually-created hierarchical net-
work of nodes (taxonomy), where each node in
</bodyText>
<footnote confidence="0.998657333333333">
3The notion of semantic distance can be generalized, of
course, to larger units such as phrases, sentences, passages,
and so on (Landauer et al., 1998).
</footnote>
<page confidence="0.998258">
776
</page>
<bodyText confidence="0.999960307692308">
the network represents a fine-grained concept or
word sense. An edge between two nodes rep-
resents a lexical semantic relation such as hy-
pernymy and troponymy. WordNet-based mea-
sures consider two terms to be close if they occur
close to each other in the network (connected by
only a few arcs), if their definitions share many
terms (Banerjee and Pedersen, 2003; Patwardhan
and Pedersen, 2006), or if they share a lot of infor-
mation (Lin, 1998; Resnik, 1999). The length of
each arc/link (distance between nodes) can be as-
sumed a unit length, or can be computed from cor-
pus statistics. Within WordNet, the is-a hierarchy
is much more well-developed than that of other
lexical semantic relations. So, not surprisingly,
the best WordNet-based measures are those that
rely only on the is-a hierarchy. Therefore, they
are good at measuring semantic similarity (e.g.,
doctor–physician), but not semantic relatedness
(e.g., doctor–scalpel). Further, the measures can
only be used in languages that have a (sufficiently
developed) WordNet. WordNet sense information
has been criticized to be too fine grained (Agirre
and Lopez de Lacalle Lekuona, 2003; Navigli,
2006). See Hirst and Budanitsky (2005) for a com-
prehensive survey of WordNet-based measures.
</bodyText>
<subsectionHeader confidence="0.996384">
2.2 Corpus-based measures
</subsectionHeader>
<bodyText confidence="0.9999733125">
Strictly corpus-based measures of distributional
similarity rely on the hypothesis that words that
occur in similar context tend to be semantically
close (Firth, 1957; Harris, 1940). The set of
contexts of each target word u is represented by
its distributional profile (DP)—the set of words
that tend to co-occur with u within a certain dis-
tance, along with numeric scores signifying this
co-occurrence tendency with u. Then measures
such as cosine or α-skew divergence are used to
determine how close the DPs of the two target
words are. See Section 3 for more details and re-
lated work. These measures are very appealing
because they rely simply on raw text, but, as de-
scribed earlier, when used to rank word pairs in
order of semantic distance, or to correct real-word
spelling errors, they perform poorly, compared
to the WordNet-based measures. See Weeds et
al. (2004), Mohammad (2008), and Curran (2004)
for detailed surveys of distributional measures.
As Mohammad and Hirst (2006) point out, the
DP of a word u conflates information about the
potentially many senses of u. For example, con-
sider the following. The noun bank has two senses
“river bank” and “financial institution”. Assume
that bank, when used in the “financial institu-
tion” sense, co-occurred with the noun money 100
times in a corpus. Similarly, assume that bank,
when used in the “river bank” sense, co-occurred
with the noun boat 80 times. So the DP of bank
will have co-occurrence information with money
as well as boat:
</bodyText>
<equation confidence="0.9404055">
DPW(bank):
money,100; boat,80; bond,70; fish,77; ...
</equation>
<bodyText confidence="0.879271">
Assume that the DP of the word ATM is:
</bodyText>
<equation confidence="0.8885425">
DPW(ATM):
money,120; boat,0; bond,90; fish,0; ...
</equation>
<bodyText confidence="0.999971444444445">
Thus the distributional distance of bank with ATM
will be some sort of an average of the seman-
tic distance between the “financial institution” and
“ATM” senses and the semantic distance between
the “river bank” and “ATM” senses. However, in
various natural language tasks, we need the se-
mantic distance between the intended senses of
bank and ATM, which often also tends to be the
semantic distance between their closest senses.
</bodyText>
<subsectionHeader confidence="0.999235">
2.3 Hybrid measures
</subsectionHeader>
<bodyText confidence="0.99995964">
Both Mohammad and Hirst (2006) and Patward-
han and Pedersen (2006) proposed measures that
are not only distributional in nature but also rely
on a lexical resource to exploit the manually en-
coded information therein as well as to overcome
the sense-conflation problem (described in sec-
tion 2.2). Since we essentially combine the Mo-
hammad and Hirst method with a “pure” word-
based distributional measure to create our hybrid
approach, we briefly describe their method here.
Mohammad and Hirst (2006) generate separate
distributional profiles for the different senses of
a word, without using any sense-annotated data.
They use the categories in a Roget-style thesaurus
(Macquaries (Bernard, 1986)) as coarse senses or
concepts. There are about 1000 categories in a
thesaurus, and each category has on average 120
closely related words. A word may be found in
more than one category if it has multiple meaning.
They use a simple unsupervised algorithm to de-
termine the vector of words that tend to co-occur
with each concept and the corresponding strength
of association (a measure of how strong the ten-
dency to co-occur is). The target word u will be
assigned one DPC for each of the concepts that
</bodyText>
<page confidence="0.981994">
777
</page>
<bodyText confidence="0.9708275">
list u. Below are example DPCs of the two con-
cepts pertaining to bank:4
the target word u co-occurs with with all other
words:5
</bodyText>
<equation confidence="0.99603625">
DPC(“fin. inst.”):
money,1000; boat,32; bond,705; fish,0; ...
DPC(“river bank”):
money,5; boat,863; bond,0; fish,948; ...
</equation>
<bodyText confidence="0.999983694444444">
The distance between two words u, v is deter-
mined by calculating the closeness of each of the
DPCs of u to each of DPCs of v, and the closest
DPC-pair distance is chosen.
Mohammad and Hirst (2006) show that their ap-
proach performs better than other strictly corpus-
based approaches that they experimented with.
However, all those experiments were on word-
pairs that were listed in the thesaurus. Their ap-
proach is not applicable otherwise. In Sections 3
and 4 we show how cosine–log-likelihood-ratio
(or any comparable distributional measure) can be
combined with the Mohammad and Hirst DPCs to
form a hybrid approach that is not limited to the
vocabulary of a lexical resource.
Erk and Pad´o (2008) proposed a way of rep-
resenting a word sense in context by biasing the
target word’s DP according to the context sur-
rounding a target (specific) occurrence of the tar-
get word. They use dependency relations and se-
lectional preferences of the target word and com-
bine multiple DPs of words appearing in the con-
text of the target occurrence, in a manner so as
to give more weight to words co-occurring with
both the target word and the target occurrence’s
context words. The advantage of this approach
is that it does not rely on a thesaurus or Word-
Net. Its disadvantage is that it relies on depen-
dency relations and selectional preferences infor-
mation, and that the context information it uses in
order to determine the word sense is quite limited
(only the words surrounding a single occurrence
of the and hence the representation of that sense
might not be sufficiently accurate. Their approach
effectively assumes that each occurrence of a word
has a unique sense.
</bodyText>
<sectionHeader confidence="0.8396295" genericHeader="method">
3 Distributional Measures with Soft
Semantic Constraints
</sectionHeader>
<bodyText confidence="0.999936333333333">
Traditional distributional profiles of words (DPW)
give word–word co-occurrence frequencies. For
example, DPW(u) gives the number of times
</bodyText>
<footnote confidence="0.993838333333333">
4The relatively large co-occurrence frequency values for
DPCs as compared to DPWs is because a concept can be ref-
ered to by many words (on average 100).
</footnote>
<equation confidence="0.929782">
DPW(u):
w1,f(u,w1); w2,f(u,w2); w3,f(u,w3); ...
</equation>
<bodyText confidence="0.999938666666667">
where f stands for co-occurrence frequency (and
can be generalized to stand for any strength
of association (SoA) measure such as the log-
likelihood ratio). Mohammad and Hirst create
concept–word co-occurrence vectors, “distribu-
tional profiles of concepts” (DPCs), from non-
annotated corpus. For example, DPC(c) gives the
number of times the concept (thesaurus category)
c co-occurs with all the words in a corpus.
</bodyText>
<equation confidence="0.9582575">
DPC(c):
w1,f(c,w1); w2,f(c,w2); w3,f(c,w3); ...
</equation>
<bodyText confidence="0.999956777777778">
A target word u that appears under thesaurus con-
cepts ci, ..., cn would be assigned to DPC(ci), ...,
DPC(cn). Therefore, if a target word v also ap-
pears under some same concept c, the DPCs of u
and v would be indistinguishable.
We propose the creation of distributional pro-
files of word senses (DPWS(uc)) that approximate
the SoA of the target word u, when used in sense
c, with each of the words in the corpus:
</bodyText>
<equation confidence="0.9527075">
DPWS(u,):
w1,f(u.,w1); w2,f(u.,w2); w3,f(u.,w3); ...
</equation>
<bodyText confidence="0.9994714">
In order to get exact counts, one needs sense-
annotated data. However, such data is expensive
to create, and is scarce. Therefore, we propose
estimating these counts from the DPW and DPC
counts:
</bodyText>
<equation confidence="0.910376">
f(uc, wi) = p(c|wi) ~ f(u, wi) (1)
</equation>
<bodyText confidence="0.999633909090909">
where the conditional probability p(c|wi) is calcu-
lated from the co-occurrence frequencies in DPCs;
and the co-occurrence count f(u, wi) is calcu-
lated from DPWs. If the target word is not in
the thesaurus’s vocabulary, then we assume uni-
form distribution over all concepts, and in prac-
tice use a single sense, and take the conditional
probability to be 1. Since the method takes sense-
proportional co-occurrence counts, we will refer
to this method as the hybrid-sense-proportional-
counts method (or, hybrid-prop for short).
</bodyText>
<footnote confidence="0.9480534">
5The dimensions of the DP co-occurrence vector can be
defined arbitrarily, and do not have to correspond to the words
in the vocabulary. The most notable alternative representation
is the Latent Semantic Analysis and its variants (Landauer et
al., 1998; Finkelstein et al., 2002; Budiu et al., 2006).
</footnote>
<page confidence="0.9926">
778
</page>
<bodyText confidence="0.999512333333333">
For example, below is the DPWS of bank in
the “financial institution” sense, calculated from
its DPW and DPCs:
</bodyText>
<equation confidence="0.994802166666667">
DPW(bank):
money,100; boat,80; bond,70; fish,77; ...
DPC(“fin. inst.”):
money,1000; boat,32; bond,705; fish,0; ...
DPC(“river bank”):
money,5; boat,863; bond,0; fish,948; ...
DPWS(bank“fin.inst.”):
32
money,(1000+5 x 100); boat, (32+863 x 80);
bond, 705
(705+0 x 70); fish,( 0
0+948 x 77); ...
</equation>
<bodyText confidence="0.99682525">
Once the DPWS are calculated, any counts-
based SoA and distance measures can be ap-
plied. For example, in this work we use log-
likelihood ratio (Dunning, 1993) to determine
the SoA between a word sense and co-occurring
words, and cosine to determine the distance be-
tween two DPWS’s log likelihood vectors (Mc-
Donald, 2000). We also contrast this measure with
cosine of conditional probabilities vectors. Given
two target words, we determine the distance be-
tween each of their DPWS pairings and the closest
DPWS-pair distance is chosen.
</bodyText>
<subsectionHeader confidence="0.998908">
3.1 The hybrid-sense-filtered-counts method
</subsectionHeader>
<bodyText confidence="0.988148125">
Since the DPCs are created in an unsupervised
manner, they are expected to be somewhat noisy.
Therefore, we also experimented with a variant of
the method proposed above, that simply makes use
of whether the conditional probability p(c|wi) is
greater than 0 or not:
� (2)
= f (u, wi) If p(c|wi) &gt; 0
f (uc, wi) 0Otherwise
Since this method essentially filters out collocates
that are likely not relevant to the target sense c of
the target word u, we will refer to this method
as the hybrid-sense-filtered-counts method (or,
just hybrid-filt for short). Below is an example
hybrid-filtered DPWS of bank in the “financial in-
stitution” sense:
</bodyText>
<equation confidence="0.948192">
DPWS(bank“fin.inst.”:
money,100); boat,80; bond,70; ...
</equation>
<bodyText confidence="0.9997755">
Note that the collocate fish is now filtered out,
whereas bank’s co-occurrence counts with money,
boat, and bond are left as is (and not sense-
proportionally attenuated).
</bodyText>
<sectionHeader confidence="0.998806" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9993665625">
We evaluated various methods on the task of
ranking word pairs in order of semantic dis-
tance. These methods included our sense-biased
methods as well as several baselines: the Mo-
hammad and Hirst (2006) DPC-based methods,
the traditional word-based distributional similar-
ity methods, and several Latent Semantic Analysis
(LSA)-based methods. We used three testsets and
their corresponding human judgment gold stan-
dards: (1) the Rubenstein and Goodenough (1965)
set of 65 noun pairs—denoted RG-65; (2) the
WordSimilarity-353 (Finkelstein et al., 2002) set
of 353 noun pairs (which include the RG-65
pairs) of which we discarded of one repeating
pair—denoted WS-353; and (3) the Resnik and
Diab (2000) set of 27 verb pairs—denoted RD-00.
</bodyText>
<subsectionHeader confidence="0.997148">
4.1 Corpora and Pre-processing
</subsectionHeader>
<bodyText confidence="0.999990076923077">
We generated distributional profiles (DPWs
and DPCs) from the British National Corpus
(BNC) (Burnard, 2000), which is a balanced cor-
pus. We lowercased the characters, and stripped
numbers, punctuation marks, and any SGML-like
syntactic tags, but kept sentence boundary mark-
ers. The BNC contained 102,100,114 tokens of
546,299 types (vocabulary size) after tokenization.
For the verb set, we also lemmatized this corpus.
We considered two words as co-occurring if
they occurred in a window of ±5 words from each
other. We stoplisted words that co-occurred with
more than 2000 word types.
</bodyText>
<sectionHeader confidence="0.522851" genericHeader="method">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999667166666667">
The Spearman rank correlations of the automatic
rankings of the RG-65, WS353, and RD-00 test-
sets with the corresponding gold-standard human
rankings is listed in Table 1.6 The higher the
Spearman rank correlation, the more accurate is
the distance measure.
</bodyText>
<subsubsectionHeader confidence="0.914267">
4.2.1 Results on the RG-65 testset
</subsubsectionHeader>
<bodyText confidence="0.999984857142857">
Baselines. We replicated the traditional word-
based distributional distance measure using co-
sine of vectors (DPs) containing conditional prob-
abilities (word-cos-cp). Its rank correlation of
.53 is close to the correlation of .54 reported in
Mohammad and Hirst (2006), hereafter MH06.
We replicated the MH06 concept-based approach
</bodyText>
<footnote confidence="0.953924">
6Certain experiments were not pursued as they were re-
dundant in supporting our claims.
</footnote>
<page confidence="0.99414">
779
</page>
<table confidence="0.996463">
Method RG-65 WS-353 RD-00
Baselines (replicated):
Traditional distributional measures
LSA .56 .47 .55
GLSA-cos-pmi .18 n.p. n.p.
GLSA-cos-ll .47 n.p. .29
New methods:
hybrid-prop-cos-ll .72 .49 .53
hybrid-prop*-cos-ll .69 .46 .45
hybrid-filt-cos-ll .73 .54 .38
hybrid-filt*-cos-ll .77 .54 .39
hybrid-prop*-cos-pmi .58 .43 .71
hybrid-filt*-cos-pmi .61 .42 .64
</table>
<tableCaption confidence="0.99939">
Table 1: Spearman rank correlation on RG-65,
</tableCaption>
<bodyText confidence="0.984729714285714">
WS-353, and RD-00 testsets, trained on BNC.
‘*’ indicates the use of a smaller bootstrapped
concept–word co-occurrence matrix. ‘n.p.’ indi-
cates that the experiment was not pursued.
(concept-cos-cp), and its bootstrapped variant that
uses a smaller concept–word co-occurrence matrix
(concept*-cos-cp). The latter yielded a correla-
tion score .65, close to the .69 reported in MH06.
We also experimented with cosine of PMI vec-
tors (word-cos-pmi) which obtained a correlation
of .62. Log likelihood ratios (word-cos-ll) gave
best results among the baseline methods (.70), and
so we it more often in the implementations of our
hybrid method.
We conducted experiments with LSA and its
GLSA variants (Budiu et al., 2006) as additional
baselines. A limited vocabulary of the 33,000
most frequent words in the BNC and all test words
was used in these experiments. (A larger vocab-
ulary was computationally expensive and 33,000
is also the vocabulary size used by Budiu et
al. (2006) in their LSA experiments.)
New Methods: The hybrid method variants
proposed in this paper (hybrid-prop-cos-ll and
hybrid-filt-cos-ll) were the best performers on the
RG-65 test set. Particularly, they performed better
than both the traditional word-distance measures
(word-cos-ll), and our concept-based methods—
variants of the MH06 method that are used with
likelihood ratios (concept-cos-ll, concept*-cos-
ll). The -pmi methods were all poorer performers
than their -ll counterparts. The -pmi hybrid vari-
ants obtained higher scores than the concept-based
ones, but almost the same scores as the word-
based ones.
</bodyText>
<subsubsectionHeader confidence="0.872864">
4.2.2 Results on WS-353 and RD-00 testsets
</subsubsectionHeader>
<bodyText confidence="0.998677711111111">
On WS-353, all our hybrid methods out-
performed their concept counterparts, and were
on par with their word-based counterparts. On
RD-00, word-cos-pmi out-performed all other
word-based methods, and the hybrid -pmi meth-
ods were best performers with scores of .64 and
.71. Our word-cos-ll, hybrid-prop-cos-ll, and
the two hybrid pmi results on RD-00 are better
than any non-WordNet results reported by Resnik
and Diab (2000), including their syntax-informed
methods—the variants of Lin (“distrib”, .43) and
Dorr (“LCS”, .39). In fact, our hybrid*-prop-cos-
pmi and hybrid*-filt-cos-pmi results reach corre-
lation levels of the WordNet-based methods re-
ported there (.66–.68). Also, on WS-353, our
hybrid sense-filtered variants and word-cos-ll ob-
tained a correlation score higher than published re-
sults using WordNet-based measures (Jarmasz and
Szpakowicz, 2003) (.33 to .35) and Wikipedia-
based methods (Ponzetto and Strube, 2006) (.19
to .48); and very close to the results obtained by
thesaurus-based (Jarmasz and Szpakowicz, 2003)
(.55) and LSA-based methods (Finkelstein et al.,
2002) (.56).
The lower correlation scores of all measures on
the WS-353 test set are possibly due to it hav-
ing politically biased word pairs (examples in-
clude: Arafat–peace, Arafat–terror, Jerusalem–
Palestinian) for which BNC texts are likely to in-
duce low correlation with the human raters of WS-
353. This testset also has disproportionately many
terms from the news domain.
The concept methods performed poorly on WS-
353 partly because many of the target words do
not exist in the thesaurus. For instance, there
were 17 such word types that occurred in 20 WS-
353 testset word pairs. When excluding these
pairs, concept-cos-cp goes up from .38 to .45, and
concept*-cos-pmi from .19 to .24. Interestingly,
results of the hybrid methods show that they were
largely unaffected by the out-of-vocabulary prob-
lem on the WS-353 dataset.
On the verbs dataset RD-00, while hybrid-prop-
cos-ll fared slightly better than word-cos-ll, using
the smaller matrix seemed to hurt performance of
</bodyText>
<note confidence="0.33744025">
.46
.51
.57
Mohammad and Hirst methods and variants
</note>
<table confidence="0.757354222222222">
concept-cos-cp .62 .38 .41
concept*-cos-cp .65 .33 .43
concept-cos-ll .60 .37 .43
concept*-cos-ll .64 .25 .27
concept*-cos-pmi .40 .19 .28
Other (LSA and variants)
word-cos-cp .53 .31
word-cos-ll .70 .54
word-cos-pmi .62 .43
</table>
<page confidence="0.993919">
780
</page>
<bodyText confidence="0.9999385">
hybrid*-prop-cos-ll compared to word-cos-ll. But
results suggest that the -pmi methods might serve
as a better measure than -ll for verbs, although this
claim should be tested more rigorously.
Human judgments of semantic distance are less
consistent on verb-pairs than on noun-pairs, as re-
flected in inter-rater agreement measures in Resnik
and Diab (2000) and others). Thus, not surpris-
ingly, the scores of almost all measures are lower
for the verb data than the RG-65 noun data.
</bodyText>
<sectionHeader confidence="0.998504" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999992358974359">
The hybrid methods proposed in this paper ob-
tained higher accuracies than all other methods on
the RG-65 testset (all of whose words were in the
published thesaurus), and on the RD-00 testset,
and their performance was at least respectable on
the WS-353 testset (many of whose words were
not in the published thesaurus). This is in con-
trast to the concept-distance methods which suf-
fer greatly when the target words are not in the
lexical resource (here, the thesaurus) they rely on,
even though these methods can make use of co-
occurrence information of words not in the the-
saurus with concepts from the thesaurus.
Amongst the two hybrid methods proposed, the
sense-filtered-counts method performed better
using the smaller bootstrapped concept–word co-
occurrence matrix whereas the sense-proportional
method performed better using the larger concept–
word co-occurrence matrix. We believe this is be-
cause the bootstrapping method proposed in Mo-
hammad and Hirst (2006) has the effect of reset-
ting to 0 the small co-occurrence counts. The
noise from these small co-occurrence counts af-
fects the sense-filtered-counts method more ad-
versely (since any non-zero value will cause the
inclusion of the corresponding collocate’s full co-
occurrence count) and so the bootstrapped matrix
is more suitable for this method.
The results also show that the cosine of log-
likelihood ratios method mostly performs better
than cosine of conditional probabilities and the
pmi methods on the noun sets. This further
supports the claim by Dunning (1993) that log-
likelihood ratio is much less sensitive than pmi
to low counts. Interestingly, on the verb set, the
pmi methods, and especially hybrid*-prop-cos-
pmi, did extremely well. Further investigation is
needed in order to determine if pmi is indeed more
suitable for verb semantic similarity, and why.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999976657142857">
Traditional distributional similarity conflates co-
occurrence information pertaining to the many
senses of the target words. Mohammad and
Hirst (2006) show how distributional measures
can be used to compute distance between very
coarse word senses or concepts (thesaurus cat-
egories), and even obtain better results than
traditional distributional similarity. However,
their method requires that the target words be
listed in the thesaurus, which is often not the
case for domain-specific terms and named enti-
ties. In this paper, we proposed hybrid meth-
ods (hybrid-sense-filtered-counts and hybrid-
sense-proportional-counts) that combine word–
word co-occurrence information (traditional dis-
tributional similarity) with word–concept co-
occurrence information (Mohammad and Hirst,
2006), with soft constraints in such a manner
that the method makes use of information en-
coded in the thesaurus when available, and de-
grades gracefully if the target word is not listed
in the thesaurus. Our method generates word-
sense-biased distributional profiles (DPs) from
non-annotated corpus-based word-based DPs and
coarser-grained aggregated thesaurus-based “con-
cept DPs” (DPCs). We showed that the hybrid
method correlates with human judgments of se-
mantic distance in most cases better than any of
the other methods we replicated.
We are now interested in improving seman-
tic distance measures for verb–verb, adjective–
adjective, and cross-part-of-speech pairs, by ex-
ploiting specific information pertaining to these
parts of speech in lexical resources in addition to
purely co-occurrence information.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999951083333333">
We thank Mona Diab for her help with her verb
test set, Raluca Budiu for her help and clarifica-
tions regarding the GLSA method and its imple-
mentation details, and the anonymous reviewers
for their valuable feedback. This work was sup-
ported, in part, by the National Science Founda-
tion under Grant No. IIS-0705832, and in part, by
the Human Language Technology Center of Ex-
cellence. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the sponsor.
</bodyText>
<page confidence="0.996434">
781
</page>
<sectionHeader confidence="0.996255" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999788491525424">
Eneko Agirre and Oier Lopez de Lacalle Lekuona.
2003. Clustering WordNet word senses. In Pro-
ceedings of the 1st International Conference on
Recent Advances in Natural Language Processing
(RANLP-2003), Borovets, Bulgaria.
Satanjeev Banerjee and Ted Pedersen. 2003. Ex-
tended gloss overlaps as a measure of semantic re-
latedness. In Proceedings of the Eighteenth Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-03), pages 805–810, Acapulco, Mexico.
John R. L. Bernard, editor. 1986. The Macquarie The-
saurus. Macquarie Library, Sydney, Australia.
Raluca Budiu, Christiaan Royer, and Peter Pirolli.
2006. Modeling information scent: A compari-
son of LSA, PMI and GLSA similarity measures
on common tests and corpora. In Proceedings of
RIAO’07, Pittsburgh, PA.
Lou Burnard. 2000. Reference Guide for the British
National Corpus. Oxford University Computing
Services, Oxford, England, world edition edition.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, School of Informatics,
University of Edinburgh, Edinburgh, UK.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of cooccurrence probabili-
ties. Machine Learning, 34(1–3):43–69.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61–74.
Katrin Erk and Sebastian Pad´o. 2008. A struc-
tured vector space model for word meaning in con-
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2086), pages 897–906, Honolulu, HI.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116–131.
John R. Firth. 1957. A synopsis of linguistic theory
193055. Studies in Linguistic Analysis, (special vol-
ume of the Philological Society):132. Distributional
Hypothesis.
Zellig S. Harris. 1940. Review of Louis H. Gray, foun-
dations of language (New York: Macmillan, 1939).
Language, 16(3):216–231.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lexical
cohesion. NLE, 11(1):87–111.
Mario Jarmasz and Stan Szpakowicz. 2003. Ro-
get’s Thesaurus and semantic similarity. In Pro-
ceedings of the International Conference on Recent
Advances in Natural Language Processing (RANLP-
2003), pages 212–219, Borovets, Bulgaria.
Thomas Landauer, Peter Foltz, and Darrell Laham.
1998. Introduction to latent semantic analysis. Dis-
course Processes, 25:259 – 284.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th In-
ternational Conference on Machine Learning, page
296304, San Francisco, CA.
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3):149.
Diana McCarthy. 2006. Relating WordNet senses for
word sense disambiguation. In Proceedings of the
European Chapter of the Association for Computa-
tional Linguistics Workshop Making Sense of Sense -
Bringing Computational Linguistics and Psycholin-
guistics Together, pages 17–24, Trento, Italy.
S. McDonald. 2000. Environmental determinants of
lexical processing effort. Ph.D. thesis, University of
Edinburgh, Edinburgh, UK.
Saif Mohammad and Graeme Hirst. 2006. Distribu-
tional measures of concept-distance: A task-oriented
evaluation. In Proceedings of EMNLP.
Saif Mohammad, Iryna Gurevych, Graeme Hirst, and
Torsten Zesch. 2007. Cross-lingual distribu-
tional profiles of concepts for measuring seman-
tic distance. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP/CoNLL-2007), pages 571–580,
Prague, Czech Republic.
Saif Mohammad. 2008. Measuring Semantic Distance
using Distributional Profiles of Concepts. Ph.D. the-
sis, Department of Computer Science, University of
Toronto, Toronto, Canada.
Jane Morris and Graeme Hirst. 2004. Non-classical
lexical semantic relations. In Proceedings of the
Workshop on Computational Lexical Semantics, Hu-
man Language Technology Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 46–51, Boston, Mas-
sachusetts.
Roberto Navigli. 2006. Meaningful clustering of
senses helps boost word sense disambiguation per-
formance. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association, pages 105–
112, Sydney, Australia.
Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing WordNet based context vectors to estimate the
semantic relatedness of concepts. In Proceedings of
Making Sense of Sense EACL Workshop, pages 1–8.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (NAACL-2006),
pages 192–199, New York, NY.
Philip Resnik and Mona Diab. 2000. Measuring verb
similarity. In 22nd Annual Meeting of the Cognitive
Science Society (COGSCI2000), Philadelphia, PA.
Philip Resnik. 1999. Semantic similarity in a taxon-
omy: An information-based measure and its appli-
cation to problems of ambiguity in natural language.
JAIR, 11:95–130.
</reference>
<page confidence="0.968879">
782
</page>
<reference confidence="0.999546">
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics
(COLING-04), pages 1015–1021, Geneva, Switzer-
land.
</reference>
<page confidence="0.998951">
783
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.524409">
<title confidence="0.992512">Estimating Semantic Distance Using Soft Semantic in Knowledge-Source–Corpus Hybrid Models</title>
<author confidence="0.810186">Saif</author>
<affiliation confidence="0.805187">of Linguistics for Computational Linguistics and Information Institute for Advanced Computer University of Maryland, College Park, MD 20742-7505,</affiliation>
<abstract confidence="0.996585458333333">Strictly corpus-based measures of semantic distance conflate co-occurrence information pertaining to the many possible senses of target words. We propose a corpus–thesaurus hybrid method that uses soft constraints to generate word-senseaware distributional profiles (DPs) from coarser “concept DPs” (derived from a Roget-like thesaurus) and sense-unaware traditional word DPs (derived from raw text). Although it uses a knowledge source, the method is not vocabularylimited: if the target word is not in the thesaurus, the method falls back gracefully on the word’s co-occurrence information. This allows the method to access valuable information encoded in a lexical resource, such as a thesaurus, while still being able to effectively handle domainspecific terms and named entities. Experiments on word-pair ranking by semantic distance show the new hybrid method to be superior to others.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Eneko Agirre and Oier Lopez de Lacalle Lekuona.</title>
<date>2003</date>
<booktitle>In Proceedings of the 1st International Conference on Recent Advances in Natural Language Processing (RANLP-2003), Borovets,</booktitle>
<marker>2003</marker>
<rawString>Eneko Agirre and Oier Lopez de Lacalle Lekuona. 2003. Clustering WordNet word senses. In Proceedings of the 1st International Conference on Recent Advances in Natural Language Processing (RANLP-2003), Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03),</booktitle>
<pages>805--810</pages>
<location>Acapulco, Mexico.</location>
<contexts>
<context position="1983" citStr="Banerjee and Pedersen, 2003" startWordPosition="287" endWordPosition="290">n meaning of two concepts. People are consistent judges of semantic distance. For example, we can easily tell that the concepts of “exercise” and “jog” are closer in meaning than “exercise” and “theater”. Studies asking native speakers of a language to rank word pairs in order of semantic distance confirm this—average inter-annotator correlation on ranking word pairs in order of semantic distance has been repeatedly shown to be around 0.9 (Rubenstein and Goodenough, 1965; Resnik, 1999). A number of natural language tasks such as machine translation (Lopez, 2008) and word sense disambiguation (Banerjee and Pedersen, 2003; McCarthy, 2006), can be framed as semantic distance problems. Thus, developing automatic measures that are in-line with human notions of semantic distance has received much attention. These automatic approaches to semantic distance rely on manually created lexical resources such as WordNet, large amounts of text corpora, or both. WordNet-based information content measures have been successful (Hirst and Budanitsky, 2005), but there are significant limitations on their applicability. They can be applied only if a WordNet exists for the language of interest (which is not the case for the “low-</context>
<context position="9011" citStr="Banerjee and Pedersen, 2003" startWordPosition="1404" endWordPosition="1407">rce-based measures WordNet is a manually-created hierarchical network of nodes (taxonomy), where each node in 3The notion of semantic distance can be generalized, of course, to larger units such as phrases, sentences, passages, and so on (Landauer et al., 1998). 776 the network represents a fine-grained concept or word sense. An edge between two nodes represents a lexical semantic relation such as hypernymy and troponymy. WordNet-based measures consider two terms to be close if they occur close to each other in the network (connected by only a few arcs), if their definitions share many terms (Banerjee and Pedersen, 2003; Patwardhan and Pedersen, 2006), or if they share a lot of information (Lin, 1998; Resnik, 1999). The length of each arc/link (distance between nodes) can be assumed a unit length, or can be computed from corpus statistics. Within WordNet, the is-a hierarchy is much more well-developed than that of other lexical semantic relations. So, not surprisingly, the best WordNet-based measures are those that rely only on the is-a hierarchy. Therefore, they are good at measuring semantic similarity (e.g., doctor–physician), but not semantic relatedness (e.g., doctor–scalpel). Further, the measures can </context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), pages 805–810, Acapulco, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R L Bernard</author>
<author>editor</author>
</authors>
<date>1986</date>
<booktitle>The Macquarie Thesaurus. Macquarie Library,</booktitle>
<location>Sydney, Australia.</location>
<marker>Bernard, editor, 1986</marker>
<rawString>John R. L. Bernard, editor. 1986. The Macquarie Thesaurus. Macquarie Library, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raluca Budiu</author>
<author>Christiaan Royer</author>
<author>Peter Pirolli</author>
</authors>
<title>Modeling information scent: A comparison of LSA, PMI and GLSA similarity measures on common tests and corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of RIAO’07,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="17498" citStr="Budiu et al., 2006" startWordPosition="2788" endWordPosition="2791">s’s vocabulary, then we assume uniform distribution over all concepts, and in practice use a single sense, and take the conditional probability to be 1. Since the method takes senseproportional co-occurrence counts, we will refer to this method as the hybrid-sense-proportionalcounts method (or, hybrid-prop for short). 5The dimensions of the DP co-occurrence vector can be defined arbitrarily, and do not have to correspond to the words in the vocabulary. The most notable alternative representation is the Latent Semantic Analysis and its variants (Landauer et al., 1998; Finkelstein et al., 2002; Budiu et al., 2006). 778 For example, below is the DPWS of bank in the “financial institution” sense, calculated from its DPW and DPCs: DPW(bank): money,100; boat,80; bond,70; fish,77; ... DPC(“fin. inst.”): money,1000; boat,32; bond,705; fish,0; ... DPC(“river bank”): money,5; boat,863; bond,0; fish,948; ... DPWS(bank“fin.inst.”): 32 money,(1000+5 x 100); boat, (32+863 x 80); bond, 705 (705+0 x 70); fish,( 0 0+948 x 77); ... Once the DPWS are calculated, any countsbased SoA and distance measures can be applied. For example, in this work we use loglikelihood ratio (Dunning, 1993) to determine the SoA between a w</context>
<context position="22558" citStr="Budiu et al., 2006" startWordPosition="3564" endWordPosition="3567">ncept–word co-occurrence matrix. ‘n.p.’ indicates that the experiment was not pursued. (concept-cos-cp), and its bootstrapped variant that uses a smaller concept–word co-occurrence matrix (concept*-cos-cp). The latter yielded a correlation score .65, close to the .69 reported in MH06. We also experimented with cosine of PMI vectors (word-cos-pmi) which obtained a correlation of .62. Log likelihood ratios (word-cos-ll) gave best results among the baseline methods (.70), and so we it more often in the implementations of our hybrid method. We conducted experiments with LSA and its GLSA variants (Budiu et al., 2006) as additional baselines. A limited vocabulary of the 33,000 most frequent words in the BNC and all test words was used in these experiments. (A larger vocabulary was computationally expensive and 33,000 is also the vocabulary size used by Budiu et al. (2006) in their LSA experiments.) New Methods: The hybrid method variants proposed in this paper (hybrid-prop-cos-ll and hybrid-filt-cos-ll) were the best performers on the RG-65 test set. Particularly, they performed better than both the traditional word-distance measures (word-cos-ll), and our concept-based methods— variants of the MH06 method</context>
</contexts>
<marker>Budiu, Royer, Pirolli, 2006</marker>
<rawString>Raluca Budiu, Christiaan Royer, and Peter Pirolli. 2006. Modeling information scent: A comparison of LSA, PMI and GLSA similarity measures on common tests and corpora. In Proceedings of RIAO’07, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>Reference Guide for the British National Corpus.</title>
<date>2000</date>
<institution>Oxford University Computing Services,</institution>
<location>Oxford, England,</location>
<note>world edition edition.</note>
<contexts>
<context position="20237" citStr="Burnard, 2000" startWordPosition="3217" endWordPosition="3218">nal similarity methods, and several Latent Semantic Analysis (LSA)-based methods. We used three testsets and their corresponding human judgment gold standards: (1) the Rubenstein and Goodenough (1965) set of 65 noun pairs—denoted RG-65; (2) the WordSimilarity-353 (Finkelstein et al., 2002) set of 353 noun pairs (which include the RG-65 pairs) of which we discarded of one repeating pair—denoted WS-353; and (3) the Resnik and Diab (2000) set of 27 verb pairs—denoted RD-00. 4.1 Corpora and Pre-processing We generated distributional profiles (DPWs and DPCs) from the British National Corpus (BNC) (Burnard, 2000), which is a balanced corpus. We lowercased the characters, and stripped numbers, punctuation marks, and any SGML-like syntactic tags, but kept sentence boundary markers. The BNC contained 102,100,114 tokens of 546,299 types (vocabulary size) after tokenization. For the verb set, we also lemmatized this corpus. We considered two words as co-occurring if they occurred in a window of ±5 words from each other. We stoplisted words that co-occurred with more than 2000 word types. 4.2 Results The Spearman rank correlations of the automatic rankings of the RG-65, WS353, and RD-00 testsets with the co</context>
</contexts>
<marker>Burnard, 2000</marker>
<rawString>Lou Burnard. 2000. Reference Guide for the British National Corpus. Oxford University Computing Services, Oxford, England, world edition edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Informatics, University of Edinburgh,</institution>
<location>Edinburgh, UK.</location>
<contexts>
<context position="10833" citStr="Curran (2004)" startWordPosition="1695" endWordPosition="1696"> tend to co-occur with u within a certain distance, along with numeric scores signifying this co-occurrence tendency with u. Then measures such as cosine or α-skew divergence are used to determine how close the DPs of the two target words are. See Section 3 for more details and related work. These measures are very appealing because they rely simply on raw text, but, as described earlier, when used to rank word pairs in order of semantic distance, or to correct real-word spelling errors, they perform poorly, compared to the WordNet-based measures. See Weeds et al. (2004), Mohammad (2008), and Curran (2004) for detailed surveys of distributional measures. As Mohammad and Hirst (2006) point out, the DP of a word u conflates information about the potentially many senses of u. For example, consider the following. The noun bank has two senses “river bank” and “financial institution”. Assume that bank, when used in the “financial institution” sense, co-occurred with the noun money 100 times in a corpus. Similarly, assume that bank, when used in the “river bank” sense, co-occurred with the noun boat 80 times. So the DP of bank will have co-occurrence information with money as well as boat: DPW(bank): </context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>James R. Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, School of Informatics, University of Edinburgh, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>Similarity-based models of cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="2830" citStr="Dagan et al., 1999" startWordPosition="417" endWordPosition="420">tance rely on manually created lexical resources such as WordNet, large amounts of text corpora, or both. WordNet-based information content measures have been successful (Hirst and Budanitsky, 2005), but there are significant limitations on their applicability. They can be applied only if a WordNet exists for the language of interest (which is not the case for the “low-density” languages); and even if there is a WordNet, a number of domainspecific terms may not be encoded in it. On the other hand, corpus-based distributional measures of semantic distance, such as cosine and α-skew divergence (Dagan et al., 1999), rely on raw text alone (Weeds et al., 2004; Mohammad, 2008). However, when used to rank word pairs in order of semantic distance or correct real-word spelling errors, they have been shown to perform poorly (Weeds et al., 2004; Mohammad and Hirst, 2006). Mohammad and Hirst (2006) and Patwardhan and Pedersen (2006) argued that word sense ambiguity is a key reason for the poor performance of traditional distributional measures, and they proposed hybrid approaches that are distributional in nature, but also make use of information in lexical resources such as published thesauri and WordNet. Howe</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>Ido Dagan, Lillian Lee, and Fernando Pereira. 1999. Similarity-based models of cooccurrence probabilities. Machine Learning, 34(1–3):43–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="18065" citStr="Dunning, 1993" startWordPosition="2878" endWordPosition="2879">; Finkelstein et al., 2002; Budiu et al., 2006). 778 For example, below is the DPWS of bank in the “financial institution” sense, calculated from its DPW and DPCs: DPW(bank): money,100; boat,80; bond,70; fish,77; ... DPC(“fin. inst.”): money,1000; boat,32; bond,705; fish,0; ... DPC(“river bank”): money,5; boat,863; bond,0; fish,948; ... DPWS(bank“fin.inst.”): 32 money,(1000+5 x 100); boat, (32+863 x 80); bond, 705 (705+0 x 70); fish,( 0 0+948 x 77); ... Once the DPWS are calculated, any countsbased SoA and distance measures can be applied. For example, in this work we use loglikelihood ratio (Dunning, 1993) to determine the SoA between a word sense and co-occurring words, and cosine to determine the distance between two DPWS’s log likelihood vectors (McDonald, 2000). We also contrast this measure with cosine of conditional probabilities vectors. Given two target words, we determine the distance between each of their DPWS pairings and the closest DPWS-pair distance is chosen. 3.1 The hybrid-sense-filtered-counts method Since the DPCs are created in an unsupervised manner, they are expected to be somewhat noisy. Therefore, we also experimented with a variant of the method proposed above, that simp</context>
<context position="27833" citStr="Dunning (1993)" startWordPosition="4388" endWordPosition="4389">pping method proposed in Mohammad and Hirst (2006) has the effect of resetting to 0 the small co-occurrence counts. The noise from these small co-occurrence counts affects the sense-filtered-counts method more adversely (since any non-zero value will cause the inclusion of the corresponding collocate’s full cooccurrence count) and so the bootstrapped matrix is more suitable for this method. The results also show that the cosine of loglikelihood ratios method mostly performs better than cosine of conditional probabilities and the pmi methods on the noun sets. This further supports the claim by Dunning (1993) that loglikelihood ratio is much less sensitive than pmi to low counts. Interestingly, on the verb set, the pmi methods, and especially hybrid*-prop-cospmi, did extremely well. Further investigation is needed in order to determine if pmi is indeed more suitable for verb semantic similarity, and why. 6 Conclusion Traditional distributional similarity conflates cooccurrence information pertaining to the many senses of the target words. Mohammad and Hirst (2006) show how distributional measures can be used to compute distance between very coarse word senses or concepts (thesaurus categories), an</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP2086),</booktitle>
<pages>897--906</pages>
<location>Honolulu, HI.</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP2086), pages 897–906, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="17477" citStr="Finkelstein et al., 2002" startWordPosition="2784" endWordPosition="2787">ord is not in the thesaurus’s vocabulary, then we assume uniform distribution over all concepts, and in practice use a single sense, and take the conditional probability to be 1. Since the method takes senseproportional co-occurrence counts, we will refer to this method as the hybrid-sense-proportionalcounts method (or, hybrid-prop for short). 5The dimensions of the DP co-occurrence vector can be defined arbitrarily, and do not have to correspond to the words in the vocabulary. The most notable alternative representation is the Latent Semantic Analysis and its variants (Landauer et al., 1998; Finkelstein et al., 2002; Budiu et al., 2006). 778 For example, below is the DPWS of bank in the “financial institution” sense, calculated from its DPW and DPCs: DPW(bank): money,100; boat,80; bond,70; fish,77; ... DPC(“fin. inst.”): money,1000; boat,32; bond,705; fish,0; ... DPC(“river bank”): money,5; boat,863; bond,0; fish,948; ... DPWS(bank“fin.inst.”): 32 money,(1000+5 x 100); boat, (32+863 x 80); bond, 705 (705+0 x 70); fish,( 0 0+948 x 77); ... Once the DPWS are calculated, any countsbased SoA and distance measures can be applied. For example, in this work we use loglikelihood ratio (Dunning, 1993) to determin</context>
<context position="19913" citStr="Finkelstein et al., 2002" startWordPosition="3164" endWordPosition="3167"> bond are left as is (and not senseproportionally attenuated). 4 Evaluation We evaluated various methods on the task of ranking word pairs in order of semantic distance. These methods included our sense-biased methods as well as several baselines: the Mohammad and Hirst (2006) DPC-based methods, the traditional word-based distributional similarity methods, and several Latent Semantic Analysis (LSA)-based methods. We used three testsets and their corresponding human judgment gold standards: (1) the Rubenstein and Goodenough (1965) set of 65 noun pairs—denoted RG-65; (2) the WordSimilarity-353 (Finkelstein et al., 2002) set of 353 noun pairs (which include the RG-65 pairs) of which we discarded of one repeating pair—denoted WS-353; and (3) the Resnik and Diab (2000) set of 27 verb pairs—denoted RD-00. 4.1 Corpora and Pre-processing We generated distributional profiles (DPWs and DPCs) from the British National Corpus (BNC) (Burnard, 2000), which is a balanced corpus. We lowercased the characters, and stripped numbers, punctuation marks, and any SGML-like syntactic tags, but kept sentence boundary markers. The BNC contained 102,100,114 tokens of 546,299 types (vocabulary size) after tokenization. For the verb </context>
<context position="24555" citStr="Finkelstein et al., 2002" startWordPosition="3862" endWordPosition="3865">ethods—the variants of Lin (“distrib”, .43) and Dorr (“LCS”, .39). In fact, our hybrid*-prop-cospmi and hybrid*-filt-cos-pmi results reach correlation levels of the WordNet-based methods reported there (.66–.68). Also, on WS-353, our hybrid sense-filtered variants and word-cos-ll obtained a correlation score higher than published results using WordNet-based measures (Jarmasz and Szpakowicz, 2003) (.33 to .35) and Wikipediabased methods (Ponzetto and Strube, 2006) (.19 to .48); and very close to the results obtained by thesaurus-based (Jarmasz and Szpakowicz, 2003) (.55) and LSA-based methods (Finkelstein et al., 2002) (.56). The lower correlation scores of all measures on the WS-353 test set are possibly due to it having politically biased word pairs (examples include: Arafat–peace, Arafat–terror, Jerusalem– Palestinian) for which BNC texts are likely to induce low correlation with the human raters of WS353. This testset also has disproportionately many terms from the news domain. The concept methods performed poorly on WS353 partly because many of the target words do not exist in the thesaurus. For instance, there were 17 such word types that occurred in 20 WS353 testset word pairs. When excluding these p</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Firth</author>
</authors>
<title>A synopsis of linguistic theory 193055. Studies in Linguistic Analysis, (special volume of the Philological Society):132. Distributional Hypothesis.</title>
<date>1957</date>
<contexts>
<context position="10090" citStr="Firth, 1957" startWordPosition="1569" endWordPosition="1570">suring semantic similarity (e.g., doctor–physician), but not semantic relatedness (e.g., doctor–scalpel). Further, the measures can only be used in languages that have a (sufficiently developed) WordNet. WordNet sense information has been criticized to be too fine grained (Agirre and Lopez de Lacalle Lekuona, 2003; Navigli, 2006). See Hirst and Budanitsky (2005) for a comprehensive survey of WordNet-based measures. 2.2 Corpus-based measures Strictly corpus-based measures of distributional similarity rely on the hypothesis that words that occur in similar context tend to be semantically close (Firth, 1957; Harris, 1940). The set of contexts of each target word u is represented by its distributional profile (DP)—the set of words that tend to co-occur with u within a certain distance, along with numeric scores signifying this co-occurrence tendency with u. Then measures such as cosine or α-skew divergence are used to determine how close the DPs of the two target words are. See Section 3 for more details and related work. These measures are very appealing because they rely simply on raw text, but, as described earlier, when used to rank word pairs in order of semantic distance, or to correct real</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>John R. Firth. 1957. A synopsis of linguistic theory 193055. Studies in Linguistic Analysis, (special volume of the Philological Society):132. Distributional Hypothesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<title>Review of</title>
<date>1940</date>
<journal>Language,</journal>
<volume>16</volume>
<issue>3</issue>
<publisher>Macmillan,</publisher>
<location>New York:</location>
<contexts>
<context position="10105" citStr="Harris, 1940" startWordPosition="1571" endWordPosition="1572">ic similarity (e.g., doctor–physician), but not semantic relatedness (e.g., doctor–scalpel). Further, the measures can only be used in languages that have a (sufficiently developed) WordNet. WordNet sense information has been criticized to be too fine grained (Agirre and Lopez de Lacalle Lekuona, 2003; Navigli, 2006). See Hirst and Budanitsky (2005) for a comprehensive survey of WordNet-based measures. 2.2 Corpus-based measures Strictly corpus-based measures of distributional similarity rely on the hypothesis that words that occur in similar context tend to be semantically close (Firth, 1957; Harris, 1940). The set of contexts of each target word u is represented by its distributional profile (DP)—the set of words that tend to co-occur with u within a certain distance, along with numeric scores signifying this co-occurrence tendency with u. Then measures such as cosine or α-skew divergence are used to determine how close the DPs of the two target words are. See Section 3 for more details and related work. These measures are very appealing because they rely simply on raw text, but, as described earlier, when used to rank word pairs in order of semantic distance, or to correct real-word spelling </context>
</contexts>
<marker>Harris, 1940</marker>
<rawString>Zellig S. Harris. 1940. Review of Louis H. Gray, foundations of language (New York: Macmillan, 1939). Language, 16(3):216–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>Alexander Budanitsky</author>
</authors>
<title>Correcting real-word spelling errors by restoring lexical cohesion.</title>
<date>2005</date>
<journal>NLE,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="2409" citStr="Hirst and Budanitsky, 2005" startWordPosition="346" endWordPosition="349">to be around 0.9 (Rubenstein and Goodenough, 1965; Resnik, 1999). A number of natural language tasks such as machine translation (Lopez, 2008) and word sense disambiguation (Banerjee and Pedersen, 2003; McCarthy, 2006), can be framed as semantic distance problems. Thus, developing automatic measures that are in-line with human notions of semantic distance has received much attention. These automatic approaches to semantic distance rely on manually created lexical resources such as WordNet, large amounts of text corpora, or both. WordNet-based information content measures have been successful (Hirst and Budanitsky, 2005), but there are significant limitations on their applicability. They can be applied only if a WordNet exists for the language of interest (which is not the case for the “low-density” languages); and even if there is a WordNet, a number of domainspecific terms may not be encoded in it. On the other hand, corpus-based distributional measures of semantic distance, such as cosine and α-skew divergence (Dagan et al., 1999), rely on raw text alone (Weeds et al., 2004; Mohammad, 2008). However, when used to rank word pairs in order of semantic distance or correct real-word spelling errors, they have </context>
<context position="7820" citStr="Hirst and Budanitsky, 2005" startWordPosition="1217" endWordPosition="1220">s are considered to be semantically close if there is a lexical semantic relation between them. Such a relation may be a classical relation such as hypernymy, troponymy, meronymy, and antonymy, or it may be what have been called an ad-hoc nonclassical relation, such as cause-and-effect (Morris and Hirst, 2004). If the closeness in meaning is due to certain specific classical relations such as hypernymy and troponymy, then the terms are said to be semantically similar. Semantic relatedness is the term used to describe the more general form of semantic closeness caused by any semantic relation (Hirst and Budanitsky, 2005). So the nouns liquid and water are both semantically similar and semantically related, whereas the nouns boat and rudder are semantically related, but not similar. The next three sub-sections describe three kinds of automatic distance measures: (1) lexicalresource-based measures that rely on a manually created resource such as WordNet; (2) corpusbased measures that rely only on co-occurrence statistics from large corpora; and (3) hybrid measures that are distributional in nature, and that also exploit the information in a lexical resource. 2.1 Lexical-resource-based measures WordNet is a manu</context>
<context position="9843" citStr="Hirst and Budanitsky (2005)" startWordPosition="1532" endWordPosition="1535"> corpus statistics. Within WordNet, the is-a hierarchy is much more well-developed than that of other lexical semantic relations. So, not surprisingly, the best WordNet-based measures are those that rely only on the is-a hierarchy. Therefore, they are good at measuring semantic similarity (e.g., doctor–physician), but not semantic relatedness (e.g., doctor–scalpel). Further, the measures can only be used in languages that have a (sufficiently developed) WordNet. WordNet sense information has been criticized to be too fine grained (Agirre and Lopez de Lacalle Lekuona, 2003; Navigli, 2006). See Hirst and Budanitsky (2005) for a comprehensive survey of WordNet-based measures. 2.2 Corpus-based measures Strictly corpus-based measures of distributional similarity rely on the hypothesis that words that occur in similar context tend to be semantically close (Firth, 1957; Harris, 1940). The set of contexts of each target word u is represented by its distributional profile (DP)—the set of words that tend to co-occur with u within a certain distance, along with numeric scores signifying this co-occurrence tendency with u. Then measures such as cosine or α-skew divergence are used to determine how close the DPs of the t</context>
</contexts>
<marker>Hirst, Budanitsky, 2005</marker>
<rawString>Graeme Hirst and Alexander Budanitsky. 2005. Correcting real-word spelling errors by restoring lexical cohesion. NLE, 11(1):87–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Jarmasz</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Roget’s Thesaurus and semantic similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP2003),</booktitle>
<pages>212--219</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="24329" citStr="Jarmasz and Szpakowicz, 2003" startWordPosition="3827" endWordPosition="3830">best performers with scores of .64 and .71. Our word-cos-ll, hybrid-prop-cos-ll, and the two hybrid pmi results on RD-00 are better than any non-WordNet results reported by Resnik and Diab (2000), including their syntax-informed methods—the variants of Lin (“distrib”, .43) and Dorr (“LCS”, .39). In fact, our hybrid*-prop-cospmi and hybrid*-filt-cos-pmi results reach correlation levels of the WordNet-based methods reported there (.66–.68). Also, on WS-353, our hybrid sense-filtered variants and word-cos-ll obtained a correlation score higher than published results using WordNet-based measures (Jarmasz and Szpakowicz, 2003) (.33 to .35) and Wikipediabased methods (Ponzetto and Strube, 2006) (.19 to .48); and very close to the results obtained by thesaurus-based (Jarmasz and Szpakowicz, 2003) (.55) and LSA-based methods (Finkelstein et al., 2002) (.56). The lower correlation scores of all measures on the WS-353 test set are possibly due to it having politically biased word pairs (examples include: Arafat–peace, Arafat–terror, Jerusalem– Palestinian) for which BNC texts are likely to induce low correlation with the human raters of WS353. This testset also has disproportionately many terms from the news domain. The</context>
</contexts>
<marker>Jarmasz, Szpakowicz, 2003</marker>
<rawString>Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s Thesaurus and semantic similarity. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP2003), pages 212–219, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Peter Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>Introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--259</pages>
<contexts>
<context position="8645" citStr="Landauer et al., 1998" startWordPosition="1342" endWordPosition="1345">hree kinds of automatic distance measures: (1) lexicalresource-based measures that rely on a manually created resource such as WordNet; (2) corpusbased measures that rely only on co-occurrence statistics from large corpora; and (3) hybrid measures that are distributional in nature, and that also exploit the information in a lexical resource. 2.1 Lexical-resource-based measures WordNet is a manually-created hierarchical network of nodes (taxonomy), where each node in 3The notion of semantic distance can be generalized, of course, to larger units such as phrases, sentences, passages, and so on (Landauer et al., 1998). 776 the network represents a fine-grained concept or word sense. An edge between two nodes represents a lexical semantic relation such as hypernymy and troponymy. WordNet-based measures consider two terms to be close if they occur close to each other in the network (connected by only a few arcs), if their definitions share many terms (Banerjee and Pedersen, 2003; Patwardhan and Pedersen, 2006), or if they share a lot of information (Lin, 1998; Resnik, 1999). The length of each arc/link (distance between nodes) can be assumed a unit length, or can be computed from corpus statistics. Within Wo</context>
<context position="17451" citStr="Landauer et al., 1998" startWordPosition="2780" endWordPosition="2783">m DPWs. If the target word is not in the thesaurus’s vocabulary, then we assume uniform distribution over all concepts, and in practice use a single sense, and take the conditional probability to be 1. Since the method takes senseproportional co-occurrence counts, we will refer to this method as the hybrid-sense-proportionalcounts method (or, hybrid-prop for short). 5The dimensions of the DP co-occurrence vector can be defined arbitrarily, and do not have to correspond to the words in the vocabulary. The most notable alternative representation is the Latent Semantic Analysis and its variants (Landauer et al., 1998; Finkelstein et al., 2002; Budiu et al., 2006). 778 For example, below is the DPWS of bank in the “financial institution” sense, calculated from its DPW and DPCs: DPW(bank): money,100; boat,80; bond,70; fish,77; ... DPC(“fin. inst.”): money,1000; boat,32; bond,705; fish,0; ... DPC(“river bank”): money,5; boat,863; bond,0; fish,948; ... DPWS(bank“fin.inst.”): 32 money,(1000+5 x 100); boat, (32+863 x 80); bond, 705 (705+0 x 70); fish,( 0 0+948 x 77); ... Once the DPWS are calculated, any countsbased SoA and distance measures can be applied. For example, in this work we use loglikelihood ratio (</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas Landauer, Peter Foltz, and Darrell Laham. 1998. Introduction to latent semantic analysis. Discourse Processes, 25:259 – 284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<pages>296304</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="9093" citStr="Lin, 1998" startWordPosition="1421" endWordPosition="1422">h node in 3The notion of semantic distance can be generalized, of course, to larger units such as phrases, sentences, passages, and so on (Landauer et al., 1998). 776 the network represents a fine-grained concept or word sense. An edge between two nodes represents a lexical semantic relation such as hypernymy and troponymy. WordNet-based measures consider two terms to be close if they occur close to each other in the network (connected by only a few arcs), if their definitions share many terms (Banerjee and Pedersen, 2003; Patwardhan and Pedersen, 2006), or if they share a lot of information (Lin, 1998; Resnik, 1999). The length of each arc/link (distance between nodes) can be assumed a unit length, or can be computed from corpus statistics. Within WordNet, the is-a hierarchy is much more well-developed than that of other lexical semantic relations. So, not surprisingly, the best WordNet-based measures are those that rely only on the is-a hierarchy. Therefore, they are good at measuring semantic similarity (e.g., doctor–physician), but not semantic relatedness (e.g., doctor–scalpel). Further, the measures can only be used in languages that have a (sufficiently developed) WordNet. WordNet se</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, page 296304, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Statistical machine translation.</title>
<date>2008</date>
<journal>ACM Computing Surveys,</journal>
<volume>40</volume>
<issue>3</issue>
<contexts>
<context position="1924" citStr="Lopez, 2008" startWordPosition="281" endWordPosition="282">tic distance is a measure of the closeness in meaning of two concepts. People are consistent judges of semantic distance. For example, we can easily tell that the concepts of “exercise” and “jog” are closer in meaning than “exercise” and “theater”. Studies asking native speakers of a language to rank word pairs in order of semantic distance confirm this—average inter-annotator correlation on ranking word pairs in order of semantic distance has been repeatedly shown to be around 0.9 (Rubenstein and Goodenough, 1965; Resnik, 1999). A number of natural language tasks such as machine translation (Lopez, 2008) and word sense disambiguation (Banerjee and Pedersen, 2003; McCarthy, 2006), can be framed as semantic distance problems. Thus, developing automatic measures that are in-line with human notions of semantic distance has received much attention. These automatic approaches to semantic distance rely on manually created lexical resources such as WordNet, large amounts of text corpora, or both. WordNet-based information content measures have been successful (Hirst and Budanitsky, 2005), but there are significant limitations on their applicability. They can be applied only if a WordNet exists for th</context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>Adam Lopez. 2008. Statistical machine translation. ACM Computing Surveys, 40(3):149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Relating WordNet senses for word sense disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the European Chapter of the Association for Computational Linguistics Workshop Making Sense of Sense -Bringing Computational Linguistics and Psycholinguistics Together,</booktitle>
<pages>17--24</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="2000" citStr="McCarthy, 2006" startWordPosition="291" endWordPosition="292">ople are consistent judges of semantic distance. For example, we can easily tell that the concepts of “exercise” and “jog” are closer in meaning than “exercise” and “theater”. Studies asking native speakers of a language to rank word pairs in order of semantic distance confirm this—average inter-annotator correlation on ranking word pairs in order of semantic distance has been repeatedly shown to be around 0.9 (Rubenstein and Goodenough, 1965; Resnik, 1999). A number of natural language tasks such as machine translation (Lopez, 2008) and word sense disambiguation (Banerjee and Pedersen, 2003; McCarthy, 2006), can be framed as semantic distance problems. Thus, developing automatic measures that are in-line with human notions of semantic distance has received much attention. These automatic approaches to semantic distance rely on manually created lexical resources such as WordNet, large amounts of text corpora, or both. WordNet-based information content measures have been successful (Hirst and Budanitsky, 2005), but there are significant limitations on their applicability. They can be applied only if a WordNet exists for the language of interest (which is not the case for the “low-density” language</context>
</contexts>
<marker>McCarthy, 2006</marker>
<rawString>Diana McCarthy. 2006. Relating WordNet senses for word sense disambiguation. In Proceedings of the European Chapter of the Association for Computational Linguistics Workshop Making Sense of Sense -Bringing Computational Linguistics and Psycholinguistics Together, pages 17–24, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S McDonald</author>
</authors>
<title>Environmental determinants of lexical processing effort.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh,</institution>
<location>Edinburgh, UK.</location>
<contexts>
<context position="18227" citStr="McDonald, 2000" startWordPosition="2904" endWordPosition="2906">s: DPW(bank): money,100; boat,80; bond,70; fish,77; ... DPC(“fin. inst.”): money,1000; boat,32; bond,705; fish,0; ... DPC(“river bank”): money,5; boat,863; bond,0; fish,948; ... DPWS(bank“fin.inst.”): 32 money,(1000+5 x 100); boat, (32+863 x 80); bond, 705 (705+0 x 70); fish,( 0 0+948 x 77); ... Once the DPWS are calculated, any countsbased SoA and distance measures can be applied. For example, in this work we use loglikelihood ratio (Dunning, 1993) to determine the SoA between a word sense and co-occurring words, and cosine to determine the distance between two DPWS’s log likelihood vectors (McDonald, 2000). We also contrast this measure with cosine of conditional probabilities vectors. Given two target words, we determine the distance between each of their DPWS pairings and the closest DPWS-pair distance is chosen. 3.1 The hybrid-sense-filtered-counts method Since the DPCs are created in an unsupervised manner, they are expected to be somewhat noisy. Therefore, we also experimented with a variant of the method proposed above, that simply makes use of whether the conditional probability p(c|wi) is greater than 0 or not: � (2) = f (u, wi) If p(c|wi) &gt; 0 f (uc, wi) 0Otherwise Since this method ess</context>
</contexts>
<marker>McDonald, 2000</marker>
<rawString>S. McDonald. 2000. Environmental determinants of lexical processing effort. Ph.D. thesis, University of Edinburgh, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Graeme Hirst</author>
</authors>
<title>Distributional measures of concept-distance: A task-oriented evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3084" citStr="Mohammad and Hirst, 2006" startWordPosition="460" endWordPosition="463">pplicability. They can be applied only if a WordNet exists for the language of interest (which is not the case for the “low-density” languages); and even if there is a WordNet, a number of domainspecific terms may not be encoded in it. On the other hand, corpus-based distributional measures of semantic distance, such as cosine and α-skew divergence (Dagan et al., 1999), rely on raw text alone (Weeds et al., 2004; Mohammad, 2008). However, when used to rank word pairs in order of semantic distance or correct real-word spelling errors, they have been shown to perform poorly (Weeds et al., 2004; Mohammad and Hirst, 2006). Mohammad and Hirst (2006) and Patwardhan and Pedersen (2006) argued that word sense ambiguity is a key reason for the poor performance of traditional distributional measures, and they proposed hybrid approaches that are distributional in nature, but also make use of information in lexical resources such as published thesauri and WordNet. However, both these approaches can be applied to estimate the semantic distance between two terms only if both terms exist in the lexical resource they rely on. We know lexical resources tend to have limited vocabulary and a large number of domain775 Proceed</context>
<context position="4733" citStr="Mohammad and Hirst (2006)" startWordPosition="718" endWordPosition="721">e semantic distance as a similarity score of .75 from another distance measure.1 Thus if one uses two independent distance measures, in this case: one resource-reliant and one only corpus-dependent, then these two measures are not comparable (and hence cannot be used in tandem), even if both rely—partially or entirely—on distributional corpus statistics. We propose a hybrid semantic distance method that inherently combines the elements of a resource-reliant measure and a strictly corpusdependent measure by imposing resource-reliant soft constraints on the corpus-dependent model. We choose the Mohammad and Hirst (2006) method as the resource-reliant method and not one of the WordNet-based measures because, unlike the WordNet-based measures, the Mohammad and Hirst method is distributional in nature and so lends itself immediately for combination with traditional distributional similarity measures. Our new hybrid method combines concept–word co-occurrence information (the Mohammad and Hirst distributional profiles of thesaurus concepts (DPC)) with word–word co-occurrence information, to generate word-sense-biased distributional profiles. The “pure” corpus-based distributional profile (a.k.a. co-occurrence vec</context>
<context position="6159" citStr="Mohammad and Hirst (2006)" startWordPosition="944" endWordPosition="947">c to u in the sense that is most related to the other words listed under c. Thus, this method can make more finegrained distinctions than the Mohammad and Hirst method, and yet uses word sense information.2 Our proposed method falls back gracefully to rely only on word-word co-occurrence information if any of the target terms is not listed in the lexical resource. Experiments on the word-pair ranking task 1All we can infer is that if w1 and w2 have a similarity score of .75 and w3 and w4 have a score of .5 by the same distance measure, then w1–w2 are closer in meaning than w3–w4. 2Even though Mohammad and Hirst (2006) use thesaurus categories as coarse concepts, their algorithm can be applied using more finer-grained thesaurus word groupings (paragraphs and semicolon units), as well. on three different datasets show that the our proposed hybrid measure outperforms all other comparable distance measures. Mohammad and Hirst (2007) show that their method can be used to compute semantic distance in a resource poor language L1 by combining its text with a thesaurus in a resource-rich language L2 using an L1–L2 bilingual lexicon to create cross-lingual distributional profiles of concepts, that is, L2 word co-occ</context>
<context position="10911" citStr="Mohammad and Hirst (2006)" startWordPosition="1704" endWordPosition="1707">eric scores signifying this co-occurrence tendency with u. Then measures such as cosine or α-skew divergence are used to determine how close the DPs of the two target words are. See Section 3 for more details and related work. These measures are very appealing because they rely simply on raw text, but, as described earlier, when used to rank word pairs in order of semantic distance, or to correct real-word spelling errors, they perform poorly, compared to the WordNet-based measures. See Weeds et al. (2004), Mohammad (2008), and Curran (2004) for detailed surveys of distributional measures. As Mohammad and Hirst (2006) point out, the DP of a word u conflates information about the potentially many senses of u. For example, consider the following. The noun bank has two senses “river bank” and “financial institution”. Assume that bank, when used in the “financial institution” sense, co-occurred with the noun money 100 times in a corpus. Similarly, assume that bank, when used in the “river bank” sense, co-occurred with the noun boat 80 times. So the DP of bank will have co-occurrence information with money as well as boat: DPW(bank): money,100; boat,80; bond,70; fish,77; ... Assume that the DP of the word ATM i</context>
<context position="12509" citStr="Mohammad and Hirst (2006)" startWordPosition="1967" endWordPosition="1970">ded senses of bank and ATM, which often also tends to be the semantic distance between their closest senses. 2.3 Hybrid measures Both Mohammad and Hirst (2006) and Patwardhan and Pedersen (2006) proposed measures that are not only distributional in nature but also rely on a lexical resource to exploit the manually encoded information therein as well as to overcome the sense-conflation problem (described in section 2.2). Since we essentially combine the Mohammad and Hirst method with a “pure” wordbased distributional measure to create our hybrid approach, we briefly describe their method here. Mohammad and Hirst (2006) generate separate distributional profiles for the different senses of a word, without using any sense-annotated data. They use the categories in a Roget-style thesaurus (Macquaries (Bernard, 1986)) as coarse senses or concepts. There are about 1000 categories in a thesaurus, and each category has on average 120 closely related words. A word may be found in more than one category if it has multiple meaning. They use a simple unsupervised algorithm to determine the vector of words that tend to co-occur with each concept and the corresponding strength of association (a measure of how strong the </context>
<context position="19565" citStr="Mohammad and Hirst (2006)" startWordPosition="3116" endWordPosition="3120">refer to this method as the hybrid-sense-filtered-counts method (or, just hybrid-filt for short). Below is an example hybrid-filtered DPWS of bank in the “financial institution” sense: DPWS(bank“fin.inst.”: money,100); boat,80; bond,70; ... Note that the collocate fish is now filtered out, whereas bank’s co-occurrence counts with money, boat, and bond are left as is (and not senseproportionally attenuated). 4 Evaluation We evaluated various methods on the task of ranking word pairs in order of semantic distance. These methods included our sense-biased methods as well as several baselines: the Mohammad and Hirst (2006) DPC-based methods, the traditional word-based distributional similarity methods, and several Latent Semantic Analysis (LSA)-based methods. We used three testsets and their corresponding human judgment gold standards: (1) the Rubenstein and Goodenough (1965) set of 65 noun pairs—denoted RG-65; (2) the WordSimilarity-353 (Finkelstein et al., 2002) set of 353 noun pairs (which include the RG-65 pairs) of which we discarded of one repeating pair—denoted WS-353; and (3) the Resnik and Diab (2000) set of 27 verb pairs—denoted RD-00. 4.1 Corpora and Pre-processing We generated distributional profile</context>
<context position="21286" citStr="Mohammad and Hirst (2006)" startWordPosition="3378" endWordPosition="3381"> words that co-occurred with more than 2000 word types. 4.2 Results The Spearman rank correlations of the automatic rankings of the RG-65, WS353, and RD-00 testsets with the corresponding gold-standard human rankings is listed in Table 1.6 The higher the Spearman rank correlation, the more accurate is the distance measure. 4.2.1 Results on the RG-65 testset Baselines. We replicated the traditional wordbased distributional distance measure using cosine of vectors (DPs) containing conditional probabilities (word-cos-cp). Its rank correlation of .53 is close to the correlation of .54 reported in Mohammad and Hirst (2006), hereafter MH06. We replicated the MH06 concept-based approach 6Certain experiments were not pursued as they were redundant in supporting our claims. 779 Method RG-65 WS-353 RD-00 Baselines (replicated): Traditional distributional measures LSA .56 .47 .55 GLSA-cos-pmi .18 n.p. n.p. GLSA-cos-ll .47 n.p. .29 New methods: hybrid-prop-cos-ll .72 .49 .53 hybrid-prop*-cos-ll .69 .46 .45 hybrid-filt-cos-ll .73 .54 .38 hybrid-filt*-cos-ll .77 .54 .39 hybrid-prop*-cos-pmi .58 .43 .71 hybrid-filt*-cos-pmi .61 .42 .64 Table 1: Spearman rank correlation on RG-65, WS-353, and RD-00 testsets, trained on BN</context>
<context position="27269" citStr="Mohammad and Hirst (2006)" startWordPosition="4295" endWordPosition="4299">o the concept-distance methods which suffer greatly when the target words are not in the lexical resource (here, the thesaurus) they rely on, even though these methods can make use of cooccurrence information of words not in the thesaurus with concepts from the thesaurus. Amongst the two hybrid methods proposed, the sense-filtered-counts method performed better using the smaller bootstrapped concept–word cooccurrence matrix whereas the sense-proportional method performed better using the larger concept– word co-occurrence matrix. We believe this is because the bootstrapping method proposed in Mohammad and Hirst (2006) has the effect of resetting to 0 the small co-occurrence counts. The noise from these small co-occurrence counts affects the sense-filtered-counts method more adversely (since any non-zero value will cause the inclusion of the corresponding collocate’s full cooccurrence count) and so the bootstrapped matrix is more suitable for this method. The results also show that the cosine of loglikelihood ratios method mostly performs better than cosine of conditional probabilities and the pmi methods on the noun sets. This further supports the claim by Dunning (1993) that loglikelihood ratio is much le</context>
<context position="28929" citStr="Mohammad and Hirst, 2006" startWordPosition="4542" endWordPosition="4545">ow distributional measures can be used to compute distance between very coarse word senses or concepts (thesaurus categories), and even obtain better results than traditional distributional similarity. However, their method requires that the target words be listed in the thesaurus, which is often not the case for domain-specific terms and named entities. In this paper, we proposed hybrid methods (hybrid-sense-filtered-counts and hybridsense-proportional-counts) that combine word– word co-occurrence information (traditional distributional similarity) with word–concept cooccurrence information (Mohammad and Hirst, 2006), with soft constraints in such a manner that the method makes use of information encoded in the thesaurus when available, and degrades gracefully if the target word is not listed in the thesaurus. Our method generates wordsense-biased distributional profiles (DPs) from non-annotated corpus-based word-based DPs and coarser-grained aggregated thesaurus-based “concept DPs” (DPCs). We showed that the hybrid method correlates with human judgments of semantic distance in most cases better than any of the other methods we replicated. We are now interested in improving semantic distance measures for </context>
</contexts>
<marker>Mohammad, Hirst, 2006</marker>
<rawString>Saif Mohammad and Graeme Hirst. 2006. Distributional measures of concept-distance: A task-oriented evaluation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Iryna Gurevych</author>
<author>Graeme Hirst</author>
<author>Torsten Zesch</author>
</authors>
<title>Cross-lingual distributional profiles of concepts for measuring semantic distance.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL-2007),</booktitle>
<pages>571--580</pages>
<location>Prague, Czech Republic.</location>
<marker>Mohammad, Gurevych, Hirst, Zesch, 2007</marker>
<rawString>Saif Mohammad, Iryna Gurevych, Graeme Hirst, and Torsten Zesch. 2007. Cross-lingual distributional profiles of concepts for measuring semantic distance. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL-2007), pages 571–580, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
</authors>
<title>Measuring Semantic Distance using Distributional Profiles of Concepts.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Toronto,</institution>
<location>Toronto, Canada.</location>
<contexts>
<context position="2891" citStr="Mohammad, 2008" startWordPosition="430" endWordPosition="431"> large amounts of text corpora, or both. WordNet-based information content measures have been successful (Hirst and Budanitsky, 2005), but there are significant limitations on their applicability. They can be applied only if a WordNet exists for the language of interest (which is not the case for the “low-density” languages); and even if there is a WordNet, a number of domainspecific terms may not be encoded in it. On the other hand, corpus-based distributional measures of semantic distance, such as cosine and α-skew divergence (Dagan et al., 1999), rely on raw text alone (Weeds et al., 2004; Mohammad, 2008). However, when used to rank word pairs in order of semantic distance or correct real-word spelling errors, they have been shown to perform poorly (Weeds et al., 2004; Mohammad and Hirst, 2006). Mohammad and Hirst (2006) and Patwardhan and Pedersen (2006) argued that word sense ambiguity is a key reason for the poor performance of traditional distributional measures, and they proposed hybrid approaches that are distributional in nature, but also make use of information in lexical resources such as published thesauri and WordNet. However, both these approaches can be applied to estimate the sem</context>
<context position="10814" citStr="Mohammad (2008)" startWordPosition="1692" endWordPosition="1693">the set of words that tend to co-occur with u within a certain distance, along with numeric scores signifying this co-occurrence tendency with u. Then measures such as cosine or α-skew divergence are used to determine how close the DPs of the two target words are. See Section 3 for more details and related work. These measures are very appealing because they rely simply on raw text, but, as described earlier, when used to rank word pairs in order of semantic distance, or to correct real-word spelling errors, they perform poorly, compared to the WordNet-based measures. See Weeds et al. (2004), Mohammad (2008), and Curran (2004) for detailed surveys of distributional measures. As Mohammad and Hirst (2006) point out, the DP of a word u conflates information about the potentially many senses of u. For example, consider the following. The noun bank has two senses “river bank” and “financial institution”. Assume that bank, when used in the “financial institution” sense, co-occurred with the noun money 100 times in a corpus. Similarly, assume that bank, when used in the “river bank” sense, co-occurred with the noun boat 80 times. So the DP of bank will have co-occurrence information with money as well a</context>
</contexts>
<marker>Mohammad, 2008</marker>
<rawString>Saif Mohammad. 2008. Measuring Semantic Distance using Distributional Profiles of Concepts. Ph.D. thesis, Department of Computer Science, University of Toronto, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Non-classical lexical semantic relations.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Computational Lexical Semantics, Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>46--51</pages>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="7504" citStr="Morris and Hirst, 2004" startWordPosition="1165" endWordPosition="1169">make use of their cross-lingual DPCs, to compute semantic distance in a resource-poor language, just as they did. We leave that for future work. 2 Background and Related Work Strictly speaking, semantic distance/closeness is a property of lexical units—a combination of the surface form and word sense.3 Two terms are considered to be semantically close if there is a lexical semantic relation between them. Such a relation may be a classical relation such as hypernymy, troponymy, meronymy, and antonymy, or it may be what have been called an ad-hoc nonclassical relation, such as cause-and-effect (Morris and Hirst, 2004). If the closeness in meaning is due to certain specific classical relations such as hypernymy and troponymy, then the terms are said to be semantically similar. Semantic relatedness is the term used to describe the more general form of semantic closeness caused by any semantic relation (Hirst and Budanitsky, 2005). So the nouns liquid and water are both semantically similar and semantically related, whereas the nouns boat and rudder are semantically related, but not similar. The next three sub-sections describe three kinds of automatic distance measures: (1) lexicalresource-based measures tha</context>
</contexts>
<marker>Morris, Hirst, 2004</marker>
<rawString>Jane Morris and Graeme Hirst. 2004. Non-classical lexical semantic relations. In Proceedings of the Workshop on Computational Lexical Semantics, Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 46–51, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Meaningful clustering of senses helps boost word sense disambiguation performance.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association,</booktitle>
<pages>105--112</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="9810" citStr="Navigli, 2006" startWordPosition="1529" endWordPosition="1530">can be computed from corpus statistics. Within WordNet, the is-a hierarchy is much more well-developed than that of other lexical semantic relations. So, not surprisingly, the best WordNet-based measures are those that rely only on the is-a hierarchy. Therefore, they are good at measuring semantic similarity (e.g., doctor–physician), but not semantic relatedness (e.g., doctor–scalpel). Further, the measures can only be used in languages that have a (sufficiently developed) WordNet. WordNet sense information has been criticized to be too fine grained (Agirre and Lopez de Lacalle Lekuona, 2003; Navigli, 2006). See Hirst and Budanitsky (2005) for a comprehensive survey of WordNet-based measures. 2.2 Corpus-based measures Strictly corpus-based measures of distributional similarity rely on the hypothesis that words that occur in similar context tend to be semantically close (Firth, 1957; Harris, 1940). The set of contexts of each target word u is represented by its distributional profile (DP)—the set of words that tend to co-occur with u within a certain distance, along with numeric scores signifying this co-occurrence tendency with u. Then measures such as cosine or α-skew divergence are used to det</context>
</contexts>
<marker>Navigli, 2006</marker>
<rawString>Roberto Navigli. 2006. Meaningful clustering of senses helps boost word sense disambiguation performance. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association, pages 105– 112, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ted Pedersen</author>
</authors>
<title>Using WordNet based context vectors to estimate the semantic relatedness of concepts.</title>
<date>2006</date>
<booktitle>In Proceedings of Making Sense of Sense EACL Workshop,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="3146" citStr="Patwardhan and Pedersen (2006)" startWordPosition="469" endWordPosition="472">s for the language of interest (which is not the case for the “low-density” languages); and even if there is a WordNet, a number of domainspecific terms may not be encoded in it. On the other hand, corpus-based distributional measures of semantic distance, such as cosine and α-skew divergence (Dagan et al., 1999), rely on raw text alone (Weeds et al., 2004; Mohammad, 2008). However, when used to rank word pairs in order of semantic distance or correct real-word spelling errors, they have been shown to perform poorly (Weeds et al., 2004; Mohammad and Hirst, 2006). Mohammad and Hirst (2006) and Patwardhan and Pedersen (2006) argued that word sense ambiguity is a key reason for the poor performance of traditional distributional measures, and they proposed hybrid approaches that are distributional in nature, but also make use of information in lexical resources such as published thesauri and WordNet. However, both these approaches can be applied to estimate the semantic distance between two terms only if both terms exist in the lexical resource they rely on. We know lexical resources tend to have limited vocabulary and a large number of domain775 Proceedings of the 2009 Conference on Empirical Methods in Natural La</context>
<context position="9043" citStr="Patwardhan and Pedersen, 2006" startWordPosition="1408" endWordPosition="1411"> a manually-created hierarchical network of nodes (taxonomy), where each node in 3The notion of semantic distance can be generalized, of course, to larger units such as phrases, sentences, passages, and so on (Landauer et al., 1998). 776 the network represents a fine-grained concept or word sense. An edge between two nodes represents a lexical semantic relation such as hypernymy and troponymy. WordNet-based measures consider two terms to be close if they occur close to each other in the network (connected by only a few arcs), if their definitions share many terms (Banerjee and Pedersen, 2003; Patwardhan and Pedersen, 2006), or if they share a lot of information (Lin, 1998; Resnik, 1999). The length of each arc/link (distance between nodes) can be assumed a unit length, or can be computed from corpus statistics. Within WordNet, the is-a hierarchy is much more well-developed than that of other lexical semantic relations. So, not surprisingly, the best WordNet-based measures are those that rely only on the is-a hierarchy. Therefore, they are good at measuring semantic similarity (e.g., doctor–physician), but not semantic relatedness (e.g., doctor–scalpel). Further, the measures can only be used in languages that h</context>
<context position="12078" citStr="Patwardhan and Pedersen (2006)" startWordPosition="1897" endWordPosition="1901">,80; bond,70; fish,77; ... Assume that the DP of the word ATM is: DPW(ATM): money,120; boat,0; bond,90; fish,0; ... Thus the distributional distance of bank with ATM will be some sort of an average of the semantic distance between the “financial institution” and “ATM” senses and the semantic distance between the “river bank” and “ATM” senses. However, in various natural language tasks, we need the semantic distance between the intended senses of bank and ATM, which often also tends to be the semantic distance between their closest senses. 2.3 Hybrid measures Both Mohammad and Hirst (2006) and Patwardhan and Pedersen (2006) proposed measures that are not only distributional in nature but also rely on a lexical resource to exploit the manually encoded information therein as well as to overcome the sense-conflation problem (described in section 2.2). Since we essentially combine the Mohammad and Hirst method with a “pure” wordbased distributional measure to create our hybrid approach, we briefly describe their method here. Mohammad and Hirst (2006) generate separate distributional profiles for the different senses of a word, without using any sense-annotated data. They use the categories in a Roget-style thesaurus</context>
</contexts>
<marker>Patwardhan, Pedersen, 2006</marker>
<rawString>Siddharth Patwardhan and Ted Pedersen. 2006. Using WordNet based context vectors to estimate the semantic relatedness of concepts. In Proceedings of Making Sense of Sense EACL Workshop, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-2006),</booktitle>
<pages>192--199</pages>
<location>New York, NY.</location>
<contexts>
<context position="24397" citStr="Ponzetto and Strube, 2006" startWordPosition="3838" endWordPosition="3841">p-cos-ll, and the two hybrid pmi results on RD-00 are better than any non-WordNet results reported by Resnik and Diab (2000), including their syntax-informed methods—the variants of Lin (“distrib”, .43) and Dorr (“LCS”, .39). In fact, our hybrid*-prop-cospmi and hybrid*-filt-cos-pmi results reach correlation levels of the WordNet-based methods reported there (.66–.68). Also, on WS-353, our hybrid sense-filtered variants and word-cos-ll obtained a correlation score higher than published results using WordNet-based measures (Jarmasz and Szpakowicz, 2003) (.33 to .35) and Wikipediabased methods (Ponzetto and Strube, 2006) (.19 to .48); and very close to the results obtained by thesaurus-based (Jarmasz and Szpakowicz, 2003) (.55) and LSA-based methods (Finkelstein et al., 2002) (.56). The lower correlation scores of all measures on the WS-353 test set are possibly due to it having politically biased word pairs (examples include: Arafat–peace, Arafat–terror, Jerusalem– Palestinian) for which BNC texts are likely to induce low correlation with the human raters of WS353. This testset also has disproportionately many terms from the news domain. The concept methods performed poorly on WS353 partly because many of th</context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-2006), pages 192–199, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Mona Diab</author>
</authors>
<title>Measuring verb similarity.</title>
<date>2000</date>
<booktitle>In 22nd Annual Meeting of the Cognitive Science Society (COGSCI2000),</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="20062" citStr="Resnik and Diab (2000)" startWordPosition="3190" endWordPosition="3193">emantic distance. These methods included our sense-biased methods as well as several baselines: the Mohammad and Hirst (2006) DPC-based methods, the traditional word-based distributional similarity methods, and several Latent Semantic Analysis (LSA)-based methods. We used three testsets and their corresponding human judgment gold standards: (1) the Rubenstein and Goodenough (1965) set of 65 noun pairs—denoted RG-65; (2) the WordSimilarity-353 (Finkelstein et al., 2002) set of 353 noun pairs (which include the RG-65 pairs) of which we discarded of one repeating pair—denoted WS-353; and (3) the Resnik and Diab (2000) set of 27 verb pairs—denoted RD-00. 4.1 Corpora and Pre-processing We generated distributional profiles (DPWs and DPCs) from the British National Corpus (BNC) (Burnard, 2000), which is a balanced corpus. We lowercased the characters, and stripped numbers, punctuation marks, and any SGML-like syntactic tags, but kept sentence boundary markers. The BNC contained 102,100,114 tokens of 546,299 types (vocabulary size) after tokenization. For the verb set, we also lemmatized this corpus. We considered two words as co-occurring if they occurred in a window of ±5 words from each other. We stoplisted </context>
<context position="23895" citStr="Resnik and Diab (2000)" startWordPosition="3768" endWordPosition="3771"> than their -ll counterparts. The -pmi hybrid variants obtained higher scores than the concept-based ones, but almost the same scores as the wordbased ones. 4.2.2 Results on WS-353 and RD-00 testsets On WS-353, all our hybrid methods outperformed their concept counterparts, and were on par with their word-based counterparts. On RD-00, word-cos-pmi out-performed all other word-based methods, and the hybrid -pmi methods were best performers with scores of .64 and .71. Our word-cos-ll, hybrid-prop-cos-ll, and the two hybrid pmi results on RD-00 are better than any non-WordNet results reported by Resnik and Diab (2000), including their syntax-informed methods—the variants of Lin (“distrib”, .43) and Dorr (“LCS”, .39). In fact, our hybrid*-prop-cospmi and hybrid*-filt-cos-pmi results reach correlation levels of the WordNet-based methods reported there (.66–.68). Also, on WS-353, our hybrid sense-filtered variants and word-cos-ll obtained a correlation score higher than published results using WordNet-based measures (Jarmasz and Szpakowicz, 2003) (.33 to .35) and Wikipediabased methods (Ponzetto and Strube, 2006) (.19 to .48); and very close to the results obtained by thesaurus-based (Jarmasz and Szpakowicz, </context>
<context position="26163" citStr="Resnik and Diab (2000)" startWordPosition="4117" endWordPosition="4120">ammad and Hirst methods and variants concept-cos-cp .62 .38 .41 concept*-cos-cp .65 .33 .43 concept-cos-ll .60 .37 .43 concept*-cos-ll .64 .25 .27 concept*-cos-pmi .40 .19 .28 Other (LSA and variants) word-cos-cp .53 .31 word-cos-ll .70 .54 word-cos-pmi .62 .43 780 hybrid*-prop-cos-ll compared to word-cos-ll. But results suggest that the -pmi methods might serve as a better measure than -ll for verbs, although this claim should be tested more rigorously. Human judgments of semantic distance are less consistent on verb-pairs than on noun-pairs, as reflected in inter-rater agreement measures in Resnik and Diab (2000) and others). Thus, not surprisingly, the scores of almost all measures are lower for the verb data than the RG-65 noun data. 5 Discussion The hybrid methods proposed in this paper obtained higher accuracies than all other methods on the RG-65 testset (all of whose words were in the published thesaurus), and on the RD-00 testset, and their performance was at least respectable on the WS-353 testset (many of whose words were not in the published thesaurus). This is in contrast to the concept-distance methods which suffer greatly when the target words are not in the lexical resource (here, the th</context>
</contexts>
<marker>Resnik, Diab, 2000</marker>
<rawString>Philip Resnik and Mona Diab. 2000. Measuring verb similarity. In 22nd Annual Meeting of the Cognitive Science Society (COGSCI2000), Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language.</title>
<date>1999</date>
<journal>JAIR,</journal>
<pages>11--95</pages>
<contexts>
<context position="1846" citStr="Resnik, 1999" startWordPosition="268" endWordPosition="269">tance show the new hybrid method to be superior to others. 1 Introduction Semantic distance is a measure of the closeness in meaning of two concepts. People are consistent judges of semantic distance. For example, we can easily tell that the concepts of “exercise” and “jog” are closer in meaning than “exercise” and “theater”. Studies asking native speakers of a language to rank word pairs in order of semantic distance confirm this—average inter-annotator correlation on ranking word pairs in order of semantic distance has been repeatedly shown to be around 0.9 (Rubenstein and Goodenough, 1965; Resnik, 1999). A number of natural language tasks such as machine translation (Lopez, 2008) and word sense disambiguation (Banerjee and Pedersen, 2003; McCarthy, 2006), can be framed as semantic distance problems. Thus, developing automatic measures that are in-line with human notions of semantic distance has received much attention. These automatic approaches to semantic distance rely on manually created lexical resources such as WordNet, large amounts of text corpora, or both. WordNet-based information content measures have been successful (Hirst and Budanitsky, 2005), but there are significant limitatio</context>
<context position="9108" citStr="Resnik, 1999" startWordPosition="1423" endWordPosition="1424">The notion of semantic distance can be generalized, of course, to larger units such as phrases, sentences, passages, and so on (Landauer et al., 1998). 776 the network represents a fine-grained concept or word sense. An edge between two nodes represents a lexical semantic relation such as hypernymy and troponymy. WordNet-based measures consider two terms to be close if they occur close to each other in the network (connected by only a few arcs), if their definitions share many terms (Banerjee and Pedersen, 2003; Patwardhan and Pedersen, 2006), or if they share a lot of information (Lin, 1998; Resnik, 1999). The length of each arc/link (distance between nodes) can be assumed a unit length, or can be computed from corpus statistics. Within WordNet, the is-a hierarchy is much more well-developed than that of other lexical semantic relations. So, not surprisingly, the best WordNet-based measures are those that rely only on the is-a hierarchy. Therefore, they are good at measuring semantic similarity (e.g., doctor–physician), but not semantic relatedness (e.g., doctor–scalpel). Further, the measures can only be used in languages that have a (sufficiently developed) WordNet. WordNet sense information</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999. Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language. JAIR, 11:95–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="1831" citStr="Rubenstein and Goodenough, 1965" startWordPosition="264" endWordPosition="267">word-pair ranking by semantic distance show the new hybrid method to be superior to others. 1 Introduction Semantic distance is a measure of the closeness in meaning of two concepts. People are consistent judges of semantic distance. For example, we can easily tell that the concepts of “exercise” and “jog” are closer in meaning than “exercise” and “theater”. Studies asking native speakers of a language to rank word pairs in order of semantic distance confirm this—average inter-annotator correlation on ranking word pairs in order of semantic distance has been repeatedly shown to be around 0.9 (Rubenstein and Goodenough, 1965; Resnik, 1999). A number of natural language tasks such as machine translation (Lopez, 2008) and word sense disambiguation (Banerjee and Pedersen, 2003; McCarthy, 2006), can be framed as semantic distance problems. Thus, developing automatic measures that are in-line with human notions of semantic distance has received much attention. These automatic approaches to semantic distance rely on manually created lexical resources such as WordNet, large amounts of text corpora, or both. WordNet-based information content measures have been successful (Hirst and Budanitsky, 2005), but there are signif</context>
<context position="19823" citStr="Rubenstein and Goodenough (1965)" startWordPosition="3151" endWordPosition="3154">the collocate fish is now filtered out, whereas bank’s co-occurrence counts with money, boat, and bond are left as is (and not senseproportionally attenuated). 4 Evaluation We evaluated various methods on the task of ranking word pairs in order of semantic distance. These methods included our sense-biased methods as well as several baselines: the Mohammad and Hirst (2006) DPC-based methods, the traditional word-based distributional similarity methods, and several Latent Semantic Analysis (LSA)-based methods. We used three testsets and their corresponding human judgment gold standards: (1) the Rubenstein and Goodenough (1965) set of 65 noun pairs—denoted RG-65; (2) the WordSimilarity-353 (Finkelstein et al., 2002) set of 353 noun pairs (which include the RG-65 pairs) of which we discarded of one repeating pair—denoted WS-353; and (3) the Resnik and Diab (2000) set of 27 verb pairs—denoted RD-00. 4.1 Corpora and Pre-processing We generated distributional profiles (DPWs and DPCs) from the British National Corpus (BNC) (Burnard, 2000), which is a balanced corpus. We lowercased the characters, and stripped numbers, punctuation marks, and any SGML-like syntactic tags, but kept sentence boundary markers. The BNC contain</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04),</booktitle>
<pages>1015--1021</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="2874" citStr="Weeds et al., 2004" startWordPosition="426" endWordPosition="429">ces such as WordNet, large amounts of text corpora, or both. WordNet-based information content measures have been successful (Hirst and Budanitsky, 2005), but there are significant limitations on their applicability. They can be applied only if a WordNet exists for the language of interest (which is not the case for the “low-density” languages); and even if there is a WordNet, a number of domainspecific terms may not be encoded in it. On the other hand, corpus-based distributional measures of semantic distance, such as cosine and α-skew divergence (Dagan et al., 1999), rely on raw text alone (Weeds et al., 2004; Mohammad, 2008). However, when used to rank word pairs in order of semantic distance or correct real-word spelling errors, they have been shown to perform poorly (Weeds et al., 2004; Mohammad and Hirst, 2006). Mohammad and Hirst (2006) and Patwardhan and Pedersen (2006) argued that word sense ambiguity is a key reason for the poor performance of traditional distributional measures, and they proposed hybrid approaches that are distributional in nature, but also make use of information in lexical resources such as published thesauri and WordNet. However, both these approaches can be applied to</context>
<context position="10797" citStr="Weeds et al. (2004)" startWordPosition="1688" endWordPosition="1691">utional profile (DP)—the set of words that tend to co-occur with u within a certain distance, along with numeric scores signifying this co-occurrence tendency with u. Then measures such as cosine or α-skew divergence are used to determine how close the DPs of the two target words are. See Section 3 for more details and related work. These measures are very appealing because they rely simply on raw text, but, as described earlier, when used to rank word pairs in order of semantic distance, or to correct real-word spelling errors, they perform poorly, compared to the WordNet-based measures. See Weeds et al. (2004), Mohammad (2008), and Curran (2004) for detailed surveys of distributional measures. As Mohammad and Hirst (2006) point out, the DP of a word u conflates information about the potentially many senses of u. For example, consider the following. The noun bank has two senses “river bank” and “financial institution”. Assume that bank, when used in the “financial institution” sense, co-occurred with the noun money 100 times in a corpus. Similarly, assume that bank, when used in the “river bank” sense, co-occurred with the noun boat 80 times. So the DP of bank will have co-occurrence information wit</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity. In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04), pages 1015–1021, Geneva, Switzerland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>