<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000454">
<title confidence="0.995935">
Bayesian Grammar Induction for Language Modeling
</title>
<author confidence="0.985852">
Stanley F. Chen
</author>
<affiliation confidence="0.980816666666667">
Aiken Computation Laboratory
Division of Applied Sciences
Harvard University
</affiliation>
<address confidence="0.956495">
Cambridge, MA 02138
</address>
<email confidence="0.978646">
sfadas.harvard.edu
</email>
<sectionHeader confidence="0.992618" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999850235294118">
We describe a corpus-based induction algo-
rithm for probabilistic context-free gram-
mars. The algorithm employs a greedy
heuristic search within a Bayesian frame-
work, and a post-pass using the Inside-
Outside algorithm. We compare the per-
formance of our algorithm to n-gram mo-
dels and the Inside-Outside algorithm in
three language modeling tasks. In two of
the tasks, the training data is generated by
a probabilistic context-free grammar and in
both tasks our algorithm outperforms the
other techniques. The third task involves
naturally-occurring data, and in this task
our algorithm does not perform as well as
n-gram models but vastly outperforms the
Inside-Outside algorithm.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999301875">
In applications such as speech recognition, hand-
writing recognition, and spelling correction, perfor-
mance is limited by the quality of the language mo-
del utilized (?; ?; ?; ?). However, static language
modeling performance has remained basically un-
changed since the advent of n-gram language mo-
dels forty years ago (?). Yet, n-gram language mo-
dels can only capture dependencies within an n-
word window, where currently the largest practical
n for natural language is three, and many dependen-
cies in natural language occur beyond a three-word
window. In addition, n-gram models are extremely
large, thus making them difficult to implement effi-
ciently in memory-constrained applications.
An appealing alternative is grammar-based lan-
guage models. Language models expressed as a pro-
babilistic grammar tend to be more compact than
n-gram language models, and have the ability to mo-
del long-distance dependencies (?; ?; ?). However,
to date there has been little success in constructing
grammar-based language models competitive with
n-gram models in problems of any magnitude.
In this paper, we describe a corpus-based induc-
tion algorithm for probabilistic context-free gram-
mars that outperforms n-gram models and the
Inside-Outside algorithm (?) in medium-sized do-
mains. This result marks the first time a grammar-
based language model has surpassed n-gram mode-
ling in a task of at least moderate size. The al-
gorithm employs a greedy heuristic search within
a Bayesian framework, and a post-pass using the
Inside-Outside algorithm.
</bodyText>
<sectionHeader confidence="0.82552" genericHeader="method">
2 Grammar Induction as Search
</sectionHeader>
<bodyText confidence="0.999970129032258">
Grammar induction can be framed as a search pro-
blem, and has been framed as such almost without
exception in past research (?). The search space is
taken to be some class of grammars; for example, in
our work we search within the space of probabilistic
context-free grammars. The objective function is ta-
ken to be some measure dependent on the training
data; one generally wants to find a grammar that in
some sense accurately models the training data.
Most work in language modeling, including n-
gram models and the Inside-Outside algorithm, falls
under the maximum-likelihood paradigm, where one
takes the objective function to be the likelihood of
the training data given the grammar. However, the
optimal grammar under this objective function is
one which generates only strings in the training data
and no other strings. Such grammars are poor lan-
guage models, as they overfit the training data and
do not model the language at large. In n-gram mo-
dels and the Inside-Outside algorithm, this issue is
evaded by bounding the size and form of the gram-
mars considered, so that the &amp;quot;optimal&amp;quot; grammar
cannot be expressed. However, in our work we do
not wish to limit the size of the grammars conside-
red.
The basic shortcoming of the maximum-likelihood
objective function is that it does not encompass the
compelling intuition behind Occam&apos;s Razor, that
simpler (or smaller) grammars are preferable over
complex (or larger) grammars. A factor in the ob-
jective function that favors smaller grammars over
</bodyText>
<page confidence="0.99108">
228
</page>
<figure confidence="0.779818166666667">
SX (1 — e)
X (e)
A (p(A)) V AEN— {S, X}
a (1) &apos;/GET
N = the set of all nonterminal symbols
T = the set of all terminal symbols
</figure>
<tableCaption confidence="0.9608315">
Probabilities for each rule are in parentheses.
Table 1: Initial hypothesis grammar
</tableCaption>
<bodyText confidence="0.997625">
large can prevent the objective function from pre-
ferring grammars that overfit the training data. ?)
presents a Bayesian grammar induction framework
that includes such a factor in a motivated manner.
The goal of grammar induction is taken to be fin-
ding the grammar with the largest a posteriori pro-
bability given the training data, that is, finding the
grammar G&apos; where
</bodyText>
<equation confidence="0.973737">
G&apos; = arg msxp(G10)
</equation>
<bodyText confidence="0.999928">
and where we denote the training data as 0, for ob-
servations. As it is unclear how to estimate p(GJO)
directly, we apply Bayes&apos; Rule and get
</bodyText>
<equation confidence="0.998856333333333">
p(OIG)p(G)
= arg max (0) = arg max p(0 IG)p(G)
G p
</equation>
<bodyText confidence="0.974209277777778">
Hence, we can frame the search for G&apos; as a search
with the objective function p(01G)p(G), the likeli-
hood of the training data multiplied by the prior
probability of the grammar.
We satisfy the goal of favoring smaller grammars
by choosing a prior that assigns higher probabilities
to such grammars. In particular, Solomonoff propo-
ses the use of the universal a priori probability (?),
which is closely related to the minimum description
length principle later proposed by (?). In the case
of grammatical language modeling, this corresponds
to taking
p(G) = 2-1(G)
where l(G) is the length of the description of the
grammar in bits. The universal a priori probabi-
lity has many elegant properties, the most salient
of which is that it dominates all other enumerable
probability distributions multiplicatively.1
</bodyText>
<sectionHeader confidence="0.97449" genericHeader="method">
3 Search Algorithm
</sectionHeader>
<bodyText confidence="0.963064565217391">
As described above, we take grammar induction to
be the search for the grammar G&apos; that optimizes the
objective function p(OIG)p(G). While this frame-
work does not restrict us to a particular grammar
formalism, in our work we consider only probabili-
stic context-free grammars.
&apos;A very thorough discussion of the universal a priori
probability is given by 7).
We assume a simple greedy search strategy. We
maintain a single hypothesis grammar which is in-
itialized to a small, trivial grammar. We then try to
find a modification to the hypothesis grammar, such
as the addition of a grammar rule, that results in a
grammar with a higher score on the objective func-
tion. When we find a superior grammar, we make
this the new hypothesis grammar. We repeat this
process until we can no longer find a modification
that improves the current hypothesis grammar.
For our initial grammar, we choose a grammar
that can generate any string, to assure that the
grammar can cover the training data. The initial
grammar is listed in Table??. The sentential symbol
S expands to a sequence of X&apos;s, where X expands
to every other nonterminal symbol in the grammar.
Initially, the set of nonterminal symbols consists of
a different nonterminal symbol expanding to each
terminal symbol.
Notice that this grammar models a sentence as
a sequence of independently generated nonterminal
symbols. We maintain this property throughout the
search process, that is, for every symbol A&apos; that we
add to the grammar, we also add a rule X -4 A&apos;.
This assures that the sentential symbol can expand
to every symbol; otherwise, adding a symbol will not
affect the probabilities that the grammar assigns to
strings.
We use the term move set to describe the set of
modifications we consider to the current hypothesis
grammar to hopefully produce a superior grammar.
Our move set includes the following moves:
Move 1: Create a rule of the form A —&gt; BC
Move 2: Create a rule of the form A —&gt; BIC
For any context-free grammar, it is possible to ex-
press a weakly equivalent grammar using only rules
of these forms. As mentioned before, with each new
symbol A we also create a rule X --+ A.
</bodyText>
<subsectionHeader confidence="0.999249">
3.1 Evaluating the Objective Function
</subsectionHeader>
<bodyText confidence="0.9896375">
Consider the task of calculating the objective func-
tion p(01G)p(G) for some grammar G. Calculating
</bodyText>
<page confidence="0.9871">
229
</page>
<figure confidence="0.9981657">
X
ABob
Bob
Atolks
talks
A talks
talks
X
Amary
Mary
</figure>
<figureCaption confidence="0.979005">
Figure 1: Initial Viterbi Parse
</figureCaption>
<figure confidence="0.8427375">
X X
AMary Atalks Aslowly ABob Atalks Aslowly
</figure>
<figureCaption confidence="0.979026">
Mary talks slowly Bob talks slowly
Figure 2: Predicted Viterbi Parse
</figureCaption>
<bodyText confidence="0.984015269230769">
p(G) 2-1(G) is inexpensive2; however, calculating
p(01G) requires a parsing of the entire training data.
We cannot afford to parse the training data for each
grammar considered; indeed, to ever be practical for
data sets of millions of words, it seems likely that we
can only afford to parse the data once.
To achieve this goal, we employ several approxi-
mations. First, notice that we do not ever need to
calculate the actual value of the objective function;
we need only to be able to distinguish when a move
applied to the current hypothesis grammar produces
a grammar that has a higher score on the objective
function, that is, we need only to be able to calcu-
late the difference in the objective function resulting
from a move. This can be done efficiently if we can
quickly approximate how the probability of the trai-
ning data changes when a move is applied.
To make this possible, we approximate the proba-
bility of the training data p(01G) by the probability
of the single most probable parse, or Viterbi parse,
of the training data. Furthermore, instead of recal-
culating the Viterbi parse of the training data from
scratch when a move is applied, we use heuristics to
predict how a move will change the Viterbi parse.
For example, consider the case where the training
data consists of the two sentences
</bodyText>
<equation confidence="0.784903">
0 = {Bob talks slowly, Mary talks slowly}
</equation>
<bodyText confidence="0.983326875">
2Due to space limitations, we do not specify our me-
thod for encoding grammars, i.e., how we calculate l(G)
for a given G. However, this will be described in the
author&apos;s forthcoming Ph.D. dissertation.
In Figure ??, we display the Viterbi parse of this
data under the initial hypothesis grammar used in
our algorithm.
Now, let us consider the move of adding the rule
</bodyText>
<subsubsectionHeader confidence="0.457579">
B Atoms Astowty
</subsubsectionHeader>
<bodyText confidence="0.99926432">
to the initial grammar (as well as the concomitant
rule X B). A reasonable heuristic for predic-
ting how the Viterbi parse will change is to replace
adjacent X&apos;s that expand to Atalks and Aslowiy re-
spectively with a single X that expands to B, as
displayed in Figure ??. This is the actual heuristic
we use for moves of the form A BC, and we have
analogous heuristics for each move in our move set.
By predicting the differences in the Viterbi parse re-
sulting from a move, we can quickly estimate the
change in the probability of the training data.
Notice that our predicted Viterbi parse can stray
a great deal from the actual Viterbi parse, as errors
can accumulate as move after move is applied. To
minimize these effects, we process the training data
incrementally. Using our initial hypothesis gram-
mar, we parse the first sentence of the training data
and search for the optimal grammar over just that
one sentence using the described search framework.
We use the resulting grammar to parse the second
sentence, and then search for the optimal grammar
over the first two sentences using the last grammar
as the starting point. We repeat this process, par-
sing the next sentence using the best grammar found
on the previous sentences and then searching for the
</bodyText>
<page confidence="0.971093">
230
</page>
<bodyText confidence="0.999595875">
best grammar taking into account this new sentence,
until the entire training corpus is covered.
Delaying the parsing of a sentence until all of the
previous sentences are processed should yield more
accurate Viterbi parses during the search process
than if we simply parse the whole corpus with the
initial hypothesis grammar. In addition, we still
achieve the goal of parsing each sentence but once.
</bodyText>
<subsectionHeader confidence="0.999378">
3.2 Parameter Training
</subsectionHeader>
<bodyText confidence="0.99983885">
In this section, we describe how the parameters of
our grammar, the probabilities associated with each
grammar rule, are set. Ideally, in evaluating the ob-
jective function for a particular grammar we should
use its optimal parameter settings given the training
data, as this is the full score that the given gram-
mar can achieve. However, searching for optimal
parameter values is extremely expensive computa-
tionally. Instead, we grossly approximate the opti-
mal values by deterministically setting parameters
based on the Viterbi parse of the training data par-
sed so far. We rely on the post-pass, described later,
to refine parameter values.
Referring to the rules in Table ??, the parameter
c is set to an arbitrary small constant. The values
of the parameters p(A) are set to the (smoothed)
frequency of the X —+ A reduction in the Viterbi
parse of the data seen so far. The remaining symbols
are set to expand uniformly among their possible
expansions.
</bodyText>
<subsectionHeader confidence="0.999317">
3.3 Constraining Moves
</subsectionHeader>
<bodyText confidence="0.966909916666667">
Consider the move of creating a rule of the form
A BC. This corresponds to k3 different specific
rules that might be created, where k is the current
number of symbols in the grammar. As it is too
computationally expensive to consider each of these
rules at every point in the search, we use heuristics
to constrain which moves are appraised.
For the left-hand side of a rule, we always create
a new symbol. This heuristic selects the optimal
choice the vast majority of the time; however, under
this constraint the moves described earlier in this
section cannot yield arbitrary context-free langua-
ges. To partially address this, we add the move
Move 3: Create a rule of the form A ABIB
With this iteration move, we can construct gram-
mars that generate arbitrary regular languages. As
yet, we have not implemented moves that enable
the construction of arbitrary context-free grammars;
this belongs to future work.
To constrain the symbols we consider on the
right-hand side of a new rule, we use what we call
triggers.3 A trigger is a phenomenon in the Viterbi
parse of a sentence that is indicative that a particular
move might lead to a better grammar. For example,
</bodyText>
<footnote confidence="0.923025">
3This is not to be confused with the use of the term
triggers in dynamic language modeling.
</footnote>
<bodyText confidence="0.998969857142857">
in Figure ?? the fact that the symbols Atalks and
Asiowty occur adjacently is indicative that it could
be profitable to create a rule B AtarksAstowty. We
have developed a set of triggers for each move in our
move set, and only consider a specific move if it is
triggered in the sentence currently being parsed in
the incremental processing.
</bodyText>
<subsectionHeader confidence="0.991412">
3.4 Post-Pass
</subsectionHeader>
<bodyText confidence="0.859779466666667">
A conspicuous shortcoming in our search framework
is that the grammars in our search space are fairly
unexpressive. Firstly, recall that our grammars mo-
del a sentence as a sequence of independently gene-
rated symbols; however, in language there is a large
dependence between adjacent constituents. Further-
more, the only free parameters in our search are the
parameters p(A); all other symbols (except S) are
fixed to expand uniformly. These choices were ne-
cessary to make the search tractable.
To address this issue, we use an Inside-Outside al-
gorithm post-pass. Our methodology is derived from
that described by ?). We create n new nonterminal
symbols {X1, ,X,}, and create all rules of the
form:
</bodyText>
<equation confidence="0.741053666666667">
—+X, Xk i,j,k E {1,...,n}
Xi —0 A
A E Nom — {S, X}
</equation>
<bodyText confidence="0.997298823529412">
!Void denotes the set of nonterminal symbols acqui-
red in the initial grammar induction phase, and X1
is taken to be the new sentential symbol. These
new rules replace the first three rules listed in Ta-
ble ??. The parameters of these rules are initialized
randomly. Using this grammar as the starting point,
we run the Inside-Outside algorithm on the training
data until convergence.
In other words, instead of using the naive S
SXIX rule to attach symbols together in parsing
data, we now use the Xi rules and depend on the
Inside-Outside algorithm to train these randomly
initialized rules intelligently. This post-pass allows
us to express dependencies between adjacent sym-
bols. In addition, it allows us to train parameters
that were fixed during the initial grammar induc-
tion phase.
</bodyText>
<sectionHeader confidence="0.999187" genericHeader="method">
4 Previous Work
</sectionHeader>
<bodyText confidence="0.998963272727273">
As mentioned, this work employs the Bayesian gram-
mar induction framework described by Solomonoff
(?; ?). However, Solomonoff does not specify a con-
crete search algorithm and only makes suggestions
as to its nature.
Similar research includes work by Cook et al.
(1976) and Stolcke and Omohundro (1994). This
work also employs a heuristic search within a Baye-
sian framework. However, a different prior proba-
bility on grammars is used, and the algorithms are
only efficient enough to be applied to small data sets.
</bodyText>
<page confidence="0.994122">
231
</page>
<bodyText confidence="0.9999037">
The grammar induction algorithms most suc-
cessful in language modeling include the Inside-
Outside algorithm (?; ?; ?), a special case of the
Expectation-Maximization algorithm (?), and work
by ?). In the latter work, McCandless uses a heu-
ristic search procedure similar to ours, but a very
different search criteria. To our knowledge, neither
algorithm has surpassed the performance of n-gram
models in a language modeling task of substantial
scale.
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.998550363636364">
To evaluate our algorithm, we compare the perfor-
mance of our algorithm to that of n-gram models
and the Inside-Outside algorithm.
For n-gram models, we tried n = 1, ..., 10 for
each domain. For smoothing a particular n-gram
model, we took a linear combination of all lower or-
der n-gram models. In particular, we follow stan-
dard practice (?; ?; ?) and take the smoothed i-
gram probability to be a linear combination of the
i-gram frequency in the training data and the smoo-
thed (i — 1)-gram probability, that is,
</bodyText>
<equation confidence="0.999744">
P(wo I W = wi- • • • w-1) =
c(Ww0)
Ai,c(w)
c(W)
(1 — )ti,c(w))P(wo Iwi-2 • • • w-i)
</equation>
<bodyText confidence="0.997285647058824">
where c(W) denotes the count of the word sequence
W in the training data. The smoothing parameters
are trained through the Forward-Backward al-
gorithm (7) on held-out data. Parameters Ai,, are
tied together for similar c to prevent data sparsity.
For the Inside-Outside algorithm, we follow the
methodology described by Lan i and Young. For a
given n, we create a probabilistic context-free gram-
mar consisting of all Chomsky normal form rules
over the n nonterminal symbols {X1,.. . Xn} and the
given terminal symbols, that is, all rules
Xi -0 Xi Xk i,j,k E {1, , n}
Xi —0 a E {1, , n}, a E T
where T denotes the set of terminal symbols in the
domain. All parameters are initialized randomly.
From this starting point, the Inside-Outside algo-
rithm is run until convergence.
For smoothing, we combine the expansion distri-
bution of each symbol with a uniform distribution,
that is, we take the smoothed parameter ps(A a)
to be
p, (A --+ a) = (1 — A )pn(A -.4 a) + A ri3 +1701
where Pu (A a) denotes the unsmoothed parame-
ter. The value n3 + nITI is the number of different
ways a symbol expands under the Lan i and Young
methodology. The parameter A is trained through
the Inside-Outside algorithm on held-out data. This
smoothing is also performed on the Inside-Outside
post-pass of our algorithm. For each domain, we
tried n= 3, ..., 10.
Because of the computational demands of our al-
gorithm, it is currently impractical to apply it to
large vocabulary or large training set problems. Ho-
wever, we present the results of our algorithm in
three medium-sized domains. In each case, we use
4500 sentences for training, with 500 of these sent-
ences held out for smoothing. We test on 500 sent-
ences, and measure performance by the entropy of
the test data.
In the first two domains, we created the training
and test data artificially so as to have an ideal gram-
mar in hand to benchmark results. In particular, we
used a probabilistic grammar to generate the data.
In the first domain, we created this grammar by
hand; the grammar was a small English-like probabi-
listic context-free grammar consisting of roughly 10
nonterminal symbols, 20 terminal symbols, and 30
rules. In the second domain, we derived the gram-
mar from manually parsed text. From a million
words of parsed Wall Street Journal data from the
Penn treebank, we extracted the 20 most frequently
occurring symbols, and the 10 most frequently oc-
curring rules expanding each of these symbols. For
each symbol that occurs on the right-hand side of a
rule but which was not one of the most frequent 20
symbols, we create a rule that expands that symbol
to a unique terminal symbol. After removing unre-
achable rules, this yields a grammar of roughly 30
nonterminals, 120 terminals, and 160 rules. Para-
meters are set to reflect the frequency of the corre-
sponding rule in the parsed corpus.
For the third domain, we took English text and
reduced the size of the vocabulary by mapping each
word to its part-of-speech tag. We used tagged Wall
Street Journal text from the Penn treebank, which
has a tag set size of about fifty.
In Tables ??-??, we summarize our results. The
ideal grammar denotes the grammar used to gene-
rate the training and test data. For each algorithm,
we list the best performance achieved over all n tried,
and the best n column states which value realized
this performance.
We achieve a moderate but significant improve-
ment in performance over n-gram models and the
Inside-Outside algorithm in the first two domains,
while in the part-of-speech domain we are outper-
formed by n-gram models but we vastly outperform
the Inside-Outside algorithm.
In Table ??, we display a sample of the number
of parameters and execution time (on a Decstation
5000/33) associated with each algorithm. We choose
n to yield approximately equivalent performance for
each algorithm. The first pass row refers to the main
grammar induction phase of our algorithm, and the
post-pass row refers to the Inside-Outside post-pass.
</bodyText>
<page confidence="0.983531">
232
</page>
<table confidence="0.999872166666667">
best entropy entr. relative
n (bits/word) to n-gram
ideal grammar 2.30 —6.5%
our algorithm 7 2.37 —3.7%
n-gram model 4 2.46
Inside-Outside 9 2.60 +5.7%
</table>
<tableCaption confidence="0.905095">
Table 2: English-like artificial grammar
</tableCaption>
<table confidence="0.999912333333333">
best entropy entr. relative
n (bits/word) to n-gram
ideal grammar 4.13 —10.4%
our algorithm 9 4.44 —3.7%
n-gram model 4 4.61
Inside-Outside 9 4.64 +0.7%
</table>
<tableCaption confidence="0.999695">
Table 3: Wall Street Journal-like artificial grammar
</tableCaption>
<bodyText confidence="0.9999375">
Notice that our algorithm produces a significantly
more compact model than the n-gram model, while
running significantly faster than the Inside-Outside
algorithm even though we use an Inside-Outside
post-pass. Part of this discrepancy is due to the fact
that we require a smaller number of new nonterminal
symbols to achieve equivalent performance, but we
have also found that our post-pass converges more
quickly even given the same number of nonterminal
symbols.
</bodyText>
<sectionHeader confidence="0.9974" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999985848484848">
Our algorithm consistently outperformed the Inside-
Outside algorithm in these experiments. While we
partially attribute this difference to using a Bayesian
instead of maximum-likelihood objective function,
we believe that part of this difference results from a
more effective search strategy. In particular, though
both algorithms employ a greedy hill-climbing strat-
egy, our algorithm gains an advantage by being able
to add new rules to the grammar.
In the Inside-Outside algorithm, the gradient des-
cent search discovers the &amp;quot;nearest&amp;quot; local minimum in
the search landscape to the initial grammar. If there
are k rules in the grammar and thus k parameters,
then the search takes place in a fixed k-dimensional
space Ie. In our algorithm, it is possible to ex-
pand the hypothesis grammar, thus increasing the
dimensionality of the parameter space that is being
searched. An apparent local minimum in the space
Lik may no longer be a local minimum in the space
le+1; the extra dimension may provide a pathway
for further improvement of the hypothesis grammar.
Hence, our algorithm should be less prone to sub-
optimal local minima than the Inside-Outside algo-
rithm.
Outperforming n-gram models in the first two do-
mains demonstrates that our algorithm is able to
take advantage of the grammatical structure present
in data. However, the superiority of n-gram models
in the part-of-speech domain indicates that to be
competitive in modeling naturally-occurring data, it
is necessary to model collocational information ac-
curately. We need to modify our algorithm to more
aggressively model n-gram information.
</bodyText>
<sectionHeader confidence="0.995532" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999963333333333">
This research represents a step forward in the quest
for developing grammar-based language models for
natural language. We induce models that, while
being substantially more compact, outperform n-
gram language models in medium-sized domains.
The algorithm runs essentially in time and space li-
near in the size of the training data, so larger do-
mains are within our reach.
However, we feel the largest contribution of this
work does not lie in the actual algorithm specified,
but rather in its indication of the potential of the in-
duction framework described by Solomonoff in 1964.
We have implemented only a subset of the moves
that we have developed, and inspection of our re-
sults gives reason to believe that these additional
moves may significantly improve the performance of
our algorithm.
Solomonoff&apos;s induction framework is not restric-
ted to probabilistic context-free grammars. After
completing the implementation of our move set, we
plan to explore the modeling of context-sensitive
phenomena. This work demonstrates that Solomo-
noff&apos;s elegant framework deserves much further con-
sideration.
</bodyText>
<sectionHeader confidence="0.992616" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996505666666667">
We are indebted to Stuart Shieber for his suggestions
and guidance, as well as his invaluable comments on
earlier drafts of this paper. This material is based
</bodyText>
<page confidence="0.995308">
233
</page>
<table confidence="0.9998032">
best entropy entr. relative
n (bits/word) to n-gram
n-gram model 6 3.01
our algorithm 7 3.15 +4.7%
Inside-Outside 7 3.93 +30.6%
</table>
<tableCaption confidence="0.987235">
Table 4: English sentence part-of-speech sequences
</tableCaption>
<table confidence="0.998453">
WSJ n entropy no. time
artif. (bits/word) params (sec)
n-gram 3 4.61 15000 50
10 9 4.64 2000 30000
first pass 800 1000
post-pass 5 4.60 4000 5000
</table>
<tableCaption confidence="0.999259">
Table 5: Parameters and Training Time
</tableCaption>
<bodyText confidence="0.982981333333333">
on work supported by the National Science Founda-
tion under Grant Number IRI-9350192 to Stuart M.
Shieber.
</bodyText>
<sectionHeader confidence="0.994315" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982177852941176">
D. Angluin and C.H. Smith. 1983. Inductive in-
ference: theory and methods. ACM Computing
Surveys, 15:237-269.
L.R. Bahl, J.K. Baker, P.S. Cohen, F. Jelinek, B.L.
Lewis, and R.L. Mercer. 1978. Recognition of a
continuously read natural corpus. In Proceedings
of the IEEE International Conference on Acou-
stics, Speech and Signal Processing, pages 422-
424, Tulsa, Oklahoma, April.
Lalit R. Bahl, Frederick Jelinek, and Robert L. Mer-
cer. 1983. A maximum likelihood approach to
continuous speech recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelli-
gence, PAMI-5(2):179-190, March.
J.K. Baker. 1975. The DRAGON system — an over-
view. IEEE Transactions on Acoustics, Speech
and Signal Processing, 23:24-29, February.
J.K. Baker. 1979. Trainable grammars for speech
recognition. In Proceedings of the Spring Confe-
rence of the Acoustical Society of America, pages
547-550, Boston, MA, June.
L.E. Baum and J.A. Eagon. 1967. An inequality
with application to statistical estimation for pro-
babilistic functions of Markov processes and to a
model for ecology. Bulletin of the American Ma-
thematicians Society, 73:360-363.
Peter F. Brown, Vincent J. DellaPietra, Peter V.
deSouza, Jennifer C. Lai, and Robert L. Mercer.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467-479,
December.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical
Society, 39(B):1-38.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of Markov source parameters
from sparse data. In Proceedings of the Workshop
on Pattern Recognition in Practice, Amsterdam,
The Netherlands: North-Holland, May.
M.D. Kernighan, K.W. Church, and W.A. Gale.
1990. A spelling correction program based on a
noisy channel model. In Proceedings of the Thir-
teenth International Conference on Computatio-
nal Linguistics, pages 205-210.
K. Lan i and S.J. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Lan-
guage, 4:35-56.
K. Lan i and S.J. Young. 1991. Applications of sto-
chastic context-free grammars using the inside-
outside algorithm. Computer Speech and Lan-
guage, 5:237-257.
Ming Li and Paul Vitanyi. 1993. An Introduction
to Kolmogorov Complexity and its Applications.
Springer-Verlag.
Michael K. McCandless and James R. Glass. 1993.
Empirical acquisition of word and phrase classes
in the ATIS domain. In Third European Confe-
rence on Speech Communication and Technology,
Berlin, Germany, September.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracket cor-
pora. In Proceedings of the 30th Annual Meeting
of the ACL, pages 128-135, Newark, Delaware.
P. Resnik. 1992. Probabilistic tree-adjoining gram-
mar as a framework for statistical natural lan-
guage processing. In Proceedings of the 14th In-
</reference>
<page confidence="0.977177">
234
</page>
<reference confidence="0.998020521739131">
ternational Conference on Computational Lingui-
stics.
J. Rissanen. 1978. Modeling by the shortest data
description. Automatica, 14:465-471.
Y. Schabes. 1992. Stochastic lexicalized tree-
adjoining grammars. In Proceedings of the 14th
International Conference on Computational Lin-
guistics.
C.E. Shannon. 1951. Prediction and entropy of
printed English. Bell Systems Technical Journal,
30:50-64, January.
R.J. Solomonoff. 1960. A preliminary report on
a general theory of inductive inference. Techni-
cal Report ZTB-138, Zator Company, Cambridge,
MA, November.
R.J. Solomonoff. 1964. A formal theory of inductive
inference. Information and Control, 7:1-22, 224-
254, March, June.
Rohini Srihari and Charlotte Baltus. 1992. Combi-
ning statistical and syntactic methods in recogni-
zing handwritten sentences. In AAAI Symposium:
Probabilistic Approaches to Natural Language, pa-
ges 121-127.
</reference>
<page confidence="0.998485">
235
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.819958">
<title confidence="0.999924">Bayesian Grammar Induction for Language Modeling</title>
<author confidence="0.999764">Stanley F Chen</author>
<affiliation confidence="0.999940333333333">Aiken Computation Laboratory Division of Applied Sciences Harvard University</affiliation>
<address confidence="0.999989">Cambridge, MA 02138</address>
<email confidence="0.999871">sfadas.harvard.edu</email>
<abstract confidence="0.989883888888889">We describe a corpus-based induction algorithm for probabilistic context-free grammars. The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using the Inside- Outside algorithm. We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks. In two of the tasks, the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques. The third task involves naturally-occurring data, and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Angluin</author>
<author>C H Smith</author>
</authors>
<title>Inductive inference: theory and methods.</title>
<date>1983</date>
<journal>ACM Computing Surveys,</journal>
<pages>15--237</pages>
<marker>Angluin, Smith, 1983</marker>
<rawString>D. Angluin and C.H. Smith. 1983. Inductive inference: theory and methods. ACM Computing Surveys, 15:237-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Bahl</author>
<author>J K Baker</author>
<author>P S Cohen</author>
<author>F Jelinek</author>
<author>B L Lewis</author>
<author>R L Mercer</author>
</authors>
<title>Recognition of a continuously read natural corpus.</title>
<date>1978</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>422--424</pages>
<location>Tulsa, Oklahoma,</location>
<marker>Bahl, Baker, Cohen, Jelinek, Lewis, Mercer, 1978</marker>
<rawString>L.R. Bahl, J.K. Baker, P.S. Cohen, F. Jelinek, B.L. Lewis, and R.L. Mercer. 1978. Recognition of a continuously read natural corpus. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 422-424, Tulsa, Oklahoma, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>A maximum likelihood approach to continuous speech recognition.</title>
<date>1983</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>5--2</pages>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. 1983. A maximum likelihood approach to continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-5(2):179-190, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>The DRAGON system — an overview.</title>
<date>1975</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<pages>23--24</pages>
<marker>Baker, 1975</marker>
<rawString>J.K. Baker. 1975. The DRAGON system — an overview. IEEE Transactions on Acoustics, Speech and Signal Processing, 23:24-29, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Proceedings of the Spring Conference of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<location>Boston, MA,</location>
<marker>Baker, 1979</marker>
<rawString>J.K. Baker. 1979. Trainable grammars for speech recognition. In Proceedings of the Spring Conference of the Acoustical Society of America, pages 547-550, Boston, MA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Baum</author>
<author>J A Eagon</author>
</authors>
<title>An inequality with application to statistical estimation for probabilistic functions of Markov processes and to a model for ecology.</title>
<date>1967</date>
<journal>Bulletin of the American Mathematicians Society,</journal>
<pages>73--360</pages>
<marker>Baum, Eagon, 1967</marker>
<rawString>L.E. Baum and J.A. Eagon. 1967. An inequality with application to statistical estimation for probabilistic functions of Markov processes and to a model for ecology. Bulletin of the American Mathematicians Society, 73:360-363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J DellaPietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<marker>Brown, DellaPietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. DellaPietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467-479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(B):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>In Proceedings of the Workshop on Pattern Recognition in Practice,</booktitle>
<location>Amsterdam, The Netherlands: North-Holland,</location>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Frederick Jelinek and Robert L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In Proceedings of the Workshop on Pattern Recognition in Practice, Amsterdam, The Netherlands: North-Holland, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Kernighan</author>
<author>K W Church</author>
<author>W A Gale</author>
</authors>
<title>A spelling correction program based on a noisy channel model.</title>
<date>1990</date>
<booktitle>In Proceedings of the Thirteenth International Conference on Computational Linguistics,</booktitle>
<pages>205--210</pages>
<marker>Kernighan, Church, Gale, 1990</marker>
<rawString>M.D. Kernighan, K.W. Church, and W.A. Gale. 1990. A spelling correction program based on a noisy channel model. In Proceedings of the Thirteenth International Conference on Computational Linguistics, pages 205-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lan i</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the insideoutside algorithm.</title>
<date>1990</date>
<journal>Computer Speech and Language,</journal>
<pages>4--35</pages>
<marker>i, Young, 1990</marker>
<rawString>K. Lan i and S.J. Young. 1990. The estimation of stochastic context-free grammars using the insideoutside algorithm. Computer Speech and Language, 4:35-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lan i</author>
<author>S J Young</author>
</authors>
<title>Applications of stochastic context-free grammars using the insideoutside algorithm.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--237</pages>
<marker>i, Young, 1991</marker>
<rawString>K. Lan i and S.J. Young. 1991. Applications of stochastic context-free grammars using the insideoutside algorithm. Computer Speech and Language, 5:237-257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming Li</author>
<author>Paul Vitanyi</author>
</authors>
<title>An Introduction to Kolmogorov Complexity and its Applications.</title>
<date>1993</date>
<publisher>Springer-Verlag.</publisher>
<marker>Li, Vitanyi, 1993</marker>
<rawString>Ming Li and Paul Vitanyi. 1993. An Introduction to Kolmogorov Complexity and its Applications. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael K McCandless</author>
<author>James R Glass</author>
</authors>
<title>Empirical acquisition of word and phrase classes in the ATIS domain. In</title>
<date>1993</date>
<booktitle>Third European Conference on Speech Communication and Technology,</booktitle>
<location>Berlin, Germany,</location>
<marker>McCandless, Glass, 1993</marker>
<rawString>Michael K. McCandless and James R. Glass. 1993. Empirical acquisition of word and phrase classes in the ATIS domain. In Third European Conference on Speech Communication and Technology, Berlin, Germany, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>Insideoutside reestimation from partially bracket corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the ACL,</booktitle>
<pages>128--135</pages>
<location>Newark, Delaware.</location>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando Pereira and Yves Schabes. 1992. Insideoutside reestimation from partially bracket corpora. In Proceedings of the 30th Annual Meeting of the ACL, pages 128-135, Newark, Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Probabilistic tree-adjoining grammar as a framework for statistical natural language processing.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics.</booktitle>
<marker>Resnik, 1992</marker>
<rawString>P. Resnik. 1992. Probabilistic tree-adjoining grammar as a framework for statistical natural language processing. In Proceedings of the 14th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Modeling by the shortest data description.</title>
<date>1978</date>
<journal>Automatica,</journal>
<pages>14--465</pages>
<marker>Rissanen, 1978</marker>
<rawString>J. Rissanen. 1978. Modeling by the shortest data description. Automatica, 14:465-471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
</authors>
<title>Stochastic lexicalized treeadjoining grammars.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics.</booktitle>
<marker>Schabes, 1992</marker>
<rawString>Y. Schabes. 1992. Stochastic lexicalized treeadjoining grammars. In Proceedings of the 14th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Shannon</author>
</authors>
<title>Prediction and entropy of printed English.</title>
<date>1951</date>
<journal>Bell Systems Technical Journal,</journal>
<pages>30--50</pages>
<marker>Shannon, 1951</marker>
<rawString>C.E. Shannon. 1951. Prediction and entropy of printed English. Bell Systems Technical Journal, 30:50-64, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Solomonoff</author>
</authors>
<title>A preliminary report on a general theory of inductive inference.</title>
<date>1960</date>
<tech>Technical Report ZTB-138,</tech>
<institution>Zator Company,</institution>
<location>Cambridge, MA,</location>
<marker>Solomonoff, 1960</marker>
<rawString>R.J. Solomonoff. 1960. A preliminary report on a general theory of inductive inference. Technical Report ZTB-138, Zator Company, Cambridge, MA, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Solomonoff</author>
</authors>
<title>A formal theory of inductive inference.</title>
<date>1964</date>
<journal>Information and Control,</journal>
<volume>7</volume>
<pages>224--254</pages>
<marker>Solomonoff, 1964</marker>
<rawString>R.J. Solomonoff. 1964. A formal theory of inductive inference. Information and Control, 7:1-22, 224-254, March, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohini Srihari</author>
<author>Charlotte Baltus</author>
</authors>
<title>Combining statistical and syntactic methods in recognizing handwritten sentences.</title>
<date>1992</date>
<booktitle>In AAAI Symposium: Probabilistic Approaches to Natural Language,</booktitle>
<pages>121--127</pages>
<marker>Srihari, Baltus, 1992</marker>
<rawString>Rohini Srihari and Charlotte Baltus. 1992. Combining statistical and syntactic methods in recognizing handwritten sentences. In AAAI Symposium: Probabilistic Approaches to Natural Language, pages 121-127.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>