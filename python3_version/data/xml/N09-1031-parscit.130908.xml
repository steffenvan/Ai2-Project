<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000244">
<title confidence="0.987587">
Predicting Risk from Financial Reports with Regression
</title>
<author confidence="0.995418">
Shimon Kogan
</author>
<affiliation confidence="0.9920695">
McCombs School of Business
University of Texas at Austin
</affiliation>
<address confidence="0.727677">
Austin, TX 78712, USA
</address>
<email confidence="0.994648">
shimon.kogan@mccombs.utexas.edu
</email>
<author confidence="0.993622">
Bryan R. Routledge
</author>
<affiliation confidence="0.871451333333333">
Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998187">
routledge@cmu.edu
</email>
<author confidence="0.993801">
Dimitry Levin
</author>
<affiliation confidence="0.886878">
Mellon College of Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.96817">
dimitrylevin@gmail.com
</email>
<author confidence="0.99403">
Jacob S. Sagi
</author>
<affiliation confidence="0.837893">
Owen Graduate School of Management
Vanderbilt University
Nashville, TN 37203, USA
</affiliation>
<email confidence="0.996386">
Jacob.Sagi@Owen.Vanderbilt.edu
</email>
<author confidence="0.996507">
Noah A. Smith
</author>
<affiliation confidence="0.943113666666667">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.993308">
nasmith@cs.cmu.edu
</email>
<sectionHeader confidence="0.992993" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999289">
We address a text regression problem: given a
piece of text, predict a real-world continuous
quantity associated with the text’s meaning. In
this work, the text is an SEC-mandated finan-
cial report published annually by a publicly-
traded company, and the quantity to be pre-
dicted is volatility of stock returns, an empiri-
cal measure of financial risk. We apply well-
known regression techniques to a large cor-
pus of freely available financial reports, con-
structing regression models of volatility for
the period following a report. Our models ri-
val past volatility (a strong baseline) in pre-
dicting the target variable, and a single model
that uses both can significantly outperform
past volatility. Interestingly, our approach is
more accurate for reports after the passage of
the Sarbanes-Oxley Act of 2002, giving some
evidence for the success of that legislation in
making financial reports more informative.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947176470588">
We consider a text regression problem: given a piece
of text, predict a R-valued quantity associated with
that text. Specifically, we use a company’s annual
financial report to predict the financial risk of invest-
ment in that company, as measured empirically by a
quantity known as stock return volatility.
Predicting financial risk is of clear interest to
anyone who invests money in stocks and central
to modern portfolio choice. Financial reports are
a government-mandated artifact of the financial
world that—one might hypothesize—contain a large
amount of information about companies and their
value. Indeed, it is an important question whether
mandated disclosures are informative, since they are
meant to protect investors but are costly to produce.
The intrinsic properties of the problem are attrac-
tive as a test-bed for NLP research. First, there is
no controversy about the usefulness or existential
reality of the output variable (volatility). Statisti-
cal NLP often deals in the prediction of variables
ranging from text categories to linguistic structures
to novel utterances. While many of these targets are
uncontroversially useful, they often suffer from eval-
uation difficulties and disagreement among annota-
tors. The output variable in this work is a statistic
summarizing facts about the real world; it is not sub-
ject to any kind of human expertise, knowledge, or
intuition. Hence this prediction task provides a new,
objective test-bed for any kind of linguistic analysis.
Second, many NLP problems rely on costly anno-
tated resources (e.g., treebanks or aligned bilingual
corpora). Because the text and historical financial
data used in this work are freely available (by law)
and are generated as a by-product of the American
</bodyText>
<page confidence="0.948984">
272
</page>
<note confidence="0.8906595">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 272–280,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999829461538461">
economy, old and new data can be obtained by any-
one with relatively little effort.
In this paper, we demonstrate that predicting fi-
nancial volatility automatically from a financial re-
port is a novel, challenging, and easily evaluated nat-
ural language understanding task. We show that a
very simple representation of the text (essentially,
bags of unigrams and bigrams) can rival and, in
combination, improve over a strong baseline that
does not use the text. Analysis of the learned models
provides insights about what can make this problem
more or less difficult, and suggests that disclosure-
related legislation led to more transparent reporting.
</bodyText>
<sectionHeader confidence="0.955695" genericHeader="introduction">
2 Stock Return Volatility
</sectionHeader>
<bodyText confidence="0.995677076923077">
Volatility is often used in finance as a measure of
risk. It is measured as the standard deviation of
a stock’s returns over a finite period of time. A
stock will have high volatility when its price fluctu-
ates widely and low volatility when its price remains
more or less constant.
Let rt = Pt
Pt−1 − 1 be the return on a given stock
between the close of trading day t − 1 and day t,
where Pt is the (dividend-adjusted) closing stock
price at date t. The measured volatility over the time
period from day t − τ to day t is equal to the sample
s.d.:
</bodyText>
<equation confidence="0.610631">
�(rt−i − ¯r)2 τ (1)
</equation>
<bodyText confidence="0.99994137037037">
where r¯ is the sample mean of rt over the period. In
this work, the above estimate will be treated as the
true output variable on training and testing data.
It is important to note that predicting volatility is
not the same as predicting returns or value. Rather
than trying to predict how well a stock will perform,
we are trying to predict how stable its price will be
over a future time period. It is, by now, received
wisdom in the field of economics that predicting a
stock’s performance, based on easily accessible pub-
lic information, is difficult. This is an attribute of
well-functioning (or “efficient”) markets and a cor-
nerstone of the so-called “efficient market hypoth-
esis” (Fama, 1970). By contrast, the idea that one
can predict a stock’s level of risk using public in-
formation is uncontroversial and a basic assumption
made by many economically sound pricing mod-
els. A large body of research in finance suggests
that the two types of quantities are very different:
while predictability of returns could be easily traded
away by the virtue of buying/selling stocks that are
under- or over-valued (Fama, 1970), similar trades
are much more costly to implement with respect to
predictability of volatility (Dumas et al., 2007). By
focusing on volatility prediction, we avoid taking
a stance on whether or not the United States stock
market is informationally efficient.
</bodyText>
<sectionHeader confidence="0.98179" genericHeader="method">
3 Problem Formulation
</sectionHeader>
<bodyText confidence="0.998609333333333">
Given a text document d, we seek to predict the
value of a continuous variable v. We do this via a
parameterized function f:
</bodyText>
<equation confidence="0.999274">
vˆ = f(d; w) (2)
</equation>
<bodyText confidence="0.999891066666667">
where w E Rd are the parameters or weights. Our
approach is to learn a human-interpretable w from
a collection of N training examples {(di, vi)�N i=1,
where each di is a document and each vi E R.
Support vector regression (Drucker et al., 1997)
is a well-known method for training a regression
model. SVR is trained by solving the following op-
timization problem:
where C is a regularization constant and c controls
the training error.1 The training algorithm finds
weights w that define a function f minimizing the
(regularized) empirical risk.
Let h be a function from documents into some
vector-space representation C_ Rd. In SVR, the func-
tion f takes the form:
</bodyText>
<equation confidence="0.995851">
N
f(d; w) = h(d)Tw = αiK(d, di) (4)
i=1
</equation>
<bodyText confidence="0.9999685">
where Equation 4 re-parameterizes f in terms of a
kernel function K with “dual” weights αi. K can
</bodyText>
<footnote confidence="0.856528">
1Given the embedding h of documents in Rd, a defines
a “slab” (region between two parallel hyperplanes, some-
times called the “e-tube”) in Rd+1 through which each
(h(di), f(di; w)) must pass in order to have zero loss.
</footnote>
<figure confidence="0.979774857142857">
v[t−T,t] = d T
i=0
min N ( )
wE[Pd 2 11w112+N i=1 max 0, ���vi − f(di; w) ��� − �
• v •
c-insensitive loss function
(3)
</figure>
<page confidence="0.997366">
273
</page>
<table confidence="0.999801538461538">
year words documents words/doc.
1996 5.5M 1,408 3,893
1997 9.3M 2,260 4,132
1998 11.8M 2,462 4,808
1999 14.5M 2,524 5,743
2000 13.4M 2,425 5,541
2001 15.4M 2,596 5,928
2002 22.7M 2,846 7,983
2003 35.3M 3,612 9,780
2004 38.9M 3,559 10,936
2005 41.9M 3,474 12,065
2006 38.8M 3,308 11,736
total 247.7M 26,806 9,240
</table>
<tableCaption confidence="0.8934606">
Table 1: Dimensions of the dataset used in this paper,
after filtering and tokenization. The near doubling in av-
erage document size during 2002–3 is possibly due to the
passage of the Sarbanes-Oxley Act of 2002 in the wake
of Enron’s accounting scandal (and numerous others).
</tableCaption>
<bodyText confidence="0.9846168">
be seen as a similarity function between two docu-
ments. At test time, a new example is compared to a
subset of the training examples (those with αi =� 0);
typically with SVR this set is sparse. With the linear
kernel, the primal and dual weights relate linearly:
</bodyText>
<equation confidence="0.998255666666667">
N
w = αih(di) (5)
i=1
</equation>
<bodyText confidence="0.999567333333333">
The full details of SVR and its implementation are
beyond the scope of this paper; interested readers are
referred to Sch¨olkopf and Smola (2002). SVMlight
(Joachims, 1999) is a freely available implementa-
tion of SVR training that we used in our experi-
ments.2
</bodyText>
<sectionHeader confidence="0.998484" genericHeader="method">
4 Dataset
</sectionHeader>
<bodyText confidence="0.9852547">
In the United States, the Securities Exchange Com-
mission mandates that all publicly-traded corpora-
tions produce annual reports known as “Form 10-
K.” The report typically includes information about
the history and organization of the company, equity
and subsidiaries, as well as financial information.
These reports are available to the public and pub-
lished on the SEC’s web site.3 The structure of the
10-K is specified in detail in the legislation. We have
collected 54,379 reports published over the period
</bodyText>
<footnote confidence="0.999756">
2Available at http://svmlight.joachims.org.
3http://www.sec.gov/edgar.shtml
</footnote>
<bodyText confidence="0.999002543478261">
1996–2006 from 10,492 different companies. Each
report comes with a date of publication, which is im-
portant for tying the text back to the financial vari-
ables we seek to predict.
From the perspective of predicting future events,
one section of the 10-K is of special interest: Section
7, known as “management’s discussion and anal-
ysis of financial conditions and results of opera-
tions” (MD&amp;A), and in particular Subsection 7A,
“quantitative and qualitative disclosures about mar-
ket risk.” Because Section 7 is where the most im-
portant forward-looking content is most likely to
be found, we filter other sections from the reports.
The filtering is done automatically using a short,
hand-written Perl script that seeks strings loosely
matching the Section 7, 7A, and 8 headers, finds the
longest reasonable “Section 7” match (in words) of
more than 1,000 whitespace-delineated tokens.
Section 7 typically begins with an introduction
like this (from ABC’s 1998 Form 10-K, before to-
kenization for readability; boldface added):
The following discussion and analysis of
ABC’s consolidated financial condition and
consolidated results of operation should be
read in conjunction with ABC’s Consoli-
dated Financial Statements and Notes thereto
included elsewhere herein. This discus-
sion contains certain forward-looking state-
ments which involve risks and uncertain-
ties. ABC’s actual results could differ mate-
rially from the results expressed in, or implied
by, such statements. See “Regarding Forward-
Looking Statements.”
Not all of the documents downloaded pass the fil-
ter at all, and for the present work we have only used
documents that do pass the filter. (One reason for the
failure of the filter is that many 10-K reports include
Section 7 “by reference,” so the text is not directly
included in the document.)
In addition to the reports, we used the Center
for Research in Security Prices (CRSP) US Stocks
Database to obtain the price return series along with
other firm characteristics.4 We proceeded to calcu-
late two volatilities for each firm/report observation:
the twelve months prior to the report (v(−12)) and
the twelve months after the report (v(+12)).
</bodyText>
<footnote confidence="0.9995895">
4The text and volatility data are publicly available at http:
//www.ark.cs.cmu.edu/10K.
</footnote>
<page confidence="0.996901">
274
</page>
<bodyText confidence="0.999971916666667">
Tokenization was applied to the text, including
punctuation removal, downcasing, collapsing all
digit sequences,5 and heuristic removal of remnant
markup. Table 1 gives statistics on the corpora
used in this research; this is a subset of the cor-
pus for which there is no missing volatility informa-
tion. The drastic increase in length during the 2002–
2003 period might be explained by the passage by
the US Congress of the Sarbanes-Oxley Act of 2002
(and related SEC and exchange rules), which im-
posed revised standards on reporting practices of
publicly-traded companies in the US.
</bodyText>
<sectionHeader confidence="0.93517" genericHeader="method">
5 Baselines and Evaluation Method
</sectionHeader>
<bodyText confidence="0.999956052631579">
Volatility displays an effect known as autoregressive
conditional heteroscedasticity (Engle, 1982). This
means that the variance in a stock’s return tends to
change gradually. Large changes in price are pre-
saged by other changes, and periods of stability tend
to continue. Volatility is, generally speaking, not
constant, yet prior volatility (e.g., v(−12)) is a very
good predictor of future volatility (e.g., v(+12)). At
the granularity of a year, which we consider here
because the 10-K reports are annual, there are no
existing models of volatility that are widely agreed
to be significantly more accurate than our histor-
ical volatility baseline. We tested a state-of-the-
art model known as GARCH(1,1) (Engle, 1982;
Bollerslev, 1986) and found that it was no stronger
than our historical volatility baseline on this sample.
Throughout this paper, we will report perfor-
mance using the mean squared error between the
predicted and true log-volatilities:6
</bodyText>
<equation confidence="0.928322">
(log(vi) − log(ˆvi))2 (6)
</equation>
<bodyText confidence="0.963435">
where N&apos; is the size of the test set, given in Table 1.
</bodyText>
<sectionHeader confidence="0.999419" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.962015117647059">
In our experiments, we vary h (the function that
maps inputs to a vector space) and the subset of the
5While numerical information is surely informative about
risk, recall that our goal is to find indicators of risk expressed in
the text; automatic predictors of risk from numerical data would
use financial data streams directly, not text reports.
6We work in the log domain because it is standard in finance,
due to the dynamic range of actual volatilities; the distribution
over log v across companies tends to have a bell shape.
data used for training. We will always report perfor-
mance over test sets consisting of one year’s worth
of data (the subcorpora described in Table 1). In
this work, we focus on predicting the volatility over
the year following the report (v(+12)). In all experi-
ments, c = 0.1 and C is set using the default choice
of SVMlight, which is the inverse of the average of
h(d)Th(d) over the training data.7
</bodyText>
<subsectionHeader confidence="0.998518">
6.1 Feature Representation
</subsectionHeader>
<bodyText confidence="0.998118666666667">
We first consider how to represent the 10-K reports.
We adopt various document representations, all us-
ing word features. Let M be the vocabulary size
derived from the training data.8 Let freq(xj; d) de-
note the number of occurrences of the jth word in
the vocabulary in document d.
</bodyText>
<listItem confidence="0.957721416666667">
• TF: hj(d) = 1
|d|freq(xj; d), bj E {1, ..., M}.
• TFIDF: hj(d) = 1
|d|freq(xj; d) x log(N/|{d :
freq(xj; d) &gt; 0}|), where N is the number of
documents in the training set. This is the classic
“TFIDF” score.
• LOG1P: hj(d) = log(1 + freq(xj; d)). Rather
than normalizing word frequencies as for TF,
this score dampens them with a logarithm. We
also include a variant of LOG1P where terms
are the union of unigrams and bigrams.
</listItem>
<bodyText confidence="0.988115375">
Note that each of these preserves sparsity; when
freq(xj; d) = 0, hj(d) = 0 in all cases.
For interpretability of results, we use a linear ker-
nel. The usual bias weight b is included. We found
it convenient to work in the logarithmic domain for
the predicted variable, predicting log v instead of v,
since volatility is always nonnegative. In this setting,
the predicted volatility takes the form:
</bodyText>
<equation confidence="0.996827">
M
log vˆ= b + wjhj(d) (7)
j=1
</equation>
<bodyText confidence="0.9782955">
Because the goal of this work is to explore how text
might be used to predict volatility, we also wish
</bodyText>
<footnote confidence="0.992580333333333">
7These values were selected after preliminary and cursory
exploration with 1996–2000 as training data and 2001 as the
test set. While the effects of e and C were not large, further
improvements may be possible with more careful tuning.
8Preliminary experiments that filtered common or rare
words showed a negligible or deleterious effect on performance.
</footnote>
<equation confidence="0.744392">
1
MSE = N&apos;
N&apos;
�
i=1
</equation>
<page confidence="0.823918">
275
</page>
<table confidence="0.997004083333333">
features 2001 2002 2003 2004 2005 2006 micro-ave.
v(�12) (baseline) 0.1747 0.1600 0.1873 0.1442 0.1365 0.1463 0.1576
v(�12) (SVR with bias) 0.2433 0.4323 0.1869 0.2717 0.3184 5.6778 1.2061
v(�12) (SVR without bias) 0.2053 0.1653 0.2051 0.1337 0.1405 0.1517 0.1655
TF 0.2219 0.2571 0.2588 0.2134 0.1850 0.1862 0.2197
TFIDF 0.2033 0.2118 0.2178 0.1660 0.1544 0.1599 0.1842
LOG1P 0.2107 0.2214 0.2040 0.1693 0.1581 0.1715 0.1873
LOG1P, bigrams 0.1968 0.2015 *0.1729 0.1500 0.1394 0.1532 0.1667
TF+ 0.1885 0.1616 0.1925 *0.1230 *0.1272 *0.1402 *0.1541
TFIDF+ 0.1919 0.1618 0.1965 *0.1246 *0.1276 *0.1403 *0.1557
LOG1P+ 0.1846 0.1764 *0.1671 *0.1309 *0.1319 0.1458 *0.1542
LOG1P+, bigrams 0.1852 0.1792 *0.1599 *0.1352 *0.1307 0.1448 *0.1538
</table>
<tableCaption confidence="0.986638">
Table 2: MSE (Eq. 6) of different models on test data predictions. Lower values are better. Boldface denotes
improvements over the baseline, and * denotes significance compared to the baseline under a permutation test (p &lt;
</tableCaption>
<figure confidence="0.954741">
0.05).
history
words
both
</figure>
<bodyText confidence="0.8936089">
to see whether text adds information beyond what
can be predicted using historical volatility alone (the
baseline, v(−12)). We therefore consider models
augmented with an additional feature, defined as
hM+1 = log v(−12). Since this is historical informa-
tion, it is always available when the 10-K report is
published. These models are denoted TF+, TFIDF+,
and LOG1P+.
The performance of these models, compared to
the baseline from Section 5, is shown in Table 2.
We used as training examples all reports from the
five-year period preceding the test year (so six ex-
periments on six different training and test sets are
shown in the figure). We also trained SVR models
on the single feature v(−12), with and without bias
weights (b in Eq. 7); these are usually worse and
never signficantly better than the baseline.
Strikingly, the models that use only the text to
predict volatility come very close to the historical
baseline in some years. That a text-only method
(LOG1P with bigrams) for predicting future risk
comes within 5% of the error of a strong baseline
(2003–6) shows promise for the overall approach.
A combined model improves substantially over the
baseline in four out of six years (2003–6), and this
difference is usually robust to the representation
used. Table 3 shows the most strongly weighted
terms in each of the text-only LOG1P models (in-
cluding bigrams). These weights are recovered us-
ing the relationship expressed in Eq. 5.
</bodyText>
<subsectionHeader confidence="0.99975">
6.2 Training Data Effects
</subsectionHeader>
<bodyText confidence="0.999940592592593">
It is well known that more training data tend to im-
prove the performance of a statistical method; how-
ever, the standard assumption is that the training
data are drawn from the same distribution as the test
data. In this work, where we seek to predict the
future based on data from past, that assumption is
obviously violated. It is therefore an open question
whether more data (i.e., looking farther into the past)
is helpful for predicting volatility, or whether it is
better to use only the most recent data.
Table 4 shows how performance varies when one,
two, or five years of historical training data are used,
averaged across test years. In most cases, using
more training data (from a longer historical period)
is helpful, but not always. One interesting trend,
not shown in the aggregate statistics of Table 4,
is that recency of the training set affected perfor-
mance much more strongly in earlier train/test splits
(2001–3) than later ones (2004–6). This experiment
leads us to conclude that temporal changes in fi-
nancial reporting make training data selection non-
trivial. Changes in the macro economy and spe-
cific businesses make older reports less relevant for
prediction. For example, regulatory changes like
Sarbanes-Oxley, variations in the business cycle,
and technological innovation like the Internet influ-
ence both the volatility and the 10-K text.
</bodyText>
<subsectionHeader confidence="0.999938">
6.3 Effects of Sarbanes-Oxley
</subsectionHeader>
<bodyText confidence="0.999864">
We noted earlier that the passage of the Sarbanes-
Oxley Act of 2002, which sought to reform financial
reporting, had a clear effect on the lengths of the
10-K reports in our collection. But are the reports
more informative? This question is important, be-
cause producing reports is costly; we present an em-
pirical argument based on our models that the legis-
</bodyText>
<page confidence="0.982652">
276
277
</page>
<table confidence="0.9988336">
features 1 2 5
TF+ 0.1509 0.1450 0.1541
TFIDF+ 0.1512 0.1455 0.1557
LOG1P+ 0.1621 0.1611 0.1542
LOG1P+, bigrams 0.1617 0.1588 0.1538
</table>
<tableCaption confidence="0.928062">
Table 4: MSE of volatility predictions using reports from
</tableCaption>
<bodyText confidence="0.989992772727273">
varying historical windows (1, 2, and 5 years), micro-
averaged across six train/test scenarios. Boldface marks
best in a row. The historical baseline achieves 0.1576
MSE (see Table 2).
lation has actually been beneficial.
Our experimental results in Section 6.1, in which
volatility in the years 2004–2006 was more accu-
rately predicted from the text than in 2001–2002,
suggest that the Sarbanes-Oxley Act led to more in-
formative reports. We compared the learned weights
(LOG1P+, unigrams) between the six overlapping
five-year windows ending in 2000–2005; measured
in Li distance, these were, in consecutive order,
(52.2, 59.9, 60.7, 55.3, 52.3); the biggest differ-
ences came between 2001 and 2002 and between
2002 and 2003. (Firms are most likely to have be-
gun compliance with the new law in 2003 or 2004.)
The same pattern held when only words appearing
in all five models were considered. Variation in the
recency/training set size tradeoff (§6.2), particularly
during 2002–3, also suggests that there were sub-
stantial changes in the reports during that time.
</bodyText>
<subsectionHeader confidence="0.975144">
6.4 Qualitative Evaluation
</subsectionHeader>
<bodyText confidence="0.998422222222222">
One of the advantages of a linear model is that we
can explore what each model discovers about dif-
ferent unigram and bigram terms. Some manually
selected examples of terms whose learned weights
(w) show interesting variation patterns over time are
shown in Figure 1, alongside term frequency pat-
terns, for the text-only LOG1P model (with bigrams).
These examples were suggested by experts in fi-
nance from terms with weights that were both large
and variable (across training sets).
A particularly interesting case, in light of
Sarbanes-Oxley, is the term accounting policies.
Sarbanes-Oxley mandated greater discussion of ac-
counting policy in the 10-K MD&amp;A section. Be-
fore 2002 this term indicates high volatility, per-
haps due to complicated off-balance sheet transac-
tions or unusual accounting policies. Starting in
2002, explicit mention of accounting policies indi-
</bodyText>
<figure confidence="0.992323129629629">
low vˆ
↑
↓
Table 3: Most strongly-weighted terms in models learned from various time periods (LOG1P model with unigrams and bigrams). “#” denotes any digit sequence.
high vˆ
2001–2005
2000–2004
1999–2003
1998–2002
1997–2001
1996–2000
loss 0.026
administrative 0.012
financing 0.013
a going 0.014
expenses 0.014
going concern 0.014
net loss 0.018
personnel 0.013
policies -0.011
dividends -0.012
earnings -0.011
by the -0.011
net income -0.018
rate -0.014
properties -0.013
unsecured -0.012
loss 0.025
administrative 0.013
a going 0.013
going concern 0.014
expenses 0.015
year # 0.016
personnel 0.013
net loss 0.017
distributions -0.011
dividends -0.013
critical accounting -0.012
insurance -0.011
net income -0.021
rate -0.017
properties -0.014
lower interest -0.012
loss 0.026
additional 0.013
a going 0.014
financing 0.014
going concern 0.015
expenses 0.017
net loss 0.020
year # 0.015
distributions -0.012
dividends -0.012
dividend -0.012
annual -0.012
rates -0.013
net income -0.023
rate -0.019
properties -0.015
loss 0.023
additional 0.014
convertible 0.014
financing 0.014
obligations 0.015
expenses 0.017
net loss 0.020
year # 0.015
unsecured -0.012
dividends -0.015
distributions -0.012
earnings -0.012
income -0.016
rate -0.022
net income -0.019
properties -0.016
year # 0.028
additional 0.014
covenants 0.015
of $# 0.015
experienced 0.015
expenses 0.020
net loss 0.023
loss 0.020
merger agreement -0.015
dividend -0.017
dividends -0.015
unsecured -0.017
rate -0.025
income -0.021
net income -0.019
properties -0.018
net loss 0.026
date 0.014
convertible 0.014
diluted 0.014
covenants 0.017
expenses 0.019
loss 0.020
year # 0.024
longterm -0.014
dividend -0.015
rates -0.015
rate -0.022
income -0.021
properties -0.018
merger agreement -0.017
unsecured -0.015
0.010
0.005
0
-0.005
-0.010
accounting policies
estimates
w 0.005
0
-0.005
-0.010
-0.015
0.005
0
-0.005
-0.010
w
higher margin
lower margin
w
mortgages
reit
ave. term frequency
ave. term frequency
ave. term frequency
8 Figure 1: Left:
learned weights for
selected terms across
4 models trained on
data from different
time periods (x-axis).
0 These weights are
from the LOG1P
0.8 (unigrams and
bigrams) models
0.6
trained on five-year
0.4 periods, the same
models whose
extreme weights are
0 summarized in
Tab. 3. Note that all
weights are within
0.15 0 ± 0.026. Right: the
terms’ average
0.10
frequencies (by
0.05 document) over the
same periods.
2
6
0.2
0.20
0
96-00 97-01 98-02 99-03 00-04 01-05
</figure>
<bodyText confidence="0.997084133333333">
cates lower volatility. The frequency of the term
also increases drastically over the same period, sug-
gesting that the earlier weights may have been in-
flated. A more striking example is estimates, which
averages one occurrence per document even in the
1996–2000 period, experiences the same term fre-
quency explosion, and goes through a similar weight
change, from strongly indicating high volatility to
strongly indicating low volatility.
As a second example, consider the terms mort-
gages and reit (Real Estate Investment Trust, a tax
designation for businesses that invest in real estate).
Given the importance of the housing and mortgage
market over the past few years, it is interesting to
note that the weight on both of these terms increases
over the period from a strong low volatility term to a
weak indicator of high volatility. It will be interest-
ing to see how the dramatic decline in housing prices
in late 2007, and the fallout created in credit markets
in 2008, is reflected in future models.
Finally, notice that high margin and low mar-
gin, whose frequency patterns are fairly flat “switch
places,” over the sample: first indicating high and
low volatility, respectively, then low and high. There
is no a priori reason to expect high or low margins
would be associated with high or low stock volatil-
ity. However, this is an interesting example where
bigrams are helpful (the word margin by itself is
uninformative) and indicates that predicting risk is
highly time-dependent.
</bodyText>
<subsectionHeader confidence="0.937202">
6.5 Delisting
</subsectionHeader>
<bodyText confidence="0.999962388888889">
An interesting but relatively infrequent phenomenon
is the delisting of a company, i.e., when it ceases to
be traded on a particular exchange due to dissolution
after bankruptcy, a merger, or violation of exchange
rules. The relationship between volatility and delist-
ing has been studied by Merton (1974), among oth-
ers. Our dataset includes a small number of cases
where the volatility figures for the period following
the publication of a 10-K report are unavailable be-
cause the company was delisted. Learning to predict
delisting is extremely difficult because fewer than
4% of the 2001–6 10-K reports precede delisting.
Using the LOG1P representation, we built a lin-
ear SVM classifier for each year in 2001–6 (trained
on the five preceding years’ data) to predict whether
a company will be delisted following its 10-K re-
port. Performance for various precision measures is
shown in Table 5. Notably, for 2001–4 we achieve
</bodyText>
<page confidence="0.979784">
278
</page>
<figure confidence="0.798075166666667">
6
5
4
3
2
1
</figure>
<table confidence="0.820859083333333">
precision (%) at ... ’01 ’02 ’03 ’04 ’05 ’06
recall = 10% 80 93 79 100 47 21
n = 5 100 100 40 100 60 80
n = 10 80 90 70 90 60 70
n = 100 38 48 53 29 24 20
oracle Fl (%) 35 42 44 36 31 16
bulletin, creditors, dip, otc
court
chapter, debtors, filing, prepetition
bankruptcy
concern, confirmation, going, liquidation
debtorinpossession, delisted, nasdaq, petition
</table>
<tableCaption confidence="0.838775">
Table 5: Left: precision of delisting predictions. The “oracle F1” row shows the maximal Fl score obtained for any
n. Right: Words most strongly predicting delisting of a company. The number is how many of the six years (2001–6)
the word is among the ten most strongly weighted. There were no clear patterns across years for words predicting that
a company would not be delisted. The word otc refers to “over-the-counter” trading, a high-risk market.
above 75% precision at 10% recall. Our best (or-
acle) F1 scores occur in 2002 and 2003, suggesting
again a difference in reports around Sarbanes-Oxley.
Table 5 shows words associated with delisting.
</tableCaption>
<sectionHeader confidence="0.999848" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.998733608695653">
In NLP, regression is not widely used, since most
natural language-related data are discrete. Regres-
sion methods were pioneered by Yang and Chute
(1992) and Yang and Chute (1993) for information
retrieval purposes, but the predicted continuous vari-
able was not an end in itself in that work. Blei
and McAuliffe (2007) used latent “topic” variables
to predict movie reviews and popularity from text.
Lavrenko et al. (2000b) and Lavrenko et al. (2000a)
modeled influences between text and time series fi-
nancial data (stock prices) using language models.
Farther afield, Albrecht and Hwa (2007) used SVR
to train machine translation evaluation metrics to
match human evaluation scores and compared tech-
niques using correlation. Regression has also been
used to order sentences in extractive summarization
(Biadsy et al., 2008).
While much of the information relevant for in-
vestors is communicated through text (rather than
numbers), only recently is this link explored. Some
papers relate news articles to earning forecasts, stock
returns, volatility, and volume (Koppel and Shtrim-
berg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa,
2007; Engelberg, 2007). Das and Chen (2001) and
Antweiler and Frank (2004) ask whether messages
posted on message boards can help explain stock
performance, while Li (2005) measures the associ-
ation between frequency of words associated with
risk and subsequent stock returns. Weiss-Hanley and
Hoberg (2008) study initial public offering disclo-
sures using word statistics. Many researchers have
focused the related problem of predicting sentiment
and opinion in text (Pang et al., 2002; Wiebe and
Riloff, 2005), sometimes connected to extrinsic val-
ues like prediction markets (Lerman et al., 2008).
In contrast to text regression, text classification
comprises a widely studied set of problems involv-
ing the prediction of categorial variables related to
text. Applications have included the categorization
of documents by topic (Joachims, 1998), language
(Cavnar and Trenkle, 1994), genre (Karlgren and
Cutting, 1994), author (Bosch and Smith, 1998),
sentiment (Pang et al., 2002), and desirability (Sa-
hami et al., 1998). Text categorization has served as
a test application for nearly every machine learning
technique for discrete classification.
</bodyText>
<sectionHeader confidence="0.997292" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999989416666667">
We have introduced and motivated a new kind of
task for NLP: text regression, in which text is used
to make predictions about measurable phenomena
in the real world. We applied the technique to pre-
dicting financial volatility from companies’ 10-K re-
ports, and found text regression model predictions
to correlate with true volatility nearly as well as his-
torical volatility, and a combined model to perform
even better. Further, improvements in accuracy and
changes in models after the passage of the Sarbanes-
Oxley Act suggest that financial reporting reform
has had interesting and measurable effects.
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99928575">
The authors are grateful to Jamie Callan, Chester Spatt,
Anthony Tomasic, Yiming Yang, and Stanley Zin for
helpful discussions, and to the anonymous reviewers for
useful feedback. This research was supported by grants
from the Institute for Quantitative Research in Finanace
and from the Center for Analytical Research in Technol-
ogy at the Tepper School of Business, Carnegie Mellon
University.
</bodyText>
<page confidence="0.997047">
279
</page>
<sectionHeader confidence="0.995785" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999913656565656">
J. S. Albrecht and R. Hwa. 2007. Regression for
sentence-level MT evaluation with pseudo references.
In Proc. of ACL.
W. Antweiler and M. Z. Frank. 2004. Is all that talk
just noise? the information content of internet stock
message boards. Journal of Finance, 59:1259–1294.
F. Biadsy, J. Hirschberg, and E. Filatova. 2008. An
unsupervised approach to biography production using
Wikipedia. In Proc. of ACL.
D. M. Blei and J. D. McAuliffe. 2007. Supervised topic
models. In Advances in NIPS 21.
T. Bollerslev. 1986. Generalized autoregressive con-
ditional heteroskedasticity. Journal of Econometrics,
31:307–327.
R. Bosch and J. Smith. 1998. Separating hyperplanes
and the authorship of the disputed Federalist papers.
American Mathematical Monthly, 105(7):601–608.
W. B. Cavnar and J. M. Trenkle. 1994. n-gram-based
text categorization. In Proc. of SDAIR.
S. Das and M. Chen. 2001. Yahoo for Amazon: Ex-
tracting market sentiment from stock mesage boards.
In Proc. of Asia Pacific Finance Association Annual
Conference.
H. Drucker, C. J. C. Burges, L. Kaufman, A. Smola, and
V. Vapnik. 1997. Support vector regression machines.
In Advances in NIPS 9.
B. Dumas, A. Kurshev, and R. Uppal. 2007. Equilibrium
portfolio strategies in the presence of sentiment risk
and excess volatility. Swiss Finance Institute Research
Paper No. 07-37.
J. Engelberg. 2007. Costly information processing: Ev-
idence from earnings announcements. Working paper,
Northwestern University.
R. F. Engle. 1982. Autoregressive conditional het-
eroscedasticity with estimates of variance of united
kingdom inflation. Econometrica, 50:987–1008.
E. F. Fama. 1970. Efficient capital markets: A review
of theory and empirical work. Journal of Finance,
25(2):383–417.
C. Gaa. 2007. Media coverage, investor inattention, and
the market’s reaction to news. Working paper, Univer-
sity of British Columbia.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In Proc. of ECML.
T. Joachims. 1999. Making large-scale SVM learning
practical. In Advances in Kernel Methods - Support
Vector Learning. MIT Press.
J. Karlgren and D. Cutting. 1994. Recognizing text gen-
res with simple metrics using discriminant analysis. In
Proc. of COLING.
M. Koppel and I. Shtrimberg. 2004. Good news or bad
news? let the market decide. In AAAI Spring Sympo-
sium on Exploring Attitude and Affect in Text: Theo-
ries and Applications.
V. Lavrenko, M. Schmill, D. Lawrie, P. Ogilvie,
D. Jensen, and J. Allan. 2000a. Language models for
financial news recommendation. In Proc. of CIKM.
V. Lavrenko, M. Schmill, D. Lawrie, P. Ogilvie,
D. Jensen, and J. Allan. 2000b. Mining of concurrent
text and time series. In Proc. of KDD.
K. Lerman, A. Gilder, M. Dredze, and F. Pereira. 2008.
Reading the markets: Forecasting public opinion of
political candidates by news analysis. In COLING.
F. Li. 2005. Do stock market investors understand the
risk sentiment of corporate annual reports? Working
Paper, University of Michigan.
R. Merton. 1974. On the pricing of corporate debt: The
risk structure of interest rates. Journal of Finance,
29:449–470.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proc. of EMNLP.
M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz.
1998. A Bayesian approach to filtering junk email. In
Proc. of AAAI Workshop on Learning for Text Catego-
rization.
B. Sch¨olkopf and A. J. Smola. 2002. Learning with Ker-
nels: Support Vector Machines, Regularization, Opti-
mization, and Beyond. MIT Press.
P. C. Tetlock, M. Saar-Tsechansky, and S. Macskassy.
2008. More than words: Quantifying language to
measure firms’ fundamentals. Journal of Finance,
63(3):1437–1467.
P. C. Tetlock. 2007. Giving content to investor senti-
ment: The role of media in the stock market. Journal
of Finance, 62(3):1139–1168.
K. Weiss-Hanley and G. Hoberg. 2008. Strategic disclo-
sure and the pricing of initial public offerings. Work-
ing paper.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In CICLing.
Y. Yang and C. G. Chute. 1992. A linear least squares fit
mapping method for information retrieval from natural
language texts. In Proc. of COLING.
Y. Yang and C. G. Chute. 1993. An application of least
squares fit mapping to text information retrieval. In
Proc. of SIGIR.
</reference>
<page confidence="0.997137">
280
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.196257">
<title confidence="0.781195">Predicting Risk from Financial Reports with Regression Shimon</title>
<author confidence="0.755261">McCombs School of</author>
<affiliation confidence="0.996342">University of Texas at</affiliation>
<address confidence="0.988549">Austin, TX 78712, USA</address>
<email confidence="0.999619">shimon.kogan@mccombs.utexas.edu</email>
<author confidence="0.999823">Bryan R Routledge</author>
<affiliation confidence="0.9918915">Tepper School of Business Carnegie Mellon University</affiliation>
<address confidence="0.998065">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999831">routledge@cmu.edu</email>
<author confidence="0.994086">Dimitry Levin</author>
<affiliation confidence="0.9649415">Mellon College of Carnegie Mellon</affiliation>
<address confidence="0.989058">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999742">dimitrylevin@gmail.com</email>
<author confidence="0.7462195">Jacob S Owen Graduate School of</author>
<affiliation confidence="0.767095">Vanderbilt</affiliation>
<address confidence="0.965613">Nashville, TN 37203, USA</address>
<email confidence="0.984765">Jacob.Sagi@Owen.Vanderbilt.edu</email>
<author confidence="0.999894">Noah A Smith</author>
<affiliation confidence="0.9999575">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.997887">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999656">nasmith@cs.cmu.edu</email>
<abstract confidence="0.999452">We address a text regression problem: given a piece of text, predict a real-world continuous quantity associated with the text’s meaning. In this work, the text is an SEC-mandated financial report published annually by a publiclytraded company, and the quantity to be preis stock returns, an empirical measure of financial risk. We apply wellknown regression techniques to a large corpus of freely available financial reports, constructing regression models of volatility for the period following a report. Our models rival past volatility (a strong baseline) in predicting the target variable, and a single model that uses both can significantly outperform past volatility. Interestingly, our approach is more accurate for reports after the passage of the Sarbanes-Oxley Act of 2002, giving some evidence for the success of that legislation in making financial reports more informative.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J S Albrecht</author>
<author>R Hwa</author>
</authors>
<title>Regression for sentence-level MT evaluation with pseudo references.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="28578" citStr="Albrecht and Hwa (2007)" startWordPosition="4645" endWordPosition="4648"> with delisting. 7 Related Work In NLP, regression is not widely used, since most natural language-related data are discrete. Regression methods were pioneered by Yang and Chute (1992) and Yang and Chute (1993) for information retrieval purposes, but the predicted continuous variable was not an end in itself in that work. Blei and McAuliffe (2007) used latent “topic” variables to predict movie reviews and popularity from text. Lavrenko et al. (2000b) and Lavrenko et al. (2000a) modeled influences between text and time series financial data (stock prices) using language models. Farther afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation. Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008). While much of the information relevant for investors is communicated through text (rather than numbers), only recently is this link explored. Some papers relate news articles to earning forecasts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler a</context>
</contexts>
<marker>Albrecht, Hwa, 2007</marker>
<rawString>J. S. Albrecht and R. Hwa. 2007. Regression for sentence-level MT evaluation with pseudo references. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Antweiler</author>
<author>M Z Frank</author>
</authors>
<title>Is all that talk just noise? the information content of internet stock message boards.</title>
<date>2004</date>
<journal>Journal of Finance,</journal>
<pages>59--1259</pages>
<contexts>
<context position="29193" citStr="Antweiler and Frank (2004)" startWordPosition="4738" endWordPosition="4741"> Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation. Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008). While much of the information relevant for investors is communicated through text (rather than numbers), only recently is this link explored. Some papers relate news articles to earning forecasts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In contrast to text regression, text classification comprises a widely studi</context>
</contexts>
<marker>Antweiler, Frank, 2004</marker>
<rawString>W. Antweiler and M. Z. Frank. 2004. Is all that talk just noise? the information content of internet stock message boards. Journal of Finance, 59:1259–1294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Biadsy</author>
<author>J Hirschberg</author>
<author>E Filatova</author>
</authors>
<title>An unsupervised approach to biography production using Wikipedia.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="28810" citStr="Biadsy et al., 2008" startWordPosition="4679" endWordPosition="4682">l purposes, but the predicted continuous variable was not an end in itself in that work. Blei and McAuliffe (2007) used latent “topic” variables to predict movie reviews and popularity from text. Lavrenko et al. (2000b) and Lavrenko et al. (2000a) modeled influences between text and time series financial data (stock prices) using language models. Farther afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation. Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008). While much of the information relevant for investors is communicated through text (rather than numbers), only recently is this link explored. Some papers relate news articles to earning forecasts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley an</context>
</contexts>
<marker>Biadsy, Hirschberg, Filatova, 2008</marker>
<rawString>F. Biadsy, J. Hirschberg, and E. Filatova. 2008. An unsupervised approach to biography production using Wikipedia. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>J D McAuliffe</author>
</authors>
<title>Supervised topic models.</title>
<date>2007</date>
<booktitle>In Advances in NIPS 21.</booktitle>
<contexts>
<context position="28304" citStr="Blei and McAuliffe (2007)" startWordPosition="4603" endWordPosition="4606"> would not be delisted. The word otc refers to “over-the-counter” trading, a high-risk market. above 75% precision at 10% recall. Our best (oracle) F1 scores occur in 2002 and 2003, suggesting again a difference in reports around Sarbanes-Oxley. Table 5 shows words associated with delisting. 7 Related Work In NLP, regression is not widely used, since most natural language-related data are discrete. Regression methods were pioneered by Yang and Chute (1992) and Yang and Chute (1993) for information retrieval purposes, but the predicted continuous variable was not an end in itself in that work. Blei and McAuliffe (2007) used latent “topic” variables to predict movie reviews and popularity from text. Lavrenko et al. (2000b) and Lavrenko et al. (2000a) modeled influences between text and time series financial data (stock prices) using language models. Farther afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation. Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008). While much of the information relevant for investors is communicated through text (rather th</context>
</contexts>
<marker>Blei, McAuliffe, 2007</marker>
<rawString>D. M. Blei and J. D. McAuliffe. 2007. Supervised topic models. In Advances in NIPS 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Bollerslev</author>
</authors>
<title>Generalized autoregressive conditional heteroskedasticity.</title>
<date>1986</date>
<journal>Journal of Econometrics,</journal>
<pages>31--307</pages>
<contexts>
<context position="12792" citStr="Bollerslev, 1986" startWordPosition="2065" endWordPosition="2066"> variance in a stock’s return tends to change gradually. Large changes in price are presaged by other changes, and periods of stability tend to continue. Volatility is, generally speaking, not constant, yet prior volatility (e.g., v(−12)) is a very good predictor of future volatility (e.g., v(+12)). At the granularity of a year, which we consider here because the 10-K reports are annual, there are no existing models of volatility that are widely agreed to be significantly more accurate than our historical volatility baseline. We tested a state-of-theart model known as GARCH(1,1) (Engle, 1982; Bollerslev, 1986) and found that it was no stronger than our historical volatility baseline on this sample. Throughout this paper, we will report performance using the mean squared error between the predicted and true log-volatilities:6 (log(vi) − log(ˆvi))2 (6) where N&apos; is the size of the test set, given in Table 1. 6 Experiments In our experiments, we vary h (the function that maps inputs to a vector space) and the subset of the 5While numerical information is surely informative about risk, recall that our goal is to find indicators of risk expressed in the text; automatic predictors of risk from numerical d</context>
</contexts>
<marker>Bollerslev, 1986</marker>
<rawString>T. Bollerslev. 1986. Generalized autoregressive conditional heteroskedasticity. Journal of Econometrics, 31:307–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bosch</author>
<author>J Smith</author>
</authors>
<title>Separating hyperplanes and the authorship of the disputed Federalist papers.</title>
<date>1998</date>
<journal>American Mathematical Monthly,</journal>
<volume>105</volume>
<issue>7</issue>
<contexts>
<context position="30067" citStr="Bosch and Smith, 1998" startWordPosition="4866" endWordPosition="4869">c offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In contrast to text regression, text classification comprises a widely studied set of problems involving the prediction of categorial variables related to text. Applications have included the categorization of documents by topic (Joachims, 1998), language (Cavnar and Trenkle, 1994), genre (Karlgren and Cutting, 1994), author (Bosch and Smith, 1998), sentiment (Pang et al., 2002), and desirability (Sahami et al., 1998). Text categorization has served as a test application for nearly every machine learning technique for discrete classification. 8 Conclusion We have introduced and motivated a new kind of task for NLP: text regression, in which text is used to make predictions about measurable phenomena in the real world. We applied the technique to predicting financial volatility from companies’ 10-K reports, and found text regression model predictions to correlate with true volatility nearly as well as historical volatility, and a combine</context>
</contexts>
<marker>Bosch, Smith, 1998</marker>
<rawString>R. Bosch and J. Smith. 1998. Separating hyperplanes and the authorship of the disputed Federalist papers. American Mathematical Monthly, 105(7):601–608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Cavnar</author>
<author>J M Trenkle</author>
</authors>
<title>n-gram-based text categorization.</title>
<date>1994</date>
<booktitle>In Proc. of SDAIR.</booktitle>
<contexts>
<context position="29999" citStr="Cavnar and Trenkle, 1994" startWordPosition="4856" endWordPosition="4859">quent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In contrast to text regression, text classification comprises a widely studied set of problems involving the prediction of categorial variables related to text. Applications have included the categorization of documents by topic (Joachims, 1998), language (Cavnar and Trenkle, 1994), genre (Karlgren and Cutting, 1994), author (Bosch and Smith, 1998), sentiment (Pang et al., 2002), and desirability (Sahami et al., 1998). Text categorization has served as a test application for nearly every machine learning technique for discrete classification. 8 Conclusion We have introduced and motivated a new kind of task for NLP: text regression, in which text is used to make predictions about measurable phenomena in the real world. We applied the technique to predicting financial volatility from companies’ 10-K reports, and found text regression model predictions to correlate with tr</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>W. B. Cavnar and J. M. Trenkle. 1994. n-gram-based text categorization. In Proc. of SDAIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Das</author>
<author>M Chen</author>
</authors>
<title>Yahoo for Amazon: Extracting market sentiment from stock mesage boards.</title>
<date>2001</date>
<booktitle>In Proc. of Asia Pacific Finance Association Annual Conference.</booktitle>
<contexts>
<context position="29162" citStr="Das and Chen (2001)" startWordPosition="4733" endWordPosition="4736">her afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation. Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008). While much of the information relevant for investors is communicated through text (rather than numbers), only recently is this link explored. Some papers relate news articles to earning forecasts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In contrast to text regression, text classifi</context>
</contexts>
<marker>Das, Chen, 2001</marker>
<rawString>S. Das and M. Chen. 2001. Yahoo for Amazon: Extracting market sentiment from stock mesage boards. In Proc. of Asia Pacific Finance Association Annual Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Drucker</author>
<author>C J C Burges</author>
<author>L Kaufman</author>
<author>A Smola</author>
<author>V Vapnik</author>
</authors>
<title>Support vector regression machines.</title>
<date>1997</date>
<booktitle>In Advances in NIPS 9.</booktitle>
<contexts>
<context position="6560" citStr="Drucker et al., 1997" startWordPosition="1053" endWordPosition="1056">ent with respect to predictability of volatility (Dumas et al., 2007). By focusing on volatility prediction, we avoid taking a stance on whether or not the United States stock market is informationally efficient. 3 Problem Formulation Given a text document d, we seek to predict the value of a continuous variable v. We do this via a parameterized function f: vˆ = f(d; w) (2) where w E Rd are the parameters or weights. Our approach is to learn a human-interpretable w from a collection of N training examples {(di, vi)�N i=1, where each di is a document and each vi E R. Support vector regression (Drucker et al., 1997) is a well-known method for training a regression model. SVR is trained by solving the following optimization problem: where C is a regularization constant and c controls the training error.1 The training algorithm finds weights w that define a function f minimizing the (regularized) empirical risk. Let h be a function from documents into some vector-space representation C_ Rd. In SVR, the function f takes the form: N f(d; w) = h(d)Tw = αiK(d, di) (4) i=1 where Equation 4 re-parameterizes f in terms of a kernel function K with “dual” weights αi. K can 1Given the embedding h of documents in Rd,</context>
</contexts>
<marker>Drucker, Burges, Kaufman, Smola, Vapnik, 1997</marker>
<rawString>H. Drucker, C. J. C. Burges, L. Kaufman, A. Smola, and V. Vapnik. 1997. Support vector regression machines. In Advances in NIPS 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dumas</author>
<author>A Kurshev</author>
<author>R Uppal</author>
</authors>
<title>Equilibrium portfolio strategies in the presence of sentiment risk and excess volatility.</title>
<date>2007</date>
<journal>Swiss Finance Institute Research Paper</journal>
<volume>No.</volume>
<pages>07--37</pages>
<contexts>
<context position="6008" citStr="Dumas et al., 2007" startWordPosition="954" endWordPosition="957">s and a cornerstone of the so-called “efficient market hypothesis” (Fama, 1970). By contrast, the idea that one can predict a stock’s level of risk using public information is uncontroversial and a basic assumption made by many economically sound pricing models. A large body of research in finance suggests that the two types of quantities are very different: while predictability of returns could be easily traded away by the virtue of buying/selling stocks that are under- or over-valued (Fama, 1970), similar trades are much more costly to implement with respect to predictability of volatility (Dumas et al., 2007). By focusing on volatility prediction, we avoid taking a stance on whether or not the United States stock market is informationally efficient. 3 Problem Formulation Given a text document d, we seek to predict the value of a continuous variable v. We do this via a parameterized function f: vˆ = f(d; w) (2) where w E Rd are the parameters or weights. Our approach is to learn a human-interpretable w from a collection of N training examples {(di, vi)�N i=1, where each di is a document and each vi E R. Support vector regression (Drucker et al., 1997) is a well-known method for training a regressio</context>
</contexts>
<marker>Dumas, Kurshev, Uppal, 2007</marker>
<rawString>B. Dumas, A. Kurshev, and R. Uppal. 2007. Equilibrium portfolio strategies in the presence of sentiment risk and excess volatility. Swiss Finance Institute Research Paper No. 07-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Engelberg</author>
</authors>
<title>Costly information processing: Evidence from earnings announcements. Working paper,</title>
<date>2007</date>
<institution>Northwestern University.</institution>
<contexts>
<context position="29141" citStr="Engelberg, 2007" startWordPosition="4731" endWordPosition="4732">guage models. Farther afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation. Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008). While much of the information relevant for investors is communicated through text (rather than numbers), only recently is this link explored. Some papers relate news articles to earning forecasts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In contrast to text regr</context>
</contexts>
<marker>Engelberg, 2007</marker>
<rawString>J. Engelberg. 2007. Costly information processing: Evidence from earnings announcements. Working paper, Northwestern University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R F Engle</author>
</authors>
<title>Autoregressive conditional heteroscedasticity with estimates of variance of united kingdom inflation.</title>
<date>1982</date>
<journal>Econometrica,</journal>
<pages>50--987</pages>
<contexts>
<context position="12154" citStr="Engle, 1982" startWordPosition="1963" endWordPosition="1964">equences,5 and heuristic removal of remnant markup. Table 1 gives statistics on the corpora used in this research; this is a subset of the corpus for which there is no missing volatility information. The drastic increase in length during the 2002– 2003 period might be explained by the passage by the US Congress of the Sarbanes-Oxley Act of 2002 (and related SEC and exchange rules), which imposed revised standards on reporting practices of publicly-traded companies in the US. 5 Baselines and Evaluation Method Volatility displays an effect known as autoregressive conditional heteroscedasticity (Engle, 1982). This means that the variance in a stock’s return tends to change gradually. Large changes in price are presaged by other changes, and periods of stability tend to continue. Volatility is, generally speaking, not constant, yet prior volatility (e.g., v(−12)) is a very good predictor of future volatility (e.g., v(+12)). At the granularity of a year, which we consider here because the 10-K reports are annual, there are no existing models of volatility that are widely agreed to be significantly more accurate than our historical volatility baseline. We tested a state-of-theart model known as GARC</context>
</contexts>
<marker>Engle, 1982</marker>
<rawString>R. F. Engle. 1982. Autoregressive conditional heteroscedasticity with estimates of variance of united kingdom inflation. Econometrica, 50:987–1008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Fama</author>
</authors>
<title>Efficient capital markets: A review of theory and empirical work.</title>
<date>1970</date>
<journal>Journal of Finance,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="5468" citStr="Fama, 1970" startWordPosition="868" endWordPosition="869"> be treated as the true output variable on training and testing data. It is important to note that predicting volatility is not the same as predicting returns or value. Rather than trying to predict how well a stock will perform, we are trying to predict how stable its price will be over a future time period. It is, by now, received wisdom in the field of economics that predicting a stock’s performance, based on easily accessible public information, is difficult. This is an attribute of well-functioning (or “efficient”) markets and a cornerstone of the so-called “efficient market hypothesis” (Fama, 1970). By contrast, the idea that one can predict a stock’s level of risk using public information is uncontroversial and a basic assumption made by many economically sound pricing models. A large body of research in finance suggests that the two types of quantities are very different: while predictability of returns could be easily traded away by the virtue of buying/selling stocks that are under- or over-valued (Fama, 1970), similar trades are much more costly to implement with respect to predictability of volatility (Dumas et al., 2007). By focusing on volatility prediction, we avoid taking a st</context>
</contexts>
<marker>Fama, 1970</marker>
<rawString>E. F. Fama. 1970. Efficient capital markets: A review of theory and empirical work. Journal of Finance, 25(2):383–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gaa</author>
</authors>
<title>Media coverage, investor inattention, and the market’s reaction to news. Working paper,</title>
<date>2007</date>
<institution>University of British Columbia.</institution>
<contexts>
<context position="29123" citStr="Gaa, 2007" startWordPosition="4729" endWordPosition="4730">) using language models. Farther afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation. Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008). While much of the information relevant for investors is communicated through text (rather than numbers), only recently is this link explored. Some papers relate news articles to earning forecasts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In con</context>
</contexts>
<marker>Gaa, 2007</marker>
<rawString>C. Gaa. 2007. Media coverage, investor inattention, and the market’s reaction to news. Working paper, University of British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proc. of ECML.</booktitle>
<contexts>
<context position="29962" citStr="Joachims, 1998" startWordPosition="4853" endWordPosition="4854">ociated with risk and subsequent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In contrast to text regression, text classification comprises a widely studied set of problems involving the prediction of categorial variables related to text. Applications have included the categorization of documents by topic (Joachims, 1998), language (Cavnar and Trenkle, 1994), genre (Karlgren and Cutting, 1994), author (Bosch and Smith, 1998), sentiment (Pang et al., 2002), and desirability (Sahami et al., 1998). Text categorization has served as a test application for nearly every machine learning technique for discrete classification. 8 Conclusion We have introduced and motivated a new kind of task for NLP: text regression, in which text is used to make predictions about measurable phenomena in the real world. We applied the technique to predicting financial volatility from companies’ 10-K reports, and found text regression m</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In Proc. of ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods - Support Vector Learning.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8505" citStr="Joachims, 1999" startWordPosition="1399" endWordPosition="1400">document size during 2002–3 is possibly due to the passage of the Sarbanes-Oxley Act of 2002 in the wake of Enron’s accounting scandal (and numerous others). be seen as a similarity function between two documents. At test time, a new example is compared to a subset of the training examples (those with αi =� 0); typically with SVR this set is sparse. With the linear kernel, the primal and dual weights relate linearly: N w = αih(di) (5) i=1 The full details of SVR and its implementation are beyond the scope of this paper; interested readers are referred to Sch¨olkopf and Smola (2002). SVMlight (Joachims, 1999) is a freely available implementation of SVR training that we used in our experiments.2 4 Dataset In the United States, the Securities Exchange Commission mandates that all publicly-traded corporations produce annual reports known as “Form 10- K.” The report typically includes information about the history and organization of the company, equity and subsidiaries, as well as financial information. These reports are available to the public and published on the SEC’s web site.3 The structure of the 10-K is specified in detail in the legislation. We have collected 54,379 reports published over the</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale SVM learning practical. In Advances in Kernel Methods - Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Karlgren</author>
<author>D Cutting</author>
</authors>
<title>Recognizing text genres with simple metrics using discriminant analysis.</title>
<date>1994</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="30035" citStr="Karlgren and Cutting, 1994" startWordPosition="4861" endWordPosition="4864">and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In contrast to text regression, text classification comprises a widely studied set of problems involving the prediction of categorial variables related to text. Applications have included the categorization of documents by topic (Joachims, 1998), language (Cavnar and Trenkle, 1994), genre (Karlgren and Cutting, 1994), author (Bosch and Smith, 1998), sentiment (Pang et al., 2002), and desirability (Sahami et al., 1998). Text categorization has served as a test application for nearly every machine learning technique for discrete classification. 8 Conclusion We have introduced and motivated a new kind of task for NLP: text regression, in which text is used to make predictions about measurable phenomena in the real world. We applied the technique to predicting financial volatility from companies’ 10-K reports, and found text regression model predictions to correlate with true volatility nearly as well as hist</context>
</contexts>
<marker>Karlgren, Cutting, 1994</marker>
<rawString>J. Karlgren and D. Cutting. 1994. Recognizing text genres with simple metrics using discriminant analysis. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Koppel</author>
<author>I Shtrimberg</author>
</authors>
<title>Good news or bad news? let the market decide.</title>
<date>2004</date>
<booktitle>In AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.</booktitle>
<contexts>
<context position="29075" citStr="Koppel and Shtrimberg, 2004" startWordPosition="4718" endWordPosition="4722">fluences between text and time series financial data (stock prices) using language models. Farther afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation. Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008). While much of the information relevant for investors is communicated through text (rather than numbers), only recently is this link explored. Some papers relate news articles to earning forecasts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like </context>
</contexts>
<marker>Koppel, Shtrimberg, 2004</marker>
<rawString>M. Koppel and I. Shtrimberg. 2004. Good news or bad news? let the market decide. In AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lavrenko</author>
<author>M Schmill</author>
<author>D Lawrie</author>
<author>P Ogilvie</author>
<author>D Jensen</author>
<author>J Allan</author>
</authors>
<title>Language models for financial news recommendation.</title>
<date>2000</date>
<booktitle>In Proc. of CIKM.</booktitle>
<contexts>
<context position="28407" citStr="Lavrenko et al. (2000" startWordPosition="4619" endWordPosition="4622">ecision at 10% recall. Our best (oracle) F1 scores occur in 2002 and 2003, suggesting again a difference in reports around Sarbanes-Oxley. Table 5 shows words associated with delisting. 7 Related Work In NLP, regression is not widely used, since most natural language-related data are discrete. Regression methods were pioneered by Yang and Chute (1992) and Yang and Chute (1993) for information retrieval purposes, but the predicted continuous variable was not an end in itself in that work. Blei and McAuliffe (2007) used latent “topic” variables to predict movie reviews and popularity from text. Lavrenko et al. (2000b) and Lavrenko et al. (2000a) modeled influences between text and time series financial data (stock prices) using language models. Farther afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation. Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008). While much of the information relevant for investors is communicated through text (rather than numbers), only recently is this link explored. Some papers relate news articles to earning forecasts</context>
</contexts>
<marker>Lavrenko, Schmill, Lawrie, Ogilvie, Jensen, Allan, 2000</marker>
<rawString>V. Lavrenko, M. Schmill, D. Lawrie, P. Ogilvie, D. Jensen, and J. Allan. 2000a. Language models for financial news recommendation. In Proc. of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lavrenko</author>
<author>M Schmill</author>
<author>D Lawrie</author>
<author>P Ogilvie</author>
<author>D Jensen</author>
<author>J Allan</author>
</authors>
<title>Mining of concurrent text and time series.</title>
<date>2000</date>
<booktitle>In Proc. of KDD.</booktitle>
<contexts>
<context position="28407" citStr="Lavrenko et al. (2000" startWordPosition="4619" endWordPosition="4622">ecision at 10% recall. Our best (oracle) F1 scores occur in 2002 and 2003, suggesting again a difference in reports around Sarbanes-Oxley. Table 5 shows words associated with delisting. 7 Related Work In NLP, regression is not widely used, since most natural language-related data are discrete. Regression methods were pioneered by Yang and Chute (1992) and Yang and Chute (1993) for information retrieval purposes, but the predicted continuous variable was not an end in itself in that work. Blei and McAuliffe (2007) used latent “topic” variables to predict movie reviews and popularity from text. Lavrenko et al. (2000b) and Lavrenko et al. (2000a) modeled influences between text and time series financial data (stock prices) using language models. Farther afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation. Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008). While much of the information relevant for investors is communicated through text (rather than numbers), only recently is this link explored. Some papers relate news articles to earning forecasts</context>
</contexts>
<marker>Lavrenko, Schmill, Lawrie, Ogilvie, Jensen, Allan, 2000</marker>
<rawString>V. Lavrenko, M. Schmill, D. Lawrie, P. Ogilvie, D. Jensen, and J. Allan. 2000b. Mining of concurrent text and time series. In Proc. of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lerman</author>
<author>A Gilder</author>
<author>M Dredze</author>
<author>F Pereira</author>
</authors>
<title>Reading the markets: Forecasting public opinion of political candidates by news analysis.</title>
<date>2008</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="29715" citStr="Lerman et al., 2008" startWordPosition="4816" endWordPosition="4819">lock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In contrast to text regression, text classification comprises a widely studied set of problems involving the prediction of categorial variables related to text. Applications have included the categorization of documents by topic (Joachims, 1998), language (Cavnar and Trenkle, 1994), genre (Karlgren and Cutting, 1994), author (Bosch and Smith, 1998), sentiment (Pang et al., 2002), and desirability (Sahami et al., 1998). Text categorization has served as a test application for nearly every machine learning technique for discrete classification. 8 Conclusion We have introduced and motivated a n</context>
</contexts>
<marker>Lerman, Gilder, Dredze, Pereira, 2008</marker>
<rawString>K. Lerman, A. Gilder, M. Dredze, and F. Pereira. 2008. Reading the markets: Forecasting public opinion of political candidates by news analysis. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Li</author>
</authors>
<title>Do stock market investors understand the risk sentiment of corporate annual reports? Working Paper,</title>
<date>2005</date>
<institution>University of Michigan.</institution>
<contexts>
<context position="29291" citStr="Li (2005)" startWordPosition="4755" endWordPosition="4756">techniques using correlation. Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008). While much of the information relevant for investors is communicated through text (rather than numbers), only recently is this link explored. Some papers relate news articles to earning forecasts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In contrast to text regression, text classification comprises a widely studied set of problems involving the prediction of categorial variables related to text. Applications </context>
</contexts>
<marker>Li, 2005</marker>
<rawString>F. Li. 2005. Do stock market investors understand the risk sentiment of corporate annual reports? Working Paper, University of Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Merton</author>
</authors>
<title>On the pricing of corporate debt: The risk structure of interest rates.</title>
<date>1974</date>
<journal>Journal of Finance,</journal>
<pages>29--449</pages>
<contexts>
<context position="26326" citStr="Merton (1974)" startWordPosition="4261" endWordPosition="4262">and high. There is no a priori reason to expect high or low margins would be associated with high or low stock volatility. However, this is an interesting example where bigrams are helpful (the word margin by itself is uninformative) and indicates that predicting risk is highly time-dependent. 6.5 Delisting An interesting but relatively infrequent phenomenon is the delisting of a company, i.e., when it ceases to be traded on a particular exchange due to dissolution after bankruptcy, a merger, or violation of exchange rules. The relationship between volatility and delisting has been studied by Merton (1974), among others. Our dataset includes a small number of cases where the volatility figures for the period following the publication of a 10-K report are unavailable because the company was delisted. Learning to predict delisting is extremely difficult because fewer than 4% of the 2001–6 10-K reports precede delisting. Using the LOG1P representation, we built a linear SVM classifier for each year in 2001–6 (trained on the five preceding years’ data) to predict whether a company will be delisted following its 10-K report. Performance for various precision measures is shown in Table 5. Notably, fo</context>
</contexts>
<marker>Merton, 1974</marker>
<rawString>R. Merton. 1974. On the pricing of corporate debt: The risk structure of interest rates. Journal of Finance, 29:449–470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="29603" citStr="Pang et al., 2002" startWordPosition="4799" endWordPosition="4802"> to earning forecasts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In contrast to text regression, text classification comprises a widely studied set of problems involving the prediction of categorial variables related to text. Applications have included the categorization of documents by topic (Joachims, 1998), language (Cavnar and Trenkle, 1994), genre (Karlgren and Cutting, 1994), author (Bosch and Smith, 1998), sentiment (Pang et al., 2002), and desirability (Sahami et al., 1998). Text categorization has served as a test application for nearly</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahami</author>
<author>S Dumais</author>
<author>D Heckerman</author>
<author>E Horvitz</author>
</authors>
<title>A Bayesian approach to filtering junk email.</title>
<date>1998</date>
<booktitle>In Proc. of AAAI Workshop on Learning for Text Categorization.</booktitle>
<contexts>
<context position="30138" citStr="Sahami et al., 1998" startWordPosition="4877" endWordPosition="4881">ed the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In contrast to text regression, text classification comprises a widely studied set of problems involving the prediction of categorial variables related to text. Applications have included the categorization of documents by topic (Joachims, 1998), language (Cavnar and Trenkle, 1994), genre (Karlgren and Cutting, 1994), author (Bosch and Smith, 1998), sentiment (Pang et al., 2002), and desirability (Sahami et al., 1998). Text categorization has served as a test application for nearly every machine learning technique for discrete classification. 8 Conclusion We have introduced and motivated a new kind of task for NLP: text regression, in which text is used to make predictions about measurable phenomena in the real world. We applied the technique to predicting financial volatility from companies’ 10-K reports, and found text regression model predictions to correlate with true volatility nearly as well as historical volatility, and a combined model to perform even better. Further, improvements in accuracy and c</context>
</contexts>
<marker>Sahami, Dumais, Heckerman, Horvitz, 1998</marker>
<rawString>M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz. 1998. A Bayesian approach to filtering junk email. In Proc. of AAAI Workshop on Learning for Text Categorization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sch¨olkopf</author>
<author>A J Smola</author>
</authors>
<title>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond.</title>
<date>2002</date>
<publisher>MIT Press.</publisher>
<marker>Sch¨olkopf, Smola, 2002</marker>
<rawString>B. Sch¨olkopf and A. J. Smola. 2002. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P C Tetlock</author>
<author>M Saar-Tsechansky</author>
<author>S Macskassy</author>
</authors>
<title>More than words: Quantifying language to measure firms’ fundamentals.</title>
<date>2008</date>
<journal>Journal of Finance,</journal>
<volume>63</volume>
<issue>3</issue>
<contexts>
<context position="29112" citStr="Tetlock et al., 2008" startWordPosition="4725" endWordPosition="4728">ial data (stock prices) using language models. Farther afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation. Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008). While much of the information relevant for investors is communicated through text (rather than numbers), only recently is this link explored. Some papers relate news articles to earning forecasts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 20</context>
</contexts>
<marker>Tetlock, Saar-Tsechansky, Macskassy, 2008</marker>
<rawString>P. C. Tetlock, M. Saar-Tsechansky, and S. Macskassy. 2008. More than words: Quantifying language to measure firms’ fundamentals. Journal of Finance, 63(3):1437–1467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P C Tetlock</author>
</authors>
<title>Giving content to investor sentiment: The role of media in the stock market.</title>
<date>2007</date>
<journal>Journal of Finance,</journal>
<volume>62</volume>
<issue>3</issue>
<contexts>
<context position="29090" citStr="Tetlock, 2007" startWordPosition="4723" endWordPosition="4724">e series financial data (stock prices) using language models. Farther afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation. Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008). While much of the information relevant for investors is communicated through text (rather than numbers), only recently is this link explored. Some papers relate news articles to earning forecasts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction mark</context>
</contexts>
<marker>Tetlock, 2007</marker>
<rawString>P. C. Tetlock. 2007. Giving content to investor sentiment: The role of media in the stock market. Journal of Finance, 62(3):1139–1168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Weiss-Hanley</author>
<author>G Hoberg</author>
</authors>
<title>Strategic disclosure and the pricing of initial public offerings. Working paper.</title>
<date>2008</date>
<contexts>
<context position="29425" citStr="Weiss-Hanley and Hoberg (2008)" startWordPosition="4772" endWordPosition="4775">y et al., 2008). While much of the information relevant for investors is communicated through text (rather than numbers), only recently is this link explored. Some papers relate news articles to earning forecasts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In contrast to text regression, text classification comprises a widely studied set of problems involving the prediction of categorial variables related to text. Applications have included the categorization of documents by topic (Joachims, 1998), language (Cavnar and Trenkle, 1994), genre (Karlgren and Cutt</context>
</contexts>
<marker>Weiss-Hanley, Hoberg, 2008</marker>
<rawString>K. Weiss-Hanley and G. Hoberg. 2008. Strategic disclosure and the pricing of initial public offerings. Working paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>E Riloff</author>
</authors>
<title>Creating subjective and objective sentence classifiers from unannotated texts.</title>
<date>2005</date>
<booktitle>In CICLing.</booktitle>
<contexts>
<context position="29628" citStr="Wiebe and Riloff, 2005" startWordPosition="4803" endWordPosition="4806">ts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In contrast to text regression, text classification comprises a widely studied set of problems involving the prediction of categorial variables related to text. Applications have included the categorization of documents by topic (Joachims, 1998), language (Cavnar and Trenkle, 1994), genre (Karlgren and Cutting, 1994), author (Bosch and Smith, 1998), sentiment (Pang et al., 2002), and desirability (Sahami et al., 1998). Text categorization has served as a test application for nearly every machine learning t</context>
</contexts>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>J. Wiebe and E. Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. In CICLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>C G Chute</author>
</authors>
<title>A linear least squares fit mapping method for information retrieval from natural language texts.</title>
<date>1992</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="28139" citStr="Yang and Chute (1992)" startWordPosition="4575" endWordPosition="4578">ow many of the six years (2001–6) the word is among the ten most strongly weighted. There were no clear patterns across years for words predicting that a company would not be delisted. The word otc refers to “over-the-counter” trading, a high-risk market. above 75% precision at 10% recall. Our best (oracle) F1 scores occur in 2002 and 2003, suggesting again a difference in reports around Sarbanes-Oxley. Table 5 shows words associated with delisting. 7 Related Work In NLP, regression is not widely used, since most natural language-related data are discrete. Regression methods were pioneered by Yang and Chute (1992) and Yang and Chute (1993) for information retrieval purposes, but the predicted continuous variable was not an end in itself in that work. Blei and McAuliffe (2007) used latent “topic” variables to predict movie reviews and popularity from text. Lavrenko et al. (2000b) and Lavrenko et al. (2000a) modeled influences between text and time series financial data (stock prices) using language models. Farther afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation. Regression has also been us</context>
</contexts>
<marker>Yang, Chute, 1992</marker>
<rawString>Y. Yang and C. G. Chute. 1992. A linear least squares fit mapping method for information retrieval from natural language texts. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>C G Chute</author>
</authors>
<title>An application of least squares fit mapping to text information retrieval.</title>
<date>1993</date>
<booktitle>In Proc. of SIGIR.</booktitle>
<contexts>
<context position="28165" citStr="Yang and Chute (1993)" startWordPosition="4580" endWordPosition="4583">2001–6) the word is among the ten most strongly weighted. There were no clear patterns across years for words predicting that a company would not be delisted. The word otc refers to “over-the-counter” trading, a high-risk market. above 75% precision at 10% recall. Our best (oracle) F1 scores occur in 2002 and 2003, suggesting again a difference in reports around Sarbanes-Oxley. Table 5 shows words associated with delisting. 7 Related Work In NLP, regression is not widely used, since most natural language-related data are discrete. Regression methods were pioneered by Yang and Chute (1992) and Yang and Chute (1993) for information retrieval purposes, but the predicted continuous variable was not an end in itself in that work. Blei and McAuliffe (2007) used latent “topic” variables to predict movie reviews and popularity from text. Lavrenko et al. (2000b) and Lavrenko et al. (2000a) modeled influences between text and time series financial data (stock prices) using language models. Farther afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation. Regression has also been used to order sentences in e</context>
</contexts>
<marker>Yang, Chute, 1993</marker>
<rawString>Y. Yang and C. G. Chute. 1993. An application of least squares fit mapping to text information retrieval. In Proc. of SIGIR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>