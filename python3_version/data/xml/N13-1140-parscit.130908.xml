<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000066">
<title confidence="0.995087">
Knowledge-Rich Morphological Priors for Bayesian Language Models
</title>
<author confidence="0.997044">
Victor Chahuneau Noah A. Smith Chris Dyer
</author>
<affiliation confidence="0.931416">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.997949">
{vchahune,nasmith,cdyer}@cs.cmu.edu
</email>
<sectionHeader confidence="0.996656" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999177">
We present a morphology-aware nonparamet-
ric Bayesian model of language whose prior
distribution uses manually constructed finite-
state transducers to capture the word forma-
tion processes of particular languages. This
relaxes the word independence assumption
and enables sharing of statistical strength
across, for example, stems or inflectional
paradigms in different contexts. Our model
can be used in virtually any scenario where
multinomial distributions over words would
be used. We obtain state-of-the-art results in
language modeling, word alignment, and un-
supervised morphological disambiguation for
a variety of morphologically rich languages.
</bodyText>
<sectionHeader confidence="0.998756" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999724024390244">
Despite morphological phenomena’s salience in
most human languages, many NLP systems treat
fully inflected forms as the atomic units of language.
By assuming independence of lexical stems’ vari-
ous surface forms, this avoidance approach exacer-
bates the problem of data sparseness. If it is em-
ployed at all, morphological analysis of text tends
to be treated as a preprocessing step to other NLP
modules. While this latter disambiguation approach
helps address data sparsity concerns, it has substan-
tial drawbacks: it requires supervised learning from
expert-annotated corpora, and determining the op-
timal morphological granularity is labor-intensive
(Habash and Sadat, 2006).
Neither approach fully exploits the finite-state
transducer (FST) technology that has been so suc-
cessful for modeling the mapping between surface
forms and their morphological analyses (Karttunen
and Beesley, 2005), and the mature collections of
high quality transducers that already exist for many
languages (e.g., Turkish, Russian, Arabic). Much
linguistic knowledge is encoded in such FSTs.
In this paper, we develop morphology-aware non-
parametric Bayesian language models that bring to-
gether hand-written FSTs with statistical modeling
and require no token-level annotation. The sparsity
issue discussed above is addressed by hierarchical
priors that share statistical strength across different
inflections of the same stem by backing off to word
formation models that piece together morphemes us-
ing FSTs. Furthermore, because of the nonparamet-
ric formulation of our models, the regular morpho-
logical patterns found in the long tail of word types
will rely more heavily on deeper analysis, while fre-
quent and idiosyncratically behaved forms are mod-
eled opaquely.
Our prior can be used in virtually any generative
model of language as a replacement for multino-
mial distributions over words, bringing morphologi-
cal awareness to numerous applications. For various
morphologically rich languages, we show that:
</bodyText>
<listItem confidence="0.996974888888889">
• our model can provide rudimentary unsuper-
vised disambiguation for a highly ambiguous
analyzer;
• integrating morphology into n-gram language
models allows better generalization to unseen
words and can improve the performance of ap-
plications that are truly open vocabulary; and
• bilingual word alignment models also bene-
fit greatly from sharing translation information
</listItem>
<page confidence="0.913505">
1206
</page>
<note confidence="0.4826965">
Proceedings of NAACL-HLT 2013, pages 1206–1215,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9888195">
across stems.
We are particularly interested in low-resource sce-
narios, where one has to make the most of the
small quantity of available data, and overcoming
data sparseness is crucial. If analyzers exist in such
settings, they tend to be highly ambiguous, and an-
notated data for learning to disambiguate are also
likely to be scarce or non-existent. Therefore, in our
experiments with Russian, we compare two analyz-
ers: a rapidly-developed guesser, which models reg-
ular inflectional paradigms but contains no lexicon
or irregular forms, and a high-quality analyzer.
</bodyText>
<sectionHeader confidence="0.974326" genericHeader="introduction">
2 Word Models with Morphology
</sectionHeader>
<bodyText confidence="0.99990625">
In this section, we describe a generative model of
word formation based on Pitman-Yor processes that
generates word types using a finite-state morpho-
logical generator. At a high level, the process first
produces lexicons of stems and inflectional patterns;
then it generates a lexicon of inflected forms us-
ing the finite-state generator. Finally, the inflected
forms are used to generate observed data. Different
independence assumptions can be made at each of
these levels to encode beliefs about where stems, in-
flections, and surface forms should share statistical
strength.
</bodyText>
<sectionHeader confidence="0.607953" genericHeader="method">
2.1 Pitman-Yor Processes
</sectionHeader>
<bodyText confidence="0.999945210526316">
Our work relies extensively on Pitman-Yor pro-
cesses, which provide a flexible framework for ex-
pressing backoff and interpolation relationships and
extending standard models with richer word distri-
butions (Pitman and Yor, 1997). They have been
shown to match the performance of state-of-the-art
language models and to give estimates that follow
appropriate power laws (Teh, 2006).
A draw from a Pitman-Yor process (PYP), de-
noted G — PY(d, 0, G0), is a discrete distribution
over a (possibly infinite) set of events, which we de-
note abstractly £. The process is parameterized by a
discount parameter 0 &lt; d &lt; 1, a strength parameter
0 &gt; —d, and a base distribution G0 over the event
space £.
In this work, our focus is on the base distribution
G0. We place vague priors on the hyperparameters
d — U([0,1]) and (0 + d) — Gamma(1,1). Infer-
ence in PYPs is discussed below.
</bodyText>
<subsectionHeader confidence="0.996533">
2.2 Unigram Morphology Model
</subsectionHeader>
<bodyText confidence="0.9995823">
The most basic expression of our model is a uni-
gram model of text. So far, we only assume that
each word can be analyzed into a stem and a se-
quence of morphemes forming an inflection pattern.
Let Gs be a distribution over stems, Gp be a distribu-
tion over inflectional patterns, and let GENERATE be
a deterministic mapping from (stem, pattern) pairs
to inflected word forms.1 An inflected word type is
generated with the following process, which we des-
ignate MP(Gs, Gd, GENERATE):
</bodyText>
<equation confidence="0.999536666666667">
stem — Gs
pattern — Gp
word = GENERATE(stem, pattern)
</equation>
<bodyText confidence="0.987008">
For example, in Russian, we might sample stem
= прочий,2 pattern = STEM+Adj+Pl+Dat, and
obtain word = прочим.
This model could be used directly to generate ob-
served tokens. However, we have said nothing about
Gs and Gp, and the assumption that stems and pat-
terns are independent is clearly unsatisfying. We
therefore assume that both the stem and the pattern
distributions are generated from PY processes, and
that MP(Gs, Gp, GENERATE) is itself the base dis-
tribution of a PYP.
</bodyText>
<equation confidence="0.997469333333333">
Gs — PY(ds, 0s, G0s)
Gp — PY(dp, 0p, G0p)
Gw — PY(d, 0, MP(Gs, Gp, GENERATE))
</equation>
<bodyText confidence="0.9912755">
A draw Gw from this PYP is a unigram distribu-
tion over tokens.
</bodyText>
<subsectionHeader confidence="0.998878">
2.3 Base Stem Model G0s
</subsectionHeader>
<bodyText confidence="0.999830166666667">
In general there are an unbounded number of stems
possible in any language, so we set G0s to be charac-
ter trigram model, which we statically estimate, with
Kneser-Ney smoothing, from a large corpus of word
types in the language being modeled. While using
fixed parameters estimated to maximize likelihood is
</bodyText>
<footnote confidence="0.823259">
1The assumption of determinism is only inappropriate in
cases of inflectional spelling variants (e.g., modeled vs. mod-
elled) or pronunciation variants (e.g., reduced forms in certain
environments).
2ripoYxfl (pronounced [pretCij]) = other
</footnote>
<page confidence="0.993153">
1207
</page>
<bodyText confidence="0.999726666666667">
questionable from the perspective of Bayesian learn-
ing, it is tremendously beneficial for computational
reasons. For some applications (e.g., word align-
ment), the set of possible stems for a corpus 5 can be
precomputed, so we will also experiment with using
a uniform stem distribution based on this set.
</bodyText>
<subsectionHeader confidence="0.996124">
2.4 Base Pattern Model G0 p
</subsectionHeader>
<bodyText confidence="0.99936045">
Several choices are possible for the base pattern dis-
tribution:
MP0 We can assume a uniform G0p when the num-
ber of patterns is small.
MP1 To be able to generalize to new patterns, we
can draw the length of the pattern from a Poisson
distribution and generate morphemes one by one
from a uniform distribution.
MP2 A more informative prior is a Markov chain
of morphemes, where each morpheme is generated
conditional on the preceding morpheme.
The choice of the base pattern distribution could
depend on the complexity of the inflectional patterns
produced by the morphological analyzer, reflecting
the type of morphological phenomena present in a
given language. For example, the number of possi-
ble patterns can practically be considered finite in
Russian, but this assumption is not valid for lan-
guages with more extensive derivational morphol-
ogy like Turkish.
</bodyText>
<subsectionHeader confidence="0.983666">
2.5 Posterior Inference
</subsectionHeader>
<bodyText confidence="0.996589294117647">
For most applications, rather than directly gener-
ating from a model using the processes outlined
above, we seek to infer posterior distributions over
latent parameters and structures, given a sample of
data.
Although there is no known analytic form of
the PYP density, it is possible to marginalize the
draws from it and to work directly with observa-
tions. This marginalization produces the classi-
cal Chinese restaurant process representation (Teh,
2006). When working with the morphology mod-
els we are proposing, we also need to marginalize
the different latent forms (stems s and patterns p)
that may have given rise to a given word w. Thus,
we require that the inverse relation of GENERATE is
available to compute the marginal base word distri-
bution:
</bodyText>
<equation confidence="0.908105">
p(w I G0w) = � p(s I Gs) p(p I Gp)
GENERATE(s,p)=w
</equation>
<bodyText confidence="0.99781">
Since our approach encodes morphology using
FSTs, which are invertible, this poses no problem.
To illustrate, consider the Russian word npo,qHM,
which may be analyzed in several ways:
</bodyText>
<figure confidence="0.7262296">
npo,qHH +Adj +Sg +Neut +Instr
npo,qHH +Adj +Sg +Masc +Instr
npo,qHH +Adj +Pl +Dat
npo,qHTb +Verb +Pl +1P
npo,qee +Pro +Sg +Ins
</figure>
<bodyText confidence="0.997056444444445">
Because the set of possible analyses is in general
small, marginalization is fast and complex blocked
sampling is not necessary.
Finally, to infer hyperparameter values (d, 0,...),
a Metropolis-Hastings update is interleaved with
Gibbs sampling steps for the rest of the hidden vari-
ables.3
Having described a model for generating words,
we now show its usage in several contexts.
</bodyText>
<sectionHeader confidence="0.994187" genericHeader="method">
3 Unsupervised Morphological
Disambiguation
</sectionHeader>
<bodyText confidence="0.999967692307692">
Given a rule-based morphological analyzer encoded
as an unweighted FST and a corpus on which the
analyzer has been run – possibly generating multi-
ple analyses for each token – we can use our un-
igram model to learn a probabilistic model of dis-
ambiguation in an unsupervised setting (i.e., with-
out annotated examples). The corpus is assumed to
be generated from the unigram distribution Gw, and
the base stem model is set to a fixed character tri-
gram model.4 After learning the parameters of the
model, we can find for each word in the vocabulary
its most likely analysis and use this as a crude dis-
ambiguation step.
</bodyText>
<footnote confidence="0.9465048">
3The proposal distribution for Metropolis-Hastings is a Beta
distribution (d) or a Gamma distribution (B + d) centered on the
previous parameter values.
4Experiments suggest that this is important to constrain the
model to realistic stems.
</footnote>
<page confidence="0.985919">
1208
</page>
<subsectionHeader confidence="0.99889">
3.1 Morphological Guessers
</subsectionHeader>
<bodyText confidence="0.992372612903226">
Finite-state morphological analyzers are usually
specified in three parts: a stem lexicon, which de-
fines the words in the language and classifies them
into several categories according to their grammat-
ical function and their morphological properties; a
set of prefixes and suffixes that can be applied to
each category to form surface words; and possibly
alternation rules that can encode exceptions and
spelling variations. The combination of these parts
provides a powerful framework for defining a gener-
ative model of words. Such models can be reversed
to obtain an analyzer. However, while the two latter
parts can be relatively easy to specify, enumerating
a comprehensive stem lexicon is a time consuming
and necessarily incomplete process, as some cate-
gories are truly open-class.
To allow unknown words to be analyzed, one
can use a guesser that attempts to analyze words
missing in the lexicon. Can we eliminate the stem
lexicon completely and use only the guesser? This
is what we try to do by designing a lexicon-free
analyzer for Russian. A guesser was developed
in three hours; it is prone to over-generation and
produces ambiguous analyses for most words
but covers a large number of morphological phe-
nomena (gender, case, tense, etc.). For example,
the word HspHTe5 can be correctly analyzed as
HspHT+Noun+Masc+Prep+Sg but also as the in-
correct forms: HspHTb+Verb+Pres+2P+Pl,
HspHTa+Noun+Fem+Dat+Sg, HspH-
Tx+Noun+Fem+Prep+Sg, and more.
</bodyText>
<subsectionHeader confidence="0.999537">
3.2 Disambiguation Experiments
</subsectionHeader>
<bodyText confidence="0.999957363636364">
We train the unigram model on a 1.7M-word cor-
pus of TED talks transcriptions translated into Rus-
sian (Cettolo et al., 2012) and evaluate our ana-
lyzer against a test set consisting of 1,500 gold-
standard analyses obtained from the morphology
disambiguation task of the DIALOG 2010 confer-
ence (Lyaševskaya et al., 2010).6
Each analysis is composed of a lemma (HspHT),
a part of speech (Noun), and a sequence of ad-
ditional functional morphemes (Masc,Prep,Sg).
We consider only open-class categories: nouns, ad-
</bodyText>
<footnote confidence="0.99439">
5xspxTe = Hebrew (masculine noun, prepositional case)
6http://ru-eval.ru
</footnote>
<bodyText confidence="0.999821176470588">
jectives, adverbs and verbs, and evaluate the output
of our model with three metrics: the lemma accu-
racy, the part-of-speech accuracy, and the morphol-
ogy F-measure.7
As a baseline, we consider picking a random anal-
ysis from output of the analyzer or choosing the
most frequent lemma and the most frequent morpho-
logical pattern.8 Then, we use our model with each
of the three versions of the pattern model described
in §2.2. Finally, as an upper bound, we use the gold
standard to select one of the analyses produced by
the guesser.
Since our evaluation is not directly comparable
to the standard for this task, we use for reference
a high-quality analyzer from Xerox9 disambiguated
with the MP0 model (all of the models have very
close accuracy in this case).
</bodyText>
<table confidence="0.999343125">
Model Lemma POS Morph.
Random 29.8 70.9 50.2
Frequency 31.1 74.4 48.8
Guesser MP0 60.0 82.2 66.3
Guesser MP1 58.9 82.5 69.5
Guesser MP2 59.9 82.4 65.5
Guesser oracle 68.4 84.9 83.0
Xerox MP0 83.6 96.4 78.1
</table>
<tableCaption confidence="0.99991">
Table 1: Russian morphological disambiguation.
</tableCaption>
<bodyText confidence="0.9999587">
Considering the amount of effort put in develop-
ing the guesser, the baseline POS tagging accuracy
is relatively good. However, the disambiguation is
largely improved by using our unigram model with
respect to all the evaluation categories. We are still
far from the performance of a high-quality analyzer
but, in absence of such a resource, our technique
might be a sensible option. We also note that there is
no clear winner in terms of pattern model, and con-
clude that this choice is task-specific.
</bodyText>
<footnote confidence="0.985962833333333">
7F-measure computed for the set of additional morphemes
and averaged over the words in the corpus.
8We estimate these frequencies by assuming each analysis of
each token is uniformly likely, then summing fractional counts.
9http://open.xerox.com/Services/
fst-nlp-tools/Pages/morphology
</footnote>
<page confidence="0.993877">
1209
</page>
<sectionHeader confidence="0.976359" genericHeader="method">
4 Open Vocabulary Language Models
</sectionHeader>
<bodyText confidence="0.922357363636363">
We now integrate our unigram model in a hierar-
chical Pitman-Yor n-gram language model (Fig. 1).
The training corpus words are assumed to be
generated from a distribution Gn w drawn from
PY(dn, On, Gn−1
w ), where Gn−1
w is defined recur-
sively down to the base model G0w. Previous work
Teh (2006) simply used G0w = N(V) where V is
the word vocabulary, but in our case G0w is the MP
defined in §2.2.
</bodyText>
<figureCaption confidence="0.986084666666667">
Figure 1: The trigram version of our language model rep-
resented as a graphical model. G1 is the unigram model
of §2.2.
</figureCaption>
<bodyText confidence="0.996405692307692">
We are interested in evaluating our model in an
open vocabulary scenario where the ability to ex-
plain new unseen words matters. We expect our
model to be able to generalize better thanks to the
combination of a morphological analyzer and a stem
distribution which is less sparse than the word dis-
tribution (for example, for the 1.6M word Turkish
corpus, |V  |Pz� 3.5|5 |Pz� 140k).
To integrate out-of-vocabulary words in our eval-
uation, we use infinite base distributions: G0w (in the
baseline model) or G0 s (in the MP) are character tri-
gram models. We define perplexity of a held-out test
corpus in the standard way:
</bodyText>
<equation confidence="0.636817333333333">
1 N �loge (wi  |wi−n+1 ··· wi−1)
ppl = exp −
N i=1
</equation>
<bodyText confidence="0.999937666666667">
but compared to the common practice, we do not
need to discount OOVs from this sum since the
model vocabulary is infinite. Note that we also
marginalize by summing over all the possible analy-
ses for a given word when computing its base prob-
ability according to the MP.
</bodyText>
<subsectionHeader confidence="0.99134">
4.1 Language Modeling Experiments
</subsectionHeader>
<bodyText confidence="0.99470925">
We train several trigram models on the Russian TED
talks corpus used in the previous section. Our base-
line is a hierarchical PY trigram model with a tri-
gram character model as the base word distribution.
We compare it with our model using the same char-
acter model for the base stem distribution. Both of
the morphological analyzers described in the previ-
ous section help obtaining perplexity reductions (Ta-
ble 2). We ran a similar experiment on the Turkish
version of this corpus (1.6M words) with a high-
quality analyzer (Oflazer, 1994) and obtain even
larger gains (Table 3).
</bodyText>
<table confidence="0.9984605">
Model ppl
PY-character LM 563
Guesser MP2 530
Xerox MP2 522
</table>
<tableCaption confidence="0.990659">
Table 2: Evaluation of the Russian n-gram model.
</tableCaption>
<table confidence="0.998670333333333">
Model ppl
PY-character LM 1,640
Oflazer MP2 1,292
</table>
<tableCaption confidence="0.99993">
Table 3: Evaluation of the Turkish n-gram model.
</tableCaption>
<bodyText confidence="0.998563">
These results can partly be attributed to the high
OOV rate in these conditions: 4% for the Russian
corpus and 6% for the Turkish corpus.
</bodyText>
<subsectionHeader confidence="0.979596">
4.2 Predictive Text Input
</subsectionHeader>
<bodyText confidence="0.999970133333334">
It is difficult to know whether a decrease in perplex-
ity, as measured in the previous section, will result in
a performance improvement in downstream applica-
tions. As a confirmation that correctly modeling new
words matters, we consider a predictive task with a
truly open vocabulary and that requires only a lan-
guage model: predictive text input.
Given some text, we encode it using a lossy de-
terministic character mapping, and try to recover the
original content by computing the most likely word
sequence. This task is inspired by predictive text
input systems available on cellphones with a 9-key
keypad. For example, the string gave me a cup
is encoded as 4283 63 2 287, which could also
be decoded as: hate of a bus.
</bodyText>
<figure confidence="0.9966815">
d ,✓
d ,✓ d ,✓ d1, ✓1
G W
G W
G1
W1
d ,✓
G G
G
G
</figure>
<page confidence="0.93194">
1210
</page>
<bodyText confidence="0.9993711">
Silfverberg et al. (2012) describe a system de-
signed for this task in Finnish, which is composed
of a weighted finite-state morphological analyzer
trained on IRC logs. However, their system is re-
stricted to words that are encoded in the analyzer’s
lexicon and does not use context for disambiguation.
In our experiments, we use the same Turkish TED
talks corpus as the previous section. As a baseline,
we use a trigram character language model. We pro-
duce a character lattice which encodes all the pos-
sible interpretations for a word and compose it with
a finite-state representation of the character LM us-
ing OpenFST (Allauzen et al., 2007). Alternatively,
we can use a unigram word model to decode this lat-
tice, backing off to the character language model if
no solution is found. Finally, to be able to make use
of word context, we can extract the k most likely
paths according to the character LM and produce a
word lattice, which is in turn decoded with a lan-
guage model defined over the extracted vocabulary.
</bodyText>
<table confidence="0.997915666666667">
Model WER CER
Character LM 48.37 16.72
1-gram + character LM 8.50 3.28
1-gram MP2 6.46 2.37
3-gram + character LM 7.86 3.07
3-gram MP2 5.73 2.15
</table>
<tableCaption confidence="0.999875">
Table 4: Evaluation of Turkish predictive text input.
</tableCaption>
<bodyText confidence="0.9999698">
We measure word and character error rate (WER,
CER) on the predicted word sequence and observe
large improvements in both of these metrics by mod-
eling morphology, both at the unigram level and
when context is used (Table 4).
Preliminary experiments with a corpus of 1.6M
Turkish tweets, an arguably more appropriate do-
main this task, show smaller but consistent improv-
ing: the trigram word error rate is reduced from 26%
to 24% when our model is used.
</bodyText>
<subsectionHeader confidence="0.998529">
4.3 Limitations
</subsectionHeader>
<bodyText confidence="0.999993909090909">
While our model is an important step forward in
practical modeling of OOVs using morphological
processes, we have made the linguistically naive as-
sumption that morphology applies inside the lan-
guage’s lexicon but has no effect on the process that
put inflected lexemes together into sentences. In this
regard, our model is a minor variant on traditional n-
gram models that work with “opaque” word forms.
How to best relax this assumption in a computation-
ally tractable way is an important open question left
for future work.
</bodyText>
<sectionHeader confidence="0.992301" genericHeader="method">
5 Word Alignment Model
</sectionHeader>
<bodyText confidence="0.999952684210526">
Monolingual models of language are not the only
models that can benefit from taking into account
morphology. In fact, alignment models are a good
candidate for using richer word distributions: they
assume a target word distribution conditioned on
each source word. When the target language is mor-
phologically rich, classic independence assumptions
produce very weak models unless some kind of pre-
processing is applied to one side of the corpus. An
alternative is to use our unigram model as a word
translation distribution for each source word in the
corpus.
Our alignment model is based on a simple variant
of IBM Model 2 where the alignment distribution is
only controlled by two parameters, A and p0 (Dyer et
al., 2013). p0 is the probability of the null alignment.
For a source sentence f of length n, a target sentence
e of length m and a latent alignment a, we define the
following alignment link probabilities (j =� 0):
</bodyText>
<equation confidence="0.950377">
p(ai = j  |n, m) a (1 − p0) exp ~−A~a ~
m n
</equation>
<bodyText confidence="0.999230916666667">
A controls the flatness of this distribution: larger val-
ues make the probabilities more peaked around the
diagonal of the alignment matrix.
Each target word is then generated given a source
word and a latent alignment link from the word
translation distribution p(ei  |fa,,, Gw). Note that
this is effectively a unigram distribution over tar-
get words, albeit conditioned on the source word
fj. Here is where our model differs from classic
alignment models: the unigram distribution Gw is
assumed be generated from a PY process. There are
two choices for the base word distribution:
</bodyText>
<listItem confidence="0.8697778">
• As a baseline, we use a uniform base distribu-
tion over the target vocabulary: G0w = N(V).
• We define a stem distribution Gs[f] for each
source word f, a shared pattern distribution Gp,
and set G0w[f] = MP(Gs[f], Gp). In this case,
</listItem>
<page confidence="0.98489">
1211
</page>
<bodyText confidence="0.98716225">
we obtain the model depicted in Fig. 2. The
stem and the pattern models are also given PY
priors with uniform base distribution (Go _
U(S)).
Finally, we put uninformative priors on the align-
ment distribution parameters: po — Beta(α, Q) is
collapsed and A — Gamma(k, 0) is inferred using
Metropolis-Hastings.
</bodyText>
<figure confidence="0.809314333333333">
↵,0
G
G
</figure>
<figureCaption confidence="0.9993215">
Figure 2: Our alignment model, represented as a graphi-
cal model.
</figureCaption>
<subsectionHeader confidence="0.38591">
Experiments
</subsectionHeader>
<bodyText confidence="0.9991219375">
We evaluate the alignment error rate of our models
for two language pairs with rich morphology on the
target side. We compare to alignments inferred us-
ing IBM Model 4 trained with EM (Brown et al.,
1993),10 a version of our baseline model (described
above) without PY priors (learned using EM), and
the PY-based baseline. We consider two language
pairs.
English-Turkish We use a 2.8M word cleaned
version of the South-East European Times corpus
(Tyers and Alperen, 2010) and gold-standard align-
ments from Qakmak et al. (2012). Our morphologi-
cal analyzer is identical to the one used in the previ-
ous sections.
English-Czech We use the 1.3M word News
Commentary corpus and gold-standard alignments
</bodyText>
<footnote confidence="0.581623">
10We use the default GIZA++ stage training scheme:
Model 1 + HMM + Model 3 + Model 4.
</footnote>
<bodyText confidence="0.998089666666667">
from Bojar and Prokopová (2006). The morpholog-
ical analyzer is provided by Xerox.
Results Results are shown in Table 5. Our lightly
parameterized model performs much better than
IBM Model 4 in these small-data conditions. With
an identical model, we find PY priors outperform
traditional multinomial distributions. Adding mor-
phology further reduced the alignment error rate, for
both languages.
</bodyText>
<table confidence="0.998047833333333">
AER
Model en-tr en-cs
Model 4 52.1 34.5
EM 43.8 28.9
PY-U(V ) 39.2 25.7
PY-U(S) 33.8 24.8
</table>
<tableCaption confidence="0.985236">
Table 5: Word alignment experiments on English-Turkish
(en-tr) and English-Czech (en-cs) data.
</tableCaption>
<bodyText confidence="0.998652111111111">
As an example of how our model generalizes bet-
ter, consider the sentence pair in Fig. 3, taken from
the evaluation data. The two words composing the
Turkish sentence are not found elsewhere in the cor-
pus, but several related inflections occur.11 It is
therefore trivial for the stem-base model to find the
correct alignment (marked in black), while all the
other models have no evidence for it and choose an
arbitrary alignment (gray points).
</bodyText>
<figureCaption confidence="0.964603333333333">
Figure 3: A complex Turkish-English word alignment
(alignment points in gray: EM/PY-U(V); black: PY-
U(S)).
</figureCaption>
<sectionHeader confidence="0.999738" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.7481964">
Computational morphology has received consider-
able attention in NLP since the early work on two-
level morphology (Koskenniemi, 1984; Kaplan and
11ödevinin, ödevini, ödevleri; bitmez, bitirilece˘ginden,
bitmesiyle, ...
</bodyText>
<figure confidence="0.998087263157895">
f e
A p
a
d, ✓,,
G,,
d ,✓
d ,✓
G
G
I
was
not
able
to
finish
my
homework
šdevimi
bitiremedim
</figure>
<page confidence="0.993449">
1212
</page>
<bodyText confidence="0.999324478873239">
Kay, 1994). It is now widely accepted that finite-
state transducers have sufficient power to express
nearly all morphological phenomena, and the XFST
toolkit (Beesley and Karttunen, 2003) has con-
tributed to the practical adoption of this modeling
approach. Recently, open-source tools have been re-
leased: in this paper, we used Foma (Hulden, 2009)
to develop the Russian guesser.
Since some inflected forms have several possible
analyses, there has been a great deal of work on se-
lecting the intended one in context (Hakkani-Tür et
al., 2000; Hajiˇc et al., 2001; Habash and Rambow,
2005; Smith et al., 2005; Habash et al., 2009). Our
disambiguation model is closely related to genera-
tive models used for this purpose (Hakkani-Tür et
al., 2000).
Rule-based analysis is not the only approach
to modeling morphology, and many unsupervised
models have been proposed.12 Heuristic segmenta-
tion approaches based on the minimum description
length principle (Goldsmith, 2001; Creutz and La-
gus, 2007; de Marcken, 1996; Brent et al., 1995)
have been shown to be effective, and Bayesian
model-based versions have been proposed as well
(Goldwater et al., 2011; Snyder and Barzilay, 2008;
Snover and Brent, 2001). In §3, we suggested a third
way between rule-based approaches and fully un-
supervised learning that combines the best of both
worlds.
Morphological analysis or segmentation is crucial
to the performance of several applications: machine
translation (Goldwater and McClosky, 2005; Al-
Haj and Lavie, 2010; Oflazer and El-Kahlout, 2007;
Minkov et al., 2007; Habash and Sadat, 2006, in-
ter alia), automatic speech recognition (Creutz et al.,
2007), and syntactic parsing (Tsarfaty et al., 2010).
Several methods have been proposed to integrate
morphology into n-gram language models, includ-
ing factored language models (Bilmes and Kirch-
hoff, 2003), discriminative language modeling (Arı-
soy et al., 2008), and more heuristic approaches
(Monz, 2011).
Despite the fundamentally open nature of the lex-
icon (Heaps, 1978), there has been distressingly lit-
12Developing a high-coverage analyzer can be a time-
consuming process even with the simplicity of modern toolkits,
and unsupervised morphology learning is an attractive problem
for computational cognitive science.
tle attention to the general problem of open vocabu-
lary language modeling problem (most applications
make a closed-vocabulary assumption). The classic
exploration of open vocabulary language modeling
is Brown et al. (1992), which proposed the strategy
of interpolating between word- and character-based
models. Character-based language models are re-
viewed by Carpenter (2005). So-called hybrid mod-
els that model both words and sublexical units have
become popular in speech recognition (Shaik et al.,
2012; Parada et al., 2011; Bazzi, 2002). Open-
vocabulary language language modeling has also re-
cently been explored in the context of assistive tech-
nologies (Roark, 2009).
Finally, Pitman-Yor processes (PYPs) have be-
come widespread in natural language processing
since they are natural power-law generators. It has
been shown that the widely used modified Kneser-
Ney estimator (Chen and Goodman, 1998) for n-
gram language models is an approximation of the
posterior predictive distribution of a language model
with hierarchical PYP priors (Goldwater et al., 2011;
Teh, 2006).
</bodyText>
<sectionHeader confidence="0.998205" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999969294117647">
We described a generative model which makes use
of morphological analyzers to produce richer word
distributions through sharing of statistical strength
between stems. We have shown how it can be in-
tegrated into several models central to NLP appli-
cations and have empirically validated the effective-
ness of these changes. Although this paper mostly
focused on languages that are well studied and for
which high-quality analyzers are available, our mod-
els are especially relevant in low-resource scenarios
because they do not require disambiguated analyses.
In future work, we plan to apply these techniques to
languages such as Kinyarwanda, a resource-poor but
morphologically rich language spoken in Rwanda.
It is our belief that knowledge-rich models can help
bridge the gap between low- and high-resource lan-
guages.
</bodyText>
<sectionHeader confidence="0.998028" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.992813">
We thank Kemal Oflazer for making his Turkish lan-
guage morphological analyzer available to us and Bren-
dan O’Connor for gathering the Turkish tweets used in
</bodyText>
<page confidence="0.951126">
1213
</page>
<bodyText confidence="0.8304945">
the predictive text experiments. This work was spon-
sored by the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant number
W911NF-10-1-0533.
</bodyText>
<sectionHeader confidence="0.96944" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999622891089109">
H. Al-Haj and A. Lavie. 2010. The impact
of Arabic morphological segmentation on broad-
coverage English-to-Arabic statistical machine trans-
lation. Proc. of AMTA.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Implementation and Application of Au-
tomata, pages 11–23.
Ebru Arısoy, Brian Roark, Izhak Shafran, and Murat
Saraçlar. 2008. Discriminative n-gram language mod-
eling for Turkish. In Proc. of Interspeech.
Issam Bazzi. 2002. Modelling out-of-vocabulary words
for robust speech recognition. Ph.D. thesis, MIT.
K.R. Beesley and L. Karttunen. 2003. Finite-state mor-
phology: Xerox tools and techniques. CSLI, Stanford.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
Proc. of NAACL.
Ondˇrej Bojar and Magdalena Prokopová. 2006. Czech-
English word alignment. In Proc. of LREC.
Michael R. Brent, Sreerama K. Murthy, and Andrew
Lundberg. 1995. Discovering morphemic suffixes: A
case study in MDL induction. In Proceedings of the
Fifth International Workshop on Artificial Intelligence
and Statistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, Robert L. Mercer, and Jennifer C. Lai.
1992. An estimate of an upper bound for the entropy
of English. Computational Linguistics, 18(1):31–40.
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263–311.
Bob Carpenter. 2005. Scaling high-order character lan-
guage models to gigabytes. In Proceedings of the ACL
Workshop on Software.
Mauro Cettolo, Christian Girardi, and Marcello Federico.
2012. WIT3: Web inventory of transcribed and trans-
lated talks. In Proc. of EAMT.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Harvard University.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1).
M. Creutz, T. Hirsimäki, M. Kurimo, A. Puurula,
J. Pylkkönen, V. Siivola, M. Varjokallio, E. Arisoy,
M. Saraçlar, and A. Stolcke. 2007. Morph-based
speech recognition and modeling of out-of-vocabulary
words across languages. ACM Transactions on Speech
and Language Processing, 5(1):3.
Carl G. de Marcken. 1996. Unsupervised Language Ac-
quisition. Ph.D. thesis, MIT.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameterization
of IBM Model 2. In Proc. of NAACL.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2):153–198.
S. Goldwater and D. McClosky. 2005. Improving statis-
tical MT through morphological analysis. In Proc. of
EMNLP.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2011. Producing power-law distributions and
damping word frequencies with two-stage language
models. Journal of Machine Learning Research,
12:2335–2382.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging, and morphological
disambiguation in one fell swoop. In Proc. of ACL.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proc. of NAACL.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Proceedings
of the Second International Conference on Arabic Lan-
guage Resources and Tools.
Jan Hajiˇc, P. Krbec, P. Kvˇetoˇn, K. Oliva, and V. Petroviˇc.
2001. Serial combination of rules and statistics. In
Proc. ofACL.
D. Z. Hakkani-Tür, Kemal Oflazer, and G. Tür. 2000.
Statistical morphological disambiguation for aggluti-
native languages. In Proc. of COLING.
Harold Stanley Heaps. 1978. Information Retrieval:
Computational and Theoretical Aspects. Academic
Press.
M. Hulden. 2009. Foma: a finite-state compiler and li-
brary. In Proc. of EACL.
Ronald M. Kaplan and Martin Kay. 1994. Regular mod-
els of phonological rule systems. Computational Lin-
guistics, 20(3):331–378.
Lauri Karttunen and Kenneth R. Beesley. 2005. Twenty-
five years of finite-state morphology. In Inquiries into
Words, Constraints and Contexts, pages 71–83. CSLI.
Kimmo Koskenniemi. 1984. A general computational
model for word-form recognition and production. In
Proc. ofACL-COLING.
</reference>
<page confidence="0.834782">
1214
</page>
<reference confidence="0.999953242424242">
O. Lya&amp;quot;sevskaya, I. Astaf’yeva, A. Bonch-Osmolovskaya,
A. Garej&amp;quot;sina, Y. Gri&amp;quot;sina, V. D’yaˇckov, M. Ionov,
A. Koroleva, M. Kudrinskij, A. Lityagina, Y. Luˇcina,
Y. Sidorova, S. Toldova, S. Savˇcuk, and S. Ko-
val’. 2010. Ocenka metodov avtomatiˇceskogo
analiza teksta: morfologiˇceskie parseri russkogo
yazyka. Komp’juternaya lingvistika i intellektual’nye
texnologii (Computational linguistics and intellectual
technologies).
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proc. of ACL.
Christof Monz. 2011. Statistical machine translation
with local language models. In Proc. of EMNLP.
Kemal Oflazer and ˙Ilknur Durgar El-Kahlout. 2007. Ex-
ploring different representational units in English-to-
Turkish statistical machine translation. In Proc. of
StatMT.
K. Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Computing,
9(2):137–148.
Carolina Parada, Mark Dredze, Abhinav Sethy, and Ariya
Rastrow. 2011. Learning sub-word units for open vo-
cabulary speech recognition. In Proc. of ACL.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25(2):855–90.
Brian Roark. 2009. Open vocabulary language model-
ing for binary response typing interfaces. Technical
Report CSLU-09-001, Oregon Health &amp; Science Uni-
versity.
M. Ali Basha Shaik, David Rybach, Stefan Hahn, Ralf
Schluüter, and Hermann Ney. 2012. Hierarchical hy-
brid language models for open vocabulary continuous
speech recognition using wfst. In Proc. of SAPA.
M. Silfverberg, K. Lindén, and M. Hyvärinen. 2012.
Predictive text entry for agglutinative languages using
unsupervised morphological segmentation. In Proc.
of Computational Linguistics and Intelligent Text Pro-
cessing.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proc. of EMNLP.
Matt G. Snover and Michael R. Brent. 2001. A Bayesian
model for morpheme and paradigm identification. In
Proc. ofACL.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proc. of ACL.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL.
Reut Tsarfaty, Djamé Seddah, Yoav Goldberg, Sandra
Kübler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing of morphologically rich languages: What, how
and whither. In Proc. of Workshop on Statistical Pars-
ing of Morphologically Rich Languages.
F. Tyers and M.S. Alperen. 2010. South-east european
times: A parallel corpus of Balkan languages. In
Proceedings of the LREC workshop on Exploitation
of multilingual resources and tools for Central and
(South) Eastern European Languages.
M. Talha Qakmak, Süleyman Acar, and Gül¸sen Eryi˘git.
2012. Word alignment for English-Turkish language
pair. In Proc. of LREC.
</reference>
<page confidence="0.991105">
1215
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.646861">
<title confidence="0.999652">Knowledge-Rich Morphological Priors for Bayesian Language Models</title>
<author confidence="0.985707">Victor Chahuneau Noah A Smith Chris</author>
<affiliation confidence="0.842573">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.958065">Pittsburgh, PA 15213,</address>
<email confidence="0.998987">vchahune@cs.cmu.edu</email>
<email confidence="0.998987">nasmith@cs.cmu.edu</email>
<email confidence="0.998987">cdyer@cs.cmu.edu</email>
<abstract confidence="0.9988871875">We present a morphology-aware nonparametric Bayesian model of language whose prior distribution uses manually constructed finitestate transducers to capture the word formation processes of particular languages. This relaxes the word independence assumption and enables sharing of statistical strength across, for example, stems or inflectional paradigms in different contexts. Our model can be used in virtually any scenario where multinomial distributions over words would be used. We obtain state-of-the-art results in language modeling, word alignment, and unsupervised morphological disambiguation for a variety of morphologically rich languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Al-Haj</author>
<author>A Lavie</author>
</authors>
<title>The impact of Arabic morphological segmentation on broadcoverage English-to-Arabic statistical machine translation.</title>
<date>2010</date>
<booktitle>Proc. of AMTA.</booktitle>
<marker>Al-Haj, Lavie, 2010</marker>
<rawString>H. Al-Haj and A. Lavie. 2010. The impact of Arabic morphological segmentation on broadcoverage English-to-Arabic statistical machine translation. Proc. of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>OpenFst: A general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In Implementation and Application of Automata,</booktitle>
<pages>11--23</pages>
<contexts>
<context position="18620" citStr="Allauzen et al., 2007" startWordPosition="3011" endWordPosition="3014">(2012) describe a system designed for this task in Finnish, which is composed of a weighted finite-state morphological analyzer trained on IRC logs. However, their system is restricted to words that are encoded in the analyzer’s lexicon and does not use context for disambiguation. In our experiments, we use the same Turkish TED talks corpus as the previous section. As a baseline, we use a trigram character language model. We produce a character lattice which encodes all the possible interpretations for a word and compose it with a finite-state representation of the character LM using OpenFST (Allauzen et al., 2007). Alternatively, we can use a unigram word model to decode this lattice, backing off to the character language model if no solution is found. Finally, to be able to make use of word context, we can extract the k most likely paths according to the character LM and produce a word lattice, which is in turn decoded with a language model defined over the extracted vocabulary. Model WER CER Character LM 48.37 16.72 1-gram + character LM 8.50 3.28 1-gram MP2 6.46 2.37 3-gram + character LM 7.86 3.07 3-gram MP2 5.73 2.15 Table 4: Evaluation of Turkish predictive text input. We measure word and charact</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A general and efficient weighted finite-state transducer library. In Implementation and Application of Automata, pages 11–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ebru Arısoy</author>
<author>Brian Roark</author>
<author>Izhak Shafran</author>
<author>Murat Saraçlar</author>
</authors>
<title>Discriminative n-gram language modeling for Turkish.</title>
<date>2008</date>
<booktitle>In Proc. of Interspeech.</booktitle>
<marker>Arısoy, Roark, Shafran, Saraçlar, 2008</marker>
<rawString>Ebru Arısoy, Brian Roark, Izhak Shafran, and Murat Saraçlar. 2008. Discriminative n-gram language modeling for Turkish. In Proc. of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Issam Bazzi</author>
</authors>
<title>Modelling out-of-vocabulary words for robust speech recognition.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<contexts>
<context position="27427" citStr="Bazzi, 2002" startWordPosition="4457" endWordPosition="4458"> learning is an attractive problem for computational cognitive science. tle attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- and character-based models. Character-based language models are reviewed by Carpenter (2005). So-called hybrid models that model both words and sublexical units have become popular in speech recognition (Shaik et al., 2012; Parada et al., 2011; Bazzi, 2002). Openvocabulary language language modeling has also recently been explored in the context of assistive technologies (Roark, 2009). Finally, Pitman-Yor processes (PYPs) have become widespread in natural language processing since they are natural power-law generators. It has been shown that the widely used modified KneserNey estimator (Chen and Goodman, 1998) for ngram language models is an approximation of the posterior predictive distribution of a language model with hierarchical PYP priors (Goldwater et al., 2011; Teh, 2006). 7 Conclusion We described a generative model which makes use of mo</context>
</contexts>
<marker>Bazzi, 2002</marker>
<rawString>Issam Bazzi. 2002. Modelling out-of-vocabulary words for robust speech recognition. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R Beesley</author>
<author>L Karttunen</author>
</authors>
<title>Finite-state morphology: Xerox tools and techniques.</title>
<date>2003</date>
<publisher>CSLI, Stanford.</publisher>
<contexts>
<context position="24817" citStr="Beesley and Karttunen, 2003" startWordPosition="4059" endWordPosition="4062">ints). Figure 3: A complex Turkish-English word alignment (alignment points in gray: EM/PY-U(V); black: PYU(S)). 6 Related Work Computational morphology has received considerable attention in NLP since the early work on twolevel morphology (Koskenniemi, 1984; Kaplan and 11ödevinin, ödevini, ödevleri; bitmez, bitirilece˘ginden, bitmesiyle, ... f e A p a d, ✓,, G,, d ,✓ d ,✓ G G I was not able to finish my homework šdevimi bitiremedim 1212 Kay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only appro</context>
</contexts>
<marker>Beesley, Karttunen, 2003</marker>
<rawString>K.R. Beesley and L. Karttunen. 2003. Finite-state morphology: Xerox tools and techniques. CSLI, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff A Bilmes</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="26470" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="4316" endWordPosition="4320">ggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arı- soy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly lit12Developing a high-coverage analyzer can be a timeconsuming process even with the simplicity of modern toolkits, and unsupervised morphology learning is an attractive problem for computational cognitive science. tle attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary l</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored language models and generalized parallel backoff. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Magdalena Prokopová</author>
</authors>
<title>CzechEnglish word alignment.</title>
<date>2006</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="23204" citStr="Bojar and Prokopová (2006)" startWordPosition="3801" endWordPosition="3804">wn et al., 1993),10 a version of our baseline model (described above) without PY priors (learned using EM), and the PY-based baseline. We consider two language pairs. English-Turkish We use a 2.8M word cleaned version of the South-East European Times corpus (Tyers and Alperen, 2010) and gold-standard alignments from Qakmak et al. (2012). Our morphological analyzer is identical to the one used in the previous sections. English-Czech We use the 1.3M word News Commentary corpus and gold-standard alignments 10We use the default GIZA++ stage training scheme: Model 1 + HMM + Model 3 + Model 4. from Bojar and Prokopová (2006). The morphological analyzer is provided by Xerox. Results Results are shown in Table 5. Our lightly parameterized model performs much better than IBM Model 4 in these small-data conditions. With an identical model, we find PY priors outperform traditional multinomial distributions. Adding morphology further reduced the alignment error rate, for both languages. AER Model en-tr en-cs Model 4 52.1 34.5 EM 43.8 28.9 PY-U(V ) 39.2 25.7 PY-U(S) 33.8 24.8 Table 5: Word alignment experiments on English-Turkish (en-tr) and English-Czech (en-cs) data. As an example of how our model generalizes better, </context>
</contexts>
<marker>Bojar, Prokopová, 2006</marker>
<rawString>Ondˇrej Bojar and Magdalena Prokopová. 2006. CzechEnglish word alignment. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
<author>Sreerama K Murthy</author>
<author>Andrew Lundberg</author>
</authors>
<title>Discovering morphemic suffixes: A case study in MDL induction.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics.</booktitle>
<contexts>
<context position="25659" citStr="Brent et al., 1995" startWordPosition="4194" endWordPosition="4197">e several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et</context>
</contexts>
<marker>Brent, Murthy, Lundberg, 1995</marker>
<rawString>Michael R. Brent, Sreerama K. Murthy, and Andrew Lundberg. 1995. Discovering morphemic suffixes: A case study in MDL induction. In Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
<author>Jennifer C Lai</author>
</authors>
<title>An estimate of an upper bound for the entropy of English.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="27109" citStr="Brown et al. (1992)" startWordPosition="4408" endWordPosition="4411">guage modeling (Arı- soy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly lit12Developing a high-coverage analyzer can be a timeconsuming process even with the simplicity of modern toolkits, and unsupervised morphology learning is an attractive problem for computational cognitive science. tle attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- and character-based models. Character-based language models are reviewed by Carpenter (2005). So-called hybrid models that model both words and sublexical units have become popular in speech recognition (Shaik et al., 2012; Parada et al., 2011; Bazzi, 2002). Openvocabulary language language modeling has also recently been explored in the context of assistive technologies (Roark, 2009). Finally, Pitman-Yor processes (PYPs) have become widespread in natural language processing since they are natural power-law generators. It has been sh</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, Lai, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, Robert L. Mercer, and Jennifer C. Lai. 1992. An estimate of an upper bound for the entropy of English. Computational Linguistics, 18(1):31–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J D Pietra</author>
<author>S A D Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="22594" citStr="Brown et al., 1993" startWordPosition="3701" endWordPosition="3704">p). In this case, 1211 we obtain the model depicted in Fig. 2. The stem and the pattern models are also given PY priors with uniform base distribution (Go _ U(S)). Finally, we put uninformative priors on the alignment distribution parameters: po — Beta(α, Q) is collapsed and A — Gamma(k, 0) is inferred using Metropolis-Hastings. ↵,0 G G Figure 2: Our alignment model, represented as a graphical model. Experiments We evaluate the alignment error rate of our models for two language pairs with rich morphology on the target side. We compare to alignments inferred using IBM Model 4 trained with EM (Brown et al., 1993),10 a version of our baseline model (described above) without PY priors (learned using EM), and the PY-based baseline. We consider two language pairs. English-Turkish We use a 2.8M word cleaned version of the South-East European Times corpus (Tyers and Alperen, 2010) and gold-standard alignments from Qakmak et al. (2012). Our morphological analyzer is identical to the one used in the previous sections. English-Czech We use the 1.3M word News Commentary corpus and gold-standard alignments 10We use the default GIZA++ stage training scheme: Model 1 + HMM + Model 3 + Model 4. from Bojar and Prokop</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>Scaling high-order character language models to gigabytes.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Software.</booktitle>
<contexts>
<context position="27262" citStr="Carpenter (2005)" startWordPosition="4430" endWordPosition="4431">e has been distressingly lit12Developing a high-coverage analyzer can be a timeconsuming process even with the simplicity of modern toolkits, and unsupervised morphology learning is an attractive problem for computational cognitive science. tle attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- and character-based models. Character-based language models are reviewed by Carpenter (2005). So-called hybrid models that model both words and sublexical units have become popular in speech recognition (Shaik et al., 2012; Parada et al., 2011; Bazzi, 2002). Openvocabulary language language modeling has also recently been explored in the context of assistive technologies (Roark, 2009). Finally, Pitman-Yor processes (PYPs) have become widespread in natural language processing since they are natural power-law generators. It has been shown that the widely used modified KneserNey estimator (Chen and Goodman, 1998) for ngram language models is an approximation of the posterior predictive </context>
</contexts>
<marker>Carpenter, 2005</marker>
<rawString>Bob Carpenter. 2005. Scaling high-order character language models to gigabytes. In Proceedings of the ACL Workshop on Software.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Christian Girardi</author>
<author>Marcello Federico</author>
</authors>
<title>WIT3: Web inventory of transcribed and translated talks.</title>
<date>2012</date>
<booktitle>In Proc. of EAMT.</booktitle>
<contexts>
<context position="12470" citStr="Cettolo et al., 2012" startWordPosition="1957" endWordPosition="1960">what we try to do by designing a lexicon-free analyzer for Russian. A guesser was developed in three hours; it is prone to over-generation and produces ambiguous analyses for most words but covers a large number of morphological phenomena (gender, case, tense, etc.). For example, the word HspHTe5 can be correctly analyzed as HspHT+Noun+Masc+Prep+Sg but also as the incorrect forms: HspHTb+Verb+Pres+2P+Pl, HspHTa+Noun+Fem+Dat+Sg, HspHTx+Noun+Fem+Prep+Sg, and more. 3.2 Disambiguation Experiments We train the unigram model on a 1.7M-word corpus of TED talks transcriptions translated into Russian (Cettolo et al., 2012) and evaluate our analyzer against a test set consisting of 1,500 goldstandard analyses obtained from the morphology disambiguation task of the DIALOG 2010 conference (Lyaševskaya et al., 2010).6 Each analysis is composed of a lemma (HspHT), a part of speech (Noun), and a sequence of additional functional morphemes (Masc,Prep,Sg). We consider only open-class categories: nouns, ad5xspxTe = Hebrew (masculine noun, prepositional case) 6http://ru-eval.ru jectives, adverbs and verbs, and evaluate the output of our model with three metrics: the lemma accuracy, the part-of-speech accuracy, and the mo</context>
</contexts>
<marker>Cettolo, Girardi, Federico, 2012</marker>
<rawString>Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. WIT3: Web inventory of transcribed and translated talks. In Proc. of EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="27787" citStr="Chen and Goodman, 1998" startWordPosition="4509" endWordPosition="4512">word- and character-based models. Character-based language models are reviewed by Carpenter (2005). So-called hybrid models that model both words and sublexical units have become popular in speech recognition (Shaik et al., 2012; Parada et al., 2011; Bazzi, 2002). Openvocabulary language language modeling has also recently been explored in the context of assistive technologies (Roark, 2009). Finally, Pitman-Yor processes (PYPs) have become widespread in natural language processing since they are natural power-law generators. It has been shown that the widely used modified KneserNey estimator (Chen and Goodman, 1998) for ngram language models is an approximation of the posterior predictive distribution of a language model with hierarchical PYP priors (Goldwater et al., 2011; Teh, 2006). 7 Conclusion We described a generative model which makes use of morphological analyzers to produce richer word distributions through sharing of statistical strength between stems. We have shown how it can be integrated into several models central to NLP applications and have empirically validated the effectiveness of these changes. Although this paper mostly focused on languages that are well studied and for which high-qua</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="25620" citStr="Creutz and Lagus, 2007" startWordPosition="4186" endWordPosition="4190">an guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), </context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Transactions on Speech and Language Processing, 4(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Creutz</author>
<author>T Hirsimäki</author>
<author>M Kurimo</author>
<author>A Puurula</author>
<author>J Pylkkönen</author>
<author>V Siivola</author>
<author>M Varjokallio</author>
<author>E Arisoy</author>
<author>M Saraçlar</author>
<author>A Stolcke</author>
</authors>
<title>Morph-based speech recognition and modeling of out-of-vocabulary words across languages.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="26270" citStr="Creutz et al., 2007" startWordPosition="4288" endWordPosition="4291">l., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arı- soy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly lit12Developing a high-coverage analyzer can be a timeconsuming process even with the simplicity of modern toolkits, and unsupervised morphology learning is an attractive problem for computational co</context>
</contexts>
<marker>Creutz, Hirsimäki, Kurimo, Puurula, Pylkkönen, Siivola, Varjokallio, Arisoy, Saraçlar, Stolcke, 2007</marker>
<rawString>M. Creutz, T. Hirsimäki, M. Kurimo, A. Puurula, J. Pylkkönen, V. Siivola, M. Varjokallio, E. Arisoy, M. Saraçlar, and A. Stolcke. 2007. Morph-based speech recognition and modeling of out-of-vocabulary words across languages. ACM Transactions on Speech and Language Processing, 5(1):3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl G de Marcken</author>
</authors>
<title>Unsupervised Language Acquisition.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<marker>de Marcken, 1996</marker>
<rawString>Carl G. de Marcken. 1996. Unsupervised Language Acquisition. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<title>A simple, fast, and effective reparameterization of IBM Model 2.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="20930" citStr="Dyer et al., 2013" startWordPosition="3405" endWordPosition="3408">ogy. In fact, alignment models are a good candidate for using richer word distributions: they assume a target word distribution conditioned on each source word. When the target language is morphologically rich, classic independence assumptions produce very weak models unless some kind of preprocessing is applied to one side of the corpus. An alternative is to use our unigram model as a word translation distribution for each source word in the corpus. Our alignment model is based on a simple variant of IBM Model 2 where the alignment distribution is only controlled by two parameters, A and p0 (Dyer et al., 2013). p0 is the probability of the null alignment. For a source sentence f of length n, a target sentence e of length m and a latent alignment a, we define the following alignment link probabilities (j =� 0): p(ai = j |n, m) a (1 − p0) exp ~−A~a ~ m n A controls the flatness of this distribution: larger values make the probabilities more peaked around the diagonal of the alignment matrix. Each target word is then generated given a source word and a latent alignment link from the word translation distribution p(ei |fa,,, Gw). Note that this is effectively a unigram distribution over target words, a</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameterization of IBM Model 2. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="25596" citStr="Goldsmith, 2001" startWordPosition="4184" endWordPosition="4185">develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sa</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>J. Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>D McClosky</author>
</authors>
<title>Improving statistical MT through morphological analysis.</title>
<date>2005</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="26107" citStr="Goldwater and McClosky, 2005" startWordPosition="4261" endWordPosition="4264">been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arı- soy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly lit12Developing a high-coverage anal</context>
</contexts>
<marker>Goldwater, McClosky, 2005</marker>
<rawString>S. Goldwater and D. McClosky. 2005. Improving statistical MT through morphological analysis. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Producing power-law distributions and damping word frequencies with two-stage language models.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2335</pages>
<contexts>
<context position="25777" citStr="Goldwater et al., 2011" startWordPosition="4213" endWordPosition="4216">Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology</context>
<context position="27947" citStr="Goldwater et al., 2011" startWordPosition="4534" endWordPosition="4537">units have become popular in speech recognition (Shaik et al., 2012; Parada et al., 2011; Bazzi, 2002). Openvocabulary language language modeling has also recently been explored in the context of assistive technologies (Roark, 2009). Finally, Pitman-Yor processes (PYPs) have become widespread in natural language processing since they are natural power-law generators. It has been shown that the widely used modified KneserNey estimator (Chen and Goodman, 1998) for ngram language models is an approximation of the posterior predictive distribution of a language model with hierarchical PYP priors (Goldwater et al., 2011; Teh, 2006). 7 Conclusion We described a generative model which makes use of morphological analyzers to produce richer word distributions through sharing of statistical strength between stems. We have shown how it can be integrated into several models central to NLP applications and have empirically validated the effectiveness of these changes. Although this paper mostly focused on languages that are well studied and for which high-quality analyzers are available, our models are especially relevant in low-resource scenarios because they do not require disambiguated analyses. In future work, w</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2011</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2011. Producing power-law distributions and damping word frequencies with two-stage language models. Journal of Machine Learning Research, 12:2335–2382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Arabic tokenization, part-of-speech tagging, and morphological disambiguation in one fell swoop.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="25217" citStr="Habash and Rambow, 2005" startWordPosition="4126" endWordPosition="4129">my homework šdevimi bitiremedim 1212 Kay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and </context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>Nizar Habash and Owen Rambow. 2005. Arabic tokenization, part-of-speech tagging, and morphological disambiguation in one fell swoop. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Fatiha Sadat</author>
</authors>
<title>Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="1576" citStr="Habash and Sadat, 2006" startWordPosition="212" endWordPosition="215"> human languages, many NLP systems treat fully inflected forms as the atomic units of language. By assuming independence of lexical stems’ various surface forms, this avoidance approach exacerbates the problem of data sparseness. If it is employed at all, morphological analysis of text tends to be treated as a preprocessing step to other NLP modules. While this latter disambiguation approach helps address data sparsity concerns, it has substantial drawbacks: it requires supervised learning from expert-annotated corpora, and determining the optimal morphological granularity is labor-intensive (Habash and Sadat, 2006). Neither approach fully exploits the finite-state transducer (FST) technology that has been so successful for modeling the mapping between surface forms and their morphological analyses (Karttunen and Beesley, 2005), and the mature collections of high quality transducers that already exist for many languages (e.g., Turkish, Russian, Arabic). Much linguistic knowledge is encoded in such FSTs. In this paper, we develop morphology-aware nonparametric Bayesian language models that bring together hand-written FSTs with statistical modeling and require no token-level annotation. The sparsity issue </context>
<context position="26205" citStr="Habash and Sadat, 2006" startWordPosition="4278" endWordPosition="4281">ldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arı- soy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly lit12Developing a high-coverage analyzer can be a timeconsuming process even with the simplicity of modern toolkits, and unsupervised </context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>Nizar Habash and Fatiha Sadat. 2006. Arabic preprocessing schemes for statistical machine translation. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Ryan Roth</author>
</authors>
<title>MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second International Conference on Arabic Language Resources and Tools.</booktitle>
<contexts>
<context position="25259" citStr="Habash et al., 2009" startWordPosition="4134" endWordPosition="4137">4). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third </context>
</contexts>
<marker>Habash, Rambow, Roth, 2009</marker>
<rawString>Nizar Habash, Owen Rambow, and Ryan Roth. 2009. MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization. In Proceedings of the Second International Conference on Arabic Language Resources and Tools.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>P Krbec</author>
<author>P Kvˇetoˇn</author>
<author>K Oliva</author>
<author>V Petroviˇc</author>
</authors>
<title>Serial combination of rules and statistics.</title>
<date>2001</date>
<booktitle>In Proc. ofACL.</booktitle>
<marker>Hajiˇc, Krbec, Kvˇetoˇn, Oliva, Petroviˇc, 2001</marker>
<rawString>Jan Hajiˇc, P. Krbec, P. Kvˇetoˇn, K. Oliva, and V. Petroviˇc. 2001. Serial combination of rules and statistics. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Z Hakkani-Tür</author>
<author>Kemal Oflazer</author>
<author>G Tür</author>
</authors>
<title>Statistical morphological disambiguation for agglutinative languages. In</title>
<date>2000</date>
<booktitle>Proc. of COLING.</booktitle>
<contexts>
<context position="25171" citStr="Hakkani-Tür et al., 2000" startWordPosition="4118" endWordPosition="4121">✓,, G,, d ,✓ d ,✓ G G I was not able to finish my homework šdevimi bitiremedim 1212 Kay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al.</context>
</contexts>
<marker>Hakkani-Tür, Oflazer, Tür, 2000</marker>
<rawString>D. Z. Hakkani-Tür, Kemal Oflazer, and G. Tür. 2000. Statistical morphological disambiguation for agglutinative languages. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Stanley Heaps</author>
</authors>
<title>Information Retrieval: Computational and Theoretical Aspects.</title>
<date>1978</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="26640" citStr="Heaps, 1978" startWordPosition="4344" endWordPosition="4345">ance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arı- soy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly lit12Developing a high-coverage analyzer can be a timeconsuming process even with the simplicity of modern toolkits, and unsupervised morphology learning is an attractive problem for computational cognitive science. tle attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- and character-based models. Character-based language models are review</context>
</contexts>
<marker>Heaps, 1978</marker>
<rawString>Harold Stanley Heaps. 1978. Information Retrieval: Computational and Theoretical Aspects. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hulden</author>
</authors>
<title>Foma: a finite-state compiler and library.</title>
<date>2009</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="24977" citStr="Hulden, 2009" startWordPosition="4087" endWordPosition="4088">ble attention in NLP since the early work on twolevel morphology (Koskenniemi, 1984; Kaplan and 11ödevinin, ödevini, ödevleri; bitmez, bitirilece˘ginden, bitmesiyle, ... f e A p a d, ✓,, G,, d ,✓ d ,✓ G G I was not able to finish my homework šdevimi bitiremedim 1212 Kay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length princip</context>
</contexts>
<marker>Hulden, 2009</marker>
<rawString>M. Hulden. 2009. Foma: a finite-state compiler and library. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Martin Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>3</issue>
<marker>Kaplan, Kay, 1994</marker>
<rawString>Ronald M. Kaplan and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3):331–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Kenneth R Beesley</author>
</authors>
<title>Twentyfive years of finite-state morphology.</title>
<date>2005</date>
<booktitle>In Inquiries into Words, Constraints and Contexts,</booktitle>
<pages>71--83</pages>
<publisher>CSLI.</publisher>
<contexts>
<context position="1792" citStr="Karttunen and Beesley, 2005" startWordPosition="242" endWordPosition="245">f data sparseness. If it is employed at all, morphological analysis of text tends to be treated as a preprocessing step to other NLP modules. While this latter disambiguation approach helps address data sparsity concerns, it has substantial drawbacks: it requires supervised learning from expert-annotated corpora, and determining the optimal morphological granularity is labor-intensive (Habash and Sadat, 2006). Neither approach fully exploits the finite-state transducer (FST) technology that has been so successful for modeling the mapping between surface forms and their morphological analyses (Karttunen and Beesley, 2005), and the mature collections of high quality transducers that already exist for many languages (e.g., Turkish, Russian, Arabic). Much linguistic knowledge is encoded in such FSTs. In this paper, we develop morphology-aware nonparametric Bayesian language models that bring together hand-written FSTs with statistical modeling and require no token-level annotation. The sparsity issue discussed above is addressed by hierarchical priors that share statistical strength across different inflections of the same stem by backing off to word formation models that piece together morphemes using FSTs. Furt</context>
</contexts>
<marker>Karttunen, Beesley, 2005</marker>
<rawString>Lauri Karttunen and Kenneth R. Beesley. 2005. Twentyfive years of finite-state morphology. In Inquiries into Words, Constraints and Contexts, pages 71–83. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>A general computational model for word-form recognition and production.</title>
<date>1984</date>
<booktitle>In Proc. ofACL-COLING.</booktitle>
<contexts>
<context position="24447" citStr="Koskenniemi, 1984" startWordPosition="3999" endWordPosition="4000"> in Fig. 3, taken from the evaluation data. The two words composing the Turkish sentence are not found elsewhere in the corpus, but several related inflections occur.11 It is therefore trivial for the stem-base model to find the correct alignment (marked in black), while all the other models have no evidence for it and choose an arbitrary alignment (gray points). Figure 3: A complex Turkish-English word alignment (alignment points in gray: EM/PY-U(V); black: PYU(S)). 6 Related Work Computational morphology has received considerable attention in NLP since the early work on twolevel morphology (Koskenniemi, 1984; Kaplan and 11ödevinin, ödevini, ödevleri; bitmez, bitirilece˘ginden, bitmesiyle, ... f e A p a d, ✓,, G,, d ,✓ d ,✓ G G I was not able to finish my homework šdevimi bitiremedim 1212 Kay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have sever</context>
</contexts>
<marker>Koskenniemi, 1984</marker>
<rawString>Kimmo Koskenniemi. 1984. A general computational model for word-form recognition and production. In Proc. ofACL-COLING.</rawString>
</citation>
<citation valid="false">
<authors>
<author>O Lyasevskaya</author>
<author>I Astaf’yeva</author>
<author>A Bonch-Osmolovskaya</author>
<author>A Garejsina</author>
<author>Y Grisina</author>
<author>V D’yaˇckov</author>
<author>M Ionov</author>
<author>A Koroleva</author>
<author>M Kudrinskij</author>
<author>A Lityagina</author>
<author>Y Luˇcina</author>
<author>Y Sidorova</author>
<author>S Toldova</author>
<author>S Savˇcuk</author>
<author>S Koval’</author>
</authors>
<title>Ocenka metodov avtomatiˇceskogo analiza teksta: morfologiˇceskie parseri russkogo yazyka. Komp’juternaya lingvistika i intellektual’nye texnologii (Computational linguistics and intellectual technologies).</title>
<date>2010</date>
<marker>Lyasevskaya, Astaf’yeva, Bonch-Osmolovskaya, Garejsina, Grisina, D’yaˇckov, Ionov, Koroleva, Kudrinskij, Lityagina, Luˇcina, Sidorova, Toldova, Savˇcuk, Koval’, 2010</marker>
<rawString>O. Lya&amp;quot;sevskaya, I. Astaf’yeva, A. Bonch-Osmolovskaya, A. Garej&amp;quot;sina, Y. Gri&amp;quot;sina, V. D’yaˇckov, M. Ionov, A. Koroleva, M. Kudrinskij, A. Lityagina, Y. Luˇcina, Y. Sidorova, S. Toldova, S. Savˇcuk, and S. Koval’. 2010. Ocenka metodov avtomatiˇceskogo analiza teksta: morfologiˇceskie parseri russkogo yazyka. Komp’juternaya lingvistika i intellektual’nye texnologii (Computational linguistics and intellectual technologies).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>Kristina Toutanova</author>
<author>Hisami Suzuki</author>
</authors>
<title>Generating complex morphology for machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="26181" citStr="Minkov et al., 2007" startWordPosition="4274" endWordPosition="4277"> length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arı- soy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly lit12Developing a high-coverage analyzer can be a timeconsuming process even with the simplicity of modern too</context>
</contexts>
<marker>Minkov, Toutanova, Suzuki, 2007</marker>
<rawString>Einat Minkov, Kristina Toutanova, and Hisami Suzuki. 2007. Generating complex morphology for machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christof Monz</author>
</authors>
<title>Statistical machine translation with local language models.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="26572" citStr="Monz, 2011" startWordPosition="4333" endWordPosition="4334">s. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arı- soy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly lit12Developing a high-coverage analyzer can be a timeconsuming process even with the simplicity of modern toolkits, and unsupervised morphology learning is an attractive problem for computational cognitive science. tle attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- an</context>
</contexts>
<marker>Monz, 2011</marker>
<rawString>Christof Monz. 2011. Statistical machine translation with local language models. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
<author>˙Ilknur Durgar El-Kahlout</author>
</authors>
<title>Exploring different representational units in English-toTurkish statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of StatMT.</booktitle>
<contexts>
<context position="26160" citStr="Oflazer and El-Kahlout, 2007" startWordPosition="4270" endWordPosition="4273">sed on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arı- soy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly lit12Developing a high-coverage analyzer can be a timeconsuming process even with the sim</context>
</contexts>
<marker>Oflazer, El-Kahlout, 2007</marker>
<rawString>Kemal Oflazer and ˙Ilknur Durgar El-Kahlout. 2007. Exploring different representational units in English-toTurkish statistical machine translation. In Proc. of StatMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oflazer</author>
</authors>
<title>Two-level description of Turkish morphology.</title>
<date>1994</date>
<journal>Literary and Linguistic Computing,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="16789" citStr="Oflazer, 1994" startWordPosition="2688" endWordPosition="2689">g its base probability according to the MP. 4.1 Language Modeling Experiments We train several trigram models on the Russian TED talks corpus used in the previous section. Our baseline is a hierarchical PY trigram model with a trigram character model as the base word distribution. We compare it with our model using the same character model for the base stem distribution. Both of the morphological analyzers described in the previous section help obtaining perplexity reductions (Table 2). We ran a similar experiment on the Turkish version of this corpus (1.6M words) with a highquality analyzer (Oflazer, 1994) and obtain even larger gains (Table 3). Model ppl PY-character LM 563 Guesser MP2 530 Xerox MP2 522 Table 2: Evaluation of the Russian n-gram model. Model ppl PY-character LM 1,640 Oflazer MP2 1,292 Table 3: Evaluation of the Turkish n-gram model. These results can partly be attributed to the high OOV rate in these conditions: 4% for the Russian corpus and 6% for the Turkish corpus. 4.2 Predictive Text Input It is difficult to know whether a decrease in perplexity, as measured in the previous section, will result in a performance improvement in downstream applications. As a confirmation that </context>
</contexts>
<marker>Oflazer, 1994</marker>
<rawString>K. Oflazer. 1994. Two-level description of Turkish morphology. Literary and Linguistic Computing, 9(2):137–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolina Parada</author>
<author>Mark Dredze</author>
<author>Abhinav Sethy</author>
<author>Ariya Rastrow</author>
</authors>
<title>Learning sub-word units for open vocabulary speech recognition.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="27413" citStr="Parada et al., 2011" startWordPosition="4453" endWordPosition="4456">supervised morphology learning is an attractive problem for computational cognitive science. tle attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- and character-based models. Character-based language models are reviewed by Carpenter (2005). So-called hybrid models that model both words and sublexical units have become popular in speech recognition (Shaik et al., 2012; Parada et al., 2011; Bazzi, 2002). Openvocabulary language language modeling has also recently been explored in the context of assistive technologies (Roark, 2009). Finally, Pitman-Yor processes (PYPs) have become widespread in natural language processing since they are natural power-law generators. It has been shown that the widely used modified KneserNey estimator (Chen and Goodman, 1998) for ngram language models is an approximation of the posterior predictive distribution of a language model with hierarchical PYP priors (Goldwater et al., 2011; Teh, 2006). 7 Conclusion We described a generative model which m</context>
</contexts>
<marker>Parada, Dredze, Sethy, Rastrow, 2011</marker>
<rawString>Carolina Parada, Mark Dredze, Abhinav Sethy, and Ariya Rastrow. 2011. Learning sub-word units for open vocabulary speech recognition. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Pitman</author>
<author>Marc Yor</author>
</authors>
<title>The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator.</title>
<date>1997</date>
<journal>Annals of Probability,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="4822" citStr="Pitman and Yor, 1997" startWordPosition="694" endWordPosition="697">uces lexicons of stems and inflectional patterns; then it generates a lexicon of inflected forms using the finite-state generator. Finally, the inflected forms are used to generate observed data. Different independence assumptions can be made at each of these levels to encode beliefs about where stems, inflections, and surface forms should share statistical strength. 2.1 Pitman-Yor Processes Our work relies extensively on Pitman-Yor processes, which provide a flexible framework for expressing backoff and interpolation relationships and extending standard models with richer word distributions (Pitman and Yor, 1997). They have been shown to match the performance of state-of-the-art language models and to give estimates that follow appropriate power laws (Teh, 2006). A draw from a Pitman-Yor process (PYP), denoted G — PY(d, 0, G0), is a discrete distribution over a (possibly infinite) set of events, which we denote abstractly £. The process is parameterized by a discount parameter 0 &lt; d &lt; 1, a strength parameter 0 &gt; —d, and a base distribution G0 over the event space £. In this work, our focus is on the base distribution G0. We place vague priors on the hyperparameters d — U([0,1]) and (0 + d) — Gamma(1,1</context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>Jim Pitman and Marc Yor. 1997. The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator. Annals of Probability, 25(2):855–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Open vocabulary language modeling for binary response typing interfaces.</title>
<date>2009</date>
<tech>Technical Report CSLU-09-001,</tech>
<institution>Oregon Health &amp; Science University.</institution>
<contexts>
<context position="27557" citStr="Roark, 2009" startWordPosition="4477" endWordPosition="4478">nguage modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- and character-based models. Character-based language models are reviewed by Carpenter (2005). So-called hybrid models that model both words and sublexical units have become popular in speech recognition (Shaik et al., 2012; Parada et al., 2011; Bazzi, 2002). Openvocabulary language language modeling has also recently been explored in the context of assistive technologies (Roark, 2009). Finally, Pitman-Yor processes (PYPs) have become widespread in natural language processing since they are natural power-law generators. It has been shown that the widely used modified KneserNey estimator (Chen and Goodman, 1998) for ngram language models is an approximation of the posterior predictive distribution of a language model with hierarchical PYP priors (Goldwater et al., 2011; Teh, 2006). 7 Conclusion We described a generative model which makes use of morphological analyzers to produce richer word distributions through sharing of statistical strength between stems. We have shown ho</context>
</contexts>
<marker>Roark, 2009</marker>
<rawString>Brian Roark. 2009. Open vocabulary language modeling for binary response typing interfaces. Technical Report CSLU-09-001, Oregon Health &amp; Science University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ali Basha Shaik</author>
<author>David Rybach</author>
<author>Stefan Hahn</author>
<author>Ralf Schluüter</author>
<author>Hermann Ney</author>
</authors>
<title>Hierarchical hybrid language models for open vocabulary continuous speech recognition using wfst.</title>
<date>2012</date>
<booktitle>In Proc. of SAPA.</booktitle>
<contexts>
<context position="27392" citStr="Shaik et al., 2012" startWordPosition="4449" endWordPosition="4452">ern toolkits, and unsupervised morphology learning is an attractive problem for computational cognitive science. tle attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- and character-based models. Character-based language models are reviewed by Carpenter (2005). So-called hybrid models that model both words and sublexical units have become popular in speech recognition (Shaik et al., 2012; Parada et al., 2011; Bazzi, 2002). Openvocabulary language language modeling has also recently been explored in the context of assistive technologies (Roark, 2009). Finally, Pitman-Yor processes (PYPs) have become widespread in natural language processing since they are natural power-law generators. It has been shown that the widely used modified KneserNey estimator (Chen and Goodman, 1998) for ngram language models is an approximation of the posterior predictive distribution of a language model with hierarchical PYP priors (Goldwater et al., 2011; Teh, 2006). 7 Conclusion We described a gen</context>
</contexts>
<marker>Shaik, Rybach, Hahn, Schluüter, Ney, 2012</marker>
<rawString>M. Ali Basha Shaik, David Rybach, Stefan Hahn, Ralf Schluüter, and Hermann Ney. 2012. Hierarchical hybrid language models for open vocabulary continuous speech recognition using wfst. In Proc. of SAPA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Silfverberg</author>
<author>K Lindén</author>
<author>M Hyvärinen</author>
</authors>
<title>Predictive text entry for agglutinative languages using unsupervised morphological segmentation.</title>
<date>2012</date>
<booktitle>In Proc. of Computational Linguistics and Intelligent Text Processing.</booktitle>
<contexts>
<context position="18004" citStr="Silfverberg et al. (2012)" startWordPosition="2907" endWordPosition="2910">tion that correctly modeling new words matters, we consider a predictive task with a truly open vocabulary and that requires only a language model: predictive text input. Given some text, we encode it using a lossy deterministic character mapping, and try to recover the original content by computing the most likely word sequence. This task is inspired by predictive text input systems available on cellphones with a 9-key keypad. For example, the string gave me a cup is encoded as 4283 63 2 287, which could also be decoded as: hate of a bus. d ,✓ d ,✓ d ,✓ d1, ✓1 G W G W G1 W1 d ,✓ G G G G 1210 Silfverberg et al. (2012) describe a system designed for this task in Finnish, which is composed of a weighted finite-state morphological analyzer trained on IRC logs. However, their system is restricted to words that are encoded in the analyzer’s lexicon and does not use context for disambiguation. In our experiments, we use the same Turkish TED talks corpus as the previous section. As a baseline, we use a trigram character language model. We produce a character lattice which encodes all the possible interpretations for a word and compose it with a finite-state representation of the character LM using OpenFST (Allauz</context>
</contexts>
<marker>Silfverberg, Lindén, Hyvärinen, 2012</marker>
<rawString>M. Silfverberg, K. Lindén, and M. Hyvärinen. 2012. Predictive text entry for agglutinative languages using unsupervised morphological segmentation. In Proc. of Computational Linguistics and Intelligent Text Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>David A Smith</author>
<author>Roy W Tromble</author>
</authors>
<title>Context-based morphological disambiguation with random fields.</title>
<date>2005</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="25237" citStr="Smith et al., 2005" startWordPosition="4130" endWordPosition="4133">emedim 1212 Kay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3,</context>
</contexts>
<marker>Smith, Smith, Tromble, 2005</marker>
<rawString>Noah A. Smith, David A. Smith, and Roy W. Tromble. 2005. Context-based morphological disambiguation with random fields. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt G Snover</author>
<author>Michael R Brent</author>
</authors>
<title>A Bayesian model for morpheme and paradigm identification.</title>
<date>2001</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="25829" citStr="Snover and Brent, 2001" startWordPosition="4221" endWordPosition="4224">ambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored lan</context>
</contexts>
<marker>Snover, Brent, 2001</marker>
<rawString>Matt G. Snover and Michael R. Brent. 2001. A Bayesian model for morpheme and paradigm identification. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for morphological segmentation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="25804" citStr="Snyder and Barzilay, 2008" startWordPosition="4217" endWordPosition="4220"> et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language model</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008. Unsupervised multilingual learning for morphological segmentation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4974" citStr="Teh, 2006" startWordPosition="719" endWordPosition="720"> used to generate observed data. Different independence assumptions can be made at each of these levels to encode beliefs about where stems, inflections, and surface forms should share statistical strength. 2.1 Pitman-Yor Processes Our work relies extensively on Pitman-Yor processes, which provide a flexible framework for expressing backoff and interpolation relationships and extending standard models with richer word distributions (Pitman and Yor, 1997). They have been shown to match the performance of state-of-the-art language models and to give estimates that follow appropriate power laws (Teh, 2006). A draw from a Pitman-Yor process (PYP), denoted G — PY(d, 0, G0), is a discrete distribution over a (possibly infinite) set of events, which we denote abstractly £. The process is parameterized by a discount parameter 0 &lt; d &lt; 1, a strength parameter 0 &gt; —d, and a base distribution G0 over the event space £. In this work, our focus is on the base distribution G0. We place vague priors on the hyperparameters d — U([0,1]) and (0 + d) — Gamma(1,1). Inference in PYPs is discussed below. 2.2 Unigram Morphology Model The most basic expression of our model is a unigram model of text. So far, we only</context>
<context position="8893" citStr="Teh, 2006" startWordPosition="1382" endWordPosition="1383">sidered finite in Russian, but this assumption is not valid for languages with more extensive derivational morphology like Turkish. 2.5 Posterior Inference For most applications, rather than directly generating from a model using the processes outlined above, we seek to infer posterior distributions over latent parameters and structures, given a sample of data. Although there is no known analytic form of the PYP density, it is possible to marginalize the draws from it and to work directly with observations. This marginalization produces the classical Chinese restaurant process representation (Teh, 2006). When working with the morphology models we are proposing, we also need to marginalize the different latent forms (stems s and patterns p) that may have given rise to a given word w. Thus, we require that the inverse relation of GENERATE is available to compute the marginal base word distribution: p(w I G0w) = � p(s I Gs) p(p I Gp) GENERATE(s,p)=w Since our approach encodes morphology using FSTs, which are invertible, this poses no problem. To illustrate, consider the Russian word npo,qHM, which may be analyzed in several ways: npo,qHH +Adj +Sg +Neut +Instr npo,qHH +Adj +Sg +Masc +Instr npo,q</context>
<context position="15060" citStr="Teh (2006)" startWordPosition="2379" endWordPosition="2380">computed for the set of additional morphemes and averaged over the words in the corpus. 8We estimate these frequencies by assuming each analysis of each token is uniformly likely, then summing fractional counts. 9http://open.xerox.com/Services/ fst-nlp-tools/Pages/morphology 1209 4 Open Vocabulary Language Models We now integrate our unigram model in a hierarchical Pitman-Yor n-gram language model (Fig. 1). The training corpus words are assumed to be generated from a distribution Gn w drawn from PY(dn, On, Gn−1 w ), where Gn−1 w is defined recursively down to the base model G0w. Previous work Teh (2006) simply used G0w = N(V) where V is the word vocabulary, but in our case G0w is the MP defined in §2.2. Figure 1: The trigram version of our language model represented as a graphical model. G1 is the unigram model of §2.2. We are interested in evaluating our model in an open vocabulary scenario where the ability to explain new unseen words matters. We expect our model to be able to generalize better thanks to the combination of a morphological analyzer and a stem distribution which is less sparse than the word distribution (for example, for the 1.6M word Turkish corpus, |V |Pz� 3.5|5 |Pz� 140k)</context>
<context position="27959" citStr="Teh, 2006" startWordPosition="4538" endWordPosition="4539">r in speech recognition (Shaik et al., 2012; Parada et al., 2011; Bazzi, 2002). Openvocabulary language language modeling has also recently been explored in the context of assistive technologies (Roark, 2009). Finally, Pitman-Yor processes (PYPs) have become widespread in natural language processing since they are natural power-law generators. It has been shown that the widely used modified KneserNey estimator (Chen and Goodman, 1998) for ngram language models is an approximation of the posterior predictive distribution of a language model with hierarchical PYP priors (Goldwater et al., 2011; Teh, 2006). 7 Conclusion We described a generative model which makes use of morphological analyzers to produce richer word distributions through sharing of statistical strength between stems. We have shown how it can be integrated into several models central to NLP applications and have empirically validated the effectiveness of these changes. Although this paper mostly focused on languages that are well studied and for which high-quality analyzers are available, our models are especially relevant in low-resource scenarios because they do not require disambiguated analyses. In future work, we plan to ap</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Djamé Seddah</author>
<author>Yoav Goldberg</author>
<author>Sandra Kübler</author>
<author>Marie Candito</author>
<author>Jennifer Foster</author>
<author>Yannick Versley</author>
<author>Ines Rehbein</author>
<author>Lamia Tounsi</author>
</authors>
<title>Statistical parsing of morphologically rich languages: What, how and whither.</title>
<date>2010</date>
<booktitle>In Proc. of Workshop on Statistical Parsing of Morphologically Rich Languages.</booktitle>
<contexts>
<context position="26317" citStr="Tsarfaty et al., 2010" startWordPosition="4295" endWordPosition="4298">d Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arı- soy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly lit12Developing a high-coverage analyzer can be a timeconsuming process even with the simplicity of modern toolkits, and unsupervised morphology learning is an attractive problem for computational cognitive science. tle attention to the general p</context>
</contexts>
<marker>Tsarfaty, Seddah, Goldberg, Kübler, Candito, Foster, Versley, Rehbein, Tounsi, 2010</marker>
<rawString>Reut Tsarfaty, Djamé Seddah, Yoav Goldberg, Sandra Kübler, Marie Candito, Jennifer Foster, Yannick Versley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical parsing of morphologically rich languages: What, how and whither. In Proc. of Workshop on Statistical Parsing of Morphologically Rich Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Tyers</author>
<author>M S Alperen</author>
</authors>
<title>South-east european times: A parallel corpus of Balkan languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC workshop on Exploitation of</booktitle>
<contexts>
<context position="22861" citStr="Tyers and Alperen, 2010" startWordPosition="3742" endWordPosition="3745">collapsed and A — Gamma(k, 0) is inferred using Metropolis-Hastings. ↵,0 G G Figure 2: Our alignment model, represented as a graphical model. Experiments We evaluate the alignment error rate of our models for two language pairs with rich morphology on the target side. We compare to alignments inferred using IBM Model 4 trained with EM (Brown et al., 1993),10 a version of our baseline model (described above) without PY priors (learned using EM), and the PY-based baseline. We consider two language pairs. English-Turkish We use a 2.8M word cleaned version of the South-East European Times corpus (Tyers and Alperen, 2010) and gold-standard alignments from Qakmak et al. (2012). Our morphological analyzer is identical to the one used in the previous sections. English-Czech We use the 1.3M word News Commentary corpus and gold-standard alignments 10We use the default GIZA++ stage training scheme: Model 1 + HMM + Model 3 + Model 4. from Bojar and Prokopová (2006). The morphological analyzer is provided by Xerox. Results Results are shown in Table 5. Our lightly parameterized model performs much better than IBM Model 4 in these small-data conditions. With an identical model, we find PY priors outperform traditional </context>
</contexts>
<marker>Tyers, Alperen, 2010</marker>
<rawString>F. Tyers and M.S. Alperen. 2010. South-east european times: A parallel corpus of Balkan languages. In Proceedings of the LREC workshop on Exploitation of multilingual resources and tools for Central and (South) Eastern European Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Talha Qakmak</author>
<author>Süleyman Acar</author>
<author>Gül¸sen Eryi˘git</author>
</authors>
<title>Word alignment for English-Turkish language pair.</title>
<date>2012</date>
<booktitle>In Proc. of LREC.</booktitle>
<marker>Qakmak, Acar, Eryi˘git, 2012</marker>
<rawString>M. Talha Qakmak, Süleyman Acar, and Gül¸sen Eryi˘git. 2012. Word alignment for English-Turkish language pair. In Proc. of LREC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>