<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006090">
<title confidence="0.8376555">
Virtual Evidence for Training Speech Recognizers using Partially Labeled
data
</title>
<author confidence="0.934225">
Amarnag Subramanya
</author>
<affiliation confidence="0.9870615">
Department of Electrical Engineering
University of Washington
</affiliation>
<address confidence="0.939124">
Seattle, WA 98195-2500
</address>
<email confidence="0.999225">
asubram@u.washington.edu
</email>
<author confidence="0.944229">
Jeff Bilmes
</author>
<affiliation confidence="0.993561">
Department of Electrical Engineering
University of Washington
</affiliation>
<address confidence="0.9418">
Seattle, WA 98195-2500
</address>
<email confidence="0.999519">
bilmes@ee.washington.edu
</email>
<sectionHeader confidence="0.996675" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999578357142857">
Collecting supervised training data for au-
tomatic speech recognition (ASR) sys-
tems is both time consuming and expen-
sive. In this paper we use the notion of vir-
tual evidence in a graphical-model based
system to reduce the amount of supervi-
sory training data required for sequence
learning tasks. We apply this approach to
a TIMIT phone recognition system, and
show that our VE-based training scheme
can, relative to a baseline trained with
the full segmentation, yield similar results
with only 15.3% of the frames labeled
(keeping the number of utterances fixed).
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943020408164">
Current state-of-the-art speech recognizers use thou-
sands of hours of training data, collected from a
large number of speakers with various backgrounds
in order to make the models more robust. It is well
known that one of the simplest ways of improv-
ing the accuracy of a recognizer is to increase the
amount of training data. Moreover, speech recog-
nition systems can benefit from being trained on
hand-transcribed data where all the appropriate word
level segmentations (i.e., the exact time of the word
boundaries) are known. However, with increasing
amounts of raw speech data being made available, it
is both time consuming and expensive to accurately
segment every word for every given sentence. More-
over, for languages for which only a small amount
of training data is available, it can be expensive and
challenging to annotate with precise word transcrip-
tions – the researcher may have no choice but to use
partially erroneous training data.
There are a number of different ways to label
data used to train a speech recognizer. First, the
most expensive case (from an annotation perspec-
tive) is fully supervised training, where both word
sequences and time segmentations are completely
specified&apos;. A second case is most commonly used
in speech recognition systems, where only the word
sequences of utterances are given, but their precise
segmentations are unknown. A third case falls un-
der the realm of semi-supervised approaches. As
one possible example, a previously trained recog-
nizer is used to generate transcripts for unlabeled
data, which are then used to re-train the recog-
nizer based on some measure of recognizer confi-
dence (Lamel et al., 2002).
The above cases do not exhaust the set of possible
training scenarios. In this paper, we show how the
notion of virtual evidence (VE) (Pearl, 1988) may
be used to obtain the benefits of data with time seg-
mentations but using only partially labeled data. Our
method lies somewhere between the first and sec-
ond cases above. This general framework has been
successfully applied in the past to the activity recog-
nition domain (Subramanya et al., 2006). Here we
make use of the TIMIT phone recognition task as an
example to show how VE may be used to deal with
partially labeled speech training data. To the best of
our knowledge, this paper presents the first system to
express training uncertainty using VE in the speech
domain.
</bodyText>
<sectionHeader confidence="0.957328" genericHeader="method">
2 Baseline System
</sectionHeader>
<bodyText confidence="0.9828035">
Figure 1 shows two consecutive time slices of a dy-
namic Bayesian network (DBN) designed for con-
</bodyText>
<footnote confidence="0.769531333333333">
&apos;This does not imply that all variables are observed during
training. While the inter-word segmentations are known, the
model is not given information about intra-word segmentations.
</footnote>
<page confidence="0.965838">
165
</page>
<note confidence="0.645284">
Proceedings of NAACL HLT 2007, Companion Volume, pages 165–168,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999632">
Figure 1: Training Graph.
</figureCaption>
<bodyText confidence="0.998758">
text independent (CI) phone recognition. All ob-
served variables are shaded, deterministic depen-
dences are depicted using solid black lines, value
specific dependences are shown using a dot-dash
lines, and random dependencies are represented us-
ing dashed lines. In this paper, given any random
variable (rv) X, x denotes a particular value of that
rv, DX is the domain of X (x ∈ DX), and |DX|
represents its cardinality.
In the above model, Pt is the rv representing
the phone variable, Ht models the current po-
sition within a phone, St is the state, Ot the
acoustic observations, At and Rt indicate state
and phone transitions respectively. Here, DXt =
DXt−1, ∀t,∀X. In our implementation here,
DHt, DAt ∈ {0, 1, 2}, DRt ∈ {0, 1}. Also
δ{c1, ... , cn} is an indicator function that turns on
when all the conditions {c1, ... , cn} are true (i.e.
a conjunction over all the conditions). The distri-
bution for Ht is given by p(ht|ht−1, rt−1, at−1) =
δPht=0,rt−1=1} + δPht=at−1+ht−1,rt−1=0}, which im-
plies that we always start a phone with Ht = 0.
We allow skips in each phone model, and At=0,
indicates no transition, At=1 implies you transi-
tion to the next state, At=2 causes a state to skip
(Ht+1 = Ht + 2). As the TI IT corpus pro-
vides phone level segmentations, Pt is observed dur-
ing training. However, for reasons that will be-
come clear in the next section, we treat Pt as hid-
den but make it the parent of a rv Ct, with, p(ct =
1|pt) = δlt=pt where lt is obtained from the tran-
scriptions (lt ∈ DPt). The above formulation has
exactly the same effect as making Pt observed and
setting it equal to lt (Bilmes, 2004). Additional de-
tails on other CPTs in this model may be found in
(Bilmes and Bartels, 2005). We provide more de-
tails on the baseline system in section 4.1.
Our main reason for choosing the TI IT phone
recognition task is that TI IT includes both se-
quence and segment transcriptions (something rare
</bodyText>
<equation confidence="0.8001555">
t0 t1 t2 t3 t4 t5 t6 t7 t8
Labeled Unlabeled
</equation>
<figureCaption confidence="0.9718985">
Figure 2: Illustration showing our rendition of Vir-
tual Evidence.
</figureCaption>
<bodyText confidence="0.999459642857143">
for LVCSR corpora such as Switchboard and
Fisher). This means that we can compare against
a model that has been trained fully supervised. It is
also well known that context-dependent (CD) mod-
els outperform CI models for the TI IT phone
recognition task (Glass et al., 1996). We used
CI models primarily for the rapid experimental
turnaround time and since it still provides a rea-
sonable test-bed for evaluating new ideas. We
do note, however, that our baseline CI system is
competitive with recently published CD systems
(Wang and Fosler-Lussier, 2006), albeit which uses
many fewer components per mixture (see Sec-
tion 4.1).
</bodyText>
<sectionHeader confidence="0.990317" genericHeader="method">
3 Soft-supervised Learning With VE
</sectionHeader>
<bodyText confidence="0.978040818181818">
Given a joint distribution over n variables
p(x1, ... , xn), “evidence” simply means that
one of the variables (w.l.o.g. x1) is known. We
denote this by ¯x1, so the probability distribution
becomes p(¯x1, ... , xn) (no longer a function of x1).
Any configuration of the variables where x1 =6 ¯x1
is never considered. We can mimic this behavior
by introducing a new virtual child variable c into
the joint distribution that is always observed to be
one (so c = 1), and have c interact only with x1
via the CPT p(c = 1|x1) = δx1=¯x1. Therefore,
</bodyText>
<equation confidence="0.4855845">
E
x1 p(c = 1, x1, ... , xn) = p(¯x1, ... , xn). Now
</equation>
<bodyText confidence="0.9873205">
consider setting p(c = 1|x1) = f(x1), where
f() is an arbitrary non-negative function. With
this, different treatment can be given to different
assignments to x1, but unlike hard evidence, we
are not insisting on only one particular value. This
represents the general notion of VE. In a certain
sense, the notion of VE is similar to the prior
distribution in Bayesian inference, but it is different
in that VE expresses preferences over combinations
of values of random variables whereas a Bayesian
prior expresses preferences over combinations of
model parameter values. For a more information on
VE, see (Bilmes, 2004; Pearl, 1988).
VE can in fact be used when accurate phone level
segmentations are not available. Consider the illus-
tration in Figure 2. As shown, t1 and t4 are the
</bodyText>
<figure confidence="0.9717420625">
Observation
Phone
Position
VE observed child
VE applied
via this CPT
Phone
State
Ht−1
Ot−1 Ot
Pt−1
St−1
C =1 C =1
t
t−1
At−1
Rt−1
VE applied
via this CPT
Pt
Ht
St
Rt
At
Transition
Transition
Phone
State
2
p
1
p
</figure>
<page confidence="0.991078">
166
</page>
<bodyText confidence="0.9950003">
start and end times respectively for phone p1, while
t4 and t7 are the start and end times for phone p2.
When the start and end times for each phone are
given, we have information about the identity of
the phone that produced each and every observation.
The general training scenario in most large vocabu-
lary speech recognition systems, however, does not
have access to these starting/ending times, and they
are trained knowing only the sequence of phone la-
bels (e.g., that p2 follows p1).
Consider a new transcription based on Figure 2,
where we know that p1 ended at some time t3 &lt; t4
and that p2 started at sometime t5 &gt; t4. In the
region between t3 and t5 we have no information
on the identity of the phone variable for each
acoustic frame, except that it is either p1 or p2. A
similar case occurs at the start of phone p1 and
the end of phone p2. The above information can
be used in our model (Figure 1) in the following
way (here given only for t2 &lt; t &lt; t6): p(Ct =
</bodyText>
<equation confidence="0.763008">
1|pt) = δ{pt=p1,t2≤t≤t3} + δ{pt=p2,t5≤t≤t6} +
ft(p1)δ{pt=p1,t3≤t≤t5} + gt(p2)δ{pt=p2,t3≤t≤t5}.
</equation>
<bodyText confidence="0.9992906">
Here ft(p1) and gt(p2) represent our relative beliefs
at time t in whether the value of Pt is either p1
or p2. It is important to highlight that rather than
the absolute values of these functions, it is their
relative values that have an effect on inference
(Bilmes, 2004). There are number of different
ways of choosing these functions. First, we can set
ft(p1) = gt(p2) = α, α &gt; 0. This encodes our
uncertainty regarding the identity of the phone in
this region while still forcing it to be either p1 or
p2, and equal preference is given for both (referred
to as “uniform over two phones”). Alternatively,
other functions could take into account the fact that,
in the frames ‘close’ to t3, it is more likely to be
p1, whereas in the frames ‘close’ to t5, it is more
likely to be p2. This can be represented by using
a decreasing function for ft(p1) and an increasing
function for gt(p2) (for example linearly increasing
or decreasing with time).
As more frames are dropped around transitions
(e.g., as t3 − t2 decreases), we use lesser amounts
of labeled data. In an extreme situation, we can drop
all the labels (t3 &lt; t2) to recover the case where only
sequence and not segment information is available.
Alternatively, we can have t3 = t2 +1, which means
that only one frame is labeled for every phone in an
utterance — all other frames of a phone are left un-
transcribed. From the perspective of a transcriber,
this simulates the task of going through an utter-
ance and identifying only one frame that belongs to
</bodyText>
<figure confidence="0.657524">
% of Unused Segmentation Data
</figure>
<figureCaption confidence="0.997831">
Figure 3: Virtual Evidence Results
</figureCaption>
<bodyText confidence="0.9977316">
each particular phone without having to identify the
phone boundary. In contrast to the task of determin-
ing the phone boundary, identifying one frame per
word unit is much simpler, less prone to error or dis-
agreement, and less costly (Greenberg, 1995).
</bodyText>
<sectionHeader confidence="0.993599" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.994472">
4.1 Baseline System
</subsectionHeader>
<bodyText confidence="0.999944909090909">
We trained a baseline TIMIT phone recognition sys-
tem that made full use of all phone level segmen-
tations (the fully supervised case). To obtain the
acoustic observations, the signal was first preem-
phasized (α = 0.97) and then windowed using a
Hamming window of size 25ms at 100Hz. We then
extracted MFCC’s from these windowed features.
Deltas and double deltas were appended to the above
observation vector. Each phone is modeled using 3
states, and 64 Gaussians per state. We follow the
standard practice of building models for 48 different
phones and then mapping them down to 39 phones
for scoring purposes (Halberstadt and Glass, 1997).
The decoding DBN graph is similar to the training
graph (Figure 1) except that the variable Ct is re-
moved when decoding. We test on the NIST Core
test set (Glass et al., 1996). All results reported in
this paper were obtained by computing the string
edit (Levenshtein) distance between the hypothesis
and the reference. All models in this paper were
implemented using the Graphical Models Toolkit
(GMTK) (Bilmes and Bartels, 2005).
</bodyText>
<subsectionHeader confidence="0.990817">
4.2 VE Based Training and Results
</subsectionHeader>
<bodyText confidence="0.999888857142857">
We tested various cases of VE-based training by
varying the amount of “dropped” frame labels on
either side of the transition (the dropped labels be-
came the unlabeled frames of Figure 2). We did this
until there was only one frame left labeled for ev-
ery phone. Moreover, in each of the above cases,
we tested a number of different functions to gener-
</bodyText>
<figure confidence="0.963919166666667">
64
62
60
58
56
54
52
0 10 20 30 40 50 60 70 80 90 100
Baseline
Uniform over 2 phones
Linear Interpolation
Phone Accuracy
</figure>
<page confidence="0.989929">
167
</page>
<bodyText confidence="0.996367075471698">
ate the VE scores (see section 3). The results of our
VE experiments are shown in Figure 3. The curves
were obtained by fitting a cubic spline to the points
shown in the figure. The phone accuracy (PA) of our
baseline system (trained in a fully supervised man-
ner) is 61.4%. If the total number of frames in the
training set is NT, and we drop labels on N frames,
the amount of unused data is given by U= NT ∗100
(the x-axis in the figure). Thus U = 0% is the fully
supervised case, whereas U = 100% corresponds
to using only the sequence information. Dropping
the label for one frame on either side of every phone
transition yielded U = 24.5%.
It can be seen that in the case of both “uniform
over 2 phones” and linear interpolation, the PA ac-
tually improves when we drop a small number (&lt;
5 frames) of frames on either side of the transition.
This seems to suggest that there might be some in-
herent errors in the frame level labels near the phone
transitions. The points on the plot at U=84.7% cor-
respond to using a single labeled frame per phone
in every utterance in the training set (average phone
length in TIMIT is about 7 frames). The PA of the
system using a single label per phone is 60.52%. In
this case, we also used a trapezoidal function defined
as follows: if t = ti were the labeled frames for
phone pi, then ft(p1) = 1, ti − 1 &lt; t &lt; ti + 1, and
a linear interpolation function for the other values
t during the transition to generate the VE weights.
This system yielded a PA of 61.29% (baseline accu-
racy 61.4%). We should highlight that even though
this system used only 15.3% of the labels used by
the baseline, the results were similar! The figure
also shows the PA of the system that used only
the sequence information was about 53% (compare
against baseline accuracy of 61.4%). This lends ev-
idence to the claim that training recognizers using
data with time segmentation information can lead to
improved performance.
Given the procedure we used to drop the frames
around transitions, the single labeled frame for ev-
ery phone is usually located on or around the mid-
point of the phone. This however cannot be guaran-
teed if a transcriber is asked to randomly label one
frame per phone. To simulate such a situation, we
randomly choose one frame to be labeled for every
phone in the utterance. We then trained this system
using the “uniform over 2 phones” technique and
tested it on the NIST core test set. This experiment
was repeated 10 times, and the PA averaged over the
10 trails was found to be 60.5% (standard deviation
0.402), thus showing the robustness of our technique
even for less carefully labeled data.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9989004">
In this paper we have shown how VE can be used
to train a TIMIT phone recognition system using
partially labeled data. The performance of this sys-
tem is not significantly worse than the baseline that
makes use of all the labels. Further, though this
method of data transcription is only slightly more
time consuming that sequence labeling, it yeilds sig-
nificant gains in performance (53% v/s 60.5%). The
results also show that even in the presence of fully
labaled data, allowing for uncertainity at the tran-
sitions during training can be beneficial for ASR
performance. It should however be pointed out
that while phone recognition accuracy is not al-
ways a good predictor of word accuracy, we still
expect that our method will ultimately generalize
to word accuracy as well, assuming we have ac-
cess to a corpus where at least one frame of each
word has been labeled with the word identity. This
work was supported by an ONR MURI grant, No.
N000140510388.
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999312724137931">
[Bilmes and Bartels2005] J. Bilmes and C. Bartels. 2005.
Graphical model architectures for speech recognition. IEEE
Signal Processing Magazine, 22(5):89–100, September.
[Bilmes2004] J. Bilmes. 2004. On soft evidence in Bayesian
networks. Technical Report UWEETR-2004-0016, Univer-
sity of Washington, Dept. of EE.
[Glass et al.1996] J. Glass, J. Chang, and M. McCandless. 1996.
A probabilistic framework for feature-based speech recogni-
tion. In Proc. ICSLP ’96, volume 4, Philadelphia, PA.
[Greenberg1995] S Greenberg. 1995. The Switchboard tran-
scription project. Technical report, The Johns Hopkins Uni-
versity (CLSP) Summer Research Workshop.
[Halberstadt and Glass1997] A. K. Halberstadt and J. R. Glass.
1997. Heterogeneous acoustic measurements for phonetic
classification. In Proc. Eurospeech ’97, pages 401–404,
Rhodes, Greece.
[Lamel et al.2002] L. Lamel, J. Gauvian, and G. Adda. 2002.
Lightly supervised and unsupervised acoustic model train-
ing. Computer Speech and Language.
[Pearl1988] J. Pearl. 1988. Probabilistic Reasoning in Intel-
ligent Systems: Networks of Plausible Inference. Morgan
Kaufmann Publishers, Inc.
[Subramanya et al.2006] A. Subramanya, A. Raj, J. Bilmes, and
D. Fox. 2006. Recognizing activities and spatial context
using wearable sensors. In Proc. of the Conference on Un-
certainty in Artificial Intelligence (UAI).
[Wang and Fosler-Lussier2006] Y. Wang and E. Fosler-Lussier.
2006. Integrating phonetic boundary discrimination explic-
ity into HMM systems. In Proc. of the Interspeech.
</reference>
<page confidence="0.997296">
168
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.610963">
<title confidence="0.817262">Virtual Evidence for Training Speech Recognizers using Partially Labeled data</title>
<author confidence="0.981736">Amarnag Subramanya</author>
<affiliation confidence="0.9997745">Department of Electrical University of</affiliation>
<address confidence="0.999667">Seattle, WA 98195-2500</address>
<email confidence="0.999431">asubram@u.washington.edu</email>
<author confidence="0.995114">Jeff</author>
<affiliation confidence="0.9998015">Department of Electrical University of</affiliation>
<address confidence="0.983865">Seattle, WA</address>
<email confidence="0.99986">bilmes@ee.washington.edu</email>
<abstract confidence="0.9984192">Collecting supervised training data for automatic speech recognition (ASR) systems is both time consuming and expensive. In this paper we use the notion of virtual evidence in a graphical-model based system to reduce the amount of supervisory training data required for sequence learning tasks. We apply this approach to a TIMIT phone recognition system, and show that our VE-based training scheme can, relative to a baseline trained with the full segmentation, yield similar results with only 15.3% of the frames labeled (keeping the number of utterances fixed).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bilmes</author>
<author>C Bartels</author>
</authors>
<title>Graphical model architectures for speech recognition.</title>
<date>2005</date>
<journal>IEEE Signal Processing Magazine,</journal>
<volume>22</volume>
<issue>5</issue>
<marker>[Bilmes and Bartels2005]</marker>
<rawString>J. Bilmes and C. Bartels. 2005. Graphical model architectures for speech recognition. IEEE Signal Processing Magazine, 22(5):89–100, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bilmes</author>
</authors>
<title>On soft evidence in Bayesian networks.</title>
<date>2004</date>
<tech>Technical Report UWEETR-2004-0016,</tech>
<institution>University of Washington, Dept. of EE.</institution>
<marker>[Bilmes2004]</marker>
<rawString>J. Bilmes. 2004. On soft evidence in Bayesian networks. Technical Report UWEETR-2004-0016, University of Washington, Dept. of EE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Glass</author>
<author>J Chang</author>
<author>M McCandless</author>
</authors>
<title>A probabilistic framework for feature-based speech recognition.</title>
<date>1996</date>
<booktitle>In Proc. ICSLP ’96,</booktitle>
<volume>4</volume>
<location>Philadelphia, PA.</location>
<marker>[Glass et al.1996]</marker>
<rawString>J. Glass, J. Chang, and M. McCandless. 1996. A probabilistic framework for feature-based speech recognition. In Proc. ICSLP ’96, volume 4, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Greenberg</author>
</authors>
<title>The Switchboard transcription project.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>The Johns Hopkins University (CLSP) Summer Research Workshop.</institution>
<marker>[Greenberg1995]</marker>
<rawString>S Greenberg. 1995. The Switchboard transcription project. Technical report, The Johns Hopkins University (CLSP) Summer Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Halberstadt</author>
<author>J R Glass</author>
</authors>
<title>Heterogeneous acoustic measurements for phonetic classification.</title>
<date>1997</date>
<booktitle>In Proc. Eurospeech ’97,</booktitle>
<pages>401--404</pages>
<location>Rhodes, Greece.</location>
<marker>[Halberstadt and Glass1997]</marker>
<rawString>A. K. Halberstadt and J. R. Glass. 1997. Heterogeneous acoustic measurements for phonetic classification. In Proc. Eurospeech ’97, pages 401–404, Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lamel</author>
<author>J Gauvian</author>
<author>G Adda</author>
</authors>
<title>Lightly supervised and unsupervised acoustic model training. Computer Speech and Language.</title>
<date>2002</date>
<marker>[Lamel et al.2002]</marker>
<rawString>L. Lamel, J. Gauvian, and G. Adda. 2002. Lightly supervised and unsupervised acoustic model training. Computer Speech and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.</title>
<date>1988</date>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<marker>[Pearl1988]</marker>
<rawString>J. Pearl. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Subramanya</author>
<author>A Raj</author>
<author>J Bilmes</author>
<author>D Fox</author>
</authors>
<title>Recognizing activities and spatial context using wearable sensors.</title>
<date>2006</date>
<booktitle>In Proc. of the Conference on Uncertainty in Artificial Intelligence (UAI).</booktitle>
<marker>[Subramanya et al.2006]</marker>
<rawString>A. Subramanya, A. Raj, J. Bilmes, and D. Fox. 2006. Recognizing activities and spatial context using wearable sensors. In Proc. of the Conference on Uncertainty in Artificial Intelligence (UAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>E Fosler-Lussier</author>
</authors>
<title>Integrating phonetic boundary discrimination explicity into HMM systems.</title>
<date>2006</date>
<booktitle>In Proc. of the Interspeech.</booktitle>
<marker>[Wang and Fosler-Lussier2006]</marker>
<rawString>Y. Wang and E. Fosler-Lussier. 2006. Integrating phonetic boundary discrimination explicity into HMM systems. In Proc. of the Interspeech.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>