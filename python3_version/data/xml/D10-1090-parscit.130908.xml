<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009511">
<title confidence="0.989103">
PEM: A Paraphrase Evaluation Metric Exploiting Parallel Texts
</title>
<author confidence="0.997949">
Chang Liu&apos; and Daniel Dahlmeier2 and Hwee Tou Ng&apos; ,2
</author>
<affiliation confidence="0.909747">
&apos;Department of Computer Science, National University of Singapore
2NUS Graduate School for Integrative Sciences and Engineering
</affiliation>
<email confidence="0.995428">
{liuchan1,danielhe,nght}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.99383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999821">
We present PEM, the first fully automatic met-
ric to evaluate the quality of paraphrases, and
consequently, that of paraphrase generation
systems. Our metric is based on three crite-
ria: adequacy, fluency, and lexical dissimilar-
ity. The key component in our metric is a ro-
bust and shallow semantic similarity measure
based on pivot language N-grams that allows
us to approximate adequacy independently of
lexical similarity. Human evaluation shows
that PEM achieves high correlation with hu-
man judgments.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982947368421">
In recent years, there has been an increasing inter-
est in the task of paraphrase generation (PG) (Barzi-
lay and Lee, 2003; Pang et al., 2003; Quirk et al.,
2004; Bannard and Callison-Burch, 2005; Kauchak
and Barzilay, 2006; Zhao et al., 2008; Zhao et al.,
2009). At the same time, the task has seen appli-
cations such as machine translation (MT) (Callison-
Burch et al., 2006; Madnani et al., 2007; Madnani
et al., 2008), MT evaluation (Kauchak and Barzilay,
2006; Zhou et al., 2006a; Owczarzak et al., 2006),
summary evaluation (Zhou et al., 2006b), and ques-
tion answering (Duboue and Chu-Carroll, 2006).
Despite the research activities, we see two major
problems in the field. First, there is currently no con-
sensus on what attributes characterize a good para-
phrase. As a result, works on the application of para-
phrases tend to build their own PG system in view
of the immediate needs instead of using an existing
system.
Second, and as a consequence, no automatic eval-
uation metric exists for paraphrases. Most works in
this area resort to ad hoc manual evaluations, such as
the percentage of “yes” judgments to the question of
“is the meaning preserved”. This type of evaluation
is incomprehensive, expensive, and non-comparable
between different studies, making progress hard to
judge.
In this work we address both problems. We pro-
pose a set of three criteria for good paraphrases: ad-
equacy, fluency, and lexical dissimilarity. Consid-
ering that paraphrase evaluation is a very subjec-
tive task with no rigid definition, we conduct ex-
periments with human judges to show that humans
generally have a consistent intuition for good para-
phrases, and that the three criteria are good indica-
tors.
Based on these criteria, we construct PEM (Para-
phrase Evaluation Metric), a fully automatic evalua-
tion metric for PG systems. PEM takes as input the
original sentence R and its paraphrase candidate P,
and outputs a single numeric score b estimating the
quality of P as a paraphrase of R. PG systems can
be compared based on the average scores of their
output paraphrases. To the best of our knowledge,
this is the first automatic metric that gives an objec-
tive and unambiguous ranking of different PG sys-
tems, which serves as a benchmark of progress in
the field of PG.
The main difficulty of deriving PEM is to measure
semantic closeness without relying on lexical level
similarity. To this end, we propose bag ofpivot lan-
guage N-grams (BPNG) as a robust, broad-coverage,
and knowledge-lean semantic representation for nat-
ural language sentences. Most importantly, BPNG
does not depend on lexical or syntactic similarity,
allowing us to address the conflicting requirements
of paraphrase evaluation. The only linguistic re-
</bodyText>
<page confidence="0.979609">
923
</page>
<note confidence="0.8177405">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 923–932,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998385">
source required to evaluate BPNG is a parallel text
of the target language and an arbitrary other lan-
guage, known as the pivot language.
We highlight that paraphrase evaluation and para-
phrase recognition (Heilman and Smith, 2010; Das
and Smith, 2009; Wan et al., 2006; Qiu et al., 2006)
are related yet distinct tasks. Consider two sentences
51 and 52 that are the same except for the substitu-
tion of a single synonym. A paraphrase recognition
system should assign them a very high score, but a
paraphrase evaluation system would assign a rela-
tively low one. Indeed, the latter is often a better
indicator of how useful a PG system potentially is
for the applications of PG described earlier.
The rest of the paper is organized as follows. We
survey other automatic evaluation metrics in natural
language processing (NLP) in Section 2. We define
the task of paraphrase evaluation in Section 3 and
develop our metric in Section 4. We conduct a hu-
man evaluation and analyze the results in Section 5.
The correlation of PEM with human judgments is
studied in Section 6. Finally, we discuss our find-
ings and future work in Section 7 and conclude in
Section 8.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999830279069767">
The most well-known automatic evaluation metric in
NLP is BLEU (Papineni et al., 2002) for MT, based
on N-gram matching precisions. The simplicity of
BLEU lends well to MT techniques that directly op-
timize the evaluation metric.
The weakness of BLEU is that it operates purely
at the lexical surface level. Later works attempt to
take more syntactic and semantic features into con-
sideration (see (Callison-Burch et al., 2009) for an
overview). The whole spectrum of NLP resources
has found application in machine translation eval-
uation, including POS tags, constituent and depen-
dency parses, WordNet (Fellbaum, 1998), semantic
roles, textual entailment features, and more. Many
of these metrics have been shown to correlate bet-
ter with human judges than BLEU (Chan and Ng,
2008; Liu et al., 2010). Interestingly, few MT eval-
uation metrics exploit parallel texts as a source of
information, when statistical MT is centered almost
entirely around mining parallel texts.
Compared to these MT evaluation metrics, our
method focuses on addressing the unique require-
ment of paraphrase evaluation: that lexical closeness
does not necessarily entail goodness, contrary to the
basis of MT evaluation.
Inspired by the success of automatic MT evalua-
tion, Lin (2004) and Hovy et al. (2006) propose au-
tomatic metrics for summary evaluation. The for-
mer is entirely lexical based, whereas the latter also
exploits constituent and dependency parses, and se-
mantic features derived from WordNet.
The only prior attempt to devise an automatic
evaluation metric for paraphrases that we are aware
of is ParaMetric (Callison-Burch et al., 2008), which
compares the collection of paraphrases discovered
by automatic paraphrasing algorithms against a
manual gold standard collected over the same sen-
tences. The recall and precision of several current
paraphrase generation systems are evaluated. Para-
Metric does not attempt to propose a single metric
to correlate well with human judgments. Rather, it
consists of a few indirect and partial measures of the
quality of PG systems.
</bodyText>
<sectionHeader confidence="0.984954" genericHeader="method">
3 Task definition
</sectionHeader>
<bodyText confidence="0.999921913043478">
The first step in defining a paraphrase evaluation
metric is to define a good paraphrase. Merriam-
Webster dictionary gives the following definition:
a restatement of a text, passage, or work giving
the meaning in another form. We identify two key
points in this definition: (1) that the meaning is pre-
served, and (2) that the lexical form is different. To
which we add a third, that the paraphrase must be
fluent.
The first and last point are similar to MT evalua-
tion, where adequacy and fluency have been estab-
lished as the standard criteria. In paraphrase evalu-
ation, we have one more: lexical dissimilarity. Al-
though lexical dissimilarity is seemingly the easiest
to judge automatically among the three, it poses an
interesting challenge to automatic evaluation met-
rics, as overlap with the reference has been the basis
of almost all evaluation metrics. That is, while MT
evaluation and paraphrase evaluation are conceptu-
ally closely related, the latter actually highlights the
deficiencies of the former, namely that in most au-
tomatic evaluations, semantic equivalence is under-
represented and substituted by lexical and syntactic
</bodyText>
<page confidence="0.996669">
924
</page>
<bodyText confidence="0.998488058823529">
equivalence.
The task of paraphrase evaluation is then defined
as follows: Given an original sentence R and a para-
phrase candidate P, output a numeric score b esti-
mating the quality of P as a paraphrase of R by con-
sidering adequacy, fluency, and lexical dissimilarity.
In this study, we use a scale of 1 to 5 (inclusive) for
b, although that can be transformed linearly into any
range desired.
We observe here that the overall assessment b is
not a linear combination of the three measures. In
particular, a high dissimilarity score is meaningless
by itself. It could simply be that the paraphrase is
unrelated to the source sentence, or is incoherent.
However, when accompanied by high adequacy and
fluency scores, it differentiates the mediocre para-
phrases from the good ones.
</bodyText>
<sectionHeader confidence="0.987435" genericHeader="method">
4 Paraphrase Evaluation Metric (PEM)
</sectionHeader>
<bodyText confidence="0.999906333333333">
In this section we devise our metric according to the
three proposed evaluation criteria, namely adequacy,
fluency, and dissimilarity. The main challenge is to
measure the adequacy, or semantic similarity, com-
pletely independent of any lexical similarity. We ad-
dress this problem in Sections 4.1 to 4.3. The re-
maining two criteria are addressed in Section 4.4,
and we describe the final combined metric PEM in
Section 4.5.
</bodyText>
<subsectionHeader confidence="0.999513">
4.1 Phrase-level semantic representation
</subsectionHeader>
<bodyText confidence="0.999980310344828">
Without loss of generality, suppose we are to eval-
uate English paraphrases, and have been supplied
many sentence-aligned parallel texts of French and
English as an additional resource. We can then align
the parallel texts at word level automatically using
well-known algorithms such as GIZA++ (Och and
Ney, 2003) or the Berkeley aligner (Liang et al.,
2006; Haghighi et al., 2009).
To measure adequacy without relying on lexical
similarity, we make the key observation that the
aligned French texts can act as a proxy of the se-
mantics to a fragment of an English text. If two En-
glish phrases are often mapped to the same French
phrase, they can be considered similar in mean-
ing. Similar observations have been made by previ-
ous researchers (Wu and Zhou, 2003; Bannard and
Callison-Burch, 2005; Callison-Burch et al., 2006;
Snover et al., 2009). We can treat the distribution
of aligned French phrases as a semantic representa-
tion of the English phrase. The semantic distance
between two English phrases can then be measured
by their degree of overlap in this representation.
In this work, we use the widely-used phrase ex-
traction heuristic in (Koehn et al., 2003) to extract
phrase pairs from parallel texts into a phrase table1.
The phrases extracted do not necessarily correspond
to the speakers’ intuition. Rather, they are units
whose boundaries are preserved during translation.
However, the distinction does not affect our work.
</bodyText>
<subsectionHeader confidence="0.999956">
4.2 Segmenting a sentence into phrases
</subsectionHeader>
<bodyText confidence="0.9999905">
Having established a way to measure the similarity
of two English phrases, we now extend the concept
to sentences. Here we discuss how to segment an
English sentence (the original or the paraphrase) into
phrases.
From the phrase table, we know the frequencies of
all the phrases and we approximate the probability
of a phrase p by:
</bodyText>
<equation confidence="0.9922475">
Pr (p) = N(p),., (1)
Epi (pl)
</equation>
<bodyText confidence="0.99773825">
N(·) is the count of a phrase in the phrase table, and
the denominator is a constant for all p. We define
the likelihood of segmenting a sentence S into a se-
quence of phrases (p1, p2, . . . , pn) by:
</bodyText>
<equation confidence="0.9638155">
n
Pr(p1,p2, ... ,pn |S) = Z(S) rl Pr(pi) (2)
</equation>
<bodyText confidence="0.993007416666667">
where Z(S) is a normalizing constant. The best seg-
mentation of S according to Equation 2 can be cal-
culated efficiently using a dynamic programming al-
gorithm. Note that Z(S) does not need to be calcu-
lated, as it is the same for all different segmentations
of S. The formula has a strong preference for longer
phrases, since every Pr(pi) has a large denominator.
Many sentences are impossible to segment into
known phrases, including all those containing out-
of-vocabulary words. We therefore allow any sin-
gle word w to be considered as a phrase, and if
N(w) = 0, we use N(w) = 0.5 instead.
</bodyText>
<footnote confidence="0.691987">
1The same heuristic is used in the popular MT package
Moses.
</footnote>
<equation confidence="0.779521">
i=1
</equation>
<page confidence="0.989054">
925
</page>
<figureCaption confidence="0.98610375">
Figure 1: A confusion network in the pivot language
Bonjour , monsieur . / 1.0
Figure 2: A degenerated confusion network in the pivot
language
</figureCaption>
<subsectionHeader confidence="0.999801">
4.3 Sentence-level semantic representation
</subsectionHeader>
<bodyText confidence="0.995459272727273">
Simply merging the phrase-level semantic represen-
tations is insufficient to produce a sensible sentence-
level semantic representation. For example, assume
the English sentence Morning, sir. is segmented as
a single phrase, because the following phrase pair is
found in the phrase table:
En: Morning, sir.
Fr: Bonjour, monsieur.
However, another English sentence Hello, Quer-
rien . has an out-of-vocabulary word Querrien
and consequently the most probable segmentation is
</bodyText>
<figure confidence="0.921889142857143">
En: Hello,
Fr: Bonjour, (Pr(Bonjour ,�Hello ,) = 0.9)
Fr: Salut , (Pr(Salut ,�Hello ,) = 0.1)
En: Querrien
Fr: Querrien
En: .
Fr: .
</figure>
<bodyText confidence="0.9766686">
A naive comparison of the bags of French phrases
aligned to Morning, sir. and Hello, Querrien . de-
picted above would conclude that the two sentences
are completely unrelated, as their bags of aligned
French phrases are completely disjoint. We tackle
this problem by constructing a confusion network
representation of the French phrases, as shown in
Figures 1 and 2. The confusion network is formed
by first joining the different French translations of
every English phrase in parallel, and then joining
these segments in series.
The confusion network is a compact representa-
tion of an exponentially large number of (likely mal-
formed) weighted French sentences. We can easily
enumerate the N-grams from the confusion network
representation and collect the statistics for this en-
semble of French sentences efficiently. In this work,
we consider N up to 4. The N-grams for Hello ,
Querrien . are:
1-grams: Bonjour (0.9), Salut (0.1), comma
(1.0), Querrien (1.0), period (1.0).
2-grams: Bonjour comma (0.9), Salut comma
(0.1), comma Querrien (1.0), Querrien period (1.0).
3-grams: Bonjour comma Querrien (0.9), Salut
comma Querrien (0.1), comma Querrien period
(1.0).
4-grams: Bonjour comma Querrien period (0.9),
Salut comma Querrien period (0.1).
We call this representation of an English sentence
a bag of pivot language N-grams (BPNG), where
French is the pivot language in our illustrating ex-
ample. We can extract the BPNG of Morning, sir.
analogously:
1-grams: Bonjour (1.0), comma (1.0), monsieur
(1.0), period (1.0).
2-grams: Bonjour comma (1.0), comma mon-
sieur (1.0), monsieur period (1.0).
3-grams: Bonjour comma monsieur (1.0),
comma monsieur period (1.0).
4-grams: Bonjour comma monsieur period (1.0).
The BPNG of Hello, Querrien. can now be com-
pared sensibly with that of the sentence Morning ,
sir. We use the F1 agreement between the two BP-
NGs as a measure of the semantic similarity. The F1
agreement is defined as
</bodyText>
<equation confidence="0.975487">
2 x Precision x Recall
Precision + Recall
</equation>
<bodyText confidence="0.9995824">
The precision and the recall for an original sen-
tence R and a paraphrase P is defined as follows.
Let French N-gram g E BPNG(R)UBPNG(P), and
WR(g) and WP(g) be the weights of g in the BPNG
of R and P respectively, then
</bodyText>
<equation confidence="0.99488675">
Precision =
Eg WP(g)
Recall = Eg min(WR(g), WP(g))
Eg WR(g)
</equation>
<bodyText confidence="0.998730666666667">
In our example, the numerators for both the preci-
sion and the recall are 0.9 + 1 + 1 + 0.9, for the N-
grams Bonjour, comma, period, and Bonjour comma
</bodyText>
<equation confidence="0.915042">
Bonjour , / 0.9
Salut , / 0.1
Querrien / 1.0 . / 1.0
F1 =
Eg min(WR(g), WP(g))
</equation>
<page confidence="0.981681">
926
</page>
<bodyText confidence="0.999850714285714">
respectively. The denominators for both terms are
10.0. Consequently, F1 = Precision = Recall =
0.38, and we conclude that the two sentences are
38% similar. We call the resulting metric the pivot
language F1. Note that since F1 is symmetric with
respect to the precision and the recall, our metric is
unaffected whether we consider Morning, sir. as the
paraphrase of Hello, Querrien . or the other way
round.
An actual example from our corpus is:
scribed earlier. Our semantic representation suc-
cessfully recognizes that put forth and put forward
are paraphrases of each other, based on their similar
Chinese translation statistics (ti2 chu1 in Chinese).
</bodyText>
<subsectionHeader confidence="0.979345">
4.4 Fluency and dissimilarity
</subsectionHeader>
<bodyText confidence="0.999995">
We measure the fluency of a paraphrase by a nor-
malized language model score P., defined by
where Pr(S) is the sentence probability predicted
by a standard 4-gram language model.
We measure dissimilarity between two English
sentences using the target language F1, where we
collect the bag of all N-grams up to 4-grams from
each English (referred to as the target language) sen-
tence. The target language F1 is then defined as the
F1 agreement of the two bags of N-grams, analogous
to the definition of the pivot language F1. The target
language F1 correlates positively with the similar-
ity of the two sentences, or equivalently, negatively
with the dissimilarity of the two sentences.
</bodyText>
<subsectionHeader confidence="0.967099">
4.5 The metric
</subsectionHeader>
<bodyText confidence="0.999948833333333">
To produce the final PEM metric, we combine the
three component automatic metrics, pivot language
F1, normalized language model, and target language
F1, which measure adequacy, fluency, and dissimi-
larity respectively.
As discussed previously, a linear combination of
the three component metrics is insufficient. We turn
to support vector machine (SVM) regression with
the radial basis function (RBF) kernel. The RBF is
a simple and expressive function, commonly used to
introduce non-linearity into large margin classifica-
tions and regressions.
</bodyText>
<equation confidence="0.946772">
RBF(xZ7 xj) = e−711xi−xj112
</equation>
<bodyText confidence="0.999938428571429">
We use the implementation in SVMlight
(Joachims, 1999). The SVM is to be trained on a set
of human-judged paraphrase pairs, where the three
component automatic metrics are fit to the human
overall assessment. After training, the model can
then be used to evaluate new paraphrase pairs in a
fully automatic fashion.
</bodyText>
<sectionHeader confidence="0.986115" genericHeader="method">
5 Human evaluation
</sectionHeader>
<bodyText confidence="0.9999858">
To validate our definition of paraphrase evaluation
and the PEM method, we conduct an experiment
to evaluate paraphrase qualities manually, which al-
lows us to judge whether paraphrase evaluation ac-
cording to our definition is an inherently coherent
and well-defined problem. The evaluation also al-
lows us to establish an upper bound for the para-
phrase evaluation task, and to validate the contribu-
tion of the three proposed criteria to the overall para-
phrase score.
</bodyText>
<subsectionHeader confidence="0.974973">
5.1 Evaluation setup
</subsectionHeader>
<bodyText confidence="0.999996066666667">
We use the Multiple-Translation Chinese Corpus
(MTC)2 as a source of paraphrases. The MTC
corpus consists of Chinese news articles (993 sen-
tences in total) and multiple sentence-aligned En-
glish translations. We select one human transla-
tion as the original text. Two other human transla-
tions and two automatic machine translations serve
as paraphrases of the original sentences. We refer to
the two human translations and the two MT systems
as paraphrase systems human1, human2, machine1,
and machine2.
We employ three human judges to manually as-
sess the quality of 300 original sentences paired
with each of the four paraphrases. Therefore, each
judge assesses 1,200 paraphrase pairs in total. The
</bodyText>
<equation confidence="0.927594666666667">
2LDC Catalog No.: LDC2002T01
P. = length(S)
log Pr(S)
</equation>
<page confidence="0.986338">
927
</page>
<bodyText confidence="0.9989405">
judgment for each paraphrase pair consists of four
scores, each given on a five-point scale:
</bodyText>
<listItem confidence="0.999746166666667">
• Adequacy (Is the meaning preserved ade-
quately?)
• Fluency (Is the paraphrase fluent English?)
• Lexical Dissimilarity (How much has the para-
phrase changed the original sentence?)
• Overall score
</listItem>
<bodyText confidence="0.999884111111111">
The instructions given to the judges for the overall
score were as follows.
A good paraphrase should convey the
same meaning as the original sentence,
while being as different as possible on the
surface form and being fluent and gram-
matical English. With respect to this defi-
nition, give an overall score from 5 (per-
fect) to 1 (unacceptable) for this para-
phrase.
The paraphrases are presented to the judges in a ran-
dom order and without any information as to which
paraphrase system produced the paraphrase.
In addition to the four paraphrase systems men-
tioned above, for each original English sentence, we
add three more artificially constructed paraphrases
with pre-determined “human” judgment scores: (1)
the original sentence itself, with adequacy 5, fluency
5, dissimilarity 1, and overall score 2; (2) a random
sentence drawn from the same domain, with ade-
quacy 1, fluency 5, dissimilarity 5, and overall score
1; and (3) a random sentence generated by a uni-
gram language model, with adequacy 1, fluency 1,
dissimilarity 5, and overall score 1. These artificial
paraphrases serve as controls in our evaluation. Our
final data set therefore consists of 2,100 paraphrase
pairs with judgments on 4 different criteria.
</bodyText>
<subsectionHeader confidence="0.971393">
5.2 Inter-judge correlation
</subsectionHeader>
<bodyText confidence="0.999803555555556">
The first step in our evaluation is to investigate the
correlation between the human judges. We use Pear-
son’s correlation coefficient, a common measure of
the linear dependence between two random vari-
ables.
We investigate inter-judge correlation at the sen-
tence and at the system level. At the sentence
level, we construct three vectors, each containing
the 1,200 sentence level judgments from one judge
</bodyText>
<table confidence="0.98424625">
Sentence Level System Level
Judge A Judge B Judge A Judge B
Judge B 0.6406 - 0.9962 -
Judge C 0.6717 0.5993 0.9995 0.9943
</table>
<tableCaption confidence="0.9306905">
Table 1: Inter-judge correlation for overall paraphrase
score
</tableCaption>
<table confidence="0.9997682">
Sentence Level System Level
Adequacy 0.7635 0.7616
Fluency 0.3736 0.3351
Dissimilarity -0.3737 -0.3937
Dissimilarity (A,F&gt;4) 0.8881 0.9956
</table>
<tableCaption confidence="0.982522">
Table 2: Correlation of paraphrase criteria with overall
score
</tableCaption>
<bodyText confidence="0.999947695652174">
for the overall score. The pair-wise correlations be-
tween these three vectors are then taken. Note that
we exclude the three artificial control paraphrase
systems from consideration, as that would inflate the
correlation. At the system level, we construct three
vectors each of size four, containing the average
scores given by one judge to each of the four para-
phrase systems human1, human2, machine1, and
machine2. The correlations are then taken in the
same fashion.
The results are listed in Table 1. The inter-judge
correlation is between 0.60 and 0.67 at the sentence
level and above 0.99 at the system level. These cor-
relation scores can be considered very high when
compared to similar results reported in MT evalu-
ations, e.g., Blatz et al. (2003). The high correlation
confirms that our evaluation task is well defined.
Having confirmed that human judgments corre-
late strongly, we combine the scores of the three
judges by taking their arithmetic mean. Together
with the three artificial control paraphrase systems,
they form the human reference evaluation which we
use for the remainder of the experiments.
</bodyText>
<subsectionHeader confidence="0.996362">
5.3 Adequacy, fluency, and dissimilarity
</subsectionHeader>
<bodyText confidence="0.999718714285714">
In this section, we empirically validate the impor-
tance of our three proposed criteria: adequacy, flu-
ency, and lexical dissimilarity. This can be done by
measuring the correlation of each criterion with the
overall score. The system and sentence level corre-
lations are shown in Table 2.
We can see a positive correlation of adequacy and
</bodyText>
<page confidence="0.995483">
928
</page>
<figureCaption confidence="0.9894585">
Figure 3: Scatter plot of dissimilarity vs. overall score
for paraphrases with high adequacy and fluency.
</figureCaption>
<bodyText confidence="0.962761233333334">
fluency with the overall score, and the correlation
with adequacy is particularly strong. Thus, higher
adequacy and to a lesser degree higher fluency indi-
cate higher paraphrase quality to the human judges.
On the other hand, dissimilarity is found to have a
negative correlation with the overall score. This can
be explained by the fact that the two human trans-
lations usually have much higher similarity with the
reference translation, and at the same time are scored
as better paraphrases. This effect dominates a sim-
ple linear fitting of the paraphrase score vs. the dis-
similarity, resulting in the counter intuitive negative
correlation. We note that a high dissimilarity alone
tells us little about the quality of the paraphrase.
Rather, we expect dissimilarity to be a differentia-
tor between the mediocre and good paraphrases.
To test this hypothesis, we select the subset of
paraphrase pairs that receive adequacy and fluency
scores of at least four and again measure the cor-
relation of the dissimilarity and the overall score.
The result is tabulated in the last row of Table 2 and
shows a strong correlation. Figure 3 shows a scatter
plot of the same result3.
The empirical results presented so far confirm that
paraphrase evaluation is a well-defined task permit-
ting consistent subjective judgments, and that ade-
quacy, fluency, and dissimilarity are suitable criteria
for paraphrase quality.
3We automatically add jitter (small amounts of noise) for
ease of presentation.
</bodyText>
<sectionHeader confidence="0.737479" genericHeader="method">
6 PEM vs. human evaluation
</sectionHeader>
<bodyText confidence="0.999975333333333">
In the last section, we have shown that the three
proposed criteria are good indicators of paraphrase
quality. In this section, we investigate how well
PEM can predict the overall paraphrase quality from
the three automatic metrics (pivot language F1, nor-
malized language model, and target language F1),
designed to match the three evaluation criteria. We
describe the experimental setup in Section 6.1, be-
fore we show the results in Section 6.2.
</bodyText>
<subsectionHeader confidence="0.998371">
6.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999988285714286">
We build the phrase table used to evaluate the pivot
language F1 from the FBIS Chinese-English corpus,
consisting of about 250,000 Chinese sentences, each
with a single English translation. The paraphrases
are taken from the MTC corpus in the same way
as the human experiment described in Section 5.1.
Both FBIS and MTC are in the Chinese newswire
domain.
We stem all English words in both data sets with
the Porter stemmer (Porter, 1980). We use the maxi-
mum entropy segmenter of (Low et al., 2005) to seg-
ment the Chinese part of the FBIS corpus. Subse-
quently, word level Chinese-English alignments are
generated using the Berkeley aligner (Liang et al.,
2006; Haghighi et al., 2009) with five iterations of
training. Phrases are then extracted with the widely-
used heuristic in Koehn et al. (2003). We extract
phrases of up to four words in length.
Bags of Chinese pivot language N-grams are ex-
tracted for all paraphrase pairs as described in Sec-
tion 4.3. For computational efficiency, we consider
only edges of the confusion network with probabil-
ities higher than 0.1, and only N-grams with proba-
bilities higher than 0.01 in the bag of N-grams. We
collect N-grams up to length four.
The language model used to judge fluency is
trained on the English side of the FBIS parallel text.
We use SRILM (Stolcke, 2002) to build a 4-gram
model with the default parameters.
The PEM SVM regression is trained on the para-
phrase pairs for the first 200 original English sen-
tences and tested on the paraphrase pairs of the re-
maining 100 original English sentences. Thus, there
are 1,400 instances for training and 700 instances for
testing. For each instance, we calculate the values
</bodyText>
<page confidence="0.996949">
929
</page>
<figureCaption confidence="0.999585">
Figure 4: Scatter plot of PEM vs. human judgment (over-
all score) at the sentence level
Figure 5: Scatter plot of PEM vs. human judgment (over-
all score) at the system level
</figureCaption>
<bodyText confidence="0.9999698">
of pivot language F1, normalized language model
score, and target language F1. These values serve
as the input features to the SVM regression and the
target value is the human assessment of the overall
score, on a scale of 1 to 5.
</bodyText>
<subsectionHeader confidence="0.965091">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.9998911">
As in the human evaluation, we investigate the cor-
relation of the PEM scores with the human judg-
ments at the sentence and at the system level. Fig-
ure 4 shows the sentence level PEM scores plotted
against the human overall scores, where each human
overall score is the arithmetic mean of the scores
given by the three judges. The Pearson correlation
between the automatic PEM scores and the human
judgments is 0.8073. This is substantially higher
than the sentence level correlation of MT metrics
</bodyText>
<note confidence="0.9617018">
Sentence Level System Level
PEM vs. Human Avg. 0.8073 0.9867
PEM vs. Judge A 0.5777 0.9757
PEM vs. Judge B 0.5281 0.9892
PEM vs. Judge C 0.5231 0.9718
</note>
<tableCaption confidence="0.993082">
Table 3: Correlation of PEM with human judgment (over-
all score)
</tableCaption>
<bodyText confidence="0.988996454545455">
like BLEU. For example, the highest sentence level
Pearson correlation by any metric in the Metrics-
MATR 2008 competition (Przybocki et al., 2009)
was 0.6855 by METEOR-v0.6; BLEU achieved a
correlation of 0.4513.
Figure 5 shows the system level PEM scores plot-
ted against the human scores. The Pearson correla-
tion between PEM scores and the human scores at
the system level is 0.9867.
We also calculate the Pearson correlation between
PEM and each individual human judge. Here, we
exclude the three artificial control paraphrase sys-
tems from the data, to make the results compara-
ble to the inter-judge correlation presented in Sec-
tion 5.2. The correlation is between 0.52 and 0.57
at the sentence level and between 0.97 and 0.98 at
the system level. As we would expect, the correla-
tion between PEM and a human judge is not as high
as the correlation between two human judges, but
PEM still shows a strong and consistent correlation
with all three judges. The results are summarized in
Table 3.
</bodyText>
<sectionHeader confidence="0.982361" genericHeader="discussions">
7 Discussion and future work
</sectionHeader>
<bodyText confidence="0.999926">
The paraphrases that we use in this study are not
actual machine generated paraphrases. Instead, the
English paraphrases are multiple translations of the
same Chinese source sentence. Our seven “para-
phrase systems” are two human translators, two ma-
chine translation systems, and three artificially cre-
ated extreme scenarios. The reason for using multi-
ple translations is that we could not find any PG sys-
tem that can paraphrase a whole input sentence and
is publicly available. We intend to obtain and evalu-
ate paraphrases generated from real PG systems and
compare their performances in a follow-up study.
Our method models paraphrasing up to the phrase
level. Unfortunately, it makes no provisions for syn-
</bodyText>
<page confidence="0.987312">
930
</page>
<bodyText confidence="0.999817">
tactic paraphrasing at the sentence level, which is
probably a much greater challenge, and the literature
offers few successes to draw inspirations from. We
hope to be able to partially address this deficiency in
future work.
The only external linguistic resource required by
PEM is a parallel text of the target language and
another arbitrary language. While we only use
Chinese-English parallel text in this study, other lan-
guage pairs need to be explored too. Another alter-
native is to collect parallel texts against multiple for-
eign languages, e.g., using Europarl (Koehn, 2005).
We leave this for future work.
Our evaluation method does not require human-
generated references like in MT evaluation. There-
fore, we can easily formulate a paraphrase genera-
tor by directly optimizing the PEM metric, although
solving it is not trivial:
</bodyText>
<equation confidence="0.9943205">
paraphrase(R) = arg max PEM(P, R)
P
</equation>
<bodyText confidence="0.999860111111111">
where R is the original sentence and P is the para-
phrase.
Finally, the PEM metric, in particular the seman-
tic representation BPNG, can be useful in many
other contexts, such as MT evaluation, summary
evaluation, and paraphrase recognition. To facil-
itate future research, we will package and release
PEM under an open source license at http://
nlp.comp.nus.edu.sg/software.
</bodyText>
<sectionHeader confidence="0.997373" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999982636363636">
We proposed PEM, a novel automatic metric for
paraphrase evaluation based on adequacy, fluency,
and lexical dissimilarity. The key component in our
metric is a novel technique to measure the seman-
tic similarity of two sentences through their N-gram
overlap in an aligned foreign language text. We
conducted an extensive human evaluation of para-
phrase quality which shows that our proposed met-
ric achieves high correlation with human judgments.
To the best of our knowledge, PEM is the first auto-
matic metric for paraphrase evaluation.
</bodyText>
<sectionHeader confidence="0.998294" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9984182">
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA)
of Singapore.
</bodyText>
<sectionHeader confidence="0.990159" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999826638297872">
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proc. of ACL.
R. Barzilay and L. Lee. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. In Proc. of HLT-NAACL.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,
A. Kulesza, A. Sanchis, and N. Ueffing. 2003. Con-
fidence estimation for machine translation. Technical
report, CLSP Workshop Johns Hopkins University.
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proc. of HLT-NAACL.
C. Callison-Burch, T. Cohn, and M. Lapata. 2008. Para-
Metric: An automatic evaluation metric for paraphras-
ing. In Proc. of COLING.
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.
2009. Findings of the 2009 Workshop on Statistical
Machine Translation. In Proceedings of WMT.
Y.S. Chan and H.T. Ng. 2008. MAXSIM: A maximum
similarity metric for machine translation evaluation.
In Proc. of ACL-08: HLT.
D. Das and N.A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition. In
Proc. ofACL-IJCNLP.
P. Duboue and J. Chu-Carroll. 2006. Answering the
question you wish they had asked: The impact of para-
phrasing for question answering. In Proc. of HLT-
NAACL Companion Volume: Short Papers.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press, Cambridge, MA.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Proc. of ACL-IJCNLP.
M. Heilman and N.A. Smith. 2010. Tree edit models
for recognizing textual entailments, paraphrases, and
answers to questions. In Proc. of NAACL.
E. Hovy, C.Y. Lin, L. Zhou, and J. Fukumoto. 2006.
Automated summarization evaluation with basic ele-
ments. In Proc. of LREC.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Schölkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT Press.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proc. of HLT-NAACL.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
</reference>
<page confidence="0.976812">
931
</page>
<reference confidence="0.999928941176471">
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit, volume 5.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of HLT-NAACL.
C.Y. Lin. 2004. ROUGE: A package for automatic eval-
uation of summaries. In Proc. of the ACL-04 Work-
shop on Text Summarization Branches Out.
C. Liu, D. Dahlmeier, and H.T. Ng. 2010. TESLA:
translation evaluation of sentences with linear-
programming-based analysis. In Proc. of WMT.
J.K. Low, H.T. Ng, and W. Guo. 2005. A maximum
entropy approach to Chinese word segmentation. In
Proc. of the 4th SIGHAN Workshop.
N. Madnani, N.F. Ayan, P. Resnik, and B.J. Dorr. 2007.
Using paraphrases for parameter tuning in statistical
machine translation. In Proc. of WMT.
N. Madnani, P. Resnik, B.J. Dorr, and R. Schwartz. 2008.
Are multiple reference translations necessary? Investi-
gating the value of paraphrased reference translations
in parameter optimization. In Proc. of AMTA.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
K. Owczarzak, D. Groves, J. Van Genabith, and A. Way.
2006. Contextual bitext-derived paraphrases in auto-
matic MT evaluation. In Proc. of WMT.
B. Pang, K. Knight, and D. Marcu. 2003. Syntax-based
alignment of multiple translations: Extracting para-
phrases and generating new sentences. In Proc. of
HLT-NAACL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
M. Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 40(3).
M. Przybocki, K. Peterson, S. Bronsart, and G Sanders.
2009. Evaluating machine translation with LFG de-
pendencies. Machine Translation, 23(2).
L. Qiu, M.Y. Kan, and T.S. Chua. 2006. Paraphrase
recognition via dissimilarity significance classifica-
tion. In Proc. of EMNLP.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolin-
gual machine translation for paraphrase generation. In
Proc. of EMNLP.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009.
Fluency, adequacy, or HTER? Exploring different hu-
man judgments with a tunable MT metric. In Proc. of
WMT.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. of ICSLP.
S. Wan, M. Dras, R. Dale, and C Paris. 2006. Using
dependency-based features to take the ’para-farce’ out
of paraphrase. In Proc. ofALTW 2006.
H. Wu and M. Zhou. 2003. Synonymous collocation
extraction using translation information. In Proc. of
ACL.
S.Q. Zhao, C. Niu, M. Zhou, T. Liu, and S. Li. 2008.
Combining multiple resources to improve SMT-based
paraphrasing model. In Proc. of ACL-08: HLT.
S.Q. Zhao, X. Lan, T. Liu, and S. Li. 2009. Application-
driven statistical paraphrase generation. In Proc. of
ACL-IJCNLP.
L. Zhou, C.Y. Lin, and E. Hovy. 2006a. Re-evaluating
machine translation results with paraphrase support.
In Proc. of EMNLP.
L. Zhou, C.Y. Lin, D.S. Munteanu, and E. Hovy. 2006b.
ParaEval: Using paraphrases to evaluate summaries
automatically. In Proc. of HLT-NAACL.
</reference>
<page confidence="0.997137">
932
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.579789">
<title confidence="0.999865">PEM: A Paraphrase Evaluation Metric Exploiting Parallel Texts</title>
<author confidence="0.981117">Tou</author>
<affiliation confidence="0.986356">of Computer Science, National University of Graduate School for Integrative Sciences and</affiliation>
<email confidence="0.621783">liuchan1@comp.nus.edu.sg</email>
<email confidence="0.621783">danielhe@comp.nus.edu.sg</email>
<email confidence="0.621783">nght@comp.nus.edu.sg</email>
<abstract confidence="0.996641076923077">We present PEM, the first fully automatic metric to evaluate the quality of paraphrases, and consequently, that of paraphrase generation systems. Our metric is based on three criteand dissimilar- The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This research was done for CSIDM Project No. CSIDM-200804 partially funded by a grant from the National Research Foundation (NRF) administered by the Media Development Authority (MDA) of Singapore.</title>
<marker></marker>
<rawString>This research was done for CSIDM Project No. CSIDM-200804 partially funded by a grant from the National Research Foundation (NRF) administered by the Media Development Authority (MDA) of Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>References C Bannard</author>
<author>C Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1004" citStr="Bannard and Callison-Burch, 2005" startWordPosition="146" endWordPosition="149">aphrases, and consequently, that of paraphrase generation systems. Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the ap</context>
<context position="10177" citStr="Bannard and Callison-Burch, 2005" startWordPosition="1640" endWordPosition="1643">s an additional resource. We can then align the parallel texts at word level automatically using well-known algorithms such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text. If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning. Similar observations have been made by previous researchers (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We can treat the distribution of aligned French phrases as a semantic representation of the English phrase. The semantic distance between two English phrases can then be measured by their degree of overlap in this representation. In this work, we use the widely-used phrase extraction heuristic in (Koehn et al., 2003) to extract phrase pairs from parallel texts into a phrase table1. The phrases extracted do not necessarily correspond to the speakers’ intuition. Rather, they are units whose boundaries are preserved during translation. However,</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>References C. Bannard and C. Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiple-sequence alignment.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="931" citStr="Barzilay and Lee, 2003" startWordPosition="133" endWordPosition="137">the first fully automatic metric to evaluate the quality of paraphrases, and consequently, that of paraphrase generation systems. Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on wha</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>R. Barzilay and L. Lee. 2003. Learning to paraphrase: An unsupervised approach using multiple-sequence alignment. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blatz</author>
<author>E Fitzgerald</author>
<author>G Foster</author>
<author>S Gandrabur</author>
<author>C Goutte</author>
<author>A Kulesza</author>
<author>A Sanchis</author>
<author>N Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>CLSP Workshop Johns Hopkins University.</institution>
<contexts>
<context position="22008" citStr="Blatz et al. (2003)" startWordPosition="3581" endWordPosition="3584">control paraphrase systems from consideration, as that would inflate the correlation. At the system level, we construct three vectors each of size four, containing the average scores given by one judge to each of the four paraphrase systems human1, human2, machine1, and machine2. The correlations are then taken in the same fashion. The results are listed in Table 1. The inter-judge correlation is between 0.60 and 0.67 at the sentence level and above 0.99 at the system level. These correlation scores can be considered very high when compared to similar results reported in MT evaluations, e.g., Blatz et al. (2003). The high correlation confirms that our evaluation task is well defined. Having confirmed that human judgments correlate strongly, we combine the scores of the three judges by taking their arithmetic mean. Together with the three artificial control paraphrase systems, they form the human reference evaluation which we use for the remainder of the experiments. 5.3 Adequacy, fluency, and dissimilarity In this section, we empirically validate the importance of our three proposed criteria: adequacy, fluency, and lexical dissimilarity. This can be done by measuring the correlation of each criterion</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2003</marker>
<rawString>J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. 2003. Confidence estimation for machine translation. Technical report, CLSP Workshop Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>M Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="10206" citStr="Callison-Burch et al., 2006" startWordPosition="1644" endWordPosition="1647">hen align the parallel texts at word level automatically using well-known algorithms such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text. If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning. Similar observations have been made by previous researchers (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We can treat the distribution of aligned French phrases as a semantic representation of the English phrase. The semantic distance between two English phrases can then be measured by their degree of overlap in this representation. In this work, we use the widely-used phrase extraction heuristic in (Koehn et al., 2003) to extract phrase pairs from parallel texts into a phrase table1. The phrases extracted do not necessarily correspond to the speakers’ intuition. Rather, they are units whose boundaries are preserved during translation. However, the distinction does not aff</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>C. Callison-Burch, P. Koehn, and M. Osborne. 2006. Improved statistical machine translation using paraphrases. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>T Cohn</author>
<author>M Lapata</author>
</authors>
<title>ParaMetric: An automatic evaluation metric for paraphrasing.</title>
<date>2008</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="6517" citStr="Callison-Burch et al., 2008" startWordPosition="1046" endWordPosition="1049">ation metrics, our method focuses on addressing the unique requirement of paraphrase evaluation: that lexical closeness does not necessarily entail goodness, contrary to the basis of MT evaluation. Inspired by the success of automatic MT evaluation, Lin (2004) and Hovy et al. (2006) propose automatic metrics for summary evaluation. The former is entirely lexical based, whereas the latter also exploits constituent and dependency parses, and semantic features derived from WordNet. The only prior attempt to devise an automatic evaluation metric for paraphrases that we are aware of is ParaMetric (Callison-Burch et al., 2008), which compares the collection of paraphrases discovered by automatic paraphrasing algorithms against a manual gold standard collected over the same sentences. The recall and precision of several current paraphrase generation systems are evaluated. ParaMetric does not attempt to propose a single metric to correlate well with human judgments. Rather, it consists of a few indirect and partial measures of the quality of PG systems. 3 Task definition The first step in defining a paraphrase evaluation metric is to define a good paraphrase. MerriamWebster dictionary gives the following definition: </context>
</contexts>
<marker>Callison-Burch, Cohn, Lapata, 2008</marker>
<rawString>C. Callison-Burch, T. Cohn, and M. Lapata. 2008. ParaMetric: An automatic evaluation metric for paraphrasing. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>J Schroeder</author>
</authors>
<date>2009</date>
<booktitle>Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of WMT.</booktitle>
<contexts>
<context position="5320" citStr="Callison-Burch et al., 2009" startWordPosition="860" endWordPosition="863">yze the results in Section 5. The correlation of PEM with human judgments is studied in Section 6. Finally, we discuss our findings and future work in Section 7 and conclude in Section 8. 2 Related work The most well-known automatic evaluation metric in NLP is BLEU (Papineni et al., 2002) for MT, based on N-gram matching precisions. The simplicity of BLEU lends well to MT techniques that directly optimize the evaluation metric. The weakness of BLEU is that it operates purely at the lexical surface level. Later works attempt to take more syntactic and semantic features into consideration (see (Callison-Burch et al., 2009) for an overview). The whole spectrum of NLP resources has found application in machine translation evaluation, including POS tags, constituent and dependency parses, WordNet (Fellbaum, 1998), semantic roles, textual entailment features, and more. Many of these metrics have been shown to correlate better with human judges than BLEU (Chan and Ng, 2008; Liu et al., 2010). Interestingly, few MT evaluation metrics exploit parallel texts as a source of information, when statistical MT is centered almost entirely around mining parallel texts. Compared to these MT evaluation metrics, our method focus</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
</authors>
<title>MAXSIM: A maximum similarity metric for machine translation evaluation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08:</booktitle>
<publisher>HLT.</publisher>
<contexts>
<context position="5672" citStr="Chan and Ng, 2008" startWordPosition="915" endWordPosition="918"> well to MT techniques that directly optimize the evaluation metric. The weakness of BLEU is that it operates purely at the lexical surface level. Later works attempt to take more syntactic and semantic features into consideration (see (Callison-Burch et al., 2009) for an overview). The whole spectrum of NLP resources has found application in machine translation evaluation, including POS tags, constituent and dependency parses, WordNet (Fellbaum, 1998), semantic roles, textual entailment features, and more. Many of these metrics have been shown to correlate better with human judges than BLEU (Chan and Ng, 2008; Liu et al., 2010). Interestingly, few MT evaluation metrics exploit parallel texts as a source of information, when statistical MT is centered almost entirely around mining parallel texts. Compared to these MT evaluation metrics, our method focuses on addressing the unique requirement of paraphrase evaluation: that lexical closeness does not necessarily entail goodness, contrary to the basis of MT evaluation. Inspired by the success of automatic MT evaluation, Lin (2004) and Hovy et al. (2006) propose automatic metrics for summary evaluation. The former is entirely lexical based, whereas the</context>
</contexts>
<marker>Chan, Ng, 2008</marker>
<rawString>Y.S. Chan and H.T. Ng. 2008. MAXSIM: A maximum similarity metric for machine translation evaluation. In Proc. of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>N A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In Proc. ofACL-IJCNLP.</booktitle>
<contexts>
<context position="3973" citStr="Das and Smith, 2009" startWordPosition="628" endWordPosition="631">importantly, BPNG does not depend on lexical or syntactic similarity, allowing us to address the conflicting requirements of paraphrase evaluation. The only linguistic re923 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 923–932, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics source required to evaluate BPNG is a parallel text of the target language and an arbitrary other language, known as the pivot language. We highlight that paraphrase evaluation and paraphrase recognition (Heilman and Smith, 2010; Das and Smith, 2009; Wan et al., 2006; Qiu et al., 2006) are related yet distinct tasks. Consider two sentences 51 and 52 that are the same except for the substitution of a single synonym. A paraphrase recognition system should assign them a very high score, but a paraphrase evaluation system would assign a relatively low one. Indeed, the latter is often a better indicator of how useful a PG system potentially is for the applications of PG described earlier. The rest of the paper is organized as follows. We survey other automatic evaluation metrics in natural language processing (NLP) in Section 2. We define the</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>D. Das and N.A. Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In Proc. ofACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Duboue</author>
<author>J Chu-Carroll</author>
</authors>
<title>Answering the question you wish they had asked: The impact of paraphrasing for question answering.</title>
<date>2006</date>
<booktitle>In Proc. of HLTNAACL Companion Volume: Short Papers.</booktitle>
<contexts>
<context position="1411" citStr="Duboue and Chu-Carroll, 2006" startWordPosition="214" endWordPosition="217">an judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the application of paraphrases tend to build their own PG system in view of the immediate needs instead of using an existing system. Second, and as a consequence, no automatic evaluation metric exists for paraphrases. Most works in this area resort to ad hoc manual evaluations, such as the percentage of “yes” judgments to the question of “is the meaning preserved”. This type of evaluation is incomprehensive, e</context>
</contexts>
<marker>Duboue, Chu-Carroll, 2006</marker>
<rawString>P. Duboue and J. Chu-Carroll. 2006. Answering the question you wish they had asked: The impact of paraphrasing for question answering. In Proc. of HLTNAACL Companion Volume: Short Papers.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An electronic lexical database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>J Blitzer</author>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>Better word alignments with supervised ITG models.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP.</booktitle>
<contexts>
<context position="9766" citStr="Haghighi et al., 2009" startWordPosition="1568" endWordPosition="1571">endent of any lexical similarity. We address this problem in Sections 4.1 to 4.3. The remaining two criteria are addressed in Section 4.4, and we describe the final combined metric PEM in Section 4.5. 4.1 Phrase-level semantic representation Without loss of generality, suppose we are to evaluate English paraphrases, and have been supplied many sentence-aligned parallel texts of French and English as an additional resource. We can then align the parallel texts at word level automatically using well-known algorithms such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text. If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning. Similar observations have been made by previous researchers (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We can treat the distribution of aligned French phrases as a semantic representation of the English phrase. The semantic distance betwee</context>
<context position="25520" citStr="Haghighi et al., 2009" startWordPosition="4148" endWordPosition="4151"> F1 from the FBIS Chinese-English corpus, consisting of about 250,000 Chinese sentences, each with a single English translation. The paraphrases are taken from the MTC corpus in the same way as the human experiment described in Section 5.1. Both FBIS and MTC are in the Chinese newswire domain. We stem all English words in both data sets with the Porter stemmer (Porter, 1980). We use the maximum entropy segmenter of (Low et al., 2005) to segment the Chinese part of the FBIS corpus. Subsequently, word level Chinese-English alignments are generated using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009) with five iterations of training. Phrases are then extracted with the widelyused heuristic in Koehn et al. (2003). We extract phrases of up to four words in length. Bags of Chinese pivot language N-grams are extracted for all paraphrase pairs as described in Section 4.3. For computational efficiency, we consider only edges of the confusion network with probabilities higher than 0.1, and only N-grams with probabilities higher than 0.01 in the bag of N-grams. We collect N-grams up to length four. The language model used to judge fluency is trained on the English side of the FBIS parallel text. </context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009. Better word alignments with supervised ITG models. In Proc. of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>N A Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="3952" citStr="Heilman and Smith, 2010" startWordPosition="624" endWordPosition="627">language sentences. Most importantly, BPNG does not depend on lexical or syntactic similarity, allowing us to address the conflicting requirements of paraphrase evaluation. The only linguistic re923 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 923–932, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics source required to evaluate BPNG is a parallel text of the target language and an arbitrary other language, known as the pivot language. We highlight that paraphrase evaluation and paraphrase recognition (Heilman and Smith, 2010; Das and Smith, 2009; Wan et al., 2006; Qiu et al., 2006) are related yet distinct tasks. Consider two sentences 51 and 52 that are the same except for the substitution of a single synonym. A paraphrase recognition system should assign them a very high score, but a paraphrase evaluation system would assign a relatively low one. Indeed, the latter is often a better indicator of how useful a PG system potentially is for the applications of PG described earlier. The rest of the paper is organized as follows. We survey other automatic evaluation metrics in natural language processing (NLP) in Sec</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>M. Heilman and N.A. Smith. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>C Y Lin</author>
<author>L Zhou</author>
<author>J Fukumoto</author>
</authors>
<title>Automated summarization evaluation with basic elements.</title>
<date>2006</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="6172" citStr="Hovy et al. (2006)" startWordPosition="993" endWordPosition="996">es, and more. Many of these metrics have been shown to correlate better with human judges than BLEU (Chan and Ng, 2008; Liu et al., 2010). Interestingly, few MT evaluation metrics exploit parallel texts as a source of information, when statistical MT is centered almost entirely around mining parallel texts. Compared to these MT evaluation metrics, our method focuses on addressing the unique requirement of paraphrase evaluation: that lexical closeness does not necessarily entail goodness, contrary to the basis of MT evaluation. Inspired by the success of automatic MT evaluation, Lin (2004) and Hovy et al. (2006) propose automatic metrics for summary evaluation. The former is entirely lexical based, whereas the latter also exploits constituent and dependency parses, and semantic features derived from WordNet. The only prior attempt to devise an automatic evaluation metric for paraphrases that we are aware of is ParaMetric (Callison-Burch et al., 2008), which compares the collection of paraphrases discovered by automatic paraphrasing algorithms against a manual gold standard collected over the same sentences. The recall and precision of several current paraphrase generation systems are evaluated. ParaM</context>
</contexts>
<marker>Hovy, Lin, Zhou, Fukumoto, 2006</marker>
<rawString>E. Hovy, C.Y. Lin, L. Zhou, and J. Fukumoto. 2006. Automated summarization evaluation with basic elements. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical. In</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<location>Learning.</location>
<contexts>
<context position="17401" citStr="Joachims, 1999" startWordPosition="2843" endWordPosition="2844">he final PEM metric, we combine the three component automatic metrics, pivot language F1, normalized language model, and target language F1, which measure adequacy, fluency, and dissimilarity respectively. As discussed previously, a linear combination of the three component metrics is insufficient. We turn to support vector machine (SVM) regression with the radial basis function (RBF) kernel. The RBF is a simple and expressive function, commonly used to introduce non-linearity into large margin classifications and regressions. RBF(xZ7 xj) = e−711xi−xj112 We use the implementation in SVMlight (Joachims, 1999). The SVM is to be trained on a set of human-judged paraphrase pairs, where the three component automatic metrics are fit to the human overall assessment. After training, the model can then be used to evaluate new paraphrase pairs in a fully automatic fashion. 5 Human evaluation To validate our definition of paraphrase evaluation and the PEM method, we conduct an experiment to evaluate paraphrase qualities manually, which allows us to judge whether paraphrase evaluation according to our definition is an inherently coherent and well-defined problem. The evaluation also allows us to establish an</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale SVM learning practical. In B. Schölkopf, C. Burges, and A. Smola, Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kauchak</author>
<author>R Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="1032" citStr="Kauchak and Barzilay, 2006" startWordPosition="150" endWordPosition="153">f paraphrase generation systems. Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the application of paraphrases ten</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>D. Kauchak and R. Barzilay. 2006. Paraphrasing for automatic evaluation. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<pages>931</pages>
<contexts>
<context position="10548" citStr="Koehn et al., 2003" startWordPosition="1701" endWordPosition="1704">gment of an English text. If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning. Similar observations have been made by previous researchers (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We can treat the distribution of aligned French phrases as a semantic representation of the English phrase. The semantic distance between two English phrases can then be measured by their degree of overlap in this representation. In this work, we use the widely-used phrase extraction heuristic in (Koehn et al., 2003) to extract phrase pairs from parallel texts into a phrase table1. The phrases extracted do not necessarily correspond to the speakers’ intuition. Rather, they are units whose boundaries are preserved during translation. However, the distinction does not affect our work. 4.2 Segmenting a sentence into phrases Having established a way to measure the similarity of two English phrases, we now extend the concept to sentences. Here we discuss how to segment an English sentence (the original or the paraphrase) into phrases. From the phrase table, we know the frequencies of all the phrases and we app</context>
<context position="25634" citStr="Koehn et al. (2003)" startWordPosition="4167" endWordPosition="4170">ranslation. The paraphrases are taken from the MTC corpus in the same way as the human experiment described in Section 5.1. Both FBIS and MTC are in the Chinese newswire domain. We stem all English words in both data sets with the Porter stemmer (Porter, 1980). We use the maximum entropy segmenter of (Low et al., 2005) to segment the Chinese part of the FBIS corpus. Subsequently, word level Chinese-English alignments are generated using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009) with five iterations of training. Phrases are then extracted with the widelyused heuristic in Koehn et al. (2003). We extract phrases of up to four words in length. Bags of Chinese pivot language N-grams are extracted for all paraphrase pairs as described in Section 4.3. For computational efficiency, we consider only edges of the confusion network with probabilities higher than 0.1, and only N-grams with probabilities higher than 0.01 in the bag of N-grams. We collect N-grams up to length four. The language model used to judge fluency is trained on the English side of the FBIS parallel text. We use SRILM (Stolcke, 2002) to build a 4-gram model with the default parameters. The PEM SVM regression is traine</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proc. of HLT-NAACL. 931</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit,</booktitle>
<volume>5</volume>
<contexts>
<context position="29953" citStr="Koehn, 2005" startWordPosition="4902" endWordPosition="4903">akes no provisions for syn930 tactic paraphrasing at the sentence level, which is probably a much greater challenge, and the literature offers few successes to draw inspirations from. We hope to be able to partially address this deficiency in future work. The only external linguistic resource required by PEM is a parallel text of the target language and another arbitrary language. While we only use Chinese-English parallel text in this study, other language pairs need to be explored too. Another alternative is to collect parallel texts against multiple foreign languages, e.g., using Europarl (Koehn, 2005). We leave this for future work. Our evaluation method does not require humangenerated references like in MT evaluation. Therefore, we can easily formulate a paraphrase generator by directly optimizing the PEM metric, although solving it is not trivial: paraphrase(R) = arg max PEM(P, R) P where R is the original sentence and P is the paraphrase. Finally, the PEM metric, in particular the semantic representation BPNG, can be useful in many other contexts, such as MT evaluation, summary evaluation, and paraphrase recognition. To facilitate future research, we will package and release PEM under a</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit, volume 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="9742" citStr="Liang et al., 2006" startWordPosition="1564" endWordPosition="1567">ty, completely independent of any lexical similarity. We address this problem in Sections 4.1 to 4.3. The remaining two criteria are addressed in Section 4.4, and we describe the final combined metric PEM in Section 4.5. 4.1 Phrase-level semantic representation Without loss of generality, suppose we are to evaluate English paraphrases, and have been supplied many sentence-aligned parallel texts of French and English as an additional resource. We can then align the parallel texts at word level automatically using well-known algorithms such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text. If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning. Similar observations have been made by previous researchers (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We can treat the distribution of aligned French phrases as a semantic representation of the English phrase. The </context>
<context position="25496" citStr="Liang et al., 2006" startWordPosition="4144" endWordPosition="4147">e the pivot language F1 from the FBIS Chinese-English corpus, consisting of about 250,000 Chinese sentences, each with a single English translation. The paraphrases are taken from the MTC corpus in the same way as the human experiment described in Section 5.1. Both FBIS and MTC are in the Chinese newswire domain. We stem all English words in both data sets with the Porter stemmer (Porter, 1980). We use the maximum entropy segmenter of (Low et al., 2005) to segment the Chinese part of the FBIS corpus. Subsequently, word level Chinese-English alignments are generated using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009) with five iterations of training. Phrases are then extracted with the widelyused heuristic in Koehn et al. (2003). We extract phrases of up to four words in length. Bags of Chinese pivot language N-grams are extracted for all paraphrase pairs as described in Section 4.3. For computational efficiency, we consider only edges of the confusion network with probabilities higher than 0.1, and only N-grams with probabilities higher than 0.01 in the bag of N-grams. We collect N-grams up to length four. The language model used to judge fluency is trained on the English side of </context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agreement. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
</authors>
<title>ROUGE: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proc. of the ACL-04 Workshop on Text Summarization Branches Out.</booktitle>
<contexts>
<context position="6149" citStr="Lin (2004)" startWordPosition="990" endWordPosition="991">tailment features, and more. Many of these metrics have been shown to correlate better with human judges than BLEU (Chan and Ng, 2008; Liu et al., 2010). Interestingly, few MT evaluation metrics exploit parallel texts as a source of information, when statistical MT is centered almost entirely around mining parallel texts. Compared to these MT evaluation metrics, our method focuses on addressing the unique requirement of paraphrase evaluation: that lexical closeness does not necessarily entail goodness, contrary to the basis of MT evaluation. Inspired by the success of automatic MT evaluation, Lin (2004) and Hovy et al. (2006) propose automatic metrics for summary evaluation. The former is entirely lexical based, whereas the latter also exploits constituent and dependency parses, and semantic features derived from WordNet. The only prior attempt to devise an automatic evaluation metric for paraphrases that we are aware of is ParaMetric (Callison-Burch et al., 2008), which compares the collection of paraphrases discovered by automatic paraphrasing algorithms against a manual gold standard collected over the same sentences. The recall and precision of several current paraphrase generation syste</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C.Y. Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Proc. of the ACL-04 Workshop on Text Summarization Branches Out.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Liu</author>
<author>D Dahlmeier</author>
<author>H T Ng</author>
</authors>
<title>TESLA: translation evaluation of sentences with linearprogramming-based analysis.</title>
<date>2010</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="5691" citStr="Liu et al., 2010" startWordPosition="919" endWordPosition="922">ues that directly optimize the evaluation metric. The weakness of BLEU is that it operates purely at the lexical surface level. Later works attempt to take more syntactic and semantic features into consideration (see (Callison-Burch et al., 2009) for an overview). The whole spectrum of NLP resources has found application in machine translation evaluation, including POS tags, constituent and dependency parses, WordNet (Fellbaum, 1998), semantic roles, textual entailment features, and more. Many of these metrics have been shown to correlate better with human judges than BLEU (Chan and Ng, 2008; Liu et al., 2010). Interestingly, few MT evaluation metrics exploit parallel texts as a source of information, when statistical MT is centered almost entirely around mining parallel texts. Compared to these MT evaluation metrics, our method focuses on addressing the unique requirement of paraphrase evaluation: that lexical closeness does not necessarily entail goodness, contrary to the basis of MT evaluation. Inspired by the success of automatic MT evaluation, Lin (2004) and Hovy et al. (2006) propose automatic metrics for summary evaluation. The former is entirely lexical based, whereas the latter also exploi</context>
</contexts>
<marker>Liu, Dahlmeier, Ng, 2010</marker>
<rawString>C. Liu, D. Dahlmeier, and H.T. Ng. 2010. TESLA: translation evaluation of sentences with linearprogramming-based analysis. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Low</author>
<author>H T Ng</author>
<author>W Guo</author>
</authors>
<title>A maximum entropy approach to Chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proc. of the 4th SIGHAN Workshop.</booktitle>
<contexts>
<context position="25335" citStr="Low et al., 2005" startWordPosition="4118" endWordPosition="4121">. We describe the experimental setup in Section 6.1, before we show the results in Section 6.2. 6.1 Experimental setup We build the phrase table used to evaluate the pivot language F1 from the FBIS Chinese-English corpus, consisting of about 250,000 Chinese sentences, each with a single English translation. The paraphrases are taken from the MTC corpus in the same way as the human experiment described in Section 5.1. Both FBIS and MTC are in the Chinese newswire domain. We stem all English words in both data sets with the Porter stemmer (Porter, 1980). We use the maximum entropy segmenter of (Low et al., 2005) to segment the Chinese part of the FBIS corpus. Subsequently, word level Chinese-English alignments are generated using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009) with five iterations of training. Phrases are then extracted with the widelyused heuristic in Koehn et al. (2003). We extract phrases of up to four words in length. Bags of Chinese pivot language N-grams are extracted for all paraphrase pairs as described in Section 4.3. For computational efficiency, we consider only edges of the confusion network with probabilities higher than 0.1, and only N-grams with probab</context>
</contexts>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>J.K. Low, H.T. Ng, and W. Guo. 2005. A maximum entropy approach to Chinese word segmentation. In Proc. of the 4th SIGHAN Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Madnani</author>
<author>N F Ayan</author>
<author>P Resnik</author>
<author>B J Dorr</author>
</authors>
<title>Using paraphrases for parameter tuning in statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="1204" citStr="Madnani et al., 2007" startWordPosition="182" endWordPosition="185">ntic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the application of paraphrases tend to build their own PG system in view of the immediate needs instead of using an existing system. Second, and as a consequence, no automatic evaluation metric exists for p</context>
</contexts>
<marker>Madnani, Ayan, Resnik, Dorr, 2007</marker>
<rawString>N. Madnani, N.F. Ayan, P. Resnik, and B.J. Dorr. 2007. Using paraphrases for parameter tuning in statistical machine translation. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Madnani</author>
<author>P Resnik</author>
<author>B J Dorr</author>
<author>R Schwartz</author>
</authors>
<title>Are multiple reference translations necessary? Investigating the value of paraphrased reference translations in parameter optimization.</title>
<date>2008</date>
<booktitle>In Proc. of AMTA.</booktitle>
<contexts>
<context position="1227" citStr="Madnani et al., 2008" startWordPosition="186" endWordPosition="189">e based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the application of paraphrases tend to build their own PG system in view of the immediate needs instead of using an existing system. Second, and as a consequence, no automatic evaluation metric exists for paraphrases. Most works </context>
</contexts>
<marker>Madnani, Resnik, Dorr, Schwartz, 2008</marker>
<rawString>N. Madnani, P. Resnik, B.J. Dorr, and R. Schwartz. 2008. Are multiple reference translations necessary? Investigating the value of paraphrased reference translations in parameter optimization. In Proc. of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="9698" citStr="Och and Ney, 2003" startWordPosition="1556" endWordPosition="1559">o measure the adequacy, or semantic similarity, completely independent of any lexical similarity. We address this problem in Sections 4.1 to 4.3. The remaining two criteria are addressed in Section 4.4, and we describe the final combined metric PEM in Section 4.5. 4.1 Phrase-level semantic representation Without loss of generality, suppose we are to evaluate English paraphrases, and have been supplied many sentence-aligned parallel texts of French and English as an additional resource. We can then align the parallel texts at word level automatically using well-known algorithms such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text. If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning. Similar observations have been made by previous researchers (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We can treat the distribution of aligned French phrases as a semanti</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F.J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Owczarzak</author>
<author>D Groves</author>
<author>J Van Genabith</author>
<author>A Way</author>
</authors>
<title>Contextual bitext-derived paraphrases in automatic MT evaluation.</title>
<date>2006</date>
<booktitle>In Proc. of WMT.</booktitle>
<marker>Owczarzak, Groves, Van Genabith, Way, 2006</marker>
<rawString>K. Owczarzak, D. Groves, J. Van Genabith, and A. Way. 2006. Contextual bitext-derived paraphrases in automatic MT evaluation. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="950" citStr="Pang et al., 2003" startWordPosition="138" endWordPosition="141">c metric to evaluate the quality of paraphrases, and consequently, that of paraphrase generation systems. Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes charac</context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>B. Pang, K. Knight, and D. Marcu. 2003. Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4981" citStr="Papineni et al., 2002" startWordPosition="805" endWordPosition="808">entially is for the applications of PG described earlier. The rest of the paper is organized as follows. We survey other automatic evaluation metrics in natural language processing (NLP) in Section 2. We define the task of paraphrase evaluation in Section 3 and develop our metric in Section 4. We conduct a human evaluation and analyze the results in Section 5. The correlation of PEM with human judgments is studied in Section 6. Finally, we discuss our findings and future work in Section 7 and conclude in Section 8. 2 Related work The most well-known automatic evaluation metric in NLP is BLEU (Papineni et al., 2002) for MT, based on N-gram matching precisions. The simplicity of BLEU lends well to MT techniques that directly optimize the evaluation metric. The weakness of BLEU is that it operates purely at the lexical surface level. Later works attempt to take more syntactic and semantic features into consideration (see (Callison-Burch et al., 2009) for an overview). The whole spectrum of NLP resources has found application in machine translation evaluation, including POS tags, constituent and dependency parses, WordNet (Fellbaum, 1998), semantic roles, textual entailment features, and more. Many of these</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>40</volume>
<issue>3</issue>
<contexts>
<context position="25275" citStr="Porter, 1980" startWordPosition="4108" endWordPosition="4109">age F1), designed to match the three evaluation criteria. We describe the experimental setup in Section 6.1, before we show the results in Section 6.2. 6.1 Experimental setup We build the phrase table used to evaluate the pivot language F1 from the FBIS Chinese-English corpus, consisting of about 250,000 Chinese sentences, each with a single English translation. The paraphrases are taken from the MTC corpus in the same way as the human experiment described in Section 5.1. Both FBIS and MTC are in the Chinese newswire domain. We stem all English words in both data sets with the Porter stemmer (Porter, 1980). We use the maximum entropy segmenter of (Low et al., 2005) to segment the Chinese part of the FBIS corpus. Subsequently, word level Chinese-English alignments are generated using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009) with five iterations of training. Phrases are then extracted with the widelyused heuristic in Koehn et al. (2003). We extract phrases of up to four words in length. Bags of Chinese pivot language N-grams are extracted for all paraphrase pairs as described in Section 4.3. For computational efficiency, we consider only edges of the confusion network with</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. Porter. 1980. An algorithm for suffix stripping. Program, 40(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Przybocki</author>
<author>K Peterson</author>
<author>S Bronsart</author>
<author>G Sanders</author>
</authors>
<title>Evaluating machine translation with LFG dependencies.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="27783" citStr="Przybocki et al., 2009" startWordPosition="4542" endWordPosition="4545">re each human overall score is the arithmetic mean of the scores given by the three judges. The Pearson correlation between the automatic PEM scores and the human judgments is 0.8073. This is substantially higher than the sentence level correlation of MT metrics Sentence Level System Level PEM vs. Human Avg. 0.8073 0.9867 PEM vs. Judge A 0.5777 0.9757 PEM vs. Judge B 0.5281 0.9892 PEM vs. Judge C 0.5231 0.9718 Table 3: Correlation of PEM with human judgment (overall score) like BLEU. For example, the highest sentence level Pearson correlation by any metric in the MetricsMATR 2008 competition (Przybocki et al., 2009) was 0.6855 by METEOR-v0.6; BLEU achieved a correlation of 0.4513. Figure 5 shows the system level PEM scores plotted against the human scores. The Pearson correlation between PEM scores and the human scores at the system level is 0.9867. We also calculate the Pearson correlation between PEM and each individual human judge. Here, we exclude the three artificial control paraphrase systems from the data, to make the results comparable to the inter-judge correlation presented in Section 5.2. The correlation is between 0.52 and 0.57 at the sentence level and between 0.97 and 0.98 at the system lev</context>
</contexts>
<marker>Przybocki, Peterson, Bronsart, Sanders, 2009</marker>
<rawString>M. Przybocki, K. Peterson, S. Bronsart, and G Sanders. 2009. Evaluating machine translation with LFG dependencies. Machine Translation, 23(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Qiu</author>
<author>M Y Kan</author>
<author>T S Chua</author>
</authors>
<title>Paraphrase recognition via dissimilarity significance classification.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="4010" citStr="Qiu et al., 2006" startWordPosition="636" endWordPosition="639">xical or syntactic similarity, allowing us to address the conflicting requirements of paraphrase evaluation. The only linguistic re923 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 923–932, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics source required to evaluate BPNG is a parallel text of the target language and an arbitrary other language, known as the pivot language. We highlight that paraphrase evaluation and paraphrase recognition (Heilman and Smith, 2010; Das and Smith, 2009; Wan et al., 2006; Qiu et al., 2006) are related yet distinct tasks. Consider two sentences 51 and 52 that are the same except for the substitution of a single synonym. A paraphrase recognition system should assign them a very high score, but a paraphrase evaluation system would assign a relatively low one. Indeed, the latter is often a better indicator of how useful a PG system potentially is for the applications of PG described earlier. The rest of the paper is organized as follows. We survey other automatic evaluation metrics in natural language processing (NLP) in Section 2. We define the task of paraphrase evaluation in Sec</context>
</contexts>
<marker>Qiu, Kan, Chua, 2006</marker>
<rawString>L. Qiu, M.Y. Kan, and T.S. Chua. 2006. Paraphrase recognition via dissimilarity significance classification. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Quirk</author>
<author>C Brockett</author>
<author>W Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="970" citStr="Quirk et al., 2004" startWordPosition="142" endWordPosition="145">e the quality of paraphrases, and consequently, that of paraphrase generation systems. Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraph</context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>C. Quirk, C. Brockett, and W. Dolan. 2004. Monolingual machine translation for paraphrase generation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>N Madnani</author>
<author>B Dorr</author>
<author>R Schwartz</author>
</authors>
<title>Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric.</title>
<date>2009</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="10228" citStr="Snover et al., 2009" startWordPosition="1648" endWordPosition="1651">at word level automatically using well-known algorithms such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text. If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning. Similar observations have been made by previous researchers (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We can treat the distribution of aligned French phrases as a semantic representation of the English phrase. The semantic distance between two English phrases can then be measured by their degree of overlap in this representation. In this work, we use the widely-used phrase extraction heuristic in (Koehn et al., 2003) to extract phrase pairs from parallel texts into a phrase table1. The phrases extracted do not necessarily correspond to the speakers’ intuition. Rather, they are units whose boundaries are preserved during translation. However, the distinction does not affect our work. 4.2 Segm</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009. Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="26148" citStr="Stolcke, 2002" startWordPosition="4259" endWordPosition="4260">rations of training. Phrases are then extracted with the widelyused heuristic in Koehn et al. (2003). We extract phrases of up to four words in length. Bags of Chinese pivot language N-grams are extracted for all paraphrase pairs as described in Section 4.3. For computational efficiency, we consider only edges of the confusion network with probabilities higher than 0.1, and only N-grams with probabilities higher than 0.01 in the bag of N-grams. We collect N-grams up to length four. The language model used to judge fluency is trained on the English side of the FBIS parallel text. We use SRILM (Stolcke, 2002) to build a 4-gram model with the default parameters. The PEM SVM regression is trained on the paraphrase pairs for the first 200 original English sentences and tested on the paraphrase pairs of the remaining 100 original English sentences. Thus, there are 1,400 instances for training and 700 instances for testing. For each instance, we calculate the values 929 Figure 4: Scatter plot of PEM vs. human judgment (overall score) at the sentence level Figure 5: Scatter plot of PEM vs. human judgment (overall score) at the system level of pivot language F1, normalized language model score, and targe</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wan</author>
<author>M Dras</author>
<author>R Dale</author>
<author>C Paris</author>
</authors>
<title>Using dependency-based features to take the ’para-farce’ out of paraphrase.</title>
<date>2006</date>
<booktitle>In Proc. ofALTW</booktitle>
<contexts>
<context position="3991" citStr="Wan et al., 2006" startWordPosition="632" endWordPosition="635">s not depend on lexical or syntactic similarity, allowing us to address the conflicting requirements of paraphrase evaluation. The only linguistic re923 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 923–932, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics source required to evaluate BPNG is a parallel text of the target language and an arbitrary other language, known as the pivot language. We highlight that paraphrase evaluation and paraphrase recognition (Heilman and Smith, 2010; Das and Smith, 2009; Wan et al., 2006; Qiu et al., 2006) are related yet distinct tasks. Consider two sentences 51 and 52 that are the same except for the substitution of a single synonym. A paraphrase recognition system should assign them a very high score, but a paraphrase evaluation system would assign a relatively low one. Indeed, the latter is often a better indicator of how useful a PG system potentially is for the applications of PG described earlier. The rest of the paper is organized as follows. We survey other automatic evaluation metrics in natural language processing (NLP) in Section 2. We define the task of paraphras</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>S. Wan, M. Dras, R. Dale, and C Paris. 2006. Using dependency-based features to take the ’para-farce’ out of paraphrase. In Proc. ofALTW 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wu</author>
<author>M Zhou</author>
</authors>
<title>Synonymous collocation extraction using translation information.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="10143" citStr="Wu and Zhou, 2003" startWordPosition="1636" endWordPosition="1639">rench and English as an additional resource. We can then align the parallel texts at word level automatically using well-known algorithms such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text. If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning. Similar observations have been made by previous researchers (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We can treat the distribution of aligned French phrases as a semantic representation of the English phrase. The semantic distance between two English phrases can then be measured by their degree of overlap in this representation. In this work, we use the widely-used phrase extraction heuristic in (Koehn et al., 2003) to extract phrase pairs from parallel texts into a phrase table1. The phrases extracted do not necessarily correspond to the speakers’ intuition. Rather, they are units whose boundaries are pres</context>
</contexts>
<marker>Wu, Zhou, 2003</marker>
<rawString>H. Wu and M. Zhou. 2003. Synonymous collocation extraction using translation information. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Q Zhao</author>
<author>C Niu</author>
<author>M Zhou</author>
<author>T Liu</author>
<author>S Li</author>
</authors>
<title>Combining multiple resources to improve SMT-based paraphrasing model.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08:</booktitle>
<publisher>HLT.</publisher>
<contexts>
<context position="1051" citStr="Zhao et al., 2008" startWordPosition="154" endWordPosition="157">ems. Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the application of paraphrases tend to build their ow</context>
</contexts>
<marker>Zhao, Niu, Zhou, Liu, Li, 2008</marker>
<rawString>S.Q. Zhao, C. Niu, M. Zhou, T. Liu, and S. Li. 2008. Combining multiple resources to improve SMT-based paraphrasing model. In Proc. of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Q Zhao</author>
<author>X Lan</author>
<author>T Liu</author>
<author>S Li</author>
</authors>
<title>Applicationdriven statistical paraphrase generation.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP.</booktitle>
<contexts>
<context position="1071" citStr="Zhao et al., 2009" startWordPosition="158" endWordPosition="161">based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the application of paraphrases tend to build their own PG system in view </context>
</contexts>
<marker>Zhao, Lan, Liu, Li, 2009</marker>
<rawString>S.Q. Zhao, X. Lan, T. Liu, and S. Li. 2009. Applicationdriven statistical paraphrase generation. In Proc. of ACL-IJCNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>