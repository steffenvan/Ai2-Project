<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000043">
<title confidence="0.977068">
Semantic Discourse Segmentation and Labeling for Route Instructions
</title>
<author confidence="0.994362">
Nobuyuki Shimizu
</author>
<affiliation confidence="0.999105">
Department of Computer Science
State University of New York at Albany
</affiliation>
<address confidence="0.773251">
Albany, NY 12222, USA
</address>
<email confidence="0.997256">
nobuyuki@shimizu.name
</email>
<sectionHeader confidence="0.993851" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999769933333333">
In order to build a simulated robot that
accepts instructions in unconstrained nat-
ural language, a corpus of 427 route in-
structions was collected from human sub-
jects in the office navigation domain. The
instructions were segmented by the steps
in the actual route and labeled with the
action taken in each step. This flat
formulation reduced the problem to an
IE/Segmentation task, to which we applied
Conditional Random Fields. We com-
pared the performance of CRFs with a set
of hand-written rules. The result showed
that CRFs perform better with a 73.7%
success rate.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947375">
To have seamless interactions with computers, ad-
vances in task-oriented deep semantic understand-
ing are of utmost importance. The examples in-
clude tutoring, dialogue systems and the one de-
scribed in this paper, a natural language interface
to mobile robots. Compared to more typical text
processing tasks on newspapers for which we at-
tempt shallow understandings and broad coverage,
for these domains vocabulary is limited and very
strong domain knowledge is available. Despite
this, deeper understanding of unrestricted natural
language instructions poses a real challenge, due
to the incredibly rich structures and creative ex-
pressions that people use. For example,
”Just head straight through the hallway
ignoring the rooms to the left and right
of you, but while going straight your go-
ing to eventually see a room facing you,
which is north, enter it.”
”Head straight. continue straight past
the first three doors until you hit a cor-
ner. On that corner there are two doors,
one straight ahead of you and one on the
right. Turn right and enter the room to
the right and stop within.”
These utterances are taken from an office navi-
gation corpus collected from undergrad volunteers
at SUNY/Albany. There is a good deal of variety.
Previous efforts in this domain include the clas-
sic SHRDLU program by Winograd (1972), us-
ing a simulated robot, and the more ambitious IBL
(Instruction-based Learning for Mobile Robots)
project (Lauria et al, 2001) which tried to inte-
grate vision, voice recognition, natural language
understanding and robotics. This group has yet to
publish performance statistics. In this paper we
will focus on the application of machine learning
to the understanding of written route instructions,
and on testing by following the instructions in a
simulated office environment.
</bodyText>
<sectionHeader confidence="0.997905" genericHeader="introduction">
2 Task
</sectionHeader>
<subsectionHeader confidence="0.967775">
2.1 Input and Output
</subsectionHeader>
<bodyText confidence="0.991838">
Three inputs are required for the task:
</bodyText>
<listItem confidence="0.9995338">
• Directions for reaching an office, written in
unrestricted English.
• A description of the building we are traveling
through.
• The agent’s initial position and orientation.
</listItem>
<bodyText confidence="0.954744">
The output is the location of the office the direc-
tions aim to reach.
</bodyText>
<page confidence="0.998247">
31
</page>
<bodyText confidence="0.63188">
Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 31–36,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.998826">
2.2 Corpus Collection
</subsectionHeader>
<bodyText confidence="0.999957705882353">
In an experiment to collect the corpus, (Haas,
1995) created a simulated office building modeled
after the actual computer science department at
SUNY/Albany. This environment was set up like
a popular first person shooter game such as Doom,
and the subject saw a demonstration of the route
he/she was asked to describe. The subject wrote
directions and sent them to the experimenter, who
sat at another computer in the next room. The
experimenter tried to follow the directions; if he
reaches the right destination, the subject got $1.
This process took place 10 times for each subject;
instructions that the experimenter could not fol-
low correctly were not added to the corpus. In this
manner, they were able to elicit 427 route instruc-
tions from the subject pool of 44 undergraduate
students.
</bodyText>
<subsectionHeader confidence="0.998543">
2.3 Abstract Map
</subsectionHeader>
<bodyText confidence="0.999425818181818">
To simplify the learning task, the map of our
computer science department was abstracted to a
graph. Imagine a track running down the halls of
the virtual building, with branches into the office
doors. The nodes of the graph are the intersec-
tions, the edges are the pieces of track between
them. We assume this map can either be prepared
ahead of time, or dynamically created as a result of
solving Simultaneous Localization and Mapping
(SLAM) problem in robotics (Montemerlo et al,
2003).
</bodyText>
<subsectionHeader confidence="0.996844">
2.4 System Components
</subsectionHeader>
<bodyText confidence="0.9953585">
Since it is difficult to jump ahead and learn the
whole input-output association as described in the
task section, we will break down the system into
two components.
</bodyText>
<figure confidence="0.26449025">
Front End:
Routelnstruction → ActionList
Back End:
ActionList x Map x Start → Goal
</figure>
<bodyText confidence="0.999944173913044">
The front-end is an information extraction sys-
tem, where the system extracts how one should
move from a route instruction. The back-end is a
reasoning system which takes a sequence of moves
and finds the destination in the map. We will first
describe the front-end, and then show how to inte-
grate the back-end to it.
One possibility is to keep the semantic repre-
sentation close to the surface structure, including
under-specification and ambiguity, and leaving the
back-end to resolve the ambiguity. We will pursue
a different route. The disambiguation will be done
in the front-end; the representation that it passes
to the back-end will be unambiguous, describing
at most one path through the building. The task
of the back-end is simply to check the sequence
of moves the front-end produced against the map
and see if there is a path leading to a point in the
map or not. The reason for this is two fold. One is
to have a minimal annotation scheme for the cor-
pus, and the other is to enable the learning of the
whole task including the disambiguation as an IE
problem.
</bodyText>
<sectionHeader confidence="0.982616" genericHeader="method">
3 Semantic Analysis
</sectionHeader>
<bodyText confidence="0.961784633333334">
Note that in this paper, given an instruction, one
step in the instruction corresponds to one action
shown to the subject, one episode of action detec-
tion and tracking, and one segment of the text.
In order to annotate unambiguously, we need to
detect and track both landmarks and actions. A
landmark is a hallway or a door, and an action
is a sequence of a few moves one will make with
respect to a specific landmark.
The moves one can make in this map are:
(M1). Advancing to x,
(M2). Turning left/right to face x, and
(M3). Entering x.
Here, x is a landmark. Note that all three moves
have to do with the same landmark, and two or
three moves on the same landmark constitute one
action. An action is ambiguous until x is filled
with an unambiguous landmark. The following is
a made-up example in which each move in an ac-
tion is mentioned explicitly.
a. ”Go down the hallway to the second
door on the right. Turn right. Enter the
door.”
But you could break it down even further.
b. ”Go down the hallway. You will see
two doors on the right. Turn right and
enter the second.”
One can add any amount of extra information to an
instruction and make it longer, which people seem
to do. However, we see the following as well.
</bodyText>
<listItem confidence="0.603837">
c. ”Enter the second door on the right.”
</listItem>
<bodyText confidence="0.9886135">
In one sentence, this sample contains the advance,
the turn and the entering. In the corpus, the norm
</bodyText>
<page confidence="0.996049">
32
</page>
<bodyText confidence="0.97620796">
is to assume the move (M1) when an expression
indicating the move (M2) is present. Similarly, an
expression of move (M3) often implicitly assumes
the move (M1) and (M2). However, in some cases
they are explicitly stated, and when this happens,
the action that involves the same landmark must
be tracked across the sentences.
Since all three samples result in the same action,
for the back-end it is best not to differentiate the
three. In order to do this, actions must be tracked
just like landmarks in the corpus.
The following two samples illustrate the need to
track actions.
d. ”Go down the hallway until you see
two doors. Turn right and enter the sec-
ond door on the right.”
In this case, there is only one action in the instruc-
tion, and ”turn right” belongs to the action ”ad-
vance to the second door on the right, and then
turn right to face it, and then enter it.”
e. ”Proceed to the first hallway on the
right. Turn right and enter the second
door on the right.”
There are two actions in this instruction. The first
is ”advance to the first hallway on the right, and
then turn right to face the hallway.” The phrase
”turn right” belongs to this first action. The second
action is the same as the one in the example (d).
Unless we can differentiate between the two, the
execution of the unnecessary turn results in failure
when following the instructions in the case (d).
This illustrates the need to track actions across
a few sentences. In the last example, it is impor-
tant to realize that ”turn right” has something to do
with a door, so that it means ”turn right to face a
door”. Furthermore, since ”enter the second door
on the right” contains ”turning right to face a door”
in its semantics as well, they can be thought of as
the same action. Thus, the critical feature required
in the annotation scheme is to track actions and
landmarks.
The simplest annotation scheme that can show
how actions are tracked across the sentences is
to segment the instruction into different episodes
of action detection and tracking. Note that each
episode corresponds to exactly one action shown
to the subject during the experiment. The annota-
tion is based on the semantics, not on the the men-
tions of moves or landmarks. Since each segment
Token Node Part Transition Part
</bodyText>
<equation confidence="0.99097975">
make (B-GHL1, 0) (B-GHL1,I-GHL1, 0, 1)
left (I-GHL1, 1) (I-GHL1, I-GHL1, 1, 2)
, (I-GHL1, 2) (I-GHL1, B-EDR1, 2, 3)
first (B-EDR1, 3) (B-EDR1, I-EDR1, 3, 4)
door (I-EDR1, 4) (I-EDR1,I-EDR1, 4, 5)
on (I-EDR1, 5) (I-EDR1, I-EDR1, 5, 6)
the (I-EDR1, 6) (I-EDR1,I-EDR1, 6, 7)
right (I-EDR1, 7)
</equation>
<tableCaption confidence="0.993131">
Table 1: Example Parts: linear-chain CRFs
</tableCaption>
<bodyText confidence="0.997847652173913">
involves exactly one landmark, we can label the
segment with an action and a specific landmark.
For example,
GHR1 := ”advance to the first hallway on the
right, then turn right to face it.”
EDR2 := ”advance to the second door on the
right, then turn right to face it, then enter it.”
GHLZ := ”advance to the hallway on the left at
the end of the hallway, then turn left to face it.”
EDSZ := ”advance to the door straight ahead of
you, then enter it.”
Note that GH=go-hall, ED=enter-door,
R1=first-right, LZ=left-at-end, SZ=ahead-of-you.
The total number of possible actions is 15.
This way, we can reduce the front-end task into
a sequence of tagging tasks, much like the noun
phrase chunking in the CoNLL-2000 shared task
(Tjong Kim Sang and Buchholz, 2000). Given
a sequence of input tokens that forms a route in-
struction, a sequence of output labels, with each
label matching an input token was prepared. We
annotated with the BIO tagging scheme used in
syntactic chunkers (Ramshaw and Marcus, 1995).
</bodyText>
<table confidence="0.972116875">
make B-GHL1
left I-GHL1
, I-GHL1
first B-EDR1
door I-EDR1
on I-EDR1
the I-EDR1
right I-EDR1
</table>
<sectionHeader confidence="0.993588" genericHeader="method">
4 Systems
</sectionHeader>
<subsectionHeader confidence="0.988050666666667">
4.1 System 1: CRFs
4.1.1 Model: A Linear-Chain Undirected
Graphical Model
</subsectionHeader>
<bodyText confidence="0.9948514">
From the output labels, we create the parts in a
linear-chain undirected graph (Table 1). Our use
of term part is based on (Bartlett et al, 2004).
For each pair (xi, yi) in the training set, xi is
the token (in the first column, Table 1), and yi
</bodyText>
<page confidence="0.992216">
33
</page>
<figure confidence="0.680330230769231">
Transition Node
(L0, L, j − 1, j) (L, j)
no lexicalization no lexicalization
xj−4
xj−3
xj−2
xj−1
xj
xj+1
xj+2
xj+3
xj−1, xj
xj+0, xj+1
</figure>
<tableCaption confidence="0.929596">
Table 2: Features
</tableCaption>
<bodyText confidence="0.999137352941176">
is the part (in the second and third column, Ta-
ble 1). There are two kinds of parts: node and
transition. A node part tells us the position and
the label, (B-GHL1, 0), (I-GHL1, 1), and so on. A
transition part encodes a transition. For example,
between tokens 0 and 1 there is a transition from
tag B-GHL1 to I-GHL1. The part that describes
this transition is: (B-GHL1,I-GHL1, 0, 1).
We factor the score of this linear node-transition
structure as the sum of the scores of all the parts in
y, where the score of a part is again the sum of the
feature weights for that part.
To score a pair (xi, yi) in the training set, we
take each part in yi and check the features associ-
ated with it via lexicalization. For example, a part
(I-GHL1,1) could give rise to binary features such
as,
</bodyText>
<listItem confidence="0.923530285714286">
• Does (xi, yi) contain a label ”I-GHL1”? (No
Lexicalization)
• Does (xi, yi) contain a token ”left” labeled
with ”I-GHL1”? (Lexicalized by x1)
• Does (xi, yi) contain a token ”left” labeled
with ”I-GHL1” that’s preceded by ”make”?
(Lexicalized by x0, x1)
</listItem>
<bodyText confidence="0.9691731875">
and so on. The features used in this experiment are
listed in Table 2.
If a feature is present, the feature weight is
added. The sum of the weights of all the parts
is the score of the pair (xi, yi). To represent
this summation, we write s(xi, yi) = w⊤f(xi, yi)
where f represents the feature vector and w is the
weight vector. We could also have w⊤f(xi, {p})
where p is a single part, in which case we just write
s(p).
Assuming an appropriate feature representation
as well as a weight vector w, we would like to find
the highest scoring y = argmaxy′(w⊤k f(y′, x))
given an input sequence x. We next present a ver-
sion of this decoding algorithm that returns the
best y consistent with the map.
</bodyText>
<subsectionHeader confidence="0.617662">
4.1.2 Decoding: the Viterbi Algorithm and
Inferring the Path in the Map
</subsectionHeader>
<bodyText confidence="0.9997855">
The action labels are unambiguous; given the
current position, the map, and the action label,
there is only one position one can go to. This back-
end computation can be integrated into the Viterbi
algorithm. The function ’go’ takes a pair of (ac-
tion label, start position) and returns the end posi-
tion or null if the action cannot be executed at the
start position according to the map. The algorithm
chooses the best among the label sequences with a
legal path in the map, as required by the condition
(cost &gt; bestc n end =� null). Once the model
is trained, we can then use the modified version of
the Viterbi algorithm (Algorithm 4.1) to find the
destination in the map.
</bodyText>
<equation confidence="0.986117142857143">
Algorithm 4.1: DECODE PATH(x, n, start, go)
for each label y1
node[0][y1].cost +— s((y1, 0))
node[0][y1].end +— start;
for j +— 1 to n − 1
for each label yj+1
bestc +— −oc;
end +— null;
for each label yj
cost +— node[j][yj].cos1t
+s((yj,yj+1, j,j `))
+
+s((yj+1,j + 1));
end +— node[j][yj].end;
</equation>
<construct confidence="0.7546508">
if (yj =� yj+1)
end +— go(yj+1, end);
if (cost &gt; bestc n end =� null)
bestc +— cost;
if (bestc =� −oc)
</construct>
<equation confidence="0.862370555555556">
node[j + 1][yj+1].cost +— bestc;
node[j + 1][yj+1].end +— end;
bestc +— −oc;
end +— null;
for each label yn
if (node[j][yn].cost &gt; bestc)
bestc +— node[j][yn].cost;
end +— node[j][yn].end;
return (bestc, end)
</equation>
<page confidence="0.995315">
34
</page>
<subsectionHeader confidence="0.571319">
4.1.3 Learning: Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.999964166666667">
Given the above problem formulation, we
trained the linear-chain undirected graphical
model as Conditional Random Fields (Lafferty et
al, 2001; Sha and Pereira, 2003), one of the best
performing chunkers. We assume the probability
of seeing y given x is
</bodyText>
<equation confidence="0.995973">
exp(s(x, y))
P(y|x) =
Ey′ exp(s(x, y′))
</equation>
<bodyText confidence="0.99864825">
where y′ is all possible labeling for x , Now, given
a training set T = {(xiyi)Jmi�j, We can learn
the weights by maximizing the log-likelihood,
Ei logP(yi|xi). A detailed description of CRFs
can be found in (Lafferty et al, 2001; Sha and
Pereira, 2003; Malouf, 2002; Peng and McCallum,
2004). We used an implementation called CRF++
which can be found in (Kudo, 2005)
</bodyText>
<subsectionHeader confidence="0.99751">
4.2 System 2: Baseline
</subsectionHeader>
<bodyText confidence="0.999854181818182">
Suppose we have clean data and there is no need to
track an action across sentences or phrases. Then,
the properties of an action are mentioned exactly
once for each episode.
For example, in ”go straight and make the first
left you can, then go into the first door on the right
side and stop” , LEFT and FIRST occur exactly
once for the first action, and FIRST, DOOR and
RIGHT are found exactly once in the next action.
In a case like that, the following baseline algo-
rithm should work well.
</bodyText>
<listItem confidence="0.9908118">
• Find all the mentions of LEFT/RIGHT,
• For each occurrence of LEFT/RIGHT, look
for an ordinal number, LAST, or END (= end
of the hallway) nearby,
• Also, for each LEFT/RIGHT, look for a men-
tion of DOOR. If DOOR is mentioned, the
action is about entering a door.
• If DOOR is not mentioned around
LEFT/RIGHT, then the action is about
going to a hallway by default,
• If DOOR is mentioned at the end of an in-
struction without LEFT/RIGHT, then the ac-
tion is to go straight into the room.
• Put the sequence of action labels together ac-
cording to the mentions collected.
</listItem>
<table confidence="0.9997690625">
count average length
GHL1 128 8.5
GHL2 4 7.7
GHLZ 36 14.4
GHR1 175 10.8
GHR2 5 15.8
GHRZ 42 13.6
EDL1 98 10.5
EDL2 81 12.3
EDL3 24 13.9
EDLZ 28 13.7
EDR1 69 10.4
EDR2 55 12.9
EDR3 6 13.0
EDRZ 11 16.4
EDSZ 55 16.2
</table>
<tableCaption confidence="0.999922">
Table 3: Steps found in the dataset
</tableCaption>
<bodyText confidence="0.999910333333333">
In this case, all that’s required is a dictionary of
how a word maps to a concept such as DOOR. In
this corpus, ”door”, ”office”, ”room”, ”doorway”
and their plural forms map to DOOR, and the or-
dinal number 1 will be represented by ”first” and
”1st”, and so on.
</bodyText>
<sectionHeader confidence="0.999288" genericHeader="method">
5 Dataset
</sectionHeader>
<bodyText confidence="0.999991933333333">
As noted, we have 427 route instructions, and the
average number of steps was 1.86 steps per in-
struction. We had 189 cases in which a sentence
boundary was found in the middle of a step. Ta-
ble 3 shows how often action steps occurred in the
corpus and average length of the segments.
One thing we noticed is that somehow people do
not use a short phrase to say the equivalent of ”en-
ter the door straight ahead of you”, as seen by the
average length of EDSZ. Also, it is more common
to say the equivalent of ”take a right at the end of
the hallway” than that of ”go to the second hallway
on the right”, as seen by the count of GHR2 and
GHRZ. The distribution is highly skewed; there
are a lot more GHL1 than GHL2.
</bodyText>
<sectionHeader confidence="0.999981" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999707428571429">
We evaluated the performance of the systems us-
ing three measures: overlap match, exact match,
and instruction follow through, using 6-fold cross-
valiadation on 427 samples. Only the action
chunks were considered for exact match and over-
lap match. Overlap match is a lenient measure
that considers a segmentation or labeling to be cor-
</bodyText>
<page confidence="0.996583">
35
</page>
<table confidence="0.93171825">
Exact Match Recall Precision F-1
CRFs 66.0% 67.0% 66.5%
Overlap Match Recall Precision F-1
Baseline 62.8% 49.9% 55.6%
CRFs 85.7% 87.0% 86.3%
Instruction Follow Through success rate
Baseline 39.5%
CRFs 73.7%
</table>
<tableCaption confidence="0.999369">
Table 4: Recall, Precision, F-1 and Success Rate
</tableCaption>
<bodyText confidence="0.99944475">
rect if it overlaps with any of the annotated labels.
Instruction follow through is the success rate for
reaching the destination, and the most important
measure of the performance in this domain. Since
the baseline algorithm does not identify the token
labeled with B-prefix, no exact match comparison
is made. The result (Table 4) shows that CRFs per-
form better with a 73.7% success rate.
</bodyText>
<sectionHeader confidence="0.999683" genericHeader="discussions">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999969333333333">
More complex models capable of representing
landmarks and actions separately may be applica-
ble to this domain, and it remains to be seen if such
models will perform better. Also, some form of
co-reference resolution or more sophisticated ac-
tion tracking should also be considered.
</bodyText>
<sectionHeader confidence="0.979916" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999924333333333">
We thank Dr. Andrew Haas for introducing us to
the problem, collecting the corpus and being very
supportive in general.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99976052173913">
P. Bartlett, M. Collins, B. Taskar and D. McAllester.
2004. Exponentiated gradient algorithms for large-
margin structured classification. In Advances in
Neural Information Processing Systems (NIPS)
A. Haas 1995. Testing a Simulated Robot that Follows
Directions. unpublished
T. Kudo 2005. CRF++: Yet An-
other CRF toolkit. Available at
http://chasen.org/˜taku/software/CRF++/
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings ofInternational Conference on Machine Learn-
ing .
R. Malouf. 2002. A Comparison of Algorithms for
Maximum Entropy Parameter Estimation. In Pro-
ceedings of Conference of Computational Natural
Language Learning
F. Peng and A. McCallum. 2004. Accurate Informa-
tion Extraction from Research Papers using Condi-
tional Random Fields. In Proceedings of Human
Language Technology Conference.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of Human
Language Technology Conference.
S. Lauria, G. Bugmann, T. Kyriacou, J. Bos, and E.
Klein. 2001. Personal Robot Training via Natural-
Language Instructions. IEEE Intelligent Systems,
16:3, pp. 38-45.
C. Manning and H. Schutze. 1999. Foundations ofSta-
tistical Natural Language Processing. MIT Press.
M. Montemerlo, S. Thrun, D. Koller, and B. Wegbreit.
2003. FastSLAM 2.0: An improved particle fil-
tering algorithm for simultaneous localization and
mapping that provably converges. In Proceedings of
the International Joint Conference on Artificial In-
telligence (IJCAI).
L. Ramshaw and M. Marcus. 1995. Text chunking us-
ing transformation-based learning. In Proceedings
of Third Workshop on Very Large Corpora. ACL
E. F. Tjong Kim Sang and S. Buchholz. 2000. In-
troduction to the CoNLL-2000 shared task: Chunk-
ing. In Proceedings of Conference of Computational
Natural Language Learning.
T. Winograd. 1972. Understanding Natural Language.
Academic Press.
</reference>
<page confidence="0.998938">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968075">
<title confidence="0.999889">Semantic Discourse Segmentation and Labeling for Route Instructions</title>
<author confidence="0.999236">Nobuyuki Shimizu</author>
<affiliation confidence="0.999726">Department of Computer Science State University of New York at Albany</affiliation>
<address confidence="0.999864">Albany, NY 12222, USA</address>
<email confidence="0.982171">nobuyuki@shimizu.name</email>
<abstract confidence="0.9991888125">In order to build a simulated robot that accepts instructions in unconstrained natural language, a corpus of 427 route instructions was collected from human subjects in the office navigation domain. The instructions were segmented by the steps in the actual route and labeled with the action taken in each step. This flat formulation reduced the problem to an IE/Segmentation task, to which we applied Conditional Random Fields. We compared the performance of CRFs with a set of hand-written rules. The result showed CRFs perform better with a success rate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Bartlett</author>
<author>M Collins</author>
<author>B Taskar</author>
<author>D McAllester</author>
</authors>
<title>Exponentiated gradient algorithms for largemargin structured classification.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS)</booktitle>
<contexts>
<context position="11034" citStr="Bartlett et al, 2004" startWordPosition="1884" endWordPosition="1887">L-2000 shared task (Tjong Kim Sang and Buchholz, 2000). Given a sequence of input tokens that forms a route instruction, a sequence of output labels, with each label matching an input token was prepared. We annotated with the BIO tagging scheme used in syntactic chunkers (Ramshaw and Marcus, 1995). make B-GHL1 left I-GHL1 , I-GHL1 first B-EDR1 door I-EDR1 on I-EDR1 the I-EDR1 right I-EDR1 4 Systems 4.1 System 1: CRFs 4.1.1 Model: A Linear-Chain Undirected Graphical Model From the output labels, we create the parts in a linear-chain undirected graph (Table 1). Our use of term part is based on (Bartlett et al, 2004). For each pair (xi, yi) in the training set, xi is the token (in the first column, Table 1), and yi 33 Transition Node (L0, L, j − 1, j) (L, j) no lexicalization no lexicalization xj−4 xj−3 xj−2 xj−1 xj xj+1 xj+2 xj+3 xj−1, xj xj+0, xj+1 Table 2: Features is the part (in the second and third column, Table 1). There are two kinds of parts: node and transition. A node part tells us the position and the label, (B-GHL1, 0), (I-GHL1, 1), and so on. A transition part encodes a transition. For example, between tokens 0 and 1 there is a transition from tag B-GHL1 to I-GHL1. The part that describes th</context>
</contexts>
<marker>Bartlett, Collins, Taskar, McAllester, 2004</marker>
<rawString>P. Bartlett, M. Collins, B. Taskar and D. McAllester. 2004. Exponentiated gradient algorithms for largemargin structured classification. In Advances in Neural Information Processing Systems (NIPS)</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haas</author>
</authors>
<title>Testing a Simulated Robot that Follows Directions. unpublished T. Kudo</title>
<date>1995</date>
<contexts>
<context position="3123" citStr="Haas, 1995" startWordPosition="495" endWordPosition="496"> on testing by following the instructions in a simulated office environment. 2 Task 2.1 Input and Output Three inputs are required for the task: • Directions for reaching an office, written in unrestricted English. • A description of the building we are traveling through. • The agent’s initial position and orientation. The output is the location of the office the directions aim to reach. 31 Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 31–36, Sydney, July 2006. c�2006 Association for Computational Linguistics 2.2 Corpus Collection In an experiment to collect the corpus, (Haas, 1995) created a simulated office building modeled after the actual computer science department at SUNY/Albany. This environment was set up like a popular first person shooter game such as Doom, and the subject saw a demonstration of the route he/she was asked to describe. The subject wrote directions and sent them to the experimenter, who sat at another computer in the next room. The experimenter tried to follow the directions; if he reaches the right destination, the subject got $1. This process took place 10 times for each subject; instructions that the experimenter could not follow correctly wer</context>
</contexts>
<marker>Haas, 1995</marker>
<rawString>A. Haas 1995. Testing a Simulated Robot that Follows Directions. unpublished T. Kudo 2005. CRF++: Yet Another CRF toolkit. Available at http://chasen.org/˜taku/software/CRF++/</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings ofInternational Conference on Machine Learning .</booktitle>
<contexts>
<context position="14562" citStr="Lafferty et al, 2001" startWordPosition="2526" endWordPosition="2529">nd +— null; for each label yj cost +— node[j][yj].cos1t +s((yj,yj+1, j,j `)) + +s((yj+1,j + 1)); end +— node[j][yj].end; if (yj =� yj+1) end +— go(yj+1, end); if (cost &gt; bestc n end =� null) bestc +— cost; if (bestc =� −oc) node[j + 1][yj+1].cost +— bestc; node[j + 1][yj+1].end +— end; bestc +— −oc; end +— null; for each label yn if (node[j][yn].cost &gt; bestc) bestc +— node[j][yn].cost; end +— node[j][yn].end; return (bestc, end) 34 4.1.3 Learning: Conditional Random Fields Given the above problem formulation, we trained the linear-chain undirected graphical model as Conditional Random Fields (Lafferty et al, 2001; Sha and Pereira, 2003), one of the best performing chunkers. We assume the probability of seeing y given x is exp(s(x, y)) P(y|x) = Ey′ exp(s(x, y′)) where y′ is all possible labeling for x , Now, given a training set T = {(xiyi)Jmi�j, We can learn the weights by maximizing the log-likelihood, Ei logP(yi|xi). A detailed description of CRFs can be found in (Lafferty et al, 2001; Sha and Pereira, 2003; Malouf, 2002; Peng and McCallum, 2004). We used an implementation called CRF++ which can be found in (Kudo, 2005) 4.2 System 2: Baseline Suppose we have clean data and there is no need to track </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings ofInternational Conference on Machine Learning .</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
</authors>
<title>A Comparison of Algorithms for Maximum Entropy Parameter Estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of Conference of Computational Natural Language Learning</booktitle>
<contexts>
<context position="14980" citStr="Malouf, 2002" startWordPosition="2602" endWordPosition="2603">, end) 34 4.1.3 Learning: Conditional Random Fields Given the above problem formulation, we trained the linear-chain undirected graphical model as Conditional Random Fields (Lafferty et al, 2001; Sha and Pereira, 2003), one of the best performing chunkers. We assume the probability of seeing y given x is exp(s(x, y)) P(y|x) = Ey′ exp(s(x, y′)) where y′ is all possible labeling for x , Now, given a training set T = {(xiyi)Jmi�j, We can learn the weights by maximizing the log-likelihood, Ei logP(yi|xi). A detailed description of CRFs can be found in (Lafferty et al, 2001; Sha and Pereira, 2003; Malouf, 2002; Peng and McCallum, 2004). We used an implementation called CRF++ which can be found in (Kudo, 2005) 4.2 System 2: Baseline Suppose we have clean data and there is no need to track an action across sentences or phrases. Then, the properties of an action are mentioned exactly once for each episode. For example, in ”go straight and make the first left you can, then go into the first door on the right side and stop” , LEFT and FIRST occur exactly once for the first action, and FIRST, DOOR and RIGHT are found exactly once in the next action. In a case like that, the following baseline algorithm s</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>R. Malouf. 2002. A Comparison of Algorithms for Maximum Entropy Parameter Estimation. In Proceedings of Conference of Computational Natural Language Learning</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>A McCallum</author>
</authors>
<title>Accurate Information Extraction from Research Papers using Conditional Random Fields.</title>
<date>2004</date>
<booktitle>In Proceedings of Human Language Technology Conference.</booktitle>
<contexts>
<context position="15006" citStr="Peng and McCallum, 2004" startWordPosition="2604" endWordPosition="2607">3 Learning: Conditional Random Fields Given the above problem formulation, we trained the linear-chain undirected graphical model as Conditional Random Fields (Lafferty et al, 2001; Sha and Pereira, 2003), one of the best performing chunkers. We assume the probability of seeing y given x is exp(s(x, y)) P(y|x) = Ey′ exp(s(x, y′)) where y′ is all possible labeling for x , Now, given a training set T = {(xiyi)Jmi�j, We can learn the weights by maximizing the log-likelihood, Ei logP(yi|xi). A detailed description of CRFs can be found in (Lafferty et al, 2001; Sha and Pereira, 2003; Malouf, 2002; Peng and McCallum, 2004). We used an implementation called CRF++ which can be found in (Kudo, 2005) 4.2 System 2: Baseline Suppose we have clean data and there is no need to track an action across sentences or phrases. Then, the properties of an action are mentioned exactly once for each episode. For example, in ”go straight and make the first left you can, then go into the first door on the right side and stop” , LEFT and FIRST occur exactly once for the first action, and FIRST, DOOR and RIGHT are found exactly once in the next action. In a case like that, the following baseline algorithm should work well. • Find al</context>
</contexts>
<marker>Peng, McCallum, 2004</marker>
<rawString>F. Peng and A. McCallum. 2004. Accurate Information Extraction from Research Papers using Conditional Random Fields. In Proceedings of Human Language Technology Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology Conference.</booktitle>
<contexts>
<context position="14586" citStr="Sha and Pereira, 2003" startWordPosition="2530" endWordPosition="2533">abel yj cost +— node[j][yj].cos1t +s((yj,yj+1, j,j `)) + +s((yj+1,j + 1)); end +— node[j][yj].end; if (yj =� yj+1) end +— go(yj+1, end); if (cost &gt; bestc n end =� null) bestc +— cost; if (bestc =� −oc) node[j + 1][yj+1].cost +— bestc; node[j + 1][yj+1].end +— end; bestc +— −oc; end +— null; for each label yn if (node[j][yn].cost &gt; bestc) bestc +— node[j][yn].cost; end +— node[j][yn].end; return (bestc, end) 34 4.1.3 Learning: Conditional Random Fields Given the above problem formulation, we trained the linear-chain undirected graphical model as Conditional Random Fields (Lafferty et al, 2001; Sha and Pereira, 2003), one of the best performing chunkers. We assume the probability of seeing y given x is exp(s(x, y)) P(y|x) = Ey′ exp(s(x, y′)) where y′ is all possible labeling for x , Now, given a training set T = {(xiyi)Jmi�j, We can learn the weights by maximizing the log-likelihood, Ei logP(yi|xi). A detailed description of CRFs can be found in (Lafferty et al, 2001; Sha and Pereira, 2003; Malouf, 2002; Peng and McCallum, 2004). We used an implementation called CRF++ which can be found in (Kudo, 2005) 4.2 System 2: Baseline Suppose we have clean data and there is no need to track an action across sentenc</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of Human Language Technology Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lauria</author>
<author>G Bugmann</author>
<author>T Kyriacou</author>
<author>J Bos</author>
<author>E Klein</author>
</authors>
<title>Personal Robot Training via NaturalLanguage Instructions.</title>
<date>2001</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>16</volume>
<pages>38--45</pages>
<contexts>
<context position="2238" citStr="Lauria et al, 2001" startWordPosition="356" endWordPosition="359">u, which is north, enter it.” ”Head straight. continue straight past the first three doors until you hit a corner. On that corner there are two doors, one straight ahead of you and one on the right. Turn right and enter the room to the right and stop within.” These utterances are taken from an office navigation corpus collected from undergrad volunteers at SUNY/Albany. There is a good deal of variety. Previous efforts in this domain include the classic SHRDLU program by Winograd (1972), using a simulated robot, and the more ambitious IBL (Instruction-based Learning for Mobile Robots) project (Lauria et al, 2001) which tried to integrate vision, voice recognition, natural language understanding and robotics. This group has yet to publish performance statistics. In this paper we will focus on the application of machine learning to the understanding of written route instructions, and on testing by following the instructions in a simulated office environment. 2 Task 2.1 Input and Output Three inputs are required for the task: • Directions for reaching an office, written in unrestricted English. • A description of the building we are traveling through. • The agent’s initial position and orientation. The o</context>
</contexts>
<marker>Lauria, Bugmann, Kyriacou, Bos, Klein, 2001</marker>
<rawString>S. Lauria, G. Bugmann, T. Kyriacou, J. Bos, and E. Klein. 2001. Personal Robot Training via NaturalLanguage Instructions. IEEE Intelligent Systems, 16:3, pp. 38-45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Schutze</author>
</authors>
<title>Foundations ofStatistical Natural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<marker>Manning, Schutze, 1999</marker>
<rawString>C. Manning and H. Schutze. 1999. Foundations ofStatistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Montemerlo</author>
<author>S Thrun</author>
<author>D Koller</author>
<author>B Wegbreit</author>
</authors>
<title>FastSLAM 2.0: An improved particle filtering algorithm for simultaneous localization and mapping that provably converges.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI).</booktitle>
<contexts>
<context position="4371" citStr="Montemerlo et al, 2003" startWordPosition="702" endWordPosition="705">. In this manner, they were able to elicit 427 route instructions from the subject pool of 44 undergraduate students. 2.3 Abstract Map To simplify the learning task, the map of our computer science department was abstracted to a graph. Imagine a track running down the halls of the virtual building, with branches into the office doors. The nodes of the graph are the intersections, the edges are the pieces of track between them. We assume this map can either be prepared ahead of time, or dynamically created as a result of solving Simultaneous Localization and Mapping (SLAM) problem in robotics (Montemerlo et al, 2003). 2.4 System Components Since it is difficult to jump ahead and learn the whole input-output association as described in the task section, we will break down the system into two components. Front End: Routelnstruction → ActionList Back End: ActionList x Map x Start → Goal The front-end is an information extraction system, where the system extracts how one should move from a route instruction. The back-end is a reasoning system which takes a sequence of moves and finds the destination in the map. We will first describe the front-end, and then show how to integrate the back-end to it. One possib</context>
</contexts>
<marker>Montemerlo, Thrun, Koller, Wegbreit, 2003</marker>
<rawString>M. Montemerlo, S. Thrun, D. Koller, and B. Wegbreit. 2003. FastSLAM 2.0: An improved particle filtering algorithm for simultaneous localization and mapping that provably converges. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of Third Workshop on Very Large Corpora. ACL</booktitle>
<contexts>
<context position="10711" citStr="Ramshaw and Marcus, 1995" startWordPosition="1828" endWordPosition="1831"> it.” EDSZ := ”advance to the door straight ahead of you, then enter it.” Note that GH=go-hall, ED=enter-door, R1=first-right, LZ=left-at-end, SZ=ahead-of-you. The total number of possible actions is 15. This way, we can reduce the front-end task into a sequence of tagging tasks, much like the noun phrase chunking in the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000). Given a sequence of input tokens that forms a route instruction, a sequence of output labels, with each label matching an input token was prepared. We annotated with the BIO tagging scheme used in syntactic chunkers (Ramshaw and Marcus, 1995). make B-GHL1 left I-GHL1 , I-GHL1 first B-EDR1 door I-EDR1 on I-EDR1 the I-EDR1 right I-EDR1 4 Systems 4.1 System 1: CRFs 4.1.1 Model: A Linear-Chain Undirected Graphical Model From the output labels, we create the parts in a linear-chain undirected graph (Table 1). Our use of term part is based on (Bartlett et al, 2004). For each pair (xi, yi) in the training set, xi is the token (in the first column, Table 1), and yi 33 Transition Node (L0, L, j − 1, j) (L, j) no lexicalization no lexicalization xj−4 xj−3 xj−2 xj−1 xj xj+1 xj+2 xj+3 xj−1, xj xj+0, xj+1 Table 2: Features is the part (in the </context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L. Ramshaw and M. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of Third Workshop on Very Large Corpora. ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>S Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of Conference of Computational Natural Language Learning.</booktitle>
<contexts>
<context position="10467" citStr="Sang and Buchholz, 2000" startWordPosition="1787" endWordPosition="1790">irst hallway on the right, then turn right to face it.” EDR2 := ”advance to the second door on the right, then turn right to face it, then enter it.” GHLZ := ”advance to the hallway on the left at the end of the hallway, then turn left to face it.” EDSZ := ”advance to the door straight ahead of you, then enter it.” Note that GH=go-hall, ED=enter-door, R1=first-right, LZ=left-at-end, SZ=ahead-of-you. The total number of possible actions is 15. This way, we can reduce the front-end task into a sequence of tagging tasks, much like the noun phrase chunking in the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000). Given a sequence of input tokens that forms a route instruction, a sequence of output labels, with each label matching an input token was prepared. We annotated with the BIO tagging scheme used in syntactic chunkers (Ramshaw and Marcus, 1995). make B-GHL1 left I-GHL1 , I-GHL1 first B-EDR1 door I-EDR1 on I-EDR1 the I-EDR1 right I-EDR1 4 Systems 4.1 System 1: CRFs 4.1.1 Model: A Linear-Chain Undirected Graphical Model From the output labels, we create the parts in a linear-chain undirected graph (Table 1). Our use of term part is based on (Bartlett et al, 2004). For each pair (xi, yi) in the t</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of Conference of Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="2109" citStr="Winograd (1972)" startWordPosition="338" endWordPosition="339">llway ignoring the rooms to the left and right of you, but while going straight your going to eventually see a room facing you, which is north, enter it.” ”Head straight. continue straight past the first three doors until you hit a corner. On that corner there are two doors, one straight ahead of you and one on the right. Turn right and enter the room to the right and stop within.” These utterances are taken from an office navigation corpus collected from undergrad volunteers at SUNY/Albany. There is a good deal of variety. Previous efforts in this domain include the classic SHRDLU program by Winograd (1972), using a simulated robot, and the more ambitious IBL (Instruction-based Learning for Mobile Robots) project (Lauria et al, 2001) which tried to integrate vision, voice recognition, natural language understanding and robotics. This group has yet to publish performance statistics. In this paper we will focus on the application of machine learning to the understanding of written route instructions, and on testing by following the instructions in a simulated office environment. 2 Task 2.1 Input and Output Three inputs are required for the task: • Directions for reaching an office, written in unre</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>T. Winograd. 1972. Understanding Natural Language. Academic Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>