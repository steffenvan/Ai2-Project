<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.995962">
Subgroup Detection in Ideological Discussions
</title>
<author confidence="0.8714">
Amjad Abu-Jbara
</author>
<affiliation confidence="0.923351">
EECS Department
University of Michigan
</affiliation>
<address confidence="0.676089">
Ann Arbor, MI, USA
</address>
<email confidence="0.990186">
amjbara@umich.edu
</email>
<author confidence="0.997858">
Mona Diab
</author>
<affiliation confidence="0.9968715">
Center for Computational Learning Systems
Columbia University
</affiliation>
<address confidence="0.808297">
New York, NY, USA
</address>
<email confidence="0.998096">
mdiab@ccls.columbia.edu
</email>
<author confidence="0.98863">
Pradeep Dasigi
</author>
<affiliation confidence="0.9975435">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.780491">
New York, NY, USA
</address>
<email confidence="0.983269">
pd2359@columbia.edu
</email>
<author confidence="0.953624">
Dragomir Radev
</author>
<affiliation confidence="0.955616">
EECS Department
University of Michigan
</affiliation>
<address confidence="0.708307">
Ann Arbor, MI, USA
</address>
<email confidence="0.99852">
radev@umich.edu
</email>
<sectionHeader confidence="0.995639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997156">
The rapid and continuous growth of social
networking sites has led to the emergence of
many communities of communicating groups.
Many of these groups discuss ideological and
political topics. It is not uncommon that the
participants in such discussions split into two
or more subgroups. The members of each sub-
group share the same opinion toward the dis-
cussion topic and are more likely to agree with
members of the same subgroup and disagree
with members from opposing subgroups. In
this paper, we propose an unsupervised ap-
proach for automatically detecting discussant
subgroups in online communities. We analyze
the text exchanged between the participants of
a discussion to identify the attitude they carry
toward each other and towards the various as-
pects of the discussion topic. We use attitude
predictions to construct an attitude vector for
each discussant. We use clustering techniques
to cluster these vectors and, hence, determine
the subgroup membership of each participant.
We compare our methods to text clustering
and other baselines, and show that our method
achieves promising results.
</bodyText>
<sectionHeader confidence="0.999326" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99895475">
Online forums discussing ideological and political
topics are common1. When people discuss a dis-
puted topic they usually split into subgroups. The
members of each subgroup carry the same opinion
</bodyText>
<footnote confidence="0.978642">
1www.politicalforum.com, www.createdebate.com,
www.forandagainst.com, etc
</footnote>
<bodyText confidence="0.998719875">
toward the discission topic. The member of a sub-
group is more likely to show positive attitude to the
members of the same subgroup, and negative atti-
tude to the members of opposing subgroups.
For example, let us consider the following two
snippets from a debate about the enforcement of a
new immigration law in Arizona state in the United
States:
</bodyText>
<listItem confidence="0.9785326">
(1) Discussant 1: Arizona immigration law is good.
Illegal immigration is bad.
(2) Discussant 2: I totally disagree with you. Ari-
zona immigration law is blatant racism, and quite
unconstitutional.
</listItem>
<bodyText confidence="0.999662411764706">
In (1), the writer is expressing positive attitude
regarding the immigration law and negative attitude
regarding illegal immigration. The writer of (2) is
expressing negative attitude towards the writer of
(1) and negative attitude regarding the immigration
law. It is clear from this short dialog that the writer
of (1) and the writer of (2) are members of two
opposing subgroups. Discussant 1 is supporting the
new law, while Discussant 2 is against it.
In this paper, we present an unsupervised ap-
proach for determining the subgroup membership of
each participant in a discussion. We use linguistic
techniques to identify attitude expressions, their po-
larities, and their targets. The target of attitude could
be another discussant or an entity mentioned in the
discussion. We use sentiment analysis techniques
to identify opinion expressions. We use named en-
</bodyText>
<page confidence="0.985824">
399
</page>
<note confidence="0.9857595">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 399–409,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99979595">
tity recognition and noun phrase chunking to iden-
tify the entities mentioned in the discussion. The
opinion-target pairs are identified using a number of
syntactic and semantic rules.
For each participant in the discussion, we con-
struct a vector of attitude features. We call this vec-
tor the discussant attitude profile. The attitude pro-
file of a discussant contains an entry for every other
discussant and an entry for every entity mentioned
in the discission. We use clustering techniques to
cluster the attitude vector space. We use the clus-
tering results to determine the subgroup structure of
the discussion group and the subgroup membership
of each participant.
The rest of this paper is organized as follows. Sec-
tion 2 examines the previous work. We describe the
data used in the paper in Section 2.4. Section 3
presents our approach. Experiments, results and
analysis are presented in Section 4. We conclude
in Section 5
</bodyText>
<sectionHeader confidence="0.999899" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.99918">
2.1 Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.999985043478261">
Our work is related to a huge body of work on sen-
timent analysis. Previous work has studied senti-
ment in text at different levels of granularity. The
first level is identifying the polarity of individual
words. Hatzivassiloglou and McKeown (1997) pro-
posed a method to identify the polarity of adjec-
tives based on conjunctions linking them. Turney
and Littman (2003) used pointwise mutual infor-
mation (PMI) and latent semantic analysis (LSA)
to compute the association between a given word
and a set of positive/negative seed words. Taka-
mura et al. (2005) proposed using a spin model to
predict word polarity. Other studies used Word-
Net to improve word polarity prediction (Hu and
Liu, 2004a; Kamps et al., 2004; Kim and Hovy,
2004; Andreevskaia and Bergler, 2006). Hassan
and Radev (2010) used a random walk model built
on top of a word relatedness network to predict the
semantic orientation of English words. Hassan et
al. (2011) proposed a method to extend their random
walk model to assist word polarity identification in
other languages including Arabic and Hindi.
Other work focused on identifying the subjectiv-
ity of words. The goal of this work is to deter-
mine whether a given word is factual or subjective.
We use previous work on subjectivity and polar-
ity prediction to identify opinion words in discus-
sions. Some of the work on this problem classi-
fies words as factual or subjective regardless of their
context (Wiebe, 2000; Hatzivassiloglou and Wiebe,
2000; Banea et al., 2008). Some other work no-
ticed that the subjectivity of a given word depends
on its context. Therefor, several studies proposed
using contextual features to determine the subjec-
tivity of a given word within its context (Riloff and
Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Na-
sukawa and Yi, 2003; Popescu and Etzioni, 2005).
The second level of granularity is the sentence
level. Hassan et al. (2010) presents a method for
identifying sentences that display an attitude from
the text writer toward the text recipient. They de-
fine attitude as the mental position of one partici-
pant with regard to another participant. A very de-
tailed survey that covers techniques and approaches
in sentiment analysis and opinion mining could be
found in (Pang and Lee, 2008).
</bodyText>
<subsectionHeader confidence="0.998722">
2.2 Opinion Target Extraction
</subsectionHeader>
<bodyText confidence="0.999956217391304">
Several methods have been proposed to identify
the target of an opinion expression. Most of the
work have been done in the context of product re-
views mining (Hu and Liu, 2004b; Kobayashi et
al., 2007; Mei et al., 2007; Stoyanov and Cardie,
2008). In this context, opinion targets usually refer
to product features (i.e. product components or at-
tributes, as defined by Liu (2009)). In the work of
Hu and Liu (2004b), they treat frequent nouns and
noun phrases as product feature candidates. In our
work, we extract as targets frequent noun phrases
and named entities that are used by two or more dif-
ferent discussants. Scaffidi et al. (2007) propose a
language model approach to product feature extrac-
tion. They assume that product features are men-
tioned more often in product reviews than they ap-
pear in general English text. However, such statistics
may not be reliable when the corpus size is small.
In another related work, Jakob and
Gurevych (2010) showed that resolving the
anaphoric links in the text significantly improves
opinion target extraction. In our work, we use
anaphora resolution to improve opinion-target
</bodyText>
<page confidence="0.993529">
400
</page>
<tableCaption confidence="0.425843111111111">
Participant A posted: I support Arizona because they have every right to do so. They are just upholding well-established
federal law. All states should enact such a law.
Participant B commented on A’s I support the law because the federal government is either afraid or indifferent to the issue. Arizona
post: has the right and the responsibility to protect the people of the State of Arizona. If this requires a
possible slight inconvenience to any citizen so be it.
Participant C commented on B’s That is such a sad thing to say. You do realize that under the 14th Amendment, the very interaction
post: of a police officer asking you to prove your citizenship is Unconstitutional? As soon as you start
trading Constitutional rights for ”security”, then you’ve lost.
Table 1: Example posts from the Arizona Immigration Law thread
</tableCaption>
<bodyText confidence="0.926833">
pairing as shown in Section 3 below.
</bodyText>
<subsectionHeader confidence="0.998734">
2.3 Community Mining
</subsectionHeader>
<bodyText confidence="0.999975693877551">
Previous work also studied community mining in so-
cial media sites. Somasundaran and Wiebe (2009)
presents an unsupervised opinion analysis method
for debate-side classification. They mine the web
to learn associations that are indicative of opinion
stances in debates and combine this knowledge with
discourse information. Anand et al. (2011) present
a supervised method for stance classification. They
use a number of linguistic and structural features
such as unigrams, bigrams, cue words, repeated
punctuation, and opinion dependencies to build a
stance classification model. This work is limited to
dual sided debates and defines the problem as a clas-
sification task where the two debate sides are know
beforehand. Our work is characterized by handling
multi-side debates and by regarding the problem as
a clustering problem where the number of sides is
not known by the algorithm. This work also uti-
lizes only discussant-to-topic attitude predictions for
debate-side classification. Out work utilizes both
discussant-to-topic and discussant-to-discussant at-
titude predictions.
In another work, Kim and Hovy (2007) predict
the results of an election by analyzing discussion
threads in online forums that discuss the elections.
They use a supervised approach that uses unigrams,
bigrams, and trigrams as features. In contrast, our
work is unsupervised and uses different types infor-
mation. Moreover, although this work is related to
ours at the goal level, it does not involve any opinion
analysis.
Another related work classifies the speakers side
in a corpus of congressional floor debates, using
the speakers final vote on the bill as a labeling
for side (Thomas et al., 2006; Bansal et al., 2008;
Yessenalina et al., 2010). This work infers agree-
ment between speakers based on cases where one
speaker mentions another by name, and a simple al-
gorithm for determining the polarity of the sentence
in which the mention occurs. This work shows that
even with the resulting sparsely connected agree-
ment structure, the MinCut algorithm can improve
over stance classification based on textual informa-
tion alone. This work also requires that the de-
bate sides be known by the algorithm and it only
identifies discussant-to-discussant attitude. In our
experiments below we show that identifying both
discussant-to-discussant and discussant-to-topic at-
titudes achieves better results.
</bodyText>
<subsectionHeader confidence="0.988104">
2.4 Data
</subsectionHeader>
<bodyText confidence="0.9999773">
In this section, we describe the datasets used in
this paper. We use three different datasets. The
first dataset (politicalforum, henceforth) consists of
5,743 posts collected from a political forum2. All
the posts are in English. The posts cover 12 dis-
puted political and ideological topics. The discus-
sants of each topic were asked to participate in a
poll. The poll asked them to determine their stance
on the discussion topic by choosing one item from a
list of possible arguments. The list of participants
who voted for each argument was published with
the poll results. Each poll was accompanied by a
discussion thread. The people who participated in
the poll were allowed to post text to that thread to
justify their choices and to argue with other partic-
ipants. We collected the votes and the discussion
thread of each poll. We used the votes to identify
the subgroup membership of each participant.
The second dataset (createdebate, henceforth)
comes from an online debating site 3. It consists of
</bodyText>
<footnote confidence="0.9999775">
2http://www.politicalforum.com
3http://www.createdebate.com
</footnote>
<page confidence="0.99115">
401
</page>
<table confidence="0.999223866666666">
Source Topic Question #Sides #Posts #Participants
Politicalforum Arizona Immigration Law Do you support Arizona in its decision to enact their 2 738 59
Immigration Enforcement law?
Airport Security Should we pick muslims out of the line and give ad- 4 735 69
ditional scrutiny/screening?
Vote for Obama Will you vote for Obama in the 2012 Presidential 2 2599 197
elections?
Createdebate Evolution Has evolution been scientifically proved? 2 194 98
Social networking sites It is easier to maintain good relationships in social 2 70 31
networking sites such as Facebook.
Abortion Should abortion be banned 3 477 70
Wikipedia Ireland Misleading description of Irland island partition 3 40 10
South Africa Goverment Was the current form of South African government 3 23 5
born in May 1910?
Oil Spill Obama’s response to gulf oil spill 3 30 12
</table>
<tableCaption confidence="0.999403">
Table 2: Example threads from our three datasets
</tableCaption>
<bodyText confidence="0.999958333333333">
30 debates containing a total of 2,712 posts. Each
debate is about one topic. The description of each
debate states two or more positions regarding the de-
bate topic. When a new participant enters the discus-
sion, she explicitly picks a position and posts text to
support it, support a post written by another partici-
pant who took the same position, or to dispute a post
written by another participant who took an opposing
position. We collected the discussion thread and the
participant positions for each debate.
The third dataset (wikipedia, henceforth) comes
from the Wikipedia4 discussion section. When a
topic on Wikipedia is disputed, the editors of that
topic start a discussion about it. We collected 117
Wikipeida discussion threads. The threads contains
a total of 1,867 posts.
The politicalforum and createdebate datasets are
self labeled as described above. To annotate the
Wikipedia data, we asked an expert annotator (a
professor in sociolinguistics who is not one of the
authors) to read each of the Wikipedia discussion
threads and determine whether the discussants split
into subgroups in which case he was asked to deter-
mine the subgroup membership of each discussant.
Table 2 lists few example threads from our three
datasets. Table 1 shows a portion of discussion
thread between three participants about enforcing a
new immigration law in Arizona. This thread ap-
peared in the polictalforum dataset. The text posted
by the three participants indicates that A’s position
</bodyText>
<footnote confidence="0.901277">
4http://www.wikipedia.com
</footnote>
<bodyText confidence="0.99905525">
is with enforcing the law, that B agrees with A, and
that C disagrees with both. This means that A and B
belong to the same opinion subgroup, while belongs
to an opposing subgroup.
We randomly selected 6 threads from our datasets
(2 from politicalforum, 2 from createdebate, and 2
from Wikipedia) and used them as development set.
This set was used to develop our approach.
</bodyText>
<sectionHeader confidence="0.994646" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.9999825">
In this section, we describe a system that takes a
discussion thread as input and outputs the subgroup
membership of each discussant. Figure 1 illustrates
the processing steps performed by our system to de-
tect subgroups. In the following subsections we de-
scribe the different stages in the system pipeline.
</bodyText>
<subsectionHeader confidence="0.999164">
3.1 Thread Parsing
</subsectionHeader>
<bodyText confidence="0.999815142857143">
We start by parsing the thread to identify posts, par-
ticipants, and the reply structure of the thread (i.e.
who replies to whom). In the datasets described in
Section 2.4, all this information was explicitly avail-
able in the thread. We tokenize the text of each post
and split it into sentences using CLAIRLib (Abu-
Jbara and Radev, 2011).
</bodyText>
<subsectionHeader confidence="0.999442">
3.2 Opinion Word Identification
</subsectionHeader>
<bodyText confidence="0.99997925">
The next step is to identify the words that express
opinion and determine their polarity (positive or
negative). Lehrer (1974) defines word polarity as
the direction the word deviates to from the norm. We
</bodyText>
<page confidence="0.993586">
402
</page>
<bodyText confidence="0.999548785714286">
use OpinionFinder (Wilson et al., 2005a) to identify
polarized words and their polarities.
The polarity of a word is usally affected by
the context in which it appears. For example, the
word fine is positive when used as an adjective and
negative when used as a noun. For another example,
a positive word that appears in a negated context
becomes negative. OpinionFinder uses a large set of
features to identify the contextual polarity of a given
polarized word given its isolated polarity and the
sentence in which it appears (Wilson et al., 2005b).
Snippet (3) below shows the result of applying this
step to snippet (1) above (O means neutral; POS
means positive; NEG means negative).
</bodyText>
<listItem confidence="0.8553065">
(3) Arizona/O Immigration/O law/O good/POS ./O
Illegal/O immigration/O bad/NEG ./O
</listItem>
<subsectionHeader confidence="0.994936">
3.3 Target Identification
</subsectionHeader>
<bodyText confidence="0.99990625">
The goal of this step is to identify the possible tar-
gets of opinion. A target could be another discus-
sant or an entity mentioned in the discussion. When
the target of opinion is another discussant, either the
discussant name is mentioned explicitly or a second
person pronoun is used to indicate that the opinion
is targeting the recipient of the post. For example,
in snippet (2) above the second person pronoun you
indicates that the opinion word disagree is targeting
Discussant 1, the recipient of the post.
The target of opinion can also be an entity
mentioned in the discussion. We use two methods to
identify such entities. The first method uses shallow
parsing to identify noun groups (NG). We use the
Edinburgh Language Technology Text Tokenization
Toolkit (LT-TTT) (Grover et al., 2000) for this pur-
pose. We consider as an entity any noun group that
is mentioned by at least two different discussants.
We replace each identified entity with a unique
placeholder (ENTITYID). For example, the noun
group Arizona immigration law is mentioned by
Discussant 1 and Discussant 2 in snippets 1 and 2
above respectively. Therefore, we replace it with a
placehold as illustrated in snippets (4) and (5) below.
</bodyText>
<listItem confidence="0.844025">
(4) Discussant 1: ENTITY, is good. Illegal im-
</listItem>
<table confidence="0.998599">
NER NP Chunking
Barack Obama the Republican nominee
Middle East the maverick economists
Bush conservative ideologues
Bob McDonell the Nobel Prize
Iraq Federal Government
</table>
<tableCaption confidence="0.971727666666667">
Table 3: Some of the entities identified using NER and
NP Chunking in a discussion thread about the US 2012
elections
</tableCaption>
<bodyText confidence="0.257041">
migration is bad.
</bodyText>
<listItem confidence="0.968487">
(5) Discussant 2: I totally disagree with you. ENTITYl
is blatant racism, and quite unconstitutional.
</listItem>
<bodyText confidence="0.998704724137931">
We only consider as entities noun groups that
contain two words or more. We impose this require-
ment because individual nouns are very common
and regarding all of them as entities will introduce
significant noise.
In addition to this shallow parsing method, we
also use named entity recognition (NER) to identify
more entities. We use the Stanford Named Entity
Recognizer (Finkel et al., 2005) for this purpose. It
recognizes three types of entities: person, location,
and organization. We impose no restrictions on the
entities identified using this method. Again, we re-
place each distinct entity with a unique placeholder.
The final set of entities identified in a thread is the
union of the entities identified by the two aforemen-
tioned methods. Table 3
Finally, a challenge that always arises when
performing text mining tasks at this level of gran-
ularity is that entities are usually expressed by
anaphorical pronouns. Previous work has shown
that For example, the following snippet contains
an explicit mention of the entity Obama in the first
sentence, and then uses a pronoun to refer to the
same entity in the second sentence. The opinion
word unbeatable appears in the second sentence
and is syntactically related to the pronoun He.
In the next subsection, it will become clear why
knowing which entity does the pronoun He refers to
is essential for opinion-target pairing.
</bodyText>
<listItem confidence="0.498256">
(6) It doesn’t matter whether you vote for Obama.
</listItem>
<page confidence="0.996254">
403
</page>
<subsectionHeader confidence="0.803999">
Thread Parsing
</subsectionHeader>
<listItem confidence="0.9441733">
• Identify posts
• Identify discussants
• Identify the reply
structure
• Tokenize text.
• Split posts into sentences
Opinion Identification
• Identify polarized words
• Identify the contextual
polarity of each word
</listItem>
<subsectionHeader confidence="0.440155">
Target Identification
</subsectionHeader>
<listItem confidence="0.955828333333333">
• Anaphora resolution
• Identify named entities
• Identify Frequent noun
phrases.
• Identify mentions of
other discussants
</listItem>
<figure confidence="0.995364625">
Discussion
Thread
Clustering
Opinion-Target Pairing
• Dependency Rules
Subgroups
Discussant Attitude
Profiles (DAPs)
</figure>
<figureCaption confidence="0.999838">
Figure 1: An overview of the subgroups detection system
</figureCaption>
<bodyText confidence="0.931570642857143">
He is unbeatable.
Jakob and Gurevych (2010) showed experi-
mentally that resolving the anaphoric links in the
text significantly improves opinion target extraction.
We use the Beautiful Anaphora Resolution Toolkit
(BART) (Versley et al., 2008) to resolve all the
anaphoric links within the text of each post sepa-
rately. The result of applying this step to snippet (6)
is:
(6) It doesn’t matter whether you vote for Obama.
Obama is unbeatable.
Now, both mentions of Obama will be recog-
nized by the Stanford NER system and will be
identified as one entity.
</bodyText>
<subsectionHeader confidence="0.869417">
3.4 Opinion-Target Pairing
</subsectionHeader>
<bodyText confidence="0.9998584">
At this point, we have all the opinion words and
the potential targets identified separately. The next
step is to determine which opinion word is target-
ing which target. We propose a rule based approach
for opinion-target pairing. Our rules are based on
the dependency relations that connect the words in
a sentence. We use the Stanford Parser (Klein and
Manning, 2003) to generate the dependency parse
tree of each sentence in the thread. An opinion word
and a target form a pair if they stratify at least one
of our dependency rules. Table 4 illustrates some
of these rules 5. The rules basically examine the
types of the dependencies on the shortest path that
connect the opinion word and the target in the de-
pendency parse tree. It has been shown in previous
work on relation extraction that the shortest depen-
dency path between any two entities captures the in-
formation required to assert a relationship between
them (Bunescu and Mooney, 2005).
If a sentence S in a post written by participant
Pi contains an opinion word OPS and a target TRk,
and if the opinion-target pair satisfies one of our de-
pendency rules, we say that Pi expresses an attitude
towards TRk. The polarity of the attitude is deter-
mined by the polarity of OPS. We represent this as
</bodyText>
<equation confidence="0.88998575">
Pi � TRk if OPS is positive and Pi
� � TRk if OPS
−
is negative.
</equation>
<bodyText confidence="0.999520666666667">
It is likely that the same participant Pi express
sentiment toward the same target TRk multiple
times in different sentences in different posts. We
keep track of the counts of all the instances of posi-
tive/negative attitude Pi expresses toward TRk. We
represent this as Pi ���
</bodyText>
<equation confidence="0.7153245">
��
n−
</equation>
<bodyText confidence="0.9976555">
number of times Pi expressed positive (negative) at-
titude toward TRk.
</bodyText>
<subsectionHeader confidence="0.92718">
3.5 Discussant Attitude Profile
</subsectionHeader>
<bodyText confidence="0.9999465">
We propose a representation of discussants´attitudes
towards the identified targets in the discussion
thread. As stated above, a target could be another
discussant or an entity mentioned in the discussion.
</bodyText>
<footnote confidence="0.710749666666667">
5The code will be made publicly available at the time of
publication
TRk where m (n) is the
</footnote>
<page confidence="0.9187">
404
</page>
<table confidence="0.998906875">
ID Rule In Words Example
R1 OP —4 nsubj —4 TR The target TR is the nominal subject of the opinion ENTITY1TR is goodOP .
word OP
R2 OP —4 dobj —4 TR The target T is a direct object of the opinion OP I hateOP ENTITY2TR
R3 OP —4 prep * —4 TR The target TR is the object of a preposition that I totally disagreeOP with youTR.
modifies the opinion word OP
R4 TR —4 amod —4 OP The opinion is an adjectival modifier of the target The badOP ENTITY3TR is spreading lies
R5 OP —4 nsubjpass —4 TR The target TR is the nominal subject of the passive ENTITY4TR is hatedOP by everybody.
opinion word OP
R6 OP —4 prep * —4 poss —4 TR The opinion word OP connected through a prep * The main flawOP in yourTR analysis is
relation as in R2 to something possessed by the that it’s based on wrong assumptions.
target TR
R7 OP —4 dobj —4 poss —4 TR The target TR possesses something that is the direct I likeOP ENTITY5TR’s brilliant ideas.
object of the opinion word OP
R8 OP —4 csubj —4 nsubj —4 TR The opinon word OP is a causal subject of a phrase What ENTITY6TR announced was
that has the target TR as its nominal subject misleadingOP .
</table>
<tableCaption confidence="0.999809">
Table 4: Examples of the dependency rules used for opinion-target pairing.
</tableCaption>
<bodyText confidence="0.999955055555556">
Our representation is a vector containing numeri-
cal values. The values correspond to the counts of
positive/negative attitudes expressed by the discus-
sant toward each of the targets. We call this vector
the discussant attitude profile (DAP). We construct a
DAP for every discussant. Given a discussion thread
with d discussants and e entity targets, each attitude
profile vector has n = (d + e) * 3 dimensions. In
other words, each target (discussant or entity) has
three corresponding values in the DAP: 1) the num-
ber of times the discussant expressed positive atti-
tude toward the target, 2) the number of times the
discussant expressed a negative attitude towards the
target, and 3) the number of times the the discussant
interacted with or mentioned the target. It has to be
noted that these values are not symmetric since the
discussions explicitly denote the source and the tar-
get of each post.
</bodyText>
<subsectionHeader confidence="0.991709">
3.6 Clustering
</subsectionHeader>
<bodyText confidence="0.999996909090909">
At this point, we have an attitude profile (or vec-
tor) constructed for each discussant. Our goal is to
use these attitude profiles to determine the subgroup
membership of each discussant. We can achieve this
goal by noticing that the attitude profiles of discus-
sants who share the same opinion are more likely to
be similar to each other than to the attitude profiles
of discussants with opposing opinions. This sug-
gests that clustering the attitude vector space will
achieve the goal and split the discussants into sub-
groups according to their opinion.
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999921416666667">
In this section, we present several levels of evalu-
ation of our system. First, we compare our sys-
tem to baseline systems. Second, we study how the
choice of the clustering algorithm impacts the re-
sults. Third, we study the impact of each component
in our system on the performance. All the results
reported in this section that show difference in the
performance are statistically significant at the 0.05
level (as indicated by a 2-tailed paired t-test). Be-
fore describing the experiments and presenting the
results, we first describe the evaluation metrics we
use.
</bodyText>
<sectionHeader confidence="0.549448" genericHeader="evaluation">
4.0.1 Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.9999855">
We use two evaluation metrics to evaluate sub-
groups detection accuracy: Purity and Entropy. To
compute Purity (Manning et al., 2008), each clus-
ter is assigned the class of the majority vote within
the cluster, and then the accuracy of this assignment
is measured by dividing the number of correctly as-
signed members by the total number of instances. It
can be formally defined as:
</bodyText>
<equation confidence="0.901569">
1 � Max IWk n cjI (1)
purity(Q, C) = j
N
k
</equation>
<bodyText confidence="0.999936333333333">
where Q = {W1, W2, ..., Wk} is the set of clusters
and C = {c1, c2, ..., cJ} is the set of classes. Wk is
interpreted as the set of documents in Wk and cj as
</bodyText>
<page confidence="0.998254">
405
</page>
<bodyText confidence="0.999748714285714">
the set of documents in cj. The purity increases as
the quality of clustering improves.
The second metric is Entropy. The Entropy of a
cluster reflects how the members of the k distinct
subgroups are distributed within each resulting clus-
ter; the global quality measure is computed by aver-
aging the entropy of all clusters:
</bodyText>
<equation confidence="0.93691325">
i
E
P(i, j) x lo92P(i,j)
(2)
</equation>
<bodyText confidence="0.999906">
where P(i, j) is the probability of finding an ele-
ment from the category i in the cluster j, nj is the
number of items in cluster j, and n the total num-
ber of items in the distribution. In contrast to purity,
the entropy decreases as the quality of clustering im-
proves.
</bodyText>
<subsectionHeader confidence="0.999224">
4.1 Comparison to Baseline Systems
</subsectionHeader>
<bodyText confidence="0.999431678571429">
We compare our system (DAPC) that was described
in Section 3 to two baseline methods. The first base-
line (GC) uses graph clustering to partition a net-
work based on the interaction frequency between
participants. We build a graph where each node
represents a participant. Edges link participants if
they exchange posts, and edge weights are based on
the number of interactions. We tried two methods
for clustering the resulting graph: spectral partition-
ing (Luxburg, 2007) and a hierarchical agglomera-
tion algorithm which works by greedily optimizing
the modularity for graphs (Clauset et al., 2004).
The second baseline (TC) is based on the premise
that the member of the same subgroup are more
likely to use vocabulary drawn from the same lan-
guage model. We collect all the text posted by each
participant and create a tf-idf representations of the
text in a high dimensional vector space. We then
cluster the vector space to identify subgroups. We
use k-means (MacQueen, 1967) as our clustering
algorithm in this experiment (comparison of vari-
ous clustering algorithms is presented in the next
subsection). The distances between vectors are
Eculidean distances. Table 5 shows that our sys-
tem performs significantly better the baselines on the
three datasets in terms of both the purity (P) and the
entropy (E) (notice that lower entropy values indi-
cate better clustering). The values reported are the
</bodyText>
<table confidence="0.999703833333333">
Method Createdebate Politicalforum Wikipedia
P E P E P E
GC - Spectral 0.50 0.85 0.50 0.88 0.49 0.89
GC - Hierarchical 0.48 0.86 0.47 0.89 0.49 0.87
TC - kmeans 0.51 0.84 0.49 0.88 0.52 0.85
DAPC - kmeans 0.64 0.68 0.61 0.80 0.66 0.55
</table>
<tableCaption confidence="0.998449">
Table 5: Comparison to baseline systems
</tableCaption>
<table confidence="0.999935">
Method Createdebate Politicalforum Wikipedia
P E P E P E
DAPC - EM 0.63 0.71 0.61 0.82 0.63 0.61
DAPC - FF 0.63 0.70 0.60 0.83 0.64 0.59
DAPC - kmeans 0.64 0.68 0.61 0.80 0.66 0.55
</table>
<tableCaption confidence="0.999875">
Table 6: Comparison of different clustering algorithms
</tableCaption>
<bodyText confidence="0.9999198">
average results of the threads of each dataset. We
believe that the baselines performed poorly because
the interaction frequency and the text similarity are
not key factors in identifying subgroup structures.
Many people would respond to people they disagree
with more, while others would mainly respond to
people they agree with most of the time. Also, peo-
ple in opposing subgroups tend to use very similar
text when discussing the same topic and hence text
clustering does not work as well.
</bodyText>
<subsectionHeader confidence="0.997445">
4.2 Choice of the clustering algorithm
</subsectionHeader>
<bodyText confidence="0.999891789473684">
We experimented with three different clustering al-
gorithms: expectation maximization (EM), and k-
means (MacQueen, 1967), and FarthestFirst (FF)
(Hochbaum and Shmoys, 1985; Dasgupta, 2002).
As we did in the previous subsection, we use
Eculidean distance to measure the distance between
vectors All the system (DAP) components are in-
cluded as described in Section 3. The purity and
entropy values using each algorithm are shown in
Table 6. Although k-means seems to be performing
slightly better than other algorithms, the differences
in the results are not significant. This indicates that
the choice of the clustering algorithm does not have
a noticeable impact on the results. We also exper-
imented with using Manhattan distance and cosine
similarity instead of Euclidean distance to measure
the distance between attitude vectors. We noticed
that the choice of the distance does not have signifi-
cant impact on the results as well.
</bodyText>
<figure confidence="0.938075333333333">
Entropy = − Enj
j
n
</figure>
<page confidence="0.994869">
406
</page>
<subsectionHeader confidence="0.998208">
4.3 Component Evaluation
</subsectionHeader>
<bodyText confidence="0.999987782608696">
In this subsection, we evaluate the impact of the dif-
ferent components in the pipeline on the system per-
formance. We do that by removing each component
from the pipeline and measuring the change in per-
formance. We perform the following experiments:
1) We run the full system with all its components
included (DAPC). 2) We run the system and in-
clude only discussant-to-discussant attitude features
in the attitude vectors (DAPC-DD). 3) We include
only discussant-to-entity attitude features in the atti-
tude vectors (DAPC-DE). 4) We include only senti-
ment features in the attitude vector; i.e. we exclude
the interaction count features (DAPC-SE). 5) We in-
clude only interaction count features to the attitude
vector; i.e. we exclude sentiment features (DAPC-
INT). 6) We skip the anaphora resolution step in the
entity identification component (DAPC-NO AR). 7)
We only use named entity recognition to identify en-
tity targets; i.e. we exclude the entities identified
through noun phrasing chunking (DAPC-NER). 8)
Finally, we only noun phrase chunking to identify
entity targets (DAPC-NP). In all these experiments
k-means is used for clustering and the number of
clusters is set as explained in the previous subsec-
tion.
The results show that all the components in the
system contribute to better performance of the sys-
tem. We notice from the results that the performance
of the system drops significantly if sentiment fea-
tures are not included. This is result corroborates
our hypothesis that interaction features are not suffi-
cient factors for detecting rift in discussion groups.
Including interaction features improve the perfor-
mance (although not by a big difference) because
they help differentiate between the case where par-
ticipants A and B never interacted with each other
and the case where they interact several time but
never posted text that indicate difference in opin-
ion between them. We also notice that the perfor-
mance drops significantly in DAPC-DD and DAPC-
DD which also supports our hypotheses that both
the sentiment discussants show toward one another
and the sentiment they show toward the aspects of
the discussed topic are important for the task. Al-
though using both named entity recognition (NER)
and noun phrase chunking achieves better results, it
</bodyText>
<table confidence="0.9998122">
Method Createdebate Politicalforum Wikipedia
P E P E P E
DAPC 0.64 0.68 0.61 0.80 0.66 0.55
DAPC-DD 0.59 0.77 0.57 0.86 0.62 0.61
DAPC-DE 0.60 0.69 0.58 0.84 0.58 0.78
DAPC-SE 0.62 0.70 0.60 0.83 0.61 0.62
DAPC-INT 0.54 0.88 0.52 0.91 0.57 0.85
DAPC-NO AR 0.62 0.72 0.60 0.84 0.64 0.60
DAPC-NER 0.61 0.71 0.58 0.86 0.63 0.59
DAPC-NP 0.63 0.75 0.59 0.84 0.65 0.62
</table>
<tableCaption confidence="0.9879885">
Table 7: Impact of system components on the perfor-
mance
</tableCaption>
<bodyText confidence="0.9993342">
can also be noted from the results that NER con-
tributes more to the system performance. Finally,
the results support Jakob and Gurevych (2010) find-
ings that anaphora resolution aids opinion mining
systems.
</bodyText>
<sectionHeader confidence="0.999483" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999941615384615">
In this paper, we presented an approach for subgroup
detection in ideological discussions. Our system
uses linguistic analysis techniques to identify the at-
titude the participants of online discussions carry to-
ward each other and toward the aspects of the discus-
sion topic. Attitude prediction as well as interaction
frequency to construct an attitude vector for each
participant. The attitude vectors of discussants are
then clustered to form subgroups. Our experiments
showed that our system outperforms text clustering
and interaction graph clustering. We also studied the
contribution of each component in our system to the
overall performance.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99978875">
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
</bodyText>
<page confidence="0.997574">
407
</page>
<sectionHeader confidence="0.98832" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996791298076923">
Amjad Abu-Jbara and Dragomir Radev. 2011. Clairlib:
A toolkit for natural language processing, information
retrieval, and network analysis. In Proceedings of the
ACL-HLT 2011 System Demonstrations, pages 121–
126, Portland, Oregon, June. Association for Compu-
tational Linguistics.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool!: Classifying stance
in online debate. In Proceedings of the 2nd Workshop
on Computational Approaches to Subjectivity and Sen-
timent Analysis (WASSA 2.011), pages 1–9, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Alina Andreevskaia and Sabine Bergler. 2006. Mining
wordnet for fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In EACL’06.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC’08.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The
power of negative thinking: Exploiting label disagree-
ment in the min-cut classification framework.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 724–731, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
Aaron Clauset, Mark E. J. Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Phys. Rev. E, 70:066111.
Sanjoy Dasgupta. 2002. Performance guarantees for
hierarchical clustering. In 15th Annual Conference
on Computational Learning Theory, pages 351–363.
Springer.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ’05,
pages 363–370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Claire Grover, Colin Matheson, Andrei Mikheev, and
Marc Moens. 2000. Lt ttt - a flexible tokenisation
tool. In In Proceedings of Second International Con-
ference on Language Resources and Evaluation, pages
1147–1154.
Ahmed Hassan and Dragomir Radev. 2010. Identifying
text polarity using random walks. In ACL’10.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What’s with the attitude?: identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1245–1255.
Ahmed Hassan, Amjad AbuJbara, Rahul Jha, and
Dragomir Radev. 2011. Identifying the semantic
orientation of foreign words. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 592–597, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL’97, pages 174–181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING, pages 299–305.
Hochbaum and Shmoys. 1985. A best possible heuristic
for the k-center problem. Mathematics of Operations
Research, 10(2):180–184.
Minqing Hu and Bing Liu. 2004a. Mining and summa-
rizing customer reviews. In KDD’04, pages 168–177.
Minqing Hu and Bing Liu. 2004b. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ’04, pages 168–
177, New York, NY, USA. ACM.
Niklas Jakob and Iryna Gurevych. 2010. Using anaphora
resolution to improve opinion target identification in
movie reviews. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 263–268, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In National Insti-
tute for, pages 1115–1118.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In COLING, pages 1367–1373.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In IN PROCEEDINGS OF
THE 41ST ANNUAL MEETING OF THE ASSOCIA-
TION FOR COMPUTATIONAL LINGUISTICS, pages
423–430.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL.
Adrienne Lehrer. 1974. Semantic fields and lezical struc-
ture. North Holland, Amsterdam and New York.
</reference>
<page confidence="0.983348">
408
</page>
<reference confidence="0.998557086021506">
Bing Liu. 2009. Web Data Mining: Exploring Hyper-
links, Contents, and Usage Data (Data-Centric Sys-
tems and Applications). Springer, 1st ed. 2007. corr.
2nd printing edition, January.
Ulrike Luxburg. 2007. A tutorial on spectral clustering.
Statistics and Computing, 17:395–416, December.
J. B. MacQueen. 1967. Some methods for classification
and analysis of multivariate observations. In L. M. Le
Cam and J. Neyman, editors, Proc. of the fifth Berkeley
Symposium on Mathematical Statistics and Probabil-
ity, volume 1, pages 281–297. University of California
Press.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Pro-
ceedings of the 16th international conference on World
Wide Web, WWW ’07, pages 171–180, New York, NY,
USA. ACM.
Soo min Kim and Eduard Hovy. 2007. Crystal: Ana-
lyzing predictive opinions on the web. In In EMNLP-
CoNLL 2007.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In K-CAP ’03: Proceedings of the 2nd
international conference on Knowledge capture, pages
70–77.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1–135.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In HLT-
EMNLP’05, pages 339–346.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP’03, pages 105–112.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
226–234, Suntec, Singapore, August. Association for
Computational Linguistics.
Veselin Stoyanov and Claire Cardie. 2008. Topic iden-
tification for fine-grained opinion analysis. In In Col-
ing.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL’05, pages 133–140.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In In Proceedings
of EMNLP, pages 327–335.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315–346.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith, Xi-
aofeng Yang, and Alessandro Moschitti. 2008. Bart:
A modular toolkit for coreference resolution. In Pro-
ceedings of the ACL-08: HLT Demo Session, pages
9–12, Columbus, Ohio, June. Association for Compu-
tational Linguistics.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 735–740.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: a system for subjectiv-
ity analysis. In Proceedings of HLT/EMNLP on Inter-
active Demonstrations, HLT-Demo ’05, pages 34–35,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP’05, Vancou-
ver, Canada.
Ainur Yessenalina, Yisong Yue, and Claire Cardie. 2010.
Multi-level structured models for document-level sen-
timent classification. In In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In EMNLP’03, pages 129–136.
</reference>
<page confidence="0.999095">
409
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.021631">
<title confidence="0.997439">Subgroup Detection in Ideological Discussions</title>
<author confidence="0.775355">Amjad</author>
<affiliation confidence="0.9650755">EECS University of</affiliation>
<author confidence="0.543313">Ann Arbor</author>
<author confidence="0.543313">MI</author>
<email confidence="0.996234">amjbara@umich.edu</email>
<author confidence="0.938315">Mona</author>
<affiliation confidence="0.999211">Center for Computational Learning</affiliation>
<address confidence="0.944858">Columbia</address>
<author confidence="0.730061">New York</author>
<author confidence="0.730061">NY</author>
<email confidence="0.999526">mdiab@ccls.columbia.edu</email>
<author confidence="0.619067">Pradeep</author>
<affiliation confidence="0.998663">Department of Computer</affiliation>
<address confidence="0.881211">Columbia</address>
<author confidence="0.651672">New York</author>
<author confidence="0.651672">NY</author>
<email confidence="0.989295">pd2359@columbia.edu</email>
<author confidence="0.490294">Dragomir</author>
<affiliation confidence="0.9844755">EECS University of</affiliation>
<author confidence="0.445043">Ann Arbor</author>
<author confidence="0.445043">MI</author>
<email confidence="0.999621">radev@umich.edu</email>
<abstract confidence="0.999811846153846">The rapid and continuous growth of social networking sites has led to the emergence of many communities of communicating groups. Many of these groups discuss ideological and political topics. It is not uncommon that the participants in such discussions split into two or more subgroups. The members of each subgroup share the same opinion toward the discussion topic and are more likely to agree with members of the same subgroup and disagree with members from opposing subgroups. In this paper, we propose an unsupervised approach for automatically detecting discussant subgroups in online communities. We analyze the text exchanged between the participants of a discussion to identify the attitude they carry toward each other and towards the various aspects of the discussion topic. We use attitude predictions to construct an attitude vector for each discussant. We use clustering techniques to cluster these vectors and, hence, determine the subgroup membership of each participant. We compare our methods to text clustering and other baselines, and show that our method achieves promising results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amjad Abu-Jbara</author>
<author>Dragomir Radev</author>
</authors>
<title>Clairlib: A toolkit for natural language processing, information retrieval, and network analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL-HLT 2011 System Demonstrations,</booktitle>
<pages>121--126</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<marker>Abu-Jbara, Radev, 2011</marker>
<rawString>Amjad Abu-Jbara and Dragomir Radev. 2011. Clairlib: A toolkit for natural language processing, information retrieval, and network analysis. In Proceedings of the ACL-HLT 2011 System Demonstrations, pages 121– 126, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pranav Anand</author>
<author>Marilyn Walker</author>
<author>Rob Abbott</author>
<author>Jean E Fox Tree</author>
<author>Robeson Bowmani</author>
<author>Michael Minor</author>
</authors>
<title>Cats rule and dogs drool!: Classifying stance in online debate.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2.011),</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<contexts>
<context position="9046" citStr="Anand et al. (2011)" startWordPosition="1439" endWordPosition="1442">lice officer asking you to prove your citizenship is Unconstitutional? As soon as you start trading Constitutional rights for ”security”, then you’ve lost. Table 1: Example posts from the Arizona Immigration Law thread pairing as shown in Section 3 below. 2.3 Community Mining Previous work also studied community mining in social media sites. Somasundaran and Wiebe (2009) presents an unsupervised opinion analysis method for debate-side classification. They mine the web to learn associations that are indicative of opinion stances in debates and combine this knowledge with discourse information. Anand et al. (2011) present a supervised method for stance classification. They use a number of linguistic and structural features such as unigrams, bigrams, cue words, repeated punctuation, and opinion dependencies to build a stance classification model. This work is limited to dual sided debates and defines the problem as a classification task where the two debate sides are know beforehand. Our work is characterized by handling multi-side debates and by regarding the problem as a clustering problem where the number of sides is not known by the algorithm. This work also utilizes only discussant-to-topic attitud</context>
</contexts>
<marker>Anand, Walker, Abbott, Tree, Bowmani, Minor, 2011</marker>
<rawString>Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox Tree, Robeson Bowmani, and Michael Minor. 2011. Cats rule and dogs drool!: Classifying stance in online debate. In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2.011), pages 1–9, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alina Andreevskaia</author>
<author>Sabine Bergler</author>
</authors>
<title>Mining wordnet for fuzzy sentiment: Sentiment tag extraction from wordnet glosses.</title>
<date>2006</date>
<booktitle>In EACL’06.</booktitle>
<contexts>
<context position="5177" citStr="Andreevskaia and Bergler, 2006" startWordPosition="802" endWordPosition="805"> granularity. The first level is identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to determine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies </context>
</contexts>
<marker>Andreevskaia, Bergler, 2006</marker>
<rawString>Alina Andreevskaia and Sabine Bergler. 2006. Mining wordnet for fuzzy sentiment: Sentiment tag extraction from wordnet glosses. In EACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
</authors>
<title>A bootstrapping method for building subjectivity lexicons for languages with scarce resources.</title>
<date>2008</date>
<booktitle>In LREC’08.</booktitle>
<contexts>
<context position="5903" citStr="Banea et al., 2008" startWordPosition="924" endWordPosition="927">e semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to determine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another </context>
</contexts>
<marker>Banea, Mihalcea, Wiebe, 2008</marker>
<rawString>Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 2008. A bootstrapping method for building subjectivity lexicons for languages with scarce resources. In LREC’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Claire Cardie</author>
<author>Lillian Lee</author>
</authors>
<title>The power of negative thinking: Exploiting label disagreement in the min-cut classification framework.</title>
<date>2008</date>
<contexts>
<context position="10406" citStr="Bansal et al., 2008" startWordPosition="1650" endWordPosition="1653">n another work, Kim and Hovy (2007) predict the results of an election by analyzing discussion threads in online forums that discuss the elections. They use a supervised approach that uses unigrams, bigrams, and trigrams as features. In contrast, our work is unsupervised and uses different types information. Moreover, although this work is related to ours at the goal level, it does not involve any opinion analysis. Another related work classifies the speakers side in a corpus of congressional floor debates, using the speakers final vote on the bill as a labeling for side (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010). This work infers agreement between speakers based on cases where one speaker mentions another by name, and a simple algorithm for determining the polarity of the sentence in which the mention occurs. This work shows that even with the resulting sparsely connected agreement structure, the MinCut algorithm can improve over stance classification based on textual information alone. This work also requires that the debate sides be known by the algorithm and it only identifies discussant-to-discussant attitude. In our experiments below we show that identifying both discu</context>
</contexts>
<marker>Bansal, Cardie, Lee, 2008</marker>
<rawString>Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The power of negative thinking: Exploiting label disagreement in the min-cut classification framework.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>724--731</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="21831" citStr="Bunescu and Mooney, 2005" startWordPosition="3505" endWordPosition="3508"> the Stanford Parser (Klein and Manning, 2003) to generate the dependency parse tree of each sentence in the thread. An opinion word and a target form a pair if they stratify at least one of our dependency rules. Table 4 illustrates some of these rules 5. The rules basically examine the types of the dependencies on the shortest path that connect the opinion word and the target in the dependency parse tree. It has been shown in previous work on relation extraction that the shortest dependency path between any two entities captures the information required to assert a relationship between them (Bunescu and Mooney, 2005). If a sentence S in a post written by participant Pi contains an opinion word OPS and a target TRk, and if the opinion-target pair satisfies one of our dependency rules, we say that Pi expresses an attitude towards TRk. The polarity of the attitude is determined by the polarity of OPS. We represent this as Pi � TRk if OPS is positive and Pi � � TRk if OPS − is negative. It is likely that the same participant Pi express sentiment toward the same target TRk multiple times in different sentences in different posts. We keep track of the counts of all the instances of positive/negative attitude Pi</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 724–731, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Clauset</author>
<author>Mark E J Newman</author>
<author>Cristopher Moore</author>
</authors>
<title>Finding community structure in very large networks.</title>
<date>2004</date>
<journal>Phys. Rev. E,</journal>
<pages>70--066111</pages>
<contexts>
<context position="28002" citStr="Clauset et al., 2004" startWordPosition="4604" endWordPosition="4607">on to Baseline Systems We compare our system (DAPC) that was described in Section 3 to two baseline methods. The first baseline (GC) uses graph clustering to partition a network based on the interaction frequency between participants. We build a graph where each node represents a participant. Edges link participants if they exchange posts, and edge weights are based on the number of interactions. We tried two methods for clustering the resulting graph: spectral partitioning (Luxburg, 2007) and a hierarchical agglomeration algorithm which works by greedily optimizing the modularity for graphs (Clauset et al., 2004). The second baseline (TC) is based on the premise that the member of the same subgroup are more likely to use vocabulary drawn from the same language model. We collect all the text posted by each participant and create a tf-idf representations of the text in a high dimensional vector space. We then cluster the vector space to identify subgroups. We use k-means (MacQueen, 1967) as our clustering algorithm in this experiment (comparison of various clustering algorithms is presented in the next subsection). The distances between vectors are Eculidean distances. Table 5 shows that our system perf</context>
</contexts>
<marker>Clauset, Newman, Moore, 2004</marker>
<rawString>Aaron Clauset, Mark E. J. Newman, and Cristopher Moore. 2004. Finding community structure in very large networks. Phys. Rev. E, 70:066111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjoy Dasgupta</author>
</authors>
<title>Performance guarantees for hierarchical clustering.</title>
<date>2002</date>
<booktitle>In 15th Annual Conference on Computational Learning Theory,</booktitle>
<pages>351--363</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="30036" citStr="Dasgupta, 2002" startWordPosition="4947" endWordPosition="4948">the interaction frequency and the text similarity are not key factors in identifying subgroup structures. Many people would respond to people they disagree with more, while others would mainly respond to people they agree with most of the time. Also, people in opposing subgroups tend to use very similar text when discussing the same topic and hence text clustering does not work as well. 4.2 Choice of the clustering algorithm We experimented with three different clustering algorithms: expectation maximization (EM), and kmeans (MacQueen, 1967), and FarthestFirst (FF) (Hochbaum and Shmoys, 1985; Dasgupta, 2002). As we did in the previous subsection, we use Eculidean distance to measure the distance between vectors All the system (DAP) components are included as described in Section 3. The purity and entropy values using each algorithm are shown in Table 6. Although k-means seems to be performing slightly better than other algorithms, the differences in the results are not significant. This indicates that the choice of the clustering algorithm does not have a noticeable impact on the results. We also experimented with using Manhattan distance and cosine similarity instead of Euclidean distance to mea</context>
</contexts>
<marker>Dasgupta, 2002</marker>
<rawString>Sanjoy Dasgupta. 2002. Performance guarantees for hierarchical clustering. In 15th Annual Conference on Computational Learning Theory, pages 351–363. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18710" citStr="Finkel et al., 2005" startWordPosition="2998" endWordPosition="3001">ome of the entities identified using NER and NP Chunking in a discussion thread about the US 2012 elections migration is bad. (5) Discussant 2: I totally disagree with you. ENTITYl is blatant racism, and quite unconstitutional. We only consider as entities noun groups that contain two words or more. We impose this requirement because individual nouns are very common and regarding all of them as entities will introduce significant noise. In addition to this shallow parsing method, we also use named entity recognition (NER) to identify more entities. We use the Stanford Named Entity Recognizer (Finkel et al., 2005) for this purpose. It recognizes three types of entities: person, location, and organization. We impose no restrictions on the entities identified using this method. Again, we replace each distinct entity with a unique placeholder. The final set of entities identified in a thread is the union of the entities identified by the two aforementioned methods. Table 3 Finally, a challenge that always arises when performing text mining tasks at this level of granularity is that entities are usually expressed by anaphorical pronouns. Previous work has shown that For example, the following snippet conta</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 363–370, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Grover</author>
<author>Colin Matheson</author>
<author>Andrei Mikheev</author>
<author>Marc Moens</author>
</authors>
<title>Lt ttt - a flexible tokenisation tool. In</title>
<date>2000</date>
<booktitle>In Proceedings of Second International Conference on Language Resources and Evaluation,</booktitle>
<pages>1147--1154</pages>
<contexts>
<context position="17451" citStr="Grover et al., 2000" startWordPosition="2794" endWordPosition="2797">s another discussant, either the discussant name is mentioned explicitly or a second person pronoun is used to indicate that the opinion is targeting the recipient of the post. For example, in snippet (2) above the second person pronoun you indicates that the opinion word disagree is targeting Discussant 1, the recipient of the post. The target of opinion can also be an entity mentioned in the discussion. We use two methods to identify such entities. The first method uses shallow parsing to identify noun groups (NG). We use the Edinburgh Language Technology Text Tokenization Toolkit (LT-TTT) (Grover et al., 2000) for this purpose. We consider as an entity any noun group that is mentioned by at least two different discussants. We replace each identified entity with a unique placeholder (ENTITYID). For example, the noun group Arizona immigration law is mentioned by Discussant 1 and Discussant 2 in snippets 1 and 2 above respectively. Therefore, we replace it with a placehold as illustrated in snippets (4) and (5) below. (4) Discussant 1: ENTITY, is good. Illegal imNER NP Chunking Barack Obama the Republican nominee Middle East the maverick economists Bush conservative ideologues Bob McDonell the Nobel P</context>
</contexts>
<marker>Grover, Matheson, Mikheev, Moens, 2000</marker>
<rawString>Claire Grover, Colin Matheson, Andrei Mikheev, and Marc Moens. 2000. Lt ttt - a flexible tokenisation tool. In In Proceedings of Second International Conference on Language Resources and Evaluation, pages 1147–1154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Dragomir Radev</author>
</authors>
<title>Identifying text polarity using random walks.</title>
<date>2010</date>
<booktitle>In ACL’10.</booktitle>
<contexts>
<context position="5202" citStr="Hassan and Radev (2010)" startWordPosition="806" endWordPosition="809">identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to determine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subje</context>
</contexts>
<marker>Hassan, Radev, 2010</marker>
<rawString>Ahmed Hassan and Dragomir Radev. 2010. Identifying text polarity using random walks. In ACL’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Vahed Qazvinian</author>
<author>Dragomir Radev</author>
</authors>
<title>What’s with the attitude?: identifying sentences with attitude in online discussions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1245--1255</pages>
<contexts>
<context position="6299" citStr="Hassan et al. (2010)" startWordPosition="989" endWordPosition="992"> prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and Liu, 2004b; Kobayashi et al., 2007; Mei et al.</context>
</contexts>
<marker>Hassan, Qazvinian, Radev, 2010</marker>
<rawString>Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev. 2010. What’s with the attitude?: identifying sentences with attitude in online discussions. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1245–1255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Amjad AbuJbara</author>
<author>Rahul Jha</author>
<author>Dragomir Radev</author>
</authors>
<title>Identifying the semantic orientation of foreign words.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>592--597</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="5345" citStr="Hassan et al. (2011)" startWordPosition="831" endWordPosition="834">n conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to determine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subject</context>
</contexts>
<marker>Hassan, AbuJbara, Jha, Radev, 2011</marker>
<rawString>Ahmed Hassan, Amjad AbuJbara, Rahul Jha, and Dragomir Radev. 2011. Identifying the semantic orientation of foreign words. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 592–597, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In EACL’97,</booktitle>
<pages>174--181</pages>
<contexts>
<context position="4660" citStr="Hatzivassiloglou and McKeown (1997)" startWordPosition="716" endWordPosition="719">bgroup structure of the discussion group and the subgroup membership of each participant. The rest of this paper is organized as follows. Section 2 examines the previous work. We describe the data used in the paper in Section 2.4. Section 3 presents our approach. Experiments, results and analysis are presented in Section 4. We conclude in Section 5 2 Related Work 2.1 Sentiment Analysis Our work is related to a huge body of work on sentiment analysis. Previous work has studied sentiment in text at different levels of granularity. The first level is identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedne</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In EACL’97, pages 174–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Janyce Wiebe</author>
</authors>
<title>Effects of adjective orientation and gradability on sentence subjectivity.</title>
<date>2000</date>
<booktitle>In COLING,</booktitle>
<pages>299--305</pages>
<contexts>
<context position="5882" citStr="Hatzivassiloglou and Wiebe, 2000" startWordPosition="920" endWordPosition="923"> relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to determine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant wi</context>
</contexts>
<marker>Hatzivassiloglou, Wiebe, 2000</marker>
<rawString>Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Effects of adjective orientation and gradability on sentence subjectivity. In COLING, pages 299–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hochbaum</author>
<author>Shmoys</author>
</authors>
<title>A best possible heuristic for the k-center problem.</title>
<date>1985</date>
<journal>Mathematics of Operations Research,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="30019" citStr="Hochbaum and Shmoys, 1985" startWordPosition="4943" endWordPosition="4946">s performed poorly because the interaction frequency and the text similarity are not key factors in identifying subgroup structures. Many people would respond to people they disagree with more, while others would mainly respond to people they agree with most of the time. Also, people in opposing subgroups tend to use very similar text when discussing the same topic and hence text clustering does not work as well. 4.2 Choice of the clustering algorithm We experimented with three different clustering algorithms: expectation maximization (EM), and kmeans (MacQueen, 1967), and FarthestFirst (FF) (Hochbaum and Shmoys, 1985; Dasgupta, 2002). As we did in the previous subsection, we use Eculidean distance to measure the distance between vectors All the system (DAP) components are included as described in Section 3. The purity and entropy values using each algorithm are shown in Table 6. Although k-means seems to be performing slightly better than other algorithms, the differences in the results are not significant. This indicates that the choice of the clustering algorithm does not have a noticeable impact on the results. We also experimented with using Manhattan distance and cosine similarity instead of Euclidea</context>
</contexts>
<marker>Hochbaum, Shmoys, 1985</marker>
<rawString>Hochbaum and Shmoys. 1985. A best possible heuristic for the k-center problem. Mathematics of Operations Research, 10(2):180–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In KDD’04,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="5103" citStr="Hu and Liu, 2004" startWordPosition="790" endWordPosition="793">s work has studied sentiment in text at different levels of granularity. The first level is identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to determine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify </context>
<context position="6862" citStr="Hu and Liu, 2004" startWordPosition="1084" endWordPosition="1087">anularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and Liu, 2004b; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008). In this context, opinion targets usually refer to product features (i.e. product components or attributes, as defined by Liu (2009)). In the work of Hu and Liu (2004b), they treat frequent nouns and noun phrases as product feature candidates. In our work, we extract as targets frequent noun phrases and named entities that are used by two or more different discussants. Scaffidi et al. (2007) propose a language model approach to product feature extraction. They assume that product features are mentioned more often in produc</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004a. Mining and summarizing customer reviews. In KDD’04, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5103" citStr="Hu and Liu, 2004" startWordPosition="790" endWordPosition="793">s work has studied sentiment in text at different levels of granularity. The first level is identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to determine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify </context>
<context position="6862" citStr="Hu and Liu, 2004" startWordPosition="1084" endWordPosition="1087">anularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and Liu, 2004b; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008). In this context, opinion targets usually refer to product features (i.e. product components or attributes, as defined by Liu (2009)). In the work of Hu and Liu (2004b), they treat frequent nouns and noun phrases as product feature candidates. In our work, we extract as targets frequent noun phrases and named entities that are used by two or more different discussants. Scaffidi et al. (2007) propose a language model approach to product feature extraction. They assume that product features are mentioned more often in produc</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004b. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’04, pages 168– 177, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niklas Jakob</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using anaphora resolution to improve opinion target identification in movie reviews.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>263--268</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="7640" citStr="Jakob and Gurevych (2010)" startWordPosition="1215" endWordPosition="1218"> components or attributes, as defined by Liu (2009)). In the work of Hu and Liu (2004b), they treat frequent nouns and noun phrases as product feature candidates. In our work, we extract as targets frequent noun phrases and named entities that are used by two or more different discussants. Scaffidi et al. (2007) propose a language model approach to product feature extraction. They assume that product features are mentioned more often in product reviews than they appear in general English text. However, such statistics may not be reliable when the corpus size is small. In another related work, Jakob and Gurevych (2010) showed that resolving the anaphoric links in the text significantly improves opinion target extraction. In our work, we use anaphora resolution to improve opinion-target 400 Participant A posted: I support Arizona because they have every right to do so. They are just upholding well-established federal law. All states should enact such a law. Participant B commented on A’s I support the law because the federal government is either afraid or indifferent to the issue. Arizona post: has the right and the responsibility to protect the people of the State of Arizona. If this requires a possible sli</context>
<context position="20346" citStr="Jakob and Gurevych (2010)" startWordPosition="3254" endWordPosition="3257">r whether you vote for Obama. 403 Thread Parsing • Identify posts • Identify discussants • Identify the reply structure • Tokenize text. • Split posts into sentences Opinion Identification • Identify polarized words • Identify the contextual polarity of each word Target Identification • Anaphora resolution • Identify named entities • Identify Frequent noun phrases. • Identify mentions of other discussants Discussion Thread Clustering Opinion-Target Pairing • Dependency Rules Subgroups Discussant Attitude Profiles (DAPs) Figure 1: An overview of the subgroups detection system He is unbeatable. Jakob and Gurevych (2010) showed experimentally that resolving the anaphoric links in the text significantly improves opinion target extraction. We use the Beautiful Anaphora Resolution Toolkit (BART) (Versley et al., 2008) to resolve all the anaphoric links within the text of each post separately. The result of applying this step to snippet (6) is: (6) It doesn’t matter whether you vote for Obama. Obama is unbeatable. Now, both mentions of Obama will be recognized by the Stanford NER system and will be identified as one entity. 3.4 Opinion-Target Pairing At this point, we have all the opinion words and the potential </context>
<context position="33659" citStr="Jakob and Gurevych (2010)" startWordPosition="5541" endWordPosition="5544">tion (NER) and noun phrase chunking achieves better results, it Method Createdebate Politicalforum Wikipedia P E P E P E DAPC 0.64 0.68 0.61 0.80 0.66 0.55 DAPC-DD 0.59 0.77 0.57 0.86 0.62 0.61 DAPC-DE 0.60 0.69 0.58 0.84 0.58 0.78 DAPC-SE 0.62 0.70 0.60 0.83 0.61 0.62 DAPC-INT 0.54 0.88 0.52 0.91 0.57 0.85 DAPC-NO AR 0.62 0.72 0.60 0.84 0.64 0.60 DAPC-NER 0.61 0.71 0.58 0.86 0.63 0.59 DAPC-NP 0.63 0.75 0.59 0.84 0.65 0.62 Table 7: Impact of system components on the performance can also be noted from the results that NER contributes more to the system performance. Finally, the results support Jakob and Gurevych (2010) findings that anaphora resolution aids opinion mining systems. 5 Conclusions In this paper, we presented an approach for subgroup detection in ideological discussions. Our system uses linguistic analysis techniques to identify the attitude the participants of online discussions carry toward each other and toward the aspects of the discussion topic. Attitude prediction as well as interaction frequency to construct an attitude vector for each participant. The attitude vectors of discussants are then clustered to form subgroups. Our experiments showed that our system outperforms text clustering </context>
</contexts>
<marker>Jakob, Gurevych, 2010</marker>
<rawString>Niklas Jakob and Iryna Gurevych. 2010. Using anaphora resolution to improve opinion target identification in movie reviews. In Proceedings of the ACL 2010 Conference Short Papers, pages 263–268, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaap Kamps</author>
<author>Maarten Marx</author>
<author>Robert J Mokken</author>
<author>Maarten De Rijke</author>
</authors>
<title>Using wordnet to measure semantic orientations of adjectives.</title>
<date>2004</date>
<booktitle>In National Institute for,</booktitle>
<pages>1115--1118</pages>
<marker>Kamps, Marx, Mokken, De Rijke, 2004</marker>
<rawString>Jaap Kamps, Maarten Marx, Robert J. Mokken, and Maarten De Rijke. 2004. Using wordnet to measure semantic orientations of adjectives. In National Institute for, pages 1115–1118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In COLING,</booktitle>
<pages>1367--1373</pages>
<contexts>
<context position="5144" citStr="Kim and Hovy, 2004" startWordPosition="798" endWordPosition="801"> different levels of granularity. The first level is identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to determine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In COLING, pages 1367–1373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In IN PROCEEDINGS OF THE 41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="21252" citStr="Klein and Manning, 2003" startWordPosition="3404" endWordPosition="3407">ult of applying this step to snippet (6) is: (6) It doesn’t matter whether you vote for Obama. Obama is unbeatable. Now, both mentions of Obama will be recognized by the Stanford NER system and will be identified as one entity. 3.4 Opinion-Target Pairing At this point, we have all the opinion words and the potential targets identified separately. The next step is to determine which opinion word is targeting which target. We propose a rule based approach for opinion-target pairing. Our rules are based on the dependency relations that connect the words in a sentence. We use the Stanford Parser (Klein and Manning, 2003) to generate the dependency parse tree of each sentence in the thread. An opinion word and a target form a pair if they stratify at least one of our dependency rules. Table 4 illustrates some of these rules 5. The rules basically examine the types of the dependencies on the shortest path that connect the opinion word and the target in the dependency parse tree. It has been shown in previous work on relation extraction that the shortest dependency path between any two entities captures the information required to assert a relationship between them (Bunescu and Mooney, 2005). If a sentence S in </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In IN PROCEEDINGS OF THE 41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nozomi Kobayashi</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Extracting aspect-evaluation and aspect-of relations in opinion mining.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL.</booktitle>
<contexts>
<context position="6887" citStr="Kobayashi et al., 2007" startWordPosition="1088" endWordPosition="1091">ntence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and Liu, 2004b; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008). In this context, opinion targets usually refer to product features (i.e. product components or attributes, as defined by Liu (2009)). In the work of Hu and Liu (2004b), they treat frequent nouns and noun phrases as product feature candidates. In our work, we extract as targets frequent noun phrases and named entities that are used by two or more different discussants. Scaffidi et al. (2007) propose a language model approach to product feature extraction. They assume that product features are mentioned more often in product reviews than they appea</context>
</contexts>
<marker>Kobayashi, Inui, Matsumoto, 2007</marker>
<rawString>Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 2007. Extracting aspect-evaluation and aspect-of relations in opinion mining. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrienne Lehrer</author>
</authors>
<title>Semantic fields and lezical structure. North Holland,</title>
<date>1974</date>
<location>Amsterdam and New York.</location>
<contexts>
<context position="15774" citStr="Lehrer (1974)" startWordPosition="2518" endWordPosition="2519"> subgroups. In the following subsections we describe the different stages in the system pipeline. 3.1 Thread Parsing We start by parsing the thread to identify posts, participants, and the reply structure of the thread (i.e. who replies to whom). In the datasets described in Section 2.4, all this information was explicitly available in the thread. We tokenize the text of each post and split it into sentences using CLAIRLib (AbuJbara and Radev, 2011). 3.2 Opinion Word Identification The next step is to identify the words that express opinion and determine their polarity (positive or negative). Lehrer (1974) defines word polarity as the direction the word deviates to from the norm. We 402 use OpinionFinder (Wilson et al., 2005a) to identify polarized words and their polarities. The polarity of a word is usally affected by the context in which it appears. For example, the word fine is positive when used as an adjective and negative when used as a noun. For another example, a positive word that appears in a negated context becomes negative. OpinionFinder uses a large set of features to identify the contextual polarity of a given polarized word given its isolated polarity and the sentence in which i</context>
</contexts>
<marker>Lehrer, 1974</marker>
<rawString>Adrienne Lehrer. 1974. Semantic fields and lezical structure. North Holland, Amsterdam and New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Web Data Mining: Exploring Hyperlinks, Contents,</title>
<date>2009</date>
<booktitle>and Usage Data (Data-Centric Systems and Applications). Springer, 1st ed. 2007. corr. 2nd printing edition,</booktitle>
<contexts>
<context position="7066" citStr="Liu (2009)" startWordPosition="1119" endWordPosition="1120">tion of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and Liu, 2004b; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008). In this context, opinion targets usually refer to product features (i.e. product components or attributes, as defined by Liu (2009)). In the work of Hu and Liu (2004b), they treat frequent nouns and noun phrases as product feature candidates. In our work, we extract as targets frequent noun phrases and named entities that are used by two or more different discussants. Scaffidi et al. (2007) propose a language model approach to product feature extraction. They assume that product features are mentioned more often in product reviews than they appear in general English text. However, such statistics may not be reliable when the corpus size is small. In another related work, Jakob and Gurevych (2010) showed that resolving the</context>
</contexts>
<marker>Liu, 2009</marker>
<rawString>Bing Liu. 2009. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data (Data-Centric Systems and Applications). Springer, 1st ed. 2007. corr. 2nd printing edition, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrike Luxburg</author>
</authors>
<title>A tutorial on spectral clustering.</title>
<date>2007</date>
<journal>Statistics and Computing,</journal>
<pages>17--395</pages>
<contexts>
<context position="27875" citStr="Luxburg, 2007" startWordPosition="4587" endWordPosition="4588">ms in the distribution. In contrast to purity, the entropy decreases as the quality of clustering improves. 4.1 Comparison to Baseline Systems We compare our system (DAPC) that was described in Section 3 to two baseline methods. The first baseline (GC) uses graph clustering to partition a network based on the interaction frequency between participants. We build a graph where each node represents a participant. Edges link participants if they exchange posts, and edge weights are based on the number of interactions. We tried two methods for clustering the resulting graph: spectral partitioning (Luxburg, 2007) and a hierarchical agglomeration algorithm which works by greedily optimizing the modularity for graphs (Clauset et al., 2004). The second baseline (TC) is based on the premise that the member of the same subgroup are more likely to use vocabulary drawn from the same language model. We collect all the text posted by each participant and create a tf-idf representations of the text in a high dimensional vector space. We then cluster the vector space to identify subgroups. We use k-means (MacQueen, 1967) as our clustering algorithm in this experiment (comparison of various clustering algorithms </context>
</contexts>
<marker>Luxburg, 2007</marker>
<rawString>Ulrike Luxburg. 2007. A tutorial on spectral clustering. Statistics and Computing, 17:395–416, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B MacQueen</author>
</authors>
<title>Some methods for classification and analysis of multivariate observations.</title>
<date>1967</date>
<booktitle>Proc. of the fifth Berkeley Symposium on Mathematical Statistics and Probability,</booktitle>
<volume>1</volume>
<pages>281--297</pages>
<editor>In L. M. Le Cam and J. Neyman, editors,</editor>
<publisher>University of California Press.</publisher>
<contexts>
<context position="28382" citStr="MacQueen, 1967" startWordPosition="4672" endWordPosition="4673">interactions. We tried two methods for clustering the resulting graph: spectral partitioning (Luxburg, 2007) and a hierarchical agglomeration algorithm which works by greedily optimizing the modularity for graphs (Clauset et al., 2004). The second baseline (TC) is based on the premise that the member of the same subgroup are more likely to use vocabulary drawn from the same language model. We collect all the text posted by each participant and create a tf-idf representations of the text in a high dimensional vector space. We then cluster the vector space to identify subgroups. We use k-means (MacQueen, 1967) as our clustering algorithm in this experiment (comparison of various clustering algorithms is presented in the next subsection). The distances between vectors are Eculidean distances. Table 5 shows that our system performs significantly better the baselines on the three datasets in terms of both the purity (P) and the entropy (E) (notice that lower entropy values indicate better clustering). The values reported are the Method Createdebate Politicalforum Wikipedia P E P E P E GC - Spectral 0.50 0.85 0.50 0.88 0.49 0.89 GC - Hierarchical 0.48 0.86 0.47 0.89 0.49 0.87 TC - kmeans 0.51 0.84 0.49</context>
<context position="29968" citStr="MacQueen, 1967" startWordPosition="4938" endWordPosition="4939">ach dataset. We believe that the baselines performed poorly because the interaction frequency and the text similarity are not key factors in identifying subgroup structures. Many people would respond to people they disagree with more, while others would mainly respond to people they agree with most of the time. Also, people in opposing subgroups tend to use very similar text when discussing the same topic and hence text clustering does not work as well. 4.2 Choice of the clustering algorithm We experimented with three different clustering algorithms: expectation maximization (EM), and kmeans (MacQueen, 1967), and FarthestFirst (FF) (Hochbaum and Shmoys, 1985; Dasgupta, 2002). As we did in the previous subsection, we use Eculidean distance to measure the distance between vectors All the system (DAP) components are included as described in Section 3. The purity and entropy values using each algorithm are shown in Table 6. Although k-means seems to be performing slightly better than other algorithms, the differences in the results are not significant. This indicates that the choice of the clustering algorithm does not have a noticeable impact on the results. We also experimented with using Manhattan</context>
</contexts>
<marker>MacQueen, 1967</marker>
<rawString>J. B. MacQueen. 1967. Some methods for classification and analysis of multivariate observations. In L. M. Le Cam and J. Neyman, editors, Proc. of the fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281–297. University of California Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Schtze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="26292" citStr="Manning et al., 2008" startWordPosition="4294" endWordPosition="4297">o baseline systems. Second, we study how the choice of the clustering algorithm impacts the results. Third, we study the impact of each component in our system on the performance. All the results reported in this section that show difference in the performance are statistically significant at the 0.05 level (as indicated by a 2-tailed paired t-test). Before describing the experiments and presenting the results, we first describe the evaluation metrics we use. 4.0.1 Evaluation Metrics We use two evaluation metrics to evaluate subgroups detection accuracy: Purity and Entropy. To compute Purity (Manning et al., 2008), each cluster is assigned the class of the majority vote within the cluster, and then the accuracy of this assignment is measured by dividing the number of correctly assigned members by the total number of instances. It can be formally defined as: 1 � Max IWk n cjI (1) purity(Q, C) = j N k where Q = {W1, W2, ..., Wk} is the set of clusters and C = {c1, c2, ..., cJ} is the set of classes. Wk is interpreted as the set of documents in Wk and cj as 405 the set of documents in cj. The purity increases as the quality of clustering improves. The second metric is Entropy. The Entropy of a cluster ref</context>
</contexts>
<marker>Manning, Raghavan, Schtze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schtze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xu Ling</author>
<author>Matthew Wondra</author>
<author>Hang Su</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Topic sentiment mixture: modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web, WWW ’07,</booktitle>
<pages>171--180</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6905" citStr="Mei et al., 2007" startWordPosition="1092" endWordPosition="1095">al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and Liu, 2004b; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008). In this context, opinion targets usually refer to product features (i.e. product components or attributes, as defined by Liu (2009)). In the work of Hu and Liu (2004b), they treat frequent nouns and noun phrases as product feature candidates. In our work, we extract as targets frequent noun phrases and named entities that are used by two or more different discussants. Scaffidi et al. (2007) propose a language model approach to product feature extraction. They assume that product features are mentioned more often in product reviews than they appear in general Engli</context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of the 16th international conference on World Wide Web, WWW ’07, pages 171–180, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Crystal: Analyzing predictive opinions on the web. In</title>
<date>2007</date>
<booktitle>In EMNLPCoNLL</booktitle>
<contexts>
<context position="9822" citStr="Kim and Hovy (2007)" startWordPosition="1554" endWordPosition="1557">d punctuation, and opinion dependencies to build a stance classification model. This work is limited to dual sided debates and defines the problem as a classification task where the two debate sides are know beforehand. Our work is characterized by handling multi-side debates and by regarding the problem as a clustering problem where the number of sides is not known by the algorithm. This work also utilizes only discussant-to-topic attitude predictions for debate-side classification. Out work utilizes both discussant-to-topic and discussant-to-discussant attitude predictions. In another work, Kim and Hovy (2007) predict the results of an election by analyzing discussion threads in online forums that discuss the elections. They use a supervised approach that uses unigrams, bigrams, and trigrams as features. In contrast, our work is unsupervised and uses different types information. Moreover, although this work is related to ours at the goal level, it does not involve any opinion analysis. Another related work classifies the speakers side in a corpus of congressional floor debates, using the speakers final vote on the bill as a labeling for side (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et</context>
</contexts>
<marker>Kim, Hovy, 2007</marker>
<rawString>Soo min Kim and Eduard Hovy. 2007. Crystal: Analyzing predictive opinions on the web. In In EMNLPCoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Nasukawa</author>
<author>Jeonghee Yi</author>
</authors>
<title>Sentiment analysis: capturing favorability using natural language processing.</title>
<date>2003</date>
<booktitle>In K-CAP ’03: Proceedings of the 2nd international conference on Knowledge capture,</booktitle>
<pages>70--77</pages>
<contexts>
<context position="6194" citStr="Nasukawa and Yi, 2003" startWordPosition="971" endWordPosition="975">determine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have </context>
</contexts>
<marker>Nasukawa, Yi, 2003</marker>
<rawString>Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment analysis: capturing favorability using natural language processing. In K-CAP ’03: Proceedings of the 2nd international conference on Knowledge capture, pages 70–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="6656" citStr="Pang and Lee, 2008" startWordPosition="1048" endWordPosition="1051">xtual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and Liu, 2004b; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008). In this context, opinion targets usually refer to product features (i.e. product components or attributes, as defined by Liu (2009)). In the work of Hu and Liu (2004b), they treat frequent nouns and noun phrases as product feature candidates. In our work, we extract as targets frequent noun phrases and named entities th</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In HLTEMNLP’05,</booktitle>
<pages>339--346</pages>
<contexts>
<context position="6222" citStr="Popescu and Etzioni, 2005" startWordPosition="976" endWordPosition="979">en word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of </context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In HLTEMNLP’05, pages 339–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions. In</title>
<date>2003</date>
<booktitle>EMNLP’03,</booktitle>
<pages>105--112</pages>
<contexts>
<context position="6140" citStr="Riloff and Wiebe, 2003" startWordPosition="963" endWordPosition="966">the subjectivity of words. The goal of this work is to determine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.2 Opinion Target Extraction Several methods have been proposed to identify the t</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In EMNLP’03, pages 105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>226--234</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="8800" citStr="Somasundaran and Wiebe (2009)" startWordPosition="1404" endWordPosition="1407">t the people of the State of Arizona. If this requires a possible slight inconvenience to any citizen so be it. Participant C commented on B’s That is such a sad thing to say. You do realize that under the 14th Amendment, the very interaction post: of a police officer asking you to prove your citizenship is Unconstitutional? As soon as you start trading Constitutional rights for ”security”, then you’ve lost. Table 1: Example posts from the Arizona Immigration Law thread pairing as shown in Section 3 below. 2.3 Community Mining Previous work also studied community mining in social media sites. Somasundaran and Wiebe (2009) presents an unsupervised opinion analysis method for debate-side classification. They mine the web to learn associations that are indicative of opinion stances in debates and combine this knowledge with discourse information. Anand et al. (2011) present a supervised method for stance classification. They use a number of linguistic and structural features such as unigrams, bigrams, cue words, repeated punctuation, and opinion dependencies to build a stance classification model. This work is limited to dual sided debates and defines the problem as a classification task where the two debate side</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2009. Recognizing stances in online debates. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 226–234, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
</authors>
<title>Topic identification for fine-grained opinion analysis.</title>
<date>2008</date>
<booktitle>In In Coling.</booktitle>
<contexts>
<context position="6933" citStr="Stoyanov and Cardie, 2008" startWordPosition="1096" endWordPosition="1099">s a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and Liu, 2004b; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008). In this context, opinion targets usually refer to product features (i.e. product components or attributes, as defined by Liu (2009)). In the work of Hu and Liu (2004b), they treat frequent nouns and noun phrases as product feature candidates. In our work, we extract as targets frequent noun phrases and named entities that are used by two or more different discussants. Scaffidi et al. (2007) propose a language model approach to product feature extraction. They assume that product features are mentioned more often in product reviews than they appear in general English text. However, such stati</context>
</contexts>
<marker>Stoyanov, Cardie, 2008</marker>
<rawString>Veselin Stoyanov and Claire Cardie. 2008. Topic identification for fine-grained opinion analysis. In In Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Takashi Inui</author>
<author>Manabu Okumura</author>
</authors>
<title>Extracting semantic orientations of words using spin model.</title>
<date>2005</date>
<booktitle>In ACL’05,</booktitle>
<pages>133--140</pages>
<contexts>
<context position="4968" citStr="Takamura et al. (2005)" startWordPosition="766" endWordPosition="770">n 4. We conclude in Section 5 2 Related Work 2.1 Sentiment Analysis Our work is related to a huge body of work on sentiment analysis. Previous work has studied sentiment in text at different levels of granularity. The first level is identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work i</context>
</contexts>
<marker>Takamura, Inui, Okumura, 2005</marker>
<rawString>Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientations of words using spin model. In ACL’05, pages 133–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from congressional floor-debate transcripts. In</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>327--335</pages>
<contexts>
<context position="10385" citStr="Thomas et al., 2006" startWordPosition="1646" endWordPosition="1649">titude predictions. In another work, Kim and Hovy (2007) predict the results of an election by analyzing discussion threads in online forums that discuss the elections. They use a supervised approach that uses unigrams, bigrams, and trigrams as features. In contrast, our work is unsupervised and uses different types information. Moreover, although this work is related to ours at the goal level, it does not involve any opinion analysis. Another related work classifies the speakers side in a corpus of congressional floor debates, using the speakers final vote on the bill as a labeling for side (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010). This work infers agreement between speakers based on cases where one speaker mentions another by name, and a simple algorithm for determining the polarity of the sentence in which the mention occurs. This work shows that even with the resulting sparsely connected agreement structure, the MinCut algorithm can improve over stance classification based on textual information alone. This work also requires that the debate sides be known by the algorithm and it only identifies discussant-to-discussant attitude. In our experiments below we show that i</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: Determining support or opposition from congressional floor-debate transcripts. In In Proceedings of EMNLP, pages 327–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<pages>21--315</pages>
<contexts>
<context position="4779" citStr="Turney and Littman (2003)" startWordPosition="736" endWordPosition="739">follows. Section 2 examines the previous work. We describe the data used in the paper in Section 2.4. Section 3 presents our approach. Experiments, results and analysis are presented in Section 4. We conclude in Section 5 2 Related Work 2.1 Sentiment Analysis Our work is related to a huge body of work on sentiment analysis. Previous work has studied sentiment in text at different levels of granularity. The first level is identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter Turney and Michael Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21:315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
<author>Simone Paolo Ponzetto</author>
<author>Massimo Poesio</author>
<author>Vladimir Eidelman</author>
<author>Alan Jern</author>
<author>Jason Smith</author>
<author>Xiaofeng Yang</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Bart: A modular toolkit for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-08: HLT Demo Session,</booktitle>
<pages>9--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="20544" citStr="Versley et al., 2008" startWordPosition="3282" endWordPosition="3285">larized words • Identify the contextual polarity of each word Target Identification • Anaphora resolution • Identify named entities • Identify Frequent noun phrases. • Identify mentions of other discussants Discussion Thread Clustering Opinion-Target Pairing • Dependency Rules Subgroups Discussant Attitude Profiles (DAPs) Figure 1: An overview of the subgroups detection system He is unbeatable. Jakob and Gurevych (2010) showed experimentally that resolving the anaphoric links in the text significantly improves opinion target extraction. We use the Beautiful Anaphora Resolution Toolkit (BART) (Versley et al., 2008) to resolve all the anaphoric links within the text of each post separately. The result of applying this step to snippet (6) is: (6) It doesn’t matter whether you vote for Obama. Obama is unbeatable. Now, both mentions of Obama will be recognized by the Stanford NER system and will be identified as one entity. 3.4 Opinion-Target Pairing At this point, we have all the opinion words and the potential targets identified separately. The next step is to determine which opinion word is targeting which target. We propose a rule based approach for opinion-target pairing. Our rules are based on the dep</context>
</contexts>
<marker>Versley, Ponzetto, Poesio, Eidelman, Jern, Smith, Yang, Moschitti, 2008</marker>
<rawString>Yannick Versley, Simone Paolo Ponzetto, Massimo Poesio, Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng Yang, and Alessandro Moschitti. 2008. Bart: A modular toolkit for coreference resolution. In Proceedings of the ACL-08: HLT Demo Session, pages 9–12, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence,</booktitle>
<pages>735--740</pages>
<contexts>
<context position="5848" citStr="Wiebe, 2000" startWordPosition="918" endWordPosition="919">top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to determine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the men</context>
</contexts>
<marker>Wiebe, 2000</marker>
<rawString>Janyce Wiebe. 2000. Learning subjective adjectives from corpora. In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence, pages 735–740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Opinionfinder: a system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP on Interactive Demonstrations, HLT-Demo ’05,</booktitle>
<pages>34--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="15895" citStr="Wilson et al., 2005" startWordPosition="2537" endWordPosition="2540">g We start by parsing the thread to identify posts, participants, and the reply structure of the thread (i.e. who replies to whom). In the datasets described in Section 2.4, all this information was explicitly available in the thread. We tokenize the text of each post and split it into sentences using CLAIRLib (AbuJbara and Radev, 2011). 3.2 Opinion Word Identification The next step is to identify the words that express opinion and determine their polarity (positive or negative). Lehrer (1974) defines word polarity as the direction the word deviates to from the norm. We 402 use OpinionFinder (Wilson et al., 2005a) to identify polarized words and their polarities. The polarity of a word is usally affected by the context in which it appears. For example, the word fine is positive when used as an adjective and negative when used as a noun. For another example, a positive word that appears in a negated context becomes negative. OpinionFinder uses a large set of features to identify the contextual polarity of a given polarized word given its isolated polarity and the sentence in which it appears (Wilson et al., 2005b). Snippet (3) below shows the result of applying this step to snippet (1) above (O means </context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005a. Opinionfinder: a system for subjectivity analysis. In Proceedings of HLT/EMNLP on Interactive Demonstrations, HLT-Demo ’05, pages 34–35, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP’05,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="15895" citStr="Wilson et al., 2005" startWordPosition="2537" endWordPosition="2540">g We start by parsing the thread to identify posts, participants, and the reply structure of the thread (i.e. who replies to whom). In the datasets described in Section 2.4, all this information was explicitly available in the thread. We tokenize the text of each post and split it into sentences using CLAIRLib (AbuJbara and Radev, 2011). 3.2 Opinion Word Identification The next step is to identify the words that express opinion and determine their polarity (positive or negative). Lehrer (1974) defines word polarity as the direction the word deviates to from the norm. We 402 use OpinionFinder (Wilson et al., 2005a) to identify polarized words and their polarities. The polarity of a word is usally affected by the context in which it appears. For example, the word fine is positive when used as an adjective and negative when used as a noun. For another example, a positive word that appears in a negated context becomes negative. OpinionFinder uses a large set of features to identify the contextual polarity of a given polarized word given its isolated polarity and the sentence in which it appears (Wilson et al., 2005b). Snippet (3) below shows the result of applying this step to snippet (1) above (O means </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005b. Recognizing contextual polarity in phraselevel sentiment analysis. In HLT/EMNLP’05, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Yisong Yue</author>
<author>Claire Cardie</author>
</authors>
<title>Multi-level structured models for document-level sentiment classification. In</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP.</booktitle>
<contexts>
<context position="10433" citStr="Yessenalina et al., 2010" startWordPosition="1654" endWordPosition="1657">nd Hovy (2007) predict the results of an election by analyzing discussion threads in online forums that discuss the elections. They use a supervised approach that uses unigrams, bigrams, and trigrams as features. In contrast, our work is unsupervised and uses different types information. Moreover, although this work is related to ours at the goal level, it does not involve any opinion analysis. Another related work classifies the speakers side in a corpus of congressional floor debates, using the speakers final vote on the bill as a labeling for side (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010). This work infers agreement between speakers based on cases where one speaker mentions another by name, and a simple algorithm for determining the polarity of the sentence in which the mention occurs. This work shows that even with the resulting sparsely connected agreement structure, the MinCut algorithm can improve over stance classification based on textual information alone. This work also requires that the debate sides be known by the algorithm and it only identifies discussant-to-discussant attitude. In our experiments below we show that identifying both discussant-to-discussant and dis</context>
</contexts>
<marker>Yessenalina, Yue, Cardie, 2010</marker>
<rawString>Ainur Yessenalina, Yisong Yue, and Claire Cardie. 2010. Multi-level structured models for document-level sentiment classification. In In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In EMNLP’03,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="6171" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="967" endWordPosition="970">s. The goal of this work is to determine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression.</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: separating facts from opinions and identifying the polarity of opinion sentences. In EMNLP’03, pages 129–136.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>