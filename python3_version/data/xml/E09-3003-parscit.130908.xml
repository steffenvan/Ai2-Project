<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000319">
<title confidence="0.996189">
Combining a Statistical Language Model with Logistic Regression to
Predict the Lexical and Syntactic Difficulty of Texts for FFL
</title>
<author confidence="0.72722">
Thomas L. Franc¸ois
</author>
<note confidence="0.39060225">
Aspirant FNRS
CENTAL (Center for Natural Language Processing)
Universit´e catholique de Louvain
1348 Louvain-la-Neuve, Belgium
</note>
<email confidence="0.995672">
thomas.francois@uclouvain.be
</email>
<sectionHeader confidence="0.993808" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979235294118">
Reading is known to be an essential task
in language learning, but finding the ap-
propriate text for every learner is far from
easy. In this context, automatic procedures
can support the teacher’s work. Some
tools exist for English, but at present there
are none for French as a foreign language
(FFL). In this paper, we present an origi-
nal approach to assessing the readability
of FFL texts using NLP techniques and
extracts from FFL textbooks as our cor-
pus. Two logistic regression models based
on lexical and grammatical features are
explored and give quite good predictions
on new texts. The results shows a slight
superiority for multinomial logistic re-
gression over the proportional odds model.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979676470588">
The current massive mobility of people has put
increasing pressure on the language teaching sec-
tor, in terms of the availability of instructors and
suitable teaching materials. The development of
Intelligent Computer Aided Language Learning
(ICALL) has helped both these needs, while the
Internet has increasingly been used as a source of
exercises. Indeed, it allows immediate access to a
huge number of texts which can be used for edu-
cational purposes, either for classical reading com-
prehension tasks, or as a corpus for the creation of
various automatically generated exercises.
However, the strength of the Internet is also its
main flaw: there are so many texts available to the
teacher that he or she can get lost. Having gathered
some documents suitable in terms of subject mat-
ter, teachers still have to check if their readabil-
ity levels are suitable for their students : a highly
time-consuming task. This is where NLP applica-
tions able to classify documents according to their
reading difficulty level can be invaluable.
Related research will be discussed in Section 2.
In Section 3, the distinctive features of the cor-
pus used in this study and a difficulty scale suit-
able for FFL text classification are described. Sec-
tion 4 focuses on the independent linguistic vari-
ables considered in this research, while the statis-
tical techniques used for predictions are covered
in Section 5. Section 6 gives some details of the
implementations, and Section 7 presents the first
results of our models. Finally, Section 8 sums up
the contribution of this article before providing a
programme for future work and improvement of
the results.
</bodyText>
<sectionHeader confidence="0.991478" genericHeader="introduction">
2 Related research
</sectionHeader>
<bodyText confidence="0.999952541666667">
The measurement of the reading difficulty of texts
has been a major concern in the English-speaking
literature since the 1920s and the first formula de-
veloped by Lively and Pressey (1923). The field
of readability has since produced many formulae
based on simple lexical and syntactic measures
such as the average number of syllables per word,
the average length of sentences in a piece of text
(Flesch, 1948; Kincaid et al., 1975), or the per-
centage of words not on a list combined with the
average sentence length (Chall and Dale, 1995).
French-speaking researchers discovered the
field of readability in 1956 through the work of
Andr´e Conquet, La lisibilit´e (1971), and the first
two formulae for French were adapted from Flesch
(1948) by Kandel and Moles (1958) and de Land-
sheere (1963). Both of these researchers stayed
quite close to the Flesch formula, and in so doing
they failed to take into account some specificities
of the French language.
Henry (1975) was the first to introduce spe-
cific formulae for French. He used a larger set
of variables to design three formulae : a com-
plete, an automatic and a short one, each of which
</bodyText>
<note confidence="0.9598055">
Proceedings of the EACL 2009 Student Research Workshop, pages 19–27,
Athens, Greece, 2 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.999211">
19
</page>
<bodyText confidence="0.999931288135593">
was adapted for three different educational lev-
els. His formulae are by far the best and most fre-
quently used in the French-speaking world. Later,
Richaudeau (1979) suggested a criteria of “lin-
guistic efficiency” based on experiments on short-
term memory, while Mesnager (1989) coined what
is still, to the best of our knowledge, the most re-
cent specific formula for French, with children as
its target.
Compared to the mass of studies in English,
readability in French has never enthused the re-
search community. The cultural reasons for this
are analysed by Boss´e-Andrieu (1993) (who basi-
cally argues that the idea of measuring text diffi-
culty objectively seems far too pragmatic for the
French spirit). It follows that there is little cur-
rent research in this field: in Belgium, the Flesch
formula is still used to assess the readability of
articles in journalism studies. This example also
shows that the French-specific formulae are not
much used, probably because of their complexity
(Boss´e-Andrieu, 1993).
Of course, if there is little work on French read-
ability, there is even less on French as a foreign
language. We only know the study of Cornaire
(1988), which tested the adaptation of Henry’s
short formula to French as a foreign language,
and that of Uitdenbogerd (2005), which developed
a new measure for English-speaking learners of
French, stressing the importance of cognates when
developing a new formula for a related language.
Therefore, we had to draw our inspiration from
the English-speaking world, which has recently
experienced a revival of interest in research on
readability. Taking advantage of the increasing
power of computers and the development of NLP
techniques, researchers have been able to exper-
iment with more complex variables. Collins-
Thompson et al. (2005) presented a variation of a
multinomial naive Bayesian classifier they called
the “Smoothed Unigram” model. We retained
from their work the use of language models in-
stead of word lists to measure lexical complex-
ity. Schwarm and Ostendorf (2005) developed
a SVM categoriser combining a classifier based
on trigram language models (one for each level
of difficulty), some parsing features such as av-
erage tree height, and variables traditionally used
in readability. Heilman et al. (2007) extended the
“Smoothed Unigram” model by the recognition of
syntactic structures, in order to assess L2 English
texts. Later, they improved the combination of
their various lexical and grammatical features us-
ing regression methods (Heilman et al., 2008). We
also found regression methods to be the most ef-
ficient of the statistical models with which we ex-
perimented. In this article, we consider some ways
to adapt these various ideas to the specific case of
FFL readability.
</bodyText>
<sectionHeader confidence="0.964328" genericHeader="method">
3 Corpus description
</sectionHeader>
<bodyText confidence="0.99928695">
In the development of a new readability formula,
the first step is to collect a corpus labelled by
reading-difficulty level, a task that implies agree-
ment on the difficulty scale. In the US, a com-
mon choice is the 12 American grade levels corre-
sponding to primary and secondary school. How-
ever, this scale is less relevant for FFL education
in Europe. So, we looked for another scale.
Given that we are looking for an automatic way
of measuring text complexity for FFL learners par-
ticipating in an educational programme, an obvi-
ous choice was the difficulty scale used for assess-
ing students’ levels in Europe, that is the Com-
mon European Framework of Reference for Lan-
guages (CEFR) (Council of Europe, 2001) . The
CEFR has six levels: A1 (Breakthrough); A2
(Waystage); B1 (Threshold); B2 (Vantage); C1
(Effective Operational Proficiency) and C2 (Mas-
tery). However differences in learners’ skills can
be quite substantial at lower levels, so we divided
each of the A1, A2 and B1 grades in two, thus ob-
taining a total of nine levels.
We still needed to find a corpus labelled accord-
ing to these nine classes. Unlike traditional ap-
proaches, based on a limited set of texts usually
standardised by applying a closure test to a target
population, our NLP-oriented approach required a
large number of texts on which the statistical mod-
els could be trained. For that reason we opted for
FFL textbooks as a corpus. With the appearance of
the CEFR, FFL textbooks have undergone a kind
of standardisation and their levels have been clari-
fied. It is thus feasible to gather a large number of
documents already labelled in terms of the CEFR
scale by experts with an educational background.
However, not every textbook can be used as a
document source. Likewise, not all the material
from FFL textbooks is appropriate. We established
the following criteria for selecting textbooks and
texts:
</bodyText>
<listItem confidence="0.924608">
• The CEFR was published in 2001, so only
</listItem>
<page confidence="0.99173">
20
</page>
<bodyText confidence="0.99848675">
textbooks published since then were con-
sidered. This restriction also ensures that
the language resembles present-day spoken
French.
</bodyText>
<listItem confidence="0.817702166666667">
• The target population for our formula is
young people and adults. Therefore, only
textbooks intended for this public were used.
• We retained only those texts made up of com-
plete sentences, linked to a reading compre-
hension task. So, all the transcriptions of
listening comprehension tasks were ignored.
Similarly, all instructions to the students were
excluded, because there is no guarantee the
language employed there is the same as the
rest of the textbook material (metalinguistic
terms and so on can be found there).
</listItem>
<bodyText confidence="0.998444454545455">
Up to now, using these criteria, we have gath-
ered more than 1,500 documents containing about
440,000 tokens. Texts cover a wide variety of sub-
jects ranging from French literature to newspaper
articles, as well as numerous dialogues, extracts
from plays, cooking recipes, etc. The goal is to
have as wide a coverage as possible, to achieve
maximum generalisability of the formula, and also
to check what sort of texts it does not fit (e.g. sta-
tistical descriptive analyses have considered songs
and poems as outliers).
</bodyText>
<sectionHeader confidence="0.791851" genericHeader="method">
4 Selection of lexical and syntactic
variables
</sectionHeader>
<bodyText confidence="0.999975483870968">
Any text classification tasks require an object
(here a text) to be parameterised into variables,
whether qualitative or quantitative. These inde-
pendent variables must correlate as strongly as
possible with the dependent variable represent-
ing difficulty in order to explain the text’s com-
plexity, and they should also account for the var-
ious dimensions of the readability phenomenon.
Traditional approaches to readability have been
sharply criticised with respect to this second re-
quirement by Kintsch and Vipond (1979) and
Kemper (1983), who both insist on the impor-
tance of including the conceptual properties of
texts (such as the relations between propositions
and the “inference load”). However, these new
approaches have not resulted in any easily repro-
ducible computational models, leading current re-
searchers to continue to use the classic semantic
and grammatical variables, enhancing them with
NLP techniques.
Because this research only spans the last year,
attempts to discover interesting variables are still
at an early stage. We explored the efficiency of
some traditional features such as the type-token
ratio, the number of letters per word, and the av-
erage sentence length, and found that, on our cor-
pus, only the word length and sentence length cor-
related significantly with difficulty. Then, we add
two NLP-oriented features, as described below: a
statistical language model and a measure of tense
difficulty.
</bodyText>
<subsectionHeader confidence="0.993043">
4.1 The language model
</subsectionHeader>
<bodyText confidence="0.967878384615385">
The lexical difficulty of a text is quite an elaborate
phenomenon to parameterise. The logistic regres-
sion models we used in this study require us to re-
duce this complex reality to just one number, the
challenge being to achieve the most informative
number. Some psychological work (Howes and
Solomon, 1951; Gerhand and Barry, 1998; Brys-
baert et al., 2000) suggests that there is a strong re-
lationship between the frequency of words and the
speed with which they are recognised. We there-
fore opted to model the lexical difficulty for read-
ing as the global probability of a text T (with N
tokens) occurring:
</bodyText>
<equation confidence="0.8933455">
P(T) = P(t1)P(t2  |t1)
··· P(tn  |t1, t2, ... , tn−1) (1)
</equation>
<bodyText confidence="0.941297">
This equation raises two issues :
</bodyText>
<listItem confidence="0.997018545454546">
1. Estimating the conditional probabilities. It
is well-known that it is impossible to train
such a model on a corpus, even the largest
one, because some sequences in this equa-
tion are unlikely to be encountered more than
once. However, following Collins-Thompson
and Callan (2005), we found that a simple
smoothed unigram model could give good re-
sults for readability. Thus, we assumed that
the global probability of a text T could be re-
duced to:
</listItem>
<equation confidence="0.978272333333333">
n
P(T) = p(ti) (2)
i=1
</equation>
<bodyText confidence="0.8093628">
where p(ti) is the probability of meeting the
token ti in French; and n is the number of
tokens in a text.
2. Deciding what is the best linguistic unit to
consider. The equations introduced above use
</bodyText>
<page confidence="0.995584">
21
</page>
<bodyText confidence="0.999892416666667">
tokens, as is traditional in readability formu-
lae, but the inflected nature of French sug-
gests that lemmas may be a better alternative.
Using tokens means that words taking numer-
ous inflected forms (such as verbs), have their
overall probability split between these differ-
ent forms. Consequently, compared to sel-
dom – or never – inflected words (such as ad-
verbs, prepositions, conjunctions), they seem
less frequent than they really are. Second, us-
ing tokens presupposes a theoretical position
according to which learners are not able to
link an inflected form with its lemma. Such
a view seems highly questionable for the ma-
jority of regular forms.
In order to settle this issue, we trained three
language models: one with lemmas (LM1),
another with inflected forms disambiguated
according to their tags (LM2), and a third
one with inflected forms (LM3). The ex-
periment was not very conclusive, since the
models all correlated with the dependent vari-
able to a similar extent, having Pearson’s r
coefficients of −0.58, −0.58, and −0.59 re-
spectively. However, three factors militate in
favour of the lemma model: as well as the-
oretical likelihood, it is the model which is
most sensitive to outliers and most prone to
measurement error. This suggests that, if we
can reduce this error, the lemma model may
prove to be the best predictor of the three.
As a consequence of these considerations, we
decided to compute the difficulty of the text by us-
ing Equation 2 adapted for lemmas and, for com-
putational reasons, the logarithm of the probabili-
ties:
</bodyText>
<equation confidence="0.984259">
n
P(T) = exp( log[p(lemi)]) (3)
i=1
</equation>
<bodyText confidence="0.999655142857143">
The resulting value is still correlated with the
length of the text, so it has to be normalised by
dividing it by N (the number of words in the text).
These operations give in a final value suitable for
the logistic regression model. More information
about the origin and smoothing of the probabilities
is given in Section 6.
</bodyText>
<subsectionHeader confidence="0.98951">
4.2 Measuring the tense difficulty
</subsectionHeader>
<bodyText confidence="0.999933571428571">
Having considered the complexity of a text’s syn-
tactic structures through the traditional factor of
the “mean number of words per sentence”, we de-
cided to also take into account the difficulty of
the conjugation of the verbs in the text. For this
purpose, we created 11 variables, each represent-
ing one tense or class of tenses: conditional, fu-
ture, imperative, imperfect, infinitive, past partici-
ple, present participle, present, simple past, sub-
junctive present and subjunctive imperfect.
The question then arose as to whether it would
be better to treat these variables as binary or con-
tinuous. Theoretical justifications for a binary pa-
rameterisation lie in the fact that a text becomes
more complex for a L2 language learner when
there is a large variety of tenses, especially dif-
ficult ones. The proportion of each tense seems
less significant. For this reason, we opted for bi-
nary variables. The other way of parameterising
the data should nevertheless be tested in further
research.
</bodyText>
<sectionHeader confidence="0.981919" genericHeader="method">
5 The regression models
</sectionHeader>
<bodyText confidence="0.991136612903226">
By the end of the parameterisation stage, each text
of the corpus has been reduced to a vector com-
prising the 14 following predictive variables : the
result of the language model, the average number
of letters per word1, the average number of words
per sentence and the 11 binary variables for tense
complexity.
Each vector also has a label representing the
level of the text, which is the dependent variable
in our classification problem. From a statisti-
cal perspective, this variable may be considered
as a nominal, ordinal, or interval variable, each
level of measurement being linked to a particu-
lar regression technique: multiple linear regres-
sion for interval data; a popular cumulative logit
model called proportional odds for ordinal data;
and multinomial logistic regression for nominal
variables. Therefore, identifying the best scale of
measurement is an important issue for readability.
From a theoretical perspective, viewing the lev-
els of difficulty as an interval scale would imply
that they are ordered and evenly spaced. How-
ever, most FFL teachers would disagree with this
assumption: it is well known that the higher levels
take longer to complete than the earlier ones. So, a
more realistic position is to consider text difficulty
as an ordinal variable (since the CEFR levels are
1Pearson’s r coefficient between the language model and
the average number of letters in the words was −0.68. This
suggests that there is some independent information in the
length of the words that can be used for prediction.
</bodyText>
<page confidence="0.983735">
22
</page>
<bodyText confidence="0.999977166666667">
ordered). The third alternative, treating the levels
as a nominal scale, is not intuitively obvious to a
language teacher, because it suggests that there is
no particular order to the CEFR levels.
From a practical perspective, things are not so
clear. Traditional approaches have usually viewed
difficulty as an interval scale and applied mul-
tiple linear regression. Recent NLP perspective
have either considered difficulty as an ordinal vari-
able (Heilman et al., 2008), making use of logis-
tic regression, or as a nominal one, implementing
classifiers such as the naive Bayes, SVM or deci-
sion tree. Such a variety of practices convinced us
that we should experiment with all three scales of
measurement.
In an exploratory phase, we compared regres-
sion methods and decision tree classifiers on the
same corpus. We found that regression was more
precise and more robust, due to the current lim-
ited size of the corpus. Linear regression was
discarded because it gave poor results during the
test phase. So we retained two logistic regression
models, the PO model and the MLR model, which
are presented in the next section.
</bodyText>
<subsectionHeader confidence="0.977662">
5.1 Proportional odds (PO) model
</subsectionHeader>
<bodyText confidence="0.998756153846154">
Logistic regression is a statistical technique first
developed for binary data. It generally de-
scribes the probability of a 0 or 1 outcome with
an S-shaped logistic function (see Hosmer and
Lemeshow (1989) for details). Adaptation of the
logistic regression for J ordinal classes involves
a model with J − 1 response curves of the same
shape. For a fixed class j, each of these response
functions is comparable to a logistic regression
curve for a binary response with outcomes Y &lt; j
and Y &gt; j (Agresti, 2002), where Y is the depen-
dent variable.
The PO model can be expressed as:
</bodyText>
<equation confidence="0.999723">
logit[P(Y &lt; j  |x)] = αj + β′x (4)
</equation>
<bodyText confidence="0.999748">
In Equation 4, x is the vector containing the inde-
pendent variables, αj is the intercept parameter for
the jth level and β is the vector of regression co-
efficients. From this formula, the particularity of
the PO model can be observed: it has the same set,
β, of parameters for each level. So, the response
functions only differ in their intercepts, αj. This
simplification is only possible under the assump-
tion of ordinality.
Using this cumulative model, when 2 &lt; j &lt; J,
the estimated probability of a text Y belonging to
</bodyText>
<equation confidence="0.887250666666667">
the class j can be computed as:
P(Y = j  |x) = logit[P(Y &lt; j  |x)]
−logit[P(Y &lt; j − 1  |x)] (5)
</equation>
<bodyText confidence="0.892714625">
When j = 1, P(Y = 1  |x) is equal to P(Y &lt; j |
x).
We said above that this model involves a simpli-
fication, based on the proportional odds assump-
tion. This assumption needs to be tested with the
chi-squared form of the score test (Agresti, 2002).
The lower the chi-squared value, the better the PO
model fits the data.
</bodyText>
<subsectionHeader confidence="0.996757">
5.2 Multinomial logistic regression
</subsectionHeader>
<bodyText confidence="0.999896333333333">
Multinomial logistic regression is also called
“baseline category”, because it compares each
class Y with a reference category, often the first
one (Y1), in order to regress to the binary case.
Each pair of classes (Yj, Y1) can then be described
by the ratio (Agresti, 2002, p. 268):
</bodyText>
<equation confidence="0.999791">
P(Y = j  |x)
log P(Y = 1  |x) = αj + βj ′x (6)
</equation>
<bodyText confidence="0.999970833333333">
where the notation is as given above. On the ba-
sis of these J-1 regression equations, it is possible
to compute the probability of a text belonging to
difficulty level j using the values of its features
contained in the vector x. This may be calculated
using the equation (Agresti, 2002, p. 271):
</bodyText>
<equation confidence="0.862695333333333">
exp(αj + βj′x)
1 + EJh=2 exp(αh + βj′x)
(7)
</equation>
<bodyText confidence="0.991217818181818">
Notice that for the baseline category (here, j = 1),
α1 and β1 = 0. Thus, when looking for the proba-
bility of a text belonging to the baseline level, it is
easy to compute the numerator, since exp(0) = 1.
The value of the denominator is the same for each
j.
Heilman et al. (2008) drew attention to the fact
that the MLR model multiplies the number of pa-
rameters by J − 1 compared to the PO model.
Because of this, they recommend using the PO
model.
</bodyText>
<sectionHeader confidence="0.912342" genericHeader="method">
6 Implementation of the models
</sectionHeader>
<bodyText confidence="0.999962333333333">
Having covered the theoretical aspects of our
model, we will now describe some of the partic-
ularities of our implementation.
</bodyText>
<equation confidence="0.996645">
P(Y = j  |x) =
</equation>
<page confidence="0.988777">
23
</page>
<bodyText confidence="0.9892446">
6.1 The language model: probabilities and
smoothing
For our language model, we need a list of French
lemmas with their frequencies of occurrence. Get-
ting robust estimates for a large number of lem-
mas requires a very large corpus and is a time-
consuming process. We used Lexique3, a lexicon
provided by New et al. (2001) and developed from
two corpora: the literary corpus Frantext contain-
ing about 15 million of words; and a corpus of film
subtitles (New et al., 2007), with about 50 million
words. The authors drew up a list of more than
50,000 tagged lemmas, each of which is associ-
ated with two frequency estimates, one from each
corpus.
We decided to use the frequencies from the sub-
title corpus, because we think it gives a more ac-
curate image of everyday language, which is the
language FFL teaching is mainly concerned with.
The frequencies were changed into probabilities,
and smoothed with the Simple Good-Turing al-
gorithm described by Gale and Sampson (1995).
This step is necessary to solve another well-known
problem in language models: the appearance in
a new text of previously unseen lemmas. In this
case, since the logarithm of probabilities is used,
an unseen lemma would result in a infinite value.
In order to prevent this, a smoothing process is
used to shift some of the model’s probability mass
from seen lemmas to unseen ones.
Once we had obtained a good estimate of the
probabilities, we could analyse the texts in the cor-
pus. Each of them was lemmatised and tagged us-
ing the TreeTagger (Schmid, 1994). This NLP tool
allows us to distinguish between homographs that
can represent different levels of difficulty. For in-
stance, the word actif is quite common as an ad-
jective, but the noun is infrequent and is only used
in the business lexicon. This distinction is possible
because Lexique3 provides tagged lemmas.
</bodyText>
<subsectionHeader confidence="0.999154">
6.2 Variable selection
</subsectionHeader>
<bodyText confidence="0.999937166666667">
Having gathered the values for the 14 dependent
variables, it was possible to train the two statis-
tical models.2 However, an essential requirement
prior to training is feature selection. This proce-
dure, described by Hosmer and Lemeshow (1989),
consists of examining models with one, two, three,
</bodyText>
<footnote confidence="0.619699333333333">
2All statistical computations were performed with the
MASS package (Venables and Ripley, 2002) of the R soft-
ware.
</footnote>
<bodyText confidence="0.999831">
etc., variables and comparing them to the full
model according to some specified criteria so as
to select one that is both efficient and parsimo-
nious. For logistic regression, the criterion se-
lected is the AIC (Akaike’s Information Criterion)
of the model. This can be obtained from:
</bodyText>
<equation confidence="0.981222">
AIC = −2log-likelihood + 2k (8)
</equation>
<bodyText confidence="0.999975571428572">
where k is the number of parameters in the model,
and the log-likelihood value is the result of a calcu-
lation detailed by Hosmer and Lemeshow (1989).
We applied the stepwise algorithm to our data,
trying both a backward and a forward procedure.
They converged to a simpler model containing
only 10 variables: the value obtained from our lan-
guage model, the number of letters per word, the
number of words per sentence, the past participle,
the present participle, and the imperfect, infinitive,
conditional, future and present subjunctive tenses.
Presumable the imperative and present tenses are
so common that they do not have much discrim-
inative power. On the other hand, the imperfect
subjunctive is so unusual that it is not useful for a
classification task. However, the non-appearance
of the simple past is surprising, since it is a nar-
rative tense which is not usually introduced until
an advanced stage in the learning of French. This
phenomenon deserves further investigation in the
future.
</bodyText>
<sectionHeader confidence="0.924505" genericHeader="method">
7 First results
</sectionHeader>
<bodyText confidence="0.999992545454546">
To the best of our knowledge, no one has pre-
viously applied NLP technologies to the specific
issue of the readability of texts for FFL learn-
ers. So, any comparisons with previous studies are
somewhat flawed by the fact that neither the target
population nor the scale of difficulty is the same.
However, our results can be roughly compared to
some of the numerous studies on L1 English read-
ability presented in Section 2. Before making this
comparison, we will analyse the predictive ability
of the two models.
</bodyText>
<subsectionHeader confidence="0.999399">
7.1 Models evaluation
</subsectionHeader>
<bodyText confidence="0.999973857142857">
The evaluation measures most commonly em-
ployed in the literature are Pearson’s product-
moment correlation coefficient, prediction accu-
racy as defined by Tan et al. (2005), and adjacent
accuracy. Adjacent accuracy is defined by Heil-
man et al. (2008) as “the proportion of predictions
that were within one level of the human-assigned
</bodyText>
<page confidence="0.998208">
24
</page>
<table confidence="0.999300666666667">
Measure PO model MLR model
Results on training folds
Correl. 0.786 0.777
Exact Acc. 32.5% 38%
Adj. Acc. 70% 71.3%
Results on test folds
Correl. 0.783 0.772
Exact Acc. 32.4% 38%
Adj. Acc. 70% 71.2%
</table>
<tableCaption confidence="0.907760666666667">
Table 1: Mean Pearson’s r coefficient, exact and
adjacent accuracies for both models with the ten-
fold cross-validation evaluation.
</tableCaption>
<bodyText confidence="0.999777877192983">
label for the given text”. They defended this mea-
sure by arguing that even human-assigned reading
levels are not always consistent. Nevertheless, it
should not be forgotten that it can give optimistic
values when the number of classes is small.
Exploratory analysis of the corpus highlighted
the importance of having a similar number of texts
per class. This requirement made it impossible
to use all the texts from the corpus. Some 465
texts were selected, distributed across the 9 levels
in such a way that each level contained about 50
texts. Within each class, an automatic procedure
discarded outliers located more than 3Q from the
mean, leaving 440 texts. Both models were trained
on these texts.
The results on the training corpus were promis-
ing, but might be biased. So, we turned to a
ten-fold cross-validation process which guarantees
more reliable values for the three evaluation mea-
sures we had chosen, as well as a better insight
into the generalisability of the two models. The
resulting evaluation measures for training and test
folds are shown in Table 1. The similarity between
them clearly shows that, with 440 observations,
both the models were quite robust. On this corpus,
multinomial logistic regression was significantly
more accurate (with 38% of texts correctly classi-
fied against 32.4% for the PO model), while Pear-
son’s R was slightly higher for the PO model.
These results suggest that the exact accuracy
may be a better indicator of performance than the
correlation coefficient. However they conflict with
Heilman et al.’s (2008) conclusion that the PO
model performed better than the MLR one. This
discrepancy might arise because the PO model
was less accurate for exact predictions, but better
when the adjacent accuracy by level was taken into
account. However, the data in Table 2 do not sup-
port this hypothesis; rather they confirm the supe-
riority of the MLR model when adjacent accuracy
is considered. In fact, PO model’s lower perfor-
mance seems to be due to a lack of fit to the data,
as revealed by the result of the score test for the
proportional-odds assumption. This yielded a p-
value below 0.0001, clearly showing that the PO
model was not a good fit to the corpus.
There remains one last issue to be discussed be-
fore comparing our results to those of other stud-
ies: the empirical evidence for tense being a good
predictor of reading difficulty. We selected tenses
because of our experience as FLE teacher rather
than on theoretical or empirical grounds. How-
ever we found that exact accuracy decreased by
10% when the tense variables were omitted from
the models. Further analysis showed that the tense
contributed significantly to the adjacent accuracy
of classifying the C1 and C2 texts.
</bodyText>
<subsectionHeader confidence="0.999804">
7.2 Comparison with other studies
</subsectionHeader>
<bodyText confidence="0.999913">
As stated above, it is not easy to compare our
results with those of previous studies, since the
scale, population of interest and often the lan-
guage are different. Furthermore, up till now, we
have not been able to run the classical formu-
lae for French (such as de Landsheere (1963) or
Henry (1975)) on our corpus. So we are limited to
comparing our evaluation measures with those in
the published literature.
With multinomial logistic regression, we ob-
tained a mean adjacent accuracy of 71% for 9
classes. This result seems quite good compared
to similar research on L1 English by Heilman et
al. (2008). Using more complex syntactic fea-
tures, they obtained an adjacent accuracy of 52%
with a PO model, and 45% with a MLR model.
However, they worked with 12 levels, which may
explain their lower percentage.
For French, Collins-Thompson and Callan
(2005) reported a Pearson’s R coefficient of 0.64
for a 5-classes naive Bayes classifier while we ob-
tained 0.77 for 9 levels with MLR. This differ-
ence might be explained by the tagging or the use
of better-estimated probabilities for the language
model. Further research on this point to determine
the specificities of an efficient approach to French
readability appears very promising.
</bodyText>
<page confidence="0.996289">
25
</page>
<table confidence="0.994139333333333">
Level A1 A1+ A2 A2+ B1 B1+ B2 C1 C2 Mean
PO model 91% 91% 67% 68% 53% 55% 56% 86% 68% 70%
MLR model 93% 90% 69% 51% 59% 56% 64% 88% 73% 71%
</table>
<tableCaption confidence="0.998483">
Table 2: Mean adjacent accuracy per level for PO model and MLR model (on the test folds).
</tableCaption>
<sectionHeader confidence="0.892269" genericHeader="discussions">
8 Discussion and future research
</sectionHeader>
<bodyText confidence="0.99998193939394">
This paper has proposed the first readability “for-
mula” for French as a foreign language using NLP
and statistical models. It takes into account some
particularities of French such as its inflected na-
ture. A new scale to assess FFL texts within the
CECR framework, and a new criteria for the cor-
pus involving the use of textbooks, have also been
proposed. The two logistic models applied to a
440-text corpus gave results consistent with the lit-
erature. They also showed the superiority of the
MLR model over the PO model. Since Heilman
et al. (2008) found the opposite, and the intuitive
view is that levels should be described by an ordi-
nal scale of measurement, this issue clearly needs
further investigation.
This research is still in progress, and further
analyses are planned. The predictive capacity of
some other lexical and grammatical features will
be explored. At the lexical level, statistical lan-
guage models seems to be best, and tagging the
texts to work with lemmas turned out to be effi-
cient for French, although it has not been shown
to be superior to disambiguated inflected forms.
Moreover, due to their higher sensibility to con-
text, smoothed n-grams might represent an alter-
native to lemmas.
Once the best unit has been selected, some
other issues remain: it is not clear whether a
model using the probabilities of this unit in the
whole language or probabilities per level (Collins-
Thompson and Callan, 2005) would be more ef-
ficient. We also wonder whether the L1 frequen-
cies of words are similar to those in L2 ? FFL
textbooks use a controlled vocabulary, linked to
specific situational tasks, which suggests that it is
highly possible that the frequencies of words in
FFL differ from those in mother-tongue French.
Grammatical features have been taken into ac-
count through simple parameterisation. More
complex measures (such as the presence of some
syntactic structures (Heilman et al., 2007) or the
characteristics of a syntactic-parsing tree) have
been explored in the literature. We hope that in-
cluding such factors may result in improved accu-
racy for our model. However, these techniques are
probably dependent on the quality of the parser’s
results. Parsers for French are less accurate than
those for English, which may generate some noise
in the analysis.
Finally, we intend to explore the performance
of other classification techniques. Logistic regres-
sion was the most efficient of the statistical mod-
els we tested, but as our corpus grows, more and
more data is becoming available, and data min-
ing approaches may become applicable to the text-
categorization problem for FFL readability. Sup-
port vector machines have already been shown to
be useful for readability purposes (Schwarm and
Ostendorf, 2005). We also want to try aggregating
approaches such as boosting, bagging, and random
forests (Breiman, 2001), since they claim to be ef-
fective when the sample is not perfectly represen-
tative of the population (which could be true for
our data). These analyses would aim to illuminate
some of the assets and flaws of each of the statis-
tical models considered.
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999787375">
Thomas L. Franc¸ois is supported by the Bel-
gian Fund for Scientific Research (FNRS), as is
the research programme from which this material
comes.
I would like to thank my directors, Prof.
C´edrick Fairon and Prof. Anne-Catherine Simon,
my colleagues, Laure Cuignet and the anonymous
reviewers for their valuable comments.
</bodyText>
<sectionHeader confidence="0.999007" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999586222222222">
Alan Agresti. 2002. Categorical Data Analysis. 2nd
edition. Wiley-Interscience, New York.
J. Boss´e-Andrieu. 1993. La question de la lisi-
bilit´e dans les pays anglophones et les pays fran-
cophones. Technostyle, Association canadienne des
professeurs de r´edaction technique et scientifique,
11(2):73–85.
L. Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5–32.
</reference>
<page confidence="0.948798">
26
</page>
<reference confidence="0.99986925">
M. Brysbaert, M. Lange, and I. Van Wijnendaele.
2000. The effects of age-of-acquisition and
frequency-of-occurrence in visual word recognition:
Further evidence from the Dutch language. Euro-
pean Journal of Cognitive Psychology, 12(1):65–85.
J.S. Chall and E. Dale. 1995. Readability Revisited:
The New Dale-Chall Readability Formula. Brook-
line Books, Cambridge.
K. Collins-Thompson and J. Callan. 2005. Predict-
ing reading difficulty with statistical language mod-
els. Journal of the American Societyfor Information
Science and Technology, 56(13):1448–1462.
A. Conquet. 1971. La lisibilit´e. Assembl´ee Perma-
nente des CCI de Paris, Paris.
C.M. Cornaire. 1988. La lisibilit´e : essai d’application
de la formule courte d’Henry au franc¸ais langue
´etrang`ere. Canadian Modern Language Review,
44(2):261–273.
Council of Europe and Education Committee and
Council for Cultural Co-operation. 2001. Common
European Framework of Reference for Languages:
Learning, Teaching, Assessment. Press Syndicate of
the University of Cambridge.
G. De Landsheere. 1963. Pour une application des
tests de lisibilit´e de Flesch a` la langue franc¸aise. Le
Travail Humain, 26:141–154.
R. Flesch. 1948. A new readability yardstick. Journal
ofApplied Psychology, 32(3):221–233.
W.A. Gale and G. Sampson. 1995. Good-Turing fre-
quency estimation without tears. Journal of Quanti-
tative Linguistics, 2(3):217–237.
S. Gerhand and C. Barry. 1998. Word frequency
effects in oral reading are not merely age-of-
acquisition effects in disguise. Journal of Experi-
mental Psychology. Learning, Memory, and Cogni-
tion, 24(2):267–283.
M. Heilman, K. Collins-Thompson, J. Callan, and
M. Eskenazi. 2007. Combining lexical and gram-
matical features to improve readability measures for
first and second language texts. In Proceedings of
NAACL HLT, pages 460–467.
M. Heilman, K. Collins-Thompson, and M. Eskenazi.
2008. An analysis of statistical models and fea-
tures for reading difficulty prediction. Association
for Computational Linguistics, The 3rd Workshop
on Innovative Use of NLP for Building Educational
Applications:1–8.
G. Henry. 1975. Comment mesurer la lisibilit´e. Labor.
D.W. Hosmer and S. Lemeshow. 1989. Applied Logis-
tic Regression. Wiley, New York.
D.H. Howes and R.L. Solomon. 1951. Visual duration
threshold as a function of word probability. Journal
of Experimental Psychology, 41(40):1–4.
L. Kandel and A. Moles. 1958. Application de l’indice
de Flesch a` la langue franc¸aise. Cahiers ´Etudes de
Radio-T´el´evision, 19:253–274.
S. Kemper. 1983. Measuring the inference load
of a text. Journal of Educational Psychology,
75(3):391–401.
J. Kincaid, R.P. Fishburne, R. Rodgers, and
B. Chissom. 1975. Derivation of new read-
ability formulas for navy enlisted personnel.
Research Branch Report, 85.
W. Kintsch and D. Vipond. 1979. Reading compre-
hension and readability in educational practice and
psychological theory. Perspectives on Memory Re-
search, pages 329–366.
B.A. Lively and S.L. Pressey. 1923. A method for
measuring the vocabulary burden of textbooks. Ed-
ucational Administration and Supervision, 9:389–
398.
J. Mesnager. 1989. Lisibilit´e des textes pour en-
fants: un nouvel outil? Communication et Lan-
gages, 79:18–38.
B. New, C. Pallier, L. Ferrand, and R. Matos. 2001.
Une base de donn´ees lexicales du franc¸ais con-
temporain sur internet: LEXIQUE. LAnn´ee Psy-
chologique, 101:447–462.
B. New, M. Brysbaert, J. Veronis, and C. Pallier. 2007.
The use of film subtitles to estimate word frequen-
cies. Applied Psycholinguistics, 28(04):661–677.
F. Richaudeau. 1979. Une nouvelle formule de lisi-
bilit´e. Communication et Langages, 44:5–26.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings ofInternational
Conference on New Methods in Language Process-
ing, volume 12. Manchester, UK.
S.E. Schwarm and M. Ostendorf. 2005. Reading level
assessment using support vector machines and sta-
tistical language models. Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 523–530.
P.-N. Tan, M. Steinbach, and V. Kumar. 2005. Intro-
duction to Data Mining. Addison-Wesley, Boston.
S. Uitdenbogerd. 2005. Readability of French as a
foreign language and its uses. In Proceedings of the
Australian Document Computing Symposium, pages
19–25.
W.N. Venables and B.D. Ripley. 2002. Modern Ap-
plied Statistics with S. Springer, New York.
</reference>
<page confidence="0.998809">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.189496">
<title confidence="0.88615425">Combining a Statistical Language Model with Logistic Regression to Predict the Lexical and Syntactic Difficulty of Texts for FFL L. Aspirant FNRS</title>
<affiliation confidence="0.709889">CENTAL (Center for Natural Language Processing) Universit´e catholique de Louvain</affiliation>
<address confidence="0.997398">1348 Louvain-la-Neuve, Belgium</address>
<email confidence="0.985724">thomas.francois@uclouvain.be</email>
<abstract confidence="0.997156">Reading is known to be an essential task in language learning, but finding the appropriate text for every learner is far from easy. In this context, automatic procedures can support the teacher’s work. Some tools exist for English, but at present there are none for French as a foreign language (FFL). In this paper, we present an original approach to assessing the readability of FFL texts using NLP techniques and extracts from FFL textbooks as our corpus. Two logistic regression models based on lexical and grammatical features are explored and give quite good predictions on new texts. The results shows a slight superiority for multinomial logistic regression over the proportional odds model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan Agresti</author>
</authors>
<title>Categorical Data Analysis. 2nd edition. Wiley-Interscience,</title>
<date>2002</date>
<location>New York.</location>
<contexts>
<context position="18883" citStr="Agresti, 2002" startWordPosition="3105" endWordPosition="3106">he PO model and the MLR model, which are presented in the next section. 5.1 Proportional odds (PO) model Logistic regression is a statistical technique first developed for binary data. It generally describes the probability of a 0 or 1 outcome with an S-shaped logistic function (see Hosmer and Lemeshow (1989) for details). Adaptation of the logistic regression for J ordinal classes involves a model with J − 1 response curves of the same shape. For a fixed class j, each of these response functions is comparable to a logistic regression curve for a binary response with outcomes Y &lt; j and Y &gt; j (Agresti, 2002), where Y is the dependent variable. The PO model can be expressed as: logit[P(Y &lt; j |x)] = αj + β′x (4) In Equation 4, x is the vector containing the independent variables, αj is the intercept parameter for the jth level and β is the vector of regression coefficients. From this formula, the particularity of the PO model can be observed: it has the same set, β, of parameters for each level. So, the response functions only differ in their intercepts, αj. This simplification is only possible under the assumption of ordinality. Using this cumulative model, when 2 &lt; j &lt; J, the estimated probabilit</context>
<context position="20230" citStr="Agresti, 2002" startWordPosition="3356" endWordPosition="3357">Y = 1 |x) is equal to P(Y &lt; j | x). We said above that this model involves a simplification, based on the proportional odds assumption. This assumption needs to be tested with the chi-squared form of the score test (Agresti, 2002). The lower the chi-squared value, the better the PO model fits the data. 5.2 Multinomial logistic regression Multinomial logistic regression is also called “baseline category”, because it compares each class Y with a reference category, often the first one (Y1), in order to regress to the binary case. Each pair of classes (Yj, Y1) can then be described by the ratio (Agresti, 2002, p. 268): P(Y = j |x) log P(Y = 1 |x) = αj + βj ′x (6) where the notation is as given above. On the basis of these J-1 regression equations, it is possible to compute the probability of a text belonging to difficulty level j using the values of its features contained in the vector x. This may be calculated using the equation (Agresti, 2002, p. 271): exp(αj + βj′x) 1 + EJh=2 exp(αh + βj′x) (7) Notice that for the baseline category (here, j = 1), α1 and β1 = 0. Thus, when looking for the probability of a text belonging to the baseline level, it is easy to compute the numerator, since exp(0) = 1</context>
</contexts>
<marker>Agresti, 2002</marker>
<rawString>Alan Agresti. 2002. Categorical Data Analysis. 2nd edition. Wiley-Interscience, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Boss´e-Andrieu</author>
</authors>
<title>La question de la lisibilit´e dans les pays anglophones et les pays francophones. Technostyle, Association canadienne des professeurs de r´edaction technique et scientifique,</title>
<date>1993</date>
<pages>11--2</pages>
<marker>Boss´e-Andrieu, 1993</marker>
<rawString>J. Boss´e-Andrieu. 1993. La question de la lisibilit´e dans les pays anglophones et les pays francophones. Technostyle, Association canadienne des professeurs de r´edaction technique et scientifique, 11(2):73–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="33152" citStr="Breiman, 2001" startWordPosition="5551" endWordPosition="5552">for English, which may generate some noise in the analysis. Finally, we intend to explore the performance of other classification techniques. Logistic regression was the most efficient of the statistical models we tested, but as our corpus grows, more and more data is becoming available, and data mining approaches may become applicable to the textcategorization problem for FFL readability. Support vector machines have already been shown to be useful for readability purposes (Schwarm and Ostendorf, 2005). We also want to try aggregating approaches such as boosting, bagging, and random forests (Breiman, 2001), since they claim to be effective when the sample is not perfectly representative of the population (which could be true for our data). These analyses would aim to illuminate some of the assets and flaws of each of the statistical models considered. Acknowledgments Thomas L. Franc¸ois is supported by the Belgian Fund for Scientific Research (FNRS), as is the research programme from which this material comes. I would like to thank my directors, Prof. C´edrick Fairon and Prof. Anne-Catherine Simon, my colleagues, Laure Cuignet and the anonymous reviewers for their valuable comments. References</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>L. Breiman. 2001. Random forests. Machine Learning, 45(1):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brysbaert</author>
<author>M Lange</author>
<author>I Van Wijnendaele</author>
</authors>
<title>The effects of age-of-acquisition and frequency-of-occurrence in visual word recognition: Further evidence from the Dutch language.</title>
<date>2000</date>
<journal>European Journal of Cognitive Psychology,</journal>
<volume>12</volume>
<issue>1</issue>
<marker>Brysbaert, Lange, Van Wijnendaele, 2000</marker>
<rawString>M. Brysbaert, M. Lange, and I. Van Wijnendaele. 2000. The effects of age-of-acquisition and frequency-of-occurrence in visual word recognition: Further evidence from the Dutch language. European Journal of Cognitive Psychology, 12(1):65–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Chall</author>
<author>E Dale</author>
</authors>
<title>Readability Revisited: The New Dale-Chall Readability Formula. Brookline Books,</title>
<date>1995</date>
<location>Cambridge.</location>
<contexts>
<context position="3227" citStr="Chall and Dale, 1995" startWordPosition="517" endWordPosition="520">ing a programme for future work and improvement of the results. 2 Related research The measurement of the reading difficulty of texts has been a major concern in the English-speaking literature since the 1920s and the first formula developed by Lively and Pressey (1923). The field of readability has since produced many formulae based on simple lexical and syntactic measures such as the average number of syllables per word, the average length of sentences in a piece of text (Flesch, 1948; Kincaid et al., 1975), or the percentage of words not on a list combined with the average sentence length (Chall and Dale, 1995). French-speaking researchers discovered the field of readability in 1956 through the work of Andr´e Conquet, La lisibilit´e (1971), and the first two formulae for French were adapted from Flesch (1948) by Kandel and Moles (1958) and de Landsheere (1963). Both of these researchers stayed quite close to the Flesch formula, and in so doing they failed to take into account some specificities of the French language. Henry (1975) was the first to introduce specific formulae for French. He used a larger set of variables to design three formulae : a complete, an automatic and a short one, each of whi</context>
</contexts>
<marker>Chall, Dale, 1995</marker>
<rawString>J.S. Chall and E. Dale. 1995. Readability Revisited: The New Dale-Chall Readability Formula. Brookline Books, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Collins-Thompson</author>
<author>J Callan</author>
</authors>
<title>Predicting reading difficulty with statistical language models.</title>
<date>2005</date>
<journal>Journal of the American Societyfor Information Science and Technology,</journal>
<volume>56</volume>
<issue>13</issue>
<contexts>
<context position="12338" citStr="Collins-Thompson and Callan (2005)" startWordPosition="2005" endWordPosition="2008">t al., 2000) suggests that there is a strong relationship between the frequency of words and the speed with which they are recognised. We therefore opted to model the lexical difficulty for reading as the global probability of a text T (with N tokens) occurring: P(T) = P(t1)P(t2 |t1) ··· P(tn |t1, t2, ... , tn−1) (1) This equation raises two issues : 1. Estimating the conditional probabilities. It is well-known that it is impossible to train such a model on a corpus, even the largest one, because some sequences in this equation are unlikely to be encountered more than once. However, following Collins-Thompson and Callan (2005), we found that a simple smoothed unigram model could give good results for readability. Thus, we assumed that the global probability of a text T could be reduced to: n P(T) = p(ti) (2) i=1 where p(ti) is the probability of meeting the token ti in French; and n is the number of tokens in a text. 2. Deciding what is the best linguistic unit to consider. The equations introduced above use 21 tokens, as is traditional in readability formulae, but the inflected nature of French suggests that lemmas may be a better alternative. Using tokens means that words taking numerous inflected forms (such as </context>
<context position="29664" citStr="Collins-Thompson and Callan (2005)" startWordPosition="4962" endWordPosition="4965">he classical formulae for French (such as de Landsheere (1963) or Henry (1975)) on our corpus. So we are limited to comparing our evaluation measures with those in the published literature. With multinomial logistic regression, we obtained a mean adjacent accuracy of 71% for 9 classes. This result seems quite good compared to similar research on L1 English by Heilman et al. (2008). Using more complex syntactic features, they obtained an adjacent accuracy of 52% with a PO model, and 45% with a MLR model. However, they worked with 12 levels, which may explain their lower percentage. For French, Collins-Thompson and Callan (2005) reported a Pearson’s R coefficient of 0.64 for a 5-classes naive Bayes classifier while we obtained 0.77 for 9 levels with MLR. This difference might be explained by the tagging or the use of better-estimated probabilities for the language model. Further research on this point to determine the specificities of an efficient approach to French readability appears very promising. 25 Level A1 A1+ A2 A2+ B1 B1+ B2 C1 C2 Mean PO model 91% 91% 67% 68% 53% 55% 56% 86% 68% 70% MLR model 93% 90% 69% 51% 59% 56% 64% 88% 73% 71% Table 2: Mean adjacent accuracy per level for PO model and MLR model (on the</context>
</contexts>
<marker>Collins-Thompson, Callan, 2005</marker>
<rawString>K. Collins-Thompson and J. Callan. 2005. Predicting reading difficulty with statistical language models. Journal of the American Societyfor Information Science and Technology, 56(13):1448–1462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Conquet</author>
</authors>
<title>La lisibilit´e. Assembl´ee Permanente des CCI de Paris,</title>
<date>1971</date>
<location>Paris.</location>
<marker>Conquet, 1971</marker>
<rawString>A. Conquet. 1971. La lisibilit´e. Assembl´ee Permanente des CCI de Paris, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Cornaire</author>
</authors>
<title>La lisibilit´e : essai d’application de la formule courte d’Henry au franc¸ais langue ´etrang`ere.</title>
<date>1988</date>
<journal>Canadian Modern Language Review,</journal>
<volume>44</volume>
<issue>2</issue>
<contexts>
<context position="5145" citStr="Cornaire (1988)" startWordPosition="836" endWordPosition="837">lysed by Boss´e-Andrieu (1993) (who basically argues that the idea of measuring text difficulty objectively seems far too pragmatic for the French spirit). It follows that there is little current research in this field: in Belgium, the Flesch formula is still used to assess the readability of articles in journalism studies. This example also shows that the French-specific formulae are not much used, probably because of their complexity (Boss´e-Andrieu, 1993). Of course, if there is little work on French readability, there is even less on French as a foreign language. We only know the study of Cornaire (1988), which tested the adaptation of Henry’s short formula to French as a foreign language, and that of Uitdenbogerd (2005), which developed a new measure for English-speaking learners of French, stressing the importance of cognates when developing a new formula for a related language. Therefore, we had to draw our inspiration from the English-speaking world, which has recently experienced a revival of interest in research on readability. Taking advantage of the increasing power of computers and the development of NLP techniques, researchers have been able to experiment with more complex variables</context>
</contexts>
<marker>Cornaire, 1988</marker>
<rawString>C.M. Cornaire. 1988. La lisibilit´e : essai d’application de la formule courte d’Henry au franc¸ais langue ´etrang`ere. Canadian Modern Language Review, 44(2):261–273.</rawString>
</citation>
<citation valid="true">
<title>Council of Europe and Education Committee and Council for Cultural Co-operation.</title>
<date>2001</date>
<booktitle>Common European Framework of Reference for Languages: Learning, Teaching, Assessment. Press Syndicate of the</booktitle>
<institution>University of Cambridge.</institution>
<contexts>
<context position="21567" citStr="(2001)" startWordPosition="3613" endWordPosition="3613">es the number of parameters by J − 1 compared to the PO model. Because of this, they recommend using the PO model. 6 Implementation of the models Having covered the theoretical aspects of our model, we will now describe some of the particularities of our implementation. P(Y = j |x) = 23 6.1 The language model: probabilities and smoothing For our language model, we need a list of French lemmas with their frequencies of occurrence. Getting robust estimates for a large number of lemmas requires a very large corpus and is a timeconsuming process. We used Lexique3, a lexicon provided by New et al. (2001) and developed from two corpora: the literary corpus Frantext containing about 15 million of words; and a corpus of film subtitles (New et al., 2007), with about 50 million words. The authors drew up a list of more than 50,000 tagged lemmas, each of which is associated with two frequency estimates, one from each corpus. We decided to use the frequencies from the subtitle corpus, because we think it gives a more accurate image of everyday language, which is the language FFL teaching is mainly concerned with. The frequencies were changed into probabilities, and smoothed with the Simple Good-Turi</context>
</contexts>
<marker>2001</marker>
<rawString>Council of Europe and Education Committee and Council for Cultural Co-operation. 2001. Common European Framework of Reference for Languages: Learning, Teaching, Assessment. Press Syndicate of the University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G De Landsheere</author>
</authors>
<title>Pour une application des tests de lisibilit´e de Flesch a` la langue franc¸aise. Le Travail Humain,</title>
<date>1963</date>
<pages>26--141</pages>
<marker>De Landsheere, 1963</marker>
<rawString>G. De Landsheere. 1963. Pour une application des tests de lisibilit´e de Flesch a` la langue franc¸aise. Le Travail Humain, 26:141–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Flesch</author>
</authors>
<title>A new readability yardstick.</title>
<date>1948</date>
<journal>Journal ofApplied Psychology,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="3097" citStr="Flesch, 1948" startWordPosition="495" endWordPosition="496">ion 7 presents the first results of our models. Finally, Section 8 sums up the contribution of this article before providing a programme for future work and improvement of the results. 2 Related research The measurement of the reading difficulty of texts has been a major concern in the English-speaking literature since the 1920s and the first formula developed by Lively and Pressey (1923). The field of readability has since produced many formulae based on simple lexical and syntactic measures such as the average number of syllables per word, the average length of sentences in a piece of text (Flesch, 1948; Kincaid et al., 1975), or the percentage of words not on a list combined with the average sentence length (Chall and Dale, 1995). French-speaking researchers discovered the field of readability in 1956 through the work of Andr´e Conquet, La lisibilit´e (1971), and the first two formulae for French were adapted from Flesch (1948) by Kandel and Moles (1958) and de Landsheere (1963). Both of these researchers stayed quite close to the Flesch formula, and in so doing they failed to take into account some specificities of the French language. Henry (1975) was the first to introduce specific formu</context>
</contexts>
<marker>Flesch, 1948</marker>
<rawString>R. Flesch. 1948. A new readability yardstick. Journal ofApplied Psychology, 32(3):221–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>G Sampson</author>
</authors>
<title>Good-Turing frequency estimation without tears.</title>
<date>1995</date>
<journal>Journal of Quantitative Linguistics,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="22216" citStr="Gale and Sampson (1995)" startWordPosition="3721" endWordPosition="3724">rpora: the literary corpus Frantext containing about 15 million of words; and a corpus of film subtitles (New et al., 2007), with about 50 million words. The authors drew up a list of more than 50,000 tagged lemmas, each of which is associated with two frequency estimates, one from each corpus. We decided to use the frequencies from the subtitle corpus, because we think it gives a more accurate image of everyday language, which is the language FFL teaching is mainly concerned with. The frequencies were changed into probabilities, and smoothed with the Simple Good-Turing algorithm described by Gale and Sampson (1995). This step is necessary to solve another well-known problem in language models: the appearance in a new text of previously unseen lemmas. In this case, since the logarithm of probabilities is used, an unseen lemma would result in a infinite value. In order to prevent this, a smoothing process is used to shift some of the model’s probability mass from seen lemmas to unseen ones. Once we had obtained a good estimate of the probabilities, we could analyse the texts in the corpus. Each of them was lemmatised and tagged using the TreeTagger (Schmid, 1994). This NLP tool allows us to distinguish be</context>
</contexts>
<marker>Gale, Sampson, 1995</marker>
<rawString>W.A. Gale and G. Sampson. 1995. Good-Turing frequency estimation without tears. Journal of Quantitative Linguistics, 2(3):217–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gerhand</author>
<author>C Barry</author>
</authors>
<title>Word frequency effects in oral reading are not merely age-ofacquisition effects in disguise.</title>
<date>1998</date>
<journal>Journal of Experimental Psychology. Learning, Memory, and Cognition,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="11691" citStr="Gerhand and Barry, 1998" startWordPosition="1892" endWordPosition="1895">d the average sentence length, and found that, on our corpus, only the word length and sentence length correlated significantly with difficulty. Then, we add two NLP-oriented features, as described below: a statistical language model and a measure of tense difficulty. 4.1 The language model The lexical difficulty of a text is quite an elaborate phenomenon to parameterise. The logistic regression models we used in this study require us to reduce this complex reality to just one number, the challenge being to achieve the most informative number. Some psychological work (Howes and Solomon, 1951; Gerhand and Barry, 1998; Brysbaert et al., 2000) suggests that there is a strong relationship between the frequency of words and the speed with which they are recognised. We therefore opted to model the lexical difficulty for reading as the global probability of a text T (with N tokens) occurring: P(T) = P(t1)P(t2 |t1) ··· P(tn |t1, t2, ... , tn−1) (1) This equation raises two issues : 1. Estimating the conditional probabilities. It is well-known that it is impossible to train such a model on a corpus, even the largest one, because some sequences in this equation are unlikely to be encountered more than once. Howeve</context>
</contexts>
<marker>Gerhand, Barry, 1998</marker>
<rawString>S. Gerhand and C. Barry. 1998. Word frequency effects in oral reading are not merely age-ofacquisition effects in disguise. Journal of Experimental Psychology. Learning, Memory, and Cognition, 24(2):267–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>K Collins-Thompson</author>
<author>J Callan</author>
<author>M Eskenazi</author>
</authors>
<title>Combining lexical and grammatical features to improve readability measures for first and second language texts.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT,</booktitle>
<pages>460--467</pages>
<contexts>
<context position="6262" citStr="Heilman et al. (2007)" startWordPosition="1006" endWordPosition="1009"> the development of NLP techniques, researchers have been able to experiment with more complex variables. CollinsThompson et al. (2005) presented a variation of a multinomial naive Bayesian classifier they called the “Smoothed Unigram” model. We retained from their work the use of language models instead of word lists to measure lexical complexity. Schwarm and Ostendorf (2005) developed a SVM categoriser combining a classifier based on trigram language models (one for each level of difficulty), some parsing features such as average tree height, and variables traditionally used in readability. Heilman et al. (2007) extended the “Smoothed Unigram” model by the recognition of syntactic structures, in order to assess L2 English texts. Later, they improved the combination of their various lexical and grammatical features using regression methods (Heilman et al., 2008). We also found regression methods to be the most efficient of the statistical models with which we experimented. In this article, we consider some ways to adapt these various ideas to the specific case of FFL readability. 3 Corpus description In the development of a new readability formula, the first step is to collect a corpus labelled by rea</context>
<context position="32228" citStr="Heilman et al., 2007" startWordPosition="5402" endWordPosition="5405">ether a model using the probabilities of this unit in the whole language or probabilities per level (CollinsThompson and Callan, 2005) would be more efficient. We also wonder whether the L1 frequencies of words are similar to those in L2 ? FFL textbooks use a controlled vocabulary, linked to specific situational tasks, which suggests that it is highly possible that the frequencies of words in FFL differ from those in mother-tongue French. Grammatical features have been taken into account through simple parameterisation. More complex measures (such as the presence of some syntactic structures (Heilman et al., 2007) or the characteristics of a syntactic-parsing tree) have been explored in the literature. We hope that including such factors may result in improved accuracy for our model. However, these techniques are probably dependent on the quality of the parser’s results. Parsers for French are less accurate than those for English, which may generate some noise in the analysis. Finally, we intend to explore the performance of other classification techniques. Logistic regression was the most efficient of the statistical models we tested, but as our corpus grows, more and more data is becoming available, </context>
</contexts>
<marker>Heilman, Collins-Thompson, Callan, Eskenazi, 2007</marker>
<rawString>M. Heilman, K. Collins-Thompson, J. Callan, and M. Eskenazi. 2007. Combining lexical and grammatical features to improve readability measures for first and second language texts. In Proceedings of NAACL HLT, pages 460–467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>K Collins-Thompson</author>
<author>M Eskenazi</author>
</authors>
<title>An analysis of statistical models and features for reading difficulty prediction.</title>
<date>2008</date>
<booktitle>Association for Computational Linguistics, The 3rd Workshop on Innovative Use of NLP for Building Educational Applications:1–8.</booktitle>
<contexts>
<context position="6516" citStr="Heilman et al., 2008" startWordPosition="1044" endWordPosition="1047"> from their work the use of language models instead of word lists to measure lexical complexity. Schwarm and Ostendorf (2005) developed a SVM categoriser combining a classifier based on trigram language models (one for each level of difficulty), some parsing features such as average tree height, and variables traditionally used in readability. Heilman et al. (2007) extended the “Smoothed Unigram” model by the recognition of syntactic structures, in order to assess L2 English texts. Later, they improved the combination of their various lexical and grammatical features using regression methods (Heilman et al., 2008). We also found regression methods to be the most efficient of the statistical models with which we experimented. In this article, we consider some ways to adapt these various ideas to the specific case of FFL readability. 3 Corpus description In the development of a new readability formula, the first step is to collect a corpus labelled by reading-difficulty level, a task that implies agreement on the difficulty scale. In the US, a common choice is the 12 American grade levels corresponding to primary and secondary school. However, this scale is less relevant for FFL education in Europe. So, </context>
<context position="17690" citStr="Heilman et al., 2008" startWordPosition="2898" endWordPosition="2901">letters in the words was −0.68. This suggests that there is some independent information in the length of the words that can be used for prediction. 22 ordered). The third alternative, treating the levels as a nominal scale, is not intuitively obvious to a language teacher, because it suggests that there is no particular order to the CEFR levels. From a practical perspective, things are not so clear. Traditional approaches have usually viewed difficulty as an interval scale and applied multiple linear regression. Recent NLP perspective have either considered difficulty as an ordinal variable (Heilman et al., 2008), making use of logistic regression, or as a nominal one, implementing classifiers such as the naive Bayes, SVM or decision tree. Such a variety of practices convinced us that we should experiment with all three scales of measurement. In an exploratory phase, we compared regression methods and decision tree classifiers on the same corpus. We found that regression was more precise and more robust, due to the current limited size of the corpus. Linear regression was discarded because it gave poor results during the test phase. So we retained two logistic regression models, the PO model and the M</context>
<context position="20906" citStr="Heilman et al. (2008)" startWordPosition="3490" endWordPosition="3493"> where the notation is as given above. On the basis of these J-1 regression equations, it is possible to compute the probability of a text belonging to difficulty level j using the values of its features contained in the vector x. This may be calculated using the equation (Agresti, 2002, p. 271): exp(αj + βj′x) 1 + EJh=2 exp(αh + βj′x) (7) Notice that for the baseline category (here, j = 1), α1 and β1 = 0. Thus, when looking for the probability of a text belonging to the baseline level, it is easy to compute the numerator, since exp(0) = 1. The value of the denominator is the same for each j. Heilman et al. (2008) drew attention to the fact that the MLR model multiplies the number of parameters by J − 1 compared to the PO model. Because of this, they recommend using the PO model. 6 Implementation of the models Having covered the theoretical aspects of our model, we will now describe some of the particularities of our implementation. P(Y = j |x) = 23 6.1 The language model: probabilities and smoothing For our language model, we need a list of French lemmas with their frequencies of occurrence. Getting robust estimates for a large number of lemmas requires a very large corpus and is a timeconsuming proce</context>
<context position="25629" citStr="Heilman et al. (2008)" startWordPosition="4285" endWordPosition="4289">isons with previous studies are somewhat flawed by the fact that neither the target population nor the scale of difficulty is the same. However, our results can be roughly compared to some of the numerous studies on L1 English readability presented in Section 2. Before making this comparison, we will analyse the predictive ability of the two models. 7.1 Models evaluation The evaluation measures most commonly employed in the literature are Pearson’s productmoment correlation coefficient, prediction accuracy as defined by Tan et al. (2005), and adjacent accuracy. Adjacent accuracy is defined by Heilman et al. (2008) as “the proportion of predictions that were within one level of the human-assigned 24 Measure PO model MLR model Results on training folds Correl. 0.786 0.777 Exact Acc. 32.5% 38% Adj. Acc. 70% 71.3% Results on test folds Correl. 0.783 0.772 Exact Acc. 32.4% 38% Adj. Acc. 70% 71.2% Table 1: Mean Pearson’s r coefficient, exact and adjacent accuracies for both models with the tenfold cross-validation evaluation. label for the given text”. They defended this measure by arguing that even human-assigned reading levels are not always consistent. Nevertheless, it should not be forgotten that it can </context>
<context position="29413" citStr="Heilman et al. (2008)" startWordPosition="4921" endWordPosition="4924">th other studies As stated above, it is not easy to compare our results with those of previous studies, since the scale, population of interest and often the language are different. Furthermore, up till now, we have not been able to run the classical formulae for French (such as de Landsheere (1963) or Henry (1975)) on our corpus. So we are limited to comparing our evaluation measures with those in the published literature. With multinomial logistic regression, we obtained a mean adjacent accuracy of 71% for 9 classes. This result seems quite good compared to similar research on L1 English by Heilman et al. (2008). Using more complex syntactic features, they obtained an adjacent accuracy of 52% with a PO model, and 45% with a MLR model. However, they worked with 12 levels, which may explain their lower percentage. For French, Collins-Thompson and Callan (2005) reported a Pearson’s R coefficient of 0.64 for a 5-classes naive Bayes classifier while we obtained 0.77 for 9 levels with MLR. This difference might be explained by the tagging or the use of better-estimated probabilities for the language model. Further research on this point to determine the specificities of an efficient approach to French read</context>
<context position="30861" citStr="Heilman et al. (2008)" startWordPosition="5177" endWordPosition="5180">el and MLR model (on the test folds). 8 Discussion and future research This paper has proposed the first readability “formula” for French as a foreign language using NLP and statistical models. It takes into account some particularities of French such as its inflected nature. A new scale to assess FFL texts within the CECR framework, and a new criteria for the corpus involving the use of textbooks, have also been proposed. The two logistic models applied to a 440-text corpus gave results consistent with the literature. They also showed the superiority of the MLR model over the PO model. Since Heilman et al. (2008) found the opposite, and the intuitive view is that levels should be described by an ordinal scale of measurement, this issue clearly needs further investigation. This research is still in progress, and further analyses are planned. The predictive capacity of some other lexical and grammatical features will be explored. At the lexical level, statistical language models seems to be best, and tagging the texts to work with lemmas turned out to be efficient for French, although it has not been shown to be superior to disambiguated inflected forms. Moreover, due to their higher sensibility to cont</context>
</contexts>
<marker>Heilman, Collins-Thompson, Eskenazi, 2008</marker>
<rawString>M. Heilman, K. Collins-Thompson, and M. Eskenazi. 2008. An analysis of statistical models and features for reading difficulty prediction. Association for Computational Linguistics, The 3rd Workshop on Innovative Use of NLP for Building Educational Applications:1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Henry</author>
</authors>
<date>1975</date>
<note>Comment mesurer la lisibilit´e. Labor.</note>
<contexts>
<context position="3655" citStr="Henry (1975)" startWordPosition="588" endWordPosition="589">e length of sentences in a piece of text (Flesch, 1948; Kincaid et al., 1975), or the percentage of words not on a list combined with the average sentence length (Chall and Dale, 1995). French-speaking researchers discovered the field of readability in 1956 through the work of Andr´e Conquet, La lisibilit´e (1971), and the first two formulae for French were adapted from Flesch (1948) by Kandel and Moles (1958) and de Landsheere (1963). Both of these researchers stayed quite close to the Flesch formula, and in so doing they failed to take into account some specificities of the French language. Henry (1975) was the first to introduce specific formulae for French. He used a larger set of variables to design three formulae : a complete, an automatic and a short one, each of which Proceedings of the EACL 2009 Student Research Workshop, pages 19–27, Athens, Greece, 2 April 2009. c�2009 Association for Computational Linguistics 19 was adapted for three different educational levels. His formulae are by far the best and most frequently used in the French-speaking world. Later, Richaudeau (1979) suggested a criteria of “linguistic efficiency” based on experiments on shortterm memory, while Mesnager (198</context>
<context position="29108" citStr="Henry (1975)" startWordPosition="4872" endWordPosition="4873">r than on theoretical or empirical grounds. However we found that exact accuracy decreased by 10% when the tense variables were omitted from the models. Further analysis showed that the tense contributed significantly to the adjacent accuracy of classifying the C1 and C2 texts. 7.2 Comparison with other studies As stated above, it is not easy to compare our results with those of previous studies, since the scale, population of interest and often the language are different. Furthermore, up till now, we have not been able to run the classical formulae for French (such as de Landsheere (1963) or Henry (1975)) on our corpus. So we are limited to comparing our evaluation measures with those in the published literature. With multinomial logistic regression, we obtained a mean adjacent accuracy of 71% for 9 classes. This result seems quite good compared to similar research on L1 English by Heilman et al. (2008). Using more complex syntactic features, they obtained an adjacent accuracy of 52% with a PO model, and 45% with a MLR model. However, they worked with 12 levels, which may explain their lower percentage. For French, Collins-Thompson and Callan (2005) reported a Pearson’s R coefficient of 0.64 </context>
</contexts>
<marker>Henry, 1975</marker>
<rawString>G. Henry. 1975. Comment mesurer la lisibilit´e. Labor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D W Hosmer</author>
<author>S Lemeshow</author>
</authors>
<title>Applied Logistic Regression.</title>
<date>1989</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="18579" citStr="Hosmer and Lemeshow (1989)" startWordPosition="3047" endWordPosition="3050">compared regression methods and decision tree classifiers on the same corpus. We found that regression was more precise and more robust, due to the current limited size of the corpus. Linear regression was discarded because it gave poor results during the test phase. So we retained two logistic regression models, the PO model and the MLR model, which are presented in the next section. 5.1 Proportional odds (PO) model Logistic regression is a statistical technique first developed for binary data. It generally describes the probability of a 0 or 1 outcome with an S-shaped logistic function (see Hosmer and Lemeshow (1989) for details). Adaptation of the logistic regression for J ordinal classes involves a model with J − 1 response curves of the same shape. For a fixed class j, each of these response functions is comparable to a logistic regression curve for a binary response with outcomes Y &lt; j and Y &gt; j (Agresti, 2002), where Y is the dependent variable. The PO model can be expressed as: logit[P(Y &lt; j |x)] = αj + β′x (4) In Equation 4, x is the vector containing the independent variables, αj is the intercept parameter for the jth level and β is the vector of regression coefficients. From this formula, the par</context>
<context position="23350" citStr="Hosmer and Lemeshow (1989)" startWordPosition="3910" endWordPosition="3913">ed and tagged using the TreeTagger (Schmid, 1994). This NLP tool allows us to distinguish between homographs that can represent different levels of difficulty. For instance, the word actif is quite common as an adjective, but the noun is infrequent and is only used in the business lexicon. This distinction is possible because Lexique3 provides tagged lemmas. 6.2 Variable selection Having gathered the values for the 14 dependent variables, it was possible to train the two statistical models.2 However, an essential requirement prior to training is feature selection. This procedure, described by Hosmer and Lemeshow (1989), consists of examining models with one, two, three, 2All statistical computations were performed with the MASS package (Venables and Ripley, 2002) of the R software. etc., variables and comparing them to the full model according to some specified criteria so as to select one that is both efficient and parsimonious. For logistic regression, the criterion selected is the AIC (Akaike’s Information Criterion) of the model. This can be obtained from: AIC = −2log-likelihood + 2k (8) where k is the number of parameters in the model, and the log-likelihood value is the result of a calculation detaile</context>
</contexts>
<marker>Hosmer, Lemeshow, 1989</marker>
<rawString>D.W. Hosmer and S. Lemeshow. 1989. Applied Logistic Regression. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Howes</author>
<author>R L Solomon</author>
</authors>
<title>Visual duration threshold as a function of word probability.</title>
<date>1951</date>
<journal>Journal of Experimental Psychology,</journal>
<volume>41</volume>
<issue>40</issue>
<contexts>
<context position="11666" citStr="Howes and Solomon, 1951" startWordPosition="1888" endWordPosition="1891">r of letters per word, and the average sentence length, and found that, on our corpus, only the word length and sentence length correlated significantly with difficulty. Then, we add two NLP-oriented features, as described below: a statistical language model and a measure of tense difficulty. 4.1 The language model The lexical difficulty of a text is quite an elaborate phenomenon to parameterise. The logistic regression models we used in this study require us to reduce this complex reality to just one number, the challenge being to achieve the most informative number. Some psychological work (Howes and Solomon, 1951; Gerhand and Barry, 1998; Brysbaert et al., 2000) suggests that there is a strong relationship between the frequency of words and the speed with which they are recognised. We therefore opted to model the lexical difficulty for reading as the global probability of a text T (with N tokens) occurring: P(T) = P(t1)P(t2 |t1) ··· P(tn |t1, t2, ... , tn−1) (1) This equation raises two issues : 1. Estimating the conditional probabilities. It is well-known that it is impossible to train such a model on a corpus, even the largest one, because some sequences in this equation are unlikely to be encounter</context>
</contexts>
<marker>Howes, Solomon, 1951</marker>
<rawString>D.H. Howes and R.L. Solomon. 1951. Visual duration threshold as a function of word probability. Journal of Experimental Psychology, 41(40):1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Kandel</author>
<author>A Moles</author>
</authors>
<title>Application de l’indice de Flesch a` la langue franc¸aise. Cahiers ´Etudes de Radio-T´el´evision,</title>
<date>1958</date>
<contexts>
<context position="3456" citStr="Kandel and Moles (1958)" startWordPosition="552" endWordPosition="555">mula developed by Lively and Pressey (1923). The field of readability has since produced many formulae based on simple lexical and syntactic measures such as the average number of syllables per word, the average length of sentences in a piece of text (Flesch, 1948; Kincaid et al., 1975), or the percentage of words not on a list combined with the average sentence length (Chall and Dale, 1995). French-speaking researchers discovered the field of readability in 1956 through the work of Andr´e Conquet, La lisibilit´e (1971), and the first two formulae for French were adapted from Flesch (1948) by Kandel and Moles (1958) and de Landsheere (1963). Both of these researchers stayed quite close to the Flesch formula, and in so doing they failed to take into account some specificities of the French language. Henry (1975) was the first to introduce specific formulae for French. He used a larger set of variables to design three formulae : a complete, an automatic and a short one, each of which Proceedings of the EACL 2009 Student Research Workshop, pages 19–27, Athens, Greece, 2 April 2009. c�2009 Association for Computational Linguistics 19 was adapted for three different educational levels. His formulae are by far</context>
</contexts>
<marker>Kandel, Moles, 1958</marker>
<rawString>L. Kandel and A. Moles. 1958. Application de l’indice de Flesch a` la langue franc¸aise. Cahiers ´Etudes de Radio-T´el´evision, 19:253–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kemper</author>
</authors>
<title>Measuring the inference load of a text.</title>
<date>1983</date>
<journal>Journal of Educational Psychology,</journal>
<volume>75</volume>
<issue>3</issue>
<contexts>
<context position="10445" citStr="Kemper (1983)" startWordPosition="1696" endWordPosition="1697">ongs and poems as outliers). 4 Selection of lexical and syntactic variables Any text classification tasks require an object (here a text) to be parameterised into variables, whether qualitative or quantitative. These independent variables must correlate as strongly as possible with the dependent variable representing difficulty in order to explain the text’s complexity, and they should also account for the various dimensions of the readability phenomenon. Traditional approaches to readability have been sharply criticised with respect to this second requirement by Kintsch and Vipond (1979) and Kemper (1983), who both insist on the importance of including the conceptual properties of texts (such as the relations between propositions and the “inference load”). However, these new approaches have not resulted in any easily reproducible computational models, leading current researchers to continue to use the classic semantic and grammatical variables, enhancing them with NLP techniques. Because this research only spans the last year, attempts to discover interesting variables are still at an early stage. We explored the efficiency of some traditional features such as the type-token ratio, the number </context>
</contexts>
<marker>Kemper, 1983</marker>
<rawString>S. Kemper. 1983. Measuring the inference load of a text. Journal of Educational Psychology, 75(3):391–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kincaid</author>
<author>R P Fishburne</author>
<author>R Rodgers</author>
<author>B Chissom</author>
</authors>
<title>Derivation of new readability formulas for navy enlisted personnel.</title>
<date>1975</date>
<journal>Research Branch Report,</journal>
<volume>85</volume>
<contexts>
<context position="3120" citStr="Kincaid et al., 1975" startWordPosition="497" endWordPosition="500"> the first results of our models. Finally, Section 8 sums up the contribution of this article before providing a programme for future work and improvement of the results. 2 Related research The measurement of the reading difficulty of texts has been a major concern in the English-speaking literature since the 1920s and the first formula developed by Lively and Pressey (1923). The field of readability has since produced many formulae based on simple lexical and syntactic measures such as the average number of syllables per word, the average length of sentences in a piece of text (Flesch, 1948; Kincaid et al., 1975), or the percentage of words not on a list combined with the average sentence length (Chall and Dale, 1995). French-speaking researchers discovered the field of readability in 1956 through the work of Andr´e Conquet, La lisibilit´e (1971), and the first two formulae for French were adapted from Flesch (1948) by Kandel and Moles (1958) and de Landsheere (1963). Both of these researchers stayed quite close to the Flesch formula, and in so doing they failed to take into account some specificities of the French language. Henry (1975) was the first to introduce specific formulae for French. He used</context>
</contexts>
<marker>Kincaid, Fishburne, Rodgers, Chissom, 1975</marker>
<rawString>J. Kincaid, R.P. Fishburne, R. Rodgers, and B. Chissom. 1975. Derivation of new readability formulas for navy enlisted personnel. Research Branch Report, 85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kintsch</author>
<author>D Vipond</author>
</authors>
<title>Reading comprehension and readability in educational practice and psychological theory. Perspectives on Memory Research,</title>
<date>1979</date>
<pages>329--366</pages>
<contexts>
<context position="10427" citStr="Kintsch and Vipond (1979)" startWordPosition="1691" endWordPosition="1694">ive analyses have considered songs and poems as outliers). 4 Selection of lexical and syntactic variables Any text classification tasks require an object (here a text) to be parameterised into variables, whether qualitative or quantitative. These independent variables must correlate as strongly as possible with the dependent variable representing difficulty in order to explain the text’s complexity, and they should also account for the various dimensions of the readability phenomenon. Traditional approaches to readability have been sharply criticised with respect to this second requirement by Kintsch and Vipond (1979) and Kemper (1983), who both insist on the importance of including the conceptual properties of texts (such as the relations between propositions and the “inference load”). However, these new approaches have not resulted in any easily reproducible computational models, leading current researchers to continue to use the classic semantic and grammatical variables, enhancing them with NLP techniques. Because this research only spans the last year, attempts to discover interesting variables are still at an early stage. We explored the efficiency of some traditional features such as the type-token </context>
</contexts>
<marker>Kintsch, Vipond, 1979</marker>
<rawString>W. Kintsch and D. Vipond. 1979. Reading comprehension and readability in educational practice and psychological theory. Perspectives on Memory Research, pages 329–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B A Lively</author>
<author>S L Pressey</author>
</authors>
<title>A method for measuring the vocabulary burden of textbooks. Educational Administration and Supervision,</title>
<date>1923</date>
<pages>9--389</pages>
<contexts>
<context position="2876" citStr="Lively and Pressey (1923)" startWordPosition="456" endWordPosition="459">ibed. Section 4 focuses on the independent linguistic variables considered in this research, while the statistical techniques used for predictions are covered in Section 5. Section 6 gives some details of the implementations, and Section 7 presents the first results of our models. Finally, Section 8 sums up the contribution of this article before providing a programme for future work and improvement of the results. 2 Related research The measurement of the reading difficulty of texts has been a major concern in the English-speaking literature since the 1920s and the first formula developed by Lively and Pressey (1923). The field of readability has since produced many formulae based on simple lexical and syntactic measures such as the average number of syllables per word, the average length of sentences in a piece of text (Flesch, 1948; Kincaid et al., 1975), or the percentage of words not on a list combined with the average sentence length (Chall and Dale, 1995). French-speaking researchers discovered the field of readability in 1956 through the work of Andr´e Conquet, La lisibilit´e (1971), and the first two formulae for French were adapted from Flesch (1948) by Kandel and Moles (1958) and de Landsheere (</context>
</contexts>
<marker>Lively, Pressey, 1923</marker>
<rawString>B.A. Lively and S.L. Pressey. 1923. A method for measuring the vocabulary burden of textbooks. Educational Administration and Supervision, 9:389– 398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mesnager</author>
</authors>
<title>Lisibilit´e des textes pour enfants: un nouvel outil? Communication et Langages,</title>
<date>1989</date>
<pages>79--18</pages>
<contexts>
<context position="4257" citStr="Mesnager (1989)" startWordPosition="687" endWordPosition="688"> Henry (1975) was the first to introduce specific formulae for French. He used a larger set of variables to design three formulae : a complete, an automatic and a short one, each of which Proceedings of the EACL 2009 Student Research Workshop, pages 19–27, Athens, Greece, 2 April 2009. c�2009 Association for Computational Linguistics 19 was adapted for three different educational levels. His formulae are by far the best and most frequently used in the French-speaking world. Later, Richaudeau (1979) suggested a criteria of “linguistic efficiency” based on experiments on shortterm memory, while Mesnager (1989) coined what is still, to the best of our knowledge, the most recent specific formula for French, with children as its target. Compared to the mass of studies in English, readability in French has never enthused the research community. The cultural reasons for this are analysed by Boss´e-Andrieu (1993) (who basically argues that the idea of measuring text difficulty objectively seems far too pragmatic for the French spirit). It follows that there is little current research in this field: in Belgium, the Flesch formula is still used to assess the readability of articles in journalism studies. T</context>
</contexts>
<marker>Mesnager, 1989</marker>
<rawString>J. Mesnager. 1989. Lisibilit´e des textes pour enfants: un nouvel outil? Communication et Langages, 79:18–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B New</author>
<author>C Pallier</author>
<author>L Ferrand</author>
<author>R Matos</author>
</authors>
<title>Une base de donn´ees lexicales du franc¸ais contemporain sur internet: LEXIQUE. LAnn´ee Psychologique,</title>
<date>2001</date>
<pages>101--447</pages>
<contexts>
<context position="21567" citStr="New et al. (2001)" startWordPosition="3610" endWordPosition="3613">el multiplies the number of parameters by J − 1 compared to the PO model. Because of this, they recommend using the PO model. 6 Implementation of the models Having covered the theoretical aspects of our model, we will now describe some of the particularities of our implementation. P(Y = j |x) = 23 6.1 The language model: probabilities and smoothing For our language model, we need a list of French lemmas with their frequencies of occurrence. Getting robust estimates for a large number of lemmas requires a very large corpus and is a timeconsuming process. We used Lexique3, a lexicon provided by New et al. (2001) and developed from two corpora: the literary corpus Frantext containing about 15 million of words; and a corpus of film subtitles (New et al., 2007), with about 50 million words. The authors drew up a list of more than 50,000 tagged lemmas, each of which is associated with two frequency estimates, one from each corpus. We decided to use the frequencies from the subtitle corpus, because we think it gives a more accurate image of everyday language, which is the language FFL teaching is mainly concerned with. The frequencies were changed into probabilities, and smoothed with the Simple Good-Turi</context>
</contexts>
<marker>New, Pallier, Ferrand, Matos, 2001</marker>
<rawString>B. New, C. Pallier, L. Ferrand, and R. Matos. 2001. Une base de donn´ees lexicales du franc¸ais contemporain sur internet: LEXIQUE. LAnn´ee Psychologique, 101:447–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B New</author>
<author>M Brysbaert</author>
<author>J Veronis</author>
<author>C Pallier</author>
</authors>
<title>The use of film subtitles to estimate word frequencies.</title>
<date>2007</date>
<journal>Applied Psycholinguistics,</journal>
<volume>28</volume>
<issue>04</issue>
<contexts>
<context position="21716" citStr="New et al., 2007" startWordPosition="3636" endWordPosition="3639"> models Having covered the theoretical aspects of our model, we will now describe some of the particularities of our implementation. P(Y = j |x) = 23 6.1 The language model: probabilities and smoothing For our language model, we need a list of French lemmas with their frequencies of occurrence. Getting robust estimates for a large number of lemmas requires a very large corpus and is a timeconsuming process. We used Lexique3, a lexicon provided by New et al. (2001) and developed from two corpora: the literary corpus Frantext containing about 15 million of words; and a corpus of film subtitles (New et al., 2007), with about 50 million words. The authors drew up a list of more than 50,000 tagged lemmas, each of which is associated with two frequency estimates, one from each corpus. We decided to use the frequencies from the subtitle corpus, because we think it gives a more accurate image of everyday language, which is the language FFL teaching is mainly concerned with. The frequencies were changed into probabilities, and smoothed with the Simple Good-Turing algorithm described by Gale and Sampson (1995). This step is necessary to solve another well-known problem in language models: the appearance in a</context>
</contexts>
<marker>New, Brysbaert, Veronis, Pallier, 2007</marker>
<rawString>B. New, M. Brysbaert, J. Veronis, and C. Pallier. 2007. The use of film subtitles to estimate word frequencies. Applied Psycholinguistics, 28(04):661–677.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Richaudeau</author>
</authors>
<title>Une nouvelle formule de lisibilit´e.</title>
<date>1979</date>
<journal>Communication et Langages,</journal>
<pages>44--5</pages>
<contexts>
<context position="4145" citStr="Richaudeau (1979)" startWordPosition="670" endWordPosition="671">to the Flesch formula, and in so doing they failed to take into account some specificities of the French language. Henry (1975) was the first to introduce specific formulae for French. He used a larger set of variables to design three formulae : a complete, an automatic and a short one, each of which Proceedings of the EACL 2009 Student Research Workshop, pages 19–27, Athens, Greece, 2 April 2009. c�2009 Association for Computational Linguistics 19 was adapted for three different educational levels. His formulae are by far the best and most frequently used in the French-speaking world. Later, Richaudeau (1979) suggested a criteria of “linguistic efficiency” based on experiments on shortterm memory, while Mesnager (1989) coined what is still, to the best of our knowledge, the most recent specific formula for French, with children as its target. Compared to the mass of studies in English, readability in French has never enthused the research community. The cultural reasons for this are analysed by Boss´e-Andrieu (1993) (who basically argues that the idea of measuring text difficulty objectively seems far too pragmatic for the French spirit). It follows that there is little current research in this fi</context>
</contexts>
<marker>Richaudeau, 1979</marker>
<rawString>F. Richaudeau. 1979. Une nouvelle formule de lisibilit´e. Communication et Langages, 44:5–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings ofInternational Conference on New Methods in Language Processing,</booktitle>
<volume>12</volume>
<location>Manchester, UK.</location>
<contexts>
<context position="22773" citStr="Schmid, 1994" startWordPosition="3820" endWordPosition="3821">od-Turing algorithm described by Gale and Sampson (1995). This step is necessary to solve another well-known problem in language models: the appearance in a new text of previously unseen lemmas. In this case, since the logarithm of probabilities is used, an unseen lemma would result in a infinite value. In order to prevent this, a smoothing process is used to shift some of the model’s probability mass from seen lemmas to unseen ones. Once we had obtained a good estimate of the probabilities, we could analyse the texts in the corpus. Each of them was lemmatised and tagged using the TreeTagger (Schmid, 1994). This NLP tool allows us to distinguish between homographs that can represent different levels of difficulty. For instance, the word actif is quite common as an adjective, but the noun is infrequent and is only used in the business lexicon. This distinction is possible because Lexique3 provides tagged lemmas. 6.2 Variable selection Having gathered the values for the 14 dependent variables, it was possible to train the two statistical models.2 However, an essential requirement prior to training is feature selection. This procedure, described by Hosmer and Lemeshow (1989), consists of examining</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>H. Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings ofInternational Conference on New Methods in Language Processing, volume 12. Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Schwarm</author>
<author>M Ostendorf</author>
</authors>
<title>Reading level assessment using support vector machines and statistical language models.</title>
<date>2005</date>
<booktitle>Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="6020" citStr="Schwarm and Ostendorf (2005)" startWordPosition="969" endWordPosition="972">new formula for a related language. Therefore, we had to draw our inspiration from the English-speaking world, which has recently experienced a revival of interest in research on readability. Taking advantage of the increasing power of computers and the development of NLP techniques, researchers have been able to experiment with more complex variables. CollinsThompson et al. (2005) presented a variation of a multinomial naive Bayesian classifier they called the “Smoothed Unigram” model. We retained from their work the use of language models instead of word lists to measure lexical complexity. Schwarm and Ostendorf (2005) developed a SVM categoriser combining a classifier based on trigram language models (one for each level of difficulty), some parsing features such as average tree height, and variables traditionally used in readability. Heilman et al. (2007) extended the “Smoothed Unigram” model by the recognition of syntactic structures, in order to assess L2 English texts. Later, they improved the combination of their various lexical and grammatical features using regression methods (Heilman et al., 2008). We also found regression methods to be the most efficient of the statistical models with which we expe</context>
<context position="33047" citStr="Schwarm and Ostendorf, 2005" startWordPosition="5533" endWordPosition="5536">chniques are probably dependent on the quality of the parser’s results. Parsers for French are less accurate than those for English, which may generate some noise in the analysis. Finally, we intend to explore the performance of other classification techniques. Logistic regression was the most efficient of the statistical models we tested, but as our corpus grows, more and more data is becoming available, and data mining approaches may become applicable to the textcategorization problem for FFL readability. Support vector machines have already been shown to be useful for readability purposes (Schwarm and Ostendorf, 2005). We also want to try aggregating approaches such as boosting, bagging, and random forests (Breiman, 2001), since they claim to be effective when the sample is not perfectly representative of the population (which could be true for our data). These analyses would aim to illuminate some of the assets and flaws of each of the statistical models considered. Acknowledgments Thomas L. Franc¸ois is supported by the Belgian Fund for Scientific Research (FNRS), as is the research programme from which this material comes. I would like to thank my directors, Prof. C´edrick Fairon and Prof. Anne-Catherin</context>
</contexts>
<marker>Schwarm, Ostendorf, 2005</marker>
<rawString>S.E. Schwarm and M. Ostendorf. 2005. Reading level assessment using support vector machines and statistical language models. Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P-N Tan</author>
<author>M Steinbach</author>
<author>V Kumar</author>
</authors>
<title>Introduction to Data Mining.</title>
<date>2005</date>
<publisher>Addison-Wesley,</publisher>
<location>Boston.</location>
<contexts>
<context position="25551" citStr="Tan et al. (2005)" startWordPosition="4273" endWordPosition="4276">pecific issue of the readability of texts for FFL learners. So, any comparisons with previous studies are somewhat flawed by the fact that neither the target population nor the scale of difficulty is the same. However, our results can be roughly compared to some of the numerous studies on L1 English readability presented in Section 2. Before making this comparison, we will analyse the predictive ability of the two models. 7.1 Models evaluation The evaluation measures most commonly employed in the literature are Pearson’s productmoment correlation coefficient, prediction accuracy as defined by Tan et al. (2005), and adjacent accuracy. Adjacent accuracy is defined by Heilman et al. (2008) as “the proportion of predictions that were within one level of the human-assigned 24 Measure PO model MLR model Results on training folds Correl. 0.786 0.777 Exact Acc. 32.5% 38% Adj. Acc. 70% 71.3% Results on test folds Correl. 0.783 0.772 Exact Acc. 32.4% 38% Adj. Acc. 70% 71.2% Table 1: Mean Pearson’s r coefficient, exact and adjacent accuracies for both models with the tenfold cross-validation evaluation. label for the given text”. They defended this measure by arguing that even human-assigned reading levels ar</context>
</contexts>
<marker>Tan, Steinbach, Kumar, 2005</marker>
<rawString>P.-N. Tan, M. Steinbach, and V. Kumar. 2005. Introduction to Data Mining. Addison-Wesley, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Uitdenbogerd</author>
</authors>
<title>Readability of French as a foreign language and its uses.</title>
<date>2005</date>
<booktitle>In Proceedings of the Australian Document Computing Symposium,</booktitle>
<pages>pages</pages>
<contexts>
<context position="5264" citStr="Uitdenbogerd (2005)" startWordPosition="855" endWordPosition="856">ar too pragmatic for the French spirit). It follows that there is little current research in this field: in Belgium, the Flesch formula is still used to assess the readability of articles in journalism studies. This example also shows that the French-specific formulae are not much used, probably because of their complexity (Boss´e-Andrieu, 1993). Of course, if there is little work on French readability, there is even less on French as a foreign language. We only know the study of Cornaire (1988), which tested the adaptation of Henry’s short formula to French as a foreign language, and that of Uitdenbogerd (2005), which developed a new measure for English-speaking learners of French, stressing the importance of cognates when developing a new formula for a related language. Therefore, we had to draw our inspiration from the English-speaking world, which has recently experienced a revival of interest in research on readability. Taking advantage of the increasing power of computers and the development of NLP techniques, researchers have been able to experiment with more complex variables. CollinsThompson et al. (2005) presented a variation of a multinomial naive Bayesian classifier they called the “Smoot</context>
</contexts>
<marker>Uitdenbogerd, 2005</marker>
<rawString>S. Uitdenbogerd. 2005. Readability of French as a foreign language and its uses. In Proceedings of the Australian Document Computing Symposium, pages 19–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Venables</author>
<author>B D Ripley</author>
</authors>
<date>2002</date>
<booktitle>Modern Applied Statistics with S.</booktitle>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="23497" citStr="Venables and Ripley, 2002" startWordPosition="3931" endWordPosition="3934"> difficulty. For instance, the word actif is quite common as an adjective, but the noun is infrequent and is only used in the business lexicon. This distinction is possible because Lexique3 provides tagged lemmas. 6.2 Variable selection Having gathered the values for the 14 dependent variables, it was possible to train the two statistical models.2 However, an essential requirement prior to training is feature selection. This procedure, described by Hosmer and Lemeshow (1989), consists of examining models with one, two, three, 2All statistical computations were performed with the MASS package (Venables and Ripley, 2002) of the R software. etc., variables and comparing them to the full model according to some specified criteria so as to select one that is both efficient and parsimonious. For logistic regression, the criterion selected is the AIC (Akaike’s Information Criterion) of the model. This can be obtained from: AIC = −2log-likelihood + 2k (8) where k is the number of parameters in the model, and the log-likelihood value is the result of a calculation detailed by Hosmer and Lemeshow (1989). We applied the stepwise algorithm to our data, trying both a backward and a forward procedure. They converged to a</context>
</contexts>
<marker>Venables, Ripley, 2002</marker>
<rawString>W.N. Venables and B.D. Ripley. 2002. Modern Applied Statistics with S. Springer, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>