<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000462">
<title confidence="0.985325">
Generating Impact-Based Summaries for Scientific Literature
</title>
<author confidence="0.994839">
Qiaozhu Mei ChengXiang Zhai
</author>
<affiliation confidence="0.8102895">
University of Illinois at Urbana- University of Illinois at Urbana-
Champaign Champaign
</affiliation>
<email confidence="0.998458">
qmei2@uiuc.edu czhai@cs.uiuc.edu
</email>
<sectionHeader confidence="0.99861" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998530714285714">
In this paper, we present a study of a novel
summarization problem, i.e., summarizing the
impact of a scientific publication. Given a pa-
per and its citation context, we study how to
extract sentences that can represent the most
influential content of the paper. We propose
language modeling methods for solving this
problem, and study how to incorporate fea-
tures such as authority and proximity to ac-
curately estimate the impact language model.
Experiment results on a SIGIR publication
collection show that the proposed methods
are effective for generating impact-based sum-
maries.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999331466666667">
The volume of scientific literature has been growing
rapidly. From recent statistics, each year 400,000
new citations are added to MEDLINE, the major
biomedical literature database 1. This fast growth
of literature makes it difficult for researchers, espe-
cially beginning researchers, to keep track of the re-
search trends and find high impact papers on unfa-
miliar topics.
Impact factors (Kaplan and Nelson, 2000) are
useful, but they are just numerical values, so they
cannot tell researchers which aspects of a paper are
influential. On the other hand, a regular content-
based summary (e.g., the abstract or conclusion sec-
tion of a paper or an automatically generated topical
summary (Giles et al., 1998)) can help a user know
</bodyText>
<footnote confidence="0.714032">
1http://www.nlm.nih.gov/bsd/history/tsld024.htm
</footnote>
<bodyText confidence="0.999866911764706">
about the main content of a paper, but not necessar-
ily the most influential content of the paper. Indeed,
the abstract of a paper mostly reflects the expected
impact of the paper as perceived by the author(s),
which could significantly deviate from the actual im-
pact of the paper in the research community. More-
over, the impact of a paper changes over time due to
the evolution and progress of research in a field. For
example, an algorithm published a decade ago may
be no longer the state of the art, but the problem def-
inition in the same paper can be still well accepted.
Although much work has been done on text sum-
marization (See Section 6 for a detailed survey), to
the best of our knowledge, the problem of impact
summarization has not been studied before. In this
paper, we study this novel summarization problem
and propose language modeling-based approaches
to solving the problem. By definition, the impact
of a paper has to be judged based on the consent of
research community, especially by people who cited
it. Thus in order to generate an impact-based sum-
mary, we must use not only the original content, but
also the descriptions of that paper provided in papers
which cited it, making it a challenging task and dif-
ferent from a regular summarization setup such as
news summarization. Indeed, unlike a regular sum-
marization system which identifies and interprets the
topic of a document, an impact summarization sys-
tem should identify and interpret the impact of a pa-
per.
We define the impact summarization problem in
the framework of extraction-based text summariza-
tion (Luhn, 1958; McKeown and Radev, 1995), and
cast the problem as an impact sentence retrieval
</bodyText>
<page confidence="0.971691">
816
</page>
<note confidence="0.715767">
Proceedings of ACL-08: HLT, pages 816–824,
</note>
<page confidence="0.538308">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999968083333333">
problem. We propose language models to exploit
both the citation context and original content of a
paper to generate an impact-based summary. We
study how to incorporate features such as author-
ity and proximity into the estimation of language
models. We propose and evaluate several different
strategies for estimating the impact language model,
which is key to impact summarization. No exist-
ing test collection is available for evaluating impact
summarization. We construct a test collection us-
ing 28 years of ACM SIGIR papers (1978 - 2005)
to evaluate the proposed methods. Experiment re-
sults on this collection show that the proposed ap-
proaches are effective for generating impact-based
summaries. The results also show that using both the
original document content and the citation contexts
is important and incorporating citation authority and
proximity is beneficial.
An impact-based summary is not only useful for
facilitating the exploration of literature, but also
helpful for suggesting query terms for literature
retrieval, understanding the evolution of research
trends, and identifying the interactions of different
research fields. The proposed methods are also ap-
plicable to summarizing the impact of documents in
other domains where citation context exists, such as
emails and weblogs.
The rest of the paper is organized as follows. In
Section 2 and 3, we define the impact-based summa-
rization problem and propose the general language
modeling approach. In Section 4, we present differ-
ent strategies and features for estimating an impact
language model, a key challenge in impact summa-
rization. We discuss our experiments and results in
Section 5. Finally, the related work and conclusions
are discussed in Section 6 and Section 7.
</bodyText>
<sectionHeader confidence="0.984244" genericHeader="introduction">
2 Impact Summarization
</sectionHeader>
<bodyText confidence="0.99975552631579">
Following the existing work on topical summariza-
tion of scientific literature (Paice, 1981; Paice and
Jones, 1993), we define an impact-based summary
of a paper as a set of sentences extracted from
a paper that can reflect the impact of the paper,
where “impact” is roughly defined as the influence
of the paper on research of similar or related top-
ics as reflected in the citations of the paper. Such
an extraction-based definition of summarization has
also been quite common in most existing general
summarization work (Radev et al., 2002).
By definition, in order to generate an impact sum-
mary of a paper, we must look at how other papers
cite the paper, use this information to infer the im-
pact of the paper, and select sentences from the orig-
inal paper that can reflect the inferred impact. Note
that we do not directly use the sentences from the ci-
tation context to form a summary. This is because in
citations, the discussion of the paper cited is usually
mixed with the content of the paper citing it, and
sometimes also with discussion about other papers
cited (Siddharthan and Teufel, 2007).
Formally, let d = (so, si, ..., s,,,) be a paper to
be summarized, where sz is a sentence. We refer
to a sentence (in another paper) in which there is
an explicit citation of d as a citing sentence of d.
When a paper is cited, it is often discussed consec-
utively in more than one sentence near the citation,
thus intuitively we would like to consider a window
of sentences centered at a citing sentence; the win-
dow size would be a parameter to set. We call such
a window of sentences a citation context, and use C
to denote the union of all the citation contexts of d
in a collection of research papers. Thus C itself is
a set (more precisely bag) of sentences. The task
of impact-based summarization is thus to 1) con-
struct a representation of the impact of d, I, based
on d and C; 2) design a scoring function Score(.)
to rank sentences in d based on how well a sentence
reflects I. A user-defined number of top-ranked sen-
tences can then be selected as the impact summary
for d.
The formulation above immediately suggests that
we can cast the impact summarization problem as
a retrieval problem where each candidate sentence
in d is regarded as a “document,” the impact of the
paper (i.e., I) as a “query,” and our goal is to “re-
trieve” sentences that can reflect the impact of the
paper as indicated by the citation context. Looking
at the problem in this way, we see that there are two
main challenges in impact summarization: first, we
must be able to infer the impact based on both the
citation contexts and the original document; second,
we should measure how well a sentence reflects this
inferred impact. To solve these challenges, in the
next section, we propose to model impact with un-
igram language models and score sentences using
</bodyText>
<page confidence="0.994434">
817
</page>
<bodyText confidence="0.99962425">
Kullback-Leibler divergence. We further propose
methods for estimating the impact language model
based on several features including the authority of
citations, and the citation proximity.
</bodyText>
<sectionHeader confidence="0.995898" genericHeader="method">
3 Language Models for Impact
Summarization
</sectionHeader>
<subsectionHeader confidence="0.999938">
3.1 Impact language models
</subsectionHeader>
<bodyText confidence="0.999693958333333">
From the retrieval perspective, our collection is the
paper to be summarized, and each sentence is a
“document” to be retrieved. However, unlike in the
case of ad hoc retrieval, we do not really have a
query describing the impact of the paper; instead,
we have a lot of citation contexts that can be used
to infer information about the query. Thus the main
challenge in impact summarization is to effectively
construct a “virtual impact query” based on the cita-
tion contexts.
What should such a virtual impact query look
like? Intuitively, it should model the impact-
reflecting content of the paper. We thus propose to
represent such a virtual impact query with a unigram
language model. Such a model is expected to assign
high probabilities to those words that can describe
the impact of paper d, just as we expect a query
language model in ad hoc retrieval to assign high
probabilities to words that tend to occur in relevant
documents (Ponte and Croft, 1998). We call such a
language model the impact language model of paper
d (denoted as BI); it can be estimated based on both
d and its citation context C as will be discussed in
Section 4.
</bodyText>
<subsectionHeader confidence="0.999692">
3.2 KL-divergence scoring
</subsectionHeader>
<bodyText confidence="0.99997545">
With the impact language model in place, we
can then adopt many existing probabilistic retrieval
models such as the classical probabilistic retrieval
models (Robertson and Sparck Jones, 1976) and the
Kullback-Leibler (KL) divergence retrieval model
(Lafferty and Zhai, 2001; Zhai and Lafferty, 2001a),
to solve the problem of impact summarization by
scoring sentences based on the estimated impact lan-
guage model. In our study, we choose to use the KL-
divergence scoring method to score sentences as this
method has performed well for regular ad hoc re-
trieval tasks (Zhai and Lafferty, 2001a) and has an
information theoretic interpretation.
To apply the KL-divergence scoring method, we
assume that a candidate sentence s is generated from
a sentence language model Bs. Given s in d and the
citation context C, we would first estimate Bs based
on s and estimate BI based on C, and then score s
with the negative KL divergence of Bs and BI. That
is,
</bodyText>
<equation confidence="0.978476">
Score(s) = −D(BI||Bs)
</equation>
<bodyText confidence="0.999933">
where V is the set of words in our vocabulary and w
denotes a word.
From the information theoretic perspective, the
KL-divergence of Bs and BI can be interpreted
as measuring the average number of bits wasted
in compressing messages generated according to
BI (i.e., impact descriptions) with coding non-
optimally designed based on Bs. If Bs and BI are
very close, the KL-divergence would be small and
Score(s) would be high, which intuitively makes
sense. Note that the second term (entropy of BI) is
independent of s, so it can be ignored for ranking s.
We see that according to the KL-divergence scor-
ing method, our main tasks are to estimate Bs and
BI. Since s can be regarded as a short document, we
can use any standard method to estimate Bs. In this
work, we use Dirichlet prior smoothing (Zhai and
Lafferty, 2001b) to estimate Bs as follows:
</bodyText>
<equation confidence="0.759926714285714">
c(w, s) + µs * P(w|D)
p(w|Bs) = (1)
|s |+ µs
where |s |is the length of s, c(w, s) is the count of
word w in s, p(w|D) is a background model esti-
mated using c�w,D)
����� c�w�,D) (D can be the set of all
</equation>
<bodyText confidence="0.9999515">
the papers available to us) and µs is a smoothing pa-
rameter to be empirically set. Note that as the length
of a sentence is very short, smoothing is critical for
addressing the data sparseness problem.
The remaining challenge is to estimate BI accu-
rately based on d and its citation contexts.
</bodyText>
<sectionHeader confidence="0.878159" genericHeader="method">
4 Estimation of Impact Language Models
</sectionHeader>
<bodyText confidence="0.999945">
Intuitively, the impact of a paper is mostly reflected
in the citation context. Thus the estimation of the
impact language model should be primarily based
on the citation context C. However, we would like
</bodyText>
<equation confidence="0.779827">
�= p(w|BI) log p(w|Bs)− � p(w|BI) log p(w|BI)
wEV wEV
</equation>
<page confidence="0.977106">
818
</page>
<bodyText confidence="0.999965882352941">
our impact model to be able to help us select impact-
reflecting sentences from d, thus it is important for
the impact model to explain well the paper content
in general. To achieve this balance, we treat the ci-
tation context C as prior information and the current
document d as the observed data, and use Bayesian
estimation to estimate the impact language model.
Specifically, let p(w|C) be a citation context lan-
guage model estimated based on the citation con-
text C. We define Dirichlet prior with parameters
1µCp(w|C)�w∈V for the impact model, where µC
encodes our confidence on this prior and effectively
serves as a weighting parameter for balancing the
contribution of C and d for estimating the impact
model. Given the observed document d, the poste-
rior mean estimate of the impact model would be
(MacKay and Peto, 1995; Zhai and Lafferty, 2001b)
</bodyText>
<equation confidence="0.998282">
c(w, d) + µcp(w|C)
P(w|θI) = (2)
|d |+ µc
</equation>
<bodyText confidence="0.999183307692308">
µc can be interpreted as the equivalent sample size of
our prior. Thus setting µc = |d |means that we put
equal weights on the citation context and the doc-
ument itself. µc = 0 yields p(w|θI) = p(w|d),
which is to say that the impact is entirely captured
by the paper itself, and our impact summarization
problem would then become the standard single doc-
ument (topical) summarization. Intuitively though,
we would want to set µc to a relatively large num-
ber to exploit the citation context in our estimation,
which is confirmed in our experiments.
An alternative way is to simply interpolate p(w|d)
and p(w|C) with a constant coefficient:
</bodyText>
<equation confidence="0.997033">
p(w|θI) = (1 − δ)p(w|d) + δp(w|C) (3)
</equation>
<bodyText confidence="0.999532333333333">
We will compare the two strategies in Section 5.
How do we estimate p(w|C)? Intuitively, words
occurring in C frequently should have high proba-
bilities. A simple way is to pool together all the sen-
tences in C and use the maximum likelihood estima-
tor,
</bodyText>
<equation confidence="0.951915333333333">
P
s∈C c(w, s) p(w|C) = P P s/∈C c(w0, s0) (4)
w/∈V
</equation>
<bodyText confidence="0.999223454545455">
where c(w, s) is the count of w in s.
One deficiency of this simple estimate is that we
treat all the (extended) citation sentences equally.
However, there are at least two reasons why we want
to assign unequal weights to different citation sen-
tences: (1) A sentence closer to the citation label
should contribute more than one far away. (2) A sen-
tence occurring in a highly authorative paper should
contribute more than that in a less authorative paper.
To capture these two heuristics, we define a weight
coefficient αs for a sentence s in C as follows:
</bodyText>
<equation confidence="0.574543">
αs = pg(s)pr(s)
</equation>
<bodyText confidence="0.999924769230769">
where pg(s) is an authority score of the paper con-
taining s and pr(s) is a proximity score that rewards
a sentence close to the citation label.
For example, pg(s) can be the PageRank value
(Brin and Page, 1998) of the document with s, which
measures the authority of the document based on a
citation graph, and is computed as follows: We con-
struct a directed graph from the collection of scien-
tific literature with each paper as a vertex and each
citation as a directed edge pointing from the citing
paper to the cited paper. We can then use the stan-
dard PageRank algorithm (Brin and Page, 1998) to
compute a PageRank value for each document. We
used this approach in our experiments.
We define pr(s) as pr(s) = α�1 , where k is the
distance (counted in terms of the number of sen-
tences) between sentence s and the center sentence
of the window containing s; by “center sentence”,
we mean the citing sentence containing the citation
label. Thus the sentence with the citation label will
have a proximity of 1 (because k = 0), while the
sentences away from the citation label will have a
decaying weight controlled by parameter α.
With αs, we can then use the following
“weighted” maximum likelihood estimate for the
impact language model:
</bodyText>
<equation confidence="0.9632425">
P
s∈C αsc(w, s)
p(w|C) = P Ps/∈C αs/c(w0, s0) (5)
w/∈V
</equation>
<bodyText confidence="0.9970035">
As we will show in Section 5, this weighted
maximum likelihood estimate performs better than
the simple maximum likelihood estimate, and both
pg(s) and pr(s) are useful.
</bodyText>
<page confidence="0.983052">
819
</page>
<figure confidence="0.665527666666667">
5 Experiments and Results
5.1 Experiment Design
5.1.1 Test set construction
</figure>
<bodyText confidence="0.999924673913044">
going beyond the 14 papers would risk reducing the
reliability of impact judgment due to the sparseness
of citations. How to develop a better test collection
is an important future direction.
Because no existing test set is available for evalu-
ating impact summarization, we opt to create a test
set based on 28 years of ACM SIGIR papers (1978
- 2005) available through the ACM Digital Library2
and the SIGIR membership. Leveraging the explicit
citation information provided by ACM Digital Li-
brary, for each of the 1303 papers, we recorded all
other papers that cited the paper and extracted the
citation context from these citing papers. Each ci-
tation context contains 5 sentences with 2 sentences
before and after the citing sentence.
Since a low-impact paper would not be useful for
evaluating impact summarization, we took all the
14 papers from the SIGIR collection that have no
less than 20 citations by papers in the same col-
lection as candidate papers for evaluation. An ex-
pert in Information Retrieval field read each paper
and its citation context, and manually created an
impact-based summary by selecting all the “impact-
capturing” sentences from the paper. Specifically,
the expert first attempted to understand the most in-
fluential content of a paper by reading the citation
contexts. The expert then read each sentence of
the paper and made a decision whether the sentence
covers some “influential content” as indicated in the
citation contexts. The sentences that were decided
as covering some influential content were then col-
lected as the gold standard impact summary for the
paper.
We assume that the title of a paper will always
be included in the summary, so we excluded the ti-
tle both when constructing the gold standard and
when generating a summary. The gold standard
summaries have a minimum length of 5 sentences
and a maximum length of 18 sentences; the me-
dian length is 9 sentences. These 14 impact-based
summaries are used as gold standards for our exper-
iments, based on which all summaries generated by
the system are evaluated. This data set is available at
http://timan.cs.uiuc.edu/data/impact.html. We must
admit that using only 14 papers and only one expert
for evaluation is a limitation of our work. However,
</bodyText>
<footnote confidence="0.707112">
2http://www.acm.org/dl
</footnote>
<subsubsectionHeader confidence="0.969938">
5.1.2 Evaluation Metrics
</subsubsectionHeader>
<bodyText confidence="0.999965333333333">
Following the current practice in evaluating sum-
marization, particularly DUC3, we use the ROUGE
evaluation package (Lin and Hovy, 2003). Among
ROUGE metrics, ROUGE-N (models n-gram co-
occurrence, N = 1, 2) and ROUGE-L (models
longest common sequence) generally perform well
in evaluating both single-document summarization
and multi-document summarization (Lin and Hovy,
2003). Since they are general evaluation measures
for summarization, they are also applicable to eval-
uating the MEAD-Doc+Cite baseline method to be
described below. Thus although we evaluated our
methods with all the metrics provided by ROUGE,
we only report ROUGE-1 and ROUGE-L in this pa-
per (other metrics give very similar results).
</bodyText>
<sectionHeader confidence="0.68122" genericHeader="method">
5.1.3 Baseline methods
</sectionHeader>
<bodyText confidence="0.999930869565217">
Since impact summarization has not been previ-
ously studied, there is no natural baseline method to
compare with. We thus adapt some state-of-the-art
conventional summarization methods implemented
in the MEAD toolkit (Radev et al., 2003)4 to obtain
three baseline methods: (1) LEAD: It simply ex-
tracts sentences from the beginning of a paper, i.e.,
sentences in the abstract or beginning of the intro-
duction section; we include LEAD to see if such
“leading sentences” reflect the impact of a paper as
authors presumably would expect to summarize a
paper’s contributions in the abstract. (2) MEAD-
Doc: It uses the single-document summarizer in
MEAD to generate a summary based solely on the
original paper; comparison with this baseline can
tell us how much better we can do than a conven-
tional topic-based summarizer that does not consider
the citation context. (3) MEAD-Doc+Cite: Here
we concatenate all the citation contexts in a paper to
form a “citation document” and then use the MEAD
multidocument summarizer to generate a summary
from the original paper plus all its citation docu-
ments; this baseline represents a reasonable way
</bodyText>
<footnote confidence="0.9868455">
3http://duc.nist.gov/
4“http://www.summarization.com/mead/”
</footnote>
<page confidence="0.986624">
820
</page>
<table confidence="0.998867333333333">
Sum. Length Metric Random LEAD MEAD-Doc MEAD-Doc+Cite KL-Divergence
3 ROUGE-1 0.163 0.167 0.301* 0.248 0.323
3 ROUGE-L 0.144 0.158 0.265 0.217 0.299
5 ROUGE-1 0.230 0.301 0.401 0.333 0.467
5 ROUGE-L 0.214 0.292 0.362 0.298 0.444
10 ROUGE-1 0.430 0.514 0.575 0.472 0.649
10 ROUGE-L 0.396 0.494 0.535 0.428 0.622
15 ROUGE-1 0.538 0.610 0.685 0.552 0.730
15 ROUGE-L 0.499 0.586 0.650 0.503 0.705
</table>
<tableCaption confidence="0.6359712">
Table 1: Performance Comparison of Summarizers
of applying an existing summarization method to
generate an impact-based summary. Note that this
method may extract sentences in the citation con-
texts but not in the original paper.
</tableCaption>
<subsectionHeader confidence="0.999381">
5.2 Basic Results
</subsectionHeader>
<bodyText confidence="0.995412381818182">
We first show some basic results of impact sum-
marization in Table 1. They are generated us-
ing constant coefficient interpolation for the impact
language model (i.e., Equation 3) with 6 = 0.8,
weighted maximum likelihood estimate for the ci-
tation context model (i.e., Equation 5) with α = 3,
and p, = 1, 000 for candidate sentence smoothing
(Equation 1). These results are not necessarily opti-
mal as will be seen when we examine parameter and
method variations.
From Table 1, we see clearly that our method
consistently outperforms all the baselines. Among
the baselines, MEAD-Doc is consistently better than
both LEAD and MEAD-Doc+Cite. While MEAD-
Doc’s outperforming LEAD is not surprising, it is
a bit surprising that MEAD-Doc also outperforms
MEAD-Doc+Cite as the latter uses both the cita-
tion context and the original document. One possi-
ble explanation may be that MEAD is not designed
for impact summarization and it has been trapped
by the distracting content in the citation context 5.
Indeed, this can also explain why MEAD-Doc+Cite
tends to perform worse than LEAD by ROUGE-L
since if MEAD-Doc+Cite picks up sentences from
the citation context rather than the original papers,
it would not match as well with the gold standard
as LEAD which selects sentences from the origi-
5One anonymous reviewer suggested an interesting im-
provement to the MEAD-Doc+Cite baseline, in which we
would first extract sentences from the citation context and then
for each extracted sentence find a similar one in the original pa-
per. Unfortunately, we did not have time to test this approach
before the deadline for the camera-ready version of this paper.
nal papers. These results thus show that conven-
tional summarization techniques are inadequate for
impact summarization, and the proposed language
modeling methods are more effective for generating
impact-based summaries.
In Table 2, we show a sample impact-based sum-
mary and the corresponding MEAD-Doc regular
summary. We see that the regular summary tends
to have general sentences about the problem, back-
ground and techniques, not very informative in con-
veying specific contributions of the paper. None of
these sentences was selected by the human expert. In
contrast, the sentences in the impact summary cover
several details of the impact of the paper (i.e., spe-
cific smoothing methods especially Dirichlet prior,
sensitivity of performance to smoothing, and dual
role of smoothing), and sentences 4 and 6 are also
among the 8 sentences picked by the human expert.
Interestingly, neither sentence is in the abstract of
the original paper, suggesting a deviation of the ac-
tual impact of a paper and that perceived by the au-
thor(s).
</bodyText>
<subsectionHeader confidence="0.999979">
5.3 Component analysis
</subsectionHeader>
<bodyText confidence="0.998748">
We now turn to examine the effectiveness of each
component in the proposed methods and different
strategies for estimating BI.
</bodyText>
<subsectionHeader confidence="0.866033">
Effectiveness of interpolation: We hypothesized
</subsectionHeader>
<bodyText confidence="0.9994595">
that we need to use both the original document and
the citation context to estimate BI. To test this hy-
pothesis, we compare the results of using only d,
only the citation context, and interpolation of them
in Table 3. We show two different strategies of inter-
polation (i.e., constant coefficient with 6 = 0.8 and
Dirichlet with p, = 20, 000) as described in Sec-
tion 4.
From Table 3, we see that both strategies of in-
terpolation indeed outperform using either the origi-
</bodyText>
<page confidence="0.994295">
821
</page>
<table confidence="0.911156423076923">
Impact-based summary:
1. Figure 5: Interpolation versus backoff for Jelinek-Mercer (top), Dirichlet smoothing (middle), and absolute discounting (bottom).
2. Second, one can de-couple the two different roles of smoothing by adopting a two stage smoothing strategy in which Dirichlet smoothing is
first applied to implement the estimation role and Jelinek-Mercer smoothing is then applied to implement the role of query modeling
3. We find that the backoff performance is more sensitive to the smoothing parameter than that of interpolation, especially in Jelinek-Mercer
and Dirichlet prior.
4. We then examined three popular interpolation-based smoothing methods (Jelinek-Mercer method, Dirichlet priors, and absolute discounting),
as well as their backoff versions, and evaluated them using several large and small TREC retrieval testing collections.
summary 5. By rewriting the query-likelihood retrieval model using a smoothed document language model, we derived a general retrieval
formula where the smoothing of the document language model can be interpreted in terms of several heuristics used intraditional models,
including TF-IDF weighting and document length normalization.
6. We find that the retrieval performance is generally sensitive to the smoothing parameters, suggesting that an understanding and appropriate
setting of smoothing parameters is very important in the language modeling approach.
Regular summary (generated using MEAD-Doc):
1. Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that
of language model estimation, which has been studied extensively in other application areas such as speech recognition.
2. The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of the
query according to the estimated language model.
3. On the one hand, theoretical studies of an underlying model have been developed; this direction is, for example, represented by the various
kinds of logic models and probabilistic models (e.g., [14, 3, 15, 22]).
4. After applying the Bayes’ formula and dropping a document-independent constant (since we are only interested in ranking documents), we
have p(djq) « (q|d)p(d).
5. As discussed in [1], the righthand side of the above equation has an interesting interpretation, where, p(d) is our prior belief that d is relevant
to any query and p(qld) is the query likelihood given the document, which captures how well the document ”fits” the particular query q.
6. The probability of an unseen word is typically taken as being proportional to the general frequency of the word, e.g., as computed using the
document collection.
</table>
<tableCaption confidence="0.9937535">
Table 2: Impact-based summary vs. regular summary for the paper “A study of smoothing methods for language
models applied to ad hoc information retrieval”.
</tableCaption>
<bodyText confidence="0.998600555555555">
nal document model (p(wId)) or the citation context
model (p(wIC)) alone, which confirms that both the
original paper and the citation context are important
for estimating BI. We also see that using the citation
context alone is better than using the original paper
alone, which is expected. Between the two strate-
gies, Dirichlet dynamic coefficient is slightly better
than constant coefficient (CC), after optimizing the
interpolation parameter for both strategy.
</bodyText>
<table confidence="0.99758225">
Interpolation
Measure P(wld) P(wlC) ConstCoef Dirichlet
ROUGE-1 0.529 0.635 0.643 0.647
ROUGE-L 0.501 0.607 0.619 0.623
</table>
<tableCaption confidence="0.999435">
Table 3: Effectiveness of interpolation
</tableCaption>
<bodyText confidence="0.956749625">
Citation authority and proximity: These heuris-
tics are very interesting to study as they are unique
to impact summarization and not well studied in the
existing summarization work.
pg(s) pr(s)=1/ak
pr(s)off a = 2 a = 3 a = 4
Off 0.685 0.711 0.714 0.700
On 0.708 0.712 0.706 0.703
</bodyText>
<tableCaption confidence="0.992165">
Table 4: Authority (pg(s)) and proximity (pr(s))
</tableCaption>
<bodyText confidence="0.99709424">
In Table 4, we show the ROUGE-L values for var-
ious combinations of these two heuristics (summary
length is 15). We turn off either pg(s) or pr(s) by
setting it to a constant; when both are turned off, we
have the unweighted MLE of p(wIC) (Equation 4).
Clearly, using weighted MLE with any of the two
heuristics is better than the unweighted MLE, indi-
cating that both heuristics are effective. However,
combining the two heuristics does not always im-
prove over using a single one. Since intuitively these
two heuristics are orthogonal, this may suggest that
our way of combining the two scores (i.e., taking a
product of them) may not be optimal; further study
is needed to better understand this. The ROUGE-1
results are similar.
Tuning of other parameters: There are three other
parameters which need to be tuned: (1) µs for can-
didate sentence smoothing (Equation 1); (2) µ, in
Dirichlet interpolation for impact model estimation
(Equation 2); and (3) S in constant coefficient inter-
polation (Equation 3). We have examined the sen-
sitivity of performance to these parameters. In gen-
eral, for a wide range of values of these parameters,
the performance is relatively stable and near opti-
mal. Specifically, the performance is near optimal as
</bodyText>
<page confidence="0.9914">
822
</page>
<bodyText confidence="0.998275333333333">
long as µs and µ, are sufficiently large (µs &gt; 1000,
µ, &gt; 20, 000), and the interpolation parameter 6 is
between 0.4 and 0.9.
</bodyText>
<sectionHeader confidence="0.997796" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999811787234043">
and summarization (Kraaij et al., 2001). However,
we do not have an explicit query and constructing
the impact model is a novel exploration. We also
proposed new language models to capture the im-
pact.
General text summarization, including single docu-
ment summarization (Luhn, 1958; Goldstein et al.,
1999) and multi-document summarization (Kraaij et
al., 2001; Radev et al., 2003) has been well stud-
ied; our work is under the framework of extractive
summarization (Luhn, 1958; McKeown and Radev,
1995; Goldstein et al., 1999; Kraaij et al., 2001),
but our problem formulation differs from any exist-
ing formulation of the summarization problem. It
differs from regular single-document summarization
because we utilize extra information (i.e. citation
contexts) to summarize the impact of a paper. It also
differs from regular multi-document summarization
because the roles of original documents and cita-
tion contexts are not equivalent. Specifically, cita-
tion contexts serve as an indicator of the impact of
the paper, but the summary is generated by extract-
ing the sentences from the original paper.
Technical paper summarization has also been
studied (Paice, 1981; Paice and Jones, 1993; Sag-
gion and Lapalme, 2002; Teufel and Moens, 2002),
but the previous work did not explore citation con-
text to emphasize the impact of papers.
Citation context has been explored in several
studies (Nakov et al., 2004; Ritchie et al., 2006;
Schwartz et al., 2007; Siddharthan and Teufel,
2007). However, none of the previous studies has
used citation context in the same way as we did,
though the potential of directly using citation sen-
tences (called citances) to summarize a paper was
pointed out in (Nakov et al., 2004).
Recently, people have explored various types of
auxiliary knowledge such as hyperlinks (Delort et
al., 2003) and clickthrough data (Sun et al., 2005), to
summarize a webpage; such work is related to ours
as anchor text is similar to citation context, but it is
based on a standard formulation of multi-document
summarization and would contain only sentences
from anchor text.
Our work is also related to work on using lan-
guage models for retrieval (Ponte and Croft, 1998;
Zhai and Lafferty, 2001b; Lafferty and Zhai, 2001)
</bodyText>
<sectionHeader confidence="0.999063" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999970148148148">
We have defined and studied the novel problem of
summarizing the impact of a research paper. We cast
the problem as an impact sentence retrieval problem,
and proposed new language models to model the im-
pact of a paper based on both the original content
of the paper and its citation contexts in a literature
collection with consideration of citation autority and
proximity.
To evaluate impact summarization, we created a
test set based on ACM SIGIR papers. Experiment
results on this test set show that the proposed im-
pact summarization methods are effective and out-
perform several baselines that represent the existing
summarization methods.
An important future work is to construct larger
test sets (e.g., of biomedical literature) to facilitate
evaluation of impact summarization. Our formula-
tion of the impact summarization problem can be
further improved by going beyond sentence retrieval
and considering factors such as redundancy and co-
herency to better organize an impact summary. Fi-
nally, automatically generating impact-based sum-
maries can not only help users access and digest
influential research publications, but also facilitate
other literature mining tasks such as milestone min-
ing and research trend monitoring. It would be in-
teresting to explore all these applications.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999324">
We are grateful to the anonymous reviewers for their
constructive comments. This work is in part sup-
ported by a Yahoo! Graduate Fellowship and NSF
grants under award numbers 0713571, 0347933, and
0428472.
</bodyText>
<sectionHeader confidence="0.996948" genericHeader="references">
References
</sectionHeader>
<subsectionHeader confidence="0.4533325">
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine. In
</subsectionHeader>
<note confidence="0.582518">
Proceedings of the Seventh International Conference
on World Wide Web, pages 107–117.
</note>
<page confidence="0.997322">
823
</page>
<reference confidence="0.998335113207547">
J.-Y. Delort, B. Bouchon-Meunier, and M. Rifqi. 2003.
Enhanced web document summarization using hyper-
links. In Proceedings of the Fourteenth ACM Confer-
ence on Hypertext and Hypermedia, pages 208–215.
C. Lee Giles, Kurt D. Bollacker, and Steve Lawrence.
1998. Citeseer: an automatic citation indexing sys-
tem. In Proceedings of the Third ACM Conference on
Digital Libraries, pages 89–98.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text documents:
sentence selection and evaluation metrics. In Proceed-
ings ofACMSIGIR 99, pages 121–128.
Nancy R. Kaplan and Michael L. Nelson. 2000. Deter-
mining the publication impact of a digital library. J.
Am. Soc. Inf. Sci., 51(4):324–339.
W. Kraaij, M. Spitters, and M. van der Heijden. 2001.
Combining a mixture language model and naive bayes
for multi-document summarisation. In Proceedings of
the DUC2001 workshop.
John Lafferty and Chengxiang Zhai. 2001. Document
language models, query models, and risk minimiza-
tion for information retrieval. In Proceedings ofACM
SIGIR 2001, pages 111–119.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pages 71–78.
H. P. Luhn. 1958. The automatic creation of literature
abstracts. IBMJournal of Research and Development,
2(2):159–165.
D. MacKay and L. Peto. 1995. A hierarchical Dirich-
let language model. Natural Language Engineering,
1(3):289–307.
Kathleen McKeown and Dragomir R. Radev. 1995. Gen-
erating summaries of multiple news articles. In Pro-
ceedings of the 18th Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 74–82.
P. Nakov, A. Schwartz, and M. Hearst. 2004. Citances:
Citation sentences for semantic analysis of bioscience
text. In Proceedings of ACM SIGIR’04 Workshop on
Search and Discovery in Bioinformatics.
Chris D. Paice and Paul A. Jones. 1993. The identifi-
cation of important concepts in highly structured tech-
nical papers. In Proceedings of the 16th Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 69–78.
C. D. Paice. 1981. The automatic generation of literature
abstracts: an approach based on the identification of
self-indicating phrases. In Proceedings of the 3rd An-
nual ACM Conference on Research and Development
in Information Retrieval, pages 172–191.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the 21st Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 275–281.
Dragomir R. Radev, Eduard Hovy, and Kathleen McKe-
own. 2002. Introduction to the special issue on sum-
marization. Comput. Linguist., 28(4):399–408.
Dragomir R. Radev, Simone Teufel, Horacio Saggion,
Wai Lam, John Blitzer, Hong Qi, Arda Celebi, Danyu
Liu, and Elliott Drabek. 2003. Evaluation challenges
in large-scale document summarization: the mead
project. In Proceedings of the 41st Annual Meeting
on Association for Computational Linguistics, pages
375–382.
A. Ritchie, S. Teufel, and S. Robertson. 2006. Creating
a test collection for citation-based ir experiments. In
Proceedings of the HLT-NAACL 2006, pages 391–398.
S. Robertson and K. Sparck Jones. 1976. Relevance
weighting of search terms. Journal of the American
Society for Information Science, 27:129–146.
Hpracop Saggion and Guy Lapalme. 2002. Generating
indicative-informative summaries with sumUM. Com-
putational Linguistics, 28(4):497–526.
A. S. Schwartz, A. Divoli, and M. A. Hearst. 2007. Mul-
tiple alignment of citation sentences with conditional
random fields and posterior decoding. In Proceedings
of the 2007 EMNLP-CoNLL, pages 847–857.
A. Siddharthan and S. Teufel. 2007. Whose idea was
this, and why does it matter? attributing scientific
work to citations. In Proceedings of NAACL/HLT-07,
pages 316–323.
Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang,
Yuchang Lu, and Zheng Chen. 2005. Web-page sum-
marization using clickthrough data. In Proceedings
of the 28th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 194–201.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28(4):409–445.
ChengXiang Zhai and John Lafferty. 2001a. Model-
based feedback in the language modeling approach
to information retrieval. In Proceedings of the Tenth
International Conference on Information and Knowl-
edge Management (CIKM 2001), pages 403–410.
Chengxiang Zhai and John Lafferty. 2001b. A study
of smoothing methods for language models applied to
ad hoc information retrieval. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 334–342.
</reference>
<page confidence="0.998719">
824
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.845598">
<title confidence="0.999719">Generating Impact-Based Summaries for Scientific Literature</title>
<author confidence="0.999383">Qiaozhu Mei ChengXiang Zhai</author>
<affiliation confidence="0.9799645">of Illinois at Urbana- University of Illinois at Urbana- Champaign Champaign</affiliation>
<email confidence="0.996467">qmei2@uiuc.educzhai@cs.uiuc.edu</email>
<abstract confidence="0.992278533333333">In this paper, we present a study of a novel summarization problem, i.e., summarizing the impact of a scientific publication. Given a paper and its citation context, we study how to extract sentences that can represent the most influential content of the paper. We propose language modeling methods for solving this problem, and study how to incorporate features such as authority and proximity to accurately estimate the impact language model. Experiment results on a SIGIR publication collection show that the proposed methods are effective for generating impact-based summaries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J-Y Delort</author>
<author>B Bouchon-Meunier</author>
<author>M Rifqi</author>
</authors>
<title>Enhanced web document summarization using hyperlinks.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fourteenth ACM Conference on Hypertext and Hypermedia,</booktitle>
<pages>208--215</pages>
<contexts>
<context position="31305" citStr="Delort et al., 2003" startWordPosition="5151" endWordPosition="5154">d Lapalme, 2002; Teufel and Moens, 2002), but the previous work did not explore citation context to emphasize the impact of papers. Citation context has been explored in several studies (Nakov et al., 2004; Ritchie et al., 2006; Schwartz et al., 2007; Siddharthan and Teufel, 2007). However, none of the previous studies has used citation context in the same way as we did, though the potential of directly using citation sentences (called citances) to summarize a paper was pointed out in (Nakov et al., 2004). Recently, people have explored various types of auxiliary knowledge such as hyperlinks (Delort et al., 2003) and clickthrough data (Sun et al., 2005), to summarize a webpage; such work is related to ours as anchor text is similar to citation context, but it is based on a standard formulation of multi-document summarization and would contain only sentences from anchor text. Our work is also related to work on using language models for retrieval (Ponte and Croft, 1998; Zhai and Lafferty, 2001b; Lafferty and Zhai, 2001) 7 Conclusions We have defined and studied the novel problem of summarizing the impact of a research paper. We cast the problem as an impact sentence retrieval problem, and proposed new </context>
</contexts>
<marker>Delort, Bouchon-Meunier, Rifqi, 2003</marker>
<rawString>J.-Y. Delort, B. Bouchon-Meunier, and M. Rifqi. 2003. Enhanced web document summarization using hyperlinks. In Proceedings of the Fourteenth ACM Conference on Hypertext and Hypermedia, pages 208–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lee Giles</author>
<author>Kurt D Bollacker</author>
<author>Steve Lawrence</author>
</authors>
<title>Citeseer: an automatic citation indexing system.</title>
<date>1998</date>
<booktitle>In Proceedings of the Third ACM Conference on Digital Libraries,</booktitle>
<pages>89--98</pages>
<contexts>
<context position="1517" citStr="Giles et al., 1998" startWordPosition="228" endWordPosition="231"> each year 400,000 new citations are added to MEDLINE, the major biomedical literature database 1. This fast growth of literature makes it difficult for researchers, especially beginning researchers, to keep track of the research trends and find high impact papers on unfamiliar topics. Impact factors (Kaplan and Nelson, 2000) are useful, but they are just numerical values, so they cannot tell researchers which aspects of a paper are influential. On the other hand, a regular contentbased summary (e.g., the abstract or conclusion section of a paper or an automatically generated topical summary (Giles et al., 1998)) can help a user know 1http://www.nlm.nih.gov/bsd/history/tsld024.htm about the main content of a paper, but not necessarily the most influential content of the paper. Indeed, the abstract of a paper mostly reflects the expected impact of the paper as perceived by the author(s), which could significantly deviate from the actual impact of the paper in the research community. Moreover, the impact of a paper changes over time due to the evolution and progress of research in a field. For example, an algorithm published a decade ago may be no longer the state of the art, but the problem definition</context>
</contexts>
<marker>Giles, Bollacker, Lawrence, 1998</marker>
<rawString>C. Lee Giles, Kurt D. Bollacker, and Steve Lawrence. 1998. Citeseer: an automatic citation indexing system. In Proceedings of the Third ACM Conference on Digital Libraries, pages 89–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Mark Kantrowitz</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
</authors>
<title>Summarizing text documents: sentence selection and evaluation metrics.</title>
<date>1999</date>
<booktitle>In Proceedings ofACMSIGIR 99,</booktitle>
<pages>121--128</pages>
<contexts>
<context position="29791" citStr="Goldstein et al., 1999" startWordPosition="4911" endWordPosition="4914">ameters. In general, for a wide range of values of these parameters, the performance is relatively stable and near optimal. Specifically, the performance is near optimal as 822 long as µs and µ, are sufficiently large (µs &gt; 1000, µ, &gt; 20, 000), and the interpolation parameter 6 is between 0.4 and 0.9. 6 Related Work and summarization (Kraaij et al., 2001). However, we do not have an explicit query and constructing the impact model is a novel exploration. We also proposed new language models to capture the impact. General text summarization, including single document summarization (Luhn, 1958; Goldstein et al., 1999) and multi-document summarization (Kraaij et al., 2001; Radev et al., 2003) has been well studied; our work is under the framework of extractive summarization (Luhn, 1958; McKeown and Radev, 1995; Goldstein et al., 1999; Kraaij et al., 2001), but our problem formulation differs from any existing formulation of the summarization problem. It differs from regular single-document summarization because we utilize extra information (i.e. citation contexts) to summarize the impact of a paper. It also differs from regular multi-document summarization because the roles of original documents and citatio</context>
</contexts>
<marker>Goldstein, Kantrowitz, Mittal, Carbonell, 1999</marker>
<rawString>Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and Jaime Carbonell. 1999. Summarizing text documents: sentence selection and evaluation metrics. In Proceedings ofACMSIGIR 99, pages 121–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy R Kaplan</author>
<author>Michael L Nelson</author>
</authors>
<title>Determining the publication impact of a digital library.</title>
<date>2000</date>
<journal>J. Am. Soc. Inf. Sci.,</journal>
<volume>51</volume>
<issue>4</issue>
<contexts>
<context position="1225" citStr="Kaplan and Nelson, 2000" startWordPosition="179" endWordPosition="182">proximity to accurately estimate the impact language model. Experiment results on a SIGIR publication collection show that the proposed methods are effective for generating impact-based summaries. 1 Introduction The volume of scientific literature has been growing rapidly. From recent statistics, each year 400,000 new citations are added to MEDLINE, the major biomedical literature database 1. This fast growth of literature makes it difficult for researchers, especially beginning researchers, to keep track of the research trends and find high impact papers on unfamiliar topics. Impact factors (Kaplan and Nelson, 2000) are useful, but they are just numerical values, so they cannot tell researchers which aspects of a paper are influential. On the other hand, a regular contentbased summary (e.g., the abstract or conclusion section of a paper or an automatically generated topical summary (Giles et al., 1998)) can help a user know 1http://www.nlm.nih.gov/bsd/history/tsld024.htm about the main content of a paper, but not necessarily the most influential content of the paper. Indeed, the abstract of a paper mostly reflects the expected impact of the paper as perceived by the author(s), which could significantly d</context>
</contexts>
<marker>Kaplan, Nelson, 2000</marker>
<rawString>Nancy R. Kaplan and Michael L. Nelson. 2000. Determining the publication impact of a digital library. J. Am. Soc. Inf. Sci., 51(4):324–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kraaij</author>
<author>M Spitters</author>
<author>M van der Heijden</author>
</authors>
<title>Combining a mixture language model and naive bayes for multi-document summarisation.</title>
<date>2001</date>
<booktitle>In Proceedings of the DUC2001</booktitle>
<pages>workshop.</pages>
<marker>Kraaij, Spitters, van der Heijden, 2001</marker>
<rawString>W. Kraaij, M. Spitters, and M. van der Heijden. 2001. Combining a mixture language model and naive bayes for multi-document summarisation. In Proceedings of the DUC2001 workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Chengxiang Zhai</author>
</authors>
<title>Document language models, query models, and risk minimization for information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings ofACM SIGIR</booktitle>
<pages>111--119</pages>
<contexts>
<context position="9718" citStr="Lafferty and Zhai, 2001" startWordPosition="1601" endWordPosition="1604">ry language model in ad hoc retrieval to assign high probabilities to words that tend to occur in relevant documents (Ponte and Croft, 1998). We call such a language model the impact language model of paper d (denoted as BI); it can be estimated based on both d and its citation context C as will be discussed in Section 4. 3.2 KL-divergence scoring With the impact language model in place, we can then adopt many existing probabilistic retrieval models such as the classical probabilistic retrieval models (Robertson and Sparck Jones, 1976) and the Kullback-Leibler (KL) divergence retrieval model (Lafferty and Zhai, 2001; Zhai and Lafferty, 2001a), to solve the problem of impact summarization by scoring sentences based on the estimated impact language model. In our study, we choose to use the KLdivergence scoring method to score sentences as this method has performed well for regular ad hoc retrieval tasks (Zhai and Lafferty, 2001a) and has an information theoretic interpretation. To apply the KL-divergence scoring method, we assume that a candidate sentence s is generated from a sentence language model Bs. Given s in d and the citation context C, we would first estimate Bs based on s and estimate BI based on</context>
<context position="31719" citStr="Lafferty and Zhai, 2001" startWordPosition="5221" endWordPosition="5224">ng citation sentences (called citances) to summarize a paper was pointed out in (Nakov et al., 2004). Recently, people have explored various types of auxiliary knowledge such as hyperlinks (Delort et al., 2003) and clickthrough data (Sun et al., 2005), to summarize a webpage; such work is related to ours as anchor text is similar to citation context, but it is based on a standard formulation of multi-document summarization and would contain only sentences from anchor text. Our work is also related to work on using language models for retrieval (Ponte and Croft, 1998; Zhai and Lafferty, 2001b; Lafferty and Zhai, 2001) 7 Conclusions We have defined and studied the novel problem of summarizing the impact of a research paper. We cast the problem as an impact sentence retrieval problem, and proposed new language models to model the impact of a paper based on both the original content of the paper and its citation contexts in a literature collection with consideration of citation autority and proximity. To evaluate impact summarization, we created a test set based on ACM SIGIR papers. Experiment results on this test set show that the proposed impact summarization methods are effective and outperform several bas</context>
</contexts>
<marker>Lafferty, Zhai, 2001</marker>
<rawString>John Lafferty and Chengxiang Zhai. 2001. Document language models, query models, and risk minimization for information retrieval. In Proceedings ofACM SIGIR 2001, pages 111–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="18477" citStr="Lin and Hovy, 2003" startWordPosition="3123" endWordPosition="3126">minimum length of 5 sentences and a maximum length of 18 sentences; the median length is 9 sentences. These 14 impact-based summaries are used as gold standards for our experiments, based on which all summaries generated by the system are evaluated. This data set is available at http://timan.cs.uiuc.edu/data/impact.html. We must admit that using only 14 papers and only one expert for evaluation is a limitation of our work. However, 2http://www.acm.org/dl 5.1.2 Evaluation Metrics Following the current practice in evaluating summarization, particularly DUC3, we use the ROUGE evaluation package (Lin and Hovy, 2003). Among ROUGE metrics, ROUGE-N (models n-gram cooccurrence, N = 1, 2) and ROUGE-L (models longest common sequence) generally perform well in evaluating both single-document summarization and multi-document summarization (Lin and Hovy, 2003). Since they are general evaluation measures for summarization, they are also applicable to evaluating the MEAD-Doc+Cite baseline method to be described below. Thus although we evaluated our methods with all the metrics provided by ROUGE, we only report ROUGE-1 and ROUGE-L in this paper (other metrics give very similar results). 5.1.3 Baseline methods Since </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBMJournal of Research and Development,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="3186" citStr="Luhn, 1958" startWordPosition="510" endWordPosition="511">y, especially by people who cited it. Thus in order to generate an impact-based summary, we must use not only the original content, but also the descriptions of that paper provided in papers which cited it, making it a challenging task and different from a regular summarization setup such as news summarization. Indeed, unlike a regular summarization system which identifies and interprets the topic of a document, an impact summarization system should identify and interpret the impact of a paper. We define the impact summarization problem in the framework of extraction-based text summarization (Luhn, 1958; McKeown and Radev, 1995), and cast the problem as an impact sentence retrieval 816 Proceedings of ACL-08: HLT, pages 816–824, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics problem. We propose language models to exploit both the citation context and original content of a paper to generate an impact-based summary. We study how to incorporate features such as authority and proximity into the estimation of language models. We propose and evaluate several different strategies for estimating the impact language model, which is key to impact summarization. No exis</context>
<context position="29766" citStr="Luhn, 1958" startWordPosition="4909" endWordPosition="4910">to these parameters. In general, for a wide range of values of these parameters, the performance is relatively stable and near optimal. Specifically, the performance is near optimal as 822 long as µs and µ, are sufficiently large (µs &gt; 1000, µ, &gt; 20, 000), and the interpolation parameter 6 is between 0.4 and 0.9. 6 Related Work and summarization (Kraaij et al., 2001). However, we do not have an explicit query and constructing the impact model is a novel exploration. We also proposed new language models to capture the impact. General text summarization, including single document summarization (Luhn, 1958; Goldstein et al., 1999) and multi-document summarization (Kraaij et al., 2001; Radev et al., 2003) has been well studied; our work is under the framework of extractive summarization (Luhn, 1958; McKeown and Radev, 1995; Goldstein et al., 1999; Kraaij et al., 2001), but our problem formulation differs from any existing formulation of the summarization problem. It differs from regular single-document summarization because we utilize extra information (i.e. citation contexts) to summarize the impact of a paper. It also differs from regular multi-document summarization because the roles of origi</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H. P. Luhn. 1958. The automatic creation of literature abstracts. IBMJournal of Research and Development, 2(2):159–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D MacKay</author>
<author>L Peto</author>
</authors>
<title>A hierarchical Dirichlet language model.</title>
<date>1995</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<issue>3</issue>
<contexts>
<context position="12887" citStr="MacKay and Peto, 1995" startWordPosition="2164" endWordPosition="2167">he citation context C as prior information and the current document d as the observed data, and use Bayesian estimation to estimate the impact language model. Specifically, let p(w|C) be a citation context language model estimated based on the citation context C. We define Dirichlet prior with parameters 1µCp(w|C)�w∈V for the impact model, where µC encodes our confidence on this prior and effectively serves as a weighting parameter for balancing the contribution of C and d for estimating the impact model. Given the observed document d, the posterior mean estimate of the impact model would be (MacKay and Peto, 1995; Zhai and Lafferty, 2001b) c(w, d) + µcp(w|C) P(w|θI) = (2) |d |+ µc µc can be interpreted as the equivalent sample size of our prior. Thus setting µc = |d |means that we put equal weights on the citation context and the document itself. µc = 0 yields p(w|θI) = p(w|d), which is to say that the impact is entirely captured by the paper itself, and our impact summarization problem would then become the standard single document (topical) summarization. Intuitively though, we would want to set µc to a relatively large number to exploit the citation context in our estimation, which is confirmed in </context>
</contexts>
<marker>MacKay, Peto, 1995</marker>
<rawString>D. MacKay and L. Peto. 1995. A hierarchical Dirichlet language model. Natural Language Engineering, 1(3):289–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
<author>Dragomir R Radev</author>
</authors>
<title>Generating summaries of multiple news articles.</title>
<date>1995</date>
<booktitle>In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>74--82</pages>
<contexts>
<context position="3212" citStr="McKeown and Radev, 1995" startWordPosition="512" endWordPosition="515">y by people who cited it. Thus in order to generate an impact-based summary, we must use not only the original content, but also the descriptions of that paper provided in papers which cited it, making it a challenging task and different from a regular summarization setup such as news summarization. Indeed, unlike a regular summarization system which identifies and interprets the topic of a document, an impact summarization system should identify and interpret the impact of a paper. We define the impact summarization problem in the framework of extraction-based text summarization (Luhn, 1958; McKeown and Radev, 1995), and cast the problem as an impact sentence retrieval 816 Proceedings of ACL-08: HLT, pages 816–824, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics problem. We propose language models to exploit both the citation context and original content of a paper to generate an impact-based summary. We study how to incorporate features such as authority and proximity into the estimation of language models. We propose and evaluate several different strategies for estimating the impact language model, which is key to impact summarization. No existing test collection is av</context>
<context position="29986" citStr="McKeown and Radev, 1995" startWordPosition="4942" endWordPosition="4945">are sufficiently large (µs &gt; 1000, µ, &gt; 20, 000), and the interpolation parameter 6 is between 0.4 and 0.9. 6 Related Work and summarization (Kraaij et al., 2001). However, we do not have an explicit query and constructing the impact model is a novel exploration. We also proposed new language models to capture the impact. General text summarization, including single document summarization (Luhn, 1958; Goldstein et al., 1999) and multi-document summarization (Kraaij et al., 2001; Radev et al., 2003) has been well studied; our work is under the framework of extractive summarization (Luhn, 1958; McKeown and Radev, 1995; Goldstein et al., 1999; Kraaij et al., 2001), but our problem formulation differs from any existing formulation of the summarization problem. It differs from regular single-document summarization because we utilize extra information (i.e. citation contexts) to summarize the impact of a paper. It also differs from regular multi-document summarization because the roles of original documents and citation contexts are not equivalent. Specifically, citation contexts serve as an indicator of the impact of the paper, but the summary is generated by extracting the sentences from the original paper. </context>
</contexts>
<marker>McKeown, Radev, 1995</marker>
<rawString>Kathleen McKeown and Dragomir R. Radev. 1995. Generating summaries of multiple news articles. In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 74–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nakov</author>
<author>A Schwartz</author>
<author>M Hearst</author>
</authors>
<title>Citances: Citation sentences for semantic analysis of bioscience text.</title>
<date>2004</date>
<booktitle>In Proceedings of ACM SIGIR’04 Workshop on Search and Discovery in Bioinformatics.</booktitle>
<contexts>
<context position="30890" citStr="Nakov et al., 2004" startWordPosition="5083" endWordPosition="5086"> a paper. It also differs from regular multi-document summarization because the roles of original documents and citation contexts are not equivalent. Specifically, citation contexts serve as an indicator of the impact of the paper, but the summary is generated by extracting the sentences from the original paper. Technical paper summarization has also been studied (Paice, 1981; Paice and Jones, 1993; Saggion and Lapalme, 2002; Teufel and Moens, 2002), but the previous work did not explore citation context to emphasize the impact of papers. Citation context has been explored in several studies (Nakov et al., 2004; Ritchie et al., 2006; Schwartz et al., 2007; Siddharthan and Teufel, 2007). However, none of the previous studies has used citation context in the same way as we did, though the potential of directly using citation sentences (called citances) to summarize a paper was pointed out in (Nakov et al., 2004). Recently, people have explored various types of auxiliary knowledge such as hyperlinks (Delort et al., 2003) and clickthrough data (Sun et al., 2005), to summarize a webpage; such work is related to ours as anchor text is similar to citation context, but it is based on a standard formulation </context>
</contexts>
<marker>Nakov, Schwartz, Hearst, 2004</marker>
<rawString>P. Nakov, A. Schwartz, and M. Hearst. 2004. Citances: Citation sentences for semantic analysis of bioscience text. In Proceedings of ACM SIGIR’04 Workshop on Search and Discovery in Bioinformatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris D Paice</author>
<author>Paul A Jones</author>
</authors>
<title>The identification of important concepts in highly structured technical papers.</title>
<date>1993</date>
<booktitle>In Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>69--78</pages>
<contexts>
<context position="5280" citStr="Paice and Jones, 1993" startWordPosition="828" endWordPosition="831">xt exists, such as emails and weblogs. The rest of the paper is organized as follows. In Section 2 and 3, we define the impact-based summarization problem and propose the general language modeling approach. In Section 4, we present different strategies and features for estimating an impact language model, a key challenge in impact summarization. We discuss our experiments and results in Section 5. Finally, the related work and conclusions are discussed in Section 6 and Section 7. 2 Impact Summarization Following the existing work on topical summarization of scientific literature (Paice, 1981; Paice and Jones, 1993), we define an impact-based summary of a paper as a set of sentences extracted from a paper that can reflect the impact of the paper, where “impact” is roughly defined as the influence of the paper on research of similar or related topics as reflected in the citations of the paper. Such an extraction-based definition of summarization has also been quite common in most existing general summarization work (Radev et al., 2002). By definition, in order to generate an impact summary of a paper, we must look at how other papers cite the paper, use this information to infer the impact of the paper, a</context>
<context position="30673" citStr="Paice and Jones, 1993" startWordPosition="5046" endWordPosition="5049">rmulation differs from any existing formulation of the summarization problem. It differs from regular single-document summarization because we utilize extra information (i.e. citation contexts) to summarize the impact of a paper. It also differs from regular multi-document summarization because the roles of original documents and citation contexts are not equivalent. Specifically, citation contexts serve as an indicator of the impact of the paper, but the summary is generated by extracting the sentences from the original paper. Technical paper summarization has also been studied (Paice, 1981; Paice and Jones, 1993; Saggion and Lapalme, 2002; Teufel and Moens, 2002), but the previous work did not explore citation context to emphasize the impact of papers. Citation context has been explored in several studies (Nakov et al., 2004; Ritchie et al., 2006; Schwartz et al., 2007; Siddharthan and Teufel, 2007). However, none of the previous studies has used citation context in the same way as we did, though the potential of directly using citation sentences (called citances) to summarize a paper was pointed out in (Nakov et al., 2004). Recently, people have explored various types of auxiliary knowledge such as </context>
</contexts>
<marker>Paice, Jones, 1993</marker>
<rawString>Chris D. Paice and Paul A. Jones. 1993. The identification of important concepts in highly structured technical papers. In Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 69–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Paice</author>
</authors>
<title>The automatic generation of literature abstracts: an approach based on the identification of self-indicating phrases.</title>
<date>1981</date>
<booktitle>In Proceedings of the 3rd Annual ACM Conference on Research and Development in Information Retrieval,</booktitle>
<pages>172--191</pages>
<contexts>
<context position="5256" citStr="Paice, 1981" startWordPosition="826" endWordPosition="827">itation context exists, such as emails and weblogs. The rest of the paper is organized as follows. In Section 2 and 3, we define the impact-based summarization problem and propose the general language modeling approach. In Section 4, we present different strategies and features for estimating an impact language model, a key challenge in impact summarization. We discuss our experiments and results in Section 5. Finally, the related work and conclusions are discussed in Section 6 and Section 7. 2 Impact Summarization Following the existing work on topical summarization of scientific literature (Paice, 1981; Paice and Jones, 1993), we define an impact-based summary of a paper as a set of sentences extracted from a paper that can reflect the impact of the paper, where “impact” is roughly defined as the influence of the paper on research of similar or related topics as reflected in the citations of the paper. Such an extraction-based definition of summarization has also been quite common in most existing general summarization work (Radev et al., 2002). By definition, in order to generate an impact summary of a paper, we must look at how other papers cite the paper, use this information to infer th</context>
<context position="30650" citStr="Paice, 1981" startWordPosition="5044" endWordPosition="5045">ur problem formulation differs from any existing formulation of the summarization problem. It differs from regular single-document summarization because we utilize extra information (i.e. citation contexts) to summarize the impact of a paper. It also differs from regular multi-document summarization because the roles of original documents and citation contexts are not equivalent. Specifically, citation contexts serve as an indicator of the impact of the paper, but the summary is generated by extracting the sentences from the original paper. Technical paper summarization has also been studied (Paice, 1981; Paice and Jones, 1993; Saggion and Lapalme, 2002; Teufel and Moens, 2002), but the previous work did not explore citation context to emphasize the impact of papers. Citation context has been explored in several studies (Nakov et al., 2004; Ritchie et al., 2006; Schwartz et al., 2007; Siddharthan and Teufel, 2007). However, none of the previous studies has used citation context in the same way as we did, though the potential of directly using citation sentences (called citances) to summarize a paper was pointed out in (Nakov et al., 2004). Recently, people have explored various types of auxil</context>
</contexts>
<marker>Paice, 1981</marker>
<rawString>C. D. Paice. 1981. The automatic generation of literature abstracts: an approach based on the identification of self-indicating phrases. In Proceedings of the 3rd Annual ACM Conference on Research and Development in Information Retrieval, pages 172–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay M Ponte</author>
<author>W Bruce Croft</author>
</authors>
<title>A language modeling approach to information retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>275--281</pages>
<contexts>
<context position="9235" citStr="Ponte and Croft, 1998" startWordPosition="1523" endWordPosition="1526">ery. Thus the main challenge in impact summarization is to effectively construct a “virtual impact query” based on the citation contexts. What should such a virtual impact query look like? Intuitively, it should model the impactreflecting content of the paper. We thus propose to represent such a virtual impact query with a unigram language model. Such a model is expected to assign high probabilities to those words that can describe the impact of paper d, just as we expect a query language model in ad hoc retrieval to assign high probabilities to words that tend to occur in relevant documents (Ponte and Croft, 1998). We call such a language model the impact language model of paper d (denoted as BI); it can be estimated based on both d and its citation context C as will be discussed in Section 4. 3.2 KL-divergence scoring With the impact language model in place, we can then adopt many existing probabilistic retrieval models such as the classical probabilistic retrieval models (Robertson and Sparck Jones, 1976) and the Kullback-Leibler (KL) divergence retrieval model (Lafferty and Zhai, 2001; Zhai and Lafferty, 2001a), to solve the problem of impact summarization by scoring sentences based on the estimated</context>
<context position="31667" citStr="Ponte and Croft, 1998" startWordPosition="5213" endWordPosition="5216">y as we did, though the potential of directly using citation sentences (called citances) to summarize a paper was pointed out in (Nakov et al., 2004). Recently, people have explored various types of auxiliary knowledge such as hyperlinks (Delort et al., 2003) and clickthrough data (Sun et al., 2005), to summarize a webpage; such work is related to ours as anchor text is similar to citation context, but it is based on a standard formulation of multi-document summarization and would contain only sentences from anchor text. Our work is also related to work on using language models for retrieval (Ponte and Croft, 1998; Zhai and Lafferty, 2001b; Lafferty and Zhai, 2001) 7 Conclusions We have defined and studied the novel problem of summarizing the impact of a research paper. We cast the problem as an impact sentence retrieval problem, and proposed new language models to model the impact of a paper based on both the original content of the paper and its citation contexts in a literature collection with consideration of citation autority and proximity. To evaluate impact summarization, we created a test set based on ACM SIGIR papers. Experiment results on this test set show that the proposed impact summarizat</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>Jay M. Ponte and W. Bruce Croft. 1998. A language modeling approach to information retrieval. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Eduard Hovy</author>
<author>Kathleen McKeown</author>
</authors>
<title>Introduction to the special issue on summarization.</title>
<date>2002</date>
<journal>Comput. Linguist.,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="5707" citStr="Radev et al., 2002" startWordPosition="901" endWordPosition="904"> conclusions are discussed in Section 6 and Section 7. 2 Impact Summarization Following the existing work on topical summarization of scientific literature (Paice, 1981; Paice and Jones, 1993), we define an impact-based summary of a paper as a set of sentences extracted from a paper that can reflect the impact of the paper, where “impact” is roughly defined as the influence of the paper on research of similar or related topics as reflected in the citations of the paper. Such an extraction-based definition of summarization has also been quite common in most existing general summarization work (Radev et al., 2002). By definition, in order to generate an impact summary of a paper, we must look at how other papers cite the paper, use this information to infer the impact of the paper, and select sentences from the original paper that can reflect the inferred impact. Note that we do not directly use the sentences from the citation context to form a summary. This is because in citations, the discussion of the paper cited is usually mixed with the content of the paper citing it, and sometimes also with discussion about other papers cited (Siddharthan and Teufel, 2007). Formally, let d = (so, si, ..., s,,,) b</context>
</contexts>
<marker>Radev, Hovy, McKeown, 2002</marker>
<rawString>Dragomir R. Radev, Eduard Hovy, and Kathleen McKeown. 2002. Introduction to the special issue on summarization. Comput. Linguist., 28(4):399–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Simone Teufel</author>
<author>Horacio Saggion</author>
<author>Wai Lam</author>
<author>John Blitzer</author>
<author>Hong Qi</author>
<author>Arda Celebi</author>
<author>Danyu Liu</author>
<author>Elliott Drabek</author>
</authors>
<title>Evaluation challenges in large-scale document summarization: the mead project.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>375--382</pages>
<contexts>
<context position="19307" citStr="Radev et al., 2003" startWordPosition="3246" endWordPosition="3249">rization (Lin and Hovy, 2003). Since they are general evaluation measures for summarization, they are also applicable to evaluating the MEAD-Doc+Cite baseline method to be described below. Thus although we evaluated our methods with all the metrics provided by ROUGE, we only report ROUGE-1 and ROUGE-L in this paper (other metrics give very similar results). 5.1.3 Baseline methods Since impact summarization has not been previously studied, there is no natural baseline method to compare with. We thus adapt some state-of-the-art conventional summarization methods implemented in the MEAD toolkit (Radev et al., 2003)4 to obtain three baseline methods: (1) LEAD: It simply extracts sentences from the beginning of a paper, i.e., sentences in the abstract or beginning of the introduction section; we include LEAD to see if such “leading sentences” reflect the impact of a paper as authors presumably would expect to summarize a paper’s contributions in the abstract. (2) MEADDoc: It uses the single-document summarizer in MEAD to generate a summary based solely on the original paper; comparison with this baseline can tell us how much better we can do than a conventional topic-based summarizer that does not conside</context>
<context position="29866" citStr="Radev et al., 2003" startWordPosition="4922" endWordPosition="4925">mance is relatively stable and near optimal. Specifically, the performance is near optimal as 822 long as µs and µ, are sufficiently large (µs &gt; 1000, µ, &gt; 20, 000), and the interpolation parameter 6 is between 0.4 and 0.9. 6 Related Work and summarization (Kraaij et al., 2001). However, we do not have an explicit query and constructing the impact model is a novel exploration. We also proposed new language models to capture the impact. General text summarization, including single document summarization (Luhn, 1958; Goldstein et al., 1999) and multi-document summarization (Kraaij et al., 2001; Radev et al., 2003) has been well studied; our work is under the framework of extractive summarization (Luhn, 1958; McKeown and Radev, 1995; Goldstein et al., 1999; Kraaij et al., 2001), but our problem formulation differs from any existing formulation of the summarization problem. It differs from regular single-document summarization because we utilize extra information (i.e. citation contexts) to summarize the impact of a paper. It also differs from regular multi-document summarization because the roles of original documents and citation contexts are not equivalent. Specifically, citation contexts serve as an </context>
</contexts>
<marker>Radev, Teufel, Saggion, Lam, Blitzer, Qi, Celebi, Liu, Drabek, 2003</marker>
<rawString>Dragomir R. Radev, Simone Teufel, Horacio Saggion, Wai Lam, John Blitzer, Hong Qi, Arda Celebi, Danyu Liu, and Elliott Drabek. 2003. Evaluation challenges in large-scale document summarization: the mead project. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 375–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritchie</author>
<author>S Teufel</author>
<author>S Robertson</author>
</authors>
<title>Creating a test collection for citation-based ir experiments.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL</booktitle>
<pages>391--398</pages>
<contexts>
<context position="30912" citStr="Ritchie et al., 2006" startWordPosition="5087" endWordPosition="5090">ffers from regular multi-document summarization because the roles of original documents and citation contexts are not equivalent. Specifically, citation contexts serve as an indicator of the impact of the paper, but the summary is generated by extracting the sentences from the original paper. Technical paper summarization has also been studied (Paice, 1981; Paice and Jones, 1993; Saggion and Lapalme, 2002; Teufel and Moens, 2002), but the previous work did not explore citation context to emphasize the impact of papers. Citation context has been explored in several studies (Nakov et al., 2004; Ritchie et al., 2006; Schwartz et al., 2007; Siddharthan and Teufel, 2007). However, none of the previous studies has used citation context in the same way as we did, though the potential of directly using citation sentences (called citances) to summarize a paper was pointed out in (Nakov et al., 2004). Recently, people have explored various types of auxiliary knowledge such as hyperlinks (Delort et al., 2003) and clickthrough data (Sun et al., 2005), to summarize a webpage; such work is related to ours as anchor text is similar to citation context, but it is based on a standard formulation of multi-document summ</context>
</contexts>
<marker>Ritchie, Teufel, Robertson, 2006</marker>
<rawString>A. Ritchie, S. Teufel, and S. Robertson. 2006. Creating a test collection for citation-based ir experiments. In Proceedings of the HLT-NAACL 2006, pages 391–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Robertson</author>
<author>K Sparck Jones</author>
</authors>
<title>Relevance weighting of search terms.</title>
<date>1976</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>27--129</pages>
<marker>Robertson, Jones, 1976</marker>
<rawString>S. Robertson and K. Sparck Jones. 1976. Relevance weighting of search terms. Journal of the American Society for Information Science, 27:129–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hpracop Saggion</author>
<author>Guy Lapalme</author>
</authors>
<title>Generating indicative-informative summaries with sumUM.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="30700" citStr="Saggion and Lapalme, 2002" startWordPosition="5050" endWordPosition="5054">any existing formulation of the summarization problem. It differs from regular single-document summarization because we utilize extra information (i.e. citation contexts) to summarize the impact of a paper. It also differs from regular multi-document summarization because the roles of original documents and citation contexts are not equivalent. Specifically, citation contexts serve as an indicator of the impact of the paper, but the summary is generated by extracting the sentences from the original paper. Technical paper summarization has also been studied (Paice, 1981; Paice and Jones, 1993; Saggion and Lapalme, 2002; Teufel and Moens, 2002), but the previous work did not explore citation context to emphasize the impact of papers. Citation context has been explored in several studies (Nakov et al., 2004; Ritchie et al., 2006; Schwartz et al., 2007; Siddharthan and Teufel, 2007). However, none of the previous studies has used citation context in the same way as we did, though the potential of directly using citation sentences (called citances) to summarize a paper was pointed out in (Nakov et al., 2004). Recently, people have explored various types of auxiliary knowledge such as hyperlinks (Delort et al., </context>
</contexts>
<marker>Saggion, Lapalme, 2002</marker>
<rawString>Hpracop Saggion and Guy Lapalme. 2002. Generating indicative-informative summaries with sumUM. Computational Linguistics, 28(4):497–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Schwartz</author>
<author>A Divoli</author>
<author>M A Hearst</author>
</authors>
<title>Multiple alignment of citation sentences with conditional random fields and posterior decoding.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 EMNLP-CoNLL,</booktitle>
<pages>847--857</pages>
<contexts>
<context position="30935" citStr="Schwartz et al., 2007" startWordPosition="5091" endWordPosition="5094">ti-document summarization because the roles of original documents and citation contexts are not equivalent. Specifically, citation contexts serve as an indicator of the impact of the paper, but the summary is generated by extracting the sentences from the original paper. Technical paper summarization has also been studied (Paice, 1981; Paice and Jones, 1993; Saggion and Lapalme, 2002; Teufel and Moens, 2002), but the previous work did not explore citation context to emphasize the impact of papers. Citation context has been explored in several studies (Nakov et al., 2004; Ritchie et al., 2006; Schwartz et al., 2007; Siddharthan and Teufel, 2007). However, none of the previous studies has used citation context in the same way as we did, though the potential of directly using citation sentences (called citances) to summarize a paper was pointed out in (Nakov et al., 2004). Recently, people have explored various types of auxiliary knowledge such as hyperlinks (Delort et al., 2003) and clickthrough data (Sun et al., 2005), to summarize a webpage; such work is related to ours as anchor text is similar to citation context, but it is based on a standard formulation of multi-document summarization and would con</context>
</contexts>
<marker>Schwartz, Divoli, Hearst, 2007</marker>
<rawString>A. S. Schwartz, A. Divoli, and M. A. Hearst. 2007. Multiple alignment of citation sentences with conditional random fields and posterior decoding. In Proceedings of the 2007 EMNLP-CoNLL, pages 847–857.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Siddharthan</author>
<author>S Teufel</author>
</authors>
<title>Whose idea was this, and why does it matter? attributing scientific work to citations.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL/HLT-07,</booktitle>
<pages>316--323</pages>
<contexts>
<context position="6266" citStr="Siddharthan and Teufel, 2007" startWordPosition="1002" endWordPosition="1005">ommon in most existing general summarization work (Radev et al., 2002). By definition, in order to generate an impact summary of a paper, we must look at how other papers cite the paper, use this information to infer the impact of the paper, and select sentences from the original paper that can reflect the inferred impact. Note that we do not directly use the sentences from the citation context to form a summary. This is because in citations, the discussion of the paper cited is usually mixed with the content of the paper citing it, and sometimes also with discussion about other papers cited (Siddharthan and Teufel, 2007). Formally, let d = (so, si, ..., s,,,) be a paper to be summarized, where sz is a sentence. We refer to a sentence (in another paper) in which there is an explicit citation of d as a citing sentence of d. When a paper is cited, it is often discussed consecutively in more than one sentence near the citation, thus intuitively we would like to consider a window of sentences centered at a citing sentence; the window size would be a parameter to set. We call such a window of sentences a citation context, and use C to denote the union of all the citation contexts of d in a collection of research pa</context>
<context position="30966" citStr="Siddharthan and Teufel, 2007" startWordPosition="5095" endWordPosition="5098">on because the roles of original documents and citation contexts are not equivalent. Specifically, citation contexts serve as an indicator of the impact of the paper, but the summary is generated by extracting the sentences from the original paper. Technical paper summarization has also been studied (Paice, 1981; Paice and Jones, 1993; Saggion and Lapalme, 2002; Teufel and Moens, 2002), but the previous work did not explore citation context to emphasize the impact of papers. Citation context has been explored in several studies (Nakov et al., 2004; Ritchie et al., 2006; Schwartz et al., 2007; Siddharthan and Teufel, 2007). However, none of the previous studies has used citation context in the same way as we did, though the potential of directly using citation sentences (called citances) to summarize a paper was pointed out in (Nakov et al., 2004). Recently, people have explored various types of auxiliary knowledge such as hyperlinks (Delort et al., 2003) and clickthrough data (Sun et al., 2005), to summarize a webpage; such work is related to ours as anchor text is similar to citation context, but it is based on a standard formulation of multi-document summarization and would contain only sentences from anchor</context>
</contexts>
<marker>Siddharthan, Teufel, 2007</marker>
<rawString>A. Siddharthan and S. Teufel. 2007. Whose idea was this, and why does it matter? attributing scientific work to citations. In Proceedings of NAACL/HLT-07, pages 316–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian-Tao Sun</author>
<author>Dou Shen</author>
<author>Hua-Jun Zeng</author>
<author>Qiang Yang</author>
<author>Yuchang Lu</author>
<author>Zheng Chen</author>
</authors>
<title>Web-page summarization using clickthrough data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>194--201</pages>
<contexts>
<context position="31346" citStr="Sun et al., 2005" startWordPosition="5158" endWordPosition="5161">t the previous work did not explore citation context to emphasize the impact of papers. Citation context has been explored in several studies (Nakov et al., 2004; Ritchie et al., 2006; Schwartz et al., 2007; Siddharthan and Teufel, 2007). However, none of the previous studies has used citation context in the same way as we did, though the potential of directly using citation sentences (called citances) to summarize a paper was pointed out in (Nakov et al., 2004). Recently, people have explored various types of auxiliary knowledge such as hyperlinks (Delort et al., 2003) and clickthrough data (Sun et al., 2005), to summarize a webpage; such work is related to ours as anchor text is similar to citation context, but it is based on a standard formulation of multi-document summarization and would contain only sentences from anchor text. Our work is also related to work on using language models for retrieval (Ponte and Croft, 1998; Zhai and Lafferty, 2001b; Lafferty and Zhai, 2001) 7 Conclusions We have defined and studied the novel problem of summarizing the impact of a research paper. We cast the problem as an impact sentence retrieval problem, and proposed new language models to model the impact of a </context>
</contexts>
<marker>Sun, Shen, Zeng, Yang, Lu, Chen, 2005</marker>
<rawString>Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang, Yuchang Lu, and Zheng Chen. 2005. Web-page summarization using clickthrough data. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 194–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Summarizing scientific articles: experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Comput. Linguist.,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="30725" citStr="Teufel and Moens, 2002" startWordPosition="5055" endWordPosition="5058"> the summarization problem. It differs from regular single-document summarization because we utilize extra information (i.e. citation contexts) to summarize the impact of a paper. It also differs from regular multi-document summarization because the roles of original documents and citation contexts are not equivalent. Specifically, citation contexts serve as an indicator of the impact of the paper, but the summary is generated by extracting the sentences from the original paper. Technical paper summarization has also been studied (Paice, 1981; Paice and Jones, 1993; Saggion and Lapalme, 2002; Teufel and Moens, 2002), but the previous work did not explore citation context to emphasize the impact of papers. Citation context has been explored in several studies (Nakov et al., 2004; Ritchie et al., 2006; Schwartz et al., 2007; Siddharthan and Teufel, 2007). However, none of the previous studies has used citation context in the same way as we did, though the potential of directly using citation sentences (called citances) to summarize a paper was pointed out in (Nakov et al., 2004). Recently, people have explored various types of auxiliary knowledge such as hyperlinks (Delort et al., 2003) and clickthrough da</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>Simone Teufel and Marc Moens. 2002. Summarizing scientific articles: experiments with relevance and rhetorical status. Comput. Linguist., 28(4):409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ChengXiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>Modelbased feedback in the language modeling approach to information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of the Tenth International Conference on Information and Knowledge Management (CIKM</booktitle>
<pages>403--410</pages>
<contexts>
<context position="9743" citStr="Zhai and Lafferty, 2001" startWordPosition="1605" endWordPosition="1608">oc retrieval to assign high probabilities to words that tend to occur in relevant documents (Ponte and Croft, 1998). We call such a language model the impact language model of paper d (denoted as BI); it can be estimated based on both d and its citation context C as will be discussed in Section 4. 3.2 KL-divergence scoring With the impact language model in place, we can then adopt many existing probabilistic retrieval models such as the classical probabilistic retrieval models (Robertson and Sparck Jones, 1976) and the Kullback-Leibler (KL) divergence retrieval model (Lafferty and Zhai, 2001; Zhai and Lafferty, 2001a), to solve the problem of impact summarization by scoring sentences based on the estimated impact language model. In our study, we choose to use the KLdivergence scoring method to score sentences as this method has performed well for regular ad hoc retrieval tasks (Zhai and Lafferty, 2001a) and has an information theoretic interpretation. To apply the KL-divergence scoring method, we assume that a candidate sentence s is generated from a sentence language model Bs. Given s in d and the citation context C, we would first estimate Bs based on s and estimate BI based on C, and then score s with</context>
<context position="11234" citStr="Zhai and Lafferty, 2001" startWordPosition="1868" endWordPosition="1871">f bits wasted in compressing messages generated according to BI (i.e., impact descriptions) with coding nonoptimally designed based on Bs. If Bs and BI are very close, the KL-divergence would be small and Score(s) would be high, which intuitively makes sense. Note that the second term (entropy of BI) is independent of s, so it can be ignored for ranking s. We see that according to the KL-divergence scoring method, our main tasks are to estimate Bs and BI. Since s can be regarded as a short document, we can use any standard method to estimate Bs. In this work, we use Dirichlet prior smoothing (Zhai and Lafferty, 2001b) to estimate Bs as follows: c(w, s) + µs * P(w|D) p(w|Bs) = (1) |s |+ µs where |s |is the length of s, c(w, s) is the count of word w in s, p(w|D) is a background model estimated using c�w,D) ����� c�w�,D) (D can be the set of all the papers available to us) and µs is a smoothing parameter to be empirically set. Note that as the length of a sentence is very short, smoothing is critical for addressing the data sparseness problem. The remaining challenge is to estimate BI accurately based on d and its citation contexts. 4 Estimation of Impact Language Models Intuitively, the impact of a paper </context>
<context position="12912" citStr="Zhai and Lafferty, 2001" startWordPosition="2168" endWordPosition="2171">s prior information and the current document d as the observed data, and use Bayesian estimation to estimate the impact language model. Specifically, let p(w|C) be a citation context language model estimated based on the citation context C. We define Dirichlet prior with parameters 1µCp(w|C)�w∈V for the impact model, where µC encodes our confidence on this prior and effectively serves as a weighting parameter for balancing the contribution of C and d for estimating the impact model. Given the observed document d, the posterior mean estimate of the impact model would be (MacKay and Peto, 1995; Zhai and Lafferty, 2001b) c(w, d) + µcp(w|C) P(w|θI) = (2) |d |+ µc µc can be interpreted as the equivalent sample size of our prior. Thus setting µc = |d |means that we put equal weights on the citation context and the document itself. µc = 0 yields p(w|θI) = p(w|d), which is to say that the impact is entirely captured by the paper itself, and our impact summarization problem would then become the standard single document (topical) summarization. Intuitively though, we would want to set µc to a relatively large number to exploit the citation context in our estimation, which is confirmed in our experiments. An alter</context>
<context position="31692" citStr="Zhai and Lafferty, 2001" startWordPosition="5217" endWordPosition="5220"> potential of directly using citation sentences (called citances) to summarize a paper was pointed out in (Nakov et al., 2004). Recently, people have explored various types of auxiliary knowledge such as hyperlinks (Delort et al., 2003) and clickthrough data (Sun et al., 2005), to summarize a webpage; such work is related to ours as anchor text is similar to citation context, but it is based on a standard formulation of multi-document summarization and would contain only sentences from anchor text. Our work is also related to work on using language models for retrieval (Ponte and Croft, 1998; Zhai and Lafferty, 2001b; Lafferty and Zhai, 2001) 7 Conclusions We have defined and studied the novel problem of summarizing the impact of a research paper. We cast the problem as an impact sentence retrieval problem, and proposed new language models to model the impact of a paper based on both the original content of the paper and its citation contexts in a literature collection with consideration of citation autority and proximity. To evaluate impact summarization, we created a test set based on ACM SIGIR papers. Experiment results on this test set show that the proposed impact summarization methods are effective</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>ChengXiang Zhai and John Lafferty. 2001a. Modelbased feedback in the language modeling approach to information retrieval. In Proceedings of the Tenth International Conference on Information and Knowledge Management (CIKM 2001), pages 403–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to ad hoc information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>334--342</pages>
<contexts>
<context position="9743" citStr="Zhai and Lafferty, 2001" startWordPosition="1605" endWordPosition="1608">oc retrieval to assign high probabilities to words that tend to occur in relevant documents (Ponte and Croft, 1998). We call such a language model the impact language model of paper d (denoted as BI); it can be estimated based on both d and its citation context C as will be discussed in Section 4. 3.2 KL-divergence scoring With the impact language model in place, we can then adopt many existing probabilistic retrieval models such as the classical probabilistic retrieval models (Robertson and Sparck Jones, 1976) and the Kullback-Leibler (KL) divergence retrieval model (Lafferty and Zhai, 2001; Zhai and Lafferty, 2001a), to solve the problem of impact summarization by scoring sentences based on the estimated impact language model. In our study, we choose to use the KLdivergence scoring method to score sentences as this method has performed well for regular ad hoc retrieval tasks (Zhai and Lafferty, 2001a) and has an information theoretic interpretation. To apply the KL-divergence scoring method, we assume that a candidate sentence s is generated from a sentence language model Bs. Given s in d and the citation context C, we would first estimate Bs based on s and estimate BI based on C, and then score s with</context>
<context position="11234" citStr="Zhai and Lafferty, 2001" startWordPosition="1868" endWordPosition="1871">f bits wasted in compressing messages generated according to BI (i.e., impact descriptions) with coding nonoptimally designed based on Bs. If Bs and BI are very close, the KL-divergence would be small and Score(s) would be high, which intuitively makes sense. Note that the second term (entropy of BI) is independent of s, so it can be ignored for ranking s. We see that according to the KL-divergence scoring method, our main tasks are to estimate Bs and BI. Since s can be regarded as a short document, we can use any standard method to estimate Bs. In this work, we use Dirichlet prior smoothing (Zhai and Lafferty, 2001b) to estimate Bs as follows: c(w, s) + µs * P(w|D) p(w|Bs) = (1) |s |+ µs where |s |is the length of s, c(w, s) is the count of word w in s, p(w|D) is a background model estimated using c�w,D) ����� c�w�,D) (D can be the set of all the papers available to us) and µs is a smoothing parameter to be empirically set. Note that as the length of a sentence is very short, smoothing is critical for addressing the data sparseness problem. The remaining challenge is to estimate BI accurately based on d and its citation contexts. 4 Estimation of Impact Language Models Intuitively, the impact of a paper </context>
<context position="12912" citStr="Zhai and Lafferty, 2001" startWordPosition="2168" endWordPosition="2171">s prior information and the current document d as the observed data, and use Bayesian estimation to estimate the impact language model. Specifically, let p(w|C) be a citation context language model estimated based on the citation context C. We define Dirichlet prior with parameters 1µCp(w|C)�w∈V for the impact model, where µC encodes our confidence on this prior and effectively serves as a weighting parameter for balancing the contribution of C and d for estimating the impact model. Given the observed document d, the posterior mean estimate of the impact model would be (MacKay and Peto, 1995; Zhai and Lafferty, 2001b) c(w, d) + µcp(w|C) P(w|θI) = (2) |d |+ µc µc can be interpreted as the equivalent sample size of our prior. Thus setting µc = |d |means that we put equal weights on the citation context and the document itself. µc = 0 yields p(w|θI) = p(w|d), which is to say that the impact is entirely captured by the paper itself, and our impact summarization problem would then become the standard single document (topical) summarization. Intuitively though, we would want to set µc to a relatively large number to exploit the citation context in our estimation, which is confirmed in our experiments. An alter</context>
<context position="31692" citStr="Zhai and Lafferty, 2001" startWordPosition="5217" endWordPosition="5220"> potential of directly using citation sentences (called citances) to summarize a paper was pointed out in (Nakov et al., 2004). Recently, people have explored various types of auxiliary knowledge such as hyperlinks (Delort et al., 2003) and clickthrough data (Sun et al., 2005), to summarize a webpage; such work is related to ours as anchor text is similar to citation context, but it is based on a standard formulation of multi-document summarization and would contain only sentences from anchor text. Our work is also related to work on using language models for retrieval (Ponte and Croft, 1998; Zhai and Lafferty, 2001b; Lafferty and Zhai, 2001) 7 Conclusions We have defined and studied the novel problem of summarizing the impact of a research paper. We cast the problem as an impact sentence retrieval problem, and proposed new language models to model the impact of a paper based on both the original content of the paper and its citation contexts in a literature collection with consideration of citation autority and proximity. To evaluate impact summarization, we created a test set based on ACM SIGIR papers. Experiment results on this test set show that the proposed impact summarization methods are effective</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2001b. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 334–342.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>