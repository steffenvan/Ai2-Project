<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000061">
<title confidence="0.9584745">
Modeling Syntactic and Semantic Structures in Hierarchical Phrase-based
Translation
</title>
<author confidence="0.999391">
Junhui Li Philip Resnik Hal Daum´e III
</author>
<affiliation confidence="0.999929">
University of Maryland University of Maryland University of Maryland
</affiliation>
<address confidence="0.80675">
College Park, USA College Park, USA College Park, USA
</address>
<email confidence="0.999693">
lijunhui@umiacs.umd.edu resnik@umd.edu hal@umiacs.umd.edu
</email>
<sectionHeader confidence="0.996796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.906805526315789">
Incorporating semantic structure into a
linguistics-free translation model is chal-
lenging, since semantic structures are
closely tied to syntax. In this paper, we
propose a two-level approach to exploiting
predicate-argument structure reordering in a
hierarchical phrase-based translation model.
First, we introduce linguistically motivated
constraints into a hierarchical model, guiding
translation phrase choices in favor of those
that respect syntactic boundaries. Second,
based on such translation phrases, we propose
a predicate-argument structure reordering
model that predicts reordering not only
between an argument and its predicate, but
also between two arguments. Experiments on
Chinese-to-English translation demonstrate
that both advances significantly improve
translation accuracy.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999850716981132">
Hierarchical phrase-based (HPB) translation mod-
els (Chiang, 2005; Chiang, 2007) that utilize syn-
chronous context free grammars (SCFG) have been
widely adopted in statistical machine translation
(SMT). Although formally syntactic, such models
rarely respect linguistically-motivated syntax, and
have no formal notion of semantics. As a re-
sult, they tend to produce translations containing
both grammatical errors and semantic role confu-
sions. Our goal is to take advantage of syntactic
and semantic parsing to improve translation qual-
ity of HPB translation models. Rather than intro-
ducing semantic structure into the HPB model di-
rectly, we construct an improved translation model
by incorporating linguistically motivated syntactic
constraints into a standard HPB model. Once the
translation phrases are linguistically constrained, we
are able to propose a predicate-argument reorder-
ing model. This reordering model aims to solve
two problems: ensure that arguments are ordered
properly after translation, and to ensure that the
proper argument structures even exist, for instance
in the case of PRO-drop languages. Experimental
results on Chinese-to-English translation show that
both the hard syntactic constraints and the predicate-
argument reordering model obtain significant im-
provements over the syntactically and semantically
uninformed baseline.
In principle, semantic frames (or, more specifi-
cally, predicate-argument structures: PAS) seem to
be a promising avenue for translational modeling.
While languages might diverge syntactically, they
are less likely to diverge semantically. This has
previously been recognized by Fung et al. (2006),
who report that approximately 84% of semantic
role mappings remained consistent across transla-
tions between English and Chinese. Subsequently,
Zhuang and Zong (2010) took advantage of this
consistency to jointly model semantic frames on
Chinese/English bitexts, yielding improved frame
recognition accuracy on both languages.
While there has been some encouraging work on
integrating syntactic knowledge into Chiang’s HPB
model, modeling semantic structure in a linguisti-
cally naive translation model is a challenge, because
the semantic structures themselves are syntactically
motivated. In previous work, Liu and Gildea (2010)
model the reordering/deletion of source-side seman-
tic roles in a tree-to-string translation model. While
it is natural to include semantic structures in a tree-
based translation model, the effect of semantic struc-
tures is presumably limited, since tree templates
themselves have already encoded semantics to some
</bodyText>
<page confidence="0.952607">
540
</page>
<note confidence="0.4713635">
Proceedings of NAACL-HLT 2013, pages 540–549,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999463857142857">
extent. For example, template (VP (VBG giving)
NP#1 NP#2) entails NP#1 as receiver and NP#2 as
thing given. Xiong et al. (2012) model the reorder-
ing between predicates and their arguments by as-
suming arguments are translated as a unit. However,
they only considered the reordering between argu-
ments and their predicates.
</bodyText>
<sectionHeader confidence="0.90909" genericHeader="introduction">
2 Syntactic Constraints for HPB
Translation Model
</sectionHeader>
<bodyText confidence="0.999796333333333">
In this section, we briefly review the HPB model,
then present our approach to incorporating syntactic
constraints into it.
</bodyText>
<subsectionHeader confidence="0.698283">
2.1 HPB Translation Model
</subsectionHeader>
<bodyText confidence="0.9984201875">
In HPB models, synchronous rules take the form
X —* (&apos;y, α, —), where X is the non-terminal sym-
bol, &apos;y and α are strings of lexical items and non-
terminals in the source and target side, respectively,
and — indicates the one-to-one correspondence be-
tween non-terminals in &apos;y and α. Each such rule
is associated with a set of translation model fea-
tures fOil, including phrase translation probabil-
ity p (α  |&apos;y) and its inverse p (&apos;y  |α), the lexical
translation probability plex (α  |&apos;y) and its inverse
plex (&apos;y  |α), and a rule penalty that affects prefer-
ence for longer or shorter derivations. Two other
widely used features are a target language model
feature and a target word penalty.
Given a derivation d, its translation probability is
estimated as:
</bodyText>
<equation confidence="0.9969975">
P (d) a � Oi (d)�&amp;quot; (1)
i
</equation>
<bodyText confidence="0.9995035">
where Ai is the corresponding weight of feature Oi.
See (Chiang, 2007) for more details.
</bodyText>
<subsectionHeader confidence="0.99943">
2.2 Syntactic Constraints
</subsectionHeader>
<bodyText confidence="0.999984857142857">
Translation rules in an HPB model are extracted
from initial phrase pairs, which must include at least
one word inside one phrase aligned to a word inside
the other, such that no word inside one phrase can
be aligned to a word outside the other phrase. It
is not surprising to observe that initial phrases fre-
quently are non-intuitive and inconsistent with lin-
guistic constituents, because they are based only on
statistical word alignments. Nothing in the frame-
work actually requires linguistic knowledge.
Koehn et al. (2003) conjectured that such non-
intuitive phrases do not help in translation. They
tested this conjecture by restricting phrases to syn-
tactically motivated constituents on both the source
and target side: only those initial phrase pairs are
subtrees in the derivations produced by the model.
However, their phrase-based translation experiments
(on Europarl data) showed the restriction to syn-
tactic constituents is actually harmful, because too
many phrases are eliminated. The idea of hard syn-
tactic constraints then seems essentially to have been
abandoned: it doesn’t appear in later work.
On the face of it, there are many possible rea-
sons Koehn et al. (2003)’s hard constraints did not
work, including, for example, tight restrictions that
unavoidably exclude useful phrases, and practical is-
sues like the quality of parse trees. Although en-
suing work moved in the direction of soft syntactic
constraints (see Section 6), our ultimate goal of cap-
turing predicate-argument structure requires linguis-
tically valid syntactic constituents, and therefore we
revisit the idea of hard constraints, avoiding prob-
lems with their strictness by relaxing them in three
ways.
First, requiring source phrases to be subtrees in
a linguistically informed syntactic parse eliminates
many reasonable phrases. Consider the English-
Chinese phrase pair (the red car, hongse de qiche).1
It is easily to get a translation entry for the whole
phrase pair. By contrast, the phrase pair (the red,
hongse de) is typically excluded because it does not
correspond to a complete subtree on the source side.
Yet translating the red is likely to be more useful
than translating the red car, since it is more general:
it can be followed by any other noun translation. To
this end, we relax the syntactic constraints by allow-
ing phrases on the source side corresponding to ei-
ther one subtree or sibling subtrees with a common
parent node in the syntactic parse. For example, the
red in Figure 1(a) is allowed since it spans two sub-
trees that have a common parent node NP.
Second, we might still exclude useful phrases be-
cause the syntactic parses of some languages, like
Chinese, prefer deep trees, resulting in a head and
its modifiers being distributed across multiple struc-
tural levels. Consider the English sentence I still
</bodyText>
<footnote confidence="0.902948">
1We use English as source language for better readability.
</footnote>
<page confidence="0.997198">
541
</page>
<bodyText confidence="0.999961083333333">
like the red car very much and its syntactic structure
as shown in Figure 1(a). Phrases I still, still like,
I still like are not allowed, since they don’t map to
either a subtree or sibling subtrees. Logically, how-
ever, it might make sense not just to include phrases
mapping to (sibling) subtrees, but to include phrases
mapping to subtrees with the same head. To this end,
we flatten the syntactic parse so that a head and all its
modifiers appear at the same level. Another advan-
tage of this flattened structure is that flattened trees
are more reliable than unflattened ones, in the sense
that some bracketing errors in unflattened trees can
be eliminated during tree flattening. Figure 1(b) il-
lustrates flattening a syntactic parse by moving the
head (like) and all its modifiers (I, still, the red car,
and very much) to the same level.
Third, initial phrase pair extraction in Chiang’s
HPB generates a very large number of rules, which
makes training and decoding very slow. To avoid
this, a widely used strategy is to limit initial phrases
to a reasonable length on either side during rule ex-
traction (e.g., 10 in Chiang (2007)). A correspond-
ing constraint to speed up decoding prohibits any X
from spanning a substring longer than a fixed length,
often the same as the maximum phrase length in rule
extraction. Although the initial phrase length limita-
tion mainly keeps non-intuitive phrases out, it also
closes the door on some useful phrases. For ex-
ample, a translation rule (I still like X, wo rengran
xihuan X) will be prohibited if the non-terminal X
covers 8 or more words. In contrast, our hard con-
straints have already filtered out dominating non-
intuitive phrases; thus there is more room to include
additional useful phrases. As a result, we can switch
off the constraints on initial phrase length in both
training and decoding.
</bodyText>
<subsectionHeader confidence="0.999574">
2.3 Reorderable Glue Rules
</subsectionHeader>
<bodyText confidence="0.9999634">
In decoding, if no good rule (e.g., a rule whose left-
hand side is X) can be applied or the length of the
potential source span is larger than a pre-defined
length, a glue rule (either 5 → (X1, X1) or 5 →
(51X2, 51X2)) will be used to simply stitch two
consequent translated phrases together in monotonic
way. This will obviously prevent some reasonable
translation derivations because in certain cases, the
order of phrases may be inverted on the target side.
Moreover, even that the syntactic constraints dis-
</bodyText>
<figure confidence="0.993236428571429">
S
VP
a. Word alignment for an English-Chinese sentence pair
with the parse tree for the English sentence
S
I still like the red car very much
b. Flattened parse tree for the English sentence
</figure>
<figureCaption confidence="0.999997">
Figure 1: Example of flattening parse tree.
</figureCaption>
<bodyText confidence="0.997189">
cussed above make translation node Xs are syntacti-
cally informed, stitching translated phrases from left
to right will unavoidably generate non-syntactically
informed node 5s. For example, the combination of
X (like) and X (the) does not make much sense in
linguistic perspective.
Alternatively, we replace glue rules of HPB with
reorderable ones:
</bodyText>
<listItem confidence="0.999387666666667">
• T → (X1, X1)
• T → (T1T2, T1T2)
• T → (T1T2, T2T1)
</listItem>
<bodyText confidence="0.9999824">
where the second (third) rule combines two trans-
lated phrases in a monotonic (inverted) way. Specif-
ically, we set the translation probability of the first
translation rule as 1 while estimating the probabil-
ities of the other two rules from training data. In
both training and decoding, we require the phrases
covered by T to satisfy our syntactic constraints.
Therefore, all translation nodes (both Xs and Ts)
in derivations are syntactically informed, providing
room to explore PAS reordering in HPB model.
</bodyText>
<sectionHeader confidence="0.998642" genericHeader="method">
3 PAS Reordering Model
</sectionHeader>
<bodyText confidence="0.999920333333333">
Ideally, we aim to model PAS reordering based on
the true semantic roles of both the source and tar-
get side, as to better cater not only consistence but
</bodyText>
<figure confidence="0.986587578947369">
VP
NP ADVP
VBP NP ADVP
still
like the red car very much
R VM *14 4V kEt M AIF
wo rengran feichang xihua hongse de qiche
NP
ADVP
VBP NP ADVP
542
ft VT A 419 4* kEt n A4_�
wo rengran feichang xihua hongse de qiche
a. Word alignment for an English-Chinese sentence
pair with semantic roles for the English sentence
PAS-S
A01 AM-TMP2 VBP3 A14 AM-MNR5
X1 X2 X5 X3 X4
b. PAS-S and PAS-T for predicate like
</figure>
<figureCaption confidence="0.9960845">
Figure 2: Example of PAS on both the source and target
side. Items are aligned by indices.
</figureCaption>
<bodyText confidence="0.999943055555556">
divergence between semantic frames of the source
and target language. However, considering there is
no efficient way of jointly performing MT and SRL,
accurate SRL on target side can only be done after
translation. Similar to related work (Liu and Gildea,
2010; Xiong et al., 2012), we obtain the PAS of
the source language (PAS-S) via a shallow seman-
tic parser and project the PAS of the target language
(PAS-T) using the word alignment derived from the
translation process. Specifically, we use PropBank
standard (Palmer et al., 2005; Xue, 2008) which de-
fines a set of numbered core arguments (i.e., A0-A5)
and adjunct-like arguments (e.g., AM-TMP for tem-
poral, AM-MNR for manner). Figure 2(b) shows
an example of PAS projection from source language
to target language.2 The PAS reordering model de-
scribes the probability of reordering PAS-S into PAS-
T. Given a predicate p, it takes the following form:
</bodyText>
<equation confidence="0.935419">
P (PAS-T I PAS-S, PRE=p) (2)
</equation>
<bodyText confidence="0.927441">
Note that cases for untranslated roles can be natu-
rally reflected in our PAS reordering model. For ex-
ample, if the argument IA0 is untranslated in Figure
2, its PAS-T will be X2X5X3X4.
2In PAS-S, we use parts-of-speech (POS) of predicates to
distinguish different types of verbs since the semantic structures
of Chinese adjective verbs are different from those of others.
</bodyText>
<subsectionHeader confidence="0.999284">
3.1 Probability Estimation
</subsectionHeader>
<bodyText confidence="0.999931666666667">
While it is hard and unnecessary to translate a pred-
icate and all its associated arguments with one rule,
especially if the sentence is long, a practicable way,
as most decoders do, is to translate them in multi-
ple level rules. In addition, some adjunct-like argu-
ments are optional, or structurally dispensable part
of a sentence, which may result in data sparsity is-
sue. Based on these observations, we decompose
Formula 2 into two parts: predicate-argument re-
ordering and argument-argument reordering.
Predicate-Argument Reordering estimates the
reordering probability between a predicate and one
of its arguments. Taking predicate like and its argu-
ment A1 the red car in Figure 2(a) as an example,
the predicate-argument pattern on the source side
(PA-S) is VBP1 A12 while the predicate-argument
pattern on the target side (PA-T) is X1X2. The re-
ordering probability is estimated as:
</bodyText>
<equation confidence="0.919984">
PP-A (PA-T=X1 X2  |PA-S=VBP1 A12, PRE=like) _
Count (PA-T=X1 X2, PA-S=VBP1 A12, PRE=like)
ETE�D(PA-S) Count (PA-T=T, PA-S=VBP1 A12, PRE=like)
</equation>
<bodyText confidence="0.998786434782609">
where 4b (PA-S) enumerates all possible reorder-
ings on the target side. Moreover, we take the pred-
icate lexicon of predicate into account. To avoid
data sparsity, we set a threshold (e.g., 100) to re-
tain frequent predicates. For infrequent predicates,
their probabilities are smoothed by replacing predi-
cate lexicon with its POS. Finally, if source side pat-
terns are infrequent (e.g., less than 10) for frequent
predicates, their probabilities are smoothed as well
with the same way.
Argument-Argument Reordering estimates the
reordering probability between two arguments, i.e.,
argument-argument pattern on the source side (AA-
S) and its counterpart on the target side (AA-T).
However, due to that arguments are driven and piv-
oted by their predicates, we also include predicate
in patterns of AA-S and AA-T. Let’s revisit Fig-
ure 2(a). A1 the red car and AM-MNR very much
are inverted on the target side, whose probability is
estimated as:
PA-A (AA-T=X3 X1 X2  |AA-S=VBP1 A12 AM-MNR3, PRE=like)
Similarly we smooth the probabilities by distin-
guishing frequent predicates from infrequent ones,
</bodyText>
<figure confidence="0.9418624">
A0 AM-TMP VBP A1 AM-MNR
I
still
like the red car very much
PAS-T
</figure>
<page confidence="0.993989">
543
</page>
<bodyText confidence="0.989607">
as well as frequent patterns from infrequent ones.
</bodyText>
<subsectionHeader confidence="0.636814">
3.2 Integrating the PAS Reordering Model into
the HPB Model
</subsectionHeader>
<bodyText confidence="0.9993575">
We integrate the PAS reordering model into the HPB
SMT by adding a new feature into the log-linear
translation model. Unlike the conventional phrase
and lexical translation features whose values are
phrase pair-determined and thus can be calculated
offline, the value of the PAS reordering model can
only be obtained with being aware of the predicate-
argument structures a hypothesis may cover. Before
we present the algorithm of integrating the PAS re-
ordering model, we define a few functions by assum-
ing p for a predicate, a for an argument, and H for a
hypothesis:
</bodyText>
<listItem confidence="0.970613047619048">
• A(i, j, p): returns arguments of p which are
fully located within the span from word i to j
on the source side. For example, in Figure 2,
A(4, 8, like) = {A1, AM-MRN}.3
• B (i, j, p): returns true if p is located within [i, j];
otherwise returns false.
• C (a, p): returns true if predicate-argument reorder-
ing for a and p has not calculated yet; otherwise re-
turns false.
• D (a1, a2, p): returns true if argument-argument
reordering for p’s arguments a1 and a2 has not cal-
culated yet; otherwise returns false.
• PP-A (H, a, p): according to Eq. 3, returns the
probability of predicate-argument reordering of a
and p, given a and p are covered by H. The po-
sitional relation of a and p on the target side can be
detected according to translation derivation of H.
• PA-A (H, a1, a2, p): according to Eq. 4, returns
the probability of argument-argument reordering of
p’s arguments a1 and a2, given a1, a2 and p are cov-
ered by H.
</listItem>
<bodyText confidence="0.99627">
Algorithm 1 integrates the PAS reordering model
into a CKY-style decoder whenever a new hypothe-
sis is generated. Given a hypothesis H, it first looks
for predicates and their arguments which are covered
</bodyText>
<footnote confidence="0.848834666666667">
3The hard constraints make sure a valid source text span
would never fully cover some roles while partially cover other
roles. For example, phrases like the red, the read car very in
</footnote>
<figureCaption confidence="0.920316">
Figure 1 are invalid.
</figureCaption>
<table confidence="0.597666571428571">
Algorithm 1: Integrating the PAS reordering
model into a CKY-style decoder
Input: Sentence f in the source language
Predicate-Argument Structures of f
Hypothesis H spanning from word i to j
Output: Log-Probability of the PAS reordering
model
</table>
<listItem confidence="0.986038444444444">
1. set prob = 0.0
2. for predicate p in f, such that B (i, j, p) is true
3. ARG = A (i, j, p)
4. for a E ARG such that C (a, p) is true
5. prob += log PP-A (H, a, p)
6. for a1, a2 E ARG such that a1 =� a2 and
D (a1, a2, p) is true
7. prob += log PA-A (H, a1, a2, p)
8. return prob
</listItem>
<bodyText confidence="0.982994666666667">
by H (line 2-3). Then it respectively calculates the
probabilities of predicate-argument reordering and
argument-argument reordering(line 4-7).
</bodyText>
<sectionHeader confidence="0.999718" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999962863636364">
We have presented our two-level approach to in-
corporating syntactic and semantic structures in a
HPB system. In this section, we test the effect of
such structural information on a Chinese-to-English
translation task. The baseline system is a reproduc-
tion of Chiang’s (2007) HPB system. The bilin-
gual training data contains 1.5M sentence pairs with
39.4M Chinese words and 46.6M English words.4
We obtain the word alignments by running GIZA++
(Och and Ney, 2000) on the corpus in both direc-
tions and applying “grow-diag-final-and” refinement
(Koehn et al., 2003). We use the SRI language mod-
eling toolkit to train a 5-gram language model on the
Xinhua portion of the Gigaword corpus and standard
MERT (Och, 2003) to tune the feature weights on
the development data.
To obtain syntactic parse trees for instantiating
syntactic constraints and predicate-argument struc-
tures for integrating the PAS reordering model, we
first parse the source sentences with the Berkeley
Parser (Petrov and Klein, 2007) trained on Chinese
TreeBank 6.0 and then ran the Chinese semantic role
</bodyText>
<footnote confidence="0.989164666666667">
4This dataset includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06
</footnote>
<page confidence="0.991638">
544
</page>
<table confidence="0.862270083333333">
System MT 02 MT 04 MT 05 Ave.
base HPB 40.00 35.33 32.97 36.10
+ basic constraints + unflattened tree 33.90 32.00 29.83 31.91
+ our constraints + unflattened tree 38.47 34.51 32.15 35.04
+ our constraints + flattened tree 38.55 35.38 32.44 35.46
+ basic constraints + unflattened tree 35.38 32.89 30.42 32.90
+ our constraints + unflattened tree 39.41 36.02 33.21 36.21
+ our constraints + flattened tree 40.01 36.24 33.65 36.71
max-phrase-length=10
max-char-span=10
max-phrase-length=oo
max-char-span=oo
</table>
<tableCaption confidence="0.9967705">
Table 1: Effects of hard constraints. Here max-phrase-length is for maximum initial phrase length in training and
max-char-span for maximum phrase length can be covered by non-terminal X in decoding.
</tableCaption>
<figure confidence="0.760646545454545">
System
base HPB
+our constraints
with reorderable
glue rules
+PAS model
MT 02 MT 04 MT 05 Ave.
40.00 35.33 32.97 36.10
40.01 36.24++ 33.65+ 36.71
40.70+ 36.00+ 33.67+ 36.79
40.41+ 36.73++** 34.24++* 37.13
</figure>
<bodyText confidence="0.996508307692308">
labeler (Li et al., 2010) on all source parse trees to
annotate semantic roles for all verbal predicates.
We use the 2003 NIST MT evaluation test data
(919 sentence pairs) as the development data, and
the 2002, 2004 and 2005 NIST MT evaluation
test data (878, 1788 and 1082 sentence pairs, re-
spectively) as the test data. For evaluation, the
NIST BLEU script (version 11b) is used to calcu-
late the NIST BLEU scores, which measures case-
insensitive matching of n-grams with n up to 4. To
test whether a performance difference is statistically
significant, we conduct significance tests following
the paired bootstrapping approach (Koehn, 2004).
</bodyText>
<subsectionHeader confidence="0.999033">
4.1 Effects of Syntactic Constraints
</subsectionHeader>
<bodyText confidence="0.9998902">
We have also tested syntactic constraints that simply
require phrases on the source side to map to a sub-
tree (called basic constraints). Similar to requiring
initial phrases on the source side to satisfy the con-
straints in training process, we only perform chart
parsing on text spans which satisfy the constraints
in decoding process. Table 1 shows the results of
applying syntactic constraints with different experi-
mental settings. From the table, we have the follow-
ing observations.
</bodyText>
<listItem confidence="0.929380666666667">
• Consistent with the conclusion in Koehn et
al. (2003), using the basic constraints is harmful to
HPB. Fortunately, our constraints consistently work
better than the basic constraints.
• Relaxing maximum phrase length in training and
maximum char span length in decoding, we obtain
an average improvement of about 1.0-1.2 BLEU
points for systems with both basic constraints and
our constraints. It is worth noting that after re-
laxing the lengths, the system with our constraints
performs on a par with the base HPB system (e.g.,
36.21 vs. 36.10).
</listItem>
<tableCaption confidence="0.92960825">
Table 2: Effects of reorderable glue rules and the PAS
reordering model. +/++: significant over base HPB at
0.05/0.01; */**: significant over the system with reorder-
able glue rules at 0.05/0.01.
</tableCaption>
<listItem confidence="0.686616">
• Flattening parse trees further improves 0.4�0.5
</listItem>
<bodyText confidence="0.969117352941177">
BLEU points on average for systems with our syn-
tactic constraints. Our final system with constraints
outperforms the base HPB system with an average
of 0.6 BLEU points improvement (36.71 vs. 36.10).
Another advantage of applying syntactic constraints
is efficiency. By comparing the base HPB system
and the system with our syntactic constraints (i.e.,
the last row in Table 1), it is not surprising to ob-
serve that the size of rules extracted from training
data drops sharply from 193M in base HPB sys-
tem to 60M in the other. Moreover, the system
with constraints needs less decoding time than base
HPB does. Observation on 2002 NIST MT test data
(26 words per sentence on average) shows that basic
HPB system needs to fill 239 cells per sentence on
average in chart parsing while the other only needs
to fill 108 cells.
</bodyText>
<subsectionHeader confidence="0.99916">
4.2 Effects of Reorderable Glue Rules
</subsectionHeader>
<bodyText confidence="0.9951615">
Based on the system with our syntactic constraints
and relaxed phrase lengths in training and decoding,
we replace traditional glue rules with reorderable
glue rules. Table 2 shows the results, from which
we find that the effect of reorderable glue rules is
elusive: surprisingly, it achieves 0.7 BLEU points
</bodyText>
<page confidence="0.999146">
545
</page>
<tableCaption confidence="0.910419333333333">
Table 3: Experimental results over different sentence
length on the three test sets. +/++: significant over base
HPB at 0.05/0.01.
</tableCaption>
<bodyText confidence="0.999294875">
improvement on NIST MT 2002 test set while hav-
ing negligible or even slightly negative impact on the
other two test sets. The reason of reorderable glue
rules having limited influence on translation results
over monotonic only glue rules may be due to that
the monotonic reordering overwhelms the inverted
one: estimated from training data, the probability of
the monotonic glue rule is 95.5%.
</bodyText>
<subsectionHeader confidence="0.996819">
4.3 Effects of the PAS Reordering Model
</subsectionHeader>
<bodyText confidence="0.9999934">
Based on the system with reorderable glue rules, we
examine whether the PAS reordering model is capa-
ble of improving translation performance. The last
row in Table 2 presents the results . It shows the sys-
tem with the PAS reordering model obtains an aver-
age of 0.34 BLEU points over the system without it
(e.g., 37.13 vs. 36.79). It is interesting to note that it
achieves significant improvement on NIST MT 2004
and 2005 test sets (p &lt; 0.05) while slightly lowering
performance on NIST MT 2002 test set (p &gt; 0.05):
the surprising improvement of applying reorderable
glue rules on NIST MT 2002 test set leaves less
room for further improvement. Finally, it shows we
obtain an average improvement of 1.03 BLEU points
on the three test sets over the base HPB system.
</bodyText>
<sectionHeader confidence="0.995863" genericHeader="method">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.99990515">
The results in Table 1 demonstrate that significant
and sometimes substantial gains over baseline can
be obtained by incorporating hard syntactic con-
straints into the HPB model. Due to the capability of
translation phrases of arbitrary length, we conjecture
that the improvement of our system over the baseline
HPB system mostly comes from long sentences. To
test the conjecture, we combine all test sentences in
the three test sets and group them in terms of sen-
tence length. Table 3 presents the sentence distri-
bution and BLEU scores over different length. The
results validate our assumption that the system with
constraints outperforms the base systems on long
sentences (e.g., sentences with 20+ words).
Figure 3 displays a translation example which
shows the difference between the base HPB
system and the system with constraints. The
inappropriate translation of the base HPB
system can be mainly blamed on the rule
X[2,5] — 02 -k&amp;3 X[4,5], X[4,5] the development of),
where 02 &amp;43 , a part of the subtree [0, 3] span-
ning from word 0 to 3, is translated immediately
to the right of X[2,5], making a direct impact that
subtree [0,3] is translated discontinuously on the
target side. On the contrary, we can see that our
constraints are able to help select appropriate phrase
segments with respect to its syntactic structure.
Although our syntactic constraints apply on the
source side, they are completely ignorant of syn-
tax on the target side, which might result in ex-
cluding some useful translation rules. Let’s re-
visit the sentence in Figure 3, where we can see
that a transition rule spanning from word 0 to 5,
say X[0,5] — X[0,3] 1E4 4At5, X[0,3] depends on)
is intuitive: the syntactic structure on the target side
satisfies the constraints, although that of the source
side doesn’t. One natural extension of this work,
therefore, would be to relax the constraints by in-
cluding translation rules whose syntactic structure
of either the source side or the target side satisfies
the constraints.
To illustrate how the PAS reordering model im-
pacts translation output, Figure 4 displays two trans-
lation examples of systems with or without it. The
predicate * i&amp;convey in the first example has three
core arguments, i.e., A0, A2, and A1. The difference
between the two outputs is the reordering of A1 and
A2 while the PAS reordering model gives priority to
pattern VV A1 A2. In the second example, we clearly
observe two serious translation errors in the system
without PAS reordering model: IMI/themA1 is un-
translated; tW/chinaA0 is moved to the immediate
right of predicate A¸/allow and plays as direct ob-
ject.
Including the PAS reordering model improves the
BLEU scores. One further direction to refine the ap-
proach is to alleviate verb sparsity via verb classes.
Another direction is to include useful context in es-
timating reordering probability. For example, the
content of a temporal argument AM-TMP can be a
</bodyText>
<figure confidence="0.914088">
1-10 11-20 21-30 31-40 41+ all
337 1001 1052 768 590 3748
32.21 37.51 36.71 34.96 35.00 35.73
31.70 37.57 37.10 36.20++ 35.78++ 36.39++
sentence length
sentence count
base HPB
+our constraints
546
X[5,9]: depends on X[9,9] of the X[6,7]
X[0,10]: X[0,3] X[5,9] .
</figure>
<figureCaption confidence="0.744099">
Figure 3: A translation example of the base HPB system (above) and the system with constraints (below).
</figureCaption>
<figure confidence="0.973134636363636">
((((很多 事情) 的) 发展) 是 (取决于 (((世界 局势) 的) 发展)) 。)
0 1 2 3 4 5 6 7 8 9 10
X[0,1]: lot X[4,5]: depends on
X[0,1]: lot
X[0,3]: X[0,1] development
X[2,5]: X[4,5] the development of
X[6,9]: the devet. of the world sit. X[10,10]: .
X[6,7]: world X[7,7]
X[7,7]: sit.
X[9,9]: devet.
w/o [korean] [will] [convey] [to the] hope of [resuming talks information] X1 X2 X4 X3 X5
w/o [friday] [allowed] [china] [to seoul through the philippines] . X2 X3 X1 X5
with
with
Ref. in friday, china allowed them to travel to seoul through philippines . ----
Source
Ref. south korean conveys its desire to resume talking with north korean ----
Source
[ ]A0 [ ]AM-ADVP [I , -V]A2 [*!9]PRE 4M [SRA %6�0 M �1WA1 A01 AM-ADVP2 A23 VV4 A15
[south korean] [will] [deliver] hope [resume talks message] [to the dprk] X1 X2 X4 X5 X3
[tIM]A0 [*MH]AM-TMP [fti�F]PRE [�C]A1 [AM- H,;rl RJA]A2 o A01 AM-TMP2 VV3 A14 A25
[china] [friday] [allowed] [them] [to seoul through the philippines] . X1 X2 X3 X4 X5
</figure>
<figureCaption confidence="0.999951">
Figure 4: Two translation examples of the system with/without PAS reordering model
</figureCaption>
<bodyText confidence="0.998751666666667">
short/simple phrase (e.g., friday) or a long/complex
one (e.g., when I was 20 years old), which has im-
pact on its reordering in translation.
</bodyText>
<sectionHeader confidence="0.999987" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999985794871795">
While there has been substantial work on linguis-
tically motivated SMT, we limit ourselves here
to several approaches that leverage syntactic con-
straints yet still allow cross-constituent transla-
tions. In terms of tree-based SMT with cross-
constituent translations, Cowan et al. (2006) al-
lowed non-constituent sub phrases on the source
side and adopted phrase-based translation model for
modifiers in clauses. Marcu (2006) and Galley
et al. (2006) inserted artificial constituent nodes in
parsing tree as to capture useful but non-constituent
phrases. The parse tree binarization approach
(Wang et al., 2007; Marcu, 2007) and the forest-
based approach (Mi et al., 2008) would also cover
non-constituent phrases to some extent. Shen et
al. (2010) defined well-formed dependency struc-
ture to cover uncompleted dependency structure in
translation rules. In addition to the fact that the
constraints of Shen et al. (2010) and this paper
are based on different syntactic perspectives (i.e.,
dependency structure vs. constituency structure),
the major difference is that in this work we don’t
limit the length of phrases to a fixed maximum size
(e.g., 10 in Hiero). Consequently, we obtain some
translation rules that are not found in Hiero sys-
tems constrained by the length. In terms of (hi-
erarchical) phrase-based SMT with syntactic con-
straints, particular related to constituent boundaries,
Koehn et al. (2003) tested constraints allowing con-
stituent matched phrases only. Chiang (2005) and
Cherry (2008) used a soft constraint to award or pe-
nalize hypotheses which respect or violate syntactic
boundaries. Marton and Resnik (2008) further ex-
plored the idea of soft constraints by distinguishing
among constituent types. Xiong et al. (2009; 2010)
presented models that learn phrase boundaries from
aligned dataset.
On the other hand, semantics motivated SMT has
also seen an increase in activity recently. Wu and
</bodyText>
<page confidence="0.990325">
547
</page>
<bodyText confidence="0.999919307692308">
Fung (2009) re-ordered arguments on the target side
translation output, seeking to maximize the cross-
lingual match of the semantic frames of the re-
ordered translation to that of the source sentence.
Liu and Gildea (2010) added two types of semantic
role features into a tree-to-string translation model.
Although Xiong et al. (2012) and our work are both
focusing on source side PAS reordering, our model
differs from theirs in two main aspects: 1) we con-
sider reordering not only between an argument and
its predicate, but also between two arguments; and
2) our reordering model can naturally model cases
of untranslated arguments or predicates.
</bodyText>
<sectionHeader confidence="0.999467" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999685625">
In this paper, we have presented an approach to
incorporating syntactic and semantic structures for
the HPB translation model. To accommodate the
close tie of semantic structures to syntax, we first
revisited the idea of hard syntactic constraints, and
we demonstrated that hard constraints can, in fact,
lead to significant improvement in translation qual-
ity when applied to Chiang’s HPB framework. Then
our PAS reordering model, thanks to the constraints
which guided translation phrases in favor of syntac-
tic boundaries, made further improvements by pre-
dicting reordering not only between an argument
and its predicate, but also between two arguments.
In the future work, we will extend the PAS reorder-
ing model to include useful context, e.g., the head
words and the syntactic categories of arguments.
</bodyText>
<sectionHeader confidence="0.998964" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99995425">
This research was supported in part by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015. Any
opinions, findings, conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the view of DARPA. The
authors would like to thank three anonymous re-
viewers for providing helpful suggestions, and also
acknowledge Ke Wu and other CLIP labmates in
MT group for useful discussions. We also thank
creators of the valuable off-the-shelf NLP packages,
such as GIZA++ and Berkeley Parser.
</bodyText>
<sectionHeader confidence="0.998477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999971673076923">
Colin Cherry. 2008. Cohesive phrase-based decoding
for statistical machine translation. In Proceedings of
ACL-HLT 2008, pages 72–80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263–270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Brooke Cowan, Ivona Ku˘cerov´a, and Michael Collins.
2006. A discriminative model for tree-to-tree transla-
tion. In Proceedings of EMNLP 2006, pages 232–241.
Pascale Fung, Zhaojun Wu, Yongsheng Yang, and Dekai
Wu. 2006. Automatic learning of Chinese-English
semantic structure mapping. In Proceedings of SLT
2006, pages 230–233.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of ACL-COLING 2006, pages 961–968.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL 2003, pages 48–54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388–395.
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
Joint syntactic and semantic parsing of Chinese. In
Proceedings of ACL 2010, pages 1108–1117.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In Proceedings of COL-
ING 2010, pages 716–724.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of EMNLP 2006, pages 44–52.
Steve DeNeefe; Kevin Knight; Wei Wang; Daniel Marcu.
2007. What can syntax-based mt learn from phrase-
based mt? In Proceedings of EMNLP-CoNLL 2007,
pages 755–763.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-HLT 2008, pages 1003–1011.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-HLT 2008,
pages 192–199.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440–447.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160–167.
</reference>
<page confidence="0.973021">
548
</page>
<reference confidence="0.99866503125">
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71–
106.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL-
HLT 2007, pages 404–411.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine translation.
Computational Linguistics, 36(4):649–671.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In Proceedings of EMNLP-
CoNLL 2007, pages 746–754.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
smt: A hybrid two-pass model. In Proceedings of
NAACL-HLT 2009, pages 13–16.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2009.
A syntax-driven bracketing model for phrase-based
translation. In Proceedings of ACL-IJCNLP 2009,
pages 315–323.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing translation boundaries for phrase-based decoding.
In Proceedings of NAACL-HLT 2010, pages 136–144.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Model-
ing the translation of predicate-argument structure for
smt. In Proceedings of ACL 2012, pages 902–911.
Nianwen Xue. 2008. Automatic labeling of semantic
roles. Computational Linguistics, 34(4):225–255.
Tao Zhuang and Chengqing Zong. 2010. Joint inference
for bilingual semantic role labeling. In Proceedings of
EMNLP 2010, pages 304–314.
</reference>
<page confidence="0.998724">
549
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.821852">
<title confidence="0.9995605">Modeling Syntactic and Semantic Structures in Hierarchical Phrase-based Translation</title>
<author confidence="0.989632">Junhui Li Philip Resnik Hal Daum´e</author>
<affiliation confidence="0.999959">University of Maryland University of Maryland University of Maryland</affiliation>
<address confidence="0.845208">College Park, USA College Park, USA College Park,</address>
<email confidence="0.999874">lijunhui@umiacs.umd.eduresnik@umd.eduhal@umiacs.umd.edu</email>
<abstract confidence="0.99915945">Incorporating semantic structure into a linguistics-free translation model is challenging, since semantic structures are closely tied to syntax. In this paper, we propose a two-level approach to exploiting predicate-argument structure reordering in a hierarchical phrase-based translation model. First, we introduce linguistically motivated constraints into a hierarchical model, guiding translation phrase choices in favor of those that respect syntactic boundaries. Second, based on such translation phrases, we propose a predicate-argument structure reordering model that predicts reordering not only between an argument and its predicate, but also between two arguments. Experiments on Chinese-to-English translation demonstrate that both advances significantly improve translation accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
</authors>
<title>Cohesive phrase-based decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT</booktitle>
<pages>72--80</pages>
<contexts>
<context position="31135" citStr="Cherry (2008)" startWordPosition="5078" endWordPosition="5079">traints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length. In terms of (hierarchical) phrase-based SMT with syntactic constraints, particular related to constituent boundaries, Koehn et al. (2003) tested constraints allowing constituent matched phrases only. Chiang (2005) and Cherry (2008) used a soft constraint to award or penalize hypotheses which respect or violate syntactic boundaries. Marton and Resnik (2008) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and 547 Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize the crosslingual match of the semantic frames of the reordered translation to that of the sourc</context>
</contexts>
<marker>Cherry, 2008</marker>
<rawString>Colin Cherry. 2008. Cohesive phrase-based decoding for statistical machine translation. In Proceedings of ACL-HLT 2008, pages 72–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1188" citStr="Chiang, 2005" startWordPosition="147" endWordPosition="148"> phrase-based translation model. First, we introduce linguistically motivated constraints into a hierarchical model, guiding translation phrase choices in favor of those that respect syntactic boundaries. Second, based on such translation phrases, we propose a predicate-argument structure reordering model that predicts reordering not only between an argument and its predicate, but also between two arguments. Experiments on Chinese-to-English translation demonstrate that both advances significantly improve translation accuracy. 1 Introduction Hierarchical phrase-based (HPB) translation models (Chiang, 2005; Chiang, 2007) that utilize synchronous context free grammars (SCFG) have been widely adopted in statistical machine translation (SMT). Although formally syntactic, such models rarely respect linguistically-motivated syntax, and have no formal notion of semantics. As a result, they tend to produce translations containing both grammatical errors and semantic role confusions. Our goal is to take advantage of syntactic and semantic parsing to improve translation quality of HPB translation models. Rather than introducing semantic structure into the HPB model directly, we construct an improved tra</context>
<context position="31117" citStr="Chiang (2005)" startWordPosition="5075" endWordPosition="5076">fact that the constraints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length. In terms of (hierarchical) phrase-based SMT with syntactic constraints, particular related to constituent boundaries, Koehn et al. (2003) tested constraints allowing constituent matched phrases only. Chiang (2005) and Cherry (2008) used a soft constraint to award or penalize hypotheses which respect or violate syntactic boundaries. Marton and Resnik (2008) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and 547 Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize the crosslingual match of the semantic frames of the reordered translation to</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL 2005, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1203" citStr="Chiang, 2007" startWordPosition="149" endWordPosition="150">translation model. First, we introduce linguistically motivated constraints into a hierarchical model, guiding translation phrase choices in favor of those that respect syntactic boundaries. Second, based on such translation phrases, we propose a predicate-argument structure reordering model that predicts reordering not only between an argument and its predicate, but also between two arguments. Experiments on Chinese-to-English translation demonstrate that both advances significantly improve translation accuracy. 1 Introduction Hierarchical phrase-based (HPB) translation models (Chiang, 2005; Chiang, 2007) that utilize synchronous context free grammars (SCFG) have been widely adopted in statistical machine translation (SMT). Although formally syntactic, such models rarely respect linguistically-motivated syntax, and have no formal notion of semantics. As a result, they tend to produce translations containing both grammatical errors and semantic role confusions. Our goal is to take advantage of syntactic and semantic parsing to improve translation quality of HPB translation models. Rather than introducing semantic structure into the HPB model directly, we construct an improved translation model </context>
<context position="5217" citStr="Chiang, 2007" startWordPosition="754" endWordPosition="755">espondence between non-terminals in &apos;y and α. Each such rule is associated with a set of translation model features fOil, including phrase translation probability p (α |&apos;y) and its inverse p (&apos;y |α), the lexical translation probability plex (α |&apos;y) and its inverse plex (&apos;y |α), and a rule penalty that affects preference for longer or shorter derivations. Two other widely used features are a target language model feature and a target word penalty. Given a derivation d, its translation probability is estimated as: P (d) a � Oi (d)�&amp;quot; (1) i where Ai is the corresponding weight of feature Oi. See (Chiang, 2007) for more details. 2.2 Syntactic Constraints Translation rules in an HPB model are extracted from initial phrase pairs, which must include at least one word inside one phrase aligned to a word inside the other, such that no word inside one phrase can be aligned to a word outside the other phrase. It is not surprising to observe that initial phrases frequently are non-intuitive and inconsistent with linguistic constituents, because they are based only on statistical word alignments. Nothing in the framework actually requires linguistic knowledge. Koehn et al. (2003) conjectured that such nonint</context>
<context position="9279" citStr="Chiang (2007)" startWordPosition="1424" endWordPosition="1425"> flattened trees are more reliable than unflattened ones, in the sense that some bracketing errors in unflattened trees can be eliminated during tree flattening. Figure 1(b) illustrates flattening a syntactic parse by moving the head (like) and all its modifiers (I, still, the red car, and very much) to the same level. Third, initial phrase pair extraction in Chiang’s HPB generates a very large number of rules, which makes training and decoding very slow. To avoid this, a widely used strategy is to limit initial phrases to a reasonable length on either side during rule extraction (e.g., 10 in Chiang (2007)). A corresponding constraint to speed up decoding prohibits any X from spanning a substring longer than a fixed length, often the same as the maximum phrase length in rule extraction. Although the initial phrase length limitation mainly keeps non-intuitive phrases out, it also closes the door on some useful phrases. For example, a translation rule (I still like X, wo rengran xihuan X) will be prohibited if the non-terminal X covers 8 or more words. In contrast, our hard constraints have already filtered out dominating nonintuitive phrases; thus there is more room to include additional useful </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brooke Cowan</author>
<author>Ivona Ku˘cerov´a</author>
<author>Michael Collins</author>
</authors>
<title>A discriminative model for tree-to-tree translation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>232--241</pages>
<marker>Cowan, Ku˘cerov´a, Collins, 2006</marker>
<rawString>Brooke Cowan, Ivona Ku˘cerov´a, and Michael Collins. 2006. A discriminative model for tree-to-tree translation. In Proceedings of EMNLP 2006, pages 232–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Zhaojun Wu</author>
<author>Yongsheng Yang</author>
<author>Dekai Wu</author>
</authors>
<title>Automatic learning of Chinese-English semantic structure mapping.</title>
<date>2006</date>
<booktitle>In Proceedings of SLT</booktitle>
<pages>230--233</pages>
<contexts>
<context position="2772" citStr="Fung et al. (2006)" startWordPosition="369" endWordPosition="372">oper argument structures even exist, for instance in the case of PRO-drop languages. Experimental results on Chinese-to-English translation show that both the hard syntactic constraints and the predicateargument reordering model obtain significant improvements over the syntactically and semantically uninformed baseline. In principle, semantic frames (or, more specifically, predicate-argument structures: PAS) seem to be a promising avenue for translational modeling. While languages might diverge syntactically, they are less likely to diverge semantically. This has previously been recognized by Fung et al. (2006), who report that approximately 84% of semantic role mappings remained consistent across translations between English and Chinese. Subsequently, Zhuang and Zong (2010) took advantage of this consistency to jointly model semantic frames on Chinese/English bitexts, yielding improved frame recognition accuracy on both languages. While there has been some encouraging work on integrating syntactic knowledge into Chiang’s HPB model, modeling semantic structure in a linguistically naive translation model is a challenge, because the semantic structures themselves are syntactically motivated. In previo</context>
</contexts>
<marker>Fung, Wu, Yang, Wu, 2006</marker>
<rawString>Pascale Fung, Zhaojun Wu, Yongsheng Yang, and Dekai Wu. 2006. Automatic learning of Chinese-English semantic structure mapping. In Proceedings of SLT 2006, pages 230–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL-COLING</booktitle>
<pages>961--968</pages>
<contexts>
<context position="30082" citStr="Galley et al. (2006)" startWordPosition="4914" endWordPosition="4917">S reordering model short/simple phrase (e.g., friday) or a long/complex one (e.g., when I was 20 years old), which has impact on its reordering in translation. 6 Related Work While there has been substantial work on linguistically motivated SMT, we limit ourselves here to several approaches that leverage syntactic constraints yet still allow cross-constituent translations. In terms of tree-based SMT with crossconstituent translations, Cowan et al. (2006) allowed non-constituent sub phrases on the source side and adopted phrase-based translation model for modifiers in clauses. Marcu (2006) and Galley et al. (2006) inserted artificial constituent nodes in parsing tree as to capture useful but non-constituent phrases. The parse tree binarization approach (Wang et al., 2007; Marcu, 2007) and the forestbased approach (Mi et al., 2008) would also cover non-constituent phrases to some extent. Shen et al. (2010) defined well-formed dependency structure to cover uncompleted dependency structure in translation rules. In addition to the fact that the constraints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the major di</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of ACL-COLING 2006, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL</booktitle>
<pages>48--54</pages>
<contexts>
<context position="5788" citStr="Koehn et al. (2003)" startWordPosition="845" endWordPosition="848">ponding weight of feature Oi. See (Chiang, 2007) for more details. 2.2 Syntactic Constraints Translation rules in an HPB model are extracted from initial phrase pairs, which must include at least one word inside one phrase aligned to a word inside the other, such that no word inside one phrase can be aligned to a word outside the other phrase. It is not surprising to observe that initial phrases frequently are non-intuitive and inconsistent with linguistic constituents, because they are based only on statistical word alignments. Nothing in the framework actually requires linguistic knowledge. Koehn et al. (2003) conjectured that such nonintuitive phrases do not help in translation. They tested this conjecture by restricting phrases to syntactically motivated constituents on both the source and target side: only those initial phrase pairs are subtrees in the derivations produced by the model. However, their phrase-based translation experiments (on Europarl data) showed the restriction to syntactic constituents is actually harmful, because too many phrases are eliminated. The idea of hard syntactic constraints then seems essentially to have been abandoned: it doesn’t appear in later work. On the face o</context>
<context position="19115" citStr="Koehn et al., 2003" startWordPosition="3103" endWordPosition="3106">nd argument-argument reordering(line 4-7). 4 Experiments We have presented our two-level approach to incorporating syntactic and semantic structures in a HPB system. In this section, we test the effect of such structural information on a Chinese-to-English translation task. The baseline system is a reproduction of Chiang’s (2007) HPB system. The bilingual training data contains 1.5M sentence pairs with 39.4M Chinese words and 46.6M English words.4 We obtain the word alignments by running GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. To obtain syntactic parse trees for instantiating syntactic constraints and predicate-argument structures for integrating the PAS reordering model, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and then ran the Chinese semantic role 4This dataset includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, </context>
<context position="21876" citStr="Koehn et al. (2003)" startWordPosition="3539" endWordPosition="3542">ed bootstrapping approach (Koehn, 2004). 4.1 Effects of Syntactic Constraints We have also tested syntactic constraints that simply require phrases on the source side to map to a subtree (called basic constraints). Similar to requiring initial phrases on the source side to satisfy the constraints in training process, we only perform chart parsing on text spans which satisfy the constraints in decoding process. Table 1 shows the results of applying syntactic constraints with different experimental settings. From the table, we have the following observations. • Consistent with the conclusion in Koehn et al. (2003), using the basic constraints is harmful to HPB. Fortunately, our constraints consistently work better than the basic constraints. • Relaxing maximum phrase length in training and maximum char span length in decoding, we obtain an average improvement of about 1.0-1.2 BLEU points for systems with both basic constraints and our constraints. It is worth noting that after relaxing the lengths, the system with our constraints performs on a par with the base HPB system (e.g., 36.21 vs. 36.10). Table 2: Effects of reorderable glue rules and the PAS reordering model. +/++: significant over base HPB at</context>
<context position="31041" citStr="Koehn et al. (2003)" startWordPosition="5063" endWordPosition="5066">o cover uncompleted dependency structure in translation rules. In addition to the fact that the constraints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length. In terms of (hierarchical) phrase-based SMT with syntactic constraints, particular related to constituent boundaries, Koehn et al. (2003) tested constraints allowing constituent matched phrases only. Chiang (2005) and Cherry (2008) used a soft constraint to award or penalize hypotheses which respect or violate syntactic boundaries. Marton and Resnik (2008) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and 547 Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize t</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL 2003, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>388--395</pages>
<contexts>
<context position="21296" citStr="Koehn, 2004" startWordPosition="3448" endWordPosition="3449">all source parse trees to annotate semantic roles for all verbal predicates. We use the 2003 NIST MT evaluation test data (919 sentence pairs) as the development data, and the 2002, 2004 and 2005 NIST MT evaluation test data (878, 1788 and 1082 sentence pairs, respectively) as the test data. For evaluation, the NIST BLEU script (version 11b) is used to calculate the NIST BLEU scores, which measures caseinsensitive matching of n-grams with n up to 4. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrapping approach (Koehn, 2004). 4.1 Effects of Syntactic Constraints We have also tested syntactic constraints that simply require phrases on the source side to map to a subtree (called basic constraints). Similar to requiring initial phrases on the source side to satisfy the constraints in training process, we only perform chart parsing on text spans which satisfy the constraints in decoding process. Table 1 shows the results of applying syntactic constraints with different experimental settings. From the table, we have the following observations. • Consistent with the conclusion in Koehn et al. (2003), using the basic co</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP 2004, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junhui Li</author>
<author>Guodong Zhou</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Joint syntactic and semantic parsing of Chinese.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>1108--1117</pages>
<contexts>
<context position="20680" citStr="Li et al., 2010" startWordPosition="3345" endWordPosition="3348">traints + unflattened tree 39.41 36.02 33.21 36.21 + our constraints + flattened tree 40.01 36.24 33.65 36.71 max-phrase-length=10 max-char-span=10 max-phrase-length=oo max-char-span=oo Table 1: Effects of hard constraints. Here max-phrase-length is for maximum initial phrase length in training and max-char-span for maximum phrase length can be covered by non-terminal X in decoding. System base HPB +our constraints with reorderable glue rules +PAS model MT 02 MT 04 MT 05 Ave. 40.00 35.33 32.97 36.10 40.01 36.24++ 33.65+ 36.71 40.70+ 36.00+ 33.67+ 36.79 40.41+ 36.73++** 34.24++* 37.13 labeler (Li et al., 2010) on all source parse trees to annotate semantic roles for all verbal predicates. We use the 2003 NIST MT evaluation test data (919 sentence pairs) as the development data, and the 2002, 2004 and 2005 NIST MT evaluation test data (878, 1788 and 1082 sentence pairs, respectively) as the test data. For evaluation, the NIST BLEU script (version 11b) is used to calculate the NIST BLEU scores, which measures caseinsensitive matching of n-grams with n up to 4. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrapping approa</context>
</contexts>
<marker>Li, Zhou, Ng, 2010</marker>
<rawString>Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010. Joint syntactic and semantic parsing of Chinese. In Proceedings of ACL 2010, pages 1108–1117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Semantic role features for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>716--724</pages>
<contexts>
<context position="3402" citStr="Liu and Gildea (2010)" startWordPosition="456" endWordPosition="459">ort that approximately 84% of semantic role mappings remained consistent across translations between English and Chinese. Subsequently, Zhuang and Zong (2010) took advantage of this consistency to jointly model semantic frames on Chinese/English bitexts, yielding improved frame recognition accuracy on both languages. While there has been some encouraging work on integrating syntactic knowledge into Chiang’s HPB model, modeling semantic structure in a linguistically naive translation model is a challenge, because the semantic structures themselves are syntactically motivated. In previous work, Liu and Gildea (2010) model the reordering/deletion of source-side semantic roles in a tree-to-string translation model. While it is natural to include semantic structures in a treebased translation model, the effect of semantic structures is presumably limited, since tree templates themselves have already encoded semantics to some 540 Proceedings of NAACL-HLT 2013, pages 540–549, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics extent. For example, template (VP (VBG giving) NP#1 NP#2) entails NP#1 as receiver and NP#2 as thing given. Xiong et al. (2012) model the reordering betwe</context>
<context position="12598" citStr="Liu and Gildea, 2010" startWordPosition="1998" endWordPosition="2001"> NP ADVP 542 ft VT A 419 4* kEt n A4_� wo rengran feichang xihua hongse de qiche a. Word alignment for an English-Chinese sentence pair with semantic roles for the English sentence PAS-S A01 AM-TMP2 VBP3 A14 AM-MNR5 X1 X2 X5 X3 X4 b. PAS-S and PAS-T for predicate like Figure 2: Example of PAS on both the source and target side. Items are aligned by indices. divergence between semantic frames of the source and target language. However, considering there is no efficient way of jointly performing MT and SRL, accurate SRL on target side can only be done after translation. Similar to related work (Liu and Gildea, 2010; Xiong et al., 2012), we obtain the PAS of the source language (PAS-S) via a shallow semantic parser and project the PAS of the target language (PAS-T) using the word alignment derived from the translation process. Specifically, we use PropBank standard (Palmer et al., 2005; Xue, 2008) which defines a set of numbered core arguments (i.e., A0-A5) and adjunct-like arguments (e.g., AM-TMP for temporal, AM-MNR for manner). Figure 2(b) shows an example of PAS projection from source language to target language.2 The PAS reordering model describes the probability of reordering PAS-S into PAST. Given</context>
<context position="31768" citStr="Liu and Gildea (2010)" startWordPosition="5177" endWordPosition="5180"> constraint to award or penalize hypotheses which respect or violate syntactic boundaries. Marton and Resnik (2008) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and 547 Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize the crosslingual match of the semantic frames of the reordered translation to that of the source sentence. Liu and Gildea (2010) added two types of semantic role features into a tree-to-string translation model. Although Xiong et al. (2012) and our work are both focusing on source side PAS reordering, our model differs from theirs in two main aspects: 1) we consider reordering not only between an argument and its predicate, but also between two arguments; and 2) our reordering model can naturally model cases of untranslated arguments or predicates. 7 Conclusion In this paper, we have presented an approach to incorporating syntactic and semantic structures for the HPB translation model. To accommodate the close tie of s</context>
</contexts>
<marker>Liu, Gildea, 2010</marker>
<rawString>Ding Liu and Daniel Gildea. 2010. Semantic role features for machine translation. In Proceedings of COLING 2010, pages 716–724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>44--52</pages>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proceedings of EMNLP 2006, pages 44–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
<author>Daniel Marcu</author>
</authors>
<title>What can syntax-based mt learn from phrasebased mt?</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL</booktitle>
<pages>755--763</pages>
<marker>DeNeefe, Knight, Wang, Marcu, 2007</marker>
<rawString>Steve DeNeefe; Kevin Knight; Wei Wang; Daniel Marcu. 2007. What can syntax-based mt learn from phrasebased mt? In Proceedings of EMNLP-CoNLL 2007, pages 755–763.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="31262" citStr="Marton and Resnik (2008)" startWordPosition="5096" endWordPosition="5099">re vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length. In terms of (hierarchical) phrase-based SMT with syntactic constraints, particular related to constituent boundaries, Koehn et al. (2003) tested constraints allowing constituent matched phrases only. Chiang (2005) and Cherry (2008) used a soft constraint to award or penalize hypotheses which respect or violate syntactic boundaries. Marton and Resnik (2008) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and 547 Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize the crosslingual match of the semantic frames of the reordered translation to that of the source sentence. Liu and Gildea (2010) added two types of semantic role features into a tree-to-string translation model. Although X</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proceedings of ACL-HLT 2008, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT</booktitle>
<pages>192--199</pages>
<contexts>
<context position="30303" citStr="Mi et al., 2008" startWordPosition="4948" endWordPosition="4951">cally motivated SMT, we limit ourselves here to several approaches that leverage syntactic constraints yet still allow cross-constituent translations. In terms of tree-based SMT with crossconstituent translations, Cowan et al. (2006) allowed non-constituent sub phrases on the source side and adopted phrase-based translation model for modifiers in clauses. Marcu (2006) and Galley et al. (2006) inserted artificial constituent nodes in parsing tree as to capture useful but non-constituent phrases. The parse tree binarization approach (Wang et al., 2007; Marcu, 2007) and the forestbased approach (Mi et al., 2008) would also cover non-constituent phrases to some extent. Shen et al. (2010) defined well-formed dependency structure to cover uncompleted dependency structure in translation rules. In addition to the fact that the constraints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length.</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of ACL-HLT 2008, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>440--447</pages>
<contexts>
<context position="19015" citStr="Och and Ney, 2000" startWordPosition="3088" endWordPosition="3091"> H (line 2-3). Then it respectively calculates the probabilities of predicate-argument reordering and argument-argument reordering(line 4-7). 4 Experiments We have presented our two-level approach to incorporating syntactic and semantic structures in a HPB system. In this section, we test the effect of such structural information on a Chinese-to-English translation task. The baseline system is a reproduction of Chiang’s (2007) HPB system. The bilingual training data contains 1.5M sentence pairs with 39.4M Chinese words and 46.6M English words.4 We obtain the word alignments by running GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. To obtain syntactic parse trees for instantiating syntactic constraints and predicate-argument structures for integrating the PAS reordering model, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and then ran the Chinese seman</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of ACL 2000, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>160--167</pages>
<contexts>
<context position="19265" citStr="Och, 2003" startWordPosition="3132" endWordPosition="3133">ystem. In this section, we test the effect of such structural information on a Chinese-to-English translation task. The baseline system is a reproduction of Chiang’s (2007) HPB system. The bilingual training data contains 1.5M sentence pairs with 39.4M Chinese words and 46.6M English words.4 We obtain the word alignments by running GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. To obtain syntactic parse trees for instantiating syntactic constraints and predicate-argument structures for integrating the PAS reordering model, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and then ran the Chinese semantic role 4This dataset includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 544 System MT 02 MT 04 MT 05 Ave. base HPB 40.00 35.33 32.97 36.10 + basic constraints + unflattened tree 33.90 32.00 29.83 </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL 2003, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>106</pages>
<contexts>
<context position="12873" citStr="Palmer et al., 2005" startWordPosition="2044" endWordPosition="2047">2: Example of PAS on both the source and target side. Items are aligned by indices. divergence between semantic frames of the source and target language. However, considering there is no efficient way of jointly performing MT and SRL, accurate SRL on target side can only be done after translation. Similar to related work (Liu and Gildea, 2010; Xiong et al., 2012), we obtain the PAS of the source language (PAS-S) via a shallow semantic parser and project the PAS of the target language (PAS-T) using the word alignment derived from the translation process. Specifically, we use PropBank standard (Palmer et al., 2005; Xue, 2008) which defines a set of numbered core arguments (i.e., A0-A5) and adjunct-like arguments (e.g., AM-TMP for temporal, AM-MNR for manner). Figure 2(b) shows an example of PAS projection from source language to target language.2 The PAS reordering model describes the probability of reordering PAS-S into PAST. Given a predicate p, it takes the following form: P (PAS-T I PAS-S, PRE=p) (2) Note that cases for untranslated roles can be naturally reflected in our PAS reordering model. For example, if the argument IA0 is untranslated in Figure 2, its PAS-T will be X2X5X3X4. 2In PAS-S, we us</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71– 106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACLHLT</booktitle>
<pages>404--411</pages>
<contexts>
<context position="19552" citStr="Petrov and Klein, 2007" startWordPosition="3172" endWordPosition="3175">6M English words.4 We obtain the word alignments by running GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. To obtain syntactic parse trees for instantiating syntactic constraints and predicate-argument structures for integrating the PAS reordering model, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and then ran the Chinese semantic role 4This dataset includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 544 System MT 02 MT 04 MT 05 Ave. base HPB 40.00 35.33 32.97 36.10 + basic constraints + unflattened tree 33.90 32.00 29.83 31.91 + our constraints + unflattened tree 38.47 34.51 32.15 35.04 + our constraints + flattened tree 38.55 35.38 32.44 35.46 + basic constraints + unflattened tree 35.38 32.89 30.42 32.90 + our constraints + unflattened tree 39.41 36.02 33.21 36.21 + our constraints + flattened tree 40</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of NAACLHLT 2007, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>String-to-dependency statistical machine translation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="30379" citStr="Shen et al. (2010)" startWordPosition="4960" endWordPosition="4963">verage syntactic constraints yet still allow cross-constituent translations. In terms of tree-based SMT with crossconstituent translations, Cowan et al. (2006) allowed non-constituent sub phrases on the source side and adopted phrase-based translation model for modifiers in clauses. Marcu (2006) and Galley et al. (2006) inserted artificial constituent nodes in parsing tree as to capture useful but non-constituent phrases. The parse tree binarization approach (Wang et al., 2007; Marcu, 2007) and the forestbased approach (Mi et al., 2008) would also cover non-constituent phrases to some extent. Shen et al. (2010) defined well-formed dependency structure to cover uncompleted dependency structure in translation rules. In addition to the fact that the constraints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length. In terms of (hierarchical) phrase-based SMT with syntactic constraints, par</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2010</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010. String-to-dependency statistical machine translation. Computational Linguistics, 36(4):649–671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Binarizing syntax trees to improve syntax-based machine translation accuracy.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL</booktitle>
<pages>746--754</pages>
<contexts>
<context position="30242" citStr="Wang et al., 2007" startWordPosition="4937" endWordPosition="4940">elated Work While there has been substantial work on linguistically motivated SMT, we limit ourselves here to several approaches that leverage syntactic constraints yet still allow cross-constituent translations. In terms of tree-based SMT with crossconstituent translations, Cowan et al. (2006) allowed non-constituent sub phrases on the source side and adopted phrase-based translation model for modifiers in clauses. Marcu (2006) and Galley et al. (2006) inserted artificial constituent nodes in parsing tree as to capture useful but non-constituent phrases. The parse tree binarization approach (Wang et al., 2007; Marcu, 2007) and the forestbased approach (Mi et al., 2008) would also cover non-constituent phrases to some extent. Shen et al. (2010) defined well-formed dependency structure to cover uncompleted dependency structure in translation rules. In addition to the fact that the constraints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules t</context>
</contexts>
<marker>Wang, Knight, Marcu, 2007</marker>
<rawString>Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Binarizing syntax trees to improve syntax-based machine translation accuracy. In Proceedings of EMNLPCoNLL 2007, pages 746–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Pascale Fung</author>
</authors>
<title>Semantic roles for smt: A hybrid two-pass model.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<pages>13--16</pages>
<marker>Wu, Fung, 2009</marker>
<rawString>Dekai Wu and Pascale Fung. 2009. Semantic roles for smt: A hybrid two-pass model. In Proceedings of NAACL-HLT 2009, pages 13–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
</authors>
<title>A syntax-driven bracketing model for phrase-based translation.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP</booktitle>
<pages>315--323</pages>
<contexts>
<context position="31370" citStr="Xiong et al. (2009" startWordPosition="5113" endWordPosition="5116">a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length. In terms of (hierarchical) phrase-based SMT with syntactic constraints, particular related to constituent boundaries, Koehn et al. (2003) tested constraints allowing constituent matched phrases only. Chiang (2005) and Cherry (2008) used a soft constraint to award or penalize hypotheses which respect or violate syntactic boundaries. Marton and Resnik (2008) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and 547 Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize the crosslingual match of the semantic frames of the reordered translation to that of the source sentence. Liu and Gildea (2010) added two types of semantic role features into a tree-to-string translation model. Although Xiong et al. (2012) and our work are both focusing on source side PAS reordering, our model differs from thei</context>
</contexts>
<marker>Xiong, Zhang, Aw, Li, 2009</marker>
<rawString>Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2009. A syntax-driven bracketing model for phrase-based translation. In Proceedings of ACL-IJCNLP 2009, pages 315–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Learning translation boundaries for phrase-based decoding.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<pages>136--144</pages>
<marker>Xiong, Zhang, Li, 2010</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learning translation boundaries for phrase-based decoding. In Proceedings of NAACL-HLT 2010, pages 136–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Modeling the translation of predicate-argument structure for smt.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL 2012,</booktitle>
<pages>902--911</pages>
<contexts>
<context position="3975" citStr="Xiong et al. (2012)" startWordPosition="541" endWordPosition="544">vated. In previous work, Liu and Gildea (2010) model the reordering/deletion of source-side semantic roles in a tree-to-string translation model. While it is natural to include semantic structures in a treebased translation model, the effect of semantic structures is presumably limited, since tree templates themselves have already encoded semantics to some 540 Proceedings of NAACL-HLT 2013, pages 540–549, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics extent. For example, template (VP (VBG giving) NP#1 NP#2) entails NP#1 as receiver and NP#2 as thing given. Xiong et al. (2012) model the reordering between predicates and their arguments by assuming arguments are translated as a unit. However, they only considered the reordering between arguments and their predicates. 2 Syntactic Constraints for HPB Translation Model In this section, we briefly review the HPB model, then present our approach to incorporating syntactic constraints into it. 2.1 HPB Translation Model In HPB models, synchronous rules take the form X —* (&apos;y, α, —), where X is the non-terminal symbol, &apos;y and α are strings of lexical items and nonterminals in the source and target side, respectively, and — </context>
<context position="12619" citStr="Xiong et al., 2012" startWordPosition="2002" endWordPosition="2005">19 4* kEt n A4_� wo rengran feichang xihua hongse de qiche a. Word alignment for an English-Chinese sentence pair with semantic roles for the English sentence PAS-S A01 AM-TMP2 VBP3 A14 AM-MNR5 X1 X2 X5 X3 X4 b. PAS-S and PAS-T for predicate like Figure 2: Example of PAS on both the source and target side. Items are aligned by indices. divergence between semantic frames of the source and target language. However, considering there is no efficient way of jointly performing MT and SRL, accurate SRL on target side can only be done after translation. Similar to related work (Liu and Gildea, 2010; Xiong et al., 2012), we obtain the PAS of the source language (PAS-S) via a shallow semantic parser and project the PAS of the target language (PAS-T) using the word alignment derived from the translation process. Specifically, we use PropBank standard (Palmer et al., 2005; Xue, 2008) which defines a set of numbered core arguments (i.e., A0-A5) and adjunct-like arguments (e.g., AM-TMP for temporal, AM-MNR for manner). Figure 2(b) shows an example of PAS projection from source language to target language.2 The PAS reordering model describes the probability of reordering PAS-S into PAST. Given a predicate p, it ta</context>
<context position="31880" citStr="Xiong et al. (2012)" startWordPosition="5194" endWordPosition="5197">) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and 547 Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize the crosslingual match of the semantic frames of the reordered translation to that of the source sentence. Liu and Gildea (2010) added two types of semantic role features into a tree-to-string translation model. Although Xiong et al. (2012) and our work are both focusing on source side PAS reordering, our model differs from theirs in two main aspects: 1) we consider reordering not only between an argument and its predicate, but also between two arguments; and 2) our reordering model can naturally model cases of untranslated arguments or predicates. 7 Conclusion In this paper, we have presented an approach to incorporating syntactic and semantic structures for the HPB translation model. To accommodate the close tie of semantic structures to syntax, we first revisited the idea of hard syntactic constraints, and we demonstrated tha</context>
</contexts>
<marker>Xiong, Zhang, Li, 2012</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Modeling the translation of predicate-argument structure for smt. In Proceedings of ACL 2012, pages 902–911.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="12885" citStr="Xue, 2008" startWordPosition="2048" endWordPosition="2049">both the source and target side. Items are aligned by indices. divergence between semantic frames of the source and target language. However, considering there is no efficient way of jointly performing MT and SRL, accurate SRL on target side can only be done after translation. Similar to related work (Liu and Gildea, 2010; Xiong et al., 2012), we obtain the PAS of the source language (PAS-S) via a shallow semantic parser and project the PAS of the target language (PAS-T) using the word alignment derived from the translation process. Specifically, we use PropBank standard (Palmer et al., 2005; Xue, 2008) which defines a set of numbered core arguments (i.e., A0-A5) and adjunct-like arguments (e.g., AM-TMP for temporal, AM-MNR for manner). Figure 2(b) shows an example of PAS projection from source language to target language.2 The PAS reordering model describes the probability of reordering PAS-S into PAST. Given a predicate p, it takes the following form: P (PAS-T I PAS-S, PRE=p) (2) Note that cases for untranslated roles can be naturally reflected in our PAS reordering model. For example, if the argument IA0 is untranslated in Figure 2, its PAS-T will be X2X5X3X4. 2In PAS-S, we use parts-of-s</context>
</contexts>
<marker>Xue, 2008</marker>
<rawString>Nianwen Xue. 2008. Automatic labeling of semantic roles. Computational Linguistics, 34(4):225–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Zhuang</author>
<author>Chengqing Zong</author>
</authors>
<title>Joint inference for bilingual semantic role labeling.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>304--314</pages>
<contexts>
<context position="2939" citStr="Zhuang and Zong (2010)" startWordPosition="392" endWordPosition="395"> syntactic constraints and the predicateargument reordering model obtain significant improvements over the syntactically and semantically uninformed baseline. In principle, semantic frames (or, more specifically, predicate-argument structures: PAS) seem to be a promising avenue for translational modeling. While languages might diverge syntactically, they are less likely to diverge semantically. This has previously been recognized by Fung et al. (2006), who report that approximately 84% of semantic role mappings remained consistent across translations between English and Chinese. Subsequently, Zhuang and Zong (2010) took advantage of this consistency to jointly model semantic frames on Chinese/English bitexts, yielding improved frame recognition accuracy on both languages. While there has been some encouraging work on integrating syntactic knowledge into Chiang’s HPB model, modeling semantic structure in a linguistically naive translation model is a challenge, because the semantic structures themselves are syntactically motivated. In previous work, Liu and Gildea (2010) model the reordering/deletion of source-side semantic roles in a tree-to-string translation model. While it is natural to include semant</context>
</contexts>
<marker>Zhuang, Zong, 2010</marker>
<rawString>Tao Zhuang and Chengqing Zong. 2010. Joint inference for bilingual semantic role labeling. In Proceedings of EMNLP 2010, pages 304–314.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>