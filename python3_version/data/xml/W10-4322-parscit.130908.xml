<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000774">
<note confidence="0.507387333333333">
Sparse Approximate Dynamic Programming for Dialog Management
Senthilkumar Chandramohan, Matthieu Geist, Olivier Pietquin
SUPELEC - IMS Research Group, Metz - France.
</note>
<email confidence="0.950497">
{senthilkumar.chandramohan, matthieu.geist, olivier.pietquin}@supelec.fr
</email>
<sectionHeader confidence="0.99408" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99973775">
Spoken dialogue management strategy op-
timization by means of Reinforcement
Learning (RL) is now part of the state of
the art. Yet, there is still a clear mis-
match between the complexity implied by
the required naturalness of dialogue sys-
tems and the inability of standard RL al-
gorithms to scale up. Another issue is the
sparsity of the data available for training in
the dialogue domain which can not ensure
convergence of most of RL algorithms.
In this paper, we propose to combine a
sample-efficient generalization framework
for RL with a feature selection algorithm
for the learning of an optimal spoken dia-
logue management strategy.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99976726984127">
Optimization of dialogue management strategies
by means of Reinforcement Learning (RL) (Sut-
ton and Barto, 1998) is now part of the state of
the art in the research area of Spoken Dialogue
Systems (SDS) (Levin and Pieraccini, 1998; Singh
et al., 1999; Pietquin and Dutoit, 2006; Williams
and Young, 2007). It consists in casting the dia-
logue management problem into the Markov Deci-
sion Processes (MDP) paradigm (Bellman, 1957)
and solving the associated optimization problem.
Yet, there is still a clear mismatch between the
complexity implied by the required naturalness of
the dialogue systems and the inability of standard
RL algorithms to scale up. Another issue is the
sparsity of the data available for training in the
dialogue domain because collecting and annotat-
ing data is very time consuming. Yet, RL algo-
rithms are very data demanding and low amounts
of data can not ensure convergence of most of
RL algorithms. This latter problem has been ex-
tensively studied in the recent years and is ad-
dressed by simulating new dialogues thanks to
a statistical model of human-machine interaction
(Pietquin, 2005) and user modeling (Eckert et al.,
1997; Pietquin and Dutoit, 2006; Schatzmann et
al., 2006). However, this results in a variability of
the learned strategy depending on the user model-
ing method (Schatzmann et al., 2005) and no com-
mon agreement exists on the best user model.
The former problem, that is dealing with com-
plex dialogue systems within the RL framework,
has received much less attention. Although some
works can be found in the SDS literature it is far
from taking advantage of the large amount of ma-
chine learning literature devoted to this problem.
In (Williams and Young, 2005), the authors reduce
the complexity of the problem (which is actually a
Partially Observable MDP) by automatically con-
densing the continuous state space in a so-called
summary space. This results in a clustering of the
state space in a discrete set of states on which stan-
dard RL algorithms are applied. In (Henderson et
al., 2008), the authors use a linear approximation
scheme and apply the SARSA(A) algorithm (Sut-
ton and Barto, 1998) in a batch setting (from data
and not from interactions or simulations). This al-
gorithm was actually designed for online learning
and is known to converge very slowly. It there-
fore requires a lot of data and especially in large
state spaces. Moreover, the choice of the features
used for the linear approximation is particularly
simple since features are the state variables them-
selves. The approximated function can therefore
not be more complex than an hyper-plane in the
state variables space. This drawback is shared by
the approach of (Li et al., 2009) where a batch al-
gorithm (Least Square Policy Iteration or LSPI) is
combined to a pruning method to only keep the
most meaningful features. In addition the com-
plexity of LSPI is O(p3).
In the machine learning community, this issue
is actually addressed by function approximation
accompanied with dimensionality reduction. The
</bodyText>
<note confidence="0.623171">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 107–115,
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</note>
<page confidence="0.998873">
107
</page>
<bodyText confidence="0.999915655172414">
data sparsity problem is also widely addressed in
this literature, and sample-efficiency is one main
trend of research in this field. In this paper, we
propose to combine a sample-efficient batch RL
algorithm (namely the Fitted Value Iteration (FVI)
algorithm) with a feature selection method in a
novel manner and to apply this original combi-
nation to the learning of an optimal spoken dia-
logue strategy. Although the algorithm uses a lin-
ear combination of features (or basis functions),
these features are much richer in their ability of
representing complex functions.
The ultimate goal of this research is to provide
a way of learning optimal dialogue policies for a
large set of situations from a small and fixed set of
annotated data in a tractable way.
The rest of this paper is structured as follows.
Section 2 gives a formal insight of MDP and
briefly reminds the casting of the dialogue prob-
lem into the MDP framework. Section 3.2 pro-
vides a description of approximate Dynamic Pro-
gramming along with LSPI and FVI algorithms.
Section 4 provides an overview on how LSPI and
FVI can be combined with a feature selection
scheme (which is employed to learn the represen-
tation of the Q-function from the dialogue corpus).
Our experimental set-up, results and a comparison
with state-of-the-art methods are presented in Sec-
tion 5. Eventually, Section 6 concludes.
</bodyText>
<sectionHeader confidence="0.95599" genericHeader="method">
2 Markov Decision Processes
</sectionHeader>
<bodyText confidence="0.999738157894737">
The MDP (Puterman, 1994) framework is used
to describe and solve sequential decision mak-
ing problems or equivalently optimal control prob-
lems in the case of stochastic dynamic systems.
An MDP is formally a tuple {5, A, P, R, -y} where
5 is the (finite) state space, A the (finite) action
space, P E P(5)SxA the family of Markovian
transition probabilities1, R E RSxAxS the reward
function and -y the discounting factor (0 &lt; -y &lt; 1).
According to this formalism, a system to be con-
trolled steps from state to state (s E 5) according
to transition probabilities P as a consequence of
the controller’s actions (a E A). After each tran-
sition, the system generates an immediate reward
(r) according to its reward function R. How the
system is controlled is modeled with a so-called
policy 7r E AS mapping states to actions. The
quality of a policy is quantified by the so-called
value function which maps each state to the ex-
</bodyText>
<footnote confidence="0.505282">
1Notation f E AB is equivalent to f : B --+ A
</footnote>
<bodyText confidence="0.99893225">
pected discounted cumulative reward given that
the agent starts in this state and follows the policy
7r: V π(s) = E[E00 i=0 -yiri|s0 = s, 7r]. An optimal
policy 7r* maximizes this function for each state:
7r* = argmaxπ V π. Suppose that we are given the
optimal value function V * (that is the value func-
tion associated to an optimal policy), deriving the
associated policy would require to know the transi-
tion probabilities P. Yet, this is usually unknown.
This is why the state-action value (or Q-) function
is introduced. It adds a degree of freedom on the
choice of the first action:
</bodyText>
<equation confidence="0.962061">
-yiri|s0 = s, a0 = a, 7r] (1)
</equation>
<bodyText confidence="0.9870255">
The optimal policy is noted 7r* and the related
Q-function Q*(s, a). An action-selection strategy
that is greedy according to this function (7r(s) =
argmaxa Q*(s, a)) provides an optimal policy.
</bodyText>
<subsectionHeader confidence="0.993628">
2.1 Dialogue as an MDP
</subsectionHeader>
<bodyText confidence="0.999961933333333">
The casting of the spoken dialogue management
problem into the MDP framework (MDP-SDS)
comes from the equivalence of this problem to
a sequential decision making problem. Indeed,
the role of the dialogue manager (or the decision
maker) is to select and perform dialogue acts (ac-
tions in the MDP paradigm) when it reaches a
given dialogue turn (state in the MDP paradigm)
while interacting with a human user. There can
be several types of system dialogue acts. For
example, in the case of a restaurant information
system, possible acts are request(cuisine type),
provide(address), confirm(price range), close etc.
The dialogue state is usually represented effi-
ciently by the Information State paradigm (Lars-
son and Traum, 2000). In this paradigm, the di-
alogue state contains a compact representation of
the history of the dialogue in terms of system acts
and its subsequent user responses (user acts). It
summarizes the information exchanged between
the user and the system until the considered state
is reached.
A dialogue management strategy is thus a map-
ping between dialogue states and dialogue acts.
Still following the MDP’s definitions, the optimal
strategy is the one that maximizes some cumula-
tive function of rewards collected all along the in-
teraction. A common choice for the immediate
reward is the contribution of each action to user
satisfaction (Singh et al., 1999). This subjective
</bodyText>
<equation confidence="0.9789545">
Qπ(s, a) = E[
00
E
i=0
</equation>
<page confidence="0.993286">
108
</page>
<bodyText confidence="0.99951275">
reward is usually approximated by a linear com-
bination of objective measures like dialogue dura-
tion, number of ASR errors, task completion etc.
(Walker et al., 1997).
</bodyText>
<sectionHeader confidence="0.988122" genericHeader="method">
3 Solving MDPs
</sectionHeader>
<subsectionHeader confidence="0.999604">
3.1 Dynamic Programming
</subsectionHeader>
<bodyText confidence="0.998647090909091">
Dynamic programming (DP) (Bellman, 1957)
aims at computing the optimal policy π∗ if the
transition probabilities and the reward function are
known.
First, the policy iteration algorithm computes
the optimal policy in an iterative way. The ini-
tial policy is arbitrary set to π0. At iteration k, the
policy πk−1 is evaluated, that is the associated Q-
function Qπk−1(s, a) is computed. To do so, the
Markovian property of the transition probabilities
is used to rewrite Equation (1) as :
</bodyText>
<equation confidence="0.9916765">
Qπ(s, a) = Es&apos;|s,a[R(s, a, s0) + γQπ(s0, π(s0))]
= TπQπ(s, a) (2)
</equation>
<bodyText confidence="0.999350473684211">
This is the so-called Bellman evaluation equa-
tion and Tπ is the Bellman evaluation opera-
tor. Tπ is linear and therefore this defines a lin-
ear system that can be solved by standard meth-
ods or by an iterative method using the fact
that Qπ is the unique fixed-point of the Bell-
man evaluation operator (Tπ being a contrac-
Qπi−1, V�Qπ0 limi→∞ �Qπi =
Qπ. Then the policy is improved, that is
πk is greedy respectively to Qπk−1: πk(s) =
argmaxa∈A Qπk−1(s, a). Evaluation and im-
provement steps are iterated until convergence of
πk to π∗ (which can be demonstrated to happen in
a finite number of iterations when πk = πk−1).
The value iteration algorithm aims at estimat-
ing directly the optimal state-action value function
Q∗ which is the solution of the Bellman optimality
equation (or equivalently the unique fixed-point of
the Bellman optimality operator T∗):
</bodyText>
<equation confidence="0.983529333333333">
Q∗(s, a) = Es1|s,a[R(s, a, s0) + γ maxQ∗(s0, b)]
b∈A
= T∗Q∗(s, a) (3)
</equation>
<bodyText confidence="0.999917">
The T∗ operator is not linear, therefore comput-
ing Q∗ via standard system-solving methods is not
possible. However, it can be shown that T∗ is
also a contraction (Puterman, 1994). Therefore,
according to Banach fixed-point theorem, Q∗ can
be estimated using the following iterative way:
</bodyText>
<equation confidence="0.954781">
�Q∗i = T∗ �Q∗i−1, V�Q∗0
i→∞
lim �Q∗i = Q∗ (4)
</equation>
<bodyText confidence="0.800418666666667">
However, the convergence takes an infinite num-
ber of iterations. Practically speaking, iterations
are stopped when some criterion is met, classi-
cally a small difference between two iterations:
11 Q∗i −
�
icy (which is what we are ultimately interested in)
is greedy respectively to the estimated optimal Q-
function: �r∗(s) = argmaxa∈A �Q∗(s, a).
</bodyText>
<subsectionHeader confidence="0.999688">
3.2 Approximate Dynamic Programming
</subsectionHeader>
<bodyText confidence="0.999937083333333">
DP-based approaches have two drawbacks. First,
they assume the transition probabilities and the re-
ward function to be known. Practically, it is rarely
true and especially in the case of spoken dialogue
systems. Most often, only examples of dialogues
are available which are actually trajectories in the
state-action space. Second, it assumes that the Q-
function can be exactly represented. However, in
real world dialogue management problems, state
and action spaces are often too large (even contin-
uous) for such an assumption to hold. Approxi-
mate Dynamic Programming (ADP) aims at esti-
mating the optimal policy from trajectories when
the state space is too large for a tabular representa-
tion. It assumes that the Q-function can be approx-
imated by some parameterized function Qθ(s, a).
In this paper, a linear approximation of the Q-
function will be assumed: Qθ(s, a) = θT φ(s, a).
where θ E Rp is the parameter vector and φ(s, a)
is the set of p basis functions. All functions ex-
pressed in this way define a so-called hypothesis
space H = {�Qθ|θ E Rp}. Any function Q can be
projected onto this hypothesis space by the opera-
tor H defined as
</bodyText>
<equation confidence="0.99057">
HQ = argmin 11Q − �Qθ112. (5)
ˆQB∈H
</equation>
<bodyText confidence="0.999980333333333">
The goal of the ADP algorithms explained in the
subsequent sections is to compute the best set of
parameters θ given the basis functions.
</bodyText>
<subsectionHeader confidence="0.814999">
3.2.1 Least-Squares Policy Iteration
</subsectionHeader>
<bodyText confidence="0.999587">
The least-squares policy iteration (LSPI) algo-
rithm has been introduced by Lagoudakis and Parr
(2003). The underlying idea is exactly the same
as for policy iteration: interleaving evaluation and
improvement steps. The improvement steps are
same as before, but the evaluation step should
learn an approximate representation of the Q-
function using samples. In LSPI, this is done using
the Least-Squares Temporal Differences (LSTD)
algorithm of Bradtke and Barto (1996).
</bodyText>
<equation confidence="0.86695575">
Qπ = Tπ
i
tion):
�Q∗i−111 &lt; ξ. The estimated optimal pol-
</equation>
<page confidence="0.988619">
109
</page>
<bodyText confidence="0.99763705">
LSTD aims at minimizing the distance between
the approximated Q-function ˆQθ and the projec-
tion onto the hypothesis space of its image through
the Bellman evaluation operator ΠTπ ˆQθ: θπ =
argminθ∈Rp kˆQθ − ΠTπˆQθk2. This can be in-
terpreted as trying to minimize the difference be-
tween the two sides of the Bellman equation (1)
(which should ideally be zero) in the hypothesis
space. Because of the approximation, this differ-
ence is most likely to be non-zero.
Practically, Tπ is not known, but a set of
N transitions {(sj, aj, rj, s0j)1≤j≤N} is available.
LSTD therefore solves the following optimiza-
tion problem: θπ = argminθ E 1 CNj ((θ) where
CNj (θ) = (rj +γˆQθ, (s0j, π(s0j)) −γˆQθ(sj, aj))2.
Notice that θπ appears in both sides of the equa-
tion, which renders this problem difficult to solve.
However, thanks to the linear parametrization, it
admits an analytical solution, which defines the
LSTD algorithm:
</bodyText>
<equation confidence="0.985113333333333">
N N
θπ = ( φjΔφπj )−1 φjrj (6)
j=1 j=1
</equation>
<bodyText confidence="0.948077">
with φj = φ(sj, aj) and Δφπj = φ(sj, aj) −
γφ(s0j, π(s0j)).
LSPI is initialized with a policy π0. Then, at
iteration k, the Q-function of policy πk−1 is esti-
mated using LSTD, and πk is greedy respectively
to this estimated state-action value function. Itera-
tions are stopped when some stopping criterion is
met (e.g., small differences between consecutive
policies or associated Q-functions).
</bodyText>
<subsectionHeader confidence="0.745962">
3.2.2 Least-Squares Fitted Value Iteration
</subsectionHeader>
<bodyText confidence="0.999913833333333">
The Fitted Value Iteration (FVI) class of algo-
rithms (Bellman and Dreyfus, 1959; Gordon,
1995; Ernst et al., 2005) generalizes value iter-
ation to model-free and large state space prob-
lems. The T∗ operator (eq. (3)) being a con-
traction, a straightforward idea would be to apply
it iteratively to the approximation similarly to eq.
(4): ˆQθk = T∗ ˆQθk−1. However, T∗ ˆQθ does not
necessarily lie in H, it should thus be projected
again onto the hypothesis space H. By consider-
ing the same projection operator Π as before, this
leads to finding the parameter vector θ satisfying:
ˆQ∗θ = ΠT∗ ˆQ∗θ. The fitted-Q algorithm (a spe-
cial case of FVI) assumes that the composed ΠT∗
operator is a contraction and therefore admits an
unique fixed point, which is searched for through
the classic iterative scheme: ˆQθk = ΠT∗ˆQθk−1.
However, the model (transition probabilities and
the reward function) is usually not known, there-
fore a sampled Bellman optimality operator Tˆ∗
is considered instead. For a transition sample
(sj, aj, rj, s0j), it is defined as: Tˆ∗Q(sj, aj) =
rj + γ maxa∈A Q(s0j, a). This defines the general
fitted-Q algorithm (θ0 being chosen by the user):
</bodyText>
<equation confidence="0.998189">
ˆ ˆ ˆ
Qθk = Π T∗ Qθk−1. Fitted-Q can then be special-
</equation>
<bodyText confidence="0.987137222222222">
ized by choosing how Tˆ∗ ˆQθk−1 is projected onto
the hypothesis space, that is the supervised learn-
ing algorithm that solves the projection problem
of eq. (5). The least squares algorithm is chosen
here.
The parametrization being linear, and a train-
ing base {(sj, aj, rj, s0j)1≤j≤N} being available,
the least-squares fitted-Q (LSFQ for short) is de-
rived as follows (we note φ(sj, aj) = φj):
</bodyText>
<equation confidence="0.994489666666667">
( �ˆ �ˆQB���(sj, aj) −ˆQB(sj, aj))2 (7)
Oj(rj + ry max
aE�(B�k−1O(s�j, a)))
</equation>
<bodyText confidence="0.999516666666667">
Equation (7) defines an iteration of the proposed
linear least-squares-based fitted-Q algorithm. An
initial parameter vector θ0 should be chosen, and
iterations are stopped when some criterion is met
(maximum number of iterations or small differ-
ence between two consecutive parameter vector
estimates). Assuming that there are M itera-
tions, the optimal policy is estimated as ˆπ∗(s) =
argmaxa∈A ˆQθM (s, a).
</bodyText>
<sectionHeader confidence="0.909384" genericHeader="method">
4 Learning a sparse parametrization
</sectionHeader>
<bodyText confidence="0.999817058823529">
LSPI and LSFQ (FVI) assume that the basis func-
tions are chosen beforehand. However, this is dif-
ficult and problem-dependent. Thus, we propose
to combine these algorithms with a scheme which
learns the representation from dialogue corpora.
Let’s place ourselves in a general context. We
want to learn a parametric representation for an
approximated function fθ(z) = θT φ(z) from
samples {z1, ... , zN}. A classical choice is to
choose a kernel-based representation (Scholkopf
and Smola, 2001). Formally, a kernel K(z, ˜zi)
is a continuous, positive and semi-definite func-
tion (e.g., Gaussian or polynomial kernels) cen-
tered on ˜zi. The feature vector φ(z) is therefore
of the form: φ(z) = (K(z, ˜z1) ... K(z, ˜zp)).
The question this section answers is the following:
given the training basis {z1, ... , zN} and a kernel
</bodyText>
<equation confidence="0.963924692307692">
Bk = argmin
BEf2P
N
E
j=1
N
E
j=1
= (
N
E
j=1
OjO j )−1
</equation>
<page confidence="0.974306">
110
</page>
<bodyText confidence="0.999987692307693">
K, how to choose the number p of basis functions
and the associated kernel centers (z1, ... , ip)?
An important result about kernels is the Mer-
cer theorem, which states that for each kernel
K there exists a mapping ϕ : z ∈ Z →
ϕ(z) ∈ F such that ∀z1, z2 ∈ Z, K(z1, z2) =
hϕ(z1), ϕ(z2)i (in short, K defines a dot prod-
uct in F). The space F is called the feature
space, and it can be of infinite dimension (e.g.,
Gaussian kernel), therefore ϕ cannot always be
explicitly built. Given this result and from the
bilinearity of the dot product, fθ can be rewrit-
ten as follows: fθ(z) = Ep 1θiK(z, zi) =
hϕ(z), EP 1 θiϕ(Z-i)i. —
parametrization corresponds to a linear approx-
imation in the feature space, the weight vector
being Ep i�1 θiϕ(zi). This is called the kernel
trick. Consequently, kernel centers (z1, ... , ip)
should be chosen such that (ϕ(z1), ... , ϕ(ip)) are
linearly independent in order to avoid using re-
dundant basis functions. Moreover, kernel cen-
ters should be chosen among the training samples.
To sum up, learning such a parametrization re-
duces to finding a dictionary D = (z1, ... , ip) ∈
{z1, ... , zN} such that (ϕ(z1), ... , ϕ(ip)) are lin-
early independent and such that they span the
same subspace as (ϕ(z1), ... , ϕ(zN)). Engel et
al. (2004) provides a dictionary method to solve
this problem, briefly sketched here.
The training base is sequentially processed, and
the dictionary is initiated with the first sample:
D1 = {z1}. At iteration k, a dictionary Dk−1
computed from {z1, ... , zk−1} is available and the
kth sample zk is considered. If ϕ(zk) is linearly
independent of ϕ(Dk−1), then it is added to the
dictionary: Dk = Dk−1 ∪ {zk}. Otherwise, the
dictionary remains unchanged: Dk = Dk−1. Lin-
ear dependency can be checked by solving the
following optimization problem (pk−1 being the
</bodyText>
<equation confidence="0.914987666666667">
size of Dk−1): δ = argminw∈Rpk_1 kϕ(zk) −
Epk�1
i�1 wiϕ(�zi)k2.
</equation>
<bodyText confidence="0.999010583333333">
is the fact that hϕ(zk), ϕ(zi)i = K(zk, ii)) and to
the bilinearity of the dot product, this optimization
problem can be solved analytically and without
computing explicitly ϕ. Formally, linear depen-
dency is satisfied if δ = 0. However, an approxi-
mate linear dependency is allowed, and ϕ(zk) will
be considered as linearly dependent of ϕ(Dk−1) if
δ &lt; ν, where ν is the so-called sparsification fac-
tor. This allows controlling the trade-off between
quality of the representation and its sparsity. See
Engel et al. (2004) for details as well as an efficient
implementation of this dictionary approach.
</bodyText>
<subsectionHeader confidence="0.996777">
4.1 Resulting algorithms
</subsectionHeader>
<bodyText confidence="0.995089608695653">
We propose to combine LSPI and LSFQ with the
sparsification approach exposed in the previous
section: a kernel is chosen, the dictionary is com-
puted and then LSPI or LSFQ is applied using the
learnt basis functions. For LSPI, this scheme has
been proposed before by Xu et al. (2007) (with
the difference that they generate new trajectories
at each iteration whereas we use the same for all
iterations). The proposed sparse LSFQ algorithm
is a novel contribution of this paper.
We start with the sparse LSFQ algorithm. In or-
der to train the dictionary, the inputs are needed
(state-action couples in this case), but not the out-
puts (reward are not used). For LSFQ, the input
space remains the same over iterations, therefore
the dictionary can be computed in a preprocessing
step from {(sj, aj)1≤j≤N}. Notice that the matrix
(ENj�1 φjφTj )−1 remains also the same over itera-
tions, therefore it can be computed in a preprocess-
ing step too. The proposed sparse LSFQ algorithm
is summarized in appendix Algorithm 1.
For the sparse LSPI algorithm, things are
different. This time, the inputs depend on
the iteration. More precisely, at iteration k,
the input is composed of state-action couples
(sj, aj) but also of transiting state-action cou-
ples (s0 j, πk−1(s0 j)). Therefore the dictionary
has to be computed at each iteration from
{(sj,aj)1≤j≤N,(s0j,πk−1(s0j))1≤j≤N}. This de-
fines the parametrization which is considered for
the Q-function evaluation. The rest of the algo-
rithm is as for the classic LSPI and it is summa-
rized in appendix Algorithm 2.
Notice that sparse LSFQ has a lower computa-
tional complexity than the sparse LSPI. For sparse
LSFQ, dictionary and the matrix P−1 are com-
puted in a preprocessing step, therefore the com-
plexity per iteration is in O(p2), with p being
the number of basis functions computed using the
dictionary method. For LSPI, the inverse matrix
depends on the iteration, as well as the dictio-
nary, therefore the computational complexity is in
O(pk) per iteration, where pk is the size of the dic-
tionary computed at the kth iteration.
Therefore, a kernel-based
Thanks to the kernel trick (that
</bodyText>
<page confidence="0.998895">
111
</page>
<sectionHeader confidence="0.949958" genericHeader="method">
5 Experimental set-up and results
</sectionHeader>
<subsectionHeader confidence="0.995499">
5.1 Dialogue task and RL parameters
</subsectionHeader>
<bodyText confidence="0.999991380952381">
The experimental setup is a form-filling dialogue
system in the tourist information domain similar to
the one studied in (Lemon et al., 2006). The sys-
tem aims to give information about restaurants in
the city based on specific user preferences. Three
slots are considered: (i) location, (ii) cuisine and
(iii) price-range of the restaurant. The dialogue
state has three continuous components ranging
from 0 to 1, each representing the average of filling
and confirmation confidence of the corresponding
slots. The MDP SDS has 13 actions: Ask-slot
(3 actions), Explicit-confirm (3 actions), Implicit-
confirm and Ask-slot value (6 actions) and Close-
dialogue (1 action). The ry parameter was set to
0.95 in order to encourage delayed rewards and
also to induce an implicit penalty for the length of
the dialogue episode. The reward function R is
presented as follows: every correct slot filling is
awarded 25, every incorrect slot filling is awarded
-75 and every empty slot filling is awarded -300.
The reward is awarded at the end of the dialogue.
</bodyText>
<subsectionHeader confidence="0.998415">
5.2 Dialogue corpora for policy optimization
</subsectionHeader>
<bodyText confidence="0.999991971428572">
So as to perform sparse LSFQ or sparse LSPI, a di-
alogue corpus which represents the problem space
is needed. As for any batch learning method, the
samples used for learning should be chosen (if
they can be chosen) to span across the problem
space. In this experiment, a user simulation tech-
nique was used to generate the data corpora. This
way, the sensibility of the method to the size of
the training data-set could be analyzed (available
human-dialogue corpora are limited in size). The
user simulator was plugged to the DIPPER (Lemon
et al., 2006) dialogue management system to gen-
erate dialogue samples. To generate data, the dia-
logue manager strategy was jointly based on a sim-
ple hand-coded policy (which aims only to fill all
the slots before closing the dialogue episode irre-
spective of slot confidence score i.e.,) and random
action selection.
Randomly selected system acts are used with
probability E and hand-coded policy selected sys-
tem acts are used with probability (1-E). During
our data generation process the E value was set to
0.9. Rather than using a fully random policy we
used an E-greedy policy to ensure that the prob-
lem space is well sampled and in the same time at
least few episodes have successful completion of
task compared to a totally random policy. We ran
56,485 episodes between the policy learner and
an unigram user simulation, using the E-greedy
policy (of which 65% are successful task com-
pletion episodes) and collected 393,896 dialogue
turns (state transitions). The maximum episode
length is set as 100 dialogue turns. The dialogue
turns (samples) are then divided into eight differ-
ent training sets each with 5.104 samples.
</bodyText>
<subsectionHeader confidence="0.99795">
5.3 Linear representation of Q-function
</subsectionHeader>
<bodyText confidence="0.999991735294118">
Two different linear representations of the Q-
function were used. First, a set of basis functions
computed using the dictionary method outlined in
Section 4 is used. A Gaussian kernel is used for
the dictionary computation (Q = 0.25). The num-
ber of elements present in the dictionary varied
based on the number of samples used for computa-
tion and the sparsification factor. It was observed
during the experiments that including a constant
term to the Q-function representation (value set
to 1) in addition to features selected by the dic-
tionary method avoided weight divergence. Our
second representation of Q-function used a set of
hand-picked features presented as a set of Gaus-
sian functions, centered in pi and with the same
standard deviation Qi = Q). Our RBF network
had 3 Gaussians for each dimension in the state
vector and considering that we have 13 actions, in
total we used 351 (i.e, 33 x 13) features for ap-
proximating the Q-function. This allows consider-
ing that each state variable contributes to the value
function differently according to its value contrar-
ily to similar work (Li et al., 2009; Henderson et
al., 2008) that considers linear contribution of each
state variable. Gaussians were centered at pi = 0.0,
0.5, 1.0 in every dimension with a standard devi-
ation Qi = Q = 0.25. Our stopping criteria was
based on comparison between L1 norm of suc-
ceeding weights and a threshold � which was set to
10−2 i.e, convergence if �i ( 0ni− BZ −1 1) &lt; ,
where n is the iteration number. For sparse LSPI
since the dictionary is computed during each iter-
ation, stopping criteria based on � is not feasible
thus the learning was stopped after 30 iterations.
</bodyText>
<subsectionHeader confidence="0.99926">
5.4 Evaluation of learned policy
</subsectionHeader>
<bodyText confidence="0.999626">
We ran a set of learning iterations using two differ-
ent representations of Q-function and with differ-
ent numbers of training samples (one sample is a
dialogue turn, that is a state transition is, a, r, s&apos;}).
The number of samples used for training ranged
</bodyText>
<page confidence="0.99667">
112
</page>
<figureCaption confidence="0.999787">
Figure 1: FittedQ policy evaluation statistics Figure 2: LSPI policy evaluation statistics
</figureCaption>
<bodyText confidence="0.999962">
from 1.103 to 50.103 samples (no convergence of
weights was observed with fewer samples than
1.103). The training is repeated for each of the 8
training data sets. Dictionary computed using dif-
ferent number of training samples and with v=0.7
and 0.8 had a maximum of 367 and 306 elements
respectively (with lower values of v the number
of features is higher than the hand-selected ver-
sion). The policies learned were then tested us-
ing a unigram user simulation and the DIPPER di-
alogue management framework. Figures 1 and 2
show the average discounted sum of rewards of
policies tested over 8×25 dialogue episodes.
</bodyText>
<subsectionHeader confidence="0.998898">
5.5 Analysis of evaluation results
</subsectionHeader>
<bodyText confidence="0.999893382978723">
Our experimental results show that the dialogue
policies learned using sparse SLFQ and LSPI with
the two different Q-function representations per-
form significantly better than the hand-coded pol-
icy. Most importantly it can be observed from
Figure 1 and 2 that the performance of sparse
LSFQ and sparse LSPI (which uses the dictionary
method for feature selection) are nearly as good
as LSFQ and LSPI (which employs more numer-
ous hand-selected basis functions). This shows the
effectiveness of using the dictionary method for
learning the representation of the Q-function from
the dialogue corpora. For this specific problem
the set of hand selected features seem to perform
better than sparse LSPI and sparse LSFQ, but this
may not be always the case. For complex dialogue
management problems feature selection methods
such as the one studied here will be handy since
the option of manually selecting a good set of fea-
tures will cease to exist.
Secondly it can be concluded that, similar to
LSFQ and LSPI, the sparse LSFQ and sparse LSPI
based dialogue management are also sample effi-
cient and needs only few thousand samples (recall
that a sample is a dialogue turn and not a dialogue
episode) to learn fairly good policies, thus exhibit-
ing a possibility to learn a good policy directly
from very limited amount of dialogue examples.
We believe this is a significant improvement when
compared to the corpora requirement for dialogue
management using other RL algorithms such as
SARSA. However, sparse LSPI seems to result in
poorer performance compared to sparse LSFQ.
One key advantage of using the dictionary
method is that only mandatory basis functions are
selected to be part of the dictionary. This results
in fewer feature weights ensuring faster conver-
gence during training. From Figure 1 it can also
be observed that the performance of both LSFQ
and LSPI (using hand selected features) are nearly
identical. From a computational complexity point
of view, LSFQ and LSPI roughly need the same
number of iterations before the stopping criterion
is met. However, reminding that the proposed
LSFQ complexity is O(p)2 per iteration whereas
LSPI complexity is O(p3) per iteration, LSFQ is
computationally less intensive.
</bodyText>
<sectionHeader confidence="0.997683" genericHeader="method">
6 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999937090909091">
In this paper, we proposed two sample-efficient
generalization techniques to learn optimal dia-
logue policies from limited amounts of dialogue
examples (namely sparse LSFQ and LSPI). Par-
ticularly, a novel sparse LSFQ method has been
proposed and was demonstrated to out-perform
handcrafted and LSPI-based policies while using
a limited number of features. By using a kernel-
based approximation scheme, the power of repre-
sentation of the state-action value function (or Q-
function) is increased with comparison to state-of-
</bodyText>
<page confidence="0.997406">
113
</page>
<bodyText confidence="0.999895142857143">
the-art algorithms (such as (Li et al., 2009; Hen-
derson et al., 2008)). Yet the number of features is
also increased. Using a sparsification algorithm,
this number is reduced while policy performances
are kept. In the future, more compact representa-
tion of the state-action value function will be in-
vestigated such as neural networks.
</bodyText>
<sectionHeader confidence="0.997544" genericHeader="evaluation">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996556">
The work presented here is part of an ongoing re-
search for CLASSiC project (Grant No. 216594,
www.classic-project.org) funded by the European
Commission’s 7th Framework Programme (FP7).
</bodyText>
<sectionHeader confidence="0.998287" genericHeader="conclusions">
References
</sectionHeader>
<reference confidence="0.999806417582417">
Richard Bellman and Stuart Dreyfus. 1959. Functional
approximation and dynamic programming. Math-
ematical Tables and Other Aids to Computation,
13:247–251.
Richard Bellman. 1957. Dynamic Programming.
Dover Publications, sixth edition.
Steven J. Bradtke and Andrew G. Barto. 1996. Lin-
ear Least-Squares algorithms for temporal differ-
ence learning. Machine Learning, 22(1-3):33–57.
Wieland Eckert, Esther Levin, and Roberto Pieraccini.
1997. User Modeling for Spoken Dialogue System
Evaluation. In ASRU’97, pages 80–87.
Yaakov Engel, Shie Mannor, and Ron Meir. 2004. The
Kernel Recursive Least Squares Algorithm. IEEE
Transactions on Signal Processing, 52:2275–2285.
Damien Ernst, Pierre Geurts, and Louis Wehenkel.
2005. Tree-Based Batch Mode Reinforcement
Learning. Journal of Machine Learning Research,
6:503–556.
Geoffrey Gordon. 1995. Stable Function Approxima-
tion in Dynamic Programming. In ICML’95.
James Henderson, Oliver Lemon, and Kallirroi
Georgila. 2008. Hybrid reinforcement/supervised
learning of dialogue policies from fixed data sets.
Computational Linguistics, vol. 34(4), pp 487-511.
Michail G. Lagoudakis and Ronald Parr. 2003. Least-
squares policy iteration. Journal of Machine Learn-
ing Research, 4:1107–1149.
Staffan Larsson and David R. Traum. 2000. Informa-
tion state and dialogue management in the TRINDI
dialogue move engine toolkit. Natural Language
Engineering, vol. 6, pp 323–340.
Oliver Lemon, Kallirroi Georgila, James Henderson,
and Matthew Stuttle. 2006. An ISU dialogue sys-
tem exhibiting reinforcement learning of dialogue
policies: generic slot-filling in the TALK in-car sys-
tem. In EACL’06, Morristown, NJ, USA.
Esther Levin and Roberto Pieraccini. 1998. Us-
ing markov decision process for learning dialogue
strategies. In ICASSP’98.
Lihong Li, Suhrid Balakrishnan, and Jason Williams.
2009. Reinforcement Learning for Dialog Man-
agement using Least-Squares Policy Iteration and
Fast Feature Selection. In InterSpeech’09, Brighton
(UK).
Olivier Pietquin and Thierry Dutoit. 2006. A prob-
abilistic framework for dialog simulation and opti-
mal strategy learning. IEEE Transactions on Audio,
Speech &amp; Language Processing, 14(2): 589-599.
Olivier Pietquin. 2005. A probabilistic descrip-
tion of man-machine spoken communication. In
ICME’05, pages 410–413, Amsterdam (The Nether-
lands), July.
Martin L. Puterman. 1994. Markov Decision Pro-
cesses: Discrete Stochastic Dynamic Programming.
Wiley-Interscience, April.
Jost Schatzmann, Matthew N. Stuttle, Karl Weilham-
mer, and Steve Young. 2005. Effects of the
user model on simulation-based learning of dialogue
strategies. In ASRU’05, December.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of dia-
logue management strategies. Knowledge Engineer-
ing Review, vol. 21(2), pp. 97–126.
Bernhard Scholkopf and Alexander J. Smola. 2001.
Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. MIT
Press, Cambridge, MA, USA.
Satinder Singh, Michael Kearns, Diane Litman, and
Marilyn Walker. 1999. Reinforcement learning for
spoken dialogue systems. In NIPS’99. Springer.
Richard S. Sutton and Andrew G. Barto. 1998. Re-
inforcement Learning: An Introduction (Adaptive
Computation and Machine Learning). The MIT
Press, 3rd edition, March.
Marilyn A. Walker, Diane J. Litman, Candace A.
Kamm, and Alicia Abella. 1997. PARADISE: A
framework for evaluating spoken dialogue agents.
In ACL’97, pages 271–280, Madrid (Spain).
Jason Williams and Steve Young. 2005. Scaling up
pomdps for dialogue management: the summary
pomdp method. In ASRU’05.
Jason D. Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken di-
alog systems. Computer Speech and Language, vol.
21(2), pp. 393–422.
Xin Xu, Dewen Hu, and Xicheng Lu. 2007. Kernel-
based least squares policy iteration for reinforce-
ment learning. IEEE Transactions on Neural Net-
works, 18(4):973–992, July.
</reference>
<page confidence="0.999514">
114
</page>
<sectionHeader confidence="0.994309" genericHeader="references">
Appendix
</sectionHeader>
<reference confidence="0.8418155">
This appendix provides pseudo code for the algo-
rithms described in the paper.
Algorithm 1: Sparse LSFQ.
Initialization;
Initialize vector θ0, choose a kernel K and a
sparsification factor ν;
Compute the dictionary;
D = I(sj, �aj)1&lt;j&lt;p} from I(sj, aj)1&lt;j&lt;N};
Define the parametrization;
Qθ(s, a) = θTφ(s, a) with φ(s, a) =
(K((s, a), (91, a1)), ... , K((s, a), (�sp, �ap)))T ;
Compute P−1;
P−1 = (EN j=1 φjφTj )−1;
fork= 1,2,...,M do
Compute θk, see Eq. (7);
end
�r* M(s) = argmaxaEA 6M (s, a);
Algorithm 2: Sparse LSPI.
Initialization;
Initialize policy π0, choose a kernel K and a
sparsification factor ν;
for k = 1, 2,... do
Compute the dictionary;
D = I(sj, aj)1&lt;j&lt;pk} from
I(sj, aj)1&lt;j&lt;N, (sj, πk−1(s�j))1&lt;j&lt;N};
Define the parametrization;
Qθ(s, a) = θT φ(s, a) with φ(s, a) =
(K((s, a), (�s1, a1)), ... , K((s, a), (�spk, &amp;quot;pk)))T ;
Compute θk−1, see Eq. (6);
Compute πk;
πk(s) = argmaxaEA 6k_1(s, a);
end
</reference>
<page confidence="0.998424">
115
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.421491">
<title confidence="0.999367">Sparse Approximate Dynamic Programming for Dialog Management</title>
<author confidence="0.913837">Senthilkumar Chandramohan</author>
<author confidence="0.913837">Matthieu Geist</author>
<author confidence="0.913837">Olivier</author>
<affiliation confidence="0.471493">SUPELEC - IMS Research Group, Metz -</affiliation>
<email confidence="0.944025">matthieu.geist,</email>
<abstract confidence="0.998615058823529">Spoken dialogue management strategy optimization by means of Reinforcement Learning (RL) is now part of the state of the art. Yet, there is still a clear mismatch between the complexity implied by the required naturalness of dialogue systems and the inability of standard RL algorithms to scale up. Another issue is the sparsity of the data available for training in the dialogue domain which can not ensure convergence of most of RL algorithms. In this paper, we propose to combine a sample-efficient generalization framework for RL with a feature selection algorithm for the learning of an optimal spoken dialogue management strategy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Richard Bellman</author>
<author>Stuart Dreyfus</author>
</authors>
<title>Functional approximation and dynamic programming.</title>
<date>1959</date>
<booktitle>Mathematical Tables and Other Aids to Computation,</booktitle>
<pages>13--247</pages>
<contexts>
<context position="14516" citStr="Bellman and Dreyfus, 1959" startWordPosition="2412" endWordPosition="2415">n, it admits an analytical solution, which defines the LSTD algorithm: N N θπ = ( φjΔφπj )−1 φjrj (6) j=1 j=1 with φj = φ(sj, aj) and Δφπj = φ(sj, aj) − γφ(s0j, π(s0j)). LSPI is initialized with a policy π0. Then, at iteration k, the Q-function of policy πk−1 is estimated using LSTD, and πk is greedy respectively to this estimated state-action value function. Iterations are stopped when some stopping criterion is met (e.g., small differences between consecutive policies or associated Q-functions). 3.2.2 Least-Squares Fitted Value Iteration The Fitted Value Iteration (FVI) class of algorithms (Bellman and Dreyfus, 1959; Gordon, 1995; Ernst et al., 2005) generalizes value iteration to model-free and large state space problems. The T∗ operator (eq. (3)) being a contraction, a straightforward idea would be to apply it iteratively to the approximation similarly to eq. (4): ˆQθk = T∗ ˆQθk−1. However, T∗ ˆQθ does not necessarily lie in H, it should thus be projected again onto the hypothesis space H. By considering the same projection operator Π as before, this leads to finding the parameter vector θ satisfying: ˆQ∗θ = ΠT∗ ˆQ∗θ. The fitted-Q algorithm (a special case of FVI) assumes that the composed ΠT∗ operator</context>
</contexts>
<marker>Bellman, Dreyfus, 1959</marker>
<rawString>Richard Bellman and Stuart Dreyfus. 1959. Functional approximation and dynamic programming. Mathematical Tables and Other Aids to Computation, 13:247–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Bellman</author>
</authors>
<title>Dynamic Programming. Dover Publications, sixth edition.</title>
<date>1957</date>
<contexts>
<context position="1325" citStr="Bellman, 1957" startWordPosition="200" endWordPosition="201"> propose to combine a sample-efficient generalization framework for RL with a feature selection algorithm for the learning of an optimal spoken dialogue management strategy. 1 Introduction Optimization of dialogue management strategies by means of Reinforcement Learning (RL) (Sutton and Barto, 1998) is now part of the state of the art in the research area of Spoken Dialogue Systems (SDS) (Levin and Pieraccini, 1998; Singh et al., 1999; Pietquin and Dutoit, 2006; Williams and Young, 2007). It consists in casting the dialogue management problem into the Markov Decision Processes (MDP) paradigm (Bellman, 1957) and solving the associated optimization problem. Yet, there is still a clear mismatch between the complexity implied by the required naturalness of the dialogue systems and the inability of standard RL algorithms to scale up. Another issue is the sparsity of the data available for training in the dialogue domain because collecting and annotating data is very time consuming. Yet, RL algorithms are very data demanding and low amounts of data can not ensure convergence of most of RL algorithms. This latter problem has been extensively studied in the recent years and is addressed by simulating ne</context>
<context position="9019" citStr="Bellman, 1957" startWordPosition="1487" endWordPosition="1488">ing between dialogue states and dialogue acts. Still following the MDP’s definitions, the optimal strategy is the one that maximizes some cumulative function of rewards collected all along the interaction. A common choice for the immediate reward is the contribution of each action to user satisfaction (Singh et al., 1999). This subjective Qπ(s, a) = E[ 00 E i=0 108 reward is usually approximated by a linear combination of objective measures like dialogue duration, number of ASR errors, task completion etc. (Walker et al., 1997). 3 Solving MDPs 3.1 Dynamic Programming Dynamic programming (DP) (Bellman, 1957) aims at computing the optimal policy π∗ if the transition probabilities and the reward function are known. First, the policy iteration algorithm computes the optimal policy in an iterative way. The initial policy is arbitrary set to π0. At iteration k, the policy πk−1 is evaluated, that is the associated Qfunction Qπk−1(s, a) is computed. To do so, the Markovian property of the transition probabilities is used to rewrite Equation (1) as : Qπ(s, a) = Es&apos;|s,a[R(s, a, s0) + γQπ(s0, π(s0))] = TπQπ(s, a) (2) This is the so-called Bellman evaluation equation and Tπ is the Bellman evaluation operato</context>
</contexts>
<marker>Bellman, 1957</marker>
<rawString>Richard Bellman. 1957. Dynamic Programming. Dover Publications, sixth edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven J Bradtke</author>
<author>Andrew G Barto</author>
</authors>
<title>Linear Least-Squares algorithms for temporal difference learning.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<pages>22--1</pages>
<contexts>
<context position="12985" citStr="Bradtke and Barto (1996)" startWordPosition="2152" endWordPosition="2155">P algorithms explained in the subsequent sections is to compute the best set of parameters θ given the basis functions. 3.2.1 Least-Squares Policy Iteration The least-squares policy iteration (LSPI) algorithm has been introduced by Lagoudakis and Parr (2003). The underlying idea is exactly the same as for policy iteration: interleaving evaluation and improvement steps. The improvement steps are same as before, but the evaluation step should learn an approximate representation of the Qfunction using samples. In LSPI, this is done using the Least-Squares Temporal Differences (LSTD) algorithm of Bradtke and Barto (1996). Qπ = Tπ i tion): �Q∗i−111 &lt; ξ. The estimated optimal pol109 LSTD aims at minimizing the distance between the approximated Q-function ˆQθ and the projection onto the hypothesis space of its image through the Bellman evaluation operator ΠTπ ˆQθ: θπ = argminθ∈Rp kˆQθ − ΠTπˆQθk2. This can be interpreted as trying to minimize the difference between the two sides of the Bellman equation (1) (which should ideally be zero) in the hypothesis space. Because of the approximation, this difference is most likely to be non-zero. Practically, Tπ is not known, but a set of N transitions {(sj, aj, rj, s0j)1≤</context>
</contexts>
<marker>Bradtke, Barto, 1996</marker>
<rawString>Steven J. Bradtke and Andrew G. Barto. 1996. Linear Least-Squares algorithms for temporal difference learning. Machine Learning, 22(1-3):33–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wieland Eckert</author>
<author>Esther Levin</author>
<author>Roberto Pieraccini</author>
</authors>
<title>User Modeling for Spoken Dialogue System Evaluation.</title>
<date>1997</date>
<booktitle>In ASRU’97,</booktitle>
<pages>80--87</pages>
<contexts>
<context position="2051" citStr="Eckert et al., 1997" startWordPosition="317" endWordPosition="320">ty implied by the required naturalness of the dialogue systems and the inability of standard RL algorithms to scale up. Another issue is the sparsity of the data available for training in the dialogue domain because collecting and annotating data is very time consuming. Yet, RL algorithms are very data demanding and low amounts of data can not ensure convergence of most of RL algorithms. This latter problem has been extensively studied in the recent years and is addressed by simulating new dialogues thanks to a statistical model of human-machine interaction (Pietquin, 2005) and user modeling (Eckert et al., 1997; Pietquin and Dutoit, 2006; Schatzmann et al., 2006). However, this results in a variability of the learned strategy depending on the user modeling method (Schatzmann et al., 2005) and no common agreement exists on the best user model. The former problem, that is dealing with complex dialogue systems within the RL framework, has received much less attention. Although some works can be found in the SDS literature it is far from taking advantage of the large amount of machine learning literature devoted to this problem. In (Williams and Young, 2005), the authors reduce the complexity of the pro</context>
</contexts>
<marker>Eckert, Levin, Pieraccini, 1997</marker>
<rawString>Wieland Eckert, Esther Levin, and Roberto Pieraccini. 1997. User Modeling for Spoken Dialogue System Evaluation. In ASRU’97, pages 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaakov Engel</author>
<author>Shie Mannor</author>
<author>Ron Meir</author>
</authors>
<title>The Kernel Recursive Least Squares Algorithm.</title>
<date>2004</date>
<journal>IEEE Transactions on Signal Processing,</journal>
<pages>52--2275</pages>
<contexts>
<context position="18706" citStr="Engel et al. (2004)" startWordPosition="3147" endWordPosition="3150">sponds to a linear approximation in the feature space, the weight vector being Ep i�1 θiϕ(zi). This is called the kernel trick. Consequently, kernel centers (z1, ... , ip) should be chosen such that (ϕ(z1), ... , ϕ(ip)) are linearly independent in order to avoid using redundant basis functions. Moreover, kernel centers should be chosen among the training samples. To sum up, learning such a parametrization reduces to finding a dictionary D = (z1, ... , ip) ∈ {z1, ... , zN} such that (ϕ(z1), ... , ϕ(ip)) are linearly independent and such that they span the same subspace as (ϕ(z1), ... , ϕ(zN)). Engel et al. (2004) provides a dictionary method to solve this problem, briefly sketched here. The training base is sequentially processed, and the dictionary is initiated with the first sample: D1 = {z1}. At iteration k, a dictionary Dk−1 computed from {z1, ... , zk−1} is available and the kth sample zk is considered. If ϕ(zk) is linearly independent of ϕ(Dk−1), then it is added to the dictionary: Dk = Dk−1 ∪ {zk}. Otherwise, the dictionary remains unchanged: Dk = Dk−1. Linear dependency can be checked by solving the following optimization problem (pk−1 being the size of Dk−1): δ = argminw∈Rpk_1 kϕ(zk) − Epk�1 </context>
</contexts>
<marker>Engel, Mannor, Meir, 2004</marker>
<rawString>Yaakov Engel, Shie Mannor, and Ron Meir. 2004. The Kernel Recursive Least Squares Algorithm. IEEE Transactions on Signal Processing, 52:2275–2285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damien Ernst</author>
<author>Pierre Geurts</author>
<author>Louis Wehenkel</author>
</authors>
<title>Tree-Based Batch Mode Reinforcement Learning.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--503</pages>
<contexts>
<context position="14551" citStr="Ernst et al., 2005" startWordPosition="2418" endWordPosition="2421">h defines the LSTD algorithm: N N θπ = ( φjΔφπj )−1 φjrj (6) j=1 j=1 with φj = φ(sj, aj) and Δφπj = φ(sj, aj) − γφ(s0j, π(s0j)). LSPI is initialized with a policy π0. Then, at iteration k, the Q-function of policy πk−1 is estimated using LSTD, and πk is greedy respectively to this estimated state-action value function. Iterations are stopped when some stopping criterion is met (e.g., small differences between consecutive policies or associated Q-functions). 3.2.2 Least-Squares Fitted Value Iteration The Fitted Value Iteration (FVI) class of algorithms (Bellman and Dreyfus, 1959; Gordon, 1995; Ernst et al., 2005) generalizes value iteration to model-free and large state space problems. The T∗ operator (eq. (3)) being a contraction, a straightforward idea would be to apply it iteratively to the approximation similarly to eq. (4): ˆQθk = T∗ ˆQθk−1. However, T∗ ˆQθ does not necessarily lie in H, it should thus be projected again onto the hypothesis space H. By considering the same projection operator Π as before, this leads to finding the parameter vector θ satisfying: ˆQ∗θ = ΠT∗ ˆQ∗θ. The fitted-Q algorithm (a special case of FVI) assumes that the composed ΠT∗ operator is a contraction and therefore adm</context>
</contexts>
<marker>Ernst, Geurts, Wehenkel, 2005</marker>
<rawString>Damien Ernst, Pierre Geurts, and Louis Wehenkel. 2005. Tree-Based Batch Mode Reinforcement Learning. Journal of Machine Learning Research, 6:503–556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Gordon</author>
</authors>
<title>Stable Function Approximation in Dynamic Programming.</title>
<date>1995</date>
<booktitle>In ICML’95.</booktitle>
<contexts>
<context position="14530" citStr="Gordon, 1995" startWordPosition="2416" endWordPosition="2417">solution, which defines the LSTD algorithm: N N θπ = ( φjΔφπj )−1 φjrj (6) j=1 j=1 with φj = φ(sj, aj) and Δφπj = φ(sj, aj) − γφ(s0j, π(s0j)). LSPI is initialized with a policy π0. Then, at iteration k, the Q-function of policy πk−1 is estimated using LSTD, and πk is greedy respectively to this estimated state-action value function. Iterations are stopped when some stopping criterion is met (e.g., small differences between consecutive policies or associated Q-functions). 3.2.2 Least-Squares Fitted Value Iteration The Fitted Value Iteration (FVI) class of algorithms (Bellman and Dreyfus, 1959; Gordon, 1995; Ernst et al., 2005) generalizes value iteration to model-free and large state space problems. The T∗ operator (eq. (3)) being a contraction, a straightforward idea would be to apply it iteratively to the approximation similarly to eq. (4): ˆQθk = T∗ ˆQθk−1. However, T∗ ˆQθ does not necessarily lie in H, it should thus be projected again onto the hypothesis space H. By considering the same projection operator Π as before, this leads to finding the parameter vector θ satisfying: ˆQ∗θ = ΠT∗ ˆQ∗θ. The fitted-Q algorithm (a special case of FVI) assumes that the composed ΠT∗ operator is a contract</context>
</contexts>
<marker>Gordon, 1995</marker>
<rawString>Geoffrey Gordon. 1995. Stable Function Approximation in Dynamic Programming. In ICML’95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Oliver Lemon</author>
<author>Kallirroi Georgila</author>
</authors>
<title>Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<pages>487--511</pages>
<contexts>
<context position="2936" citStr="Henderson et al., 2008" startWordPosition="467" endWordPosition="470">is dealing with complex dialogue systems within the RL framework, has received much less attention. Although some works can be found in the SDS literature it is far from taking advantage of the large amount of machine learning literature devoted to this problem. In (Williams and Young, 2005), the authors reduce the complexity of the problem (which is actually a Partially Observable MDP) by automatically condensing the continuous state space in a so-called summary space. This results in a clustering of the state space in a discrete set of states on which standard RL algorithms are applied. In (Henderson et al., 2008), the authors use a linear approximation scheme and apply the SARSA(A) algorithm (Sutton and Barto, 1998) in a batch setting (from data and not from interactions or simulations). This algorithm was actually designed for online learning and is known to converge very slowly. It therefore requires a lot of data and especially in large state spaces. Moreover, the choice of the features used for the linear approximation is particularly simple since features are the state variables themselves. The approximated function can therefore not be more complex than an hyper-plane in the state variables spac</context>
<context position="26079" citStr="Henderson et al., 2008" startWordPosition="4373" endWordPosition="4376">atures selected by the dictionary method avoided weight divergence. Our second representation of Q-function used a set of hand-picked features presented as a set of Gaussian functions, centered in pi and with the same standard deviation Qi = Q). Our RBF network had 3 Gaussians for each dimension in the state vector and considering that we have 13 actions, in total we used 351 (i.e, 33 x 13) features for approximating the Q-function. This allows considering that each state variable contributes to the value function differently according to its value contrarily to similar work (Li et al., 2009; Henderson et al., 2008) that considers linear contribution of each state variable. Gaussians were centered at pi = 0.0, 0.5, 1.0 in every dimension with a standard deviation Qi = Q = 0.25. Our stopping criteria was based on comparison between L1 norm of succeeding weights and a threshold � which was set to 10−2 i.e, convergence if �i ( 0ni− BZ −1 1) &lt; , where n is the iteration number. For sparse LSPI since the dictionary is computed during each iteration, stopping criteria based on � is not feasible thus the learning was stopped after 30 iterations. 5.4 Evaluation of learned policy We ran a set of learning iteratio</context>
</contexts>
<marker>Henderson, Lemon, Georgila, 2008</marker>
<rawString>James Henderson, Oliver Lemon, and Kallirroi Georgila. 2008. Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets. Computational Linguistics, vol. 34(4), pp 487-511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michail G Lagoudakis</author>
<author>Ronald Parr</author>
</authors>
<title>Leastsquares policy iteration.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>4--1107</pages>
<contexts>
<context position="12619" citStr="Lagoudakis and Parr (2003)" startWordPosition="2097" endWordPosition="2100">the Qfunction will be assumed: Qθ(s, a) = θT φ(s, a). where θ E Rp is the parameter vector and φ(s, a) is the set of p basis functions. All functions expressed in this way define a so-called hypothesis space H = {�Qθ|θ E Rp}. Any function Q can be projected onto this hypothesis space by the operator H defined as HQ = argmin 11Q − �Qθ112. (5) ˆQB∈H The goal of the ADP algorithms explained in the subsequent sections is to compute the best set of parameters θ given the basis functions. 3.2.1 Least-Squares Policy Iteration The least-squares policy iteration (LSPI) algorithm has been introduced by Lagoudakis and Parr (2003). The underlying idea is exactly the same as for policy iteration: interleaving evaluation and improvement steps. The improvement steps are same as before, but the evaluation step should learn an approximate representation of the Qfunction using samples. In LSPI, this is done using the Least-Squares Temporal Differences (LSTD) algorithm of Bradtke and Barto (1996). Qπ = Tπ i tion): �Q∗i−111 &lt; ξ. The estimated optimal pol109 LSTD aims at minimizing the distance between the approximated Q-function ˆQθ and the projection onto the hypothesis space of its image through the Bellman evaluation operat</context>
</contexts>
<marker>Lagoudakis, Parr, 2003</marker>
<rawString>Michail G. Lagoudakis and Ronald Parr. 2003. Leastsquares policy iteration. Journal of Machine Learning Research, 4:1107–1149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Staffan Larsson</author>
<author>David R Traum</author>
</authors>
<title>Information state and dialogue management in the TRINDI dialogue move engine toolkit.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<pages>323--340</pages>
<contexts>
<context position="8074" citStr="Larsson and Traum, 2000" startWordPosition="1330" endWordPosition="1334">equivalence of this problem to a sequential decision making problem. Indeed, the role of the dialogue manager (or the decision maker) is to select and perform dialogue acts (actions in the MDP paradigm) when it reaches a given dialogue turn (state in the MDP paradigm) while interacting with a human user. There can be several types of system dialogue acts. For example, in the case of a restaurant information system, possible acts are request(cuisine type), provide(address), confirm(price range), close etc. The dialogue state is usually represented efficiently by the Information State paradigm (Larsson and Traum, 2000). In this paradigm, the dialogue state contains a compact representation of the history of the dialogue in terms of system acts and its subsequent user responses (user acts). It summarizes the information exchanged between the user and the system until the considered state is reached. A dialogue management strategy is thus a mapping between dialogue states and dialogue acts. Still following the MDP’s definitions, the optimal strategy is the one that maximizes some cumulative function of rewards collected all along the interaction. A common choice for the immediate reward is the contribution of</context>
</contexts>
<marker>Larsson, Traum, 2000</marker>
<rawString>Staffan Larsson and David R. Traum. 2000. Information state and dialogue management in the TRINDI dialogue move engine toolkit. Natural Language Engineering, vol. 6, pp 323–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Lemon</author>
<author>Kallirroi Georgila</author>
<author>James Henderson</author>
<author>Matthew Stuttle</author>
</authors>
<title>An ISU dialogue system exhibiting reinforcement learning of dialogue policies: generic slot-filling in the TALK in-car system.</title>
<date>2006</date>
<booktitle>In EACL’06,</booktitle>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="22290" citStr="Lemon et al., 2006" startWordPosition="3743" endWordPosition="3746">ore the complexity per iteration is in O(p2), with p being the number of basis functions computed using the dictionary method. For LSPI, the inverse matrix depends on the iteration, as well as the dictionary, therefore the computational complexity is in O(pk) per iteration, where pk is the size of the dictionary computed at the kth iteration. Therefore, a kernel-based Thanks to the kernel trick (that 111 5 Experimental set-up and results 5.1 Dialogue task and RL parameters The experimental setup is a form-filling dialogue system in the tourist information domain similar to the one studied in (Lemon et al., 2006). The system aims to give information about restaurants in the city based on specific user preferences. Three slots are considered: (i) location, (ii) cuisine and (iii) price-range of the restaurant. The dialogue state has three continuous components ranging from 0 to 1, each representing the average of filling and confirmation confidence of the corresponding slots. The MDP SDS has 13 actions: Ask-slot (3 actions), Explicit-confirm (3 actions), Implicitconfirm and Ask-slot value (6 actions) and Closedialogue (1 action). The ry parameter was set to 0.95 in order to encourage delayed rewards and</context>
<context position="23791" citStr="Lemon et al., 2006" startWordPosition="3992" endWordPosition="3995">e end of the dialogue. 5.2 Dialogue corpora for policy optimization So as to perform sparse LSFQ or sparse LSPI, a dialogue corpus which represents the problem space is needed. As for any batch learning method, the samples used for learning should be chosen (if they can be chosen) to span across the problem space. In this experiment, a user simulation technique was used to generate the data corpora. This way, the sensibility of the method to the size of the training data-set could be analyzed (available human-dialogue corpora are limited in size). The user simulator was plugged to the DIPPER (Lemon et al., 2006) dialogue management system to generate dialogue samples. To generate data, the dialogue manager strategy was jointly based on a simple hand-coded policy (which aims only to fill all the slots before closing the dialogue episode irrespective of slot confidence score i.e.,) and random action selection. Randomly selected system acts are used with probability E and hand-coded policy selected system acts are used with probability (1-E). During our data generation process the E value was set to 0.9. Rather than using a fully random policy we used an E-greedy policy to ensure that the problem space </context>
</contexts>
<marker>Lemon, Georgila, Henderson, Stuttle, 2006</marker>
<rawString>Oliver Lemon, Kallirroi Georgila, James Henderson, and Matthew Stuttle. 2006. An ISU dialogue system exhibiting reinforcement learning of dialogue policies: generic slot-filling in the TALK in-car system. In EACL’06, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esther Levin</author>
<author>Roberto Pieraccini</author>
</authors>
<title>Using markov decision process for learning dialogue strategies.</title>
<date>1998</date>
<booktitle>In ICASSP’98.</booktitle>
<contexts>
<context position="1129" citStr="Levin and Pieraccini, 1998" startWordPosition="167" endWordPosition="170">lity of standard RL algorithms to scale up. Another issue is the sparsity of the data available for training in the dialogue domain which can not ensure convergence of most of RL algorithms. In this paper, we propose to combine a sample-efficient generalization framework for RL with a feature selection algorithm for the learning of an optimal spoken dialogue management strategy. 1 Introduction Optimization of dialogue management strategies by means of Reinforcement Learning (RL) (Sutton and Barto, 1998) is now part of the state of the art in the research area of Spoken Dialogue Systems (SDS) (Levin and Pieraccini, 1998; Singh et al., 1999; Pietquin and Dutoit, 2006; Williams and Young, 2007). It consists in casting the dialogue management problem into the Markov Decision Processes (MDP) paradigm (Bellman, 1957) and solving the associated optimization problem. Yet, there is still a clear mismatch between the complexity implied by the required naturalness of the dialogue systems and the inability of standard RL algorithms to scale up. Another issue is the sparsity of the data available for training in the dialogue domain because collecting and annotating data is very time consuming. Yet, RL algorithms are ver</context>
</contexts>
<marker>Levin, Pieraccini, 1998</marker>
<rawString>Esther Levin and Roberto Pieraccini. 1998. Using markov decision process for learning dialogue strategies. In ICASSP’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lihong Li</author>
<author>Suhrid Balakrishnan</author>
<author>Jason Williams</author>
</authors>
<title>Reinforcement Learning for Dialog Management using Least-Squares Policy Iteration and Fast Feature Selection. In InterSpeech’09,</title>
<date>2009</date>
<location>Brighton (UK).</location>
<contexts>
<context position="3599" citStr="Li et al., 2009" startWordPosition="577" endWordPosition="580"> and apply the SARSA(A) algorithm (Sutton and Barto, 1998) in a batch setting (from data and not from interactions or simulations). This algorithm was actually designed for online learning and is known to converge very slowly. It therefore requires a lot of data and especially in large state spaces. Moreover, the choice of the features used for the linear approximation is particularly simple since features are the state variables themselves. The approximated function can therefore not be more complex than an hyper-plane in the state variables space. This drawback is shared by the approach of (Li et al., 2009) where a batch algorithm (Least Square Policy Iteration or LSPI) is combined to a pruning method to only keep the most meaningful features. In addition the complexity of LSPI is O(p3). In the machine learning community, this issue is actually addressed by function approximation accompanied with dimensionality reduction. The Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 107–115, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 107 data sparsity problem is also widely addressed i</context>
<context position="26054" citStr="Li et al., 2009" startWordPosition="4369" endWordPosition="4372">in addition to features selected by the dictionary method avoided weight divergence. Our second representation of Q-function used a set of hand-picked features presented as a set of Gaussian functions, centered in pi and with the same standard deviation Qi = Q). Our RBF network had 3 Gaussians for each dimension in the state vector and considering that we have 13 actions, in total we used 351 (i.e, 33 x 13) features for approximating the Q-function. This allows considering that each state variable contributes to the value function differently according to its value contrarily to similar work (Li et al., 2009; Henderson et al., 2008) that considers linear contribution of each state variable. Gaussians were centered at pi = 0.0, 0.5, 1.0 in every dimension with a standard deviation Qi = Q = 0.25. Our stopping criteria was based on comparison between L1 norm of succeeding weights and a threshold � which was set to 10−2 i.e, convergence if �i ( 0ni− BZ −1 1) &lt; , where n is the iteration number. For sparse LSPI since the dictionary is computed during each iteration, stopping criteria based on � is not feasible thus the learning was stopped after 30 iterations. 5.4 Evaluation of learned policy We ran a</context>
</contexts>
<marker>Li, Balakrishnan, Williams, 2009</marker>
<rawString>Lihong Li, Suhrid Balakrishnan, and Jason Williams. 2009. Reinforcement Learning for Dialog Management using Least-Squares Policy Iteration and Fast Feature Selection. In InterSpeech’09, Brighton (UK).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Pietquin</author>
<author>Thierry Dutoit</author>
</authors>
<title>A probabilistic framework for dialog simulation and optimal strategy learning.</title>
<date>2006</date>
<journal>IEEE Transactions on Audio, Speech &amp; Language Processing,</journal>
<volume>14</volume>
<issue>2</issue>
<pages>589--599</pages>
<contexts>
<context position="1176" citStr="Pietquin and Dutoit, 2006" startWordPosition="175" endWordPosition="178">her issue is the sparsity of the data available for training in the dialogue domain which can not ensure convergence of most of RL algorithms. In this paper, we propose to combine a sample-efficient generalization framework for RL with a feature selection algorithm for the learning of an optimal spoken dialogue management strategy. 1 Introduction Optimization of dialogue management strategies by means of Reinforcement Learning (RL) (Sutton and Barto, 1998) is now part of the state of the art in the research area of Spoken Dialogue Systems (SDS) (Levin and Pieraccini, 1998; Singh et al., 1999; Pietquin and Dutoit, 2006; Williams and Young, 2007). It consists in casting the dialogue management problem into the Markov Decision Processes (MDP) paradigm (Bellman, 1957) and solving the associated optimization problem. Yet, there is still a clear mismatch between the complexity implied by the required naturalness of the dialogue systems and the inability of standard RL algorithms to scale up. Another issue is the sparsity of the data available for training in the dialogue domain because collecting and annotating data is very time consuming. Yet, RL algorithms are very data demanding and low amounts of data can no</context>
</contexts>
<marker>Pietquin, Dutoit, 2006</marker>
<rawString>Olivier Pietquin and Thierry Dutoit. 2006. A probabilistic framework for dialog simulation and optimal strategy learning. IEEE Transactions on Audio, Speech &amp; Language Processing, 14(2): 589-599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Pietquin</author>
</authors>
<title>A probabilistic description of man-machine spoken communication.</title>
<date>2005</date>
<booktitle>In ICME’05,</booktitle>
<pages>410--413</pages>
<location>Amsterdam (The Netherlands),</location>
<contexts>
<context position="2012" citStr="Pietquin, 2005" startWordPosition="312" endWordPosition="313">clear mismatch between the complexity implied by the required naturalness of the dialogue systems and the inability of standard RL algorithms to scale up. Another issue is the sparsity of the data available for training in the dialogue domain because collecting and annotating data is very time consuming. Yet, RL algorithms are very data demanding and low amounts of data can not ensure convergence of most of RL algorithms. This latter problem has been extensively studied in the recent years and is addressed by simulating new dialogues thanks to a statistical model of human-machine interaction (Pietquin, 2005) and user modeling (Eckert et al., 1997; Pietquin and Dutoit, 2006; Schatzmann et al., 2006). However, this results in a variability of the learned strategy depending on the user modeling method (Schatzmann et al., 2005) and no common agreement exists on the best user model. The former problem, that is dealing with complex dialogue systems within the RL framework, has received much less attention. Although some works can be found in the SDS literature it is far from taking advantage of the large amount of machine learning literature devoted to this problem. In (Williams and Young, 2005), the a</context>
</contexts>
<marker>Pietquin, 2005</marker>
<rawString>Olivier Pietquin. 2005. A probabilistic description of man-machine spoken communication. In ICME’05, pages 410–413, Amsterdam (The Netherlands), July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin L Puterman</author>
</authors>
<date>1994</date>
<booktitle>Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley-Interscience,</booktitle>
<contexts>
<context position="5570" citStr="Puterman, 1994" startWordPosition="897" endWordPosition="898">ows. Section 2 gives a formal insight of MDP and briefly reminds the casting of the dialogue problem into the MDP framework. Section 3.2 provides a description of approximate Dynamic Programming along with LSPI and FVI algorithms. Section 4 provides an overview on how LSPI and FVI can be combined with a feature selection scheme (which is employed to learn the representation of the Q-function from the dialogue corpus). Our experimental set-up, results and a comparison with state-of-the-art methods are presented in Section 5. Eventually, Section 6 concludes. 2 Markov Decision Processes The MDP (Puterman, 1994) framework is used to describe and solve sequential decision making problems or equivalently optimal control problems in the case of stochastic dynamic systems. An MDP is formally a tuple {5, A, P, R, -y} where 5 is the (finite) state space, A the (finite) action space, P E P(5)SxA the family of Markovian transition probabilities1, R E RSxAxS the reward function and -y the discounting factor (0 &lt; -y &lt; 1). According to this formalism, a system to be controlled steps from state to state (s E 5) according to transition probabilities P as a consequence of the controller’s actions (a E A). After ea</context>
<context position="10630" citStr="Puterman, 1994" startWordPosition="1767" endWordPosition="1768">re iterated until convergence of πk to π∗ (which can be demonstrated to happen in a finite number of iterations when πk = πk−1). The value iteration algorithm aims at estimating directly the optimal state-action value function Q∗ which is the solution of the Bellman optimality equation (or equivalently the unique fixed-point of the Bellman optimality operator T∗): Q∗(s, a) = Es1|s,a[R(s, a, s0) + γ maxQ∗(s0, b)] b∈A = T∗Q∗(s, a) (3) The T∗ operator is not linear, therefore computing Q∗ via standard system-solving methods is not possible. However, it can be shown that T∗ is also a contraction (Puterman, 1994). Therefore, according to Banach fixed-point theorem, Q∗ can be estimated using the following iterative way: �Q∗i = T∗ �Q∗i−1, V�Q∗0 i→∞ lim �Q∗i = Q∗ (4) However, the convergence takes an infinite number of iterations. Practically speaking, iterations are stopped when some criterion is met, classically a small difference between two iterations: 11 Q∗i − � icy (which is what we are ultimately interested in) is greedy respectively to the estimated optimal Qfunction: �r∗(s) = argmaxa∈A �Q∗(s, a). 3.2 Approximate Dynamic Programming DP-based approaches have two drawbacks. First, they assume the t</context>
</contexts>
<marker>Puterman, 1994</marker>
<rawString>Martin L. Puterman. 1994. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley-Interscience, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jost Schatzmann</author>
<author>Matthew N Stuttle</author>
<author>Karl Weilhammer</author>
<author>Steve Young</author>
</authors>
<title>Effects of the user model on simulation-based learning of dialogue strategies.</title>
<date>2005</date>
<booktitle>In ASRU’05,</booktitle>
<contexts>
<context position="2232" citStr="Schatzmann et al., 2005" startWordPosition="346" endWordPosition="349">training in the dialogue domain because collecting and annotating data is very time consuming. Yet, RL algorithms are very data demanding and low amounts of data can not ensure convergence of most of RL algorithms. This latter problem has been extensively studied in the recent years and is addressed by simulating new dialogues thanks to a statistical model of human-machine interaction (Pietquin, 2005) and user modeling (Eckert et al., 1997; Pietquin and Dutoit, 2006; Schatzmann et al., 2006). However, this results in a variability of the learned strategy depending on the user modeling method (Schatzmann et al., 2005) and no common agreement exists on the best user model. The former problem, that is dealing with complex dialogue systems within the RL framework, has received much less attention. Although some works can be found in the SDS literature it is far from taking advantage of the large amount of machine learning literature devoted to this problem. In (Williams and Young, 2005), the authors reduce the complexity of the problem (which is actually a Partially Observable MDP) by automatically condensing the continuous state space in a so-called summary space. This results in a clustering of the state sp</context>
</contexts>
<marker>Schatzmann, Stuttle, Weilhammer, Young, 2005</marker>
<rawString>Jost Schatzmann, Matthew N. Stuttle, Karl Weilhammer, and Steve Young. 2005. Effects of the user model on simulation-based learning of dialogue strategies. In ASRU’05, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jost Schatzmann</author>
<author>Karl Weilhammer</author>
<author>Matt Stuttle</author>
<author>Steve Young</author>
</authors>
<title>A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies.</title>
<date>2006</date>
<journal>Knowledge Engineering Review,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>97--126</pages>
<contexts>
<context position="2104" citStr="Schatzmann et al., 2006" startWordPosition="325" endWordPosition="328">alogue systems and the inability of standard RL algorithms to scale up. Another issue is the sparsity of the data available for training in the dialogue domain because collecting and annotating data is very time consuming. Yet, RL algorithms are very data demanding and low amounts of data can not ensure convergence of most of RL algorithms. This latter problem has been extensively studied in the recent years and is addressed by simulating new dialogues thanks to a statistical model of human-machine interaction (Pietquin, 2005) and user modeling (Eckert et al., 1997; Pietquin and Dutoit, 2006; Schatzmann et al., 2006). However, this results in a variability of the learned strategy depending on the user modeling method (Schatzmann et al., 2005) and no common agreement exists on the best user model. The former problem, that is dealing with complex dialogue systems within the RL framework, has received much less attention. Although some works can be found in the SDS literature it is far from taking advantage of the large amount of machine learning literature devoted to this problem. In (Williams and Young, 2005), the authors reduce the complexity of the problem (which is actually a Partially Observable MDP) b</context>
</contexts>
<marker>Schatzmann, Weilhammer, Stuttle, Young, 2006</marker>
<rawString>Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and Steve Young. 2006. A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies. Knowledge Engineering Review, vol. 21(2), pp. 97–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Scholkopf</author>
<author>Alexander J Smola</author>
</authors>
<title>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond.</title>
<date>2001</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="17055" citStr="Scholkopf and Smola, 2001" startWordPosition="2833" endWordPosition="2836">ng that there are M iterations, the optimal policy is estimated as ˆπ∗(s) = argmaxa∈A ˆQθM (s, a). 4 Learning a sparse parametrization LSPI and LSFQ (FVI) assume that the basis functions are chosen beforehand. However, this is difficult and problem-dependent. Thus, we propose to combine these algorithms with a scheme which learns the representation from dialogue corpora. Let’s place ourselves in a general context. We want to learn a parametric representation for an approximated function fθ(z) = θT φ(z) from samples {z1, ... , zN}. A classical choice is to choose a kernel-based representation (Scholkopf and Smola, 2001). Formally, a kernel K(z, ˜zi) is a continuous, positive and semi-definite function (e.g., Gaussian or polynomial kernels) centered on ˜zi. The feature vector φ(z) is therefore of the form: φ(z) = (K(z, ˜z1) ... K(z, ˜zp)). The question this section answers is the following: given the training basis {z1, ... , zN} and a kernel Bk = argmin BEf2P N E j=1 N E j=1 = ( N E j=1 OjO j )−1 110 K, how to choose the number p of basis functions and the associated kernel centers (z1, ... , ip)? An important result about kernels is the Mercer theorem, which states that for each kernel K there exists a mapp</context>
</contexts>
<marker>Scholkopf, Smola, 2001</marker>
<rawString>Bernhard Scholkopf and Alexander J. Smola. 2001. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satinder Singh</author>
<author>Michael Kearns</author>
<author>Diane Litman</author>
<author>Marilyn Walker</author>
</authors>
<title>Reinforcement learning for spoken dialogue systems.</title>
<date>1999</date>
<booktitle>In NIPS’99.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="1149" citStr="Singh et al., 1999" startWordPosition="171" endWordPosition="174">ms to scale up. Another issue is the sparsity of the data available for training in the dialogue domain which can not ensure convergence of most of RL algorithms. In this paper, we propose to combine a sample-efficient generalization framework for RL with a feature selection algorithm for the learning of an optimal spoken dialogue management strategy. 1 Introduction Optimization of dialogue management strategies by means of Reinforcement Learning (RL) (Sutton and Barto, 1998) is now part of the state of the art in the research area of Spoken Dialogue Systems (SDS) (Levin and Pieraccini, 1998; Singh et al., 1999; Pietquin and Dutoit, 2006; Williams and Young, 2007). It consists in casting the dialogue management problem into the Markov Decision Processes (MDP) paradigm (Bellman, 1957) and solving the associated optimization problem. Yet, there is still a clear mismatch between the complexity implied by the required naturalness of the dialogue systems and the inability of standard RL algorithms to scale up. Another issue is the sparsity of the data available for training in the dialogue domain because collecting and annotating data is very time consuming. Yet, RL algorithms are very data demanding and</context>
<context position="8728" citStr="Singh et al., 1999" startWordPosition="1437" endWordPosition="1440">ate contains a compact representation of the history of the dialogue in terms of system acts and its subsequent user responses (user acts). It summarizes the information exchanged between the user and the system until the considered state is reached. A dialogue management strategy is thus a mapping between dialogue states and dialogue acts. Still following the MDP’s definitions, the optimal strategy is the one that maximizes some cumulative function of rewards collected all along the interaction. A common choice for the immediate reward is the contribution of each action to user satisfaction (Singh et al., 1999). This subjective Qπ(s, a) = E[ 00 E i=0 108 reward is usually approximated by a linear combination of objective measures like dialogue duration, number of ASR errors, task completion etc. (Walker et al., 1997). 3 Solving MDPs 3.1 Dynamic Programming Dynamic programming (DP) (Bellman, 1957) aims at computing the optimal policy π∗ if the transition probabilities and the reward function are known. First, the policy iteration algorithm computes the optimal policy in an iterative way. The initial policy is arbitrary set to π0. At iteration k, the policy πk−1 is evaluated, that is the associated Qf</context>
</contexts>
<marker>Singh, Kearns, Litman, Walker, 1999</marker>
<rawString>Satinder Singh, Michael Kearns, Diane Litman, and Marilyn Walker. 1999. Reinforcement learning for spoken dialogue systems. In NIPS’99. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Sutton</author>
<author>Andrew G Barto</author>
</authors>
<date>1998</date>
<booktitle>Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning). The MIT Press, 3rd edition,</booktitle>
<contexts>
<context position="1011" citStr="Sutton and Barto, 1998" startWordPosition="144" endWordPosition="148">still a clear mismatch between the complexity implied by the required naturalness of dialogue systems and the inability of standard RL algorithms to scale up. Another issue is the sparsity of the data available for training in the dialogue domain which can not ensure convergence of most of RL algorithms. In this paper, we propose to combine a sample-efficient generalization framework for RL with a feature selection algorithm for the learning of an optimal spoken dialogue management strategy. 1 Introduction Optimization of dialogue management strategies by means of Reinforcement Learning (RL) (Sutton and Barto, 1998) is now part of the state of the art in the research area of Spoken Dialogue Systems (SDS) (Levin and Pieraccini, 1998; Singh et al., 1999; Pietquin and Dutoit, 2006; Williams and Young, 2007). It consists in casting the dialogue management problem into the Markov Decision Processes (MDP) paradigm (Bellman, 1957) and solving the associated optimization problem. Yet, there is still a clear mismatch between the complexity implied by the required naturalness of the dialogue systems and the inability of standard RL algorithms to scale up. Another issue is the sparsity of the data available for tra</context>
<context position="3041" citStr="Sutton and Barto, 1998" startWordPosition="483" endWordPosition="487">ugh some works can be found in the SDS literature it is far from taking advantage of the large amount of machine learning literature devoted to this problem. In (Williams and Young, 2005), the authors reduce the complexity of the problem (which is actually a Partially Observable MDP) by automatically condensing the continuous state space in a so-called summary space. This results in a clustering of the state space in a discrete set of states on which standard RL algorithms are applied. In (Henderson et al., 2008), the authors use a linear approximation scheme and apply the SARSA(A) algorithm (Sutton and Barto, 1998) in a batch setting (from data and not from interactions or simulations). This algorithm was actually designed for online learning and is known to converge very slowly. It therefore requires a lot of data and especially in large state spaces. Moreover, the choice of the features used for the linear approximation is particularly simple since features are the state variables themselves. The approximated function can therefore not be more complex than an hyper-plane in the state variables space. This drawback is shared by the approach of (Li et al., 2009) where a batch algorithm (Least Square Pol</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richard S. Sutton and Andrew G. Barto. 1998. Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning). The MIT Press, 3rd edition, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Diane J Litman</author>
<author>Candace A Kamm</author>
<author>Alicia Abella</author>
</authors>
<title>PARADISE: A framework for evaluating spoken dialogue agents.</title>
<date>1997</date>
<booktitle>In ACL’97,</booktitle>
<pages>271--280</pages>
<location>Madrid</location>
<contexts>
<context position="8938" citStr="Walker et al., 1997" startWordPosition="1474" endWordPosition="1477">em until the considered state is reached. A dialogue management strategy is thus a mapping between dialogue states and dialogue acts. Still following the MDP’s definitions, the optimal strategy is the one that maximizes some cumulative function of rewards collected all along the interaction. A common choice for the immediate reward is the contribution of each action to user satisfaction (Singh et al., 1999). This subjective Qπ(s, a) = E[ 00 E i=0 108 reward is usually approximated by a linear combination of objective measures like dialogue duration, number of ASR errors, task completion etc. (Walker et al., 1997). 3 Solving MDPs 3.1 Dynamic Programming Dynamic programming (DP) (Bellman, 1957) aims at computing the optimal policy π∗ if the transition probabilities and the reward function are known. First, the policy iteration algorithm computes the optimal policy in an iterative way. The initial policy is arbitrary set to π0. At iteration k, the policy πk−1 is evaluated, that is the associated Qfunction Qπk−1(s, a) is computed. To do so, the Markovian property of the transition probabilities is used to rewrite Equation (1) as : Qπ(s, a) = Es&apos;|s,a[R(s, a, s0) + γQπ(s0, π(s0))] = TπQπ(s, a) (2) This is t</context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1997</marker>
<rawString>Marilyn A. Walker, Diane J. Litman, Candace A. Kamm, and Alicia Abella. 1997. PARADISE: A framework for evaluating spoken dialogue agents. In ACL’97, pages 271–280, Madrid (Spain).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Williams</author>
<author>Steve Young</author>
</authors>
<title>Scaling up pomdps for dialogue management: the summary pomdp method.</title>
<date>2005</date>
<booktitle>In ASRU’05.</booktitle>
<contexts>
<context position="2605" citStr="Williams and Young, 2005" startWordPosition="411" endWordPosition="414">ine interaction (Pietquin, 2005) and user modeling (Eckert et al., 1997; Pietquin and Dutoit, 2006; Schatzmann et al., 2006). However, this results in a variability of the learned strategy depending on the user modeling method (Schatzmann et al., 2005) and no common agreement exists on the best user model. The former problem, that is dealing with complex dialogue systems within the RL framework, has received much less attention. Although some works can be found in the SDS literature it is far from taking advantage of the large amount of machine learning literature devoted to this problem. In (Williams and Young, 2005), the authors reduce the complexity of the problem (which is actually a Partially Observable MDP) by automatically condensing the continuous state space in a so-called summary space. This results in a clustering of the state space in a discrete set of states on which standard RL algorithms are applied. In (Henderson et al., 2008), the authors use a linear approximation scheme and apply the SARSA(A) algorithm (Sutton and Barto, 1998) in a batch setting (from data and not from interactions or simulations). This algorithm was actually designed for online learning and is known to converge very slo</context>
</contexts>
<marker>Williams, Young, 2005</marker>
<rawString>Jason Williams and Steve Young. 2005. Scaling up pomdps for dialogue management: the summary pomdp method. In ASRU’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Steve Young</author>
</authors>
<title>Partially observable markov decision processes for spoken dialog systems.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>393--422</pages>
<contexts>
<context position="1203" citStr="Williams and Young, 2007" startWordPosition="179" endWordPosition="182">f the data available for training in the dialogue domain which can not ensure convergence of most of RL algorithms. In this paper, we propose to combine a sample-efficient generalization framework for RL with a feature selection algorithm for the learning of an optimal spoken dialogue management strategy. 1 Introduction Optimization of dialogue management strategies by means of Reinforcement Learning (RL) (Sutton and Barto, 1998) is now part of the state of the art in the research area of Spoken Dialogue Systems (SDS) (Levin and Pieraccini, 1998; Singh et al., 1999; Pietquin and Dutoit, 2006; Williams and Young, 2007). It consists in casting the dialogue management problem into the Markov Decision Processes (MDP) paradigm (Bellman, 1957) and solving the associated optimization problem. Yet, there is still a clear mismatch between the complexity implied by the required naturalness of the dialogue systems and the inability of standard RL algorithms to scale up. Another issue is the sparsity of the data available for training in the dialogue domain because collecting and annotating data is very time consuming. Yet, RL algorithms are very data demanding and low amounts of data can not ensure convergence of mos</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>Jason D. Williams and Steve Young. 2007. Partially observable markov decision processes for spoken dialog systems. Computer Speech and Language, vol. 21(2), pp. 393–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Xu</author>
<author>Dewen Hu</author>
<author>Xicheng Lu</author>
</authors>
<title>Kernelbased least squares policy iteration for reinforcement learning.</title>
<date>2007</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="20229" citStr="Xu et al. (2007)" startWordPosition="3402" endWordPosition="3405"> will be considered as linearly dependent of ϕ(Dk−1) if δ &lt; ν, where ν is the so-called sparsification factor. This allows controlling the trade-off between quality of the representation and its sparsity. See Engel et al. (2004) for details as well as an efficient implementation of this dictionary approach. 4.1 Resulting algorithms We propose to combine LSPI and LSFQ with the sparsification approach exposed in the previous section: a kernel is chosen, the dictionary is computed and then LSPI or LSFQ is applied using the learnt basis functions. For LSPI, this scheme has been proposed before by Xu et al. (2007) (with the difference that they generate new trajectories at each iteration whereas we use the same for all iterations). The proposed sparse LSFQ algorithm is a novel contribution of this paper. We start with the sparse LSFQ algorithm. In order to train the dictionary, the inputs are needed (state-action couples in this case), but not the outputs (reward are not used). For LSFQ, the input space remains the same over iterations, therefore the dictionary can be computed in a preprocessing step from {(sj, aj)1≤j≤N}. Notice that the matrix (ENj�1 φjφTj )−1 remains also the same over iterations, th</context>
</contexts>
<marker>Xu, Hu, Lu, 2007</marker>
<rawString>Xin Xu, Dewen Hu, and Xicheng Lu. 2007. Kernelbased least squares policy iteration for reinforcement learning. IEEE Transactions on Neural Networks, 18(4):973–992, July.</rawString>
</citation>
<citation valid="false">
<title>This appendix provides pseudo code for the algorithms described in the paper. Algorithm 1: Sparse LSFQ.</title>
<marker></marker>
<rawString>This appendix provides pseudo code for the algorithms described in the paper. Algorithm 1: Sparse LSFQ.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Initialization</author>
</authors>
<title>Initialize vector θ0, choose a kernel K and a sparsification factor ν; Compute the dictionary; D = I(sj,</title>
<booktitle>aj)1&lt;j&lt;p} from I(sj, aj)1&lt;j&lt;N}; Define the parametrization; Qθ(s, a) = θTφ(s, a) with φ(s, a) = (K((s, a), (91, a1)), ... , K((s, a), (�sp, �ap)))T ; Compute P−1; P−1 = (EN j=1 φjφTj )−1;</booktitle>
<marker>Initialization, </marker>
<rawString>Initialization; Initialize vector θ0, choose a kernel K and a sparsification factor ν; Compute the dictionary; D = I(sj, �aj)1&lt;j&lt;p} from I(sj, aj)1&lt;j&lt;N}; Define the parametrization; Qθ(s, a) = θTφ(s, a) with φ(s, a) = (K((s, a), (91, a1)), ... , K((s, a), (�sp, �ap)))T ; Compute P−1; P−1 = (EN j=1 φjφTj )−1;</rawString>
</citation>
<citation valid="false">
<title>fork= 1,2,...,M do Compute θk, see Eq.</title>
<note>(7); end</note>
<marker></marker>
<rawString>fork= 1,2,...,M do Compute θk, see Eq. (7); end</rawString>
</citation>
<citation valid="false">
<authors>
<author>M �r</author>
</authors>
<title>argmaxaEA 6M (s, a); Algorithm 2: Sparse LSPI.</title>
<marker>�r, </marker>
<rawString>�r* M(s) = argmaxaEA 6M (s, a); Algorithm 2: Sparse LSPI.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Initialization</author>
</authors>
<title>Initialize policy π0, choose a kernel K and a sparsification factor ν; for k = 1, 2,... do Compute the dictionary; D = I(sj,</title>
<booktitle>aj)1&lt;j&lt;pk} from I(sj, aj)1&lt;j&lt;N, (sj, πk−1(s�j))1&lt;j&lt;N}; Define the parametrization; Qθ(s, a) = θT φ(s, a) with φ(s, a) = (K((s, a), (�s1, a1)), ... , K((s, a), (�spk, &amp;quot;pk)))T ; Compute θk−1, see Eq. (6); Compute πk; πk(s) = argmaxaEA 6k_1(s,</booktitle>
<note>a); end</note>
<marker>Initialization, </marker>
<rawString>Initialization; Initialize policy π0, choose a kernel K and a sparsification factor ν; for k = 1, 2,... do Compute the dictionary; D = I(sj, aj)1&lt;j&lt;pk} from I(sj, aj)1&lt;j&lt;N, (sj, πk−1(s�j))1&lt;j&lt;N}; Define the parametrization; Qθ(s, a) = θT φ(s, a) with φ(s, a) = (K((s, a), (�s1, a1)), ... , K((s, a), (�spk, &amp;quot;pk)))T ; Compute θk−1, see Eq. (6); Compute πk; πk(s) = argmaxaEA 6k_1(s, a); end</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>