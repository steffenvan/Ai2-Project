<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000683">
<title confidence="0.838496">
Resolving It, This, and That in Unrestricted Multi-Party Dialog
</title>
<author confidence="0.843042">
Christoph M¨uller
</author>
<affiliation confidence="0.640727">
EML Research gGmbH
</affiliation>
<address confidence="0.774784">
Villa Bosch
Schloß-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
</address>
<email confidence="0.884088">
christoph.mueller@eml-research.de
</email>
<sectionHeader confidence="0.993262" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995213">
We present an implemented system for the
resolution of it, this, and that in tran-
scribed multi-party dialog. The system han-
dles NP-anaphoric as well as discourse-
deictic anaphors, i.e. pronouns with VP an-
tecedents. Selectional preferences for NP or
VP antecedents are determined on the basis
of corpus counts. Our results show that the
system performs significantly better than a
recency-based baseline.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999888363636364">
This paper describes a fully automatic system for
resolving the pronouns it, this, and that in unre-
stricted multi-party dialog. The system processes
manual transcriptions from the ICSI Meeting Cor-
pus (Janin et al., 2003). The following is a short
fragment from one of these transcripts. The letters
FN in the speaker tag mean that the speaker is a fe-
male non-native speaker of English. The brackets
and subscript numbers are not part of the original
transcript.
FN083: Maybe you can also read through the - all the text
which is on the web pages cuz I’d like to change the text
a bit cuz sometimes [it]1’s too long, sometimes [it]2’s too
short, inbreath maybe the English is not that good, so in-
breath um, but anyways - So I tried to do [this]3 today
and if you could do [it]4 afterwards [it]5 would be really
nice cuz I’m quite sure that I can’t find every, like, ortho-
graphic mistake in [it]6 or something. (Bns003)
For each of the six 3rd-person pronouns in the exam-
ple, the task is to automatically identify its referent,
i.e. the entity (if any) to which the speaker makes
reference. Once a referent has been identified, the
pronoun is resolved by linking it to one of its an-
tecedents, i.e. one of the referent’s earlier mentions.
For humans, identification of a pronoun’s referent
is often easy: it1, it2, and it6 are probably used to
refer to the text on the web pages, while it4 is prob-
ably used to refer to reading this text. Humans also
have no problem determining that its is not a normal
pronoun at all. In other cases, resolving a pronoun
is difficult even for humans: this3 could be used to
refer to either reading or changing the text on the
web pages. The pronoun is ambiguous because evi-
dence for more than one interpretation can be found.
Ambiguous pronouns are common in spoken dialog
(Poesio &amp; Artstein, 2005), a fact that has to be taken
into account when building a spoken dialog pronoun
resolution system. Our system is intended as a com-
ponent in an extractive dialog summarization sys-
tem. There are several ways in which coreference in-
formation can be integrated into extractive summa-
rization. Kabadjov et al. (2005) e.g. obtained their
best extraction results by specifying for each sen-
tence whether it contained a mention of a particular
anaphoric chain. Apart from improving the extrac-
tion itself, coreference information can also be used
to substitute anaphors with their antecedents, thus
improving the readability of a summary by minimiz-
ing the number of dangling anaphors, i.e. anaphors
whose antecedents occur in utterances that are not
part of the summary. The paper is structured as fol-
lows: Section 2 outlines the most important chal-
lenges and the state of the art in spoken dialog pro-
noun resolution. Section 3 describes our annotation
experiments, and Section 4 describes the automatic
</bodyText>
<page confidence="0.978317">
816
</page>
<note confidence="0.923669">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 816–823,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
dialog preprocessing. Resolution experiments and
results can be found in Section 5.
</note>
<sectionHeader confidence="0.444081" genericHeader="method">
2 Pronoun Resolution in Spoken Dialog
</sectionHeader>
<bodyText confidence="0.999632727272727">
Spoken language poses some challenges for pro-
noun resolution. Some of these arise from nonrefer-
ential resp. nonresolvable pronouns, which are im-
portant to identify because failure to do so can harm
pronoun resolution precision. One common type
of nonreferential pronoun is pleonastic it. Another
cause of nonreferentiality that only applies to spoken
language is that the pronoun is discarded, i.e. it is
part of an incomplete or abandoned utterance. Dis-
carded pronouns occur in utterances that are aban-
doned altogether.
</bodyText>
<listItem confidence="0.6533915">
ME010: Yeah. Yeah. No, no. There was a whole co- There
was a little contract signed. It was - Yeah. (Bed017)
</listItem>
<bodyText confidence="0.956380576923077">
If the utterance contains a speech repair (Heeman &amp;
Allen, 1999), a pronoun in the reparandum part is
also treated as discarded because it is not part of the
final utterance.
ME10: That’s - that’s - so that’s a - that’s a very good question,
then - now that it - I understand it. (Bro004)
In the corpus of task-oriented TRAINS dialogs de-
scribed in Byron (2004), the rate of discarded pro-
nouns is 7 out of 57 (12.3%) for it and 7 out of
100 (7.0%) for that. Schiffman (1985) reports that
in her corpus of career-counseling interviews, 164
out of 838 (19.57%) instances of it and 80 out of
582 (13.75%) instances of that occur in abandoned
utterances.
There is a third class of pronouns which is referen-
tial but nonetheless unresolvable: vague pronouns
(Eckert &amp; Strube, 2000) are characterized by having
no clearly defined textual antecedent. Rather, vague
pronouns are often used to refer to the topic of the
current (sub-)dialog as a whole.
Finally, in spoken language the pronouns it, this, and
that are often discourse deictic (Webber, 1991), i.e.
they are used to refer to an abstract object (Asher,
1993). We treat as abstract objects all referents of
VP antecedents, and do not distinguish between VP
and S antecedents.
</bodyText>
<footnote confidence="0.972372">
ME013: Well, I mean there’s this Cyber Transcriber service,
right?
ME025: Yeah, that’s true, that’s true. (Bmr001)
</footnote>
<bodyText confidence="0.997324">
Discourse deixis is very frequent in spoken dialog:
The rate of discourse deictic expressions reported in
Eckert &amp; Strube (2000) is 11.8% for pronouns and
as much as 70.9% for demonstratives.
</bodyText>
<subsectionHeader confidence="0.997832">
2.1 State of the Art
</subsectionHeader>
<bodyText confidence="0.999630119047619">
Pronoun resolution in spoken dialog has not received
much attention yet, and a major limitation of the few
implemented systems is that they are not fully au-
tomatic. Instead, they depend on manual removal
of unresolvable pronouns like pleonastic it and dis-
carded and vague pronouns, which are thus pre-
vented from triggering a resolution attempt. This
eliminates a major source of error, but it renders the
systems inapplicable in a real-world setting where
no such manual preprocessing is feasible.
One of the earliest empirically based works adress-
ing (discourse deictic) pronoun resolution in spo-
ken dialog is Eckert &amp; Strube (2000). The au-
thors outline two algorithms for identifying the an-
tecedents of personal and demonstrative pronouns in
two-party telephone conversations from the Switch-
board corpus. The algorithms depend on two non-
trivial types of information: the incompatibility of
a given pronoun with either concrete or abstract an-
tecedents, and the structure of the dialog in terms of
dialog acts. The algorithms are not implemented,
and Eckert &amp; Strube (2000) report results of the
manual application to a set of three dialogs (199 ex-
pressions, including other pronouns than it, this, and
that). Precision and recall are 66.2 resp. 68.2 for
pronouns and 63.6 resp. 70.0 for demonstratives.
An implemented system for resolving personal and
demonstrative pronouns in task-oriented TRAINS
dialogs is described in Byron (2004). The system
uses an explicit representation of domain-dependent
semantic category restrictions for predicate argu-
ment positions, and achieves a precision of 75.0 and
a recall of 65.0 for it (50 instances) and a precision
of 67.0 and a recall of 62.0 for that (93 instances)
if all available restrictions are used. Precision drops
to 52.0 for it and 43.0 for that when only domain-
independent restrictions are used.
To our knowledge, there is only one implemented
system so far that resolves normal and discourse de-
ictic pronouns in unrestricted spoken dialog (Strube
&amp; M¨uller, 2003). The system runs on dialogs from
the Switchboard portion of the Penn Treebank. For
</bodyText>
<page confidence="0.948558">
817
</page>
<bodyText confidence="0.997772870967742">
it, this and that, the authors report 40.41 precision instructions were deliberately kept simple, explain-
and 12.64 recall. The recall does not reflect the ac- ing and illustrating the basic notions of anaphora
tual pronoun resolution performance as it is calcu- and discourse deixis, and describing how markables
lated against all coreferential links in the corpus, not were to be created and linked in the annotation tool.
just those with pronominal anaphors. The system This practice of using a higher number of naive –
draws some non-trivial information from the Penn rather than fewer, highly trained – annotators was
Treebank, including correct NP chunks, grammati- motivated by our intention to elicit as many plau-
cal function tags (subject, object, etc.) and discarded sible interpretations as possible in the presence of
pronouns (based on the -UNF-tag). The treebank ambiguity. It was inspired by the annotation ex-
information is also used for determining the acces- periments of Poesio &amp; Artstein (2005) and Artstein
sibility of potential candidates for discourse deictic &amp; Poesio (2006). Their experiments employed up
pronouns. to 20 annotators, and they allowed for the explicit
In contrast to these approaches, the work described annotation of ambiguity. In contrast, our annota-
in the following is fully automatic, using only infor- tors were instructed to choose the single most plau-
mation from the raw, transcribed corpus. No manual sible interpretation in case of perceived ambigu-
preprocessing is performed, so that during testing, ity. The annotation covered the pronouns it, this,
the system is exposed to the full range of discarded, and that only. Markables for these tokens were
pleonastic, and other unresolvable pronouns. created automatically. From among the pronomi-
nal3 instances, the annotators then identified normal,
vague, and nonreferential pronouns. For normal pro-
nouns, they also marked the most recent antecedent
using the annotation tool’s coreference annotation
function. Markables for antecedents other than it,
this, and that had to be created by the annotators
by dragging the mouse over the respective words
in the tool’s GUI. Nominal antecedents could be ei-
ther noun phrases (NP) or pronouns (PRO). VP an-
tecedents (for discourse deictic pronouns) spanned
only the verb phrase head, i.e. the verb, not the en-
tire phrase. By this, we tried to reduce the number
of disagreements caused by differing markable de-
marcations. The annotation of discourse deixis was
limited to cases where the antecedent was a finite or
infinite verb phrase expressing a proposition, event
type, etc.4
3 Data Collection
The ICSI Meeting Corpus (Janin et al., 2003) is
a collection of 75 manually transcribed group dis-
cussions of about one hour each, involving three
to ten speakers. A considerable number of partic-
ipants are non-native speakers of English, whose
proficiency is sometimes poor, resulting in disflu-
ent or incomprehensible speech. The discussions are
real, unstaged meetings on various, technical topics.
Most of the discussions are regular weekly meet-
ings of a quite informal conversational style, con-
taining many interrupts, asides, and jokes (Janin,
2002). The corpus features a semi-automatically
generated segmentation in which each segment is as-
sociated with a speaker tag and a start and end time
stamp. Time stamps on the word level are not avail-
able. The transcription contains capitalization and
punctuation, and it also explicitly records interrup-
tion points and word fragments (Heeman &amp; Allen,
1999), but not the extent of the related disfluencies.
3.2 Reliability
Inter-annotator agreement was checked by comput-
ing the variant of Krippendorff’s α described in Pas-
sonneau (2004). This metric requires all annotations
to contain the same set of markables, a condition
that is not met in our case. Therefore, we report
α values computed on the intersection of the com-
3.1 Annotation
The annotation was done by naive project-external
annotators, two non-native and two native speak-
ers of English, with the annotation tool MMAX21
on five randomly selected dialogs2. The annotation
3The automatically created markables included all instances
of this and that, i.e. also relative pronouns, determiners, com-
plementizers, etc.
4Arbitrary spans of text could not serve as antecedents for
discourse deictic pronouns. The respective pronouns were to be
treated as vague, due to lack of a well-defined antecedent.
1http://mmax.eml-research.de
2Bed017, Bmr001, Bns003, Bro004, and Bro005.
818
pared annotations, i.e. on those markables that can
be found in all four annotations. Only a subset of
the markables in each annotation is relevant for the
determination of inter-annotator agreement: all non-
pronominal markables, i.e. all antecedent markables
manually created by the annotators, and all referen-
tial instances of it, this, and that. The second column
in Table 1 contains the cardinality of the union of
all four annotators’ markables, i.e. the number of all
distinct relevant markables in all four annotations.
The third and fourth column contain the cardinality
and the relative size of the intersection of these four
markable sets. The fifth column contains α calcu-
lated on the markables in the intersection only. The
four annotators only agreed in the identification of
markables in approx. 28% of cases. α in the five
dialogs ranges from .43 to .52.
</bodyText>
<table confidence="0.998478666666667">
 |1 U 2 U 3 U 4   ||1 n 2 n 3 n 4  |α
Bed017 397 109 27.46 % .47
Bmr001 619 195 31.50 % .43
Bns003 529 131 24.76 % .45
Bro004 703 142 20.20 % .45
Bro005 530 132 24.91 % .52
</table>
<tableCaption confidence="0.999297">
Table 1: Krippendorff’s α for four annotators.
</tableCaption>
<subsectionHeader confidence="0.991995">
3.3 Data Subsets
</subsectionHeader>
<bodyText confidence="0.999688307692308">
In view of the subjectivity of the annotation task,
which is partly reflected in the low agreement even
on markable identification, the manual creation of a
consensus-based gold standard data set did not seem
feasible. Instead, we created core data sets from
all four annotations by means of majority decisions.
The core data sets were generated by automatically
collecting in each dialog those anaphor-antecedent
pairs that at least three annotators identified indepen-
dently of each other. The rationale for this approach
was that an anaphoric link is the more plausible the
more annotators identify it. Such a data set certainly
contains some spurious or dubious links, while lack-
ing some correct but more difficult ones. However,
we argue that it constitutes a plausible subset of
anaphoric links that are useful to resolve.
Table 2 shows the number and lengths of anaphoric
chains in the core data set, broken down accord-
ing to the type of the chain-initial antecedent. The
rare type OTHER mainly contains adjectival an-
tecedents. More than 75% of all chains consist of
two elements only. More than 33% begin with a
pronoun. From the perspective of extractive sum-
marization, the resolution of these latter chains is not
helpful since there is no non-pronominal antecedent
that it can be linked to or substituted with.
</bodyText>
<table confidence="0.999921791666667">
length 2 3 4 5 6 &gt; 6 total
NP 17 3 2 - 1 - 23
PRO 14 - 2 - - - 16
VP 6 1 - - - - 7
Bed017 - - - - - - -
OTHER
all
37 4 4 - 1 - 46
80.44%
NP 14 4 1 1 1 2 23
PRO 19 9 2 2 1 1 34
VP 9 5 - - - - 14
Bmr001 - - - - - - -
OTHER
all
42 18 3 3 2 3 71
59.16%
NP 18 3 3 1 - - 25
PRO 18 1 1 - - - 20
VP 14 4 - - - - 18
Bns003 - - - - - - -
OTHER
all
50 8 4 1 - - 63
79.37%
NP 38 5 3 1 - - 47
PRO 21 4 - 1 - - 26
VP 8 1 1 - - - 10
Bro004 2 1 - - - - 3
OTHER
all
69 11 4 2 - - 86
80.23%
NP 37 7 1 - - - 45
PRO 15 3 1 - - - 19
VP 8 1 - 1 - - 10
Bro005 3 - - - - - 3
OTHER
all
63 11 2 1 - - 77
81.82%
NP 124 22 10 3 2 2 163
PRO 87 17 6 3 1 1 115
VP 45 12 1 1 - - 59
r 5 1 - - - - 6
OTHER
all
76261% 52 17 7 3 3 343
</table>
<tableCaption confidence="0.996798">
Table 2: Anaphoric chains in core data set.
</tableCaption>
<sectionHeader confidence="0.985942" genericHeader="method">
4 Automatic Preprocessing
</sectionHeader>
<bodyText confidence="0.999930166666667">
Data preprocessing was done fully automatically,
using only information from the manual tran-
scription. Punctuation signs and some heuristics
were used to split each dialog into a sequence
of graphemic sentences. Then, a shallow disflu-
ency detection and removal method was applied,
which removed direct repetitions, nonlexicalized
filled pauses like uh, um, interruption points, and
word fragments. Each sentence was then matched
against a list of potential discourse markers (actu-
ally, like, you know, I mean, etc.) If a sentence
contained one or more matches, string variants were
created in which the respective words were deleted.
Each of these variants was then submitted to a parser
trained on written text (Charniak, 2000). The vari-
ant with the highest probability (as determined by
the parser) was chosen. NP chunk markables were
created for all non-recursive NP constituents identi-
</bodyText>
<page confidence="0.971196">
819
</page>
<bodyText confidence="0.99939558974359">
fied by the parser. Then, VP chunk markables were has an antecedent with a similar function. The sub-
created. Complex verbal constructions like MD + ject preference, in contrast, states that subject an-
INFINITIVE were modelled by creating markables tecedents are generally preferred over those with
for the individual expressions, and attaching them less salient functions, independent of the grammat-
to each other with labelled relations like INFINI- ical function of the anaphor. Some of our features
TIVE COMP. NP chunks were also attached, using encode this functional and structural parallelism, in-
relations like SUBJECT, OBJECT, etc. cluding identity of form (for PRO antecedents) and
5 Automatic Pronoun Resolution identity of grammatical function or governing verb.
We model pronoun resolution as binary classifica- A more sophisticated constraint on NP an-
tion, i.e. as the mapping of anaphoric mentions to tecedents is what Eckert &amp; Strube (2000) call I-
previous mentions of the same referent. This method Incompatibility, i.e. the semantic incompatibility of
is not incremental, i.e. it cannot take into account a pronoun with an individual (i.e. NP) antecedent.
earlier resolution decisions or any other information As Eckert &amp; Strube (2000) note, subject pronouns
beyond that which is conveyed by the two mentions. in copula constructions with adjectives that can only
Since more than 75% of the anaphoric chains in our modify abstract entities (like e.g. true, correct, right)
data set would not benefit from incremental process- are incompatible with concrete antecedents like car.
ing because they contain one anaphor only, we see We postulate that the preference of an adjective to
this limitation as acceptable. In addition, incremen- modify an abstract entity (in the sense of Eckert &amp;
tal processing bears the risk of system degradation Strube (2000)) can be operationalized as the condi-
due to error propagation. tional probability of the adjective to appear with a
5.1 Features to-infinitive resp. a that-sentence complement, and
In the binary classification model, a pronoun is re- introduce two features which calculate the respec-
solved by creating a set of candidate antecedents and tive preference on the basis of corpus5 counts. For
searching this set for a matching one. This search the first feature, the following query is used:
process is mainly influenced by two factors: ex-
clusion of candidates due to constraints, and selec-
tion of candidates due to preferences (Mitkov, 2002).
Our features encode information relevant to these
two factors, plus more generally descriptive factors
like distance etc. Computation of all features was
fully automatic.
Shallow constraints for nominal antecedents include
number, gender and person incompatibility, embed-
ding of the anaphor into the antecedent, and coar-
gumenthood (i.e. the antecedent and anaphor must
not be governed by the same verb). For VP an-
tecedents, a common shallow constraint is that the
anaphor must not be governed by the VP antecedent
(so-called argumenthood). Preferences, on the other
hand, define conditions under which a candidate
probably is the correct antecedent for a given pro-
noun. A common shallow preference for nomi-
nal antecedents is the parallel function preference,
which states that a pronoun with a particular gram-
matical function (i.e. subject or object) preferably
820
# it (’s|is|was|were) ADJ to
# it (’s|is|was|were) ADJ
According to Eckert &amp; Strube (2000), pronouns that
are objects of verbs which mainly take sentence
complements (like assume, say) exhibit a similar
incompatibility with NP antecedents, and we cap-
ture this with a similar feature. Constraints for
VPs include the following: VPs are inaccessible for
discourse deictic reference if they fail to meet the
right frontier condition (Webber, 1991). We use
a feature which is similar to that used by Strube
&amp; M¨uller (2003) in that it approximates the right
frontier on the basis of syntactic (rather than dis-
course structural) relations. Another constraint is
A-Incompatibility, i.e. the incompatibility of a pro-
noun with an abstract (i.e. VP) antecedent. Accord-
ing to Eckert &amp; Strube (2000), subject pronouns in
copula constructions with adjectives that can only
modify concrete entities (like e.g. expensive, tasty)
are incompatible with abstract antecedents, i.e. they
SBased on the approx. 250,000,000 word TIPSTER corpus
(Harman &amp; Liberman, 1994).
cannot be discourse deictic. The function of this
constraint is already covered by the two corpus-
based features described above in the context of I-
Incompatibility. Another feature, based on Yang
et al. (2005), encodes the semantic compatibility
of anaphor and NP antecedent. We operationalize
the concept of semantic compatibility by substitut-
ing the anaphor with the antecedent head and per-
forming corpus queries. E.g., if the anaphor is ob-
ject, the following query6 is used:
</bodyText>
<equation confidence="0.991619">
# (V|Vs|Ved|Ving) (0|a|an|the|this|that) ANTE+
# (V|Vs|Ved|Ving) (0|the|these|those) ANTES
# (ANTE|ANTES)
</equation>
<bodyText confidence="0.992438">
If the anaphor is the subject in an adjective cop-
ula construction, we use the following corpus count
to quantify the compatibility between the predi-
cated adjective and the NP antecedent (Lapata et al.,
1999):
</bodyText>
<equation confidence="0.337229666666667">
# ADJ (ANTE|ANTES) + # ANTE (is|was) ADJ+
# ANTES (are|were) ADJ
# ADJ
</equation>
<bodyText confidence="0.999529294117647">
A third class of more general properties of the po-
tential anaphor-antecedent pair includes the type of
anaphor (personal vs. demonstrative), type of an-
tecedent (definite vs. indefinite noun phrase, pro-
noun, finite vs. infinite verb phrase, etc.). Special
features for the identification of discarded expres-
sions include the distance (in words) to the closest
preceeding resp. following disfluency (indicated in
the transcription as an interruption point, word frag-
ment, or uh resp. um). The relation between po-
tential anaphor and (any type of) antecedent is de-
scribed in terms of distance in seconds7 and words.
For VP antecedents, the distance is calculated from
the last word in the entire phrase, not from the
phrase head. Another feature which is relevant for
dialog encodes whether both expressions are uttered
by the same speaker.
</bodyText>
<footnote confidence="0.995669111111111">
6V is the verb governing the anaphor. Correct inflected
forms were also generated for irregular verbs. ANTE resp.
ANTES is the singular resp. plural head of the antecedent.
7Since the data does not contain word-level time stamps, this
distance is determined on the basis of a simple forced align-
ment. For this, we estimated the number of syllables in each
word on the basis of its vowel clusters, and simply distributed
the known duration of the segment evenly on all words it con-
tains.
</footnote>
<subsectionHeader confidence="0.996803">
5.2 Data Representation and Generation
</subsectionHeader>
<bodyText confidence="0.999958263157895">
Machine learning data for training and testing was
created by pairing each anaphor with each of its
compatible potential antecedents within a certain
temporal distance (9 seconds for NP and 7 seconds
for VP antecedents), and labelling the resulting data
instance as positive resp. negative. VP antecedent
candidates were created only if the anaphor was ei-
ther that8 or the object of a form of do.
Our core data set does not contain any nonreferen-
tial pronouns, though the classifier is exposed to the
full range of pronouns, including discarded and oth-
erwise nonreferential ones, during testing. We try
to make the classifier robust against nonreferential
pronouns in the following way: From the manual
annotations, we select instances of it, this, and that
that at least three annotators identified as nonrefer-
ential. For each of these, we add the full range of
all-negative instances to the training data, applying
the constraints mentioned above.
</bodyText>
<subsectionHeader confidence="0.997685">
5.3 Evaluation Measure
</subsectionHeader>
<bodyText confidence="0.9999677">
As Bagga &amp; Baldwin (1998) point out, in an
application-oriented setting, not all anaphoric links
are equally important: If a pronoun is resolved to
an anaphoric chain that contains only pronouns, this
resolution can be treated as neutral because it has
no application-level effect. The common corefer-
ence evaluation measure described in Vilain et al.
(1995) is inappropriate in this setting. We calculate
precision, recall and F-measure on the basis of the
following definitions: A pronoun is resolved cor-
rectly resp. incorrectly only if it is linked (directly
or transitively) to the correct resp. incorrect non-
pronominal antecedent. Likewise, the number of
maximally resolvable pronouns in the core data set
(i.e. the evaluation key) is determined by consider-
ing only pronouns in those chains that do not begin
with a pronoun. Note that our definition of precision
is stricter (and yields lower figures) than that ap-
plied in the ACE context, as the latter ignores incor-
rect links between two expressions in the response
</bodyText>
<footnote confidence="0.976879166666667">
8It is a common observation that demonstratives (in partic-
ular that) are preferred over it for discourse deictic reference
(Schiffman, 1985; Webber, 1991; Asher, 1993; Eckert &amp; Strube,
2000; Byron, 2004; Poesio &amp; Artstein, 2005). This preference
can also be observed in our core data set: 44 out of 59 VP an-
tecedents (69.49%) are anaphorically referred to by that.
</footnote>
<page confidence="0.996727">
821
</page>
<bodyText confidence="0.999903875">
if these expressions happen to be unannotated in the
key, while we treat them as precision errors unless
the antecedent is a pronoun. The same is true for
links in the response that were identified by less than
three annotators in the key. While it is practical to
treat those links as wrong, it is also simplistic be-
cause it does not do justice to ambiguous pronouns
(cf. Section 6).
</bodyText>
<subsectionHeader confidence="0.91365">
5.4 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.999997117647059">
Our best machine learning results were obtained
with the Weka9 Logistic Regression classifier.10 All
experiments were performed with dialog-wise cross-
validation. For each run, training data was created
from the manually annotated markables in four di-
alogs from the core data set, while testing was per-
formed on the automatically detected chunks in the
remaining fifth dialog. For training and testing, the
person, number11, gender, and (co-)argument con-
straints were used. If an anaphor gave rise to a pos-
itive instance, no negative training instances were
created beyond that instance. If a referential anaphor
did not give rise to a positive training instance (be-
cause its antecedent fell outside the search scope
or because it was removed by a constraint), no in-
stances were created for that anaphor. Instances for
nonreferential pronouns were added to the training
data as described in Section 5.2.
During testing, we select for each potential anaphor
the positive antecedent with the highest overall con-
fidence. Testing parameters include it-filter,
which switches on and off the module for the detec-
tion of nonreferential it described in M¨uller (2006).
When evaluated alone, this module yields a preci-
sion of 80.0 and a recall of 60.9 for the detection
of pleonastic and discarded it in the five ICSI di-
alogs. For training, this module was always on.
We also vary the parameter tipster, which con-
trols whether or not the corpus frequency features
are used. If tipster is off, we ignore the corpus
frequency features both during training and testing.
We first ran a simple baseline system which re-
solved pronouns to their most recent compatible an-
tecedent, applying the same settings and constraints
</bodyText>
<footnote confidence="0.99838275">
9http://www.cs.waikato.ac.nz/ml/weka/
10The full set of experiments is described in M¨uller (2007).
11The number constraint applies to it only, as this and that
can have both singular and plural antecedents (Byron, 2004).
</footnote>
<bodyText confidence="0.999772416666667">
as for testing (cf. above). The results can be found
in the first part of Table 3. Precision, recall and F-
measure are provided for ALL and for NP and VP
antecedents individually. The parameter tipster
is not available for the baseline system. The best
baseline performance is precision 4.88, recall 20.06
and F-measure 7.85 in the setting with it-filter
on. As expected, this filter yields an increase in pre-
cision and a decrease in recall. The negative effect
is outweighed by the positive effect, leading to a
small but insignificant12 increase in F-measure for
all types of antecedents.
</bodyText>
<table confidence="0.967749071428571">
Baseline Logistic Regression
Setting Ante P R F P R F
-it-filter -tipster NP 4.62 27.12 7.90 18.53 20.34 19.39∗
VP 1.72 2.63 2.08 13.79 10.53 11.94
ALL 4.40 20.69 7.25 17.67 17.56 17.61∗
+tipster NP - - - 19.33 22.03 20.59∗∗∗
VP - - - 13.43 11.84 12.59
ALL - - - 18.16 19.12 18.63∗∗
+it-filter -tipster NP 5.18 26.27 8.65 17.87 17.80 17.83∗
VP 1.77 2.63 2.12 13.12 10.53 11.68
ALL 4.88 20.06 7.85 16.89 15.67 16.26∗
+tipster NP - - - 20.82 21.61 21.21∗∗
VP - - - 11.27 10.53 10.88
ALL - - - 18.67 18.50 18.58∗∗
</table>
<tableCaption confidence="0.999845">
Table 3: Resolution results.
</tableCaption>
<bodyText confidence="0.94322388">
The second part of Table 3 shows the results of the
Logistic Regression classifier. When compared to
the best baseline, the F-measures are consistently
better for NP, VP, and ALL. The improvement is
(sometimes highly) significant for NP and ALL, but
never for VP. The best F-measure for ALL is 18.63,
yielded by the setting with it-filter off and
tipster on. This setting also yields the best F-
measure for VP and the second best for NP. The
contribution of the it-filter is disappointing: In both
tipster settings, the it-filter causes F-measure for
ALL to go down. The contribution of the corpus
features, on the other hand, is somewhat inconclu-
sive: In both it-filter settings, they cause an in-
crease in F-measure for ALL. In the first setting, this
increase is accompanied by an increase in F-measure
for VP, while in the second setting, F-measure for
VP goes down. It has to be noted, however, that
none of the improvements brought about by the it-
filter or the tipster corpus features is statistically sig-
nificant. This also confirms some of the findings of
Kehler et al. (2004), who found features similar to
12Significance of improvement in F-measure is tested using
a paired one-tailed t-test and P &lt;= 0.05 (∗), P &lt;= 0.01 (∗∗),
and P &lt;= 0.005 (∗∗∗).
</bodyText>
<page confidence="0.946839">
822
</page>
<bodyText confidence="0.991831837837838">
our tipster corpus features not to be significant for Charniak, E. (2000). A maximum-entropy-inspired parser. In
NP-anaphoric pronoun resolution in written text. Proc. ofNAACL-00, pp. 132–139.
6 Conclusions and Future Work Eckert, M. &amp; M. Strube (2000). Dialogue acts, synchronis-
The system described in this paper is – to our knowl- ing units and anaphora resolution. Journal of Semantics,
edge – the first attempt towards fully automatic res- 17(1):51–89.
olution of NP-anaphoric and discourse deictic pro- Harman, D. &amp; M. Liberman (1994). TIPSTER Complete
nouns (it, this, and that) in multi-party dialog. Un- LDC93T3A. 3 CD-ROMS. Linguistic Data Consortium,
like other implemented systems, it is usable in a re- Philadelphia, Penn., USA.
alistic setting because it does not depend on manual Heeman, P. &amp; J. Allen (1999). Speech repairs, intonational
pronoun preselection or non-trivial discourse struc- phrases, and discourse markers: Modeling speakers’ ut-
ture or domain knowledge. The downside is that, terances in spoken dialogue. Computational Linguistics,
at least in our strict evaluation scheme, the perfor- 25(4):527–571.
mance is rather low, especially when compared to Janin, A. (2002). Meeting recorder. In Proceedings of the
that of state-of-the-art systems for pronoun resolu- Applied Voice Input/Output Society Conference (AVIOS),
tion in written text. In future work, it might be San Jose, California, USA, May 2002.
worthwhile to consider less rigorous and thus more Janin, A., D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Mor-
appropriate evaluation schemes in which links are gan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke &amp;
weighted according to how many annotators identi- C. Wooters (2003). The ICSI Meeting Corpus. In Pro-
fied them. ceedings of the IEEE International Conference on Acous-
In its current state, the system only processes man- tics, Speech and Signal Processing, Hong Kong, pp. 364–
ual dialog transcripts, but it also needs to be eval- 367.
uated on the output of an automatic speech recog- Kabadjov, M. A., M. Poesio &amp; J. Steinberger (2005). Task-
nizer. While this will add more noise, it will also based evaluation of anaphora resolution: The case of
give access to useful prosodic features like stress. summarization. In Proceedings of the RANLP Workshop
Finally, the system also needs to be evaluated extrin- on Crossing Barriers in Text Summarization Research,
sically, i.e. with respect to its contribution to dialog Borovets, Bulgaria.
summarization. It might turn out that our system al- Kehler, A., D. Appelt, L. Taylor &amp; A. Simma (2004). The
ready has a positive effect on extractive summariza- (non)utility of predicate-argument frequencies for pro-
tion, even though its performance is low in absolute noun interpretation. In Proc. ofHLT-NAACL-04, pp. 289–
terms. 296.
Acknowledgments. This work has been funded Lapata, M., S. McDonald &amp; F. Keller (1999). Determinants
by the Deutsche Forschungsgemeinschaft as part of of adjective-noun plausibility. In Proc. of EACL-99, pp.
the DIANA-Summ project (STR-545/2-1,2) and by 30–36.
the Klaus Tschira Foundation. We are grateful to the Mitkov, R. (2002). Anaphora Resolution. London, UK: Long-
anonymous ACL reviewers for helpful comments man.
and suggestions. We also thank Ron Artstein for M¨uller, C. (2006). Automatic detection of nonreferential it in
help with significance testing. spoken multi-party dialog. In Proc. of EACL-06, pp. 49–
</bodyText>
<sectionHeader confidence="0.420943" genericHeader="method">
References 56.
</sectionHeader>
<reference confidence="0.99035452">
Artstein, R. &amp; M. Poesio (2006). Identifying reference to ab- M¨uller, C. (2007). Fully automatic resolution of it, this, and
stract objects in dialogue. In Proc. of BranDial-06, pp. that in unrestricted multi-party dialog., (Ph.D. thesis).
56–63. Eberhard Karls Universit¨at T¨ubingen, Germany. To ap-
Asher, N. (1993). Reference to Abstract Objects in Discourse. pear.
Dordrecht, The Netherlands: Kluwer. Passonneau, R. J. (2004). Computing reliability for co-
Bagga, A. &amp; B. Baldwin (1998). Algorithms for scoring coref- reference annotation. In Proc. ofLREC-04.
erence chains. In Proc. ofLREC-98, pp. 79–85. Poesio, M. &amp; R. Artstein (2005). The reliability of anaphoric
Byron, D. K. (2004). Resolving pronominal reference to ab- annotation, reconsidered: Taking ambiguity into account.
stract entities., (Ph.D. thesis). University of Rochester. In Proceedings of the ACL Workshop on Frontiers in Cor-
823 pus Annotation II: Pie in the Sky, pp. 76–83.
Schiffman, R. J. (1985). Discourse constraints on ’it’ and
’that’: A Study of Language Use in Career Counseling
Interviews., (Ph.D. thesis). University of Chicago.
Strube, M. &amp; C. M¨uller (2003). A machine learning approach to
pronoun resolution in spoken dialogue. In Proc. ofACL-
03, pp. 168–175.
Vilain, M., J. Burger, J. Aberdeen, D. Connolly &amp; L. Hirschman
(1995). A model-theoretic coreference scoring scheme.
In Proc. ofMUC-6, pp. 45–52.
Webber, B. L. (1991). Structure and ostension in the interpre-
tation of discourse deixis. Language and Cognitive Pro-
cesses, 6(2):107–135.
Yang, X., J. Su &amp; C. L. Tan (2005). Improving pronoun reso-
lution using statistics-based semantic compatibility infor-
mation. In Proc. ofACL-05, pp. 165–172.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.476692">
<title confidence="0.99966">and Unrestricted Multi-Party Dialog</title>
<author confidence="0.995168">Christoph M¨uller</author>
<affiliation confidence="0.956747">EML Research gGmbH</affiliation>
<author confidence="0.729328">Villa Bosch</author>
<address confidence="0.88839">Schloß-Wolfsbrunnenweg 33 69118 Heidelberg, Germany</address>
<email confidence="0.999251">christoph.mueller@eml-research.de</email>
<abstract confidence="0.997993636363636">We present an implemented system for the of and transcribed multi-party dialog. The system handles NP-anaphoric as well as discoursedeictic anaphors, i.e. pronouns with VP antecedents. Selectional preferences for NP or VP antecedents are determined on the basis of corpus counts. Our results show that the system performs significantly better than a recency-based baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>