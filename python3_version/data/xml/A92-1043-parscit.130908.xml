<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.342299">
<title confidence="0.9989445">
Learning a Scanning Understanding for
&amp;quot;Real-world&amp;quot; Library Categorization
</title>
<author confidence="0.999685">
Stefan Wermter*
</author>
<affiliation confidence="0.9243605">
Computer Science Department
University of Hamburg
2000 Hamburg 50
Federal Republic of Germany
</affiliation>
<sectionHeader confidence="0.970808" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999232">
This paper describes, compares, and evaluates three dif-
ferent approaches for learning a semantic classification of
library titles: 1) syntactically condensed titles, 2) com-
plete titles, and 3) titles without insignificant words are
used for learning the classification in connectionist re-
current plausibility networks. In particular, we demon-
strate in this paper that automatically derived feature
representations and recurrent plausibility networks can
scale up to several thousand library titles and reach al-
most perfect classification accuracy (&gt;98%) compared
to a real-world library classification.
</bodyText>
<sectionHeader confidence="0.998791" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999809461538462">
Our goal is to examine hybrid symbolic/connectionist
and connectionist approaches for classifying a substan-
tial number of real-world title phrases. These approaches
are embedded in the framework of SCAN (Wermter 92),
a Symbolic Connectionist Approach for Natural lan-
guage phrases, aimed towards a scanning understanding
of natural language rather than focusing on an in-depth
understanding. For our experiments we took an existing
classification from the online catalog of the main library
at Dortmund University and as a first subclassification
we selected titles from three classes: &amp;quot;computer science&amp;quot;
(CS), &amp;quot;history/politics&amp;quot; (HP), and &amp;quot;materials/geology&amp;quot;
(MG).
</bodyText>
<subsectionHeader confidence="0.8182435">
2 Preprocessing of Title Phrases
2.1 Symbolic Syntactic Condensation
</subsectionHeader>
<bodyText confidence="0.999563222222222">
The first approach used syntactic condensation based on
a chart parser and a headnoun extractor. The symbolic
chart parser built a syntactic structure for a title using
a context-free grammar and a syntactic lexicon. Then
the headnoun extractor retrieved the sequence of head-
nouns for building a compound noun. For instance, the
compound noun &amp;quot;software access guidelines&amp;quot; was gen-
erated from &amp;quot;guidelines on subject access to microcom-
puter software&amp;quot;. This headnoun extractor was motivated
</bodyText>
<footnote confidence="0.98839775">
`This research was supported in part by the Fed-
eral Secretary for Research and Technology under contract
#01IV101A0 and by the Computer Science Department of
Dortmund University.
</footnote>
<bodyText confidence="0.999692857142857">
by the close relationship between noun phrases and com-
pound nouns and by the importance of nouns as content
words (Finin 80).
Each noun in a compound noun was represented
with 16 binary manually encoded semantic features, like
measuring-event, changing-event, scientific-field, prop-
erty, mechanism etc. The set of semantic features had
been developed as a noun representation for a related
scientific technical domain and had been used for struc-
tural disambiguation (Wermter 89). The first approach
contained a relatively small set of 76 titles since for each
noun 16 features had to be determined manually and for
each word in the title the syntactic category had to be
in the lexicon which contained 900 entries.
</bodyText>
<subsectionHeader confidence="0.999763">
2.2 Unrestricted Complete Phrases
</subsectionHeader>
<bodyText confidence="0.999992125">
In our second approach, we used an automatically ac-
quired significance vector for each word based on the
occurrence of the words in certain classes. Each value
v(w, ci) in a significance vector represented the frequency
of occurrence of word w in class ci divided by the total
frequency of word w in all classes. These significance
vectors were computed for the words of 2493 library ti-
tles from the three classes CS, HP, and MG.
</bodyText>
<subsectionHeader confidence="0.999897">
2.3 Elimination of Insignificant Words
</subsectionHeader>
<bodyText confidence="0.999968">
In the third approach we analyzed the most frequent
words in the 2493 titles of the second approach. We
eliminated words that occured more than five times in
our corpus and that were prepositions, conjunctions, ar-
ticles, and pronouns. Words were represented with the
same significance vectors as in the second approach. This
elimination of frequently occuring domain-independent
words was expected to make classification easier since
many domain-independent insignificant words were re-
moved from the titles.
</bodyText>
<sectionHeader confidence="0.9810615" genericHeader="method">
3 The Architecture of the Recurrent
Plausibility Network
</sectionHeader>
<bodyText confidence="0.999939714285714">
The semantic classification was learned by using a con-
nectionist recurrent plausibility network. A recurrent
plausibility network is similar to a simple recurrent net-
work (Elman 89) but instead of learning to predict
words, recurrent connections support the assignment of
plausible classes (see figure 1). The recurrent plausi-
bility network was trained in a supervised mode using
</bodyText>
<page confidence="0.987382">
251
</page>
<bodyText confidence="0.9995135">
the backpropagation learning algorithm (Rumelhart et
al. 86). In each training step the feature representa-
tion of a word and its preceding context was presented
to the network in the word bank and context bank to-
gether with the desired class. A unit in the output layer
received the value 1 if the unit represented the particular
class of the title, otherwise the unit received the value 0.
The real-valued hidden layer represented the context of
preceding words. At the beginning of a title the context
bank was initialized with values of 0 since there was no
preceding context. After the first word had been pre-
sented the context bank was initialized with the values
of the hidden layer that encoded the reduced preceding
context.
</bodyText>
<figureCaption confidence="0.997409">
Figure 1: Recurrent Plausibility Network for Titles
</figureCaption>
<sectionHeader confidence="0.998592" genericHeader="evaluation">
4 Results and Conclusions
</sectionHeader>
<bodyText confidence="0.99990875862069">
For the first approach the 76 titles were divided into 61
titles for training and 15 titles for testing. For the 2493
titles in the second and third approach we used 1249 ti-
tles for training and 1244 for testing. Using these train-
ing and test sets we examined different network archi-
tectures and training parameters. For the first approach
a configuration with 6 hidden units and a learning rate
of 0.001 showed the smallest number of errors on the
training and test set. For the second and third approach
3 hidden units and the learning rate 0.0001 performed
best.
Below we show example titles, the titles after pre-
processing, and their sequential class assignment. The
first two titles illustrate that two titles with the same fi-
nal headnoun (&amp;quot;design&amp;quot;) are assigned to different classes
due to their different learned preceding context. The
third title illustrates the second approach of classifying
an unrestricted complete phrase. The network first as-
signs the CS class for the initial phrase &amp;quot;On the op-
erating experience of the...&amp;quot; since such initial represen-
tations have occurred in the CS class. However, when
more specific knowledge is available (&amp;quot;doppler sodar sys-
tem...&amp;quot;) the assigned class is changed to the MG class.
In the fourth example the same title is shown for the
third approach which eliminates insignificant domain-
independent words. In general, the second and third
approach have the potential to deal with unanticipated
grammatical and even ungrammatical titles since they
do not rely on a predefined grammar.
</bodyText>
<listItem confidence="0.994749888888889">
1. Title: Design of relational database schemes by
deleting attributes in the canonical decomposition;
Approachl: Compound noun: Decomposition (CS)
attribute (CS) scheme (CS) design (CS)
2. Title: Design of bulkheads for controlling water in
underground mines; Approachl: Compound noun:
Mine (MG) water (MG) bulkhead (MG) design
(MG)
3. Title: On the operating experience of the doppler
sodar system at the Forschungszentrum Juelich;
Approach2: Unrestricted complete title: On (CS)
the (CS) operating (CS) experience (CS) of (CS)
the (CS) doppler (MG) sodar (MG) system (MG) at
(MG) the (MG) Forschungszentrum (MG) Juelich
(MG)
4. Title: On the operating experience of the doppler
sodar system at the Forschungszentrum Juelich;
Approach3: Unrestricted reduced title:
</listItem>
<bodyText confidence="0.995889153846154">
operating (CS) experience (CS) doppler (MG) so-
dar (MG) system (MG) Forschungszentrum (MG)
Juelich (MG)
The overall performance of the three approaches as
recorded in the best found configuration is summarized
in table 1. The first approach performed worst for clas-
sifying new titles from the test set although the titles
in the training set were learned completely. The second
approach performed better on the test set for a much
bigger training and test set of unrestricted phrases. The
third approach demonstrated that the elimination of in-
significant words from unrestricted phrases can improve
performance for the big set of titles.
</bodyText>
<table confidence="0.999598333333333">
Performance Approachl Approach2 Approach3
Training 100% 98.4% 99.9%
Testing 93% 97.7% 99.4%
</table>
<tableCaption confidence="0.999195">
Table 1: Performance for Semantic Classification
</tableCaption>
<bodyText confidence="0.999944">
In conclusion, we described and evaluated three dif-
ferent approaches for semantic classification which use
hybrid symbolic/connectionist and connectionist repre-
sentations. Our results show that recurrent plausibility
networks and automatically acquired feature representa-
tions can provide an efficient basis for learning and gen-
eralizing a scanning understanding of real-world library
classifications.
</bodyText>
<sectionHeader confidence="0.999072" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997418411764706">
Elman J.L. 1989. Structured representations and connection-
ist models. Proceedings of the Eleventh Annual Conference
of the Cognitive Science Society, Ann Arbor.
Finin T.W. 1980. The semantic Interpretation of Com-
pound Nominals. PhD Thesis. University of Illinois at
Urbana-Champaign.
Rumelhart D.E., Hinton G.E., Williams R.J. 1986. Learn-
ing Internal Representations by Error Propagation. In:
Rumelhart D.E., McClelland J.L. (Eds.) Parallel distributed
Processing Vol. I. MIT Press, Cambridge, MA.
Wermter, S. 1989. Integration of Semantic and Syntac-
tic Constraints for Structural Noun Phrase Disambiguation.
Proceedings of the Eleventh International Joint Conference
on Artificial Intelligence, Detroit.
Wermter, S. 1992 (forthcoming). Scanning Understand-
ing: A Symbolic Connectionist Approach for Natural Lan-
guage Phrases. Technical Report. University of Hamburg.
</reference>
<figure confidence="0.98992575">
Classes
Hidden layer
Word bank Context bank
Semantic input features
</figure>
<page confidence="0.962713">
252
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.369743">
<title confidence="0.999954">Learning a Scanning Understanding for &amp;quot;Real-world&amp;quot; Library Categorization</title>
<author confidence="0.999991">Stefan Wermter</author>
<affiliation confidence="0.999843">Science Department University of Hamburg</affiliation>
<address confidence="0.996865">2000 Hamburg 50</address>
<note confidence="0.379714">Federal Republic of Germany</note>
<abstract confidence="0.998200083333333">This paper describes, compares, and evaluates three different approaches for learning a semantic classification of library titles: 1) syntactically condensed titles, 2) complete titles, and 3) titles without insignificant words are used for learning the classification in connectionist recurrent plausibility networks. In particular, we demonstrate in this paper that automatically derived feature representations and recurrent plausibility networks can scale up to several thousand library titles and reach almost perfect classification accuracy (&gt;98%) compared to a real-world library classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J L Elman</author>
</authors>
<title>Structured representations and connectionist models.</title>
<date>1989</date>
<booktitle>Proceedings of the Eleventh Annual Conference of the Cognitive Science Society,</booktitle>
<location>Ann Arbor.</location>
<marker>Elman, 1989</marker>
<rawString>Elman J.L. 1989. Structured representations and connectionist models. Proceedings of the Eleventh Annual Conference of the Cognitive Science Society, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T W Finin</author>
</authors>
<title>The semantic Interpretation of Compound Nominals. PhD Thesis.</title>
<date>1980</date>
<institution>University of Illinois at Urbana-Champaign.</institution>
<marker>Finin, 1980</marker>
<rawString>Finin T.W. 1980. The semantic Interpretation of Compound Nominals. PhD Thesis. University of Illinois at Urbana-Champaign.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Rumelhart</author>
<author>G E Hinton</author>
<author>R J Williams</author>
</authors>
<title>Learning Internal Representations by Error Propagation. In:</title>
<date>1986</date>
<booktitle>Rumelhart D.E., McClelland J.L. (Eds.) Parallel distributed Processing Vol. I.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>Rumelhart D.E., Hinton G.E., Williams R.J. 1986. Learning Internal Representations by Error Propagation. In: Rumelhart D.E., McClelland J.L. (Eds.) Parallel distributed Processing Vol. I. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wermter</author>
</authors>
<title>Integration of Semantic and Syntactic Constraints for Structural Noun Phrase Disambiguation.</title>
<date>1989</date>
<booktitle>Proceedings of the Eleventh International Joint Conference on Artificial Intelligence,</booktitle>
<location>Detroit.</location>
<marker>Wermter, 1989</marker>
<rawString>Wermter, S. 1989. Integration of Semantic and Syntactic Constraints for Structural Noun Phrase Disambiguation. Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, Detroit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wermter</author>
</authors>
<title>(forthcoming). Scanning Understanding: A Symbolic Connectionist Approach for Natural Language Phrases.</title>
<date>1992</date>
<tech>Technical</tech>
<institution>Report. University of Hamburg.</institution>
<marker>Wermter, 1992</marker>
<rawString>Wermter, S. 1992 (forthcoming). Scanning Understanding: A Symbolic Connectionist Approach for Natural Language Phrases. Technical Report. University of Hamburg.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>