<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.965683">
On the Applicability of Global Index Grammars
</title>
<author confidence="0.999137">
Jos´e M. Casta˜no
</author>
<affiliation confidence="0.985295">
Computer Science Department
Brandeis University
</affiliation>
<email confidence="0.996883">
jcastano@cs.brandeis.edu
</email>
<sectionHeader confidence="0.983132" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999567727272727">
We investigate Global Index Gram-
mars (GIGs), a grammar formalism
that uses a stack of indices associated
with productions and has restricted
context-sensitive power. We discuss
some of the structural descriptions
that GIGs can generate compared with
those generated by LIGs. We show
also how GIGs can represent structural
descriptions corresponding to HPSGs
(Pollard and Sag, 1994) schemas.
</bodyText>
<sectionHeader confidence="0.996299" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996170166666667">
The notion of Mildly context-sensitivity was in-
troduced in (Joshi, 1985) as a possible model
to express the required properties of formalisms
that might describe Natural Language (NL)
phenomena. It requires three properties:1 a)
constant growth property (or the stronger semi-
linearity property); b) polynomial parsability;
c) limited cross-serial dependencies, i.e. some
limited context-sensitivity. The canonical NL
problems which exceed context free power are:
multiple agreements, reduplication, crossing de-
pendencies.2
</bodyText>
<subsubsectionHeader confidence="0.564978">
Mildly Context-sensitive Languages (MCSLs)
</subsubsectionHeader>
<bodyText confidence="0.9924355">
have been characterized by a geometric hierar-
chy of grammar levels. A level-2 MCSL (eg.
</bodyText>
<footnote confidence="0.99147275">
1See for example, (Joshi et al., 1991), (Weir, 1988).
2However other phenomena (e.g. scrambling, Geor-
gian Case and Chinese numbers) might be considered to
be beyond certain mildly context-sensitive formalisms.
</footnote>
<bodyText confidence="0.998737838709677">
TALs/LILs) is able to capture up to 4 counting
dependencies (includes L4 = {anbncndn|n ≥ 1}
but not L5 = {anbncndnen|n ≥ 1}). They were
proven to have recognition algorithms with time
complexity O(n6) (Satta, 1994). In general for
a level-k MCSL the recognition problem is in
O(n3·2k−1) and the descriptive power regard-
ing counting dependencies is 2k (Weir, 1988).
Even the descriptive power of level-2 MCSLs
(Tree Adjoining Grammars (TAGs), Linear In-
dexed Grammars (LIGs), Combinatory Catego-
rial Grammars (CCGs) might be considered in-
sufficient for some NL problems, therefore there
have been many proposals3 to extend or modify
them. On our view the possibility of modeling
coordination phenomena is probably the most
crucial in this respect.
In (Casta˜no, 2003) we introduced Global In-
dex Grammars (GIGs) - and GILs the corre-
sponding languages - as an alternative grammar
formalism that has a restricted context sensitive
power. We showed that GIGs have enough de-
scriptive power to capture the three phenomena
mentioned above (reduplication, multiple agree-
ments, crossed agreements) in their generalized
forms. Recognition of the language generated by
a GIG is in bounded polynomial time: O(n6).
We presented a Chomsky-Sch¨utzenberger repre-
sentation theorem for GILs. In (Casta˜no, 2003c)
we presented the equivalent automaton model:
LR-2PDA and provided a characterization the-
</bodyText>
<footnote confidence="0.926216666666667">
3There are extensions or modifications of TAGs,
CCGs, IGs, and many other proposals that would be
impossible to mention here.
</footnote>
<bodyText confidence="0.999758514285714">
orems of GILs in terms of the LR-2PDA and
GIGs. The family of GILs is an Abstract Fam-
ily of Language.
The goal of this paper is to show the relevance
of GIGs for NL modeling and processing. This
should not be understood as claim to propose
GIGs as a grammar model with “linguistic con-
tent” that competes with grammar models such
as HPSG or LFG. It should be rather seen as
a formal language resource which can be used
to model and process NL phenomena beyond
context free, or beyond the level-2 MCSLs (like
those mentioned above) or to compile grammars
created in other framework into GIGs. LIGs
played a similar role to model the treatment of
the SLASH feature in GPSGs and HPSGs, and
to compile TAGs for parsing. GIGs offer addi-
tional descriptive power as compared to LIGs
or TAGs regarding the canonical NL problems
mentioned above, and the same computational
cost in terms of asymptotic complexity. They
also offer additional descriptive power in terms
of the structural descriptions they can generate
for the same set of string languages, being able
to produce dependent paths.4
This paper is organized as follows: section 2
reviews Global Index Grammars and their prop-
erties and we give examples of its weak descrip-
tive power. Section 3 discusses the relevance
of the strong descriptive power of GIGs. We
discuss the structural description for the palin-
drome, copy and the multiple copies languages
{ww+|w E E*}. Finally in section 4 we discuss
how this descriptive power can be used to en-
code HPSGs schemata.
</bodyText>
<sectionHeader confidence="0.991712" genericHeader="method">
2 Global Index Grammars
</sectionHeader>
<subsectionHeader confidence="0.85758">
2.1 Linear Indexed Grammars
</subsectionHeader>
<bodyText confidence="0.998104142857143">
Indexed grammars, (IGs) (Aho, 1968), and
Linear Index Grammars, (LIGs;LILs) (Gazdar,
1988), have the capability to associate stacks of
indices with symbols in the grammar rules. IGs
are not semilinear. LIGs are Indexed Grammars
with an additional constraint in the form of the
productions: the stack of indices can be “trans-
</bodyText>
<footnote confidence="0.995181">
4For the notion of dependent paths see for instance
(Vijay-Shanker et al., 1987) or (Joshi, 2000).
</footnote>
<bodyText confidence="0.9762505">
mitted” only to one non-terminal. As a con-
sequence they are semilinear and belong to the
class of MCSGs. The class of LILs contains L4
but not L5 (see above).
A Linear Indexed Grammar is a 5-tuple
(V, T, I, P, S), where V is the set of variables,
T the set of terminals, I the set of indices, S
in V is the start symbol, and P is a finite set
of productions of the form, where A, B E V ,
α,-y E (V U T)*, i E I:
</bodyText>
<listItem confidence="0.4257905">
a. A[..] —* α B[..] -y b. A[i..] —* α B[..] -y
c. A[..] —* αB[i..] -y
</listItem>
<equation confidence="0.9957156">
Example 1 L(Gwcw) = {wcw |w ∈ {a, b}∗},
Gww = ({S, R}, {a, b}, {i, j}, S, P) and P is:
1.S[..] → aS[i..] 2.S[..] → bS[j..]
3.S[..] → cR[..] 4.R[i..] → R[..]a
5.R[j..] → R[..]b 6. R[] → 2
</equation>
<subsectionHeader confidence="0.989497">
2.2 Global Indexed Grammars
</subsectionHeader>
<bodyText confidence="0.999953714285714">
GIGs use the stack of indices as a global con-
trol structure. This formalism provides a global
but restricted context that can be updated at
any local point in the derivation. GIGs are a
kind of regulated rewriting mechanisms (Dassow
and P˘aun, 1989) with global context and his-
tory of the derivation (or ordered derivation) as
the main characteristics of its regulating device.
The introduction of indices in the derivation is
restricted to rules that have terminals in the
right-hand side. An additional constraint that
is imposed on GIGs is strict leftmost derivation
whenever indices are introduced or removed by
the derivation.
</bodyText>
<construct confidence="0.997871571428571">
Definition 1 A GIG is a 6-tuple G =
(N, T, I, S, #, P) where N, T, I are finite pair-
wise disjoint sets and 1) N are non-terminals
2) T are terminals 3) I a set of stack indices 4)
S E N is the start symbol 5) # is the start stack
symbol (not in I,N,T) and 6) P is a finite set of
productions, having the following form,5 where
</construct>
<footnote confidence="0.99725875">
5The notation in the rules makes explicit that oper-
ation on the stack is associated to the production and
neither to terminals nor to non-terminals. It also makes
explicit that the operations are associated to the com-
putation of a Dyck language (using such notation as
used in e.g. (Harrison, 1978)). In another notation: a.1
[y..]A → [y..]α, a.2 [y..]A → [y..]α, b. [..]A → [x..]a β
and c. [x..]A → [..]α
</footnote>
<equation confidence="0.986363375">
x ∈ I, y ∈ {I ∪#}, A ∈ N, α,Q ∈ (N ∪T)* and
a ∈ T.
a.i A → α (epsilon)
2
a.ii A →
[y]
a Q (push)
α a Q (pop)
</equation>
<bodyText confidence="0.998640333333333">
Note the difference between push (type b) and
pop rules (type c): push rules require the right-
hand side of the rule to contain a terminal in the
first position. Pop rules do not require a termi-
nal at all. That constraint on push rules is a
crucial property of GIGs. Derivations in a GIG
are similar to those in a CFG except that it is
possible to modify a string of indices. We de-
fine the derives relation ⇒ on sentential forms,
which are strings in I*#(N ∪T)* as follows. Let
Q and -y be in (N ∪ T)*, δ be in I*, x in I, w be
in T* and Xi in (N ∪ T).
</bodyText>
<listItem confidence="0.969199">
1. If A → X1 ...Xn is a production of type (a.)
</listItem>
<equation confidence="0.96878875">
µ
(i.e. µ = 2 or µ = [x], x ∈ I) then:
i. δ#QA-y ⇒ δ#QX1...Xn-y
µ
</equation>
<bodyText confidence="0.999436">
there is only one stack affected at each deriva-
tion step, with the consequence of the semilin-
earity property of LILs. GIGs share this unique-
ness of the stack with LIGs: there is only one
stack to be considered. Unlike LIGs and IGs the
stack of indices is independent of non-terminals
in the GIG case. GIGs can have rules where the
right-hand side of the rule is composed only of
terminals and affect the stack of indices. Indeed
push rules (type b) are constrained to start the
right-hand side with a terminal as specified in
(6.b) in the GIG definition. The derives def-
inition requires a leftmost derivation for those
rules ( push and pop rules) that affect the stack
of indices. The constraint imposed on the push
productions can be seen as constraining the con-
text sensitive dependencies to the introduction
of lexical information. This constraint prevents
GIGs from being equivalent to a Turing Machine
as is shown in (Casta˜no, 2003c).
</bodyText>
<subsectionHeader confidence="0.589275">
2.2.1 Examples
</subsectionHeader>
<bodyText confidence="0.99841625">
The following example shows that GILs con-
tain a language not contained in LILs, nor in the
family of MCSLs. This language is relevant for
modeling coordination in NL.
</bodyText>
<equation confidence="0.9302515">
Example 2 (Multiple Copies) .
L(Gwwn) = {ww+  |w ∈ {a, b}*}
</equation>
<bodyText confidence="0.782395">
Gwwn = ({S, R, A, B, C, L}, {a, b}, {i, j}, S, #, P)
and where Pis: S → AS  |BS  |C C → RC  |L
</bodyText>
<figure confidence="0.973632333333333">
α (epsilon with constraints)
b. A →
x
c. A →
x¯
R → RA R → RB R → 2
¯i j¯ [#]
b L →
¯i
La  |a L →
j¯
Lb  |b
A → a B →
i j
The derivation of ababab:
ii. xδ#QA-y ⇒ xδ#QX1...Xn-y
µ
2. If A → aX1...Xn is a production of type
µ
(b.) or push: µ = x, x ∈ I, then:
δ#wA-y ⇒ xδ#waX1...Xn-y
µ
3. If A → X1 ...Xn is a production of type (c.)
µ
</figure>
<equation confidence="0.92599175">
or pop: µ =
¯x, x ∈ I, then:
xδ#wA-y ⇒ δ#wX1 Xn-y
µ
</equation>
<bodyText confidence="0.99976825">
The reflexive and transitive closure of ⇒ is
denoted, as usual by *⇒. We define the language
of a GIG, G, L(G) to be: {w|#S ⇒* #w and w
is in T*}
The main difference between, IGs, LIGs and
GIGs, corresponds to the interpretation of the
derives relation relative to the behavior of the
stack of indices. In IGs the stacks of indices are
distributed over the non-terminals of the right-
hand side of the rule. In LIGs, indices are asso-
ciated with only one non-terminal at right-hand
side of the rule. This produces the effect that
</bodyText>
<equation confidence="0.9975445">
#S ⇒ #AS ⇒ i#aS ⇒ i#aBS ⇒ ji#abS ⇒
ji#abC ⇒ ji#abRC ⇒ i#abRBC ⇒ #abRABC ⇒
#abABC ⇒ i#abaBC ⇒ ji#ababC ⇒ ji#ababL ⇒
i#ababLb ⇒ #ababab
</equation>
<bodyText confidence="0.808464111111111">
The next example shows the MIX (or Bach)
language. (Gazdar, 1988) conjectured the MIX
language is not an IL. GILs are semilinear,
(Casta˜no, 2003c) therefore ILs and GILs could
be incomparable under set inclusion.
Example 3 (MIX language) .L(Gmix) =
{w|w ∈ {a, b, c}* and |a|w = |b|w = |c|w ≥ 1}
G.i. = ({S, D, F, L}, {a, b, c}, {i, j, k, l, m, n}, S, #, P)
where P is:
</bodyText>
<equation confidence="0.996745727272727">
S → FS  |DS  |LS  |2 F →c F →
i j
D →
¯i aSb  |bSa D →
j¯
b F → a
k
aSc  |cSa D → bSc  |cSb
k¯
L → c L → b L → a
l¯ m¯ n¯
</equation>
<bodyText confidence="0.999367666666667">
The following example shows that the family
of GILs contains languages which do not belong
to the MCSL family.
</bodyText>
<equation confidence="0.989334">
Example 4 (Multiple dependencies)
L(Ggdp) = { a&apos;(b&apos;c&apos;)+  |n ≥ 1},
Ggdp = ({S, A, R, E, O, L}, {a, b, c}, {i}, S, #, P)
and P is:
S → AR A → aAE A → a E →
i
b L L → OR  |C C →c C  |c
¯i
c OE  |c
</equation>
<bodyText confidence="0.9489425">
The derivation of the string aabbccbbcc shows
five dependencies.
</bodyText>
<equation confidence="0.99969775">
#S ⇒ #AR ⇒ #aAER ⇒ #aaER ⇒ i#aabR ⇒
ii#aabbL ⇒ ii#aabbOR ⇒ i#aabbcOER ⇒
#aabbccER ⇒ i#aabbccbR ⇒ ii#aabbccbbL ⇒
ii#aabbccbbC ⇒ i#aabbccbbcC ⇒ #aabbccbbcc
</equation>
<subsectionHeader confidence="0.921673">
2.3 GILs Recognition
</subsectionHeader>
<bodyText confidence="0.991635277777778">
The recognition algorithm for GILs we presented
in (Casta˜no, 2003) is an extension of Earley’s al-
gorithm (cf. (Earley, 1970)) for CFLs. It has to
be modified to perform the computations of the
stack of indices in a GIG. In (Casta˜no, 2003) a
graph-structured stack (Tomita, 1987) was used
to efficiently represent ambiguous index opera-
tions in a GIG stack. Earley items are modified
adding three parameters δ, c, o:
[δ, c, o, A → α•AQ, i, j]
The first two represent a pointer to an active
node in the graph-structured stack ( δ ∈ I and
c ≤ n). The third parameter (o ≤ n) is used
to record the ordering of the rules affecting the
stack.
The O(n6) time-complexity of this algorithm
reported in (Casta˜no, 2003) can be easily ver-
ified. The complete operation is typically the
costly one in an Earley type algorithm. It can
be verified that there are at most n6 instances of
the indices (c1, c2, o, i, k, j) involved in this oper-
ation. The counter parameters c1 and c2, might
be state bound, even for grammars with ambigu-
ous indexing. In such cases the time complex-
ity would be determined by the CFG backbone
properties. The computation of the operations
on the graph-structured stack of indices are per-
formed at a constant time where the constant is
determined by the size of the index vocabulary.
O(n6) is the worst case; O(n3) holds for gram-
mars with state-bound indexing (which includes
unambiguous indexing)6; O(n2) holds for unam-
biguous context free back-bone grammars with
state-bound indexing and O(n) for bounded-
state7 context free back-bone grammars with
state-bound indexing.
</bodyText>
<sectionHeader confidence="0.98268" genericHeader="method">
3 GIGs and structural description
</sectionHeader>
<bodyText confidence="0.999723090909091">
(Gazdar, 1988) introduces Linear Indexed
Grammars and discusses its applicability to Nat-
ural Language problems. This discussion is ad-
dressed not in terms of weak generative capac-
ity but in terms of strong-generative capacity.
Similar approaches are also presented in (Vijay-
Shanker et al., 1987) and (Joshi, 2000) (see
(Miller, 1999) concerning weak and strong gen-
erative capacity). In this section we review some
of the abstract configurations that are argued for
in (Gazdar, 1988).
</bodyText>
<subsectionHeader confidence="0.992267">
3.1 The palindrome language
</subsectionHeader>
<bodyText confidence="0.9976042">
CFGs can recognize the language {wwR|w ∈
Σ∗} but they cannot generate the structural de-
scription depicted in figure 1 (we follow Gazdar’s
notation: the leftmost element within the brack-
ets corresponds to the top of the stack):
</bodyText>
<figureCaption confidence="0.940486">
Figure 1: A non context-free structural descrip-
</figureCaption>
<bodyText confidence="0.721030666666667">
tion for the language wwR (Gazdar, 1988)
Gazdar suggests that such configuration
would be necessary to represent Scandinavian
</bodyText>
<footnote confidence="0.9985234">
6Unambiguous indexing should be understood as
those grammars that produce for each string in the lan-
guage a unique indexing derivation.
7Context Free grammars where the set of items in each
state set is bounded by a constant.
</footnote>
<figure confidence="0.938345137931035">
aSb  |bSa D →
m
aSc  |cSa D →
n
D →
l
bSc  |cSb
b
[..]
[a]
[b,a]
b
c
[c,b,a]
d
d
c
a
[c,b,a]
[b,a]
[a]
[d,c,b,a]
a
[..]
b
R →
i
O →
¯i
</figure>
<bodyText confidence="0.9904746">
unbounded dependencies.Such an structure can
be obtained using a GIG (and of course a LIG).
But the mirror image of that structure can-
not be generated by a GIG because it would
require to allow push productions with a non
terminal in the first position of the right-hand
side. However the English adjective construc-
tions that Gazdar argues that can motivate the
LIG derivation, can be obtained with the follow-
ing GIG productions as shown in figure 2.
</bodyText>
<figure confidence="0.644553">
Example 5 (Comparative Construction) .
</figure>
<figureCaption confidence="0.9216885">
Figure 2: A GIG structural description for the
language wwR
</figureCaption>
<bodyText confidence="0.999864333333333">
It should be noted that the operations on indices
follow the reverse order as in the LIG case. On
the other hand, it can be noticed also that the
introduction of indices is dependent on the pres-
ence of lexical information and its transmission
is not carried through a top-down spine, as in
the LIG or TAG cases. The arrows show the
leftmost derivation order that is required by the
operations on the stack.
</bodyText>
<subsectionHeader confidence="0.996904">
3.2 The Copy Language
</subsectionHeader>
<bodyText confidence="0.998937444444444">
Gazdar presents two possible LIG structural de-
scriptions for the copy language. Similar struc-
tural descriptions can be obtained using GIGs.
However he argues that another tree structure
could be more appropriate for some Natural
Language phenomenon that might be modeled
with a copy language. Such structure cannot
be generated by a LIG, and can by an IG (see
(Casta˜no, 2003b) for a complete discussion and
comparasion of GIG and LIG generated trees).
GIGs cannot produce this structural descrip-
tion, but they can generate the one presented in
figure 3, where the arrows depict the leftmost
derivation order. GIGs can also produce similar
structural descriptions for the language of mul-
tiple copies (the language {ww+ |w E Σ∗} as
shown in figure 4, corresponding to the gram-
mar shown in example 2.
</bodyText>
<figureCaption confidence="0.99588">
Figure 3: A GIG structural description for the
copy language
Figure 4: A GIG structural description for the
multiple copy language
</figureCaption>
<sectionHeader confidence="0.88647" genericHeader="method">
4 GIGs and HPSGs
</sectionHeader>
<bodyText confidence="0.999926142857143">
We showed in the last section how GIGs can
produce structural descriptions similar to those
of LIGs, and others which are beyond LIGs and
TAGs descriptive power. Those structural de-
scriptions corresponding to figure 1 were corre-
lated to the use of the SLASH feature in GPSGs
and HPSGs. In this section we will show how
</bodyText>
<figure confidence="0.999807781609196">
AP → AP NP AP → A¯ A¯→ A¯A
A → a A → b A → c
d 7 k
NP → a NP NP → b NP NP → c NP
¯d 7¯ k¯
[..] AP
a [b,c] NP
[..]
A
A A
A
[a,b,c]
a
b
A
c [c]
[b,c]
A
AP
AP
NP
AP
b [c] NP
NP
[..]
c NP
NP
[b,a,b,a] [b,a,b,a]
d
b
[a,b,a]
[a,b,a]
[a]
a
b
a
[b,a]
[b,a]
c
[a]
b
[ ]
a
[ ]
[ ]
[a]
[b,a]
a
b
a
b
[b,a,b,a]
[a,b,a] [b,a,b,a]
[b,a]
[c,b,a]
[b,a,b,a]
[a,b,a] [b,a,b,a] [b,a,b,a]
[a]
[b,a]
b
[a,b,a]
[a,b,a]
[b,a]
a
a
[ ]
ε
b
a
[a]
a
[b,a]
[a]
[a]
[b,a]
b
b
[ ]
ε
[a]
b
[a,b,a]
[b,a,b,a]
a
[ ]
b
a
</figure>
<bodyText confidence="0.9572214">
the structural description power of GIGs, is not
only able to capture those phenomena but also
additional structural descriptions, compatible
with those generated by HPSGs. This follows
from the ability of GIGs to capture dependen-
cies through different paths in the derivation.
There has been some work compiling HPSGs
into TAGs (cf. (Kasper et al., 1995), (Becker
and Lopez, 2000)). One of the motivations
was the potential to improve the processing
efficiency of HPSG, performing HPSG deriva-
tions at compile time. Such compilation process
allowed to identify significant parts of HPSG
grammars that were mildly context-sensitive.
We will introduce informally some slight mod-
ifications to the operations on the stacks per-
formed by a GIG. We will allow the productions
of a GIG to be annotated with finite strings
in I U I instead of single symbols. This does
not change the power of the formalism. It is a
standard change in PDAs (cf. (Harrison, 1978))
to allow to push/pop several symbols from the
stack. Also the symbols will be interpreted rel-
ative to the elements in the top of the stack
(as a Dyck set). Therefore different derivations
might be produced using the same production
according to what are the topmost elements of
the stack. This is exemplified with the produc-
x, in particular in the
first three cases where different actions are taken
(the actions are explained in the parenthesis) :
nnδ#wXβ ==&gt;.
¯nv vnδ#wxβ (pop n and push v)
n¯vδ#wXβ ==&gt;. δ#wxβ (pop n and ¯v)
¯nv v¯nvnδ#wxβ (push n¯ and v)
vnδ#wXβ ==&gt;.
¯nv
nδ#wXβ ==&gt;. vnδ#wxβ ( check and push)
[n]v
We exemplify how GIGs can generate similar
structural descriptions as HPSGs do, in a very
oversimplified and abstract way. We will ignore
many details and try give an rough idea on how
the transmission of features can be carried out
from the lexical items by the GIG stack, obtain-
ing very similar structural descriptions.
Head-Subj-Schema
Figure 5 depicts the tree structure corre-
sponding to the Head-Subject Schema in HPSG
(Pollard and Sag, 1994).
</bodyText>
<figureCaption confidence="0.993766">
Figure 5: Head-Subject Schema
</figureCaption>
<bodyText confidence="0.996536">
Figure 6 shows an equivalent structural de-
scription corresponding to the GIG produc-
tions and derivation shown in the next exam-
ple (which might correspond to an intransitive
verb). The arrows indicate how the transmis-
sion of features is encoded in the leftmost deriva-
tion order, an how the elements contained in the
stack can be correlated to constituents or lexical
items (terminal symbols) in a constituent recog-
nition process.
</bodyText>
<figureCaption confidence="0.995582">
Figure 6: Head-Subject in GIG format
</figureCaption>
<figure confidence="0.543756">
Example 6 (Intransitive verb) XP → Y P XP
XP → X Y P → Y X → x Y → y
¯nv n
#XP ⇒ #YPXP ⇒ #yXP ⇒ n#YXP ⇒
n#yX ⇒ v#yx
</figure>
<figureCaption confidence="0.667272">
Head-Comps-Schema Figure 7 shows the
tree structure corresponding to the Head-
Complement schema in HPSG.
Figure 7: Head-Comps Schema tree representa-
tion
</figureCaption>
<bodyText confidence="0.9984925">
The following GIG productions generate the
structural description corresponding to figure 8,
where the initial configuration of the stack is
assumed to be [n]:
</bodyText>
<equation confidence="0.94806">
Example 7 (transitive verb) .
XP → X CP CP → Y CP X →
¯nv¯
n
Y →
n y
</equation>
<figure confidence="0.994489661971831">
HEAD 1
SUBJ
&lt; &gt;
H
SUBJ
2 HEAD
1
SUBJ
&lt; &gt;
2
[..]
XP
[v..]
YP
XP
[v..]
[n..]
Y
X
[n..]
y
[v..]
x
HEAD
1
COMP
&lt; &gt;
2
C C
1
H
n-2
COMP
HEAD
&lt; 2 ,3&gt;
n
1
n
3
tions X —* x and X —*
¯nv [n]v
x CP → 2
[ ] XP
[n]
YP
[nn]
we
YP [n]
Kim
know
YP
[ n v n ]
Sandy
X
[ v n ]
claims
XP
XP
XP
YP
[ n v n ]
[ v n ]
Dana X
[ ]
XP
XP
X
XP
hates
CP
ε
</figure>
<figureCaption confidence="0.995546">
Figure 9: SLASH in GIG format
</figureCaption>
<figure confidence="0.886764666666667">
The derivation:
n#XP ⇒ n#XCP ⇒ ¯nv#xCP ⇒ ¯nv#xY CP ⇒
v#xyCP ⇒ v#xy
</figure>
<figureCaption confidence="0.999771">
Figure 8: Head-Comp in GIG format
</figureCaption>
<bodyText confidence="0.999843">
The productions of example 8 (which use
some of the previous examples) generate the
structural description represented in figure 9,
corresponding to the derivation given in exam-
ple 8. We show the contents of the stack when
each lexical item is introduced in the derivation.
</bodyText>
<figure confidence="0.97200144">
Example 8 (SLASH in GIG format) .
CP
[ v ]
X
[n v]
[n]
XP
[ v ]
y
ε
[n v]
Y
x CP
[ v ]
XP → Y P XP XP → X CP XP → X XP
CP → YP CP X → hates CP → 2
n,vn,
X → know X → claims
ft n,vv
Y P →
n Kim|Sandy|Dana|we
A derivation of ‘Kim we know Sandy claims Dana
hates’:
#XP ⇒ #Y P XP ⇒ n#Kim XP ⇒
n#Kim Y P XP ⇒ nn#Kim we XP ⇒
</figure>
<bodyText confidence="0.919003818181818">
nn#Kim we X XP ⇒ ¯vn#Kim we know XP ⇒
¯vn#Kim we know Y P XP ⇒
n¯vn#Kim we know Sandy XP ⇒
n¯vn#Kim we know Sandy X XP ⇒
¯vn#Kim we know Sandy claims XP ⇒
¯vn#Kim we know Sandy claims Y P XP ⇒
n¯vn#Kim we know Sandy claims Dana XP ∗ ⇒
#Kim we know Sandy claims Dana hates
Finally the last example and figure 10 show
how coordination can be encoded.
Example 9 (SLASH and Coordination)
</bodyText>
<equation confidence="0.452081625">
XP → Y P XP XP → X CP XP → X XP
CP → YP CP CP → 2 X → visit
[nvn]c
X → talk to C → and CXP → XP CXP
�nv�
n
CXP → C XP X →
ft
</equation>
<sectionHeader confidence="0.989886" genericHeader="evaluation">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99999208">
We presented GIGs and GILs and showed the
descriptive power of GIGs is beyond CFGs.
CFLs are properly included in GILs by def-
inition. We showed also that GIGs include
some languages that are not in the LIL/TAL
family. GILs do include those languages that
are beyond context free and might be required
for NL modelling. The similarity between GIGs
and LIGs, suggests that LILs might be included
in GILs. We presented a succinct comparison
of the structural descriptions that can be gen-
erated both by LIGs and GIGs, we have shown
that GIGs generate structural descriptions for
the copy language which can not be generated
by LIGs. We showed also that this is the
case for other languages that can be generated
by both LIGs and GIGs. This corresponds
to the ability of GIGs to generate dependent
paths without copying the stack. We have
shown also that those non-local relationships
that are usually encoded in HPSGs as feature
transmission, can be encoded in GIGs using its
stack, exploiting the ability of Global stacks to
encode dependencies through dependent paths
and not only through a spine.
</bodyText>
<sectionHeader confidence="0.996498" genericHeader="conclusions">
Acknowledgments:
</sectionHeader>
<bodyText confidence="0.986552">
Thanks to J. Pustejovsky for his continuous support and
encouragement on this project. Many thanks also to the
anonymous reviewers who provided many helpful com-
ments. This work was partially supported by NLM Grant
</bodyText>
<figure confidence="0.968423333333333">
did Y P → W ho|you
n
ε
</figure>
<figureCaption confidence="0.986405">
Figure 10: SLASH in GIG format
</figureCaption>
<bodyText confidence="0.366284">
R01 LM06649-02.
</bodyText>
<sectionHeader confidence="0.98424" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999953253521127">
A. V. Aho. 1968. Indexed grammars - an extension
of context-free grammars. Journal of the Associ-
ation for Computing Machinery, 15(4):647–671.
T. Becker and P. Lopez. 2000. Adapting hpsg-to-tag
compilation to wide-coverage grammars.
J. Casta˜no. 2003. GIGs: Restricted context-
sensitive descriptive power in bounded
polynomial-time. In Proc. of Cicling 2003,
Mexico City, February 16-22.
J. Casta˜no. 2003b. Global index grammars and de-
scriptive power. In R. Oehrle and J. Rogers, edi-
tors, Proc. of Mathematics of Language, MOL 8.
Bloomington, Indiana, June.
J. Casta˜no. 2003c. LR Parsing for Global Index Lan-
guages (GILs). In In Proceeding of CIAA 2003,
Santa Barbara,CA.
J. Dassow and G. P˘aun. 1989. Regulated Rewrit-
ing in Formal Language Theory. Springer, Berlin,
Heidelberg, New York.
J. Earley. 1970. An Efficient Context-free Parsing
Algorithm. Communications of the ACM, 13:94–
102.
G. Gazdar. 1988. Applicability of indexed grammars
to natural languages. In U. Reyle and C. Rohrer,
editors, Natural Language Parsing and Linguistic
Theories, pages 69–94. D. Reidel, Dordrecht.
M. H. Harrison. 1978. Introduction to Formal Lan-
guage Theory. Addison-Wesley Publishing Com-
pany, Inc., Reading, MA.
A. Joshi, K. Vijay-Shanker, and D. Weir. 1991. The
convergence of mildly context-sensitive grammat-
ical formalisms. In Peter Sells, Stuart Shieber,
and Thomas Wasow, editors, Foundational issues
in natural language processing, pages 31–81. MIT
Press, Cambridge, MA.
A. Joshi. 1985. Tree adjoining grammars: How much
context-sensitivity is required to provide reason-
able structural description? In D. Dowty, L. Kart-
tunen, and A. Zwicky, editors, Natural language
processing: psycholinguistic, computational and
theoretical perspectives, pages 206–250. Chicago
University Press, New York.
A. Joshi. 2000. Relationship between strong and
weak generative power of formal systems. In
Proceedings of the Fifth International Workshop
on Tree Adjoining Grammars and Related For-
malisms (TAG+5), pages 107–114, Paris, France.
R. Kasper, B. Kiefer, K. Netter, and K. Vijay-
Shanker. 1995. Compilation of HPSG into TAG.
In Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics, pages
92–99. Cambridge, Mass.
P. Miller. 1999. Strong Generative Capacity. CSLI
Publications, Stanford University, Stanford CA,
USA.
C. Pollard and I. A. Sag. 1994. Head-driven Phrase
Structure Grammar. University of Chicago Press,
Chicago, IL.
G. Satta. 1994. Tree-adjoining grammar parsing and
boolean matrix multiplication. Computational lin-
guistics, 20, No. 2.
M. Tomita. 1987. An efficiente augmented-context-
free parsing algorithm. Computational linguistics,
13:31–46.
K. Vijay-Shanker, D. J. Weir, and A. K. Joshi. 1987.
Characterizing structural descriptions produced
by various grammatical formalisms. In Proc. of
the 25th ACL, pages 104–111, Stanford, CA.
D. Weir. 1988. Characterizing mildly context-
sensitive grammar formalisms. Ph.D. thesis, Uni-
versity of Pennsylvania.
</reference>
<figure confidence="0.999464074074074">
YP
CXP
[ n v n ]
you
CP
talk to
[ ]
XP
YP
Who
X
[nv]
did
XP
XP
[n]
XP
[ c n v n ]
visit
C
[ n v n]
and
CXP
ε
[ n v n ]
XP
[ ]
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.605325">
<title confidence="0.999863">On the Applicability of Global Index Grammars</title>
<author confidence="0.725104">M</author>
<affiliation confidence="0.9997135">Computer Science Department Brandeis University</affiliation>
<email confidence="0.999333">jcastano@cs.brandeis.edu</email>
<abstract confidence="0.985672">We investigate Global Index Grammars (GIGs), a grammar formalism that uses a stack of indices associated with productions and has restricted context-sensitive power. We discuss some of the structural descriptions that GIGs can generate compared with those generated by LIGs. We show also how GIGs can represent structural descriptions corresponding to HPSGs (Pollard and Sag, 1994) schemas.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
</authors>
<title>Indexed grammars - an extension of context-free grammars.</title>
<date>1968</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="4525" citStr="Aho, 1968" startWordPosition="701" endWordPosition="702">can generate for the same set of string languages, being able to produce dependent paths.4 This paper is organized as follows: section 2 reviews Global Index Grammars and their properties and we give examples of its weak descriptive power. Section 3 discusses the relevance of the strong descriptive power of GIGs. We discuss the structural description for the palindrome, copy and the multiple copies languages {ww+|w E E*}. Finally in section 4 we discuss how this descriptive power can be used to encode HPSGs schemata. 2 Global Index Grammars 2.1 Linear Indexed Grammars Indexed grammars, (IGs) (Aho, 1968), and Linear Index Grammars, (LIGs;LILs) (Gazdar, 1988), have the capability to associate stacks of indices with symbols in the grammar rules. IGs are not semilinear. LIGs are Indexed Grammars with an additional constraint in the form of the productions: the stack of indices can be “trans4For the notion of dependent paths see for instance (Vijay-Shanker et al., 1987) or (Joshi, 2000). mitted” only to one non-terminal. As a consequence they are semilinear and belong to the class of MCSGs. The class of LILs contains L4 but not L5 (see above). A Linear Indexed Grammar is a 5-tuple (V, T, I, P, S)</context>
</contexts>
<marker>Aho, 1968</marker>
<rawString>A. V. Aho. 1968. Indexed grammars - an extension of context-free grammars. Journal of the Association for Computing Machinery, 15(4):647–671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Becker</author>
<author>P Lopez</author>
</authors>
<title>Adapting hpsg-to-tag compilation to wide-coverage grammars.</title>
<date>2000</date>
<contexts>
<context position="17096" citStr="Becker and Lopez, 2000" startWordPosition="3113" endWordPosition="3116">,a] c [a] b [ ] a [ ] [ ] [a] [b,a] a b a b [b,a,b,a] [a,b,a] [b,a,b,a] [b,a] [c,b,a] [b,a,b,a] [a,b,a] [b,a,b,a] [b,a,b,a] [a] [b,a] b [a,b,a] [a,b,a] [b,a] a a [ ] ε b a [a] a [b,a] [a] [a] [b,a] b b [ ] ε [a] b [a,b,a] [b,a,b,a] a [ ] b a the structural description power of GIGs, is not only able to capture those phenomena but also additional structural descriptions, compatible with those generated by HPSGs. This follows from the ability of GIGs to capture dependencies through different paths in the derivation. There has been some work compiling HPSGs into TAGs (cf. (Kasper et al., 1995), (Becker and Lopez, 2000)). One of the motivations was the potential to improve the processing efficiency of HPSG, performing HPSG derivations at compile time. Such compilation process allowed to identify significant parts of HPSG grammars that were mildly context-sensitive. We will introduce informally some slight modifications to the operations on the stacks performed by a GIG. We will allow the productions of a GIG to be annotated with finite strings in I U I instead of single symbols. This does not change the power of the formalism. It is a standard change in PDAs (cf. (Harrison, 1978)) to allow to push/pop severa</context>
</contexts>
<marker>Becker, Lopez, 2000</marker>
<rawString>T. Becker and P. Lopez. 2000. Adapting hpsg-to-tag compilation to wide-coverage grammars.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Casta˜no</author>
</authors>
<title>GIGs: Restricted contextsensitive descriptive power in bounded polynomial-time.</title>
<date>2003</date>
<booktitle>In Proc. of Cicling 2003,</booktitle>
<pages>16--22</pages>
<location>Mexico City,</location>
<marker>Casta˜no, 2003</marker>
<rawString>J. Casta˜no. 2003. GIGs: Restricted contextsensitive descriptive power in bounded polynomial-time. In Proc. of Cicling 2003, Mexico City, February 16-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Casta˜no</author>
</authors>
<title>Global index grammars and descriptive power.</title>
<date>2003</date>
<booktitle>Proc. of Mathematics of Language, MOL 8.</booktitle>
<editor>In R. Oehrle and J. Rogers, editors,</editor>
<location>Bloomington, Indiana,</location>
<marker>Casta˜no, 2003</marker>
<rawString>J. Casta˜no. 2003b. Global index grammars and descriptive power. In R. Oehrle and J. Rogers, editors, Proc. of Mathematics of Language, MOL 8. Bloomington, Indiana, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Casta˜no</author>
</authors>
<title>LR Parsing for Global Index Languages (GILs).</title>
<date>2003</date>
<booktitle>In In Proceeding of CIAA 2003,</booktitle>
<location>Santa Barbara,CA.</location>
<marker>Casta˜no, 2003</marker>
<rawString>J. Casta˜no. 2003c. LR Parsing for Global Index Languages (GILs). In In Proceeding of CIAA 2003, Santa Barbara,CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dassow</author>
<author>G P˘aun</author>
</authors>
<title>Regulated Rewriting in Formal Language Theory.</title>
<date>1989</date>
<publisher>Springer,</publisher>
<location>Berlin, Heidelberg, New York.</location>
<marker>Dassow, P˘aun, 1989</marker>
<rawString>J. Dassow and G. P˘aun. 1989. Regulated Rewriting in Formal Language Theory. Springer, Berlin, Heidelberg, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An Efficient Context-free Parsing Algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<pages>102</pages>
<contexts>
<context position="11220" citStr="Earley, 1970" startWordPosition="2057" endWordPosition="2058">ontains languages which do not belong to the MCSL family. Example 4 (Multiple dependencies) L(Ggdp) = { a&apos;(b&apos;c&apos;)+ |n ≥ 1}, Ggdp = ({S, A, R, E, O, L}, {a, b, c}, {i}, S, #, P) and P is: S → AR A → aAE A → a E → i b L L → OR |C C →c C |c ¯i c OE |c The derivation of the string aabbccbbcc shows five dependencies. #S ⇒ #AR ⇒ #aAER ⇒ #aaER ⇒ i#aabR ⇒ ii#aabbL ⇒ ii#aabbOR ⇒ i#aabbcOER ⇒ #aabbccER ⇒ i#aabbccbR ⇒ ii#aabbccbbL ⇒ ii#aabbccbbC ⇒ i#aabbccbbcC ⇒ #aabbccbbcc 2.3 GILs Recognition The recognition algorithm for GILs we presented in (Casta˜no, 2003) is an extension of Earley’s algorithm (cf. (Earley, 1970)) for CFLs. It has to be modified to perform the computations of the stack of indices in a GIG. In (Casta˜no, 2003) a graph-structured stack (Tomita, 1987) was used to efficiently represent ambiguous index operations in a GIG stack. Earley items are modified adding three parameters δ, c, o: [δ, c, o, A → α•AQ, i, j] The first two represent a pointer to an active node in the graph-structured stack ( δ ∈ I and c ≤ n). The third parameter (o ≤ n) is used to record the ordering of the rules affecting the stack. The O(n6) time-complexity of this algorithm reported in (Casta˜no, 2003) can be easily </context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>J. Earley. 1970. An Efficient Context-free Parsing Algorithm. Communications of the ACM, 13:94– 102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Applicability of indexed grammars to natural languages.</title>
<date>1988</date>
<booktitle>Natural Language Parsing and Linguistic Theories,</booktitle>
<pages>69--94</pages>
<editor>In U. Reyle and C. Rohrer, editors,</editor>
<location>Dordrecht.</location>
<contexts>
<context position="4580" citStr="Gazdar, 1988" startWordPosition="708" endWordPosition="709">eing able to produce dependent paths.4 This paper is organized as follows: section 2 reviews Global Index Grammars and their properties and we give examples of its weak descriptive power. Section 3 discusses the relevance of the strong descriptive power of GIGs. We discuss the structural description for the palindrome, copy and the multiple copies languages {ww+|w E E*}. Finally in section 4 we discuss how this descriptive power can be used to encode HPSGs schemata. 2 Global Index Grammars 2.1 Linear Indexed Grammars Indexed grammars, (IGs) (Aho, 1968), and Linear Index Grammars, (LIGs;LILs) (Gazdar, 1988), have the capability to associate stacks of indices with symbols in the grammar rules. IGs are not semilinear. LIGs are Indexed Grammars with an additional constraint in the form of the productions: the stack of indices can be “trans4For the notion of dependent paths see for instance (Vijay-Shanker et al., 1987) or (Joshi, 2000). mitted” only to one non-terminal. As a consequence they are semilinear and belong to the class of MCSGs. The class of LILs contains L4 but not L5 (see above). A Linear Indexed Grammar is a 5-tuple (V, T, I, P, S), where V is the set of variables, T the set of termina</context>
<context position="10133" citStr="Gazdar, 1988" startWordPosition="1817" endWordPosition="1818"> be: {w|#S ⇒* #w and w is in T*} The main difference between, IGs, LIGs and GIGs, corresponds to the interpretation of the derives relation relative to the behavior of the stack of indices. In IGs the stacks of indices are distributed over the non-terminals of the righthand side of the rule. In LIGs, indices are associated with only one non-terminal at right-hand side of the rule. This produces the effect that #S ⇒ #AS ⇒ i#aS ⇒ i#aBS ⇒ ji#abS ⇒ ji#abC ⇒ ji#abRC ⇒ i#abRBC ⇒ #abRABC ⇒ #abABC ⇒ i#abaBC ⇒ ji#ababC ⇒ ji#ababL ⇒ i#ababLb ⇒ #ababab The next example shows the MIX (or Bach) language. (Gazdar, 1988) conjectured the MIX language is not an IL. GILs are semilinear, (Casta˜no, 2003c) therefore ILs and GILs could be incomparable under set inclusion. Example 3 (MIX language) .L(Gmix) = {w|w ∈ {a, b, c}* and |a|w = |b|w = |c|w ≥ 1} G.i. = ({S, D, F, L}, {a, b, c}, {i, j, k, l, m, n}, S, #, P) where P is: S → FS |DS |LS |2 F →c F → i j D → ¯i aSb |bSa D → j¯ b F → a k aSc |cSa D → bSc |cSb k¯ L → c L → b L → a l¯ m¯ n¯ The following example shows that the family of GILs contains languages which do not belong to the MCSL family. Example 4 (Multiple dependencies) L(Ggdp) = { a&apos;(b&apos;c&apos;)+ |n ≥ 1}, Ggd</context>
<context position="12728" citStr="Gazdar, 1988" startWordPosition="2318" endWordPosition="2319">us indexing. In such cases the time complexity would be determined by the CFG backbone properties. The computation of the operations on the graph-structured stack of indices are performed at a constant time where the constant is determined by the size of the index vocabulary. O(n6) is the worst case; O(n3) holds for grammars with state-bound indexing (which includes unambiguous indexing)6; O(n2) holds for unambiguous context free back-bone grammars with state-bound indexing and O(n) for boundedstate7 context free back-bone grammars with state-bound indexing. 3 GIGs and structural description (Gazdar, 1988) introduces Linear Indexed Grammars and discusses its applicability to Natural Language problems. This discussion is addressed not in terms of weak generative capacity but in terms of strong-generative capacity. Similar approaches are also presented in (VijayShanker et al., 1987) and (Joshi, 2000) (see (Miller, 1999) concerning weak and strong generative capacity). In this section we review some of the abstract configurations that are argued for in (Gazdar, 1988). 3.1 The palindrome language CFGs can recognize the language {wwR|w ∈ Σ∗} but they cannot generate the structural description depict</context>
</contexts>
<marker>Gazdar, 1988</marker>
<rawString>G. Gazdar. 1988. Applicability of indexed grammars to natural languages. In U. Reyle and C. Rohrer, editors, Natural Language Parsing and Linguistic Theories, pages 69–94. D. Reidel, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M H Harrison</author>
</authors>
<title>Introduction to Formal Language Theory.</title>
<date>1978</date>
<publisher>Addison-Wesley Publishing Company, Inc.,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="6867" citStr="Harrison, 1978" startWordPosition="1142" endWordPosition="1143"> 1 A GIG is a 6-tuple G = (N, T, I, S, #, P) where N, T, I are finite pairwise disjoint sets and 1) N are non-terminals 2) T are terminals 3) I a set of stack indices 4) S E N is the start symbol 5) # is the start stack symbol (not in I,N,T) and 6) P is a finite set of productions, having the following form,5 where 5The notation in the rules makes explicit that operation on the stack is associated to the production and neither to terminals nor to non-terminals. It also makes explicit that the operations are associated to the computation of a Dyck language (using such notation as used in e.g. (Harrison, 1978)). In another notation: a.1 [y..]A → [y..]α, a.2 [y..]A → [y..]α, b. [..]A → [x..]a β and c. [x..]A → [..]α x ∈ I, y ∈ {I ∪#}, A ∈ N, α,Q ∈ (N ∪T)* and a ∈ T. a.i A → α (epsilon) 2 a.ii A → [y] a Q (push) α a Q (pop) Note the difference between push (type b) and pop rules (type c): push rules require the righthand side of the rule to contain a terminal in the first position. Pop rules do not require a terminal at all. That constraint on push rules is a crucial property of GIGs. Derivations in a GIG are similar to those in a CFG except that it is possible to modify a string of indices. We defin</context>
<context position="17667" citStr="Harrison, 1978" startWordPosition="3211" endWordPosition="3212">sper et al., 1995), (Becker and Lopez, 2000)). One of the motivations was the potential to improve the processing efficiency of HPSG, performing HPSG derivations at compile time. Such compilation process allowed to identify significant parts of HPSG grammars that were mildly context-sensitive. We will introduce informally some slight modifications to the operations on the stacks performed by a GIG. We will allow the productions of a GIG to be annotated with finite strings in I U I instead of single symbols. This does not change the power of the formalism. It is a standard change in PDAs (cf. (Harrison, 1978)) to allow to push/pop several symbols from the stack. Also the symbols will be interpreted relative to the elements in the top of the stack (as a Dyck set). Therefore different derivations might be produced using the same production according to what are the topmost elements of the stack. This is exemplified with the producx, in particular in the first three cases where different actions are taken (the actions are explained in the parenthesis) : nnδ#wXβ ==&gt;. ¯nv vnδ#wxβ (pop n and push v) n¯vδ#wXβ ==&gt;. δ#wxβ (pop n and ¯v) ¯nv v¯nvnδ#wxβ (push n¯ and v) vnδ#wXβ ==&gt;. ¯nv nδ#wXβ ==&gt;. vnδ#wxβ ( </context>
</contexts>
<marker>Harrison, 1978</marker>
<rawString>M. H. Harrison. 1978. Introduction to Formal Language Theory. Addison-Wesley Publishing Company, Inc., Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>K Vijay-Shanker</author>
<author>D Weir</author>
</authors>
<title>The convergence of mildly context-sensitive grammatical formalisms.</title>
<date>1991</date>
<booktitle>Foundational issues in natural language processing,</booktitle>
<pages>31--81</pages>
<editor>In Peter Sells, Stuart Shieber, and Thomas Wasow, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1244" citStr="Joshi et al., 1991" startWordPosition="169" endWordPosition="172">as a possible model to express the required properties of formalisms that might describe Natural Language (NL) phenomena. It requires three properties:1 a) constant growth property (or the stronger semilinearity property); b) polynomial parsability; c) limited cross-serial dependencies, i.e. some limited context-sensitivity. The canonical NL problems which exceed context free power are: multiple agreements, reduplication, crossing dependencies.2 Mildly Context-sensitive Languages (MCSLs) have been characterized by a geometric hierarchy of grammar levels. A level-2 MCSL (eg. 1See for example, (Joshi et al., 1991), (Weir, 1988). 2However other phenomena (e.g. scrambling, Georgian Case and Chinese numbers) might be considered to be beyond certain mildly context-sensitive formalisms. TALs/LILs) is able to capture up to 4 counting dependencies (includes L4 = {anbncndn|n ≥ 1} but not L5 = {anbncndnen|n ≥ 1}). They were proven to have recognition algorithms with time complexity O(n6) (Satta, 1994). In general for a level-k MCSL the recognition problem is in O(n3·2k−1) and the descriptive power regarding counting dependencies is 2k (Weir, 1988). Even the descriptive power of level-2 MCSLs (Tree Adjoining Gra</context>
</contexts>
<marker>Joshi, Vijay-Shanker, Weir, 1991</marker>
<rawString>A. Joshi, K. Vijay-Shanker, and D. Weir. 1991. The convergence of mildly context-sensitive grammatical formalisms. In Peter Sells, Stuart Shieber, and Thomas Wasow, editors, Foundational issues in natural language processing, pages 31–81. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
</authors>
<title>Tree adjoining grammars: How much context-sensitivity is required to provide reasonable structural description? In</title>
<date>1985</date>
<booktitle>Natural language processing: psycholinguistic, computational and theoretical perspectives,</booktitle>
<pages>206--250</pages>
<editor>D. Dowty, L. Karttunen, and A. Zwicky, editors,</editor>
<publisher>Chicago University Press,</publisher>
<location>New York.</location>
<contexts>
<context position="624" citStr="Joshi, 1985" startWordPosition="86" endWordPosition="87">cability of Global Index Grammars Jos´e M. Casta˜no Computer Science Department Brandeis University jcastano@cs.brandeis.edu Abstract We investigate Global Index Grammars (GIGs), a grammar formalism that uses a stack of indices associated with productions and has restricted context-sensitive power. We discuss some of the structural descriptions that GIGs can generate compared with those generated by LIGs. We show also how GIGs can represent structural descriptions corresponding to HPSGs (Pollard and Sag, 1994) schemas. 1 Introduction The notion of Mildly context-sensitivity was introduced in (Joshi, 1985) as a possible model to express the required properties of formalisms that might describe Natural Language (NL) phenomena. It requires three properties:1 a) constant growth property (or the stronger semilinearity property); b) polynomial parsability; c) limited cross-serial dependencies, i.e. some limited context-sensitivity. The canonical NL problems which exceed context free power are: multiple agreements, reduplication, crossing dependencies.2 Mildly Context-sensitive Languages (MCSLs) have been characterized by a geometric hierarchy of grammar levels. A level-2 MCSL (eg. 1See for example, </context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>A. Joshi. 1985. Tree adjoining grammars: How much context-sensitivity is required to provide reasonable structural description? In D. Dowty, L. Karttunen, and A. Zwicky, editors, Natural language processing: psycholinguistic, computational and theoretical perspectives, pages 206–250. Chicago University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
</authors>
<title>Relationship between strong and weak generative power of formal systems.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fifth International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+5),</booktitle>
<pages>107--114</pages>
<location>Paris, France.</location>
<contexts>
<context position="4911" citStr="Joshi, 2000" startWordPosition="763" endWordPosition="764">e multiple copies languages {ww+|w E E*}. Finally in section 4 we discuss how this descriptive power can be used to encode HPSGs schemata. 2 Global Index Grammars 2.1 Linear Indexed Grammars Indexed grammars, (IGs) (Aho, 1968), and Linear Index Grammars, (LIGs;LILs) (Gazdar, 1988), have the capability to associate stacks of indices with symbols in the grammar rules. IGs are not semilinear. LIGs are Indexed Grammars with an additional constraint in the form of the productions: the stack of indices can be “trans4For the notion of dependent paths see for instance (Vijay-Shanker et al., 1987) or (Joshi, 2000). mitted” only to one non-terminal. As a consequence they are semilinear and belong to the class of MCSGs. The class of LILs contains L4 but not L5 (see above). A Linear Indexed Grammar is a 5-tuple (V, T, I, P, S), where V is the set of variables, T the set of terminals, I the set of indices, S in V is the start symbol, and P is a finite set of productions of the form, where A, B E V , α,-y E (V U T)*, i E I: a. A[..] —* α B[..] -y b. A[i..] —* α B[..] -y c. A[..] —* αB[i..] -y Example 1 L(Gwcw) = {wcw |w ∈ {a, b}∗}, Gww = ({S, R}, {a, b}, {i, j}, S, P) and P is: 1.S[..] → aS[i..] 2.S[..] → b</context>
<context position="13026" citStr="Joshi, 2000" startWordPosition="2364" endWordPosition="2365">e; O(n3) holds for grammars with state-bound indexing (which includes unambiguous indexing)6; O(n2) holds for unambiguous context free back-bone grammars with state-bound indexing and O(n) for boundedstate7 context free back-bone grammars with state-bound indexing. 3 GIGs and structural description (Gazdar, 1988) introduces Linear Indexed Grammars and discusses its applicability to Natural Language problems. This discussion is addressed not in terms of weak generative capacity but in terms of strong-generative capacity. Similar approaches are also presented in (VijayShanker et al., 1987) and (Joshi, 2000) (see (Miller, 1999) concerning weak and strong generative capacity). In this section we review some of the abstract configurations that are argued for in (Gazdar, 1988). 3.1 The palindrome language CFGs can recognize the language {wwR|w ∈ Σ∗} but they cannot generate the structural description depicted in figure 1 (we follow Gazdar’s notation: the leftmost element within the brackets corresponds to the top of the stack): Figure 1: A non context-free structural description for the language wwR (Gazdar, 1988) Gazdar suggests that such configuration would be necessary to represent Scandinavian 6</context>
</contexts>
<marker>Joshi, 2000</marker>
<rawString>A. Joshi. 2000. Relationship between strong and weak generative power of formal systems. In Proceedings of the Fifth International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+5), pages 107–114, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kasper</author>
<author>B Kiefer</author>
<author>K Netter</author>
<author>K VijayShanker</author>
</authors>
<title>Compilation of HPSG into TAG.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>92--99</pages>
<location>Cambridge, Mass.</location>
<contexts>
<context position="17070" citStr="Kasper et al., 1995" startWordPosition="3109" endWordPosition="3112">b,a] [a] a b a [b,a] [b,a] c [a] b [ ] a [ ] [ ] [a] [b,a] a b a b [b,a,b,a] [a,b,a] [b,a,b,a] [b,a] [c,b,a] [b,a,b,a] [a,b,a] [b,a,b,a] [b,a,b,a] [a] [b,a] b [a,b,a] [a,b,a] [b,a] a a [ ] ε b a [a] a [b,a] [a] [a] [b,a] b b [ ] ε [a] b [a,b,a] [b,a,b,a] a [ ] b a the structural description power of GIGs, is not only able to capture those phenomena but also additional structural descriptions, compatible with those generated by HPSGs. This follows from the ability of GIGs to capture dependencies through different paths in the derivation. There has been some work compiling HPSGs into TAGs (cf. (Kasper et al., 1995), (Becker and Lopez, 2000)). One of the motivations was the potential to improve the processing efficiency of HPSG, performing HPSG derivations at compile time. Such compilation process allowed to identify significant parts of HPSG grammars that were mildly context-sensitive. We will introduce informally some slight modifications to the operations on the stacks performed by a GIG. We will allow the productions of a GIG to be annotated with finite strings in I U I instead of single symbols. This does not change the power of the formalism. It is a standard change in PDAs (cf. (Harrison, 1978)) t</context>
</contexts>
<marker>Kasper, Kiefer, Netter, VijayShanker, 1995</marker>
<rawString>R. Kasper, B. Kiefer, K. Netter, and K. VijayShanker. 1995. Compilation of HPSG into TAG. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 92–99. Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Miller</author>
</authors>
<title>Strong Generative Capacity.</title>
<date>1999</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford University, Stanford CA, USA.</location>
<contexts>
<context position="13046" citStr="Miller, 1999" startWordPosition="2367" endWordPosition="2368">grammars with state-bound indexing (which includes unambiguous indexing)6; O(n2) holds for unambiguous context free back-bone grammars with state-bound indexing and O(n) for boundedstate7 context free back-bone grammars with state-bound indexing. 3 GIGs and structural description (Gazdar, 1988) introduces Linear Indexed Grammars and discusses its applicability to Natural Language problems. This discussion is addressed not in terms of weak generative capacity but in terms of strong-generative capacity. Similar approaches are also presented in (VijayShanker et al., 1987) and (Joshi, 2000) (see (Miller, 1999) concerning weak and strong generative capacity). In this section we review some of the abstract configurations that are argued for in (Gazdar, 1988). 3.1 The palindrome language CFGs can recognize the language {wwR|w ∈ Σ∗} but they cannot generate the structural description depicted in figure 1 (we follow Gazdar’s notation: the leftmost element within the brackets corresponds to the top of the stack): Figure 1: A non context-free structural description for the language wwR (Gazdar, 1988) Gazdar suggests that such configuration would be necessary to represent Scandinavian 6Unambiguous indexing</context>
</contexts>
<marker>Miller, 1999</marker>
<rawString>P. Miller. 1999. Strong Generative Capacity. CSLI Publications, Stanford University, Stanford CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I A Sag</author>
</authors>
<title>Head-driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="18735" citStr="Pollard and Sag, 1994" startWordPosition="3392" endWordPosition="3395">s) : nnδ#wXβ ==&gt;. ¯nv vnδ#wxβ (pop n and push v) n¯vδ#wXβ ==&gt;. δ#wxβ (pop n and ¯v) ¯nv v¯nvnδ#wxβ (push n¯ and v) vnδ#wXβ ==&gt;. ¯nv nδ#wXβ ==&gt;. vnδ#wxβ ( check and push) [n]v We exemplify how GIGs can generate similar structural descriptions as HPSGs do, in a very oversimplified and abstract way. We will ignore many details and try give an rough idea on how the transmission of features can be carried out from the lexical items by the GIG stack, obtaining very similar structural descriptions. Head-Subj-Schema Figure 5 depicts the tree structure corresponding to the Head-Subject Schema in HPSG (Pollard and Sag, 1994). Figure 5: Head-Subject Schema Figure 6 shows an equivalent structural description corresponding to the GIG productions and derivation shown in the next example (which might correspond to an intransitive verb). The arrows indicate how the transmission of features is encoded in the leftmost derivation order, an how the elements contained in the stack can be correlated to constituents or lexical items (terminal symbols) in a constituent recognition process. Figure 6: Head-Subject in GIG format Example 6 (Intransitive verb) XP → Y P XP XP → X Y P → Y X → x Y → y ¯nv n #XP ⇒ #YPXP ⇒ #yXP ⇒ n#YXP </context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>C. Pollard and I. A. Sag. 1994. Head-driven Phrase Structure Grammar. University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Satta</author>
</authors>
<title>Tree-adjoining grammar parsing and boolean matrix multiplication.</title>
<date>1994</date>
<journal>Computational linguistics,</journal>
<volume>20</volume>
<contexts>
<context position="1630" citStr="Satta, 1994" startWordPosition="230" endWordPosition="231">tiple agreements, reduplication, crossing dependencies.2 Mildly Context-sensitive Languages (MCSLs) have been characterized by a geometric hierarchy of grammar levels. A level-2 MCSL (eg. 1See for example, (Joshi et al., 1991), (Weir, 1988). 2However other phenomena (e.g. scrambling, Georgian Case and Chinese numbers) might be considered to be beyond certain mildly context-sensitive formalisms. TALs/LILs) is able to capture up to 4 counting dependencies (includes L4 = {anbncndn|n ≥ 1} but not L5 = {anbncndnen|n ≥ 1}). They were proven to have recognition algorithms with time complexity O(n6) (Satta, 1994). In general for a level-k MCSL the recognition problem is in O(n3·2k−1) and the descriptive power regarding counting dependencies is 2k (Weir, 1988). Even the descriptive power of level-2 MCSLs (Tree Adjoining Grammars (TAGs), Linear Indexed Grammars (LIGs), Combinatory Categorial Grammars (CCGs) might be considered insufficient for some NL problems, therefore there have been many proposals3 to extend or modify them. On our view the possibility of modeling coordination phenomena is probably the most crucial in this respect. In (Casta˜no, 2003) we introduced Global Index Grammars (GIGs) - and </context>
</contexts>
<marker>Satta, 1994</marker>
<rawString>G. Satta. 1994. Tree-adjoining grammar parsing and boolean matrix multiplication. Computational linguistics, 20, No. 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>An efficiente augmented-contextfree parsing algorithm.</title>
<date>1987</date>
<booktitle>Computational linguistics,</booktitle>
<pages>13--31</pages>
<contexts>
<context position="11375" citStr="Tomita, 1987" startWordPosition="2084" endWordPosition="2085">b, c}, {i}, S, #, P) and P is: S → AR A → aAE A → a E → i b L L → OR |C C →c C |c ¯i c OE |c The derivation of the string aabbccbbcc shows five dependencies. #S ⇒ #AR ⇒ #aAER ⇒ #aaER ⇒ i#aabR ⇒ ii#aabbL ⇒ ii#aabbOR ⇒ i#aabbcOER ⇒ #aabbccER ⇒ i#aabbccbR ⇒ ii#aabbccbbL ⇒ ii#aabbccbbC ⇒ i#aabbccbbcC ⇒ #aabbccbbcc 2.3 GILs Recognition The recognition algorithm for GILs we presented in (Casta˜no, 2003) is an extension of Earley’s algorithm (cf. (Earley, 1970)) for CFLs. It has to be modified to perform the computations of the stack of indices in a GIG. In (Casta˜no, 2003) a graph-structured stack (Tomita, 1987) was used to efficiently represent ambiguous index operations in a GIG stack. Earley items are modified adding three parameters δ, c, o: [δ, c, o, A → α•AQ, i, j] The first two represent a pointer to an active node in the graph-structured stack ( δ ∈ I and c ≤ n). The third parameter (o ≤ n) is used to record the ordering of the rules affecting the stack. The O(n6) time-complexity of this algorithm reported in (Casta˜no, 2003) can be easily verified. The complete operation is typically the costly one in an Earley type algorithm. It can be verified that there are at most n6 instances of the ind</context>
</contexts>
<marker>Tomita, 1987</marker>
<rawString>M. Tomita. 1987. An efficiente augmented-contextfree parsing algorithm. Computational linguistics, 13:31–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
<author>A K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In Proc. of the 25th ACL,</booktitle>
<pages>104--111</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="4894" citStr="Vijay-Shanker et al., 1987" startWordPosition="758" endWordPosition="761"> for the palindrome, copy and the multiple copies languages {ww+|w E E*}. Finally in section 4 we discuss how this descriptive power can be used to encode HPSGs schemata. 2 Global Index Grammars 2.1 Linear Indexed Grammars Indexed grammars, (IGs) (Aho, 1968), and Linear Index Grammars, (LIGs;LILs) (Gazdar, 1988), have the capability to associate stacks of indices with symbols in the grammar rules. IGs are not semilinear. LIGs are Indexed Grammars with an additional constraint in the form of the productions: the stack of indices can be “trans4For the notion of dependent paths see for instance (Vijay-Shanker et al., 1987) or (Joshi, 2000). mitted” only to one non-terminal. As a consequence they are semilinear and belong to the class of MCSGs. The class of LILs contains L4 but not L5 (see above). A Linear Indexed Grammar is a 5-tuple (V, T, I, P, S), where V is the set of variables, T the set of terminals, I the set of indices, S in V is the start symbol, and P is a finite set of productions of the form, where A, B E V , α,-y E (V U T)*, i E I: a. A[..] —* α B[..] -y b. A[i..] —* α B[..] -y c. A[..] —* αB[i..] -y Example 1 L(Gwcw) = {wcw |w ∈ {a, b}∗}, Gww = ({S, R}, {a, b}, {i, j}, S, P) and P is: 1.S[..] → aS</context>
</contexts>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>K. Vijay-Shanker, D. J. Weir, and A. K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In Proc. of the 25th ACL, pages 104–111, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Weir</author>
</authors>
<title>Characterizing mildly contextsensitive grammar formalisms.</title>
<date>1988</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1258" citStr="Weir, 1988" startWordPosition="173" endWordPosition="174"> express the required properties of formalisms that might describe Natural Language (NL) phenomena. It requires three properties:1 a) constant growth property (or the stronger semilinearity property); b) polynomial parsability; c) limited cross-serial dependencies, i.e. some limited context-sensitivity. The canonical NL problems which exceed context free power are: multiple agreements, reduplication, crossing dependencies.2 Mildly Context-sensitive Languages (MCSLs) have been characterized by a geometric hierarchy of grammar levels. A level-2 MCSL (eg. 1See for example, (Joshi et al., 1991), (Weir, 1988). 2However other phenomena (e.g. scrambling, Georgian Case and Chinese numbers) might be considered to be beyond certain mildly context-sensitive formalisms. TALs/LILs) is able to capture up to 4 counting dependencies (includes L4 = {anbncndn|n ≥ 1} but not L5 = {anbncndnen|n ≥ 1}). They were proven to have recognition algorithms with time complexity O(n6) (Satta, 1994). In general for a level-k MCSL the recognition problem is in O(n3·2k−1) and the descriptive power regarding counting dependencies is 2k (Weir, 1988). Even the descriptive power of level-2 MCSLs (Tree Adjoining Grammars (TAGs), </context>
</contexts>
<marker>Weir, 1988</marker>
<rawString>D. Weir. 1988. Characterizing mildly contextsensitive grammar formalisms. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>