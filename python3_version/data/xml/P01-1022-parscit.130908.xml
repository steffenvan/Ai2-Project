<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000465">
<title confidence="0.978333">
Practical Issues in Compiling Typed Unification Grammars for Speech
Recognition
</title>
<author confidence="0.927053">
Jean Mark Gawron
</author>
<affiliation confidence="0.951449">
Dept. of Linguistics
</affiliation>
<address confidence="0.7727775">
San Diego State University
San Diego, CA
</address>
<email confidence="0.9472">
gawron@mail.sdsu.edu
</email>
<note confidence="0.6577085">
Christopher Culy
SRI International
</note>
<address confidence="0.8481475">
333 Ravenswood Avenue
Menlo Park, CA 94025
</address>
<email confidence="0.984624">
culy@ai.sri.com
</email>
<author confidence="0.972083">
John Dowding Beth Ann Hockey
</author>
<affiliation confidence="0.9344645">
RIACS RIALIST Group
NASA Ames Research Center
</affiliation>
<address confidence="0.984097">
Moffett Field, CA 94035
</address>
<email confidence="0.9963795">
jdowding@riacs.edu
bahockey@riacs.edu
</email>
<sectionHeader confidence="0.995602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999598">
Current alternatives for language mod-
eling are statistical techniques based
on large amounts of training data, and
hand-crafted context-free or finite-state
grammars that are difficult to build
and maintain. One way to address
the problems of the grammar-based ap-
proach is to compile recognition gram-
mars from grammars written in a more
expressive formalism. While theoreti-
cally straight-forward, the compilation
process can exceed memory and time
bounds, and might not always result in
accurate and efficient speech recogni-
tion. We will describe and evaluate two
approaches to this compilation prob-
lem. We will also describe and evalu-
ate additional techniques to reduce the
structural ambiguity of the language
model.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952260273973">
Language models to constrain speech recogni-
tion are a crucial component of interactive spo-
ken language systems. The more varied the lan-
guage that must be recognized, the more critical
good language modeling becomes. Research in
language modeling has heavily favored statisti-
cal approaches (Cohen 1995, Ward 1995, Hu et
al. 1996, Iyer and Ostendorf 1997, Bellegarda
1999, Stolcke and Shriberg 1996) while hand-
coded finite-state or context-free language models
dominate the commercial sector (Nuance 2001,
SpeechWorks 2001, TellMe 2001, BeVocal 2001,
HeyAnita 2001, W3C 2001). The difference re-
volves around the availability of data. Research
systems can achieve impressive performance us-
ing statistical language models trained on large
amounts of domain-targeted data, but for many
domains sufficient data is not available. Data may
be unavailable because the domain has not been
explored before, the relevant data may be con-
fidential, or the system may be designed to do
new functions for which there is no human-human
analog interaction. The statistical approach is un-
workable in such cases for both the commercial
developers and for some research systems (Moore
et al. 1997, Rayner et al. 2000, Lemon et al.
2001, Gauthron and Colineau 1999). Even in
cases for which there is no impediment to col-
lecting data, the expense and time required to col-
lect a corpus can be prohibitive. The existence
of the ATIS database (Dahl et al. 1994) is no
doubt a factor in the popularity of the travel do-
main among the research community for exactly
this reason.
A major problem with grammar-based finite-
state or context-free language models is that they
can be tedious to build and difficult to maintain,
as they can become quite large very quickly as
the scope of the grammar increases. One way
to address this problem is to write the gram-
mar in a more expressive formalism and gener-
ate an approximation of this grammar in the for-
mat needed by the recognizer. This approach
has been used in several systems, CommandTalk
(Moore et al. 1997), RIALIST PSA simula-
tor (Rayner et al. 2000), WITAS (Lemon et al.
2001), and SETHIVoice (Gauthron and Colin-
eau 1999). While theoretically straight-forward,
this approach is more demanding in practice, as
each of the compilation stages contains the po-
tential for a combinatorial explosion that will ex-
ceed memory and time bounds. There is also no
guarantee that the resulting language model will
lead to accurate and efficient speech recognition.
We will be interested in this paper in sound ap-
proximations (Pereira and Wright 1991) in which
the language accepted by the approximation is
a superset of language accepted by the original
grammar. While we conceed that alternative tech-
niques that are not sound (Black 1989, (Johnson
1998, Rayner and Carter 1996) may still be useful
for many purposes, we prefer sound approxima-
tions because there is no chance that the correct
hypothesis will be eliminated. Thus, further pro-
cessing techniques (for instance, N-best search)
will still have an opportunity to find the optimal
solution.
We will describe and evaluate two compilation
approaches to approximating a typed unification
grammar with a context-free grammar. We will
also describe and evaluate additional techniques
to reduce the size and structural ambiguity of the
language model.
</bodyText>
<sectionHeader confidence="0.975333" genericHeader="method">
2 Typed Unification Grammars
</sectionHeader>
<bodyText confidence="0.964297636363636">
Typed Unification Grammars (TUG), like HPSG
(Pollard and Sag 1994) and Gemini (Dowding et
al. 1993) are a more expressive formalism in
which to write formal grammars1. As opposed to
atomic nonterminal symbols in a CFG, each non-
terminal in a TUG is a complex feature structure
(Shieber 1986) where features with values can be
attached. For example, the rule:
s[] np:[num=N] vp:[num=N]
can be considered a shorthand for 2 context free
rules (assuming just two values for number):
</bodyText>
<footnote confidence="0.91718">
s np singular vp singular
s np plural vp plural
1This paper specifically concerns grammars written in
the Gemini formalism. However, the basic issues involved in
compiling typed unification grammars to context-free gram-
mars remain the same across formalisms.
</footnote>
<bodyText confidence="0.987561076923077">
This expressiveness allows us to write grammars
with a small number of rules (from dozens to a
few hundred) that correspond to grammars with
large numbers of CF rules. Note that the approx-
imation need not incorporate all of the features
from the original grammar in order to provide a
sound approximation. In particular, in order to de-
rive a finite CF grammar, we will need to consider
only those features that have a finite number of
possible values, or at least consider only finitely
many of the possible values for infinitely valued
features. We can use the technique of restriction
(Shieber 1985) to remove these features from our
feature structures. Removing these features may
give us a more permissive language model, but it
will still be a sound approximation.
The experimental results reported in this pa-
per are based on a grammar under development
at RIACS for a spoken dialogue interface to a
semi-autonomous robot, the Personal Satellite
Assistant (PSA). We consider this grammar to be
medium-sized, with 61 grammar rules and 424
lexical entries. While this may sound small, if
the grammar were expanded by instantiating vari-
ables in all legal permutations, it would contain
over context-free rules.
</bodyText>
<sectionHeader confidence="0.98878" genericHeader="method">
3 The Compilation Process
</sectionHeader>
<bodyText confidence="0.999211470588235">
We will be studying the compilation process
to convert typed unification grammars expressed
in Gemini notation into language models for
use with the Nuance speech recognizer (Nuance,
2001). We are using Nuance in part because it
supports context-free language models, which is
not yet industry standard.2 Figure 1 illustrates the
stages of processing: a typed unification grammar
is first compiled to a context-free grammar. This
is in turn converted into a grammar in Nuance’s
Grammar Specification Language (GSL), which
is a form of context-free grammar in a BNF-like
notation, with one rule defining each nonterminal,
and allowing alternation and Kleene closure on
the right-hand-side. Critically, the GSL must not
contain any left-recursion, which must be elimi-
nated before the GSL representation is produced.
</bodyText>
<footnote confidence="0.93544125">
2The standard is moving in the direction of context-
free language models, as can be seen in the draft standard
for Speech Recognition Grammars being developed by the
World Wide Web Consortium (W3C 2001).
</footnote>
<figure confidence="0.996524823529412">
Typed Unification Grammar (TUG)
TUG to CFG Compiler
Context Free Grammar
CFG to GSL Conversion
GSL Grammar
nuance_compiler
local
t
for each r
for each t
t t
if
then return
else return
for each l
l l
l
</figure>
<tableCaption confidence="0.661737">
Table 1: Construction of the fixed-point
</tableCaption>
<figure confidence="0.9604455">
Recognition System
Package
</figure>
<figureCaption confidence="0.999987">
Figure 1: Compilation Process
</figureCaption>
<bodyText confidence="0.99974225">
The GSL representation is then compiled into a
Nuance package with the nuance compiler.
This package is the input to the speech recognizer.
In our experience, each of the compilation stages,
as well as speech recognition itself, has the po-
tential to lead to a combinatorial explosion that
exceeds practical memory or time bounds.
We will now describe implementations of the
first stage, generating a context-free grammar
from a typed unification grammar, by two differ-
ent algorithms, one defined by Kiefer and Krieger
(2000) and one by Moore and Gawron, described
in Moore (1998) The critical difficulty for both
of these approaches is how to select the set of
derived nonterminals that will appear in the final
CFG.
</bodyText>
<subsectionHeader confidence="0.990702">
3.1 Kiefer&amp;Krieger’s Algorithm
</subsectionHeader>
<bodyText confidence="0.999124869565217">
The algorithm of Kiefer&amp;Krieger (K&amp;K) divides
this compilation step into two phases: first, the
set of context-free nonterminals is determined by
iterating a bottom-up search until a least fixed-
point is reached; second, this least fixed-point is
used to instantiate the set of context-free produc-
tions.
The computation of the fixed-point , de-
scribed in Table 1, proceeds as follows. First,
is constructed by finding the most-general set of
feature structures that occur in the lexicon (lines
1-4). Each feature structure has the lexical restric-
tor L applied to it before being added to (line 3)
with the operator. This operator maintains the
set of most-general feature structures. A new
feature structure is added to the set only when it
is not subsumed by any current members of the
set, and any current members that are subsumed
by the new member are removed as the new el-
ement is added. The computation of proceeds
with the call to Iterate (line 6), which adds new
feature structures that can be derived bottom-up.
Each call to Iterate generates a new set , in-
cluding as its base (line 8). It then adds new
feature structures to by instantiating every
grammar rule r in , the set of grammar rules.
The first step in the instantiation is to unify every
combination of daughters with all possible feature
structures from (FillDaughters, line 10). The
rule restrictor is applied to each resulting feature
structure (line 11) before it is added to using
the operator (line 12), similar to the lexical
case. If after checking all rule applications bottom
up, no new feature structures have been added to
(line 13), then the least fixed-point had been
found, and the process terminates. Otherwise, It-
erate is called recursively. See Kiefer and Krieger
(2000) for proof that this terminates, and finds the
appropriate fixed-point.
Having computed the least fixed-point , the
next step is to compute the set of corresponding
CF productions. For each r in , of the form
, instantiate the daughters using
all combinations of unifiable feature structures
from . Context-free productions
will be added, where and
</bodyText>
<subsectionHeader confidence="0.999692">
3.2 Moore and Gawron’s Algorithm
</subsectionHeader>
<bodyText confidence="0.99992575862069">
While K&amp;K uses subsumption to generate the set
of most-general nonterminals, the algorithm of
Moore and Gawron (M&amp;G), described in Moore
(1998) attempts to propagate features values both
bottom-up and top-down through the grammar to
generate a set of nonterminals that contains no
variables. Also unlike K&amp;K, the production of
the CF rules and associated nonterminals is inter-
leaved. The process consists of a preprocessing
stage to eliminate singleton variables, a bottom-
up propagation stage, and a top-down propagation
stage.
The preprocessing stage rewrites the grammar
to eliminate singleton variables. This step ef-
fective replaces singleton variables with a new
unique atomic symbol ’ANY’. The feature struc-
ture for each lexical item and grammar rule is
rewritten such that singleton variables are uni-
fied with a special value ’ANY’, and every non-
singleton variable expression is embedded in a
val() term. After this transformation, singleton
variables will not unify with non-singleton vari-
able expressions, only with other singletons. Ad-
ditional rules are then introduced to deal with the
singleton variable cases. For each daughter in a
grammar rule in which a singleton variable ap-
pears, new lexical items and grammar rules are
introduced which unify with that daughter in the
original grammar. As an example, consider the
</bodyText>
<footnote confidence="0.9599">
3There is a minor bug in K&amp;K where they state that the
</footnote>
<bodyText confidence="0.967056352941177">
result will always be in and will be a CF
production in the approximation, but this may not be true
if was removed from by . Instead, the subsuming
nonterminal should be the new mother.
grammar fragment:
vp:[num=N] v:[num=N] np:[]
np:[num=N] det:[num=N] n:[num=N]
np:[num=pl] n:[num=pl]
Here, the np object of vp is underspecified for
num (as English does not generally require num-
ber agreement between the verb and its object), so
it will be a singleton variable. So, the following
rules will be generated:
vp:[num=val(N)]
v:[num=val(N)] np:[num=’ANY’]
np:[num=val(N)]
det:[num=val(N)] n:[num=val(N)]
np:[num=val(pl)] n:[num=val(pl)]
np:[num=’ANY’]
det:[num=val(N)] n:[num=val(N)]
np:[num=’ANY’] n:[num=val(pl)]
After preprocessing, any variables remaining
in the bodies of grammar rules will be shared
variables. Singleton variable elimination by it-
self is very effective at shrinking the size of the
CF grammar space, reducing the size of the rule
space for the PSA grammar from rules
to rules.
The bottom-up stage starts from this grammar,
and derives a new grammar by propagating fea-
ture values up from the lexicon. The process acts
like a chart parser, except that indicies are not
kept. When a rule transitions from an active edge
to an inactive edge, a new rule with those feature
instantiations is recorded. As a side-effect of this
compilation, -productions are eliminated.
Top-down processing fires last, and performs
a recursive-descent walk of the grammar starting
atthe start symbol , generating a new grammar
that propagates features downward through the
grammar. A side-effect of this computation is that
useless-productions (rules not reachable from )
are removed. It might still be possible that after
top-down propagation there would still be vari-
ables present in the grammar. For example, if the
grammar allows sentences like “the deer walked”,
which are ambiguous for number, then there will
be a rule in the grammar that contains a shared
variable for the number feature. To address this,
as top-down propagation is progressing, all re-
maining variables are identified and unified with
</bodyText>
<page confidence="0.648422">
.3
</page>
<bodyText confidence="0.99927925">
a special value ’ALL’. Since each nonterminal is
now ground, it is trivial to assign each nontermi-
nal a unique atomic symbol, and rewrite the gram-
mar as a CFG.
</bodyText>
<subsectionHeader confidence="0.993297">
3.3 Comparison
</subsectionHeader>
<bodyText confidence="0.999951325">
Table 2 contains a summary of some key statistics
generated using both techniques. The recognition
results were obtained on a test set of 250 utter-
ances. Recognition accuracy is measured in word
error rate, and recognition speed is measured in
multiples of real time (RT), the length of the ut-
terance compared with the length of the CPU time
required for the recognition result4. The size of
the resulting language model is measured in terms
of the number of nonterminals in the grammar,
and the size of the Nuance node array, a binary
representation of the recursive transition network
it uses to search the grammar. Ambiguity counts
the average number of parses per sentence that
were allowed by the CF grammar. As can be read-
ily seen, the compilation time for the K&amp;K algo-
rithm is dramatically lower than the M&amp;G algo-
rithm, while producing a similarly lower recog-
nition performance, measured in both word error
rate and recognition speed.
Given that the two techniques generate gram-
mars of roughly similar sizes, the difference in
performance is striking. We believe that the use of
the in K&amp;K is partially responsible. Consider
a grammar that contains a lexical item like “deer”
that is underspecified for number, and will contain
a singleton variable. This will lead to a nontermi-
nal feature structure for noun phrase that is also
underspecified for number, which will be more
general than any noun phrase feature structures
that are marked for number. The operator will
remove those noun phrases as being less general,
effectively removing the number agreement con-
straint between subject and verb from the context-
free approximation. The use of allows a single
grammar rule or lexical item to have non-local ef-
fects on the approximation. As seen in Table 2,
the grammar derived from the K&amp;K algorithm is
much more ambiguous than the grammar derived
the M&amp;G algorithm, and, as is further elaborated
</bodyText>
<footnote confidence="0.905688666666667">
4All timing results presented in this paper were executed
on a Sun Ultra 60 workstation, running at 330MHz with 1.5
GB physical memory and an additional 1GB swap.
</footnote>
<table confidence="0.999427857142857">
K&amp;K M&amp;G
Compilation Time 11 min. 192 min.
Nonterminals 2,284 1,837
Node Array Size 224KB 204KB
Word Error Rate 25.05% 11.91%
Recognition Time 13.8xRT 1.7xRT
Ambiguity 15.4 1.9
</table>
<tableCaption confidence="0.999681">
Table 2: Comparison Results
</tableCaption>
<bodyText confidence="0.999785538461539">
in Section 4, we believe that the amount of am-
biguity can be a significant factor in recognition
performance.
On the other hand, attention must be paid to
the amount of time and memory required by the
Moore algorithm. On a medium-sized grammar,
this compilation step took over 3 hours, and was
close to exceeding the memory capacity of our
computer, with a process size of over 1GB. The
approximation is only valuable if we can succeed
in computing it. Finally, it should also be noted
that M&amp;G’s algorithm removes -productions and
useless-productions, while we had to add a sepa-
rate postprocessing stage to K&amp;K’s algorithm to
get comparable results.
For future work we plan to explore possible in-
tegrations of these two algorithms. One possi-
bility is to include the singleton-elimination pro-
cess as an early stage in the K&amp;K algorithm.
This is a relatively fast step, but may lead to a
significant increase in the size of the grammar.
Another possibility is to embed a variant of the
K&amp;K algorithm, and its clean separation of gen-
erating nonterminals from generating CF produc-
tions, in place of the bottom-up processing stage
in M&amp;G’s algorithm.
</bodyText>
<sectionHeader confidence="0.981388" genericHeader="method">
4 Reducing Structural Ambiguity
</sectionHeader>
<bodyText confidence="0.999980666666667">
It has been observed (Bratt and Stolcke 1999)
that a potential difficulty with using linguistically-
motivated grammars as language models is that
ambiguity in the grammar will lead to multiple
paths in the language model for the same recog-
nition hypothesis. In a standard beam-search ar-
chitecture, depending on the level of ambiguity,
this may tend to fill the beam with multiple hy-
potheses for the same word sequence, and force
other good hypotheses out of the beam, poten-
tially increasing word error rate. This observation
appears to be supported in practice. The original
form of the PSA grammar allows an average of
1.4 parses per sentence, and while both the K&amp;K
and M&amp;G algorigthm increase the level of ambi-
guity, the K&amp;K algorithm increases much more
dramatically.
We are investigating techniques to transform a
CFG into one weakly equivalent but with less am-
biguity. While it is not possible in general to re-
move all ambiguity (Hopcroft and Ullman 1979)
we hope that reducing the amount of ambiguity
in the resulting grammar will result in improved
recognition performance.
</bodyText>
<subsectionHeader confidence="0.995119">
4.1 Grammar Compactor
</subsectionHeader>
<bodyText confidence="0.995371195652174">
The first technique is actually a combination of
three related transformations:
Duplicate Nonterminal Elimination – If two
nonterminals A and B have exactly the same
set of productions
then remove the productions for B, and
rewrite B as A everywhere it occurs in the
grammar.
Unit Rule Elimination – If there is only one
production for a nonterminal A, and it has a
single daughter on its right-hand side
then remove the production for A, and
rewrite A as everywhere it occurs in the
grammar.
Duplicate Production Elimination – If a non-
terminal A has two productions that are iden-
tical
These transformations are applied repeatedly un-
til they can no longer be applied. Each of these
transformations may introduce opportunities for
the others to apply, so the process needs to be
order insensitive. This technique can be applied
after the traditional reduction techniques of -
elimination, cycle-elimination, and left-recursion
elimination, since they don’t introduce any new
-productions or any new left-recursion.
Although these transformations seem rather
specialized, they were surprisingly effective at re-
ducing the size of the grammar. For the K&amp;K
algorithm, the number of grammar rules was re-
duced from 3,246 to 2,893, a reduction of 9.2%,
and for the M&amp;G algorithm, the number of rules
was reduced from 4,758 to 1,837, a reduction of
61%. While these transforms do reduce the size
of the grammar, and modestly reduce the level
of ambiguity from 1.96 to 1.92, they did not ini-
tially appear to improve recognition performance.
However, that was with the nuance parameter
-node array optimization level set to
the default value FULL. When set to the value
MIN, the compacted grammar was approxi-
mately 60% faster, and about 9% reduction
in the word error rate, suggesting that the
nuance compiler is performing a similar
form of compaction during node array optimiza-
tion.
</bodyText>
<subsectionHeader confidence="0.98523">
4.2 Immediate Recursion Detection
</subsectionHeader>
<bodyText confidence="0.999863714285714">
Another technique to reduce ambiguity was moti-
vated by a desire to reduce the amount of preposi-
tional phrase attachment ambiguity in our gram-
mar. This technique detects when a Kleene clo-
sure will be introduced into the final form of the
grammar, and takes advantage of this to remove
ambiguity. Consider this grammar fragment:
</bodyText>
<equation confidence="0.602986">
NP NP PP
VP V NP PP
</equation>
<bodyText confidence="0.979663111111111">
The first rule tells us that an NP can be followed
by an arbitrary number of PPs, and that the PP
following the NP in the second rule will be am-
biguous. In addition, any nonterminal that has an
NP as its rightmost daughter can also be followed
by an arbitrary number of PPs, so we can detect
ambiguity following those nonterminals as well.
We define a predicate follows as:
A follows B iff
</bodyText>
<listItem confidence="0.332572">
B B A or
B C and A follows C
</listItem>
<bodyText confidence="0.993052627906977">
Now, the follows relation can be used to reduce
ambiguity by modifying other productions where
then remove the production for .
a B is followed by an A:
There is an exactly analogous transformation
involving immediate right-recursion and a simi-
lar predicate preceeds. These transformation pro-
duce almost the same language, but can modify
it by possibly allowing constructions that were
not allowed in the original grammar. In our case,
the initial grammar fragment above would require
that at least one PP be generated within the scope
of the VP, but after the transformation that is no
longer required. So, while these transformations
are not exact, they are still sound aproximations,
as the resulting language is a superset of the orig-
inal language.
Unfortunately, we have had mixed results with
applying these transformations. In earlier ver-
sions of our implementation, applying these trans-
formations succeeded in improving the recogni-
tion speed up to 20%, while having some modest
improvements in word error rate. But, as we im-
proved other aspects of the compilation process,
notably the grammar compaction techniques and
the left-recursion elimination technique, those
improvements disappeared, and the transforma-
tions actually made things worse. The problem
appears to be that both transformations can in-
troduce cycles, and the right-recursive case can
introduce left-recursion even in cases where cy-
cles are not introduced. When the introduced cy-
cles and left-recursions are later removed, the size
of the grammar is increased, which can lead to
poorer recognition performance. In the earlier im-
plementations, cycles were fortuitously avoided,
probably due to the fact that there were more
unique nonterminals overall. We expect that these
transformations may be effective for some gram-
mars, but not others. We plan to continue to ex-
plore refinements to these techiques to prevent
them from applying in cases where cycles or left-
recursion may be introduced.
</bodyText>
<sectionHeader confidence="0.99422" genericHeader="method">
5 Left Recursion Elimination
</sectionHeader>
<bodyText confidence="0.999960466666667">
We have used two left-recursion elimination tech-
niques, the traditional one based on Paull’s al-
gorithm, as reported by Hopcroft and Ullman
(1979), and one described by Moore (2000)5,
based on a technique described by Johnson
(1998). Our experience concurs with Moore that
the left-corner transform he describes produces a
more compact left-recursion free grammar than
that of Paull’s algorithm. For the K&amp;K approx-
imation, we were unable to get any grammar to
compile through to a working language model
using Paull’s algorithm (the models built with
Paull’s algorithm caused the recognizer to ex-
ceed memory bounds), and only succeeded with
Moore’s left-recursion elimination technique.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99996075">
We have presented descriptions of two algorithms
for approximating typed unification grammars
with context-free grammars, and evaluated their
performance during speech recognition. Initial re-
sults show that high levels of ambiguity coorelate
with poor recognition performance, and that size
of the resuling language model does not appear to
directly coorelate with recognition performance.
We have developed new techniques for further re-
ducing the size and amount of ambiguity in these
context-free grammars, but have so far met with
mixed results.
</bodyText>
<sectionHeader confidence="0.998526" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997650545454546">
J. Bellegarda. Context scope selection in multi-span
statistical language modeling. In Proceedings of the
6th European Conference on Speech Communica-
tion and Technology (EuroSpeech99), pages 2163–
2166, 1999.
BeVocal. http://www.bevocal.com, 2001. As of 31
January 2001.
A. Black. Finite state machines from feature gram-
mars. In International Workshop on Parsing Tech-
nologies, pages 277–285, 1989.
H. Bratt and A. Stolcke. private communication, 1999.
</reference>
<footnote confidence="0.8566924">
5There is a minor bug in the description of Moore’s algo-
rithm that occurs in his paper, that the set of “retained non-
terminals” needs to be extended to include any nonterminals
that occur either in the non-initial daughter of a left-recursive
nonterminal, or in any daughter of a non-left-recursive non-
terminal. Thanks to Robert Moore for providing the solution
to this bug. This bug applies only to the description of his
algorithm, not to the implementation on which the empirical
results reported is based. Please see Moore (2000) for more
details.
</footnote>
<bodyText confidence="0.992515083333333">
where follows and
can be rewritten as
of the Fifth Conference on Applied Natural Lan-
guage Processing, pages 1–7, 1997.
M. Cohen, Z. Rivlin, and H. Bratt. Speech recog-
nition in the ATIS domain using multiple knowl-
edge sources. In Proceedings of the Spoken Lan-
guage Systems Technology Workshop, pages 257–
260, 1995.
R. Moore. Using natural language knowledge sources
in speech recognition. In Proceedings of the NATO
Advanced Studies Institute, 1998.
</bodyText>
<reference confidence="0.998998368421053">
D. Dahl, M. Bates, M. Brown, K. Hunicke-Smith,
D. Pallet, C. Pao, A. Rudnicky, and E. Shriberg.
Expanding the scope of the atis task: The atis-3 cor-
pus. In Proceedings of the ARPA Human Language
Technology Workshop, Princeton, NJ, March 1994.
J. Dowding, M. Gawron, D. Appelt, L. Cherny,
R. Moore, and D. Moran. Gemini: A natural lan-
guage system for spoken language understanding.
In Proceedings of the Thirty-First Annual Meeting
of the Association for Computational Linguistics,
1993.
O. Gauthron and N. Colineau. SETHIVoice: Cgf
control by speech-recognition/interpretation. In
I/ITSEC ’99 (Interservice/Industry Training, Simu-
lation and Education Conference), Synthetic Solu-
tions for the 21st Century, Orlando, FL, 1999.
HeyAnita. http://www.heyanita.com, 2001. As of 31
January 2001.
J. Hu, W. Turin, and M.K. Brown. Language model-
ing with stochastic automata. In Proceedings of the
Fourth International Conference on Spoken Lan-
guage Processing (ICSLP), pages 406–413, 1996.
J. Hopcroft and J. Ullman. Introduction to Automata
Theory, Languages, and Computation. Addison-
Wesley, Reading, MA, 1979.
R. Iyer and M. Ostendorf. Transforming out-of-
domain estimates to improve in-domain language
models. In Proceedings of the 5th European Con-
ference on Speech Communication and Technology
(EuroSpeech97), pages 1975–1978, 1997.
M. Johnson. Finite-state approximation of constraint-
based grammars using left-corner grammar trans-
forms. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics,
pages 619–623, 1998.
B. Kiefer and H. Krieger. A context-free approxima-
tion of head-driven phrase structure grammar. In
Proceedings of the 6th International Workshop on
Parsing Technologies, pages 135–146, 2000.
Oliver Lemon, Anne Bracy, Alexander Gruenstein,
and Stanley Peters. A multi-modal dialogue sys-
tem for human-robot conversation. In Proceedings
of North American Association for Computational
Linguistics (NAACL 2001), 2001.
R. Moore, J. Dowding, H. Bratt, J. Gawron, Y. Gorfu,
and A. Cheyer. CommandTalk: A spoken-language
interface for battlefield simulations. In Proceedings
R. Moore. Removing left-recusion from context-free
grammars. In Proceedings of 1st Meeting of the
North Americal Chapter of the Associations for
Computational Linguistics, pages 249–255, 2000.
Nuance. http://www.nuance.com, 1999. As of 1 April
1999.
C. Pollard and I. Sag. Head-Driven Phrase Structure
Grammar. University of Chicago Press, 1994.
F. Pereira and R. Wright. Finite-state approximation
of phrase structure grammars. In Proceedings of the
29th Annual Meeting of the Assocationsfor Compu-
tational Linguistics, pages 246–255, 1991.
M. Rayner and D.M. Carter. Fast parsing using prun-
ing and grammar specialization. In Proceedings of
the Thirty-Fourth Annual Meeting of the Associa-
tion for Computational Linguistics, pages 223–230,
Santa Cruz, California, 1996.
M. Rayner, B.A. Hockey, and F. James. A com-
pact architecture for dialogue management based on
scripts and meta-outputs. In Proceedings ofApplied
Natural Language Processing (ANLP), 2000.
S. Shieber. Using restriction to extend parsing algo-
rithms for complex-feature-based formalisms. In
Proceedings of the 23rd Annual Meeting of the
Assocations for Computational Linguistics, pages
145–152, 1985.
S. Shieber. An Introduction to Unification-based Ap-
proaches to Grammar. CLSI Lecture Notes no. 4.
Center for the Study of Language and Information,
1986. (distributed by the University of Chicago
Press).
SpeechWorks. http://www.speechworks.com, 2001.
As of 31 January 2001.
A. Stolcke and E. Shriberg. Statistical language
modeling for speech disfluencies. In Proceedings
of the IEEE International Conference on Acous-
tics, Speech and Signal Processing, pages 405–408,
1996.
TellMe. http://www.tellme.com, 2001. As of 31 Jan-
uary 2001.
World Wide Web Consortium (W3C). Speech
Recognition Grammar Specification for
the W3C Speech Interface Framework.
http://www.w3.org/TR/speech-grammar, 2001.
As of 3 January 2001.
W. Ward and S. Issar. The cmu atis system. In Spo-
ken Language System Technology Workshop, pages
249–251, 1995.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.895140">
<title confidence="0.999679">Practical Issues in Compiling Typed Unification Grammars for Speech Recognition</title>
<author confidence="0.999988">Jean Mark Gawron</author>
<affiliation confidence="0.9979485">Dept. of Linguistics San Diego State University</affiliation>
<address confidence="0.944145">San Diego, CA</address>
<email confidence="0.999539">gawron@mail.sdsu.edu</email>
<author confidence="0.998731">Christopher Culy</author>
<affiliation confidence="0.999978">SRI International</affiliation>
<address confidence="0.998462">333 Ravenswood Avenue Menlo Park, CA 94025</address>
<email confidence="0.998994">culy@ai.sri.com</email>
<author confidence="0.999812">John Dowding Beth Ann Hockey</author>
<affiliation confidence="0.9986095">RIACS RIALIST Group NASA Ames Research Center</affiliation>
<address confidence="0.999621">Moffett Field, CA 94035</address>
<email confidence="0.9975765">jdowding@riacs.edubahockey@riacs.edu</email>
<abstract confidence="0.998349571428571">Current alternatives for language modeling are statistical techniques based on large amounts of training data, and hand-crafted context-free or finite-state grammars that are difficult to build and maintain. One way to address the problems of the grammar-based approach is to compile recognition grammars from grammars written in a more expressive formalism. While theoretically straight-forward, the compilation process can exceed memory and time bounds, and might not always result in accurate and efficient speech recognition. We will describe and evaluate two approaches to this compilation problem. We will also describe and evaluate additional techniques to reduce the structural ambiguity of the language model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bellegarda</author>
</authors>
<title>Context scope selection in multi-span statistical language modeling.</title>
<date>1999</date>
<booktitle>In Proceedings of the 6th European Conference on Speech Communication and Technology (EuroSpeech99),</booktitle>
<pages>2163--2166</pages>
<contexts>
<context position="1523" citStr="Bellegarda 1999" startWordPosition="222" endWordPosition="223">ccurate and efficient speech recognition. We will describe and evaluate two approaches to this compilation problem. We will also describe and evaluate additional techniques to reduce the structural ambiguity of the language model. 1 Introduction Language models to constrain speech recognition are a crucial component of interactive spoken language systems. The more varied the language that must be recognized, the more critical good language modeling becomes. Research in language modeling has heavily favored statistical approaches (Cohen 1995, Ward 1995, Hu et al. 1996, Iyer and Ostendorf 1997, Bellegarda 1999, Stolcke and Shriberg 1996) while handcoded finite-state or context-free language models dominate the commercial sector (Nuance 2001, SpeechWorks 2001, TellMe 2001, BeVocal 2001, HeyAnita 2001, W3C 2001). The difference revolves around the availability of data. Research systems can achieve impressive performance using statistical language models trained on large amounts of domain-targeted data, but for many domains sufficient data is not available. Data may be unavailable because the domain has not been explored before, the relevant data may be confidential, or the system may be designed to d</context>
</contexts>
<marker>Bellegarda, 1999</marker>
<rawString>J. Bellegarda. Context scope selection in multi-span statistical language modeling. In Proceedings of the 6th European Conference on Speech Communication and Technology (EuroSpeech99), pages 2163– 2166, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>http www bevocal com</author>
</authors>
<date>2001</date>
<journal>As of</journal>
<volume>31</volume>
<marker>com, 2001</marker>
<rawString>BeVocal. http://www.bevocal.com, 2001. As of 31 January 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Black</author>
</authors>
<title>Finite state machines from feature grammars.</title>
<date>1989</date>
<booktitle>In International Workshop on Parsing Technologies,</booktitle>
<pages>277--285</pages>
<contexts>
<context position="3901" citStr="Black 1989" startWordPosition="615" endWordPosition="616">u 1999). While theoretically straight-forward, this approach is more demanding in practice, as each of the compilation stages contains the potential for a combinatorial explosion that will exceed memory and time bounds. There is also no guarantee that the resulting language model will lead to accurate and efficient speech recognition. We will be interested in this paper in sound approximations (Pereira and Wright 1991) in which the language accepted by the approximation is a superset of language accepted by the original grammar. While we conceed that alternative techniques that are not sound (Black 1989, (Johnson 1998, Rayner and Carter 1996) may still be useful for many purposes, we prefer sound approximations because there is no chance that the correct hypothesis will be eliminated. Thus, further processing techniques (for instance, N-best search) will still have an opportunity to find the optimal solution. We will describe and evaluate two compilation approaches to approximating a typed unification grammar with a context-free grammar. We will also describe and evaluate additional techniques to reduce the size and structural ambiguity of the language model. 2 Typed Unification Grammars Typ</context>
</contexts>
<marker>Black, 1989</marker>
<rawString>A. Black. Finite state machines from feature grammars. In International Workshop on Parsing Technologies, pages 277–285, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Bratt</author>
<author>A Stolcke</author>
</authors>
<title>private communication,</title>
<date>1999</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop,</booktitle>
<location>Princeton, NJ,</location>
<contexts>
<context position="17842" citStr="Bratt and Stolcke 1999" startWordPosition="2872" endWordPosition="2875">age to K&amp;K’s algorithm to get comparable results. For future work we plan to explore possible integrations of these two algorithms. One possibility is to include the singleton-elimination process as an early stage in the K&amp;K algorithm. This is a relatively fast step, but may lead to a significant increase in the size of the grammar. Another possibility is to embed a variant of the K&amp;K algorithm, and its clean separation of generating nonterminals from generating CF productions, in place of the bottom-up processing stage in M&amp;G’s algorithm. 4 Reducing Structural Ambiguity It has been observed (Bratt and Stolcke 1999) that a potential difficulty with using linguisticallymotivated grammars as language models is that ambiguity in the grammar will lead to multiple paths in the language model for the same recognition hypothesis. In a standard beam-search architecture, depending on the level of ambiguity, this may tend to fill the beam with multiple hypotheses for the same word sequence, and force other good hypotheses out of the beam, potentially increasing word error rate. This observation appears to be supported in practice. The original form of the PSA grammar allows an average of 1.4 parses per sentence, a</context>
</contexts>
<marker>Bratt, Stolcke, 1999</marker>
<rawString>H. Bratt and A. Stolcke. private communication, 1999. D. Dahl, M. Bates, M. Brown, K. Hunicke-Smith, D. Pallet, C. Pao, A. Rudnicky, and E. Shriberg. Expanding the scope of the atis task: The atis-3 corpus. In Proceedings of the ARPA Human Language Technology Workshop, Princeton, NJ, March 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dowding</author>
<author>M Gawron</author>
<author>D Appelt</author>
<author>L Cherny</author>
<author>R Moore</author>
<author>D Moran</author>
</authors>
<title>Gemini: A natural language system for spoken language understanding.</title>
<date>1993</date>
<booktitle>In Proceedings of the Thirty-First Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="4597" citStr="Dowding et al. 1993" startWordPosition="719" endWordPosition="722">es, we prefer sound approximations because there is no chance that the correct hypothesis will be eliminated. Thus, further processing techniques (for instance, N-best search) will still have an opportunity to find the optimal solution. We will describe and evaluate two compilation approaches to approximating a typed unification grammar with a context-free grammar. We will also describe and evaluate additional techniques to reduce the size and structural ambiguity of the language model. 2 Typed Unification Grammars Typed Unification Grammars (TUG), like HPSG (Pollard and Sag 1994) and Gemini (Dowding et al. 1993) are a more expressive formalism in which to write formal grammars1. As opposed to atomic nonterminal symbols in a CFG, each nonterminal in a TUG is a complex feature structure (Shieber 1986) where features with values can be attached. For example, the rule: s[] np:[num=N] vp:[num=N] can be considered a shorthand for 2 context free rules (assuming just two values for number): s np singular vp singular s np plural vp plural 1This paper specifically concerns grammars written in the Gemini formalism. However, the basic issues involved in compiling typed unification grammars to context-free gramma</context>
</contexts>
<marker>Dowding, Gawron, Appelt, Cherny, Moore, Moran, 1993</marker>
<rawString>J. Dowding, M. Gawron, D. Appelt, L. Cherny, R. Moore, and D. Moran. Gemini: A natural language system for spoken language understanding. In Proceedings of the Thirty-First Annual Meeting of the Association for Computational Linguistics, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Gauthron</author>
<author>N Colineau</author>
</authors>
<title>SETHIVoice: Cgf control by speech-recognition/interpretation.</title>
<date>1999</date>
<booktitle>In I/ITSEC ’99 (Interservice/Industry Training, Simulation and Education Conference), Synthetic Solutions for the 21st Century,</booktitle>
<location>Orlando, FL,</location>
<contexts>
<context position="2397" citStr="Gauthron and Colineau 1999" startWordPosition="356" endWordPosition="359">bility of data. Research systems can achieve impressive performance using statistical language models trained on large amounts of domain-targeted data, but for many domains sufficient data is not available. Data may be unavailable because the domain has not been explored before, the relevant data may be confidential, or the system may be designed to do new functions for which there is no human-human analog interaction. The statistical approach is unworkable in such cases for both the commercial developers and for some research systems (Moore et al. 1997, Rayner et al. 2000, Lemon et al. 2001, Gauthron and Colineau 1999). Even in cases for which there is no impediment to collecting data, the expense and time required to collect a corpus can be prohibitive. The existence of the ATIS database (Dahl et al. 1994) is no doubt a factor in the popularity of the travel domain among the research community for exactly this reason. A major problem with grammar-based finitestate or context-free language models is that they can be tedious to build and difficult to maintain, as they can become quite large very quickly as the scope of the grammar increases. One way to address this problem is to write the grammar in a more e</context>
</contexts>
<marker>Gauthron, Colineau, 1999</marker>
<rawString>O. Gauthron and N. Colineau. SETHIVoice: Cgf control by speech-recognition/interpretation. In I/ITSEC ’99 (Interservice/Industry Training, Simulation and Education Conference), Synthetic Solutions for the 21st Century, Orlando, FL, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>http www heyanita com</author>
</authors>
<date>2001</date>
<journal>As of</journal>
<volume>31</volume>
<marker>com, 2001</marker>
<rawString>HeyAnita. http://www.heyanita.com, 2001. As of 31 January 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hu</author>
<author>W Turin</author>
<author>M K Brown</author>
</authors>
<title>Language modeling with stochastic automata.</title>
<date>1996</date>
<booktitle>In Proceedings of the Fourth International Conference on Spoken Language Processing (ICSLP),</booktitle>
<pages>406--413</pages>
<contexts>
<context position="1481" citStr="Hu et al. 1996" startWordPosition="214" endWordPosition="217"> bounds, and might not always result in accurate and efficient speech recognition. We will describe and evaluate two approaches to this compilation problem. We will also describe and evaluate additional techniques to reduce the structural ambiguity of the language model. 1 Introduction Language models to constrain speech recognition are a crucial component of interactive spoken language systems. The more varied the language that must be recognized, the more critical good language modeling becomes. Research in language modeling has heavily favored statistical approaches (Cohen 1995, Ward 1995, Hu et al. 1996, Iyer and Ostendorf 1997, Bellegarda 1999, Stolcke and Shriberg 1996) while handcoded finite-state or context-free language models dominate the commercial sector (Nuance 2001, SpeechWorks 2001, TellMe 2001, BeVocal 2001, HeyAnita 2001, W3C 2001). The difference revolves around the availability of data. Research systems can achieve impressive performance using statistical language models trained on large amounts of domain-targeted data, but for many domains sufficient data is not available. Data may be unavailable because the domain has not been explored before, the relevant data may be confid</context>
</contexts>
<marker>Hu, Turin, Brown, 1996</marker>
<rawString>J. Hu, W. Turin, and M.K. Brown. Language modeling with stochastic automata. In Proceedings of the Fourth International Conference on Spoken Language Processing (ICSLP), pages 406–413, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hopcroft</author>
<author>J Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages, and Computation.</title>
<date>1979</date>
<publisher>AddisonWesley,</publisher>
<location>Reading, MA,</location>
<contexts>
<context position="18757" citStr="Hopcroft and Ullman 1979" startWordPosition="3025" endWordPosition="3028">his may tend to fill the beam with multiple hypotheses for the same word sequence, and force other good hypotheses out of the beam, potentially increasing word error rate. This observation appears to be supported in practice. The original form of the PSA grammar allows an average of 1.4 parses per sentence, and while both the K&amp;K and M&amp;G algorigthm increase the level of ambiguity, the K&amp;K algorithm increases much more dramatically. We are investigating techniques to transform a CFG into one weakly equivalent but with less ambiguity. While it is not possible in general to remove all ambiguity (Hopcroft and Ullman 1979) we hope that reducing the amount of ambiguity in the resulting grammar will result in improved recognition performance. 4.1 Grammar Compactor The first technique is actually a combination of three related transformations: Duplicate Nonterminal Elimination – If two nonterminals A and B have exactly the same set of productions then remove the productions for B, and rewrite B as A everywhere it occurs in the grammar. Unit Rule Elimination – If there is only one production for a nonterminal A, and it has a single daughter on its right-hand side then remove the production for A, and rewrite A as e</context>
<context position="23714" citStr="Hopcroft and Ullman (1979)" startWordPosition="3839" endWordPosition="3842">grammar is increased, which can lead to poorer recognition performance. In the earlier implementations, cycles were fortuitously avoided, probably due to the fact that there were more unique nonterminals overall. We expect that these transformations may be effective for some grammars, but not others. We plan to continue to explore refinements to these techiques to prevent them from applying in cases where cycles or leftrecursion may be introduced. 5 Left Recursion Elimination We have used two left-recursion elimination techniques, the traditional one based on Paull’s algorithm, as reported by Hopcroft and Ullman (1979), and one described by Moore (2000)5, based on a technique described by Johnson (1998). Our experience concurs with Moore that the left-corner transform he describes produces a more compact left-recursion free grammar than that of Paull’s algorithm. For the K&amp;K approximation, we were unable to get any grammar to compile through to a working language model using Paull’s algorithm (the models built with Paull’s algorithm caused the recognizer to exceed memory bounds), and only succeeded with Moore’s left-recursion elimination technique. 6 Conclusions We have presented descriptions of two algorit</context>
</contexts>
<marker>Hopcroft, Ullman, 1979</marker>
<rawString>J. Hopcroft and J. Ullman. Introduction to Automata Theory, Languages, and Computation. AddisonWesley, Reading, MA, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iyer</author>
<author>M Ostendorf</author>
</authors>
<title>Transforming out-ofdomain estimates to improve in-domain language models.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th European Conference on Speech Communication and Technology (EuroSpeech97),</booktitle>
<pages>1975--1978</pages>
<contexts>
<context position="1506" citStr="Iyer and Ostendorf 1997" startWordPosition="218" endWordPosition="221">ht not always result in accurate and efficient speech recognition. We will describe and evaluate two approaches to this compilation problem. We will also describe and evaluate additional techniques to reduce the structural ambiguity of the language model. 1 Introduction Language models to constrain speech recognition are a crucial component of interactive spoken language systems. The more varied the language that must be recognized, the more critical good language modeling becomes. Research in language modeling has heavily favored statistical approaches (Cohen 1995, Ward 1995, Hu et al. 1996, Iyer and Ostendorf 1997, Bellegarda 1999, Stolcke and Shriberg 1996) while handcoded finite-state or context-free language models dominate the commercial sector (Nuance 2001, SpeechWorks 2001, TellMe 2001, BeVocal 2001, HeyAnita 2001, W3C 2001). The difference revolves around the availability of data. Research systems can achieve impressive performance using statistical language models trained on large amounts of domain-targeted data, but for many domains sufficient data is not available. Data may be unavailable because the domain has not been explored before, the relevant data may be confidential, or the system may</context>
</contexts>
<marker>Iyer, Ostendorf, 1997</marker>
<rawString>R. Iyer and M. Ostendorf. Transforming out-ofdomain estimates to improve in-domain language models. In Proceedings of the 5th European Conference on Speech Communication and Technology (EuroSpeech97), pages 1975–1978, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Finite-state approximation of constraintbased grammars using left-corner grammar transforms.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>619--623</pages>
<contexts>
<context position="3916" citStr="Johnson 1998" startWordPosition="617" endWordPosition="618">e theoretically straight-forward, this approach is more demanding in practice, as each of the compilation stages contains the potential for a combinatorial explosion that will exceed memory and time bounds. There is also no guarantee that the resulting language model will lead to accurate and efficient speech recognition. We will be interested in this paper in sound approximations (Pereira and Wright 1991) in which the language accepted by the approximation is a superset of language accepted by the original grammar. While we conceed that alternative techniques that are not sound (Black 1989, (Johnson 1998, Rayner and Carter 1996) may still be useful for many purposes, we prefer sound approximations because there is no chance that the correct hypothesis will be eliminated. Thus, further processing techniques (for instance, N-best search) will still have an opportunity to find the optimal solution. We will describe and evaluate two compilation approaches to approximating a typed unification grammar with a context-free grammar. We will also describe and evaluate additional techniques to reduce the size and structural ambiguity of the language model. 2 Typed Unification Grammars Typed Unification </context>
<context position="23800" citStr="Johnson (1998)" startWordPosition="3855" endWordPosition="3856">ions, cycles were fortuitously avoided, probably due to the fact that there were more unique nonterminals overall. We expect that these transformations may be effective for some grammars, but not others. We plan to continue to explore refinements to these techiques to prevent them from applying in cases where cycles or leftrecursion may be introduced. 5 Left Recursion Elimination We have used two left-recursion elimination techniques, the traditional one based on Paull’s algorithm, as reported by Hopcroft and Ullman (1979), and one described by Moore (2000)5, based on a technique described by Johnson (1998). Our experience concurs with Moore that the left-corner transform he describes produces a more compact left-recursion free grammar than that of Paull’s algorithm. For the K&amp;K approximation, we were unable to get any grammar to compile through to a working language model using Paull’s algorithm (the models built with Paull’s algorithm caused the recognizer to exceed memory bounds), and only succeeded with Moore’s left-recursion elimination technique. 6 Conclusions We have presented descriptions of two algorithms for approximating typed unification grammars with context-free grammars, and evalu</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>M. Johnson. Finite-state approximation of constraintbased grammars using left-corner grammar transforms. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, pages 619–623, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kiefer</author>
<author>H Krieger</author>
</authors>
<title>A context-free approximation of head-driven phrase structure grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th International Workshop on Parsing Technologies,</booktitle>
<pages>135--146</pages>
<contexts>
<context position="8314" citStr="Kiefer and Krieger (2000)" startWordPosition="1318" endWordPosition="1321"> l Table 1: Construction of the fixed-point Recognition System Package Figure 1: Compilation Process The GSL representation is then compiled into a Nuance package with the nuance compiler. This package is the input to the speech recognizer. In our experience, each of the compilation stages, as well as speech recognition itself, has the potential to lead to a combinatorial explosion that exceeds practical memory or time bounds. We will now describe implementations of the first stage, generating a context-free grammar from a typed unification grammar, by two different algorithms, one defined by Kiefer and Krieger (2000) and one by Moore and Gawron, described in Moore (1998) The critical difficulty for both of these approaches is how to select the set of derived nonterminals that will appear in the final CFG. 3.1 Kiefer&amp;Krieger’s Algorithm The algorithm of Kiefer&amp;Krieger (K&amp;K) divides this compilation step into two phases: first, the set of context-free nonterminals is determined by iterating a bottom-up search until a least fixedpoint is reached; second, this least fixed-point is used to instantiate the set of context-free productions. The computation of the fixed-point , described in Table 1, proceeds as fo</context>
<context position="10294" citStr="Kiefer and Krieger (2000)" startWordPosition="1651" endWordPosition="1654">o by instantiating every grammar rule r in , the set of grammar rules. The first step in the instantiation is to unify every combination of daughters with all possible feature structures from (FillDaughters, line 10). The rule restrictor is applied to each resulting feature structure (line 11) before it is added to using the operator (line 12), similar to the lexical case. If after checking all rule applications bottom up, no new feature structures have been added to (line 13), then the least fixed-point had been found, and the process terminates. Otherwise, Iterate is called recursively. See Kiefer and Krieger (2000) for proof that this terminates, and finds the appropriate fixed-point. Having computed the least fixed-point , the next step is to compute the set of corresponding CF productions. For each r in , of the form , instantiate the daughters using all combinations of unifiable feature structures from . Context-free productions will be added, where and 3.2 Moore and Gawron’s Algorithm While K&amp;K uses subsumption to generate the set of most-general nonterminals, the algorithm of Moore and Gawron (M&amp;G), described in Moore (1998) attempts to propagate features values both bottom-up and top-down through </context>
</contexts>
<marker>Kiefer, Krieger, 2000</marker>
<rawString>B. Kiefer and H. Krieger. A context-free approximation of head-driven phrase structure grammar. In Proceedings of the 6th International Workshop on Parsing Technologies, pages 135–146, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Lemon</author>
<author>Anne Bracy</author>
<author>Alexander Gruenstein</author>
<author>Stanley Peters</author>
</authors>
<title>A multi-modal dialogue system for human-robot conversation.</title>
<date>2001</date>
<booktitle>In Proceedings of North American Association for Computational Linguistics (NAACL</booktitle>
<contexts>
<context position="2368" citStr="Lemon et al. 2001" startWordPosition="352" endWordPosition="355">s around the availability of data. Research systems can achieve impressive performance using statistical language models trained on large amounts of domain-targeted data, but for many domains sufficient data is not available. Data may be unavailable because the domain has not been explored before, the relevant data may be confidential, or the system may be designed to do new functions for which there is no human-human analog interaction. The statistical approach is unworkable in such cases for both the commercial developers and for some research systems (Moore et al. 1997, Rayner et al. 2000, Lemon et al. 2001, Gauthron and Colineau 1999). Even in cases for which there is no impediment to collecting data, the expense and time required to collect a corpus can be prohibitive. The existence of the ATIS database (Dahl et al. 1994) is no doubt a factor in the popularity of the travel domain among the research community for exactly this reason. A major problem with grammar-based finitestate or context-free language models is that they can be tedious to build and difficult to maintain, as they can become quite large very quickly as the scope of the grammar increases. One way to address this problem is to </context>
</contexts>
<marker>Lemon, Bracy, Gruenstein, Peters, 2001</marker>
<rawString>Oliver Lemon, Anne Bracy, Alexander Gruenstein, and Stanley Peters. A multi-modal dialogue system for human-robot conversation. In Proceedings of North American Association for Computational Linguistics (NAACL 2001), 2001.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Moore</author>
<author>J Dowding</author>
<author>H Bratt</author>
<author>J Gawron</author>
<author>Y Gorfu</author>
<author>A Cheyer</author>
</authors>
<title>CommandTalk: A spoken-language interface for battlefield simulations.</title>
<booktitle>In Proceedings</booktitle>
<marker>Moore, Dowding, Bratt, Gawron, Gorfu, Cheyer, </marker>
<rawString>R. Moore, J. Dowding, H. Bratt, J. Gawron, Y. Gorfu, and A. Cheyer. CommandTalk: A spoken-language interface for battlefield simulations. In Proceedings</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
</authors>
<title>Removing left-recusion from context-free grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of 1st Meeting of the North Americal Chapter of the Associations for Computational Linguistics,</booktitle>
<pages>249--255</pages>
<contexts>
<context position="23749" citStr="Moore (2000)" startWordPosition="3847" endWordPosition="3848">cognition performance. In the earlier implementations, cycles were fortuitously avoided, probably due to the fact that there were more unique nonterminals overall. We expect that these transformations may be effective for some grammars, but not others. We plan to continue to explore refinements to these techiques to prevent them from applying in cases where cycles or leftrecursion may be introduced. 5 Left Recursion Elimination We have used two left-recursion elimination techniques, the traditional one based on Paull’s algorithm, as reported by Hopcroft and Ullman (1979), and one described by Moore (2000)5, based on a technique described by Johnson (1998). Our experience concurs with Moore that the left-corner transform he describes produces a more compact left-recursion free grammar than that of Paull’s algorithm. For the K&amp;K approximation, we were unable to get any grammar to compile through to a working language model using Paull’s algorithm (the models built with Paull’s algorithm caused the recognizer to exceed memory bounds), and only succeeded with Moore’s left-recursion elimination technique. 6 Conclusions We have presented descriptions of two algorithms for approximating typed unifica</context>
</contexts>
<marker>Moore, 2000</marker>
<rawString>R. Moore. Removing left-recusion from context-free grammars. In Proceedings of 1st Meeting of the North Americal Chapter of the Associations for Computational Linguistics, pages 249–255, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>http www nuance com</author>
</authors>
<date>1999</date>
<journal>As of</journal>
<volume>1</volume>
<marker>com, 1999</marker>
<rawString>Nuance. http://www.nuance.com, 1999. As of 1 April 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<contexts>
<context position="4564" citStr="Pollard and Sag 1994" startWordPosition="713" endWordPosition="716">ay still be useful for many purposes, we prefer sound approximations because there is no chance that the correct hypothesis will be eliminated. Thus, further processing techniques (for instance, N-best search) will still have an opportunity to find the optimal solution. We will describe and evaluate two compilation approaches to approximating a typed unification grammar with a context-free grammar. We will also describe and evaluate additional techniques to reduce the size and structural ambiguity of the language model. 2 Typed Unification Grammars Typed Unification Grammars (TUG), like HPSG (Pollard and Sag 1994) and Gemini (Dowding et al. 1993) are a more expressive formalism in which to write formal grammars1. As opposed to atomic nonterminal symbols in a CFG, each nonterminal in a TUG is a complex feature structure (Shieber 1986) where features with values can be attached. For example, the rule: s[] np:[num=N] vp:[num=N] can be considered a shorthand for 2 context free rules (assuming just two values for number): s np singular vp singular s np plural vp plural 1This paper specifically concerns grammars written in the Gemini formalism. However, the basic issues involved in compiling typed unificatio</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>C. Pollard and I. Sag. Head-Driven Phrase Structure Grammar. University of Chicago Press, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>R Wright</author>
</authors>
<title>Finite-state approximation of phrase structure grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Assocationsfor Computational Linguistics,</booktitle>
<pages>246--255</pages>
<contexts>
<context position="3713" citStr="Pereira and Wright 1991" startWordPosition="582" endWordPosition="585">e recognizer. This approach has been used in several systems, CommandTalk (Moore et al. 1997), RIALIST PSA simulator (Rayner et al. 2000), WITAS (Lemon et al. 2001), and SETHIVoice (Gauthron and Colineau 1999). While theoretically straight-forward, this approach is more demanding in practice, as each of the compilation stages contains the potential for a combinatorial explosion that will exceed memory and time bounds. There is also no guarantee that the resulting language model will lead to accurate and efficient speech recognition. We will be interested in this paper in sound approximations (Pereira and Wright 1991) in which the language accepted by the approximation is a superset of language accepted by the original grammar. While we conceed that alternative techniques that are not sound (Black 1989, (Johnson 1998, Rayner and Carter 1996) may still be useful for many purposes, we prefer sound approximations because there is no chance that the correct hypothesis will be eliminated. Thus, further processing techniques (for instance, N-best search) will still have an opportunity to find the optimal solution. We will describe and evaluate two compilation approaches to approximating a typed unification gramm</context>
</contexts>
<marker>Pereira, Wright, 1991</marker>
<rawString>F. Pereira and R. Wright. Finite-state approximation of phrase structure grammars. In Proceedings of the 29th Annual Meeting of the Assocationsfor Computational Linguistics, pages 246–255, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>D M Carter</author>
</authors>
<title>Fast parsing using pruning and grammar specialization.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirty-Fourth Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>223--230</pages>
<location>Santa Cruz, California,</location>
<contexts>
<context position="3941" citStr="Rayner and Carter 1996" startWordPosition="619" endWordPosition="622">y straight-forward, this approach is more demanding in practice, as each of the compilation stages contains the potential for a combinatorial explosion that will exceed memory and time bounds. There is also no guarantee that the resulting language model will lead to accurate and efficient speech recognition. We will be interested in this paper in sound approximations (Pereira and Wright 1991) in which the language accepted by the approximation is a superset of language accepted by the original grammar. While we conceed that alternative techniques that are not sound (Black 1989, (Johnson 1998, Rayner and Carter 1996) may still be useful for many purposes, we prefer sound approximations because there is no chance that the correct hypothesis will be eliminated. Thus, further processing techniques (for instance, N-best search) will still have an opportunity to find the optimal solution. We will describe and evaluate two compilation approaches to approximating a typed unification grammar with a context-free grammar. We will also describe and evaluate additional techniques to reduce the size and structural ambiguity of the language model. 2 Typed Unification Grammars Typed Unification Grammars (TUG), like HPSG</context>
</contexts>
<marker>Rayner, Carter, 1996</marker>
<rawString>M. Rayner and D.M. Carter. Fast parsing using pruning and grammar specialization. In Proceedings of the Thirty-Fourth Annual Meeting of the Association for Computational Linguistics, pages 223–230, Santa Cruz, California, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>B A Hockey</author>
<author>F James</author>
</authors>
<title>A compact architecture for dialogue management based on scripts and meta-outputs.</title>
<date>2000</date>
<booktitle>In Proceedings ofApplied Natural Language Processing (ANLP),</booktitle>
<contexts>
<context position="2349" citStr="Rayner et al. 2000" startWordPosition="348" endWordPosition="351">e difference revolves around the availability of data. Research systems can achieve impressive performance using statistical language models trained on large amounts of domain-targeted data, but for many domains sufficient data is not available. Data may be unavailable because the domain has not been explored before, the relevant data may be confidential, or the system may be designed to do new functions for which there is no human-human analog interaction. The statistical approach is unworkable in such cases for both the commercial developers and for some research systems (Moore et al. 1997, Rayner et al. 2000, Lemon et al. 2001, Gauthron and Colineau 1999). Even in cases for which there is no impediment to collecting data, the expense and time required to collect a corpus can be prohibitive. The existence of the ATIS database (Dahl et al. 1994) is no doubt a factor in the popularity of the travel domain among the research community for exactly this reason. A major problem with grammar-based finitestate or context-free language models is that they can be tedious to build and difficult to maintain, as they can become quite large very quickly as the scope of the grammar increases. One way to address </context>
</contexts>
<marker>Rayner, Hockey, James, 2000</marker>
<rawString>M. Rayner, B.A. Hockey, and F. James. A compact architecture for dialogue management based on scripts and meta-outputs. In Proceedings ofApplied Natural Language Processing (ANLP), 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
</authors>
<title>Using restriction to extend parsing algorithms for complex-feature-based formalisms.</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd Annual Meeting of the Assocations for Computational Linguistics,</booktitle>
<pages>145--152</pages>
<contexts>
<context position="5836" citStr="Shieber 1985" startWordPosition="926" endWordPosition="927">formalisms. This expressiveness allows us to write grammars with a small number of rules (from dozens to a few hundred) that correspond to grammars with large numbers of CF rules. Note that the approximation need not incorporate all of the features from the original grammar in order to provide a sound approximation. In particular, in order to derive a finite CF grammar, we will need to consider only those features that have a finite number of possible values, or at least consider only finitely many of the possible values for infinitely valued features. We can use the technique of restriction (Shieber 1985) to remove these features from our feature structures. Removing these features may give us a more permissive language model, but it will still be a sound approximation. The experimental results reported in this paper are based on a grammar under development at RIACS for a spoken dialogue interface to a semi-autonomous robot, the Personal Satellite Assistant (PSA). We consider this grammar to be medium-sized, with 61 grammar rules and 424 lexical entries. While this may sound small, if the grammar were expanded by instantiating variables in all legal permutations, it would contain over context-</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>S. Shieber. Using restriction to extend parsing algorithms for complex-feature-based formalisms. In Proceedings of the 23rd Annual Meeting of the Assocations for Computational Linguistics, pages 145–152, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
</authors>
<title>An Introduction to Unification-based Approaches to Grammar.</title>
<date>1986</date>
<booktitle>CLSI Lecture Notes no. 4. Center for the Study of Language and Information,</booktitle>
<institution>University of Chicago Press).</institution>
<note>distributed by the</note>
<contexts>
<context position="4788" citStr="Shieber 1986" startWordPosition="754" endWordPosition="755">ortunity to find the optimal solution. We will describe and evaluate two compilation approaches to approximating a typed unification grammar with a context-free grammar. We will also describe and evaluate additional techniques to reduce the size and structural ambiguity of the language model. 2 Typed Unification Grammars Typed Unification Grammars (TUG), like HPSG (Pollard and Sag 1994) and Gemini (Dowding et al. 1993) are a more expressive formalism in which to write formal grammars1. As opposed to atomic nonterminal symbols in a CFG, each nonterminal in a TUG is a complex feature structure (Shieber 1986) where features with values can be attached. For example, the rule: s[] np:[num=N] vp:[num=N] can be considered a shorthand for 2 context free rules (assuming just two values for number): s np singular vp singular s np plural vp plural 1This paper specifically concerns grammars written in the Gemini formalism. However, the basic issues involved in compiling typed unification grammars to context-free grammars remain the same across formalisms. This expressiveness allows us to write grammars with a small number of rules (from dozens to a few hundred) that correspond to grammars with large number</context>
</contexts>
<marker>Shieber, 1986</marker>
<rawString>S. Shieber. An Introduction to Unification-based Approaches to Grammar. CLSI Lecture Notes no. 4. Center for the Study of Language and Information, 1986. (distributed by the University of Chicago Press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>http www speechworks com</author>
</authors>
<date>2001</date>
<journal>As of</journal>
<volume>31</volume>
<marker>com, 2001</marker>
<rawString>SpeechWorks. http://www.speechworks.com, 2001. As of 31 January 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>E Shriberg</author>
</authors>
<title>Statistical language modeling for speech disfluencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>405--408</pages>
<contexts>
<context position="1551" citStr="Stolcke and Shriberg 1996" startWordPosition="224" endWordPosition="227">ient speech recognition. We will describe and evaluate two approaches to this compilation problem. We will also describe and evaluate additional techniques to reduce the structural ambiguity of the language model. 1 Introduction Language models to constrain speech recognition are a crucial component of interactive spoken language systems. The more varied the language that must be recognized, the more critical good language modeling becomes. Research in language modeling has heavily favored statistical approaches (Cohen 1995, Ward 1995, Hu et al. 1996, Iyer and Ostendorf 1997, Bellegarda 1999, Stolcke and Shriberg 1996) while handcoded finite-state or context-free language models dominate the commercial sector (Nuance 2001, SpeechWorks 2001, TellMe 2001, BeVocal 2001, HeyAnita 2001, W3C 2001). The difference revolves around the availability of data. Research systems can achieve impressive performance using statistical language models trained on large amounts of domain-targeted data, but for many domains sufficient data is not available. Data may be unavailable because the domain has not been explored before, the relevant data may be confidential, or the system may be designed to do new functions for which th</context>
</contexts>
<marker>Stolcke, Shriberg, 1996</marker>
<rawString>A. Stolcke and E. Shriberg. Statistical language modeling for speech disfluencies. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 405–408, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>http www tellme com</author>
</authors>
<date>2001</date>
<journal>As of</journal>
<volume>31</volume>
<marker>com, 2001</marker>
<rawString>TellMe. http://www.tellme.com, 2001. As of 31 January 2001.</rawString>
</citation>
<citation valid="false">
<authors>
<author>World Wide</author>
</authors>
<title>Web Consortium (W3C). Speech Recognition Grammar Specification for the W3C Speech Interface Framework.</title>
<marker>Wide, </marker>
<rawString>World Wide Web Consortium (W3C). Speech Recognition Grammar Specification for the W3C Speech Interface Framework.</rawString>
</citation>
<citation valid="true">
<authors>
<author>http www w3 orgTRspeech-grammar</author>
</authors>
<date>2001</date>
<journal>As of</journal>
<volume>3</volume>
<marker>orgTRspeech-grammar, 2001</marker>
<rawString>http://www.w3.org/TR/speech-grammar, 2001. As of 3 January 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Ward</author>
<author>S Issar</author>
</authors>
<title>The cmu atis system.</title>
<date>1995</date>
<booktitle>In Spoken Language System Technology Workshop,</booktitle>
<pages>249--251</pages>
<marker>Ward, Issar, 1995</marker>
<rawString>W. Ward and S. Issar. The cmu atis system. In Spoken Language System Technology Workshop, pages 249–251, 1995.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>