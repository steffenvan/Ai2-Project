<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007852">
<note confidence="0.71502">
Proceedings of EACL &apos;99
</note>
<title confidence="0.988199">
Parsing with an Extended Domain of Locality
</title>
<author confidence="0.999786">
John Carroll, Nicolas Nicolov, Olga Shaumyan, Martine Smets Sz David Weir
</author>
<affiliation confidence="0.9959705">
School of Cognitive and Computing Sciences
University of Sussex
</affiliation>
<address confidence="0.845063">
Brighton, BN1 9QH, UK
</address>
<sectionHeader confidence="0.990012" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998723">
One of the claimed benefits of Tree Ad-
joining Grammars is that they have an
extended domain of locality (ED0L). We
consider how this can be exploited to
limit the need for feature structure uni-
fication during parsing. We compare
two wide-coverage lexicalized grammars
of English, LEXSYS and XTAG, finding
that the two grammars exploit EDOL in
different ways.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996419347826087">
One of the most basic properties of Tree Adjoining
Grammars (TAGS) is that they have an extended
domain of locality (ED0L) (Joshi, 1994). This
refers to the fact that the elementary trees that
make up the grammar are larger than the cor-
responding units (the productions) that are used
in phrase-structure rule-based frameworks. The
claim is that in Lexicalized TAGs (LTAGs) the el-
ementary trees provide a domain of locality large
enough to state co-occurrence relationships be-
tween a lexical item (the anchor of the elemen-
tary tree) and the nodes it imposes constraints
on. We will call this the extended domain of
locality hypothesis.
For example, wh-movement can be expressed
locally in a tree that will be anchored by a verb
of which an argument is extracted. Consequently,
features which are shared by the extraction site
and the wh-word, such as case, do not need to be
percolated, but are directly identified in the tree.
Figure 1 shows a tree in which the case feature
at the extraction site and the wh-word share the
same value.&apos;
&apos;The anchor, substitution and foot nodes of trees
are marked with the symbols o, 1. and *, respectively.
Words in parenthesis are included in trees to provide
examples of strings this tree can derive.
Much of the research on TAGS can be seen as
illustrating how its EDOL can be exploited in vari-
ous ways. However, to date, only indirect evidence
has been given regarding the beneficial effects of
the EDOL on parsing efficiency. The argument,
due to Schabes (1990), is that benefits to parsing
arise from lexicalization, and that lexicalization is
only possible because of the EDOL. A parser deal-
ing with a lexicalized grammar needs to consider
only those elementary structures that can be as-
sociated with the lexical items appearing in the
input. This can substantially reduce the effective
grammar size at parse time. The argument that
an EDOL is required for lexicalization is based on
the observation that not every set of trees that
can be generated by a CFG can be generated by
a lexicalized CFG. But does the EDOL have any
other more direct effects on parsing efficiency?
On the one hand, it is a consequence of the
EDOL that wide-coverage crAcs are larger than
their rule-based counterparts. With larger ele-
mentary structures, generalizations are lost re-
garding the internal structure of the elementary
trees. Since parse time depends on grammar size,
this could have an adverse effect on parsing effi-
ciency. However, the problem of grammar size in
TAG has to some extent been addressed both with
respect to grammar encoding (Evans et al., 1995;
Candito, 1996) and parsing (Joshi and Srinivas,
1994; Evans and Weir, 1998).
On the other hand, if the EDOL hypothesis holds
for those dependencies that are being checked by
the parser, then the burden of passing feature val-
ues around during parsing will be less than in a
rule-based framework. If all dependencies that
the parser is checking can be stated directly within
the elementary structures of the grammar, they
do not need to be computed dynamically during
the parsing process by means of feature percola-
tion. For example, there is no need to use a slash
feature to establish filler-gap dependencies over
unbounded distances across the tree if the EDOL
</bodyText>
<page confidence="0.995452">
217
</page>
<figure confidence="0.988846">
Proceedings of EACL &apos;99
NP [case acc]
(whom)
NP VP
(you)
o V NP [case . acc]
(see)
</figure>
<figureCaption confidence="0.999651">
Figure 1: Localizing a filler-gap dependency
</figureCaption>
<bodyText confidence="0.999747545454545">
makes it possible for the gap and its filler to be
located within the same elementary structure.
This paper presents an investigation into the ex-
tent to which the EDOL reduces the need for fea-
ture passing in two existing wide-coverage gram-
mars: the XTAG grammar (XTAG-Group, 1995),
and the LEXSYS grammar (Carroll et al., 1998).
It can be seen as an evaluation of how well these
two grammars make use of the EDOL hypothesis
with respect to those dependencies that are being
checked by the parser.
</bodyText>
<sectionHeader confidence="0.963913" genericHeader="method">
2 Parsing Unification-Based
Grammars
</sectionHeader>
<bodyText confidence="0.999981919354839">
In phrase-structure rule-based parsing, each rule
corresponds to a local tree. A rule is applied to
a sequence of existing contiguous constituents, if
they are compatible with the daughters. In the
case of context-free grammar (cFG), the compat-
ibility check is just equality of atomic symbols,
and an instantiated daughter is merely the corre-
sponding sub-constituent.
However, unification-based extensions of
phrase-structures grammars are used because
they are able to encode local and non-local
syntactic dependencies (for example, subject-
verb agreement in English) with re-entrant
features and feature percolation, respectively.
Constituents are represented by DAGS (directed
acyclic graphs), the compatibility check is unifi-
cation, and it is the result of each unification that
is used to instantiate the daughters. Graph uni-
fication based on the UNION-FIND algorithm has
time complexity that is near-linear in the number
of feature structure nodes in the inputs (Huet,
1975; Ait-Kaci, 1984); however, feature structures
in wide-coverage grammars can contain hundreds
of nodes (see e.g., HPSG (Pollard and Sag, 1994)),
and since unification is a primitive operation the
overall number of unification attempts during
parsing can be very large. Unification therefore
has a substantial practical cost.
Efficient graph unification must also ensure that
it does not destructively modify the input struc-
tures, since the same rule may be used several
times within a single derivation, and also the
same constituent may be used within different
partial analyses with features instantiated in dif-
ferent ways. Copying input feature structures
in their entirety before each unification would
solve this problem, but the space usage renders
this approach impractical. Therefore, various
&apos;quasi-destructive&apos; algorithms (Karttunen, 1986;
Kogure, 1990; Tomabechi, 1991) and algorithms
using &apos;skeletal&apos; DAGS with updates (Pereira, 1985;
Emele, 1991) have been proposed, which at-
tempt to minimize copying. But even with good
implementations of the best of these improved
algorithms, parsers designed for wide-coverage
unification-based phrase-structure grammars us-
ing large HPsG-style feature graphs spend around
85-90% of their time unifying and copying feature
structures (Tomabechi, 1991), and may allocate in
the region of 1-2 Mbytes memory while parsing
sentences of only eight words or so (Flickinger,
p.c.). Although comfortably within main mem-
ory capacities of current workstations, such large
amounts of short-term storage allocation overflow
CPU caches, and storage management overheads
become significant.
In the case of unification-based LTAG the situa-
tion is even more problematic. Elementary struc-
tures are larger than productions, and the poten-
tial is that the parser will have to make copies
of entire trees and associated feature structures.
Furthermore, the number of trees that an LTAG
</bodyText>
<page confidence="0.985333">
218
</page>
<figure confidence="0.751184333333333">
Proceedings of EACL &apos;99
4. NP[agr: ij VP 4. Np[agr: 3sg 1 VP
case : nom
*V [agr : 3sg] 4, NP
1
loves
</figure>
<figureCaption confidence="0.998846">
Figure 2: Unanchored and anchored trees localizing subject/verb agreement
</figureCaption>
<bodyText confidence="0.995392184615385">
oV[agr:Ii] ,J,NP
parser must consider tends to be far larger than
the number of rules in a corresponding phrase-
structure grammar. On the other hand, the EDOL
has the potential to eliminate some or all of feature
percolation, and in the remainder of this section,
we explain how.
An LTAG consists of a set of unanchored trees
such as the one shown on the left of Figure 2.
This shows a tree for transitive verbs where sub-
ject/verb agreement is captured directly with re-
entrancy between the value of agr feature struc-
tures at the anchor (verb) node and the subject
node. Notice the re-entrancy between the anchor
node and the substitution node for the subject.
However these are not the trees that the parser
works with; the parser is given trees that have
been anchored by the morphologically analysed
words in the input sentence. For example, the tree
shown on the right is the result of anchoring the
tree shown on the left with the word loves. An-
choring instantiates the agr feature of the anchor
node as 3sg which has the effect (due to the re-
entrancy in the unanchored tree) of instantiating
the agr feature at the subject node as 3sg.
Anchored elementary trees are translated by the
parser into a sequence of what we will refer to
as parser actions. For example, once the tree
shown on the right of Figure 2 has been asso-
ciated with the word loves in the input, it can
be recognized with a sequence of parser actions
that involve finding a NP constituent on the right
(corresponding to the object), possibly perform-
ing adjunctions at the VP node, and then finding
another NP constituent on the left (corresponding
to the subject). We say that the two NP substitu-
tion nodes and the VP node are the sites of parser
actions in this tree. Problems arise, and the EDOL
hypothesis is violated, when there is a dependence
between different parser actions.
The EDOL hypothesis states that elementary
trees provide a domain of locality large enough to
state co-occurrence relationship between the an-
chor of the tree and the nodes it imposes con-
straints on. If all dependencies relevant to the
parser can be captured in this way then, once an
elementary tree has been anchored by a particu-
lar lexical item, the settings of feature values at
all of the dependent nodes will have been fixed,
and no feature percolation can occur. Each uni-
fication is a purely local operation with no reper-
cussions on the rest of the parsing process. No
copying of feature structures is required, so mem-
ory usage is greatly reduced, and complex quasi-
destructive algorithms with their associated com-
putational overheads can be dispensed with.
Note that, although feature percolation is elim-
inated when the EDOL hypothesis holds, the fea-
ture structure at a node can still change. For ex-
ample, substituting a tree for a proper noun at
the subject position of the tree in Figure 2 would
cause the .feature structure at the node for the
subject to include pn:+. This, however, does not
violate the EDOL hypothesis since this feature is
not coreferenced with any other feature in the tree.
</bodyText>
<sectionHeader confidence="0.546763" genericHeader="method">
3 Analysis of two wide-coverage
grammars
</sectionHeader>
<bodyText confidence="0.999918857142857">
As we have seen, the EDOL of LTAGS makes it pos-
sible, at least in principle, to locally express de-
pendencies which cannot be localized in a CFG-
based formalism. In this section we consider two
existing grammars: the XTAG grammar, a wide-
coverage LTAG, and the LEXSYS grammar, a wide-
coverage D-Tree Substitution Grammar (Rambow
et al., 1995). For each grammar we investigate the
extend to which they do not take full advantage
of the EDOL and require percolation of features at
parse time.
There are a number of instances in which depen-
dencies are not localized in the XTAG grammar,
most of which involve auxiliary trees. There are
</bodyText>
<page confidence="0.994991">
219
</page>
<bodyText confidence="0.978456071428571">
Proceedings of EACL &apos;99
three types of auxiliary trees: predicative, modi-
fier and coordination auxiliary trees. In predica-
tive auxiliary trees the anchor is also the head of
the tree and becomes the head of the tree resulting
from the adjunction. In modifier auxiliary trees,
the anchor is not the head of the tree, and the sub-
tree headed by the anchor usually plays a role of
adjunct in the resulting tree. Coordination auxil-
iary trees are similar to modifier auxiliary trees in
that they are anchored by the conjunction which
is not the head of the phrase. One of the conjoined
nodes is a foot node, the other one a substitution
node.
</bodyText>
<subsectionHeader confidence="0.998155">
3.1 Modifier Auxiliary Trees
</subsectionHeader>
<bodyText confidence="0.999925333333333">
In modifier auxiliary trees — an example of which
is shown in Figure 33 — the feature values at the
root and foot nodes are set by the node at which
the auxiliary tree is adjoined, and have to be per-
colated between the foot node and the root node.
The LEXSYS grammar adopts a similar account of
modification.
From a parsing point of view, this does not re-
sult in the need for feature percolation: only the
foot node of the modifier tree is the site of a parser
action, and the root node is ignored by the process
that interprets the tree for the parser.
</bodyText>
<subsectionHeader confidence="0.999789">
3.2 Coordination Auxiliary Trees
</subsectionHeader>
<bodyText confidence="0.999987681818182">
An example of an XTAG coordination auxiliary
tree is shown on the left of Figure 4. This case is
different from the modification case since features
of the substitution node have to be identical to fea-
tures of the foot node (which will match those at
the adjunction site). From a parsing point of view
these nodes are both the sites of actions, resulting
in the need for feature percolation. For example,
for the NP coordination tree shown in Figure 4,
if one of the conjuncts is a wh-phrase, the other
conjunct must be a wh-phrase too, as in who or
what did this? but *John and who did this? The
wh-feature has to be percolated between the two
nodes on each side of the conjunction.
In the LEXSYS grammar, a coordination tree is
anchored by a head of the tree, not by the con-
junction. To illustrate (see the tree on the right
of Figure 4), NP-coordination trees are anchored
by a noun, and features such as wh and case are
ground during anchoring. As a result, there is no
need for passing of these features in the coordina-
tion trees of the LEXSYS grammar.
</bodyText>
<footnote confidence="0.905824">
2All examples relating to the XTAG grammar come
from the XTAG report (XTAG-Group, 1995). They
have been simplified to the extent that only details
relevant to the discussion are included.
</footnote>
<bodyText confidence="0.999925625">
As for agreement features, there are two cases
to consider: if the conjunction is and, the number
feature of the whole phrase is plural; if the con-
junction is or, the number feature is the same as
the last conjunct&apos;s (XTAG-Group, 1999). In both
the XTAG and LEXSYS grammars, this is achieved
by having separate trees for each type of conjunc-
tion.
</bodyText>
<subsectionHeader confidence="0.99922">
3.3 Predicative Auxiliary Trees
</subsectionHeader>
<bodyText confidence="0.999993195121951">
In the XTAG grammar, subject raising and aux-
iliary verbs anchor auxiliary trees rooted in VP,
without a subject3; they can be adjoined at the
VP node of any compatible verb tree. With this
arrangement, subject-verb agreement must be es-
tablished dynamically. The agr feature of the NP
subject must match the agr feature of whichever
VP ends up being highest at the end of the deriva-
tion. In Figure 5, the bought tree has been an-
chored in such a way that adjunction at the VP
node is obligatory, since a matrix clause cannot
have mode:ppart.4 When the tree for has is ad-
joined at the VP node the agr features of the sub-
ject will agree with those of bought. The feature
structure at the root of the tree for has is unified
with the upper feature structure at the VP node
of the tree for bought, and the feature structure
at the foot of the tree for has is unified with the
lower feature structure at the VP node of the tree
for bought. The foot node of the has tree is the VP
node on the frontier of the tree. Note that even
after the tree has been anchored, re-entrancy of
features occurs in the tree.
Thus, there are two sites in the tree for bought
(the subject NP node and the VP node) at which
parser actions will take place (substitution and
adjunction, respectively) such that a dependency
between the values of the features at these two
nodes must be established by the parser.
The situation is similar for case assignment
(also shown in the Figure 5): the value of a fea-
ture ass-case (the assign case feature) on the high-
est VP is coreferred with the value of the feature
case on the subject NP. For finite verbs, the value
of the feature ass-case is determined by the mode
of the verb. For infinitive verbs, case is assigned
in various ways, the details of which are not rele-
vant to the discussion here. The subject is in the
nominative case if the verb is finite, and in the
accusative otherwise. As with the agr feature, the
value of the case feature cannot be instantiated
</bodyText>
<footnote confidence="0.9973058">
3To allow for long distance dependency, subject
raising verbs must anchor an auxiliary tree, with iden-
tical root and foot nodes, a VP.
4Unifying the two feature structures at the VP node
would cause a matrix clause to have mode:ppart.
</footnote>
<page confidence="0.995291">
220
</page>
<figure confidence="0.9797565">
Proceedings of EACL &apos;99
ltl
o Adj * N
red
</figure>
<figureCaption confidence="0.993851">
Figure 3: XTAG example of modifying auxiliary tree
</figureCaption>
<figure confidence="0.9967495625">
NPI_ &apos;clahse nom/acc]
Lcase :
NP rwh
NP rwh —
&apos;case : nom/acci NP
[case:
NPI wh
Conj NP
* Np [wh :
case : o Con j
N
and
o N
—
icase :nom/acci
apples
</figure>
<figureCaption confidence="0.999993">
Figure 4: Coordination in XTAG (left) and LEXSYS (right)
</figureCaption>
<bodyText confidence="0.999738102040817">
in the anchored elementary tree of the main verb
because auxiliary verb trees can be adjoined.
The same observations apply to the XTAG treat-
ment of copula with predicative categories such as
an adjective. As shown in Figure 6, these pred-
icative AP trees have a subject but no verb; trees
for raising verbs or the copula can be adjoined
into them. As in the previous example, the agr
features of the verb and subject cannot be instan-
tiated in the elementary tree because the verb and
its subject are not present in the same tree.
From the examples we have seen, it appears that
the XTAG grammar does not take full advantage
of the EDOL with respect to a number of syntactic
features, for example those relating to agreement
and case. The LEXSYS grammar takes a rather dif-
ferent approach to phenomena that XTAG handles
with predicative auxiliary trees.
The LEXSYS grammar has been designed to lo-
calize syntactic dependencies in elementary trees.
As in the XTAG grammar, unbounded dependen-
cies between gap and filler are localized in elemen-
tary trees; but unlike the XTAG grammar, other
types of syntactic dependencies, such as agree-
ment, are also localized. All finite verbs, including
auxiliary and raising verbs, anchor a tree rooted
in S, and thus are in the same tree as the subject
with which they agree. An example involving fi-
nite verbs is shown in Figure 7. Since verb trees
cannot be substituted between the subject and the
verb, the agr feature can be grounded when ele-
mentary trees are anchored, rather than during
the derivation. The case feature of the subject
can be specified even in the unanchored elemen-
tary tree: in trees for finite verbs the subject has
nominative case; in trees for for . . . to clauses it
has accusative case.
As can be seen from the tree on the right of
Figure 7, subject raising and auxiliary verbs are
rooted in S and take a VP complement. So the
sentence He seems to like apples is produced by
substituting a VP-rooted tree for to like into a tree
for seems.
Thus, for all three trees shown in Figure 7,
once anchoring has taken place, all of the syn-
tactic features being checked by the parser are
grounded. Hence, the parser does not have to
check for dependencies between the parser actions
taking place at different sites in the tree.
</bodyText>
<subsectionHeader confidence="0.996059">
3.4 Semantic Dependencies
</subsectionHeader>
<bodyText confidence="0.999894">
There are many examples where the XTAG gram-
mar, but not the LEXSYS grammar, localizes se-
mantic dependencies: for example, dependencies
</bodyText>
<page confidence="0.993846">
221
</page>
<figure confidence="0.994071736842105">
Proceedings of EACL &apos;99
[mode : ppart]
ass-case :
NP agr
case : VI&apos; mode :
[
o V [mode : ppart]
bought
.... agr : 3sg 1
VP ass-case : nom
mode : ind
..c----------------
[
, , agr : 3sg
V ass-case : nom .* VP [mode : ppart]
1 mode : ind
S[mode:
[agr :
NP has
</figure>
<figureCaption confidence="0.995748">
Figure 5: XTAG example with a raising verb
</figureCaption>
<figure confidence="0.99804475">
S [mode : n]
NP vp [mode :
: nom]
VP [mode : ind]
o V [mode: ind] * VP V AP
o Adv
upset
is
</figure>
<figureCaption confidence="0.85047475">
Figure 6: XTAG example of a predicative adjective
between an adjective and its subject. As shown in
Figure 6, in XTAG the predicative adjective and its
subject are localized in the same elementary tree,
</figureCaption>
<bodyText confidence="0.975090875">
and selectional restrictions can be locally imposed
by the adjective on the subject without the need
for feature percolation. On the other hand, in the
LEXSYS grammar, the dependency between upset
and he in he looks upset could not be checked dur-
ing parsing without the use of feature passing be-
tween the subject and AP node of the tree in the
middle of Figure 7.
</bodyText>
<subsectionHeader confidence="0.999361">
3.5 Percolation of Features in LEXSYS
</subsectionHeader>
<bodyText confidence="0.999987483870968">
This section considers a limited number of cases
where it appears that it is not possible to set
all syntactic features by anchoring an elementary
tree.
When two nodes other than the anchor of the
tree are syntactically dependent, feature values
may have to be percolated between these nodes
(the anchor does not determine the value of these
features). For example, in English adjectives that
can have S subjects determine the verb form of the
subject. Hence, in Figure 8, the verb form feature
of the subject is not determined by the anchor
of the tree (the verb) but by the complement of
the anchor (the adjective). The verb form feature
must therefore be percolated from the adjective
phrase to the subject.
The XTAG grammar localizes this dependency
(see Figure 6). However, as we have seen, agree-
ment features are not localized in this analysis.
The problem then is that it does not seem to be
possible to localize all syntactic features in this
case.
Feature percolation is also required in the
LEXSYS grammar for prepositional phrases which
contain a wh-word, because the value of the wh
feature is not set by the anchor of the phrase (the
preposition) but by the complement (as in these
reports, the wording on the covers of which has
caused so much controversy, are to be destroyed5).
The value of the feature wh is set by the NP-
complement, and percolated to the root of the PP.
</bodyText>
<sectionHeader confidence="0.997637" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.6973925">
In XTAG both syntactic and semantic features are
considered during parsing, whereas in the LEXSYS
</bodyText>
<footnote confidence="0.524985">
&apos;Example borrowed from Gazdar et al. 1985.
</footnote>
<page confidence="0.985294">
222
</page>
<figure confidence="0.9540102">
Proceedings of EACL &apos;99
N p {agr : 3sg1 VP N p [agr : 3sg 1 VP Np {agr : .3sg - VP
case : nom case : nom case . nom
0 V agr 3sg oV [ai3sf]4AP c&apos;V[ag3sf[mode : ind Zce.ad mLe-ndi 1, VP {mode
likes looks seems
</figure>
<figureCaption confidence="0.998077">
Figure 7: LEXSYS example for case and agr features
</figureCaption>
<figure confidence="0.976859">
S [mode : El] VP
o V [agr 3sg AP [subi : mode :
mode : ind El
looks
</figure>
<figureCaption confidence="0.999959">
Figure 8: LEXSYS example for subject/adjective syntactic dependency
</figureCaption>
<bodyText confidence="0.99997247826087">
system only syntactic dependencies are considered
during parsing; semantic dependencies are left
for a later processing phase. The LEXSYS parser
returns a complete set of all syntactically well-
formed derivations. Semantic information can
be recovered from derivation trees and then pro-
cessed as desired.
From a processing point of view, the XTAG and
LEXSYS grammars are examples that show that
the checking of dependencies involves a trade-off.6
On the one hand, a greater number of parses may
be returned if the only dependencies checked are
syntactic, since possible violations of semantic de-
pendencies are ignored. On the other hand, as
we have seen in this paper, there are potentially
substantial benefits to parsing efficiency if all de-
pendencies that the parser is checking can be lo-
calized with the EDOL. It is too early to say how
best to make the trade-off, but by comparing the
way that the XTAG and LExsys grammars exploit
the EDOL, we hope to have shed some light on
the role that the EDOL can play with respect to
parsing efficiency.
</bodyText>
<footnote confidence="0.983809666666667">
6These are both grammars for English. Hence,
whether the conclusions we draw apply to other lan-
guages is outside the scope of the present work.
</footnote>
<sectionHeader confidence="0.988349" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999056">
This work is supported by UK EPSRC project
GR/K97400 and by an EPSRC Advanced Fellow-
ship to the first author. We would like to thank
Roger Evans, Gerald Gazdar &amp; K. Vijay-Shanker
for helpful discussions.
</bodyText>
<sectionHeader confidence="0.997317" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997889352941177">
Hassan Ait-Kaci. 1984. A Lattice Theoretic Ap-
proach to Computation Based on a Calculus of
Partially Ordered Type Structures. Ph.D. the-
sis, Department of Computer and Information
Science, University of Pennsylvania, Philadel-
phia, PA.
Marie-Hélène Candito. 1996. A principle-
based hierarchical representation of LTAGs.
In Proceedings of the 16th International Con-
ference on Computational Linguistics, Copen-
hagen, Denmark, August.
John Carroll, Nicolas Nicolov, Olga Shaumyan,
Martine Smets, and David Weir. 1998. The
LEXSYS Project. In Proceedings of the
Fourth International Workshop on Tree Adjoin-
ing Grammars and Related Frameworks, pages
29-33.
</reference>
<page confidence="0.984991">
223
</page>
<reference confidence="0.999543102941177">
Proceedings of EACL &apos;99
Martin Emele. 1991. Unification with lazy non-
redundant copying. In Proceedings of the 29th
Meeting of the Association for Computational
Linguistics, pages 323-330, Berkeley, CA.
Roger Evans and David Weir. 1998. A structure-
sharing parser for lexicalized grammars. In Pro-
ceedings of the 36th Meeting of the Association
for Computational Linguistics and the 17th In-
ternational Conference on Computational Lin-
guistics, pages 372-378.
Roger Evans, Gerald Gazdar, and David Weir.
1995. Encoding lexicalized Tree Adjoining
Grammars with a nonmonotonic inheritance hi-
erarchy. In Proceedings of the 33rd Meeting of
the Association for Computational Linguistics,
pages 77-84.
G. P. Huet. 1975. A unification algorithm for
typed .\-calculus. Theoretical Computer Sci-
ence, 1:27-57.
Aravind Joshi and B. Srinivas. 1994. Disambigua-
tion of super parts of speech (or supertags): Al-
most parsing. In Proceedings of the 15th Inter-
national Conference on Computational Linguis-
tics, pages 154-160.
Aravind Joshi. 1994. Preface to special issue on
Tree-Adjoining Grammars. Computational In-
telligence, 10(4):vii-xv.
Lauri Karttunen. 1986. D-PATR: A development
environment for unification-based grammars.
In Proceedings of the 11th International Confer-
ence on Computational Linguistics, pages 74-
80, Bonn, Germany.
Kiyoshi Kogure. 1990. Strategic lazy incremen-
tal copy graph unification. In Proceedings of
the 13th International Conference on Compu-
tational Linguistics, pages 223-228, Helsinki.
Fernando Pereira. 1985. A structure-sharing rep-
resentation for unification-based grammar for-
malisms. In Proceedings of the 23rd Meeting of
the Association for Computational Linguistics,
pages 137-144.
Carl Pollard and Ivan Sag. 1994. Head-Driven
Phrase Structure Grammar. University of
Chicago Press, Chicago.
Owen Rambow, K. Vijay-Shanker, and David
Weir. 1995. D-Tree Grammars. In Proceed-
ings of the 33rd Meeting of the Association for
Computational Linguistics, pages 151-158.
Yves Schabes. 1990. Mathematical and Computa-
tional Aspects of Lexicalized Grammars. Ph.D.
thesis, Department of Computer and Informa-
tion Science, University of Pennsylvania.
Hideto Tomabechi. 1991. Quasi-destructive
graph unification. In Proceedings of the 29th
Meeting of the Association for Computational
Linguistics, pages 315-322, Berkeley, CA.
The XTAG-Group. 1995. A lexicalized Tree Ad-
joining Grammar for English. Technical Report
IRCS Report 95-03, The Institute for Research
in Cognitive Science, University of Pennsylva-
nia.
The XTAG-Group. 1999. A lexicalized Tree
Adjoining Grammar for English. Technical
Report http : I/www. cis . upenn . eduf-xtag/
tech-report/tech-report.html, The In-
stitute for Research in Cognitive Science,
University of Pennsylvania.
</reference>
<page confidence="0.998714">
224
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.767374">
<note confidence="0.794407">Proceedings of EACL &apos;99</note>
<title confidence="0.991396">Parsing with an Extended Domain of Locality</title>
<author confidence="0.999965">John Carroll</author>
<author confidence="0.999965">Nicolas Nicolov</author>
<author confidence="0.999965">Olga Shaumyan</author>
<author confidence="0.999965">Martine Smets Sz David Weir</author>
<affiliation confidence="0.9998855">School of Cognitive and Computing Sciences University of Sussex</affiliation>
<address confidence="0.99264">9QH, UK</address>
<abstract confidence="0.998305636363636">One of the claimed benefits of Tree Adjoining Grammars is that they have an extended domain of locality (ED0L). We consider how this can be exploited to limit the need for feature structure unification during parsing. We compare two wide-coverage lexicalized grammars English, the two grammars exploit different ways.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hassan Ait-Kaci</author>
</authors>
<title>A Lattice Theoretic Approach to Computation Based on a Calculus of Partially Ordered Type Structures.</title>
<date>1984</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="5526" citStr="Ait-Kaci, 1984" startWordPosition="907" endWordPosition="908">fication-based extensions of phrase-structures grammars are used because they are able to encode local and non-local syntactic dependencies (for example, subjectverb agreement in English) with re-entrant features and feature percolation, respectively. Constituents are represented by DAGS (directed acyclic graphs), the compatibility check is unification, and it is the result of each unification that is used to instantiate the daughters. Graph unification based on the UNION-FIND algorithm has time complexity that is near-linear in the number of feature structure nodes in the inputs (Huet, 1975; Ait-Kaci, 1984); however, feature structures in wide-coverage grammars can contain hundreds of nodes (see e.g., HPSG (Pollard and Sag, 1994)), and since unification is a primitive operation the overall number of unification attempts during parsing can be very large. Unification therefore has a substantial practical cost. Efficient graph unification must also ensure that it does not destructively modify the input structures, since the same rule may be used several times within a single derivation, and also the same constituent may be used within different partial analyses with features instantiated in differe</context>
</contexts>
<marker>Ait-Kaci, 1984</marker>
<rawString>Hassan Ait-Kaci. 1984. A Lattice Theoretic Approach to Computation Based on a Calculus of Partially Ordered Type Structures. Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Hélène Candito</author>
</authors>
<title>A principlebased hierarchical representation of LTAGs.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="3217" citStr="Candito, 1996" startWordPosition="536" endWordPosition="537"> by a CFG can be generated by a lexicalized CFG. But does the EDOL have any other more direct effects on parsing efficiency? On the one hand, it is a consequence of the EDOL that wide-coverage crAcs are larger than their rule-based counterparts. With larger elementary structures, generalizations are lost regarding the internal structure of the elementary trees. Since parse time depends on grammar size, this could have an adverse effect on parsing efficiency. However, the problem of grammar size in TAG has to some extent been addressed both with respect to grammar encoding (Evans et al., 1995; Candito, 1996) and parsing (Joshi and Srinivas, 1994; Evans and Weir, 1998). On the other hand, if the EDOL hypothesis holds for those dependencies that are being checked by the parser, then the burden of passing feature values around during parsing will be less than in a rule-based framework. If all dependencies that the parser is checking can be stated directly within the elementary structures of the grammar, they do not need to be computed dynamically during the parsing process by means of feature percolation. For example, there is no need to use a slash feature to establish filler-gap dependencies over </context>
</contexts>
<marker>Candito, 1996</marker>
<rawString>Marie-Hélène Candito. 1996. A principlebased hierarchical representation of LTAGs. In Proceedings of the 16th International Conference on Computational Linguistics, Copenhagen, Denmark, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Nicolas Nicolov</author>
<author>Olga Shaumyan</author>
<author>Martine Smets</author>
<author>David Weir</author>
</authors>
<title>The LEXSYS Project.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks,</booktitle>
<pages>29--33</pages>
<contexts>
<context position="4324" citStr="Carroll et al., 1998" startWordPosition="724" endWordPosition="727">ature percolation. For example, there is no need to use a slash feature to establish filler-gap dependencies over unbounded distances across the tree if the EDOL 217 Proceedings of EACL &apos;99 NP [case acc] (whom) NP VP (you) o V NP [case . acc] (see) Figure 1: Localizing a filler-gap dependency makes it possible for the gap and its filler to be located within the same elementary structure. This paper presents an investigation into the extent to which the EDOL reduces the need for feature passing in two existing wide-coverage grammars: the XTAG grammar (XTAG-Group, 1995), and the LEXSYS grammar (Carroll et al., 1998). It can be seen as an evaluation of how well these two grammars make use of the EDOL hypothesis with respect to those dependencies that are being checked by the parser. 2 Parsing Unification-Based Grammars In phrase-structure rule-based parsing, each rule corresponds to a local tree. A rule is applied to a sequence of existing contiguous constituents, if they are compatible with the daughters. In the case of context-free grammar (cFG), the compatibility check is just equality of atomic symbols, and an instantiated daughter is merely the corresponding sub-constituent. However, unification-base</context>
</contexts>
<marker>Carroll, Nicolov, Shaumyan, Smets, Weir, 1998</marker>
<rawString>John Carroll, Nicolas Nicolov, Olga Shaumyan, Martine Smets, and David Weir. 1998. The LEXSYS Project. In Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks, pages 29-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Emele</author>
</authors>
<title>Unification with lazy nonredundant copying.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>323--330</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="6468" citStr="Emele, 1991" startWordPosition="1042" endWordPosition="1043">ation must also ensure that it does not destructively modify the input structures, since the same rule may be used several times within a single derivation, and also the same constituent may be used within different partial analyses with features instantiated in different ways. Copying input feature structures in their entirety before each unification would solve this problem, but the space usage renders this approach impractical. Therefore, various &apos;quasi-destructive&apos; algorithms (Karttunen, 1986; Kogure, 1990; Tomabechi, 1991) and algorithms using &apos;skeletal&apos; DAGS with updates (Pereira, 1985; Emele, 1991) have been proposed, which attempt to minimize copying. But even with good implementations of the best of these improved algorithms, parsers designed for wide-coverage unification-based phrase-structure grammars using large HPsG-style feature graphs spend around 85-90% of their time unifying and copying feature structures (Tomabechi, 1991), and may allocate in the region of 1-2 Mbytes memory while parsing sentences of only eight words or so (Flickinger, p.c.). Although comfortably within main memory capacities of current workstations, such large amounts of short-term storage allocation overflo</context>
</contexts>
<marker>Emele, 1991</marker>
<rawString>Martin Emele. 1991. Unification with lazy nonredundant copying. In Proceedings of the 29th Meeting of the Association for Computational Linguistics, pages 323-330, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Evans</author>
<author>David Weir</author>
</authors>
<title>A structuresharing parser for lexicalized grammars.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics,</booktitle>
<pages>372--378</pages>
<contexts>
<context position="3278" citStr="Evans and Weir, 1998" startWordPosition="544" endWordPosition="547">oes the EDOL have any other more direct effects on parsing efficiency? On the one hand, it is a consequence of the EDOL that wide-coverage crAcs are larger than their rule-based counterparts. With larger elementary structures, generalizations are lost regarding the internal structure of the elementary trees. Since parse time depends on grammar size, this could have an adverse effect on parsing efficiency. However, the problem of grammar size in TAG has to some extent been addressed both with respect to grammar encoding (Evans et al., 1995; Candito, 1996) and parsing (Joshi and Srinivas, 1994; Evans and Weir, 1998). On the other hand, if the EDOL hypothesis holds for those dependencies that are being checked by the parser, then the burden of passing feature values around during parsing will be less than in a rule-based framework. If all dependencies that the parser is checking can be stated directly within the elementary structures of the grammar, they do not need to be computed dynamically during the parsing process by means of feature percolation. For example, there is no need to use a slash feature to establish filler-gap dependencies over unbounded distances across the tree if the EDOL 217 Proceedin</context>
</contexts>
<marker>Evans, Weir, 1998</marker>
<rawString>Roger Evans and David Weir. 1998. A structuresharing parser for lexicalized grammars. In Proceedings of the 36th Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics, pages 372-378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Evans</author>
<author>Gerald Gazdar</author>
<author>David Weir</author>
</authors>
<title>Encoding lexicalized Tree Adjoining Grammars with a nonmonotonic inheritance hierarchy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>77--84</pages>
<contexts>
<context position="3201" citStr="Evans et al., 1995" startWordPosition="532" endWordPosition="535">hat can be generated by a CFG can be generated by a lexicalized CFG. But does the EDOL have any other more direct effects on parsing efficiency? On the one hand, it is a consequence of the EDOL that wide-coverage crAcs are larger than their rule-based counterparts. With larger elementary structures, generalizations are lost regarding the internal structure of the elementary trees. Since parse time depends on grammar size, this could have an adverse effect on parsing efficiency. However, the problem of grammar size in TAG has to some extent been addressed both with respect to grammar encoding (Evans et al., 1995; Candito, 1996) and parsing (Joshi and Srinivas, 1994; Evans and Weir, 1998). On the other hand, if the EDOL hypothesis holds for those dependencies that are being checked by the parser, then the burden of passing feature values around during parsing will be less than in a rule-based framework. If all dependencies that the parser is checking can be stated directly within the elementary structures of the grammar, they do not need to be computed dynamically during the parsing process by means of feature percolation. For example, there is no need to use a slash feature to establish filler-gap de</context>
</contexts>
<marker>Evans, Gazdar, Weir, 1995</marker>
<rawString>Roger Evans, Gerald Gazdar, and David Weir. 1995. Encoding lexicalized Tree Adjoining Grammars with a nonmonotonic inheritance hierarchy. In Proceedings of the 33rd Meeting of the Association for Computational Linguistics, pages 77-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G P Huet</author>
</authors>
<title>A unification algorithm for typed .\-calculus.</title>
<date>1975</date>
<journal>Theoretical Computer Science,</journal>
<pages>1--27</pages>
<contexts>
<context position="5509" citStr="Huet, 1975" startWordPosition="905" endWordPosition="906">However, unification-based extensions of phrase-structures grammars are used because they are able to encode local and non-local syntactic dependencies (for example, subjectverb agreement in English) with re-entrant features and feature percolation, respectively. Constituents are represented by DAGS (directed acyclic graphs), the compatibility check is unification, and it is the result of each unification that is used to instantiate the daughters. Graph unification based on the UNION-FIND algorithm has time complexity that is near-linear in the number of feature structure nodes in the inputs (Huet, 1975; Ait-Kaci, 1984); however, feature structures in wide-coverage grammars can contain hundreds of nodes (see e.g., HPSG (Pollard and Sag, 1994)), and since unification is a primitive operation the overall number of unification attempts during parsing can be very large. Unification therefore has a substantial practical cost. Efficient graph unification must also ensure that it does not destructively modify the input structures, since the same rule may be used several times within a single derivation, and also the same constituent may be used within different partial analyses with features instan</context>
</contexts>
<marker>Huet, 1975</marker>
<rawString>G. P. Huet. 1975. A unification algorithm for typed .\-calculus. Theoretical Computer Science, 1:27-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
<author>B Srinivas</author>
</authors>
<title>Disambiguation of super parts of speech (or supertags): Almost parsing.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>154--160</pages>
<contexts>
<context position="3255" citStr="Joshi and Srinivas, 1994" startWordPosition="540" endWordPosition="543">y a lexicalized CFG. But does the EDOL have any other more direct effects on parsing efficiency? On the one hand, it is a consequence of the EDOL that wide-coverage crAcs are larger than their rule-based counterparts. With larger elementary structures, generalizations are lost regarding the internal structure of the elementary trees. Since parse time depends on grammar size, this could have an adverse effect on parsing efficiency. However, the problem of grammar size in TAG has to some extent been addressed both with respect to grammar encoding (Evans et al., 1995; Candito, 1996) and parsing (Joshi and Srinivas, 1994; Evans and Weir, 1998). On the other hand, if the EDOL hypothesis holds for those dependencies that are being checked by the parser, then the burden of passing feature values around during parsing will be less than in a rule-based framework. If all dependencies that the parser is checking can be stated directly within the elementary structures of the grammar, they do not need to be computed dynamically during the parsing process by means of feature percolation. For example, there is no need to use a slash feature to establish filler-gap dependencies over unbounded distances across the tree if</context>
</contexts>
<marker>Joshi, Srinivas, 1994</marker>
<rawString>Aravind Joshi and B. Srinivas. 1994. Disambiguation of super parts of speech (or supertags): Almost parsing. In Proceedings of the 15th International Conference on Computational Linguistics, pages 154-160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
</authors>
<title>Preface to special issue on Tree-Adjoining Grammars.</title>
<date>1994</date>
<journal>Computational Intelligence,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="747" citStr="Joshi, 1994" startWordPosition="118" endWordPosition="119">d Weir School of Cognitive and Computing Sciences University of Sussex Brighton, BN1 9QH, UK Abstract One of the claimed benefits of Tree Adjoining Grammars is that they have an extended domain of locality (ED0L). We consider how this can be exploited to limit the need for feature structure unification during parsing. We compare two wide-coverage lexicalized grammars of English, LEXSYS and XTAG, finding that the two grammars exploit EDOL in different ways. 1 Introduction One of the most basic properties of Tree Adjoining Grammars (TAGS) is that they have an extended domain of locality (ED0L) (Joshi, 1994). This refers to the fact that the elementary trees that make up the grammar are larger than the corresponding units (the productions) that are used in phrase-structure rule-based frameworks. The claim is that in Lexicalized TAGs (LTAGs) the elementary trees provide a domain of locality large enough to state co-occurrence relationships between a lexical item (the anchor of the elementary tree) and the nodes it imposes constraints on. We will call this the extended domain of locality hypothesis. For example, wh-movement can be expressed locally in a tree that will be anchored by a verb of which</context>
</contexts>
<marker>Joshi, 1994</marker>
<rawString>Aravind Joshi. 1994. Preface to special issue on Tree-Adjoining Grammars. Computational Intelligence, 10(4):vii-xv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>D-PATR: A development environment for unification-based grammars.</title>
<date>1986</date>
<booktitle>In Proceedings of the 11th International Conference on Computational Linguistics,</booktitle>
<pages>74--80</pages>
<location>Bonn, Germany.</location>
<contexts>
<context position="6357" citStr="Karttunen, 1986" startWordPosition="1027" endWordPosition="1028">s during parsing can be very large. Unification therefore has a substantial practical cost. Efficient graph unification must also ensure that it does not destructively modify the input structures, since the same rule may be used several times within a single derivation, and also the same constituent may be used within different partial analyses with features instantiated in different ways. Copying input feature structures in their entirety before each unification would solve this problem, but the space usage renders this approach impractical. Therefore, various &apos;quasi-destructive&apos; algorithms (Karttunen, 1986; Kogure, 1990; Tomabechi, 1991) and algorithms using &apos;skeletal&apos; DAGS with updates (Pereira, 1985; Emele, 1991) have been proposed, which attempt to minimize copying. But even with good implementations of the best of these improved algorithms, parsers designed for wide-coverage unification-based phrase-structure grammars using large HPsG-style feature graphs spend around 85-90% of their time unifying and copying feature structures (Tomabechi, 1991), and may allocate in the region of 1-2 Mbytes memory while parsing sentences of only eight words or so (Flickinger, p.c.). Although comfortably wit</context>
</contexts>
<marker>Karttunen, 1986</marker>
<rawString>Lauri Karttunen. 1986. D-PATR: A development environment for unification-based grammars. In Proceedings of the 11th International Conference on Computational Linguistics, pages 74-80, Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoshi Kogure</author>
</authors>
<title>Strategic lazy incremental copy graph unification.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics,</booktitle>
<pages>223--228</pages>
<location>Helsinki.</location>
<contexts>
<context position="6371" citStr="Kogure, 1990" startWordPosition="1029" endWordPosition="1030">can be very large. Unification therefore has a substantial practical cost. Efficient graph unification must also ensure that it does not destructively modify the input structures, since the same rule may be used several times within a single derivation, and also the same constituent may be used within different partial analyses with features instantiated in different ways. Copying input feature structures in their entirety before each unification would solve this problem, but the space usage renders this approach impractical. Therefore, various &apos;quasi-destructive&apos; algorithms (Karttunen, 1986; Kogure, 1990; Tomabechi, 1991) and algorithms using &apos;skeletal&apos; DAGS with updates (Pereira, 1985; Emele, 1991) have been proposed, which attempt to minimize copying. But even with good implementations of the best of these improved algorithms, parsers designed for wide-coverage unification-based phrase-structure grammars using large HPsG-style feature graphs spend around 85-90% of their time unifying and copying feature structures (Tomabechi, 1991), and may allocate in the region of 1-2 Mbytes memory while parsing sentences of only eight words or so (Flickinger, p.c.). Although comfortably within main memor</context>
</contexts>
<marker>Kogure, 1990</marker>
<rawString>Kiyoshi Kogure. 1990. Strategic lazy incremental copy graph unification. In Proceedings of the 13th International Conference on Computational Linguistics, pages 223-228, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
</authors>
<title>A structure-sharing representation for unification-based grammar formalisms.</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>137--144</pages>
<contexts>
<context position="6454" citStr="Pereira, 1985" startWordPosition="1040" endWordPosition="1041">nt graph unification must also ensure that it does not destructively modify the input structures, since the same rule may be used several times within a single derivation, and also the same constituent may be used within different partial analyses with features instantiated in different ways. Copying input feature structures in their entirety before each unification would solve this problem, but the space usage renders this approach impractical. Therefore, various &apos;quasi-destructive&apos; algorithms (Karttunen, 1986; Kogure, 1990; Tomabechi, 1991) and algorithms using &apos;skeletal&apos; DAGS with updates (Pereira, 1985; Emele, 1991) have been proposed, which attempt to minimize copying. But even with good implementations of the best of these improved algorithms, parsers designed for wide-coverage unification-based phrase-structure grammars using large HPsG-style feature graphs spend around 85-90% of their time unifying and copying feature structures (Tomabechi, 1991), and may allocate in the region of 1-2 Mbytes memory while parsing sentences of only eight words or so (Flickinger, p.c.). Although comfortably within main memory capacities of current workstations, such large amounts of short-term storage allo</context>
</contexts>
<marker>Pereira, 1985</marker>
<rawString>Fernando Pereira. 1985. A structure-sharing representation for unification-based grammar formalisms. In Proceedings of the 23rd Meeting of the Association for Computational Linguistics, pages 137-144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="5651" citStr="Pollard and Sag, 1994" startWordPosition="923" endWordPosition="926">yntactic dependencies (for example, subjectverb agreement in English) with re-entrant features and feature percolation, respectively. Constituents are represented by DAGS (directed acyclic graphs), the compatibility check is unification, and it is the result of each unification that is used to instantiate the daughters. Graph unification based on the UNION-FIND algorithm has time complexity that is near-linear in the number of feature structure nodes in the inputs (Huet, 1975; Ait-Kaci, 1984); however, feature structures in wide-coverage grammars can contain hundreds of nodes (see e.g., HPSG (Pollard and Sag, 1994)), and since unification is a primitive operation the overall number of unification attempts during parsing can be very large. Unification therefore has a substantial practical cost. Efficient graph unification must also ensure that it does not destructively modify the input structures, since the same rule may be used several times within a single derivation, and also the same constituent may be used within different partial analyses with features instantiated in different ways. Copying input feature structures in their entirety before each unification would solve this problem, but the space u</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>K Vijay-Shanker</author>
<author>David Weir</author>
</authors>
<title>D-Tree Grammars.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>151--158</pages>
<contexts>
<context position="11040" citStr="Rambow et al., 1995" startWordPosition="1818" endWordPosition="1821"> position of the tree in Figure 2 would cause the .feature structure at the node for the subject to include pn:+. This, however, does not violate the EDOL hypothesis since this feature is not coreferenced with any other feature in the tree. 3 Analysis of two wide-coverage grammars As we have seen, the EDOL of LTAGS makes it possible, at least in principle, to locally express dependencies which cannot be localized in a CFGbased formalism. In this section we consider two existing grammars: the XTAG grammar, a widecoverage LTAG, and the LEXSYS grammar, a widecoverage D-Tree Substitution Grammar (Rambow et al., 1995). For each grammar we investigate the extend to which they do not take full advantage of the EDOL and require percolation of features at parse time. There are a number of instances in which dependencies are not localized in the XTAG grammar, most of which involve auxiliary trees. There are 219 Proceedings of EACL &apos;99 three types of auxiliary trees: predicative, modifier and coordination auxiliary trees. In predicative auxiliary trees the anchor is also the head of the tree and becomes the head of the tree resulting from the adjunction. In modifier auxiliary trees, the anchor is not the head of</context>
</contexts>
<marker>Rambow, Vijay-Shanker, Weir, 1995</marker>
<rawString>Owen Rambow, K. Vijay-Shanker, and David Weir. 1995. D-Tree Grammars. In Proceedings of the 33rd Meeting of the Association for Computational Linguistics, pages 151-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Mathematical and Computational Aspects of Lexicalized Grammars.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="2113" citStr="Schabes (1990)" startWordPosition="352" endWordPosition="353">ut are directly identified in the tree. Figure 1 shows a tree in which the case feature at the extraction site and the wh-word share the same value.&apos; &apos;The anchor, substitution and foot nodes of trees are marked with the symbols o, 1. and *, respectively. Words in parenthesis are included in trees to provide examples of strings this tree can derive. Much of the research on TAGS can be seen as illustrating how its EDOL can be exploited in various ways. However, to date, only indirect evidence has been given regarding the beneficial effects of the EDOL on parsing efficiency. The argument, due to Schabes (1990), is that benefits to parsing arise from lexicalization, and that lexicalization is only possible because of the EDOL. A parser dealing with a lexicalized grammar needs to consider only those elementary structures that can be associated with the lexical items appearing in the input. This can substantially reduce the effective grammar size at parse time. The argument that an EDOL is required for lexicalization is based on the observation that not every set of trees that can be generated by a CFG can be generated by a lexicalized CFG. But does the EDOL have any other more direct effects on parsi</context>
</contexts>
<marker>Schabes, 1990</marker>
<rawString>Yves Schabes. 1990. Mathematical and Computational Aspects of Lexicalized Grammars. Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideto Tomabechi</author>
</authors>
<title>Quasi-destructive graph unification.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>315--322</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="6389" citStr="Tomabechi, 1991" startWordPosition="1031" endWordPosition="1032">rge. Unification therefore has a substantial practical cost. Efficient graph unification must also ensure that it does not destructively modify the input structures, since the same rule may be used several times within a single derivation, and also the same constituent may be used within different partial analyses with features instantiated in different ways. Copying input feature structures in their entirety before each unification would solve this problem, but the space usage renders this approach impractical. Therefore, various &apos;quasi-destructive&apos; algorithms (Karttunen, 1986; Kogure, 1990; Tomabechi, 1991) and algorithms using &apos;skeletal&apos; DAGS with updates (Pereira, 1985; Emele, 1991) have been proposed, which attempt to minimize copying. But even with good implementations of the best of these improved algorithms, parsers designed for wide-coverage unification-based phrase-structure grammars using large HPsG-style feature graphs spend around 85-90% of their time unifying and copying feature structures (Tomabechi, 1991), and may allocate in the region of 1-2 Mbytes memory while parsing sentences of only eight words or so (Flickinger, p.c.). Although comfortably within main memory capacities of cu</context>
</contexts>
<marker>Tomabechi, 1991</marker>
<rawString>Hideto Tomabechi. 1991. Quasi-destructive graph unification. In Proceedings of the 29th Meeting of the Association for Computational Linguistics, pages 315-322, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>The XTAG-Group</author>
</authors>
<title>A lexicalized Tree Adjoining Grammar for English.</title>
<date>1995</date>
<tech>Technical Report IRCS Report 95-03,</tech>
<institution>The Institute for Research in Cognitive Science, University of Pennsylvania.</institution>
<contexts>
<context position="4277" citStr="XTAG-Group, 1995" startWordPosition="718" endWordPosition="719">y during the parsing process by means of feature percolation. For example, there is no need to use a slash feature to establish filler-gap dependencies over unbounded distances across the tree if the EDOL 217 Proceedings of EACL &apos;99 NP [case acc] (whom) NP VP (you) o V NP [case . acc] (see) Figure 1: Localizing a filler-gap dependency makes it possible for the gap and its filler to be located within the same elementary structure. This paper presents an investigation into the extent to which the EDOL reduces the need for feature passing in two existing wide-coverage grammars: the XTAG grammar (XTAG-Group, 1995), and the LEXSYS grammar (Carroll et al., 1998). It can be seen as an evaluation of how well these two grammars make use of the EDOL hypothesis with respect to those dependencies that are being checked by the parser. 2 Parsing Unification-Based Grammars In phrase-structure rule-based parsing, each rule corresponds to a local tree. A rule is applied to a sequence of existing contiguous constituents, if they are compatible with the daughters. In the case of context-free grammar (cFG), the compatibility check is just equality of atomic symbols, and an instantiated daughter is merely the correspon</context>
<context position="13733" citStr="XTAG-Group, 1995" startWordPosition="2310" endWordPosition="2311">ase too, as in who or what did this? but *John and who did this? The wh-feature has to be percolated between the two nodes on each side of the conjunction. In the LEXSYS grammar, a coordination tree is anchored by a head of the tree, not by the conjunction. To illustrate (see the tree on the right of Figure 4), NP-coordination trees are anchored by a noun, and features such as wh and case are ground during anchoring. As a result, there is no need for passing of these features in the coordination trees of the LEXSYS grammar. 2All examples relating to the XTAG grammar come from the XTAG report (XTAG-Group, 1995). They have been simplified to the extent that only details relevant to the discussion are included. As for agreement features, there are two cases to consider: if the conjunction is and, the number feature of the whole phrase is plural; if the conjunction is or, the number feature is the same as the last conjunct&apos;s (XTAG-Group, 1999). In both the XTAG and LEXSYS grammars, this is achieved by having separate trees for each type of conjunction. 3.3 Predicative Auxiliary Trees In the XTAG grammar, subject raising and auxiliary verbs anchor auxiliary trees rooted in VP, without a subject3; they c</context>
</contexts>
<marker>XTAG-Group, 1995</marker>
<rawString>The XTAG-Group. 1995. A lexicalized Tree Adjoining Grammar for English. Technical Report IRCS Report 95-03, The Institute for Research in Cognitive Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>The XTAG-Group</author>
</authors>
<title>A lexicalized Tree Adjoining Grammar for English. Technical Report http : I/www. cis . upenn . eduf-xtag/ tech-report/tech-report.html, The Institute for Research in Cognitive Science,</title>
<date>1999</date>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="14069" citStr="XTAG-Group, 1999" startWordPosition="2368" endWordPosition="2369">re anchored by a noun, and features such as wh and case are ground during anchoring. As a result, there is no need for passing of these features in the coordination trees of the LEXSYS grammar. 2All examples relating to the XTAG grammar come from the XTAG report (XTAG-Group, 1995). They have been simplified to the extent that only details relevant to the discussion are included. As for agreement features, there are two cases to consider: if the conjunction is and, the number feature of the whole phrase is plural; if the conjunction is or, the number feature is the same as the last conjunct&apos;s (XTAG-Group, 1999). In both the XTAG and LEXSYS grammars, this is achieved by having separate trees for each type of conjunction. 3.3 Predicative Auxiliary Trees In the XTAG grammar, subject raising and auxiliary verbs anchor auxiliary trees rooted in VP, without a subject3; they can be adjoined at the VP node of any compatible verb tree. With this arrangement, subject-verb agreement must be established dynamically. The agr feature of the NP subject must match the agr feature of whichever VP ends up being highest at the end of the derivation. In Figure 5, the bought tree has been anchored in such a way that adj</context>
</contexts>
<marker>XTAG-Group, 1999</marker>
<rawString>The XTAG-Group. 1999. A lexicalized Tree Adjoining Grammar for English. Technical Report http : I/www. cis . upenn . eduf-xtag/ tech-report/tech-report.html, The Institute for Research in Cognitive Science, University of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>