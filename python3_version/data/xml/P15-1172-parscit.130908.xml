<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.99505">
Coupled Sequence Labeling on Heterogeneous Annotations: POS Tagging
as a Case Study
</title>
<author confidence="0.999418">
Zhenghua Li, Jiayuan Chao, Min Zhang∗, Wenliang Chen
</author>
<affiliation confidence="0.964306">
(1) Soochow University
(2) Collaborative Innovation Center of Novel Software Technology and Industrialization
Jiangsu Province, China
</affiliation>
<email confidence="0.983249">
{zhli13,minzhang,wlchen}@suda.edu.cn; china cjy@163.com
</email>
<sectionHeader confidence="0.997102" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999985714285714">
In order to effectively utilize multiple
datasets with heterogeneous annotations,
this paper proposes a coupled sequence
labeling model that can directly learn and
infer two heterogeneous annotations
simultaneously, and to facilitate
discussion we use Chinese part-of-
speech (POS) tagging as our case study.
The key idea is to bundle two sets of
POS tags together (e.g. “[NN, n]”), and
build a conditional random field (CRF)
based tagging model in the enlarged
space of bundled tags with the help of
ambiguous labelings. To train our model
on two non-overlapping datasets that each
has only one-side tags, we transform a
one-side tag into a set of bundled tags
by considering all possible mappings at
the missing side and derive an objective
function based on ambiguous labelings.
The key advantage of our coupled model
is to provide us with the flexibility of
1) incorporating joint features on the
bundled tags to implicitly learn the
loose mapping between heterogeneous
annotations, and 2) exploring separate
features on one-side tags to overcome the
data sparseness problem of using only
bundled tags. Experiments on benchmark
datasets show that our coupled model
significantly outperforms the state-of-
the-art baselines on both one-side POS
tagging and annotation conversion tasks.
The codes and newly annotated data are
released for non-commercial usage.1
</bodyText>
<note confidence="0.861536">
∗Correspondence author.
</note>
<footnote confidence="0.913777">
1http://hlt.suda.edu.cn/˜zhli
</footnote>
<sectionHeader confidence="0.997754" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979918918919">
The scale of available labeled data significantly
affects the performance of statistical data-driven
models. As a widely-used structural classification
problem, sequence labeling is prone to suffer
from the data sparseness issue. However, the
heavy cost of manual annotation typically limits
one labeled resource in both scale and genre.
As a promising research line, semi-supervised
learning for sequence labeling has been exten-
sively studied. Huang et al. (2009) show that
standard self-training can boost the performance
of a simple hidden Markov model (HMM) based
part-of-speech (POS) tagger. Søgaard (2011) ap-
ply tri-training to English POS tagging, boost-
ing accuracy from 97.27% to 97.50%. Sun and
Uszkoreit (2012) derive word clusters from large-
scale unlabeled data as extra features for Chi-
nese POS tagging. Recently, the use of natural
annotation has becomes a hot topic in Chinese
word segmentation (Jiang et al., 2013; Liu et
al., 2014; Yang and Vozila, 2014). The idea is
to derive segmentation boundaries from implicit
information encoded in web texts, such as anchor
texts and punctuation marks, and use them as
partially labeled training data in sequence labeling
models.
The existence of multiple annotated resources
opens another door for alleviating data sparse-
ness. For example, Penn Chinese Treebank (CTB)
contains about 20 thousand sentences annotated
with word boundaries, POS tags, and syntactic
structures (Xue et al., 2005), which is widely used
for research on Chinese word segmentation and
POS tagging. People’s Daily corpus (PD)2 is a
large-scale corpus annotated with word segments
and POS tags, containing about 300 thousand
sentences from the first half of 1998 of People’s
</bodyText>
<footnote confidence="0.978373">
2http://icl.pku.edu.cn/icl_groups/
corpustagging.asp
</footnote>
<page confidence="0.621687">
1783
</page>
<note confidence="0.7199102">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1783–1792,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
China focuses on economic development
Our nation strongly develops education
</note>
<figureCaption confidence="0.9460276">
Figure 1: An example to illustrate the annotation
differences between CTB (above) and PD (below),
and how to transform a one-side tag into a set
of bundled tags. “NN” and “n” represent nouns;
“VV”and “v” represent verbs.
</figureCaption>
<bodyText confidence="0.953213297297297">
Daily newspaper (see Table 2). The two resources
were independently built for different purposes.
CTB was designed to serve syntactic analysis,
whereas PD was developed to support information
extraction systems. However, the key challenge
of exploiting the two resources is that they adopt
different sets of POS tags which are impossible to
be precisely converted from one to another based
on heuristic rules. Figure 1 shows two example
sentences from CTB and PD. Please refer to Table
B.3 in Xia (2000) for detailed comparison of the
two guidelines.
Previous work on exploiting heterogeneous data
(CTB and PD) mainly focuses on indirect guide-
feature based methods. The basic idea is to use
one resource to generate extra guide features on
another resource (Jiang et al., 2009; Sun and
Wan, 2012), which is similar to stacked learning
(Nivre and McDonald, 2008). First, PD is used
as source data to train a source model TaggerPD.
Then, TaggerPD generates automatic POS tags
on the target data CTB, called source annota-
tions. Finally, a target model TaggerCTB-guided
is trained on CTB, using source annotations as
extra guide features. Although the guide-feature
based method is effective in boosting performance
of the target model, we argue that it may have
two potential drawbacks. First, the target model
TaggerCTB-guided does not directly use PD as train-
ing data, and therefore fails to make full use of rich
language phenomena in PD. Second, the method
is more complicated in real applications since it
needs to parse a test sentence twice to get the final
results.
This paper proposes a coupled sequence label-
ing model that can directly learn and infer two
heterogeneous annotations simultaneously. We
use Chinese part-of-speech (POS) tagging as our
case study.3 The key idea is to bundle two sets
of POS tags together (e.g. “[NN, n]”), and build
a conditional random field (CRF) based tagging
model in the enlarged space of bundled tags. To
make use of two non-overlapping datasets that
each has only one-side tags, we transform a one-
side tag into a set of bundled tags by considering
all possible mappings at the missing side and
derive an objective function based on ambiguous
labelings. During training, the CRF-based cou-
pled model is supervised by such ambiguous label-
ings. The advantages of our coupled model are to
provide us the flexibility of 1) incorporating joint
features on the bundled tags to implicitly learn the
loose mapping between two sets of annotations,
and 2) exploring separate features on one-side tags
to overcome the data sparseness problem of using
bundled tags. In summary, this work makes two
major contributions:
1. We propose a coupled model which can more
effectively make use of multiple resources
with heterogeneous annotations, compared
with both the baseline and guide-feature
based method. Experiments show our
approach can significantly improve POS
tagging accuracy from 94.10% to 95.00% on
CTB.
2. We have manually annotated CTB tags for
1, 000 PD sentences, which is the first dataset
with two-side annotations and can be used
for annotation-conversion evaluation. Exper-
iments on the newly annotated data show
that our coupled model also works effectively
on the annotation conversion task, improving
conversion accuracy from 90.59% to 93.90%
(+3.31%).
</bodyText>
<sectionHeader confidence="0.985699" genericHeader="introduction">
2 Traditional POS Tagging (TaggerCTB)
</sectionHeader>
<bodyText confidence="0.9991458">
Given an input sentence of n words, denoted by
x = w1...wn, POS tagging aims to find an optimal
tag sequence t = t1...tn, where ti E T (1 G i G
n) and T is a predefined tag set. As a log-linear
probabilistic model (Lafferty et al., 2001), CRF
</bodyText>
<footnote confidence="0.993558">
3There are some slight differences in the word segmenta-
tion guidelines between CTB and PD, which are ignored in
this work for simplicity.
</footnote>
<figure confidence="0.9686094">
Bundled tags
[NN,n]
[NN,Ng]
[NN,vn]
[VV,v] [VE,v]
[VC,v] [VA,v]
tPR1 -1A2 -q;*j�3 A*4
APR1 kb2 A*3 9x-94
1784
01: ti o ti−1 02: ti o wi
03: ti o wi−1 04: ti o wi+1
05: ti o wi o ci−1,−1 06: ti o wi o ci+1,0
07: ti o ci,0 08: ti o ci,−1
9: ti o ci,k, 0 &lt; k &lt; #ci − 1
10: ti o ci,0 o ci,k, 0 &lt; k &lt; #ci − 1
11: ti o ci,−1 o ci,k, 0 &lt; k &lt; #ci − 1
12: if #ci = 1 then ti o wi o ci−1,−1 o ci+1,0
13: if ci,k = ci,k+1 then ti o ci,k o “consecutive”
14: ti o prefix(wi, k), 1 &lt; k &lt; 4, k &lt; #ci
15: ti o suffix(wi, k), 1 &lt; k &lt; 4, k &lt; #ci
</figure>
<tableCaption confidence="0.991338">
Table 1: POS tagging features f(x, i, ti_1, ti). o
</tableCaption>
<bodyText confidence="0.836299857142857">
means string concatenation; ci,k denotes the kth
Chinese character of wi; ci,0 is the first Chinese
character; ci,_1 is the last Chinese character;
#ci is the total number of Chinese characters
contained in wi; prefix/suffix(wi, k) denote the k-
Character prefix/suffix of wi.
defines the probability of a tag sequence as:
</bodyText>
<equation confidence="0.996986">
exp(Score(x, t; θ))
P(t|x; θ) _ &apos;
t′ exp(Score(x, t′; θ)) ( )
Score(x, t; θ) _ 1: θ · f (x, i, ti_1, ti) 1
1&lt;i&lt;n
</equation>
<bodyText confidence="0.99969175">
where f(x, i, ti_1, ti) is the feature vector at the
ith word and θ is the weight vector. We adopt the
state-of-the-art tagging features in Table 1 (Zhang
and Clark, 2008).
</bodyText>
<sectionHeader confidence="0.997362" genericHeader="method">
3 Coupled POS Tagging (TaggerCTB&amp;PD)
</sectionHeader>
<bodyText confidence="0.999875090909091">
In this section, we introduce our coupled model,
which is able to learn and predict two heteroge-
neous annotations simultaneously. The idea is to
bundle two sets of POS tags together and let the
CRF-based model work in the enlarged tag space.
For example, a CTB tag “NN” and a PD tag “n”
would be bundled into “[NN,n]”. Figure 2 shows
the graphical structure of our model.
Different from the traditional model in Eq. (1),
our coupled model defines the score of a bundled
tag sequence as follows:
</bodyText>
<equation confidence="0.9812792">
Score(x, [ta, tb]; θ) _
� f(x, i, [ta i_1, tbi_1], [tai , tbi])
�
θ · �f(x, i, tai_1, tai )
f(x, i, tbi_1, tbi)
</equation>
<bodyText confidence="0.97137875">
where the first item of the enlarged feature vector
is called joint features, which can be obtained by
Figure 2: Graphical structure of our coupled CRF
model.
instantiating Table 1 by replacing ti with bundled
tags [tai , tbi]; the second and third items are called
separate features, which are based on single-side
tags. The advantages of our coupled model over
the traditional model are to provide us with the
flexibility of using both kinds of features, which
significantly contributes to the accuracy improve-
ment as shown in the following experiments.
</bodyText>
<subsectionHeader confidence="0.999723">
3.1 Mapping Functions
</subsectionHeader>
<bodyText confidence="0.999969625">
The key challenge of our idea is that both CTB and
PD are non-overlapping and each contains only
one-side POS tags. Therefore, the problem is how
to construct training data for our coupled model.
We denote the tag set of CTB as Ta, and that of
PD as Tb, and the bundled tag set as Ta&amp;b. Since
the full Cartetian Ta x Tb would lead to a very
large number of bundled tags, making the model
very slow, we would like to come up with a much
smaller Ta&amp;b C Ta x Tb, based on linguistic
insights of the annotation guidelines of the two
datasets.
To obtain a proper Ta&amp;b, we introduce a map-
ping function between the two sets of tags as m :
Ta x Tb → {0, 1}, which only allow specific tag
pairs to be bundled together.
</bodyText>
<equation confidence="0.76194125">
�
1 if the two tags can be bundled
0 otherwise
(3)
</equation>
<bodyText confidence="0.999924416666667">
where one mapping function m corresponds to
one Ta&amp;b. When the mapping function becomes
looser, the tag set size |Ta&amp;b |becomes larger.
Then, based on the mapping function, we can
map a single-side POS tag into a set of bundled
tags by considering all possible tags at the missing
side, as illustrated in Figure 1. The word “�k&amp;4”
is tagged as “NN” at the CTB side. Suppose that
the mapping function m tells that “NN” can be
mapped into three tags at the PD side, i.e., “n”,
“Ng”, and “vn”. Then, we create three bundled
tags for the word, i.e., “[NN, n]”, “[NN, Ng]”,
</bodyText>
<equation confidence="0.987943">
W1 ... wi_,
Wi ... Wn
�
1&lt;i&lt;n
1 (2)
m(ta, tb) _
</equation>
<page confidence="0.897804">
1785
</page>
<bodyText confidence="0.972019208333333">
“[NN, vn]” as its gold-standard references during
training. It is known as ambiguous labelings when
a training instance has multiple gold-standard la-
bels. Similarly, we can obtain bundled tags for all
other words in sentences of CTB and PD. After
such transformation, the two datasets are now in
the same tag space.
At the beginning of this work, our intuition is
that the coupled model would achieve the best
performance if we build a tight and linguistical-
ly motivated mapping function. However, our
preliminary experiments show that our intuitive
assumption is actually incorrect. Therefore, we
experiment with the following four mapping func-
tions to manage to figure out the reasons behind
and to better understand our coupled model.
• The tight mapping function produces 145
tags, and is constructed by strictly following
linguistic principles and our careful study of
the two guidelines and datasets.
• The relaxed mapping function results in 179
tags, which is an looser version of the tight
mapping function by including extra 34 weak
mapping relationships.
</bodyText>
<listItem confidence="0.9996475">
• The automatic mapping function generates
346 tags. We use the baseline TaggerCTB to
parse PD, and collect all automatic mapping
relationships.
• The complete mapping function obtains
1, 254 tags (|Ta |× |Tb |= 33 × 38).
</listItem>
<subsectionHeader confidence="0.975796">
3.2 Training Objective with Ambiguous
Labelings
</subsectionHeader>
<bodyText confidence="0.999317266666667">
So far, we have formally defined a coupled model
and prepared both CTB and PD in the same
bundled tag space. The next problem is how to
learn the model parameters θ. Note that after our
transformation, a sentence in CTB or PD have
many tag sequences as gold-standard references
due to the loose mapping function, known as
ambiguous labelings. Here, we derive a training
objective based on ambiguous labelings. For
simplicity, we illustrate the idea based on the
notations of the baseline CRF model in Eq. (1).
Given a sentence x, we denote a set of ambigu-
ous tag sequences as V. Then, the probability of
V is the sum of probabilities of all tag sequences
contained in V:
</bodyText>
<equation confidence="0.9681205">
p(V|x;θ) = 1: p(t|x; θ) (4)
t∈V
</equation>
<bodyText confidence="0.7974575">
Algorithm 1 SGD training with two labeled
datasets.
</bodyText>
<listItem confidence="0.685898">
1: Input: Two labeled datasets: D(1) =
</listItem>
<equation confidence="0.899186666666667">
{(x(1)
i , V(1)
i }N i=1, D(2) = {(x(2)
i , V(2)
i )}M i=1;
Parameters: I, N′, M′, b
</equation>
<listItem confidence="0.98539">
2: Output: θ
3: Initialization: θ0 = 0, k = 0;
4: for i = 1 to I do {iterations}
5: Randomly select N′ instances from D(1)
and M′ instances from D(2) to compose a
new dataset Di, and shuffle it.
6: Traverse Di, and use a small batch Dbk ⊆
Di at one step.
7: θk+1 = θk + ηk 1b∇L(Dbk; θk)
8: k = k + 1
9: end for
</listItem>
<bodyText confidence="0.9958955">
Suppose the training data is D = {(xi, Vi)}Ni=1.
Then the log likelihood is:
</bodyText>
<equation confidence="0.992683333333333">
N
L(D; θ) = log p(Vi|xi; θ) (5)
i=1
</equation>
<bodyText confidence="0.901261916666667">
After derivation, the gradient is:
(Et∈Vi[f(xi, t)] − Et[f(xi, t)])
(6)
where f(xi, t) is an aggregated feature vector for
tagging xi as t; Et∈Vi[.] means model expectation
of the features in the constrained space of Vi;
Et[.] is model expectation with no constraint.
This function can be efficiently solved by the
forward-backward algorithm. Please note that the
training objective of a traditional CRF model can
be understood as a special case where Vi contains
one sequence.
</bodyText>
<subsectionHeader confidence="0.990705">
3.3 SGD Training with Two Datasets
</subsectionHeader>
<bodyText confidence="0.9999775">
We adopt stochastic gradient descent (SGD) to
iteratively learn θ for our baseline and coupled
models. However, we have two separate training
data, and CTB may be overwhelmed by PD if
directly merging the two datasets into one, since
PD is 15 times larger than CTB (see Table 2),
Therefore, we propose a simple corpus-weighting
strategy, as shown in Algorithm 1, where Dbk is a
subset of training data used in kth step update; b
is the batch size; ηk is a update step. The idea is
to randomly sample instances from each training
data in a certain proportion before each iteration.
</bodyText>
<equation confidence="0.9777785">
∂L(D; θ) N
∂θ i=1
</equation>
<page confidence="0.905331">
1786
</page>
<bodyText confidence="0.9994988">
The sampled data is then used for one-iteration
training. Later experiments will investigate the
effect of the weighting proportion. In this work,
we use b = 30, and follow the implementation in
CRFsuite4 to decide ηk.
</bodyText>
<sectionHeader confidence="0.995573" genericHeader="method">
4 Manually Annotating PD Sentences
with CTB Tags
</sectionHeader>
<bodyText confidence="0.999972121951219">
To evaluate different methods on annotation con-
version, we build the first dataset that contains
1, 000 sentences with POS tags on both sides of
CTB and PD. The sentences are randomly sam-
pled from PD. To save annotation effort, we only
select 20% most difficult tokens to manually anno-
tate. The difficulty of a word wi is measured based
on marginal probabilities produced by the baseline
TaggerCTB. p(tiJx, wi; B) denotes the marginal
probability of tagging wi as ti. The basic assump-
tion is that wi is more difficult to annotate if its
most likely tag candidate (arg maxt p(tIx, wi; B))
gets lower marginal probability.
We build a visualized online annotation system
to facilitate manual labeling. The annotation task
is designed in such way that at a time an annotator
is provided with a sentence and one focus word,
and is required to decide the CTB POS tag of the
word. To further simplify annotation, we provide
two or three most likely tag candidates as well,
so that annotators can choose one either among
the candidates or from a full list. We employ 8
undergraduate students as our annotators. Anno-
tators are trained on simulated tasks from CTB
data for several hours, and and start real annotation
once reaching certain accuracy. To guarantee
annotation quality, we adopt multiple annotation.
Initially, one task is randomly assigned to two
annotators. Later, if the two annotators submit
different results, the system will assign the task
to two more annotators. To aggregate annotation
results, we only retain annotation tasks that the
first two annotators agree (91.0%) or three anno-
tators among four agree (5.6%), and discard other
tasks (3.4%). Finally, we obtain 5,769 words
with both CTB and PD tags, with each annotator’s
detailed submissions, and could be used as a
non-synthesized dataset for studying aggregating
submissions from non-expert annotators in crowd-
sourcing platforms (Qing et al., 2014). The data is
also fully released for non-commercial usage.
</bodyText>
<footnote confidence="0.987401">
4http://www.chokkan.org/software/
crfsuite/
</footnote>
<sectionHeader confidence="0.998684" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999986583333333">
In this section, we conduct experiments to verify
the effectiveness of our approach. We adopt CTB
(version 5.1) with the standard data split, and
randomly split PD into four sets, among which
one set is 20% partially annotated with CTB tags.
The data statistics is shown in Table 2. The main
concern of this work is to improve accuracy on
CTB by exploring large-scale PD, since CTB is
relatively small, but is widely-used benchmark
data in the research community.
We use the standard token-wise tagging accu-
racy as the evaluation metric. For significance
test, we adopt Dan Bikel’s randomized parsing
evaluation comparator (Noreen, 1989).5.
The baseline CRF is trained on either CTB
training data with 33 tags, or PD training data
with 38 tags. The coupled CRF is trained on
both two separate training datasets with bundled
tags (179 tags for the relaxed mapping function).
During evaluation, the coupled CRF is not directly
evaluated on bundled tags, since bundled tags are
unavailable in either CTB or PD test data. Instead,
the coupled and baseline CRFs are both evaluated
on one-side tags.
</bodyText>
<subsectionHeader confidence="0.989833">
5.1 Model Development
</subsectionHeader>
<bodyText confidence="0.993543954545455">
Our coupled model has two major parameters to
be decided. The first parameter is to determine
the mapping function between CTB and PD an-
notations, and the second parameter is the relative
weights of the two datasets during training (N′ vs.
M′: number of sentences in each dataset used for
training at one iteration).
Effect of mapping functions (described
in Subsection 3.1) is illustrated in Figure 3.
Empirically, we adopt N′ = 5K vs. M′ = 20K
to merge the two training datasets at each iteration.
Our intuition is that using this proportion, CTB
should not be overwhelmed by PD, and both
training data can be used up in relatively similar
speed. Specifically, all training data of CTB can
be consumed in about 3 iterations, whereas PD
can be consumed in about 14 iterations. We also
present the results of the baseline model trained
using 5K sentences in one iteration for better
comparison.
Contrary to our intuitive assumption, it actually
leads to very bad performance when using the
</bodyText>
<footnote confidence="0.9646845">
5http://www.cis.upenn.edu/˜dbikel/
software.html
</footnote>
<page confidence="0.965174">
1787
</page>
<table confidence="0.999322">
#sentences #tokens with CTB tags #tokens with PD tags
train 16,091 437,991 –
CTB dev 803 20,454 –
test 1,910 50,319 –
train 273,883 – 6,488,208
dev 1,000 – 23,427
PD
test 2,500 – 58,301
newly labeled 1,000 5,769 27,942
</table>
<tableCaption confidence="0.9405">
Table 2: Data statistics. Please kindly note that the 1, 000 sentences originally from PD are only partially
annotated with CTB tags (about 20% most ambiguous tokens).
</tableCaption>
<figureCaption confidence="0.99752475">
Figure 3: Accuracy on CTB-dev regarding to
mapping functions.
Figure 4: Accuracy on CTB-dev with different
weighting settings.
</figureCaption>
<figure confidence="0.994615161290323">
1 11 21 31 41 51 61 71 81 91
Iteration Number
Accuracy on CTB-dev (%)
95.5
94.5
93.5
92.5
95
94
93
92
Complete
Automatic
Relaxed
Tight
Baseline:CTB(5K)
1 31 61 91 121 151 181 211 241 271
Iteration Number
Accuracy on CTB-dev (%)
94.5
93.5
92.5
95
94
93
92
CTB(5K)+PD(100K)
CTB(5K)+PD(20K)
CTB(5K)+PD(5K)
CTB(5K)+PD(1K)
Baseline:CTB(5K)
</figure>
<bodyText confidence="0.991073729166667">
tight mapping function that is carefully created
based on linguistic insights, which is even inferior
to the baseline model. The relaxed mapping
function outperforms the tight function by large
margin. The automatic function works slightly
better than the relaxed one. The complete function
achieves similar accuracy with the automatic one.
In summary, we can conclude that our coupled
model achieves much better performance when
the mapping function becomes looser. In other
words, this suggests that our coupled model can
effectively learn the implicit mapping between
heterogeneous annotations, and does not rely on
a carefully designed mapping function.
Since a looser mapping function leads to a
larger number of bundled tags and makes the
model slower, we implement a paralleled training
procedure based on Algorithm 1, and run each
experiment with five threads. However, it still
takes about 20 hours for one iteration when using
the complete mapping function; whereas the other
three mapping functions need about 6, 2, and 1
hours respectively. Therefore, as a compromise,
we adopt the relaxed mapping function in the fol-
lowing experiments, which achieves slightly lower
accuracy than the complete mapping function, but
is much faster.
Effect of weighting CTB and PD is investi-
gated in Figure 4 and 5. Since the scale of PD
is much larger than CTB, we adopt Algorithm 1
to merge the training data in a certain proportion
(N′ CTB sentences and M′ PD sentences) at
each iteration. We use N′ = 5K, and vary
M′ = 1K/5K/20K/100K. Figure 4 shows the
accuracy curves on CTB development data. We
find that when M′ = 100K, our coupled model
achieve very low accuracy, which is even worse
than the baseline model. The reason should be that
the training instances in CTB are overwhelmed by
those in PD when M′ is large. In contrast, when
M′ = 1K, the accuracy is also inferior to the
case of M′ = 5K, which indicates that PD is
not effectively utilized in this setting. Our model
works best when M′ = 5K, which is slightly
better than the case of M′ = 1K/20K.
Figure 5 shows the accuracy curves on PD
development data. The baseline model is trained
using 100K sentences in one iteration. We find
</bodyText>
<page confidence="0.993138">
1788
</page>
<figureCaption confidence="0.9958435">
Figure 5: Accuracy on PD-dev with different
weighting settings.
</figureCaption>
<bodyText confidence="0.999834071428571">
that when M′ = 100K, our coupled model
achieves similar accuracy with the baseline model.
When M′ becomes smaller, our coupled model
becomes inferior to the baseline model. Particu-
larly, when M′ = 1K, the model converges very
slowly. However, from the trend of the curves, we
expect that the accuracy gap between our coupled
model with M′ = 5K/20K and the baseline
model should be much smaller when reaching
convergence. Based on the above observation,
we adopt N′ = 5K and M′ = 5K in the
following experiments. Moreover, we select the
best iteration on the development data, and use the
corresponding model to parse the test data.
</bodyText>
<subsectionHeader confidence="0.99757">
5.2 Final Results
</subsectionHeader>
<bodyText confidence="0.9999518">
Table 3 shows the final results on the CTB test
data. We re-implement the guide-feature based
method of Jiang et al. (2009), referred to as two-
stage CRF. Li et al. (2012) jointly models Chinese
POS tagging and dependency parsing, and report
the best tagging accuracy on CTB. The results
show that our coupled model outperforms the
baseline model by large margin, and also achieves
slightly higher accuracy than the guide-feature
based method.
</bodyText>
<subsectionHeader confidence="0.999292">
5.3 Feature Study
</subsectionHeader>
<bodyText confidence="0.9991475">
We conduct more experiments to measure individ-
ual contribution of each feature set, namely the
joint features based on bundled tags and separate
features based on single-side tags, as defined in
Eq. (2). Table 4 shows the results. We can see that
when only using separate features, our coupled
model achieves only slightly better accuracy than
the baseline model. This is because there is
</bodyText>
<table confidence="0.9135232">
Accuracy
Baseline CRF 94.10
94.81 (+0.71) †
95.00 (+0.90) †‡
Best result (Li et al., 2012) 94.60
</table>
<tableCaption confidence="0.909134833333333">
Table 3: Final results on CTB test data. †
means the corresponding approach significantly
outperforms the baseline at confidence level of
p &lt; 10−5; whereas ‡ means the accuracy
difference between the two-stage CRF and the
coupled CRF is significant at confidence level of
</tableCaption>
<table confidence="0.9566015">
p &lt; 10−2.
dev test
Baseline CRF 94.28 94.10
Coupled CRF (w/ separate feat) 94.36 94.43 (+0.33)
Coupled CRF (w/ joint feat) 92.92 92.90 (-1.20)
Coupled CRF (full) 95.10 95.00 (+0.90)
</table>
<tableCaption confidence="0.999887">
Table 4: Accuracy on CTB: feature study.
</tableCaption>
<bodyText confidence="0.999312545454545">
little connection and help between the two sets
annotations. When only using joint features,
our coupled model becomes largely inferior to
the baseline, which is due to the data sparseness
problem for the joint features. However, when
the two sets of features are combined, the coupled
model largely outperforms the baseline model.
These results indicate that both joint features and
separate features are indispensable components
and complementary to each other for the success
of our coupled model.
</bodyText>
<subsectionHeader confidence="0.962655">
5.4 Results on Annotation Conversion
</subsectionHeader>
<bodyText confidence="0.992911">
In this subsection, we evaluate different methods
on the annotation conversion task using our newly
annotated 1, 000 sentences. The gold-standard
</bodyText>
<table confidence="0.995388">
PD-to-CTB conversion
Baseline CRF 90.59
Two-stage CRF (guide-feature) 93.22 (+2.63) †
Coupled CRF 93.90 (+3.31) †‡
</table>
<tableCaption confidence="0.992905">
Table 5: Conversion accuracy on our annotated
</tableCaption>
<bodyText confidence="0.989098833333333">
data. † means the corresponding approach sig-
nificantly outperforms the baseline at confidence
level of p &lt; 10−5; whereas ‡ means the accuracy
difference between the two-stage CRF and the
coupled CRF is significant at confidence level of
p &lt; 10−2.
</bodyText>
<figure confidence="0.998253894736842">
1 31 61 91 121 151 181 211 241 271
Iteration Number
Accuracy on PD-dev (%)
97.5
96.5
95.5
94.5
93.5
97
96
95
94
CTB(5K)+PD(100K)
CTB(5K)+PD(20K)
CTB(5K)+PD(5K)
CTB(5K)+PD(1K)
Baseline:PD(100K)
Two-stage CRF (guide-feature)
Coupled CRF
</figure>
<page confidence="0.977081">
1789
</page>
<table confidence="0.9978025">
dev test
Baseline CRF 94.28 94.10
Coupled CRF 95.10 95.00 (+0.90) †
Baseline CRF + converted PD 95.01 94.81 (+0.71) †‡
</table>
<tableCaption confidence="0.994228">
Table 6: Accuracy on CTB: using converted PD.
</tableCaption>
<bodyText confidence="0.986535208333333">
† means the corresponding approach significantly
outperforms the baseline at confidence level of
p &lt; 10−5; whereas ‡ means the accuracy
difference between the coupled CRF and the
baseline CRF with converted PD is significant at
confidence level of p &lt; 10−2.
PD-side tags are provided, and the goal is to obtain
the CTB-side tags via annotation conversion. We
evaluate accuracy on the 5, 769 words having
manually annotated CTB-side tags.
Our coupled model can be naturally used for
annotation conversion. The idea is to perform
constrained decoding on the test data, using the
PD-side tags as hard constraints. The guide-
feature based method can also perform annotation
conversion by using the gold-standard PD-side
tags to compose guide features. Table 5 shows
the results. The accuracy is much lower than
those in Table 3, because the 5, 769 words used
for evaluation are 20% most ambiguous tokens in
the 1, 000 test sentence (partial annotation to save
annotation effort). From Table 5, we can see that
our coupled model outperforms both the baseline
and guide-feature based methods by large margin.
</bodyText>
<subsectionHeader confidence="0.973467">
5.5 Results of Training with Converted Data
</subsectionHeader>
<bodyText confidence="0.999984642857143">
One weakness of our coupled model is the in-
efficiency problem due to the large bundled tag
set. In practice, we usually only need results
following one annotation style. Therefore, we
employ our coupled model to convert PD into the
style of CTB, and train our baseline model with
two training data with homogeneous annotations.
Again, Algorithm 1 is used to merge the two
data with N′ = 5K and M′ = 5K. The
results are shown in the bottom row in Table 6.
We can see that with the extra converted data,
the baseline model can achieve slightly lower
accuracy with the coupled model and avoid the
inefficiency problem at the meantime.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999990830188679">
This work is partially inspired by Qiu et al. (2013),
who propose a model that performs heterogeneous
Chinese word segmentation and POS tagging and
produces two sets of results following CTB and
PD styles respectively. Different from our CRF-
based coupled model, their approach adopts a lin-
ear model, which directly combines two separate
sets of features based on single-side tags, without
considering the interacting joint features between
the two annotations. They adopt an approximate
decoding algorithm which tries to find the best
single-side tag sequence with reference to tags
at the other side. In contrast, our approach is a
direct extension of traditional CRF, and is more
theoretically simple from the perspective of mod-
elling. The use of both joint and separate features
is proven to be crucial for the success of our
coupled model. In addition, their work indicates
that their model relies on a hand-crafted loose
mapping between annotations, which is opposite
to our findings. The naming of the “coupled”
CRF is borrowed from the work of Qiu et al.
(2012), which treats the joint task of Chinese word
segmentation and POS tagging as two coupled
sequence labeling problems.
Zhang et al. (2014) propose a shift-reduce de-
pendency parsing model which can simultaneous-
ly learn and produce two heterogeneous parse
trees. However, their approach assumes the ex-
istence of data with annotations at both sides,
which is obtained by converting phrase-structure
trees into dependency trees with different heuristic
rules.
This work is also closely related with multi-
task learning, which aims to jointly learn multiple
related tasks with the benefit of using interac-
tive features under a share representation (Ben-
David and Schuller, 2003; Ando and Zhang, 2005;
Parameswaran and Weinberger, 2010). However,
according to our knowledge, multi-task learning
typically assumes the existence of data with labels
for multiple tasks at the same time, which is
unavailable in our situation.
As one reviewer kindly pointed out that our
model is a factorial CRF (Sutton et al., 2004), in
the sense that the bundled tags can be factorized
two connected latent variables. Initially, factorial
CRFs are designed to jointly model two relat-
ed (and typically hierarchical) sequential labeling
tasks, such as POS tagging and chunking. In this
work, our coupled CRF jointly models two same
tasks which have different annotation schemes.
Moreover, this work provides a natural way to
</bodyText>
<page confidence="0.963233">
1790
</page>
<bodyText confidence="0.999929">
learn from incomplete annotations where one sen-
tence only contains one-side labels. The reviewer
also suggests that our objective can be optimized
with the latent variable structured perceptron of
Sun et al. (2009), which we leave as future work.
Learning with ambiguous labelings are previ-
ously explored for classification (Jin and Ghahra-
mani, 2002), sequence labeling (Dredze et al.,
2009), parsing (Riezler et al., 2002; T¨ackstr¨om
et al., 2013; Li et al., 2014a; Li et al., 2014b).
Recently, researchers derive natural annotations
from web data, transform them into ambiguous
labelings to supervise Chinese word segmentation
models (Jiang et al., 2013; Liu et al., 2014; Yang
and Vozila, 2014).
</bodyText>
<sectionHeader confidence="0.999657" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999979636363636">
This paper proposes an effective coupled
sequence labeling model for exploiting multiple
non-overlapping datasets with heterogeneous
annotations. Please note that our model can also
be naturally trained on datasets with both-side
annotations if such data exists. Experimental
results demonstrate that our model work better
than the baseline and guide-feature based methods
on both one-side POS tagging and annotation
conversion. Specifically, detailed analysis
shows several interesting findings. First, both
the separate features and joint features are
indispensable components for the success of our
coupled model. Second, our coupled model does
not rely on a carefully hand-crafted mapping
function. Our linguistically motivated mapping
function is only used to reduce the size of the
bundled tag set for the sake of efficiency. Finally,
using the extra training data converted with
our coupled model, the baseline tagging model
achieves similar accuracy improvement. In this
way, we can avoid the inefficiency problem of our
coupled model in real application.
For future, our immediate plan is to annotate
more data with both CTB and PD tags (a few t-
housand sentences), and to investigate our coupled
model with small amount of such annotation as
extra training data. Meanwhile, Algorithm 1 is
empirically effective in merging two training data,
but still needs manual tuning of the weighting
factor on held-out data. Thus, we would like
to find a more principled and theoretically sound
method to merge multiple training data.
</bodyText>
<sectionHeader confidence="0.997714" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999296">
The authors would like to thank the undergraduate
students Fangli Lu and Xiaojing Wang for building
our annotation system, and Le Lu, Die Hu, Yue
Zhang, Jian Zhang, Qiuyi Yan, Xinzhou Jiang
for data annotation. We are also grateful that Yu
Ding kindly shared her earlier codes on which our
annotation system was built. We also thank the
helpful comments from our anonymous reviewers.
This work was supported by National Natural Sci-
ence Foundation of China (Grant No. 61432013,
61203314) and Jiangsu Planned Projects for Post-
doctoral Research Funds (No. 1401075B), and
was also partially supported by Collaborative In-
novation Center of Novel Software Technology
and Industrialization of Jiangsu Province.
</bodyText>
<sectionHeader confidence="0.999339" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999162090909091">
Rie Kubota Ando and Tong Zhang. 2005. A
framework for learning predictive structures from
multiple tasks and unlabeled data. Journal of
Machine Learn Research, 6:1817–1853.
Shai Ben-David and Reba Schuller. 2003. Exploiting
task relatedness for multiple task learning. In COLT.
Mark Dredze, Partha Pratim Talukdar, and Koby
Crammer. 2009. Sequence learning from data
with multiple labels. In ECML/PKDD Workshop on
Learningfrom Multi-Label Data.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm
part-of-speech tagger by latent annotation and self-
training. In Proceedings ofNAACL, pages 213–216.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and POS tagging – a case study.
In Proceedings ofACL, pages 522–530.
Wenbin Jiang, Meng Sun, Yajuan L¨u, Yating Yang,
and Qun Liu. 2013. Discriminative learning with
natural annotations: Word segmentation as a case
study. In Proceedings ofACL, pages 761–769.
Rong Jin and Zoubin Ghahramani. 2002. Learning
with multiple labels. In Proceedings ofNIPS.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings ofICML 2001, pages
282–289.
Zhenghua Li, Min Zhang, Wanxiang Che, and Ting
Liu. 2012. A separately passive-aggressive training
algorithm for joint POS tagging and dependency
parsing. In COLING, pages 1681–1698.
</reference>
<page confidence="0.810598">
1791
</page>
<reference confidence="0.999800137931035">
Zhenghua Li, Min Zhang, and Wenliang Chen.
2014a. Ambiguity-aware ensemble training for
semi-supervised dependency parsing. In ACL, pages
457–467.
Zhenghua Li, Min Zhang, and Wenliang Chen. 2014b.
Soft cross-lingual syntax projection for dependency
parsing. In COLING, pages 783–793.
Yijia Liu, Yue Zhang, Wanxiang Che, Ting Liu, and
Fan Wu. 2014. Domain adaptation for CRF-based
Chinese word segmentation using free annotations.
In Proceedings of EMNLP, pages 864–874.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings ofACL, pages 950–958.
Eric W. Noreen. 1989. Computer-intensive methods
for testing hypotheses: An introduction. John Wiley
&amp; Sons, Inc., New York.
S. Parameswaran and K.Q. Weinberger. 2010. Large
margin multi-task metric learning. In J. Lafferty,
C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and
A. Culotta, editors, Advances in Neural Information
Processing Systems 23, pages 1867–1875.
Ciyang Qing, Ulle Endriss, Raquel Fernandez, and
Justin Kruger. 2014. Empirical analysis of
aggregation methods for collective annotation. In
COLING, pages 1533–1542.
Xipeng Qiu, Feng Ji, Jiayi Zhao, and Xuanjing Huang.
2012. Joint segmentation and tagging with coupled
sequences labeling. In Proceedings of COLING
2012: Posters, pages 951–964, Mumbai, India.
Xipeng Qiu, Jiayi Zhao, and Xuanjing Huang. 2013.
Joint Chinese word segmentation and POS tagging
on heterogeneous annotated corpora with multiple
task learning. In Proceedings of EMNLP, pages
658–668.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. III Maxwell, and Mark
Johnson. 2002. Parsing the wall street journal using
a lexical-functional grammar and discriminative
estimation techniques. In Proceedings of ACL,
pages 271–278.
Anders Søgaard. 2011. Semi-supervised condensed
nearest neighbor for part-of-speech tagging. In
Proceedings ofACL, pages 48–52.
Weiwei Sun and Hans Uszkoreit. 2012. Capturing
paradigmatic and syntagmatic lexical relations:
Towards accurate Chinese part-of-speech tagging.
In Proceedings ofACL, pages 242–252.
Weiwei Sun and Xiaojun Wan. 2012. Reducing
approximation and estimation errors for Chinese
lexical processing with heterogeneous annotations.
In Proceedings ofACL, pages 232–241.
Xu Sun, Takuya Matsuzaki, Daisuke Okanohara,
and Jun’ichi Tsujii. 2009. Latent variable
perceptron algorithm for structured classification.
In Proceedings of the 21st International Joint
Conference on Artificial Intelligence (IJCAI 2009),
pages 1236–1242.
Charles Sutton, Khashayar Rohanimanesh, and An-
drew McCallum. 2004. Dynamic conditional
random fields: Factorized probabilistic models
for labeling and segmenting sequence data. In
International Conference on Machine Learning
(ICML).
Oscar T¨ackstr¨om, Ryan McDonald, and Joakim Nivre.
2013. Target language adaptation of discriminative
transfer parsers. In Proceedings of NAACL, pages
1061–1071.
Fei Xia. 2000. The part-of-speech tagging guidelines
for the penn Chinese treebank 3.0. In Technical
Report, Linguistic Data Consortium.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering, volume 11, pages 207–238.
Fan Yang and Paul Vozila. 2014. Semi-supervised
Chinese word segmentation using partial-label
learning with conditional random fields. In
Proceedings ofEMNLP, pages 90–98.
Yue Zhang and Stephen Clark. 2008. Joint word
segmentation and POS tagging using a single
perceptron. In Proceedings ofACL-08: HLT, pages
888–896.
Meishan Zhang, Wanxiang Che, Yanqiu Shao, and
Ting Liu. 2014. Jointly or separately: Which is
better for parsing heterogeneous dependencies? In
Proceedings of COLING, pages 530–540.
</reference>
<page confidence="0.994203">
1792
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.150676">
<title confidence="0.7822615">Coupled Sequence Labeling on Heterogeneous Annotations: POS Tagging as a Case Study</title>
<author confidence="0.960259">Jiayuan Chao Li</author>
<author confidence="0.960259">Min Wenliang</author>
<note confidence="0.529561">(1) Soochow (2) Collaborative Innovation Center of Novel Software Technology and Jiangsu Province, china cjy@163.com</note>
<abstract confidence="0.99768645945946">In order to effectively utilize multiple datasets with heterogeneous annotations, this paper proposes a coupled sequence labeling model that can directly learn and infer two heterogeneous annotations simultaneously, and to facilitate discussion we use Chinese part-ofspeech (POS) tagging as our case study. The key idea is to bundle two sets of tags together (e.g. and build a conditional random field (CRF) based tagging model in the enlarged space of bundled tags with the help of To train our model on two non-overlapping datasets that each has only one-side tags, we transform a one-side tag into a set of bundled tags by considering all possible mappings at the missing side and derive an objective function based on ambiguous labelings. The key advantage of our coupled model is to provide us with the flexibility of 1) incorporating joint features on the bundled tags to implicitly learn the loose mapping between heterogeneous annotations, and 2) exploring separate features on one-side tags to overcome the data sparseness problem of using only bundled tags. Experiments on benchmark datasets show that our coupled model significantly outperforms the state-ofthe-art baselines on both one-side POS tagging and annotation conversion tasks. The codes and newly annotated data are for non-commercial author.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<journal>Journal of Machine Learn Research,</journal>
<pages>6--1817</pages>
<contexts>
<context position="30071" citStr="Ando and Zhang, 2005" startWordPosition="5030" endWordPosition="5033">ing as two coupled sequence labeling problems. Zhang et al. (2014) propose a shift-reduce dependency parsing model which can simultaneously learn and produce two heterogeneous parse trees. However, their approach assumes the existence of data with annotations at both sides, which is obtained by converting phrase-structure trees into dependency trees with different heuristic rules. This work is also closely related with multitask learning, which aims to jointly learn multiple related tasks with the benefit of using interactive features under a share representation (BenDavid and Schuller, 2003; Ando and Zhang, 2005; Parameswaran and Weinberger, 2010). However, according to our knowledge, multi-task learning typically assumes the existence of data with labels for multiple tasks at the same time, which is unavailable in our situation. As one reviewer kindly pointed out that our model is a factorial CRF (Sutton et al., 2004), in the sense that the bundled tags can be factorized two connected latent variables. Initially, factorial CRFs are designed to jointly model two related (and typically hierarchical) sequential labeling tasks, such as POS tagging and chunking. In this work, our coupled CRF jointly mode</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learn Research, 6:1817–1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>Reba Schuller</author>
</authors>
<title>Exploiting task relatedness for multiple task learning.</title>
<date>2003</date>
<booktitle>In COLT.</booktitle>
<marker>Ben-David, Schuller, 2003</marker>
<rawString>Shai Ben-David and Reba Schuller. 2003. Exploiting task relatedness for multiple task learning. In COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
</authors>
<title>Partha Pratim Talukdar, and Koby Crammer.</title>
<date>2009</date>
<booktitle>In ECML/PKDD Workshop on Learningfrom Multi-Label Data.</booktitle>
<marker>Dredze, 2009</marker>
<rawString>Mark Dredze, Partha Pratim Talukdar, and Koby Crammer. 2009. Sequence learning from data with multiple labels. In ECML/PKDD Workshop on Learningfrom Multi-Label Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Vladimir Eidelman</author>
<author>Mary Harper</author>
</authors>
<title>Improving a simple bigram hmm part-of-speech tagger by latent annotation and selftraining.</title>
<date>2009</date>
<booktitle>In Proceedings ofNAACL,</booktitle>
<pages>213--216</pages>
<contexts>
<context position="2229" citStr="Huang et al. (2009)" startWordPosition="318" endWordPosition="321">ion tasks. The codes and newly annotated data are released for non-commercial usage.1 ∗Correspondence author. 1http://hlt.suda.edu.cn/˜zhli 1 Introduction The scale of available labeled data significantly affects the performance of statistical data-driven models. As a widely-used structural classification problem, sequence labeling is prone to suffer from the data sparseness issue. However, the heavy cost of manual annotation typically limits one labeled resource in both scale and genre. As a promising research line, semi-supervised learning for sequence labeling has been extensively studied. Huang et al. (2009) show that standard self-training can boost the performance of a simple hidden Markov model (HMM) based part-of-speech (POS) tagger. Søgaard (2011) apply tri-training to English POS tagging, boosting accuracy from 97.27% to 97.50%. Sun and Uszkoreit (2012) derive word clusters from largescale unlabeled data as extra features for Chinese POS tagging. Recently, the use of natural annotation has becomes a hot topic in Chinese word segmentation (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). The idea is to derive segmentation boundaries from implicit information encoded in web texts</context>
</contexts>
<marker>Huang, Eidelman, Harper, 2009</marker>
<rawString>Zhongqiang Huang, Vladimir Eidelman, and Mary Harper. 2009. Improving a simple bigram hmm part-of-speech tagger by latent annotation and selftraining. In Proceedings ofNAACL, pages 213–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic adaptation of annotation standards: Chinese word segmentation and POS tagging – a case study.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>522--530</pages>
<contexts>
<context position="4865" citStr="Jiang et al., 2009" startWordPosition="721" endWordPosition="724"> developed to support information extraction systems. However, the key challenge of exploiting the two resources is that they adopt different sets of POS tags which are impossible to be precisely converted from one to another based on heuristic rules. Figure 1 shows two example sentences from CTB and PD. Please refer to Table B.3 in Xia (2000) for detailed comparison of the two guidelines. Previous work on exploiting heterogeneous data (CTB and PD) mainly focuses on indirect guidefeature based methods. The basic idea is to use one resource to generate extra guide features on another resource (Jiang et al., 2009; Sun and Wan, 2012), which is similar to stacked learning (Nivre and McDonald, 2008). First, PD is used as source data to train a source model TaggerPD. Then, TaggerPD generates automatic POS tags on the target data CTB, called source annotations. Finally, a target model TaggerCTB-guided is trained on CTB, using source annotations as extra guide features. Although the guide-feature based method is effective in boosting performance of the target model, we argue that it may have two potential drawbacks. First, the target model TaggerCTB-guided does not directly use PD as training data, and ther</context>
<context position="23703" citStr="Jiang et al. (2009)" startWordPosition="3997" endWordPosition="4000">he baseline model. Particularly, when M′ = 1K, the model converges very slowly. However, from the trend of the curves, we expect that the accuracy gap between our coupled model with M′ = 5K/20K and the baseline model should be much smaller when reaching convergence. Based on the above observation, we adopt N′ = 5K and M′ = 5K in the following experiments. Moreover, we select the best iteration on the development data, and use the corresponding model to parse the test data. 5.2 Final Results Table 3 shows the final results on the CTB test data. We re-implement the guide-feature based method of Jiang et al. (2009), referred to as twostage CRF. Li et al. (2012) jointly models Chinese POS tagging and dependency parsing, and report the best tagging accuracy on CTB. The results show that our coupled model outperforms the baseline model by large margin, and also achieves slightly higher accuracy than the guide-feature based method. 5.3 Feature Study We conduct more experiments to measure individual contribution of each feature set, namely the joint features based on bundled tags and separate features based on single-side tags, as defined in Eq. (2). Table 4 shows the results. We can see that when only using</context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word segmentation and POS tagging – a case study. In Proceedings ofACL, pages 522–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Meng Sun</author>
<author>Yajuan L¨u</author>
<author>Yating Yang</author>
<author>Qun Liu</author>
</authors>
<title>Discriminative learning with natural annotations: Word segmentation as a case study.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>761--769</pages>
<marker>Jiang, Sun, L¨u, Yang, Liu, 2013</marker>
<rawString>Wenbin Jiang, Meng Sun, Yajuan L¨u, Yating Yang, and Qun Liu. 2013. Discriminative learning with natural annotations: Word segmentation as a case study. In Proceedings ofACL, pages 761–769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Jin</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning with multiple labels.</title>
<date>2002</date>
<booktitle>In Proceedings ofNIPS.</booktitle>
<contexts>
<context position="31131" citStr="Jin and Ghahramani, 2002" startWordPosition="5194" endWordPosition="5198">ned to jointly model two related (and typically hierarchical) sequential labeling tasks, such as POS tagging and chunking. In this work, our coupled CRF jointly models two same tasks which have different annotation schemes. Moreover, this work provides a natural way to 1790 learn from incomplete annotations where one sentence only contains one-side labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). 7 Conclusions This paper proposes an effective coupled sequence labeling model for exploiting multiple non-overlapping datasets with heterogeneous annotations. Please note that our model can also be naturally trained on datasets with both-side annota</context>
</contexts>
<marker>Jin, Ghahramani, 2002</marker>
<rawString>Rong Jin and Zoubin Ghahramani. 2002. Learning with multiple labels. In Proceedings ofNIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings ofICML</booktitle>
<pages>282--289</pages>
<contexts>
<context position="7663" citStr="Lafferty et al., 2001" startWordPosition="1180" endWordPosition="1183">anually annotated CTB tags for 1, 000 PD sentences, which is the first dataset with two-side annotations and can be used for annotation-conversion evaluation. Experiments on the newly annotated data show that our coupled model also works effectively on the annotation conversion task, improving conversion accuracy from 90.59% to 93.90% (+3.31%). 2 Traditional POS Tagging (TaggerCTB) Given an input sentence of n words, denoted by x = w1...wn, POS tagging aims to find an optimal tag sequence t = t1...tn, where ti E T (1 G i G n) and T is a predefined tag set. As a log-linear probabilistic model (Lafferty et al., 2001), CRF 3There are some slight differences in the word segmentation guidelines between CTB and PD, which are ignored in this work for simplicity. Bundled tags [NN,n] [NN,Ng] [NN,vn] [VV,v] [VE,v] [VC,v] [VA,v] tPR1 -1A2 -q;*j�3 A*4 APR1 kb2 A*3 9x-94 1784 01: ti o ti−1 02: ti o wi 03: ti o wi−1 04: ti o wi+1 05: ti o wi o ci−1,−1 06: ti o wi o ci+1,0 07: ti o ci,0 08: ti o ci,−1 9: ti o ci,k, 0 &lt; k &lt; #ci − 1 10: ti o ci,0 o ci,k, 0 &lt; k &lt; #ci − 1 11: ti o ci,−1 o ci,k, 0 &lt; k &lt; #ci − 1 12: if #ci = 1 then ti o wi o ci−1,−1 o ci+1,0 13: if ci,k = ci,k+1 then ti o ci,k o “consecutive” 14: ti o prefi</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings ofICML 2001, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Min Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>A separately passive-aggressive training algorithm for joint POS tagging and dependency parsing.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<pages>1681--1698</pages>
<contexts>
<context position="23750" citStr="Li et al. (2012)" startWordPosition="4007" endWordPosition="4010"> model converges very slowly. However, from the trend of the curves, we expect that the accuracy gap between our coupled model with M′ = 5K/20K and the baseline model should be much smaller when reaching convergence. Based on the above observation, we adopt N′ = 5K and M′ = 5K in the following experiments. Moreover, we select the best iteration on the development data, and use the corresponding model to parse the test data. 5.2 Final Results Table 3 shows the final results on the CTB test data. We re-implement the guide-feature based method of Jiang et al. (2009), referred to as twostage CRF. Li et al. (2012) jointly models Chinese POS tagging and dependency parsing, and report the best tagging accuracy on CTB. The results show that our coupled model outperforms the baseline model by large margin, and also achieves slightly higher accuracy than the guide-feature based method. 5.3 Feature Study We conduct more experiments to measure individual contribution of each feature set, namely the joint features based on bundled tags and separate features based on single-side tags, as defined in Eq. (2). Table 4 shows the results. We can see that when only using separate features, our coupled model achieves </context>
</contexts>
<marker>Li, Zhang, Che, Liu, 2012</marker>
<rawString>Zhenghua Li, Min Zhang, Wanxiang Che, and Ting Liu. 2012. A separately passive-aggressive training algorithm for joint POS tagging and dependency parsing. In COLING, pages 1681–1698.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Min Zhang</author>
<author>Wenliang Chen</author>
</authors>
<title>Ambiguity-aware ensemble training for semi-supervised dependency parsing.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>457--467</pages>
<contexts>
<context position="31246" citStr="Li et al., 2014" startWordPosition="5214" endWordPosition="5217"> this work, our coupled CRF jointly models two same tasks which have different annotation schemes. Moreover, this work provides a natural way to 1790 learn from incomplete annotations where one sentence only contains one-side labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). 7 Conclusions This paper proposes an effective coupled sequence labeling model for exploiting multiple non-overlapping datasets with heterogeneous annotations. Please note that our model can also be naturally trained on datasets with both-side annotations if such data exists. Experimental results demonstrate that our model work better than the baseline and guide-</context>
</contexts>
<marker>Li, Zhang, Chen, 2014</marker>
<rawString>Zhenghua Li, Min Zhang, and Wenliang Chen. 2014a. Ambiguity-aware ensemble training for semi-supervised dependency parsing. In ACL, pages 457–467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Min Zhang</author>
<author>Wenliang Chen</author>
</authors>
<title>Soft cross-lingual syntax projection for dependency parsing. In</title>
<date>2014</date>
<booktitle>COLING,</booktitle>
<pages>783--793</pages>
<contexts>
<context position="31246" citStr="Li et al., 2014" startWordPosition="5214" endWordPosition="5217"> this work, our coupled CRF jointly models two same tasks which have different annotation schemes. Moreover, this work provides a natural way to 1790 learn from incomplete annotations where one sentence only contains one-side labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). 7 Conclusions This paper proposes an effective coupled sequence labeling model for exploiting multiple non-overlapping datasets with heterogeneous annotations. Please note that our model can also be naturally trained on datasets with both-side annotations if such data exists. Experimental results demonstrate that our model work better than the baseline and guide-</context>
</contexts>
<marker>Li, Zhang, Chen, 2014</marker>
<rawString>Zhenghua Li, Min Zhang, and Wenliang Chen. 2014b. Soft cross-lingual syntax projection for dependency parsing. In COLING, pages 783–793.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yijia Liu</author>
<author>Yue Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
<author>Fan Wu</author>
</authors>
<title>Domain adaptation for CRF-based Chinese word segmentation using free annotations.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>864--874</pages>
<contexts>
<context position="2711" citStr="Liu et al., 2014" startWordPosition="396" endWordPosition="399"> genre. As a promising research line, semi-supervised learning for sequence labeling has been extensively studied. Huang et al. (2009) show that standard self-training can boost the performance of a simple hidden Markov model (HMM) based part-of-speech (POS) tagger. Søgaard (2011) apply tri-training to English POS tagging, boosting accuracy from 97.27% to 97.50%. Sun and Uszkoreit (2012) derive word clusters from largescale unlabeled data as extra features for Chinese POS tagging. Recently, the use of natural annotation has becomes a hot topic in Chinese word segmentation (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). The idea is to derive segmentation boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in sequence labeling models. The existence of multiple annotated resources opens another door for alleviating data sparseness. For example, Penn Chinese Treebank (CTB) contains about 20 thousand sentences annotated with word boundaries, POS tags, and syntactic structures (Xue et al., 2005), which is widely used for research on Chinese word segmentation and POS tagging. People’s Daily c</context>
<context position="31455" citStr="Liu et al., 2014" startWordPosition="5245" endWordPosition="5248">contains one-side labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). 7 Conclusions This paper proposes an effective coupled sequence labeling model for exploiting multiple non-overlapping datasets with heterogeneous annotations. Please note that our model can also be naturally trained on datasets with both-side annotations if such data exists. Experimental results demonstrate that our model work better than the baseline and guide-feature based methods on both one-side POS tagging and annotation conversion. Specifically, detailed analysis shows several interesting findings. First, both the separate features and joint features are indisp</context>
</contexts>
<marker>Liu, Zhang, Che, Liu, Wu, 2014</marker>
<rawString>Yijia Liu, Yue Zhang, Wanxiang Che, Ting Liu, and Fan Wu. 2014. Domain adaptation for CRF-based Chinese word segmentation using free annotations. In Proceedings of EMNLP, pages 864–874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>950--958</pages>
<contexts>
<context position="4950" citStr="Nivre and McDonald, 2008" startWordPosition="735" endWordPosition="738">ge of exploiting the two resources is that they adopt different sets of POS tags which are impossible to be precisely converted from one to another based on heuristic rules. Figure 1 shows two example sentences from CTB and PD. Please refer to Table B.3 in Xia (2000) for detailed comparison of the two guidelines. Previous work on exploiting heterogeneous data (CTB and PD) mainly focuses on indirect guidefeature based methods. The basic idea is to use one resource to generate extra guide features on another resource (Jiang et al., 2009; Sun and Wan, 2012), which is similar to stacked learning (Nivre and McDonald, 2008). First, PD is used as source data to train a source model TaggerPD. Then, TaggerPD generates automatic POS tags on the target data CTB, called source annotations. Finally, a target model TaggerCTB-guided is trained on CTB, using source annotations as extra guide features. Although the guide-feature based method is effective in boosting performance of the target model, we argue that it may have two potential drawbacks. First, the target model TaggerCTB-guided does not directly use PD as training data, and therefore fails to make full use of rich language phenomena in PD. Second, the method is </context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings ofACL, pages 950–958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer-intensive methods for testing hypotheses: An introduction.</title>
<date>1989</date>
<publisher>John Wiley &amp; Sons, Inc.,</publisher>
<location>New York.</location>
<contexts>
<context position="18296" citStr="Noreen, 1989" startWordPosition="3091" endWordPosition="3092">uct experiments to verify the effectiveness of our approach. We adopt CTB (version 5.1) with the standard data split, and randomly split PD into four sets, among which one set is 20% partially annotated with CTB tags. The data statistics is shown in Table 2. The main concern of this work is to improve accuracy on CTB by exploring large-scale PD, since CTB is relatively small, but is widely-used benchmark data in the research community. We use the standard token-wise tagging accuracy as the evaluation metric. For significance test, we adopt Dan Bikel’s randomized parsing evaluation comparator (Noreen, 1989).5. The baseline CRF is trained on either CTB training data with 33 tags, or PD training data with 38 tags. The coupled CRF is trained on both two separate training datasets with bundled tags (179 tags for the relaxed mapping function). During evaluation, the coupled CRF is not directly evaluated on bundled tags, since bundled tags are unavailable in either CTB or PD test data. Instead, the coupled and baseline CRFs are both evaluated on one-side tags. 5.1 Model Development Our coupled model has two major parameters to be decided. The first parameter is to determine the mapping function betwee</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer-intensive methods for testing hypotheses: An introduction. John Wiley &amp; Sons, Inc., New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Parameswaran</author>
<author>K Q Weinberger</author>
</authors>
<title>Large margin multi-task metric learning.</title>
<date>2010</date>
<booktitle>Advances in Neural Information Processing Systems 23,</booktitle>
<pages>1867--1875</pages>
<editor>In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors,</editor>
<contexts>
<context position="30107" citStr="Parameswaran and Weinberger, 2010" startWordPosition="5034" endWordPosition="5037">uence labeling problems. Zhang et al. (2014) propose a shift-reduce dependency parsing model which can simultaneously learn and produce two heterogeneous parse trees. However, their approach assumes the existence of data with annotations at both sides, which is obtained by converting phrase-structure trees into dependency trees with different heuristic rules. This work is also closely related with multitask learning, which aims to jointly learn multiple related tasks with the benefit of using interactive features under a share representation (BenDavid and Schuller, 2003; Ando and Zhang, 2005; Parameswaran and Weinberger, 2010). However, according to our knowledge, multi-task learning typically assumes the existence of data with labels for multiple tasks at the same time, which is unavailable in our situation. As one reviewer kindly pointed out that our model is a factorial CRF (Sutton et al., 2004), in the sense that the bundled tags can be factorized two connected latent variables. Initially, factorial CRFs are designed to jointly model two related (and typically hierarchical) sequential labeling tasks, such as POS tagging and chunking. In this work, our coupled CRF jointly models two same tasks which have differe</context>
</contexts>
<marker>Parameswaran, Weinberger, 2010</marker>
<rawString>S. Parameswaran and K.Q. Weinberger. 2010. Large margin multi-task metric learning. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1867–1875.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ciyang Qing</author>
</authors>
<title>Ulle Endriss,</title>
<location>Raquel Fernandez, and</location>
<marker>Qing, </marker>
<rawString>Ciyang Qing, Ulle Endriss, Raquel Fernandez, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Kruger</author>
</authors>
<title>Empirical analysis of aggregation methods for collective annotation. In</title>
<date>2014</date>
<booktitle>COLING,</booktitle>
<pages>1533--1542</pages>
<marker>Kruger, 2014</marker>
<rawString>Justin Kruger. 2014. Empirical analysis of aggregation methods for collective annotation. In COLING, pages 1533–1542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xipeng Qiu</author>
<author>Feng Ji</author>
<author>Jiayi Zhao</author>
<author>Xuanjing Huang</author>
</authors>
<title>Joint segmentation and tagging with coupled sequences labeling.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>951--964</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="29380" citStr="Qiu et al. (2012)" startWordPosition="4923" endWordPosition="4926">otations. They adopt an approximate decoding algorithm which tries to find the best single-side tag sequence with reference to tags at the other side. In contrast, our approach is a direct extension of traditional CRF, and is more theoretically simple from the perspective of modelling. The use of both joint and separate features is proven to be crucial for the success of our coupled model. In addition, their work indicates that their model relies on a hand-crafted loose mapping between annotations, which is opposite to our findings. The naming of the “coupled” CRF is borrowed from the work of Qiu et al. (2012), which treats the joint task of Chinese word segmentation and POS tagging as two coupled sequence labeling problems. Zhang et al. (2014) propose a shift-reduce dependency parsing model which can simultaneously learn and produce two heterogeneous parse trees. However, their approach assumes the existence of data with annotations at both sides, which is obtained by converting phrase-structure trees into dependency trees with different heuristic rules. This work is also closely related with multitask learning, which aims to jointly learn multiple related tasks with the benefit of using interacti</context>
</contexts>
<marker>Qiu, Ji, Zhao, Huang, 2012</marker>
<rawString>Xipeng Qiu, Feng Ji, Jiayi Zhao, and Xuanjing Huang. 2012. Joint segmentation and tagging with coupled sequences labeling. In Proceedings of COLING 2012: Posters, pages 951–964, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xipeng Qiu</author>
<author>Jiayi Zhao</author>
<author>Xuanjing Huang</author>
</authors>
<title>Joint Chinese word segmentation and POS tagging on heterogeneous annotated corpora with multiple task learning.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>658--668</pages>
<contexts>
<context position="28364" citStr="Qiu et al. (2013)" startWordPosition="4759" endWordPosition="4762">dled tag set. In practice, we usually only need results following one annotation style. Therefore, we employ our coupled model to convert PD into the style of CTB, and train our baseline model with two training data with homogeneous annotations. Again, Algorithm 1 is used to merge the two data with N′ = 5K and M′ = 5K. The results are shown in the bottom row in Table 6. We can see that with the extra converted data, the baseline model can achieve slightly lower accuracy with the coupled model and avoid the inefficiency problem at the meantime. 6 Related Work This work is partially inspired by Qiu et al. (2013), who propose a model that performs heterogeneous Chinese word segmentation and POS tagging and produces two sets of results following CTB and PD styles respectively. Different from our CRFbased coupled model, their approach adopts a linear model, which directly combines two separate sets of features based on single-side tags, without considering the interacting joint features between the two annotations. They adopt an approximate decoding algorithm which tries to find the best single-side tag sequence with reference to tags at the other side. In contrast, our approach is a direct extension of</context>
</contexts>
<marker>Qiu, Zhao, Huang, 2013</marker>
<rawString>Xipeng Qiu, Jiayi Zhao, and Xuanjing Huang. 2013. Joint Chinese word segmentation and POS tagging on heterogeneous annotated corpora with multiple task learning. In Proceedings of EMNLP, pages 658–668.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the wall street journal using a lexical-functional grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="31203" citStr="Riezler et al., 2002" startWordPosition="5206" endWordPosition="5209">ling tasks, such as POS tagging and chunking. In this work, our coupled CRF jointly models two same tasks which have different annotation schemes. Moreover, this work provides a natural way to 1790 learn from incomplete annotations where one sentence only contains one-side labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). 7 Conclusions This paper proposes an effective coupled sequence labeling model for exploiting multiple non-overlapping datasets with heterogeneous annotations. Please note that our model can also be naturally trained on datasets with both-side annotations if such data exists. Experimental results demonstrate that our mod</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. III Maxwell, and Mark Johnson. 2002. Parsing the wall street journal using a lexical-functional grammar and discriminative estimation techniques. In Proceedings of ACL, pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Semi-supervised condensed nearest neighbor for part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>48--52</pages>
<contexts>
<context position="2376" citStr="Søgaard (2011)" startWordPosition="341" endWordPosition="342">on The scale of available labeled data significantly affects the performance of statistical data-driven models. As a widely-used structural classification problem, sequence labeling is prone to suffer from the data sparseness issue. However, the heavy cost of manual annotation typically limits one labeled resource in both scale and genre. As a promising research line, semi-supervised learning for sequence labeling has been extensively studied. Huang et al. (2009) show that standard self-training can boost the performance of a simple hidden Markov model (HMM) based part-of-speech (POS) tagger. Søgaard (2011) apply tri-training to English POS tagging, boosting accuracy from 97.27% to 97.50%. Sun and Uszkoreit (2012) derive word clusters from largescale unlabeled data as extra features for Chinese POS tagging. Recently, the use of natural annotation has becomes a hot topic in Chinese word segmentation (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). The idea is to derive segmentation boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in sequence labeling models. The existence of multipl</context>
</contexts>
<marker>Søgaard, 2011</marker>
<rawString>Anders Søgaard. 2011. Semi-supervised condensed nearest neighbor for part-of-speech tagging. In Proceedings ofACL, pages 48–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Hans Uszkoreit</author>
</authors>
<title>Capturing paradigmatic and syntagmatic lexical relations: Towards accurate Chinese part-of-speech tagging.</title>
<date>2012</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>242--252</pages>
<contexts>
<context position="2485" citStr="Sun and Uszkoreit (2012)" startWordPosition="357" endWordPosition="360">iven models. As a widely-used structural classification problem, sequence labeling is prone to suffer from the data sparseness issue. However, the heavy cost of manual annotation typically limits one labeled resource in both scale and genre. As a promising research line, semi-supervised learning for sequence labeling has been extensively studied. Huang et al. (2009) show that standard self-training can boost the performance of a simple hidden Markov model (HMM) based part-of-speech (POS) tagger. Søgaard (2011) apply tri-training to English POS tagging, boosting accuracy from 97.27% to 97.50%. Sun and Uszkoreit (2012) derive word clusters from largescale unlabeled data as extra features for Chinese POS tagging. Recently, the use of natural annotation has becomes a hot topic in Chinese word segmentation (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). The idea is to derive segmentation boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in sequence labeling models. The existence of multiple annotated resources opens another door for alleviating data sparseness. For example, Penn Chinese Treebank </context>
</contexts>
<marker>Sun, Uszkoreit, 2012</marker>
<rawString>Weiwei Sun and Hans Uszkoreit. 2012. Capturing paradigmatic and syntagmatic lexical relations: Towards accurate Chinese part-of-speech tagging. In Proceedings ofACL, pages 242–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Xiaojun Wan</author>
</authors>
<title>Reducing approximation and estimation errors for Chinese lexical processing with heterogeneous annotations.</title>
<date>2012</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>232--241</pages>
<contexts>
<context position="4885" citStr="Sun and Wan, 2012" startWordPosition="725" endWordPosition="728">t information extraction systems. However, the key challenge of exploiting the two resources is that they adopt different sets of POS tags which are impossible to be precisely converted from one to another based on heuristic rules. Figure 1 shows two example sentences from CTB and PD. Please refer to Table B.3 in Xia (2000) for detailed comparison of the two guidelines. Previous work on exploiting heterogeneous data (CTB and PD) mainly focuses on indirect guidefeature based methods. The basic idea is to use one resource to generate extra guide features on another resource (Jiang et al., 2009; Sun and Wan, 2012), which is similar to stacked learning (Nivre and McDonald, 2008). First, PD is used as source data to train a source model TaggerPD. Then, TaggerPD generates automatic POS tags on the target data CTB, called source annotations. Finally, a target model TaggerCTB-guided is trained on CTB, using source annotations as extra guide features. Although the guide-feature based method is effective in boosting performance of the target model, we argue that it may have two potential drawbacks. First, the target model TaggerCTB-guided does not directly use PD as training data, and therefore fails to make </context>
</contexts>
<marker>Sun, Wan, 2012</marker>
<rawString>Weiwei Sun and Xiaojun Wan. 2012. Reducing approximation and estimation errors for Chinese lexical processing with heterogeneous annotations. In Proceedings ofACL, pages 232–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Takuya Matsuzaki</author>
<author>Daisuke Okanohara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Latent variable perceptron algorithm for structured classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<pages>1236--1242</pages>
<contexts>
<context position="30995" citStr="Sun et al. (2009)" startWordPosition="5174" endWordPosition="5177"> 2004), in the sense that the bundled tags can be factorized two connected latent variables. Initially, factorial CRFs are designed to jointly model two related (and typically hierarchical) sequential labeling tasks, such as POS tagging and chunking. In this work, our coupled CRF jointly models two same tasks which have different annotation schemes. Moreover, this work provides a natural way to 1790 learn from incomplete annotations where one sentence only contains one-side labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). 7 Conclusions This paper proposes an effective coupled sequence labeling model for exploiting multiple non-overlap</context>
</contexts>
<marker>Sun, Matsuzaki, Okanohara, Tsujii, 2009</marker>
<rawString>Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, and Jun’ichi Tsujii. 2009. Latent variable perceptron algorithm for structured classification. In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI 2009), pages 1236–1242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Khashayar Rohanimanesh</author>
<author>Andrew McCallum</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2004</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="30384" citStr="Sutton et al., 2004" startWordPosition="5079" endWordPosition="5082">-structure trees into dependency trees with different heuristic rules. This work is also closely related with multitask learning, which aims to jointly learn multiple related tasks with the benefit of using interactive features under a share representation (BenDavid and Schuller, 2003; Ando and Zhang, 2005; Parameswaran and Weinberger, 2010). However, according to our knowledge, multi-task learning typically assumes the existence of data with labels for multiple tasks at the same time, which is unavailable in our situation. As one reviewer kindly pointed out that our model is a factorial CRF (Sutton et al., 2004), in the sense that the bundled tags can be factorized two connected latent variables. Initially, factorial CRFs are designed to jointly model two related (and typically hierarchical) sequential labeling tasks, such as POS tagging and chunking. In this work, our coupled CRF jointly models two same tasks which have different annotation schemes. Moreover, this work provides a natural way to 1790 learn from incomplete annotations where one sentence only contains one-side labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et</context>
</contexts>
<marker>Sutton, Rohanimanesh, McCallum, 2004</marker>
<rawString>Charles Sutton, Khashayar Rohanimanesh, and Andrew McCallum. 2004. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Target language adaptation of discriminative transfer parsers.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>1061--1071</pages>
<marker>T¨ackstr¨om, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Joakim Nivre. 2013. Target language adaptation of discriminative transfer parsers. In Proceedings of NAACL, pages 1061–1071.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>The part-of-speech tagging guidelines for the penn Chinese treebank 3.0. In</title>
<date>2000</date>
<tech>Technical Report, Linguistic Data Consortium.</tech>
<contexts>
<context position="4592" citStr="Xia (2000)" startWordPosition="679" endWordPosition="680">form a one-side tag into a set of bundled tags. “NN” and “n” represent nouns; “VV”and “v” represent verbs. Daily newspaper (see Table 2). The two resources were independently built for different purposes. CTB was designed to serve syntactic analysis, whereas PD was developed to support information extraction systems. However, the key challenge of exploiting the two resources is that they adopt different sets of POS tags which are impossible to be precisely converted from one to another based on heuristic rules. Figure 1 shows two example sentences from CTB and PD. Please refer to Table B.3 in Xia (2000) for detailed comparison of the two guidelines. Previous work on exploiting heterogeneous data (CTB and PD) mainly focuses on indirect guidefeature based methods. The basic idea is to use one resource to generate extra guide features on another resource (Jiang et al., 2009; Sun and Wan, 2012), which is similar to stacked learning (Nivre and McDonald, 2008). First, PD is used as source data to train a source model TaggerPD. Then, TaggerPD generates automatic POS tags on the target data CTB, called source annotations. Finally, a target model TaggerCTB-guided is trained on CTB, using source annot</context>
</contexts>
<marker>Xia, 2000</marker>
<rawString>Fei Xia. 2000. The part-of-speech tagging guidelines for the penn Chinese treebank 3.0. In Technical Report, Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese Treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>In Natural Language Engineering,</journal>
<volume>11</volume>
<pages>207--238</pages>
<contexts>
<context position="3213" citStr="Xue et al., 2005" startWordPosition="471" endWordPosition="474">se of natural annotation has becomes a hot topic in Chinese word segmentation (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). The idea is to derive segmentation boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in sequence labeling models. The existence of multiple annotated resources opens another door for alleviating data sparseness. For example, Penn Chinese Treebank (CTB) contains about 20 thousand sentences annotated with word boundaries, POS tags, and syntactic structures (Xue et al., 2005), which is widely used for research on Chinese word segmentation and POS tagging. People’s Daily corpus (PD)2 is a large-scale corpus annotated with word segments and POS tags, containing about 300 thousand sentences from the first half of 1998 of People’s 2http://icl.pku.edu.cn/icl_groups/ corpustagging.asp 1783 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1783–1792, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics China focuses on econo</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese Treebank: Phrase structure annotation of a large corpus. In Natural Language Engineering, volume 11, pages 207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Yang</author>
<author>Paul Vozila</author>
</authors>
<title>Semi-supervised Chinese word segmentation using partial-label learning with conditional random fields.</title>
<date>2014</date>
<booktitle>In Proceedings ofEMNLP,</booktitle>
<pages>90--98</pages>
<contexts>
<context position="2735" citStr="Yang and Vozila, 2014" startWordPosition="400" endWordPosition="403">sing research line, semi-supervised learning for sequence labeling has been extensively studied. Huang et al. (2009) show that standard self-training can boost the performance of a simple hidden Markov model (HMM) based part-of-speech (POS) tagger. Søgaard (2011) apply tri-training to English POS tagging, boosting accuracy from 97.27% to 97.50%. Sun and Uszkoreit (2012) derive word clusters from largescale unlabeled data as extra features for Chinese POS tagging. Recently, the use of natural annotation has becomes a hot topic in Chinese word segmentation (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). The idea is to derive segmentation boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in sequence labeling models. The existence of multiple annotated resources opens another door for alleviating data sparseness. For example, Penn Chinese Treebank (CTB) contains about 20 thousand sentences annotated with word boundaries, POS tags, and syntactic structures (Xue et al., 2005), which is widely used for research on Chinese word segmentation and POS tagging. People’s Daily corpus (PD)2 is a large-s</context>
<context position="31479" citStr="Yang and Vozila, 2014" startWordPosition="5249" endWordPosition="5252">labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). 7 Conclusions This paper proposes an effective coupled sequence labeling model for exploiting multiple non-overlapping datasets with heterogeneous annotations. Please note that our model can also be naturally trained on datasets with both-side annotations if such data exists. Experimental results demonstrate that our model work better than the baseline and guide-feature based methods on both one-side POS tagging and annotation conversion. Specifically, detailed analysis shows several interesting findings. First, both the separate features and joint features are indispensable components for t</context>
</contexts>
<marker>Yang, Vozila, 2014</marker>
<rawString>Fan Yang and Paul Vozila. 2014. Semi-supervised Chinese word segmentation using partial-label learning with conditional random fields. In Proceedings ofEMNLP, pages 90–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and POS tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>888--896</pages>
<contexts>
<context position="8991" citStr="Zhang and Clark, 2008" startWordPosition="1465" endWordPosition="1468"> i, ti_1, ti). o means string concatenation; ci,k denotes the kth Chinese character of wi; ci,0 is the first Chinese character; ci,_1 is the last Chinese character; #ci is the total number of Chinese characters contained in wi; prefix/suffix(wi, k) denote the kCharacter prefix/suffix of wi. defines the probability of a tag sequence as: exp(Score(x, t; θ)) P(t|x; θ) _ &apos; t′ exp(Score(x, t′; θ)) ( ) Score(x, t; θ) _ 1: θ · f (x, i, ti_1, ti) 1 1&lt;i&lt;n where f(x, i, ti_1, ti) is the feature vector at the ith word and θ is the weight vector. We adopt the state-of-the-art tagging features in Table 1 (Zhang and Clark, 2008). 3 Coupled POS Tagging (TaggerCTB&amp;PD) In this section, we introduce our coupled model, which is able to learn and predict two heterogeneous annotations simultaneously. The idea is to bundle two sets of POS tags together and let the CRF-based model work in the enlarged tag space. For example, a CTB tag “NN” and a PD tag “n” would be bundled into “[NN,n]”. Figure 2 shows the graphical structure of our model. Different from the traditional model in Eq. (1), our coupled model defines the score of a bundled tag sequence as follows: Score(x, [ta, tb]; θ) _ � f(x, i, [ta i_1, tbi_1], [tai , tbi]) � </context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and POS tagging using a single perceptron. In Proceedings ofACL-08: HLT, pages 888–896.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meishan Zhang</author>
<author>Wanxiang Che</author>
<author>Yanqiu Shao</author>
<author>Ting Liu</author>
</authors>
<title>Jointly or separately: Which is better for parsing heterogeneous dependencies?</title>
<date>2014</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>530--540</pages>
<contexts>
<context position="29517" citStr="Zhang et al. (2014)" startWordPosition="4945" endWordPosition="4948">the other side. In contrast, our approach is a direct extension of traditional CRF, and is more theoretically simple from the perspective of modelling. The use of both joint and separate features is proven to be crucial for the success of our coupled model. In addition, their work indicates that their model relies on a hand-crafted loose mapping between annotations, which is opposite to our findings. The naming of the “coupled” CRF is borrowed from the work of Qiu et al. (2012), which treats the joint task of Chinese word segmentation and POS tagging as two coupled sequence labeling problems. Zhang et al. (2014) propose a shift-reduce dependency parsing model which can simultaneously learn and produce two heterogeneous parse trees. However, their approach assumes the existence of data with annotations at both sides, which is obtained by converting phrase-structure trees into dependency trees with different heuristic rules. This work is also closely related with multitask learning, which aims to jointly learn multiple related tasks with the benefit of using interactive features under a share representation (BenDavid and Schuller, 2003; Ando and Zhang, 2005; Parameswaran and Weinberger, 2010). However,</context>
</contexts>
<marker>Zhang, Che, Shao, Liu, 2014</marker>
<rawString>Meishan Zhang, Wanxiang Che, Yanqiu Shao, and Ting Liu. 2014. Jointly or separately: Which is better for parsing heterogeneous dependencies? In Proceedings of COLING, pages 530–540.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>