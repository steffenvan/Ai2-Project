<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000960">
<title confidence="0.9975385">
Joint Determination of Anaphoricity and Coreference Resolution using
Integer Programming
</title>
<author confidence="0.996313">
Pascal Denis and Jason Baldridge
</author>
<affiliation confidence="0.9983895">
Department of Linguistics
University of Texas at Austin
</affiliation>
<email confidence="0.99939">
{denis,jbaldrid}@mail.utexas.edu
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999776642857143">
Standard pairwise coreference resolution
systems are subject to errors resulting
from their performing anaphora identifi-
cation as an implicit part of coreference
resolution. In this paper, we propose
an integer linear programming (ILP) for-
mulation for coreference resolution which
models anaphoricity and coreference as a
joint task, such that each local model in-
forms the other for the final assignments.
This joint ILP formulation provides f-
score improvements of 3.7-5.3% over a
base coreference classifier on the ACE
datasets.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99982744">
The task of coreference resolution involves impos-
ing a partition on a set of entity mentions in a docu-
ment, where each partition corresponds to some en-
tity in an underlying discourse model. Most work
treats coreference resolution as a binary classifica-
tion task in which each decision is made in a pair-
wise fashion, independently of the others (McCarthy
and Lehnert, 1995; Soon et al., 2001; Ng and Cardie,
2002b; Morton, 2000; Kehler et al., 2004).
There are two major drawbacks with most sys-
tems that make pairwise coreference decisions. The
first is that identification of anaphora is done implic-
itly as part of the coreference resolution. Two com-
mon types of errors with these systems are cases
where: (i) the system mistakenly identifies an an-
tecedent for non-anaphoric mentions, and (ii) the
system does not try to resolve an actual anaphoric
mention. To reduce such errors, Ng and Cardie
(2002a) and Ng (2004) use an anaphoricity classi-
fier –which has the sole task of saying whether or
not any antecedents should be identified for each
mention– as a filter for their coreference system.
They achieve higher performance by doing so; how-
ever, their setup uses the two classifiers in a cascade.
This requires careful determination of an anaphoric-
ity threshold in order to not remove too many men-
tions from consideration (Ng, 2004). This sensi-
tivity is unsurprising, given that the tasks are co-
dependent.
The second problem is that most coreference sys-
tems make each decision independently of previous
ones in a greedy fashion (McCallum and Wellner,
2004). Clearly, the determination of membership of
a particular mention into a partition should be condi-
tioned on how well it matches the entity as a whole.
Since independence between decisions is an unwar-
ranted assumption for the task, models that consider
a more global context are likely to be more appropri-
ate. Recent work has examined such models; Luo et
al. (2004) using Bell trees, and McCallum and Well-
ner (2004) using conditional random fields, and Ng
(2005) using rerankers.
In this paper, we propose to recast the task of
coreference resolution as an optimization problem,
namely an integer linear programming (ILP) prob-
lem. This framework has several properties that
make it highly suitable for addressing the two afore-
mentioned problems. The first is that it can uti-
lize existing classifiers; ILP performs global infer-
ence based on their output rather than formulating a
</bodyText>
<page confidence="0.977505">
236
</page>
<note confidence="0.798198">
Proceedings of NAACL HLT 2007, pages 236–243,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999954195121951">
new inference procedure for solving the basic task.
Second, the ILP approach supports inference over
multiple classifiers, without having to fiddle with
special parameterization. Third, it is much more
efficient than conditional random fields, especially
when long-distance features are utilized (Roth and
Yih, 2005). Finally, it is straightforward to create
categorical global constraints with ILP; this is done
in a declarative manner using inequalities on the as-
signments to indicator variables.
This paper focuses on the first problem, and
proposes to model anaphoricity determination and
coreference resolution as a joint task, wherein the
decisions made by each locally trained model are
mutually constrained. The presentation of the ILP
model proceeds in two steps. In the first, interme-
diary step, we simply use ILP to find a global as-
signment based on decisions made by the corefer-
ence classifier alone. The resulting assignment is
one that maximally agrees with the decisions of the
classifier, that is, where all and only the links pre-
dicted to be coreferential are used for constructing
the chains. This is in contrast with the usual clus-
tering algorithms, in which a unique antecedent is
typically picked for each anaphor (e.g., the most
probable or the most recent). The second step pro-
vides the joint formulation: the coreference classi-
fier is now combined with an anaphoricity classifier
and constraints are added to ensure that the ultimate
coreference and anaphoricity decisions are mutually
consistent. Both of these formulations achieve sig-
nificant performance gains over the base classifier.
Specifically, the joint model achieves f-score im-
provements of 3.7-5.3% on three datasets.
We begin by presenting the basic coreference
classifier and anaphoricity classifier and their per-
formance, including an upperbound that shows the
limitation of using them in a cascade. We then give
the details of our ILP formulations and evaluate their
performance with respect to each other and the base
classifier.
</bodyText>
<sectionHeader confidence="0.786576" genericHeader="introduction">
2 Base models: coreference classifier
</sectionHeader>
<bodyText confidence="0.999873823529412">
The classification approach tackles coreference
in two steps by: (i) estimating the probability,
PC(COREF|hi, ji), of having a coreferential out-
come given a pair of mentions hi, ji, and (ii) apply-
ing a selection algorithm that will single out a unique
candidate out of the subset of candidates i for which
the probability PC(COREF|hi, ji) reaches a particu-
lar value (typically .5).
We use a maximum entropy model for the coref-
erence classifier. Such models are well-suited for
coreference, because they are able to handle many
different, potentially overlapping learning features
without making independence assumptions. Previ-
ous work on coreference using maximum entropy
includes (Kehler, 1997; Morton, 1999; Morton,
2000). The model is defined in a standard fashion
as follows:
</bodyText>
<equation confidence="0.91263175">
λkfk(hi,ji, COREF))
Z(hi, ji)
(1)
Z(hi, ji) is a normalization factor over both out-
</equation>
<bodyText confidence="0.999336814814815">
comes (COREF and ¬COREF). Model parameters
are estimated using maximum entropy (Berger et al.,
1996). Specifically, we estimate parameters with
the limited memory variable metric algorithm imple-
mented in the Toolkit for Advanced Discriminative
Modeling1 (Malouf, 2002). We use a Gaussian prior
with a variance of 1000 — no attempt was made to
optimize this value.
Training instances for the coreference classifier
are constructed based on pairs of mentions of the
form hi, ji, where j and i are the descriptions for
an anaphor and one of its candidate antecedents, re-
spectively. Each such pair is assigned either a label
COREF (i.e. a positive instance) or a label ¬COREF
(i.e. a negative instance) depending on whether or
not the two mentions corefer. In generating the train-
ing data, we followed the method of (Soon et al.,
2001) creating for each anaphor: (i) a positive in-
stance for the pair hi, ji where i is the closest an-
tecedent for j, and (ii) a negative instance for each
pair hi, ki where k intervenes between i and j.
Once trained, the classifier is used to create a set
of coreferential links for each test document; these
links in turn define a partition over the entire set of
mentions. In the system of Soon et. al. (2001) sys-
tem, this is done by pairing each mention j with each
preceding mention i. Each test instance hi, ji thus
</bodyText>
<footnote confidence="0.880911">
1Available from tadm.sf.net.
</footnote>
<equation confidence="0.998142">
PC(COREF|hi,ji) =
n
E
k=1
exp(
</equation>
<page confidence="0.991084">
237
</page>
<bodyText confidence="0.99997145">
formed is then evaluated by the classifier, which re-
turns a probability representing the likelihood that
these two mentions are coreferential. Soon et. al.
(2001) use “Closest-First” selection: that is, the pro-
cess terminates as soon as an antecedent (i.e., a test
instance with probability &gt; .5) is found or the be-
ginning of the text is reached. Another option is to
pick the antecedent with the best overall probability
(Ng and Cardie, 2002b).
Our features for the coreference classifier fall into
three main categories: (i) features of the anaphor, (ii)
features of antecedent mention, and (iii) relational
features (i.e., features that describe properties which
hold between the two mentions, e.g. distance). This
feature set is similar (though not equivalent) to that
used by Ng and Cardie (2002a). We omit details
here for the sake of brevity — the ILP systems we
employ here could be equally well applied to many
different base classifiers using many different fea-
ture sets.
</bodyText>
<sectionHeader confidence="0.940778" genericHeader="method">
3 Base models: anaphoricity classifier
</sectionHeader>
<bodyText confidence="0.99966478125">
As mentioned in the introduction, coreference clas-
sifiers such as that presented in section 2 suf-
fer from errors in which (a) they assign an an-
tecedent to a non-anaphor mention or (b) they as-
sign no antecedents to an anaphoric mention. Ng
and Cardie (2002a) suggest overcoming such fail-
ings by augmenting their coreference classifier with
an anaphoricity classifier which acts as a filter dur-
ing model usage. Only the mentions that are deemed
anaphoric are considered for coreference resolu-
tion. Interestingly, they find a degredation in per-
formance. In particular, they obtain significant im-
provements in precision, but with larger losses in
recall (especially for proper names and common
nouns). To counteract this, they add ad hoc con-
straints based on string matching and extended men-
tion matching which force certain mentions to be
resolved as anaphors regardless of the anaphoric-
ity classifier. This allows them to improve overall
f-scores by 1-3%. Ng (2004) obtains f-score im-
provements of 2.8-4.5% by tuning the anaphoricity
threshold on held-out data.
The task for the anaphoricity determination com-
ponent is the following: one wants to decide for each
mention i in a document whether i is anaphoric or
not. That is, this task can be performed using a sim-
ple binary classifier with two outcomes: ANAPH and
ANAPH. The classifier estimates the conditional
probabilities P(ANAPH|i) and predicts ANAPH for i
when P(ANAPH|i) &gt; .5.
We use the following model for our anaphoricity
classifier:
</bodyText>
<equation confidence="0.741622333333333">
Akfk(i, ANAPH))
(2)
Z(i)
</equation>
<bodyText confidence="0.9991028">
This model is trained in the same manner as the
coreference classifier, also with a Gaussian prior
with a variance of 1000.
The features used for the anaphoricity classifier
are quite simple. They include information regard-
ing (1) the mention itself, such as the number of
words and whether it is a pronoun, and (2) properties
of the potential antecedent set, such as the number of
preceding mentions and whether there is a previous
mention with a matching string.
</bodyText>
<sectionHeader confidence="0.973466" genericHeader="method">
4 Base model results
</sectionHeader>
<bodyText confidence="0.999880583333333">
This section provides the performance of the pair-
wise coreference classifier, both when used alone
(COREF-PAIRWISE) and when used in a cascade
where the anaphoricity classifier acts as a filter on
which mentions should be resolved (AC-CASCADE).
In both systems, antecedents are determined in the
manner described in section 2.
To demonstrate the inherent limitations of cas-
cading, we also give results for an oracle sys-
tem, ORACLE-LINK, which assumes perfect linkage.
That is, it always picks the correct antecedent for
an anaphor. Its only errors are due to being un-
able to resolve mentions which were marked as non-
anaphoric (by the imperfect anaphoricity classifier)
when in fact they were anaphoric.
We evaluate these systems on the datasets from
the ACE corpus (Phase 2). This corpus is di-
vided into three parts, each corresponding to a dif-
ferent genre: newspaper texts (NPAPER), newswire
texts (NWIRE), and broadcasted news transcripts
(BNEWS). Each of these is split into a train
part and a devtest part. Progress during the de-
velopment phase was determined by using cross-
validation on only the training set for the NPAPER
</bodyText>
<equation confidence="0.9517628">
PA(ANAPH|i) =
n
E
k=1
exp(
</equation>
<page confidence="0.981204">
238
</page>
<table confidence="0.9975036">
System BNEWS NPAPER NWIRE
R P F R P F R P F
COREF-PAIRWISE 54.4 77.4 63.9 58.1 80.7 67.6 53.8 78.2 63.8
AC-CASCADE 51.1 79.7 62.3 53.7 79.0 63.9 53.0 81.8 64.3
ORACLE-LINK 69.4 100 82.0 71.2 100 83.1 66.7 100 80.0
</table>
<tableCaption confidence="0.997639">
Table 1: Recall (R), precision (P), and f-score (F) on the three ACE datasets for the basic coreference system
</tableCaption>
<bodyText confidence="0.973872815789474">
(COREF-PAIRWISE), the anaphoricity-coreference cascade system (AC-CASCADE), and the oracle which
performs perfect linkage (ORACLE-LINK). The first two systems make strictly local pairwise coreference
decisions.
section. No human-annotated linguistic information
is used in the input. The corpus text was prepro-
cessed with the OpenNLP Toolkit2 (i.e., a sentence
detector, a tokenizer, a POS tagger, and a Named
Entity Recognizer).
In our experiments, we consider only the true
ACE mentions. This is because our focus is on eval-
uating pairwise local approaches versus the global
ILP approach rather than on building a full coref-
erence resolution system. It is worth noting that
previous work tends to be vague in both these re-
spects: details on mention filtering or providing
performance figures for markable identification are
rarely given.
Following common practice, results are given in
terms of recall and precision according to the stan-
dard model-theoretic metric (Vilain et al., 1995).
This method operates by comparing the equivalence
classes defined by the resolutions produced by the
system with the gold standard classes: these are the
two “models”. Roughly, the scores are obtained by
determining the minimal perturbations brought to
one model in order to map it onto the other model.
Recall is computed by trying to map the predicted
chains onto the true chains, while precision is com-
puted the other way around. We test significant dif-
ferences with paired t-tests (p &lt; .05).
The anaphoricity classifier has an average accu-
racy of 80.2% on the three ACE datasets (using a
threshold of .5). This score is slightly lower than
the scores reported by Ng and Cardie (2002a) for
another data set (MUC).
Table 1 summarizes the results, in terms of recall
(R), precision (P), and f-score (F) on the three ACE
data sets. As can be seen, the AC-CASCADE system
</bodyText>
<footnote confidence="0.521142">
2Available from opennlp.sf.net.
</footnote>
<bodyText confidence="0.9999798">
generally provides slightly better precision at the ex-
pense of recall than the COREF-PAIRWISE system,
but the performance varies across the three datasets.
The source of this variance is likely due to the fact
that we applied a uniform anaphoricity threshold
of .5 across all datasets; Ng (2004) optimizes this
threshold for each of the datasets: .3 for BNEWS
and NWIRE and .35 for NPAPER. This variance re-
inforces our argument for determining anaphoricity
and coreference jointly.
The limitations of the cascade approach are also
shown by the oracle results. Even if we had a sys-
tem that can pick the correct antecedents for all truly
anaphoric mentions, it would have a maximum re-
call of roughly 70% for the different datasets.
</bodyText>
<sectionHeader confidence="0.993905" genericHeader="method">
5 Integer programming formulations
</sectionHeader>
<bodyText confidence="0.999904947368421">
The results in the previous section demonstrate the
limitations of a cascading approach for determin-
ing anaphoricity and coreference with separate mod-
els. The other thing to note is that the results in
general provide a lot of room for improvement —
this is true for other state-of-the-art systems as well.
The integer programming formulation we provide
here has qualities which address both of these is-
sues. In particular, we define two objective func-
tions for coreference resolution to be optimized with
ILP. The first uses only information from the coref-
erence classifier (COREF-ILP) and the second inte-
grates both anaphoricity and coreference in a joint
formulation (JOINT-ILP). Our problem formulation
and use of ILP are based on both (Roth and Yih,
2004) and (Barzilay and Lapata, 2006).
For solving the ILP problem, we use lp solve,
an open-source linear programming solver which
implements the simplex and the Branch-and-Bound
</bodyText>
<page confidence="0.995842">
239
</page>
<bodyText confidence="0.916549666666667">
methods.3 In practice, each test document is pro-
cessed to define a distinct ILP problem that is then
submitted to the solver.
</bodyText>
<note confidence="0.4543275">
5.2 JOINT-ILP: joint anaphoricity-coreference
formulation
</note>
<subsectionHeader confidence="0.886834">
5.1 COREF-ILP: coreference-only formulation
</subsectionHeader>
<bodyText confidence="0.99984619047619">
Barzilay and Lapata (2006) use ILP for the problem
of aggregation in natural language generation: clus-
tering sets of propositions together to create more
concise texts. They cast it as a set partitioning prob-
lem. This is very much like coreference, where
each partition corresponds to an entity in a discourse
model.
COREF-ILP uses an objective function that is
based on only the coreference classifier and the
probabilities it produces. Given that the classifier
produces probabilities pC = PC(COREF|i, j), the
assignment cost of commiting to a coreference link
is cC(i,j) = −log(pC). A complement assignment
cost cC(i,j) = −log(1−pC) is associated with choos-
ing not to establish a link. In what follows, M de-
notes the set of mentions in the document, and P the
set of possible coreference links over these mentions
(i.e., P = {hi, ji|hi, ji ∈ M × M and i &lt; j}). Fi-
nally, we use indicator variables x(i,j) that are set to
1 if mentions i and j are coreferent, and 0 otherwise.
The objective function takes the following form:
</bodyText>
<equation confidence="0.99949825">
�min cC(i,j) ·x(i,j) + cC(i,j) · (1 − x(i,j)) (3)
(i,j)EP
subject to:
x(i,j) ∈ {0, 1} ∀hi, ji ∈ P (4)
</equation>
<bodyText confidence="0.999831">
This is essentially identical to Barzilay and Lapata’s
objective function, except that we consider only
pairs in which the i precedes the j (due to the struc-
ture of the problem). Also, we minimize rather than
maximize due to the fact we transform the model
probabilities with −log (like Roth and Yih (2004)).
This preliminary objective function merely guar-
antees that ILP will find a global assignment that
maximally agrees with the decisions made by the
coreference classifier. Concretely, this amounts to
taking all (and only) those links for which the classi-
fier returns a probability above .5. This formulation
does not yet take advantage of information from a
classifier that specializes in anaphoricity; this is the
subject of the next section.
</bodyText>
<footnote confidence="0.936071">
3Available from http://lpsolve.sourceforge.net/.
</footnote>
<bodyText confidence="0.999889341463415">
Roth and Yih (2004) use ILP to deal with the joint
inference problem of named entity and relation iden-
tification. This requires labeling a set of named enti-
ties in a text with labels such as person and loca-
tion, and identifying relations between them such
as spouse of and work for. In theory, each of these
tasks would likely benefit from utilizing the infor-
mation produced by the other, but if done as a cas-
cade will be subject to propogation of errors. Roth
and Yih thus set this up as problem in which each
task is performed separately; their output is used to
assign costs associated with indicator variables in an
objective function, which is then minimized subject
to constraints that relate the two kinds of outputs.
These constraints express qualities of what a global
assignment of values for these tasks must respect,
such as the fact that the arguments to the spouse of
relation must be entities with person labels. Impor-
tantly, the ILP objective function encodes not only
the best label produced by each classifier for each
decision; it utilizes the probabilities (or scores) as-
signed to each label and attempts to find a global
optimum (subject to the constraints).
The parallels to our anaphoricity/coreference sce-
nario are straightforward. The anaphoricity problem
is like the problem of identifying the type of entity
(where the labels are now ANAPH and ¬ANAPH),
and the coreference problem is like that of determin-
ing the relations between mentions (where the labels
are now COREF or ¬COREF).
Based on these parallels, the JOINT-ILP system
brings the two decisions of anaphoricity and corefer-
ence together by including both in a single objective
function and including constraints that ensure the
consistency of a solution for both tasks. Let cAj and
cAj be defined analogously to the coreference clas-
sifier costs for pA = PA(ANAPH|j), the probability
the anaphoricity classifier assigns to a mention j be-
ing anaphoric. Also, we have indicator variables yj
that are set to 1 if mention j is anaphoric and 0 oth-
erwise. The objective function takes the following
</bodyText>
<page confidence="0.960451">
240
</page>
<bodyText confidence="0.591974">
form:
</bodyText>
<equation confidence="0.993858142857143">
�min cChi,ji ·xhi,ji + cChi,ji · (1−xhi,ji)
hi,ji∈P
+ � cAj · yj + cAj · (1−yj) (5)
j∈M
subject to:
xhi,ji ∈ {0, 1} ∀hi, ji ∈ P (6)
yj ∈ {0, 1} ∀j ∈ M (7)
</equation>
<bodyText confidence="0.9993880625">
The structure of this objective function is very sim-
ilar to Roth and Yih’s, except that we do not uti-
lize constraint costs in the objective function itself.
Roth and Yih use these to make certain combina-
tions impossible (like a location being an argument
to a spouse of relation); we enforce such effects in
the constraint equations instead.
The joint objective function (5) does not constrain
the assignment of the xhi,ji and yj variables to be
consistent with one another. To enforce consistency,
we add further constraints. In what follows, Mj is
the set of all mentions preceding mention j in the
document.
Resolve only anaphors: if a pair of mentions hi, ji
is coreferent (xhi,ji=1), then mention j must be
anaphoric (yj=1).
</bodyText>
<equation confidence="0.818095636363636">
xhi,ji ≤ yj ∀hi, ji ∈ P (8)
Resolve anaphors: if a mention is anaphoric
(yj=1), it must be coreferent with at least one an-
tecedent.
yj ≤ � xhi,ji ∀j ∈ M (9)
i∈Mj
Do not resolve non-anaphors: if a mention is non-
anaphoric (yj=0), it should have no antecedents.
1
 |1  |� xhi,ji ∀j ∈ M (10)
i∈Mj
</equation>
<bodyText confidence="0.999919333333333">
These constraints thus directly relate the two
tasks. By formulating the problem this way, the de-
cisions of the anaphoricity classifier are not taken
on faith as they were with AC-CASCADE. Instead,
we optimize over consideration of both possibilities
in the objective function (relative to the probability
output by the classifier) while ensuring that the final
assignments respect the signifance of what it is to be
anaphoric or non-anaphoric.
</bodyText>
<sectionHeader confidence="0.952323" genericHeader="method">
6 Joint Results
</sectionHeader>
<bodyText confidence="0.999971324324324">
Table 2 summarizes the results for these different
systems. Both ILP systems are significantly better
than the baseline system COREF-PAIRWISE. Despite
having lower precision than COREF-PAIRWISE, the
COREF-ILP system obtains very large gains in recall
to end up with overall f-score gains of 4.3%, 4.2%,
and 3.0% across BNEWS, NPAPER, and NWIRE, re-
spectively. The fundamental reason for the increase
in recall and drop in precision is that COREF-ILP can
posit multiple antecedents for each mention. This
is an extra degree of freedom that allows COREF-
ILP to cast a wider net, with a consequent risk of
capturing incorrect antecedents. Precision is not
completely degraded because the optimization per-
formed by ILP utilizes the pairwise probabilities of
mention pairs as weights in the objective function
to make its assignments. Thus, highly improbable
links are still heavily penalized and are not chosen
as coreferential.
The JOINT-ILP system demonstrates the benefit
ILP’s ability to support joint task formulations. It
produces significantly better f-scores by regaining
some of the ground on precision lost by COREF-
ILP. The most likely source of the improved pre-
cision of JOINT-ILP is that weights corresponding
to the anaphoricity probabilities and constraints (8)
and (10) reduce the number of occurrences of non-
anaphors being assigned antecedents. There are also
improvements in recall over COREF-ILP for NPAPER
and NWIRE. A possible source of this difference is
constraint (9), which ensures that mentions which
are considered anaphoric must have at least one an-
tecedent.
Compared to COREF-PAIRWISE, JOINT-ILP dra-
matically improves recall with relatively small
losses in precision, providing overall f-score gains
of 5.3%, 4.9%, and 3.7% on the three datasets.
</bodyText>
<sectionHeader confidence="0.999905" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.93754775">
As was just demonstrated, ILP provides a principled
way to model dependencies between anaphoricity
decisions and coreference decisions. In a simi-
lar manner, this framework could also be used to
capture dependencies among coreference decisions
themselves. This option —which we will leave for
future work— would make such an approach akin to
yj ≥
</bodyText>
<page confidence="0.991426">
241
</page>
<table confidence="0.9948414">
System BNEWS NPAPER NWIRE
R P F R P F R P F
COREF-PAIRWISE 54.4 77.4 63.9 58.1 80.7 67.6 53.8 78.2 63.8
COREF-ILP 62.2 75.5 68.2 67.1 77.3 71.8 60.1 74.8 66.8
JOINT-ILP 62.1 78.0 69.2 68.0 77.6 72.5 60.8 75.8 67.5
</table>
<tableCaption confidence="0.9275">
Table 2: Recall (R), precision (P), and f-score (F) on the three ACE datasets for the basic coreference system
(COREF-PAIRWISE), the coreference only ILP system (COREF-ILP), and the joint anaphoricity-coreference
ILP system (JOINT-ILP). All f-score differences are significant (p &lt; .05).
a number of recent global approaches.
</tableCaption>
<bodyText confidence="0.999921272727272">
Luo et al. (2004) use Bell trees to represent the
search space of the coreference resolution problem
(where each leaf is possible partition). The prob-
lem is thus recast as that of finding the “best” path
through the tree. Given the rapidly growing size of
Bell trees, Luo et al. resort to a beam search al-
gorithm and various pruning strategies, potentially
resulting in picking a non-optimal solution. The re-
sults provided by Luo et al. are difficult to compare
with ours, since they use a different evaluation met-
ric.
Another global approach to coreference is the
application of Conditional Random Fields (CRFs)
(McCallum and Wellner, 2004). Although both are
global approaches, CRFs and ILP have important
differences. ILP uses separate local classifiers which
are learned without knowledge of the output con-
straints and are then integrated into a larger infer-
ence task. CRFs estimate a global model that di-
rectly uses the constraints of the domain. This in-
volves heavy computations which cause CRFs to
generally be slow and inefficient (even using dy-
namic programming). Again, the results presented
in McCallum and Wellner (2004) are hard to com-
pare with our own results. They only consider
proper names, and they only tackled the task of
identifying the correct antecedent only for mentions
which have a true antecedent.
A third global approach is offered by Ng (2005),
who proposes a global reranking over partitions gen-
erated by different coreference systems. This ap-
proach proceeds by first generating 54 candidate
partitions, which are each generated by a differ-
ent system. These different coreference systems
are obtained as combinations over three different
learners (C4.5, Ripper, and Maxent), three sam-
pling methods, two feature sets (Soon et al., 2001;
Ng and Cardie, 2002b), and three clustering al-
gorithms (Best-First, Closest-First, and aggressive-
merge). The features used by the reranker are of
two types: (i) partition-based features which are
here simple functions of the local features, and (ii)
method-based features which simply identify the
coreference system used for generating the given
partition. Although this approach leads to significant
gains on the both the MUC and the ACE datasets,
it has some weaknesses. Most importantly, the dif-
ferent systems employed for generating the different
partitions are all instances of the local classification
approach, and they all use very similar features. This
renders them likely to make the same types of errors.
The ILP approach could in fact be integrated with
these other approaches, potentially realizing the ad-
vantages of multiple global systems, with ILP con-
ducting their interactions.
</bodyText>
<sectionHeader confidence="0.999309" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999989">
We have provided two ILP formulations for resolv-
ing coreference and demonstrated their superiority
to a pairwise classifier that makes its coreference as-
signments greedily.
In particular, we have also shown that ILP pro-
vides a natural means to express the use of both
anaphoricity classification and coreference classifi-
cation in a single system, and that doing so provides
even further performance improvements, specifi-
cally f-score improvements of 5.3%, 4.9%, and
3.7% over the base coreference classifier on the ACE
datasets.
With ILP, it is not necessary to carefully control
the anaphoricity threshold. This is in stark contrast
to systems which use the anaphoricity classifier as a
filter for the coreference classifier in a cascade setup.
</bodyText>
<page confidence="0.988239">
242
</page>
<bodyText confidence="0.999995684210526">
The ILP objective function incorporates the proba-
bilities produced by both classifiers as weights on
variables that indicate the ILP assignments for those
tasks. The indicator variables associated with those
assignments allow several constraints between the
tasks to be straightforwardly stated to ensure consis-
tency to the assignments. We thus achieve large im-
provements with a simple formulation and no fuss.
ILP solutions are also obtained very quickly for the
objective functions and constraints we use.
In future work, we will explore the use of global
constraints, similar to those used by (Barzilay and
Lapata, 2006) to improve both precision and recall.
For example, we expect transitivity constraints over
coreference pairs, as well as constraints on the en-
tire partition (e.g., the number of entities in the doc-
ument), to help considerably. We will also consider
linguistic constraints (e.g., restrictions on pronouns)
in order to improve precision.
</bodyText>
<sectionHeader confidence="0.999075" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999935">
We would like to thank Ray Mooney, Rohit Kate,
and the three anonymous reviewers for their com-
ments. This work was supported by NSF grant IIS-
0535154.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999347355932203">
Regina Barzilay and Mirella Lapata. 2006. Aggregation
via set partitioning for natural language generation. In
Proceedings of the HLT/NAACL, pages 359–366, New
York, NY.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39–71.
A. Kehler, D. Appelt, L. Taylor, and A. Simma.
2004. The (non)utility of predicate-argument frequen-
cies for pronoun interpretation. In Proceedings of
HLT/NAACL, pages 289–296.
Andrew Kehler. 1997. Probabilistic coreference in infor-
mation extraction. In Proceedings of EMNLP, pages
163–173.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, , and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of the ACL.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Workshop on Natural Language
Learning, pages 49–55, Taipei, Taiwan.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Proceedings ofNIPS.
Joseph F. McCarthy and Wendy G. Lehnert. 1995. Using
decision trees for coreference resolution. In Proceed-
ings ofIJCAI, pages 1050–1055.
Thomas Morton. 1999. Using coreference for ques-
tion answering. In Proceedings of ACL Workshop on
Coreference and Its Applications.
Thomas Morton. 2000. Coreference for NLP applica-
tions. In Proceedings ofACL, Hong Kong.
Vincent Ng and Claire Cardie. 2002a. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In Proceedings of COLING.
Vincent Ng and Claire Cardie. 2002b. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings ofACL, pages 104–111.
Vincent Ng. 2004. Learning noun phrase anaphoricity to
improve coreference resolution: Issues in representa-
tion and optimization. In Proceedings ofACL.
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Proceedings ofACL.
Dan Roth and Wen-tau Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proceedings of CoNLL.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields. In
Proceedings ofICML, pages 737–744.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521–544.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
fo the 6th Message Understanding Conference (MUC-
6), pages 45–52, San Mateo, CA. Morgan Kaufmann.
</reference>
<page confidence="0.999044">
243
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.804530">
<title confidence="0.992558">Joint Determination of Anaphoricity and Coreference Resolution Integer Programming</title>
<author confidence="0.913266">Denis</author>
<affiliation confidence="0.978828">Department of University of Texas at</affiliation>
<abstract confidence="0.994352266666667">Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution. In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. joint ILP formulation provides score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Aggregation via set partitioning for natural language generation.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT/NAACL,</booktitle>
<pages>359--366</pages>
<location>New York, NY.</location>
<contexts>
<context position="15623" citStr="Barzilay and Lapata, 2006" startWordPosition="2523" endWordPosition="2526">hing to note is that the results in general provide a lot of room for improvement — this is true for other state-of-the-art systems as well. The integer programming formulation we provide here has qualities which address both of these issues. In particular, we define two objective functions for coreference resolution to be optimized with ILP. The first uses only information from the coreference classifier (COREF-ILP) and the second integrates both anaphoricity and coreference in a joint formulation (JOINT-ILP). Our problem formulation and use of ILP are based on both (Roth and Yih, 2004) and (Barzilay and Lapata, 2006). For solving the ILP problem, we use lp solve, an open-source linear programming solver which implements the simplex and the Branch-and-Bound 239 methods.3 In practice, each test document is processed to define a distinct ILP problem that is then submitted to the solver. 5.2 JOINT-ILP: joint anaphoricity-coreference formulation 5.1 COREF-ILP: coreference-only formulation Barzilay and Lapata (2006) use ILP for the problem of aggregation in natural language generation: clustering sets of propositions together to create more concise texts. They cast it as a set partitioning problem. This is very</context>
</contexts>
<marker>Barzilay, Lapata, 2006</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2006. Aggregation via set partitioning for natural language generation. In Proceedings of the HLT/NAACL, pages 359–366, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="6351" citStr="Berger et al., 1996" startWordPosition="990" endWordPosition="993">hes a particular value (typically .5). We use a maximum entropy model for the coreference classifier. Such models are well-suited for coreference, because they are able to handle many different, potentially overlapping learning features without making independence assumptions. Previous work on coreference using maximum entropy includes (Kehler, 1997; Morton, 1999; Morton, 2000). The model is defined in a standard fashion as follows: λkfk(hi,ji, COREF)) Z(hi, ji) (1) Z(hi, ji) is a normalization factor over both outcomes (COREF and ¬COREF). Model parameters are estimated using maximum entropy (Berger et al., 1996). Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling1 (Malouf, 2002). We use a Gaussian prior with a variance of 1000 — no attempt was made to optimize this value. Training instances for the coreference classifier are constructed based on pairs of mentions of the form hi, ji, where j and i are the descriptions for an anaphor and one of its candidate antecedents, respectively. Each such pair is assigned either a label COREF (i.e. a positive instance) or a label ¬COREF (i.e. a negative instance) d</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
<author>D Appelt</author>
<author>L Taylor</author>
<author>A Simma</author>
</authors>
<title>The (non)utility of predicate-argument frequencies for pronoun interpretation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>289--296</pages>
<contexts>
<context position="1213" citStr="Kehler et al., 2004" startWordPosition="179" endWordPosition="182">other for the final assignments. This joint ILP formulation provides fscore improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets. 1 Introduction The task of coreference resolution involves imposing a partition on a set of entity mentions in a document, where each partition corresponds to some entity in an underlying discourse model. Most work treats coreference resolution as a binary classification task in which each decision is made in a pairwise fashion, independently of the others (McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002b; Morton, 2000; Kehler et al., 2004). There are two major drawbacks with most systems that make pairwise coreference decisions. The first is that identification of anaphora is done implicitly as part of the coreference resolution. Two common types of errors with these systems are cases where: (i) the system mistakenly identifies an antecedent for non-anaphoric mentions, and (ii) the system does not try to resolve an actual anaphoric mention. To reduce such errors, Ng and Cardie (2002a) and Ng (2004) use an anaphoricity classifier –which has the sole task of saying whether or not any antecedents should be identified for each ment</context>
</contexts>
<marker>Kehler, Appelt, Taylor, Simma, 2004</marker>
<rawString>A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The (non)utility of predicate-argument frequencies for pronoun interpretation. In Proceedings of HLT/NAACL, pages 289–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kehler</author>
</authors>
<title>Probabilistic coreference in information extraction.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>163--173</pages>
<contexts>
<context position="6082" citStr="Kehler, 1997" startWordPosition="949" endWordPosition="950">e probability, PC(COREF|hi, ji), of having a coreferential outcome given a pair of mentions hi, ji, and (ii) applying a selection algorithm that will single out a unique candidate out of the subset of candidates i for which the probability PC(COREF|hi, ji) reaches a particular value (typically .5). We use a maximum entropy model for the coreference classifier. Such models are well-suited for coreference, because they are able to handle many different, potentially overlapping learning features without making independence assumptions. Previous work on coreference using maximum entropy includes (Kehler, 1997; Morton, 1999; Morton, 2000). The model is defined in a standard fashion as follows: λkfk(hi,ji, COREF)) Z(hi, ji) (1) Z(hi, ji) is a normalization factor over both outcomes (COREF and ¬COREF). Model parameters are estimated using maximum entropy (Berger et al., 1996). Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling1 (Malouf, 2002). We use a Gaussian prior with a variance of 1000 — no attempt was made to optimize this value. Training instances for the coreference classifier are constructed b</context>
</contexts>
<marker>Kehler, 1997</marker>
<rawString>Andrew Kehler. 1997. Probabilistic coreference in information extraction. In Proceedings of EMNLP, pages 163–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
</authors>
<title>A mentionsynchronous coreference resolution algorithm based on the Bell tree.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="2689" citStr="Luo et al. (2004)" startWordPosition="425" endWordPosition="428">rom consideration (Ng, 2004). This sensitivity is unsurprising, given that the tasks are codependent. The second problem is that most coreference systems make each decision independently of previous ones in a greedy fashion (McCallum and Wellner, 2004). Clearly, the determination of membership of a particular mention into a partition should be conditioned on how well it matches the entity as a whole. Since independence between decisions is an unwarranted assumption for the task, models that consider a more global context are likely to be more appropriate. Recent work has examined such models; Luo et al. (2004) using Bell trees, and McCallum and Wellner (2004) using conditional random fields, and Ng (2005) using rerankers. In this paper, we propose to recast the task of coreference resolution as an optimization problem, namely an integer linear programming (ILP) problem. This framework has several properties that make it highly suitable for addressing the two aforementioned problems. The first is that it can utilize existing classifiers; ILP performs global inference based on their output rather than formulating a 236 Proceedings of NAACL HLT 2007, pages 236–243, Rochester, NY, April 2007. c�2007 As</context>
<context position="24344" citStr="Luo et al. (2004)" startWordPosition="3969" endWordPosition="3972">r future work— would make such an approach akin to yj ≥ 241 System BNEWS NPAPER NWIRE R P F R P F R P F COREF-PAIRWISE 54.4 77.4 63.9 58.1 80.7 67.6 53.8 78.2 63.8 COREF-ILP 62.2 75.5 68.2 67.1 77.3 71.8 60.1 74.8 66.8 JOINT-ILP 62.1 78.0 69.2 68.0 77.6 72.5 60.8 75.8 67.5 Table 2: Recall (R), precision (P), and f-score (F) on the three ACE datasets for the basic coreference system (COREF-PAIRWISE), the coreference only ILP system (COREF-ILP), and the joint anaphoricity-coreference ILP system (JOINT-ILP). All f-score differences are significant (p &lt; .05). a number of recent global approaches. Luo et al. (2004) use Bell trees to represent the search space of the coreference resolution problem (where each leaf is possible partition). The problem is thus recast as that of finding the “best” path through the tree. Given the rapidly growing size of Bell trees, Luo et al. resort to a beam search algorithm and various pruning strategies, potentially resulting in picking a non-optimal solution. The results provided by Luo et al. are difficult to compare with ours, since they use a different evaluation metric. Another global approach to coreference is the application of Conditional Random Fields (CRFs) (McC</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, 2004</marker>
<rawString>Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, , and Salim Roukos. 2004. A mentionsynchronous coreference resolution algorithm based on the Bell tree. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Sixth Workshop on Natural Language Learning,</booktitle>
<pages>49--55</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="6519" citStr="Malouf, 2002" startWordPosition="1014" endWordPosition="1015">dle many different, potentially overlapping learning features without making independence assumptions. Previous work on coreference using maximum entropy includes (Kehler, 1997; Morton, 1999; Morton, 2000). The model is defined in a standard fashion as follows: λkfk(hi,ji, COREF)) Z(hi, ji) (1) Z(hi, ji) is a normalization factor over both outcomes (COREF and ¬COREF). Model parameters are estimated using maximum entropy (Berger et al., 1996). Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling1 (Malouf, 2002). We use a Gaussian prior with a variance of 1000 — no attempt was made to optimize this value. Training instances for the coreference classifier are constructed based on pairs of mentions of the form hi, ji, where j and i are the descriptions for an anaphor and one of its candidate antecedents, respectively. Each such pair is assigned either a label COREF (i.e. a positive instance) or a label ¬COREF (i.e. a negative instance) depending on whether or not the two mentions corefer. In generating the training data, we followed the method of (Soon et al., 2001) creating for each anaphor: (i) a pos</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of the Sixth Workshop on Natural Language Learning, pages 49–55, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Ben Wellner</author>
</authors>
<title>Conditional models of identity uncertainty with application to noun coreference.</title>
<date>2004</date>
<booktitle>In Proceedings ofNIPS.</booktitle>
<contexts>
<context position="2324" citStr="McCallum and Wellner, 2004" startWordPosition="363" endWordPosition="366">city classifier –which has the sole task of saying whether or not any antecedents should be identified for each mention– as a filter for their coreference system. They achieve higher performance by doing so; however, their setup uses the two classifiers in a cascade. This requires careful determination of an anaphoricity threshold in order to not remove too many mentions from consideration (Ng, 2004). This sensitivity is unsurprising, given that the tasks are codependent. The second problem is that most coreference systems make each decision independently of previous ones in a greedy fashion (McCallum and Wellner, 2004). Clearly, the determination of membership of a particular mention into a partition should be conditioned on how well it matches the entity as a whole. Since independence between decisions is an unwarranted assumption for the task, models that consider a more global context are likely to be more appropriate. Recent work has examined such models; Luo et al. (2004) using Bell trees, and McCallum and Wellner (2004) using conditional random fields, and Ng (2005) using rerankers. In this paper, we propose to recast the task of coreference resolution as an optimization problem, namely an integer lin</context>
<context position="24968" citStr="McCallum and Wellner, 2004" startWordPosition="4072" endWordPosition="4075">04) use Bell trees to represent the search space of the coreference resolution problem (where each leaf is possible partition). The problem is thus recast as that of finding the “best” path through the tree. Given the rapidly growing size of Bell trees, Luo et al. resort to a beam search algorithm and various pruning strategies, potentially resulting in picking a non-optimal solution. The results provided by Luo et al. are difficult to compare with ours, since they use a different evaluation metric. Another global approach to coreference is the application of Conditional Random Fields (CRFs) (McCallum and Wellner, 2004). Although both are global approaches, CRFs and ILP have important differences. ILP uses separate local classifiers which are learned without knowledge of the output constraints and are then integrated into a larger inference task. CRFs estimate a global model that directly uses the constraints of the domain. This involves heavy computations which cause CRFs to generally be slow and inefficient (even using dynamic programming). Again, the results presented in McCallum and Wellner (2004) are hard to compare with our own results. They only consider proper names, and they only tackled the task of</context>
</contexts>
<marker>McCallum, Wellner, 2004</marker>
<rawString>Andrew McCallum and Ben Wellner. 2004. Conditional models of identity uncertainty with application to noun coreference. In Proceedings ofNIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph F McCarthy</author>
<author>Wendy G Lehnert</author>
</authors>
<title>Using decision trees for coreference resolution.</title>
<date>1995</date>
<booktitle>In Proceedings ofIJCAI,</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="1136" citStr="McCarthy and Lehnert, 1995" startWordPosition="165" endWordPosition="168">aphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. This joint ILP formulation provides fscore improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets. 1 Introduction The task of coreference resolution involves imposing a partition on a set of entity mentions in a document, where each partition corresponds to some entity in an underlying discourse model. Most work treats coreference resolution as a binary classification task in which each decision is made in a pairwise fashion, independently of the others (McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002b; Morton, 2000; Kehler et al., 2004). There are two major drawbacks with most systems that make pairwise coreference decisions. The first is that identification of anaphora is done implicitly as part of the coreference resolution. Two common types of errors with these systems are cases where: (i) the system mistakenly identifies an antecedent for non-anaphoric mentions, and (ii) the system does not try to resolve an actual anaphoric mention. To reduce such errors, Ng and Cardie (2002a) and Ng (2004) use an anaphoricity classifier –which has the sole tas</context>
</contexts>
<marker>McCarthy, Lehnert, 1995</marker>
<rawString>Joseph F. McCarthy and Wendy G. Lehnert. 1995. Using decision trees for coreference resolution. In Proceedings ofIJCAI, pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Morton</author>
</authors>
<title>Using coreference for question answering.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL Workshop on Coreference and Its Applications.</booktitle>
<contexts>
<context position="6096" citStr="Morton, 1999" startWordPosition="951" endWordPosition="952"> PC(COREF|hi, ji), of having a coreferential outcome given a pair of mentions hi, ji, and (ii) applying a selection algorithm that will single out a unique candidate out of the subset of candidates i for which the probability PC(COREF|hi, ji) reaches a particular value (typically .5). We use a maximum entropy model for the coreference classifier. Such models are well-suited for coreference, because they are able to handle many different, potentially overlapping learning features without making independence assumptions. Previous work on coreference using maximum entropy includes (Kehler, 1997; Morton, 1999; Morton, 2000). The model is defined in a standard fashion as follows: λkfk(hi,ji, COREF)) Z(hi, ji) (1) Z(hi, ji) is a normalization factor over both outcomes (COREF and ¬COREF). Model parameters are estimated using maximum entropy (Berger et al., 1996). Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling1 (Malouf, 2002). We use a Gaussian prior with a variance of 1000 — no attempt was made to optimize this value. Training instances for the coreference classifier are constructed based on pairs </context>
</contexts>
<marker>Morton, 1999</marker>
<rawString>Thomas Morton. 1999. Using coreference for question answering. In Proceedings of ACL Workshop on Coreference and Its Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Morton</author>
</authors>
<title>Coreference for NLP applications.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL, Hong Kong.</booktitle>
<contexts>
<context position="1191" citStr="Morton, 2000" startWordPosition="177" endWordPosition="178">l informs the other for the final assignments. This joint ILP formulation provides fscore improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets. 1 Introduction The task of coreference resolution involves imposing a partition on a set of entity mentions in a document, where each partition corresponds to some entity in an underlying discourse model. Most work treats coreference resolution as a binary classification task in which each decision is made in a pairwise fashion, independently of the others (McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002b; Morton, 2000; Kehler et al., 2004). There are two major drawbacks with most systems that make pairwise coreference decisions. The first is that identification of anaphora is done implicitly as part of the coreference resolution. Two common types of errors with these systems are cases where: (i) the system mistakenly identifies an antecedent for non-anaphoric mentions, and (ii) the system does not try to resolve an actual anaphoric mention. To reduce such errors, Ng and Cardie (2002a) and Ng (2004) use an anaphoricity classifier –which has the sole task of saying whether or not any antecedents should be id</context>
<context position="6111" citStr="Morton, 2000" startWordPosition="953" endWordPosition="954">ji), of having a coreferential outcome given a pair of mentions hi, ji, and (ii) applying a selection algorithm that will single out a unique candidate out of the subset of candidates i for which the probability PC(COREF|hi, ji) reaches a particular value (typically .5). We use a maximum entropy model for the coreference classifier. Such models are well-suited for coreference, because they are able to handle many different, potentially overlapping learning features without making independence assumptions. Previous work on coreference using maximum entropy includes (Kehler, 1997; Morton, 1999; Morton, 2000). The model is defined in a standard fashion as follows: λkfk(hi,ji, COREF)) Z(hi, ji) (1) Z(hi, ji) is a normalization factor over both outcomes (COREF and ¬COREF). Model parameters are estimated using maximum entropy (Berger et al., 1996). Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling1 (Malouf, 2002). We use a Gaussian prior with a variance of 1000 — no attempt was made to optimize this value. Training instances for the coreference classifier are constructed based on pairs of mentions of </context>
</contexts>
<marker>Morton, 2000</marker>
<rawString>Thomas Morton. 2000. Coreference for NLP applications. In Proceedings ofACL, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1176" citStr="Ng and Cardie, 2002" startWordPosition="173" endWordPosition="176">h that each local model informs the other for the final assignments. This joint ILP formulation provides fscore improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets. 1 Introduction The task of coreference resolution involves imposing a partition on a set of entity mentions in a document, where each partition corresponds to some entity in an underlying discourse model. Most work treats coreference resolution as a binary classification task in which each decision is made in a pairwise fashion, independently of the others (McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002b; Morton, 2000; Kehler et al., 2004). There are two major drawbacks with most systems that make pairwise coreference decisions. The first is that identification of anaphora is done implicitly as part of the coreference resolution. Two common types of errors with these systems are cases where: (i) the system mistakenly identifies an antecedent for non-anaphoric mentions, and (ii) the system does not try to resolve an actual anaphoric mention. To reduce such errors, Ng and Cardie (2002a) and Ng (2004) use an anaphoricity classifier –which has the sole task of saying whether or not any anteceden</context>
<context position="8105" citStr="Ng and Cardie, 2002" startWordPosition="1295" endWordPosition="1298">01) system, this is done by pairing each mention j with each preceding mention i. Each test instance hi, ji thus 1Available from tadm.sf.net. PC(COREF|hi,ji) = n E k=1 exp( 237 formed is then evaluated by the classifier, which returns a probability representing the likelihood that these two mentions are coreferential. Soon et. al. (2001) use “Closest-First” selection: that is, the process terminates as soon as an antecedent (i.e., a test instance with probability &gt; .5) is found or the beginning of the text is reached. Another option is to pick the antecedent with the best overall probability (Ng and Cardie, 2002b). Our features for the coreference classifier fall into three main categories: (i) features of the anaphor, (ii) features of antecedent mention, and (iii) relational features (i.e., features that describe properties which hold between the two mentions, e.g. distance). This feature set is similar (though not equivalent) to that used by Ng and Cardie (2002a). We omit details here for the sake of brevity — the ILP systems we employ here could be equally well applied to many different base classifiers using many different feature sets. 3 Base models: anaphoricity classifier As mentioned in the i</context>
<context position="13850" citStr="Ng and Cardie (2002" startWordPosition="2233" endWordPosition="2236">y the resolutions produced by the system with the gold standard classes: these are the two “models”. Roughly, the scores are obtained by determining the minimal perturbations brought to one model in order to map it onto the other model. Recall is computed by trying to map the predicted chains onto the true chains, while precision is computed the other way around. We test significant differences with paired t-tests (p &lt; .05). The anaphoricity classifier has an average accuracy of 80.2% on the three ACE datasets (using a threshold of .5). This score is slightly lower than the scores reported by Ng and Cardie (2002a) for another data set (MUC). Table 1 summarizes the results, in terms of recall (R), precision (P), and f-score (F) on the three ACE data sets. As can be seen, the AC-CASCADE system 2Available from opennlp.sf.net. generally provides slightly better precision at the expense of recall than the COREF-PAIRWISE system, but the performance varies across the three datasets. The source of this variance is likely due to the fact that we applied a uniform anaphoricity threshold of .5 across all datasets; Ng (2004) optimizes this threshold for each of the datasets: .3 for BNEWS and NWIRE and .35 for NP</context>
<context position="26112" citStr="Ng and Cardie, 2002" startWordPosition="4255" endWordPosition="4258">results. They only consider proper names, and they only tackled the task of identifying the correct antecedent only for mentions which have a true antecedent. A third global approach is offered by Ng (2005), who proposes a global reranking over partitions generated by different coreference systems. This approach proceeds by first generating 54 candidate partitions, which are each generated by a different system. These different coreference systems are obtained as combinations over three different learners (C4.5, Ripper, and Maxent), three sampling methods, two feature sets (Soon et al., 2001; Ng and Cardie, 2002b), and three clustering algorithms (Best-First, Closest-First, and aggressivemerge). The features used by the reranker are of two types: (i) partition-based features which are here simple functions of the local features, and (ii) method-based features which simply identify the coreference system used for generating the given partition. Although this approach leads to significant gains on the both the MUC and the ACE datasets, it has some weaknesses. Most importantly, the different systems employed for generating the different partitions are all instances of the local classification approach, </context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002a. Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="1176" citStr="Ng and Cardie, 2002" startWordPosition="173" endWordPosition="176">h that each local model informs the other for the final assignments. This joint ILP formulation provides fscore improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets. 1 Introduction The task of coreference resolution involves imposing a partition on a set of entity mentions in a document, where each partition corresponds to some entity in an underlying discourse model. Most work treats coreference resolution as a binary classification task in which each decision is made in a pairwise fashion, independently of the others (McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002b; Morton, 2000; Kehler et al., 2004). There are two major drawbacks with most systems that make pairwise coreference decisions. The first is that identification of anaphora is done implicitly as part of the coreference resolution. Two common types of errors with these systems are cases where: (i) the system mistakenly identifies an antecedent for non-anaphoric mentions, and (ii) the system does not try to resolve an actual anaphoric mention. To reduce such errors, Ng and Cardie (2002a) and Ng (2004) use an anaphoricity classifier –which has the sole task of saying whether or not any anteceden</context>
<context position="8105" citStr="Ng and Cardie, 2002" startWordPosition="1295" endWordPosition="1298">01) system, this is done by pairing each mention j with each preceding mention i. Each test instance hi, ji thus 1Available from tadm.sf.net. PC(COREF|hi,ji) = n E k=1 exp( 237 formed is then evaluated by the classifier, which returns a probability representing the likelihood that these two mentions are coreferential. Soon et. al. (2001) use “Closest-First” selection: that is, the process terminates as soon as an antecedent (i.e., a test instance with probability &gt; .5) is found or the beginning of the text is reached. Another option is to pick the antecedent with the best overall probability (Ng and Cardie, 2002b). Our features for the coreference classifier fall into three main categories: (i) features of the anaphor, (ii) features of antecedent mention, and (iii) relational features (i.e., features that describe properties which hold between the two mentions, e.g. distance). This feature set is similar (though not equivalent) to that used by Ng and Cardie (2002a). We omit details here for the sake of brevity — the ILP systems we employ here could be equally well applied to many different base classifiers using many different feature sets. 3 Base models: anaphoricity classifier As mentioned in the i</context>
<context position="13850" citStr="Ng and Cardie (2002" startWordPosition="2233" endWordPosition="2236">y the resolutions produced by the system with the gold standard classes: these are the two “models”. Roughly, the scores are obtained by determining the minimal perturbations brought to one model in order to map it onto the other model. Recall is computed by trying to map the predicted chains onto the true chains, while precision is computed the other way around. We test significant differences with paired t-tests (p &lt; .05). The anaphoricity classifier has an average accuracy of 80.2% on the three ACE datasets (using a threshold of .5). This score is slightly lower than the scores reported by Ng and Cardie (2002a) for another data set (MUC). Table 1 summarizes the results, in terms of recall (R), precision (P), and f-score (F) on the three ACE data sets. As can be seen, the AC-CASCADE system 2Available from opennlp.sf.net. generally provides slightly better precision at the expense of recall than the COREF-PAIRWISE system, but the performance varies across the three datasets. The source of this variance is likely due to the fact that we applied a uniform anaphoricity threshold of .5 across all datasets; Ng (2004) optimizes this threshold for each of the datasets: .3 for BNEWS and NWIRE and .35 for NP</context>
<context position="26112" citStr="Ng and Cardie, 2002" startWordPosition="4255" endWordPosition="4258">results. They only consider proper names, and they only tackled the task of identifying the correct antecedent only for mentions which have a true antecedent. A third global approach is offered by Ng (2005), who proposes a global reranking over partitions generated by different coreference systems. This approach proceeds by first generating 54 candidate partitions, which are each generated by a different system. These different coreference systems are obtained as combinations over three different learners (C4.5, Ripper, and Maxent), three sampling methods, two feature sets (Soon et al., 2001; Ng and Cardie, 2002b), and three clustering algorithms (Best-First, Closest-First, and aggressivemerge). The features used by the reranker are of two types: (i) partition-based features which are here simple functions of the local features, and (ii) method-based features which simply identify the coreference system used for generating the given partition. Although this approach leads to significant gains on the both the MUC and the ACE datasets, it has some weaknesses. Most importantly, the different systems employed for generating the different partitions are all instances of the local classification approach, </context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002b. Improving machine learning approaches to coreference resolution. In Proceedings ofACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Learning noun phrase anaphoricity to improve coreference resolution: Issues in representation and optimization.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1681" citStr="Ng (2004)" startWordPosition="259" endWordPosition="260">fashion, independently of the others (McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002b; Morton, 2000; Kehler et al., 2004). There are two major drawbacks with most systems that make pairwise coreference decisions. The first is that identification of anaphora is done implicitly as part of the coreference resolution. Two common types of errors with these systems are cases where: (i) the system mistakenly identifies an antecedent for non-anaphoric mentions, and (ii) the system does not try to resolve an actual anaphoric mention. To reduce such errors, Ng and Cardie (2002a) and Ng (2004) use an anaphoricity classifier –which has the sole task of saying whether or not any antecedents should be identified for each mention– as a filter for their coreference system. They achieve higher performance by doing so; however, their setup uses the two classifiers in a cascade. This requires careful determination of an anaphoricity threshold in order to not remove too many mentions from consideration (Ng, 2004). This sensitivity is unsurprising, given that the tasks are codependent. The second problem is that most coreference systems make each decision independently of previous ones in a </context>
<context position="9647" citStr="Ng (2004)" startWordPosition="1544" endWordPosition="1545"> classifier which acts as a filter during model usage. Only the mentions that are deemed anaphoric are considered for coreference resolution. Interestingly, they find a degredation in performance. In particular, they obtain significant improvements in precision, but with larger losses in recall (especially for proper names and common nouns). To counteract this, they add ad hoc constraints based on string matching and extended mention matching which force certain mentions to be resolved as anaphors regardless of the anaphoricity classifier. This allows them to improve overall f-scores by 1-3%. Ng (2004) obtains f-score improvements of 2.8-4.5% by tuning the anaphoricity threshold on held-out data. The task for the anaphoricity determination component is the following: one wants to decide for each mention i in a document whether i is anaphoric or not. That is, this task can be performed using a simple binary classifier with two outcomes: ANAPH and ANAPH. The classifier estimates the conditional probabilities P(ANAPH|i) and predicts ANAPH for i when P(ANAPH|i) &gt; .5. We use the following model for our anaphoricity classifier: Akfk(i, ANAPH)) (2) Z(i) This model is trained in the same manner as </context>
<context position="14361" citStr="Ng (2004)" startWordPosition="2319" endWordPosition="2320">g a threshold of .5). This score is slightly lower than the scores reported by Ng and Cardie (2002a) for another data set (MUC). Table 1 summarizes the results, in terms of recall (R), precision (P), and f-score (F) on the three ACE data sets. As can be seen, the AC-CASCADE system 2Available from opennlp.sf.net. generally provides slightly better precision at the expense of recall than the COREF-PAIRWISE system, but the performance varies across the three datasets. The source of this variance is likely due to the fact that we applied a uniform anaphoricity threshold of .5 across all datasets; Ng (2004) optimizes this threshold for each of the datasets: .3 for BNEWS and NWIRE and .35 for NPAPER. This variance reinforces our argument for determining anaphoricity and coreference jointly. The limitations of the cascade approach are also shown by the oracle results. Even if we had a system that can pick the correct antecedents for all truly anaphoric mentions, it would have a maximum recall of roughly 70% for the different datasets. 5 Integer programming formulations The results in the previous section demonstrate the limitations of a cascading approach for determining anaphoricity and coreferen</context>
</contexts>
<marker>Ng, 2004</marker>
<rawString>Vincent Ng. 2004. Learning noun phrase anaphoricity to improve coreference resolution: Issues in representation and optimization. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Machine learning for coreference resolution: From local classification to global ranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2786" citStr="Ng (2005)" startWordPosition="443" endWordPosition="444"> second problem is that most coreference systems make each decision independently of previous ones in a greedy fashion (McCallum and Wellner, 2004). Clearly, the determination of membership of a particular mention into a partition should be conditioned on how well it matches the entity as a whole. Since independence between decisions is an unwarranted assumption for the task, models that consider a more global context are likely to be more appropriate. Recent work has examined such models; Luo et al. (2004) using Bell trees, and McCallum and Wellner (2004) using conditional random fields, and Ng (2005) using rerankers. In this paper, we propose to recast the task of coreference resolution as an optimization problem, namely an integer linear programming (ILP) problem. This framework has several properties that make it highly suitable for addressing the two aforementioned problems. The first is that it can utilize existing classifiers; ILP performs global inference based on their output rather than formulating a 236 Proceedings of NAACL HLT 2007, pages 236–243, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics new inference procedure for solving the basic task. Secon</context>
<context position="25699" citStr="Ng (2005)" startWordPosition="4194" endWordPosition="4195">are learned without knowledge of the output constraints and are then integrated into a larger inference task. CRFs estimate a global model that directly uses the constraints of the domain. This involves heavy computations which cause CRFs to generally be slow and inefficient (even using dynamic programming). Again, the results presented in McCallum and Wellner (2004) are hard to compare with our own results. They only consider proper names, and they only tackled the task of identifying the correct antecedent only for mentions which have a true antecedent. A third global approach is offered by Ng (2005), who proposes a global reranking over partitions generated by different coreference systems. This approach proceeds by first generating 54 candidate partitions, which are each generated by a different system. These different coreference systems are obtained as combinations over three different learners (C4.5, Ripper, and Maxent), three sampling methods, two feature sets (Soon et al., 2001; Ng and Cardie, 2002b), and three clustering algorithms (Best-First, Closest-First, and aggressivemerge). The features used by the reranker are of two types: (i) partition-based features which are here simpl</context>
</contexts>
<marker>Ng, 2005</marker>
<rawString>Vincent Ng. 2005. Machine learning for coreference resolution: From local classification to global ranking. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="15591" citStr="Roth and Yih, 2004" startWordPosition="2518" endWordPosition="2521">arate models. The other thing to note is that the results in general provide a lot of room for improvement — this is true for other state-of-the-art systems as well. The integer programming formulation we provide here has qualities which address both of these issues. In particular, we define two objective functions for coreference resolution to be optimized with ILP. The first uses only information from the coreference classifier (COREF-ILP) and the second integrates both anaphoricity and coreference in a joint formulation (JOINT-ILP). Our problem formulation and use of ILP are based on both (Roth and Yih, 2004) and (Barzilay and Lapata, 2006). For solving the ILP problem, we use lp solve, an open-source linear programming solver which implements the simplex and the Branch-and-Bound 239 methods.3 In practice, each test document is processed to define a distinct ILP problem that is then submitted to the solver. 5.2 JOINT-ILP: joint anaphoricity-coreference formulation 5.1 COREF-ILP: coreference-only formulation Barzilay and Lapata (2006) use ILP for the problem of aggregation in natural language generation: clustering sets of propositions together to create more concise texts. They cast it as a set pa</context>
<context position="17433" citStr="Roth and Yih (2004)" startWordPosition="2828" endWordPosition="2831"> (i.e., P = {hi, ji|hi, ji ∈ M × M and i &lt; j}). Finally, we use indicator variables x(i,j) that are set to 1 if mentions i and j are coreferent, and 0 otherwise. The objective function takes the following form: �min cC(i,j) ·x(i,j) + cC(i,j) · (1 − x(i,j)) (3) (i,j)EP subject to: x(i,j) ∈ {0, 1} ∀hi, ji ∈ P (4) This is essentially identical to Barzilay and Lapata’s objective function, except that we consider only pairs in which the i precedes the j (due to the structure of the problem). Also, we minimize rather than maximize due to the fact we transform the model probabilities with −log (like Roth and Yih (2004)). This preliminary objective function merely guarantees that ILP will find a global assignment that maximally agrees with the decisions made by the coreference classifier. Concretely, this amounts to taking all (and only) those links for which the classifier returns a probability above .5. This formulation does not yet take advantage of information from a classifier that specializes in anaphoricity; this is the subject of the next section. 3Available from http://lpsolve.sourceforge.net/. Roth and Yih (2004) use ILP to deal with the joint inference problem of named entity and relation identifi</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>Integer linear programming inference for conditional random fields.</title>
<date>2005</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>737--744</pages>
<contexts>
<context position="3645" citStr="Roth and Yih, 2005" startWordPosition="568" endWordPosition="571">addressing the two aforementioned problems. The first is that it can utilize existing classifiers; ILP performs global inference based on their output rather than formulating a 236 Proceedings of NAACL HLT 2007, pages 236–243, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics new inference procedure for solving the basic task. Second, the ILP approach supports inference over multiple classifiers, without having to fiddle with special parameterization. Third, it is much more efficient than conditional random fields, especially when long-distance features are utilized (Roth and Yih, 2005). Finally, it is straightforward to create categorical global constraints with ILP; this is done in a declarative manner using inequalities on the assignments to indicator variables. This paper focuses on the first problem, and proposes to model anaphoricity determination and coreference resolution as a joint task, wherein the decisions made by each locally trained model are mutually constrained. The presentation of the ILP model proceeds in two steps. In the first, intermediary step, we simply use ILP to find a global assignment based on decisions made by the coreference classifier alone. The</context>
</contexts>
<marker>Roth, Yih, 2005</marker>
<rawString>Dan Roth and Wen-tau Yih. 2005. Integer linear programming inference for conditional random fields. In Proceedings ofICML, pages 737–744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Soon</author>
<author>H Ng</author>
<author>D Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="1155" citStr="Soon et al., 2001" startWordPosition="169" endWordPosition="172">s a joint task, such that each local model informs the other for the final assignments. This joint ILP formulation provides fscore improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets. 1 Introduction The task of coreference resolution involves imposing a partition on a set of entity mentions in a document, where each partition corresponds to some entity in an underlying discourse model. Most work treats coreference resolution as a binary classification task in which each decision is made in a pairwise fashion, independently of the others (McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002b; Morton, 2000; Kehler et al., 2004). There are two major drawbacks with most systems that make pairwise coreference decisions. The first is that identification of anaphora is done implicitly as part of the coreference resolution. Two common types of errors with these systems are cases where: (i) the system mistakenly identifies an antecedent for non-anaphoric mentions, and (ii) the system does not try to resolve an actual anaphoric mention. To reduce such errors, Ng and Cardie (2002a) and Ng (2004) use an anaphoricity classifier –which has the sole task of saying whether</context>
<context position="7082" citStr="Soon et al., 2001" startWordPosition="1112" endWordPosition="1115">it for Advanced Discriminative Modeling1 (Malouf, 2002). We use a Gaussian prior with a variance of 1000 — no attempt was made to optimize this value. Training instances for the coreference classifier are constructed based on pairs of mentions of the form hi, ji, where j and i are the descriptions for an anaphor and one of its candidate antecedents, respectively. Each such pair is assigned either a label COREF (i.e. a positive instance) or a label ¬COREF (i.e. a negative instance) depending on whether or not the two mentions corefer. In generating the training data, we followed the method of (Soon et al., 2001) creating for each anaphor: (i) a positive instance for the pair hi, ji where i is the closest antecedent for j, and (ii) a negative instance for each pair hi, ki where k intervenes between i and j. Once trained, the classifier is used to create a set of coreferential links for each test document; these links in turn define a partition over the entire set of mentions. In the system of Soon et. al. (2001) system, this is done by pairing each mention j with each preceding mention i. Each test instance hi, ji thus 1Available from tadm.sf.net. PC(COREF|hi,ji) = n E k=1 exp( 237 formed is then eval</context>
<context position="26091" citStr="Soon et al., 2001" startWordPosition="4251" endWordPosition="4254">mpare with our own results. They only consider proper names, and they only tackled the task of identifying the correct antecedent only for mentions which have a true antecedent. A third global approach is offered by Ng (2005), who proposes a global reranking over partitions generated by different coreference systems. This approach proceeds by first generating 54 candidate partitions, which are each generated by a different system. These different coreference systems are obtained as combinations over three different learners (C4.5, Ripper, and Maxent), three sampling methods, two feature sets (Soon et al., 2001; Ng and Cardie, 2002b), and three clustering algorithms (Best-First, Closest-First, and aggressivemerge). The features used by the reranker are of two types: (i) partition-based features which are here simple functions of the local features, and (ii) method-based features which simply identify the coreference system used for generating the given partition. Although this approach leads to significant gains on the both the MUC and the ACE datasets, it has some weaknesses. Most importantly, the different systems employed for generating the different partitions are all instances of the local clas</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. Soon, H. Ng, and D. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings fo the 6th Message Understanding Conference (MUC6),</booktitle>
<pages>45--52</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="13162" citStr="Vilain et al., 1995" startWordPosition="2116" endWordPosition="2119">r, a tokenizer, a POS tagger, and a Named Entity Recognizer). In our experiments, we consider only the true ACE mentions. This is because our focus is on evaluating pairwise local approaches versus the global ILP approach rather than on building a full coreference resolution system. It is worth noting that previous work tends to be vague in both these respects: details on mention filtering or providing performance figures for markable identification are rarely given. Following common practice, results are given in terms of recall and precision according to the standard model-theoretic metric (Vilain et al., 1995). This method operates by comparing the equivalence classes defined by the resolutions produced by the system with the gold standard classes: these are the two “models”. Roughly, the scores are obtained by determining the minimal perturbations brought to one model in order to map it onto the other model. Recall is computed by trying to map the predicted chains onto the true chains, while precision is computed the other way around. We test significant differences with paired t-tests (p &lt; .05). The anaphoricity classifier has an average accuracy of 80.2% on the three ACE datasets (using a thresh</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings fo the 6th Message Understanding Conference (MUC6), pages 45–52, San Mateo, CA. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>