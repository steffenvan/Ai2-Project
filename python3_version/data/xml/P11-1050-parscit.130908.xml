<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000040">
<title confidence="0.977589">
Discovery of Topically Coherent Sentences for Extractive Summarization
</title>
<author confidence="0.674158">
Asli Celikyilmaz Dilek Hakkani-T¨ur
</author>
<affiliation confidence="0.638533">
Microsoft Speech Labs Microsoft Speech Labs  |Microsoft Research
</affiliation>
<address confidence="0.773602">
Mountain View, CA, 94041 Mountain View, CA, 94041
</address>
<email confidence="0.995517">
asli@ieee.org dilek@ieee.org
</email>
<sectionHeader confidence="0.99555" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995605">
Extractive methods for multi-document sum-
marization are mainly governed by informa-
tion overlap, coherence, and content con-
straints. We present an unsupervised proba-
bilistic approach to model the hidden abstract
concepts across documents as well as the cor-
relation between these concepts, to generate
topically coherent and non-redundant sum-
maries. Based on human evaluations our mod-
els generate summaries with higher linguistic
quality in terms of coherence, readability, and
redundancy compared to benchmark systems.
Although our system is unsupervised and opti-
mized for topical coherence, we achieve a 44.1
ROUGE on the DUC-07 test set, roughly in the
range of state-of-the-art supervised models.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99948928">
A query-focused multi-document summarization
model produces a short-summary text of a set of
documents, which are retrieved based on a user’s
query. An ideal generated summary text should con-
tain the shared relevant content among set of doc-
uments only once, plus other unique information
from individual documents that are directly related
to the user’s query addressing different levels of de-
tail. Recent approaches to the summarization task
has somewhat focused on the redundancy and co-
herence issues. In this paper, we introduce a series
of new generative models for multiple-documents,
based on a discovery of hierarchical topics and their
correlations to extract topically coherent sentences.
Prior research has demonstrated the usefulness
of sentence extraction for generating summary text
taking advantage of surface level features such as
word repetition, position in text, cue phrases, etc,
(Radev, 2004; Nenkova and Vanderwende, 2005a;
Wan and Yang, 2006; Nenkova et al., 2006). Be-
cause documents have pre-defined structures (e.g.,
sections, paragraphs, sentences) for different levels
of concepts in a hierarchy, most recent summariza-
tion work has focused on structured probabilistic
models to represent the corpus concepts (Barzilay
et al., 1999; Daume-III and Marcu, 2006; Eisenstein
and Barzilay, 2008; Tang et al., 2009; Chen et al.,
2000; Wang et al., 2009). In particular (Haghighi
and Vanderwende, 2009; Celikyilmaz and Hakkani-
Tur, 2010) build hierarchical topic models to iden-
tify salient sentences that contain abstract concepts
rather than specific concepts. Nonetheless, all these
systems crucially rely on extracting various levels of
generality from documents, focusing little on redun-
dancy and coherence issues in model building. A
model than can focus on both issues is deemed to be
more beneficial for a summarization task.
Topical coherence in text involves identifying key
concepts, the relationships between these concepts,
and linking these relationships into a hierarchy. In
this paper, we present a novel, fully generative
Bayesian model of document corpus, which can dis-
cover topically coherent sentences that contain key
shared information with as little detail and redun-
dancy as possible. Our model can discover hierar-
chical latent structure of multi-documents, in which
some words are governed by low-level topics (T)
and others by high-level topics (H). The main con-
tributions of this work are:
− construction of a novel bayesian framework to
</bodyText>
<page confidence="0.983279">
491
</page>
<note confidence="0.97954">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 491–499,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.997896441860466">
capture higher level topics (concepts) related to sum- we utilize the advantages of previous topic models
mary text discussed in §3, and build an unsupervised generative model that can
− representation of a linguistic system as a sequence associate each word in each document with three
of increasingly enriched models, which use posterior random variables: a sentence S, a higher-level topic
topic correlation probabilities in sentences to design H, and a lower-level topic T, in an analogical way
a novel sentence ranking method in §4 and 5, to PAM models (Li and McCallum, 2006), i.e., a di-
− application of the new hierarchical learning rected acyclic graph (DAG) representing mixtures of
method for generation of less redundant summaries hierarchical structure, where super-topics are multi-
discussed in §6. Our models achieve compara- nomials over sub-topics at lower levels in the DAG.
ble qualitative results on summarization of multiple We define a tiered-topic clustering in which the up-
newswire documents. Human evaluations of gener- per nodes in the DAG are higher-level topics H, rep-
ated summaries confirm that our model can generate resenting common co-occurence patterns (correla-
non-redundant and topically coherent summaries. tions) between lower-level topics T in documents.
2 Multi-Document Summarization Models This has not been the focus in prior work on genera-
Prior research has demonstrated the usefulness of tive approaches for summarization task. Mainly, our
sentence extraction for summarization based on lex- model can discover correlated topics to eliminate re-
ical, semantic, and discourse constraints. Such dundant sentences in summary text.
models often rely on different approaches includ- Rather than representing sentences as a layer in
ing: identifying important keywords (Nenkova et al., hierarchical models, e.g., (Haghighi and Vander-
2006); topic signatures based on user queries (Lin wende, 2009; Celikyilmaz and Hakkani-Tur, 2010),
and Hovy, 2002; Conroy et al., 2006; Harabagiu we model sentences as meta-variables. This is sim-
et al., 2007); high frequency content word feature ilar to author-topic models (Rosen-Zvi et al., 2004),
based learning (Nenkova and Vanderwende, 2005a; in which words are generated by first selecting an
Nenkova and Vanderwende, 2005b), to name a few. author uniformly from an observed author list and
Recent research focusing on the extraction of la- then selecting a topic from a distribution over topics
tent concepts from document clusters are close in that is specific to that author. In our model, words
spirit to our work (Barzilay and Lee, 2004; Daume- are generated from different topics of documents by
III and Marcu, 2006; Eisenstein and Barzilay, 2008; first selecting a sentence containing the word and
Tang et al., 2009; Wang et al., 2009). Some of these then topics that are specific to that sentence. This
work (Haghighi and Vanderwende, 2009; Celikyil- way we can directly extract from documents the
maz and Hakkani-Tur, 2010) focus on the discov- summary related sentences that contain high-level
ery of hierarchical concepts from documents (from topics. In addition in (Celikyilmaz and Hakkani-Tur,
abstract to specific) using extensions of hierarchal 2010), the sentences can only share topics if the sen-
topic models (Blei et al., 2004) and reflect this hier- tences are represented on the same path of captured
archy on the sentences. Hierarchical concept learn- topic hierarchy, restricting topic sharing across sen-
ing models help to discover, for instance, that ”base- tences on different paths. Our DAG identifies tiered
ball” and ”football” are both contained in a general topics distributed over document clusters that can be
class ”sports”, so that the summaries reference terms shared by each sentence.
related to more abstract concepts like ”sports”. 3 Topic Coherence for Summarization
Although successful, the issue with concept learn- In this section we discuss the main contribution,
ing methods for summarization is that the extracted our two hierarchical mixture models, which improve
sentences usually contain correlated concepts. We summary generation performance through the use of
need a model that can identify salient sentences re- tiered topic models. Our models can identify lower-
ferring to general concepts of documents and there level topics T (concepts) defined as distributions
should be minimum correlation between them. over words or higher-level topics H, which represent
Our approach differs from the early work, in that, correlations between these lower level topics given
492
sentences. We present our synthetic experiment for
model development to evaluate extracted summaries
on redundancy measure. In §6, we demonstrate the
performance of our models on coherence and infor-
mativeness of generated summaries by qualitative
and intrinsic evaluations.
For model development we use the DUC 2005
dataset1, which consists of 45 document clusters,
each of which include 1-4 set of human gener-
ated summaries (10-15 sentences each). Each doc-
ument cluster consists — 25 documents (25-30 sen-
tences/document) retrieved based on a user query.
We consider each document cluster as a corpus and
build 45 separate models.
For the synthetic experiments, we include the pro-
vided human generated summaries of each corpus
as additional documents. The sentences in human
summaries include general concepts mentioned in
the corpus, the salient sentences of documents. Con-
trary to usual qualitative evaluations of summariza-
tion tasks, our aim during development is to measure
the percentage of sentences in a human summary
that our model can identify as salient among all other
document cluster sentences. Because human pro-
duced summaries generally contain non-redundant
sentences, we use total number of top-ranked hu-
man summary sentences as a qualitative redundancy
measure in our synthetic experiments.
In each model, a document d is a vector of Nd
words wd, where each wid is chosen from a vocabu-
lary of size V , and a vector of sentences S, represent-
ing all sentences in a corpus of size SD. We identify
sentences as meta-variables of document clusters,
which the generative process models both sentences
and documents using tiered topics. A sentence’s re-
latedness to summary text is tied to the document
cluster’s user query. The idea is that a lexical word
present or related to a query should increase its sen-
tence’s probability of relatedness.
</bodyText>
<sectionHeader confidence="0.997879" genericHeader="method">
4 Two-Tiered Topic Model - TTM
</sectionHeader>
<bodyText confidence="0.9983728">
Our base model, the two-tiered topic model (TTM),
is inspired by the hierarchical topic model, PAM,
proposed by Li and McCallum (2006). PAM struc-
tures documents to represent and learn arbitrary,
nested, and possibly sparse topic correlations using
</bodyText>
<footnote confidence="0.912567">
1www-nlpir.nist.gov/projects/duc/data.html
</footnote>
<note confidence="0.429945">
Documents in a Document Cluster
</note>
<figureCaption confidence="0.995279166666667">
Figure 1: Graphical model depiction of two-tiered topic model
(TTM) described in section §4. S are sentences si=1..SD in doc-
ument clusters. The high-level topics (Hk1=1...x1), represent-
ing topic correlations, are modeled as distributions over low-
level-topics (Tk2=1...x2). Shaded nodes indicate observed vari-
ables. Hyper-parameters for 0, BH, BT, B are omitted.
</figureCaption>
<bodyText confidence="0.999873818181818">
a directed acyclic graph. Our goals are not so dif-
ferent: we aim to discover concepts from documents
that would attribute for the general topics related to a
user query, however, we want to relate this informa-
tion to sentences. We represent sentences S by dis-
covery of general (more general) to specific topics
(Fig.1). Similarly, we represent summary unrelated
(document specific) sentences as corpus specific dis-
tributions 0 over background words WB, (functional
words like prepositions, etc.).
Our two-tiered topic model for salient sentence
discovery can be generated for each word in the doc-
ument (Algorithm 1) as follows: For a word wid in
document d, a random variable xid is drawn, which
determines if wid is query related, i.e., wid either ex-
ists in the query or is related to the query2. Oth-
erwise, wid is unrelated to the user query. Then
sentence si is chosen uniformly at random (ysi—
Uniform(si)) from sentences in the document con-
taining wid (deterministic if there is only one sen-
tence containing wid). We assume that if a word is
related to a query, it is likely to be summary-related
</bodyText>
<footnote confidence="0.900190333333333">
2We measure relatedness to a query if a word exists in the
query or it is synonymous based on information extracted from
WordNet (Miller, 1995).
</footnote>
<figure confidence="0.999539166666666">
Document
Summary
Content
Indicator
Parameters
SD
Sentence
selector y
Higher-Level
Topics H
x
OH
K1
Higher-Level
Topic
Parameters
K1xK2
T
BT
Lower-Level
Topic
Parameters
Nd
Lower-
Level
Topics
K2
W
(Background)
Specific
Content
Parameters
Summary Related
Word Indicator
Sentences
S
</figure>
<page confidence="0.947311">
493
</page>
<figureCaption confidence="0.8418128">
Figure 2: Depiction of TTM given the query ”D0718D: Star-
bucks Coffee : How has Starbucks Coffee attempted to ex-
pand and diversify through joint ventures, acquisitions, or
subsidiaries?”. If a word is query/summary related sentence
S, first a sentence then a high-level (H) and a low-level (T)
</figureCaption>
<bodyText confidence="0.968395230769231">
c
topic is sampled. ( represents that a random variable is a
parent of all C random variables.) The bolded links from H −T
represent correlated low-level topics.
(so as the sampled sentence si). We keep track of
the frequency of si’s in a vector, DS ∈ ZSD. Ev-
ery time an si is sampled for a query related wid, we
increment its count, a degree of sentence saliency.
Given that wid is related to a query, it is as-
sociated with two-tiered multinomial distributions:
high-level H topics and low-level T topics. A high-
level topic Hki is chosen first from a distribution
over low-level topics T specific to that si and one
low-level topic Tk, is chosen from a distribution
over words, and wid is generated from the sampled
low-level topic. If wid is not query-related, it is gen-
erated as a background word wB.
The resulting tiered model is shown as a graph
and plate diagrams in Fig.1 &amp; 2. A sentence sampled
from a query related word is associated with a dis-
tribution over K1 number of high-level topics Hki,
each of which are also associated with K2 number
of low-level topics Tk,, a multinomial over lexical
words of a corpus. In Fig.2 the most confident words
of four low-level topics is shown. The bolded links
between Hki and Tk, represent the strength of cor-
</bodyText>
<construct confidence="0.370745">
Algorithm 1 Two-Tiered Topic Model Generation
</construct>
<listItem confidence="0.959763666666667">
1: Sample: si = 1..SD: 41 ∼ Beta(q),
2: k1 = 1...K1: BH ∼ Dirichlet(aH),
3: k2 = 1...K1 × K2: BT ∼ Dirichlet(aT ),
4: and k = 1..K2: 0 ∼ Dirichlet(/).
5: for documents d ← 1, ..., D do
6: for words wid, i ← 1, ..., Nd do
7: - Draw a discrete x ∼ Binomial(T...)*
8: - If x = 1, wid is summary related;
9: · conditioned on S draw a sentence
10: y, ∼ Uniform(si) containing wi,
11: · sample a high-level topic Hk1 ∼ BHk1(aH),
12: and a low-level topic Tk2 ∼ OTk2(aT ),
13: · sample a word wik1k2 ∼ OHk1Tk2 (a),
14: - If x = 0, the word is unrelated **
15: sample a word wB ∼ 0(a),
16: corpus specific distribution.
17: end for
18: end for
</listItem>
<equation confidence="0.371716333333333">
* if wid exists or related to the the query then x = 1 deterministic,
otherwise it is stochastically assigned x — Bin(T).
** wid is a background word.
</equation>
<bodyText confidence="0.99926">
relation between Tk,’s, e.g., the topic ”acquisition”
is found to be more correlated with ”retail” than the
”network” topic given H1. This information is used
to rank sentences based on the correlated topics.
</bodyText>
<subsectionHeader confidence="0.99925">
4.1 Learning and Inference for TTM
</subsectionHeader>
<bodyText confidence="0.999887619047619">
Our learning procedure involves finding parame-
ters, which likely integrates out model’s posterior
distribution P(H, T|Wd, S), d∈D. EM algorithms
might face problems with local maxima in topic
models (Blei et al., 2003) suggesting implementa-
tion of approximate methods in which some of the
parameters, e.g., 0H, OT, 0, and 0, can be integrated
out, resulting in standard Dirichlet-multinomial as
well as binomial distributions. We use Gibbs sam-
pling which allows a combination of estimates from
several local maxima of the posterior distribution.
For each word, xid is sampled from a sentence
specific binomial 0 which in turn has a smooth-
ing prior q to determine if the sampled word wid is
(query) summary-related or document-specific. De-
pending on xid, we either sample a sentence along
with a high/low-level topic pair or just sample back-
ground words wB. The probability distribution over
sentence assignments, P(ysi = s|S) si ∈ S, is as-
sumed to be uniform over the elements of S, and de-
terministic if there is only one sentence in the docu-
</bodyText>
<figure confidence="0.994740088888889">
Document
Sentences
S Specific
Words
T
High-Level Topics
H1 H2 H3
H4
...
HK1
wB
T T T
T
T
Low-
Level
Topics
W W
W
T4
...
W
TK2
T1 T2 T3
W
H1
T2 :”coffee” H2 T4 :”retail”
H3
starbucks,
coffee, schultz,
tazo, pasqua,
states, subsidiary
T1 :”acquisition”
acquire, bought,
purchase,
disclose, joint-
venture, johnson
T3 :”network”
francisco, pepsi,
area, profit,
network, internet,
Francisco-based
starbucks, coffee,
retailer,
C4frappaccino
</figure>
<page confidence="0.997135">
494
</page>
<bodyText confidence="0.996991">
ment containing the corresponding word. The opti-
mum hyper-parameters are set based on the training
dataset model performance via cross-validation 3.
For each word we sample a high-level Hki and
a low-level Tk. topic if the word is query related
(xid = 1). The sampling distribution for TTM
for a word given the remaining topics and hyper-
parameters αH, αT , α, β, η is:
</bodyText>
<equation confidence="0.990695666666667">
pTTM(Hk1, Tk2, x = 1|w, H−k1, T−k2) a
αT+ nk1k2
d
ETi αTi + ndH
βw + nw
k1k2x
� Ewi βwi + nk1k2x
and when x = 0 (a corpus specific word),
pTTM(x = 0|w, zH−k, zt−k) a
αw + nw
Ewi αwi + n
The nk1
</equation>
<bodyText confidence="0.988980416666667">
d is the number of occurrences of high-level
topic k1 in document d, and nk1k2
d is the number of
times the low-level topic k2 is sampled together with
high-level topic k1 in d, nwk1k2x is the number of oc-
currences of word w sampled from path H-T given
that the word is query related. Note that the number
of tiered topics in the model is fixed to K1 and K2,
which is optimized with validation experiments. It
is also possible to construct extended models of TTM
using non-parametric priors, e.g., hierarchal Dirich-
let processes (Li et al., 2007) (left for future work).
</bodyText>
<subsectionHeader confidence="0.977389">
4.2 Summary Generation with TTM
</subsectionHeader>
<bodyText confidence="0.9999326">
We can observe the frequency of draws of every sen-
tence in a document cluster S, given it’s words are
related, through DS E ZSD. We obtain DS during
Gibbs sampling (in §4.1), which indicates a saliency
score of each sentence sj E S, j = 1..SD:
</bodyText>
<equation confidence="0.872176">
scoreTTM(sj) OC # [wid E sj, xid = 1] /nwj (1)
</equation>
<bodyText confidence="0.999937">
where wid indicates a word in a document d that ex-
ists in sj and is sampled as summary related based
on random indicator variable xid. nwj is the num-
ber of words in sj and normalizes the score favoring
</bodyText>
<footnote confidence="0.755807666666667">
3An alternative way would be to use Dirichlet priors (Blei et
al., 2003) which we opted for due to computational reasons but
will be investigated as future research.
</footnote>
<bodyText confidence="0.999311636363636">
sentences with many related words. We rank sen-
tences based on (1). We compare TTM results on
synthetic experiments against PAM (Li and McCal-
lum, 2006) a similar topic model that clusters topics
in a hierarchical structure, where super-topics are
distributions over sub-topics. We obtain sentence
scores for PAM models by calculating the sub-topic
significance (TS) based on super-topic correlations,
and discover topic correlations over the entire docu-
ment space (corpus wide). Hence; we calculate the
TS of a given sub-topic, k = 1,.., K2 by:
</bodyText>
<equation confidence="0.950426333333333">
1 Y
TS(zk) = D
dED
</equation>
<bodyText confidence="0.999485727272727">
where zksub is a sub-topic k = 1..K2 and zk1
sup is a
super-topic k1. The conditional probability of a sub-
topic k given a super-topic k1, p(zk sub|zk1
sup), explains
the variation of that sub-topic in relation to other
sub-topics. The higher the variation over the entire
corpus, the better it represents the general theme of
the documents. So, sentences including such topics
will have higher saliency scores, which we quantify
by imposing topic’s significance on vocabulary:
</bodyText>
<equation confidence="0.99176175">
scorePAM(si) = Y
K2
1 K2 TT _(_..|_.k ) , TS(_. )
k wEsi
</equation>
<bodyText confidence="0.939521">
(3)
Fig. 4 illustrates the average salience sentence se-
lection performance of TTM and PAM models (for
45 models). The x-axis represents the percentage of
sentences selected by the model among all sentences
in the DUC2005 corpus. 100% means all sentences
in the corpus included in the summary text. The
y-axis is the % of selected human sentences over
all sentences. The higher the human summary sen-
tences are ranked, the better the model is in select-
ing the salient sentences. Hence, the system which
peaks sooner indicates a better model.
In Fig.4 TTM is significantly better in identifying
human sentences as salient in comparison to PAM.
The statistical significance is measured based on the
area under the curve averaged over 45 models.
</bodyText>
<sectionHeader confidence="0.994675" genericHeader="method">
5 Enriched Two-Tiered Topic Model
</sectionHeader>
<bodyText confidence="0.9988635">
Our model can discover words that are related to
summary text using posteriors Pˆ(θH) and Pˆ(θT),
</bodyText>
<equation confidence="0.9772498">
αH k1
+ n
d
EHi αHi + nd
η + nk1k2
x
2η + nk1k2
η + nxk1k2 �
2η + nk1k2
p(zk sub|zk1
sup) (2)
1
K1
YK1
k1
</equation>
<page confidence="0.994172">
495
</page>
<figure confidence="0.544807">
Documents in a Document Cluster
</figure>
<figureCaption confidence="0.942799">
Figure 3: Graphical model depiction of sentence level enriched two-tiered model (ETTM) described in section §5. Each path
defined by H/T pair k,k2, has a multinomial C over which level of the path outputs a given word. L indicates which level, i.e, high
or low, the word is sampled from. On the right is the high-level topic-word and low-level topic-word distributions characterized by
ETTM. Each Hk1 also represented as distributions over general words WH as well as indicates the degree of correlation between
low-level topics denoted by boldness of the arrows.
</figureCaption>
<figure confidence="0.998974558139535">
Indicator
(Background)
Specific
Content
Parameters
Word
Level
L
!
&amp;quot;
Document
Higher-Level
Topics
Sentence
selector
Lower-
Level
Topics
Sentences
H
w
y
S
T
Summary Related
Word Indicator
x
Nd
SD
K1xK2
&amp;quot;H
K1 +K2
$
#
&amp;quot;T
K1
Summary
Content
Indicator
Parameters
Higher-Level
Topic
Parameters
Lower-Level
Topic
Parameters
Low-Level Topics
purchase,
disclose,
joint-venture,
johnson
WL
WL
schultz, tazo,
pasqua,
states,
subsidiary
“acquisition”
“coffee”
T1 H1 T3
L=2 L=1 WL L=2
T
T2
L=2
High-Level Topics
L=1
T,WH
“retail”
,WH
coffee,
starbucks...
seattle,
acquire, sales,
billion...
H2
pepsi, area,
profit,network
francisco
Low-Level Topics
frappaccino,
retailer,
mocca,
organic
WL
“network”
L=2
</figure>
<bodyText confidence="0.997143428571428">
as well as words wB specific to documents (via
16(0)) (Fig.1). TTM can discover topic correlations,
but cannot differentiate if a word in a sentence is
more general or specific given a query. Sentences
with general words would be more suitable to in-
clude in summary text compared to sentences con-
taining specific words. For instance for a given sen-
tence: ”Starbucks Coffee has attempted to expand
and diversify through joint ventures, and acquisi-
tions.”, ”starbucks” and ”coffee” are more gen-
eral words given the document clusters compared
to ”joint” and ”ventures” (see Fig.2), because they
appear more frequently in document clusters. How-
ever, TTM has no way of knowing that ”starbucks”
and ”coffee” are common terms given the context.
We would like to associate general words with high-
level topics, and context specific words with low-
level topics. Sentence containing words that are
sampled from high-level topics would be a bet-
ter candidate for summary text. Thus; we present
enriched TTM (ETTM) generative process (Fig.3),
which samples words not only from low-level top-
ics but also from high-level topics as well.
ETTM discovers three separate distributions over
words: (i) high-level topics H as distributions over
corpus general words WH, (ii) low-level topics T
as distributions over corpus specific words WL, and
Level Generation for Enriched TTM
</bodyText>
<equation confidence="0.811834666666667">
Fetch (k — Beta(-y); k = 1...K1 x K2.
For wid, i = 1, ..., Nd, d = 1, ..D:
If x = 1, sentence si is summary related;
</equation>
<bodyText confidence="0.923776142857143">
- sample Hk1 and Tk2
- sample a level L from Bin((k1k2)
- If L = 1 (general word); wid — OHk,
- else if L = 2 (context specific); wid — OHk1Tk2
else if x = 0, do Step 14-16 in Alg. 1.
(iii) background word distributions, i.e,. document
specific WB (less confidence for summary text).
Similar to TTM’s generative process, if wid is re-
lated to a given query, then x = 1 is determin-
istic, otherwise x E 10, 11 is stochastically deter-
mined if wid should be sampled as a background
word (wB) or through hierarchical path, i.e., H-T
pairs. We first sample a sentence si for wid uni-
formly at random from the sentences containing the
word wi—Uniform(si)). At this stage we sample a
level L,,,id E 11, 21 for wid to determine if it is a
high-level word, e.g., more general to context like
”starbucks” or ”coffee” or more specific to related
context such as ”subsidiary”, ”frappucino”. Each
path through the DAG, defined by a H-T pair (total
of K1K2 pairs), has a binomial (K,K2 over which
</bodyText>
<page confidence="0.998097">
496
</page>
<figure confidence="0.986038">
0 10 20 30 40 50 60 70 80 90 100
% of sentences added to the generated summary text.
</figure>
<figureCaption confidence="0.997281">
Figure 4: Average saliency performance of four systems over
45 different DUC models. The area under each curve is shown
in legend. Inseam is the magnified view of top-ranked 10% of
sentences in corpus.
</figureCaption>
<bodyText confidence="0.999996">
level of the path outputs sampled word. If the word
is a specific type, x = 0, then it is sampled from the
background word distribution θ, a document specific
multinomial. Once the level and conditional path is
drawn (see level generation for ETTM above) the rest
of the generative model is same as TTM.
</bodyText>
<subsectionHeader confidence="0.999108">
5.1 Learning and Inference for ETTM
</subsectionHeader>
<bodyText confidence="0.999968333333333">
For each word, x is sampled from a sentence spe-
cific binomial ψ, just like TTM. If the word is related
to the query x = 1, we sample a high and low-level
topic pair H − T as well as an additional level L is
sampled to determine which level of topics the word
should be sampled from. L is a corpus specific bi-
nomial one for all H − T pairs. If L = 1, the word
is one of corpus general words and sampled from
the high-level topic, otherwise (L = 2) the word
is corpus specific and sampled from a the low-level
topic. The optimum hyper-parameters are set based
on training performance via cross validation.
The conditional probabilities are similar to TTM,
but with additional random variables, which deter-
mine the level of generality of words as follows:
</bodyText>
<equation confidence="0.878502">
pETTM(Tk1, Tk2, L|w, T−k1, T−k2, L) a
pTTM(Tk1, Tk2, x = 1|.) � γ+Nik2 2γ+nk1k2
</equation>
<subsectionHeader confidence="0.994605">
5.2 Summary Generation with ETTM
</subsectionHeader>
<bodyText confidence="0.915619">
For ETTM models, we extend the TTM sentence
score to be able to include the effect of the general
words in sentences (as word sequences in language
models) using probabilities of K1 high-level topic
distributions, φwH , as:
</bodyText>
<equation confidence="0.978181666666667">
k-_ 1 Kl
scoreETTM(si) « # [wid E sj, xid = 1] /nwj �
K1 Ek=1..K1 HwCsi p(w |Tk)
</equation>
<bodyText confidence="0.9999275625">
where p(w|Tk) is the probability of a word in si
being generated from high-level topic Hk. Using
this score, we re-rank the sentences in documents
of the synthetic experiment. We compare the re-
sults of ETTM to a structurally similar probabilis-
tic model, entitled hierarchical PAM (Mimno et al.,
2007), which is designed to capture topics on a hi-
erarchy of two layers, i.e., super topics and sub-
topics, where super-topics are distributions over ab-
stract words. In Fig. 4 out of 45 models ETTM has
the best performance in ranking the human gener-
ated sentences at the top, better than the TTM model.
Thus; ETTM is capable of capturing focused sen-
tences with general words related to the main con-
cepts of the documents and much less redundant
sentences containing concepts specific to user query.
</bodyText>
<sectionHeader confidence="0.989975" genericHeader="method">
6 Final Experiments
</sectionHeader>
<bodyText confidence="0.992264695652174">
In this section, we qualitatively compare our models
against state-of-the art models and later apply an in-
trinsic evaluation of generated summaries on topical
coherence and informativeness.
For a qualitative comparison with the previous
state-of-the models, we use the standard summariza-
tion datasets on this task. We train our models on the
datasets provided by DUC2005 task and validate the
results on DUC 2006 task, which consist of a total
of 100 document clusters. We evaluate the perfor-
mance of our models on DUC2007 datasets, which
comprise of 45 document clusters, each containing
25 news articles. The task is to create max. 250
word long summary for each document cluster.
6.1. ROUGE Evaluations: We train each docu-
ment cluster as a separate corpus to find the optimum
parameters of each model and evaluate on test docu-
ment clusters. ROUGE is a commonly used measure,
a standard DUC evaluation metric, which computes
recall over various n-grams statistics from a model
generated summary against a set of human generated
summaries. We report results in R-1 (recall against
unigrams), R-2 (recall against bigrams), and R-SU4
</bodyText>
<figure confidence="0.998828903225806">
TIM
ETIM
hPAM
PAM
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 2 4 6 8 10
ETIM TIM
hPAM
TIM
ETIM
PAM
BPAM
PAM
% of human generated sentences 1
used in the generated summary 0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
</figure>
<page confidence="0.989874">
497
</page>
<table confidence="0.998587222222222">
ROUGE w/o stop words w/ stop words
R-1 R-2 R-4 R-1 R-2 R-4
PYTHY 35.7 8.9 12.1 42.6 11.9 16.8
HIERSUM 33.8 9.3 11.6 42.4 11.8 16.7
HybHSum 35.1 8.3 11.8 45.6 11.4 17.2
PAM 32.1 7.1 11.0 41.7 9.1 15.3
hPAM 31.9 7.0 11.1 41.2 8.9 15.2
TTM* 34.0 8.7 11.5 44.7 10.7 16.5
ETTM* 32.4 8.3 11.2 44.1 10.4 16.4
</table>
<tableCaption confidence="0.985448">
Table 1: ROUGE results of the best systems on DUC2007
dataset (best results are bolded.) * indicate our models.
</tableCaption>
<bodyText confidence="0.998707711111111">
(recall against skip-4 bigrams) ROUGE scores w/ and
w/o stop words included.
For our models, we ran Gibbs samplers for 2000
iterations for each configuration throwing out first
500 samples as burn-in. We iterated different values
for hyperparameters and measured the performance
on validation dataset to capture the optimum values.
The following models are used as benchmark:
(i) PYTHY (Toutanova et al., 2007): Utilizes hu-
man generated summaries to train a sentence rank-
ing system using a classifier model; (ii) HIERSUM
(Haghighi and Vanderwende, 2009): Based on hier-
archical topic models. Using an approximation for
inference, sentences are greedily added to a sum-
mary so long as they decrease KL-divergence of the
generated summary concept distributions from doc-
ument word-frequency distributions. (iii) HybHSum
(Celikyilmaz and Hakkani-Tur, 2010): A semi-
supervised model, which builds a hierarchial LDA to
probabilistically score sentences in training dataset
as summary or non-summary sentences. Using these
probabilities as output variables, it learns a discrim-
inative classifier model to infer the scores of new
sentences in testing dataset. (iv) PAM (Li and Mc-
Callum, 2006) and hPAM (Mimno et al., 2007): Two
hierarchical topic models to discover high and low-
level concepts from documents, baselines for syn-
thetic experiments in §4 &amp; §5.
Results of our experiments are illustrated in Table
6. Our unsupervised TTM and ETTM systems yield a
44.1 R-1 (w/ stop-words) outperforming the rest of
the models, except HybHSum. Because HybHSum
uses the human generated summaries as supervision
during model development and our systems do not,
our performance is quite promising considering the
generation is completely unsupervised without see-
ing any human generated summaries during train-
ing. However, the R-2 evaluation (as well as R-4) w/
stop-words does not outperform other models. This
is because R-2 is a measure of bi-gram recall and
neither of our models represent bi-grams whereas,
for instance, PHTHY includes several bi-gram and
higher order n-gram statistics. For topic models bi-
grams tend to degenerate due to generating inconsis-
tent bag of bi-grams (Wallach, 2006).
</bodyText>
<subsectionHeader confidence="0.894094">
6.2. Manual Evaluations: A common DUC
</subsectionHeader>
<bodyText confidence="0.999986137931034">
task is to manually evaluate models on the qual-
ity of generated summaries. We compare our best
model ETTM to the results of PAM, our benchmark
model in synthetic experiments, as well as hybrid
hierarchical summarization model, hLDA (Celiky-
ilmaz and Hakkani-Tur, 2010). Human annotators
are given two sets of summary text for each docu-
ment set, generated from either one of the two ap-
proaches: best ETTM and PAM or best ETTM and
HybHSum models. The annotators are asked to
mark the better summary according to five criteria:
non-redundancy (which summary is less redundant),
coherence (which summary is more coherent), fo-
cus and readability (content and no unnecessary de-
tails), responsiveness and overall performance.
We asked 3 annotators to rate DUC2007 predicted
summaries (45 summary pairs per annotator). A to-
tal of 42 pairs are judged for ETTM vs. PAM mod-
els and 49 pairs for ETTM vs. HybHSum models.
The evaluation results in frequencies are shown in
Table 6. The participants rated ETTM generated
summaries more coherent and focused compared to
PAM, where the results are statistically significant
(based on t-test on 95% confidence level) indicat-
ing that ETTM summaries are rated significantly bet-
ter. The results of ETTM are slightly better than
HybHSum. We consider our results promising be-
cause, being unsupervised, ETTM does not utilize
human summaries for model development.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999858">
We introduce two new models for extracting topi-
cally coherent sentences from documents, an impor-
tant property in extractive multi-document summa-
rization systems. Our models combine approaches
from the hierarchical topic models. We empha-
</bodyText>
<page confidence="0.997462">
498
</page>
<table confidence="0.996226833333333">
PAM ETTM Tie HybHSum ETTM Tie
Non-Redundancy 13 26 3 12 18 19
Coherence 13 26 3 15 18 16
Focus 14 24 4 14 17 18
Responsiveness 15 24 3 19 12 18
Overall 15 25 2 17 22 10
</table>
<tableCaption confidence="0.9933085">
Table 2: Frequency results of manual evaluations. Tie in-
dicates evaluations where two summaries are rated equal.
</tableCaption>
<bodyText confidence="0.999835">
size capturing correlated semantic concepts in docu-
ments as well as characterizing general and specific
words, in order to identify topically coherent sen-
tences in documents. We showed empirically that a
fully unsupervised model for extracting general sen-
tences performs well at summarization task using
datasets that were originally used in building auto-
matic summarization system challenges. The suc-
cess of our model can be traced to its capability
of directly capturing coherent topics in documents,
which makes it able to identify salient sentences.
</bodyText>
<sectionHeader confidence="0.998201" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999512">
The authors would like to thank Dr. Zhaleh Feizol-
lahi for her useful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.999094" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987132">
R. Barzilay and L. Lee. 2004. Catching the drift: Proba-
bilistic content models with applications to generation
and summarization. In Proc. HLT-NAACL’04.
R. Barzilay, K.R. McKeown, and M. Elhadad. 1999.
Information fusion in the context of multi-document
summarization. Proc. 37th ACL, pages 550–557.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.
2004. Hierarchical topic models and the nested chi-
nese restaurant process. In Neural Information Pro-
cessing Systems [NIPS].
A. Celikyilmaz and D. Hakkani-Tur. 2010. A hybrid hi-
erarchical model for multi-document summarization.
Proc. 48th ACL 2010.
D. Chen, J. Tang, L. Yao, J. Li, and L. Zhou. 2000.
Query-focused summarization by combining topic
model and affinity propagation. LNCS– Advances in
Data and Web Development.
J. Conroy, H. Schlesinger, and D. OLeary. 2006. Topic-
focused multi-document summarization using an ap-
proximate oracle score. Proc. ACL.
H. Daum&amp;III and D. Marcu. 2006. Bayesian query fo-
cused summarization. Proc. ACL-06.
J. Eisenstein and R. Barzilay. 2008. Bayesian unsuper-
vised topic segmentation. Proc. EMNLP-SIGDAT.
A. Haghighi and L. Vanderwende. 2009. Exploring
content models for multi-document summarization.
NAACL HLT-09.
S. Harabagiu, A. Hickl, and F. Lacatusu. 2007. Sat-
isfying information needs with multi-document sum-
maries. Information Processing and Management.
W. Li and A. McCallum. 2006. Pachinko allocation:
Dag-structure mixture models of topic correlations.
Proc. ICML.
W. Li, D. Blei, and A. McCallum. 2007. Nonparametric
bayes pachinko allocation. The 23rd Conference on
Uncertainty in Artificial Intelligence.
C.Y. Lin and E. Hovy. 2002. The automated acquisi-
tion of topic signatures fro text summarization. Proc.
CoLing.
G. A. Miller. 1995. Wordnet: A lexical database for
english. ACM, Vol. 38, No. 11: 39-41.
D. Mimno, W. Li, and A. McCallum. 2007. Mixtures
of hierarchical topics with pachinko allocation. Proc.
ICML.
A. Nenkova and L. Vanderwende. 2005a. Document
summarization using conditional random fields. Tech-
nical report, Microsoft Research.
A. Nenkova and L. Vanderwende. 2005b. The impact
of frequency on summarization. Technical report, Mi-
crosoft Research.
A. Nenkova, L. Vanderwende, and K. McKowen. 2006.
A composition context sensitive multi-document sum-
marizer. Prof. SIGIR.
D. R. Radev. 2004. Lexrank: graph-based centrality as
salience in text summarization. Jrnl. Artificial Intelli-
gence Research.
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth.
2004. The author-topic model for authors and docu-
ments. UAI.
J. Tang, L. Yao, and D. Chens. 2009. Multi-topic based
query-oriented summarization. SIAM International
Conference Data Mining.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The ph-
thy summarization system: Microsoft research at duc
2007. In Proc. DUC.
H. Wallach. 2006. Topic modeling: Beyond bag-of-
words. Proc. ICML 2006.
X. Wan and J. Yang. 2006. Improved affinity graph
based multi-document summarization. HLT-NAACL.
D. Wang, S. Zhu, T. Li, and Y. Gong. 2009. Multi-
document summarization using sentence-based topic
models. Proc. ACL 2009.
</reference>
<page confidence="0.99918">
499
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.739110">
<title confidence="0.998182">Discovery of Topically Coherent Sentences for Extractive Summarization</title>
<author confidence="0.882516">Asli Celikyilmaz Dilek Hakkani-T¨ur</author>
<affiliation confidence="0.827666">Speech Labs Microsoft Speech Labs Research</affiliation>
<address confidence="0.996522">Mountain View, CA, 94041 Mountain View, CA, 94041</address>
<email confidence="0.912969">asli@ieee.orgdilek@ieee.org</email>
<abstract confidence="0.999240117647059">Extractive methods for multi-document summarization are mainly governed by information overlap, coherence, and content constraints. We present an unsupervised probabilistic approach to model the hidden abstract concepts across documents as well as the correlation between these concepts, to generate topically coherent and non-redundant summaries. Based on human evaluations our models generate summaries with higher linguistic quality in terms of coherence, readability, and redundancy compared to benchmark systems. Although our system is unsupervised and optimized for topical coherence, we achieve a 44.1 the DUC-07 test set, roughly in the range of state-of-the-art supervised models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL’04.</booktitle>
<contexts>
<context position="6262" citStr="Barzilay and Lee, 2004" startWordPosition="932" endWordPosition="935">al., 2006; Harabagiu we model sentences as meta-variables. This is simet al., 2007); high frequency content word feature ilar to author-topic models (Rosen-Zvi et al., 2004), based learning (Nenkova and Vanderwende, 2005a; in which words are generated by first selecting an Nenkova and Vanderwende, 2005b), to name a few. author uniformly from an observed author list and Recent research focusing on the extraction of la- then selecting a topic from a distribution over topics tent concepts from document clusters are close in that is specific to that author. In our model, words spirit to our work (Barzilay and Lee, 2004; Daume- are generated from different topics of documents by III and Marcu, 2006; Eisenstein and Barzilay, 2008; first selecting a sentence containing the word and Tang et al., 2009; Wang et al., 2009). Some of these then topics that are specific to that sentence. This work (Haghighi and Vanderwende, 2009; Celikyil- way we can directly extract from documents the maz and Hakkani-Tur, 2010) focus on the discov- summary related sentences that contain high-level ery of hierarchical concepts from documents (from topics. In addition in (Celikyilmaz and Hakkani-Tur, abstract to specific) using extens</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>R. Barzilay and L. Lee. 2004. Catching the drift: Probabilistic content models with applications to generation and summarization. In Proc. HLT-NAACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K R McKeown</author>
<author>M Elhadad</author>
</authors>
<title>Information fusion in the context of multi-document summarization.</title>
<date>1999</date>
<booktitle>Proc. 37th ACL,</booktitle>
<pages>550--557</pages>
<contexts>
<context position="2230" citStr="Barzilay et al., 1999" startWordPosition="319" endWordPosition="322">d their correlations to extract topically coherent sentences. Prior research has demonstrated the usefulness of sentence extraction for generating summary text taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daume-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract concepts rather than specific concepts. Nonetheless, all these systems crucially rely on extracting various levels of generality from documents, focusing little on redundancy and coherence issues in model building. A model than can focus on both issues is deemed to be more beneficial for a summarization task. Topica</context>
</contexts>
<marker>Barzilay, McKeown, Elhadad, 1999</marker>
<rawString>R. Barzilay, K.R. McKeown, and M. Elhadad. 1999. Information fusion in the context of multi-document summarization. Proc. 37th ACL, pages 550–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="15187" citStr="Blei et al., 2003" startWordPosition="2407" endWordPosition="2410"> for * if wid exists or related to the the query then x = 1 deterministic, otherwise it is stochastically assigned x — Bin(T). ** wid is a background word. relation between Tk,’s, e.g., the topic ”acquisition” is found to be more correlated with ”retail” than the ”network” topic given H1. This information is used to rank sentences based on the correlated topics. 4.1 Learning and Inference for TTM Our learning procedure involves finding parameters, which likely integrates out model’s posterior distribution P(H, T|Wd, S), d∈D. EM algorithms might face problems with local maxima in topic models (Blei et al., 2003) suggesting implementation of approximate methods in which some of the parameters, e.g., 0H, OT, 0, and 0, can be integrated out, resulting in standard Dirichlet-multinomial as well as binomial distributions. We use Gibbs sampling which allows a combination of estimates from several local maxima of the posterior distribution. For each word, xid is sampled from a sentence specific binomial 0 which in turn has a smoothing prior q to determine if the sampled word wid is (query) summary-related or document-specific. Depending on xid, we either sample a sentence along with a high/low-level topic pa</context>
<context position="18178" citStr="Blei et al., 2003" startWordPosition="2950" endWordPosition="2953">ft for future work). 4.2 Summary Generation with TTM We can observe the frequency of draws of every sentence in a document cluster S, given it’s words are related, through DS E ZSD. We obtain DS during Gibbs sampling (in §4.1), which indicates a saliency score of each sentence sj E S, j = 1..SD: scoreTTM(sj) OC # [wid E sj, xid = 1] /nwj (1) where wid indicates a word in a document d that exists in sj and is sampled as summary related based on random indicator variable xid. nwj is the number of words in sj and normalizes the score favoring 3An alternative way would be to use Dirichlet priors (Blei et al., 2003) which we opted for due to computational reasons but will be investigated as future research. sentences with many related words. We rank sentences based on (1). We compare TTM results on synthetic experiments against PAM (Li and McCallum, 2006) a similar topic model that clusters topics in a hierarchical structure, where super-topics are distributions over sub-topics. We obtain sentence scores for PAM models by calculating the sub-topic significance (TS) based on super-topic correlations, and discover topic correlations over the entire document space (corpus wide). Hence; we calculate the TS o</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>T Griffiths</author>
<author>M Jordan</author>
<author>J Tenenbaum</author>
</authors>
<title>Hierarchical topic models and the nested chinese restaurant process.</title>
<date>2004</date>
<booktitle>In Neural Information Processing Systems [NIPS].</booktitle>
<contexts>
<context position="6966" citStr="Blei et al., 2004" startWordPosition="1042" endWordPosition="1045">nstein and Barzilay, 2008; first selecting a sentence containing the word and Tang et al., 2009; Wang et al., 2009). Some of these then topics that are specific to that sentence. This work (Haghighi and Vanderwende, 2009; Celikyil- way we can directly extract from documents the maz and Hakkani-Tur, 2010) focus on the discov- summary related sentences that contain high-level ery of hierarchical concepts from documents (from topics. In addition in (Celikyilmaz and Hakkani-Tur, abstract to specific) using extensions of hierarchal 2010), the sentences can only share topics if the sentopic models (Blei et al., 2004) and reflect this hier- tences are represented on the same path of captured archy on the sentences. Hierarchical concept learn- topic hierarchy, restricting topic sharing across sening models help to discover, for instance, that ”base- tences on different paths. Our DAG identifies tiered ball” and ”football” are both contained in a general topics distributed over document clusters that can be class ”sports”, so that the summaries reference terms shared by each sentence. related to more abstract concepts like ”sports”. 3 Topic Coherence for Summarization Although successful, the issue with conc</context>
</contexts>
<marker>Blei, Griffiths, Jordan, Tenenbaum, 2004</marker>
<rawString>D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum. 2004. Hierarchical topic models and the nested chinese restaurant process. In Neural Information Processing Systems [NIPS].</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Celikyilmaz</author>
<author>D Hakkani-Tur</author>
</authors>
<title>A hybrid hierarchical model for multi-document summarization.</title>
<date>2010</date>
<booktitle>Proc. 48th ACL</booktitle>
<contexts>
<context position="5612" citStr="Celikyilmaz and Hakkani-Tur, 2010" startWordPosition="826" endWordPosition="829">s has not been the focus in prior work on generaPrior research has demonstrated the usefulness of tive approaches for summarization task. Mainly, our sentence extraction for summarization based on lex- model can discover correlated topics to eliminate reical, semantic, and discourse constraints. Such dundant sentences in summary text. models often rely on different approaches includ- Rather than representing sentences as a layer in ing: identifying important keywords (Nenkova et al., hierarchical models, e.g., (Haghighi and Vander2006); topic signatures based on user queries (Lin wende, 2009; Celikyilmaz and Hakkani-Tur, 2010), and Hovy, 2002; Conroy et al., 2006; Harabagiu we model sentences as meta-variables. This is simet al., 2007); high frequency content word feature ilar to author-topic models (Rosen-Zvi et al., 2004), based learning (Nenkova and Vanderwende, 2005a; in which words are generated by first selecting an Nenkova and Vanderwende, 2005b), to name a few. author uniformly from an observed author list and Recent research focusing on the extraction of la- then selecting a topic from a distribution over topics tent concepts from document clusters are close in that is specific to that author. In our model</context>
<context position="29304" citStr="Celikyilmaz and Hakkani-Tur, 2010" startWordPosition="4862" endWordPosition="4865">ferent values for hyperparameters and measured the performance on validation dataset to capture the optimum values. The following models are used as benchmark: (i) PYTHY (Toutanova et al., 2007): Utilizes human generated summaries to train a sentence ranking system using a classifier model; (ii) HIERSUM (Haghighi and Vanderwende, 2009): Based on hierarchical topic models. Using an approximation for inference, sentences are greedily added to a summary so long as they decrease KL-divergence of the generated summary concept distributions from document word-frequency distributions. (iii) HybHSum (Celikyilmaz and Hakkani-Tur, 2010): A semisupervised model, which builds a hierarchial LDA to probabilistically score sentences in training dataset as summary or non-summary sentences. Using these probabilities as output variables, it learns a discriminative classifier model to infer the scores of new sentences in testing dataset. (iv) PAM (Li and McCallum, 2006) and hPAM (Mimno et al., 2007): Two hierarchical topic models to discover high and lowlevel concepts from documents, baselines for synthetic experiments in §4 &amp; §5. Results of our experiments are illustrated in Table 6. Our unsupervised TTM and ETTM systems yield a 44.</context>
<context position="30938" citStr="Celikyilmaz and Hakkani-Tur, 2010" startWordPosition="5118" endWordPosition="5122"> does not outperform other models. This is because R-2 is a measure of bi-gram recall and neither of our models represent bi-grams whereas, for instance, PHTHY includes several bi-gram and higher order n-gram statistics. For topic models bigrams tend to degenerate due to generating inconsistent bag of bi-grams (Wallach, 2006). 6.2. Manual Evaluations: A common DUC task is to manually evaluate models on the quality of generated summaries. We compare our best model ETTM to the results of PAM, our benchmark model in synthetic experiments, as well as hybrid hierarchical summarization model, hLDA (Celikyilmaz and Hakkani-Tur, 2010). Human annotators are given two sets of summary text for each document set, generated from either one of the two approaches: best ETTM and PAM or best ETTM and HybHSum models. The annotators are asked to mark the better summary according to five criteria: non-redundancy (which summary is less redundant), coherence (which summary is more coherent), focus and readability (content and no unnecessary details), responsiveness and overall performance. We asked 3 annotators to rate DUC2007 predicted summaries (45 summary pairs per annotator). A total of 42 pairs are judged for ETTM vs. PAM models an</context>
</contexts>
<marker>Celikyilmaz, Hakkani-Tur, 2010</marker>
<rawString>A. Celikyilmaz and D. Hakkani-Tur. 2010. A hybrid hierarchical model for multi-document summarization. Proc. 48th ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chen</author>
<author>J Tang</author>
<author>L Yao</author>
<author>J Li</author>
<author>L Zhou</author>
</authors>
<title>Query-focused summarization by combining topic model and affinity propagation.</title>
<date>2000</date>
<booktitle>LNCS– Advances in Data and Web Development.</booktitle>
<contexts>
<context position="2326" citStr="Chen et al., 2000" startWordPosition="335" endWordPosition="338">efulness of sentence extraction for generating summary text taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daume-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract concepts rather than specific concepts. Nonetheless, all these systems crucially rely on extracting various levels of generality from documents, focusing little on redundancy and coherence issues in model building. A model than can focus on both issues is deemed to be more beneficial for a summarization task. Topical coherence in text involves identifying key concepts, the relationships between these concepts,</context>
</contexts>
<marker>Chen, Tang, Yao, Li, Zhou, 2000</marker>
<rawString>D. Chen, J. Tang, L. Yao, J. Li, and L. Zhou. 2000. Query-focused summarization by combining topic model and affinity propagation. LNCS– Advances in Data and Web Development.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Conroy</author>
<author>H Schlesinger</author>
<author>D OLeary</author>
</authors>
<title>Topicfocused multi-document summarization using an approximate oracle score.</title>
<date>2006</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="5649" citStr="Conroy et al., 2006" startWordPosition="833" endWordPosition="836">r research has demonstrated the usefulness of tive approaches for summarization task. Mainly, our sentence extraction for summarization based on lex- model can discover correlated topics to eliminate reical, semantic, and discourse constraints. Such dundant sentences in summary text. models often rely on different approaches includ- Rather than representing sentences as a layer in ing: identifying important keywords (Nenkova et al., hierarchical models, e.g., (Haghighi and Vander2006); topic signatures based on user queries (Lin wende, 2009; Celikyilmaz and Hakkani-Tur, 2010), and Hovy, 2002; Conroy et al., 2006; Harabagiu we model sentences as meta-variables. This is simet al., 2007); high frequency content word feature ilar to author-topic models (Rosen-Zvi et al., 2004), based learning (Nenkova and Vanderwende, 2005a; in which words are generated by first selecting an Nenkova and Vanderwende, 2005b), to name a few. author uniformly from an observed author list and Recent research focusing on the extraction of la- then selecting a topic from a distribution over topics tent concepts from document clusters are close in that is specific to that author. In our model, words spirit to our work (Barzilay </context>
</contexts>
<marker>Conroy, Schlesinger, OLeary, 2006</marker>
<rawString>J. Conroy, H. Schlesinger, and D. OLeary. 2006. Topicfocused multi-document summarization using an approximate oracle score. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum&amp;III</author>
<author>D Marcu</author>
</authors>
<title>Bayesian query focused summarization.</title>
<date>2006</date>
<booktitle>Proc. ACL-06.</booktitle>
<marker>Daum&amp;III, Marcu, 2006</marker>
<rawString>H. Daum&amp;III and D. Marcu. 2006. Bayesian query focused summarization. Proc. ACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>R Barzilay</author>
</authors>
<title>Bayesian unsupervised topic segmentation.</title>
<date>2008</date>
<booktitle>Proc. EMNLP-SIGDAT.</booktitle>
<contexts>
<context position="2288" citStr="Eisenstein and Barzilay, 2008" startWordPosition="327" endWordPosition="330"> sentences. Prior research has demonstrated the usefulness of sentence extraction for generating summary text taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daume-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract concepts rather than specific concepts. Nonetheless, all these systems crucially rely on extracting various levels of generality from documents, focusing little on redundancy and coherence issues in model building. A model than can focus on both issues is deemed to be more beneficial for a summarization task. Topical coherence in text involves identifying key concepts, the</context>
<context position="6373" citStr="Eisenstein and Barzilay, 2008" startWordPosition="949" endWordPosition="952">ent word feature ilar to author-topic models (Rosen-Zvi et al., 2004), based learning (Nenkova and Vanderwende, 2005a; in which words are generated by first selecting an Nenkova and Vanderwende, 2005b), to name a few. author uniformly from an observed author list and Recent research focusing on the extraction of la- then selecting a topic from a distribution over topics tent concepts from document clusters are close in that is specific to that author. In our model, words spirit to our work (Barzilay and Lee, 2004; Daume- are generated from different topics of documents by III and Marcu, 2006; Eisenstein and Barzilay, 2008; first selecting a sentence containing the word and Tang et al., 2009; Wang et al., 2009). Some of these then topics that are specific to that sentence. This work (Haghighi and Vanderwende, 2009; Celikyil- way we can directly extract from documents the maz and Hakkani-Tur, 2010) focus on the discov- summary related sentences that contain high-level ery of hierarchical concepts from documents (from topics. In addition in (Celikyilmaz and Hakkani-Tur, abstract to specific) using extensions of hierarchal 2010), the sentences can only share topics if the sentopic models (Blei et al., 2004) and re</context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>J. Eisenstein and R. Barzilay. 2008. Bayesian unsupervised topic segmentation. Proc. EMNLP-SIGDAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>L Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization. NAACL HLT-09.</title>
<date>2009</date>
<contexts>
<context position="2393" citStr="Haghighi and Vanderwende, 2009" startWordPosition="345" endWordPosition="348"> text taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daume-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract concepts rather than specific concepts. Nonetheless, all these systems crucially rely on extracting various levels of generality from documents, focusing little on redundancy and coherence issues in model building. A model than can focus on both issues is deemed to be more beneficial for a summarization task. Topical coherence in text involves identifying key concepts, the relationships between these concepts, and linking these relationships into a hierarchy. In this paper, w</context>
<context position="6568" citStr="Haghighi and Vanderwende, 2009" startWordPosition="982" endWordPosition="985"> 2005b), to name a few. author uniformly from an observed author list and Recent research focusing on the extraction of la- then selecting a topic from a distribution over topics tent concepts from document clusters are close in that is specific to that author. In our model, words spirit to our work (Barzilay and Lee, 2004; Daume- are generated from different topics of documents by III and Marcu, 2006; Eisenstein and Barzilay, 2008; first selecting a sentence containing the word and Tang et al., 2009; Wang et al., 2009). Some of these then topics that are specific to that sentence. This work (Haghighi and Vanderwende, 2009; Celikyil- way we can directly extract from documents the maz and Hakkani-Tur, 2010) focus on the discov- summary related sentences that contain high-level ery of hierarchical concepts from documents (from topics. In addition in (Celikyilmaz and Hakkani-Tur, abstract to specific) using extensions of hierarchal 2010), the sentences can only share topics if the sentopic models (Blei et al., 2004) and reflect this hier- tences are represented on the same path of captured archy on the sentences. Hierarchical concept learn- topic hierarchy, restricting topic sharing across sening models help to di</context>
<context position="29007" citStr="Haghighi and Vanderwende, 2009" startWordPosition="4820" endWordPosition="4823"> systems on DUC2007 dataset (best results are bolded.) * indicate our models. (recall against skip-4 bigrams) ROUGE scores w/ and w/o stop words included. For our models, we ran Gibbs samplers for 2000 iterations for each configuration throwing out first 500 samples as burn-in. We iterated different values for hyperparameters and measured the performance on validation dataset to capture the optimum values. The following models are used as benchmark: (i) PYTHY (Toutanova et al., 2007): Utilizes human generated summaries to train a sentence ranking system using a classifier model; (ii) HIERSUM (Haghighi and Vanderwende, 2009): Based on hierarchical topic models. Using an approximation for inference, sentences are greedily added to a summary so long as they decrease KL-divergence of the generated summary concept distributions from document word-frequency distributions. (iii) HybHSum (Celikyilmaz and Hakkani-Tur, 2010): A semisupervised model, which builds a hierarchial LDA to probabilistically score sentences in training dataset as summary or non-summary sentences. Using these probabilities as output variables, it learns a discriminative classifier model to infer the scores of new sentences in testing dataset. (iv)</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>A. Haghighi and L. Vanderwende. 2009. Exploring content models for multi-document summarization. NAACL HLT-09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>A Hickl</author>
<author>F Lacatusu</author>
</authors>
<title>Satisfying information needs with multi-document summaries. Information Processing and Management.</title>
<date>2007</date>
<marker>Harabagiu, Hickl, Lacatusu, 2007</marker>
<rawString>S. Harabagiu, A. Hickl, and F. Lacatusu. 2007. Satisfying information needs with multi-document summaries. Information Processing and Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Li</author>
<author>A McCallum</author>
</authors>
<title>Pachinko allocation: Dag-structure mixture models of topic correlations.</title>
<date>2006</date>
<booktitle>Proc. ICML.</booktitle>
<contexts>
<context position="4228" citStr="Li and McCallum, 2006" startWordPosition="626" endWordPosition="629">ssociation for Computational Linguistics capture higher level topics (concepts) related to sum- we utilize the advantages of previous topic models mary text discussed in §3, and build an unsupervised generative model that can − representation of a linguistic system as a sequence associate each word in each document with three of increasingly enriched models, which use posterior random variables: a sentence S, a higher-level topic topic correlation probabilities in sentences to design H, and a lower-level topic T, in an analogical way a novel sentence ranking method in §4 and 5, to PAM models (Li and McCallum, 2006), i.e., a di− application of the new hierarchical learning rected acyclic graph (DAG) representing mixtures of method for generation of less redundant summaries hierarchical structure, where super-topics are multidiscussed in §6. Our models achieve compara- nomials over sub-topics at lower levels in the DAG. ble qualitative results on summarization of multiple We define a tiered-topic clustering in which the upnewswire documents. Human evaluations of gener- per nodes in the DAG are higher-level topics H, repated summaries confirm that our model can generate resenting common co-occurence patter</context>
<context position="10259" citStr="Li and McCallum (2006)" startWordPosition="1557" endWordPosition="1560">cabulary of size V , and a vector of sentences S, representing all sentences in a corpus of size SD. We identify sentences as meta-variables of document clusters, which the generative process models both sentences and documents using tiered topics. A sentence’s relatedness to summary text is tied to the document cluster’s user query. The idea is that a lexical word present or related to a query should increase its sentence’s probability of relatedness. 4 Two-Tiered Topic Model - TTM Our base model, the two-tiered topic model (TTM), is inspired by the hierarchical topic model, PAM, proposed by Li and McCallum (2006). PAM structures documents to represent and learn arbitrary, nested, and possibly sparse topic correlations using 1www-nlpir.nist.gov/projects/duc/data.html Documents in a Document Cluster Figure 1: Graphical model depiction of two-tiered topic model (TTM) described in section §4. S are sentences si=1..SD in document clusters. The high-level topics (Hk1=1...x1), representing topic correlations, are modeled as distributions over lowlevel-topics (Tk2=1...x2). Shaded nodes indicate observed variables. Hyper-parameters for 0, BH, BT, B are omitted. a directed acyclic graph. Our goals are not so di</context>
<context position="18422" citStr="Li and McCallum, 2006" startWordPosition="2990" endWordPosition="2994">tes a saliency score of each sentence sj E S, j = 1..SD: scoreTTM(sj) OC # [wid E sj, xid = 1] /nwj (1) where wid indicates a word in a document d that exists in sj and is sampled as summary related based on random indicator variable xid. nwj is the number of words in sj and normalizes the score favoring 3An alternative way would be to use Dirichlet priors (Blei et al., 2003) which we opted for due to computational reasons but will be investigated as future research. sentences with many related words. We rank sentences based on (1). We compare TTM results on synthetic experiments against PAM (Li and McCallum, 2006) a similar topic model that clusters topics in a hierarchical structure, where super-topics are distributions over sub-topics. We obtain sentence scores for PAM models by calculating the sub-topic significance (TS) based on super-topic correlations, and discover topic correlations over the entire document space (corpus wide). Hence; we calculate the TS of a given sub-topic, k = 1,.., K2 by: 1 Y TS(zk) = D dED where zksub is a sub-topic k = 1..K2 and zk1 sup is a super-topic k1. The conditional probability of a subtopic k given a super-topic k1, p(zk sub|zk1 sup), explains the variation of that</context>
<context position="29635" citStr="Li and McCallum, 2006" startWordPosition="4912" endWordPosition="4916">d on hierarchical topic models. Using an approximation for inference, sentences are greedily added to a summary so long as they decrease KL-divergence of the generated summary concept distributions from document word-frequency distributions. (iii) HybHSum (Celikyilmaz and Hakkani-Tur, 2010): A semisupervised model, which builds a hierarchial LDA to probabilistically score sentences in training dataset as summary or non-summary sentences. Using these probabilities as output variables, it learns a discriminative classifier model to infer the scores of new sentences in testing dataset. (iv) PAM (Li and McCallum, 2006) and hPAM (Mimno et al., 2007): Two hierarchical topic models to discover high and lowlevel concepts from documents, baselines for synthetic experiments in §4 &amp; §5. Results of our experiments are illustrated in Table 6. Our unsupervised TTM and ETTM systems yield a 44.1 R-1 (w/ stop-words) outperforming the rest of the models, except HybHSum. Because HybHSum uses the human generated summaries as supervision during model development and our systems do not, our performance is quite promising considering the generation is completely unsupervised without seeing any human generated summaries during</context>
</contexts>
<marker>Li, McCallum, 2006</marker>
<rawString>W. Li and A. McCallum. 2006. Pachinko allocation: Dag-structure mixture models of topic correlations. Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Li</author>
<author>D Blei</author>
<author>A McCallum</author>
</authors>
<title>Nonparametric bayes pachinko allocation.</title>
<date>2007</date>
<booktitle>The 23rd Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="17556" citStr="Li et al., 2007" startWordPosition="2830" endWordPosition="2833">ecific word), pTTM(x = 0|w, zH−k, zt−k) a αw + nw Ewi αwi + n The nk1 d is the number of occurrences of high-level topic k1 in document d, and nk1k2 d is the number of times the low-level topic k2 is sampled together with high-level topic k1 in d, nwk1k2x is the number of occurrences of word w sampled from path H-T given that the word is query related. Note that the number of tiered topics in the model is fixed to K1 and K2, which is optimized with validation experiments. It is also possible to construct extended models of TTM using non-parametric priors, e.g., hierarchal Dirichlet processes (Li et al., 2007) (left for future work). 4.2 Summary Generation with TTM We can observe the frequency of draws of every sentence in a document cluster S, given it’s words are related, through DS E ZSD. We obtain DS during Gibbs sampling (in §4.1), which indicates a saliency score of each sentence sj E S, j = 1..SD: scoreTTM(sj) OC # [wid E sj, xid = 1] /nwj (1) where wid indicates a word in a document d that exists in sj and is sampled as summary related based on random indicator variable xid. nwj is the number of words in sj and normalizes the score favoring 3An alternative way would be to use Dirichlet prio</context>
</contexts>
<marker>Li, Blei, McCallum, 2007</marker>
<rawString>W. Li, D. Blei, and A. McCallum. 2007. Nonparametric bayes pachinko allocation. The 23rd Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E Hovy</author>
</authors>
<title>The automated acquisition of topic signatures fro text summarization.</title>
<date>2002</date>
<booktitle>Proc. CoLing.</booktitle>
<marker>Lin, Hovy, 2002</marker>
<rawString>C.Y. Lin and E. Hovy. 2002. The automated acquisition of topic signatures fro text summarization. Proc. CoLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>Wordnet: A lexical database for english.</title>
<date>1995</date>
<journal>ACM,</journal>
<volume>38</volume>
<pages>39--41</pages>
<contexts>
<context position="12055" citStr="Miller, 1995" startWordPosition="1846" endWordPosition="1847">a word wid in document d, a random variable xid is drawn, which determines if wid is query related, i.e., wid either exists in the query or is related to the query2. Otherwise, wid is unrelated to the user query. Then sentence si is chosen uniformly at random (ysi— Uniform(si)) from sentences in the document containing wid (deterministic if there is only one sentence containing wid). We assume that if a word is related to a query, it is likely to be summary-related 2We measure relatedness to a query if a word exists in the query or it is synonymous based on information extracted from WordNet (Miller, 1995). Document Summary Content Indicator Parameters SD Sentence selector y Higher-Level Topics H x OH K1 Higher-Level Topic Parameters K1xK2 T BT Lower-Level Topic Parameters Nd LowerLevel Topics K2 W (Background) Specific Content Parameters Summary Related Word Indicator Sentences S 493 Figure 2: Depiction of TTM given the query ”D0718D: Starbucks Coffee : How has Starbucks Coffee attempted to expand and diversify through joint ventures, acquisitions, or subsidiaries?”. If a word is query/summary related sentence S, first a sentence then a high-level (H) and a low-level (T) c topic is sampled. ( </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G. A. Miller. 1995. Wordnet: A lexical database for english. ACM, Vol. 38, No. 11: 39-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mimno</author>
<author>W Li</author>
<author>A McCallum</author>
</authors>
<title>Mixtures of hierarchical topics with pachinko allocation.</title>
<date>2007</date>
<booktitle>Proc. ICML.</booktitle>
<contexts>
<context position="26192" citStr="Mimno et al., 2007" startWordPosition="4335" endWordPosition="4338">ion with ETTM For ETTM models, we extend the TTM sentence score to be able to include the effect of the general words in sentences (as word sequences in language models) using probabilities of K1 high-level topic distributions, φwH , as: k-_ 1 Kl scoreETTM(si) « # [wid E sj, xid = 1] /nwj � K1 Ek=1..K1 HwCsi p(w |Tk) where p(w|Tk) is the probability of a word in si being generated from high-level topic Hk. Using this score, we re-rank the sentences in documents of the synthetic experiment. We compare the results of ETTM to a structurally similar probabilistic model, entitled hierarchical PAM (Mimno et al., 2007), which is designed to capture topics on a hierarchy of two layers, i.e., super topics and subtopics, where super-topics are distributions over abstract words. In Fig. 4 out of 45 models ETTM has the best performance in ranking the human generated sentences at the top, better than the TTM model. Thus; ETTM is capable of capturing focused sentences with general words related to the main concepts of the documents and much less redundant sentences containing concepts specific to user query. 6 Final Experiments In this section, we qualitatively compare our models against state-of-the art models an</context>
<context position="29665" citStr="Mimno et al., 2007" startWordPosition="4919" endWordPosition="4922">sing an approximation for inference, sentences are greedily added to a summary so long as they decrease KL-divergence of the generated summary concept distributions from document word-frequency distributions. (iii) HybHSum (Celikyilmaz and Hakkani-Tur, 2010): A semisupervised model, which builds a hierarchial LDA to probabilistically score sentences in training dataset as summary or non-summary sentences. Using these probabilities as output variables, it learns a discriminative classifier model to infer the scores of new sentences in testing dataset. (iv) PAM (Li and McCallum, 2006) and hPAM (Mimno et al., 2007): Two hierarchical topic models to discover high and lowlevel concepts from documents, baselines for synthetic experiments in §4 &amp; §5. Results of our experiments are illustrated in Table 6. Our unsupervised TTM and ETTM systems yield a 44.1 R-1 (w/ stop-words) outperforming the rest of the models, except HybHSum. Because HybHSum uses the human generated summaries as supervision during model development and our systems do not, our performance is quite promising considering the generation is completely unsupervised without seeing any human generated summaries during training. However, the R-2 ev</context>
</contexts>
<marker>Mimno, Li, McCallum, 2007</marker>
<rawString>D. Mimno, W. Li, and A. McCallum. 2007. Mixtures of hierarchical topics with pachinko allocation. Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>L Vanderwende</author>
</authors>
<title>Document summarization using conditional random fields.</title>
<date>2005</date>
<tech>Technical report, Microsoft Research.</tech>
<contexts>
<context position="1916" citStr="Nenkova and Vanderwende, 2005" startWordPosition="273" endWordPosition="276">that are directly related to the user’s query addressing different levels of detail. Recent approaches to the summarization task has somewhat focused on the redundancy and coherence issues. In this paper, we introduce a series of new generative models for multiple-documents, based on a discovery of hierarchical topics and their correlations to extract topically coherent sentences. Prior research has demonstrated the usefulness of sentence extraction for generating summary text taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daume-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract con</context>
<context position="5860" citStr="Nenkova and Vanderwende, 2005" startWordPosition="864" endWordPosition="867">eical, semantic, and discourse constraints. Such dundant sentences in summary text. models often rely on different approaches includ- Rather than representing sentences as a layer in ing: identifying important keywords (Nenkova et al., hierarchical models, e.g., (Haghighi and Vander2006); topic signatures based on user queries (Lin wende, 2009; Celikyilmaz and Hakkani-Tur, 2010), and Hovy, 2002; Conroy et al., 2006; Harabagiu we model sentences as meta-variables. This is simet al., 2007); high frequency content word feature ilar to author-topic models (Rosen-Zvi et al., 2004), based learning (Nenkova and Vanderwende, 2005a; in which words are generated by first selecting an Nenkova and Vanderwende, 2005b), to name a few. author uniformly from an observed author list and Recent research focusing on the extraction of la- then selecting a topic from a distribution over topics tent concepts from document clusters are close in that is specific to that author. In our model, words spirit to our work (Barzilay and Lee, 2004; Daume- are generated from different topics of documents by III and Marcu, 2006; Eisenstein and Barzilay, 2008; first selecting a sentence containing the word and Tang et al., 2009; Wang et al., 20</context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>A. Nenkova and L. Vanderwende. 2005a. Document summarization using conditional random fields. Technical report, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>L Vanderwende</author>
</authors>
<title>The impact of frequency on summarization.</title>
<date>2005</date>
<tech>Technical report, Microsoft Research.</tech>
<contexts>
<context position="1916" citStr="Nenkova and Vanderwende, 2005" startWordPosition="273" endWordPosition="276">that are directly related to the user’s query addressing different levels of detail. Recent approaches to the summarization task has somewhat focused on the redundancy and coherence issues. In this paper, we introduce a series of new generative models for multiple-documents, based on a discovery of hierarchical topics and their correlations to extract topically coherent sentences. Prior research has demonstrated the usefulness of sentence extraction for generating summary text taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daume-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract con</context>
<context position="5860" citStr="Nenkova and Vanderwende, 2005" startWordPosition="864" endWordPosition="867">eical, semantic, and discourse constraints. Such dundant sentences in summary text. models often rely on different approaches includ- Rather than representing sentences as a layer in ing: identifying important keywords (Nenkova et al., hierarchical models, e.g., (Haghighi and Vander2006); topic signatures based on user queries (Lin wende, 2009; Celikyilmaz and Hakkani-Tur, 2010), and Hovy, 2002; Conroy et al., 2006; Harabagiu we model sentences as meta-variables. This is simet al., 2007); high frequency content word feature ilar to author-topic models (Rosen-Zvi et al., 2004), based learning (Nenkova and Vanderwende, 2005a; in which words are generated by first selecting an Nenkova and Vanderwende, 2005b), to name a few. author uniformly from an observed author list and Recent research focusing on the extraction of la- then selecting a topic from a distribution over topics tent concepts from document clusters are close in that is specific to that author. In our model, words spirit to our work (Barzilay and Lee, 2004; Daume- are generated from different topics of documents by III and Marcu, 2006; Eisenstein and Barzilay, 2008; first selecting a sentence containing the word and Tang et al., 2009; Wang et al., 20</context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>A. Nenkova and L. Vanderwende. 2005b. The impact of frequency on summarization. Technical report, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>L Vanderwende</author>
<author>K McKowen</author>
</authors>
<title>A composition context sensitive multi-document summarizer.</title>
<date>2006</date>
<tech>Prof. SIGIR.</tech>
<contexts>
<context position="1960" citStr="Nenkova et al., 2006" startWordPosition="281" endWordPosition="284">sing different levels of detail. Recent approaches to the summarization task has somewhat focused on the redundancy and coherence issues. In this paper, we introduce a series of new generative models for multiple-documents, based on a discovery of hierarchical topics and their correlations to extract topically coherent sentences. Prior research has demonstrated the usefulness of sentence extraction for generating summary text taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daume-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract concepts rather than specific concepts. Nonethe</context>
</contexts>
<marker>Nenkova, Vanderwende, McKowen, 2006</marker>
<rawString>A. Nenkova, L. Vanderwende, and K. McKowen. 2006. A composition context sensitive multi-document summarizer. Prof. SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
</authors>
<title>Lexrank: graph-based centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Jrnl. Artificial Intelligence Research.</journal>
<contexts>
<context position="1885" citStr="Radev, 2004" startWordPosition="271" endWordPosition="272">al documents that are directly related to the user’s query addressing different levels of detail. Recent approaches to the summarization task has somewhat focused on the redundancy and coherence issues. In this paper, we introduce a series of new generative models for multiple-documents, based on a discovery of hierarchical topics and their correlations to extract topically coherent sentences. Prior research has demonstrated the usefulness of sentence extraction for generating summary text taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daume-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sent</context>
</contexts>
<marker>Radev, 2004</marker>
<rawString>D. R. Radev. 2004. Lexrank: graph-based centrality as salience in text summarization. Jrnl. Artificial Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rosen-Zvi</author>
<author>T Griffiths</author>
<author>M Steyvers</author>
<author>P Smyth</author>
</authors>
<title>The author-topic model for authors and documents.</title>
<date>2004</date>
<publisher>UAI.</publisher>
<contexts>
<context position="5813" citStr="Rosen-Zvi et al., 2004" startWordPosition="858" endWordPosition="861">discover correlated topics to eliminate reical, semantic, and discourse constraints. Such dundant sentences in summary text. models often rely on different approaches includ- Rather than representing sentences as a layer in ing: identifying important keywords (Nenkova et al., hierarchical models, e.g., (Haghighi and Vander2006); topic signatures based on user queries (Lin wende, 2009; Celikyilmaz and Hakkani-Tur, 2010), and Hovy, 2002; Conroy et al., 2006; Harabagiu we model sentences as meta-variables. This is simet al., 2007); high frequency content word feature ilar to author-topic models (Rosen-Zvi et al., 2004), based learning (Nenkova and Vanderwende, 2005a; in which words are generated by first selecting an Nenkova and Vanderwende, 2005b), to name a few. author uniformly from an observed author list and Recent research focusing on the extraction of la- then selecting a topic from a distribution over topics tent concepts from document clusters are close in that is specific to that author. In our model, words spirit to our work (Barzilay and Lee, 2004; Daume- are generated from different topics of documents by III and Marcu, 2006; Eisenstein and Barzilay, 2008; first selecting a sentence containing </context>
</contexts>
<marker>Rosen-Zvi, Griffiths, Steyvers, Smyth, 2004</marker>
<rawString>M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth. 2004. The author-topic model for authors and documents. UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tang</author>
<author>L Yao</author>
<author>D Chens</author>
</authors>
<title>Multi-topic based query-oriented summarization.</title>
<date>2009</date>
<booktitle>SIAM International Conference Data Mining.</booktitle>
<contexts>
<context position="2307" citStr="Tang et al., 2009" startWordPosition="331" endWordPosition="334">demonstrated the usefulness of sentence extraction for generating summary text taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daume-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract concepts rather than specific concepts. Nonetheless, all these systems crucially rely on extracting various levels of generality from documents, focusing little on redundancy and coherence issues in model building. A model than can focus on both issues is deemed to be more beneficial for a summarization task. Topical coherence in text involves identifying key concepts, the relationships betw</context>
<context position="6443" citStr="Tang et al., 2009" startWordPosition="961" endWordPosition="964">ing (Nenkova and Vanderwende, 2005a; in which words are generated by first selecting an Nenkova and Vanderwende, 2005b), to name a few. author uniformly from an observed author list and Recent research focusing on the extraction of la- then selecting a topic from a distribution over topics tent concepts from document clusters are close in that is specific to that author. In our model, words spirit to our work (Barzilay and Lee, 2004; Daume- are generated from different topics of documents by III and Marcu, 2006; Eisenstein and Barzilay, 2008; first selecting a sentence containing the word and Tang et al., 2009; Wang et al., 2009). Some of these then topics that are specific to that sentence. This work (Haghighi and Vanderwende, 2009; Celikyil- way we can directly extract from documents the maz and Hakkani-Tur, 2010) focus on the discov- summary related sentences that contain high-level ery of hierarchical concepts from documents (from topics. In addition in (Celikyilmaz and Hakkani-Tur, abstract to specific) using extensions of hierarchal 2010), the sentences can only share topics if the sentopic models (Blei et al., 2004) and reflect this hier- tences are represented on the same path of captured a</context>
</contexts>
<marker>Tang, Yao, Chens, 2009</marker>
<rawString>J. Tang, L. Yao, and D. Chens. 2009. Multi-topic based query-oriented summarization. SIAM International Conference Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C Brockett</author>
<author>M Gamon</author>
<author>J Jagarlamudi</author>
<author>H Suzuki</author>
<author>L Vanderwende</author>
</authors>
<title>The phthy summarization system: Microsoft research at duc 2007. In</title>
<date>2007</date>
<booktitle>Proc. DUC.</booktitle>
<contexts>
<context position="28864" citStr="Toutanova et al., 2007" startWordPosition="4798" endWordPosition="4801">PAM 31.9 7.0 11.1 41.2 8.9 15.2 TTM* 34.0 8.7 11.5 44.7 10.7 16.5 ETTM* 32.4 8.3 11.2 44.1 10.4 16.4 Table 1: ROUGE results of the best systems on DUC2007 dataset (best results are bolded.) * indicate our models. (recall against skip-4 bigrams) ROUGE scores w/ and w/o stop words included. For our models, we ran Gibbs samplers for 2000 iterations for each configuration throwing out first 500 samples as burn-in. We iterated different values for hyperparameters and measured the performance on validation dataset to capture the optimum values. The following models are used as benchmark: (i) PYTHY (Toutanova et al., 2007): Utilizes human generated summaries to train a sentence ranking system using a classifier model; (ii) HIERSUM (Haghighi and Vanderwende, 2009): Based on hierarchical topic models. Using an approximation for inference, sentences are greedily added to a summary so long as they decrease KL-divergence of the generated summary concept distributions from document word-frequency distributions. (iii) HybHSum (Celikyilmaz and Hakkani-Tur, 2010): A semisupervised model, which builds a hierarchial LDA to probabilistically score sentences in training dataset as summary or non-summary sentences. Using the</context>
</contexts>
<marker>Toutanova, Brockett, Gamon, Jagarlamudi, Suzuki, Vanderwende, 2007</marker>
<rawString>K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi, H. Suzuki, and L. Vanderwende. 2007. The phthy summarization system: Microsoft research at duc 2007. In Proc. DUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wallach</author>
</authors>
<title>Topic modeling: Beyond bag-ofwords.</title>
<date>2006</date>
<booktitle>Proc. ICML</booktitle>
<contexts>
<context position="30631" citStr="Wallach, 2006" startWordPosition="5072" endWordPosition="5073">ted summaries as supervision during model development and our systems do not, our performance is quite promising considering the generation is completely unsupervised without seeing any human generated summaries during training. However, the R-2 evaluation (as well as R-4) w/ stop-words does not outperform other models. This is because R-2 is a measure of bi-gram recall and neither of our models represent bi-grams whereas, for instance, PHTHY includes several bi-gram and higher order n-gram statistics. For topic models bigrams tend to degenerate due to generating inconsistent bag of bi-grams (Wallach, 2006). 6.2. Manual Evaluations: A common DUC task is to manually evaluate models on the quality of generated summaries. We compare our best model ETTM to the results of PAM, our benchmark model in synthetic experiments, as well as hybrid hierarchical summarization model, hLDA (Celikyilmaz and Hakkani-Tur, 2010). Human annotators are given two sets of summary text for each document set, generated from either one of the two approaches: best ETTM and PAM or best ETTM and HybHSum models. The annotators are asked to mark the better summary according to five criteria: non-redundancy (which summary is les</context>
</contexts>
<marker>Wallach, 2006</marker>
<rawString>H. Wallach. 2006. Topic modeling: Beyond bag-ofwords. Proc. ICML 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>J Yang</author>
</authors>
<title>Improved affinity graph based multi-document summarization.</title>
<date>2006</date>
<publisher>HLT-NAACL.</publisher>
<contexts>
<context position="1937" citStr="Wan and Yang, 2006" startWordPosition="277" endWordPosition="280"> user’s query addressing different levels of detail. Recent approaches to the summarization task has somewhat focused on the redundancy and coherence issues. In this paper, we introduce a series of new generative models for multiple-documents, based on a discovery of hierarchical topics and their correlations to extract topically coherent sentences. Prior research has demonstrated the usefulness of sentence extraction for generating summary text taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daume-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract concepts rather than spe</context>
</contexts>
<marker>Wan, Yang, 2006</marker>
<rawString>X. Wan and J. Yang. 2006. Improved affinity graph based multi-document summarization. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wang</author>
<author>S Zhu</author>
<author>T Li</author>
<author>Y Gong</author>
</authors>
<title>Multidocument summarization using sentence-based topic models.</title>
<date>2009</date>
<booktitle>Proc. ACL</booktitle>
<contexts>
<context position="2346" citStr="Wang et al., 2009" startWordPosition="339" endWordPosition="342">e extraction for generating summary text taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daume-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract concepts rather than specific concepts. Nonetheless, all these systems crucially rely on extracting various levels of generality from documents, focusing little on redundancy and coherence issues in model building. A model than can focus on both issues is deemed to be more beneficial for a summarization task. Topical coherence in text involves identifying key concepts, the relationships between these concepts, and linking these r</context>
<context position="6463" citStr="Wang et al., 2009" startWordPosition="965" endWordPosition="968">nderwende, 2005a; in which words are generated by first selecting an Nenkova and Vanderwende, 2005b), to name a few. author uniformly from an observed author list and Recent research focusing on the extraction of la- then selecting a topic from a distribution over topics tent concepts from document clusters are close in that is specific to that author. In our model, words spirit to our work (Barzilay and Lee, 2004; Daume- are generated from different topics of documents by III and Marcu, 2006; Eisenstein and Barzilay, 2008; first selecting a sentence containing the word and Tang et al., 2009; Wang et al., 2009). Some of these then topics that are specific to that sentence. This work (Haghighi and Vanderwende, 2009; Celikyil- way we can directly extract from documents the maz and Hakkani-Tur, 2010) focus on the discov- summary related sentences that contain high-level ery of hierarchical concepts from documents (from topics. In addition in (Celikyilmaz and Hakkani-Tur, abstract to specific) using extensions of hierarchal 2010), the sentences can only share topics if the sentopic models (Blei et al., 2004) and reflect this hier- tences are represented on the same path of captured archy on the sentence</context>
</contexts>
<marker>Wang, Zhu, Li, Gong, 2009</marker>
<rawString>D. Wang, S. Zhu, T. Li, and Y. Gong. 2009. Multidocument summarization using sentence-based topic models. Proc. ACL 2009.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>