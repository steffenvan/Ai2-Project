<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.978015">
REASONING ON A HIGHLIGHTED USER MODEL
TO RESPOND TO MISCONCEPTIONS
</title>
<author confidence="0.991603">
Kathleen F. McCoy
</author>
<affiliation confidence="0.9746455">
Department of Computer and Information Sciences
University of Delaware
</affiliation>
<address confidence="0.441841">
Newark, DE 19716
</address>
<subsectionHeader confidence="0.326021">
Responses to misconceptions given by human conversational partners very often contain information
</subsectionHeader>
<bodyText confidence="0.995230235294118">
refuting possible reasoning which may have led to the misconceptions. Surprisingly there is a great deal
of regularity in these responses across different domains of discourse. For instance, one reason a user
might have given an object a property it does not have is that the user confused the object with another
similar object. In correcting such a misconception, a human conversational partner is likely to point out
this possible confusion.
This work describes a method for generating responses like the one just described by reasoning on a
highlighted model of the user to identify possible sources of the error. Through a transcript study a
number of response strategies were abstracted. Each strategy was associated with a structural
configuration of the user model. For example, the above mentioned strategy of pointing out a similar
confused object is associated with a configuration of the user model that indicates the user believes there
is an important similar object that has the property involved in the misconception. Upon finding that
configuration in the highlighted user model, the system can respond with the associated strategy.
Notice that the reasoning must be done on a highlighted user model since the perception of both an
object&apos;s importance and its similarity with another object change with the perspective being taken on the
domain. This paper investigates how domain perspective can be modeled to provide the needed
highlighting and introduces a similarity metric that is sensitive to the highlighting provided by the
domain perspective. Finally, the paper shows how the highlighting affects misconception responses.
</bodyText>
<sectionHeader confidence="0.992962" genericHeader="abstract">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999917942857143">
When people interact with a database or expert system,
it is reasonable to expect that they might reveal a
misconception about an object modeled by the system.
Since a human conversational partner would correct
such a misconception if it was important to the current
goals of the conversation, our database and expert
systems should also be equipped with this ability.
In order to investigate how the process of correcting
misconceptions might be automated, a study of tran-
scripts of both humans interacting with what they
thought were expert systems (Malhotra 1975, Malhotra
and Sheridan 1976, Schuster 1982), and humans inter-
acting with other humans to achieve some goal (Pollack,
Hirschberg, and Webber 1982) was undertaken. The
transcripts, which varied greatly in their domains of
discourse, were analyzed to determine if there was any
regularity in the content and rhetorical force of re-
sponses given to misconceptions. The intention of this
analysis was not to mimic the actual behavior found in
the transcripts, but to use them as a source of intuitions
about the context and textual shape of responses as well
as the process of generating them.
The study revealed that a response to a misconcep-
tion important to the current discourse goals of the
participants can be viewed as consisting of three parts:
1. a denial of the incorrect information; 2. a statement of
the correct information; and 3. justification for the
denial and correction given. For a particular type of
misconception (i.e., one involving a particular kind of
knowledge), variations in responses could be found in
the form of the justification given. The justification
often seemed to refute support that might have led to
the misconception. While the kind of support someone
might have for a misconception seems unrestricted, the
form of the justification was limited for misconceptions
</bodyText>
<footnote confidence="0.7232825">
Copyright 1988 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided
that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To
copy otherwise, or to republish, requires a fee and/or specific permission.
0362-613X/ 88 /0100e-e$03 .00
</footnote>
<page confidence="0.88909">
52 Computational Linguistics, Volume 14, Number 3, September 1988
</page>
<note confidence="0.838838">
Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions
</note>
<bodyText confidence="0.999881164556962">
involving a particular kind of knowledge. A large num-
ber of responses found could be accounted for by a
small number of correction strategies based on the kind
of justification given (and hence the faulty reasoning
refuted).
If a principled reason for using one strategy over
another could be developed, these strategies could be
used by a natural language generation system for re-
sponding to a misconception. In this work the faulty
reasoning refuted by several of the found correction
strategies is characterized in a domain-independent
fashion in terms of the user&apos;s beliefs about the domain.
Therefore, given a highlighted model of the user&apos;s
beliefs about the domain, a generation system can look
for possible support for the misconception. The re-
sponse strategy that refuted the kind of support found
could then be instantiated. Notice that the domain-
independent characterization of the faulty reasoning
enables the same strategies and same method for choos-
ing a strategy to be used given a highlighted user model
for any domain.
It is crucial that the user model given to the miscon-
ception corrector be highlighted by previous discourse
since the kind of response given by a human conversa-
tion partner is apparently not only dependent on the
beliefs about the person being corrected, but also on the
context in which the misconception occurred. For in-
stance, we could imagine the following dialog where the
user exhibits the misconception that T-bills have a
penalty. A reasonable response is shown.
U: I am interested in investing in some securities to
use as savings instruments. I want something
short-term and I don&apos;t have a lot of money to
invest, so the instrument must have small denom-
inations. I am a bit concerned about the penalties
for early withdrawal. What is the penalty on a
T-Bill?
R: T-Bills don&apos;t have a penalty. Were you thinking
of Money Market Certificates?
This response might be prompted by R thinking that U
came to the misconception by confusing T-Bills with
Money Market Certificates.
On the other hand, it is reasonable that the response
might be different given a different preceding dialog.
For example:
U: I am interested in investing in some securities.
Safety is very important to me, so I would
probably like to get something from the govern-
ment. I am a bit concerned about the penalties for
early withdrawal. What is the penalty on a T-Bill?
R: T-Bills don&apos;t have a penalty. Were you thinking
of T-Bonds?
This response may have been prompted by R thinking
that a confusion between T-Bills and T-Bonds was the
source of the misconception. The two different re-
sponses to the same misconception suggest that the user
model should be influenced by previous discourse. I will
show how part of this influence can be achieved by a
highlighting due to the perspective being taken on the
domain.
Currently, when a user model containing what the
system takes to be the user&apos;s model for the domain is
accessed, all user knowledge has equal importance.
When people engage in a conversation, however, cer-
tain aspects of their domain model become more impor-
tant than others. This importance is more than just a
highlighting of those things that have been explicitly
mentioned. Rather, certain things that are somehow
related to those things explicitly mentioned in previous
discourse are also highlighted. In fact, an orientation on
the domain is usually established.
In section 8 I will introduce a notion of object
perspective that will enable this highlighting effect of
previous discourse to be incorporated into the user
model. Thus when the user model is accessed, certain
things in it will be highlighted while other things will be
suppressed. I will show how this variable highlighting of
the user model can explain why the response to a
particular misconception by a particular user may vary.
</bodyText>
<sectionHeader confidence="0.994182" genericHeader="related work">
2 RELATED MISCONCEPTION WORK
</sectionHeader>
<bodyText confidence="0.989898428571429">
The method of correcting misconceptions outlined
above should be contrasted with the way that miscon-
ceptions are handled by Al systems today. For the most
part misconceptions have been left to the Intelligent
Computer Aided Instruction systems, which basically
use an a priori listing of misconception-response pairs
(see, e.g., Brown and Burton 1978; Stevens, Collins,
and Goldin 1979; Stevens and Collins 1980; Woolf and
McDonald 1983; Woolf 1984). The major problem with
these systems is due to their inability to reason about
the misconception itself, they are completely at a loss
when faced with a misconception absent from their a
priori listing.
The work of Sleeman (1982) on inferring defective
algebra rules (mal-rules) is based on the observation
that the a priori listing of misconceptions is a difficult, if
not impossible, task. Sleeman proposes on-line infer-
ence of mal-rules based on the answer the student has
given to a particular problem. Although Sleeman&apos;s work
is a major improvement over the a priori listing ap-
proach, it still has several problems. First, there is no
measure of how reasonable or likely a particular mal-
rule is. In addition, once a mal-rule has been inferred,
no indication is given concerning how the misconcep-
tion should be corrected.
The work of Kaplan (1979) and Mays (1980) is closer
to the work described here in that they were concerned
with handling and reasoning about whole classes of
misconceptions, thereby giving the system the ability to
handle a potentially infinite number of misconceptions.
Kaplan and Mays were concerned with responding to
Computational Linguistics, Volume 14, Number 3, September 1988 53
Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions
certain types of misconceptions in the context of a
natural language interface to a database system. They
worked on detecting and correcting such misconcep-
tions based on domain independent linguistic cues from
the user and an enhanced model of the domain. For
instance, the query &amp;quot;Which faculty take courses?&amp;quot;
indicates a presumption failure. A truthful response of
&amp;quot;none&amp;quot; to this query would confirm the user&apos;s errone-
ous belief that faculty can take courses. Mays suggests
correcting a query like the one above by 1. denying that
the &amp;quot;take&amp;quot; relation can hold between faculty and
courses, and 2. describing all correct alternatives which
can be reached by abstracting on each of the objects and
the relation involved. This method would produce the
following kind of response:
R: I don&apos;t believe that faculty can take courses.
Faculty teach courses. Students take courses.
Although responses such as this would probably be
helpful to the user, they have the potential (given a
complicated domain) for being overly verbose and con-
taining information that the user does not care about.
One goal of this work is to provide a more pointed and
natural response to these same kinds of errors.
</bodyText>
<sectionHeader confidence="0.987628" genericHeader="method">
3 KNOWLEDGE AVAILABLE
</sectionHeader>
<bodyText confidence="0.9999665625">
The work being done here is in the context of a natural
language interface to a database or expert system. It is
an attempt to define a module of an interface that could
generate a cooperative response to a misconception—
the kind of response that would be generated by a
helpful human conversational partner. In this work, a
misconception is defined to be some discrepancy be-
tween system beliefs and user beliefs (as exhibited
through the conversation). Upon encountering such a
misconception, the assumption is that the system
knowledge is correct, and therefore the job of the
misconception corrector module is to attempt to bring
the user&apos;s knowledge into line with the system&apos;s knowl-
edge.
The scope of this work is limited by several assump-
tions about the kind of knowledge available.
</bodyText>
<listItem confidence="0.695625642857143">
• The system&apos;s model of the world contains an object
taxonomy with attribute/value pairs attached to the
objects.
• The system has available to it a user model that
includes the user&apos;s beliefs about the world.&apos; This is
what the system takes to be the user&apos;s model of the
domain. Although the content of the user model and
the system&apos;s model of the world may differ greatly,
the user model is in the same form as the system&apos;s
model of the world. Thus while both of these models
contain an object taxonomy with attribute/value pairs
attached to the objects, the set of objects in the
taxonomies and the way these objects are classified,
as well as the particular attribute/value pairs associ-
</listItem>
<bodyText confidence="0.993548586206897">
ated with an object, may vary. This model of the user
may be updated as the conversation progresses.
• The system has available to it certain pieces of
contextual and discourse information that serve to
highlight the user model. This highlighting (explored
below) is gained from a new notion of object perspec-
tive and from a record of items and attributes which
have been explicitly focused on in the discourse.
Given the kind of information assumed in the system&apos;s
and user&apos;s models of the world, there are two kinds of
misconceptions that may occur: misclassifications (a
user may classify an object wrong) and misattributions
(a user may give an object an attribute/value pair it does
not have). For each of these kinds of misconceptions, a
small number of response strategies2 were found in the
transcript study. These were abstracted into response
schemas. In section 4, examples of the response strat-
egies found for misclassifications will be examined. For
each strategy an abstract specification of the content
will be given. Next we will look at what beliefs about
the user might have prompted the use of each strategy,
and characterize these beliefs in terms of the structure
of a highlighted user model. With this pairing of user
model structures and response schemas, a misconcep-
tion can be responded to by looking in the highlighted
user model for one of the user model structures and
instantiating the associated schema. Section 5 examines
response strategies for misattributions. The sections
following that will concentrate on the highlighting from
object perspective. A new notion of object perspective
will be defined and it will be shown how object perspec-
tive aids in generating context sensitive responses to
misconceptions.
This paper is concerned with reasoning on the user
model to decide how to respond to a misconception. It
is not concerned with inferring the user model; it is
assumed that the user model is already available. The
emphasis in this work is on using the user model, in a
domain-independent fashion, to respond to a miscon-
ception in a manner similar to a human conversational
partner&apos;s response.
The methods described here have been implemented
in the ROMPER system (Responding to Object-related
Misconceptions using PERspective). The system takes
as input a specification of the information that is incon-
sistent with the system&apos;s model of the world, the current
perspective (described below), and a record of past
focus. It produces a formal specification of the re-
sponse. This response specification is passed into the
Mumble system (McDonald 1980), which, using a gram-
mar and dictionary written by Robin Karlin (1985),
produces an actual English response.
The implemented system works on the financial
securities domain. In order to show the generality of
this approach, two different domains will be used in this
paper. The motivation for the system&apos;s method of
choosing a response strategy will use examples from the
domain containing whales and fish. In the last sections
</bodyText>
<page confidence="0.981781">
54 Computational Linguistics, Volume 14, Number 3, September 1988
</page>
<note confidence="0.40895">
Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions
</note>
<bodyText confidence="0.99938225">
of the paper (those pertaining to the new notion of
object perspective), the examples will be taken from the
domain of financial securities, since the ideas can be
better motivated with a more complex domain.
</bodyText>
<sectionHeader confidence="0.999156" genericHeader="method">
4 MISCLASSIFICATIONS
</sectionHeader>
<bodyText confidence="0.994389">
One kind of misconception concentrated on in the
transcript study was that involving the classification of
an object. From that study three major correction
strategies, which could be supported with the kind of
knowledge available in our knowledge base, were ab-
stracted. These are used to illustrate the type of analysis
advocated; no claim is being made about the complete-
ness of this set. The first strategy is exemplified by the
following dialog.
U: I thought whales were fish.
R: No, they are mammals. You may have thought
they were fish because they are fin-bearing and
live in the water. However, they are mammals
since, (while fish have gills) whales breathe
through lungs and feed their young with milk.
Let us first examine the content of this response. It can
be seen as consisting of three parts. In the first sentence
we have the denial of the incorrect information and the
statement of the corresponding correct information.
The remaining sentences comprise the justification for
the denial and correction given.
The content of this response can be abstracted into
the following:
</bodyText>
<equation confidence="0.994588333333333">
MISCONCEPTION = X is-a Y
RESPONSE =
I. X is-NOT-a Y
</equation>
<listItem confidence="0.8584854">
2. X is-a Type-of(X)
3. X is like Y because both share attributes-of(X) fl
attributes-of(Y)
4. BUT X has attributes-of(X) — attributes-of(Y)
5. WHILE Y has attributes-of(Y) — attributes-of(X)
</listItem>
<bodyText confidence="0.999150388888889">
The justification is contained in (3-5) of the rule. Notice
that it can be seen as a concede/override pair. First,
information is conceded that actually supports the mis-
conception. This consists of attributes that whales and
fish have in common. These attributes are a potential
source of the misconception since an object that shares
many attributes with a superordinate is likely to belong
to that superordinate class. One way to view this
conceding of intersecting attributes is as R acknowledg-
ing the apparent support for the misconception lest U
think that that part of his/her domain model is wrong.
The response goes on to override the conceded
information by offering attributes that differentiate
whales and fish. These are attributes that whales have
but fish cannot, and attributes that fish have that whales
do not.
By analyzing the justification in this way, not only do
we get an abstract representation that can be used by a
generation system, but we get some insight into when it
would be reasonable to use such a response. It makes
sense to use a response that first concedes a similarity
between the misclassified object and the object that was
wrongly given as its superordinate (from now called the
posited superordinate) and then overrides that con-
ceded information, when such a similarity is a possible
source of the misconception. This might be the case
when the user model shows that the user believes that
such a similarity exists. Thus a generation system may
generate its response by instantiating the above rule if
the user has misclassified the object X as a Y, and an
analysis of the user model shows that the user believes
X is similar to Y. Because of this reason for using this
response strategy, it has been termed the like super
response strategy.
A second kind of strategy is exemplified by the
following dialog. Imagine a recent conversation cen-
tered on sharks being large fish that people are typically
afraid of. Following that, the conversation turns to
classifying whales. The user says:
U: I thought whales were fish, too.
R: No, they are mammals. You may have thought
they were fish since they are like the fish, sharks,
in that both are large aquatic creatures and both
scare people. However, whales are mammals
since, (while fish have gills) whales breathe
through lungs and feed their young with milk.
Again, this response can be seen as consisting of a
denial, a correction, and a justification. The denial and
correction are contained in the first sentence. As in the
previous example, the remaining sentences comprise
the justification in the form of a concede/override pair.
The conceded information, while dependent on at-
tributes, differs from that given above. The abstract
content rule can perhaps best illustrate this difference:
</bodyText>
<equation confidence="0.9939675">
MISCONCEPTION = X is-a Y
RESPONSE =
</equation>
<listItem confidence="0.8844635">
1. X is-NOT-a Y
2. X is-a Type-of(X)
3. 3Z (Z is-a Y A X is like Z because both have
attributes-of(X) fl attributes-of(Z))
4. HOWEVER X is-a Type-of(X) because X has at-
tributes-of(Type-of(X)) — attributes-of(Y)
</listItem>
<bodyText confidence="0.99361835">
The major difference between this response and the like
super response is that instead of conceding a similarity
between the misclassified object and the posited super-
ordinate itself, a similarity between the misclassified
object and a descendant of the posited superordinate is
conceded. This response is reasonable if the user came
to the misconception by reasoning: &amp;quot;I don&apos;t know how
to classify whales, but I do know how to classify sharks.
Computational Linguistics, Volume 14, Number 3, September 1988 55
Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions
Sharks are fish and whales are very similar to sharks.
Therefore it is reasonable to believe that they are
classified the same.&amp;quot; Evidence for this kind of reason-
ing might be a similarity found in the user model
between the misclassified object and some descendant
of the posited superordinate. Finding such a similarity
would justify the use of this like-some-super strategy.
A final strategy found for misclassifications is exem-
plified by the following:
U: I thought whales were fish.
R: No, they are mammals.
This is the degenerate no support strategy, which con-
tains no justification for the correction. At first glance
the use of this strategy seems rather puzzling. Since I
have assumed that the misconception is important, one
would expect a full response to be given. However, no
justification was included in the response.
This last response is an example of a response in a
situation where no evidence was found in the high-
lighted user model for any of the preidentified kinds of
faulty reasoning. If R could not figure out what led to
the misconception, then R could not include justifica-
tion to refute the support! The most that R could do in
this situation was deny the incorrect information and
state the corresponding correct information. This is
exactly what was done in the response above. For this
reason the response has been termed the no support
response, and it can be used when the user model
contains none of the evidence that would key off any of
the other correction strategies.
</bodyText>
<sectionHeader confidence="0.999058" genericHeader="method">
5 MISATTRIBUTIONS
</sectionHeader>
<bodyText confidence="0.999321653846154">
Misattributions are the second kind of misconceptions
that may arise given the kind of knowledge assumed. A
misattribution occurs when the user gives an object an
attribute/value that the object does not have in the
system&apos;s domain model.
The transcript study revealed that a misattribution
response could also be broken into three parts. In this
case, however, the justification took the form of offer-
ing another statement that would change what the user
said in a small way in order to make it a correct
statement. This is illustrated in the following dialog,
which is an example of the first kind of correction
strategy found. Imagine that we are discussing sea
animals and have previously discussed sharks and their
attributes. We move to whales.
U: I thought whales had gills.
R: No, they breathe through lungs. Were you think-
ing of sharks?
The response might go on to discuss the similarities and
differences between whales and sharks.
Notice what was done in this response. We have the
denial and the correction as before. This time the
justification offers another object that is similar to the
object involved in the misconception but for which the
misattributed attribute/value pair is true. This content is
captured in the following rule:
</bodyText>
<listItem confidence="0.9368345">
MISCONCEPTION = X has attribute Y with value V
then RESPONSE =
1. X NOT have attribute Y with value V
2. X has attribute Y with value W
3. 3 Z (Z has attribute Y with value V), did you mean
Z?
</listItem>
<bodyText confidence="0.999379424242424">
This kind of strategy would be effective if the user&apos;s
misconception results from either confusing the object
being discussed with some other object with the misat-
tributed property, or through an incorrect analogy with
such an object. Mention of the object in the case of a
confusion prevents continuation of the confusion. In the
case of an incorrect analogy, mention of the other object
alerts the user to the inappropriateness of analogy and
also prevents the continued usage of the analogy. No-
tice that in either case, a failure to mention the second
object will probably bring protest on the part of the
user. For instance, suppose that the preceding miscon-
ception was arrived at because the user was confusing
whales and sharks. If the expert just responds, &amp;quot;No,
they breathe through lungs&amp;quot;, it is likely that the user
will counter, &amp;quot;But you just told us that they had gills!&amp;quot;
By mentioning the confused object, the user becomes
aware of the confusion.
It makes sense to use this wrong object strategy
when a possible source of the misconception is either an
object confusion or the usage of a bad analogy. Evi-
dence for this would be the existence of some similar
object from the highlighted user model that has the
misattributed property.
A second kind of correction strategy used for misat-
tributions is shown here:
U: I thought whales had gills.
R: No, whales breathe through lungs. Did you mean
you thought they had fins?
This is very similar to the last response except, instead
of offering another object, the strategy offers a similar
attribute which makes the user&apos;s statement correct. The
content rule for this response follows:
</bodyText>
<listItem confidence="0.880585">
MISCONCEPTION = X has attribute Y with value V
then RESPONSE =
I. X does not have attribute Y with value V
2. X has attribute Y with value W
3. 3Q3U(X has attribute Q with value U A simi-
lar(Y,Q), did you mean X has attribute Q with value
U?)
</listItem>
<bodyText confidence="0.957845">
This strategy, termed the wrong attribute strategy, is
used when there is a similarity of attributes within the
</bodyText>
<page confidence="0.91193">
56 Computational Linguistics, Volume 14, Number 3, September 1988
</page>
<note confidence="0.648891">
Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions
</note>
<bodyText confidence="0.9962245">
highlighted user model. (The no support strategy also
occurs in the case of misattributions.)
</bodyText>
<sectionHeader confidence="0.995667" genericHeader="method">
6 USER MODEL ANALYSIS
</sectionHeader>
<bodyText confidence="0.9988477">
Given the association of user model configurations with
response strategies given in the previous sections, we
can come up with a method for deciding how to respond
based on the type of misconception along with an
analysis of the highlighted user model. Basically the
user model analysis looks for one of the preidentified
configurations and, if one is found, suggests instantiat-
ing the associated strategy. The following rule captures
the way that the ROMPER system implements what has
been discussed so far.
</bodyText>
<figure confidence="0.357047923076923">
IF misconception = &amp;quot;X is-a Y&amp;quot;
THEN
IF similar(X,Y)
THEN instantiate like super schema
ELSEIF 3Z (Z is-a Y) A similar(X,Z)
THEN instantiate like-some-super schema
ELSE instantiate no support schema
ELSEIF misconception = &amp;quot;X has attribute Y with
value V&amp;quot;
THEN
IF 3Z ((Z has attribute Y with value V) A
similar(X,Z))
THEN instantiate wrong object schema
</figure>
<bodyText confidence="0.989154">
ELSEIF 3Q3U ((X has attribute Q with value U) A
similar(Y,Q))
THEN instantiate wrong attribute schema
ELSE instantiate no support schema
Notice that each of the tests for instantiating a schema
hinges on the similarity assessment of two objects.
These assessments must be context dependent. Be-
cause of this, it is crucial that the user model analysis be
done on a user model highlighted by previous discourse
and that the similarity metric take advantage of this
highlighting. The highlighting and similarity metric used
by ROMPER will be discussed below.
This method for correcting misconceptions suggests
a model of natural language generation that is similar to
that put forth by McKeown (1982) but which differs
from McKeown&apos;s model in several ways.
Both McKeown and this work concentrate on deter-
mining the content and textual shape of a response.
McKeown is concerned with responses to questions
about the structure of a data base. Upon encountering
such a question McKeown first delimits a relevant
knowledge pool using fairly simple mechanisms. This
relevant knowledge pool contains that information from
the knowledge base that could possibly be included in
the response; the actual generated response need not
exhaust this pool. Next, based on the goal of the
discourse (as determined by the question type) and a
characterization of the relevant knowledge pool (again,
a simple test) a response schema is chosen for the
response. The response schema dictates the textual
structure of the response. This schema is filled by
stepping through it and matching its predicates against
the relevant knowledge pool. A focusing mechanism is
used to mediate between choices arising during this
process.
The schemas used by the ROMPER system are more
complicated than those advocated by McKeown. In
effect, ROMPER&apos;s schemas are responsible both for
determining the textual shape of a response and for
determining what information from the knowledge base
to include in the response. Thus they are applicable to a
much more restricted generation problem (e.g., re-
sponding to a misconception of a particular type).
Because of this, the test for determining which schema
to use can be much more specific than the tests em-
ployed by McKeown.
</bodyText>
<sectionHeader confidence="0.992182" genericHeader="method">
7 HIGHLIGHTING AND OBJECT SIMILARITY
</sectionHeader>
<bodyText confidence="0.990411120689655">
We claim that in order for the above strategy for
correcting misconceptions to work, the similarity metric
that is used to assess object similarity must be affected
by the preceding discourse.
To date, most Al systems do not assess object
similarity in a way that is context dependent. Several
systems that do assess object similarity (Rumelhart and
Abrahamson 1973, McKeown 1982, Carberry 1984,
Weiner 1984) use a metric based on distance in some
space. Most often, this space is the generalization
hierarchy. Basically, two objects that have a common
immediate superordinate (i.e., are siblings in the hier-
archy) are seen as very similar, while objects whose
lowest common ancestor is several levels up in the
hierarchy are seen as quite different.
One problem with this metric arises when objects can
be classified in more than one way and there are several
lowest common ancestors of the objects being com-
pared. A decision must be made about which of these
lowest common ancestors should be considered since
the similarity assessment of the objects might vary
widely as a result. For instance, a treasury bond and a
corporate bond may be assessed as being very similar
since they have a common immediate superordinate of
bonds. On the other hand, both of these objects can be
classified along other dimensions. If we look at a
treasury bond as being a treasury issue, which is a type
of US Government security, it seems quite different
from a corporate bond, which is a corporate security.
A second major problem with a similarity metric
based on distance in the generalization hierarchy is that
it is context invariant; contextual information has no
way of affecting the assessments. As shown by Tversky
(1977) and others, human judgments of object similarity
have been found to shift both when the set of objects
under discussion are altered (e.g., a violin and an
electric guitar may be judged quite similar when in a
group with a clarinet and an oboe, and may be judged
quite different when the other members of the group are
Computational Linguistics, Volume 14, Number 3, September 1988 57
Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions
a cello and an electric bass), and when the salience of
attributes are altered (e.g., in a group containing a red
triangle, a blue triangle, and a red square, the red
triangle might be judged similar to the blue triangle
when attribute shape is stressed, but may be judged
similar to the red square when attribute color is
stressed).
One metric that avoids these problems was intro-
duced by Tversky (1977). Tversky&apos;s metric, rather than
relying on distance in some space, is based on the
common and disjoint features of the objects involved.
The metric, termed a contrast model, allows context to
be taken into account in several places.
Suppose we have two objects a and b where A is the
set of properties associated with object a and B is the
set of properties associated with object b. Tversky&apos;s
measure can be expressed as:
</bodyText>
<equation confidence="0.405534">
s(a,b)= Of(A n B) — af(A — B) — pf(B — A)
</equation>
<bodyText confidence="0.99878075">
for some 0, a, and 0.
In the above equation 0, a, and 13 are parameters
which represent the importance of each piece of the
equation. The function f maps over the features and
yields a salience rating for each. In essence, the contrast
model states that the similarity of two objects is some
function of their common features minus some function
of their disjoint features. The importance of each par-
ticular feature involved (determined by the function f)
and the importance of each piece of the equation
(determined by 0, a, and 13) may change with context.
Although Tversky discusses in general terms how
these functions might be set, he gives no concrete
methods for doing so. For instance to set 0, a, and p he
turns to the relative prominence of objects a and b in the
discourse. The more prominent an object is, the more
its attributes should have an impact on the similarity
rating. Thus, finding the relative prominence of objects
a and b in the discourse would help set these values. If
a is relatively more important, then functions 0 and a
should be greater than /3 resulting in the attributes of the
more prominent object having a greater influence over
the similarity assessment. While I would conjecture that
information about the focus of the discourse (Grosz
1981, Sidner 1983, Grosz, Joshi, and Weinstein 1983)
might give an indication of an object&apos;s prominence and
would therefore be useful in setting the values of 0, a,
and p, in this work I have assumed a value of 1 for the
0, a, and 13 and have concentrated on setting the f
function.
We turn, then, to the problem of finding a value for
the f function: the measure of salience for each property
of the objects involved. Other work, such as Carbonnell
and Collins (1970) and Weiner (1984), has hand-encoded
salience values for attributes of individual objects di-
rectly into the knowledge base, permanently setting the
f function. This approach is not sufficient for setting the
f function for Tversky&apos;s metric, since it is crucial that
the f function be able to change with context. In order to
make this happen, the salience values computed by f
must change with context.
To see this, consider our ability to explicitly mention
an attribute to increase its salience. In the example
earlier with the red and blue triangles and the red
square, if the request for a similarity judgment had been
preceded by &amp;quot;Look at the pretty colors of these
objects&amp;quot;, the red triangle and red square would have
probably been judged to be more similar than the red
triangle and the blue triangle. Thus we see that explicit
mention of an attribute in a discourse is one way that the
f function might be affected by previous discourse.
Explicit mention of an attribute is only one way in
which the salience of the attributes may change dynam-
ically. Another aspect of dynamic salience comes from
the point of view, or perspective, applied to the domain.
For instance, a building can be referred to as being an
architectural work, for example, or as being someone&apos;s
home. The two different views of the building cause
different sets of attributes to become salient. Notice
that this set of attributes is in addition to the attributes
that have been explicitly mentioned in the discourse.
From an architectural work point of view, attributes like
the architect&apos;s name, date of building, and particular
architectural features become salient. On the other
hand, from the home point of view, attributes like the
kitchen size, number of bedrooms, and living space
become important. If we can find a way of modeling
how these &amp;quot;precompiled&amp;quot; sets of attributes become
highlighted in a discourse, we will have a principled
method for setting the f function needed for Tversky&apos;s
similarity metric. The next section discusses how this
highlighting can be modeled by a computer system.
</bodyText>
<sectionHeader confidence="0.994232" genericHeader="method">
8 OBJECT PERSPECTIVE
</sectionHeader>
<bodyText confidence="0.999816181818182">
The notion of point of view or object perspective has
been noted by other researchers in artificial intelligence.
Perspective&apos;s ability to explain the changing attribute
salience has been attributed to a limited inheritance
mechanism (see, e.g., Grosz 1977; Bobrow and Wino-
grad 1977; Tou et al. 1982). An object viewed from a
particular perspective is seen as having one particular
superordinate, although in fact it may have many. The
object inherits properties only from the superordinate in
perspective. Therefore different perspectives on the
same object cause different properties to be inherited
(and therefore highlighted).
While explaining object perspective via a limited
inheritance mechanism is intuitively appealing, it is
unable to handle some effects which intuitively should
be handled by object perspective. The first has to do
with the availability of object attributes. A limited
inheritance mechanism makes attributes inherited from
superordinates other than the one in perspective un-
available. This seems a bit too strong. When we discuss
a building as an architectural work, I may comment on
the number of bedrooms the building has. While you
</bodyText>
<page confidence="0.977281">
58 Computational Linguistics, Volume 14, Number 3, September 1988
</page>
<note confidence="0.606445">
Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions
</note>
<bodyText confidence="0.999918571428572">
may think my comment irrelevant to the current con-
versation, you would still be able to understand it and
even evaluate its truth or falsity. In the limited inheri-
tance account of object perspective, however, this
would not be possible. As far as the system would be
concerned the number of bedrooms would not even be
an attribute to the building.
A second problem with the limited inheritance ac-
count of object perspective has to do with deciding what
attributes the superordinate in perspective has. The
superordinate itself may have multiple classifications
and thus potentially multiple perspectives. Thus, in
order to figure out what attributes a particular concept
should inherit, we must figure out not only what per-
spective it is being viewed from, but also what perspec-
tive the perspective superordinate is being viewed from,
and so on. But this seems to be much more work than is
necessary.
Another problem is that a limited inheritance mech-
anism explains the perspective for a single object only.
However, during the course of a conversation it is
usually the case that more than one object will be
discussed. When this happens, usually the same kinds
of things are discussed about the objects. In essence, a
particular highlighting of attributes (or point of view)
seems to be in force during the conversation. Yet, this
highlighting is applied to different objects—some of
which may not even have the same superordinates.
What seems to be happening is that the conversational
partners are viewing an entire group of objects from the
same perspective. A limited inheritance mechanism
cannot account for this unless each of the objects under
discussion can be said to have the same (immediate)
superordinate.
A final effect that is not accounted for by the limited
inheritance mechanism, yet seems to hinge on the view
being taken on the domain, has to do with the height-
ened importance of some objects during a discourse.
Like the importance of attributes, the relative impor-
tance of some objects in the discourse cannot solely be
accounted for by explicit mention. Some objects are
more likely to be mentioned and discussed in a dis-
course than others. For instance, when discussing a
particular building as an architectural work, I might
reasonably mention the library down the street that was
designed by the same architect. On the other hand, I
will probably not mention my apartment. Along the
same lines, in discussing that building as a home, my
apartment is a likely candidate for mention in the
conversation. Although this effect seems to be in some
way tied to the notion of object perspective, the limited
inheritance mechanism does not address this issue.
We want to retain the dynamic highlighting of &amp;quot;pre-
compiled&amp;quot; groups of attributes. Instead of the limited
inheritance mechanism, we propose that the following
account be used:
I. Instead of tying perspective into the generaliza-
tion hierarchy of objects as has been done in the
past, the new notion of perspective is independent
of that hierarchy. Perspectives that can be taken
on the objects in the domain will be defined and
will sit in a structure that is orthogonal to the
generalization hierarchy.
</bodyText>
<listItem confidence="0.9993554">
2. A number of perspectives are available for any
domain of discourse and any given domain object
may be viewed from any one of several perspec-
tives for that domain.
3. Each perspective comprises a set of attributes
with associated salience values. It is these sa-
lience values that dictate which attributes are
highlighted and which are suppressed.
4. One such perspective is designated active at any
particular point in the discourse.3
</listItem>
<bodyText confidence="0.999863321428571">
Our solution is that any object that is accessed by the
system is viewed through the current active perspec-
tive. However, instead of dictating which attributes an
object inherits, the active perspective affects the sa-
lience values of the attributes that an object possesses
(either directly or inherited through the generalization
hierarchy). The active perspective essentially acts as a
filter on an object&apos;s attributes. By raising the salience of
the attributes, it highlights those attributes which have a
high salience rating in the active perspective. By low-
ering the salience of the attributes, it suppresses those
attributes that are either given a low salience value or do
not appear in the active perspective.
By defining object perspective in this way, we have
retained the desirable results of the limited inheritance
account of object perspective while avoiding its prob-
lems. In addition, since any object accessed by the
system is viewed through the active perspective, we
gain the feeling of perspective on the entire domain. The
object importance aspect of perspective is gotten by
saying that those objects that contribute attributes
which are highly salient to a perspective are important
while that perspective is active.
We propose that the ffunction in Tversky&apos;s metric be
set by taking into account the salience values derived
from the active perspective. This would yield an f
function that is context dependent and would help the
similarity metric exhibit many desirable properties.
</bodyText>
<sectionHeader confidence="0.892752" genericHeader="method">
9 MODELING A DOMAIN WITH PERSPECTIVES
</sectionHeader>
<bodyText confidence="0.9997752">
In some natural language systems, a model of a partic-
ular domain includes a usual object taxonomy contain-
ing all of the objects in the domain and all of the
attributes associated with those objects. We will show
an example of building a domain model with perspec-
tives. In order to do this, one must build the domain
model as usual. In addition, the perspectives that can be
taken on the domain objects must be defined. The result
of viewing the domain model through the perspectives
will be shown.
</bodyText>
<table confidence="0.974489272727273">
Computational Linguistics, Volume 14, Number 3, September 1988 59
Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions
Money Market Certificates
Maturity: 3 months
Denominations: $1,000
Issuer: Commercial Bank
Penalty for Early Withdrawal: 10%
Purchase Place: Commercial Bank
Safety: Medium
Treasury Bills
Maturity: 3 months
Denominations: $1,000
Issuer: US Government
Purchase Place: Federal Reserve
Safety: High
Treasury Bond
Maturity: 7 years
Denominations: $500
Issuer: US Government
Penalty for Early Withdrawal: 20%
Purchase Place: Federal Reserve
Safety: High
</table>
<figureCaption confidence="0.997692">
Figure 1. Objects in &amp;quot;Flat&amp;quot; Domain Model.
</figureCaption>
<bodyText confidence="0.999827176470588">
Figure I shows a small piece of a typical domain
model. The domain is that of financial securities. Three
of the objects from this domain, Money Market Certif-
icates, Treasury Bills, and Treasury Bonds, are shown
with the attributes they possess. In systems as they are
defined today, a group of objects defined in this way and
arranged in a generalization hierarchy would constitute
the domain model. If a system were to access any one of
the objects in the domain it would be given the object
with the attributes as listed in the figure.
In order to get a dynamic highlighting of the domain
model, we must build, in addition to the object taxon-
omy, a separate structure containing the perspectives
that can be taken on the domain objects. This means
that we must think about the different points of view
that can be taken on the objects in the domain and
compile sets of attributes from our model which capture
the important domain concepts in that point of view.
There will be attributes in each perspective that do not
occur with all of the objects in the domain. At the same
time, there will be attributes of individual objects that
do not appear in a particular perspective. The perspec-
tive simply captures the attributes that are important in
a particular point of view.
Figure 2 contains two perspectives that might be
reasonable for this domain (here we are assuming
salience values from low salience of 0 to high salience of
1). The perspective of Savings Instruments highlights
maturity and denominations, and somewhat highlights
safety and yield. This indicates that when people are
discussing securities as savings instruments, they are
most interested in how long their money will be tied up
and in what denominations they can save their money.
The perspective of Issuing Company, on the other hand,
</bodyText>
<figure confidence="0.990625272727273">
Savings Instruments
Maturity-1.0
denominations-1.0
safety-0.5
yield-0.5
Issuing Company
issuer-1.0
safety-1.0
purchase-place--0.5
yield-0.5
tax-0.5
</figure>
<figureCaption confidence="0.999989">
Figure 2. Sample Perspectives.
</figureCaption>
<bodyText confidence="0.999852115384616">
highlights different attributes. When securities are dis-
cussed from this perspective, things like the name of the
company and the stability of an investment in the
company become important. Other attributes of the
securities are ignored (recall that attributes not men-
tioned in the perspective get assigned a low salience
rating).
Notice how the objects look when accessed, depend-
ing on which of the two different perspectives are
active. For instance, through the savings instruments
perspective the objects&apos; attributes take on the salience
values shown in Figure 3. Attributes are not shown in
the figure that have 0 salience. When no static salience
is included in the domain model, the attributes of the
objects derive their salience directly from values given
in the active perspective. Attributes in the perspective
that the objects do not have (e.g., yield) are ignored.
Attributes of the objects not occurring in the perspec-
tive are given the lowest salience rating. The very same
objects look different when viewed through the other
perspective. This is shown in Figure 4.
The salience values derived from perspective can be
used for many tasks. Section 11 will show how they can
be used in conjunction with Tversky&apos;s similarity metric
to generate context sensitive responses to misconcep-
tions.
</bodyText>
<sectionHeader confidence="0.995102" genericHeader="method">
10 CHOOSING THE ACTIVE PERSPECTIVE
</sectionHeader>
<bodyText confidence="0.999970642857143">
In order for the notion of object perspective to be truly
beneficial, there must be a mechanism for choosing the
active perspective based on previous discourse. While
this topic is still very much open to investigation, some
preliminary research has revealed several factors that
might influence the choice of active perspective.
Perhaps one of the most influential pieces of infor-
mation useful in choosing a perspective is the user&apos;s
current goal. In (McKeown, Wish, and Matthews 1985)
the user&apos;s goal completely determines which perspec-
tive is active. In their work each perspective that can be
taken on the domain objects is indexed by potential
goals. Thus once the system has determined what the
user&apos;s goal probably is, it has also determined what
</bodyText>
<page confidence="0.93232">
60 Computational Linguistics, Volume 14, Number 3, September 1988
</page>
<table confidence="0.748321153846154">
Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions
Money Market Certificartes
Maturity: 3 months-1
Denominations: $1,000-1
Safety: Medium-0.5
Treasury Bills
Maturity: 3 months-1
Denominations: $1,000-1
Safety: High-0.5
Treasury Bond
Maturity: 7 years-1
Denominations: $500-1
Safety: High-0.5
</table>
<figureCaption confidence="0.9919245">
Figure 3. Objects Through Savings Instruments
Perspective.
</figureCaption>
<bodyText confidence="0.999775608695652">
perspective the user has probably taken on the domain
objects.
While the user&apos;s goal is a good source of information
to use to determine the probable perspective, other
factors may also influence this choice. These include
the attributes and objects mentioned so far in the dialog.
The mentioned attributes are obviously thought to be
important and one would therefore expect them to be
given a fairly high salience rating in the active perspec-
tive. Thus the choice of active perspective can be
narrowed down to those in which the mentioned at-
tributes appear with high salience.
By the same token, the objects mentioned so far in
the dialog can also give a clue concerning the active
perspective. One would expect that the active perspec-
tive would deem these objects important. Therefore the
system might look for perspectives that give high sa-
lience ratings to many of the attributes associated with
objects that have been mentioned in the discourse.
In this section I have identified several factors that
influence the choice of active perspective. While the
success of other systems (McKeown, Wish, and Mat-
thews 1985) has shown that a reasonable choice of
</bodyText>
<table confidence="0.895786">
Monrey Market Certificates
Issuer: Commercial Bank-1
Purchase Place: Commercial Bank-0.5
Safety: Medium-1
Treasury Bills
Issuer: US Government-1
Purchase Place: Federal Reserve-0.5
Safety: High-1
Treasury Bond
Issuer: US Government-1
Purchase Place: Federal Reserve-0.5
Safety: High-1
</table>
<figureCaption confidence="0.995739">
Figure 4. Objects Through Issuing Company Perspective.
</figureCaption>
<bodyText confidence="0.999891090909091">
perspective can be made based on discourse goals, the
nature of establishing and perhaps &amp;quot;shifting&amp;quot; perspec-
tive during a discourse must still be investigated. Still
unanswered are questions such as: When does a per-
spective change? How long is a perspective active? Is
there a relationship between a discourse unit (Grosz and
Sidner 1985) and perspective? Is there any structure to
the space of perspectives that would put constraints on
moving from one active perspective to another? These
questions must be taken up in future research on
perspective.
</bodyText>
<sectionHeader confidence="0.976185" genericHeader="method">
11 PERSPECTIVE&apos;S INFLUENCE ON RESPONSES
</sectionHeader>
<bodyText confidence="0.985392786516854">
In this section we will show how using the active
perspective to highlight the user model, the misconcep-
tion correcting algorithm from Section 6, and the
Tversky similarity metric can account for context sen-
sitive corrections to misconceptions. Recall that in
correcting a misattribution one of the correction sche-
mas used by ROMPER called for a similar object to be
offered as a possible object of confusion. A study of
transcripts reveals, however, that this schema may be
instantiated in different ways depending on the context.
Consider once again the following dialogs that were first
seen in the introduction.
U: I am interested in investing in some securities to
use as savings instruments. I want something
short-term and I don&apos;t have a lot of money to
invest so the instrument must have small denom-
inations. I am a bit concerned about the penalties
for early withdrawal. What is the penalty on a
T-Bill?
R: T-Bills don&apos;t have a penalty. Were you thinking
of Money Market Certificates?
In this case money market certificates were seen as
being similar to T-bills and therefore included in the
response. A different object might be used in a different
context. Consider:
U: I am interested in investing in some securities.
Safety is very important to me, so I would
probably like to get something from the govern-
ment. I am a bit concerned about the penalties for
early withdrawal. What is the penalty on a T-Bill?
R: T-Bills don&apos;t have a penalty. Were you thinking
of T-Bonds?
The difference in these two responses can be explained
by different perspectives being taken on the objects.
Suppose we are given the objects, attributes, and per-
spectives from Section 9. The dialog preceding the first
example could lead to the establishment of savings
instruments as the active perspective4 since it mentions
the perspective by name and explicitly mentions several
of the attributes made important by the perspective.
If ROMPER were given this information it would
Computational Linguistics, Volume 14, Number 3, September 1988 61
Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions
proceed by attempting to instantiate the wrong object
schema described in section 5. Recall that this schema is
applicable when there is a similar object that has the
property involved in the misconception. The system
would collect all objects having the attribute in question
and then test their similarity with the object involved in
the misconception. In our knowledge base there are two
objects that have the attribute involved in the miscon-
ception: Money Market Certificates and T-Bonds.
Assume that the f function in the metric is set solely
on the basis of the salience values given by the perspec-
tive. Also assume that we have decided that two objects
are highly similar if Tversky&apos;s metric returns a number
greater than 0, and not similar otherwise. Applying the
Tversky metric using the salience values attached by
the savings instrument perspective (and assuming a
value of 1 for 0, a, and 0) we get:
s(T-Bill, MM-Cert) = f(maturity, denom) — f(safety)
= 2 — .5 = 1.5 = = =&gt; high similarity
s(T-Bill, 1-Bond) = f(safety) — f(maturity, denom)
= .5 — 2 = —1.5 = = = &gt; low similarity
With these calculations the system would choose the
Money Market Certificate as the possible object of
confusion and respond:
R: Treasury Bills don&apos;t have a penalty. Were you
thinking of Money Market Certificates?
Contrast the above calculations with calculations that
might occur given a different active perspective. The
discourse preceding the misconception utterance in the
second example suggests the active perspective of &amp;quot;Is-
suing Company&amp;quot;. Using the salience values attached by
this perspective the similarity metric would produce the
following calculations:
s(T-Bill, MM-Cert)
= f() — f(issuer, safety, purchase)
= 0 — 2.5 = —2.5 = = = &gt; low similarity
s(T-Bill, T-Bond)
= f(issuer, safety, purchase) —f()
= 2.5 — 0 = 2.5 = = =&gt; high similarity
In this case a reasonable response by the system would
be:
R: Treasury Bills don&apos;t have a penalty. Were you
thinking of a Treasury Bond?
As the examples show, changes in the active perspec-
tive can account for the same misconception being
responded to in two different ways.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="conclusions">
12 CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.999414117647059">
This paper has described a new method for responding
to misconceptions that relies on an analysis of a high-
lighted user model to generate a response that is most
likely to benefit the user. A number of strategies were
abstracted from a study of transcripts. Each strategy
was associated with a distinguished structure in the user
model which could explain its use in a given situation. A
system can use this pairing to decide how to respond by
looking in the highlighted user model for evidence of
one of the distinguished structures. The corresponding
strategy can be used to respond.
In addition, the paper has described a new notion of
object perspective that is able to model one aspect of
the dynamic highlighting of the user model due to
previous discourse. It was shown how perspective
could account for certain contextual affects on re-
sponses to misconceptions.
</bodyText>
<sectionHeader confidence="0.997677" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.598591888888889">
I would like to thank Sandee Carberry, Julia Hirschberg, Aravind
Joshi, Martha Pollack, Bonnie Webber, and the participants of the
User Model Workshop for their helpful comments and discussions on
various aspects of this work. Many thanks also to the anonymous
reviewers for their constructive input concerning the style and content
of this paper.
Some of this work was done while the author was at the University
of Pennsylvania and was partially supported by the ARO grant
DAA20-84-K-0061 and the NFS grant MCS81-07290.
</bodyText>
<sectionHeader confidence="0.747812" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.987812776699029">
Bobrow, D.G. and Winograd, T. 1977 An overview of krl, a knowl-
edge representation language. Cognitive Science 1(1): 3-46.
Brown, J.S. and Burton, R.R. 1978 Diagnostic models for procedural
bugs in basic mathematical skills. Cognitive Science 2(2): 155-192,
1978.
Carberry, Sandra M. 1984 Understanding pragmatically ill-formed
input. In 10th International Conference on Computational Lin-
guistics and 22nd Annual Meeting of the Association of Compu-
tational Linguistics, Stanford University, CA: 200-206.
Carbonnell, Jaime R. and Collins, Allan M. 1970 Mixed-Initiative
Systems For Training and Decision-Aid Applications. Technical
Report ESD-TR-70-373, Electronics Systems Division, Laurence
G. Hanscom Field, U.S. Air Force, Bedford, MA.
Grosz, B. 1977 The Representation and Use of Focus in Dialog
Understanding. Technical Report 151, SRI International, Menlo
Park, CA.
Grosz, B.; Joshi, A.K.; and Weinstein S. 1983 Providing a unified
account of definite noun phrases in discourse. In Proceedings of
the 21st Annual Meeting of the Association for Computational
Linguistics, Cambridge, MA: 44-50.
Grosz, B. 1981 Focusing and description in natural language dialog. In
Webber B, Joshi, A, and Sag, I. (ed.), Elements of Discourse
Understanding, Cambridge University Press, Cambridge, En-
gland: 85-105.
Grosz, B. and Sidner, C. 1985 Discourse structure and the proper
treatment of interruptions. In Proceedings of the 1985 Joint
Conference on Artificial Intelligence, IJCAI85, Los Angeles, CA.
Kaplan, S.J. 1979 Cooperative Responses From a Portable Natural
Language Database Query System. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Karlin, Robin 1985 Romper Mumbles. Technical Report, University
of Pennsylvania, Philadelphia, PA.
Malhotra, A. 1975 Design Criteria for a Knowledge-Based English
Language System for Management. Technical Report TR-146,
Project MAC, MIT, Cambridge, MA.
Malhotra, A. and Sheridan, P. 1976 Experimental Determination of
Design Requirements for a Program Explanation System. Tech-
62 Computational Linguistics, Volume 14, Number 3, September 1988
Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions
nical Report RC 5831, IBM Research Center, Yorktown Heights,
NY.
Mays, E. 1980 Correcting misconceptions about database structure.
In Proceedings of the 3rd CSCSI Biennial Meeting, Victoria, BC,
Canada.
McDonald, D.D. 1980 Natural Language Production as a Process of
Decision Making Under Constraint. Ph.D. thesis, MIT, Cam-
bridge, MA.
McKeown, K. 1982 Generating Natural Language Text in Response
to Questions About Database Structure. Ph.D. thesis, University
of Pennsylvania, Philadelphia, PA.
McKeown, K.; Wish, M.; and Matthews, K. 1985 Tailoring explana-
tions for the user. In Proceedings of the 1985 International Joint
Conference on Artificial Intelligence, Los Angeles, CA.
Pollack, M.; Hirschberg, J.; and Webber, B. 1982 User participation
in the reasoning processes of expert systems. In Proceedings of
the 1982 National Conference on Artificial Intelligence, AAAI,
Pittsburgh, PA.
Rumelhart, D.E. and Abrahamson, A.A. 1973 A model for analogical
reasoning. Cognitive Psychology 5: 1-28.
Schuster, E. 1982 Explaining and Expounding. Technical Report
MS-CIS-82-49, University of Pennsylvania, Philadelphia, PA.
Sidner, C.L. 1983 Focusing in the comprehension of definite ana-
phora. In Brady, Michael and Berwick, Robert (eds.), Computa-
tional Models of Discourse, MIT Press, Cambridge, MA: 267-330.
Sleeman, D. 1982 Inferring (mal) rules from pupil&apos;s protocols. In
Proceedings of ECAI-82, Orsay, France: 160-164.
Stevens, A.L. and Collins, A. 1980 Multiple conceptual models of a
complex system. In Federico, Pat-Anthony; Snow, Richard E.;
and Montague, William E. (eds.), Aptitude, Learning, and Instruc-
tion, Erlbaum, Hillsdale, NJ: 177-197.
Stevens, A.; Collins, A.; and Goldin, S.E. 1979 Misconceptions in
student&apos;s understanding. International Journal of Man-Machine
Studies 11: 145-156.
Tou, F.; Williams, M.; Fikes, R.; Henderson, A.; and Malone, T.
1982 Rabbit: an intelligent database assistant. In Proceedings of
AAAI-82, Carnegie-Mellon University, Pittsburgh, PA: 314-317.
Tversky, A. 1977 Features of similarity. Psychological Review 84:
327-352.
Weiner, E. Judith 1984 A knowledge representation approach to
understanding metaphors. Computational Linguistics 19(1):1-14.
Woolf, Beverly P. 1984 Context Dependent Planning in a Machine
Tutor. Ph.D. thesis, University of Massachusetts, Amherst, MA.
Woolf, B. and McDonald, D. 1983 Human-computer discourse in the
design of a pascal tutor. In Janda, Ann (ed.), CHI&apos;83 Conference
Proceedings—Human Factors in Computing Systems, Boston,
MA: 230-234.
NOTES
1. While in general it may be necessary for the user model to contain
more information about the user (e.g., goals and plans), these are
not required for the restricted task set out in this paper.
2. The dialogs contained in this paper were not taken directly from
the transcripts. They are used to illustrate the kinds of responses
found.
3. Saying that exactly one perspective is active is actually a simpli-
fication. It may be the case that a number of perspectives can be
active. In this case the resulting &amp;quot;active perspective&amp;quot; will be
some function of the individual active perspectives. The exact
nature of this function is an open research question.
4. Note that the ROMPER system does not choose the active
perspective—it is given as input to the system. This example is
simply used to illustrate perspective&apos;s influence on misconception
responses.
Computational Linguistics, Volume 14, Number 3, September 1988 63
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.640464">
<title confidence="0.9623165">REASONING ON A HIGHLIGHTED USER TO RESPOND TO MISCONCEPTIONS</title>
<author confidence="0.742855">F</author>
<affiliation confidence="0.9992175">Department of Computer and Information University of Delaware</affiliation>
<address confidence="0.982481">Newark, DE 19716</address>
<abstract confidence="0.994017888888889">Responses to misconceptions given by human conversational partners very often contain information refuting possible reasoning which may have led to the misconceptions. Surprisingly there is a great deal of regularity in these responses across different domains of discourse. For instance, one reason a user might have given an object a property it does not have is that the user confused the object with another similar object. In correcting such a misconception, a human conversational partner is likely to point out this possible confusion. This work describes a method for generating responses like the one just described by reasoning on a highlighted model of the user to identify possible sources of the error. Through a transcript study a number of response strategies were abstracted. Each strategy was associated with a structural configuration of the user model. For example, the above mentioned strategy of pointing out a similar confused object is associated with a configuration of the user model that indicates the user believes there is an important similar object that has the property involved in the misconception. Upon finding that configuration in the highlighted user model, the system can respond with the associated strategy. Notice that the reasoning must be done on a highlighted user model since the perception of both an object&apos;s importance and its similarity with another object change with the perspective being taken on the domain. This paper investigates how domain perspective can be modeled to provide the needed highlighting and introduces a similarity metric that is sensitive to the highlighting provided by the domain perspective. Finally, the paper shows how the highlighting affects misconception responses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D G Bobrow</author>
<author>T Winograd</author>
</authors>
<title>An overview of krl, a knowledge representation language.</title>
<date>1977</date>
<journal>Cognitive Science</journal>
<volume>1</volume>
<issue>1</issue>
<pages>3--46</pages>
<contexts>
<context position="36546" citStr="Bobrow and Winograd 1977" startWordPosition="6030" endWordPosition="6034">space become important. If we can find a way of modeling how these &amp;quot;precompiled&amp;quot; sets of attributes become highlighted in a discourse, we will have a principled method for setting the f function needed for Tversky&apos;s similarity metric. The next section discusses how this highlighting can be modeled by a computer system. 8 OBJECT PERSPECTIVE The notion of point of view or object perspective has been noted by other researchers in artificial intelligence. Perspective&apos;s ability to explain the changing attribute salience has been attributed to a limited inheritance mechanism (see, e.g., Grosz 1977; Bobrow and Winograd 1977; Tou et al. 1982). An object viewed from a particular perspective is seen as having one particular superordinate, although in fact it may have many. The object inherits properties only from the superordinate in perspective. Therefore different perspectives on the same object cause different properties to be inherited (and therefore highlighted). While explaining object perspective via a limited inheritance mechanism is intuitively appealing, it is unable to handle some effects which intuitively should be handled by object perspective. The first has to do with the availability of object attrib</context>
</contexts>
<marker>Bobrow, Winograd, 1977</marker>
<rawString>Bobrow, D.G. and Winograd, T. 1977 An overview of krl, a knowledge representation language. Cognitive Science 1(1): 3-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Brown</author>
<author>R R Burton</author>
</authors>
<title>Diagnostic models for procedural bugs in basic mathematical skills.</title>
<date>1978</date>
<journal>Cognitive Science</journal>
<volume>2</volume>
<issue>2</issue>
<pages>155--192</pages>
<contexts>
<context position="8552" citStr="Brown and Burton 1978" startWordPosition="1369" endWordPosition="1372"> accessed, certain things in it will be highlighted while other things will be suppressed. I will show how this variable highlighting of the user model can explain why the response to a particular misconception by a particular user may vary. 2 RELATED MISCONCEPTION WORK The method of correcting misconceptions outlined above should be contrasted with the way that misconceptions are handled by Al systems today. For the most part misconceptions have been left to the Intelligent Computer Aided Instruction systems, which basically use an a priori listing of misconception-response pairs (see, e.g., Brown and Burton 1978; Stevens, Collins, and Goldin 1979; Stevens and Collins 1980; Woolf and McDonald 1983; Woolf 1984). The major problem with these systems is due to their inability to reason about the misconception itself, they are completely at a loss when faced with a misconception absent from their a priori listing. The work of Sleeman (1982) on inferring defective algebra rules (mal-rules) is based on the observation that the a priori listing of misconceptions is a difficult, if not impossible, task. Sleeman proposes on-line inference of mal-rules based on the answer the student has given to a particular p</context>
</contexts>
<marker>Brown, Burton, 1978</marker>
<rawString>Brown, J.S. and Burton, R.R. 1978 Diagnostic models for procedural bugs in basic mathematical skills. Cognitive Science 2(2): 155-192, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra M Carberry</author>
</authors>
<title>Understanding pragmatically ill-formed input.</title>
<date>1984</date>
<booktitle>In 10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>200--206</pages>
<location>Stanford University, CA:</location>
<contexts>
<context position="29758" citStr="Carberry 1984" startWordPosition="4872" endWordPosition="4873">.g., responding to a misconception of a particular type). Because of this, the test for determining which schema to use can be much more specific than the tests employed by McKeown. 7 HIGHLIGHTING AND OBJECT SIMILARITY We claim that in order for the above strategy for correcting misconceptions to work, the similarity metric that is used to assess object similarity must be affected by the preceding discourse. To date, most Al systems do not assess object similarity in a way that is context dependent. Several systems that do assess object similarity (Rumelhart and Abrahamson 1973, McKeown 1982, Carberry 1984, Weiner 1984) use a metric based on distance in some space. Most often, this space is the generalization hierarchy. Basically, two objects that have a common immediate superordinate (i.e., are siblings in the hierarchy) are seen as very similar, while objects whose lowest common ancestor is several levels up in the hierarchy are seen as quite different. One problem with this metric arises when objects can be classified in more than one way and there are several lowest common ancestors of the objects being compared. A decision must be made about which of these lowest common ancestors should be</context>
</contexts>
<marker>Carberry, 1984</marker>
<rawString>Carberry, Sandra M. 1984 Understanding pragmatically ill-formed input. In 10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association of Computational Linguistics, Stanford University, CA: 200-206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime R Carbonnell</author>
<author>Allan M Collins</author>
</authors>
<title>Mixed-Initiative Systems For Training and Decision-Aid Applications.</title>
<date>1970</date>
<tech>Technical Report ESD-TR-70-373,</tech>
<institution>Electronics Systems Division, Laurence</institution>
<location>Bedford, MA.</location>
<contexts>
<context position="34161" citStr="Carbonnell and Collins (1970)" startWordPosition="5638" endWordPosition="5641">re prominent object having a greater influence over the similarity assessment. While I would conjecture that information about the focus of the discourse (Grosz 1981, Sidner 1983, Grosz, Joshi, and Weinstein 1983) might give an indication of an object&apos;s prominence and would therefore be useful in setting the values of 0, a, and p, in this work I have assumed a value of 1 for the 0, a, and 13 and have concentrated on setting the f function. We turn, then, to the problem of finding a value for the f function: the measure of salience for each property of the objects involved. Other work, such as Carbonnell and Collins (1970) and Weiner (1984), has hand-encoded salience values for attributes of individual objects directly into the knowledge base, permanently setting the f function. This approach is not sufficient for setting the f function for Tversky&apos;s metric, since it is crucial that the f function be able to change with context. In order to make this happen, the salience values computed by f must change with context. To see this, consider our ability to explicitly mention an attribute to increase its salience. In the example earlier with the red and blue triangles and the red square, if the request for a simila</context>
</contexts>
<marker>Carbonnell, Collins, 1970</marker>
<rawString>Carbonnell, Jaime R. and Collins, Allan M. 1970 Mixed-Initiative Systems For Training and Decision-Aid Applications. Technical Report ESD-TR-70-373, Electronics Systems Division, Laurence G. Hanscom Field, U.S. Air Force, Bedford, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
</authors>
<title>The Representation and Use of Focus in Dialog Understanding.</title>
<date>1977</date>
<tech>Technical Report 151, SRI International,</tech>
<location>Menlo Park, CA.</location>
<contexts>
<context position="36520" citStr="Grosz 1977" startWordPosition="6028" endWordPosition="6029"> and living space become important. If we can find a way of modeling how these &amp;quot;precompiled&amp;quot; sets of attributes become highlighted in a discourse, we will have a principled method for setting the f function needed for Tversky&apos;s similarity metric. The next section discusses how this highlighting can be modeled by a computer system. 8 OBJECT PERSPECTIVE The notion of point of view or object perspective has been noted by other researchers in artificial intelligence. Perspective&apos;s ability to explain the changing attribute salience has been attributed to a limited inheritance mechanism (see, e.g., Grosz 1977; Bobrow and Winograd 1977; Tou et al. 1982). An object viewed from a particular perspective is seen as having one particular superordinate, although in fact it may have many. The object inherits properties only from the superordinate in perspective. Therefore different perspectives on the same object cause different properties to be inherited (and therefore highlighted). While explaining object perspective via a limited inheritance mechanism is intuitively appealing, it is unable to handle some effects which intuitively should be handled by object perspective. The first has to do with the ava</context>
</contexts>
<marker>Grosz, 1977</marker>
<rawString>Grosz, B. 1977 The Representation and Use of Focus in Dialog Understanding. Technical Report 151, SRI International, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>A K Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Providing a unified account of definite noun phrases in discourse.</title>
<date>1983</date>
<booktitle>In Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>44--50</pages>
<location>Cambridge, MA:</location>
<marker>Grosz, Joshi, Weinstein, 1983</marker>
<rawString>Grosz, B.; Joshi, A.K.; and Weinstein S. 1983 Providing a unified account of definite noun phrases in discourse. In Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics, Cambridge, MA: 44-50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
</authors>
<title>Focusing and description in natural language dialog.</title>
<date>1981</date>
<booktitle>Elements of Discourse Understanding,</booktitle>
<pages>85--105</pages>
<editor>In Webber B, Joshi, A, and Sag, I. (ed.),</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England:</location>
<contexts>
<context position="33697" citStr="Grosz 1981" startWordPosition="5554" endWordPosition="5555"> doing so. For instance to set 0, a, and p he turns to the relative prominence of objects a and b in the discourse. The more prominent an object is, the more its attributes should have an impact on the similarity rating. Thus, finding the relative prominence of objects a and b in the discourse would help set these values. If a is relatively more important, then functions 0 and a should be greater than /3 resulting in the attributes of the more prominent object having a greater influence over the similarity assessment. While I would conjecture that information about the focus of the discourse (Grosz 1981, Sidner 1983, Grosz, Joshi, and Weinstein 1983) might give an indication of an object&apos;s prominence and would therefore be useful in setting the values of 0, a, and p, in this work I have assumed a value of 1 for the 0, a, and 13 and have concentrated on setting the f function. We turn, then, to the problem of finding a value for the f function: the measure of salience for each property of the objects involved. Other work, such as Carbonnell and Collins (1970) and Weiner (1984), has hand-encoded salience values for attributes of individual objects directly into the knowledge base, permanently </context>
</contexts>
<marker>Grosz, 1981</marker>
<rawString>Grosz, B. 1981 Focusing and description in natural language dialog. In Webber B, Joshi, A, and Sag, I. (ed.), Elements of Discourse Understanding, Cambridge University Press, Cambridge, England: 85-105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>C Sidner</author>
</authors>
<title>Discourse structure and the proper treatment of interruptions.</title>
<date>1985</date>
<booktitle>In Proceedings of the 1985 Joint Conference on Artificial Intelligence, IJCAI85,</booktitle>
<location>Los Angeles, CA.</location>
<contexts>
<context position="50252" citStr="Grosz and Sidner 1985" startWordPosition="8212" endWordPosition="8215">rchase Place: Commercial Bank-0.5 Safety: Medium-1 Treasury Bills Issuer: US Government-1 Purchase Place: Federal Reserve-0.5 Safety: High-1 Treasury Bond Issuer: US Government-1 Purchase Place: Federal Reserve-0.5 Safety: High-1 Figure 4. Objects Through Issuing Company Perspective. perspective can be made based on discourse goals, the nature of establishing and perhaps &amp;quot;shifting&amp;quot; perspective during a discourse must still be investigated. Still unanswered are questions such as: When does a perspective change? How long is a perspective active? Is there a relationship between a discourse unit (Grosz and Sidner 1985) and perspective? Is there any structure to the space of perspectives that would put constraints on moving from one active perspective to another? These questions must be taken up in future research on perspective. 11 PERSPECTIVE&apos;S INFLUENCE ON RESPONSES In this section we will show how using the active perspective to highlight the user model, the misconception correcting algorithm from Section 6, and the Tversky similarity metric can account for context sensitive corrections to misconceptions. Recall that in correcting a misattribution one of the correction schemas used by ROMPER called for a</context>
</contexts>
<marker>Grosz, Sidner, 1985</marker>
<rawString>Grosz, B. and Sidner, C. 1985 Discourse structure and the proper treatment of interruptions. In Proceedings of the 1985 Joint Conference on Artificial Intelligence, IJCAI85, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Kaplan</author>
</authors>
<title>Cooperative Responses From a Portable Natural Language Database Query System.</title>
<date>1979</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="9504" citStr="Kaplan (1979)" startWordPosition="1528" endWordPosition="1529">ring defective algebra rules (mal-rules) is based on the observation that the a priori listing of misconceptions is a difficult, if not impossible, task. Sleeman proposes on-line inference of mal-rules based on the answer the student has given to a particular problem. Although Sleeman&apos;s work is a major improvement over the a priori listing approach, it still has several problems. First, there is no measure of how reasonable or likely a particular malrule is. In addition, once a mal-rule has been inferred, no indication is given concerning how the misconception should be corrected. The work of Kaplan (1979) and Mays (1980) is closer to the work described here in that they were concerned with handling and reasoning about whole classes of misconceptions, thereby giving the system the ability to handle a potentially infinite number of misconceptions. Kaplan and Mays were concerned with responding to Computational Linguistics, Volume 14, Number 3, September 1988 53 Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions certain types of misconceptions in the context of a natural language interface to a database system. They worked on detecting and correcting such misconc</context>
</contexts>
<marker>Kaplan, 1979</marker>
<rawString>Kaplan, S.J. 1979 Cooperative Responses From a Portable Natural Language Database Query System. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Karlin</author>
</authors>
<title>Romper Mumbles.</title>
<date>1985</date>
<tech>Technical Report,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="15299" citStr="Karlin (1985)" startWordPosition="2478" endWordPosition="2479">ond to a misconception in a manner similar to a human conversational partner&apos;s response. The methods described here have been implemented in the ROMPER system (Responding to Object-related Misconceptions using PERspective). The system takes as input a specification of the information that is inconsistent with the system&apos;s model of the world, the current perspective (described below), and a record of past focus. It produces a formal specification of the response. This response specification is passed into the Mumble system (McDonald 1980), which, using a grammar and dictionary written by Robin Karlin (1985), produces an actual English response. The implemented system works on the financial securities domain. In order to show the generality of this approach, two different domains will be used in this paper. The motivation for the system&apos;s method of choosing a response strategy will use examples from the domain containing whales and fish. In the last sections 54 Computational Linguistics, Volume 14, Number 3, September 1988 Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions of the paper (those pertaining to the new notion of object perspective), the examples will </context>
</contexts>
<marker>Karlin, 1985</marker>
<rawString>Karlin, Robin 1985 Romper Mumbles. Technical Report, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Malhotra</author>
</authors>
<title>Design Criteria for a Knowledge-Based English Language System for Management.</title>
<date>1975</date>
<tech>Technical Report TR-146,</tech>
<location>Project MAC, MIT, Cambridge, MA.</location>
<contexts>
<context position="2499" citStr="Malhotra 1975" startWordPosition="384" endWordPosition="385">affects misconception responses. 1 INTRODUCTION When people interact with a database or expert system, it is reasonable to expect that they might reveal a misconception about an object modeled by the system. Since a human conversational partner would correct such a misconception if it was important to the current goals of the conversation, our database and expert systems should also be equipped with this ability. In order to investigate how the process of correcting misconceptions might be automated, a study of transcripts of both humans interacting with what they thought were expert systems (Malhotra 1975, Malhotra and Sheridan 1976, Schuster 1982), and humans interacting with other humans to achieve some goal (Pollack, Hirschberg, and Webber 1982) was undertaken. The transcripts, which varied greatly in their domains of discourse, were analyzed to determine if there was any regularity in the content and rhetorical force of responses given to misconceptions. The intention of this analysis was not to mimic the actual behavior found in the transcripts, but to use them as a source of intuitions about the context and textual shape of responses as well as the process of generating them. The study r</context>
</contexts>
<marker>Malhotra, 1975</marker>
<rawString>Malhotra, A. 1975 Design Criteria for a Knowledge-Based English Language System for Management. Technical Report TR-146, Project MAC, MIT, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Malhotra</author>
<author>P Sheridan</author>
</authors>
<title>Experimental Determination of Design Requirements for a Program Explanation System.</title>
<date>1976</date>
<booktitle>Tech62 Computational Linguistics, Volume 14, Number 3,</booktitle>
<contexts>
<context position="2527" citStr="Malhotra and Sheridan 1976" startWordPosition="386" endWordPosition="389">eption responses. 1 INTRODUCTION When people interact with a database or expert system, it is reasonable to expect that they might reveal a misconception about an object modeled by the system. Since a human conversational partner would correct such a misconception if it was important to the current goals of the conversation, our database and expert systems should also be equipped with this ability. In order to investigate how the process of correcting misconceptions might be automated, a study of transcripts of both humans interacting with what they thought were expert systems (Malhotra 1975, Malhotra and Sheridan 1976, Schuster 1982), and humans interacting with other humans to achieve some goal (Pollack, Hirschberg, and Webber 1982) was undertaken. The transcripts, which varied greatly in their domains of discourse, were analyzed to determine if there was any regularity in the content and rhetorical force of responses given to misconceptions. The intention of this analysis was not to mimic the actual behavior found in the transcripts, but to use them as a source of intuitions about the context and textual shape of responses as well as the process of generating them. The study revealed that a response to a</context>
</contexts>
<marker>Malhotra, Sheridan, 1976</marker>
<rawString>Malhotra, A. and Sheridan, P. 1976 Experimental Determination of Design Requirements for a Program Explanation System. Tech62 Computational Linguistics, Volume 14, Number 3, September 1988</rawString>
</citation>
<citation valid="false">
<authors>
<author>F Kathleen</author>
</authors>
<title>McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions</title>
<tech>nical Report RC 5831,</tech>
<institution>IBM Research Center,</institution>
<location>Yorktown Heights, NY.</location>
<marker>Kathleen, </marker>
<rawString>Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions nical Report RC 5831, IBM Research Center, Yorktown Heights, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Mays</author>
</authors>
<title>Correcting misconceptions about database structure.</title>
<date>1980</date>
<booktitle>In Proceedings of the 3rd CSCSI Biennial Meeting,</booktitle>
<location>Victoria, BC,</location>
<contexts>
<context position="9520" citStr="Mays (1980)" startWordPosition="1531" endWordPosition="1532">ebra rules (mal-rules) is based on the observation that the a priori listing of misconceptions is a difficult, if not impossible, task. Sleeman proposes on-line inference of mal-rules based on the answer the student has given to a particular problem. Although Sleeman&apos;s work is a major improvement over the a priori listing approach, it still has several problems. First, there is no measure of how reasonable or likely a particular malrule is. In addition, once a mal-rule has been inferred, no indication is given concerning how the misconception should be corrected. The work of Kaplan (1979) and Mays (1980) is closer to the work described here in that they were concerned with handling and reasoning about whole classes of misconceptions, thereby giving the system the ability to handle a potentially infinite number of misconceptions. Kaplan and Mays were concerned with responding to Computational Linguistics, Volume 14, Number 3, September 1988 53 Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions certain types of misconceptions in the context of a natural language interface to a database system. They worked on detecting and correcting such misconceptions based on</context>
</contexts>
<marker>Mays, 1980</marker>
<rawString>Mays, E. 1980 Correcting misconceptions about database structure. In Proceedings of the 3rd CSCSI Biennial Meeting, Victoria, BC, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D McDonald</author>
</authors>
<title>Natural Language Production as a Process of Decision Making Under Constraint.</title>
<date>1980</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT,</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="15229" citStr="McDonald 1980" startWordPosition="2466" endWordPosition="2467">rk is on using the user model, in a domain-independent fashion, to respond to a misconception in a manner similar to a human conversational partner&apos;s response. The methods described here have been implemented in the ROMPER system (Responding to Object-related Misconceptions using PERspective). The system takes as input a specification of the information that is inconsistent with the system&apos;s model of the world, the current perspective (described below), and a record of past focus. It produces a formal specification of the response. This response specification is passed into the Mumble system (McDonald 1980), which, using a grammar and dictionary written by Robin Karlin (1985), produces an actual English response. The implemented system works on the financial securities domain. In order to show the generality of this approach, two different domains will be used in this paper. The motivation for the system&apos;s method of choosing a response strategy will use examples from the domain containing whales and fish. In the last sections 54 Computational Linguistics, Volume 14, Number 3, September 1988 Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions of the paper (those p</context>
</contexts>
<marker>McDonald, 1980</marker>
<rawString>McDonald, D.D. 1980 Natural Language Production as a Process of Decision Making Under Constraint. Ph.D. thesis, MIT, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
</authors>
<title>Generating Natural Language Text in Response to Questions About Database Structure.</title>
<date>1982</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="27789" citStr="McKeown (1982)" startWordPosition="4558" endWordPosition="4559">g attribute schema ELSE instantiate no support schema Notice that each of the tests for instantiating a schema hinges on the similarity assessment of two objects. These assessments must be context dependent. Because of this, it is crucial that the user model analysis be done on a user model highlighted by previous discourse and that the similarity metric take advantage of this highlighting. The highlighting and similarity metric used by ROMPER will be discussed below. This method for correcting misconceptions suggests a model of natural language generation that is similar to that put forth by McKeown (1982) but which differs from McKeown&apos;s model in several ways. Both McKeown and this work concentrate on determining the content and textual shape of a response. McKeown is concerned with responses to questions about the structure of a data base. Upon encountering such a question McKeown first delimits a relevant knowledge pool using fairly simple mechanisms. This relevant knowledge pool contains that information from the knowledge base that could possibly be included in the response; the actual generated response need not exhaust this pool. Next, based on the goal of the discourse (as determined by</context>
<context position="29743" citStr="McKeown 1982" startWordPosition="4870" endWordPosition="4871">ion problem (e.g., responding to a misconception of a particular type). Because of this, the test for determining which schema to use can be much more specific than the tests employed by McKeown. 7 HIGHLIGHTING AND OBJECT SIMILARITY We claim that in order for the above strategy for correcting misconceptions to work, the similarity metric that is used to assess object similarity must be affected by the preceding discourse. To date, most Al systems do not assess object similarity in a way that is context dependent. Several systems that do assess object similarity (Rumelhart and Abrahamson 1973, McKeown 1982, Carberry 1984, Weiner 1984) use a metric based on distance in some space. Most often, this space is the generalization hierarchy. Basically, two objects that have a common immediate superordinate (i.e., are siblings in the hierarchy) are seen as very similar, while objects whose lowest common ancestor is several levels up in the hierarchy are seen as quite different. One problem with this metric arises when objects can be classified in more than one way and there are several lowest common ancestors of the objects being compared. A decision must be made about which of these lowest common ance</context>
</contexts>
<marker>McKeown, 1982</marker>
<rawString>McKeown, K. 1982 Generating Natural Language Text in Response to Questions About Database Structure. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>M Wish</author>
<author>K Matthews</author>
</authors>
<title>Tailoring explanations for the user.</title>
<date>1985</date>
<booktitle>In Proceedings of the 1985 International Joint Conference on Artificial Intelligence,</booktitle>
<location>Los Angeles, CA.</location>
<marker>McKeown, Wish, Matthews, 1985</marker>
<rawString>McKeown, K.; Wish, M.; and Matthews, K. 1985 Tailoring explanations for the user. In Proceedings of the 1985 International Joint Conference on Artificial Intelligence, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pollack</author>
<author>J Hirschberg</author>
<author>B Webber</author>
</authors>
<title>User participation in the reasoning processes of expert systems.</title>
<date>1982</date>
<booktitle>In Proceedings of the 1982 National Conference on Artificial Intelligence,</booktitle>
<location>AAAI, Pittsburgh, PA.</location>
<marker>Pollack, Hirschberg, Webber, 1982</marker>
<rawString>Pollack, M.; Hirschberg, J.; and Webber, B. 1982 User participation in the reasoning processes of expert systems. In Proceedings of the 1982 National Conference on Artificial Intelligence, AAAI, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Rumelhart</author>
<author>A A Abrahamson</author>
</authors>
<title>A model for analogical reasoning.</title>
<date>1973</date>
<journal>Cognitive Psychology</journal>
<volume>5</volume>
<pages>1--28</pages>
<contexts>
<context position="29729" citStr="Rumelhart and Abrahamson 1973" startWordPosition="4866" endWordPosition="4869"> a much more restricted generation problem (e.g., responding to a misconception of a particular type). Because of this, the test for determining which schema to use can be much more specific than the tests employed by McKeown. 7 HIGHLIGHTING AND OBJECT SIMILARITY We claim that in order for the above strategy for correcting misconceptions to work, the similarity metric that is used to assess object similarity must be affected by the preceding discourse. To date, most Al systems do not assess object similarity in a way that is context dependent. Several systems that do assess object similarity (Rumelhart and Abrahamson 1973, McKeown 1982, Carberry 1984, Weiner 1984) use a metric based on distance in some space. Most often, this space is the generalization hierarchy. Basically, two objects that have a common immediate superordinate (i.e., are siblings in the hierarchy) are seen as very similar, while objects whose lowest common ancestor is several levels up in the hierarchy are seen as quite different. One problem with this metric arises when objects can be classified in more than one way and there are several lowest common ancestors of the objects being compared. A decision must be made about which of these lowe</context>
</contexts>
<marker>Rumelhart, Abrahamson, 1973</marker>
<rawString>Rumelhart, D.E. and Abrahamson, A.A. 1973 A model for analogical reasoning. Cognitive Psychology 5: 1-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Schuster</author>
</authors>
<title>Explaining and Expounding.</title>
<date>1982</date>
<tech>Technical Report MS-CIS-82-49,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2543" citStr="Schuster 1982" startWordPosition="390" endWordPosition="391">TION When people interact with a database or expert system, it is reasonable to expect that they might reveal a misconception about an object modeled by the system. Since a human conversational partner would correct such a misconception if it was important to the current goals of the conversation, our database and expert systems should also be equipped with this ability. In order to investigate how the process of correcting misconceptions might be automated, a study of transcripts of both humans interacting with what they thought were expert systems (Malhotra 1975, Malhotra and Sheridan 1976, Schuster 1982), and humans interacting with other humans to achieve some goal (Pollack, Hirschberg, and Webber 1982) was undertaken. The transcripts, which varied greatly in their domains of discourse, were analyzed to determine if there was any regularity in the content and rhetorical force of responses given to misconceptions. The intention of this analysis was not to mimic the actual behavior found in the transcripts, but to use them as a source of intuitions about the context and textual shape of responses as well as the process of generating them. The study revealed that a response to a misconception i</context>
</contexts>
<marker>Schuster, 1982</marker>
<rawString>Schuster, E. 1982 Explaining and Expounding. Technical Report MS-CIS-82-49, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Sidner</author>
</authors>
<title>Focusing in the comprehension of definite anaphora.</title>
<date>1983</date>
<booktitle>Computational Models of Discourse,</booktitle>
<pages>267--330</pages>
<editor>In Brady, Michael and Berwick, Robert (eds.),</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="33710" citStr="Sidner 1983" startWordPosition="5556" endWordPosition="5557">or instance to set 0, a, and p he turns to the relative prominence of objects a and b in the discourse. The more prominent an object is, the more its attributes should have an impact on the similarity rating. Thus, finding the relative prominence of objects a and b in the discourse would help set these values. If a is relatively more important, then functions 0 and a should be greater than /3 resulting in the attributes of the more prominent object having a greater influence over the similarity assessment. While I would conjecture that information about the focus of the discourse (Grosz 1981, Sidner 1983, Grosz, Joshi, and Weinstein 1983) might give an indication of an object&apos;s prominence and would therefore be useful in setting the values of 0, a, and p, in this work I have assumed a value of 1 for the 0, a, and 13 and have concentrated on setting the f function. We turn, then, to the problem of finding a value for the f function: the measure of salience for each property of the objects involved. Other work, such as Carbonnell and Collins (1970) and Weiner (1984), has hand-encoded salience values for attributes of individual objects directly into the knowledge base, permanently setting the f</context>
</contexts>
<marker>Sidner, 1983</marker>
<rawString>Sidner, C.L. 1983 Focusing in the comprehension of definite anaphora. In Brady, Michael and Berwick, Robert (eds.), Computational Models of Discourse, MIT Press, Cambridge, MA: 267-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sleeman</author>
</authors>
<title>Inferring (mal) rules from pupil&apos;s protocols.</title>
<date>1982</date>
<booktitle>In Proceedings of ECAI-82,</booktitle>
<pages>160--164</pages>
<location>Orsay, France:</location>
<contexts>
<context position="8882" citStr="Sleeman (1982)" startWordPosition="1425" endWordPosition="1426">ontrasted with the way that misconceptions are handled by Al systems today. For the most part misconceptions have been left to the Intelligent Computer Aided Instruction systems, which basically use an a priori listing of misconception-response pairs (see, e.g., Brown and Burton 1978; Stevens, Collins, and Goldin 1979; Stevens and Collins 1980; Woolf and McDonald 1983; Woolf 1984). The major problem with these systems is due to their inability to reason about the misconception itself, they are completely at a loss when faced with a misconception absent from their a priori listing. The work of Sleeman (1982) on inferring defective algebra rules (mal-rules) is based on the observation that the a priori listing of misconceptions is a difficult, if not impossible, task. Sleeman proposes on-line inference of mal-rules based on the answer the student has given to a particular problem. Although Sleeman&apos;s work is a major improvement over the a priori listing approach, it still has several problems. First, there is no measure of how reasonable or likely a particular malrule is. In addition, once a mal-rule has been inferred, no indication is given concerning how the misconception should be corrected. The</context>
</contexts>
<marker>Sleeman, 1982</marker>
<rawString>Sleeman, D. 1982 Inferring (mal) rules from pupil&apos;s protocols. In Proceedings of ECAI-82, Orsay, France: 160-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Stevens</author>
<author>A Collins</author>
</authors>
<title>Multiple conceptual models of a complex system.</title>
<date>1980</date>
<booktitle>Aptitude, Learning, and Instruction, Erlbaum,</booktitle>
<pages>177--197</pages>
<editor>In Federico, Pat-Anthony; Snow, Richard E.; and Montague, William E. (eds.),</editor>
<location>Hillsdale, NJ:</location>
<contexts>
<context position="8613" citStr="Stevens and Collins 1980" startWordPosition="1378" endWordPosition="1381">other things will be suppressed. I will show how this variable highlighting of the user model can explain why the response to a particular misconception by a particular user may vary. 2 RELATED MISCONCEPTION WORK The method of correcting misconceptions outlined above should be contrasted with the way that misconceptions are handled by Al systems today. For the most part misconceptions have been left to the Intelligent Computer Aided Instruction systems, which basically use an a priori listing of misconception-response pairs (see, e.g., Brown and Burton 1978; Stevens, Collins, and Goldin 1979; Stevens and Collins 1980; Woolf and McDonald 1983; Woolf 1984). The major problem with these systems is due to their inability to reason about the misconception itself, they are completely at a loss when faced with a misconception absent from their a priori listing. The work of Sleeman (1982) on inferring defective algebra rules (mal-rules) is based on the observation that the a priori listing of misconceptions is a difficult, if not impossible, task. Sleeman proposes on-line inference of mal-rules based on the answer the student has given to a particular problem. Although Sleeman&apos;s work is a major improvement over t</context>
</contexts>
<marker>Stevens, Collins, 1980</marker>
<rawString>Stevens, A.L. and Collins, A. 1980 Multiple conceptual models of a complex system. In Federico, Pat-Anthony; Snow, Richard E.; and Montague, William E. (eds.), Aptitude, Learning, and Instruction, Erlbaum, Hillsdale, NJ: 177-197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stevens</author>
<author>A Collins</author>
<author>S E Goldin</author>
</authors>
<title>Misconceptions in student&apos;s understanding.</title>
<date>1979</date>
<journal>International Journal of Man-Machine Studies</journal>
<volume>11</volume>
<pages>145--156</pages>
<marker>Stevens, Collins, Goldin, 1979</marker>
<rawString>Stevens, A.; Collins, A.; and Goldin, S.E. 1979 Misconceptions in student&apos;s understanding. International Journal of Man-Machine Studies 11: 145-156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Tou</author>
<author>M Williams</author>
<author>R Fikes</author>
<author>A Henderson</author>
<author>T Malone</author>
</authors>
<title>Rabbit: an intelligent database assistant.</title>
<date>1982</date>
<booktitle>In Proceedings of AAAI-82,</booktitle>
<pages>314--317</pages>
<institution>Carnegie-Mellon University,</institution>
<location>Pittsburgh, PA:</location>
<contexts>
<context position="36564" citStr="Tou et al. 1982" startWordPosition="6035" endWordPosition="6038"> we can find a way of modeling how these &amp;quot;precompiled&amp;quot; sets of attributes become highlighted in a discourse, we will have a principled method for setting the f function needed for Tversky&apos;s similarity metric. The next section discusses how this highlighting can be modeled by a computer system. 8 OBJECT PERSPECTIVE The notion of point of view or object perspective has been noted by other researchers in artificial intelligence. Perspective&apos;s ability to explain the changing attribute salience has been attributed to a limited inheritance mechanism (see, e.g., Grosz 1977; Bobrow and Winograd 1977; Tou et al. 1982). An object viewed from a particular perspective is seen as having one particular superordinate, although in fact it may have many. The object inherits properties only from the superordinate in perspective. Therefore different perspectives on the same object cause different properties to be inherited (and therefore highlighted). While explaining object perspective via a limited inheritance mechanism is intuitively appealing, it is unable to handle some effects which intuitively should be handled by object perspective. The first has to do with the availability of object attributes. A limited in</context>
</contexts>
<marker>Tou, Williams, Fikes, Henderson, Malone, 1982</marker>
<rawString>Tou, F.; Williams, M.; Fikes, R.; Henderson, A.; and Malone, T. 1982 Rabbit: an intelligent database assistant. In Proceedings of AAAI-82, Carnegie-Mellon University, Pittsburgh, PA: 314-317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tversky</author>
</authors>
<title>Features of similarity.</title>
<date>1977</date>
<journal>Psychological Review</journal>
<volume>84</volume>
<pages>327--352</pages>
<contexts>
<context position="31079" citStr="Tversky (1977)" startWordPosition="5094" endWordPosition="5095">reasury bond and a corporate bond may be assessed as being very similar since they have a common immediate superordinate of bonds. On the other hand, both of these objects can be classified along other dimensions. If we look at a treasury bond as being a treasury issue, which is a type of US Government security, it seems quite different from a corporate bond, which is a corporate security. A second major problem with a similarity metric based on distance in the generalization hierarchy is that it is context invariant; contextual information has no way of affecting the assessments. As shown by Tversky (1977) and others, human judgments of object similarity have been found to shift both when the set of objects under discussion are altered (e.g., a violin and an electric guitar may be judged quite similar when in a group with a clarinet and an oboe, and may be judged quite different when the other members of the group are Computational Linguistics, Volume 14, Number 3, September 1988 57 Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions a cello and an electric bass), and when the salience of attributes are altered (e.g., in a group containing a red triangle, a blue</context>
</contexts>
<marker>Tversky, 1977</marker>
<rawString>Tversky, A. 1977 Features of similarity. Psychological Review 84: 327-352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Judith Weiner</author>
</authors>
<title>A knowledge representation approach to understanding metaphors.</title>
<date>1984</date>
<journal>Computational Linguistics</journal>
<pages>19--1</pages>
<contexts>
<context position="29772" citStr="Weiner 1984" startWordPosition="4874" endWordPosition="4875"> to a misconception of a particular type). Because of this, the test for determining which schema to use can be much more specific than the tests employed by McKeown. 7 HIGHLIGHTING AND OBJECT SIMILARITY We claim that in order for the above strategy for correcting misconceptions to work, the similarity metric that is used to assess object similarity must be affected by the preceding discourse. To date, most Al systems do not assess object similarity in a way that is context dependent. Several systems that do assess object similarity (Rumelhart and Abrahamson 1973, McKeown 1982, Carberry 1984, Weiner 1984) use a metric based on distance in some space. Most often, this space is the generalization hierarchy. Basically, two objects that have a common immediate superordinate (i.e., are siblings in the hierarchy) are seen as very similar, while objects whose lowest common ancestor is several levels up in the hierarchy are seen as quite different. One problem with this metric arises when objects can be classified in more than one way and there are several lowest common ancestors of the objects being compared. A decision must be made about which of these lowest common ancestors should be considered si</context>
<context position="34179" citStr="Weiner (1984)" startWordPosition="5643" endWordPosition="5644">er influence over the similarity assessment. While I would conjecture that information about the focus of the discourse (Grosz 1981, Sidner 1983, Grosz, Joshi, and Weinstein 1983) might give an indication of an object&apos;s prominence and would therefore be useful in setting the values of 0, a, and p, in this work I have assumed a value of 1 for the 0, a, and 13 and have concentrated on setting the f function. We turn, then, to the problem of finding a value for the f function: the measure of salience for each property of the objects involved. Other work, such as Carbonnell and Collins (1970) and Weiner (1984), has hand-encoded salience values for attributes of individual objects directly into the knowledge base, permanently setting the f function. This approach is not sufficient for setting the f function for Tversky&apos;s metric, since it is crucial that the f function be able to change with context. In order to make this happen, the salience values computed by f must change with context. To see this, consider our ability to explicitly mention an attribute to increase its salience. In the example earlier with the red and blue triangles and the red square, if the request for a similarity judgment had </context>
</contexts>
<marker>Weiner, 1984</marker>
<rawString>Weiner, E. Judith 1984 A knowledge representation approach to understanding metaphors. Computational Linguistics 19(1):1-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beverly P Woolf</author>
</authors>
<title>Context Dependent Planning in a Machine Tutor.</title>
<date>1984</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Massachusetts,</institution>
<location>Amherst, MA.</location>
<contexts>
<context position="8651" citStr="Woolf 1984" startWordPosition="1386" endWordPosition="1387">is variable highlighting of the user model can explain why the response to a particular misconception by a particular user may vary. 2 RELATED MISCONCEPTION WORK The method of correcting misconceptions outlined above should be contrasted with the way that misconceptions are handled by Al systems today. For the most part misconceptions have been left to the Intelligent Computer Aided Instruction systems, which basically use an a priori listing of misconception-response pairs (see, e.g., Brown and Burton 1978; Stevens, Collins, and Goldin 1979; Stevens and Collins 1980; Woolf and McDonald 1983; Woolf 1984). The major problem with these systems is due to their inability to reason about the misconception itself, they are completely at a loss when faced with a misconception absent from their a priori listing. The work of Sleeman (1982) on inferring defective algebra rules (mal-rules) is based on the observation that the a priori listing of misconceptions is a difficult, if not impossible, task. Sleeman proposes on-line inference of mal-rules based on the answer the student has given to a particular problem. Although Sleeman&apos;s work is a major improvement over the a priori listing approach, it still</context>
</contexts>
<marker>Woolf, 1984</marker>
<rawString>Woolf, Beverly P. 1984 Context Dependent Planning in a Machine Tutor. Ph.D. thesis, University of Massachusetts, Amherst, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Woolf</author>
<author>D McDonald</author>
</authors>
<title>Human-computer discourse in the design of a pascal tutor.</title>
<date>1983</date>
<booktitle>CHI&apos;83 Conference Proceedings—Human Factors in Computing Systems,</booktitle>
<pages>230--234</pages>
<editor>In Janda, Ann (ed.),</editor>
<location>Boston, MA:</location>
<contexts>
<context position="8638" citStr="Woolf and McDonald 1983" startWordPosition="1382" endWordPosition="1385">essed. I will show how this variable highlighting of the user model can explain why the response to a particular misconception by a particular user may vary. 2 RELATED MISCONCEPTION WORK The method of correcting misconceptions outlined above should be contrasted with the way that misconceptions are handled by Al systems today. For the most part misconceptions have been left to the Intelligent Computer Aided Instruction systems, which basically use an a priori listing of misconception-response pairs (see, e.g., Brown and Burton 1978; Stevens, Collins, and Goldin 1979; Stevens and Collins 1980; Woolf and McDonald 1983; Woolf 1984). The major problem with these systems is due to their inability to reason about the misconception itself, they are completely at a loss when faced with a misconception absent from their a priori listing. The work of Sleeman (1982) on inferring defective algebra rules (mal-rules) is based on the observation that the a priori listing of misconceptions is a difficult, if not impossible, task. Sleeman proposes on-line inference of mal-rules based on the answer the student has given to a particular problem. Although Sleeman&apos;s work is a major improvement over the a priori listing appro</context>
</contexts>
<marker>Woolf, McDonald, 1983</marker>
<rawString>Woolf, B. and McDonald, D. 1983 Human-computer discourse in the design of a pascal tutor. In Janda, Ann (ed.), CHI&apos;83 Conference Proceedings—Human Factors in Computing Systems, Boston, MA: 230-234.</rawString>
</citation>
<citation valid="false">
<title>While in general it may be necessary for the user model to contain more information about the user (e.g., goals and plans), these are not required for the restricted task set out in this paper.</title>
<marker></marker>
<rawString>1. While in general it may be necessary for the user model to contain more information about the user (e.g., goals and plans), these are not required for the restricted task set out in this paper.</rawString>
</citation>
<citation valid="false">
<title>The dialogs contained in this paper were not taken directly from the transcripts. They are used to illustrate the kinds of responses found.</title>
<marker></marker>
<rawString>2. The dialogs contained in this paper were not taken directly from the transcripts. They are used to illustrate the kinds of responses found.</rawString>
</citation>
<citation valid="false">
<title>Saying that exactly one perspective is active is actually a simplification. It may be the case that a number of perspectives can be active. In this case the resulting &amp;quot;active perspective&amp;quot; will be some function of the individual active perspectives. The exact nature of this function is an open research question.</title>
<marker></marker>
<rawString>3. Saying that exactly one perspective is active is actually a simplification. It may be the case that a number of perspectives can be active. In this case the resulting &amp;quot;active perspective&amp;quot; will be some function of the individual active perspectives. The exact nature of this function is an open research question.</rawString>
</citation>
<citation valid="false">
<title>Note that the ROMPER system does not choose the active perspective—it is given as input to the system. This example is simply used to illustrate perspective&apos;s influence on misconception responses.</title>
<marker></marker>
<rawString>4. Note that the ROMPER system does not choose the active perspective—it is given as input to the system. This example is simply used to illustrate perspective&apos;s influence on misconception responses.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Computational Linguistics</author>
</authors>
<date>1988</date>
<volume>14</volume>
<pages>63</pages>
<marker>Linguistics, 1988</marker>
<rawString>Computational Linguistics, Volume 14, Number 3, September 1988 63</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>