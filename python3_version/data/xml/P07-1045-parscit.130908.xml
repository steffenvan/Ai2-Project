<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002924">
<title confidence="0.993316">
Conditional Modality Fusion for Coreference Resolution
</title>
<author confidence="0.998883">
Jacob Eisenstein and Randall Davis
</author>
<affiliation confidence="0.998825">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<address confidence="0.860604">
Cambridge, MA 02139 USA
</address>
<email confidence="0.99978">
{jacobe,davis}@csail.mit.edu
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999893272727273">
Non-verbal modalities such as gesture can
improve processing of spontaneous spoken
language. For example, similar hand ges-
tures tend to predict semantic similarity, so
features that quantify gestural similarity can
improve semantic tasks such as coreference
resolution. However, not all hand move-
ments are informative gestures; psycholog-
ical research has shown that speakers are
more likely to gesture meaningfully when
their speech is ambiguous. Ideally, one
would attend to gesture only in such cir-
cumstances, and ignore other hand move-
ments. We present conditional modality
fusion, which formalizes this intuition by
treating the informativeness of gesture as a
hidden variable to be learned jointly with
the class label. Applied to coreference
resolution, conditional modality fusion sig-
nificantly outperforms both early and late
modality fusion, which are current tech-
niques for modality combination.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985359243243243">
Non-verbal modalities such as gesture and prosody
can increase the robustness of NLP systems to the
inevitable disfluency of spontaneous speech. For ex-
ample, consider the following excerpt from a dia-
logue in which the speaker describes a mechanical
device:
“So this moves up, and it – everything moves up.
And this top one clears this area here, and goes all
the way up to the top.”
The references in this passage are difficult to
disambiguate, but the gestures shown in Figure 1
make the meaning more clear. However, non-verbal
modalities are often noisy, and their interactions
with speech are complex (McNeill, 1992). Ges-
ture, for example, is sometimes communicative, but
other times merely distracting. While people have
little difficulty distinguishing between meaningful
gestures and irrelevant hand motions (e.g., self-
touching, adjusting glasses) (Goodwin and Good-
win, 1986), NLP systems may be confused by such
seemingly random movements. Our goal is to in-
clude non-verbal features only in the specific cases
when they are helpful and necessary.
We present a model that learns in an unsupervised
fashion when non-verbal features are useful, allow-
ing it to gate the contribution of those features. The
relevance of the non-verbal features is treated as a
hidden variable, which is learned jointly with the
class label in a conditional model. We demonstrate
that this improves performance on binary corefer-
ence resolution, the task of determining whether a
noun phrases refers to a single semantic entity. Con-
ditional modality fusion yields a relative increase of
73% in the contribution of hand-gesture features.
The model is not specifically tailored to gesture-
speech integration, and may also be applicable to
other non-verbal modalities.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99960975">
Most of the existing work on integrating non-verbal
features relates to prosody. For example, Shriberg
et al. (2000) explore the use of prosodic features for
sentence and topic segmentation. The first modal-
</bodyText>
<page confidence="0.963311">
352
</page>
<note confidence="0.842398333333333">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 352–359,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
So this moves up. And it – everything moves up.
1
And this top one clears this area here, and goes
all the way up to the top...
</note>
<page confidence="0.845994">
2
</page>
<figureCaption confidence="0.999638">
Figure 1: An example where gesture helps to disambiguate meaning.
</figureCaption>
<bodyText confidence="0.999914052631579">
ity combination technique that they consider trains a
single classifier with all modalities combined into a
single feature vector; this is sometimes called “early
fusion.” Shriberg et al. also consider training sepa-
rate classifiers and combining their posteriors, either
through weighted addition or multiplication; this is
sometimes called “late fusion.” Late fusion is also
employed for gesture-speech combination in (Chen
et al., 2004). Experiments in both (Shriberg et al.,
2000) and (Kim et al., 2004) find no conclusive win-
ner among early fusion, additive late fusion, and
multiplicative late fusion.
Toyama and Horvitz (2000) introduce a Bayesian
network approach to modality combination for
speaker identification. As in late fusion, modality-
specific classifiers are trained independently. How-
ever, the Bayesian approach also learns to predict
the reliability of each modality on a given instance,
and incorporates this information into the Bayes
net. While more flexible than the interpolation tech-
niques described in (Shriberg et al., 2000), training
modality-specific classifiers separately is still sub-
optimal compared to training them jointly, because
independent training of the modality-specific classi-
fiers forces them to account for data that they can-
not possibly explain. For example, if the speaker is
not gesturing meaningfully, it is counterproductive
to train a gesture-modality classifier on the features
at this instant; doing so can lead to overfitting and
poor generalization.
Our approach combines aspects of both early and
late fusion. As in early fusion, classifiers for all
modalities are trained jointly. But as in Toyama and
Horvitz’s Bayesian late fusion model, modalities can
be weighted based on their predictive power for spe-
cific instances. In addition, our model is trained to
maximize conditional likelihood, rather than joint
likelihood.
</bodyText>
<sectionHeader confidence="0.976832" genericHeader="method">
3 Conditional modality fusion
</sectionHeader>
<bodyText confidence="0.999731416666667">
The goal of our approach is to learn to weight the
non-verbal features x,,,,, only when they are rele-
vant. To do this, we introduce a hidden variable
m E {−1,1}, which governs whether the non-
verbal features are included. p(m) is conditioned on
a subset of features xm, which may belong to any
modality; p(m|xm) is learned jointly with the class
label p(y|x), with y E {−1,1}. For our coreference
resolution model, y corresponds to whether a given
pair of noun phrases refers to the same entity.
In a log-linear model, parameterized by weights
w, we have:
</bodyText>
<equation confidence="0.94201825">
p(y|x; w) = � p(y, m|x; w)
m
Em exp(ψ(y, m, x; w))
= E y&amp;quot;m exp(ψ(y&apos;, m, x; w)) .
</equation>
<bodyText confidence="0.997120888888889">
Here, ψ is a potential function representing the
compatibility between the label y, the hidden vari-
able m, and the observations x; this potential is pa-
rameterized by a vector of weights, w. The numera-
tor expresses the compatibility of the label y and ob-
servations x, summed over all possible values of the
hidden variable m. The denominator sums over both
m and all possible labels y&apos;, yielding the conditional
probability p(y|x; w). The use of hidden variables
</bodyText>
<page confidence="0.998711">
353
</page>
<bodyText confidence="0.9979165">
in a conditionally-trained model follows (Quattoni
et al., 2004).
This model can be trained by a gradient-based
optimization to maximize the conditional log-
likelihood of the observations. The unregularized
log-likelihood and gradient are given by:
</bodyText>
<equation confidence="0.976423">
ln(p(yi|xi; w)) (1)
ln Pm exp(ψ(yi, m, xi; w)) (2)
Py&apos;,m exp(ψ(y&amp;quot; m, xi; w))
p(m|yi, xi; w) ∂
∂wj ψ(yi, m, xi; w)
p(m, y�|xi; w) ∂
∂w . ψ(y&amp;quot; m, xi; w)
7
</equation>
<bodyText confidence="0.999991130434783">
The form of the potential function ψ is where our
intuitions about the role of the hidden variable are
formalized. Our goal is to include the non-verbal
features xnv only when they are relevant; conse-
quently, the weight for these features should go to
zero for some settings of the hidden variable m. In
addition, verbal language is different when used in
combination with meaningful non-verbal commu-
nication than when it is used unimodally (Kehler,
2000; Melinger and Levelt, 2004). Thus, we learn
a different set of feature weights for each case: wv,1
when the non-verbal features are included, and wv,2
otherwise. The formal definition of the potential
function for conditional modality fusion is:
We apply conditional modality fusion to corefer-
ence resolution – the problem of partitioning the
noun phrases in a document into clusters, where all
members of a cluster refer to the same semantic en-
tity. Coreference resolution on text datasets is well-
studied (e.g., (Cardie and Wagstaff, 1999)). This
prior work provides the departure point for our in-
vestigation of coreference resolution on spontaneous
and unconstrained speech and gesture.
</bodyText>
<subsectionHeader confidence="0.995841">
4.1 Form of the model
</subsectionHeader>
<bodyText confidence="0.999981741935484">
The form of the model used in this application is
slightly different from that shown in Equation 3.
When determining whether two noun phrases core-
fer, the features at each utterance must be consid-
ered. For example, if we are to compare the simi-
larity of the gestures that accompany the two noun
phrases, it should be the case that gesture is relevant
during both time periods.
For this reason, we create two hidden variables,
m1 and m2; they indicate the relevance of ges-
ture over the first (antecedent) and second (anaphor)
noun phrases, respectively. Since gesture similarity
is only meaningful if the gesture is relevant during
both NPs, the gesture features are included only if
m1 = m2 = 1. Similarly, the linguistic feature
weights wv,1 are used when m1 = m2 = 1; oth-
erwise the weights wv,2 are used. This yields the
model shown in Equation 4.
The vector of meta features xm1 includes all
single-phrase verbal and gesture features from Ta-
ble 1, computed at the antecedent noun phrase;
xm2 includes the single-phrase verbal and gesture
features, computed at the anaphoric noun phrase.
The label-dependent verbal features xv include both
pairwise and single phrase verbal features from the
table, while the label-dependent non-verbal features
xnv include only the pairwise gesture features. The
single-phrase non-verbal features were not included
because they were not thought to be informative as
to whether the associated noun-phrase would partic-
ipate in coreference relations.
</bodyText>
<subsectionHeader confidence="0.991275">
4.2 Verbal features
</subsectionHeader>
<bodyText confidence="0.999782615384615">
We employ a set of verbal features that is similar
to the features used by state-of-the-art coreference
resolution systems that operate on text (e.g., (Cardie
and Wagstaff, 1999)). Pairwise verbal features in-
clude: several string-match variants; distance fea-
tures, measured in terms of the number of interven-
ing noun phrases and sentences between the candi-
date NPs; and some syntactic features that can be
computed from part of speech tags. Single-phrase
verbal features describe the type of the noun phrase
(definite, indefinite, demonstrative (e.g., this ball),
or pronoun), the number of times it appeared in
the document, and whether there were any adjecti-
</bodyText>
<equation confidence="0.9679208">
1.(3)
4 Application to coreference resolution
ψ(y, m, x; w) �
�
y(wTv,1xv + wTnvxnv) + wTmxm m = 1
ywTv,2xv − wTmxm m = −
X
l(w) =
i
X=
i
∂li
∂wj
X=
m
yX
354
_ y(wV iXv + WT Xnv) + m1wmxry,tl + m2wmxry,t27 mi = m2 = 1 ( )
&apos;�(y, m1, m2, x; w) = , 4
ywTv,2xv + m1wTmxm1 + m2wTmxm2, otherwise.
</equation>
<bodyText confidence="0.999687333333333">
val modifiers. The continuous-valued features were
binned using a supervised technique (Fayyad and
Irani, 1993).
Note that some features commonly used for coref-
erence on the MUC and ACE corpora are not appli-
cable here. For example, gazetteers listing names of
nations or corporations are not relevant to our cor-
pus, which focuses on discussions of mechanical de-
vices (see section 5). Because we are working from
transcripts rather than text, features dependent on
punctuation and capitalization, such as apposition,
are also not applicable.
</bodyText>
<subsectionHeader confidence="0.976788">
4.3 Non-verbal features
</subsectionHeader>
<bodyText confidence="0.999955625">
Our non-verbal features attempt to capture similar-
ity between the speaker’s hand gestures; similar ges-
tures are thought to suggest semantic similarity (Mc-
Neill, 1992). For example, two noun phrases may
be more likely to corefer if they are accompanied by
identically-located pointing gestures. In this section,
we describe features that quantify various aspects of
gestural similarity.
The most straightforward measure of similarity is
the Euclidean distance between the average hand po-
sition during each noun phrase – we call this the
FOCUS-DISTANCE feature. Euclidean distance cap-
tures cases in which the speaker is performing a ges-
tural “hold” in roughly the same location (McNeill,
1992).
However, Euclidean distance may not correlate
directly with semantic similarity. For example,
when gesturing at a detailed part of a diagram,
very small changes in hand position may be se-
mantically meaningful, while in other regions posi-
tional similarity may be defined more loosely. Ide-
ally, we would compute a semantic feature cap-
turing the object of the speaker’s reference (e.g.,
“the red block”), but this is not possible in gen-
eral, since a complete taxonomy of all possible ob-
jects of reference is usually unknown. Instead, we
use a hidden Markov model (HMM) to perform a
spatio-temporal clustering on hand position and ve-
locity. The SAME-CLUSTER feature reports whether
the hand positions during two noun phrases were
usually grouped in the same cluster by the HMM.
JS-DIV reports the Jensen-Shannon divergence, a
continuous-valued feature used to measure the simi-
larity in cluster assignment probabilities between the
two gestures (Lin, 1991).
The gesture features described thus far capture the
similarity between static gestures; that is, gestures
in which the hand position is nearly constant. How-
ever, these features do not capture the similarity be-
tween gesture trajectories, which may also be used
to communicate meaning. For example, a descrip-
tion of two identical motions might be expressed
by very similar gesture trajectories. To measure the
similarity between gesture trajectories, we use dy-
namic time warping (Huang et al., 2001), which
gives a similarity metric for temporal data that is
invariant to speed. This is reported in the DTW-
DISTANCE feature.
All features are computed from hand and body
pixel coordinates, which are obtained via computer
vision; our vision system is similar to (Deutscher et
al., 2000). The feature set currently supports only
single-hand gestures, using the hand that is farthest
from the body center. As with the verbal feature set,
supervised binning was applied to the continuous-
valued features.
</bodyText>
<subsectionHeader confidence="0.996543">
4.4 Meta features
</subsectionHeader>
<bodyText confidence="0.999851153846154">
The role of the meta features is to determine whether
the gesture features are relevant at a given point in
time. To make this determination, both verbal and
non-verbal features are applied; the only require-
ment is that they be computable at a single instant
in time (unlike features that measure the similarity
between two NPs or gestures).
Verbal meta features Meaningful gesture has
been shown to be more frequent when the associated
speech is ambiguous (Melinger and Levelt, 2004).
Kehler finds that fully-specified noun phrases are
less likely to receive multimodal support (Kehler,
2000). These findings lead us to expect that pro-
</bodyText>
<page confidence="0.991181">
355
</page>
<figureCaption confidence="0.888026266666667">
Pairwise verbal features
edit-distance a numerical measure of the string simi-
exact-match larity between the two NPs
str-match true if the two NPs have identical sur-
nonpro-str face forms
pro-str true if the NPs are identical after re-
j-substring-i moving articles
i-substring-j true if i and j are not pronouns, and str-
overlap match is true
np-dist true if i and j are pronouns, and str-
sent-dist match is true
both-subj true if the anaphor j is a substring of
same-verb the antecedent i
number-match true if i is a substring of j
true if there are any shared words be-
tween i and j
the number of noun phrases between i
and j in the document
the number of sentences between i and
j in the document
true if both i and j precede the first verb
of their sentences
true if the first verb in the sentences for
i and j is identical
true if i and j have the same number
Single-phrase verbal features
pronoun true if the NP is a pronoun
count number of times the NP appears in the
has-modifiers document
indef-np true if the NP has adjective modifiers
def-np true if the NP is an indefinite NP (e.g.,
dem-np a fish)
lexical features true if the NP is a definite NP (e.g., the
scooter)
true if the NP begins with this, that,
these, or those
lexical features are defined for the most
common pronouns: it, that, this, and
they
Pairwise gesture features
focus-distance the Euclidean distance in pixels be-
DTW-agreement tween the average hand position during
same-cluster the two NPs
JS-div a measure of the agreement of the hand-
trajectories during the two NPs, com-
puted using dynamic time warping
true if the hand positions during the two
NPs fall in the same cluster
the Jensen-Shannon divergence be-
tween the cluster assignment likeli-
hoods
Single-phrase gesture features
dist-to-rest distance of the hand from rest position
jitter sum of instantaneous motion across NP
speed total displacement over NP, divided by
rest-cluster duration
movement-cluster true if the hand is usually in the cluster
associated with rest position
true if the hand is usually in the cluster
associated with movement
</figureCaption>
<tableCaption confidence="0.977266">
Table 1: The feature set
</tableCaption>
<bodyText confidence="0.999890870967742">
nouns should be likely to co-occur with meaningful
gestures, while definite NPs and noun phrases that
include adjectival modifiers should be unlikely to do
so. To capture these intuitions, all single-phrase ver-
bal features are included as meta features.
Non-verbal meta features Research on gesture
has shown that semantically meaningful hand mo-
tions usually take place away from “rest position,”
which is located at the speaker’s lap or sides (Mc-
Neill, 1992). Effortful movements away from these
default positions can thus be expected to predict that
gesture is being used to communicate. We iden-
tify rest position as the center of the body on the
x-axis, and at a fixed, predefined location on the y-
axis. The DIST-TO-REST feature computes the av-
erage Euclidean distance of the hands from the rest
position, over the duration of the NP.
As noted in the previous section, a spatio-
temporal clustering was performed on the hand po-
sitions and velocities, using an HMM. The REST-
CLUSTER feature takes the value “true” iff the most
frequently occupied cluster during the NP is the
cluster closest to rest position. In addition, pa-
rameter tying in the HMM forces all clusters but
one to represent static hold, with the remaining
cluster accounting for the transition movements be-
tween holds. Only this last cluster is permitted to
have an expected non-zero speed; if the hand is
most frequently in this cluster during the NP, then
the MOVEMENT-CLUSTER feature takes the value
“true.”
</bodyText>
<subsectionHeader confidence="0.966087">
4.5 Implementation
</subsectionHeader>
<bodyText confidence="0.9999805">
The objective function (Equation 1) is optimized
using a Java implementation of L-BFGS, a quasi-
Newton numerical optimization technique (Liu and
Nocedal, 1989). Standard L2-norm regulariza-
tion is employed to prevent overfitting, with cross-
validation to select the regularization constant. Al-
though standard logistic regression optimizes a con-
vex objective, the inclusion of the hidden variable
renders our objective non-convex. Thus, conver-
gence to a global minimum is not guaranteed.
</bodyText>
<sectionHeader confidence="0.996935" genericHeader="method">
5 Evaluation setup
</sectionHeader>
<bodyText confidence="0.973542">
Dataset Our dataset consists of sixteen short dia-
logues, in which participants explained the behavior
</bodyText>
<page confidence="0.99787">
356
</page>
<bodyText confidence="0.99954435">
of mechanical devices to a friend. There are nine
different pairs of participants; each contributed two
dialogues, with two thrown out due to recording er-
rors. One participant, the “speaker,” saw a short
video describing the function of the device prior
to the dialogue; the other participant was tested on
comprehension of the device’s behavior after the di-
alogue. The speaker was given a pre-printed dia-
gram to aid in the discussion. For simplicity, only
the speaker’s utterances were included in these ex-
periments.
The dialogues were limited to three minutes in du-
ration, and most of the participants used the entire
allotted time. “Markable” noun phrases – those that
are permitted to participate in coreference relations
– were annotated by the first author, in accordance
with the MUC task definition (Hirschman and Chin-
chor, 1997). A total of 1141 “markable” NPs were
transcribed, roughly half the size of the MUC6 de-
velopment set, which includes 2072 markable NPs
over 30 documents.
Evaluation metric Coreference resolution is of-
ten performed in two phases: a binary classifi-
cation phase, in which the likelihood of corefer-
ence for each pair of noun phrases is assessed;
and a partitioning phase, in which the clusters of
mutually-coreferring NPs are formed, maximizing
some global criterion (Cardie and Wagstaff, 1999).
Our model does not address the formation of noun-
phrase clusters, but only the question of whether
each pair of noun phrases in the document corefer.
Consequently, we evaluate only the binary classifi-
cation phase, and report results in terms of the area
under the ROC curve (AUC). As the small size of
the corpus did not permit dedicated test and devel-
opment sets, results are computed using leave-one-
out cross-validation, with one fold for each of the
sixteen documents in the corpus.
Baselines Three types of baselines were compared
to our conditional modality fusion (CMF) technique:
</bodyText>
<listItem confidence="0.997605363636364">
• Early fusion. The early fusion baseline in-
cludes all features in a single vector, ignor-
ing modality. This is equivalent to standard
maximum-entropy classification. Early fusion
is implemented with a conditionally-trained
linear classifier; it uses the same code as the
CMF model, but always includes all features.
• Late fusion. The late fusion baselines train
separate classifiers for gesture and speech, and
then combine their posteriors. The modality-
specific classifiers are conditionally-trained lin-
ear models, and again use the same code as the
CMF model. For simplicity, a parameter sweep
identifies the interpolation weights that maxi-
mize performance on the test set. Thus, it is
likely that these results somewhat overestimate
the performance of these baseline models. We
report results for both additive and multiplica-
tive combination of posteriors.
• No fusion. These baselines include the fea-
tures from only a single modality, and again
build a conditionally-trained linear classifier.
</listItem>
<bodyText confidence="0.979607947368421">
Implementation uses the same code as the CMF
model, but weights on features outside the tar-
get modality are forced to zero.
Although a comparison with existing state-of-the-
art coreference systems would be ideal, all such
available systems use verbal features that are inap-
plicable to our dataset, such as punctuation, capital-
ization, and gazetteers. The verbal features that we
have included are a representative sample from the
literature (e.g., (Cardie and Wagstaff, 1999)). The
“no fusion, verbal features only” baseline thus pro-
vides a reasonable representation of prior work on
coreference, by applying a maximum-entropy clas-
sifier to this set of typical verbal features.
Parameter tuning Continuous features are
binned separately for each cross-validation fold,
using only the training data. The regularization
constant is selected by cross-validation within each
training subset.
</bodyText>
<sectionHeader confidence="0.99991" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.9957946">
Conditional modality fusion outperforms all other
approaches by a statistically significant margin (Ta-
ble 2). Compared with early fusion, CMF offers an
absolute improvement of 1.20% in area under the
ROC curve (AUC).1 A paired t-test shows that this
</bodyText>
<footnote confidence="0.998038333333333">
1AUC quantifies the ranking accuracy of a classifier. If the
AUC is 1, all positively-labeled examples are ranked higher than
all negative-labeled ones.
</footnote>
<page confidence="0.985307">
357
</page>
<table confidence="0.999081285714286">
model AUC
Conditional modality fusion .8226
Early fusion .8109
Late fusion, multiplicative .8103
Late fusion, additive .8068
No fusion (verbal features only) .7945
No fusion (gesture features only) .6732
</table>
<tableCaption confidence="0.756256333333333">
Table 2: Results, in terms of areas under the ROC
curve
log of regularization constant
</tableCaption>
<figureCaption confidence="0.978744">
Figure 2: Conditional modality fusion is robust to
variations in the regularization constant.
</figureCaption>
<bodyText confidence="0.997470923076923">
result is statistically significant (p &lt; .002, t(15) =
3.73). CMF obtains higher performance on fourteen
of the sixteen test folds. Both additive and multi-
plicative late fusion perform on par with early fu-
sion.
Early fusion with gesture features is superior to
unimodal verbal classification by an absolute im-
provement of 1.64% AUC (p &lt; 4 * 10−4, t(15) =
4.45). Thus, while gesture features improve coref-
erence resolution on this dataset, their effectiveness
is increased by a relative 73% when conditional
modality fusion is applied. Figure 2 shows how per-
formance varies with the regularization constant.
</bodyText>
<sectionHeader confidence="0.99978" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.944299714285714">
The feature weights learned by the system to deter-
mine coreference largely confirm our linguistic in-
tuitions. Among the textual features, a large pos-
itive weight was assigned to the string match fea-
tures, while a large negative weight was assigned to
features such as number incompatibility (i.e., sin-
Weights learned with verbal meta features
</bodyText>
<figureCaption confidence="0.992647">
Figure 3: Weights for verbal meta features
</figureCaption>
<bodyText confidence="0.999781566666667">
gular versus plural). The system also learned that
gestures with similar hand positions and trajectories
were likely to indicate coreferring noun phrases; all
of our similarity metrics were correlated positively
with coreference. A chi-squared analysis found that
the EDIT DISTANCE was the most informative ver-
bal feature. The most informative gesture feature
was DTW-AGREEMENT feature, which measures
the similarity between gesture trajectories.
As described in section 4, both textual and gestu-
ral features are used to determine whether the ges-
ture is relevant. Among textual features, definite
and indefinite noun phrases were assigned nega-
tive weights, suggesting gesture would not be use-
ful to disambiguate coreference for such NPs. Pro-
nouns were assigned positive weights, with “this”
and the much less frequently used “they” receiving
the strongest weights. “It” and “that” received lower
weights; we observed that these pronouns were fre-
quently used to refer to the immediately preceding
noun phrase, so multimodal support was often un-
necessary. Last, we note that NPs with adjectival
modifiers were assigned negative weights, support-
ing the finding of (Kehler, 2000) that fully-specified
NPs are less likely to receive multimodal support. A
summary of the weights assigned to the verbal meta
features is shown in Figure 3. Among gesture meta
features, the weights learned by the system indicate
that non-moving hand gestures away from the body
are most likely to be informative in this dataset.
</bodyText>
<figure confidence="0.998284807692308">
0.83
CMF
Early Fusion
Speech Only
0.815
0.81
0.805
0.8
0.795
0.79
2 3 4 5 6 7 8
AUC
0.825
0.82
pronoun def dem indef &amp;quot;this&amp;quot; &amp;quot;it&amp;quot; &amp;quot;that&amp;quot; &amp;quot;they&amp;quot;modifiers
0.4
0.3
0.2
0.1
0
−0.1
−0.2
−0.3
−0.4
−0.5
−0.6
</figure>
<page confidence="0.988772">
358
</page>
<sectionHeader confidence="0.998894" genericHeader="method">
8 Future work
</sectionHeader>
<bodyText confidence="0.999990695652174">
We have assumed that the relevance of gesture to
semantics is dependent only on the currently avail-
able features, and not conditioned on prior history.
In reality, meaningful gestures occur over contigu-
ous blocks of time, rather than at randomly dis-
tributed instances. Indeed, the psychology literature
describes a finite-state model of gesture, proceed-
ing from “preparation,” to “stroke,” “hold,” and then
“retraction” (McNeill, 1992). These units are called
movement phases. The relevance of various gesture
features may be expected to depend on the move-
ment phase. During strokes, the trajectory of the
gesture may be the most relevant feature, while dur-
ing holds, static features such as hand position and
hand shape may dominate; during preparation and
retraction, gesture features are likely to be irrelevant.
The identification of these movement phases
should be independent of the specific problem of
coreference resolution. Thus, additional labels for
other linguistic phenomena (e.g., topic segmenta-
tion, disfluency) could be combined into the model.
Ideally, each additional set of labels would transfer
performance gains to the other labeling problems.
</bodyText>
<sectionHeader confidence="0.999082" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.99991">
We have presented a new method for combining
multiple modalities, which we feel is especially rel-
evant to non-verbal modalities that are used to com-
municate only intermittently. Our model treats the
relevance of the non-verbal modality as a hidden
variable, learned jointly with the class labels. Ap-
plied to coreference resolution, this model yields a
relative increase of 73% in the contribution of the
gesture features. This gain is attained by identify-
ing instances in which gesture features are especially
relevant, and weighing their contribution more heav-
ily. We next plan to investigate models with a tem-
poral component, so that the behavior of the hidden
variable is governed by a finite-state transducer.
</bodyText>
<sectionHeader confidence="0.652262" genericHeader="acknowledgments">
Acknowledgments We thank Aaron Adler, Regina
</sectionHeader>
<reference confidence="0.943063">
Barzilay, S. R. K. Branavan, Sonya Cates, Erdong Chen,
Michael Collins, Lisa Guttentag, Michael Oltmans, and Tom
Ouyang. This research is supported in part by MIT Project Oxy-
gen.
</reference>
<sectionHeader confidence="0.895617" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99948187755102">
Claire Cardie and Kiri Wagstaff. 1999. Noun phrase corefer-
ence as clustering. In Proceedings of EMNLP, pages 82–89.
Lei Chen, Yang Liu, Mary P. Harper, and Elizabeth Shriberg.
2004. Multimodal model integration for sentence unit de-
tection. In Proceedings ofICMI, pages 121–128.
Jonathan Deutscher, Andrew Blake, and Ian Reid. 2000. Artic-
ulated body motion capture by annealed particle filtering. In
Proceedings of CVPR, volume 2, pages 126–133.
Usama M. Fayyad and Keki B. Irani. 1993. Multi-interval
discretization of continuousvalued attributes for classifica-
tion learning. In Proceedings of IJCAI-93, volume 2, pages
1022–1027. Morgan Kaufmann.
M.H. Goodwin and C. Goodwin. 1986. Gesture and co-
participation in the activity of searching for a word. Semiot-
ica, 62:51–75.
Lynette Hirschman and Nancy Chinchor. 1997. MUC-7 coref-
erence task definition. In Proceedings of the Message Un-
derstanding Conference.
Xuedong Huang, Alex Acero, and Hsiao-Wuen Hon. 2001.
Spoken Language Processing. Prentice Hall.
Andrew Kehler. 2000. Cognitive status and form of reference
in multimodal human-computer interaction. In Proceedings
ofAAAI, pages 685–690.
Joungbum Kim, Sarah E. Schwarm, and Mari Osterdorf.
2004. Detecting structural metadata with decision trees
and transformation-based learning. In Proceedings of HLT-
NAACL’04. ACL Press.
Jianhua Lin. 1991. Divergence measures based on the shannon
entropy. IEEE transactions on information theory, 37:145–
151.
Dong C. Liu and Jorge Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathematical
Programming, 45:503–528.
David McNeill. 1992. Hand and Mind. The University of
Chicago Press.
Alissa Melinger and Willem J. M. Levelt. 2004. Gesture and
communicative intention of the speaker. Gesture, 4(2):119–
141.
Ariadna Quattoni, Michael Collins, and Trevor Darrell. 2004.
Conditional random fields for object recognition. In Neural
Information Processing Systems, pages 1097–1104.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tur, and
Gokhan Tur. 2000. Prosody-based automatic segmentation
of speech into sentences and topics. Speech Communication,
32.
Kentaro Toyama and Eric Horvitz. 2000. Bayesian modal-
ity fusion: Probabilistic integration of multiple vision al-
gorithms for head tracking. In Proceedings of ACCV ’00,
Fourth Asian Conference on Computer Vision.
</reference>
<page confidence="0.999055">
359
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.941373">
<title confidence="0.99956">Conditional Modality Fusion for Coreference Resolution</title>
<author confidence="0.999816">Jacob Eisenstein</author>
<author confidence="0.999816">Randall Davis</author>
<affiliation confidence="0.9999925">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology</affiliation>
<address confidence="0.999953">Cambridge, MA 02139 USA</address>
<abstract confidence="0.997467608695652">Non-verbal modalities such as gesture can improve processing of spontaneous spoken language. For example, similar hand gestures tend to predict semantic similarity, so features that quantify gestural similarity can improve semantic tasks such as coreference resolution. However, not all hand movements are informative gestures; psychological research has shown that speakers are more likely to gesture meaningfully when their speech is ambiguous. Ideally, one would attend to gesture only in such circumstances, and ignore other hand move- We present modality which formalizes this intuition by treating the informativeness of gesture as a hidden variable to be learned jointly with the class label. Applied to coreference resolution, conditional modality fusion significantly outperforms both early and late modality fusion, which are current techniques for modality combination.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>S R K Branavan Barzilay</author>
</authors>
<title>Sonya Cates, Erdong Chen, Michael Collins, Lisa Guttentag, Michael Oltmans, and Tom Ouyang. This research is supported in part by MIT Project Oxygen.</title>
<marker>Barzilay, </marker>
<rawString>Barzilay, S. R. K. Branavan, Sonya Cates, Erdong Chen, Michael Collins, Lisa Guttentag, Michael Oltmans, and Tom Ouyang. This research is supported in part by MIT Project Oxygen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
<author>Kiri Wagstaff</author>
</authors>
<title>Noun phrase coreference as clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>82--89</pages>
<contexts>
<context position="7929" citStr="Cardie and Wagstaff, 1999" startWordPosition="1245" endWordPosition="1248">ith meaningful non-verbal communication than when it is used unimodally (Kehler, 2000; Melinger and Levelt, 2004). Thus, we learn a different set of feature weights for each case: wv,1 when the non-verbal features are included, and wv,2 otherwise. The formal definition of the potential function for conditional modality fusion is: We apply conditional modality fusion to coreference resolution – the problem of partitioning the noun phrases in a document into clusters, where all members of a cluster refer to the same semantic entity. Coreference resolution on text datasets is wellstudied (e.g., (Cardie and Wagstaff, 1999)). This prior work provides the departure point for our investigation of coreference resolution on spontaneous and unconstrained speech and gesture. 4.1 Form of the model The form of the model used in this application is slightly different from that shown in Equation 3. When determining whether two noun phrases corefer, the features at each utterance must be considered. For example, if we are to compare the similarity of the gestures that accompany the two noun phrases, it should be the case that gesture is relevant during both time periods. For this reason, we create two hidden variables, m1 </context>
<context position="9782" citStr="Cardie and Wagstaff, 1999" startWordPosition="1544" endWordPosition="1547">ted at the anaphoric noun phrase. The label-dependent verbal features xv include both pairwise and single phrase verbal features from the table, while the label-dependent non-verbal features xnv include only the pairwise gesture features. The single-phrase non-verbal features were not included because they were not thought to be informative as to whether the associated noun-phrase would participate in coreference relations. 4.2 Verbal features We employ a set of verbal features that is similar to the features used by state-of-the-art coreference resolution systems that operate on text (e.g., (Cardie and Wagstaff, 1999)). Pairwise verbal features include: several string-match variants; distance features, measured in terms of the number of intervening noun phrases and sentences between the candidate NPs; and some syntactic features that can be computed from part of speech tags. Single-phrase verbal features describe the type of the noun phrase (definite, indefinite, demonstrative (e.g., this ball), or pronoun), the number of times it appeared in the document, and whether there were any adjecti1.(3) 4 Application to coreference resolution ψ(y, m, x; w) � � y(wTv,1xv + wTnvxnv) + wTmxm m = 1 ywTv,2xv − wTmxm m </context>
<context position="19956" citStr="Cardie and Wagstaff, 1999" startWordPosition="3215" endWordPosition="3218">in coreference relations – were annotated by the first author, in accordance with the MUC task definition (Hirschman and Chinchor, 1997). A total of 1141 “markable” NPs were transcribed, roughly half the size of the MUC6 development set, which includes 2072 markable NPs over 30 documents. Evaluation metric Coreference resolution is often performed in two phases: a binary classification phase, in which the likelihood of coreference for each pair of noun phrases is assessed; and a partitioning phase, in which the clusters of mutually-coreferring NPs are formed, maximizing some global criterion (Cardie and Wagstaff, 1999). Our model does not address the formation of nounphrase clusters, but only the question of whether each pair of noun phrases in the document corefer. Consequently, we evaluate only the binary classification phase, and report results in terms of the area under the ROC curve (AUC). As the small size of the corpus did not permit dedicated test and development sets, results are computed using leave-oneout cross-validation, with one fold for each of the sixteen documents in the corpus. Baselines Three types of baselines were compared to our conditional modality fusion (CMF) technique: • Early fusi</context>
<context position="22017" citStr="Cardie and Wagstaff, 1999" startWordPosition="3537" endWordPosition="3540">f posteriors. • No fusion. These baselines include the features from only a single modality, and again build a conditionally-trained linear classifier. Implementation uses the same code as the CMF model, but weights on features outside the target modality are forced to zero. Although a comparison with existing state-of-theart coreference systems would be ideal, all such available systems use verbal features that are inapplicable to our dataset, such as punctuation, capitalization, and gazetteers. The verbal features that we have included are a representative sample from the literature (e.g., (Cardie and Wagstaff, 1999)). The “no fusion, verbal features only” baseline thus provides a reasonable representation of prior work on coreference, by applying a maximum-entropy classifier to this set of typical verbal features. Parameter tuning Continuous features are binned separately for each cross-validation fold, using only the training data. The regularization constant is selected by cross-validation within each training subset. 6 Results Conditional modality fusion outperforms all other approaches by a statistically significant margin (Table 2). Compared with early fusion, CMF offers an absolute improvement of 1</context>
</contexts>
<marker>Cardie, Wagstaff, 1999</marker>
<rawString>Claire Cardie and Kiri Wagstaff. 1999. Noun phrase coreference as clustering. In Proceedings of EMNLP, pages 82–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Chen</author>
<author>Yang Liu</author>
<author>Mary P Harper</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Multimodal model integration for sentence unit detection.</title>
<date>2004</date>
<booktitle>In Proceedings ofICMI,</booktitle>
<pages>121--128</pages>
<contexts>
<context position="3961" citStr="Chen et al., 2004" startWordPosition="595" endWordPosition="598">. And it – everything moves up. 1 And this top one clears this area here, and goes all the way up to the top... 2 Figure 1: An example where gesture helps to disambiguate meaning. ity combination technique that they consider trains a single classifier with all modalities combined into a single feature vector; this is sometimes called “early fusion.” Shriberg et al. also consider training separate classifiers and combining their posteriors, either through weighted addition or multiplication; this is sometimes called “late fusion.” Late fusion is also employed for gesture-speech combination in (Chen et al., 2004). Experiments in both (Shriberg et al., 2000) and (Kim et al., 2004) find no conclusive winner among early fusion, additive late fusion, and multiplicative late fusion. Toyama and Horvitz (2000) introduce a Bayesian network approach to modality combination for speaker identification. As in late fusion, modalityspecific classifiers are trained independently. However, the Bayesian approach also learns to predict the reliability of each modality on a given instance, and incorporates this information into the Bayes net. While more flexible than the interpolation techniques described in (Shriberg e</context>
</contexts>
<marker>Chen, Liu, Harper, Shriberg, 2004</marker>
<rawString>Lei Chen, Yang Liu, Mary P. Harper, and Elizabeth Shriberg. 2004. Multimodal model integration for sentence unit detection. In Proceedings ofICMI, pages 121–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Deutscher</author>
<author>Andrew Blake</author>
<author>Ian Reid</author>
</authors>
<title>Articulated body motion capture by annealed particle filtering.</title>
<date>2000</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<volume>2</volume>
<pages>126--133</pages>
<contexts>
<context position="13548" citStr="Deutscher et al., 2000" startWordPosition="2157" endWordPosition="2160">r, these features do not capture the similarity between gesture trajectories, which may also be used to communicate meaning. For example, a description of two identical motions might be expressed by very similar gesture trajectories. To measure the similarity between gesture trajectories, we use dynamic time warping (Huang et al., 2001), which gives a similarity metric for temporal data that is invariant to speed. This is reported in the DTWDISTANCE feature. All features are computed from hand and body pixel coordinates, which are obtained via computer vision; our vision system is similar to (Deutscher et al., 2000). The feature set currently supports only single-hand gestures, using the hand that is farthest from the body center. As with the verbal feature set, supervised binning was applied to the continuousvalued features. 4.4 Meta features The role of the meta features is to determine whether the gesture features are relevant at a given point in time. To make this determination, both verbal and non-verbal features are applied; the only requirement is that they be computable at a single instant in time (unlike features that measure the similarity between two NPs or gestures). Verbal meta features Mean</context>
</contexts>
<marker>Deutscher, Blake, Reid, 2000</marker>
<rawString>Jonathan Deutscher, Andrew Blake, and Ian Reid. 2000. Articulated body motion capture by annealed particle filtering. In Proceedings of CVPR, volume 2, pages 126–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Usama M Fayyad</author>
<author>Keki B Irani</author>
</authors>
<title>Multi-interval discretization of continuousvalued attributes for classification learning.</title>
<date>1993</date>
<booktitle>In Proceedings of IJCAI-93,</booktitle>
<volume>2</volume>
<pages>1022--1027</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="10666" citStr="Fayyad and Irani, 1993" startWordPosition="1703" endWordPosition="1706">. Single-phrase verbal features describe the type of the noun phrase (definite, indefinite, demonstrative (e.g., this ball), or pronoun), the number of times it appeared in the document, and whether there were any adjecti1.(3) 4 Application to coreference resolution ψ(y, m, x; w) � � y(wTv,1xv + wTnvxnv) + wTmxm m = 1 ywTv,2xv − wTmxm m = − X l(w) = i X= i ∂li ∂wj X= m yX 354 _ y(wV iXv + WT Xnv) + m1wmxry,tl + m2wmxry,t27 mi = m2 = 1 ( ) &apos;�(y, m1, m2, x; w) = , 4 ywTv,2xv + m1wTmxm1 + m2wTmxm2, otherwise. val modifiers. The continuous-valued features were binned using a supervised technique (Fayyad and Irani, 1993). Note that some features commonly used for coreference on the MUC and ACE corpora are not applicable here. For example, gazetteers listing names of nations or corporations are not relevant to our corpus, which focuses on discussions of mechanical devices (see section 5). Because we are working from transcripts rather than text, features dependent on punctuation and capitalization, such as apposition, are also not applicable. 4.3 Non-verbal features Our non-verbal features attempt to capture similarity between the speaker’s hand gestures; similar gestures are thought to suggest semantic simila</context>
</contexts>
<marker>Fayyad, Irani, 1993</marker>
<rawString>Usama M. Fayyad and Keki B. Irani. 1993. Multi-interval discretization of continuousvalued attributes for classification learning. In Proceedings of IJCAI-93, volume 2, pages 1022–1027. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M H Goodwin</author>
<author>C Goodwin</author>
</authors>
<title>Gesture and coparticipation in the activity of searching for a word.</title>
<date>1986</date>
<journal>Semiotica,</journal>
<pages>62--51</pages>
<contexts>
<context position="2048" citStr="Goodwin and Goodwin, 1986" startWordPosition="294" endWordPosition="298">: “So this moves up, and it – everything moves up. And this top one clears this area here, and goes all the way up to the top.” The references in this passage are difficult to disambiguate, but the gestures shown in Figure 1 make the meaning more clear. However, non-verbal modalities are often noisy, and their interactions with speech are complex (McNeill, 1992). Gesture, for example, is sometimes communicative, but other times merely distracting. While people have little difficulty distinguishing between meaningful gestures and irrelevant hand motions (e.g., selftouching, adjusting glasses) (Goodwin and Goodwin, 1986), NLP systems may be confused by such seemingly random movements. Our goal is to include non-verbal features only in the specific cases when they are helpful and necessary. We present a model that learns in an unsupervised fashion when non-verbal features are useful, allowing it to gate the contribution of those features. The relevance of the non-verbal features is treated as a hidden variable, which is learned jointly with the class label in a conditional model. We demonstrate that this improves performance on binary coreference resolution, the task of determining whether a noun phrases refer</context>
</contexts>
<marker>Goodwin, Goodwin, 1986</marker>
<rawString>M.H. Goodwin and C. Goodwin. 1986. Gesture and coparticipation in the activity of searching for a word. Semiotica, 62:51–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Nancy Chinchor</author>
</authors>
<title>MUC-7 coreference task definition.</title>
<date>1997</date>
<booktitle>In Proceedings of the Message Understanding Conference.</booktitle>
<contexts>
<context position="19466" citStr="Hirschman and Chinchor, 1997" startWordPosition="3136" endWordPosition="3140">o describing the function of the device prior to the dialogue; the other participant was tested on comprehension of the device’s behavior after the dialogue. The speaker was given a pre-printed diagram to aid in the discussion. For simplicity, only the speaker’s utterances were included in these experiments. The dialogues were limited to three minutes in duration, and most of the participants used the entire allotted time. “Markable” noun phrases – those that are permitted to participate in coreference relations – were annotated by the first author, in accordance with the MUC task definition (Hirschman and Chinchor, 1997). A total of 1141 “markable” NPs were transcribed, roughly half the size of the MUC6 development set, which includes 2072 markable NPs over 30 documents. Evaluation metric Coreference resolution is often performed in two phases: a binary classification phase, in which the likelihood of coreference for each pair of noun phrases is assessed; and a partitioning phase, in which the clusters of mutually-coreferring NPs are formed, maximizing some global criterion (Cardie and Wagstaff, 1999). Our model does not address the formation of nounphrase clusters, but only the question of whether each pair </context>
</contexts>
<marker>Hirschman, Chinchor, 1997</marker>
<rawString>Lynette Hirschman and Nancy Chinchor. 1997. MUC-7 coreference task definition. In Proceedings of the Message Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuedong Huang</author>
<author>Alex Acero</author>
<author>Hsiao-Wuen Hon</author>
</authors>
<title>Spoken Language Processing.</title>
<date>2001</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="13263" citStr="Huang et al., 2001" startWordPosition="2110" endWordPosition="2113">s-valued feature used to measure the similarity in cluster assignment probabilities between the two gestures (Lin, 1991). The gesture features described thus far capture the similarity between static gestures; that is, gestures in which the hand position is nearly constant. However, these features do not capture the similarity between gesture trajectories, which may also be used to communicate meaning. For example, a description of two identical motions might be expressed by very similar gesture trajectories. To measure the similarity between gesture trajectories, we use dynamic time warping (Huang et al., 2001), which gives a similarity metric for temporal data that is invariant to speed. This is reported in the DTWDISTANCE feature. All features are computed from hand and body pixel coordinates, which are obtained via computer vision; our vision system is similar to (Deutscher et al., 2000). The feature set currently supports only single-hand gestures, using the hand that is farthest from the body center. As with the verbal feature set, supervised binning was applied to the continuousvalued features. 4.4 Meta features The role of the meta features is to determine whether the gesture features are rel</context>
</contexts>
<marker>Huang, Acero, Hon, 2001</marker>
<rawString>Xuedong Huang, Alex Acero, and Hsiao-Wuen Hon. 2001. Spoken Language Processing. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kehler</author>
</authors>
<title>Cognitive status and form of reference in multimodal human-computer interaction.</title>
<date>2000</date>
<booktitle>In Proceedings ofAAAI,</booktitle>
<pages>685--690</pages>
<contexts>
<context position="7388" citStr="Kehler, 2000" startWordPosition="1161" endWordPosition="1162"> by: ln(p(yi|xi; w)) (1) ln Pm exp(ψ(yi, m, xi; w)) (2) Py&apos;,m exp(ψ(y&amp;quot; m, xi; w)) p(m|yi, xi; w) ∂ ∂wj ψ(yi, m, xi; w) p(m, y�|xi; w) ∂ ∂w . ψ(y&amp;quot; m, xi; w) 7 The form of the potential function ψ is where our intuitions about the role of the hidden variable are formalized. Our goal is to include the non-verbal features xnv only when they are relevant; consequently, the weight for these features should go to zero for some settings of the hidden variable m. In addition, verbal language is different when used in combination with meaningful non-verbal communication than when it is used unimodally (Kehler, 2000; Melinger and Levelt, 2004). Thus, we learn a different set of feature weights for each case: wv,1 when the non-verbal features are included, and wv,2 otherwise. The formal definition of the potential function for conditional modality fusion is: We apply conditional modality fusion to coreference resolution – the problem of partitioning the noun phrases in a document into clusters, where all members of a cluster refer to the same semantic entity. Coreference resolution on text datasets is wellstudied (e.g., (Cardie and Wagstaff, 1999)). This prior work provides the departure point for our inv</context>
<context position="14374" citStr="Kehler, 2000" startWordPosition="2290" endWordPosition="2291">. 4.4 Meta features The role of the meta features is to determine whether the gesture features are relevant at a given point in time. To make this determination, both verbal and non-verbal features are applied; the only requirement is that they be computable at a single instant in time (unlike features that measure the similarity between two NPs or gestures). Verbal meta features Meaningful gesture has been shown to be more frequent when the associated speech is ambiguous (Melinger and Levelt, 2004). Kehler finds that fully-specified noun phrases are less likely to receive multimodal support (Kehler, 2000). These findings lead us to expect that pro355 Pairwise verbal features edit-distance a numerical measure of the string simiexact-match larity between the two NPs str-match true if the two NPs have identical surnonpro-str face forms pro-str true if the NPs are identical after rej-substring-i moving articles i-substring-j true if i and j are not pronouns, and stroverlap match is true np-dist true if i and j are pronouns, and strsent-dist match is true both-subj true if the anaphor j is a substring of same-verb the antecedent i number-match true if i is a substring of j true if there are any sha</context>
<context position="25413" citStr="Kehler, 2000" startWordPosition="4058" endWordPosition="4059">. Among textual features, definite and indefinite noun phrases were assigned negative weights, suggesting gesture would not be useful to disambiguate coreference for such NPs. Pronouns were assigned positive weights, with “this” and the much less frequently used “they” receiving the strongest weights. “It” and “that” received lower weights; we observed that these pronouns were frequently used to refer to the immediately preceding noun phrase, so multimodal support was often unnecessary. Last, we note that NPs with adjectival modifiers were assigned negative weights, supporting the finding of (Kehler, 2000) that fully-specified NPs are less likely to receive multimodal support. A summary of the weights assigned to the verbal meta features is shown in Figure 3. Among gesture meta features, the weights learned by the system indicate that non-moving hand gestures away from the body are most likely to be informative in this dataset. 0.83 CMF Early Fusion Speech Only 0.815 0.81 0.805 0.8 0.795 0.79 2 3 4 5 6 7 8 AUC 0.825 0.82 pronoun def dem indef &amp;quot;this&amp;quot; &amp;quot;it&amp;quot; &amp;quot;that&amp;quot; &amp;quot;they&amp;quot;modifiers 0.4 0.3 0.2 0.1 0 −0.1 −0.2 −0.3 −0.4 −0.5 −0.6 358 8 Future work We have assumed that the relevance of gesture to sema</context>
</contexts>
<marker>Kehler, 2000</marker>
<rawString>Andrew Kehler. 2000. Cognitive status and form of reference in multimodal human-computer interaction. In Proceedings ofAAAI, pages 685–690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joungbum Kim</author>
<author>Sarah E Schwarm</author>
<author>Mari Osterdorf</author>
</authors>
<title>Detecting structural metadata with decision trees and transformation-based learning.</title>
<date>2004</date>
<booktitle>In Proceedings of HLTNAACL’04.</booktitle>
<publisher>ACL Press.</publisher>
<contexts>
<context position="4029" citStr="Kim et al., 2004" startWordPosition="607" endWordPosition="610">ere, and goes all the way up to the top... 2 Figure 1: An example where gesture helps to disambiguate meaning. ity combination technique that they consider trains a single classifier with all modalities combined into a single feature vector; this is sometimes called “early fusion.” Shriberg et al. also consider training separate classifiers and combining their posteriors, either through weighted addition or multiplication; this is sometimes called “late fusion.” Late fusion is also employed for gesture-speech combination in (Chen et al., 2004). Experiments in both (Shriberg et al., 2000) and (Kim et al., 2004) find no conclusive winner among early fusion, additive late fusion, and multiplicative late fusion. Toyama and Horvitz (2000) introduce a Bayesian network approach to modality combination for speaker identification. As in late fusion, modalityspecific classifiers are trained independently. However, the Bayesian approach also learns to predict the reliability of each modality on a given instance, and incorporates this information into the Bayes net. While more flexible than the interpolation techniques described in (Shriberg et al., 2000), training modality-specific classifiers separately is s</context>
</contexts>
<marker>Kim, Schwarm, Osterdorf, 2004</marker>
<rawString>Joungbum Kim, Sarah E. Schwarm, and Mari Osterdorf. 2004. Detecting structural metadata with decision trees and transformation-based learning. In Proceedings of HLTNAACL’04. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Lin</author>
</authors>
<title>Divergence measures based on the shannon entropy.</title>
<date>1991</date>
<journal>IEEE transactions on information theory,</journal>
<pages>37--145</pages>
<contexts>
<context position="12764" citStr="Lin, 1991" startWordPosition="2034" endWordPosition="2035">e object of the speaker’s reference (e.g., “the red block”), but this is not possible in general, since a complete taxonomy of all possible objects of reference is usually unknown. Instead, we use a hidden Markov model (HMM) to perform a spatio-temporal clustering on hand position and velocity. The SAME-CLUSTER feature reports whether the hand positions during two noun phrases were usually grouped in the same cluster by the HMM. JS-DIV reports the Jensen-Shannon divergence, a continuous-valued feature used to measure the similarity in cluster assignment probabilities between the two gestures (Lin, 1991). The gesture features described thus far capture the similarity between static gestures; that is, gestures in which the hand position is nearly constant. However, these features do not capture the similarity between gesture trajectories, which may also be used to communicate meaning. For example, a description of two identical motions might be expressed by very similar gesture trajectories. To measure the similarity between gesture trajectories, we use dynamic time warping (Huang et al., 2001), which gives a similarity metric for temporal data that is invariant to speed. This is reported in t</context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Jianhua Lin. 1991. Divergence measures based on the shannon entropy. IEEE transactions on information theory, 37:145– 151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<pages>45--503</pages>
<contexts>
<context position="18178" citStr="Liu and Nocedal, 1989" startWordPosition="2935" endWordPosition="2938">ntly occupied cluster during the NP is the cluster closest to rest position. In addition, parameter tying in the HMM forces all clusters but one to represent static hold, with the remaining cluster accounting for the transition movements between holds. Only this last cluster is permitted to have an expected non-zero speed; if the hand is most frequently in this cluster during the NP, then the MOVEMENT-CLUSTER feature takes the value “true.” 4.5 Implementation The objective function (Equation 1) is optimized using a Java implementation of L-BFGS, a quasiNewton numerical optimization technique (Liu and Nocedal, 1989). Standard L2-norm regularization is employed to prevent overfitting, with crossvalidation to select the regularization constant. Although standard logistic regression optimizes a convex objective, the inclusion of the hidden variable renders our objective non-convex. Thus, convergence to a global minimum is not guaranteed. 5 Evaluation setup Dataset Our dataset consists of sixteen short dialogues, in which participants explained the behavior 356 of mechanical devices to a friend. There are nine different pairs of participants; each contributed two dialogues, with two thrown out due to recordi</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45:503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McNeill</author>
</authors>
<title>Hand and Mind.</title>
<date>1992</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="1786" citStr="McNeill, 1992" startWordPosition="262" endWordPosition="263">erbal modalities such as gesture and prosody can increase the robustness of NLP systems to the inevitable disfluency of spontaneous speech. For example, consider the following excerpt from a dialogue in which the speaker describes a mechanical device: “So this moves up, and it – everything moves up. And this top one clears this area here, and goes all the way up to the top.” The references in this passage are difficult to disambiguate, but the gestures shown in Figure 1 make the meaning more clear. However, non-verbal modalities are often noisy, and their interactions with speech are complex (McNeill, 1992). Gesture, for example, is sometimes communicative, but other times merely distracting. While people have little difficulty distinguishing between meaningful gestures and irrelevant hand motions (e.g., selftouching, adjusting glasses) (Goodwin and Goodwin, 1986), NLP systems may be confused by such seemingly random movements. Our goal is to include non-verbal features only in the specific cases when they are helpful and necessary. We present a model that learns in an unsupervised fashion when non-verbal features are useful, allowing it to gate the contribution of those features. The relevance </context>
<context position="11286" citStr="McNeill, 1992" startWordPosition="1801" endWordPosition="1803"> that some features commonly used for coreference on the MUC and ACE corpora are not applicable here. For example, gazetteers listing names of nations or corporations are not relevant to our corpus, which focuses on discussions of mechanical devices (see section 5). Because we are working from transcripts rather than text, features dependent on punctuation and capitalization, such as apposition, are also not applicable. 4.3 Non-verbal features Our non-verbal features attempt to capture similarity between the speaker’s hand gestures; similar gestures are thought to suggest semantic similarity (McNeill, 1992). For example, two noun phrases may be more likely to corefer if they are accompanied by identically-located pointing gestures. In this section, we describe features that quantify various aspects of gestural similarity. The most straightforward measure of similarity is the Euclidean distance between the average hand position during each noun phrase – we call this the FOCUS-DISTANCE feature. Euclidean distance captures cases in which the speaker is performing a gestural “hold” in roughly the same location (McNeill, 1992). However, Euclidean distance may not correlate directly with semantic simi</context>
<context position="16983" citStr="McNeill, 1992" startWordPosition="2738" endWordPosition="2740"> hand is usually in the cluster associated with rest position true if the hand is usually in the cluster associated with movement Table 1: The feature set nouns should be likely to co-occur with meaningful gestures, while definite NPs and noun phrases that include adjectival modifiers should be unlikely to do so. To capture these intuitions, all single-phrase verbal features are included as meta features. Non-verbal meta features Research on gesture has shown that semantically meaningful hand motions usually take place away from “rest position,” which is located at the speaker’s lap or sides (McNeill, 1992). Effortful movements away from these default positions can thus be expected to predict that gesture is being used to communicate. We identify rest position as the center of the body on the x-axis, and at a fixed, predefined location on the yaxis. The DIST-TO-REST feature computes the average Euclidean distance of the hands from the rest position, over the duration of the NP. As noted in the previous section, a spatiotemporal clustering was performed on the hand positions and velocities, using an HMM. The RESTCLUSTER feature takes the value “true” iff the most frequently occupied cluster durin</context>
<context position="26395" citStr="McNeill, 1992" startWordPosition="4222" endWordPosition="4223"> 0.81 0.805 0.8 0.795 0.79 2 3 4 5 6 7 8 AUC 0.825 0.82 pronoun def dem indef &amp;quot;this&amp;quot; &amp;quot;it&amp;quot; &amp;quot;that&amp;quot; &amp;quot;they&amp;quot;modifiers 0.4 0.3 0.2 0.1 0 −0.1 −0.2 −0.3 −0.4 −0.5 −0.6 358 8 Future work We have assumed that the relevance of gesture to semantics is dependent only on the currently available features, and not conditioned on prior history. In reality, meaningful gestures occur over contiguous blocks of time, rather than at randomly distributed instances. Indeed, the psychology literature describes a finite-state model of gesture, proceeding from “preparation,” to “stroke,” “hold,” and then “retraction” (McNeill, 1992). These units are called movement phases. The relevance of various gesture features may be expected to depend on the movement phase. During strokes, the trajectory of the gesture may be the most relevant feature, while during holds, static features such as hand position and hand shape may dominate; during preparation and retraction, gesture features are likely to be irrelevant. The identification of these movement phases should be independent of the specific problem of coreference resolution. Thus, additional labels for other linguistic phenomena (e.g., topic segmentation, disfluency) could be</context>
</contexts>
<marker>McNeill, 1992</marker>
<rawString>David McNeill. 1992. Hand and Mind. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alissa Melinger</author>
<author>Willem J M Levelt</author>
</authors>
<title>Gesture and communicative intention of the speaker.</title>
<date>2004</date>
<journal>Gesture,</journal>
<volume>4</volume>
<issue>2</issue>
<pages>141</pages>
<contexts>
<context position="7416" citStr="Melinger and Levelt, 2004" startWordPosition="1163" endWordPosition="1166">i; w)) (1) ln Pm exp(ψ(yi, m, xi; w)) (2) Py&apos;,m exp(ψ(y&amp;quot; m, xi; w)) p(m|yi, xi; w) ∂ ∂wj ψ(yi, m, xi; w) p(m, y�|xi; w) ∂ ∂w . ψ(y&amp;quot; m, xi; w) 7 The form of the potential function ψ is where our intuitions about the role of the hidden variable are formalized. Our goal is to include the non-verbal features xnv only when they are relevant; consequently, the weight for these features should go to zero for some settings of the hidden variable m. In addition, verbal language is different when used in combination with meaningful non-verbal communication than when it is used unimodally (Kehler, 2000; Melinger and Levelt, 2004). Thus, we learn a different set of feature weights for each case: wv,1 when the non-verbal features are included, and wv,2 otherwise. The formal definition of the potential function for conditional modality fusion is: We apply conditional modality fusion to coreference resolution – the problem of partitioning the noun phrases in a document into clusters, where all members of a cluster refer to the same semantic entity. Coreference resolution on text datasets is wellstudied (e.g., (Cardie and Wagstaff, 1999)). This prior work provides the departure point for our investigation of coreference re</context>
<context position="14265" citStr="Melinger and Levelt, 2004" startWordPosition="2273" endWordPosition="2276">hest from the body center. As with the verbal feature set, supervised binning was applied to the continuousvalued features. 4.4 Meta features The role of the meta features is to determine whether the gesture features are relevant at a given point in time. To make this determination, both verbal and non-verbal features are applied; the only requirement is that they be computable at a single instant in time (unlike features that measure the similarity between two NPs or gestures). Verbal meta features Meaningful gesture has been shown to be more frequent when the associated speech is ambiguous (Melinger and Levelt, 2004). Kehler finds that fully-specified noun phrases are less likely to receive multimodal support (Kehler, 2000). These findings lead us to expect that pro355 Pairwise verbal features edit-distance a numerical measure of the string simiexact-match larity between the two NPs str-match true if the two NPs have identical surnonpro-str face forms pro-str true if the NPs are identical after rej-substring-i moving articles i-substring-j true if i and j are not pronouns, and stroverlap match is true np-dist true if i and j are pronouns, and strsent-dist match is true both-subj true if the anaphor j is a</context>
</contexts>
<marker>Melinger, Levelt, 2004</marker>
<rawString>Alissa Melinger and Willem J. M. Levelt. 2004. Gesture and communicative intention of the speaker. Gesture, 4(2):119– 141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariadna Quattoni</author>
<author>Michael Collins</author>
<author>Trevor Darrell</author>
</authors>
<title>Conditional random fields for object recognition.</title>
<date>2004</date>
<booktitle>In Neural Information Processing Systems,</booktitle>
<pages>1097--1104</pages>
<contexts>
<context position="6597" citStr="Quattoni et al., 2004" startWordPosition="1022" endWordPosition="1025">, we have: p(y|x; w) = � p(y, m|x; w) m Em exp(ψ(y, m, x; w)) = E y&amp;quot;m exp(ψ(y&apos;, m, x; w)) . Here, ψ is a potential function representing the compatibility between the label y, the hidden variable m, and the observations x; this potential is parameterized by a vector of weights, w. The numerator expresses the compatibility of the label y and observations x, summed over all possible values of the hidden variable m. The denominator sums over both m and all possible labels y&apos;, yielding the conditional probability p(y|x; w). The use of hidden variables 353 in a conditionally-trained model follows (Quattoni et al., 2004). This model can be trained by a gradient-based optimization to maximize the conditional loglikelihood of the observations. The unregularized log-likelihood and gradient are given by: ln(p(yi|xi; w)) (1) ln Pm exp(ψ(yi, m, xi; w)) (2) Py&apos;,m exp(ψ(y&amp;quot; m, xi; w)) p(m|yi, xi; w) ∂ ∂wj ψ(yi, m, xi; w) p(m, y�|xi; w) ∂ ∂w . ψ(y&amp;quot; m, xi; w) 7 The form of the potential function ψ is where our intuitions about the role of the hidden variable are formalized. Our goal is to include the non-verbal features xnv only when they are relevant; consequently, the weight for these features should go to zero for so</context>
</contexts>
<marker>Quattoni, Collins, Darrell, 2004</marker>
<rawString>Ariadna Quattoni, Michael Collins, and Trevor Darrell. 2004. Conditional random fields for object recognition. In Neural Information Processing Systems, pages 1097–1104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Dilek Hakkani-Tur</author>
<author>Gokhan Tur</author>
</authors>
<title>Prosody-based automatic segmentation of speech into sentences and topics.</title>
<date>2000</date>
<journal>Speech Communication,</journal>
<volume>32</volume>
<contexts>
<context position="3046" citStr="Shriberg et al. (2000)" startWordPosition="454" endWordPosition="457">as a hidden variable, which is learned jointly with the class label in a conditional model. We demonstrate that this improves performance on binary coreference resolution, the task of determining whether a noun phrases refers to a single semantic entity. Conditional modality fusion yields a relative increase of 73% in the contribution of hand-gesture features. The model is not specifically tailored to gesturespeech integration, and may also be applicable to other non-verbal modalities. 2 Related work Most of the existing work on integrating non-verbal features relates to prosody. For example, Shriberg et al. (2000) explore the use of prosodic features for sentence and topic segmentation. The first modal352 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 352–359, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics So this moves up. And it – everything moves up. 1 And this top one clears this area here, and goes all the way up to the top... 2 Figure 1: An example where gesture helps to disambiguate meaning. ity combination technique that they consider trains a single classifier with all modalities combined into a single feature </context>
<context position="4573" citStr="Shriberg et al., 2000" startWordPosition="687" endWordPosition="690">al., 2004). Experiments in both (Shriberg et al., 2000) and (Kim et al., 2004) find no conclusive winner among early fusion, additive late fusion, and multiplicative late fusion. Toyama and Horvitz (2000) introduce a Bayesian network approach to modality combination for speaker identification. As in late fusion, modalityspecific classifiers are trained independently. However, the Bayesian approach also learns to predict the reliability of each modality on a given instance, and incorporates this information into the Bayes net. While more flexible than the interpolation techniques described in (Shriberg et al., 2000), training modality-specific classifiers separately is still suboptimal compared to training them jointly, because independent training of the modality-specific classifiers forces them to account for data that they cannot possibly explain. For example, if the speaker is not gesturing meaningfully, it is counterproductive to train a gesture-modality classifier on the features at this instant; doing so can lead to overfitting and poor generalization. Our approach combines aspects of both early and late fusion. As in early fusion, classifiers for all modalities are trained jointly. But as in Toya</context>
</contexts>
<marker>Shriberg, Stolcke, Hakkani-Tur, Tur, 2000</marker>
<rawString>Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tur, and Gokhan Tur. 2000. Prosody-based automatic segmentation of speech into sentences and topics. Speech Communication, 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Toyama</author>
<author>Eric Horvitz</author>
</authors>
<title>Bayesian modality fusion: Probabilistic integration of multiple vision algorithms for head tracking.</title>
<date>2000</date>
<booktitle>In Proceedings of ACCV ’00, Fourth Asian Conference on Computer Vision.</booktitle>
<contexts>
<context position="4155" citStr="Toyama and Horvitz (2000)" startWordPosition="626" endWordPosition="629">bination technique that they consider trains a single classifier with all modalities combined into a single feature vector; this is sometimes called “early fusion.” Shriberg et al. also consider training separate classifiers and combining their posteriors, either through weighted addition or multiplication; this is sometimes called “late fusion.” Late fusion is also employed for gesture-speech combination in (Chen et al., 2004). Experiments in both (Shriberg et al., 2000) and (Kim et al., 2004) find no conclusive winner among early fusion, additive late fusion, and multiplicative late fusion. Toyama and Horvitz (2000) introduce a Bayesian network approach to modality combination for speaker identification. As in late fusion, modalityspecific classifiers are trained independently. However, the Bayesian approach also learns to predict the reliability of each modality on a given instance, and incorporates this information into the Bayes net. While more flexible than the interpolation techniques described in (Shriberg et al., 2000), training modality-specific classifiers separately is still suboptimal compared to training them jointly, because independent training of the modality-specific classifiers forces th</context>
</contexts>
<marker>Toyama, Horvitz, 2000</marker>
<rawString>Kentaro Toyama and Eric Horvitz. 2000. Bayesian modality fusion: Probabilistic integration of multiple vision algorithms for head tracking. In Proceedings of ACCV ’00, Fourth Asian Conference on Computer Vision.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>