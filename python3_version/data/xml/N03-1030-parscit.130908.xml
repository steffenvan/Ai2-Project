<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000023">
<note confidence="0.959636333333333">
Proceedings of HLT-NAACL 2003
Main Papers , pp. 149-156
Edmonton, May-June 2003
</note>
<title confidence="0.983867">
Sentence Level Discourse Parsing using Syntactic and Lexical Information
</title>
<author confidence="0.869318">
Radu Soricut and Daniel Marcu
</author>
<affiliation confidence="0.720724333333333">
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.573339">
Marina del Rey, CA 90292
</address>
<email confidence="0.993297">
radu, marcu @isi.edu
</email>
<sectionHeader confidence="0.995869" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999865083333333">
We introduce two probabilistic models that can
be used to identify elementary discourse units
and build sentence-level discourse parse trees.
The models use syntactic and lexical features.
A discourse parsing algorithm that implements
these models derives discourse parse trees with
an error reduction of 18.8% over a state-of-
the-art decision-based discourse parser. A set
of empirical evaluations shows that our dis-
course parsing model is sophisticated enough
to yield discourse trees at an accuracy level that
matches near-human levels of performance.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999908318181818">
By exploiting information encoded in human-produced
syntactic trees (Marcus et al., 1993), research on prob-
abilistic models of syntax has driven the performance of
syntactic parsers to about 90% accuracy (Charniak, 2000;
Collins, 2000). The absence of semantic and discourse
annotated corpora prevented similar developments in se-
mantic/discourse parsing. Fortunately, recent annotation
projects have taken significant steps towards developing
semantic (Fillmore et al., 2002; Kingsbury and Palmer,
2002) and discourse (Carlson et al., 2003) annotated cor-
pora. Some of these annotation efforts have already had
a computational impact. For example, Gildea and Juraf-
sky (2002) developed statistical models for automatically
inducing semantic roles. In this paper, we describe proba-
bilistic models and algorithms that exploit the discourse-
annotated corpus produced by Carlson et al. (2003).
A discourse structure is a tree whose leaves correspond
to elementary discourse units (edu)s, and whose internal
nodes correspond to contiguous text spans (called dis-
course spans). An example of a discourse structure is
the tree given in Figure 1. Each internal node in a dis-
course tree is characterized by a rhetorical relation, such
</bodyText>
<figure confidence="0.4867052">
[ The bank also says ] 1
ENABLEMENT
2
[ it will use its network ] [ to channel investments.]
2
</figure>
<figureCaption confidence="0.999734">
Figure 1: Discourse structure of a sentence.
</figureCaption>
<bodyText confidence="0.999945652173913">
as ATTRIBUTION and ENABLEMENT. Within a rhetorical re-
lation a discourse span is also labeled as either NUCLEUS
or SATELLITE. The distinction between nuclei and satel-
lites comes from the empirical observation that a nucleus
expresses what is more essential to the writer’s purpose
than a satellite. Discourse trees can be represented graph-
ically in the style shown in Figure 1. The arrows link the
satellite to the nucleus of a rhetorical relation. Arrows are
labeled with the name of the rhetorical relation that holds
between the linked units. Horizontal lines correspond to
text spans, and vertical lines identify text spans which are
nuclei.
In this paper, we introduce two probabilistic models
that can be used to identify elementary discourse units
and build sentence-level discourse parse trees. We show
how syntactic and lexical information can be exploited in
the process of identifying elementary units of discourse
and building sentence-level discourse trees. Our evalu-
ation indicates that the discourse parsing model we pro-
pose is sophisticated enough to achieve near-human lev-
els of performance on the task of deriving sentence-level
discourse trees, when working with human-produced
syntactic trees and discourse segments.
</bodyText>
<sectionHeader confidence="0.989898" genericHeader="method">
2 The Corpus
</sectionHeader>
<bodyText confidence="0.9925085">
For the experiments described in this paper, we use a pub-
licly available corpus (RST-DT, 2002) that contains 385
</bodyText>
<figure confidence="0.9174726">
[2,3]
ATTRIBUTION
1
3
3
</figure>
<bodyText confidence="0.997572822222222">
Wall Street Journal articles from the Penn Treebank. The
corpus comes conveniently partitioned into a Training set
of 347 articles (6132 sentences) and a Test set of 38 ar-
ticles (991 sentences). Each document in the corpus is
paired with a discourse structure (tree) that was manually
built in the style of Rhetorical Structure Theory (Mann
and Thompson, 1988). (See (Carlson et al., 2003) for de-
tails concerning the corpus and the annotation process.)
Out of the 385 articles in the corpus, 53 have been inde-
pendently annotated by two human annotators. We used
this doubly-annotated subset to compute human agree-
ment on the task of discourse structure derivation. In our
experiments we used as discourse structures only the dis-
course sub-trees spanning over individual sentences.
Because the discourse structures had been built on top
of sentences already associated with syntactic trees from
the Penn Treebank, we were able to create a composite
corpus which allowed us to perform an empirically driven
syntax-discourse relationship study. This composite cor-
pus was created by associating each sentence in the dis-
course corpus with its corresponding Penn Treebank syn-
tactic parse tree and its correspond-
ing sentence-level discourse tree . Al-
though human annotators were free to build their dis-
course structures without enforcing the existence of well-
formed discourse sub-trees for each sentence, in about
95% of the cases in the (RST-DT, 2002) corpus, there
exists a discourse sub-tree associated
with each sentence . The remaining 5% of the sentences
cannot be used in our approach, as no well-formed dis-
course tree can be associated with these sentences.
Therefore, our Training section consists of a set of
5809 triples of the form
which are used to train the parameters of the statistical
models. Our Test section consists of a set of 946 triples
of a similar form, which are used to evaluate the perfor-
mance of our discourse parser.
The (RST-DT, 2002) corpus uses 110 different rhetori-
cal relations. We found it useful to also compact these re-
lations into classes, as described by Carlson et al. (2003),
and operate with the resulting 18 labels as well (seen as
coarser granularity rhetorical relations). Operating with
different levels of granularity allows one to get deeper
insight into the difficulties of assigning the appropriate
rhetorical relation, if any, to two adjacent text spans.
</bodyText>
<sectionHeader confidence="0.979264" genericHeader="method">
3 The Discourse Segmenter
</sectionHeader>
<bodyText confidence="0.9987072">
We break down the problem of building sentence-level
discourse trees into two sub-problems: discourse seg-
mentation and discourse parsing. Discourse segmenta-
tion is covered by this section, while discourse parsing is
covered by Section 4.
</bodyText>
<figure confidence="0.877069">
S
Np
</figure>
<figureCaption confidence="0.9965975">
Figure 2: Discourse segmentation using lexicalized syn-
tactic trees.
</figureCaption>
<bodyText confidence="0.9983253125">
Discourse segmentation is the process in which a given
text is broken into non-overlapping segments called ele-
mentary discourse units (edus). In the present work, ele-
mentary discourse units are taken to be clauses or clause-
like units that are unequivocally the NUCLEUS or SATEL-
LITE of a rhetorical relation that holds between two adja-
cent spans of text (see (Carlson et al., 2003) for details).
Our approach to discourse segmentation breaks the prob-
lem further into two sub-problems: sentence segmen-
tation and sentence-level discourse segmentation. The
problem of sentence segmentation has been studied ex-
tensively, and tools such as those described by Palmer
and Hearst (1997) and Ratnaparkhi (1998) can handle it
well. In this section, we present a discourse segmenta-
tion algorithm that deals with segmenting sentences into
elementary discourse units.
</bodyText>
<subsectionHeader confidence="0.999253">
3.1 The Discourse Segmentation Model
</subsectionHeader>
<bodyText confidence="0.999804285714286">
The discourse segmenter proposed here takes as input a
sentence and outputs its elementary discourse unit bound-
aries. Our statistical approach to sentence segmentation
uses two components: a statistical model which assigns
a probability to the insertion of a discourse boundary af-
ter each word in a sentence, and a segmenter, which uses
the probabilities computed by the model for inserting dis-
course boundaries. We first focus on the statistical model.
A good model of discourse segmentation needs to ac-
count both for local interactions at the word level and
for global interactions at more abstract levels. Consider,
for example, the syntactic tree in Figure 2. According
to our hypothesis, the discourse boundary inserted be-
tween the words says and it is best explained not by
the words alone, but by the lexicalized syntactic structure
[VP(says) [VBZ(says) SBAR(will)]], sig-
naled by the boxed nodes in Figure 2. Hence, we hy-
pothesize that the discourse boundary in our example is
best explained by the global interaction between the verb
(the act of saying) and its clausal complement (what is
being said).
</bodyText>
<figure confidence="0.997207964285714">
DT NN RB
ADVP
VP (says)
(use)
VP
NP
VBZ (says)
NP
MD
Nr
S
VP
SBAR (will)
Nw
PRP
VB(use)
TO
(to)
VP
VP
[The bank also says ]1
S
NN
PRP
VB NNS
NP(network)
[it will use its network]2 [to channel investments.]3
VP(passed) VP(priced)
</figure>
<figureCaption confidence="0.922292333333333">
Figure 3: The same syntactic information indicates dis-
course boundaries depending on the lexical heads in-
volved.
</figureCaption>
<bodyText confidence="0.999154346153846">
Given a sentence , we first find
the syntactic parse tree of . We used in our exper-
iments both syntactic parse trees obtained using Char-
niak’s parser (2000) and syntactic parse trees from the
PennTree bank. Our statistical model assigns a segment-
ing probability for each word , where
boundary, no-boundary . Because our model is
concerned with discourse segmentation at sentence level,
we define boundary , i.e., the sentence
boundary is always a discourse boundary as well.
Our model uses both lexical and syntactic features
for determining the probability of inserting discourse
boundaries. We apply canonical lexical head projection
rules (Magerman, 1995) in order to lexicalize syntactic
trees. For each word , the upper-most node with lex-
ical head which has a right sibling node determines
the features on the basis of which we decide whether to
insert a discourse boundary. We denote such node ,
and the features we use are node , its parent , and
the siblings of . In the example in Figure 2, we de-
termine whether to insert a discourse boundary after the
word says using as features node and
its children and .
We use our corpus to estimate the likelihood of inserting
a discourse boundary between word and the next word
using formula (1),
</bodyText>
<equation confidence="0.820602">
(1)
</equation>
<bodyText confidence="0.985508208333333">
where the numerator represents all the counts of the rule
for which a discourse boundary has
been inserted after word , and the denominator repre-
sents all the counts of the rule.
Because we want to account for boundaries that are
motivated lexically as well, the counts used in formula (1)
are defined over lexicalized rules. Without lexicalization,
the syntactic context alone is too general and fails to dis-
tinguish genuine cases of discourse boundaries from in-
correct ones. As can be seen in Figure 3, the same syn-
tactic context may indicate a discourse boundary when
the lexical heads passed and without are present, but
it may not indicate a boundary when the lexical heads
priced and at are present.
The discourse segmentation model uses the corpus pre-
sented in Section 2 in order to estimate probabilities for
inserting discourse boundaries using equation (1). We
also use a simple interpolation method for smoothing lex-
icalized rules to accommodate data sparseness.
Once we have the segmenting probabilities given by
the statistical model, a straightforward algorithm is used
to implement the segmenter. Given a syntactic tree , the
algorithm inserts a boundary after each word for which
boundary .
</bodyText>
<sectionHeader confidence="0.967883" genericHeader="method">
4 The Discourse Parser
</sectionHeader>
<bodyText confidence="0.99903982051282">
In the setting presented here, the input to the discourse
parser is a Discourse Segmented Lexicalized Syntactic
Tree (i.e., a lexicalized syntactic parse tree in which the
discourse boundaries have been identified), henceforth
called a DS-LST. An example of a DS-LST in the tree
in Figure 2. The output of the discourse parser is a dis-
course parse tree, such as the one presented in Figure 1.
As in other statistical approaches, we identify two
components that perform the discourse parsing task. The
first component is the parsing model, which assigns a
probability to every potential candidate parse tree. For-
mally, given a discourse tree and a set of parameters
, the parsing model estimates the conditional probabil-
ity . The most likely parse is then given by
formula (2).
The second component is called the discourse parser, and
it is an algorithm for finding . We first focus on the
parsing model.
A discourse parse tree can be formally represented
as a set of tuples. The discourse tree in Figure 1, for
example, can be formally written as the set of tuples
ATTRIBUTION-SN[1,1,3] ENABLEMENT-NS[2,2,3] . A tu-
ple is of the form , and denotes a discourse rela-
tion that holds between the discourse span that contains
edus through , and the discourse span that contains
edus through . Each relation also signals explic-
itly the nuclearity assignment, which can be NUCLEUS-
SATELLITE (NS), SATELLITE-NUCLEUS (SN), or NUCLEUS-
NUCLEUS (NN). This notation assumes that all relations
are binary relations. The assumption is justified empiri-
cally: 99% of the nodes of the discourse trees in our cor-
pus are binary nodes. Using only binary relations makes
our discourse model easier to build and reason with.
In what follows we make use of two functions: func-
tion applied to a tuple yields the discourse
relation ; function applied to a tuple yields
the structure . Given a set of adequate parameters
, our discourse model estimates the goodness of a dis-
course parse tree using formula (3).
</bodyText>
<equation confidence="0.9957755">
VBN(passed)
PP(without)
VBN(priced)
PP(at)
</equation>
<bodyText confidence="0.987709615384615">
ce set extracted from a DS-LST.
goodness of the structure of
We expect these proba-
bilities to prefer the hierarchical structure (1, (2, 3)) over
((1,2), 3) for the discourse tree in Figure 1. For each tu-
ple ,the probability estimates the goodness
of the discourse relation of . We expect these probabili-
ties to prefer the rhetorical relation ATTRIBUTION-NS
.
over
CONTRAST-NN for the relation between spans 1 and
in the discourse tree in Figure 1. The overall probability
of a discourse tree is obtained multiplying the structural
probabilities and the relational probabilities for all
the tuples in the discourse tree.
Our discourse model uses as the information present
in the input DS-LST. However, given such a tree
as input, one cannot estimate probabilities such as
without running into a severe sparseness
problem. To overcome this, we map the input DS-LST
into a more abstract representation that contains only the
salient features of the DS-LST. This mapping leads to the
notion of a dominance set over a discourse segmented
lexicalized syntactic tree. In what follows, we define this
notion and show that it provides adequate parameteriza-
tion for the discourse parsing problem.
</bodyText>
<subsectionHeader confidence="0.966305">
4.1 The Dominance Set of a DS-LST
</subsectionHeader>
<bodyText confidence="0.979373595238095">
that such
points in the structure of a DS-
LST (the boxed nodes in the tree in Figure 4) carry the
most indicative information with respect to the potential
discourse tree we want to build. A set representation of
the
points of a DS-LST is called the domi-
“attachment”
“attachment”
ce set of a DS-LST.
For each edu we identify a word in as the head
word of edu and denote it
is defined as the word
with the highest occurrence as a lexical head in the lexi-
calized tree among all
.
the words in .The node in which
occurs highest is called the head node of edu and is
denoted . The edu which has as head node the root of
the DS-LST is called the exception edu. In our example,
the head word for edu 2 is , and its head node is
; the head word for edu 3 is ,
and its head node is . The exception edu is
edu 1.
For each edu which is not the exception edu, there
exists a node which is the parent of the head node of ,
and the lexical head of this node is guaranteed to belong
to a different edu than , call it . We call this node the
attachment node of and denote it . In our example,
the attachment node of edu 2 is , and
its lexical head says belongs to edu 1; the attachment
node of edu 3 is , and its lexical head use
belongs to edu 2. We write formally that two edus and
are linked through a head node and an attachment
node as .
The dominance set of a DS-LST is given by all the
edu pairs linked through a head node and an attachment
node in the DS-LST. Each element in the dominance set
represents a dominance relationship between the edus in-
volved. Figure 4 shows the dominance set for our ex-
ample DS-LST. We say that edu 2 is dominated by edu 1
(shortly written ), and edu 3 is dominated by edu 2
</bodyText>
<subsectionHeader confidence="0.88307">
4.2 The Discourse Model
</subsectionHeader>
<bodyText confidence="0.984156">
Our discourse parsing model uses the dominan
( ).
ce set
of a DS-LST as the conditioning parameter in equa-
tion (3). The discourse parsing model we propose uses
the dominance set to compute the probability of a dis-
course parse tree according to formula (4).
</bodyText>
<sectionHeader confidence="0.284086" genericHeader="method">
(4)
</sectionHeader>
<subsectionHeader confidence="0.65978">
Different projections of are used to accurately estimate
</subsectionHeader>
<bodyText confidence="0.9811868">
the structure probabilities and the relation probabili-
ties associated with a tuple in a discourse tree. The
projection functions and ensure that, for
each tuple , only the information in relevant to
is to be conditioned upon. In the case of (the prob-
ability of the structure ), we filter out the lexical
heads and keep only the syntactic labels; also, we filter
out all the elements of which do not have at least one
edu inside the span of . In our running example, for in-
stance, for ENABLEMENT-NS
</bodyText>
<figure confidence="0.9927546">
[The bank also says]1
PRP
NA
VB
NNS
</figure>
<figureCaption confidence="0.999944">
Figure 4: Dominan
</figureCaption>
<bodyText confidence="0.940835444444445">
For each tuple ,the probability estimates the
The dominance set of a DS-LST contains feature repre-
sentations of a discourse segmented lexicalized syntactic
tree. Each feature is a representation of the syntactic and
lexical information that is found at the point where two
edus are joined together in a DS-LST. Our hypothesis is
nan
. The span
of is ,and set has two elements involving edus
</bodyText>
<figure confidence="0.940175804878049">
D = {(2, SBAR(will)) &lt; (1, VP(says)) , (3, S(to)) &lt; (2, VP(use)) }
DT NN RB
NP ADVP
H =says
S(says)
VBZ
VP (says)
(says)
NP
MD(will)
NA
SBAR(will)
VB(use)
H = will
S(will)
VP(will)
PRP NN
[it will use its
network]2
NH
NP
VP
(use)
TO(to) VP
[to
channelinvestments.]3
S
(to)
VP
(to)
NH
H = to
,
1
[ The bank also says] 1
S1 = 1
2
3
ENABLEMENT
[ it will use its network] [ to channel investments. ]
2 3
</figure>
<equation confidence="0.9972924375">
P ( [2,2,3]  |(2,SBAR)&lt;(1,VP), (3,S)&lt;(2,VP) ) =
s
P ( ENABLEMENT−NS  |S(to)&lt;VP(use) ) =
r
S2 = P * P r = 0.40
s
[2,3]
ENABLEMENT
2
[ it will use its network] [ to channel investments. ]
2 3
P ( [1,1,3]  |(2, SBAR) &lt; (1, VP), (3, S) &lt; (2, VP) ) = 0.37
s
P ( ATTRIBUTION−SN  |SBAR(will) &lt; VP(says) ) = 0.009
r
Score1 = S1*S2* Ps * P r = 0.001
</equation>
<figureCaption confidence="0.996732">
Figure 5: Bottom-up discourse parsing.
</figureCaption>
<bodyText confidence="0.99697512">
from it, namely the dominance relationships and
. To decide the appropriate structure, keeps
them both; this is because a different dominance relation-
ship between edus 1 and 2, namely , would most
likely influence the structure probability of .
In the case of (the probability of the relation ),
we keep both the lexical heads and the syntactic la-
bels, but filter out the edu identifiers (clearly, the rela-
tion between two spans does not depend on the posi-
tions of the spans involved); also, we filter out all the
elements of whose dominance relationship does not
hold across the two sub-spans of . In our running ex-
ample, for ENABLEMENT-NS ,
. The two sub-spans of are
and , and only the dominance relationship
holds across these spans; the other dominance relation-
ship in , , does not influence the choice for the
relation label of .
The conditional probabilities involved in equation (4)
are estimated from the training corpus using maximum
likelihood estimation. A simple interpolation method is
used for smoothing to accommodate data sparseness. The
counts for the dependency sets are also smoothed using
symbolic names for the edu identifiers and accounting
only for the distance between them.
</bodyText>
<subsectionHeader confidence="0.997781">
4.3 The Discourse Parser
</subsectionHeader>
<bodyText confidence="0.999983235294118">
Our discourse parser implements a classical bottom-up
algorithm. The parser searches through the space of
all legal discourse parse trees and uses a dynamic pro-
gramming algorithm. If two constituents are derived for
the same discourse span, then the constituent for which
the model assigns a lower probability can be safely dis-
carded.
Figure 5 shows a discourse structure created in a
bottom-up manner for the DS-LST in Figure 2. Tu-
ple ENABLEMENT-NS[2,2,3] has a score of 0.40, obtained
ATTRIBUTION-SN[1,1,3] has a score of 0.37 for the struc-
ture, and a score of 0.009 for the relation. The final score
for the entire discourse structure is 0.001. All probabil-
ities used were estimated from our training corpus. Ac-
cording to our discourse model, the discourse structure in
Figure 5 is the most likely among all the legal discourse
structures for our example sentence.
</bodyText>
<sectionHeader confidence="0.997093" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999801166666667">
In this section we present the evaluations carried out for
both the discourse segmentation task and the discourse
parsing task. For this evaluation, we re-trained Char-
niak’s parser (2000) such that the test sentences from the
discourse corpus were not seen by the syntactic parser
during training.
</bodyText>
<subsectionHeader confidence="0.996044">
5.1 Evaluation of the Discourse Segmenter
</subsectionHeader>
<bodyText confidence="0.9997325625">
We train our discourse segmenter on the Training sec-
tion of the corpus described in Section 2, and test it on
the Test section. The training regime uses syntactic trees
from the Penn Treebank. The metric we use to evalu-
ate the discourse segmenter records the accuracy of the
discourse segmenter with respect to its ability to insert
inside-sentence discourse boundaries. That is, if a sen-
tence has 3 edus, which correspond to 2 inside-sentence
discourse boundaries, we measure the ability of our al-
gorithm to correctly identify these 2 boundaries. We re-
port our evaluation results using recall, precision, and F-
score figures. This metric is harsher than the metric pre-
viously used by Marcu (2000), who assesses the perfor-
mance of a discourse segmentation algorithm by count-
ing how often the algorithm makes boundary and no-
boundary decisions for every word in a sentence.
We compare the performance of our probabilistic dis-
course segmenter with the performance of the decision-
based segmenter proposed by (Marcu, 2000) and the per-
formance of two baseline algorithms. The first base-
line ( ) uses punctuation to determine when to in-
sert a boundary; because commas are often used to in-
dicate breaks inside long sentences, inserts dis-
course boundaries after each comma. The second base-
line ( ) uses syntactic information; because long
sentences often have embedded sentences, in-
serts discourse boundaries after each text span whose
corresponding syntactic subtree is labeled S, SBAR, or
STNV. We also compute the agreement between human
annotators on the discourse segmentation task ( ),
using the doubly-annotated discourse corpus mentioned
in Section 2.
</bodyText>
<figure confidence="0.925294428571428">
ATTRIBUTION
1
[ The bank also says] 1
3
as the product between the structure probability of
0.47 0.47 and the relation probability of 0.88. Tuple
0.88
</figure>
<table confidence="0.991709428571429">
Recall Precision F-score
28.2 37.1 32.0
25.4 64.9 36.5
77.1 83.3 80.1
82.7 83.5 83.1
85.4 84.1 84.7
98.2 98.5 98.3
</table>
<tableCaption confidence="0.9695245">
Table 1: Discourse segmenter evaluation
Table 1 shows the results obtained by the algorithm
</tableCaption>
<bodyText confidence="0.999782416666667">
described in this paper ( ) using syntactic
trees produced by Charniak’s parser (2000), in com-
parison with the results obtained by the algorithm de-
scribed in (Marcu, 2000) ( ), and baseline algo-
rithms and , on the same test set. Cru-
cial to the performance of the discourse segmenter is
the recall figure, because we want to find as many dis-
course boundaries as possible. The baseline algorithms
are too simplistic to yield good results (recall figures of
28.2% and 25.4%). The algorithm presented in this pa-
per gives an error reduction in missed discourse bound-
aries of 24.5% (recall accuracy improvement from 77.1%
to 82.7%) over (Marcu, 2000). The overall error reduc-
tion is of 15.1% (improvement in F-score from 80.1% to
83.1%).
In order to asses the impact on the performance of
the discourse segmenter due to incorrect syntactic parse
trees, we also carry an evaluation using syntactic trees
from the Penn Treebank. The results are shown in row
. Perfect syntactic trees lead to a further er-
ror reduction of 9.5% (F-score improvement from 83.1%
to 84.7%). The performance ceiling for discourse seg-
mentation is given by the human annotation agreement
F-score of 98.3%.
</bodyText>
<subsectionHeader confidence="0.999567">
5.2 Evaluation of the Discourse Parser
</subsectionHeader>
<bodyText confidence="0.9880766875">
We train our discourse parsing model on the Training sec-
tion of the corpus described in Section 2, and test it on
the Test section. The training regime uses syntactic trees
from the Penn Treebank. The performance is assessed us-
ing labeled recall and labeled precision as defined by the
standard Parseval metric (Black et al., 1991). As men-
tioned in Section 2, we use both 18 labels and 110 la-
bels for the discourse relations. The recall and precision
figures are combined into an F-score figure in the usual
manner.
The discourse parsing model uses syntactic trees pro-
duced by Charniak’s parser (2000) and discourse seg-
ments produced by the algorithm described in Section 3.
We compare the performance of our model ( )
with the performance of the decision-based discourse
parsing model ( ) proposed by (Marcu, 2000), and
</bodyText>
<table confidence="0.997321666666667">
Unlabeled 64.0 67.0 70.5 92.8
18 Labels 23.4 37.2 49.0 77.0
110 Labels 20.7 35.5 45.6 71.9
</table>
<tableCaption confidence="0.976439">
Table 2: performance compared to baseline,
state-of-the-art, and human performance
</tableCaption>
<table confidence="0.999610333333333">
Unlabeled 70.5 73.0 92.8 96.2
18 Labels 49.0 56.4 63.8 75.5
110 Labels 45.6 52.6 59.5 70.3
</table>
<tableCaption confidence="0.8827025">
Table 3: performance with human-level accu-
racy for syntactic trees and discourse boundaries.
</tableCaption>
<bodyText confidence="0.999056859375">
with the performance of a baseline algorithm ( ).
The baseline algorithm builds right-branching discourse
trees labeled with the most frequent relation encountered
in the training set (i.e., ELABORATION-NS). We also com-
pute the agreement between human annotators on the dis-
course parsing task ( ), using the doubly-annotated
discourse corpus mentioned in Section 2. The results are
shown in Table 2. The baseline algorithm has a perfor-
mance of 23.4% and 20.7% F-score, when using 18 la-
bels and 110 labels, respectively. Our algorithm has a
performance of 49.0% and 45.6% F-score, when using
18 labels and 110 labels, respectively. These results rep-
resent an error reduction of 18.8% (F-score improvement
from 37.2% to 49.0%) over a state-of-the-art discourse
parser (Marcu, 2000) when using 18 labels, and an error
reduction of 15.7% (F-score improvement from 35.5% to
45.6%) when using 110 labels. The performance ceiling
for sentence-level discourse structure derivation is given
by the human annotation agreement F-score of 77.0% and
71.9%, when using 18 labels and 110 labels, respectively.
The performance gap between the results of and
human agreement is still large, and it can be attributed
to three possible causes: errors made by the syntactic
parser, errors made by the discourse segmenter, and the
weakness of our discourse model.
In order to quantitatively asses the impact in perfor-
mance of each possible cause of error, we perform further
experiments. We replace the syntactic parse trees pro-
duced by Charniak’s parser at 90% accuracy ( ) with
the corresponding Penn Treebank syntactic parse trees
produced by human annotators ( ). We also replace
the discourse boundaries produced by our discourse seg-
menter at 83% accuracy ( ) with the discourse bound-
aries taken from (RST-DT, 2002), which are produced by
the human annotators ( ).
The results are shown in Table 3. The results in col-
umn show that using perfect syntactic trees leads
to an error reduction of 14.5% (F-score improvement
from 49.0% to 56.4%) when using 18 labels, and an error
reduction of 12.9% (F-score improvement from 45.6%
to 52.6%) when using 110 labels. The results in col-
umn show that the impact of perfect discourse
segmentation is double the impact of perfect syntactic
trees. Human-level performance on discourse segmen-
tation leads to an error reduction of 29.0% (F-score im-
provement from 49.0% to 63.8%) when using 18 labels,
and an error reduction of 25.6% (F-score improvement
from 45.6% to 59.5%) when using 110 labels. Together,
perfect syntactic trees and perfect discourse segmentation
lead to an error reduction of 52.0% (F-score improvement
from 49.0% to 75.5%) when using 18 labels, and an error
reduction of 45.5% (F-score improvement from 45.6% to
70.3%) when using 110 labels. The results in column
in Table 3 compare extremely favorable with the
results in column in Table 2. The discourse parsing
model produces unlabeled discourse structure at a per-
formance level similar to human annotators (F-score of
96.2%). When using 18 labels, the distance between our
discourse parsing model performance level and human
annotators performance level is of absolute 1.5% (75.5%
versus 77%). When using 110 labels, the distance is of
absolute 1.6% (70.3% versus 71.9%). Our evaluation
shows that our discourse model is sophisticated enough
to match near-human levels of performance.
</bodyText>
<sectionHeader confidence="0.999359" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999681875">
In this paper, we have introduced a discourse parsing
model that uses syntactic and lexical features to estimate
the adequacy of sentence-level discourse structures. Our
model defines and exploits a set of syntactically moti-
vated lexico-grammatical dominance relations that fall
naturally from a syntactic representation of sentences.
The most interesting finding is that these dominance
relations encode sufficient information to enable the
derivation of discourse structures that are almost indis-
tinguishable from those built by human annotators. Our
experiments empirically show that, at the sentence level,
there is an extremely strong correlation between syntax
and discourse. This is even more remarkable given that
the discourse corpus (RST-DT, 2002) was built with no
syntactic theory in mind. The annotators used by Carlson
et al. (2003) were not instructed to build discourse trees
that were consistent with the syntax of the sentences. Yet,
they built discourse structures at sentence level that are
not only consistent with the syntactic structures of sen-
tences, but also derivable from them.
Recent work on Tree Adjoining Grammar-based lexi-
calized models of discourse (Forbes et al., 2001) has al-
ready shown how to exploit within a single framework
lexical, syntactic, and discourse cues. Various linguis-
tics studies have also shown how intertwined syntax and
discourse are (Maynard, 1998). However, to our knowl-
edge, this is the first paper that empirically shows that the
connection between syntax and discourse can be compu-
tationally exploited at high levels of accuracy on open
domain, newspaper text.
Another interesting finding is that the performance of
current state-of-the-art syntactic parsers (Charniak, 2000)
is not a bottleneck for coming up with a good solution
to the sentence-level discourse parsing problem. Little
improvement comes from using manually built syntactic
parse trees instead of automatically derived trees. How-
ever, experiments show that there is much to be gained if
better discourse segmentation algorithms are found; 83%
accuracy on this task is not sufficient for building highly
accurate discourse trees.
We believe that semantic/discourse segmentation is
a notoriously under-researched problem. For example,
Gildea and Jurafsky (2002) present a semantic parser that
optimistically assumes that has access to perfect seman-
tic segments. Our results suggest that more effort needs
to be put on semantic/discourse-based segmentation. Im-
provements in this area will have a significant impact on
both semantic and discourse parsing.
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999921423728814">
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitatively comparing the syntactic cover-
age of English grammars. In Proceedings of Speech
and Natural Language Workshop, pages 306–311, Pa-
cific Groove, CA. DARPA.
L. Carlson, D. Marcu, and M. E. Okurowski. 2003.
Building a discourse-tagged corpus in the framework
of Rhetorical Structure Theory. In Jan van Kuppevelt
and Ronnie Smith, editors, Current Directions in Dis-
course and Dialogue. Kluwer Academic Publishers. To
appear.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the NAACL 2000, pages 132–
139, Seattle, Washington, April 29 – May 3.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of ICML 2000,
Stanford University, Palo Alto, CA, June 29–July 2.
C. J. Fillmore, C. F. Baker, and S. Hiroaki. 2002. The
framenet database and software tools. In Proceedings
of the LREC 2002, pages 1157–1160, LREC.
K. Forbes, E. Miltsakaki, R. Prasad, A. Sarkar, A. Joshi,
and B. Webber. 2001. D-LTAG System: Discourse
parsing with a lexicalized tree-adjoining grammar. In
ESSLLI’2001 Workshop on Information Structure, Dis-
course Structure and Discourse Semantics.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic role. Computational Linguistics,
28(3):245–288.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to Propbank. In Proceedings of the LREC 2002,
Las Palmas, Canary Islands, Spain, May 28-June 3.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the ACL 1995,
pages 276–283, Cambridge, Massachusetts, June 26-
30.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. The MIT Press,
Cambridge, Massachusetts.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313–330.
Senko K. Maynard. 1998. Principles of Japanese Dis-
course: A Handbook. Cambridge University Press.
David D. Palmer and Marti A. Hearst. 1997. Adaptive
multilingual sentence boundary disambiguation. Com-
putational Linguistics, 23(2):241–269, June.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania.
RST-DT. 2002. RST Discourse Tree-
bank. Linguistic Data Consortium.
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2002T07.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.520449">
<note confidence="0.958525333333333">Proceedings of HLT-NAACL 2003 Main Papers , pp. 149-156 Edmonton, May-June 2003</note>
<title confidence="0.993848">Sentence Level Discourse Parsing using Syntactic and Lexical Information</title>
<author confidence="0.955922">Radu Soricut</author>
<author confidence="0.955922">Daniel</author>
<affiliation confidence="0.9993225">Information Sciences University of Southern</affiliation>
<address confidence="0.996048">4676 Admiralty Way, Suite</address>
<author confidence="0.627064">Marina del Rey</author>
<author confidence="0.627064">CA</author>
<email confidence="0.998509">radu,marcu@isi.edu</email>
<abstract confidence="0.999725461538461">We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees. The models use syntactic and lexical features. A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-ofthe-art decision-based discourse parser. A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickinger</author>
<author>C Gdaniec</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of Speech and Natural Language Workshop,</booktitle>
<pages>306--311</pages>
<publisher>DARPA.</publisher>
<location>Pacific Groove, CA.</location>
<contexts>
<context position="24215" citStr="Black et al., 1991" startWordPosition="4068" endWordPosition="4071"> results are shown in row . Perfect syntactic trees lead to a further error reduction of 9.5% (F-score improvement from 83.1% to 84.7%). The performance ceiling for discourse segmentation is given by the human annotation agreement F-score of 98.3%. 5.2 Evaluation of the Discourse Parser We train our discourse parsing model on the Training section of the corpus described in Section 2, and test it on the Test section. The training regime uses syntactic trees from the Penn Treebank. The performance is assessed using labeled recall and labeled precision as defined by the standard Parseval metric (Black et al., 1991). As mentioned in Section 2, we use both 18 labels and 110 labels for the discourse relations. The recall and precision figures are combined into an F-score figure in the usual manner. The discourse parsing model uses syntactic trees produced by Charniak’s parser (2000) and discourse segments produced by the algorithm described in Section 3. We compare the performance of our model ( ) with the performance of the decision-based discourse parsing model ( ) proposed by (Marcu, 2000), and Unlabeled 64.0 67.0 70.5 92.8 18 Labels 23.4 37.2 49.0 77.0 110 Labels 20.7 35.5 45.6 71.9 Table 2: performanc</context>
</contexts>
<marker>Black, Abney, Flickinger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. In Proceedings of Speech and Natural Language Workshop, pages 306–311, Pacific Groove, CA. DARPA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
<author>M E Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory.</title>
<date>2003</date>
<booktitle>Current Directions in Discourse and Dialogue.</booktitle>
<editor>In Jan van Kuppevelt and Ronnie Smith, editors,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<note>To appear.</note>
<contexts>
<context position="1443" citStr="Carlson et al., 2003" startWordPosition="200" endWordPosition="203"> an accuracy level that matches near-human levels of performance. 1 Introduction By exploiting information encoded in human-produced syntactic trees (Marcus et al., 1993), research on probabilistic models of syntax has driven the performance of syntactic parsers to about 90% accuracy (Charniak, 2000; Collins, 2000). The absence of semantic and discourse annotated corpora prevented similar developments in semantic/discourse parsing. Fortunately, recent annotation projects have taken significant steps towards developing semantic (Fillmore et al., 2002; Kingsbury and Palmer, 2002) and discourse (Carlson et al., 2003) annotated corpora. Some of these annotation efforts have already had a computational impact. For example, Gildea and Jurafsky (2002) developed statistical models for automatically inducing semantic roles. In this paper, we describe probabilistic models and algorithms that exploit the discourseannotated corpus produced by Carlson et al. (2003). A discourse structure is a tree whose leaves correspond to elementary discourse units (edu)s, and whose internal nodes correspond to contiguous text spans (called discourse spans). An example of a discourse structure is the tree given in Figure 1. Each </context>
<context position="4040" citStr="Carlson et al., 2003" startWordPosition="615" endWordPosition="618">e trees, when working with human-produced syntactic trees and discourse segments. 2 The Corpus For the experiments described in this paper, we use a publicly available corpus (RST-DT, 2002) that contains 385 [2,3] ATTRIBUTION 1 3 3 Wall Street Journal articles from the Penn Treebank. The corpus comes conveniently partitioned into a Training set of 347 articles (6132 sentences) and a Test set of 38 articles (991 sentences). Each document in the corpus is paired with a discourse structure (tree) that was manually built in the style of Rhetorical Structure Theory (Mann and Thompson, 1988). (See (Carlson et al., 2003) for details concerning the corpus and the annotation process.) Out of the 385 articles in the corpus, 53 have been independently annotated by two human annotators. We used this doubly-annotated subset to compute human agreement on the task of discourse structure derivation. In our experiments we used as discourse structures only the discourse sub-trees spanning over individual sentences. Because the discourse structures had been built on top of sentences already associated with syntactic trees from the Penn Treebank, we were able to create a composite corpus which allowed us to perform an emp</context>
<context position="5760" citStr="Carlson et al. (2003)" startWordPosition="897" endWordPosition="900">associated with each sentence . The remaining 5% of the sentences cannot be used in our approach, as no well-formed discourse tree can be associated with these sentences. Therefore, our Training section consists of a set of 5809 triples of the form which are used to train the parameters of the statistical models. Our Test section consists of a set of 946 triples of a similar form, which are used to evaluate the performance of our discourse parser. The (RST-DT, 2002) corpus uses 110 different rhetorical relations. We found it useful to also compact these relations into classes, as described by Carlson et al. (2003), and operate with the resulting 18 labels as well (seen as coarser granularity rhetorical relations). Operating with different levels of granularity allows one to get deeper insight into the difficulties of assigning the appropriate rhetorical relation, if any, to two adjacent text spans. 3 The Discourse Segmenter We break down the problem of building sentence-level discourse trees into two sub-problems: discourse segmentation and discourse parsing. Discourse segmentation is covered by this section, while discourse parsing is covered by Section 4. S Np Figure 2: Discourse segmentation using l</context>
<context position="29302" citStr="Carlson et al. (2003)" startWordPosition="4877" endWordPosition="4880">ated lexico-grammatical dominance relations that fall naturally from a syntactic representation of sentences. The most interesting finding is that these dominance relations encode sufficient information to enable the derivation of discourse structures that are almost indistinguishable from those built by human annotators. Our experiments empirically show that, at the sentence level, there is an extremely strong correlation between syntax and discourse. This is even more remarkable given that the discourse corpus (RST-DT, 2002) was built with no syntactic theory in mind. The annotators used by Carlson et al. (2003) were not instructed to build discourse trees that were consistent with the syntax of the sentences. Yet, they built discourse structures at sentence level that are not only consistent with the syntactic structures of sentences, but also derivable from them. Recent work on Tree Adjoining Grammar-based lexicalized models of discourse (Forbes et al., 2001) has already shown how to exploit within a single framework lexical, syntactic, and discourse cues. Various linguistics studies have also shown how intertwined syntax and discourse are (Maynard, 1998). However, to our knowledge, this is the fir</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2003</marker>
<rawString>L. Carlson, D. Marcu, and M. E. Okurowski. 2003. Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory. In Jan van Kuppevelt and Ronnie Smith, editors, Current Directions in Discourse and Dialogue. Kluwer Academic Publishers. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the NAACL</booktitle>
<pages>132--139</pages>
<location>Seattle, Washington,</location>
<contexts>
<context position="1122" citStr="Charniak, 2000" startWordPosition="160" endWordPosition="161">ical features. A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-ofthe-art decision-based discourse parser. A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance. 1 Introduction By exploiting information encoded in human-produced syntactic trees (Marcus et al., 1993), research on probabilistic models of syntax has driven the performance of syntactic parsers to about 90% accuracy (Charniak, 2000; Collins, 2000). The absence of semantic and discourse annotated corpora prevented similar developments in semantic/discourse parsing. Fortunately, recent annotation projects have taken significant steps towards developing semantic (Fillmore et al., 2002; Kingsbury and Palmer, 2002) and discourse (Carlson et al., 2003) annotated corpora. Some of these annotation efforts have already had a computational impact. For example, Gildea and Jurafsky (2002) developed statistical models for automatically inducing semantic roles. In this paper, we describe probabilistic models and algorithms that explo</context>
<context position="30189" citStr="Charniak, 2000" startWordPosition="5015" endWordPosition="5016">n Tree Adjoining Grammar-based lexicalized models of discourse (Forbes et al., 2001) has already shown how to exploit within a single framework lexical, syntactic, and discourse cues. Various linguistics studies have also shown how intertwined syntax and discourse are (Maynard, 1998). However, to our knowledge, this is the first paper that empirically shows that the connection between syntax and discourse can be computationally exploited at high levels of accuracy on open domain, newspaper text. Another interesting finding is that the performance of current state-of-the-art syntactic parsers (Charniak, 2000) is not a bottleneck for coming up with a good solution to the sentence-level discourse parsing problem. Little improvement comes from using manually built syntactic parse trees instead of automatically derived trees. However, experiments show that there is much to be gained if better discourse segmentation algorithms are found; 83% accuracy on this task is not sufficient for building highly accurate discourse trees. We believe that semantic/discourse segmentation is a notoriously under-researched problem. For example, Gildea and Jurafsky (2002) present a semantic parser that optimistically as</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the NAACL 2000, pages 132– 139, Seattle, Washington, April 29 – May 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML 2000,</booktitle>
<institution>Stanford University,</institution>
<location>Palo Alto, CA,</location>
<contexts>
<context position="1138" citStr="Collins, 2000" startWordPosition="162" endWordPosition="163"> discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-ofthe-art decision-based discourse parser. A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance. 1 Introduction By exploiting information encoded in human-produced syntactic trees (Marcus et al., 1993), research on probabilistic models of syntax has driven the performance of syntactic parsers to about 90% accuracy (Charniak, 2000; Collins, 2000). The absence of semantic and discourse annotated corpora prevented similar developments in semantic/discourse parsing. Fortunately, recent annotation projects have taken significant steps towards developing semantic (Fillmore et al., 2002; Kingsbury and Palmer, 2002) and discourse (Carlson et al., 2003) annotated corpora. Some of these annotation efforts have already had a computational impact. For example, Gildea and Jurafsky (2002) developed statistical models for automatically inducing semantic roles. In this paper, we describe probabilistic models and algorithms that exploit the discourse</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of ICML 2000, Stanford University, Palo Alto, CA, June 29–July 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
<author>C F Baker</author>
<author>S Hiroaki</author>
</authors>
<title>The framenet database and software tools.</title>
<date>2002</date>
<booktitle>In Proceedings of the LREC 2002,</booktitle>
<pages>1157--1160</pages>
<contexts>
<context position="1377" citStr="Fillmore et al., 2002" startWordPosition="190" endWordPosition="193"> parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance. 1 Introduction By exploiting information encoded in human-produced syntactic trees (Marcus et al., 1993), research on probabilistic models of syntax has driven the performance of syntactic parsers to about 90% accuracy (Charniak, 2000; Collins, 2000). The absence of semantic and discourse annotated corpora prevented similar developments in semantic/discourse parsing. Fortunately, recent annotation projects have taken significant steps towards developing semantic (Fillmore et al., 2002; Kingsbury and Palmer, 2002) and discourse (Carlson et al., 2003) annotated corpora. Some of these annotation efforts have already had a computational impact. For example, Gildea and Jurafsky (2002) developed statistical models for automatically inducing semantic roles. In this paper, we describe probabilistic models and algorithms that exploit the discourseannotated corpus produced by Carlson et al. (2003). A discourse structure is a tree whose leaves correspond to elementary discourse units (edu)s, and whose internal nodes correspond to contiguous text spans (called discourse spans). An exa</context>
</contexts>
<marker>Fillmore, Baker, Hiroaki, 2002</marker>
<rawString>C. J. Fillmore, C. F. Baker, and S. Hiroaki. 2002. The framenet database and software tools. In Proceedings of the LREC 2002, pages 1157–1160, LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes</author>
<author>E Miltsakaki</author>
<author>R Prasad</author>
<author>A Sarkar</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>D-LTAG System: Discourse parsing with a lexicalized tree-adjoining grammar.</title>
<date>2001</date>
<booktitle>In ESSLLI’2001 Workshop on Information Structure, Discourse Structure and Discourse Semantics.</booktitle>
<contexts>
<context position="29658" citStr="Forbes et al., 2001" startWordPosition="4933" endWordPosition="4936">that, at the sentence level, there is an extremely strong correlation between syntax and discourse. This is even more remarkable given that the discourse corpus (RST-DT, 2002) was built with no syntactic theory in mind. The annotators used by Carlson et al. (2003) were not instructed to build discourse trees that were consistent with the syntax of the sentences. Yet, they built discourse structures at sentence level that are not only consistent with the syntactic structures of sentences, but also derivable from them. Recent work on Tree Adjoining Grammar-based lexicalized models of discourse (Forbes et al., 2001) has already shown how to exploit within a single framework lexical, syntactic, and discourse cues. Various linguistics studies have also shown how intertwined syntax and discourse are (Maynard, 1998). However, to our knowledge, this is the first paper that empirically shows that the connection between syntax and discourse can be computationally exploited at high levels of accuracy on open domain, newspaper text. Another interesting finding is that the performance of current state-of-the-art syntactic parsers (Charniak, 2000) is not a bottleneck for coming up with a good solution to the senten</context>
</contexts>
<marker>Forbes, Miltsakaki, Prasad, Sarkar, Joshi, Webber, 2001</marker>
<rawString>K. Forbes, E. Miltsakaki, R. Prasad, A. Sarkar, A. Joshi, and B. Webber. 2001. D-LTAG System: Discourse parsing with a lexicalized tree-adjoining grammar. In ESSLLI’2001 Workshop on Information Structure, Discourse Structure and Discourse Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic role.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1576" citStr="Gildea and Jurafsky (2002)" startWordPosition="220" endWordPosition="224">uced syntactic trees (Marcus et al., 1993), research on probabilistic models of syntax has driven the performance of syntactic parsers to about 90% accuracy (Charniak, 2000; Collins, 2000). The absence of semantic and discourse annotated corpora prevented similar developments in semantic/discourse parsing. Fortunately, recent annotation projects have taken significant steps towards developing semantic (Fillmore et al., 2002; Kingsbury and Palmer, 2002) and discourse (Carlson et al., 2003) annotated corpora. Some of these annotation efforts have already had a computational impact. For example, Gildea and Jurafsky (2002) developed statistical models for automatically inducing semantic roles. In this paper, we describe probabilistic models and algorithms that exploit the discourseannotated corpus produced by Carlson et al. (2003). A discourse structure is a tree whose leaves correspond to elementary discourse units (edu)s, and whose internal nodes correspond to contiguous text spans (called discourse spans). An example of a discourse structure is the tree given in Figure 1. Each internal node in a discourse tree is characterized by a rhetorical relation, such [ The bank also says ] 1 ENABLEMENT 2 [ it will use</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic role. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>From Treebank to Propbank.</title>
<date>2002</date>
<booktitle>In Proceedings of the LREC 2002,</booktitle>
<pages>3</pages>
<location>Las Palmas, Canary Islands, Spain,</location>
<contexts>
<context position="1406" citStr="Kingsbury and Palmer, 2002" startWordPosition="194" endWordPosition="197">sticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance. 1 Introduction By exploiting information encoded in human-produced syntactic trees (Marcus et al., 1993), research on probabilistic models of syntax has driven the performance of syntactic parsers to about 90% accuracy (Charniak, 2000; Collins, 2000). The absence of semantic and discourse annotated corpora prevented similar developments in semantic/discourse parsing. Fortunately, recent annotation projects have taken significant steps towards developing semantic (Fillmore et al., 2002; Kingsbury and Palmer, 2002) and discourse (Carlson et al., 2003) annotated corpora. Some of these annotation efforts have already had a computational impact. For example, Gildea and Jurafsky (2002) developed statistical models for automatically inducing semantic roles. In this paper, we describe probabilistic models and algorithms that exploit the discourseannotated corpus produced by Carlson et al. (2003). A discourse structure is a tree whose leaves correspond to elementary discourse units (edu)s, and whose internal nodes correspond to contiguous text spans (called discourse spans). An example of a discourse structure</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2002. From Treebank to Propbank. In Proceedings of the LREC 2002, Las Palmas, Canary Islands, Spain, May 28-June 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>276--283</pages>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="9390" citStr="Magerman, 1995" startWordPosition="1477" endWordPosition="1478">tic parse tree of . We used in our experiments both syntactic parse trees obtained using Charniak’s parser (2000) and syntactic parse trees from the PennTree bank. Our statistical model assigns a segmenting probability for each word , where boundary, no-boundary . Because our model is concerned with discourse segmentation at sentence level, we define boundary , i.e., the sentence boundary is always a discourse boundary as well. Our model uses both lexical and syntactic features for determining the probability of inserting discourse boundaries. We apply canonical lexical head projection rules (Magerman, 1995) in order to lexicalize syntactic trees. For each word , the upper-most node with lexical head which has a right sibling node determines the features on the basis of which we decide whether to insert a discourse boundary. We denote such node , and the features we use are node , its parent , and the siblings of . In the example in Figure 2, we determine whether to insert a discourse boundary after the word says using as features node and its children and . We use our corpus to estimate the likelihood of inserting a discourse boundary between word and the next word using formula (1), (1) where t</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David M. Magerman. 1995. Statistical decision-tree models for parsing. In Proceedings of the ACL 1995, pages 276–283, Cambridge, Massachusetts, June 26-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="4011" citStr="Mann and Thompson, 1988" startWordPosition="610" endWordPosition="613">deriving sentence-level discourse trees, when working with human-produced syntactic trees and discourse segments. 2 The Corpus For the experiments described in this paper, we use a publicly available corpus (RST-DT, 2002) that contains 385 [2,3] ATTRIBUTION 1 3 3 Wall Street Journal articles from the Penn Treebank. The corpus comes conveniently partitioned into a Training set of 347 articles (6132 sentences) and a Test set of 38 articles (991 sentences). Each document in the corpus is paired with a discourse structure (tree) that was manually built in the style of Rhetorical Structure Theory (Mann and Thompson, 1988). (See (Carlson et al., 2003) for details concerning the corpus and the annotation process.) Out of the 385 articles in the corpus, 53 have been independently annotated by two human annotators. We used this doubly-annotated subset to compute human agreement on the task of discourse structure derivation. In our experiments we used as discourse structures only the discourse sub-trees spanning over individual sentences. Because the discourse structures had been built on top of sentences already associated with syntactic trees from the Penn Treebank, we were able to create a composite corpus which</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical Structure Theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="21360" citStr="Marcu (2000)" startWordPosition="3594" endWordPosition="3595">Section 2, and test it on the Test section. The training regime uses syntactic trees from the Penn Treebank. The metric we use to evaluate the discourse segmenter records the accuracy of the discourse segmenter with respect to its ability to insert inside-sentence discourse boundaries. That is, if a sentence has 3 edus, which correspond to 2 inside-sentence discourse boundaries, we measure the ability of our algorithm to correctly identify these 2 boundaries. We report our evaluation results using recall, precision, and Fscore figures. This metric is harsher than the metric previously used by Marcu (2000), who assesses the performance of a discourse segmentation algorithm by counting how often the algorithm makes boundary and noboundary decisions for every word in a sentence. We compare the performance of our probabilistic discourse segmenter with the performance of the decisionbased segmenter proposed by (Marcu, 2000) and the performance of two baseline algorithms. The first baseline ( ) uses punctuation to determine when to insert a boundary; because commas are often used to indicate breaks inside long sentences, inserts discourse boundaries after each comma. The second baseline ( ) uses syn</context>
<context position="22843" citStr="Marcu, 2000" startWordPosition="3837" endWordPosition="3838">tion task ( ), using the doubly-annotated discourse corpus mentioned in Section 2. ATTRIBUTION 1 [ The bank also says] 1 3 as the product between the structure probability of 0.47 0.47 and the relation probability of 0.88. Tuple 0.88 Recall Precision F-score 28.2 37.1 32.0 25.4 64.9 36.5 77.1 83.3 80.1 82.7 83.5 83.1 85.4 84.1 84.7 98.2 98.5 98.3 Table 1: Discourse segmenter evaluation Table 1 shows the results obtained by the algorithm described in this paper ( ) using syntactic trees produced by Charniak’s parser (2000), in comparison with the results obtained by the algorithm described in (Marcu, 2000) ( ), and baseline algorithms and , on the same test set. Crucial to the performance of the discourse segmenter is the recall figure, because we want to find as many discourse boundaries as possible. The baseline algorithms are too simplistic to yield good results (recall figures of 28.2% and 25.4%). The algorithm presented in this paper gives an error reduction in missed discourse boundaries of 24.5% (recall accuracy improvement from 77.1% to 82.7%) over (Marcu, 2000). The overall error reduction is of 15.1% (improvement in F-score from 80.1% to 83.1%). In order to asses the impact on the per</context>
<context position="24699" citStr="Marcu, 2000" startWordPosition="4153" endWordPosition="4154">rformance is assessed using labeled recall and labeled precision as defined by the standard Parseval metric (Black et al., 1991). As mentioned in Section 2, we use both 18 labels and 110 labels for the discourse relations. The recall and precision figures are combined into an F-score figure in the usual manner. The discourse parsing model uses syntactic trees produced by Charniak’s parser (2000) and discourse segments produced by the algorithm described in Section 3. We compare the performance of our model ( ) with the performance of the decision-based discourse parsing model ( ) proposed by (Marcu, 2000), and Unlabeled 64.0 67.0 70.5 92.8 18 Labels 23.4 37.2 49.0 77.0 110 Labels 20.7 35.5 45.6 71.9 Table 2: performance compared to baseline, state-of-the-art, and human performance Unlabeled 70.5 73.0 92.8 96.2 18 Labels 49.0 56.4 63.8 75.5 110 Labels 45.6 52.6 59.5 70.3 Table 3: performance with human-level accuracy for syntactic trees and discourse boundaries. with the performance of a baseline algorithm ( ). The baseline algorithm builds right-branching discourse trees labeled with the most frequent relation encountered in the training set (i.e., ELABORATION-NS). We also compute the agreemen</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="992" citStr="Marcus et al., 1993" startWordPosition="138" endWordPosition="141">that can be used to identify elementary discourse units and build sentence-level discourse parse trees. The models use syntactic and lexical features. A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-ofthe-art decision-based discourse parser. A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance. 1 Introduction By exploiting information encoded in human-produced syntactic trees (Marcus et al., 1993), research on probabilistic models of syntax has driven the performance of syntactic parsers to about 90% accuracy (Charniak, 2000; Collins, 2000). The absence of semantic and discourse annotated corpora prevented similar developments in semantic/discourse parsing. Fortunately, recent annotation projects have taken significant steps towards developing semantic (Fillmore et al., 2002; Kingsbury and Palmer, 2002) and discourse (Carlson et al., 2003) annotated corpora. Some of these annotation efforts have already had a computational impact. For example, Gildea and Jurafsky (2002) developed stati</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Senko K Maynard</author>
</authors>
<title>Principles of Japanese Discourse: A Handbook.</title>
<date>1998</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="29858" citStr="Maynard, 1998" startWordPosition="4966" endWordPosition="4967">eory in mind. The annotators used by Carlson et al. (2003) were not instructed to build discourse trees that were consistent with the syntax of the sentences. Yet, they built discourse structures at sentence level that are not only consistent with the syntactic structures of sentences, but also derivable from them. Recent work on Tree Adjoining Grammar-based lexicalized models of discourse (Forbes et al., 2001) has already shown how to exploit within a single framework lexical, syntactic, and discourse cues. Various linguistics studies have also shown how intertwined syntax and discourse are (Maynard, 1998). However, to our knowledge, this is the first paper that empirically shows that the connection between syntax and discourse can be computationally exploited at high levels of accuracy on open domain, newspaper text. Another interesting finding is that the performance of current state-of-the-art syntactic parsers (Charniak, 2000) is not a bottleneck for coming up with a good solution to the sentence-level discourse parsing problem. Little improvement comes from using manually built syntactic parse trees instead of automatically derived trees. However, experiments show that there is much to be </context>
</contexts>
<marker>Maynard, 1998</marker>
<rawString>Senko K. Maynard. 1998. Principles of Japanese Discourse: A Handbook. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Palmer</author>
<author>Marti A Hearst</author>
</authors>
<title>Adaptive multilingual sentence boundary disambiguation.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="7065" citStr="Palmer and Hearst (1997)" startWordPosition="1097" endWordPosition="1100">n text is broken into non-overlapping segments called elementary discourse units (edus). In the present work, elementary discourse units are taken to be clauses or clauselike units that are unequivocally the NUCLEUS or SATELLITE of a rhetorical relation that holds between two adjacent spans of text (see (Carlson et al., 2003) for details). Our approach to discourse segmentation breaks the problem further into two sub-problems: sentence segmentation and sentence-level discourse segmentation. The problem of sentence segmentation has been studied extensively, and tools such as those described by Palmer and Hearst (1997) and Ratnaparkhi (1998) can handle it well. In this section, we present a discourse segmentation algorithm that deals with segmenting sentences into elementary discourse units. 3.1 The Discourse Segmentation Model The discourse segmenter proposed here takes as input a sentence and outputs its elementary discourse unit boundaries. Our statistical approach to sentence segmentation uses two components: a statistical model which assigns a probability to the insertion of a discourse boundary after each word in a sentence, and a segmenter, which uses the probabilities computed by the model for inser</context>
</contexts>
<marker>Palmer, Hearst, 1997</marker>
<rawString>David D. Palmer and Marti A. Hearst. 1997. Adaptive multilingual sentence boundary disambiguation. Computational Linguistics, 23(2):241–269, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="7088" citStr="Ratnaparkhi (1998)" startWordPosition="1102" endWordPosition="1103">rlapping segments called elementary discourse units (edus). In the present work, elementary discourse units are taken to be clauses or clauselike units that are unequivocally the NUCLEUS or SATELLITE of a rhetorical relation that holds between two adjacent spans of text (see (Carlson et al., 2003) for details). Our approach to discourse segmentation breaks the problem further into two sub-problems: sentence segmentation and sentence-level discourse segmentation. The problem of sentence segmentation has been studied extensively, and tools such as those described by Palmer and Hearst (1997) and Ratnaparkhi (1998) can handle it well. In this section, we present a discourse segmentation algorithm that deals with segmenting sentences into elementary discourse units. 3.1 The Discourse Segmentation Model The discourse segmenter proposed here takes as input a sentence and outputs its elementary discourse unit boundaries. Our statistical approach to sentence segmentation uses two components: a statistical model which assigns a probability to the insertion of a discourse boundary after each word in a sentence, and a segmenter, which uses the probabilities computed by the model for inserting discourse boundari</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RST-DT</author>
</authors>
<title>RST Discourse Treebank. Linguistic Data Consortium.</title>
<date>2002</date>
<note>http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp? catalogId=LDC2002T07.</note>
<contexts>
<context position="3608" citStr="RST-DT, 2002" startWordPosition="545" endWordPosition="546">mentary discourse units and build sentence-level discourse parse trees. We show how syntactic and lexical information can be exploited in the process of identifying elementary units of discourse and building sentence-level discourse trees. Our evaluation indicates that the discourse parsing model we propose is sophisticated enough to achieve near-human levels of performance on the task of deriving sentence-level discourse trees, when working with human-produced syntactic trees and discourse segments. 2 The Corpus For the experiments described in this paper, we use a publicly available corpus (RST-DT, 2002) that contains 385 [2,3] ATTRIBUTION 1 3 3 Wall Street Journal articles from the Penn Treebank. The corpus comes conveniently partitioned into a Training set of 347 articles (6132 sentences) and a Test set of 38 articles (991 sentences). Each document in the corpus is paired with a discourse structure (tree) that was manually built in the style of Rhetorical Structure Theory (Mann and Thompson, 1988). (See (Carlson et al., 2003) for details concerning the corpus and the annotation process.) Out of the 385 articles in the corpus, 53 have been independently annotated by two human annotators. We </context>
<context position="5096" citStr="RST-DT, 2002" startWordPosition="784" endWordPosition="785">top of sentences already associated with syntactic trees from the Penn Treebank, we were able to create a composite corpus which allowed us to perform an empirically driven syntax-discourse relationship study. This composite corpus was created by associating each sentence in the discourse corpus with its corresponding Penn Treebank syntactic parse tree and its corresponding sentence-level discourse tree . Although human annotators were free to build their discourse structures without enforcing the existence of wellformed discourse sub-trees for each sentence, in about 95% of the cases in the (RST-DT, 2002) corpus, there exists a discourse sub-tree associated with each sentence . The remaining 5% of the sentences cannot be used in our approach, as no well-formed discourse tree can be associated with these sentences. Therefore, our Training section consists of a set of 5809 triples of the form which are used to train the parameters of the statistical models. Our Test section consists of a set of 946 triples of a similar form, which are used to evaluate the performance of our discourse parser. The (RST-DT, 2002) corpus uses 110 different rhetorical relations. We found it useful to also compact the</context>
<context position="26862" citStr="RST-DT, 2002" startWordPosition="4499" endWordPosition="4500">e attributed to three possible causes: errors made by the syntactic parser, errors made by the discourse segmenter, and the weakness of our discourse model. In order to quantitatively asses the impact in performance of each possible cause of error, we perform further experiments. We replace the syntactic parse trees produced by Charniak’s parser at 90% accuracy ( ) with the corresponding Penn Treebank syntactic parse trees produced by human annotators ( ). We also replace the discourse boundaries produced by our discourse segmenter at 83% accuracy ( ) with the discourse boundaries taken from (RST-DT, 2002), which are produced by the human annotators ( ). The results are shown in Table 3. The results in column show that using perfect syntactic trees leads to an error reduction of 14.5% (F-score improvement from 49.0% to 56.4%) when using 18 labels, and an error reduction of 12.9% (F-score improvement from 45.6% to 52.6%) when using 110 labels. The results in column show that the impact of perfect discourse segmentation is double the impact of perfect syntactic trees. Human-level performance on discourse segmentation leads to an error reduction of 29.0% (F-score improvement from 49.0% to 63.8%) w</context>
<context position="29213" citStr="RST-DT, 2002" startWordPosition="4863" endWordPosition="4864">discourse structures. Our model defines and exploits a set of syntactically motivated lexico-grammatical dominance relations that fall naturally from a syntactic representation of sentences. The most interesting finding is that these dominance relations encode sufficient information to enable the derivation of discourse structures that are almost indistinguishable from those built by human annotators. Our experiments empirically show that, at the sentence level, there is an extremely strong correlation between syntax and discourse. This is even more remarkable given that the discourse corpus (RST-DT, 2002) was built with no syntactic theory in mind. The annotators used by Carlson et al. (2003) were not instructed to build discourse trees that were consistent with the syntax of the sentences. Yet, they built discourse structures at sentence level that are not only consistent with the syntactic structures of sentences, but also derivable from them. Recent work on Tree Adjoining Grammar-based lexicalized models of discourse (Forbes et al., 2001) has already shown how to exploit within a single framework lexical, syntactic, and discourse cues. Various linguistics studies have also shown how intertw</context>
</contexts>
<marker>RST-DT, 2002</marker>
<rawString>RST-DT. 2002. RST Discourse Treebank. Linguistic Data Consortium. http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp? catalogId=LDC2002T07.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>