<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001632">
<title confidence="0.996811">
Modeling Morphologically Rich Languages Using Split Words and
Unstructured Dependencies
</title>
<author confidence="0.984806">
Deniz Yuret
</author>
<affiliation confidence="0.96939">
Koc¸ University
</affiliation>
<address confidence="0.674925">
34450 Sariyer, Istanbul, Turkey
</address>
<email confidence="0.998088">
dyuret@ku.edu.tr
</email>
<sectionHeader confidence="0.993875" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999847857142857">
We experiment with splitting words into
their stem and suffix components for mod-
eling morphologically rich languages. We
show that using a morphological ana-
lyzer and disambiguator results in a sig-
nificant perplexity reduction in Turkish.
We present flexible n-gram models, Flex-
Grams, which assume that the n−1 tokens
that determine the probability of a given
token can be chosen anywhere in the sen-
tence rather than the preceding n −1 posi-
tions. Our final model achieves 27% per-
plexity reduction compared to the standard
n-gram model.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999838391304348">
Language models, i.e. models that assign prob-
abilities to sequences of words, have been proven
useful in a variety of applications including speech
recognition and machine translation (Bahl et al.,
1983; Brown et al., 1990). More recently, good re-
sults on lexical substitution and word sense disam-
biguation using language models have also been
reported (Hawker, 2007; Yuret, 2007). Morpho-
logically rich languages pose a challenge to stan-
dard modeling techniques because of their rela-
tively large out-of-vocabulary rates and the regu-
larities they possess at the sub-word level.
The standard n-gram language model ignores
long-distance relationships between words and
uses the independence assumption of a Markov
chain of order n − 1. Morphemes play an im-
portant role in the syntactic dependency structure
in morphologically rich languages. The depen-
dencies are not only between stems but also be-
tween stems and suffixes and if we use complete
words as unit tokens, we will not be able to rep-
resent these sub-word dependencies. Our work-
ing hypothesis is that the performance of a lan-
</bodyText>
<note confidence="0.862876333333333">
Ergun Bic¸ici
Koc¸ University
34450 Sariyer, Istanbul, Turkey
</note>
<email confidence="0.936952">
ebicici@ku.edu.tr
</email>
<bodyText confidence="0.999861181818182">
guage model is correlated by how much the prob-
abilistic dependencies mirror the syntactic depen-
dencies. We present flexible n-grams, FlexGrams,
in which each token can be conditioned on tokens
anywhere in the sentence, not just the preceding
n −1 tokens. We also experiment with words split
into their stem and suffix forms, and define stem-
suffix FlexGrams where one set of offsets is ap-
plied to stems and another to suffixes. We evaluate
the performance of these models on a morpholog-
ically rich language, Turkish.
</bodyText>
<sectionHeader confidence="0.984219" genericHeader="method">
2 The FlexGram Model
</sectionHeader>
<bodyText confidence="0.999952357142857">
The FlexGram model relaxes the contextual as-
sumption of n-grams and assumes that the n − 1
tokens that determine the probability of a given to-
ken can be chosen anywhere in the sentence rather
than at the preceding n − 1 positions. This allows
the ability to model long-distance relationships be-
tween tokens without a predefined left-to-right or-
dering and opens the possibility of using different
dependency patterns for different token types.
Formal definition An order-n FlexGram model
is specified by a tuple of dependency offsets
[d1, d2,.. . , do−1] and decomposes the probability
of a given sequence of tokens into a product of
conditional probabilities for every token:
</bodyText>
<equation confidence="0.8541165">
11 p(w1, ... , wk) = p(wz|wz+d1 ... wz+dn−1)
wi∈S
</equation>
<bodyText confidence="0.99995225">
The offsets can be positive or negative and the
same set of offsets is applied to all tokens in the
sequence. In order to represent a properly nor-
malized probability model over the set of all finite
length sequences, we check that the offsets of a
FlexGram model does not result in a cycle. We
show that using differing dependency offsets for
stems and suffixes can improve the perplexity.
</bodyText>
<page confidence="0.989598">
345
</page>
<note confidence="0.973555">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 345–348,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<page confidence="0.481155">
3 Dataset Table 1: Dataset statistics (K for thousands, M for millions)
</page>
<bodyText confidence="0.999976375">
We used the Turkish newspaper corpus of Milliyet
after removing sentences with 100 or more tokens.
The dataset contains about 600 thousand sentences
in the training set and 60 thousand sentences in the
test set (giving a total of about 10 million words).
The versions of the corpus we use developed by
using different word-split strategies along with a
sample sentence are explained below:
</bodyText>
<listItem confidence="0.8361435">
1. The unsplit dataset contains the raw corpus:
Kasparov b¨ukemedi˜gi eli ¨opecek
(Kasparov is going to kiss the hand he cannot bend)
2. The morfessor dataset was prepared using the
Morfessor (Creutz et al., 2007) algorithm:
Kasparov b¨uke +medi˜gi eli ¨op +ecek
3. The auto-split dataset is obtained after using
our unsupervised morphological splitter:
Kaspar +ov b¨uk +emedi˜gi eli ¨op +ecek
4. The split dataset contains words that are split
into their stem and suffix forms by using a
highly accurate supervised morphological an-
alyzer (Yuret and T¨ure, 2006):
Kasparov b¨uk +yAmA+dHk+sH el +sH ¨op
+yAcAk
5. The split+0 version is derived from the split
dataset by adding a zero-suffix to any stem that
is not followed by a suffix:
</listItem>
<bodyText confidence="0.986217941176471">
Kasparov +0 b¨uk +yAmA+dHk+sH el +sH
¨op +yAcAk
Some statistics of the dataset are presented in
Table 1. The vocabulary is taken to be the to-
kens that occur more than once in the training set
and the OOV column shows the number of out-
of-vocabulary tokens in the test set. The unique
and 1-count columns give the number of unique
tokens and the number of tokens that only occur
once in the training set. Approximately 5% of the
tokens in the unsplit test set are OOV tokens. In
comparison, the ratio for a comparably sized En-
glish dataset is around 1%. Splitting the words
into stems and suffixes brings the OOV ratio closer
to that of English.
Model evaluation When comparing language
models that tokenize data differently:
</bodyText>
<listItem confidence="0.988607">
1. We take into account the true cost of the OOV
tokens using a separate character-based model
similar to Brown et al. (1992).
2. When reporting averages (perplexity, bits-per-
word) we use a common denominator: the
number of unsplit words.
</listItem>
<table confidence="0.997965333333333">
Dataset Train Test OOV Unique 1-count
unsplit 8.88M 0.91M 44.8K (4.94%) 430K 206K
morfessor 9.45M 0.98M 10.3K (1.05%) 167K 34.4K
auto-split 14.3M 1.46M 13.0K (0.89%) 128K 44.8K
split 12.8M 1.31M 17.1K (1.31%) 152K 75.4K
split+0 17.8M 1.81M 17.1K (0.94%) 152K 75.4K
</table>
<sectionHeader confidence="0.994652" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999903615384615">
In this section we present a number of experiments
that demonstrate that when modeling a morpho-
logically rich language like Turkish, (i) splitting
words into their stem and suffix forms is beneficial
when the split is performed using a morphologi-
cal analyzer and (ii) allowing the model to choose
stem and suffix dependencies separately and flex-
ibly results in a perplexity reduction, however the
reduction does not offset the cost of zero suffixes.
We used the SRILM toolkit (Stolcke, 2002) to
simulate the behavior of FlexGram models by us-
ing count files as input. The interpolated Kneser-
Ney smoothing was used in all our experiments.
</bodyText>
<tableCaption confidence="0.990121">
Table 2: Total log probability (M for millions of bits).
</tableCaption>
<table confidence="0.99633">
N Split Dataset Unsplit Dataset
Word logp OOV logp Word logp OOV logp
1 14.2M 0.81M 11.7M 2.32M
2 10.5M 0.64M 9.64M 1.85M
3 9.79M 0.56M 9.46M 1.59M
4 9.72M 0.53M 9.45M 1.38M
5 9.71M 0.51M 9.45M 1.25M
6 9.71M 0.50M 9.45M 1.19M
</table>
<subsectionHeader confidence="0.7068295">
4.1 Using a morphological tagger and
disambiguator
</subsectionHeader>
<bodyText confidence="0.999664666666667">
The split version of the corpus contains words
that are split into their stem and suffix forms by
using a previously developed morphological an-
alyzer (Oflazer, 1994) and morphological disam-
biguator (Yuret and T¨ure, 2006). The analyzer
produces all possible parses of a Turkish word us-
ing the two-level morphological paradigm and the
disambiguator chooses the best parse based on the
analysis of the context using decision lists. The in-
tegrated system was found to discover the correct
morphological analysis for 96% of the words on
a hand annotated out-of-sample test set. Table 2
gives the total log-probability (using log2) for the
split and unsplit datasets using n-gram models
of different order. We compute the perplexity
of the two datasets using a common denomina-
tor: 2− log2(P)/N where N=906,172 is taken to be
the number of unsplit tokens. The best combina-
tion (order-6 word model combined with an order-
9 letter model) gives a perplexity of 2,465 for
the split dataset and 3,397 for the unsplit dataset,
</bodyText>
<page confidence="0.99796">
346
</page>
<bodyText confidence="0.944774">
which corresponds to a 27% improvement.
</bodyText>
<subsectionHeader confidence="0.997113">
4.2 Separation of stem and suffix models
</subsectionHeader>
<bodyText confidence="0.999944230769231">
Only 45% of the words in the split dataset have
suffixes. Each sentence in the split+0 dataset has
a regular [stem suffix stem suffix ...] structure. Ta-
ble 3 gives the average cost of stems and suffixes in
the two datasets for a regular 6-gram word model
(ignoring the common OOV words). The log-
probability spent on the zero suffixes in the split+0
dataset has to be spent on trying to decide whether
to include a stem or suffix following a stem in the
split dataset. As a result the difference in total log-
probability between the two datasets is small (only
6% perplexity difference). The set of OOV tokens
is the same for both the split and split+0 datasets;
therefore we ignore the cost of the OOV tokens as
is the default SRILM behavior.
the left side of Figure 1 for order 3. Table 5 gives
the best combined models without cycles. We
were able to exhaustively search all the patterns
for orders 2 to 4 and we used beam search for or-
ders 5 and 6. Each model is represented by its
offset tuple and the resulting perplexity is given
in parentheses. Compared to the regular n-gram
models from Table 4 we see significant perplexity
reductions up to order 4. The best order-3 stem-
suffix FlexGram model can be seen on the right
side of Figure 1.
</bodyText>
<tableCaption confidence="0.962591">
Table 5: Best stem-suffix flexgram model combinations for
</tableCaption>
<table confidence="0.931474428571428">
the split+0 dataset.
N flexgram-stem flexgram-suffix perplexity reduction
2 -2 (596) -1 (5.69) 52.3%
3 -4,-2 (496) +1,-1 (4.21) 5.58%
4 -4, -2,-1 (363) -3, -2,-1 (4.79) 11.3%
5 -6,-4, -2,-1 (361) -3, -2,-1 (4.79) 1.29%
6 -6,-4, -2,-1 (361) -3, -2,-1 (4.79) 1.52%
</table>
<tableCaption confidence="0.992753">
Table 3: Total log probability for the 6-gram word models
on split and split+0 data.
</tableCaption>
<table confidence="0.998922142857143">
split dataset split+0 dataset
token number of total number of total
type tokens − log, P tokens − log, P
stem 0.91M 7.80M 0.91M 7.72M
suffix 0.41M 1.89M 0.41M 1.84M
0-suffix – – 0.50M 0.21M
all 1.31M 9.69M 1.81M 9.78M
</table>
<subsectionHeader confidence="0.999742">
4.3 Using the FlexGram model
</subsectionHeader>
<bodyText confidence="0.9993261">
We perform a search over the space of dependency
offsets using the split+0 dataset and considered n-
gram orders 2 to 6 and picked the dependency off-
sets within a window of 4n + 1 tokens centered
around the target. Table 4 gives the best mod-
els discovered for stems and suffixes separately
and compares them to the corresponding regular
n-gram models on the split+0 dataset. The num-
bers in parentheses give perplexity and significant
reductions can be observed for each n-gram order.
</bodyText>
<tableCaption confidence="0.998812">
Table 4: Regular ngram vs FlexGram models.
</tableCaption>
<table confidence="0.995717272727273">
N ngram-stem ngram-suffix
2 -1 (1252) -1 (5.69)
3 -2,-1 (418) -2,-1 (5.29)
4 -3,-2,-1 (409) -3,-2,-1 (4.79)
5 -4,-3,-2,-1 (365) -4,-3,-2,-1 (4.80)
6 -5,-4,-3,-2,-1 (367) -5,-4,-3,-2,-1 (4.79)
N flexgram-stem flexgram-suffix
2 -2 (596) -1 (5.69)
3 +1,-2 (289) +1,-1 (4.21)
4 +2, +1,-1 (189) -2, +1,-1 (4.19)
5 +4, +2, +1,-1 (176) -3,-2, +1,-1 (4.12)
</table>
<page confidence="0.706958">
6 +4,+3, +2, +1,-1 (172) -4,-3,-2, +1,-1 (4.13)
</page>
<bodyText confidence="0.9958185">
However, some of these models cannot be used
in combination because of cycles as we depict on
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.9999751">
Several approaches attempt to relax the rigid or-
dering enforced by the standard n-gram model.
The skip-gram model (Siu and Ostendorf, Jan
2000) allows the skipping of one word within a
given n-gram. Variable context length language
modeling (Kneser, 1996) achieves a 10% per-
plexity reduction when compared to the trigrams
by varying the order of the n-gram model based
on the context. Dependency models (Rosenfeld,
2000) use the parsed dependency structure of sen-
tences to build the language model as in grammat-
ical trigrams (Lafferty et al., 1992), structured lan-
guage models (Chelba and Jelinek, 2000), and de-
pendency language models (Chelba et al., 1997).
The dependency model governs the whole sen-
tence and each word in a sentence is likely to have
a different dependency structure whereas in our
experiments with FlexGrams we use two connec-
tivity patterns: one for stems and one for suffixes
without the need for parsing.
</bodyText>
<sectionHeader confidence="0.99885" genericHeader="conclusions">
6 Contributions
</sectionHeader>
<bodyText confidence="0.999948444444445">
We have analyzed the effect of word splitting and
unstructured dependencies on modeling Turkish, a
morphologically complex language. Table 6 com-
pares the models we have tested on our test corpus.
We find that splitting words into their stem and
suffix components using a morphological analyzer
and disambiguator results in significant perplexity
reductions of up to 27%. FlexGram models out-
perform regular n-gram models (Tables 4 and 5)
</bodyText>
<page confidence="0.995446">
347
</page>
<figureCaption confidence="0.78185025">
Figure 1: Two FlexGram models where W represents a stem, s represents a suffix, and the arrows represent dependencies.
The left model has stem offsets [+1,-2] and suffix offsets [+1,-1] and cannot be used as a directed graphical model because
of the cycles. The right model has stem offsets [-4,-2] and suffix offsets [+1,-1] and is the best order-3 FlexGram model for
Turkish.
</figureCaption>
<tableCaption confidence="0.6509985">
Table 6: Perplexity for compared models.
N unsplit split flexgram
</tableCaption>
<figure confidence="0.9897422">
2 3929 4360 5043
3 3421 2610 3083
4 3397 2487 2557
5 3397 2468 2539
6 3397 2465 2539
</figure>
<bodyText confidence="0.99725975">
when using an alternating stem-suffix representa-
tion of the sentences; however Table 6 shows that
the cost of the alternating stem-suffix representa-
tion (zero-suffixes) offsets this gain.
</bodyText>
<sectionHeader confidence="0.997637" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995572696969697">
Lalit R. Bahl, Frederick Jelinek, and Robert L.
Mercer. A maximum likelihood approach to
continuous speech recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelli-
gence, 5(2):179–190, 1983.
Peter F. Brown, John Cocke, Stephen A.
Della Pietra, Vincent J. Della Pietra, Frederick
Jelinek, John D. Lafferty, Robert L. Mercer, and
Paul S. Roossin. A statistical approach to ma-
chine translation. Computational Linguistics,
16(2):79–85, 1990.
Peter F. Brown et al. An estimate of an upper
bound for the entropy of english. Computa-
tional Linguistics, 18(1):31–40, 1992.
Ciprian Chelba and Frederick Jelinek. Recog-
nition performance of a structured language
model. CoRR, cs.CL/0001022, 2000.
Ciprian Chelba, David Engle, Frederick Jelinek,
Victor M. Jimenez, Sanjeev Khudanpur, Lidia
Mangu, Harry Printz, Eric Ristad, Ronald
Rosenfeld, Andreas Stolcke, and Dekai Wu.
Structure and performance of a dependency lan-
guage model. In Proc. Eurospeech ’97, pages
2775–2778, Rhodes, Greece, September 1997.
Mathias Creutz, Teemu Hirsim¨aki, Mikko Ku-
rimo, Antti Puurula, Janne Pylkk¨onen, Vesa
Siivola, Matti Varjokallio, Ebru Arisoy, Mu-
rat Saraclar, and Andreas Stolcke. Morph-
based speech recognition and modeling of out-
of-vocabulary words across languages. TSLP, 5
(1), 2007.
Tobias Hawker. USYD: WSD and lexical substitu-
tion using the Web1T corpus. In SemEval-2007:
4th International Workshop on Semantic Evalu-
ations, 2007.
R. Kneser. Statistical language modeling using a
variable context length. In Proc. ICSLP ’96,
volume 1, pages 494–497, Philadelphia, PA,
October 1996.
John Lafferty, Daniel Sleator, and Davy Tem-
perley. Grammatical trigrams: a probabilistic
model of link grammar. In AAAI Fall Sym-
posium on Probabilistic Approaches to NLP,
1992.
Kemal Oflazer. Two-level description of turkish
morphology. Literary and Linguistic Comput-
ing, 9(2):137–148, 1994.
Ronald Rosenfeld. Two decades of statistical lan-
guage modeling: Where do we go from here.
In Proceedings of the IEEE, volume 88, pages
1270–1278, 2000.
Manhung Siu and M. Ostendorf. Variable n-grams
and extensions for conversational speech lan-
guage modeling. Speech and Audio Processing,
IEEE Transactions on, 8(1):63–75, Jan 2000.
ISSN 1063-6676. doi: 10.1109/89.817454.
Andreas Stolcke. Srilm – an extensible language
modeling toolkit. In Proc. Int. Conf. Spoken
Language Processing (ICSLP 2002), 2002.
Deniz Yuret. KU: Word sense disambiguation by
substitution. In SemEval-2007: 4th Interna-
tional Workshop on Semantic Evaluations, June
2007.
Deniz Yuret and Ferhan T¨ure. Learning mor-
phological disambiguation rules for turkish. In
HLT-NAACL 06, June 2006.
</reference>
<page confidence="0.998033">
348
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.959744">
<title confidence="0.999396">Modeling Morphologically Rich Languages Using Split Words and Unstructured Dependencies</title>
<author confidence="0.991883">Deniz Yuret</author>
<affiliation confidence="0.996005">University</affiliation>
<address confidence="0.999987">34450 Sariyer, Istanbul, Turkey</address>
<email confidence="0.995041">dyuret@ku.edu.tr</email>
<abstract confidence="0.9984246">We experiment with splitting words into their stem and suffix components for modeling morphologically rich languages. We show that using a morphological analyzer and disambiguator results in a significant perplexity reduction in Turkish. present flexible models, Flexwhich assume that the that determine the probability of a given token can be chosen anywhere in the senrather than the preceding positions. Our final model achieves 27% perplexity reduction compared to the standard n-gram model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>A maximum likelihood approach to continuous speech recognition.</title>
<date>1983</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="926" citStr="Bahl et al., 1983" startWordPosition="136" endWordPosition="139">g a morphological analyzer and disambiguator results in a significant perplexity reduction in Turkish. We present flexible n-gram models, FlexGrams, which assume that the n−1 tokens that determine the probability of a given token can be chosen anywhere in the sentence rather than the preceding n −1 positions. Our final model achieves 27% perplexity reduction compared to the standard n-gram model. 1 Introduction Language models, i.e. models that assign probabilities to sequences of words, have been proven useful in a variety of applications including speech recognition and machine translation (Bahl et al., 1983; Brown et al., 1990). More recently, good results on lexical substitution and word sense disambiguation using language models have also been reported (Hawker, 2007; Yuret, 2007). Morphologically rich languages pose a challenge to standard modeling techniques because of their relatively large out-of-vocabulary rates and the regularities they possess at the sub-word level. The standard n-gram language model ignores long-distance relationships between words and uses the independence assumption of a Markov chain of order n − 1. Morphemes play an important role in the syntactic dependency structur</context>
</contexts>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. A maximum likelihood approach to continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2):179–190, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="947" citStr="Brown et al., 1990" startWordPosition="140" endWordPosition="143">nalyzer and disambiguator results in a significant perplexity reduction in Turkish. We present flexible n-gram models, FlexGrams, which assume that the n−1 tokens that determine the probability of a given token can be chosen anywhere in the sentence rather than the preceding n −1 positions. Our final model achieves 27% perplexity reduction compared to the standard n-gram model. 1 Introduction Language models, i.e. models that assign probabilities to sequences of words, have been proven useful in a variety of applications including speech recognition and machine translation (Bahl et al., 1983; Brown et al., 1990). More recently, good results on lexical substitution and word sense disambiguation using language models have also been reported (Hawker, 2007; Yuret, 2007). Morphologically rich languages pose a challenge to standard modeling techniques because of their relatively large out-of-vocabulary rates and the regularities they possess at the sub-word level. The standard n-gram language model ignores long-distance relationships between words and uses the independence assumption of a Markov chain of order n − 1. Morphemes play an important role in the syntactic dependency structure in morphologically </context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
</authors>
<title>An estimate of an upper bound for the entropy of english.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>1</issue>
<marker>Brown, 1992</marker>
<rawString>Peter F. Brown et al. An estimate of an upper bound for the entropy of english. Computational Linguistics, 18(1):31–40, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Recognition performance of a structured language model.</title>
<date>2000</date>
<location>CoRR, cs.CL/0001022,</location>
<contexts>
<context position="11713" citStr="Chelba and Jelinek, 2000" startWordPosition="1953" endWordPosition="1956">ict on 5 Related work Several approaches attempt to relax the rigid ordering enforced by the standard n-gram model. The skip-gram model (Siu and Ostendorf, Jan 2000) allows the skipping of one word within a given n-gram. Variable context length language modeling (Kneser, 1996) achieves a 10% perplexity reduction when compared to the trigrams by varying the order of the n-gram model based on the context. Dependency models (Rosenfeld, 2000) use the parsed dependency structure of sentences to build the language model as in grammatical trigrams (Lafferty et al., 1992), structured language models (Chelba and Jelinek, 2000), and dependency language models (Chelba et al., 1997). The dependency model governs the whole sentence and each word in a sentence is likely to have a different dependency structure whereas in our experiments with FlexGrams we use two connectivity patterns: one for stems and one for suffixes without the need for parsing. 6 Contributions We have analyzed the effect of word splitting and unstructured dependencies on modeling Turkish, a morphologically complex language. Table 6 compares the models we have tested on our test corpus. We find that splitting words into their stem and suffix componen</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. Recognition performance of a structured language model. CoRR, cs.CL/0001022, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>David Engle</author>
<author>Frederick Jelinek</author>
<author>Victor M Jimenez</author>
<author>Sanjeev Khudanpur</author>
<author>Lidia Mangu</author>
<author>Harry Printz</author>
</authors>
<title>Eric Ristad, Ronald Rosenfeld, Andreas Stolcke, and Dekai Wu. Structure and performance of a dependency language model.</title>
<date>1997</date>
<booktitle>In Proc. Eurospeech ’97,</booktitle>
<pages>2775--2778</pages>
<location>Rhodes, Greece,</location>
<contexts>
<context position="11767" citStr="Chelba et al., 1997" startWordPosition="1962" endWordPosition="1965">he rigid ordering enforced by the standard n-gram model. The skip-gram model (Siu and Ostendorf, Jan 2000) allows the skipping of one word within a given n-gram. Variable context length language modeling (Kneser, 1996) achieves a 10% perplexity reduction when compared to the trigrams by varying the order of the n-gram model based on the context. Dependency models (Rosenfeld, 2000) use the parsed dependency structure of sentences to build the language model as in grammatical trigrams (Lafferty et al., 1992), structured language models (Chelba and Jelinek, 2000), and dependency language models (Chelba et al., 1997). The dependency model governs the whole sentence and each word in a sentence is likely to have a different dependency structure whereas in our experiments with FlexGrams we use two connectivity patterns: one for stems and one for suffixes without the need for parsing. 6 Contributions We have analyzed the effect of word splitting and unstructured dependencies on modeling Turkish, a morphologically complex language. Table 6 compares the models we have tested on our test corpus. We find that splitting words into their stem and suffix components using a morphological analyzer and disambiguator re</context>
</contexts>
<marker>Chelba, Engle, Jelinek, Jimenez, Khudanpur, Mangu, Printz, 1997</marker>
<rawString>Ciprian Chelba, David Engle, Frederick Jelinek, Victor M. Jimenez, Sanjeev Khudanpur, Lidia Mangu, Harry Printz, Eric Ristad, Ronald Rosenfeld, Andreas Stolcke, and Dekai Wu. Structure and performance of a dependency language model. In Proc. Eurospeech ’97, pages 2775–2778, Rhodes, Greece, September 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
</authors>
<title>Teemu Hirsim¨aki, Mikko Kurimo, Antti Puurula, Janne Pylkk¨onen, Vesa Siivola, Matti Varjokallio, Ebru Arisoy, Murat Saraclar, and Andreas Stolcke. Morphbased speech recognition and modeling of outof-vocabulary words across languages.</title>
<date>2007</date>
<journal>TSLP,</journal>
<volume>5</volume>
<issue>1</issue>
<marker>Creutz, 2007</marker>
<rawString>Mathias Creutz, Teemu Hirsim¨aki, Mikko Kurimo, Antti Puurula, Janne Pylkk¨onen, Vesa Siivola, Matti Varjokallio, Ebru Arisoy, Murat Saraclar, and Andreas Stolcke. Morphbased speech recognition and modeling of outof-vocabulary words across languages. TSLP, 5 (1), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Hawker</author>
</authors>
<title>USYD: WSD and lexical substitution using the Web1T corpus.</title>
<date>2007</date>
<booktitle>In SemEval-2007: 4th International Workshop on Semantic Evaluations,</booktitle>
<contexts>
<context position="1090" citStr="Hawker, 2007" startWordPosition="164" endWordPosition="165">e n−1 tokens that determine the probability of a given token can be chosen anywhere in the sentence rather than the preceding n −1 positions. Our final model achieves 27% perplexity reduction compared to the standard n-gram model. 1 Introduction Language models, i.e. models that assign probabilities to sequences of words, have been proven useful in a variety of applications including speech recognition and machine translation (Bahl et al., 1983; Brown et al., 1990). More recently, good results on lexical substitution and word sense disambiguation using language models have also been reported (Hawker, 2007; Yuret, 2007). Morphologically rich languages pose a challenge to standard modeling techniques because of their relatively large out-of-vocabulary rates and the regularities they possess at the sub-word level. The standard n-gram language model ignores long-distance relationships between words and uses the independence assumption of a Markov chain of order n − 1. Morphemes play an important role in the syntactic dependency structure in morphologically rich languages. The dependencies are not only between stems but also between stems and suffixes and if we use complete words as unit tokens, we</context>
</contexts>
<marker>Hawker, 2007</marker>
<rawString>Tobias Hawker. USYD: WSD and lexical substitution using the Web1T corpus. In SemEval-2007: 4th International Workshop on Semantic Evaluations, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
</authors>
<title>Statistical language modeling using a variable context length.</title>
<date>1996</date>
<booktitle>In Proc. ICSLP ’96,</booktitle>
<volume>1</volume>
<pages>494--497</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="11365" citStr="Kneser, 1996" startWordPosition="1898" endWordPosition="1899"> -5,-4,-3,-2,-1 (367) -5,-4,-3,-2,-1 (4.79) N flexgram-stem flexgram-suffix 2 -2 (596) -1 (5.69) 3 +1,-2 (289) +1,-1 (4.21) 4 +2, +1,-1 (189) -2, +1,-1 (4.19) 5 +4, +2, +1,-1 (176) -3,-2, +1,-1 (4.12) 6 +4,+3, +2, +1,-1 (172) -4,-3,-2, +1,-1 (4.13) However, some of these models cannot be used in combination because of cycles as we depict on 5 Related work Several approaches attempt to relax the rigid ordering enforced by the standard n-gram model. The skip-gram model (Siu and Ostendorf, Jan 2000) allows the skipping of one word within a given n-gram. Variable context length language modeling (Kneser, 1996) achieves a 10% perplexity reduction when compared to the trigrams by varying the order of the n-gram model based on the context. Dependency models (Rosenfeld, 2000) use the parsed dependency structure of sentences to build the language model as in grammatical trigrams (Lafferty et al., 1992), structured language models (Chelba and Jelinek, 2000), and dependency language models (Chelba et al., 1997). The dependency model governs the whole sentence and each word in a sentence is likely to have a different dependency structure whereas in our experiments with FlexGrams we use two connectivity pat</context>
</contexts>
<marker>Kneser, 1996</marker>
<rawString>R. Kneser. Statistical language modeling using a variable context length. In Proc. ICSLP ’96, volume 1, pages 494–497, Philadelphia, PA, October 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Grammatical trigrams: a probabilistic model of link grammar.</title>
<date>1992</date>
<booktitle>In AAAI Fall Symposium on Probabilistic Approaches to NLP,</booktitle>
<contexts>
<context position="11658" citStr="Lafferty et al., 1992" startWordPosition="1945" endWordPosition="1948">t be used in combination because of cycles as we depict on 5 Related work Several approaches attempt to relax the rigid ordering enforced by the standard n-gram model. The skip-gram model (Siu and Ostendorf, Jan 2000) allows the skipping of one word within a given n-gram. Variable context length language modeling (Kneser, 1996) achieves a 10% perplexity reduction when compared to the trigrams by varying the order of the n-gram model based on the context. Dependency models (Rosenfeld, 2000) use the parsed dependency structure of sentences to build the language model as in grammatical trigrams (Lafferty et al., 1992), structured language models (Chelba and Jelinek, 2000), and dependency language models (Chelba et al., 1997). The dependency model governs the whole sentence and each word in a sentence is likely to have a different dependency structure whereas in our experiments with FlexGrams we use two connectivity patterns: one for stems and one for suffixes without the need for parsing. 6 Contributions We have analyzed the effect of word splitting and unstructured dependencies on modeling Turkish, a morphologically complex language. Table 6 compares the models we have tested on our test corpus. We find t</context>
</contexts>
<marker>Lafferty, Sleator, Temperley, 1992</marker>
<rawString>John Lafferty, Daniel Sleator, and Davy Temperley. Grammatical trigrams: a probabilistic model of link grammar. In AAAI Fall Symposium on Probabilistic Approaches to NLP, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Two-level description of turkish morphology.</title>
<date>1994</date>
<journal>Literary and Linguistic Computing,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="7248" citStr="Oflazer, 1994" startWordPosition="1189" endWordPosition="1190">exGram models by using count files as input. The interpolated KneserNey smoothing was used in all our experiments. Table 2: Total log probability (M for millions of bits). N Split Dataset Unsplit Dataset Word logp OOV logp Word logp OOV logp 1 14.2M 0.81M 11.7M 2.32M 2 10.5M 0.64M 9.64M 1.85M 3 9.79M 0.56M 9.46M 1.59M 4 9.72M 0.53M 9.45M 1.38M 5 9.71M 0.51M 9.45M 1.25M 6 9.71M 0.50M 9.45M 1.19M 4.1 Using a morphological tagger and disambiguator The split version of the corpus contains words that are split into their stem and suffix forms by using a previously developed morphological analyzer (Oflazer, 1994) and morphological disambiguator (Yuret and T¨ure, 2006). The analyzer produces all possible parses of a Turkish word using the two-level morphological paradigm and the disambiguator chooses the best parse based on the analysis of the context using decision lists. The integrated system was found to discover the correct morphological analysis for 96% of the words on a hand annotated out-of-sample test set. Table 2 gives the total log-probability (using log2) for the split and unsplit datasets using n-gram models of different order. We compute the perplexity of the two datasets using a common de</context>
</contexts>
<marker>Oflazer, 1994</marker>
<rawString>Kemal Oflazer. Two-level description of turkish morphology. Literary and Linguistic Computing, 9(2):137–148, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>Two decades of statistical language modeling: Where do we go from here.</title>
<date>2000</date>
<booktitle>In Proceedings of the IEEE,</booktitle>
<volume>88</volume>
<pages>1270--1278</pages>
<contexts>
<context position="11530" citStr="Rosenfeld, 2000" startWordPosition="1925" endWordPosition="1926">4, +2, +1,-1 (176) -3,-2, +1,-1 (4.12) 6 +4,+3, +2, +1,-1 (172) -4,-3,-2, +1,-1 (4.13) However, some of these models cannot be used in combination because of cycles as we depict on 5 Related work Several approaches attempt to relax the rigid ordering enforced by the standard n-gram model. The skip-gram model (Siu and Ostendorf, Jan 2000) allows the skipping of one word within a given n-gram. Variable context length language modeling (Kneser, 1996) achieves a 10% perplexity reduction when compared to the trigrams by varying the order of the n-gram model based on the context. Dependency models (Rosenfeld, 2000) use the parsed dependency structure of sentences to build the language model as in grammatical trigrams (Lafferty et al., 1992), structured language models (Chelba and Jelinek, 2000), and dependency language models (Chelba et al., 1997). The dependency model governs the whole sentence and each word in a sentence is likely to have a different dependency structure whereas in our experiments with FlexGrams we use two connectivity patterns: one for stems and one for suffixes without the need for parsing. 6 Contributions We have analyzed the effect of word splitting and unstructured dependencies o</context>
</contexts>
<marker>Rosenfeld, 2000</marker>
<rawString>Ronald Rosenfeld. Two decades of statistical language modeling: Where do we go from here. In Proceedings of the IEEE, volume 88, pages 1270–1278, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manhung Siu</author>
<author>M Ostendorf</author>
</authors>
<title>Variable n-grams and extensions for conversational speech language modeling. Speech and Audio Processing,</title>
<date>2000</date>
<journal>IEEE Transactions on,</journal>
<volume>8</volume>
<issue>1</issue>
<pages>1063--6676</pages>
<marker>Siu, Ostendorf, 2000</marker>
<rawString>Manhung Siu and M. Ostendorf. Variable n-grams and extensions for conversational speech language modeling. Speech and Audio Processing, IEEE Transactions on, 8(1):63–75, Jan 2000. ISSN 1063-6676. doi: 10.1109/89.817454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. Int. Conf. Spoken Language Processing (ICSLP</booktitle>
<contexts>
<context position="6603" citStr="Stolcke, 2002" startWordPosition="1077" endWordPosition="1078">K (0.89%) 128K 44.8K split 12.8M 1.31M 17.1K (1.31%) 152K 75.4K split+0 17.8M 1.81M 17.1K (0.94%) 152K 75.4K 4 Experiments In this section we present a number of experiments that demonstrate that when modeling a morphologically rich language like Turkish, (i) splitting words into their stem and suffix forms is beneficial when the split is performed using a morphological analyzer and (ii) allowing the model to choose stem and suffix dependencies separately and flexibly results in a perplexity reduction, however the reduction does not offset the cost of zero suffixes. We used the SRILM toolkit (Stolcke, 2002) to simulate the behavior of FlexGram models by using count files as input. The interpolated KneserNey smoothing was used in all our experiments. Table 2: Total log probability (M for millions of bits). N Split Dataset Unsplit Dataset Word logp OOV logp Word logp OOV logp 1 14.2M 0.81M 11.7M 2.32M 2 10.5M 0.64M 9.64M 1.85M 3 9.79M 0.56M 9.46M 1.59M 4 9.72M 0.53M 9.45M 1.38M 5 9.71M 0.51M 9.45M 1.25M 6 9.71M 0.50M 9.45M 1.19M 4.1 Using a morphological tagger and disambiguator The split version of the corpus contains words that are split into their stem and suffix forms by using a previously dev</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. Srilm – an extensible language modeling toolkit. In Proc. Int. Conf. Spoken Language Processing (ICSLP 2002), 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>KU: Word sense disambiguation by substitution.</title>
<date>2007</date>
<booktitle>In SemEval-2007: 4th International Workshop on Semantic Evaluations,</booktitle>
<contexts>
<context position="1104" citStr="Yuret, 2007" startWordPosition="166" endWordPosition="167">hat determine the probability of a given token can be chosen anywhere in the sentence rather than the preceding n −1 positions. Our final model achieves 27% perplexity reduction compared to the standard n-gram model. 1 Introduction Language models, i.e. models that assign probabilities to sequences of words, have been proven useful in a variety of applications including speech recognition and machine translation (Bahl et al., 1983; Brown et al., 1990). More recently, good results on lexical substitution and word sense disambiguation using language models have also been reported (Hawker, 2007; Yuret, 2007). Morphologically rich languages pose a challenge to standard modeling techniques because of their relatively large out-of-vocabulary rates and the regularities they possess at the sub-word level. The standard n-gram language model ignores long-distance relationships between words and uses the independence assumption of a Markov chain of order n − 1. Morphemes play an important role in the syntactic dependency structure in morphologically rich languages. The dependencies are not only between stems but also between stems and suffixes and if we use complete words as unit tokens, we will not be a</context>
</contexts>
<marker>Yuret, 2007</marker>
<rawString>Deniz Yuret. KU: Word sense disambiguation by substitution. In SemEval-2007: 4th International Workshop on Semantic Evaluations, June 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
<author>Ferhan T¨ure</author>
</authors>
<title>Learning morphological disambiguation rules for turkish.</title>
<date>2006</date>
<booktitle>In HLT-NAACL 06,</booktitle>
<marker>Yuret, T¨ure, 2006</marker>
<rawString>Deniz Yuret and Ferhan T¨ure. Learning morphological disambiguation rules for turkish. In HLT-NAACL 06, June 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>