<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022448">
<title confidence="0.9989415">
Modeling Semantic Relevance for Question-Answer Pairs
in Web Social Communities
</title>
<author confidence="0.999657">
Baoxun Wang, Xiaolong Wang, Chengjie Sun, Bingquan Liu, Lin Sun
</author>
<affiliation confidence="0.9975565">
School of Computer Science and Technology
Harbin Institute of Technology
</affiliation>
<address confidence="0.716234">
Harbin, China
</address>
<email confidence="0.988989">
{bxwang, wangxl, cjsun, liubq, lsun}@insun.hit.edu.cn
</email>
<sectionHeader confidence="0.99372" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998885">
Quantifying the semantic relevance be-
tween questions and their candidate an-
swers is essential to answer detection in
social media corpora. In this paper, a deep
belief network is proposed to model the
semantic relevance for question-answer
pairs. Observing the textual similarity
between the community-driven question-
answering (cQA) dataset and the forum
dataset, we present a novel learning strat-
egy to promote the performance of our
method on the social community datasets
without hand-annotating work. The ex-
perimental results show that our method
outperforms the traditional approaches on
both the cQA and the forum corpora.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999527">
In natural language processing (NLP) and infor-
mation retrieval (IR) fields, question answering
(QA) problem has attracted much attention over
the past few years. Nevertheless, most of the QA
researches mainly focus on locating the exact an-
swer to a given factoid question in the related doc-
uments. The most well known international evalu-
ation on the factoid QA task is the Text REtrieval
Conference (TREC)1, and the annotated questions
and answers released by TREC have become im-
portant resources for the researchers. However,
when facing a non-factoid question such as why,
how, or what about, however, almost no automatic
QA systems work very well.
The user-generated question-answer pairs are
definitely of great importance to solve the non-
factoid questions. Obviously, these natural QA
pairs are usually created during people’s com-
munication via Internet social media, among
which we are interested in the community-driven
</bodyText>
<footnote confidence="0.904084">
1http://trec.nist.gov
</footnote>
<bodyText confidence="0.953031641025641">
question-answering (cQA) sites and online fo-
rums. The cQA sites (or systems) provide plat-
forms where users can either ask questions or de-
liver answers, and best answers are selected man-
ually (e.g., Baidu Zhidao2 and Yahoo! Answers3).
Comparing with cQA sites, online forums have
more virtual society characteristics, where people
hold discussions in certain domains, such as tech-
niques, travel, sports, etc. Online forums contain
a huge number of QA pairs, and much noise infor-
mation is involved.
To make use of the QA pairs in cQA sites and
online forums, one has to face the challenging
problem of distinguishing the questions and their
answers from the noise. According to our investi-
gation, the data in the community based sites, es-
pecially for the forums, have two obvious charac-
teristics: (a) a post usually includes a very short
content, and when a person is initializing or re-
plying a post, an informal tone tends to be used;
(b) most of the posts are useless, which makes
the community become a noisy environment for
question-answer detection.
In this paper, a novel approach for modeling the
semantic relevance for QA pairs in the social me-
dia sites is proposed. We concentrate on the fol-
lowing two problems:
1. How to model the semantic relationship be-
tween two short texts using simple textual fea-
tures? As mentioned above, the user generated
questions and their answers via social media are
always short texts. The limitation of length leads
to the sparsity of the word features. In addition,
the word frequency is usually either 0 or 1, that is,
the frequency offers little information except the
occurrence of a word. Because of this situation,
the traditional relevance computing methods based
on word co-occurrence, such as Cosine similarity
and KL-divergence, are not effective for question-
</bodyText>
<footnote confidence="0.999966">
2http://zhidao.baidu.com
3http://answers.yahoo.com
</footnote>
<page confidence="0.775812">
1230
</page>
<note confidence="0.949437">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1230–1238,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.994367956521739">
answer semantic modeling. Most researchers try
to introduce structural features or users’ behavior
to improve the models performance, by contrast,
the effect of textual features is not obvious.
2. How to train a model so that it has good per-
formance on both cQA and forum datasets? So
far, people have been doing QA researches on the
cQA and the forum datasets separately (Ding et
al., 2008; Surdeanu et al., 2008), and no one has
noticed the relationship between the two kinds of
data. Since both the cQA systems and the online
forums are open platforms for people to commu-
nicate, the QA pairs in the cQA systems have sim-
ilarity with those in the forums. In this case, it is
highly valuable and desirable to propose a train-
ing strategy to improve the model’s performance
on both of the two kinds of datasets. In addition,
it is possible to avoid the expensive and arduous
hand-annotating work by introducing the method.
To solve the first problem, we present a deep
belief network (DBN) to model the semantic rel-
evance between questions and their answers. The
network establishes the semantic relationship for
QA pairs by minimizing the answer-to-question
reconstructing error. Using only word features,
our model outperforms the traditional methods on
question-answer relevance calculating.
For the second problem, we make our model
to learn the semantic knowledge from the solved
question threads in the cQA system. Instead of
mining the structure based features from cQA
pages and forum threads individually, we con-
sider the textual similarity between the two kinds
of data. The semantic information learned from
cQA corpus is helpful to detect answers in forums,
which makes our model show good performance
on social media corpora. Thanks to the labels for
the best answers existing in the threads, no manual
work is needed in our strategy.
The rest of this paper is organized as follows:
Section 2 surveys the related work. Section 3 in-
troduces the deep belief network for answer de-
tection. In Section 4, the homogenous data based
learning strategy is described. Experimental result
is given in Section 5. Finally, conclusions and fu-
ture directions are drawn in Section 6.
</bodyText>
<sectionHeader confidence="0.999802" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999746981481481">
The value of the naturally generated question-
answer pairs has not been recognized until recent
years. Early studies mainly focus on extracting
QA pairs from frequently asked questions (FAQ)
pages (Jijkoun and de Rijke, 2005; Riezler et al.,
2007) or service call-center dialogues (Berger et
al., 2000).
Judging whether a candidate answer is seman-
tically related to the question in the cQA page
automatically is a challenging task. A frame-
work for predicting the quality of answers has
been presented in (Jeon et al., 2006). Bernhard
and Gurevych (2009) have developed a transla-
tion based method to find answers. Surdeanu et
al. (2008) propose an approach to rank the an-
swers retrieved by Yahoo! Answers. Our work is
partly similar to Surdeanu et al. (2008), for we also
aim to rank the candidate answers reasonably, but
our ranking algorithm needs only word informa-
tion, instead of the combination of different kinds
of features.
Because people have considerable freedom to
post on forums, there are a great number of irrel-
evant posts for answering questions, which makes
it more difficult to detect answers in the forums.
In this field, exploratory studies have been done by
Feng et al. (2006) and Huang et al. (2007), who ex-
tract input-reply pairs for the discussion-bot. Ding
et al.(2008) and Cong et al.(2008) have also pre-
sented outstanding research works on forum QA
extraction. Ding et al. (2008) detect question con-
texts and answers using the conditional random
fields, and a ranking algorithm based on the au-
thority of forum users is proposed by Cong et al.
(2008). Treating answer detection as a binary clas-
sification problem is an intuitive idea, thus there
are some studies trying to solve it from this view
(Hong and Davison, 2009; Wang et al., 2009). Es-
pecially Hong and Davison (2009) have achieved
a rather high precision on the corpora with less
noise, which also shows the importance of “social”
features.
In order to select the answers for a given ques-
tion, one has to face the problem of lexical gap.
One of the problems with lexical gap embedding
is to find similar questions in QA achieves (Jeon et
al., 2005). Recently, the statistical machine trans-
lation (SMT) strategy has become popular. Lee et
al. (2008) use translate models to bridge the lexi-
cal gap between queries and questions in QA col-
lections. The SMT based methods are effective on
modeling the semantic relationship between ques-
tions and answers and expending users’ queries in
answer retrieval (Riezler et al., 2007; Berger et al.,
</bodyText>
<page confidence="0.982899">
1231
</page>
<bodyText confidence="0.99908325">
2000; Bernhard and Gurevych, 2009). In (Sur-
deanu et al., 2008), the translation model is used
to provide features for answer ranking.
The structural features (e.g., authorship, ac-
knowledgement, post position, etc), also called
non-textual features, play an important role in an-
swer extraction. Such features are used in (Ding
et al., 2008; Cong et al., 2008), and have signifi-
cantly improved the performance. The studies of
Jeon et al. (2006) and Hong et al. (2009) show that
the structural features have even more contribution
than the textual features. In this case, the mining
of textual features tends to be ignored.
There are also some other research topics in this
field. Cong et al. (2008) and Wang et al. (2009)
both propose the strategies to detect questions in
the social media corpus, which is proved to be a
non-trivial task. The deep research on question
detection has been taken by Duan et al. (2008).
A graph based algorithm is presented to answer
opinion questions (Li et al., 2009). In email sum-
marization field, the QA pairs are also extracted
from email contents as the main elements of email
summarization (Shrestha and McKeown, 2004).
</bodyText>
<sectionHeader confidence="0.981882" genericHeader="method">
3 The Deep Belief Network for QA pairs
</sectionHeader>
<bodyText confidence="0.999987375">
Due to the feature sparsity and the low word fre-
quency of the social media corpus, it is difficult
to model the semantic relevance between ques-
tions and answers using only co-occurrence fea-
tures. It is clear that the semantic link exists be-
tween the question and its answers, even though
they have totally different lexical representations.
Thus a specially designed model may learn se-
mantic knowledge by reconstructing a great num-
ber of questions using the information in the cor-
responding answers. In this section, we propose
a deep belief network for modeling the seman-
tic relationship between questions and their an-
swers. Our model is able to map the QA data into
a low-dimensional semantic-feature space, where
a question is close to its answers.
</bodyText>
<subsectionHeader confidence="0.999623">
3.1 The Restricted Boltzmann Machine
</subsectionHeader>
<bodyText confidence="0.999967705882353">
An ensemble of binary vectors can be modeled us-
ing a two-layer network called a “restricted Boltz-
mann machine” (RBM) (Hinton, 2002). The di-
mension reducing approach based on RBM ini-
tially shows good performance on image process-
ing (Hinton and Salakhutdinov, 2006). Salakhut-
dinov and Hinton (2009) propose a deep graphical
model composed of RBMs into the information re-
trieval field, which shows that this model is able to
obtain semantic information hidden in the word-
count vectors.
As shown in Figure 1, the RBM is a two-layer
network. The bottom layer represents a visible
vector v and the top layer represents a latent fea-
ture h. The matrix W contains the symmetric in-
teraction terms between the visible units and the
hidden units. Given an input vector v, the trained
</bodyText>
<figureCaption confidence="0.999622">
Figure 1: Restricted Boltzmann machine
</figureCaption>
<bodyText confidence="0.999946428571429">
RBM model provides a hidden feature h, which
can be used to reconstruct v with a minimum er-
ror. The training algorithm for this paper will be
described in the next subsection. The ability of the
RBM suggests us to build a deep belief network
based on RBM so that the semantic relevance be-
tween questions and answers can be modeled.
</bodyText>
<subsectionHeader confidence="0.999947">
3.2 Pretraining a Deep Belief Network
</subsectionHeader>
<bodyText confidence="0.999977875">
In the social media corpora, the answers are al-
ways descriptive, containing one or several sen-
tences. Noticing that an answer has strong seman-
tic association with the question and involves more
information than the question, we propose to train
a deep belief network by reconstructing the ques-
tion using its answers. The training object is to
minimize the error of reconstruction, and after the
pretraining process, a point that lies in a good re-
gion of parameter space can be achieved.
Firstly, the illustration of the DBN model is
given in Figure 2. This model is composed of
three layers, and here each layer stands for the
RBM or its variant. The bottom layer is a variant
form of RBM’s designed for the QA pairs. This
layer we design is a little different from the classi-
cal RBM’s, so that the bottom layer can generate
the hidden features according to the visible answer
vector and reconstruct the question vector using
the hidden features. The pre-training procedure of
this architecture is practically convergent. In the
bottom layer, the binary feature vectors based on
the statistics of the word occurrence in the answers
are used to compute the “hidden features” in the
</bodyText>
<page confidence="0.993322">
1232
</page>
<figureCaption confidence="0.999833">
Figure 2: The Deep Belief Network for QA Pairs
</figureCaption>
<bodyText confidence="0.998122333333333">
hidden units. The model can reconstruct the ques-
tions using the hidden features. The processes can
be modeled as follows:
</bodyText>
<equation confidence="0.99049925">
p(hj = 1|a) = �(bj + � wijai) (1)
i
p(qi = 1|h) = c-(bi + � wijhj) (2)
j
</equation>
<bodyText confidence="0.999997">
where c-(x) = 1/(1 + e−x), a denotes the visible
feature vector of the answer, qi is the ith element
of the question vector, and h stands for the hid-
den feature vector for reconstructing the questions.
wi j is a symmetric interaction term between word
i and hidden feature j, bi stands for the bias of the
model for word i, and bj denotes the bias of hidden
feature j.
Given the training set of answer vectors, the bot-
tom layer generates the corresponding hidden fea-
tures using Equation 1. Equation 2 is used to re-
construct the Bernoulli rates for each word in the
question vectors after stochastically activating the
hidden features. Then Equation 1 is taken again
to make the hidden features active. We use 1-step
Contrastive Divergence (Hinton, 2002) to update
the parameters by performing gradient ascent:
</bodyText>
<equation confidence="0.876089">
Awij = c(&lt; qihj &gt;qData − &lt; qihj &gt;qRecon) (3)
</equation>
<bodyText confidence="0.999987647058823">
where &lt; qihj &gt;qData denotes the expectation of
the frequency with which the word i in a ques-
tion and the feature j are on together when the
hidden features are driven by the question data.
&lt; qihj &gt;qRecon defines the corresponding expec-
tation when the hidden features are driven by the
reconstructed question data. c is the learning rate.
The classical RBM structure is taken to build
the middle layer and the top layer of the network.
The training method for the higher two layer is
similar to that of the bottom one, and we only have
to make each RBM to reconstruct the input data
using its hidden features. The parameter updates
still obeying the rule defined by gradient ascent,
which is quite similar to Equation 3. After train-
ing one layer, the h vectors are then sent to the
higher-level layer as its “training data”.
</bodyText>
<subsectionHeader confidence="0.999613">
3.3 Fine-tuning the Weights
</subsectionHeader>
<bodyText confidence="0.999988416666667">
Notice that a greedy strategy is taken to train each
layer individually during the pre-training proce-
dure, it is necessary to fine-tune the weights of the
entire network for optimal reconstruction. To fine-
tune the weights, the network is unrolled, taking
the answers as the input data to generate the corre-
sponding questions at the output units. Using the
cross-entropy error function, we can then tune the
network by performing backpropagation through
it. The experiment results in section 5.2 will show
fine-tuning makes the network performs better for
answer detection.
</bodyText>
<subsectionHeader confidence="0.998902">
3.4 Best answer detection
</subsectionHeader>
<bodyText confidence="0.999974727272727">
After pre-training and fine-tuning, a deep belief
network for QA pairs is established. To detect the
best answer to a given question, we just have to
send the vectors of the question and its candidate
answers into the input units of the network and
perform a level-by-level calculation to obtain the
corresponding feature vectors. Then we calculate
the distance between the mapped question vector
and each candidate answer vector. We consider the
candidate answer with the smallest distance as the
best one.
</bodyText>
<sectionHeader confidence="0.875891" genericHeader="method">
4 Learning with Homogenous Data
</sectionHeader>
<bodyText confidence="0.9999455">
In this section, we propose our strategy to make
our DBN model to detect answers in both cQA and
forum datasets, while the existing studies focus on
one single dataset.
</bodyText>
<subsectionHeader confidence="0.955573">
4.1 Homogenous QA Corpora from Different
Sources
</subsectionHeader>
<bodyText confidence="0.999898714285714">
Our motivation of finding the homogenous
question-answer corpora from different kind of so-
cial media is to guarantee the model’s performance
and avoid hand-annotating work.
In this paper, we get the “solved question” pages
in the computer technology domain from Baidu
Zhidao as the cQA corpus, and the threads of
</bodyText>
<page confidence="0.976288">
1233
</page>
<figureCaption confidence="0.999798">
Figure 3: Comparison of the post content lengths in the cQA and the forum datasets
</figureCaption>
<bodyText confidence="0.995021545454545">
ComputerFansClub Forum4 as the online forum
corpus. The domains of the corpora are the same.
To further explain that the two corpora are ho-
mogenous, we will give the detail comparison on
text style and word distribution.
As shown in Figure 3, we have compared the
post content lengths of the cQA and the forum
in our corpora. For the comparison, 5,000 posts
from the cQA corpus and 5,000 posts from the fo-
rum corpus are randomly selected. The left panel
shows the statistical result on the Baidu Zhidao
data, and the right panel shows the one on the fo-
rum data. The number i on the horizontal axis de-
notes the post contents whose lengths range from
10(i −1) + 1 to 10i bytes, and the vertical axis rep-
resents the counts of the post contents. From Fig-
ure 3 we observe that the contents of most posts
in both the cQA corpus and the forum corpus are
short, with the lengths not exceeding 400 bytes.
The content length reflects the text style of the
posts in cQA systems and online forums. From
Figure 3 it can be also seen that the distributions
of the content lengths in the two figures are very
similar. It shows that the contents in the two cor-
pora are both mainly short texts.
Figure 4 shows the percentage of the concurrent
words in the top-ranked content words with high
frequency. In detail, we firstly rank the words by
frequency in the two corpora. The words are cho-
sen based on a professional dictionary to guarantee
that they are meaningful in the computer knowl-
edge field. The number k on the horizontal axis in
Figure 4 represents the top k content words in the
</bodyText>
<footnote confidence="0.887202">
4http://bbs.cfanclub.net/
</footnote>
<bodyText confidence="0.936621">
corpora, and the vertical axis stands for the per-
centage of the words shared by the two corpora in
the top k words.
</bodyText>
<figureCaption confidence="0.999251">
Figure 4: Distribution of concurrent content words
</figureCaption>
<bodyText confidence="0.985322466666667">
Figure 4 shows that a large number of meaning-
ful words appear in both of the two corpora with
high frequencies. The percentage of the concur-
rent words maintains above 64% in the top 1,400
words. It indicates that the word distributions of
the two corpora are quite similar, although they
come from different social media sites.
Because the cQA corpus and the forum corpus
used in this study have homogenous characteris-
tics for answer detecting task, a simple strategy
may be used to avoid the hand-annotating work.
Apparently, in every “solved question” page of
Baidu Zhidao, the best answer is selected by the
user who asks this question. We can easily extract
the QA pairs from the cQA corpus as the training
</bodyText>
<page confidence="0.987223">
1234
</page>
<bodyText confidence="0.9999235">
set. Because the two corpora are similar, we can
apply the deep belief network trained by the cQA
corpus to detect answers on both the cQA data and
the forum data.
</bodyText>
<subsectionHeader confidence="0.839134">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.999951173913043">
The task of detecting answers in social media cor-
pora suffers from the problem of feature sparsity
seriously. High-dimensional feature vectors with
only several non-zero dimensions bring large time
consumption to our model. Thus it is necessary to
reduce the dimension of the feature vectors.
In this paper, we adopt two kinds of word fea-
tures. Firstly, we consider the 1,300 most fre-
quent words in the training set as Salakhutdinov
and Hinton (2009) did. According to our statis-
tics, the frequencies of the rest words are all less
then 10, which are not statistically significant and
may introduce much noise.
We take the occurrence of some function words
as another kind of features. The function words
are quite meaningful for judging whether a short
text is an answer or not, especially for the non-
factoid questions. For example, in the answers to
the causation questions, the words such as because
and so are more likely to appear; and the words
such as firstly, then, and should may suggest the
answers to the manner questions. We give an ex-
ample for function word selection in Figure 5.
</bodyText>
<figureCaption confidence="0.990111">
Figure 5: An example for function word selection
</figureCaption>
<bodyText confidence="0.999973428571429">
For this reason, we collect 200 most frequent
function words in the answers of the training set.
Then for every short text, either a question or an
answer, a 1,500-dimensional vector can be gener-
ated. Specifically, all the features we have adopted
are binary, for they only have to denote whether
the corresponding word appears in the text or not.
</bodyText>
<sectionHeader confidence="0.999609" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99988975">
To evaluate our question-answer semantic rele-
vance computing method, we compare our ap-
proach with the popular methods on the answer
detecting task.
</bodyText>
<subsectionHeader confidence="0.990654">
5.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999889604651163">
Architecture of the Network: To build the deep
belief network, we use a 1500-1500-1000-600 ar-
chitecture, which means the three layers of the net-
work have individually 1,500×1,500, 1,500×1,000
and 1,000×600 units. Using the network, a 1,500-
dimensional binary vector is finally mapped to a
600-dimensional real-value vector.
During the pretraining stage, the bottom layer
is greedily pretrained for 200 passes through the
entire training set, and each of the rest two layers is
greedily pretrained for 50 passes. For fine-tuning
we apply the method of conjugate gradients5, with
three line searches performed in each pass. This
algorithm is performed for 50 passes to fine-tune
the network.
Dataset: we have crawled 20,000 pages of
“solved question” from the computer and network
category of Baidu Zhidao as the cQA corpus. Cor-
respondingly we obtain 90,000 threads from Com-
puterFansClub, which is an online forum on com-
puter knowledge. We take the forum threads as
our forum corpus.
From the cQA corpus, we extract 12,600 human
generated QA pairs as the training set without any
manual work to label the best answers. We get the
contents from another 2,000 cQA pages to form
a testing set, each content of which includes one
question and 4.5 candidate answers on average,
with one best answer among them. To get another
testing dataset, we randomly select 2,000 threads
from the forum corpus. For this training set, hu-
man work are necessary to label the best answers
in the posts of the threads. There are 7 posts in-
cluded in each thread on average, among which
one question and at least one answer exist.
Baseline: To show the performance of our
method, three main popular relevance computing
methods for ranking candidate answers are con-
sidered as our baselines. We will briefly introduce
them:
Cosine Similarity. Given a question q and its
candidate answer a, their cosine similarity can be
computed as follows:
</bodyText>
<equation confidence="0.955138333333333">
En
cos(q, a) = �k=1 w qk X wa
k (4)
</equation>
<bodyText confidence="0.8356745">
jyn n
-k=1 w2qk Xjy-k=1 w2ak
where wqk and wak stand for the weight of the kth
word in the question and the answer respectively.
</bodyText>
<footnote confidence="0.990563">
5Code is available at
http://www.kyb.tuebingen.mpg.de/bs/people/carl/code/minimize/
</footnote>
<page confidence="0.994542">
1235
</page>
<bodyText confidence="0.999885947368421">
The weights can be get by computing the product
of term frequency (tf) and inverse document fre-
quency (idf)
HowNet based Similarity. HowNet6 is an elec-
tronic world knowledge system, which serves as
a powerful tool for meaning computation in hu-
man language technology. Normally the similar-
ity between two passages can be calculated by
two steps: (1) matching the most semantic-similar
words in each passages greedily using the API’s
provided by HowNet; (2) computing the weighted
average similarities of the word pairs. This strat-
egy is taken as a baseline method for computing
the relevance between questions and answers.
KL-divergence Language Model. Given a ques-
tion q and its candidate answer a, we can con-
struct unigram language model Mq and unigram
language model Ma. Then we compute KL-
divergence between Mq and Ma as below:
</bodyText>
<equation confidence="0.871111666666667">
� p(w|Ma) log(p(w|Ma)/p(w|Mq))
KL(Ma||Mq) _ (5)
w
</equation>
<subsectionHeader confidence="0.973484">
5.2 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.99996308">
We evaluate the performance of our approach for
answer detection using two metrics: Precision@1
(P@1) and Mean Reciprocal Rank (MRR). Ap-
plying the two metrics, we perform the baseline
methods and our DBN based methods on the two
testing set above.
Table 1 lists the results achieved on the forum
data using the baseline methods and ours. The ad-
ditional “Nearest Answer” stands for the method
without any ranking strategies, which returns the
nearest candidate answer from the question by po-
sition. To illustrate the effect of the fine-tuning for
our model, we list the results of our method with-
out fine-tuning and the results with fine-tuning.
As shown in Table 1, our deep belief network
based methods outperform the baseline methods
as expected. The main reason for the improve-
ments is that the DBN based approach is able to
learn semantic relationship between the words in
QA pairs from the training set. Although the train-
ing set we offer to the network comes from a dif-
ferent source (the cQA corpus), it still provide
enough knowledge to the network to perform bet-
ter than the baseline methods. This phenomena in-
dicates that the homogenous corpora for training is
</bodyText>
<footnote confidence="0.9874155">
6Detail information can be found in:
http://www.keenage.com/
</footnote>
<table confidence="0.958726375">
effective and meaningful.
Method P@1 (%) MRR (%)
Nearest Answer 21.25 38.72
Cosine Similarity 23.15 43.50
HowNet 22.55 41.63
KL divergence 25.30 51.40
DBN (without FT) 41.45 59.64
DBN (with FT) 45.00 62.03
</table>
<tableCaption confidence="0.999974">
Table 1: Results on Forum Dataset
</tableCaption>
<bodyText confidence="0.99956555">
We have also investigated the reasons for the un-
satisfying performance of the baseline approaches.
Basically, the low precision is ascribable to the
forum corpus we have obtained. As mentioned
in Section 1, the contents of the forum posts are
short, which leads to the sparsity of the features.
Besides, when users post messages in the online
forums, they are accustomed to be casual and use
some synonymous words interchangeably in the
posts, which is believed to be a significant situ-
ation in Chinese forums especially. Because the
features for QA pairs are quite sparse and the con-
tent words in the questions are usually morpholog-
ically different from the ones with the same mean-
ing in the answers, the Cosine Similarity method
become less powerful. For HowNet based ap-
proaches, there are a large number of words not
included by HowNet, thus it fails to compute the
similarity between questions and answers. KL-
divergence suffers from the same problems with
the Cosine Similarity method. Compared with
the Cosine Similarity method, this approach has
achieved the improvement of 9.3% in P@1, but
it performs much better than the other baseline
methods in MRR.
The baseline results indicate that the online fo-
rum is a complex environment with large amount
of noise for answer detection. Traditional IR
methods using pure textual features can hardly
achieve good results. The similar baseline results
for forum answer ranking are also achieved by
Hong and Davison (2009), which takes some non-
textual features to improve the algorithm’s perfor-
mance. We also notice that, however, the baseline
methods have obtained better results on forum cor-
pus (Cong et al., 2008). One possible reason is that
the baseline approaches are suitable for their data,
since we observe that the “nearest answer” strat-
egy has obtained a 73.5% precision in their work.
Our model has achieved the precision of
</bodyText>
<page confidence="0.97962">
1236
</page>
<bodyText confidence="0.999922448275862">
45.00% in P@1 and 62.03% in MRR for answer
detecting on forum data after fine-tuning, while
some related works have reported the results with
the precision over 90% (Cong et al., 2008; Hong
and Davison, 2009). There are mainly two rea-
sons for this phenomena: Firstly, both of the pre-
vious works have adopt non-textual features based
on the forum structure, such as authorship, po-
sition and quotes, etc. The non-textual (or so-
cial based) features have played a significant role
in improving the algorithms’ performance. Sec-
ondly, the quality of corpora influences the results
of the ranking strategies significantly, and even
the same algorithm may perform differently when
the dataset is changed (Hong and Davison, 2009).
For the experiments of this paper, large amount of
noise is involved in the forum corpus and we have
done nothing extra to filter it.
Table 2 shows the experimental results on the
cQA dataset. In this experiment, each sample is
composed of one question and its following sev-
eral candidate answers. We delete the ones with
only one answer to confirm there are at least two
candidate answers for each question. The candi-
date answers are rearranged by post time, so that
the real answers do not always appear next to the
questions. In this group of experiment, no hand-
annotating work is needed because the real an-
swers have been labeled by cQA users.
</bodyText>
<table confidence="0.999554571428572">
Method P@1 (%) MRR (%)
Nearest Answer 36.05 56.33
Cosine Similarity 44.05 62.84
HowNet 41.10 58.75
KL divergence 43.75 63.10
DBN (without FT) 56.20 70.56
DBN (with FT) 58.15 72.74
</table>
<tableCaption confidence="0.999798">
Table 2: Results on cQA Dataset
</tableCaption>
<bodyText confidence="0.999958482758621">
From Table 2 we observe that all the approaches
perform much better on this dataset. We attribute
the improvements to the high quality QA corpus
Baidu Zhidao offers: the candidate answers tend to
be more formal than the ones in the forums, with
less noise information included. In addition, the
“Nearest Answer” strategy has reached 36.05% in
P@1 on this dataset, which indicates quite a num-
ber of askers receive the real answers at the first
answer post. This result has supported the idea of
introducing position features. What’s more, if the
best answer appear immediately, the asker tends
to lock down the question thread, which helps to
reduce the noise information in the cQA corpus.
Despite the baseline methods’ performances
have been improved, our approaches still outper-
form them, with a 32.0% improvement in P@1
and a 15.3% improvement in MRR at least. On
the cQA dataset, our model shows better perfor-
mance than the previous experiment, which is ex-
pected because the training set and the testing set
come from the same corpus, and the DBN model
is more adaptive to the cQA data.
We have observed that, from both of the two
groups of experiments, fine-tuning is effective for
enhancing the performance of our model. On the
forum data, the results have been improved by
8.6% in P@1 and 4.0% in MRR, and the improve-
ments are 3.5% and 3.1% individually.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99992328">
In this paper, we have proposed a deep belief net-
work based approach to model the semantic rel-
evance for the question answering pairs in social
community corpora.
The contributions of this paper can be summa-
rized as follows: (1) The deep belief network we
present shows good performance on modeling the
QA pairs’ semantic relevance using only word fea-
tures. As a data driven approach, our model learns
semantic knowledge from large amount of QA
pairs to represent the semantic relevance between
questions and their answers. (2) We have stud-
ied the textual similarity between the cQA and the
forum datasets for QA pair extraction, and intro-
duce a novel learning strategy to make our method
show good performance on both cQA and forum
datasets. The experimental results show that our
method outperforms the traditional approaches on
both the cQA and the forum corpora.
Our future work will be carried out along two
directions. Firstly, we will further improve the
performance of our method by adopting the non-
textual features. Secondly, more research will be
taken to put forward other architectures of the deep
networks for QA detection.
</bodyText>
<sectionHeader confidence="0.998293" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9994006">
The authors are grateful to the anonymous re-
viewers for their constructive comments. Special
thanks to Deyuan Zhang, Bin Liu, Beidong Liu
and Ke Sun for insightful suggestions. This work
is supported by NSFC (60973076).
</bodyText>
<page confidence="0.991529">
1237
</page>
<sectionHeader confidence="0.987615" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998972162162163">
Adam Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the lexi-
cal chasm: Statistical approaches to answer-finding.
In In Proceedings of the 23rd annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 192–199.
Delphine Bernhard and Iryna Gurevych. 2009. Com-
bining lexical semantic resources with question &amp;
answer archives for translation-based answer find-
ing. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 728–736, Suntec,
Singapore, August. Association for Computational
Linguistics.
Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,
and Yueheng Sun. 2008. Finding question-answer
pairs from online forums. In SIGIR ’08: Proceed-
ings of the 31st annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, pages 467–474, New York, NY,
USA. ACM.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan
Zhu. 2008. Using conditional random fields to ex-
tract contexts and answers of questions from online
forums. In Proceedings of ACL-08: HLT, pages
710–718, Columbus, Ohio, June. Association for
Computational Linguistics.
Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and Yong
Yu. 2008. Searching questions by identifying ques-
tion topic and question focus. In Proceedings of
ACL-08: HLT, pages 156–164, Columbus, Ohio,
June. Association for Computational Linguistics.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard H.
Hovy. 2006. An intelligent discussion-bot for an-
swering student queries in threaded discussions. In
Ccile Paris and Candace L. Sidner, editors, IUI,
pages 171–177. ACM.
G. E. Hinton and R. R. Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504–507.
Georey E. Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural Com-
putation, 14.
Liangjie Hong and Brian D. Davison. 2009. A
classification-based approach to question answering
in discussion boards. In SIGIR ’09: Proceedings
of the 32nd international ACM SIGIR conference on
Research and development in information retrieval,
pages 171–178, New York, NY, USA. ACM.
Jizhou Huang, Ming Zhou, and Dan Yang. 2007. Ex-
tracting chatbot knowledge from online discussion
forums. In IJCAI’07: Proceedings of the 20th in-
ternational joint conference on Artifical intelligence,
pages 423–428, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In CIKM ’05, pages 84–90, New
York, NY, USA. ACM.
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon
Park. 2006. A framework to predict the quality of
answers with non-textual features. In SIGIR ’06,
pages 228–235, New York, NY, USA. ACM.
Valentin Jijkoun and Maarten de Rijke. 2005. Retriev-
ing answers from frequently asked questions pages
on the web. In CIKM ’05, pages 76–83, New York,
NY, USA. ACM.
Jung-Tae Lee, Sang-Bum Kim, Young-In Song, and
Hae-Chang Rim. 2008. Bridging lexical gaps be-
tween queries and questions on large online q&amp;a
collections with compact translation models. In
EMNLP ’08: Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 410–418, Morristown, NJ, USA. Association
for Computational Linguistics.
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with
random walks on graphs. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
737–745, Suntec, Singapore, August. Association
for Computational Linguistics.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In Proceedings of the 45th
Annual Meeting of the Association of Computa-
tional Linguistics, pages 464–471, Prague, Czech
Republic, June. Association for Computational
Linguistics.
Ruslan Salakhutdinov and Geoffrey Hinton. 2009.
Semantic hashing. Int. J. Approx. Reasoning,
50(7):969–978.
Lokesh Shrestha and Kathleen McKeown. 2004. De-
tection of question-answer pairs in email conversa-
tions. In Proceedings of Coling 2004, pages 889–
895, Geneva, Switzerland, Aug 23–Aug 27. COL-
ING.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to rank answers on large
online QA collections. In Proceedings of ACL-08:
HLT, pages 719–727, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Baoxun Wang, Bingquan Liu, Chengjie Sun, Xiao-
long Wang, and Lin Sun. 2009. Extracting chinese
question-answer pairs from online forums. In SMC
2009: Proceedings of the IEEE International Con-
ference on Systems, Man and Cybernetics, 2009.,
pages 1159–1164.
</reference>
<page confidence="0.992375">
1238
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970846">
<title confidence="0.999732">Modeling Semantic Relevance for Question-Answer Pairs in Web Social Communities</title>
<author confidence="0.999919">Baoxun Wang</author>
<author confidence="0.999919">Xiaolong Wang</author>
<author confidence="0.999919">Chengjie Sun</author>
<author confidence="0.999919">Bingquan Liu</author>
<author confidence="0.999919">Lin Sun</author>
<affiliation confidence="0.9999445">School of Computer Science and Technology Harbin Institute of Technology</affiliation>
<address confidence="0.985047">Harbin, China</address>
<email confidence="0.992124">wangxl,cjsun,liubq,</email>
<abstract confidence="0.999637529411765">Quantifying the semantic relevance between questions and their candidate answers is essential to answer detection in social media corpora. In this paper, a deep belief network is proposed to model the semantic relevance for question-answer pairs. Observing the textual similarity between the community-driven questionanswering (cQA) dataset and the forum dataset, we present a novel learning strategy to promote the performance of our method on the social community datasets without hand-annotating work. The experimental results show that our method outperforms the traditional approaches on both the cQA and the forum corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Rich Caruana</author>
<author>David Cohn</author>
<author>Dayne Freitag</author>
<author>Vibhu Mittal</author>
</authors>
<title>Bridging the lexical chasm: Statistical approaches to answer-finding. In</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="6435" citStr="Berger et al., 2000" startWordPosition="1015" endWordPosition="1018">anized as follows: Section 2 surveys the related work. Section 3 introduces the deep belief network for answer detection. In Section 4, the homogenous data based learning strategy is described. Experimental result is given in Section 5. Finally, conclusions and future directions are drawn in Section 6. 2 Related Work The value of the naturally generated questionanswer pairs has not been recognized until recent years. Early studies mainly focus on extracting QA pairs from frequently asked questions (FAQ) pages (Jijkoun and de Rijke, 2005; Riezler et al., 2007) or service call-center dialogues (Berger et al., 2000). Judging whether a candidate answer is semantically related to the question in the cQA page automatically is a challenging task. A framework for predicting the quality of answers has been presented in (Jeon et al., 2006). Bernhard and Gurevych (2009) have developed a translation based method to find answers. Surdeanu et al. (2008) propose an approach to rank the answers retrieved by Yahoo! Answers. Our work is partly similar to Surdeanu et al. (2008), for we also aim to rank the candidate answers reasonably, but our ranking algorithm needs only word information, instead of the combination of </context>
</contexts>
<marker>Berger, Caruana, Cohn, Freitag, Mittal, 2000</marker>
<rawString>Adam Berger, Rich Caruana, David Cohn, Dayne Freitag, and Vibhu Mittal. 2000. Bridging the lexical chasm: Statistical approaches to answer-finding. In In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delphine Bernhard</author>
<author>Iryna Gurevych</author>
</authors>
<title>Combining lexical semantic resources with question &amp; answer archives for translation-based answer finding.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>728--736</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="6686" citStr="Bernhard and Gurevych (2009)" startWordPosition="1057" endWordPosition="1060">y, conclusions and future directions are drawn in Section 6. 2 Related Work The value of the naturally generated questionanswer pairs has not been recognized until recent years. Early studies mainly focus on extracting QA pairs from frequently asked questions (FAQ) pages (Jijkoun and de Rijke, 2005; Riezler et al., 2007) or service call-center dialogues (Berger et al., 2000). Judging whether a candidate answer is semantically related to the question in the cQA page automatically is a challenging task. A framework for predicting the quality of answers has been presented in (Jeon et al., 2006). Bernhard and Gurevych (2009) have developed a translation based method to find answers. Surdeanu et al. (2008) propose an approach to rank the answers retrieved by Yahoo! Answers. Our work is partly similar to Surdeanu et al. (2008), for we also aim to rank the candidate answers reasonably, but our ranking algorithm needs only word information, instead of the combination of different kinds of features. Because people have considerable freedom to post on forums, there are a great number of irrelevant posts for answering questions, which makes it more difficult to detect answers in the forums. In this field, exploratory st</context>
<context position="8692" citStr="Bernhard and Gurevych, 2009" startWordPosition="1398" endWordPosition="1401">er to select the answers for a given question, one has to face the problem of lexical gap. One of the problems with lexical gap embedding is to find similar questions in QA achieves (Jeon et al., 2005). Recently, the statistical machine translation (SMT) strategy has become popular. Lee et al. (2008) use translate models to bridge the lexical gap between queries and questions in QA collections. The SMT based methods are effective on modeling the semantic relationship between questions and answers and expending users’ queries in answer retrieval (Riezler et al., 2007; Berger et al., 1231 2000; Bernhard and Gurevych, 2009). In (Surdeanu et al., 2008), the translation model is used to provide features for answer ranking. The structural features (e.g., authorship, acknowledgement, post position, etc), also called non-textual features, play an important role in answer extraction. Such features are used in (Ding et al., 2008; Cong et al., 2008), and have significantly improved the performance. The studies of Jeon et al. (2006) and Hong et al. (2009) show that the structural features have even more contribution than the textual features. In this case, the mining of textual features tends to be ignored. There are als</context>
</contexts>
<marker>Bernhard, Gurevych, 2009</marker>
<rawString>Delphine Bernhard and Iryna Gurevych. 2009. Combining lexical semantic resources with question &amp; answer archives for translation-based answer finding. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 728–736, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gao Cong</author>
<author>Long Wang</author>
<author>Chin-Yew Lin</author>
<author>Young-In Song</author>
<author>Yueheng Sun</author>
</authors>
<title>Finding question-answer pairs from online forums.</title>
<date>2008</date>
<booktitle>In SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>467--474</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7707" citStr="Cong et al. (2008)" startWordPosition="1231" endWordPosition="1234">derable freedom to post on forums, there are a great number of irrelevant posts for answering questions, which makes it more difficult to detect answers in the forums. In this field, exploratory studies have been done by Feng et al. (2006) and Huang et al. (2007), who extract input-reply pairs for the discussion-bot. Ding et al.(2008) and Cong et al.(2008) have also presented outstanding research works on forum QA extraction. Ding et al. (2008) detect question contexts and answers using the conditional random fields, and a ranking algorithm based on the authority of forum users is proposed by Cong et al. (2008). Treating answer detection as a binary classification problem is an intuitive idea, thus there are some studies trying to solve it from this view (Hong and Davison, 2009; Wang et al., 2009). Especially Hong and Davison (2009) have achieved a rather high precision on the corpora with less noise, which also shows the importance of “social” features. In order to select the answers for a given question, one has to face the problem of lexical gap. One of the problems with lexical gap embedding is to find similar questions in QA achieves (Jeon et al., 2005). Recently, the statistical machine transl</context>
<context position="9016" citStr="Cong et al., 2008" startWordPosition="1450" endWordPosition="1453">he lexical gap between queries and questions in QA collections. The SMT based methods are effective on modeling the semantic relationship between questions and answers and expending users’ queries in answer retrieval (Riezler et al., 2007; Berger et al., 1231 2000; Bernhard and Gurevych, 2009). In (Surdeanu et al., 2008), the translation model is used to provide features for answer ranking. The structural features (e.g., authorship, acknowledgement, post position, etc), also called non-textual features, play an important role in answer extraction. Such features are used in (Ding et al., 2008; Cong et al., 2008), and have significantly improved the performance. The studies of Jeon et al. (2006) and Hong et al. (2009) show that the structural features have even more contribution than the textual features. In this case, the mining of textual features tends to be ignored. There are also some other research topics in this field. Cong et al. (2008) and Wang et al. (2009) both propose the strategies to detect questions in the social media corpus, which is proved to be a non-trivial task. The deep research on question detection has been taken by Duan et al. (2008). A graph based algorithm is presented to an</context>
<context position="27256" citStr="Cong et al., 2008" startWordPosition="4553" endWordPosition="4556">ch has achieved the improvement of 9.3% in P@1, but it performs much better than the other baseline methods in MRR. The baseline results indicate that the online forum is a complex environment with large amount of noise for answer detection. Traditional IR methods using pure textual features can hardly achieve good results. The similar baseline results for forum answer ranking are also achieved by Hong and Davison (2009), which takes some nontextual features to improve the algorithm’s performance. We also notice that, however, the baseline methods have obtained better results on forum corpus (Cong et al., 2008). One possible reason is that the baseline approaches are suitable for their data, since we observe that the “nearest answer” strategy has obtained a 73.5% precision in their work. Our model has achieved the precision of 1236 45.00% in P@1 and 62.03% in MRR for answer detecting on forum data after fine-tuning, while some related works have reported the results with the precision over 90% (Cong et al., 2008; Hong and Davison, 2009). There are mainly two reasons for this phenomena: Firstly, both of the previous works have adopt non-textual features based on the forum structure, such as authorshi</context>
</contexts>
<marker>Cong, Wang, Lin, Song, Sun, 2008</marker>
<rawString>Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song, and Yueheng Sun. 2008. Finding question-answer pairs from online forums. In SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 467–474, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shilin Ding</author>
<author>Gao Cong</author>
<author>Chin-Yew Lin</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Using conditional random fields to extract contexts and answers of questions from online forums.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>710--718</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="4330" citStr="Ding et al., 2008" startWordPosition="670" endWordPosition="673">du.com 3http://answers.yahoo.com 1230 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1230–1238, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics answer semantic modeling. Most researchers try to introduce structural features or users’ behavior to improve the models performance, by contrast, the effect of textual features is not obvious. 2. How to train a model so that it has good performance on both cQA and forum datasets? So far, people have been doing QA researches on the cQA and the forum datasets separately (Ding et al., 2008; Surdeanu et al., 2008), and no one has noticed the relationship between the two kinds of data. Since both the cQA systems and the online forums are open platforms for people to communicate, the QA pairs in the cQA systems have similarity with those in the forums. In this case, it is highly valuable and desirable to propose a training strategy to improve the model’s performance on both of the two kinds of datasets. In addition, it is possible to avoid the expensive and arduous hand-annotating work by introducing the method. To solve the first problem, we present a deep belief network (DBN) to</context>
<context position="7537" citStr="Ding et al. (2008)" startWordPosition="1201" endWordPosition="1204"> candidate answers reasonably, but our ranking algorithm needs only word information, instead of the combination of different kinds of features. Because people have considerable freedom to post on forums, there are a great number of irrelevant posts for answering questions, which makes it more difficult to detect answers in the forums. In this field, exploratory studies have been done by Feng et al. (2006) and Huang et al. (2007), who extract input-reply pairs for the discussion-bot. Ding et al.(2008) and Cong et al.(2008) have also presented outstanding research works on forum QA extraction. Ding et al. (2008) detect question contexts and answers using the conditional random fields, and a ranking algorithm based on the authority of forum users is proposed by Cong et al. (2008). Treating answer detection as a binary classification problem is an intuitive idea, thus there are some studies trying to solve it from this view (Hong and Davison, 2009; Wang et al., 2009). Especially Hong and Davison (2009) have achieved a rather high precision on the corpora with less noise, which also shows the importance of “social” features. In order to select the answers for a given question, one has to face the proble</context>
<context position="8996" citStr="Ding et al., 2008" startWordPosition="1446" endWordPosition="1449"> models to bridge the lexical gap between queries and questions in QA collections. The SMT based methods are effective on modeling the semantic relationship between questions and answers and expending users’ queries in answer retrieval (Riezler et al., 2007; Berger et al., 1231 2000; Bernhard and Gurevych, 2009). In (Surdeanu et al., 2008), the translation model is used to provide features for answer ranking. The structural features (e.g., authorship, acknowledgement, post position, etc), also called non-textual features, play an important role in answer extraction. Such features are used in (Ding et al., 2008; Cong et al., 2008), and have significantly improved the performance. The studies of Jeon et al. (2006) and Hong et al. (2009) show that the structural features have even more contribution than the textual features. In this case, the mining of textual features tends to be ignored. There are also some other research topics in this field. Cong et al. (2008) and Wang et al. (2009) both propose the strategies to detect questions in the social media corpus, which is proved to be a non-trivial task. The deep research on question detection has been taken by Duan et al. (2008). A graph based algorith</context>
</contexts>
<marker>Ding, Cong, Lin, Zhu, 2008</marker>
<rawString>Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan Zhu. 2008. Using conditional random fields to extract contexts and answers of questions from online forums. In Proceedings of ACL-08: HLT, pages 710–718, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huizhong Duan</author>
<author>Yunbo Cao</author>
<author>Chin-Yew Lin</author>
<author>Yong Yu</author>
</authors>
<title>Searching questions by identifying question topic and question focus.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>156--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="9572" citStr="Duan et al. (2008)" startWordPosition="1547" endWordPosition="1550">ch features are used in (Ding et al., 2008; Cong et al., 2008), and have significantly improved the performance. The studies of Jeon et al. (2006) and Hong et al. (2009) show that the structural features have even more contribution than the textual features. In this case, the mining of textual features tends to be ignored. There are also some other research topics in this field. Cong et al. (2008) and Wang et al. (2009) both propose the strategies to detect questions in the social media corpus, which is proved to be a non-trivial task. The deep research on question detection has been taken by Duan et al. (2008). A graph based algorithm is presented to answer opinion questions (Li et al., 2009). In email summarization field, the QA pairs are also extracted from email contents as the main elements of email summarization (Shrestha and McKeown, 2004). 3 The Deep Belief Network for QA pairs Due to the feature sparsity and the low word frequency of the social media corpus, it is difficult to model the semantic relevance between questions and answers using only co-occurrence features. It is clear that the semantic link exists between the question and its answers, even though they have totally different lex</context>
</contexts>
<marker>Duan, Cao, Lin, Yu, 2008</marker>
<rawString>Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and Yong Yu. 2008. Searching questions by identifying question topic and question focus. In Proceedings of ACL-08: HLT, pages 156–164, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donghui Feng</author>
<author>Erin Shaw</author>
<author>Jihie Kim</author>
<author>Eduard H Hovy</author>
</authors>
<title>An intelligent discussion-bot for answering student queries in threaded discussions.</title>
<date>2006</date>
<booktitle>In Ccile Paris and Candace</booktitle>
<pages>171--177</pages>
<editor>L. Sidner, editors, IUI,</editor>
<publisher>ACM.</publisher>
<contexts>
<context position="7328" citStr="Feng et al. (2006)" startWordPosition="1166" endWordPosition="1169">ation based method to find answers. Surdeanu et al. (2008) propose an approach to rank the answers retrieved by Yahoo! Answers. Our work is partly similar to Surdeanu et al. (2008), for we also aim to rank the candidate answers reasonably, but our ranking algorithm needs only word information, instead of the combination of different kinds of features. Because people have considerable freedom to post on forums, there are a great number of irrelevant posts for answering questions, which makes it more difficult to detect answers in the forums. In this field, exploratory studies have been done by Feng et al. (2006) and Huang et al. (2007), who extract input-reply pairs for the discussion-bot. Ding et al.(2008) and Cong et al.(2008) have also presented outstanding research works on forum QA extraction. Ding et al. (2008) detect question contexts and answers using the conditional random fields, and a ranking algorithm based on the authority of forum users is proposed by Cong et al. (2008). Treating answer detection as a binary classification problem is an intuitive idea, thus there are some studies trying to solve it from this view (Hong and Davison, 2009; Wang et al., 2009). Especially Hong and Davison (</context>
</contexts>
<marker>Feng, Shaw, Kim, Hovy, 2006</marker>
<rawString>Donghui Feng, Erin Shaw, Jihie Kim, and Eduard H. Hovy. 2006. An intelligent discussion-bot for answering student queries in threaded discussions. In Ccile Paris and Candace L. Sidner, editors, IUI, pages 171–177. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
<author>R R Salakhutdinov</author>
</authors>
<title>Reducing the dimensionality of data with neural networks.</title>
<date>2006</date>
<journal>Science,</journal>
<volume>313</volume>
<issue>5786</issue>
<contexts>
<context position="10905" citStr="Hinton and Salakhutdinov, 2006" startWordPosition="1768" endWordPosition="1771"> great number of questions using the information in the corresponding answers. In this section, we propose a deep belief network for modeling the semantic relationship between questions and their answers. Our model is able to map the QA data into a low-dimensional semantic-feature space, where a question is close to its answers. 3.1 The Restricted Boltzmann Machine An ensemble of binary vectors can be modeled using a two-layer network called a “restricted Boltzmann machine” (RBM) (Hinton, 2002). The dimension reducing approach based on RBM initially shows good performance on image processing (Hinton and Salakhutdinov, 2006). Salakhutdinov and Hinton (2009) propose a deep graphical model composed of RBMs into the information retrieval field, which shows that this model is able to obtain semantic information hidden in the wordcount vectors. As shown in Figure 1, the RBM is a two-layer network. The bottom layer represents a visible vector v and the top layer represents a latent feature h. The matrix W contains the symmetric interaction terms between the visible units and the hidden units. Given an input vector v, the trained Figure 1: Restricted Boltzmann machine RBM model provides a hidden feature h, which can be </context>
</contexts>
<marker>Hinton, Salakhutdinov, 2006</marker>
<rawString>G. E. Hinton and R. R. Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georey E Hinton</author>
</authors>
<title>Training products of experts by minimizing contrastive divergence.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<volume>14</volume>
<contexts>
<context position="10773" citStr="Hinton, 2002" startWordPosition="1749" endWordPosition="1750">ifferent lexical representations. Thus a specially designed model may learn semantic knowledge by reconstructing a great number of questions using the information in the corresponding answers. In this section, we propose a deep belief network for modeling the semantic relationship between questions and their answers. Our model is able to map the QA data into a low-dimensional semantic-feature space, where a question is close to its answers. 3.1 The Restricted Boltzmann Machine An ensemble of binary vectors can be modeled using a two-layer network called a “restricted Boltzmann machine” (RBM) (Hinton, 2002). The dimension reducing approach based on RBM initially shows good performance on image processing (Hinton and Salakhutdinov, 2006). Salakhutdinov and Hinton (2009) propose a deep graphical model composed of RBMs into the information retrieval field, which shows that this model is able to obtain semantic information hidden in the wordcount vectors. As shown in Figure 1, the RBM is a two-layer network. The bottom layer represents a visible vector v and the top layer represents a latent feature h. The matrix W contains the symmetric interaction terms between the visible units and the hidden uni</context>
<context position="14004" citStr="Hinton, 2002" startWordPosition="2315" endWordPosition="2316">r the hidden feature vector for reconstructing the questions. wi j is a symmetric interaction term between word i and hidden feature j, bi stands for the bias of the model for word i, and bj denotes the bias of hidden feature j. Given the training set of answer vectors, the bottom layer generates the corresponding hidden features using Equation 1. Equation 2 is used to reconstruct the Bernoulli rates for each word in the question vectors after stochastically activating the hidden features. Then Equation 1 is taken again to make the hidden features active. We use 1-step Contrastive Divergence (Hinton, 2002) to update the parameters by performing gradient ascent: Awij = c(&lt; qihj &gt;qData − &lt; qihj &gt;qRecon) (3) where &lt; qihj &gt;qData denotes the expectation of the frequency with which the word i in a question and the feature j are on together when the hidden features are driven by the question data. &lt; qihj &gt;qRecon defines the corresponding expectation when the hidden features are driven by the reconstructed question data. c is the learning rate. The classical RBM structure is taken to build the middle layer and the top layer of the network. The training method for the higher two layer is similar to that</context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>Georey E. Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural Computation, 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liangjie Hong</author>
<author>Brian D Davison</author>
</authors>
<title>A classification-based approach to question answering in discussion boards.</title>
<date>2009</date>
<booktitle>In SIGIR ’09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>171--178</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7877" citStr="Hong and Davison, 2009" startWordPosition="1260" endWordPosition="1263">. In this field, exploratory studies have been done by Feng et al. (2006) and Huang et al. (2007), who extract input-reply pairs for the discussion-bot. Ding et al.(2008) and Cong et al.(2008) have also presented outstanding research works on forum QA extraction. Ding et al. (2008) detect question contexts and answers using the conditional random fields, and a ranking algorithm based on the authority of forum users is proposed by Cong et al. (2008). Treating answer detection as a binary classification problem is an intuitive idea, thus there are some studies trying to solve it from this view (Hong and Davison, 2009; Wang et al., 2009). Especially Hong and Davison (2009) have achieved a rather high precision on the corpora with less noise, which also shows the importance of “social” features. In order to select the answers for a given question, one has to face the problem of lexical gap. One of the problems with lexical gap embedding is to find similar questions in QA achieves (Jeon et al., 2005). Recently, the statistical machine translation (SMT) strategy has become popular. Lee et al. (2008) use translate models to bridge the lexical gap between queries and questions in QA collections. The SMT based m</context>
<context position="27062" citStr="Hong and Davison (2009)" startWordPosition="4521" endWordPosition="4524">t fails to compute the similarity between questions and answers. KLdivergence suffers from the same problems with the Cosine Similarity method. Compared with the Cosine Similarity method, this approach has achieved the improvement of 9.3% in P@1, but it performs much better than the other baseline methods in MRR. The baseline results indicate that the online forum is a complex environment with large amount of noise for answer detection. Traditional IR methods using pure textual features can hardly achieve good results. The similar baseline results for forum answer ranking are also achieved by Hong and Davison (2009), which takes some nontextual features to improve the algorithm’s performance. We also notice that, however, the baseline methods have obtained better results on forum corpus (Cong et al., 2008). One possible reason is that the baseline approaches are suitable for their data, since we observe that the “nearest answer” strategy has obtained a 73.5% precision in their work. Our model has achieved the precision of 1236 45.00% in P@1 and 62.03% in MRR for answer detecting on forum data after fine-tuning, while some related works have reported the results with the precision over 90% (Cong et al., 2</context>
</contexts>
<marker>Hong, Davison, 2009</marker>
<rawString>Liangjie Hong and Brian D. Davison. 2009. A classification-based approach to question answering in discussion boards. In SIGIR ’09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 171–178, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jizhou Huang</author>
<author>Ming Zhou</author>
<author>Dan Yang</author>
</authors>
<title>Extracting chatbot knowledge from online discussion forums.</title>
<date>2007</date>
<booktitle>In IJCAI’07: Proceedings of the 20th international joint conference on Artifical intelligence,</booktitle>
<pages>423--428</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="7352" citStr="Huang et al. (2007)" startWordPosition="1171" endWordPosition="1174">ind answers. Surdeanu et al. (2008) propose an approach to rank the answers retrieved by Yahoo! Answers. Our work is partly similar to Surdeanu et al. (2008), for we also aim to rank the candidate answers reasonably, but our ranking algorithm needs only word information, instead of the combination of different kinds of features. Because people have considerable freedom to post on forums, there are a great number of irrelevant posts for answering questions, which makes it more difficult to detect answers in the forums. In this field, exploratory studies have been done by Feng et al. (2006) and Huang et al. (2007), who extract input-reply pairs for the discussion-bot. Ding et al.(2008) and Cong et al.(2008) have also presented outstanding research works on forum QA extraction. Ding et al. (2008) detect question contexts and answers using the conditional random fields, and a ranking algorithm based on the authority of forum users is proposed by Cong et al. (2008). Treating answer detection as a binary classification problem is an intuitive idea, thus there are some studies trying to solve it from this view (Hong and Davison, 2009; Wang et al., 2009). Especially Hong and Davison (2009) have achieved a ra</context>
</contexts>
<marker>Huang, Zhou, Yang, 2007</marker>
<rawString>Jizhou Huang, Ming Zhou, and Dan Yang. 2007. Extracting chatbot knowledge from online discussion forums. In IJCAI’07: Proceedings of the 20th international joint conference on Artifical intelligence, pages 423–428, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
</authors>
<title>Finding similar questions in large question and answer archives.</title>
<date>2005</date>
<booktitle>In CIKM ’05,</booktitle>
<pages>84--90</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8265" citStr="Jeon et al., 2005" startWordPosition="1330" endWordPosition="1333">e authority of forum users is proposed by Cong et al. (2008). Treating answer detection as a binary classification problem is an intuitive idea, thus there are some studies trying to solve it from this view (Hong and Davison, 2009; Wang et al., 2009). Especially Hong and Davison (2009) have achieved a rather high precision on the corpora with less noise, which also shows the importance of “social” features. In order to select the answers for a given question, one has to face the problem of lexical gap. One of the problems with lexical gap embedding is to find similar questions in QA achieves (Jeon et al., 2005). Recently, the statistical machine translation (SMT) strategy has become popular. Lee et al. (2008) use translate models to bridge the lexical gap between queries and questions in QA collections. The SMT based methods are effective on modeling the semantic relationship between questions and answers and expending users’ queries in answer retrieval (Riezler et al., 2007; Berger et al., 1231 2000; Bernhard and Gurevych, 2009). In (Surdeanu et al., 2008), the translation model is used to provide features for answer ranking. The structural features (e.g., authorship, acknowledgement, post position</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005. Finding similar questions in large question and answer archives. In CIKM ’05, pages 84–90, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
<author>Soyeon Park</author>
</authors>
<title>A framework to predict the quality of answers with non-textual features.</title>
<date>2006</date>
<booktitle>In SIGIR ’06,</booktitle>
<pages>228--235</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6656" citStr="Jeon et al., 2006" startWordPosition="1053" endWordPosition="1056">in Section 5. Finally, conclusions and future directions are drawn in Section 6. 2 Related Work The value of the naturally generated questionanswer pairs has not been recognized until recent years. Early studies mainly focus on extracting QA pairs from frequently asked questions (FAQ) pages (Jijkoun and de Rijke, 2005; Riezler et al., 2007) or service call-center dialogues (Berger et al., 2000). Judging whether a candidate answer is semantically related to the question in the cQA page automatically is a challenging task. A framework for predicting the quality of answers has been presented in (Jeon et al., 2006). Bernhard and Gurevych (2009) have developed a translation based method to find answers. Surdeanu et al. (2008) propose an approach to rank the answers retrieved by Yahoo! Answers. Our work is partly similar to Surdeanu et al. (2008), for we also aim to rank the candidate answers reasonably, but our ranking algorithm needs only word information, instead of the combination of different kinds of features. Because people have considerable freedom to post on forums, there are a great number of irrelevant posts for answering questions, which makes it more difficult to detect answers in the forums.</context>
<context position="9100" citStr="Jeon et al. (2006)" startWordPosition="1464" endWordPosition="1467">s are effective on modeling the semantic relationship between questions and answers and expending users’ queries in answer retrieval (Riezler et al., 2007; Berger et al., 1231 2000; Bernhard and Gurevych, 2009). In (Surdeanu et al., 2008), the translation model is used to provide features for answer ranking. The structural features (e.g., authorship, acknowledgement, post position, etc), also called non-textual features, play an important role in answer extraction. Such features are used in (Ding et al., 2008; Cong et al., 2008), and have significantly improved the performance. The studies of Jeon et al. (2006) and Hong et al. (2009) show that the structural features have even more contribution than the textual features. In this case, the mining of textual features tends to be ignored. There are also some other research topics in this field. Cong et al. (2008) and Wang et al. (2009) both propose the strategies to detect questions in the social media corpus, which is proved to be a non-trivial task. The deep research on question detection has been taken by Duan et al. (2008). A graph based algorithm is presented to answer opinion questions (Li et al., 2009). In email summarization field, the QA pairs</context>
</contexts>
<marker>Jeon, Croft, Lee, Park, 2006</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon Park. 2006. A framework to predict the quality of answers with non-textual features. In SIGIR ’06, pages 228–235, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin Jijkoun</author>
<author>Maarten de Rijke</author>
</authors>
<title>Retrieving answers from frequently asked questions pages on the web.</title>
<date>2005</date>
<booktitle>In CIKM ’05,</booktitle>
<pages>76--83</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Jijkoun, de Rijke, 2005</marker>
<rawString>Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving answers from frequently asked questions pages on the web. In CIKM ’05, pages 76–83, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jung-Tae Lee</author>
<author>Sang-Bum Kim</author>
<author>Young-In Song</author>
<author>Hae-Chang Rim</author>
</authors>
<title>Bridging lexical gaps between queries and questions on large online q&amp;a collections with compact translation models.</title>
<date>2008</date>
<booktitle>In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>410--418</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8365" citStr="Lee et al. (2008)" startWordPosition="1345" endWordPosition="1348">lassification problem is an intuitive idea, thus there are some studies trying to solve it from this view (Hong and Davison, 2009; Wang et al., 2009). Especially Hong and Davison (2009) have achieved a rather high precision on the corpora with less noise, which also shows the importance of “social” features. In order to select the answers for a given question, one has to face the problem of lexical gap. One of the problems with lexical gap embedding is to find similar questions in QA achieves (Jeon et al., 2005). Recently, the statistical machine translation (SMT) strategy has become popular. Lee et al. (2008) use translate models to bridge the lexical gap between queries and questions in QA collections. The SMT based methods are effective on modeling the semantic relationship between questions and answers and expending users’ queries in answer retrieval (Riezler et al., 2007; Berger et al., 1231 2000; Bernhard and Gurevych, 2009). In (Surdeanu et al., 2008), the translation model is used to provide features for answer ranking. The structural features (e.g., authorship, acknowledgement, post position, etc), also called non-textual features, play an important role in answer extraction. Such features</context>
</contexts>
<marker>Lee, Kim, Song, Rim, 2008</marker>
<rawString>Jung-Tae Lee, Sang-Bum Kim, Young-In Song, and Hae-Chang Rim. 2008. Bridging lexical gaps between queries and questions on large online q&amp;a collections with compact translation models. In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 410–418, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Yang Tang</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Answering opinion questions with random walks on graphs.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>737--745</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="9656" citStr="Li et al., 2009" startWordPosition="1561" endWordPosition="1564"> improved the performance. The studies of Jeon et al. (2006) and Hong et al. (2009) show that the structural features have even more contribution than the textual features. In this case, the mining of textual features tends to be ignored. There are also some other research topics in this field. Cong et al. (2008) and Wang et al. (2009) both propose the strategies to detect questions in the social media corpus, which is proved to be a non-trivial task. The deep research on question detection has been taken by Duan et al. (2008). A graph based algorithm is presented to answer opinion questions (Li et al., 2009). In email summarization field, the QA pairs are also extracted from email contents as the main elements of email summarization (Shrestha and McKeown, 2004). 3 The Deep Belief Network for QA pairs Due to the feature sparsity and the low word frequency of the social media corpus, it is difficult to model the semantic relevance between questions and answers using only co-occurrence features. It is clear that the semantic link exists between the question and its answers, even though they have totally different lexical representations. Thus a specially designed model may learn semantic knowledge b</context>
</contexts>
<marker>Li, Tang, Huang, Zhu, 2009</marker>
<rawString>Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan Zhu. 2009. Answering opinion questions with random walks on graphs. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 737–745, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
<author>Ioannis Tsochantaridis</author>
<author>Vibhu Mittal</author>
<author>Yi Liu</author>
</authors>
<title>Statistical machine translation for query expansion in answer retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>464--471</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6380" citStr="Riezler et al., 2007" startWordPosition="1007" endWordPosition="1010">is needed in our strategy. The rest of this paper is organized as follows: Section 2 surveys the related work. Section 3 introduces the deep belief network for answer detection. In Section 4, the homogenous data based learning strategy is described. Experimental result is given in Section 5. Finally, conclusions and future directions are drawn in Section 6. 2 Related Work The value of the naturally generated questionanswer pairs has not been recognized until recent years. Early studies mainly focus on extracting QA pairs from frequently asked questions (FAQ) pages (Jijkoun and de Rijke, 2005; Riezler et al., 2007) or service call-center dialogues (Berger et al., 2000). Judging whether a candidate answer is semantically related to the question in the cQA page automatically is a challenging task. A framework for predicting the quality of answers has been presented in (Jeon et al., 2006). Bernhard and Gurevych (2009) have developed a translation based method to find answers. Surdeanu et al. (2008) propose an approach to rank the answers retrieved by Yahoo! Answers. Our work is partly similar to Surdeanu et al. (2008), for we also aim to rank the candidate answers reasonably, but our ranking algorithm need</context>
<context position="8636" citStr="Riezler et al., 2007" startWordPosition="1389" endWordPosition="1392">hows the importance of “social” features. In order to select the answers for a given question, one has to face the problem of lexical gap. One of the problems with lexical gap embedding is to find similar questions in QA achieves (Jeon et al., 2005). Recently, the statistical machine translation (SMT) strategy has become popular. Lee et al. (2008) use translate models to bridge the lexical gap between queries and questions in QA collections. The SMT based methods are effective on modeling the semantic relationship between questions and answers and expending users’ queries in answer retrieval (Riezler et al., 2007; Berger et al., 1231 2000; Bernhard and Gurevych, 2009). In (Surdeanu et al., 2008), the translation model is used to provide features for answer ranking. The structural features (e.g., authorship, acknowledgement, post position, etc), also called non-textual features, play an important role in answer extraction. Such features are used in (Ding et al., 2008; Cong et al., 2008), and have significantly improved the performance. The studies of Jeon et al. (2006) and Hong et al. (2009) show that the structural features have even more contribution than the textual features. In this case, the minin</context>
</contexts>
<marker>Riezler, Vasserman, Tsochantaridis, Mittal, Liu, 2007</marker>
<rawString>Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007. Statistical machine translation for query expansion in answer retrieval. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 464–471, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Salakhutdinov</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Semantic hashing.</title>
<date>2009</date>
<journal>Int. J. Approx. Reasoning,</journal>
<volume>50</volume>
<issue>7</issue>
<contexts>
<context position="10938" citStr="Salakhutdinov and Hinton (2009)" startWordPosition="1772" endWordPosition="1776">the information in the corresponding answers. In this section, we propose a deep belief network for modeling the semantic relationship between questions and their answers. Our model is able to map the QA data into a low-dimensional semantic-feature space, where a question is close to its answers. 3.1 The Restricted Boltzmann Machine An ensemble of binary vectors can be modeled using a two-layer network called a “restricted Boltzmann machine” (RBM) (Hinton, 2002). The dimension reducing approach based on RBM initially shows good performance on image processing (Hinton and Salakhutdinov, 2006). Salakhutdinov and Hinton (2009) propose a deep graphical model composed of RBMs into the information retrieval field, which shows that this model is able to obtain semantic information hidden in the wordcount vectors. As shown in Figure 1, the RBM is a two-layer network. The bottom layer represents a visible vector v and the top layer represents a latent feature h. The matrix W contains the symmetric interaction terms between the visible units and the hidden units. Given an input vector v, the trained Figure 1: Restricted Boltzmann machine RBM model provides a hidden feature h, which can be used to reconstruct v with a mini</context>
<context position="19824" citStr="Salakhutdinov and Hinton (2009)" startWordPosition="3319" endWordPosition="3322">234 set. Because the two corpora are similar, we can apply the deep belief network trained by the cQA corpus to detect answers on both the cQA data and the forum data. 4.2 Features The task of detecting answers in social media corpora suffers from the problem of feature sparsity seriously. High-dimensional feature vectors with only several non-zero dimensions bring large time consumption to our model. Thus it is necessary to reduce the dimension of the feature vectors. In this paper, we adopt two kinds of word features. Firstly, we consider the 1,300 most frequent words in the training set as Salakhutdinov and Hinton (2009) did. According to our statistics, the frequencies of the rest words are all less then 10, which are not statistically significant and may introduce much noise. We take the occurrence of some function words as another kind of features. The function words are quite meaningful for judging whether a short text is an answer or not, especially for the nonfactoid questions. For example, in the answers to the causation questions, the words such as because and so are more likely to appear; and the words such as firstly, then, and should may suggest the answers to the manner questions. We give an examp</context>
</contexts>
<marker>Salakhutdinov, Hinton, 2009</marker>
<rawString>Ruslan Salakhutdinov and Geoffrey Hinton. 2009. Semantic hashing. Int. J. Approx. Reasoning, 50(7):969–978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lokesh Shrestha</author>
<author>Kathleen McKeown</author>
</authors>
<title>Detection of question-answer pairs in email conversations.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>889--895</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="9812" citStr="Shrestha and McKeown, 2004" startWordPosition="1586" endWordPosition="1589"> than the textual features. In this case, the mining of textual features tends to be ignored. There are also some other research topics in this field. Cong et al. (2008) and Wang et al. (2009) both propose the strategies to detect questions in the social media corpus, which is proved to be a non-trivial task. The deep research on question detection has been taken by Duan et al. (2008). A graph based algorithm is presented to answer opinion questions (Li et al., 2009). In email summarization field, the QA pairs are also extracted from email contents as the main elements of email summarization (Shrestha and McKeown, 2004). 3 The Deep Belief Network for QA pairs Due to the feature sparsity and the low word frequency of the social media corpus, it is difficult to model the semantic relevance between questions and answers using only co-occurrence features. It is clear that the semantic link exists between the question and its answers, even though they have totally different lexical representations. Thus a specially designed model may learn semantic knowledge by reconstructing a great number of questions using the information in the corresponding answers. In this section, we propose a deep belief network for model</context>
</contexts>
<marker>Shrestha, McKeown, 2004</marker>
<rawString>Lokesh Shrestha and Kathleen McKeown. 2004. Detection of question-answer pairs in email conversations. In Proceedings of Coling 2004, pages 889– 895, Geneva, Switzerland, Aug 23–Aug 27. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Massimiliano Ciaramita</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Learning to rank answers on large online QA collections.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>719--727</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="4354" citStr="Surdeanu et al., 2008" startWordPosition="674" endWordPosition="677">ers.yahoo.com 1230 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1230–1238, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics answer semantic modeling. Most researchers try to introduce structural features or users’ behavior to improve the models performance, by contrast, the effect of textual features is not obvious. 2. How to train a model so that it has good performance on both cQA and forum datasets? So far, people have been doing QA researches on the cQA and the forum datasets separately (Ding et al., 2008; Surdeanu et al., 2008), and no one has noticed the relationship between the two kinds of data. Since both the cQA systems and the online forums are open platforms for people to communicate, the QA pairs in the cQA systems have similarity with those in the forums. In this case, it is highly valuable and desirable to propose a training strategy to improve the model’s performance on both of the two kinds of datasets. In addition, it is possible to avoid the expensive and arduous hand-annotating work by introducing the method. To solve the first problem, we present a deep belief network (DBN) to model the semantic rele</context>
<context position="6768" citStr="Surdeanu et al. (2008)" startWordPosition="1071" endWordPosition="1074"> the naturally generated questionanswer pairs has not been recognized until recent years. Early studies mainly focus on extracting QA pairs from frequently asked questions (FAQ) pages (Jijkoun and de Rijke, 2005; Riezler et al., 2007) or service call-center dialogues (Berger et al., 2000). Judging whether a candidate answer is semantically related to the question in the cQA page automatically is a challenging task. A framework for predicting the quality of answers has been presented in (Jeon et al., 2006). Bernhard and Gurevych (2009) have developed a translation based method to find answers. Surdeanu et al. (2008) propose an approach to rank the answers retrieved by Yahoo! Answers. Our work is partly similar to Surdeanu et al. (2008), for we also aim to rank the candidate answers reasonably, but our ranking algorithm needs only word information, instead of the combination of different kinds of features. Because people have considerable freedom to post on forums, there are a great number of irrelevant posts for answering questions, which makes it more difficult to detect answers in the forums. In this field, exploratory studies have been done by Feng et al. (2006) and Huang et al. (2007), who extract in</context>
<context position="8720" citStr="Surdeanu et al., 2008" startWordPosition="1403" endWordPosition="1407">en question, one has to face the problem of lexical gap. One of the problems with lexical gap embedding is to find similar questions in QA achieves (Jeon et al., 2005). Recently, the statistical machine translation (SMT) strategy has become popular. Lee et al. (2008) use translate models to bridge the lexical gap between queries and questions in QA collections. The SMT based methods are effective on modeling the semantic relationship between questions and answers and expending users’ queries in answer retrieval (Riezler et al., 2007; Berger et al., 1231 2000; Bernhard and Gurevych, 2009). In (Surdeanu et al., 2008), the translation model is used to provide features for answer ranking. The structural features (e.g., authorship, acknowledgement, post position, etc), also called non-textual features, play an important role in answer extraction. Such features are used in (Ding et al., 2008; Cong et al., 2008), and have significantly improved the performance. The studies of Jeon et al. (2006) and Hong et al. (2009) show that the structural features have even more contribution than the textual features. In this case, the mining of textual features tends to be ignored. There are also some other research topics</context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2008</marker>
<rawString>Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2008. Learning to rank answers on large online QA collections. In Proceedings of ACL-08: HLT, pages 719–727, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baoxun Wang</author>
<author>Bingquan Liu</author>
<author>Chengjie Sun</author>
<author>Xiaolong Wang</author>
<author>Lin Sun</author>
</authors>
<title>Extracting chinese question-answer pairs from online forums.</title>
<date>2009</date>
<booktitle>In SMC 2009: Proceedings of the IEEE International Conference on Systems, Man and Cybernetics,</booktitle>
<pages>1159--1164</pages>
<contexts>
<context position="7897" citStr="Wang et al., 2009" startWordPosition="1264" endWordPosition="1267">tory studies have been done by Feng et al. (2006) and Huang et al. (2007), who extract input-reply pairs for the discussion-bot. Ding et al.(2008) and Cong et al.(2008) have also presented outstanding research works on forum QA extraction. Ding et al. (2008) detect question contexts and answers using the conditional random fields, and a ranking algorithm based on the authority of forum users is proposed by Cong et al. (2008). Treating answer detection as a binary classification problem is an intuitive idea, thus there are some studies trying to solve it from this view (Hong and Davison, 2009; Wang et al., 2009). Especially Hong and Davison (2009) have achieved a rather high precision on the corpora with less noise, which also shows the importance of “social” features. In order to select the answers for a given question, one has to face the problem of lexical gap. One of the problems with lexical gap embedding is to find similar questions in QA achieves (Jeon et al., 2005). Recently, the statistical machine translation (SMT) strategy has become popular. Lee et al. (2008) use translate models to bridge the lexical gap between queries and questions in QA collections. The SMT based methods are effective</context>
<context position="9377" citStr="Wang et al. (2009)" startWordPosition="1513" endWordPosition="1516">ide features for answer ranking. The structural features (e.g., authorship, acknowledgement, post position, etc), also called non-textual features, play an important role in answer extraction. Such features are used in (Ding et al., 2008; Cong et al., 2008), and have significantly improved the performance. The studies of Jeon et al. (2006) and Hong et al. (2009) show that the structural features have even more contribution than the textual features. In this case, the mining of textual features tends to be ignored. There are also some other research topics in this field. Cong et al. (2008) and Wang et al. (2009) both propose the strategies to detect questions in the social media corpus, which is proved to be a non-trivial task. The deep research on question detection has been taken by Duan et al. (2008). A graph based algorithm is presented to answer opinion questions (Li et al., 2009). In email summarization field, the QA pairs are also extracted from email contents as the main elements of email summarization (Shrestha and McKeown, 2004). 3 The Deep Belief Network for QA pairs Due to the feature sparsity and the low word frequency of the social media corpus, it is difficult to model the semantic rel</context>
</contexts>
<marker>Wang, Liu, Sun, Wang, Sun, 2009</marker>
<rawString>Baoxun Wang, Bingquan Liu, Chengjie Sun, Xiaolong Wang, and Lin Sun. 2009. Extracting chinese question-answer pairs from online forums. In SMC 2009: Proceedings of the IEEE International Conference on Systems, Man and Cybernetics, 2009., pages 1159–1164.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>