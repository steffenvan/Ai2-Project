<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010928">
<title confidence="0.8968125">
LIMSIILES: Basic English Substitution for Student Answer Assessment at
SemEval 2013
</title>
<author confidence="0.692185">
Martin Gleize
</author>
<note confidence="0.567074">
LIMSI-CNRS &amp; ENS
B.P. 133 91403 ORSAY CEDEX, France
</note>
<email confidence="0.978204">
gleize@limsi.fr
</email>
<note confidence="0.780569">
Brigitte Grau
LIMSI-CNRS &amp; ENSIIE
B.P. 133 91403 ORSAY CEDEX, France
</note>
<email confidence="0.991704">
bg@limsi.fr
</email>
<sectionHeader confidence="0.995448" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9991821875">
In this paper, we describe a method for as-
sessing student answers, modeled as a para-
phrase identification problem, based on sub-
stitution by Basic English variants. Basic En-
glish paraphrases are acquired from the Sim-
ple English Wiktionary. Substitutions are ap-
plied both on reference answers and student
answers in order to reduce the diversity of
their vocabulary and map them to a common
vocabulary. The evaluation of our approach
on the SemEval 2013 Joint Student Response
Analysis and 8th Recognizing Textual Entail-
ment Challenge data shows promising results,
and this work is a first step toward an open-
domain system able to exhibit deep text un-
derstanding capabilities.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962227272727">
Automatically assessing student answers is a chal-
lenging natural language processing task (NLP). It
is a way to make test grading easier and improve
adaptive tutoring (Dzikovska et al., 2010), and is the
goal of the SemEval 2013’s task 7, titled Joint Stu-
dent Response Analysis. More specifically, given a
question, a known correct “reference answer” and a
1- or 2-sentence student answer, the goal is to deter-
mine the student’s answer accuracy (Dzikovska et
al., 2013). This can be seen as a paraphrase identi-
fication problem between student answers and refer-
ence answers.
Paraphrase identification searches whether two
sentences have essentially the same meaning (Culi-
cover, 1968). Automatically generating or extract-
ing semantic equivalences for the various units of
language – words, phrases, and sentences – is an im-
portant problem in NLP and is being increasingly
employed to improve the performance of several
NLP applications (Madnani and Dorr, 2010), like
question-answering and machine translation.
Paraphrase identification would benefit from
a precise and broad-coverage semantic language
model. This is unfortunately difficult to obtain to its
full extent for any natural language, due to the size
of a typical lexicon and the complexity of grammat-
ical constructions. Our hypothesis is that the sim-
pler the language lexicon is, the easier it will be to
access and compare meaning of sentences. This as-
sumption is justified by the multiple attempts at con-
trolled natural languages (Schwitter, 2010) and es-
pecially simplified forms of English. One of them,
Basic English (Ogden, 1930), has been adopted by
the Wikipedia Project as the preferred language of
the Simple English Wikipedia1 and its sister project
the Simple English Wiktionary2.
Our method starts with acquiring paraphrases
from the Simple English Wiktionary’s definitions.
Using those, we generate variants of both sentences
whose meanings are to be compared. Finally, we
compute traditional lexical and semantic similarity
measures on those two sets of variants to produce
features to train a classifier on the SemEval 2013
datasets in order to take the final decision.
</bodyText>
<sectionHeader confidence="0.536509" genericHeader="method">
2 Acquiring simplifying paraphrases
</sectionHeader>
<bodyText confidence="0.9599425">
Simple Wiktionary word definitions are different
from usual dictionary definitions. Aside from the
</bodyText>
<footnote confidence="0.9999955">
1http://simple.wikipedia.org
2http://simple.wiktionary.org
</footnote>
<page confidence="0.892123">
598
</page>
<bodyText confidence="0.985521857142857">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 598–602, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
simplified language, they often prefer to give a
complete sentence where the word – e.g. a verb – is
used in context, along with an explanation of what it
means. To define the verb link, Simple Wiktionary
states that If you link two or more things, you make a
connection between them (1), whereas the standard
Wiktionary uses the shorter and more cryptic To
connect two or more things.
We notice in this example that the definition
from Simple Wiktionary consists of two clauses,
linked by a subordination relation. It’s actually the
case for a lot of verb definitions: a quick statistical
study shows that 70% of these definitions are
composed of two clauses, an independent clause,
and a subordinate clause (often an adverbial clause).
One clause illustrates how the verb is used, the
other gives the explanation and the actual dictionary
definition, as in example (1). These definitions are
the basis of our method for acquiring paraphrases.
</bodyText>
<subsectionHeader confidence="0.999161">
2.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.999955071428571">
We use the Stanford Parser to parse the definitions
and get a dependency graph (De Marneffe and Man-
ning, 2008). Using a few hand-written rules, we then
retrieve both parts of the definition, which we call
the word part and the defining part (see table 1 page
3 for examples). We can do this for definitions of
verbs, but also for nouns, like the giraffe is the tallest
land animal in the world to define giraffe, or adjec-
tives, like if something is bright it gives out or fills
with much light to define bright. We only provide
the details of our method for processing verb defini-
tions, as they correspond to the most complex cases,
but we proceed similarly for noun, adjective and ad-
verb definitions.
</bodyText>
<subsectionHeader confidence="0.999727">
2.2 Argument matching
</subsectionHeader>
<bodyText confidence="0.998679">
Word and defining parts alone are not paraphrases,
but we can obtain phrasal paraphrases from them. If
we see word part and defining part as two semanti-
cally equivalent predications, we have to identify the
two predicates with their arguments, then match ar-
guments with corresponding meaning, i.e. match ar-
guments which designate the same entity or assume
the same semantic function in both parts, as showed
in Table 2.
For verb definitions, we identify the predicates as
</bodyText>
<equation confidence="0.651367">
you you
link make
0 a connection
0 __+ between
</equation>
<bodyText confidence="0.838301">
two or more things —* them
</bodyText>
<tableCaption confidence="0.988163">
Table 2: Complete matching for the definition of verb link
</tableCaption>
<bodyText confidence="0.999429047619048">
the main verbs in both clauses (hence link matching
with make in table 2) and their arguments as a POS-
filtered list of their syntactic descendants. Then,
our assumption is that every argument of the word
part predicate is present in the defining part, and
the defining part predicate can have extra arguments
(like a connection).
We define s(A, B), the score of the pair of argu-
ments (A, B), with argument A in the word part and
argument B in the defining part. We then define a
matching M as a set of such pairs, such that ev-
ery element of every possible pair of arguments is
found at most one time in M. A complete match-
ing is a matching M that matches every argument
in the word part, i.e., for each word part argument
A, there exists a pair of arguments in M which con-
tains A. Finally, we compute the matching score of
M, 5(M), as the sum of scores of all pairs of M.
The score function s(A, B) is a hand-crafted lin-
ear combination of several features computed on a
pair of arguments (A, B) including:
</bodyText>
<listItem confidence="0.9992675">
• Raw string similarity. Sometimes the same
word is reused in the defining part.
• Having an equal/compatible dependency rela-
tion with their respective main verb.
• Relative position in clause.
• Relative depth in parsing tree. These last 3 fea-
tures assess if the two arguments play the same
syntactic role.
• Same gender and number. If different, it’s
unlikely that the two arguments designate the
same entity.
• If (A, B) is a pair (noun phrase, pronoun). We
hope to capture an anaphoric expression and its
antecedent.
</listItem>
<page confidence="0.977411">
599
</page>
<figure confidence="0.959422166666667">
Word (POS-tag)
Defining part
Word part
link (V)
giraffe (N)
bright (Adj)
</figure>
<bodyText confidence="0.998369833333333">
you link two or more things
the giraffe
something is bright
you make a connection between them
the tallest land animal in the world
it gives out or fills with much light
</bodyText>
<tableCaption confidence="0.997616">
Table 1: Word part and defining part of some Simple Wiktionary definitions
</tableCaption>
<listItem confidence="0.992372">
• WordNet similarity (Pedersen et al., 2004). If
words belong to close synsets, they’re more
likely to identify the same entity.
</listItem>
<subsectionHeader confidence="0.997479">
2.3 Phrasal paraphrases
</subsectionHeader>
<bodyText confidence="0.999984666666667">
We compute the complete matching M which maxi-
mizes the matching score 5(M). Although it is pos-
sible to enumerate all matchings, it is intractable;
therefore when predicates have more than 4 argu-
ments, we prefer constructing a best matching with a
beam search algorithm. After replacing each pair of
arguments with linked variables, and attaching un-
matched arguments to the predicates, we finally ob-
tain phrasal paraphrases of this form:
</bodyText>
<equation confidence="0.831606">
( X link Y , X make a connection between Y )
</equation>
<sectionHeader confidence="0.983757" genericHeader="method">
3 Paraphrasing exercise answers
</sectionHeader>
<subsectionHeader confidence="0.999938">
3.1 Paraphrase generation and pre-ranking
</subsectionHeader>
<bodyText confidence="0.96823124">
Given a sentence, and our Simple Wiktionary para-
phrases (about 20,650 extracted paraphrases), we
can generate sentential paraphrases by simple syn-
tactic pattern matching –and do so recursively by
taking previous outputs as input–, with the intent
that these new sentences use increasingly more Ba-
sic English. We generate as many variants starting
from both reference answers and student answers as
we can in a fixed amount of time, as an anytime al-
gorithm would do. We prioritize substituting verbs
and adjectives over nouns, and non Basic English
words over Basic English words.
Given a student answer and reference answers, we
then use a simple Jaccard distance (on lowercased
lemmatized non-stopwords) to score the closeness
of student answer variants to reference answer vari-
ants: we measure how close the vocabulary used in
the two statements has become. More specifically,
for each reference answer A, we compute the n clos-
est variants of the student answer to A’s variant set.
In our experiments, n = 10. We finally rank the
reference answers according to the average distance
from their n closest variants to A’s variant set and
keep the top-ranked one for our classification exper-
iment. Figure 1 illustrates the whole process.
</bodyText>
<figureCaption confidence="0.977309">
Figure 1: Variants are generated from all reference an-
swers (RA) and the student answer (SA). For each ref-
erence answer RA, student answer variants are ranked
based on their lexical distance from the variants of RA.
The reference with the n closer variants to the student
variants is kept (here: RA1).
</figureCaption>
<subsectionHeader confidence="0.99969">
3.2 Classifying student answers
</subsectionHeader>
<bodyText confidence="0.9997446">
SemEval 2013 task 7 offers 3 problems: a 5-way
task, with 5 different answer judgements, and 3-way
and 2-way tasks, conflating more judgement cate-
gories each time. Two different corpora, Beetle and
SciEntsBank, were labeled with the 5 following la-
bels: Correct, Partially correct incomplete, Contra-
dictory, Irrelevant and Non Domain, as described in
(Dzikovska et al., 2012). We see the n-way task as a
n-way classification problem. The instances of this
problem are the pairs (student answer, reference an-
swer).
We compute for each instance the following fea-
tures: For each of the n closest variants of the stu-
dent answer to some variant of the reference answer
computed in the pre-ranking phase:
</bodyText>
<listItem confidence="0.99874">
• Jaccard similarity coefficient on non-
stopwords.
• A boolean representing if the two statements
have the same polarity or not, where polarity
</listItem>
<figure confidence="0.99840195">
RA1
RA2
...
A B
SA
0
1
2
C
3
5
RA1
2. 3
1. 5
4
...
RA2
1. 1
2. 3
...
</figure>
<page confidence="0.984092">
600
</page>
<bodyText confidence="0.9721485">
is defined as the number of neg dependencies
in the Stanford Parser dependency graph.
</bodyText>
<listItem confidence="0.99393175">
• Number of “paraphrasing steps” necessary to
obtain the variant from a raw student answer.
• Highest WordNet similarity of their respective
nouns.
• WordNet similarity of the main verbs.
General features:
• Answer count (how many students typed this
answer), provided in the datasets.
• Length ratio between the student answer and
the closest reference answer.
• Number of (non-stop)words which appear nei-
ther in the question nor the reference answers.
</listItem>
<bodyText confidence="0.990696">
We train an SVM classifier (with a one-against-one
approach to multiclass classification) on both Beetle
and SciEntsBank, for each n-way task.
</bodyText>
<subsectionHeader confidence="0.997644">
3.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.9984748">
Table 3 presents our system’s overall accuracy on the
5-way task, along with the top scores at SemEval
2013, mean scores, and baselines –majority class
and lexical overlap– described in (Dzikovska et al.,
2012).
</bodyText>
<table confidence="0.998927">
System Beetle SciEntsBank
unseen answers unseen questions
Majority 0.4010 0.4110
Lexical 0.5190 0.4130
overlap
Mean 0.5326 0.4078
ETS-run-1 0.5740 0.5320
ETS-run-2 0.7150 0.4010
Simple 0.5330 0.4820
Wiktio
</table>
<tableCaption confidence="0.999454">
Table 3: SemEval 2013 evaluation results.
</tableCaption>
<bodyText confidence="0.999904818181818">
Our system performs slightly better in overall ac-
curacy on Beetle unseen answers and SciEntsBank
unseen questions than both baselines and the mean
scores. While results are clearly below the best sys-
tem trained on the Beetle corpus questions, we hold
the third best score for the 5-way task on SciEnts-
Bank unseen questions, while not fine-tuning our
system specifically for this corpus. This is rather
encouraging as to how suitable Simple Wiktionary
is as a resource to extract open-domain knowledge
from.
</bodyText>
<sectionHeader confidence="0.999768" genericHeader="discussions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999951740740741">
The system we present in this paper is the first
step towards an open-domain machine reading sys-
tem capable of understanding and reasoning. Di-
rect modeling of the semantics of a full natural lan-
guage appears too difficult. We therefore decide to
first project the English language onto a simpler En-
glish, so that it is easier to model and draw infer-
ences from.
One complementary approach to a minimalistic
language model, is to accept that texts are replete
with gaps: missing information that cannot be in-
ferred by reasoning on the text alone, but require
a certain amount of background knowledge. Penas
and Hovy (2010) show that these gaps can be filled
by maintaining a background knowledge base built
from a large corpus.
Although Simple Wiktionary is not a large corpus
by any means, it can serve our purpose of acquiring
basic knowledge for assessing exercise answers, and
has the advantage to be in constant evolution and ex-
pansion, as well as interfacing very easily with the
richer Wiktionary and Wikipedia.
Our future work will be focused on enriching and
improving the robustness of our knowledge acqui-
sition step from Simple Wiktionary, as well as in-
troducing a true normalization of English to Basic
English.
</bodyText>
<sectionHeader confidence="0.997484" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999951333333333">
We acknowledge the Wikimedia Foundation for
their willingness to provide easily usable versions
of their online collaborative resources.
</bodyText>
<sectionHeader confidence="0.993371" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.7054695">
P.W. Culicover. 1968. Paraphrase generation and
information retrieval from stored text. In Mechanical
Translation and Computational Linguistics, 11(12),
7888.
</reference>
<page confidence="0.983558">
601
</page>
<reference confidence="0.999379222222222">
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Technical report, Stanford University.
Myroslava O. Dzikovska, Diana Bental, Johanna D.
Moore, Natalie Steinhauser, Gwendolyn Campbell,
Elaine Farrow, and Charles B. Callaway. 2010.
Intelligent tutoring with natural language support
in the BEETLE II system. In Proceedings of Fifth
European Conference on Technology Enhanced
Learning (EC-TEL 2010), Barcelona.
Myroslava O. Dzikovska, Rodney D. Nielsen and Chris
Brew. 2012. Towards Effective Tutorial Feedback
for Explanation Questions: A Dataset and Baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2012), Montreal.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan and Hoa Trang Dang.
2013. SemEval-2013 Task 7: The Joint Student
Response Analysis and 8th Recognizing Textual
Entailment Challenge. In Proceedings of the 7th
International Workshop on Semantic Evaluation
(SemEval 2013), in conjunction with the Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM 2013). Atlanta, Georgia, USA. 13-14 June.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of
data-driven methods. In Computational Linguistics
36 (3), 341-387.
Charles Kay Ogden. 1930. Basic English: A General
Introduction with Rules and Grammar. Paul Treber,
London.
Ted Pedersen, Siddharth Patwardhan, and Jason
Michelizzi. 2004. WordNet::similarity–measuring
the relatedness of concepts. In Proceedings of
the Nineteenth National Conference on Artificial
Intelligence(AAAI-04), pages 10241025.
Anselmo Penas and Eduard H. Hovy. 2010. Filling
Knowledge Gaps in Text for Machine Reading. COL-
ING (Posters) 2010: 979-987, Beijing.
Rolf Schwitter. 2010. Controlled Natural Languages for
Knowledge Representation. COLING (Posters) 2010,
Beijing.
</reference>
<page confidence="0.998176">
602
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.215155">
<title confidence="0.60835">LIMSIILES: Basic English Substitution for Student Answer Assessment at SemEval 2013</title>
<author confidence="0.831782">Martin</author>
<affiliation confidence="0.944194">LIMSI-CNRS &amp;</affiliation>
<address confidence="0.817843">B.P. 133 91403 ORSAY CEDEX,</address>
<email confidence="0.936957">gleize@limsi.fr</email>
<author confidence="0.690137">Brigitte</author>
<affiliation confidence="0.885132">LIMSI-CNRS &amp;</affiliation>
<address confidence="0.790631">B.P. 133 91403 ORSAY CEDEX,</address>
<email confidence="0.968725">bg@limsi.fr</email>
<abstract confidence="0.996811764705882">In this paper, we describe a method for assessing student answers, modeled as a paraphrase identification problem, based on substitution by Basic English variants. Basic English paraphrases are acquired from the Simple English Wiktionary. Substitutions are applied both on reference answers and student answers in order to reduce the diversity of their vocabulary and map them to a common vocabulary. The evaluation of our approach on the SemEval 2013 Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge data shows promising results, and this work is a first step toward an opendomain system able to exhibit deep text understanding capabilities.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P W Culicover</author>
</authors>
<title>Paraphrase generation and information retrieval from stored text.</title>
<date>1968</date>
<booktitle>In Mechanical Translation and Computational Linguistics,</booktitle>
<volume>11</volume>
<issue>12</issue>
<pages>7888</pages>
<contexts>
<context position="1628" citStr="Culicover, 1968" startWordPosition="253" endWordPosition="255">anguage processing task (NLP). It is a way to make test grading easier and improve adaptive tutoring (Dzikovska et al., 2010), and is the goal of the SemEval 2013’s task 7, titled Joint Student Response Analysis. More specifically, given a question, a known correct “reference answer” and a 1- or 2-sentence student answer, the goal is to determine the student’s answer accuracy (Dzikovska et al., 2013). This can be seen as a paraphrase identification problem between student answers and reference answers. Paraphrase identification searches whether two sentences have essentially the same meaning (Culicover, 1968). Automatically generating or extracting semantic equivalences for the various units of language – words, phrases, and sentences – is an important problem in NLP and is being increasingly employed to improve the performance of several NLP applications (Madnani and Dorr, 2010), like question-answering and machine translation. Paraphrase identification would benefit from a precise and broad-coverage semantic language model. This is unfortunately difficult to obtain to its full extent for any natural language, due to the size of a typical lexicon and the complexity of grammatical constructions. O</context>
</contexts>
<marker>Culicover, 1968</marker>
<rawString>P.W. Culicover. 1968. Paraphrase generation and information retrieval from stored text. In Mechanical Translation and Computational Linguistics, 11(12), 7888.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>Stanford typed dependencies manual.</title>
<date>2008</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. Stanford typed dependencies manual. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Diana Bental</author>
<author>Johanna D Moore</author>
<author>Natalie Steinhauser</author>
<author>Gwendolyn Campbell</author>
<author>Elaine Farrow</author>
<author>Charles B Callaway</author>
</authors>
<title>Intelligent tutoring with natural language support in the BEETLE II system.</title>
<date>2010</date>
<booktitle>In Proceedings of Fifth European Conference on Technology Enhanced Learning (EC-TEL 2010),</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="1137" citStr="Dzikovska et al., 2010" startWordPosition="174" endWordPosition="177">pplied both on reference answers and student answers in order to reduce the diversity of their vocabulary and map them to a common vocabulary. The evaluation of our approach on the SemEval 2013 Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge data shows promising results, and this work is a first step toward an opendomain system able to exhibit deep text understanding capabilities. 1 Introduction Automatically assessing student answers is a challenging natural language processing task (NLP). It is a way to make test grading easier and improve adaptive tutoring (Dzikovska et al., 2010), and is the goal of the SemEval 2013’s task 7, titled Joint Student Response Analysis. More specifically, given a question, a known correct “reference answer” and a 1- or 2-sentence student answer, the goal is to determine the student’s answer accuracy (Dzikovska et al., 2013). This can be seen as a paraphrase identification problem between student answers and reference answers. Paraphrase identification searches whether two sentences have essentially the same meaning (Culicover, 1968). Automatically generating or extracting semantic equivalences for the various units of language – words, phr</context>
</contexts>
<marker>Dzikovska, Bental, Moore, Steinhauser, Campbell, Farrow, Callaway, 2010</marker>
<rawString>Myroslava O. Dzikovska, Diana Bental, Johanna D. Moore, Natalie Steinhauser, Gwendolyn Campbell, Elaine Farrow, and Charles B. Callaway. 2010. Intelligent tutoring with natural language support in the BEETLE II system. In Proceedings of Fifth European Conference on Technology Enhanced Learning (EC-TEL 2010), Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Rodney D Nielsen</author>
<author>Chris Brew</author>
</authors>
<title>Towards Effective Tutorial Feedback for Explanation Questions: A Dataset and Baselines.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2012),</booktitle>
<location>Montreal.</location>
<contexts>
<context position="10336" citStr="Dzikovska et al., 2012" startWordPosition="1692" endWordPosition="1695">er (SA). For each reference answer RA, student answer variants are ranked based on their lexical distance from the variants of RA. The reference with the n closer variants to the student variants is kept (here: RA1). 3.2 Classifying student answers SemEval 2013 task 7 offers 3 problems: a 5-way task, with 5 different answer judgements, and 3-way and 2-way tasks, conflating more judgement categories each time. Two different corpora, Beetle and SciEntsBank, were labeled with the 5 following labels: Correct, Partially correct incomplete, Contradictory, Irrelevant and Non Domain, as described in (Dzikovska et al., 2012). We see the n-way task as a n-way classification problem. The instances of this problem are the pairs (student answer, reference answer). We compute for each instance the following features: For each of the n closest variants of the student answer to some variant of the reference answer computed in the pre-ranking phase: • Jaccard similarity coefficient on nonstopwords. • A boolean representing if the two statements have the same polarity or not, where polarity RA1 RA2 ... A B SA 0 1 2 C 3 5 RA1 2. 3 1. 5 4 ... RA2 1. 1 2. 3 ... 600 is defined as the number of neg dependencies in the Stanford</context>
<context position="11784" citStr="Dzikovska et al., 2012" startWordPosition="1940" endWordPosition="1943">tures: • Answer count (how many students typed this answer), provided in the datasets. • Length ratio between the student answer and the closest reference answer. • Number of (non-stop)words which appear neither in the question nor the reference answers. We train an SVM classifier (with a one-against-one approach to multiclass classification) on both Beetle and SciEntsBank, for each n-way task. 3.3 Evaluation Table 3 presents our system’s overall accuracy on the 5-way task, along with the top scores at SemEval 2013, mean scores, and baselines –majority class and lexical overlap– described in (Dzikovska et al., 2012). System Beetle SciEntsBank unseen answers unseen questions Majority 0.4010 0.4110 Lexical 0.5190 0.4130 overlap Mean 0.5326 0.4078 ETS-run-1 0.5740 0.5320 ETS-run-2 0.7150 0.4010 Simple 0.5330 0.4820 Wiktio Table 3: SemEval 2013 evaluation results. Our system performs slightly better in overall accuracy on Beetle unseen answers and SciEntsBank unseen questions than both baselines and the mean scores. While results are clearly below the best system trained on the Beetle corpus questions, we hold the third best score for the 5-way task on SciEntsBank unseen questions, while not fine-tuning our </context>
</contexts>
<marker>Dzikovska, Nielsen, Brew, 2012</marker>
<rawString>Myroslava O. Dzikovska, Rodney D. Nielsen and Chris Brew. 2012. Towards Effective Tutorial Feedback for Explanation Questions: A Dataset and Baselines. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2012), Montreal.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Rodney Nielsen</author>
<author>Chris Brew</author>
<author>Claudia Leacock</author>
<author>Danilo Giampiccolo</author>
<author>Luisa Bentivogli</author>
<author>Peter Clark</author>
</authors>
<title>Ido Dagan and Hoa Trang Dang.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics (*SEM 2013).</booktitle>
<pages>13--14</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="1415" citStr="Dzikovska et al., 2013" startWordPosition="221" endWordPosition="224">a shows promising results, and this work is a first step toward an opendomain system able to exhibit deep text understanding capabilities. 1 Introduction Automatically assessing student answers is a challenging natural language processing task (NLP). It is a way to make test grading easier and improve adaptive tutoring (Dzikovska et al., 2010), and is the goal of the SemEval 2013’s task 7, titled Joint Student Response Analysis. More specifically, given a question, a known correct “reference answer” and a 1- or 2-sentence student answer, the goal is to determine the student’s answer accuracy (Dzikovska et al., 2013). This can be seen as a paraphrase identification problem between student answers and reference answers. Paraphrase identification searches whether two sentences have essentially the same meaning (Culicover, 1968). Automatically generating or extracting semantic equivalences for the various units of language – words, phrases, and sentences – is an important problem in NLP and is being increasingly employed to improve the performance of several NLP applications (Madnani and Dorr, 2010), like question-answering and machine translation. Paraphrase identification would benefit from a precise and b</context>
</contexts>
<marker>Dzikovska, Nielsen, Brew, Leacock, Giampiccolo, Bentivogli, Clark, 2013</marker>
<rawString>Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew, Claudia Leacock, Danilo Giampiccolo, Luisa Bentivogli, Peter Clark, Ido Dagan and Hoa Trang Dang. 2013. SemEval-2013 Task 7: The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics (*SEM 2013). Atlanta, Georgia, USA. 13-14 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of data-driven methods.</title>
<date>2010</date>
<journal>In Computational Linguistics</journal>
<volume>36</volume>
<issue>3</issue>
<pages>341--387</pages>
<contexts>
<context position="1904" citStr="Madnani and Dorr, 2010" startWordPosition="295" endWordPosition="298">ference answer” and a 1- or 2-sentence student answer, the goal is to determine the student’s answer accuracy (Dzikovska et al., 2013). This can be seen as a paraphrase identification problem between student answers and reference answers. Paraphrase identification searches whether two sentences have essentially the same meaning (Culicover, 1968). Automatically generating or extracting semantic equivalences for the various units of language – words, phrases, and sentences – is an important problem in NLP and is being increasingly employed to improve the performance of several NLP applications (Madnani and Dorr, 2010), like question-answering and machine translation. Paraphrase identification would benefit from a precise and broad-coverage semantic language model. This is unfortunately difficult to obtain to its full extent for any natural language, due to the size of a typical lexicon and the complexity of grammatical constructions. Our hypothesis is that the simpler the language lexicon is, the easier it will be to access and compare meaning of sentences. This assumption is justified by the multiple attempts at controlled natural languages (Schwitter, 2010) and especially simplified forms of English. One</context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Nitin Madnani and Bonnie J. Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of data-driven methods. In Computational Linguistics 36 (3), 341-387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Kay Ogden</author>
</authors>
<title>Basic English: A General Introduction with Rules and Grammar. Paul Treber,</title>
<date>1930</date>
<location>London.</location>
<contexts>
<context position="2541" citStr="Ogden, 1930" startWordPosition="396" endWordPosition="397"> and machine translation. Paraphrase identification would benefit from a precise and broad-coverage semantic language model. This is unfortunately difficult to obtain to its full extent for any natural language, due to the size of a typical lexicon and the complexity of grammatical constructions. Our hypothesis is that the simpler the language lexicon is, the easier it will be to access and compare meaning of sentences. This assumption is justified by the multiple attempts at controlled natural languages (Schwitter, 2010) and especially simplified forms of English. One of them, Basic English (Ogden, 1930), has been adopted by the Wikipedia Project as the preferred language of the Simple English Wikipedia1 and its sister project the Simple English Wiktionary2. Our method starts with acquiring paraphrases from the Simple English Wiktionary’s definitions. Using those, we generate variants of both sentences whose meanings are to be compared. Finally, we compute traditional lexical and semantic similarity measures on those two sets of variants to produce features to train a classifier on the SemEval 2013 datasets in order to take the final decision. 2 Acquiring simplifying paraphrases Simple Wiktio</context>
</contexts>
<marker>Ogden, 1930</marker>
<rawString>Charles Kay Ogden. 1930. Basic English: A General Introduction with Rules and Grammar. Paul Treber, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::similarity–measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Nineteenth National Conference on Artificial Intelligence(AAAI-04),</booktitle>
<pages>10241025</pages>
<contexts>
<context position="7727" citStr="Pedersen et al., 2004" startWordPosition="1268" endWordPosition="1271">wo arguments play the same syntactic role. • Same gender and number. If different, it’s unlikely that the two arguments designate the same entity. • If (A, B) is a pair (noun phrase, pronoun). We hope to capture an anaphoric expression and its antecedent. 599 Word (POS-tag) Defining part Word part link (V) giraffe (N) bright (Adj) you link two or more things the giraffe something is bright you make a connection between them the tallest land animal in the world it gives out or fills with much light Table 1: Word part and defining part of some Simple Wiktionary definitions • WordNet similarity (Pedersen et al., 2004). If words belong to close synsets, they’re more likely to identify the same entity. 2.3 Phrasal paraphrases We compute the complete matching M which maximizes the matching score 5(M). Although it is possible to enumerate all matchings, it is intractable; therefore when predicates have more than 4 arguments, we prefer constructing a best matching with a beam search algorithm. After replacing each pair of arguments with linked variables, and attaching unmatched arguments to the predicates, we finally obtain phrasal paraphrases of this form: ( X link Y , X make a connection between Y ) 3 Paraphr</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::similarity–measuring the relatedness of concepts. In Proceedings of the Nineteenth National Conference on Artificial Intelligence(AAAI-04), pages 10241025.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anselmo Penas</author>
<author>Eduard H Hovy</author>
</authors>
<title>Filling Knowledge Gaps in Text for Machine Reading. COLING (Posters)</title>
<date>2010</date>
<pages>979--987</pages>
<location>Beijing.</location>
<contexts>
<context position="13174" citStr="Penas and Hovy (2010)" startWordPosition="2163" endWordPosition="2166"> The system we present in this paper is the first step towards an open-domain machine reading system capable of understanding and reasoning. Direct modeling of the semantics of a full natural language appears too difficult. We therefore decide to first project the English language onto a simpler English, so that it is easier to model and draw inferences from. One complementary approach to a minimalistic language model, is to accept that texts are replete with gaps: missing information that cannot be inferred by reasoning on the text alone, but require a certain amount of background knowledge. Penas and Hovy (2010) show that these gaps can be filled by maintaining a background knowledge base built from a large corpus. Although Simple Wiktionary is not a large corpus by any means, it can serve our purpose of acquiring basic knowledge for assessing exercise answers, and has the advantage to be in constant evolution and expansion, as well as interfacing very easily with the richer Wiktionary and Wikipedia. Our future work will be focused on enriching and improving the robustness of our knowledge acquisition step from Simple Wiktionary, as well as introducing a true normalization of English to Basic English</context>
</contexts>
<marker>Penas, Hovy, 2010</marker>
<rawString>Anselmo Penas and Eduard H. Hovy. 2010. Filling Knowledge Gaps in Text for Machine Reading. COLING (Posters) 2010: 979-987, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rolf Schwitter</author>
</authors>
<title>Controlled Natural Languages for Knowledge Representation. COLING (Posters)</title>
<date>2010</date>
<location>Beijing.</location>
<contexts>
<context position="2456" citStr="Schwitter, 2010" startWordPosition="382" endWordPosition="383">performance of several NLP applications (Madnani and Dorr, 2010), like question-answering and machine translation. Paraphrase identification would benefit from a precise and broad-coverage semantic language model. This is unfortunately difficult to obtain to its full extent for any natural language, due to the size of a typical lexicon and the complexity of grammatical constructions. Our hypothesis is that the simpler the language lexicon is, the easier it will be to access and compare meaning of sentences. This assumption is justified by the multiple attempts at controlled natural languages (Schwitter, 2010) and especially simplified forms of English. One of them, Basic English (Ogden, 1930), has been adopted by the Wikipedia Project as the preferred language of the Simple English Wikipedia1 and its sister project the Simple English Wiktionary2. Our method starts with acquiring paraphrases from the Simple English Wiktionary’s definitions. Using those, we generate variants of both sentences whose meanings are to be compared. Finally, we compute traditional lexical and semantic similarity measures on those two sets of variants to produce features to train a classifier on the SemEval 2013 datasets i</context>
</contexts>
<marker>Schwitter, 2010</marker>
<rawString>Rolf Schwitter. 2010. Controlled Natural Languages for Knowledge Representation. COLING (Posters) 2010, Beijing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>