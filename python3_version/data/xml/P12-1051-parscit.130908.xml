<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000530">
<title confidence="0.878872">
Semantic Parsing with Bayesian Tree Transducers
</title>
<author confidence="0.821647">
Bevan Keeley Jones*† Mark Johnson† Sharon Goldwater*
</author>
<affiliation confidence="0.724025333333333">
b.k.jones@sms.ed.ac.uk Mark.Johnson@mq.edu.au sgwater@inf.ed.ac.uk
* School of Informatics † Department of Computing
University of Edinburgh Macquarie University
</affiliation>
<address confidence="0.727838">
Edinburgh, EH8 9AB, UK Sydney, NSW 2109, Australia
</address>
<sectionHeader confidence="0.988987" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999474529411765">
Many semantic parsing models use tree trans-
formations to map between natural language
and meaning representation. However, while
tree transformations are central to several
state-of-the-art approaches, little use has been
made of the rich literature on tree automata.
This paper makes the connection concrete
with a tree transducer based semantic parsing
model and suggests that other models can be
interpreted in a similar framework, increasing
the generality of their contributions. In par-
ticular, this paper further introduces a varia-
tional Bayesian inference algorithm that is ap-
plicable to a wide class of tree transducers,
producing state-of-the-art semantic parsing re-
sults while remaining applicable to any do-
main employing probabilistic tree transducers.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999794142857143">
Semantic parsing is the task of mapping natural lan-
guage sentences to a formal representation of mean-
ing. Typically, a system is trained on pairs of natural
language sentences (NLs) and their meaning repre-
sentation expressions (MRs), as in figure 1(a), and
the system must generalize to novel sentences.
Most semantic parsing models rely on an assump-
tion of structural similarity between MR and NL.
Since strict isomorphism is overly restrictive, this
assumption is often relaxed by applying transforma-
tions. Several approaches assume a tree structure to
the NL, MR, or both (Ge and Mooney, 2005; Kate
and Mooney, 2006; Wong and Mooney, 2006; Lu
et al., 2008; B¨orschinger et al., 2011), and often in-
</bodyText>
<figureCaption confidence="0.981416666666667">
Figure 1: (a) An example sentence/meaning pair, (b) a
tree transformation based mapping, and (c) a tree trans-
ducer that performs the mapping.
</figureCaption>
<bodyText confidence="0.994448333333333">
volve tree transformations either between two trees
or a tree and a string.
The tree transducer, a formalism from automata
theory which has seen interest in machine transla-
tion (Yamada and Knight, 2001; Graehl et al., 2008)
and has potential applications in many other areas,
is well suited to formalizing such tree transforma-
tion based models. Yet, while many semantic pars-
ing systems resemble the formalism, each was pro-
posed as an independent model requiring custom al-
gorithms, leaving it unclear how developments in
one line of inquiry relate to others. We argue for a
unifying theory of tree transformation based seman-
tic parsing by presenting a tree transducer model and
drawing connections to other similar systems.
We make a further contribution by bringing to
tree transducers the benefits of the Bayesian frame-
work for principled handling of data sparsity and
</bodyText>
<page confidence="0.975433">
488
</page>
<note confidence="0.985863">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 488–496,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999440823529412">
prior knowledge. Graehl et al. (2008) present an EM
training procedure for top down tree transducers, but
while there are Bayesian approaches to string trans-
ducers (Chiang et al., 2010) and PCFGs (Kurihara
and Sato, 2006), there has yet to be a proposal for
Bayesian inference in tree transducers. Our vari-
ational algorithm produces better semantic parses
than EM while remaining general to a broad class
of transducers appropriate for other domains.
In short, our contributions are three-fold: we
present a new state-of-the-art semantic parsing
model, propose a broader theory for tree transforma-
tion based semantic parsing, and present a general
inference algorithm for the tree transducer frame-
work. We recommend the last of these as just one
benefit of working within a general theory: contri-
butions are more broadly applicable.
</bodyText>
<sectionHeader confidence="0.937629" genericHeader="introduction">
2 Meaning representations and regular
tree grammars
</sectionHeader>
<bodyText confidence="0.980385041666667">
In semantic parsing, an MR is typically an expres-
sion from a machine interpretable language (e.g., a
database query language or a logical language like
Prolog). In this paper we assume MRs can be rep-
resented as trees, either by pre-parsing or because
they are already trees (often the case for functional
languages like LISP).1 More specifically, we assume
the MR language is a regular tree language.
A regular tree grammar (RTG) closely resembles
a context free grammar (CFG), and is a way of de-
scribing a language of trees. Formally, define TE as
the set of trees with symbols from alphabet E, and
TE(A) as the set of all trees in TEUA where symbols
from A only occur at the leaves. Then an RTG is a
tuple (Q, E, qstart, R), where Q is a set of states, E
is an alphabet, qstart ∈ Q is the initial state, and R
is a set of grammar rules of the form q → t, where q
is a state from Q and t is a tree from TE(Q).
A rule typically consists of a parent state (left) and
its child states and output symbol (right). We indi-
cate states using all capital letters:
NUM → population(PLACE).
Intuitively, an RTG is a CFG where the yield of
every parse is itself a tree. In fact, for any CFG G, it
</bodyText>
<footnote confidence="0.9765695">
1See Liang et al. (2011) for work in representing lambda
calculus expressions with trees.
</footnote>
<bodyText confidence="0.9996794">
is straightforward to produce a corresponding RTG
that generates the set of parses of G. Consequently,
while we assume we have an RTG for the MR lan-
guage, there is no loss of generality if the MR lan-
guage is actually context free.
</bodyText>
<sectionHeader confidence="0.8306245" genericHeader="method">
3 Weighted root-to-frontier, linear,
non-deleting tree-to-string transducers
</sectionHeader>
<bodyText confidence="0.999982743589743">
Tree transducers (Rounds, 1970; Thatcher, 1970) are
generalizations of finite state machines that operate
on trees. Mirroring the branching nature of its in-
put, the transducer may simultaneously transition to
several successor states, assigning a separate state to
each subtree.
There are many classes of transducer with dif-
ferent formal properties (Knight and Greahl, 2005;
Maletti et al., 2009). Figure 1(c) is an example of
a root-to-frontier, linear, non-deleting tree-to-string
transducer. It is defined using rules where the left
hand side identifies a state of the transducer and a
fragment of the input tree, and the right hand side
describes a portion of the output string. Variables
xi stand for entire sub-trees, and state-variable pairs
qj.xi stand for strings produced by applying the
transducer starting at state qj to subtree xi. Fig-
ure 1(b) illustrates an application of the transducer,
taking the tree on the left as input and outputting the
string on the right.
Formally, a weighted root-to-frontier, tree-to-
string transducer is a 5-tuple (Q, E, A, qstart, R). Q
is a finite set of states, E and A are the input and out-
put alphabets, qstart is the start state, and R is the
set of rules. Denote a pair of symbols, a and b by
a.b, the cross product of two sets A and B by A.B,
and let X be the set of variables {x0, x1, ...}. Then,
each rule r ∈ R is of the form [q.t → u].v, where
v ∈ ℜ≥0 is the rule weight, q ∈ Q, t ∈ TE(X), and
u is a string in (A ∪ Q.X)∗ such that every x ∈ X
in u also occurs in t.
We say q.t is the left hand side of rule r and u its
right hand side. The transducer is linear iff no vari-
able appears more than once on the right hand side.
It is non-deleting iff all variables on the left hand
side also occur on the right hand side. In this paper
we assume that every tree t on the left hand side is ei-
ther a single variable x0 or of the form σ(x0, ...xr,,),
where σ ∈ E (i.e., it is a tree of depth ≤ 1).
</bodyText>
<page confidence="0.998682">
489
</page>
<bodyText confidence="0.999993928571429">
A weighted tree transducer may define a probabil-
ity distribution, either a joint distribution over input
and output pairs or a conditional distribution of the
output given the input. Here, we will use joint dis-
tributions, which can be defined by ensuring that the
weights of all rules with the same state on the left-
hand side sum to one. In this case, it can be help-
ful to view the transducer as simultaneously gener-
ating both the input and output, rather than the usual
view of mapping input trees into output strings. A
joint distribution allows us to model with a single
machine both the input and output languages, which
is important during decoding when we want to infer
the input given the output.
</bodyText>
<sectionHeader confidence="0.994513" genericHeader="method">
4 A generative model of semantic parsing
</sectionHeader>
<bodyText confidence="0.9999335">
Like the hybrid tree semantic parser (Lu et al., 2008)
and the synchronous grammar based WASP (Wong
and Mooney, 2006), our model simultaneously gen-
erates the input MR tree and the output NL string.
The MR tree is built up according to the provided
MR grammar, one grammar rule at a time. Coupled
with the application of the MR rule, similar CFG-
like productions are applied to the NL side, repeated
until both the MR and NL are fully generated. In
each step, we select an MR rule and then build the
NL by first choosing a pattern with which to expand
it and then filling out that pattern with words drawn
from a unigram distribution.
This kind of coupled generative process can
be naturally formalized with tree transducer rules,
where the input tree fragment on the left side of each
rule describes the derivation of the MR and the right
describes the corresponding NL derivation.
For a simple example of a tree-to-string trans-
ducer rule consider
</bodyText>
<equation confidence="0.934429">
q.population(x1) → ‘population of’ q.x1 (1)
</equation>
<bodyText confidence="0.999950444444444">
which simultaneously generates tree fragment
population(x1) on the left and sub-string “popula-
tion of q.x1” on the right. Variable x1 stands for
an MR subtree under population, and, on the right,
state-variable pair q.x1 stands for the NL substring
generated while processing subtree x1 starting from
q. While this rule can serve as a single step of
an MR-to-NL map such as the example transducer
shown in Figure 1(c), such rules do not model the
</bodyText>
<equation confidence="0.99969268">
NUM → population(PLACE) (m)
PLACE → cityid(CITY, STATE) (r)
CITY → portland (u)
STATE → maine (v)
qmMR NL
,1.x1 → qr .x1
qMR
r,1 .x1 → qNL
u .x1
NL
qrM2R.x1 → qv .x1
qNLm .population(w1, x1, w2) →
qWm.w1 qMR
m,1.x1 qEND.w2 (3)
qNL
r .cityid(w1, x1, w2, x2, w3) →
qEND.w1 qMR
r,2 .x2 qW r .w2 qMR
r,1 .x1 qEND.w3 (4)
qWm.w1 → ‘population’ qWm.w1 (5)
qWm.w1 → ‘of’ qWm.w1
qWm.w1 → ... qWm.w1
qWm.w1 → ‘of’ qEND.w1 (6)
qWm.w1 → ... qEND.w1
qEND.W → e (7)
</equation>
<figureCaption confidence="0.993417">
Figure 2: Examples of transducer rules (bottom) that gen-
erate MR and NL associated with MR rules m-v (top).
Transducer rule 2 selects MR rule r from the MR gram-
mar. Rule 3 simultaneously writes the MR associated
with rule m and chooses an NL pattern (as does 4 for
r). Rules 5-7 generate the words associated with m ac-
cording to a unigram distribution specific to m.
</figureCaption>
<bodyText confidence="0.999936733333333">
grammaticality of the MR and lack flexibility since
sub-strings corresponding to a given tree fragment
must be completely pre-specified. Instead, we break
transductions down into a three stage process of
choosing the (i) MR grammar rule, (ii) NL expan-
sion pattern, and (iii) individual words according to
a unigram distribution. Such a decomposition in-
corporates independence assumptions that improve
generalizability. See Figure 2 for example rules
from our transducer and Figure 3 for a derivation.
To ensure that only grammatical MRs are gener-
ated, each state of our transducer encodes the iden-
tity of exactly one MR grammar rule. Transitions
between qMR and qNL states implicitly select the em-
bedded rule. For instance, rule 2 in Figure 2 selects
</bodyText>
<figure confidence="0.421231">
(2)
</figure>
<page confidence="0.981555">
490
</page>
<bodyText confidence="0.993651616438357">
MR grammar rule r to expand the ith child of the
parent produced by rule m. Aside from ensuring
the grammaticality of the generated MR, rules of
this type also model the probability of the MR, con-
ditioning the probability of a rule both on the par-
ent rule and the index of the child being expanded.
Thus, parent state qMR
��1 encodes not only the identity
of rule m, but also the child index, 1 in this case.
Once the MR rule is selected, qNL states are ap-
plied to select among rules such as 3 and 4 to gen-
erate the MR entity and choose the NL expansion
pattern. These rules determine the word order of the
language by deciding (i) whether or not to generate
words in a given location and (ii) where to insert the
result of processing each MR subtree. Decision (i) is
made by either transitioning to state qW� to generate
words or to qEND to generate the empty string. De-
cision (ii) is made with the order of xi’s on the right
hand side. Rule 4 illustrates the case where port-
land and maine in cityid(portland, maine) would be
realized in reverse order as “maine ... portland”.
The particular set of patterns that appear on the
right of rules such as 3 embodies the binary word at-
tachment decisions and the particular permutation of
xi in the NL. We allow words to be generated at the
beginning and end of each pattern and between the
xis. Thus, rule 4 is just one of 16 such possible pat-
terns (3 binary decisions and 2 permutations), while
rule 3 is one of 4. We instantiate all such rules and
allow the system to learn weights for them according
to the language of the training data.
Finally, the NL is filled out with words chosen ac-
cording to a unigram distribution, implemented in a
PCFG-like fashion, using a different rule for each
word which recursively chooses the next word un-
til a string termination rule is reached.2 Generating
word sequence “population of” entails first choosing
rule 5 in Figure 2. State qW� is then recursively ap-
plied to choose rule 6, generating “of” at the same
time as deciding to terminate the string by transi-
tioning to a new state qEND which deterministically
concludes by writing the empty string e.
On the MR side, rules 5-7 do very little: the tree
on the left side of rules 5 and 6 consists entirely of a
2There are roughly 25,000 rules in the transducers in our
experiments, and the majority of these implement the unigram
word distributions since every entity in the MR may potentially
produce any of the words it is paired with in training.
subtree variable wl, indicating that nothing is gener-
ated in the MR. Rule 7 subsequently generates these
subtrees as W symbols, marking corresponding lo-
cations where words might be produced in the NL,
which are later removed during post processing.3
Figure 3(b) illustrates the coupled generative pro-
cess. At each step of the derivation, an MR rule is
chosen to expand a node of the MR tree, and then a
corresponding part of the NL is expanded. Step 1.1
of the example chooses MR rule m, NUM →
population(PLACE). Transducer rule 3 then gener-
ates population in the MR (shown in the left column)
at the same time as choosing an NL expansion pat-
tern (Step 1.2) which is subsequently filled out with
specific words “population” (1.3) and “of” (1.4).
This coupled derivation can be represented by a
tree, shown in Figure 3(c), which explicitly repre-
sents the dependency structure of the coupled MR
and NL (a simplified version is shown in (d) for clar-
ity). In our transducer, which defines a joint distri-
bution over both the MR and NL, the probability of
a rule is conditioned on the parent state. Since each
state encodes an MR rule, MR rule specific distribu-
tions are learned for both the words and their order.
</bodyText>
<sectionHeader confidence="0.991831" genericHeader="method">
5 Relation to existing models
</sectionHeader>
<bodyText confidence="0.999978833333333">
The tree transducer model can be viewed either as
a generative procedure for building up two separate
structures or as a transformative machine that takes
one as input and produces another as output. Dif-
ferent semantic parsing approaches have taken one
or the other view, and both can be captured in this
single framework.
WASP (Wong and Mooney, 2006) is an exam-
ple of the former perspective, coupling the genera-
tion of the MR and NL with a synchronous gram-
mar, a formalism closely related to tree transducers.
The most significant difference from our approach
is that they use machine translation techniques for
automatically extracting rules from parallel corpora;
similar techniques can be applied to tree transduc-
ers (Galley et al., 2004). In fact, synchronous gram-
mars and tree transducers can be seen as instances of
the same more general class of automata (Shieber,
</bodyText>
<footnote confidence="0.868077">
3The addition of W symbols is a convenience; it is easier to
design transducer rules where every substring on the right side
corresponds to a subtree on the left.
</footnote>
<page confidence="0.994035">
491
</page>
<figureCaption confidence="0.998948">
Figure 3: Coupled derivation of an (MR, NL) pair. At each step an MR grammar rule is chosen to expand the MR and
the corresponding portion of the NL is then generated. Symbols W stand for locations in the tree corresponding to
substrings of the output and are removed in a post-processing step. (a) The (MR, NL) pair. (b) Step by step derivation.
(c) The same derivation shown in tree form. (d) The underlying dependency structure of the derivation.
</figureCaption>
<bodyText confidence="0.999954538461538">
2004). Rather than argue for one or the other, we
suggest that other approaches could also be inter-
preted in terms of general model classes, grounding
them in a broader base of theory.
The hybrid tree model (Lu et al., 2008) takes
a transformative perspective that is in some ways
more similar to our model. In fact, there is a one-
to-one relationship between the multinomial param-
eters of the two models. However, they represent the
MR and NL with a single tree and apply tree walk-
ing algorithms to extract them. Furthermore, they
implement a custom training procedure for search-
ing over the potential MR transformations. The tree
transducer, on the other hand, naturally captures the
same probabilistic dependencies while maintaining
the separation between MR and NL, and further al-
lows us to build upon a larger body of theory.
KRISP (Kate and Mooney, 2006) uses string clas-
sifiers to label substrings of the NL with entities
from the MR. To focus search, they impose an or-
dering constraint based on the structure of the MR
tree, which they relax by allowing the re-ordering
of sibling nodes and devise a procedure for recover-
ing the MR from the permuted tree. This procedure
corresponds to backward-application in tree trans-
ducers, identifying the most likely input tree given a
</bodyText>
<page confidence="0.997077">
492
</page>
<bodyText confidence="0.99903555">
particular output string.
SCISSOR (Ge and Mooney, 2005) takes syntactic
parses rather than NL strings and attempts to trans-
late them into MR expressions. While few seman-
tic parsers attempt to exploit syntactic information,
there are techniques from machine translation for
using tree transducers to map between parsed par-
allel corpora, and these techniques could likely be
applied to semantic parsing.
B¨orschinger et al. (2011) argue for the PCFG as
an alternative model class, permitting conventional
grammar induction techniques, and tree transducers
are similar enough that many techniques are applica-
ble to both. However, the PCFG is less amenable to
conceptualizing correspondences between parallel
structures, and their model is more restrictive, only
applicable to domains with finite MR languages,
since their non-terminals encode entire MRs. The
tree transducer framework, on the other hand, allows
us to condition on individual MR rules.
</bodyText>
<sectionHeader confidence="0.990561" genericHeader="method">
6 Variational Bayes for tree transducers
</sectionHeader>
<bodyText confidence="0.999971142857143">
As seen in the example in Figure 3(c), tree trans-
ducers not only operate on trees, their derivations
are themselves trees, making them amenable to dy-
namic programming and an EM training procedure
resembling inside-outside (Graehl et al., 2008). EM
assigns zero probability to events not seen in the
training data, however, limiting the ability to gen-
eralize to novel items. The Bayesian framework of-
fers an elegant solution to this problem, introducing
a prior over rule weights which simultaneously en-
sures that all rules receive non-zero probability and
allows the incorporation of prior knowledge and in-
tuitions. Unfortunately, the introduction of a prior
makes exact inference intractable, so we use an ap-
proximate method, variational Bayesian inference
(Bishop, 2006), deriving an algorithm similar to that
for PCFGs (Kurihara and Sato, 2006).
The tree transducer defines a joint distribution
over the input y, output w, and their derivation x
as the product of the weights of the rules appearing
in x. That is,
</bodyText>
<equation confidence="0.915842">
p(y, x, w|θ) = rl θ(r)c,(x)
rER
</equation>
<bodyText confidence="0.999935">
where θ is the set of multinomial parameters, r is a
transducer rule, θ(r) is its weight, and cr(x) is the
number of times r appears in x. In EM, we are in-
terested in the point estimate for θ that maximizes
p(Y, W|θ), where Y and W are the N input-output
pairs in the training data. In the Bayesian setting,
however, we place a symmetric Dirichlet prior over
θ and estimate a posterior distribution over both X
and θ.
</bodyText>
<equation confidence="0.9991346">
p(θ, X |Y, W) = p(Y, X ,W, θ)
p(Y, W)
p(θ) HNi=1 p(yi, xi, wi|θ)
f p(θ) HN �xEXi p(yi, x, wi|θ)dθ
i=1
</equation>
<bodyText confidence="0.999869625">
Since the integral in the denominator is in-
tractable, we look for an appropriate approximation
q(θ, X) ≈ p(θ, X|Y, W). In particular, we assume
the rule weights and the derivations are independent,
i.e., q(θ, X) = q(θ)q(X). The basic idea is then to
define a lower bound F ≤ lnp(Y, W) in terms of q
and then apply the calculus of variations to find a q
that maximizes F.
</bodyText>
<equation confidence="0.99989375">
lnp(Y,W|α) = lnEq[p(Y,X,W|θ)
q(θ, X) ]
≥ Eq[ln p(Y,X ,W|θ)
q(θ, X ) ] = F,
</equation>
<bodyText confidence="0.99965725">
Applying our independence assumption, we arrive at
the following expression for F, where θt is the par-
ticular parameter vector corresponding to the rules
with parent state t:
</bodyText>
<equation confidence="0.9994048">
�F = (Eq(θt)[lnp(θt|αt)] − Eq(θt)[lnq(θt)])
tEQ
N
+ (Eq[lnp(wi, xi, yi|θ)] − Eq(xi)[lnq(xi)]) .
i=1
</equation>
<bodyText confidence="0.999934666666667">
We find the q(θt) and q(xi) that maximize F by
taking derivatives of the Lagrangian, setting them to
zero, and solving, which yields:
</bodyText>
<equation confidence="0.99002425">
q(θt) = Dirichlet(θt|ˆαt)
ˆθ(r)c&apos;(xi)
� H
xEXi rER ˆθ(r)c*(x)
</equation>
<bodyText confidence="0.843083">
where
</bodyText>
<equation confidence="0.99778775">
ˆα(r) = α(r) + � Eq(xi)[cr(xi)]
i
� �
�
ˆθ(r) = exp �Ψ(ˆα(r)) − Ψ( ˆα(r)) � .
r:s(r)=t
=
q(xi) = HrER
</equation>
<page confidence="0.996207">
493
</page>
<bodyText confidence="0.999925166666667">
The parameters of q(Bt) are defined with respect
to q(xi) and the parameters of q(xi) with respect
to the parameters of q(Bt). q(xi) can be computed
efficiently using inside-outside. Thus, we can per-
form an EM-like alternation between calculating α�
and �B.4
It is also possible to estimate the hyper-
parameters α from data, a practice known as em-
pirical Bayes, by optimizing T. We explore learn-
ing separate hyper-parameters αt for each Bt, us-
ing a fixed point update described by Minka (2000),
where kt is the number of rules with parent state t:
</bodyText>
<equation confidence="0.766861333333333">
−1
1 t (a2F2 −1 ( aF
ktα2 Bαt JJ aαt JJ
</equation>
<sectionHeader confidence="0.939906" genericHeader="method">
7 Training and decoding
</sectionHeader>
<bodyText confidence="0.999989833333333">
We implement our VB training algorithm inside the
tree transducer package Tiburon (May and Knight,
2006), and experiment with both manually set and
automatically estimated priors. For our manually
set priors, we explore different hyper-parameter set-
tings for three different priors, one for each of the
main decision types: MR rule, NL pattern, and word
generation. For the automatic priors, we estimate
separate hyper-parameters for each multinomial (of
which there are hundreds). As is standard, we ini-
tialize the word distributions using a variant of IBM
model 1, and make use of NP lists (a manually cre-
ated list of the constants in the MR language paired
with the words that refer to them in the corpus).
At test time, since finding the most probable MR
for a sentence involves summing over all possible
derivations, we instead find the MR associated with
the most probable derivation.
</bodyText>
<sectionHeader confidence="0.857393" genericHeader="evaluation">
8 Experimental setup and evaluation
</sectionHeader>
<bodyText confidence="0.9998345">
We evaluate the system on GeoQuery (Wong and
Mooney, 2006), a parallel corpus of 880 English
questions and database queries about United States
geography, 250 of which were translated into Span-
ish, Japanese, and Turkish. We present here ad-
ditional translations of the full 880 sentences into
</bodyText>
<footnote confidence="0.916636666666667">
4Because of the resemblance to EM, this procedure has been
called VBEM. Unlike EM, however, this procedure alternates
between two estimation steps and has no maximization step.
</footnote>
<bodyText confidence="0.997124409090909">
German, Greek, and Thai. For evaluation, follow-
ing from Kwiatkowski et al. (2010), we reserve 280
sentences for test and train on the remaining 600.
During development, we use cross-validation on the
600 sentence training set. At test, we run once on the
remaining 280 and perform 10 fold cross-validation
on the 250 sentence sets.
To judge correctness, we follow standard prac-
tice and submit each parse as a GeoQuery database
query, and say the parse is correct only if the answer
matches the gold standard. We report raw accuracy
(the percentage of sentences with correct answers),
as well as F1: the harmonic mean of precision (the
proportion of correct answers out of sentences with
a parse) and recall (the proportion of correct answers
out of all sentences).5
We run three other state-of-the-art systems for
comparison. WASP (Wong and Mooney, 2006) and
the hybrid tree (Lu et al., 2008) are chosen to rep-
resent tree transformation based approaches, and,
while this comparison is our primary focus, we also
report UBL-S (Kwiatkowski et al., 2010) as a non-
tree based top-performing system.6 The hybrid tree
is notable as the only other system based on a gen-
erative model, and uni-hybrid, a version that uses a
unigram distribution over words, is very similar to
our own model. We also report the best performing
version, re-hybrid, which incorporates a discrimina-
tive re-ranking step.
We report transducer performance under three dif-
ferent training conditions: tsEM using EM, tsVB-
auto using VB with empirical Bayes, and tsVB-hand
using hyper-parameters manually tuned on the Ger-
man training data (α of 0.3, 0.8, and 0.25 for MR
rule, NL pattern, and word choices, respectively).
Table 1 shows results for 10 fold cross-validation
on the training set. The results highlight the benefit
of the Dirichlet prior, whether manually or automat-
ically set. VB improves over EM considerably, most
likely because (1) the handling of unknown words
and MR entities allows it to return an analysis for all
sentences, and (2) the sparse Dirichlet prior favors
fewer rules, reasonable in this setting where only a
few words are likely to share the same meaning.
</bodyText>
<footnote confidence="0.73249725">
5Note that accuracy and f-score reduce to the same formula
if there are no parse failures.
6UBL-S is based on CCG, which can be viewed as a map-
ping between graphs more general than trees.
</footnote>
<figure confidence="0.6086046">
1
α′ =
t
+
αt
</figure>
<page confidence="0.993158">
494
</page>
<table confidence="0.999933222222222">
DEV geo600 - 10 fold cross-val
German Greek
Acc F1 Acc F1
UBL-S 76.7 76.9 76.2 76.5
WASP 66.3 75.0 71.2 79.7
uni-hybrid 61.7 66.1 71.0 75.4
re-hybrid 62.3 69.5 70.2 76.8
tsEM 61.7 67.9 67.3 73.2
tsVB-auto 74.0 74.0 •79.8 •79.8
tsVB-hand •78.0 •78.0 79.0 79.0
English Thai
UBL-S 85.3 85.4 74.0 74.1
WASP 73.5 79.4 69.8 73.9
uni-hybrid 76.3 79.0 71.3 73.7
re-hybrid 77.0 82.2 71.7 76.0
tsEM 73.5 78.1 69.8 72.9
tsVB-auto 81.2 81.2 74.7 74.7
tsVB-hand •83.7 •83.7 •76.7 •76.7
</table>
<tableCaption confidence="0.9625696">
Table 1: Accuracy and F1 score comparisons on the
geo600 training set. Highest scores are in bold, while
the highest among the tree based models are marked with
a bullet. The dotted line separates the tree based from
non-tree based models.
</tableCaption>
<bodyText confidence="0.9995735">
On the test set (Table 2), we only run the model
variants that perform best on the training set. Test set
accuracy is consistently higher for the VB trained
tree transducer than the other tree transformation
based models (and often highest overall), while f-
score remains competitive.7
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999787916666667">
We have argued that tree transformation based se-
mantic parsing can benefit from the literature on for-
mal language theory and tree automata, and have
taken a step in this direction by presenting a tree
transducer based semantic parser. Drawing this con-
nection facilitates a greater flow of ideas in the
research community, allowing semantic parsing to
leverage ideas from other work with tree automata,
while making clearer how seemingly isolated ef-
forts might relate to one another. We demonstrate
this by both building on previous work in train-
ing tree transducers using EM (Graehl et al., 2008),
</bodyText>
<footnote confidence="0.564063666666667">
7Numbers differ slightly here from previously published re-
sults due to the fact that we have standardized the inputs to the
different systems.
</footnote>
<table confidence="0.999946565217391">
TEST geo880 - 600 train/280 test
German Greek
Acc F1 Acc F1
UBL-S 75.0 75.0 73.6 73.7
WASP 65.7 • 74.9 70.7 • 78.6
re-hybrid 62.1 68.5 69.3 74.6
tsVB-hand • 74.6 74.6 •75.4 75.4
English Thai
UBL-S 82.1 82.1 66.4 66.4
WASP 71.1 77.7 71.4 75.0
re-hybrid 76.8 • 81.0 73.6 76.7
tsVB-hand • 79.3 79.3 • 78.2 • 78.2
geo250 - 10 fold cross-val
English Spanish
UBL-S 80.4 80.6 79.7 80.1
WASP 70.0 80.8 72.4 81.0
re-hybrid 74.8 82.6 78.8 • 86.2
tsVB-hand • 83.2 • 83.2 • 80.0 80.0
Japanese Turkish
UBL-S 80.5 80.6 74.2 74.9
WASP 74.4 • 82.9 62.4 75.9
re-hybrid 76.8 82.4 66.8 • 77.5
tsVB-hand • 78.0 78.0 • 75.6 75.6
</table>
<tableCaption confidence="0.7721112">
Table 2: Accuracy and F1 score comparisons on the
geo880 and geo250 test sets. Highest scores are in
bold, while the highest among the tree based models are
marked with a bullet. The dotted line separates the tree
based from non-tree based models.7
</tableCaption>
<bodyText confidence="0.999989">
and describing a general purpose variational infer-
ence algorithm for adapting tree transducers to the
Bayesian framework. The new VB algorithm re-
sults in an overall performance improvement for the
transducer over EM training, and the general effec-
tiveness of the approach is further demonstrated by
the Bayesian transducer achieving highest accuracy
among other tree transformation based approaches.
</bodyText>
<sectionHeader confidence="0.999015" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.940132142857143">
We thank Joel Lang, Michael Auli, Stella Frank,
Prachya Boonkwan, Christos Christodoulopoulos,
Ioannis Konstas, and Tom Kwiatkowski for provid-
ing the new translations of GeoQuery. This research
was supported in part under the Australian Re-
search Council’s Discovery Projects funding scheme
(project number DP110102506).
</bodyText>
<page confidence="0.999007">
495
</page>
<sectionHeader confidence="0.998346" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999593081081081">
Christopher M. Bishop. Pattern Recognition and Ma-
chine Learning. Springer, 2006.
Benjamin B¨orschinger, Bevan K. Jones, and Mark John-
son. Reducing grounded learning tasks to grammati-
cal inference. In Proc. of the Conference on Empirical
Methods in Natural Language Processing, 2011.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. Bayesian inference for finite-
state transducers. In Proc. of the annual meeting of
the North American Association for Computational Lin-
guistics, 2010.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. What’s in a translation rule? In Proc. of the
annual meeting of the North American Association for
Computational Linguistics, 2004.
Ruifang Ge and Raymond J. Mooney. A statistical se-
mantic parser that integrates syntax and semantics. In
Proceedings of the Conference on Computational Natu-
ral Language Learning, 2005.
Jonathon Graehl, Kevin Knight, and Jon May. Training
tree transducers. Computational Linguistics, 34:391–
427, 2008.
Rohit J. Kate and Raymond J. Mooney. Using string-
kernels for learning semantic parsers. In Proc. of the
International Conference on Computational Linguistics
and the annual meeting of the Association for Compu-
tational Linguistics, 2006.
Kevin Knight and Jonathon Greahl. An overview of prob-
abilistic tree transducers for natural language process-
ing. In Proc. of the 6th International Conference on
Intelligent Text Processing and Computational Linguis-
tics, 2005.
Kenichi Kurihara and Taisuke Sato. Variational Bayesian
grammar induction for natural language. In Proc. of
the 8th International Colloquium on Grammatical In-
ference, 2006.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. Inducing probabilistic CCG
grammars from logical form with higher-order unifica-
tion. In Proc. of the Conference on Empirical Methods
in Natural Language Processing, 2010.
Percy Liang, Michael I. Jordan, and Dan Klein. Learning
dependency-based compositional semantics. In Proc.
of the annual meeting of the Association for Computa-
tional Linguistics, 2011.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. A generative model for parsing natural language
to meaning representations. In Proc. of the Conference
on Empirical Methods in Natural Language Processing,
2008.
Andreas Maletti, Jonathan Graehl, Mark Hopkins, and
Kevin Knight. The power of extended top-down tree
transducers. SIAMJ. Comput., 39:410–430, June 2009.
Jon May and Kevin Knight. Tiburon: A weighted tree au-
tomata toolkit. In Proc. of the International Conference
on Implementation and Application ofAutomata, 2006.
Tom Minka. Estimating a Dirichlet distribution. Techni-
cal report, M.I.T., 2000.
W.C. Rounds. Mappings and grammars on trees. Mathe-
matical Systems Theory 4, pages 257–287, 1970.
Stuart M. Shieber. Synchronous grammars as tree trans-
ducers. In Proc. of the Seventh International Workshop
on Tree Adjoining Grammar and Related Formalisms,
2004.
J.W. Thatcher. Generalized sequential machine maps. J.
Comput. System Sci. 4, pages 339–367, 1970.
Yuk Wah Wong and Raymond J. Mooney. Learning for
semantic parsing with statistical machine translation. In
Proc. of Human Language Technology Conference and
the annual meeting of the North American Chapter of
the Association for Computational Linguistics, 2006.
Kenji Yamada and Kevin Knight. A syntax-based statis-
tical translation model. In Proc. of the annual meeting
of the Association for Computational Linguistics, 2001.
</reference>
<page confidence="0.999121">
496
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.857107">
<title confidence="0.999851">Semantic Parsing with Bayesian Tree Transducers</title>
<author confidence="0.932925">Keeley b k jonessms ed ac uk Mark Johnsonmq edu au sgwaterinf ed ac uk</author>
<affiliation confidence="0.9980665">of Informatics of Computing University of Edinburgh Macquarie University</affiliation>
<address confidence="0.999713">Edinburgh, EH8 9AB, UK Sydney, NSW 2109, Australia</address>
<abstract confidence="0.999449166666667">Many semantic parsing models use tree transformations to map between natural language and meaning representation. However, while tree transformations are central to several state-of-the-art approaches, little use has been made of the rich literature on tree automata. This paper makes the connection concrete with a tree transducer based semantic parsing model and suggests that other models can be interpreted in a similar framework, increasing the generality of their contributions. In particular, this paper further introduces a variational Bayesian inference algorithm that is applicable to a wide class of tree transducers, producing state-of-the-art semantic parsing results while remaining applicable to any domain employing probabilistic tree transducers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Pattern Recognition and Machine Learning.</title>
<date>2006</date>
<publisher>Springer,</publisher>
<contexts>
<context position="19457" citStr="Bishop, 2006" startWordPosition="3333" endWordPosition="3334">ramming and an EM training procedure resembling inside-outside (Graehl et al., 2008). EM assigns zero probability to events not seen in the training data, however, limiting the ability to generalize to novel items. The Bayesian framework offers an elegant solution to this problem, introducing a prior over rule weights which simultaneously ensures that all rules receive non-zero probability and allows the incorporation of prior knowledge and intuitions. Unfortunately, the introduction of a prior makes exact inference intractable, so we use an approximate method, variational Bayesian inference (Bishop, 2006), deriving an algorithm similar to that for PCFGs (Kurihara and Sato, 2006). The tree transducer defines a joint distribution over the input y, output w, and their derivation x as the product of the weights of the rules appearing in x. That is, p(y, x, w|θ) = rl θ(r)c,(x) rER where θ is the set of multinomial parameters, r is a transducer rule, θ(r) is its weight, and cr(x) is the number of times r appears in x. In EM, we are interested in the point estimate for θ that maximizes p(Y, W|θ), where Y and W are the N input-output pairs in the training data. In the Bayesian setting, however, we pla</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin B¨orschinger</author>
<author>Bevan K Jones</author>
<author>Mark Johnson</author>
</authors>
<title>Reducing grounded learning tasks to grammatical inference.</title>
<date>2011</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<marker>B¨orschinger, Jones, Johnson, 2011</marker>
<rawString>Benjamin B¨orschinger, Bevan K. Jones, and Mark Johnson. Reducing grounded learning tasks to grammatical inference. In Proc. of the Conference on Empirical Methods in Natural Language Processing, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Adam Pauls</author>
<author>Sujith Ravi</author>
</authors>
<title>Bayesian inference for finitestate transducers.</title>
<date>2010</date>
<booktitle>In Proc. of the annual meeting of the North American Association for Computational Linguistics,</booktitle>
<contexts>
<context position="3197" citStr="Chiang et al., 2010" startWordPosition="483" endWordPosition="486">g by presenting a tree transducer model and drawing connections to other similar systems. We make a further contribution by bringing to tree transducers the benefits of the Bayesian framework for principled handling of data sparsity and 488 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 488–496, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics prior knowledge. Graehl et al. (2008) present an EM training procedure for top down tree transducers, but while there are Bayesian approaches to string transducers (Chiang et al., 2010) and PCFGs (Kurihara and Sato, 2006), there has yet to be a proposal for Bayesian inference in tree transducers. Our variational algorithm produces better semantic parses than EM while remaining general to a broad class of transducers appropriate for other domains. In short, our contributions are three-fold: we present a new state-of-the-art semantic parsing model, propose a broader theory for tree transformation based semantic parsing, and present a general inference algorithm for the tree transducer framework. We recommend the last of these as just one benefit of working within a general the</context>
</contexts>
<marker>Chiang, Graehl, Knight, Pauls, Ravi, 2010</marker>
<rawString>David Chiang, Jonathan Graehl, Kevin Knight, Adam Pauls, and Sujith Ravi. Bayesian inference for finitestate transducers. In Proc. of the annual meeting of the North American Association for Computational Linguistics, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. of the annual meeting of the North American Association for Computational Linguistics,</booktitle>
<contexts>
<context position="15661" citStr="Galley et al., 2004" startWordPosition="2714" endWordPosition="2717">ransformative machine that takes one as input and produces another as output. Different semantic parsing approaches have taken one or the other view, and both can be captured in this single framework. WASP (Wong and Mooney, 2006) is an example of the former perspective, coupling the generation of the MR and NL with a synchronous grammar, a formalism closely related to tree transducers. The most significant difference from our approach is that they use machine translation techniques for automatically extracting rules from parallel corpora; similar techniques can be applied to tree transducers (Galley et al., 2004). In fact, synchronous grammars and tree transducers can be seen as instances of the same more general class of automata (Shieber, 3The addition of W symbols is a convenience; it is easier to design transducer rules where every substring on the right side corresponds to a subtree on the left. 491 Figure 3: Coupled derivation of an (MR, NL) pair. At each step an MR grammar rule is chosen to expand the MR and the corresponding portion of the NL is then generated. Symbols W stand for locations in the tree corresponding to substrings of the output and are removed in a post-processing step. (a) The</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. What’s in a translation rule? In Proc. of the annual meeting of the North American Association for Computational Linguistics, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning,</booktitle>
<contexts>
<context position="1697" citStr="Ge and Mooney, 2005" startWordPosition="243" endWordPosition="246">sducers. 1 Introduction Semantic parsing is the task of mapping natural language sentences to a formal representation of meaning. Typically, a system is trained on pairs of natural language sentences (NLs) and their meaning representation expressions (MRs), as in figure 1(a), and the system must generalize to novel sentences. Most semantic parsing models rely on an assumption of structural similarity between MR and NL. Since strict isomorphism is overly restrictive, this assumption is often relaxed by applying transformations. Several approaches assume a tree structure to the NL, MR, or both (Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; B¨orschinger et al., 2011), and often inFigure 1: (a) An example sentence/meaning pair, (b) a tree transformation based mapping, and (c) a tree transducer that performs the mapping. volve tree transformations either between two trees or a tree and a string. The tree transducer, a formalism from automata theory which has seen interest in machine translation (Yamada and Knight, 2001; Graehl et al., 2008) and has potential applications in many other areas, is well suited to formalizing such tree transformation based models. Yet, whi</context>
<context position="17750" citStr="Ge and Mooney, 2005" startWordPosition="3072" endWordPosition="3075">e maintaining the separation between MR and NL, and further allows us to build upon a larger body of theory. KRISP (Kate and Mooney, 2006) uses string classifiers to label substrings of the NL with entities from the MR. To focus search, they impose an ordering constraint based on the structure of the MR tree, which they relax by allowing the re-ordering of sibling nodes and devise a procedure for recovering the MR from the permuted tree. This procedure corresponds to backward-application in tree transducers, identifying the most likely input tree given a 492 particular output string. SCISSOR (Ge and Mooney, 2005) takes syntactic parses rather than NL strings and attempts to translate them into MR expressions. While few semantic parsers attempt to exploit syntactic information, there are techniques from machine translation for using tree transducers to map between parsed parallel corpora, and these techniques could likely be applied to semantic parsing. B¨orschinger et al. (2011) argue for the PCFG as an alternative model class, permitting conventional grammar induction techniques, and tree transducers are similar enough that many techniques are applicable to both. However, the PCFG is less amenable to</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>Ruifang Ge and Raymond J. Mooney. A statistical semantic parser that integrates syntax and semantics. In Proceedings of the Conference on Computational Natural Language Learning, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathon Graehl</author>
<author>Kevin Knight</author>
<author>Jon May</author>
</authors>
<title>Training tree transducers.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<pages>427</pages>
<contexts>
<context position="2167" citStr="Graehl et al., 2008" startWordPosition="322" endWordPosition="325">, this assumption is often relaxed by applying transformations. Several approaches assume a tree structure to the NL, MR, or both (Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; B¨orschinger et al., 2011), and often inFigure 1: (a) An example sentence/meaning pair, (b) a tree transformation based mapping, and (c) a tree transducer that performs the mapping. volve tree transformations either between two trees or a tree and a string. The tree transducer, a formalism from automata theory which has seen interest in machine translation (Yamada and Knight, 2001; Graehl et al., 2008) and has potential applications in many other areas, is well suited to formalizing such tree transformation based models. Yet, while many semantic parsing systems resemble the formalism, each was proposed as an independent model requiring custom algorithms, leaving it unclear how developments in one line of inquiry relate to others. We argue for a unifying theory of tree transformation based semantic parsing by presenting a tree transducer model and drawing connections to other similar systems. We make a further contribution by bringing to tree transducers the benefits of the Bayesian framewor</context>
<context position="18928" citStr="Graehl et al., 2008" startWordPosition="3250" endWordPosition="3253">oth. However, the PCFG is less amenable to conceptualizing correspondences between parallel structures, and their model is more restrictive, only applicable to domains with finite MR languages, since their non-terminals encode entire MRs. The tree transducer framework, on the other hand, allows us to condition on individual MR rules. 6 Variational Bayes for tree transducers As seen in the example in Figure 3(c), tree transducers not only operate on trees, their derivations are themselves trees, making them amenable to dynamic programming and an EM training procedure resembling inside-outside (Graehl et al., 2008). EM assigns zero probability to events not seen in the training data, however, limiting the ability to generalize to novel items. The Bayesian framework offers an elegant solution to this problem, introducing a prior over rule weights which simultaneously ensures that all rules receive non-zero probability and allows the incorporation of prior knowledge and intuitions. Unfortunately, the introduction of a prior makes exact inference intractable, so we use an approximate method, variational Bayesian inference (Bishop, 2006), deriving an algorithm similar to that for PCFGs (Kurihara and Sato, 2</context>
<context position="27253" citStr="Graehl et al., 2008" startWordPosition="4677" endWordPosition="4680">petitive.7 9 Conclusion We have argued that tree transformation based semantic parsing can benefit from the literature on formal language theory and tree automata, and have taken a step in this direction by presenting a tree transducer based semantic parser. Drawing this connection facilitates a greater flow of ideas in the research community, allowing semantic parsing to leverage ideas from other work with tree automata, while making clearer how seemingly isolated efforts might relate to one another. We demonstrate this by both building on previous work in training tree transducers using EM (Graehl et al., 2008), 7Numbers differ slightly here from previously published results due to the fact that we have standardized the inputs to the different systems. TEST geo880 - 600 train/280 test German Greek Acc F1 Acc F1 UBL-S 75.0 75.0 73.6 73.7 WASP 65.7 • 74.9 70.7 • 78.6 re-hybrid 62.1 68.5 69.3 74.6 tsVB-hand • 74.6 74.6 •75.4 75.4 English Thai UBL-S 82.1 82.1 66.4 66.4 WASP 71.1 77.7 71.4 75.0 re-hybrid 76.8 • 81.0 73.6 76.7 tsVB-hand • 79.3 79.3 • 78.2 • 78.2 geo250 - 10 fold cross-val English Spanish UBL-S 80.4 80.6 79.7 80.1 WASP 70.0 80.8 72.4 81.0 re-hybrid 74.8 82.6 78.8 • 86.2 tsVB-hand • 83.2 • </context>
</contexts>
<marker>Graehl, Knight, May, 2008</marker>
<rawString>Jonathon Graehl, Kevin Knight, and Jon May. Training tree transducers. Computational Linguistics, 34:391– 427, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using stringkernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In Proc. of the International Conference on Computational Linguistics and the annual meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="1720" citStr="Kate and Mooney, 2006" startWordPosition="247" endWordPosition="250">on Semantic parsing is the task of mapping natural language sentences to a formal representation of meaning. Typically, a system is trained on pairs of natural language sentences (NLs) and their meaning representation expressions (MRs), as in figure 1(a), and the system must generalize to novel sentences. Most semantic parsing models rely on an assumption of structural similarity between MR and NL. Since strict isomorphism is overly restrictive, this assumption is often relaxed by applying transformations. Several approaches assume a tree structure to the NL, MR, or both (Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; B¨orschinger et al., 2011), and often inFigure 1: (a) An example sentence/meaning pair, (b) a tree transformation based mapping, and (c) a tree transducer that performs the mapping. volve tree transformations either between two trees or a tree and a string. The tree transducer, a formalism from automata theory which has seen interest in machine translation (Yamada and Knight, 2001; Graehl et al., 2008) and has potential applications in many other areas, is well suited to formalizing such tree transformation based models. Yet, while many semantic parsin</context>
<context position="17268" citStr="Kate and Mooney, 2006" startWordPosition="2991" endWordPosition="2994">sformative perspective that is in some ways more similar to our model. In fact, there is a oneto-one relationship between the multinomial parameters of the two models. However, they represent the MR and NL with a single tree and apply tree walking algorithms to extract them. Furthermore, they implement a custom training procedure for searching over the potential MR transformations. The tree transducer, on the other hand, naturally captures the same probabilistic dependencies while maintaining the separation between MR and NL, and further allows us to build upon a larger body of theory. KRISP (Kate and Mooney, 2006) uses string classifiers to label substrings of the NL with entities from the MR. To focus search, they impose an ordering constraint based on the structure of the MR tree, which they relax by allowing the re-ordering of sibling nodes and devise a procedure for recovering the MR from the permuted tree. This procedure corresponds to backward-application in tree transducers, identifying the most likely input tree given a 492 particular output string. SCISSOR (Ge and Mooney, 2005) takes syntactic parses rather than NL strings and attempts to translate them into MR expressions. While few semantic </context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. Using stringkernels for learning semantic parsers. In Proc. of the International Conference on Computational Linguistics and the annual meeting of the Association for Computational Linguistics, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathon Greahl</author>
</authors>
<title>An overview of probabilistic tree transducers for natural language processing.</title>
<date>2005</date>
<booktitle>In Proc. of the 6th International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<contexts>
<context position="5854" citStr="Knight and Greahl, 2005" startWordPosition="939" endWordPosition="942">erates the set of parses of G. Consequently, while we assume we have an RTG for the MR language, there is no loss of generality if the MR language is actually context free. 3 Weighted root-to-frontier, linear, non-deleting tree-to-string transducers Tree transducers (Rounds, 1970; Thatcher, 1970) are generalizations of finite state machines that operate on trees. Mirroring the branching nature of its input, the transducer may simultaneously transition to several successor states, assigning a separate state to each subtree. There are many classes of transducer with different formal properties (Knight and Greahl, 2005; Maletti et al., 2009). Figure 1(c) is an example of a root-to-frontier, linear, non-deleting tree-to-string transducer. It is defined using rules where the left hand side identifies a state of the transducer and a fragment of the input tree, and the right hand side describes a portion of the output string. Variables xi stand for entire sub-trees, and state-variable pairs qj.xi stand for strings produced by applying the transducer starting at state qj to subtree xi. Figure 1(b) illustrates an application of the transducer, taking the tree on the left as input and outputting the string on the </context>
</contexts>
<marker>Knight, Greahl, 2005</marker>
<rawString>Kevin Knight and Jonathon Greahl. An overview of probabilistic tree transducers for natural language processing. In Proc. of the 6th International Conference on Intelligent Text Processing and Computational Linguistics, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenichi Kurihara</author>
<author>Taisuke Sato</author>
</authors>
<title>Variational Bayesian grammar induction for natural language.</title>
<date>2006</date>
<booktitle>In Proc. of the 8th International Colloquium on Grammatical Inference,</booktitle>
<contexts>
<context position="3233" citStr="Kurihara and Sato, 2006" startWordPosition="489" endWordPosition="492">r model and drawing connections to other similar systems. We make a further contribution by bringing to tree transducers the benefits of the Bayesian framework for principled handling of data sparsity and 488 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 488–496, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics prior knowledge. Graehl et al. (2008) present an EM training procedure for top down tree transducers, but while there are Bayesian approaches to string transducers (Chiang et al., 2010) and PCFGs (Kurihara and Sato, 2006), there has yet to be a proposal for Bayesian inference in tree transducers. Our variational algorithm produces better semantic parses than EM while remaining general to a broad class of transducers appropriate for other domains. In short, our contributions are three-fold: we present a new state-of-the-art semantic parsing model, propose a broader theory for tree transformation based semantic parsing, and present a general inference algorithm for the tree transducer framework. We recommend the last of these as just one benefit of working within a general theory: contributions are more broadly </context>
<context position="19532" citStr="Kurihara and Sato, 2006" startWordPosition="3343" endWordPosition="3346">Graehl et al., 2008). EM assigns zero probability to events not seen in the training data, however, limiting the ability to generalize to novel items. The Bayesian framework offers an elegant solution to this problem, introducing a prior over rule weights which simultaneously ensures that all rules receive non-zero probability and allows the incorporation of prior knowledge and intuitions. Unfortunately, the introduction of a prior makes exact inference intractable, so we use an approximate method, variational Bayesian inference (Bishop, 2006), deriving an algorithm similar to that for PCFGs (Kurihara and Sato, 2006). The tree transducer defines a joint distribution over the input y, output w, and their derivation x as the product of the weights of the rules appearing in x. That is, p(y, x, w|θ) = rl θ(r)c,(x) rER where θ is the set of multinomial parameters, r is a transducer rule, θ(r) is its weight, and cr(x) is the number of times r appears in x. In EM, we are interested in the point estimate for θ that maximizes p(Y, W|θ), where Y and W are the N input-output pairs in the training data. In the Bayesian setting, however, we place a symmetric Dirichlet prior over θ and estimate a posterior distribution</context>
</contexts>
<marker>Kurihara, Sato, 2006</marker>
<rawString>Kenichi Kurihara and Taisuke Sato. Variational Bayesian grammar induction for natural language. In Proc. of the 8th International Colloquium on Grammatical Inference, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Inducing probabilistic CCG grammars from logical form with higher-order unification.</title>
<date>2010</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<contexts>
<context position="23371" citStr="Kwiatkowski et al. (2010)" startWordPosition="4021" endWordPosition="4024">ted with the most probable derivation. 8 Experimental setup and evaluation We evaluate the system on GeoQuery (Wong and Mooney, 2006), a parallel corpus of 880 English questions and database queries about United States geography, 250 of which were translated into Spanish, Japanese, and Turkish. We present here additional translations of the full 880 sentences into 4Because of the resemblance to EM, this procedure has been called VBEM. Unlike EM, however, this procedure alternates between two estimation steps and has no maximization step. German, Greek, and Thai. For evaluation, following from Kwiatkowski et al. (2010), we reserve 280 sentences for test and train on the remaining 600. During development, we use cross-validation on the 600 sentence training set. At test, we run once on the remaining 280 and perform 10 fold cross-validation on the 250 sentence sets. To judge correctness, we follow standard practice and submit each parse as a GeoQuery database query, and say the parse is correct only if the answer matches the gold standard. We report raw accuracy (the percentage of sentences with correct answers), as well as F1: the harmonic mean of precision (the proportion of correct answers out of sentences</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. Inducing probabilistic CCG grammars from logical form with higher-order unification. In Proc. of the Conference on Empirical Methods in Natural Language Processing, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Proc. of the annual meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="5107" citStr="Liang et al. (2011)" startWordPosition="825" endWordPosition="828">t E, and TE(A) as the set of all trees in TEUA where symbols from A only occur at the leaves. Then an RTG is a tuple (Q, E, qstart, R), where Q is a set of states, E is an alphabet, qstart ∈ Q is the initial state, and R is a set of grammar rules of the form q → t, where q is a state from Q and t is a tree from TE(Q). A rule typically consists of a parent state (left) and its child states and output symbol (right). We indicate states using all capital letters: NUM → population(PLACE). Intuitively, an RTG is a CFG where the yield of every parse is itself a tree. In fact, for any CFG G, it 1See Liang et al. (2011) for work in representing lambda calculus expressions with trees. is straightforward to produce a corresponding RTG that generates the set of parses of G. Consequently, while we assume we have an RTG for the MR language, there is no loss of generality if the MR language is actually context free. 3 Weighted root-to-frontier, linear, non-deleting tree-to-string transducers Tree transducers (Rounds, 1970; Thatcher, 1970) are generalizations of finite state machines that operate on trees. Mirroring the branching nature of its input, the transducer may simultaneously transition to several successor</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. Learning dependency-based compositional semantics. In Proc. of the annual meeting of the Association for Computational Linguistics, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
<author>Wee Sun Lee</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>A generative model for parsing natural language to meaning representations.</title>
<date>2008</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<contexts>
<context position="1760" citStr="Lu et al., 2008" startWordPosition="255" endWordPosition="258">ural language sentences to a formal representation of meaning. Typically, a system is trained on pairs of natural language sentences (NLs) and their meaning representation expressions (MRs), as in figure 1(a), and the system must generalize to novel sentences. Most semantic parsing models rely on an assumption of structural similarity between MR and NL. Since strict isomorphism is overly restrictive, this assumption is often relaxed by applying transformations. Several approaches assume a tree structure to the NL, MR, or both (Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; B¨orschinger et al., 2011), and often inFigure 1: (a) An example sentence/meaning pair, (b) a tree transformation based mapping, and (c) a tree transducer that performs the mapping. volve tree transformations either between two trees or a tree and a string. The tree transducer, a formalism from automata theory which has seen interest in machine translation (Yamada and Knight, 2001; Graehl et al., 2008) and has potential applications in many other areas, is well suited to formalizing such tree transformation based models. Yet, while many semantic parsing systems resemble the formalism, each w</context>
<context position="8231" citStr="Lu et al., 2008" startWordPosition="1398" endWordPosition="1401">Here, we will use joint distributions, which can be defined by ensuring that the weights of all rules with the same state on the lefthand side sum to one. In this case, it can be helpful to view the transducer as simultaneously generating both the input and output, rather than the usual view of mapping input trees into output strings. A joint distribution allows us to model with a single machine both the input and output languages, which is important during decoding when we want to infer the input given the output. 4 A generative model of semantic parsing Like the hybrid tree semantic parser (Lu et al., 2008) and the synchronous grammar based WASP (Wong and Mooney, 2006), our model simultaneously generates the input MR tree and the output NL string. The MR tree is built up according to the provided MR grammar, one grammar rule at a time. Coupled with the application of the MR rule, similar CFGlike productions are applied to the NL side, repeated until both the MR and NL are fully generated. In each step, we select an MR rule and then build the NL by first choosing a pattern with which to expand it and then filling out that pattern with words drawn from a unigram distribution. This kind of coupled </context>
<context position="16633" citStr="Lu et al., 2008" startWordPosition="2886" endWordPosition="2889">step an MR grammar rule is chosen to expand the MR and the corresponding portion of the NL is then generated. Symbols W stand for locations in the tree corresponding to substrings of the output and are removed in a post-processing step. (a) The (MR, NL) pair. (b) Step by step derivation. (c) The same derivation shown in tree form. (d) The underlying dependency structure of the derivation. 2004). Rather than argue for one or the other, we suggest that other approaches could also be interpreted in terms of general model classes, grounding them in a broader base of theory. The hybrid tree model (Lu et al., 2008) takes a transformative perspective that is in some ways more similar to our model. In fact, there is a oneto-one relationship between the multinomial parameters of the two models. However, they represent the MR and NL with a single tree and apply tree walking algorithms to extract them. Furthermore, they implement a custom training procedure for searching over the potential MR transformations. The tree transducer, on the other hand, naturally captures the same probabilistic dependencies while maintaining the separation between MR and NL, and further allows us to build upon a larger body of th</context>
<context position="24182" citStr="Lu et al., 2008" startWordPosition="4157" endWordPosition="4160">form 10 fold cross-validation on the 250 sentence sets. To judge correctness, we follow standard practice and submit each parse as a GeoQuery database query, and say the parse is correct only if the answer matches the gold standard. We report raw accuracy (the percentage of sentences with correct answers), as well as F1: the harmonic mean of precision (the proportion of correct answers out of sentences with a parse) and recall (the proportion of correct answers out of all sentences).5 We run three other state-of-the-art systems for comparison. WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al., 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al., 2010) as a nontree based top-performing system.6 The hybrid tree is notable as the only other system based on a generative model, and uni-hybrid, a version that uses a unigram distribution over words, is very similar to our own model. We also report the best performing version, re-hybrid, which incorporates a discriminative re-ranking step. We report transducer performance under three different training conditions: tsEM using EM, tsVBauto usi</context>
</contexts>
<marker>Lu, Ng, Lee, Zettlemoyer, 2008</marker>
<rawString>Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettlemoyer. A generative model for parsing natural language to meaning representations. In Proc. of the Conference on Empirical Methods in Natural Language Processing, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
<author>Jonathan Graehl</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
</authors>
<title>The power of extended top-down tree transducers.</title>
<date>2009</date>
<journal>SIAMJ. Comput.,</journal>
<volume>39</volume>
<contexts>
<context position="5877" citStr="Maletti et al., 2009" startWordPosition="943" endWordPosition="946">of G. Consequently, while we assume we have an RTG for the MR language, there is no loss of generality if the MR language is actually context free. 3 Weighted root-to-frontier, linear, non-deleting tree-to-string transducers Tree transducers (Rounds, 1970; Thatcher, 1970) are generalizations of finite state machines that operate on trees. Mirroring the branching nature of its input, the transducer may simultaneously transition to several successor states, assigning a separate state to each subtree. There are many classes of transducer with different formal properties (Knight and Greahl, 2005; Maletti et al., 2009). Figure 1(c) is an example of a root-to-frontier, linear, non-deleting tree-to-string transducer. It is defined using rules where the left hand side identifies a state of the transducer and a fragment of the input tree, and the right hand side describes a portion of the output string. Variables xi stand for entire sub-trees, and state-variable pairs qj.xi stand for strings produced by applying the transducer starting at state qj to subtree xi. Figure 1(b) illustrates an application of the transducer, taking the tree on the left as input and outputting the string on the right. Formally, a weig</context>
</contexts>
<marker>Maletti, Graehl, Hopkins, Knight, 2009</marker>
<rawString>Andreas Maletti, Jonathan Graehl, Mark Hopkins, and Kevin Knight. The power of extended top-down tree transducers. SIAMJ. Comput., 39:410–430, June 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon May</author>
<author>Kevin Knight</author>
</authors>
<title>Tiburon: A weighted tree automata toolkit.</title>
<date>2006</date>
<booktitle>In Proc. of the International Conference on Implementation and Application ofAutomata,</booktitle>
<contexts>
<context position="21998" citStr="May and Knight, 2006" startWordPosition="3799" endWordPosition="3802">pect to the parameters of q(Bt). q(xi) can be computed efficiently using inside-outside. Thus, we can perform an EM-like alternation between calculating α� and �B.4 It is also possible to estimate the hyperparameters α from data, a practice known as empirical Bayes, by optimizing T. We explore learning separate hyper-parameters αt for each Bt, using a fixed point update described by Minka (2000), where kt is the number of rules with parent state t: −1 1 t (a2F2 −1 ( aF ktα2 Bαt JJ aαt JJ 7 Training and decoding We implement our VB training algorithm inside the tree transducer package Tiburon (May and Knight, 2006), and experiment with both manually set and automatically estimated priors. For our manually set priors, we explore different hyper-parameter settings for three different priors, one for each of the main decision types: MR rule, NL pattern, and word generation. For the automatic priors, we estimate separate hyper-parameters for each multinomial (of which there are hundreds). As is standard, we initialize the word distributions using a variant of IBM model 1, and make use of NP lists (a manually created list of the constants in the MR language paired with the words that refer to them in the cor</context>
</contexts>
<marker>May, Knight, 2006</marker>
<rawString>Jon May and Kevin Knight. Tiburon: A weighted tree automata toolkit. In Proc. of the International Conference on Implementation and Application ofAutomata, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Minka</author>
</authors>
<title>Estimating a Dirichlet distribution.</title>
<date>2000</date>
<tech>Technical report, M.I.T.,</tech>
<contexts>
<context position="21775" citStr="Minka (2000)" startWordPosition="3758" endWordPosition="3759">r)c*(x) where ˆα(r) = α(r) + � Eq(xi)[cr(xi)] i � � � ˆθ(r) = exp �Ψ(ˆα(r)) − Ψ( ˆα(r)) � . r:s(r)=t = q(xi) = HrER 493 The parameters of q(Bt) are defined with respect to q(xi) and the parameters of q(xi) with respect to the parameters of q(Bt). q(xi) can be computed efficiently using inside-outside. Thus, we can perform an EM-like alternation between calculating α� and �B.4 It is also possible to estimate the hyperparameters α from data, a practice known as empirical Bayes, by optimizing T. We explore learning separate hyper-parameters αt for each Bt, using a fixed point update described by Minka (2000), where kt is the number of rules with parent state t: −1 1 t (a2F2 −1 ( aF ktα2 Bαt JJ aαt JJ 7 Training and decoding We implement our VB training algorithm inside the tree transducer package Tiburon (May and Knight, 2006), and experiment with both manually set and automatically estimated priors. For our manually set priors, we explore different hyper-parameter settings for three different priors, one for each of the main decision types: MR rule, NL pattern, and word generation. For the automatic priors, we estimate separate hyper-parameters for each multinomial (of which there are hundreds).</context>
</contexts>
<marker>Minka, 2000</marker>
<rawString>Tom Minka. Estimating a Dirichlet distribution. Technical report, M.I.T., 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Rounds</author>
</authors>
<title>Mappings and grammars on trees.</title>
<date>1970</date>
<booktitle>Mathematical Systems Theory 4,</booktitle>
<pages>257--287</pages>
<contexts>
<context position="5511" citStr="Rounds, 1970" startWordPosition="890" endWordPosition="891">right). We indicate states using all capital letters: NUM → population(PLACE). Intuitively, an RTG is a CFG where the yield of every parse is itself a tree. In fact, for any CFG G, it 1See Liang et al. (2011) for work in representing lambda calculus expressions with trees. is straightforward to produce a corresponding RTG that generates the set of parses of G. Consequently, while we assume we have an RTG for the MR language, there is no loss of generality if the MR language is actually context free. 3 Weighted root-to-frontier, linear, non-deleting tree-to-string transducers Tree transducers (Rounds, 1970; Thatcher, 1970) are generalizations of finite state machines that operate on trees. Mirroring the branching nature of its input, the transducer may simultaneously transition to several successor states, assigning a separate state to each subtree. There are many classes of transducer with different formal properties (Knight and Greahl, 2005; Maletti et al., 2009). Figure 1(c) is an example of a root-to-frontier, linear, non-deleting tree-to-string transducer. It is defined using rules where the left hand side identifies a state of the transducer and a fragment of the input tree, and the right</context>
</contexts>
<marker>Rounds, 1970</marker>
<rawString>W.C. Rounds. Mappings and grammars on trees. Mathematical Systems Theory 4, pages 257–287, 1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Synchronous grammars as tree transducers.</title>
<date>2004</date>
<booktitle>In Proc. of the Seventh International Workshop on Tree Adjoining Grammar and Related Formalisms,</booktitle>
<marker>Shieber, 2004</marker>
<rawString>Stuart M. Shieber. Synchronous grammars as tree transducers. In Proc. of the Seventh International Workshop on Tree Adjoining Grammar and Related Formalisms, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Thatcher</author>
</authors>
<title>Generalized sequential machine maps.</title>
<date>1970</date>
<journal>J. Comput. System Sci.</journal>
<volume>4</volume>
<pages>339--367</pages>
<contexts>
<context position="5528" citStr="Thatcher, 1970" startWordPosition="892" endWordPosition="893">icate states using all capital letters: NUM → population(PLACE). Intuitively, an RTG is a CFG where the yield of every parse is itself a tree. In fact, for any CFG G, it 1See Liang et al. (2011) for work in representing lambda calculus expressions with trees. is straightforward to produce a corresponding RTG that generates the set of parses of G. Consequently, while we assume we have an RTG for the MR language, there is no loss of generality if the MR language is actually context free. 3 Weighted root-to-frontier, linear, non-deleting tree-to-string transducers Tree transducers (Rounds, 1970; Thatcher, 1970) are generalizations of finite state machines that operate on trees. Mirroring the branching nature of its input, the transducer may simultaneously transition to several successor states, assigning a separate state to each subtree. There are many classes of transducer with different formal properties (Knight and Greahl, 2005; Maletti et al., 2009). Figure 1(c) is an example of a root-to-frontier, linear, non-deleting tree-to-string transducer. It is defined using rules where the left hand side identifies a state of the transducer and a fragment of the input tree, and the right hand side descri</context>
</contexts>
<marker>Thatcher, 1970</marker>
<rawString>J.W. Thatcher. Generalized sequential machine maps. J. Comput. System Sci. 4, pages 339–367, 1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of Human Language Technology Conference and the annual meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="1743" citStr="Wong and Mooney, 2006" startWordPosition="251" endWordPosition="254">the task of mapping natural language sentences to a formal representation of meaning. Typically, a system is trained on pairs of natural language sentences (NLs) and their meaning representation expressions (MRs), as in figure 1(a), and the system must generalize to novel sentences. Most semantic parsing models rely on an assumption of structural similarity between MR and NL. Since strict isomorphism is overly restrictive, this assumption is often relaxed by applying transformations. Several approaches assume a tree structure to the NL, MR, or both (Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; B¨orschinger et al., 2011), and often inFigure 1: (a) An example sentence/meaning pair, (b) a tree transformation based mapping, and (c) a tree transducer that performs the mapping. volve tree transformations either between two trees or a tree and a string. The tree transducer, a formalism from automata theory which has seen interest in machine translation (Yamada and Knight, 2001; Graehl et al., 2008) and has potential applications in many other areas, is well suited to formalizing such tree transformation based models. Yet, while many semantic parsing systems resemble the </context>
<context position="8294" citStr="Wong and Mooney, 2006" startWordPosition="1408" endWordPosition="1411">ed by ensuring that the weights of all rules with the same state on the lefthand side sum to one. In this case, it can be helpful to view the transducer as simultaneously generating both the input and output, rather than the usual view of mapping input trees into output strings. A joint distribution allows us to model with a single machine both the input and output languages, which is important during decoding when we want to infer the input given the output. 4 A generative model of semantic parsing Like the hybrid tree semantic parser (Lu et al., 2008) and the synchronous grammar based WASP (Wong and Mooney, 2006), our model simultaneously generates the input MR tree and the output NL string. The MR tree is built up according to the provided MR grammar, one grammar rule at a time. Coupled with the application of the MR rule, similar CFGlike productions are applied to the NL side, repeated until both the MR and NL are fully generated. In each step, we select an MR rule and then build the NL by first choosing a pattern with which to expand it and then filling out that pattern with words drawn from a unigram distribution. This kind of coupled generative process can be naturally formalized with tree transd</context>
<context position="15270" citStr="Wong and Mooney, 2006" startWordPosition="2651" endWordPosition="2654">r, which defines a joint distribution over both the MR and NL, the probability of a rule is conditioned on the parent state. Since each state encodes an MR rule, MR rule specific distributions are learned for both the words and their order. 5 Relation to existing models The tree transducer model can be viewed either as a generative procedure for building up two separate structures or as a transformative machine that takes one as input and produces another as output. Different semantic parsing approaches have taken one or the other view, and both can be captured in this single framework. WASP (Wong and Mooney, 2006) is an example of the former perspective, coupling the generation of the MR and NL with a synchronous grammar, a formalism closely related to tree transducers. The most significant difference from our approach is that they use machine translation techniques for automatically extracting rules from parallel corpora; similar techniques can be applied to tree transducers (Galley et al., 2004). In fact, synchronous grammars and tree transducers can be seen as instances of the same more general class of automata (Shieber, 3The addition of W symbols is a convenience; it is easier to design transducer</context>
<context position="22879" citStr="Wong and Mooney, 2006" startWordPosition="3944" endWordPosition="3947">n. For the automatic priors, we estimate separate hyper-parameters for each multinomial (of which there are hundreds). As is standard, we initialize the word distributions using a variant of IBM model 1, and make use of NP lists (a manually created list of the constants in the MR language paired with the words that refer to them in the corpus). At test time, since finding the most probable MR for a sentence involves summing over all possible derivations, we instead find the MR associated with the most probable derivation. 8 Experimental setup and evaluation We evaluate the system on GeoQuery (Wong and Mooney, 2006), a parallel corpus of 880 English questions and database queries about United States geography, 250 of which were translated into Spanish, Japanese, and Turkish. We present here additional translations of the full 880 sentences into 4Because of the resemblance to EM, this procedure has been called VBEM. Unlike EM, however, this procedure alternates between two estimation steps and has no maximization step. German, Greek, and Thai. For evaluation, following from Kwiatkowski et al. (2010), we reserve 280 sentences for test and train on the remaining 600. During development, we use cross-validat</context>
<context position="24144" citStr="Wong and Mooney, 2006" startWordPosition="4149" endWordPosition="4152">st, we run once on the remaining 280 and perform 10 fold cross-validation on the 250 sentence sets. To judge correctness, we follow standard practice and submit each parse as a GeoQuery database query, and say the parse is correct only if the answer matches the gold standard. We report raw accuracy (the percentage of sentences with correct answers), as well as F1: the harmonic mean of precision (the proportion of correct answers out of sentences with a parse) and recall (the proportion of correct answers out of all sentences).5 We run three other state-of-the-art systems for comparison. WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al., 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al., 2010) as a nontree based top-performing system.6 The hybrid tree is notable as the only other system based on a generative model, and uni-hybrid, a version that uses a unigram distribution over words, is very similar to our own model. We also report the best performing version, re-hybrid, which incorporates a discriminative re-ranking step. We report transducer performance under three different training c</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. Learning for semantic parsing with statistical machine translation. In Proc. of Human Language Technology Conference and the annual meeting of the North American Chapter of the Association for Computational Linguistics, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. of the annual meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="2145" citStr="Yamada and Knight, 2001" startWordPosition="318" endWordPosition="321">ism is overly restrictive, this assumption is often relaxed by applying transformations. Several approaches assume a tree structure to the NL, MR, or both (Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; B¨orschinger et al., 2011), and often inFigure 1: (a) An example sentence/meaning pair, (b) a tree transformation based mapping, and (c) a tree transducer that performs the mapping. volve tree transformations either between two trees or a tree and a string. The tree transducer, a formalism from automata theory which has seen interest in machine translation (Yamada and Knight, 2001; Graehl et al., 2008) and has potential applications in many other areas, is well suited to formalizing such tree transformation based models. Yet, while many semantic parsing systems resemble the formalism, each was proposed as an independent model requiring custom algorithms, leaving it unclear how developments in one line of inquiry relate to others. We argue for a unifying theory of tree transformation based semantic parsing by presenting a tree transducer model and drawing connections to other similar systems. We make a further contribution by bringing to tree transducers the benefits of</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. A syntax-based statistical translation model. In Proc. of the annual meeting of the Association for Computational Linguistics, 2001.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>