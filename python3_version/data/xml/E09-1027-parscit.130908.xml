<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.039689">
<title confidence="0.995804">
Cognitively Motivated Features for Readability Assessment
</title>
<author confidence="0.990104">
Lijun Feng Noémie Elhadad Matt Huenerfauth
</author>
<affiliation confidence="0.971337">
The City University of New York, Columbia University The City University of New York,
Graduate Center New York, NY, USA Queens College &amp; Graduate Center
</affiliation>
<address confidence="0.880183">
New York, NY, USA noemie@dbmi.columbia.edu New York, NY, USA
</address>
<email confidence="0.998916">
lijun7.feng@gmail.com matt@cs.qc.cuny.edu
</email>
<sectionHeader confidence="0.993885" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999666692307692">
We investigate linguistic features that correlate
with the readability of texts for adults with in-
tellectual disabilities (ID). Based on a corpus
of texts (including some experimentally meas-
ured for comprehension by adults with ID), we
analyze the significance of novel discourse-
level features related to the cognitive factors
underlying our users’ literacy challenges. We
develop and evaluate a tool for automatically
rating the readability of texts for these users.
Our experiments show that our discourse-
level, cognitively-motivated features improve
automatic readability assessment.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971924242424">
Assessing the degree of readability of a text has
been a field of research as early as the 1920&apos;s.
Dale and Chall define readability as “the sum
total (including all the interactions) of all those
elements within a given piece of printed material
that affect the success a group of readers have
with it. The success is the extent to which they
understand it, read it at optimal speed, and find it
interesting” (Dale and Chall, 1949). It has long
been acknowledged that readability is a function
of text characteristics, but also of the readers
themselves. The literacy skills of the readers,
their motivations, background knowledge, and
other internal characteristics play an important
role in determining whether a text is readable for
a particular group of people. In our work, we
investigate how to assess the readability of a text
for people with intellectual disabilities (ID).
Previous work in automatic readability as-
sessment has focused on generic features of a
text at the lexical and syntactic level. While such
features are essential, we argue that audience-
specific features that model the cognitive charac-
teristics of a user group can improve the accura-
cy of a readability assessment tool. The contri-
butions of this paper are: (1) we present a corpus
of texts with readability judgments from adults
with ID; (2) we propose a set of cognitively-
motivated features which operate at the discourse
level; (3) we evaluate the utility of these features
in predicting readability for adults with ID.
Our framework is to create tools that benefit
people with intellectual disabilities (ID), specifi-
cally those classified in the “mild level” of men-
tal retardation, IQ scores 55-70. About 3% of
the U.S. population has intelligence test scores of
70 or lower (U.S. Census Bureau, 2000). People
with ID face challenges in reading literacy. They
are better at decoding words (sounding them out)
than at comprehending their meaning (Drew &amp;
Hardman, 2004), and most read below their men-
tal age-level (Katims, 2000). Our research ad-
dresses two literacy impairments that distinguish
people with ID from other low-literacy adults:
limitations in (1) working memory and (2) dis-
course representation. People with ID have
problems remembering and inferring information
from text (Fowler, 1998). They have a slower
speed of semantic encoding and thus units are
lost from the working memory before they are
processed (Perfetti &amp; Lesgold, 1977; Hickson-
Bilsky, 1985). People with ID also have trouble
building cohesive representations of discourse
(Hickson-Bilsky, 1985). As less information is
integrated into the mental representation of the
current discourse, less is comprehended.
Adults with ID are limited in their choice of
reading material. Most texts that they can readi-
ly understand are targeted at the level of reada-
bility of children. However, the topics of these
texts often fail to match their interests since they
are meant for younger readers. Because of the
mismatch between their literacy and their inter-
ests, users may not read for pleasure and there-
fore miss valuable reading-skills practice time.
In a feasibility study we conducted with adults
</bodyText>
<note confidence="0.9229285">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 229–237,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998526">
229
</page>
<bodyText confidence="0.999944172413793">
with ID, we asked participants what they enjoyed
learning or reading about. The majority of our
subjects mentioned enjoying watching the news,
in particular local news. Many mentioned they
were interested in information that would be re-
levant to their daily lives. While for some ge-
nres, human editors can prepare texts for these
users, this is not practical for news sources that
are frequently updated and specific to a limited
geographic area (like local news). Our goal is to
create an automatic metric to predict the reada-
bility of local news articles for adults with ID.
Because of the low levels of written literacy
among our target users, we intend to focus on
comprehension of texts displayed on a computer
screen and read aloud by text-to-speech software;
although some users may depend on the text-to-
speech software, we use the term readability.
This paper is organized as follows. Section 2
presents related work on readability assessment.
Section 3 states our research hypotheses and de-
scribes our methodology. Section 4 focuses on
the data sets used in our experiments, while sec-
tion 5 describes the feature set we used for rea-
dability assessment along with a corpus-based
analysis of each feature. Section 6 describes a
readability assessment tool and reports on evalu-
ation. Section 7 discusses the implications of the
work and proposes direction for future work.
</bodyText>
<sectionHeader confidence="0.992004" genericHeader="related work">
2 Related Work on Readability Metrics
</sectionHeader>
<bodyText confidence="0.999939384615385">
Many readability metrics have been established
as a function of shallow features of texts, such as
the number of syllables per word and number of
words per sentence (Flesch, 1948; McLaughlin,
1969; Kincaid et al., 1975). These so-called tra-
ditional readability metrics are still used today in
many settings and domains, in part because they
are very easy to compute. Their results, however,
are not always representative of the complexity
of a text (Davison and Kantor, 1982). They can
easily misrepresent the complexity of technical
texts, or reveal themselves un-adapted to a set of
readers with particular reading difficulties. Other
formulas rely on lexical information; e.g., the
New Dale-Chall readability formula consults a
static, manually-built list of “easy” words to de-
termine whether a text contains unfamiliar words
(Chall and Dale, 1995).
Researchers in computational linguistics have
investigated the use of statistical language mod-
els (unigram in particular) to capture the range of
vocabulary from one grade level to another (Si
and Callan, 2001; Collins-Thompson and Callan,
2004). These metrics predicted readability better
than traditional formulas when tested against a
corpus of web pages. The use of syntactic fea-
tures was also investigated (Schwarm and Osten-
dorf, 2005; Heilman et al., 2007; Petersen and
Ostendorf, 2009) in the assessment of text reada-
bility for English as a Second Language readers.
While lexical features alone outperform syntactic
features in classifying texts according to their
reading levels, combining the lexical and syntac-
tic features yields the best results.
Several elegant metrics that focus solely on
the syntax of a text have also been developed.
The Yngve (1960) measure, for instance, focuses
on the depth of embedding of nodes in the parse
tree; others use the ratio of terminal to non-
terminal nodes in the parse tree of a sentence
(Miller and Chomsky, 1963; Frazier, 1985).
These metrics have been used to analyze the
writing of potential Alzheimer&apos;s patients to detect
mild cognitive impairments (Roark, Mitchell,
and Hollingshead, 2007), thereby indicating that
cognitively motivated features of text are valua-
ble when creating tools for specific populations.
Barzilay and Lapata (2008) presented early
work in investigating the use of discourse to dis-
tinguish abridged from original encyclopedia
articles. Their focus, however, is on style detec-
tion rather than readability assessment per se.
Coh-Metrix is a tool for automatically calculat-
ing text coherence based on features such as re-
petition of lexical items across sentences and
latent semantic analysis (McNamara et al.,
2006). The tool is based on comprehension data
collected from children and college students.
Our research differs from related work in that
we seek to produce an automatic readability me-
tric that is tailored to the literacy skills of adults
with ID. Because of the specific cognitive cha-
racteristics of these users, it is an open question
whether existing readability metrics and features
are useful for assessing readability for adults
with ID. Many of these earlier metrics have fo-
cused on the task of assigning texts to particular
elementary school grade levels. Traditional
grade levels may not be the ideal way to score
texts to indicate how readable they are for adults
with ID. Other related work has used models of
vocabulary (Collins-Thompson and Callan,
2004). Since we would like to use our tool to
give adults with ID access to local news stories,
we choose to keep our metric topic-independent.
Another difference between our approach and
previous approaches is that we have designed the
features used by our readability metric based on
</bodyText>
<page confidence="0.989222">
230
</page>
<bodyText confidence="0.999954888888889">
the cognitive aspects of our target users. For ex-
ample, these users are better at decoding words
than at comprehending text meaning (Drew &amp;
Hardman, 2004); so, shallow features like “sylla-
ble count per word” or unigram models of word
frequency (based on texts designed for children)
may be less important indicators of reading diffi-
culty. A critical challenge for our users is to
create a cohesive representation of discourse.
Due to their impairments in semantic encoding
speed, our users may have particular difficulty
with texts that place a significant burden on
working memory (items fall out of memory be-
fore they can be semantically encoded).
While we focus on readability of texts, other
projects have automatically generated texts for
people with aphasia (Carroll et al., 1999) or low
reading skills (Williams and Reiter, 2005).
</bodyText>
<sectionHeader confidence="0.949242" genericHeader="method">
3 Research Hypothesis and Methods
</sectionHeader>
<bodyText confidence="0.9999645">
We hypothesize that the complexity of a text for
adults with ID is related to the number of entities
referred to in the text overall. If a paragraph or a
text refers to too many entities at once, the reader
has to work harder at mapping each entity to a
semantic representation and deciding how each
entity is related to others. On the other hand,
when a text refers to few entities, less work is
required both for semantic encoding and for in-
tegrating the entities into a cohesive mental re-
presentation. Section 5.2 discusses some novel
discourse-level features (based on the “entity
density” of a text) that we believe will correlate
to comprehension by adults with ID.
To test our hypothesis, we used the following
methodology. We collected four corpora (as de-
scribed in Section 4). Three of them (Britannica,
LiteracyNet and WeeklyReader) have been ex-
amined in previous work on readability. The
fourth (LocalNews) is novel and results from a
user study we conducted with adults with ID.
We then analyzed how significant each feature is
on our Britannica and LiteracyNet corpora. Fi-
nally, we combined the significant features into a
linear regression model and experimented with
several feature combinations. We evaluated our
model on the WeeklyReader and LocalNews
corpora.
</bodyText>
<sectionHeader confidence="0.982109" genericHeader="method">
4 Corpora and Readability Judgments
</sectionHeader>
<bodyText confidence="0.999985294117647">
To study how certain linguistic features indicate
the readability of a text, we collected a corpus of
English text at different levels of readability. An
ideal corpus for our research would contain texts
that have been written specifically for our au-
dience of adults with intellectual disabilities – in
particular if such texts were paired with alternate
versions of each text written for a general au-
dience. We are not aware of such texts available
electronically, and so we have instead mostly
collected texts written for an audience of child-
ren. The texts come from online and commercial
sources, and some have been analyzed previous-
ly by text simplification researchers (Petersen
and Ostendorf, 2009). Our corpus also contains
some novel texts produced as part of an experi-
mental study involving adults with ID.
</bodyText>
<subsectionHeader confidence="0.389746333333333">
4.1 Paired and Graded Generic Corpora:
Britannica, LiteracyNet, and Weekly
Reader
</subsectionHeader>
<bodyText confidence="0.999991868421052">
The first section of our corpus (which we refer to
as Britannica) has 228 articles from the Encyclo-
pedia Britannica, originally collected by (Barzi-
lay and Elhadad, 2003). This consists of 114
articles in two forms: original articles written for
adults and corresponding articles rewritten for an
audience of children. While the texts are paired,
the content of the texts is not identical: some de-
tails are omitted from the child version, and addi-
tional background is sometimes inserted. The
resulting corpus is comparable in content.
Because we are particularly interested in mak-
ing local news articles accessible to adults with
ID, we collected a second paired corpus, which
we refer to as LiteracyNet, consisting of 115
news articles made available through (West-
ern/Pacific Literacy Network / LiteracyNet,
2008). The collection of local CNN stories is
available in an original and simplified/abridged
form (230 total news articles) designed for use in
literacy education.
The third corpus we collected (Weekly Reader)
was obtained from the Weekly Reader corpora-
tion (Weekly Reader, 2008). It contains articles
for students in elementary school. Each text is
labeled with its target grade level (grade 2: 174
articles, grade 3: 289 articles, grade 4: 428 ar-
ticles, grade 5: 542 articles). Overall, the corpus
has 1433 articles. (U.S. elementary school grades
2 to 5 generally are for children ages 7 to 10.)
The corpora discussed above are similar to
those used by Petersen and Ostendorf (2009).
While the focus of our research is adults with ID,
most of the texts discussed in this section have
been simplified or written by human authors to
be readable for children. Despite the texts being
intended for a different audience than the focus
of our research, we still believe these texts to be
</bodyText>
<page confidence="0.983373">
231
</page>
<bodyText confidence="0.929566333333333">
of value. It is rare to encounter electronically
available corpora in which an original and a sim-
plified version of a text is paired (as in the Bri-
tannica and LiteracyNet corpora) or texts labeled
as being at specific levels of readability (as in the
Weekly Reader corpus).
</bodyText>
<subsectionHeader confidence="0.989949">
4.2 Readability-Specific Corpus: LocalNews
</subsectionHeader>
<bodyText confidence="0.999885033898306">
The final section of our corpus contains local
news articles that are labeled with comprehen-
sion scores. These texts were produced for a fea-
sibility study involving adults with ID. Each text
was read by adults with ID, who then answered
comprehension questions to measure their under-
standing of the texts. Unlike the previous corpo-
ra, LocalNews is novel and was not investigated
by previous research in readability.
After obtaining university approval for our ex-
perimental protocol and informed consent
process, we conducted a study with 14 adults
with mild intellectual disabilities who participate
in daytime educational programs in the New
York area. Participants were presented with ten
articles collected from various local New York
based news websites. Some subjects saw the
original form of an article and others saw a sim-
plified form (edited by a human author); no sub-
ject saw both versions. The texts were presented
in random order using software that displayed
the text on the screen, read it aloud using text-to-
speech software, and highlighted each word as it
was read. Afterward, subjects were asked aloud
multiple-choice comprehension questions. We
defined the readability score of a story as the
percentage of correct answers averaged across
the subjects who read that particular story.
A human editor performed the text simplifica-
tion with the goal of making the text more reada-
ble for adults with mild ID. The editor made the
following types of changes to the original news
stories: breaking apart complex sentences, un-
embedding information in complex prepositional
phrases and reintegrating it as separate sentences,
replacing infrequent vocabulary items with more
common/colloquial equivalents, omitting sen-
tences and phrases from the story that mention
entities and phrases extraneous to the main
theme of the article. For instance, the original
sentence “They’re installing an induction loop
system in cabs that would allow passengers with
hearing aids to tune in specifically to the driver’s
voice.” was transformed into “They’re installing
a system in cabs. It would allow passengers with
hearing aids to listen to the driver’s voice.”
This corpus of local news articles that have
been human edited and scored for comprehen-
sion by adults with ID is small in size (20 news
articles), but we consider it a valuable resource.
Unlike the texts that have been simplified for
children (the rest of our corpus), these texts have
been rated for readability by actual adults with
ID. Furthermore, comprehension scores are de-
rived from actual reader comprehension tests,
rather than self-perceived comprehension. Be-
cause of the small size of this part of our corpus,
however, we primarily use it for evaluation pur-
poses (not for training the readability models).
</bodyText>
<sectionHeader confidence="0.940004" genericHeader="method">
5 Linguistic Features and Readability
</sectionHeader>
<bodyText confidence="0.9998823">
We now describe the set of features we investi-
gated for assessing readability automatically.
Table 1 contains a list of the features – including
a short code name for each feature which may be
used throughout this paper. We have begun by
implementing the simple features used by the
Flesh-Kincaid and FOG metrics: average number
of words per sentence, average number of syl-
lables per word, and percentage of words in the
document with 3+ syllables.
</bodyText>
<subsectionHeader confidence="0.992733">
5.1 Basic Features Used in Earlier Work
</subsectionHeader>
<bodyText confidence="0.9999725">
We have also implemented features inspired by
earlier research on readability. Petersen and Os-
tendorf (2009) included features calculated from
parsing the sentences in their corpus using the
Charniak parser (Charniak, 2000): average parse
tree height, average number of noun phrases per
sentence, average number of verb phrases per
sentence, and average number of SBARs per sen-
tence. We have implemented versions of most of
these parse-tree-related features for our project.
We also parse the sentences in our corpus using
Charniak’s parser and calculate the following
features listed in Table 1: aNP, aN, aVP, aAdj,
aSBr, aPP, nNP, nN, nVP, nAdj, nSBr, and nPP.
</bodyText>
<subsectionHeader confidence="0.998085">
5.2 Novel Cognitively-Motivated Features
</subsectionHeader>
<bodyText confidence="0.9999551">
Because of the special reading characteristics of
our target users, we have designed a set of cogni-
tively motivated features to predict readability of
texts for adults with ID. We have discussed how
working memory limits the semantic encoding of
new information by these users; so, our features
indicate the number of entities in a text that the
reader must keep in mind while reading each
sentence and throughout the entire document. It
is our hypothesis that this “entity density” of a
</bodyText>
<page confidence="0.957643">
232
</page>
<figureCaption confidence="0.621824384615385">
Code Feature
aWPS average number of words per sentence
aSPW average number of syllables per word
%3+S % of words in document with 3+ syllables
aNP avg. num. NPs per sentence
aN avg. num. common+proper nouns per sentence
aVP avg. num. VPs per sentence
aAdj avg. num. Adjectives per sentence
aSBr avg. num. SBARs per sentence
aPP avg. num. prepositional phrases per sentence
nNP total number of NPs per sentence
nN total num. of common+proper nouns in document
nVP total number of VPs in the document
nAdj total number of Adjectives in the document
nSBr total number of SBARs in the document
nPP total num. of prepositional phrases in document
nEM number of entity mentions in document
nUE number of unique entities in document
aEM avg. num. entity mentions per sentence
aUE avg. num. unique entities per sentence
nLC number of lexical chains in document
nLC2 num. lex. chains, span &gt; half document length
aLCL average lexical chain length
aLCS average lexical chain span
aLCw avg. num. lexical chains active at each word
aLCn avg. num. lexical chains active at each NP
</figureCaption>
<tableCaption confidence="0.985857">
Table 1: Implemented Features
</tableCaption>
<bodyText confidence="0.999869297297298">
text plays an important role in the difficulty of
that text for readers with intellectual disabilities.
The first set of features incorporates the Ling-
Pipe named entity detection software (Alias-i,
2008), which detects three types of entities: per-
son, location, and organization. We also use the
part-of-speech tagger in LingPipe to identify the
common nouns in the document, and we find the
union of the common nouns and the named entity
noun phrases in the text. The union of these two
sets is our definition of “entity” for this set of
features. We count both the total number of
“entity mentions” in a text (each token appear-
ance of an entity) and the total number of unique
entities (exact-string-match duplicates only
counted once). Table 1 lists these features: nEM,
nUE, aEM, and aUE. We count the totals per
document to capture how many entities the read-
er must keep track of while reading the docu-
ment. We also expect sentences with more enti-
ties to be more difficult for our users to semanti-
cally encode due to working memory limitations;
so, we also count the averages per sentence to
capture how many entities the reader must keep
in mind to understand each sentence.
To measure the working memory burden of a
text, we’d like to capture the number of dis-
course entities that a reader must keep in mind.
However, the “unique entities” identified by the
named entity recognition tool may not be a per-
fect representation of this – several unique enti-
ties may actually refer to the same real-world
entity under discussion. To better model how
multiple noun phrases in a text refer to the same
entity or concept, we have also built features us-
ing lexical chains (Galley and McKeown, 2003).
Lexical chains link nouns in a document con-
nected by relations like synonymy or hyponomy;
chains can indicate concepts that recur through-
out a text. A lexical chain has both a length
(number of noun phrases it includes) and a span
(number of words in the document between the
first noun phrase at the beginning of the chain
and the last noun phrase that is part of the chain).
We calculate the number of lexical chains in the
document (nLC) and those with a span greater
than half the document length (nLC2). We be-
lieve these features may indicate the number of
entities/concepts that a reader must keep in mind
during a document and the subset of very impor-
tant entities/concepts that are the main topic of
the document. The average length and average
span of the lexical chains in a document (aLCL
and aLCS) may also indicate how many of the
chains in the document are short-lived, which
may mean that they are ancillary enti-
ties/concepts, not the main topics.
The final two features in Table 1 (aLCw and
aLCe) use the concept of an “active” chain. At a
particular location in a text, we define a lexical
chain to be “active” if the span (between the first
and last noun in the lexical chain) includes the
current location. We expect these features may
indicate the total number of concepts that the
reader needs to keep in mind during a specific
moment in time when reading a text. Measuring
the average number of concepts that the reader of
a text must keep in mind may suggest the work-
ing memory burden of the text over time. We
were unsure if individual words or individual
noun-phrases in the document should be used as
the basic unit of “time” for the purpose of aver-
aging the number of active lexical chains; so, we
included both features.
</bodyText>
<subsectionHeader confidence="0.998319">
5.3 Testing the Significance of Features
</subsectionHeader>
<bodyText confidence="0.9994925">
To select which features to include in our auto-
matic readability assessment tool (in Section 6),
</bodyText>
<page confidence="0.996357">
233
</page>
<bodyText confidence="0.999972264705883">
we analyzed the documents in our paired corpora
(Britannica and LiteracyNet). Because they con-
tain a complex and a simplified version of each
article, we can examine differences in readability
while holding the topic and genre constant. We
calculated the value of each feature for each doc-
ument, and we used a paired t-test to determine if
the difference between the complex and simple
documents was significant for that corpus.
Table 2 contains the results of this feature se-
lection process; the columns in the table indicate
the values for the following corpora: Britannica
complex, Britannica simple, LiteracyNet com-
plex, and LiteracyNet simple. An asterisk ap-
pears in the “Sig” column if the difference be-
tween the feature values for the complex vs.
simple documents is statistically significant for
that corpus (significance level: p&lt;0.00001).
The only two features which did not show a
significant difference (p&gt;0.01) between the com-
plex and simple versions of the articles were:
average lexical chain length (aLCL) and number
of lexical chains with span greater than half the
document length (nLC2). The lack of signific-
ance for aLCL may be explained by the vast ma-
jority of lexical chains containing few members;
complex articles contained more of these chains
– but their chains did not contain more members.
In the case of nLC2, over 80% of the articles in
each category contained no lexical chains whose
span was greater than half the document length.
The rarity of a lexical chain spanning the majori-
ty of a document may have led to there being no
significant difference between complex/simple.
</bodyText>
<sectionHeader confidence="0.995652" genericHeader="method">
6 A Readability Assessment Tool
</sectionHeader>
<bodyText confidence="0.999934666666667">
After testing the significance of features using
paired corpora, we used linear regression and our
graded corpus (Weekly Reader) to build a reada-
bility assessment tool. To evaluate the tool’s
usefulness for adults with ID, we test the correla-
tion of its scores with the LocalNews corpus.
</bodyText>
<subsectionHeader confidence="0.999564">
6.1 Versions of Our Model
</subsectionHeader>
<bodyText confidence="0.9998454">
We began our evaluation by implementing three
versions of our automatic readability assessment
tool. The first version uses only those features
studied by previous researchers (aWPS, aSPW,
%3+S, aNP, aN, aVP, aAdj, aSBr, aPP, nNP, nN,
nVP, nAdj, nSBr, nPP). The second version uses
only our novel cognitively motivated features
(section 5.2). The third version uses the union of
both sets of features. By building three versions
of the tool, we can compare the relative impact
</bodyText>
<table confidence="0.999254592592593">
Feature Brit. Brit. Sig LitN. LitN. Sig
Com. Simp. Com. Simp.
aWPS 20.13 14.37 * 17.97 12.95 *
aSPW 1.708 1.655 * 1.501 1.455 *
%3+S 0.196 0.177 * 0.12 0.101 *
aNP 8.363 6.018 * 6.519 4.691 *
aN 7.024 5.215 * 5.319 3.929 *
aVP 2.334 1.868 * 3.806 2.964 *
aAdj 1.95 1.281 * 1.214 0.876 *
aSBr 0.266 0.205 * 0.793 0.523 *
aPP 2.858 1.936 * 1.791 1.22 *
nNP 798 219.2 * 150.2 102.9 *
nN 668.4 190.4 * 121.4 85.75 *
nVP 242.8 69.19 * 88.2 65.52 *
nAdj 205 47.32 * 28.11 19.04 *
nSBr 31.33 7.623 * 18.16 11.43 *
nPP 284.7 70.75 * 41.06 26.79 *
nEM 624.2 172.7 * 115.2 82.83 *
nUE 355 117 * 81.56 54.94 *
aEM 6.441 4.745 * 5.035 3.789 *
aUE 4.579 3.305 * 3.581 2.55 *
nLC 59.21 17.57 * 12.43 8.617 *
nLC2 0.175 0.211 0.191 0.226
aLCL 3.009 3.022 2.817 2.847
aLCS 357 246.1 * 271.9 202.9 *
aLCw 1.803 1.358 * 1.407 1.091 *
aLCn 1.852 1.42 * 1.53 1.201 *
</table>
<tableCaption confidence="0.997369">
Table 2: Feature Values of Paired Corpora
</tableCaption>
<bodyText confidence="0.9999302">
of our novel cognitively-motivated features. For
all versions, we have only included those fea-
tures that showed a significant difference be-
tween the complex and simple articles in our
paired corpora (as discussed in section 5.3).
</bodyText>
<subsectionHeader confidence="0.999994">
6.2 Learning Technique and Training Data
</subsectionHeader>
<bodyText confidence="0.9998498125">
Early work on automatic readability analysis
framed the problem as a classification task:
creating multiple classifiers for labeling a text as
being one of several elementary school grade
levels (Collins-Thompson and Callan, 2004).
Because we are focusing on a unique user group
with special reading challenges, we do not know
a priori what level of text difficulty is ideal for
our users. We would not know where to draw
category boundaries for classification. We also
prefer that our assessment tool assign numerical
difficulty scores to texts. Thus, after creating
this tool, we can conduct further reading com-
prehension experiments with adults with ID to
determine what threshold (for readability scores
assigned by our tool) is appropriate for our users.
</bodyText>
<page confidence="0.994452">
234
</page>
<bodyText confidence="0.999929769230769">
To select features for our model, we used our
paired corpora (Britannica and LiteracyNet) to
measure the significance of each feature. Now
that we are training a model, we make use of our
graded corpus (articles from Weekly Reader).
This corpus contains articles that have each been
labeled with an elementary school grade level for
which it was written. We divide this corpus –
using 80% of articles as training data and 20% as
testing data. We model the grade level of the
articles using linear regression; our model is im-
plemented using R (R Development Core Team,
2008).
</bodyText>
<subsectionHeader confidence="0.999617">
6.3 Evaluation of Our Readability Tool
</subsectionHeader>
<bodyText confidence="0.999584692307692">
We conducted two rounds of training and evalua-
tion of our three regression models. We also
compare our models to a baseline readability as-
sessment tool: the popular Flesh-Kincaid Grade
Level index (Kincaid et al., 1975).
In the first round of evaluation, we trained and
tested our regression models on the Weekly
Reader corpus. This round of evaluation helped
to determine whether our feature-set and regres-
sion technique were successfully modeling those
aspects of the texts that were relevant to their
grade level. Our results from this round of eval-
uation are presented in the form of average error
scores. (For each article in the Weekly Reader
testing data, we calculate the difference between
the output score of the model and the correct
grade-level for that article.) Table 3 presents the
average error results for the baseline system and
our three regression models. We can see that the
model trained on the shallow and parse-related
features out-performs the model trained only on
our novel features; however, the best model
overall is the one is trained on all of the features.
This model predicts the grade level of Weekly
Reader articles to within roughly 0.565 grade
levels on average.
</bodyText>
<table confidence="0.999514">
Readability Model (or baseline) Average Error
Baseline: Flesh-Kincaid Index 2.569
Basic Features Only 0.6032
Cognitively Motivated Features Only 0.6110
Basic + Cognitively-Motiv. Features 0.5650
</table>
<tableCaption confidence="0.999925">
Table 3: Predicting Grade Level of Weekly Reader
</tableCaption>
<bodyText confidence="0.999831148148148">
In our second round of evaluation, we trained
the regression model on the Weekly Reader cor-
pus, but we tested it against the LocalNews cor-
pus. We measured the correlation between our
regression models’ output and the comprehen-
sion scores of adults with ID on each text. For
this reason, we do not calculate the “average er-
ror”; instead, we simply measure the correlation
between the models’ output and the comprehen-
sion scores. (We expect negative correlations
because comprehension scores should increase as
the predicted grade level of the text goes down.)
Table 4 presents the correlations for our three
models and the baseline system in the form of
Pearson’s R-values. We see a surprising result:
the model trained only on the cognitively-
motivated features is more tightly correlated with
the comprehension scores of the adults with ID.
While the model trained on all features was bet-
ter at assigning grade levels to Weekly Reader
articles, when we tested it on the local news ar-
ticles from our user-study, it was not the top-
performing model. This result suggests that the
shallow and parse-related features of texts de-
signed for children (the Weekly Reader articles,
our training data) are not the best predictors of
text readability for adults with ID.
</bodyText>
<table confidence="0.9994198">
Readability Model (or baseline) Pearson’s R
Baseline: Flesh-Kincaid Index -0.270
Basic Features Only -0.283
Cognitively Motivated Features Only -0.352
Basic + Cognitively-Motiv. Features -0.342
</table>
<tableCaption confidence="0.999504">
Table 4: Correlation to User-Study Comprehension
</tableCaption>
<sectionHeader confidence="0.998587" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999971666666667">
Based on the cognitive and literacy skills of
adults with ID, we designed novel features that
were useful in assessing the readability of texts
for these users. The results of our study have
supported our hypothesis that the complexity of a
text for adults with ID is related to the number of
entities referred to in the text. These “entity den-
sity” features enabled us to build models that
were better at predicting text readability for
adults with intellectual disabilities.
This study has also demonstrated the value of
collecting readability judgments from target us-
ers when designing a readability assessment tool.
The results in Table 4 suggest that models
trained on corpora containing texts designed for
children may not always lead to accurate models
of the readability of texts for other groups of
low-literacy users. Using features targeting spe-
cific aspects of literacy impairment have allowed
us to make better use of children’s texts when
designing a model for adults with ID.
</bodyText>
<subsectionHeader confidence="0.763221">
7.1 Future Work
</subsectionHeader>
<bodyText confidence="0.999936">
In order to study more features and models of
readability, we will require more testing data for
tracking progress of our readability regression
</bodyText>
<page confidence="0.992872">
235
</page>
<bodyText confidence="0.999964209302326">
models. Our current study has illustrated the
usefulness of texts that have been evaluated by
adults with ID, and we therefore plan to increase
the size of this corpus in future work. In addi-
tion to using this corpus for evaluation, we may
want to use it to train our regression models. For
this study, we trained on Weekly Reader text
labeled with elementary school grade levels, but
this is not ideal. Texts designed for children may
differ from those that are best for adults with ID,
and “grade levels” may not be the best way to
rank/rate text readability for these users. While
our user-study comprehension-test corpus is cur-
rently too small for training, we intend to grow
the size of this corpus in future work.
We also plan on refining our cognitively moti-
vated features for measuring the difficulty of a
text for our users. Currently, we use lexical
chain software to link noun phrases in a docu-
ment that may refer to similar entities/concepts.
In future work, we plan to use co-reference reso-
lution software to model how multiple “entity
mentions” may refer to a single discourse entity.
For comparison purposes, we plan to imple-
ment other features that have been used in earlier
readability assessment systems. For example,
Petersen and Ostendorf (2009) created lists of the
most common words from the Weekly Reader
articles, and they used the percentage of words in
a document not on this list as a feature.
The overall goal of our research is to develop
a software system that can automatically simplify
the reading level of local news articles and
present them in an accessible way to adults with
ID. Our automatic readability assessment tool
will be a component in this future text simplifica-
tion system. We have therefore preferred to in-
clude features in our tool that focus on aspects of
the text that can be modified during a simplifica-
tion process. In future work, we will study how
to use our readability assessment tool to guide
how a text revision system decides to modify a
text to increase its readability for these users.
</bodyText>
<subsectionHeader confidence="0.998167">
7.2 Summary of Contributions
</subsectionHeader>
<bodyText confidence="0.999988393939394">
We have contributed to research on automatic
readability assessment by designing a new me-
thod for assessing the complexity of a text at the
level of discourse. Our novel “entity density”
features are based on named entity and lexical
chain software, and they are inspired by the cog-
nitive underpinnings of the literacy challenges of
adults with ID – specifically, the role of slow
semantic encoding and working memory limita-
tions. We have demonstrated the usefulness of
these novel features in modeling the grade level
of elementary school texts and in correlating to
readability judgments from adults with ID.
Another contribution of our work is the collec-
tion of an initial corpus of texts of local news
stories that have been manually simplified by a
human editor. Both the original and the simpli-
fied versions of these stories have been evaluated
by adults with intellectual disabilities. We have
used these comprehension scores in the evalua-
tion phase of this study, and we have suggested
how constructing a larger corpus of such articles
could be useful for training readability tools.
More broadly, this project has demonstrated
how focusing on a specific user population, ana-
lyzing their cognitive skills, and involving them
in a user-study has led to new insights in model-
ing text readability. As Dale and Chall’s defini-
tion (1949) originally argued, characteristics of
the reader are central to the issue of readability.
We believe our user-focused research paradigm
may be used to drive further advances in reada-
bility assessment for other groups of users.
</bodyText>
<sectionHeader confidence="0.994726" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99998225">
We thank the Weekly Reader Corporation for
making its corpus available for our research. We
are grateful to Martin Jansche for his assistance
with the statistical data analysis and regression.
</bodyText>
<sectionHeader confidence="0.998527" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999416714285714">
Alias-i. 2008. LingPipe 3.6.0. http://alias-
i.com/lingpipe (accessed October 1, 2008)
Barzilay, R., Elhadad, N., 2003. Sentence alignment
for monolingual comparable corpora. In Proc
EMNLP, pp. 25-32.
Barzilay R., Lapata, M., 2008. Modeling Local Cohe-
rence: An Entity-based Approach. Computational
Linguistics. 34(1):1-34.
Carroll, J., Minnen, G., Pearce, D., Canning, Y., Dev-
lin, S., Tait, J. 1999. Simplifying text for language-
impaired readers. In Proc. EACL Poster, p. 269.
Chall, J.S., Dale, E., 1995. Readability Revisited: The
New Dale-Chall Readability Formula. Brookline
Books, Cambridge, MA.
Charniak, E. 2000. A maximum-entropy-inspired
parser. In Proc. NAACL, pp. 132-139.
Collins-Thompson, K., and Callan, J. 2004. A lan-
guage modeling approach to predicting reading dif-
ficulty. In Proc. NAACL, pp. 193-200.
Dale, E. and J. S. Chall. 1949. The concept of reada-
bility. Elementary English 26(23).
</reference>
<page confidence="0.97705">
236
</page>
<reference confidence="0.999670378947368">
Davison, A., and Kantor, R. 1982. On the failure of
readability formulas to define readable texts: A case
study from adaptations. Reading Research Quar-
terly, 17(2):187-209.
Drew, C.J., and Hardman, M.L. 2004. Mental retar-
dation: A lifespan approach to people with intellec-
tual disabilities (8th ed.). Columbus, OH: Merrill.
Flesch, R. 1948. A new readability yardstick. Jour-
nal of Applied Psychology, 32:221-233.
Fowler, A.E. 1998. Language in mental retardation.
In Burack, Hodapp, and Zigler (Eds.), Handbook of
Mental Retardation and Development. Cambridge,
UK: Cambridge Univ. Press, pp. 290-333.
Frazier, L. 1985. Natural Language Parsing: Psy-
chological, Computational, and Theoretical Pers-
pectives, chapter Syntactic complexity, pp. 129-
189. Cambridge University Press.
Galley, M., McKeown, K. 2003. Improving Word
Sense Disambiguation in Lexical Chaining. In
Proc. IJCAI, pp. 1486-1488.
Gunning, R. 1952. The Technique of Clear Writing.
McGraw-Hill.
Heilman, M., Collins-Thompson, K., Callan, J., and
Eskenazi, M. 2007. Combining lexical and gram-
matical features to improve readability measures for
first and second language texts. In Proc. NAACL,
pp. 460-467.
Hickson-Bilsky, L. 1985. Comprehension and men-
tal retardation. International Review of Research in
Mental Retardation, 13: 215-246.
Katims, D.S. 2000. Literacy instruction for people
with mental retardation: Historical highlights and
contemporary analysis. Education and Training in
Mental Retardation and Developmental Disabili-
ties, 35(1): 3-15.
Kincaid, J. P., Fishburne, R. P., Rogers, R. L., and
Chissom, B. S. 1975. Derivation of new readabili-
ty formulas for Navy enlisted personnel, Research
Branch Report 8-75, Millington, TN.
Kincaid, J., Fishburne, R., Rodgers, R., and Chisson,
B. 1975. Derivation of new readability formulas
for navy enlisted personnel. Technical report, Re-
search Branch Report 8-75, U.S. Naval Air Station.
McLaughlin, G.H. 1969. SMOG grading - a new
readability formula. Journal of Reading,
12(8):639-646.
McNamara, D.S., Ozuru, Y., Graesser, A.C., &amp; Lou-
werse, M. (2006) Validating Coh-Metrix., In Proc.
Conference of the Cognitive Science Society, pp.
573.
Miller, G., and Chomsky, N. 1963. Handbook of
Mathematical Psychology, chapter Finatary models
of language users, pp. 419-491. Wiley.
Perfetti, C., and Lesgold, A. 1977. Cognitive
Processes in Comprehension, chapter Discourse
Comprehension and sources of individual differ-
ences. Erlbaum.
Petersen, S.E., Ostendorf, M. 2009. A machine learn-
ing approach to reading level assessment. Computer
Speech and Language, 23: 89-106.
R Development Core Team. 2008. R: A Language
and Environment for Statistical Computing. Vienna,
Austria: R Foundation for Statistical Computing.
http://www.R-project.org
Roark, B., Mitchell, M., and Hollingshead, K. 2007.
Syntactic complexity measures for detecting mild
cognitive impairment. In Proc. ACL Workshop on
Biological, Translational, and Clinical Language
Processing (BioNLP&apos;07), pp. 1-8.
Schwarm, S., and Ostendorf, M. 2005. Reading level
assessment using support vector machines and sta-
tistical language models. In Proc. ACL, pp. 523-
530.
Si, L., and Callan, J. 2001. A statistical model for
scientific readability. In Proc. CIKM, pp. 574-576.
Stenner, A.J. 1996. Measuring reading comprehension
with the Lexile framework. 4th North American
Conference on Adolescent/Adult Literacy.
U.S. Census Bureau. 2000. Projections of the total
resident population by five-year age groups and
sex, with special age categories: Middle series
2025-2045. Washington: U.S. Census Bureau, Po-
pulations Projections Program, Population Division.
Weekly Reader, 2008. http://www.weeklyreader.com
(Accessed Oct., 2008).
Western/Pacific Literacy Network / Literacyworks,
2008. CNN SF learning resources.
http://literacynet.org/cnnsf/ (Accessed Oct., 2008).
Williams, S., Reiter, E. 2005. Generating readable
texts for readers with low basic skills. In Proc. Eu-
ropean Workshop on Natural Language Genera-
tion, pp. 140-147.
Yngve, V. 1960. A model and a hypothesis for lan-
guage structure. American Philosophical Society,
104: 446-466.
</reference>
<page confidence="0.997406">
237
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.934362">
<title confidence="0.999366">Cognitively Motivated Features for Readability Assessment</title>
<author confidence="0.998167">Lijun Feng Noémie Elhadad Matt Huenerfauth</author>
<affiliation confidence="0.974227">The City University of New York, Columbia University The City University of New York, Graduate Center New York, NY, USA Queens College &amp; Graduate Center</affiliation>
<address confidence="0.999401">York, NY, USA York, NY, USA</address>
<email confidence="0.999343">lijun7.feng@gmail.commatt@cs.qc.cuny.edu</email>
<abstract confidence="0.999027357142857">We investigate linguistic features that correlate with the readability of texts for adults with intellectual disabilities (ID). Based on a corpus of texts (including some experimentally measured for comprehension by adults with ID), we analyze the significance of novel discourselevel features related to the cognitive factors underlying our users’ literacy challenges. We develop and evaluate a tool for automatically rating the readability of texts for these users. Our experiments show that our discourselevel, cognitively-motivated features improve automatic readability assessment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alias-i</author>
</authors>
<title>LingPipe 3.6.0. http://aliasi.com/lingpipe (accessed</title>
<date>2008</date>
<contexts>
<context position="20411" citStr="Alias-i, 2008" startWordPosition="3266" endWordPosition="3267"> number of unique entities in document aEM avg. num. entity mentions per sentence aUE avg. num. unique entities per sentence nLC number of lexical chains in document nLC2 num. lex. chains, span &gt; half document length aLCL average lexical chain length aLCS average lexical chain span aLCw avg. num. lexical chains active at each word aLCn avg. num. lexical chains active at each NP Table 1: Implemented Features text plays an important role in the difficulty of that text for readers with intellectual disabilities. The first set of features incorporates the LingPipe named entity detection software (Alias-i, 2008), which detects three types of entities: person, location, and organization. We also use the part-of-speech tagger in LingPipe to identify the common nouns in the document, and we find the union of the common nouns and the named entity noun phrases in the text. The union of these two sets is our definition of “entity” for this set of features. We count both the total number of “entity mentions” in a text (each token appearance of an entity) and the total number of unique entities (exact-string-match duplicates only counted once). Table 1 lists these features: nEM, nUE, aEM, and aUE. We count t</context>
</contexts>
<marker>Alias-i, 2008</marker>
<rawString>Alias-i. 2008. LingPipe 3.6.0. http://aliasi.com/lingpipe (accessed October 1, 2008)</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>N Elhadad</author>
</authors>
<title>Sentence alignment for monolingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proc EMNLP,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="12665" citStr="Barzilay and Elhadad, 2003" startWordPosition="2008" endWordPosition="2012">available electronically, and so we have instead mostly collected texts written for an audience of children. The texts come from online and commercial sources, and some have been analyzed previously by text simplification researchers (Petersen and Ostendorf, 2009). Our corpus also contains some novel texts produced as part of an experimental study involving adults with ID. 4.1 Paired and Graded Generic Corpora: Britannica, LiteracyNet, and Weekly Reader The first section of our corpus (which we refer to as Britannica) has 228 articles from the Encyclopedia Britannica, originally collected by (Barzilay and Elhadad, 2003). This consists of 114 articles in two forms: original articles written for adults and corresponding articles rewritten for an audience of children. While the texts are paired, the content of the texts is not identical: some details are omitted from the child version, and additional background is sometimes inserted. The resulting corpus is comparable in content. Because we are particularly interested in making local news articles accessible to adults with ID, we collected a second paired corpus, which we refer to as LiteracyNet, consisting of 115 news articles made available through (Western/P</context>
</contexts>
<marker>Barzilay, Elhadad, 2003</marker>
<rawString>Barzilay, R., Elhadad, N., 2003. Sentence alignment for monolingual comparable corpora. In Proc EMNLP, pp. 25-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Lapata</author>
</authors>
<title>Modeling Local Coherence: An Entity-based Approach. Computational Linguistics.</title>
<date>2008</date>
<pages>34--1</pages>
<contexts>
<context position="7966" citStr="Barzilay and Lapata (2008)" startWordPosition="1246" endWordPosition="1249">gant metrics that focus solely on the syntax of a text have also been developed. The Yngve (1960) measure, for instance, focuses on the depth of embedding of nodes in the parse tree; others use the ratio of terminal to nonterminal nodes in the parse tree of a sentence (Miller and Chomsky, 1963; Frazier, 1985). These metrics have been used to analyze the writing of potential Alzheimer&apos;s patients to detect mild cognitive impairments (Roark, Mitchell, and Hollingshead, 2007), thereby indicating that cognitively motivated features of text are valuable when creating tools for specific populations. Barzilay and Lapata (2008) presented early work in investigating the use of discourse to distinguish abridged from original encyclopedia articles. Their focus, however, is on style detection rather than readability assessment per se. Coh-Metrix is a tool for automatically calculating text coherence based on features such as repetition of lexical items across sentences and latent semantic analysis (McNamara et al., 2006). The tool is based on comprehension data collected from children and college students. Our research differs from related work in that we seek to produce an automatic readability metric that is tailored </context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Barzilay R., Lapata, M., 2008. Modeling Local Coherence: An Entity-based Approach. Computational Linguistics. 34(1):1-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>G Minnen</author>
<author>D Pearce</author>
<author>Y Canning</author>
<author>S Devlin</author>
<author>J Tait</author>
</authors>
<title>Simplifying text for languageimpaired readers.</title>
<date>1999</date>
<booktitle>In Proc. EACL Poster,</booktitle>
<pages>269</pages>
<contexts>
<context position="10198" citStr="Carroll et al., 1999" startWordPosition="1607" endWordPosition="1610">ow features like “syllable count per word” or unigram models of word frequency (based on texts designed for children) may be less important indicators of reading difficulty. A critical challenge for our users is to create a cohesive representation of discourse. Due to their impairments in semantic encoding speed, our users may have particular difficulty with texts that place a significant burden on working memory (items fall out of memory before they can be semantically encoded). While we focus on readability of texts, other projects have automatically generated texts for people with aphasia (Carroll et al., 1999) or low reading skills (Williams and Reiter, 2005). 3 Research Hypothesis and Methods We hypothesize that the complexity of a text for adults with ID is related to the number of entities referred to in the text overall. If a paragraph or a text refers to too many entities at once, the reader has to work harder at mapping each entity to a semantic representation and deciding how each entity is related to others. On the other hand, when a text refers to few entities, less work is required both for semantic encoding and for integrating the entities into a cohesive mental representation. Section 5</context>
</contexts>
<marker>Carroll, Minnen, Pearce, Canning, Devlin, Tait, 1999</marker>
<rawString>Carroll, J., Minnen, G., Pearce, D., Canning, Y., Devlin, S., Tait, J. 1999. Simplifying text for languageimpaired readers. In Proc. EACL Poster, p. 269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Chall</author>
<author>E Dale</author>
</authors>
<title>Readability Revisited: The New Dale-Chall Readability Formula. Brookline Books,</title>
<date>1995</date>
<location>Cambridge, MA.</location>
<contexts>
<context position="6569" citStr="Chall and Dale, 1995" startWordPosition="1031" endWordPosition="1034">called traditional readability metrics are still used today in many settings and domains, in part because they are very easy to compute. Their results, however, are not always representative of the complexity of a text (Davison and Kantor, 1982). They can easily misrepresent the complexity of technical texts, or reveal themselves un-adapted to a set of readers with particular reading difficulties. Other formulas rely on lexical information; e.g., the New Dale-Chall readability formula consults a static, manually-built list of “easy” words to determine whether a text contains unfamiliar words (Chall and Dale, 1995). Researchers in computational linguistics have investigated the use of statistical language models (unigram in particular) to capture the range of vocabulary from one grade level to another (Si and Callan, 2001; Collins-Thompson and Callan, 2004). These metrics predicted readability better than traditional formulas when tested against a corpus of web pages. The use of syntactic features was also investigated (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009) in the assessment of text readability for English as a Second Language readers. While lexical features al</context>
</contexts>
<marker>Chall, Dale, 1995</marker>
<rawString>Chall, J.S., Dale, E., 1995. Readability Revisited: The New Dale-Chall Readability Formula. Brookline Books, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. NAACL,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="18137" citStr="Charniak, 2000" startWordPosition="2890" endWordPosition="2891">e 1 contains a list of the features – including a short code name for each feature which may be used throughout this paper. We have begun by implementing the simple features used by the Flesh-Kincaid and FOG metrics: average number of words per sentence, average number of syllables per word, and percentage of words in the document with 3+ syllables. 5.1 Basic Features Used in Earlier Work We have also implemented features inspired by earlier research on readability. Petersen and Ostendorf (2009) included features calculated from parsing the sentences in their corpus using the Charniak parser (Charniak, 2000): average parse tree height, average number of noun phrases per sentence, average number of verb phrases per sentence, and average number of SBARs per sentence. We have implemented versions of most of these parse-tree-related features for our project. We also parse the sentences in our corpus using Charniak’s parser and calculate the following features listed in Table 1: aNP, aN, aVP, aAdj, aSBr, aPP, nNP, nN, nVP, nAdj, nSBr, and nPP. 5.2 Novel Cognitively-Motivated Features Because of the special reading characteristics of our target users, we have designed a set of cognitively motivated fea</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, E. 2000. A maximum-entropy-inspired parser. In Proc. NAACL, pp. 132-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Collins-Thompson</author>
<author>J Callan</author>
</authors>
<title>A language modeling approach to predicting reading difficulty.</title>
<date>2004</date>
<booktitle>In Proc. NAACL,</booktitle>
<pages>193--200</pages>
<contexts>
<context position="6816" citStr="Collins-Thompson and Callan, 2004" startWordPosition="1067" endWordPosition="1070">ntor, 1982). They can easily misrepresent the complexity of technical texts, or reveal themselves un-adapted to a set of readers with particular reading difficulties. Other formulas rely on lexical information; e.g., the New Dale-Chall readability formula consults a static, manually-built list of “easy” words to determine whether a text contains unfamiliar words (Chall and Dale, 1995). Researchers in computational linguistics have investigated the use of statistical language models (unigram in particular) to capture the range of vocabulary from one grade level to another (Si and Callan, 2001; Collins-Thompson and Callan, 2004). These metrics predicted readability better than traditional formulas when tested against a corpus of web pages. The use of syntactic features was also investigated (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009) in the assessment of text readability for English as a Second Language readers. While lexical features alone outperform syntactic features in classifying texts according to their reading levels, combining the lexical and syntactic features yields the best results. Several elegant metrics that focus solely on the syntax of a text have also been develo</context>
<context position="9127" citStr="Collins-Thompson and Callan, 2004" startWordPosition="1432" endWordPosition="1435">that we seek to produce an automatic readability metric that is tailored to the literacy skills of adults with ID. Because of the specific cognitive characteristics of these users, it is an open question whether existing readability metrics and features are useful for assessing readability for adults with ID. Many of these earlier metrics have focused on the task of assigning texts to particular elementary school grade levels. Traditional grade levels may not be the ideal way to score texts to indicate how readable they are for adults with ID. Other related work has used models of vocabulary (Collins-Thompson and Callan, 2004). Since we would like to use our tool to give adults with ID access to local news stories, we choose to keep our metric topic-independent. Another difference between our approach and previous approaches is that we have designed the features used by our readability metric based on 230 the cognitive aspects of our target users. For example, these users are better at decoding words than at comprehending text meaning (Drew &amp; Hardman, 2004); so, shallow features like “syllable count per word” or unigram models of word frequency (based on texts designed for children) may be less important indicators</context>
<context position="27617" citStr="Collins-Thompson and Callan, 2004" startWordPosition="4525" endWordPosition="4528"> 357 246.1 * 271.9 202.9 * aLCw 1.803 1.358 * 1.407 1.091 * aLCn 1.852 1.42 * 1.53 1.201 * Table 2: Feature Values of Paired Corpora of our novel cognitively-motivated features. For all versions, we have only included those features that showed a significant difference between the complex and simple articles in our paired corpora (as discussed in section 5.3). 6.2 Learning Technique and Training Data Early work on automatic readability analysis framed the problem as a classification task: creating multiple classifiers for labeling a text as being one of several elementary school grade levels (Collins-Thompson and Callan, 2004). Because we are focusing on a unique user group with special reading challenges, we do not know a priori what level of text difficulty is ideal for our users. We would not know where to draw category boundaries for classification. We also prefer that our assessment tool assign numerical difficulty scores to texts. Thus, after creating this tool, we can conduct further reading comprehension experiments with adults with ID to determine what threshold (for readability scores assigned by our tool) is appropriate for our users. 234 To select features for our model, we used our paired corpora (Brit</context>
</contexts>
<marker>Collins-Thompson, Callan, 2004</marker>
<rawString>Collins-Thompson, K., and Callan, J. 2004. A language modeling approach to predicting reading difficulty. In Proc. NAACL, pp. 193-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Dale</author>
<author>J S Chall</author>
</authors>
<title>The concept of readability.</title>
<date>1949</date>
<journal>Elementary English</journal>
<volume>26</volume>
<issue>23</issue>
<contexts>
<context position="1400" citStr="Dale and Chall, 1949" startWordPosition="207" endWordPosition="210">cally rating the readability of texts for these users. Our experiments show that our discourselevel, cognitively-motivated features improve automatic readability assessment. 1 Introduction Assessing the degree of readability of a text has been a field of research as early as the 1920&apos;s. Dale and Chall define readability as “the sum total (including all the interactions) of all those elements within a given piece of printed material that affect the success a group of readers have with it. The success is the extent to which they understand it, read it at optimal speed, and find it interesting” (Dale and Chall, 1949). It has long been acknowledged that readability is a function of text characteristics, but also of the readers themselves. The literacy skills of the readers, their motivations, background knowledge, and other internal characteristics play an important role in determining whether a text is readable for a particular group of people. In our work, we investigate how to assess the readability of a text for people with intellectual disabilities (ID). Previous work in automatic readability assessment has focused on generic features of a text at the lexical and syntactic level. While such features a</context>
</contexts>
<marker>Dale, Chall, 1949</marker>
<rawString>Dale, E. and J. S. Chall. 1949. The concept of readability. Elementary English 26(23).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Davison</author>
<author>R Kantor</author>
</authors>
<title>On the failure of readability formulas to define readable texts: A case study from adaptations. Reading Research Quarterly,</title>
<date>1982</date>
<pages>17--2</pages>
<contexts>
<context position="6193" citStr="Davison and Kantor, 1982" startWordPosition="976" endWordPosition="979">reports on evaluation. Section 7 discusses the implications of the work and proposes direction for future work. 2 Related Work on Readability Metrics Many readability metrics have been established as a function of shallow features of texts, such as the number of syllables per word and number of words per sentence (Flesch, 1948; McLaughlin, 1969; Kincaid et al., 1975). These so-called traditional readability metrics are still used today in many settings and domains, in part because they are very easy to compute. Their results, however, are not always representative of the complexity of a text (Davison and Kantor, 1982). They can easily misrepresent the complexity of technical texts, or reveal themselves un-adapted to a set of readers with particular reading difficulties. Other formulas rely on lexical information; e.g., the New Dale-Chall readability formula consults a static, manually-built list of “easy” words to determine whether a text contains unfamiliar words (Chall and Dale, 1995). Researchers in computational linguistics have investigated the use of statistical language models (unigram in particular) to capture the range of vocabulary from one grade level to another (Si and Callan, 2001; Collins-Tho</context>
</contexts>
<marker>Davison, Kantor, 1982</marker>
<rawString>Davison, A., and Kantor, R. 1982. On the failure of readability formulas to define readable texts: A case study from adaptations. Reading Research Quarterly, 17(2):187-209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Drew</author>
<author>M L Hardman</author>
</authors>
<title>Mental retardation: A lifespan approach to people with intellectual disabilities (8th ed.).</title>
<date>2004</date>
<publisher>Merrill.</publisher>
<location>Columbus, OH:</location>
<contexts>
<context position="2920" citStr="Drew &amp; Hardman, 2004" startWordPosition="451" endWordPosition="454">ose a set of cognitivelymotivated features which operate at the discourse level; (3) we evaluate the utility of these features in predicting readability for adults with ID. Our framework is to create tools that benefit people with intellectual disabilities (ID), specifically those classified in the “mild level” of mental retardation, IQ scores 55-70. About 3% of the U.S. population has intelligence test scores of 70 or lower (U.S. Census Bureau, 2000). People with ID face challenges in reading literacy. They are better at decoding words (sounding them out) than at comprehending their meaning (Drew &amp; Hardman, 2004), and most read below their mental age-level (Katims, 2000). Our research addresses two literacy impairments that distinguish people with ID from other low-literacy adults: limitations in (1) working memory and (2) discourse representation. People with ID have problems remembering and inferring information from text (Fowler, 1998). They have a slower speed of semantic encoding and thus units are lost from the working memory before they are processed (Perfetti &amp; Lesgold, 1977; HicksonBilsky, 1985). People with ID also have trouble building cohesive representations of discourse (Hickson-Bilsky, </context>
<context position="9566" citStr="Drew &amp; Hardman, 2004" startWordPosition="1506" endWordPosition="1509">els may not be the ideal way to score texts to indicate how readable they are for adults with ID. Other related work has used models of vocabulary (Collins-Thompson and Callan, 2004). Since we would like to use our tool to give adults with ID access to local news stories, we choose to keep our metric topic-independent. Another difference between our approach and previous approaches is that we have designed the features used by our readability metric based on 230 the cognitive aspects of our target users. For example, these users are better at decoding words than at comprehending text meaning (Drew &amp; Hardman, 2004); so, shallow features like “syllable count per word” or unigram models of word frequency (based on texts designed for children) may be less important indicators of reading difficulty. A critical challenge for our users is to create a cohesive representation of discourse. Due to their impairments in semantic encoding speed, our users may have particular difficulty with texts that place a significant burden on working memory (items fall out of memory before they can be semantically encoded). While we focus on readability of texts, other projects have automatically generated texts for people wit</context>
</contexts>
<marker>Drew, Hardman, 2004</marker>
<rawString>Drew, C.J., and Hardman, M.L. 2004. Mental retardation: A lifespan approach to people with intellectual disabilities (8th ed.). Columbus, OH: Merrill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Flesch</author>
</authors>
<title>A new readability yardstick.</title>
<date>1948</date>
<journal>Journal of Applied Psychology,</journal>
<pages>32--221</pages>
<contexts>
<context position="5896" citStr="Flesch, 1948" startWordPosition="931" endWordPosition="932">ypotheses and describes our methodology. Section 4 focuses on the data sets used in our experiments, while section 5 describes the feature set we used for readability assessment along with a corpus-based analysis of each feature. Section 6 describes a readability assessment tool and reports on evaluation. Section 7 discusses the implications of the work and proposes direction for future work. 2 Related Work on Readability Metrics Many readability metrics have been established as a function of shallow features of texts, such as the number of syllables per word and number of words per sentence (Flesch, 1948; McLaughlin, 1969; Kincaid et al., 1975). These so-called traditional readability metrics are still used today in many settings and domains, in part because they are very easy to compute. Their results, however, are not always representative of the complexity of a text (Davison and Kantor, 1982). They can easily misrepresent the complexity of technical texts, or reveal themselves un-adapted to a set of readers with particular reading difficulties. Other formulas rely on lexical information; e.g., the New Dale-Chall readability formula consults a static, manually-built list of “easy” words to </context>
</contexts>
<marker>Flesch, 1948</marker>
<rawString>Flesch, R. 1948. A new readability yardstick. Journal of Applied Psychology, 32:221-233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A E Fowler</author>
</authors>
<title>Language in mental retardation.</title>
<date>1998</date>
<booktitle>In Burack, Hodapp, and Zigler (Eds.), Handbook of Mental Retardation and Development.</booktitle>
<pages>290--333</pages>
<publisher>Cambridge Univ. Press,</publisher>
<location>Cambridge, UK:</location>
<contexts>
<context position="3252" citStr="Fowler, 1998" startWordPosition="502" endWordPosition="503">cores 55-70. About 3% of the U.S. population has intelligence test scores of 70 or lower (U.S. Census Bureau, 2000). People with ID face challenges in reading literacy. They are better at decoding words (sounding them out) than at comprehending their meaning (Drew &amp; Hardman, 2004), and most read below their mental age-level (Katims, 2000). Our research addresses two literacy impairments that distinguish people with ID from other low-literacy adults: limitations in (1) working memory and (2) discourse representation. People with ID have problems remembering and inferring information from text (Fowler, 1998). They have a slower speed of semantic encoding and thus units are lost from the working memory before they are processed (Perfetti &amp; Lesgold, 1977; HicksonBilsky, 1985). People with ID also have trouble building cohesive representations of discourse (Hickson-Bilsky, 1985). As less information is integrated into the mental representation of the current discourse, less is comprehended. Adults with ID are limited in their choice of reading material. Most texts that they can readily understand are targeted at the level of readability of children. However, the topics of these texts often fail to m</context>
</contexts>
<marker>Fowler, 1998</marker>
<rawString>Fowler, A.E. 1998. Language in mental retardation. In Burack, Hodapp, and Zigler (Eds.), Handbook of Mental Retardation and Development. Cambridge, UK: Cambridge Univ. Press, pp. 290-333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Frazier</author>
</authors>
<date>1985</date>
<booktitle>Natural Language Parsing: Psychological, Computational, and Theoretical Perspectives, chapter Syntactic complexity,</booktitle>
<pages>129--189</pages>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7650" citStr="Frazier, 1985" startWordPosition="1204" endWordPosition="1205">tersen and Ostendorf, 2009) in the assessment of text readability for English as a Second Language readers. While lexical features alone outperform syntactic features in classifying texts according to their reading levels, combining the lexical and syntactic features yields the best results. Several elegant metrics that focus solely on the syntax of a text have also been developed. The Yngve (1960) measure, for instance, focuses on the depth of embedding of nodes in the parse tree; others use the ratio of terminal to nonterminal nodes in the parse tree of a sentence (Miller and Chomsky, 1963; Frazier, 1985). These metrics have been used to analyze the writing of potential Alzheimer&apos;s patients to detect mild cognitive impairments (Roark, Mitchell, and Hollingshead, 2007), thereby indicating that cognitively motivated features of text are valuable when creating tools for specific populations. Barzilay and Lapata (2008) presented early work in investigating the use of discourse to distinguish abridged from original encyclopedia articles. Their focus, however, is on style detection rather than readability assessment per se. Coh-Metrix is a tool for automatically calculating text coherence based on f</context>
</contexts>
<marker>Frazier, 1985</marker>
<rawString>Frazier, L. 1985. Natural Language Parsing: Psychological, Computational, and Theoretical Perspectives, chapter Syntactic complexity, pp. 129-189. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
</authors>
<title>Improving Word Sense Disambiguation in Lexical Chaining.</title>
<date>2003</date>
<booktitle>In Proc. IJCAI,</booktitle>
<pages>1486--1488</pages>
<contexts>
<context position="21904" citStr="Galley and McKeown, 2003" startWordPosition="3527" endWordPosition="3530"> averages per sentence to capture how many entities the reader must keep in mind to understand each sentence. To measure the working memory burden of a text, we’d like to capture the number of discourse entities that a reader must keep in mind. However, the “unique entities” identified by the named entity recognition tool may not be a perfect representation of this – several unique entities may actually refer to the same real-world entity under discussion. To better model how multiple noun phrases in a text refer to the same entity or concept, we have also built features using lexical chains (Galley and McKeown, 2003). Lexical chains link nouns in a document connected by relations like synonymy or hyponomy; chains can indicate concepts that recur throughout a text. A lexical chain has both a length (number of noun phrases it includes) and a span (number of words in the document between the first noun phrase at the beginning of the chain and the last noun phrase that is part of the chain). We calculate the number of lexical chains in the document (nLC) and those with a span greater than half the document length (nLC2). We believe these features may indicate the number of entities/concepts that a reader must</context>
</contexts>
<marker>Galley, McKeown, 2003</marker>
<rawString>Galley, M., McKeown, K. 2003. Improving Word Sense Disambiguation in Lexical Chaining. In Proc. IJCAI, pp. 1486-1488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gunning</author>
</authors>
<title>The Technique of Clear Writing.</title>
<date>1952</date>
<publisher>McGraw-Hill.</publisher>
<marker>Gunning, 1952</marker>
<rawString>Gunning, R. 1952. The Technique of Clear Writing. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>K Collins-Thompson</author>
<author>J Callan</author>
<author>M Eskenazi</author>
</authors>
<title>Combining lexical and grammatical features to improve readability measures for first and second language texts.</title>
<date>2007</date>
<booktitle>In Proc. NAACL,</booktitle>
<pages>460--467</pages>
<contexts>
<context position="7032" citStr="Heilman et al., 2007" startWordPosition="1101" endWordPosition="1104">Chall readability formula consults a static, manually-built list of “easy” words to determine whether a text contains unfamiliar words (Chall and Dale, 1995). Researchers in computational linguistics have investigated the use of statistical language models (unigram in particular) to capture the range of vocabulary from one grade level to another (Si and Callan, 2001; Collins-Thompson and Callan, 2004). These metrics predicted readability better than traditional formulas when tested against a corpus of web pages. The use of syntactic features was also investigated (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009) in the assessment of text readability for English as a Second Language readers. While lexical features alone outperform syntactic features in classifying texts according to their reading levels, combining the lexical and syntactic features yields the best results. Several elegant metrics that focus solely on the syntax of a text have also been developed. The Yngve (1960) measure, for instance, focuses on the depth of embedding of nodes in the parse tree; others use the ratio of terminal to nonterminal nodes in the parse tree of a sentence (Miller and Chomsky, 19</context>
</contexts>
<marker>Heilman, Collins-Thompson, Callan, Eskenazi, 2007</marker>
<rawString>Heilman, M., Collins-Thompson, K., Callan, J., and Eskenazi, M. 2007. Combining lexical and grammatical features to improve readability measures for first and second language texts. In Proc. NAACL, pp. 460-467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hickson-Bilsky</author>
</authors>
<title>Comprehension and mental retardation.</title>
<date>1985</date>
<journal>International Review of Research in Mental Retardation,</journal>
<volume>13</volume>
<pages>215--246</pages>
<contexts>
<context position="3525" citStr="Hickson-Bilsky, 1985" startWordPosition="543" endWordPosition="544">&amp; Hardman, 2004), and most read below their mental age-level (Katims, 2000). Our research addresses two literacy impairments that distinguish people with ID from other low-literacy adults: limitations in (1) working memory and (2) discourse representation. People with ID have problems remembering and inferring information from text (Fowler, 1998). They have a slower speed of semantic encoding and thus units are lost from the working memory before they are processed (Perfetti &amp; Lesgold, 1977; HicksonBilsky, 1985). People with ID also have trouble building cohesive representations of discourse (Hickson-Bilsky, 1985). As less information is integrated into the mental representation of the current discourse, less is comprehended. Adults with ID are limited in their choice of reading material. Most texts that they can readily understand are targeted at the level of readability of children. However, the topics of these texts often fail to match their interests since they are meant for younger readers. Because of the mismatch between their literacy and their interests, users may not read for pleasure and therefore miss valuable reading-skills practice time. In a feasibility study we conducted with adults Proc</context>
</contexts>
<marker>Hickson-Bilsky, 1985</marker>
<rawString>Hickson-Bilsky, L. 1985. Comprehension and mental retardation. International Review of Research in Mental Retardation, 13: 215-246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Katims</author>
</authors>
<title>Literacy instruction for people with mental retardation: Historical highlights and contemporary analysis.</title>
<date>2000</date>
<booktitle>Education and Training in Mental Retardation and Developmental Disabilities,</booktitle>
<volume>35</volume>
<issue>1</issue>
<pages>3--15</pages>
<contexts>
<context position="2979" citStr="Katims, 2000" startWordPosition="463" endWordPosition="464">course level; (3) we evaluate the utility of these features in predicting readability for adults with ID. Our framework is to create tools that benefit people with intellectual disabilities (ID), specifically those classified in the “mild level” of mental retardation, IQ scores 55-70. About 3% of the U.S. population has intelligence test scores of 70 or lower (U.S. Census Bureau, 2000). People with ID face challenges in reading literacy. They are better at decoding words (sounding them out) than at comprehending their meaning (Drew &amp; Hardman, 2004), and most read below their mental age-level (Katims, 2000). Our research addresses two literacy impairments that distinguish people with ID from other low-literacy adults: limitations in (1) working memory and (2) discourse representation. People with ID have problems remembering and inferring information from text (Fowler, 1998). They have a slower speed of semantic encoding and thus units are lost from the working memory before they are processed (Perfetti &amp; Lesgold, 1977; HicksonBilsky, 1985). People with ID also have trouble building cohesive representations of discourse (Hickson-Bilsky, 1985). As less information is integrated into the mental re</context>
</contexts>
<marker>Katims, 2000</marker>
<rawString>Katims, D.S. 2000. Literacy instruction for people with mental retardation: Historical highlights and contemporary analysis. Education and Training in Mental Retardation and Developmental Disabilities, 35(1): 3-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Kincaid</author>
<author>R P Fishburne</author>
<author>R L Rogers</author>
<author>B S Chissom</author>
</authors>
<title>Derivation of new readability formulas for Navy enlisted personnel, Research Branch Report 8-75,</title>
<date>1975</date>
<location>Millington, TN.</location>
<contexts>
<context position="5937" citStr="Kincaid et al., 1975" startWordPosition="935" endWordPosition="938">odology. Section 4 focuses on the data sets used in our experiments, while section 5 describes the feature set we used for readability assessment along with a corpus-based analysis of each feature. Section 6 describes a readability assessment tool and reports on evaluation. Section 7 discusses the implications of the work and proposes direction for future work. 2 Related Work on Readability Metrics Many readability metrics have been established as a function of shallow features of texts, such as the number of syllables per word and number of words per sentence (Flesch, 1948; McLaughlin, 1969; Kincaid et al., 1975). These so-called traditional readability metrics are still used today in many settings and domains, in part because they are very easy to compute. Their results, however, are not always representative of the complexity of a text (Davison and Kantor, 1982). They can easily misrepresent the complexity of technical texts, or reveal themselves un-adapted to a set of readers with particular reading difficulties. Other formulas rely on lexical information; e.g., the New Dale-Chall readability formula consults a static, manually-built list of “easy” words to determine whether a text contains unfamil</context>
<context position="28984" citStr="Kincaid et al., 1975" startWordPosition="4753" endWordPosition="4756">from Weekly Reader). This corpus contains articles that have each been labeled with an elementary school grade level for which it was written. We divide this corpus – using 80% of articles as training data and 20% as testing data. We model the grade level of the articles using linear regression; our model is implemented using R (R Development Core Team, 2008). 6.3 Evaluation of Our Readability Tool We conducted two rounds of training and evaluation of our three regression models. We also compare our models to a baseline readability assessment tool: the popular Flesh-Kincaid Grade Level index (Kincaid et al., 1975). In the first round of evaluation, we trained and tested our regression models on the Weekly Reader corpus. This round of evaluation helped to determine whether our feature-set and regression technique were successfully modeling those aspects of the texts that were relevant to their grade level. Our results from this round of evaluation are presented in the form of average error scores. (For each article in the Weekly Reader testing data, we calculate the difference between the output score of the model and the correct grade-level for that article.) Table 3 presents the average error results </context>
</contexts>
<marker>Kincaid, Fishburne, Rogers, Chissom, 1975</marker>
<rawString>Kincaid, J. P., Fishburne, R. P., Rogers, R. L., and Chissom, B. S. 1975. Derivation of new readability formulas for Navy enlisted personnel, Research Branch Report 8-75, Millington, TN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kincaid</author>
<author>R Fishburne</author>
<author>R Rodgers</author>
<author>B Chisson</author>
</authors>
<title>Derivation of new readability formulas for navy enlisted personnel.</title>
<date>1975</date>
<tech>Technical report, Research Branch Report 8-75,</tech>
<institution>U.S. Naval Air Station.</institution>
<contexts>
<context position="5937" citStr="Kincaid et al., 1975" startWordPosition="935" endWordPosition="938">odology. Section 4 focuses on the data sets used in our experiments, while section 5 describes the feature set we used for readability assessment along with a corpus-based analysis of each feature. Section 6 describes a readability assessment tool and reports on evaluation. Section 7 discusses the implications of the work and proposes direction for future work. 2 Related Work on Readability Metrics Many readability metrics have been established as a function of shallow features of texts, such as the number of syllables per word and number of words per sentence (Flesch, 1948; McLaughlin, 1969; Kincaid et al., 1975). These so-called traditional readability metrics are still used today in many settings and domains, in part because they are very easy to compute. Their results, however, are not always representative of the complexity of a text (Davison and Kantor, 1982). They can easily misrepresent the complexity of technical texts, or reveal themselves un-adapted to a set of readers with particular reading difficulties. Other formulas rely on lexical information; e.g., the New Dale-Chall readability formula consults a static, manually-built list of “easy” words to determine whether a text contains unfamil</context>
<context position="28984" citStr="Kincaid et al., 1975" startWordPosition="4753" endWordPosition="4756">from Weekly Reader). This corpus contains articles that have each been labeled with an elementary school grade level for which it was written. We divide this corpus – using 80% of articles as training data and 20% as testing data. We model the grade level of the articles using linear regression; our model is implemented using R (R Development Core Team, 2008). 6.3 Evaluation of Our Readability Tool We conducted two rounds of training and evaluation of our three regression models. We also compare our models to a baseline readability assessment tool: the popular Flesh-Kincaid Grade Level index (Kincaid et al., 1975). In the first round of evaluation, we trained and tested our regression models on the Weekly Reader corpus. This round of evaluation helped to determine whether our feature-set and regression technique were successfully modeling those aspects of the texts that were relevant to their grade level. Our results from this round of evaluation are presented in the form of average error scores. (For each article in the Weekly Reader testing data, we calculate the difference between the output score of the model and the correct grade-level for that article.) Table 3 presents the average error results </context>
</contexts>
<marker>Kincaid, Fishburne, Rodgers, Chisson, 1975</marker>
<rawString>Kincaid, J., Fishburne, R., Rodgers, R., and Chisson, B. 1975. Derivation of new readability formulas for navy enlisted personnel. Technical report, Research Branch Report 8-75, U.S. Naval Air Station.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H McLaughlin</author>
</authors>
<title>SMOG grading - a new readability formula.</title>
<date>1969</date>
<journal>Journal of Reading,</journal>
<pages>12--8</pages>
<contexts>
<context position="5914" citStr="McLaughlin, 1969" startWordPosition="933" endWordPosition="934">describes our methodology. Section 4 focuses on the data sets used in our experiments, while section 5 describes the feature set we used for readability assessment along with a corpus-based analysis of each feature. Section 6 describes a readability assessment tool and reports on evaluation. Section 7 discusses the implications of the work and proposes direction for future work. 2 Related Work on Readability Metrics Many readability metrics have been established as a function of shallow features of texts, such as the number of syllables per word and number of words per sentence (Flesch, 1948; McLaughlin, 1969; Kincaid et al., 1975). These so-called traditional readability metrics are still used today in many settings and domains, in part because they are very easy to compute. Their results, however, are not always representative of the complexity of a text (Davison and Kantor, 1982). They can easily misrepresent the complexity of technical texts, or reveal themselves un-adapted to a set of readers with particular reading difficulties. Other formulas rely on lexical information; e.g., the New Dale-Chall readability formula consults a static, manually-built list of “easy” words to determine whether </context>
</contexts>
<marker>McLaughlin, 1969</marker>
<rawString>McLaughlin, G.H. 1969. SMOG grading - a new readability formula. Journal of Reading, 12(8):639-646.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S McNamara</author>
<author>Y Ozuru</author>
<author>A C Graesser</author>
<author>M Louwerse</author>
</authors>
<title>Validating Coh-Metrix.,</title>
<date>2006</date>
<booktitle>In Proc. Conference of the Cognitive Science Society,</booktitle>
<pages>573</pages>
<contexts>
<context position="8363" citStr="McNamara et al., 2006" startWordPosition="1307" endWordPosition="1310">detect mild cognitive impairments (Roark, Mitchell, and Hollingshead, 2007), thereby indicating that cognitively motivated features of text are valuable when creating tools for specific populations. Barzilay and Lapata (2008) presented early work in investigating the use of discourse to distinguish abridged from original encyclopedia articles. Their focus, however, is on style detection rather than readability assessment per se. Coh-Metrix is a tool for automatically calculating text coherence based on features such as repetition of lexical items across sentences and latent semantic analysis (McNamara et al., 2006). The tool is based on comprehension data collected from children and college students. Our research differs from related work in that we seek to produce an automatic readability metric that is tailored to the literacy skills of adults with ID. Because of the specific cognitive characteristics of these users, it is an open question whether existing readability metrics and features are useful for assessing readability for adults with ID. Many of these earlier metrics have focused on the task of assigning texts to particular elementary school grade levels. Traditional grade levels may not be the</context>
</contexts>
<marker>McNamara, Ozuru, Graesser, Louwerse, 2006</marker>
<rawString>McNamara, D.S., Ozuru, Y., Graesser, A.C., &amp; Louwerse, M. (2006) Validating Coh-Metrix., In Proc. Conference of the Cognitive Science Society, pp. 573.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>N Chomsky</author>
</authors>
<date>1963</date>
<booktitle>Handbook of Mathematical Psychology, chapter Finatary models of language users,</booktitle>
<pages>419--491</pages>
<publisher>Wiley.</publisher>
<contexts>
<context position="7634" citStr="Miller and Chomsky, 1963" startWordPosition="1200" endWordPosition="1203">; Heilman et al., 2007; Petersen and Ostendorf, 2009) in the assessment of text readability for English as a Second Language readers. While lexical features alone outperform syntactic features in classifying texts according to their reading levels, combining the lexical and syntactic features yields the best results. Several elegant metrics that focus solely on the syntax of a text have also been developed. The Yngve (1960) measure, for instance, focuses on the depth of embedding of nodes in the parse tree; others use the ratio of terminal to nonterminal nodes in the parse tree of a sentence (Miller and Chomsky, 1963; Frazier, 1985). These metrics have been used to analyze the writing of potential Alzheimer&apos;s patients to detect mild cognitive impairments (Roark, Mitchell, and Hollingshead, 2007), thereby indicating that cognitively motivated features of text are valuable when creating tools for specific populations. Barzilay and Lapata (2008) presented early work in investigating the use of discourse to distinguish abridged from original encyclopedia articles. Their focus, however, is on style detection rather than readability assessment per se. Coh-Metrix is a tool for automatically calculating text cohe</context>
</contexts>
<marker>Miller, Chomsky, 1963</marker>
<rawString>Miller, G., and Chomsky, N. 1963. Handbook of Mathematical Psychology, chapter Finatary models of language users, pp. 419-491. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Perfetti</author>
<author>A Lesgold</author>
</authors>
<title>Cognitive Processes in Comprehension, chapter Discourse Comprehension and sources of individual differences.</title>
<date>1977</date>
<publisher>Erlbaum.</publisher>
<contexts>
<context position="3399" citStr="Perfetti &amp; Lesgold, 1977" startWordPosition="525" endWordPosition="528"> challenges in reading literacy. They are better at decoding words (sounding them out) than at comprehending their meaning (Drew &amp; Hardman, 2004), and most read below their mental age-level (Katims, 2000). Our research addresses two literacy impairments that distinguish people with ID from other low-literacy adults: limitations in (1) working memory and (2) discourse representation. People with ID have problems remembering and inferring information from text (Fowler, 1998). They have a slower speed of semantic encoding and thus units are lost from the working memory before they are processed (Perfetti &amp; Lesgold, 1977; HicksonBilsky, 1985). People with ID also have trouble building cohesive representations of discourse (Hickson-Bilsky, 1985). As less information is integrated into the mental representation of the current discourse, less is comprehended. Adults with ID are limited in their choice of reading material. Most texts that they can readily understand are targeted at the level of readability of children. However, the topics of these texts often fail to match their interests since they are meant for younger readers. Because of the mismatch between their literacy and their interests, users may not re</context>
</contexts>
<marker>Perfetti, Lesgold, 1977</marker>
<rawString>Perfetti, C., and Lesgold, A. 1977. Cognitive Processes in Comprehension, chapter Discourse Comprehension and sources of individual differences. Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Petersen</author>
<author>M Ostendorf</author>
</authors>
<title>A machine learning approach to reading level assessment.</title>
<date>2009</date>
<journal>Computer Speech and Language,</journal>
<volume>23</volume>
<pages>89--106</pages>
<contexts>
<context position="7063" citStr="Petersen and Ostendorf, 2009" startWordPosition="1105" endWordPosition="1108">ula consults a static, manually-built list of “easy” words to determine whether a text contains unfamiliar words (Chall and Dale, 1995). Researchers in computational linguistics have investigated the use of statistical language models (unigram in particular) to capture the range of vocabulary from one grade level to another (Si and Callan, 2001; Collins-Thompson and Callan, 2004). These metrics predicted readability better than traditional formulas when tested against a corpus of web pages. The use of syntactic features was also investigated (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009) in the assessment of text readability for English as a Second Language readers. While lexical features alone outperform syntactic features in classifying texts according to their reading levels, combining the lexical and syntactic features yields the best results. Several elegant metrics that focus solely on the syntax of a text have also been developed. The Yngve (1960) measure, for instance, focuses on the depth of embedding of nodes in the parse tree; others use the ratio of terminal to nonterminal nodes in the parse tree of a sentence (Miller and Chomsky, 1963; Frazier, 1985). These metri</context>
<context position="12302" citStr="Petersen and Ostendorf, 2009" startWordPosition="1951" endWordPosition="1954">, we collected a corpus of English text at different levels of readability. An ideal corpus for our research would contain texts that have been written specifically for our audience of adults with intellectual disabilities – in particular if such texts were paired with alternate versions of each text written for a general audience. We are not aware of such texts available electronically, and so we have instead mostly collected texts written for an audience of children. The texts come from online and commercial sources, and some have been analyzed previously by text simplification researchers (Petersen and Ostendorf, 2009). Our corpus also contains some novel texts produced as part of an experimental study involving adults with ID. 4.1 Paired and Graded Generic Corpora: Britannica, LiteracyNet, and Weekly Reader The first section of our corpus (which we refer to as Britannica) has 228 articles from the Encyclopedia Britannica, originally collected by (Barzilay and Elhadad, 2003). This consists of 114 articles in two forms: original articles written for adults and corresponding articles rewritten for an audience of children. While the texts are paired, the content of the texts is not identical: some details are </context>
<context position="13991" citStr="Petersen and Ostendorf (2009)" startWordPosition="2221" endWordPosition="2224"> an original and simplified/abridged form (230 total news articles) designed for use in literacy education. The third corpus we collected (Weekly Reader) was obtained from the Weekly Reader corporation (Weekly Reader, 2008). It contains articles for students in elementary school. Each text is labeled with its target grade level (grade 2: 174 articles, grade 3: 289 articles, grade 4: 428 articles, grade 5: 542 articles). Overall, the corpus has 1433 articles. (U.S. elementary school grades 2 to 5 generally are for children ages 7 to 10.) The corpora discussed above are similar to those used by Petersen and Ostendorf (2009). While the focus of our research is adults with ID, most of the texts discussed in this section have been simplified or written by human authors to be readable for children. Despite the texts being intended for a different audience than the focus of our research, we still believe these texts to be 231 of value. It is rare to encounter electronically available corpora in which an original and a simplified version of a text is paired (as in the Britannica and LiteracyNet corpora) or texts labeled as being at specific levels of readability (as in the Weekly Reader corpus). 4.2 Readability-Specif</context>
<context position="18022" citStr="Petersen and Ostendorf (2009)" startWordPosition="2871" endWordPosition="2875">uistic Features and Readability We now describe the set of features we investigated for assessing readability automatically. Table 1 contains a list of the features – including a short code name for each feature which may be used throughout this paper. We have begun by implementing the simple features used by the Flesh-Kincaid and FOG metrics: average number of words per sentence, average number of syllables per word, and percentage of words in the document with 3+ syllables. 5.1 Basic Features Used in Earlier Work We have also implemented features inspired by earlier research on readability. Petersen and Ostendorf (2009) included features calculated from parsing the sentences in their corpus using the Charniak parser (Charniak, 2000): average parse tree height, average number of noun phrases per sentence, average number of verb phrases per sentence, and average number of SBARs per sentence. We have implemented versions of most of these parse-tree-related features for our project. We also parse the sentences in our corpus using Charniak’s parser and calculate the following features listed in Table 1: aNP, aN, aVP, aAdj, aSBr, aPP, nNP, nN, nVP, nAdj, nSBr, and nPP. 5.2 Novel Cognitively-Motivated Features Beca</context>
<context position="34145" citStr="Petersen and Ostendorf (2009)" startWordPosition="5594" endWordPosition="5597">y too small for training, we intend to grow the size of this corpus in future work. We also plan on refining our cognitively motivated features for measuring the difficulty of a text for our users. Currently, we use lexical chain software to link noun phrases in a document that may refer to similar entities/concepts. In future work, we plan to use co-reference resolution software to model how multiple “entity mentions” may refer to a single discourse entity. For comparison purposes, we plan to implement other features that have been used in earlier readability assessment systems. For example, Petersen and Ostendorf (2009) created lists of the most common words from the Weekly Reader articles, and they used the percentage of words in a document not on this list as a feature. The overall goal of our research is to develop a software system that can automatically simplify the reading level of local news articles and present them in an accessible way to adults with ID. Our automatic readability assessment tool will be a component in this future text simplification system. We have therefore preferred to include features in our tool that focus on aspects of the text that can be modified during a simplification proce</context>
</contexts>
<marker>Petersen, Ostendorf, 2009</marker>
<rawString>Petersen, S.E., Ostendorf, M. 2009. A machine learning approach to reading level assessment. Computer Speech and Language, 23: 89-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Development Core Team</author>
</authors>
<title>R: A Language and Environment for Statistical Computing.</title>
<date>2008</date>
<location>Vienna, Austria:</location>
<note>http://www.R-project.org</note>
<contexts>
<context position="28724" citStr="Team, 2008" startWordPosition="4713" endWordPosition="4714">ol) is appropriate for our users. 234 To select features for our model, we used our paired corpora (Britannica and LiteracyNet) to measure the significance of each feature. Now that we are training a model, we make use of our graded corpus (articles from Weekly Reader). This corpus contains articles that have each been labeled with an elementary school grade level for which it was written. We divide this corpus – using 80% of articles as training data and 20% as testing data. We model the grade level of the articles using linear regression; our model is implemented using R (R Development Core Team, 2008). 6.3 Evaluation of Our Readability Tool We conducted two rounds of training and evaluation of our three regression models. We also compare our models to a baseline readability assessment tool: the popular Flesh-Kincaid Grade Level index (Kincaid et al., 1975). In the first round of evaluation, we trained and tested our regression models on the Weekly Reader corpus. This round of evaluation helped to determine whether our feature-set and regression technique were successfully modeling those aspects of the texts that were relevant to their grade level. Our results from this round of evaluation </context>
</contexts>
<marker>Team, 2008</marker>
<rawString>R Development Core Team. 2008. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. http://www.R-project.org</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>M Mitchell</author>
<author>K Hollingshead</author>
</authors>
<title>Syntactic complexity measures for detecting mild cognitive impairment.</title>
<date>2007</date>
<booktitle>In Proc. ACL Workshop on Biological, Translational, and Clinical Language Processing (BioNLP&apos;07),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="7815" citStr="Roark, Mitchell, and Hollingshead, 2007" startWordPosition="1224" endWordPosition="1228">perform syntactic features in classifying texts according to their reading levels, combining the lexical and syntactic features yields the best results. Several elegant metrics that focus solely on the syntax of a text have also been developed. The Yngve (1960) measure, for instance, focuses on the depth of embedding of nodes in the parse tree; others use the ratio of terminal to nonterminal nodes in the parse tree of a sentence (Miller and Chomsky, 1963; Frazier, 1985). These metrics have been used to analyze the writing of potential Alzheimer&apos;s patients to detect mild cognitive impairments (Roark, Mitchell, and Hollingshead, 2007), thereby indicating that cognitively motivated features of text are valuable when creating tools for specific populations. Barzilay and Lapata (2008) presented early work in investigating the use of discourse to distinguish abridged from original encyclopedia articles. Their focus, however, is on style detection rather than readability assessment per se. Coh-Metrix is a tool for automatically calculating text coherence based on features such as repetition of lexical items across sentences and latent semantic analysis (McNamara et al., 2006). The tool is based on comprehension data collected </context>
</contexts>
<marker>Roark, Mitchell, Hollingshead, 2007</marker>
<rawString>Roark, B., Mitchell, M., and Hollingshead, K. 2007. Syntactic complexity measures for detecting mild cognitive impairment. In Proc. ACL Workshop on Biological, Translational, and Clinical Language Processing (BioNLP&apos;07), pp. 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schwarm</author>
<author>M Ostendorf</author>
</authors>
<title>Reading level assessment using support vector machines and statistical language models.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="7010" citStr="Schwarm and Ostendorf, 2005" startWordPosition="1096" endWordPosition="1100">ormation; e.g., the New Dale-Chall readability formula consults a static, manually-built list of “easy” words to determine whether a text contains unfamiliar words (Chall and Dale, 1995). Researchers in computational linguistics have investigated the use of statistical language models (unigram in particular) to capture the range of vocabulary from one grade level to another (Si and Callan, 2001; Collins-Thompson and Callan, 2004). These metrics predicted readability better than traditional formulas when tested against a corpus of web pages. The use of syntactic features was also investigated (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009) in the assessment of text readability for English as a Second Language readers. While lexical features alone outperform syntactic features in classifying texts according to their reading levels, combining the lexical and syntactic features yields the best results. Several elegant metrics that focus solely on the syntax of a text have also been developed. The Yngve (1960) measure, for instance, focuses on the depth of embedding of nodes in the parse tree; others use the ratio of terminal to nonterminal nodes in the parse tree of a sentence (</context>
</contexts>
<marker>Schwarm, Ostendorf, 2005</marker>
<rawString>Schwarm, S., and Ostendorf, M. 2005. Reading level assessment using support vector machines and statistical language models. In Proc. ACL, pp. 523-530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Si</author>
<author>J Callan</author>
</authors>
<title>A statistical model for scientific readability.</title>
<date>2001</date>
<booktitle>In Proc. CIKM,</booktitle>
<pages>574--576</pages>
<contexts>
<context position="6780" citStr="Si and Callan, 2001" startWordPosition="1063" endWordPosition="1066"> text (Davison and Kantor, 1982). They can easily misrepresent the complexity of technical texts, or reveal themselves un-adapted to a set of readers with particular reading difficulties. Other formulas rely on lexical information; e.g., the New Dale-Chall readability formula consults a static, manually-built list of “easy” words to determine whether a text contains unfamiliar words (Chall and Dale, 1995). Researchers in computational linguistics have investigated the use of statistical language models (unigram in particular) to capture the range of vocabulary from one grade level to another (Si and Callan, 2001; Collins-Thompson and Callan, 2004). These metrics predicted readability better than traditional formulas when tested against a corpus of web pages. The use of syntactic features was also investigated (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009) in the assessment of text readability for English as a Second Language readers. While lexical features alone outperform syntactic features in classifying texts according to their reading levels, combining the lexical and syntactic features yields the best results. Several elegant metrics that focus solely on the sy</context>
</contexts>
<marker>Si, Callan, 2001</marker>
<rawString>Si, L., and Callan, J. 2001. A statistical model for scientific readability. In Proc. CIKM, pp. 574-576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Stenner</author>
</authors>
<title>Measuring reading comprehension with the Lexile framework.</title>
<date>1996</date>
<booktitle>4th North American Conference on Adolescent/Adult Literacy.</booktitle>
<marker>Stenner, 1996</marker>
<rawString>Stenner, A.J. 1996. Measuring reading comprehension with the Lexile framework. 4th North American Conference on Adolescent/Adult Literacy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U S Census Bureau</author>
</authors>
<title>Projections of the total resident population by five-year age groups and sex, with special age categories: Middle series 2025-2045.</title>
<date>2000</date>
<institution>U.S. Census Bureau, Populations Projections Program, Population Division.</institution>
<location>Washington:</location>
<contexts>
<context position="2754" citStr="Bureau, 2000" startWordPosition="427" endWordPosition="428">readability assessment tool. The contributions of this paper are: (1) we present a corpus of texts with readability judgments from adults with ID; (2) we propose a set of cognitivelymotivated features which operate at the discourse level; (3) we evaluate the utility of these features in predicting readability for adults with ID. Our framework is to create tools that benefit people with intellectual disabilities (ID), specifically those classified in the “mild level” of mental retardation, IQ scores 55-70. About 3% of the U.S. population has intelligence test scores of 70 or lower (U.S. Census Bureau, 2000). People with ID face challenges in reading literacy. They are better at decoding words (sounding them out) than at comprehending their meaning (Drew &amp; Hardman, 2004), and most read below their mental age-level (Katims, 2000). Our research addresses two literacy impairments that distinguish people with ID from other low-literacy adults: limitations in (1) working memory and (2) discourse representation. People with ID have problems remembering and inferring information from text (Fowler, 1998). They have a slower speed of semantic encoding and thus units are lost from the working memory before</context>
</contexts>
<marker>Bureau, 2000</marker>
<rawString>U.S. Census Bureau. 2000. Projections of the total resident population by five-year age groups and sex, with special age categories: Middle series 2025-2045. Washington: U.S. Census Bureau, Populations Projections Program, Population Division.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weekly Reader</author>
</authors>
<date>2008</date>
<note>http://www.weeklyreader.com (Accessed</note>
<contexts>
<context position="13585" citStr="Reader, 2008" startWordPosition="2154" endWordPosition="2155"> inserted. The resulting corpus is comparable in content. Because we are particularly interested in making local news articles accessible to adults with ID, we collected a second paired corpus, which we refer to as LiteracyNet, consisting of 115 news articles made available through (Western/Pacific Literacy Network / LiteracyNet, 2008). The collection of local CNN stories is available in an original and simplified/abridged form (230 total news articles) designed for use in literacy education. The third corpus we collected (Weekly Reader) was obtained from the Weekly Reader corporation (Weekly Reader, 2008). It contains articles for students in elementary school. Each text is labeled with its target grade level (grade 2: 174 articles, grade 3: 289 articles, grade 4: 428 articles, grade 5: 542 articles). Overall, the corpus has 1433 articles. (U.S. elementary school grades 2 to 5 generally are for children ages 7 to 10.) The corpora discussed above are similar to those used by Petersen and Ostendorf (2009). While the focus of our research is adults with ID, most of the texts discussed in this section have been simplified or written by human authors to be readable for children. Despite the texts b</context>
</contexts>
<marker>Reader, 2008</marker>
<rawString>Weekly Reader, 2008. http://www.weeklyreader.com (Accessed Oct., 2008).</rawString>
</citation>
<citation valid="true">
<title>Western/Pacific Literacy Network / Literacyworks,</title>
<date>2008</date>
<note>http://literacynet.org/cnnsf/ (Accessed</note>
<contexts>
<context position="7966" citStr="(2008)" startWordPosition="1249" endWordPosition="1249">cus solely on the syntax of a text have also been developed. The Yngve (1960) measure, for instance, focuses on the depth of embedding of nodes in the parse tree; others use the ratio of terminal to nonterminal nodes in the parse tree of a sentence (Miller and Chomsky, 1963; Frazier, 1985). These metrics have been used to analyze the writing of potential Alzheimer&apos;s patients to detect mild cognitive impairments (Roark, Mitchell, and Hollingshead, 2007), thereby indicating that cognitively motivated features of text are valuable when creating tools for specific populations. Barzilay and Lapata (2008) presented early work in investigating the use of discourse to distinguish abridged from original encyclopedia articles. Their focus, however, is on style detection rather than readability assessment per se. Coh-Metrix is a tool for automatically calculating text coherence based on features such as repetition of lexical items across sentences and latent semantic analysis (McNamara et al., 2006). The tool is based on comprehension data collected from children and college students. Our research differs from related work in that we seek to produce an automatic readability metric that is tailored </context>
</contexts>
<marker>2008</marker>
<rawString>Western/Pacific Literacy Network / Literacyworks, 2008. CNN SF learning resources. http://literacynet.org/cnnsf/ (Accessed Oct., 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Williams</author>
<author>E Reiter</author>
</authors>
<title>Generating readable texts for readers with low basic skills. In</title>
<date>2005</date>
<booktitle>Proc. European Workshop on Natural Language Generation,</booktitle>
<pages>140--147</pages>
<contexts>
<context position="10248" citStr="Williams and Reiter, 2005" startWordPosition="1615" endWordPosition="1618"> unigram models of word frequency (based on texts designed for children) may be less important indicators of reading difficulty. A critical challenge for our users is to create a cohesive representation of discourse. Due to their impairments in semantic encoding speed, our users may have particular difficulty with texts that place a significant burden on working memory (items fall out of memory before they can be semantically encoded). While we focus on readability of texts, other projects have automatically generated texts for people with aphasia (Carroll et al., 1999) or low reading skills (Williams and Reiter, 2005). 3 Research Hypothesis and Methods We hypothesize that the complexity of a text for adults with ID is related to the number of entities referred to in the text overall. If a paragraph or a text refers to too many entities at once, the reader has to work harder at mapping each entity to a semantic representation and deciding how each entity is related to others. On the other hand, when a text refers to few entities, less work is required both for semantic encoding and for integrating the entities into a cohesive mental representation. Section 5.2 discusses some novel discourse-level features (</context>
</contexts>
<marker>Williams, Reiter, 2005</marker>
<rawString>Williams, S., Reiter, E. 2005. Generating readable texts for readers with low basic skills. In Proc. European Workshop on Natural Language Generation, pp. 140-147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Yngve</author>
</authors>
<title>A model and a hypothesis for language structure.</title>
<date>1960</date>
<journal>American Philosophical Society,</journal>
<volume>104</volume>
<pages>446--466</pages>
<contexts>
<context position="7437" citStr="Yngve (1960)" startWordPosition="1166" endWordPosition="1167">etrics predicted readability better than traditional formulas when tested against a corpus of web pages. The use of syntactic features was also investigated (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009) in the assessment of text readability for English as a Second Language readers. While lexical features alone outperform syntactic features in classifying texts according to their reading levels, combining the lexical and syntactic features yields the best results. Several elegant metrics that focus solely on the syntax of a text have also been developed. The Yngve (1960) measure, for instance, focuses on the depth of embedding of nodes in the parse tree; others use the ratio of terminal to nonterminal nodes in the parse tree of a sentence (Miller and Chomsky, 1963; Frazier, 1985). These metrics have been used to analyze the writing of potential Alzheimer&apos;s patients to detect mild cognitive impairments (Roark, Mitchell, and Hollingshead, 2007), thereby indicating that cognitively motivated features of text are valuable when creating tools for specific populations. Barzilay and Lapata (2008) presented early work in investigating the use of discourse to distingu</context>
</contexts>
<marker>Yngve, 1960</marker>
<rawString>Yngve, V. 1960. A model and a hypothesis for language structure. American Philosophical Society, 104: 446-466.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>