<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017156">
<title confidence="0.809523">
A CLASS-BASED APPROACH TO LEXICAL DISCOVERY
</title>
<author confidence="0.968635">
Philip Resnik*
</author>
<affiliation confidence="0.889934">
Department of Computer and Information Science, University of Pennsylvania
Philadelphia, Pennsylvania 19104, USA
</affiliation>
<email confidence="0.992619">
Internet: resnik@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.999495" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999520363636364">
In this paper I propose a generalization of lexical
association techniques that is intended to facilitate
statistical discovery of facts involving word classes
rather than individual words. Although defining as-
sociation measures over classes (as sets of words) is
straightforward in theory, making direct use of such
a definition is impractical because there are simply
too many classes to consider. Rather than consid-
ering all possible classes, I propose constraining the
set of possible word classes by using a broad-coverage
lexical/conceptual hierarchy [Miller, 1990].
</bodyText>
<sectionHeader confidence="0.99649" genericHeader="method">
2 Word/Word Relationships
</sectionHeader>
<bodyText confidence="0.99884975">
Mutual information is an information-theoretic mea-
sure of association frequently used with natural lan-
guage data to gauge the &amp;quot;relatedness&amp;quot; between two
words x and y. It is defined as follows:
</bodyText>
<equation confidence="0.971784">
1(x; y) . log Pr(x)Pr(y) (1)
</equation>
<bodyText confidence="0.9959140625">
As an example of its use, consider Hindle&apos;s [1990]
application of mutual information to the discovery
of predicate argument relations. Hindle investigates
word co-occurrences as mediated by syntactic struc-
ture. A six-million-word sample of Associated Press
news stories was parsed in order to construct a collec-
tion of subject/verb/object instances. On the basis
of these data, Hindle calculates a co-occurrence score
(an estimate of mutual information) for verb/object
pairs and verb/subject pairs. Table 1 shows some of
the verb/object pairs for the verb drink that occurred
more than once, ranked by co-occurrence score, &amp;quot;in
effect giving the answer to the question &apos;what can you
drink?&apos; &amp;quot; [Hindle, 1990], p. 270.
Word/word relationships have proven useful, but
are not appropriate for all applications. For example,
</bodyText>
<footnote confidence="0.998600833333333">
*This work was supported by the following grants: ARO
DAAL 03-89-C-0031, DARPA N00014-90-J-1863, NSF IRI 90-
16592, Ben Franklin 91S.3078C-1. I am indebted to Eric
Brill, Henry Gleitman, Lila Gleitman, Aravind Joshi, Chris-
tine Nakatani, and Michael Niv for helpful discussions, and to
George Miller and colleagues for making WordNet available.
</footnote>
<table confidence="0.999863714285714">
Co-occurrence score I verb object
11.75 drink tea
11.75 drink Pepsi
11.75 drink champagne
10.53 drink liquid
10.20 drink beer
9.34 drink wine
</table>
<tableCaption confidence="0.99985">
Table 1: High-scoring verb/object pairs for
</tableCaption>
<subsectionHeader confidence="0.457136">
drink (part of Hindle 1990, Table 2).
</subsectionHeader>
<bodyText confidence="0.995858333333333">
the selectional preferences of a verb constitute a re-
lationship between a verb and a class of nouns rather
than an individual noun.
</bodyText>
<sectionHeader confidence="0.979831" genericHeader="method">
3 Word/Class Relationships
</sectionHeader>
<subsectionHeader confidence="0.969375">
3.1 A Measure of Association
</subsectionHeader>
<bodyText confidence="0.998602461538462">
In this section, I propose a method for discovering
class-based relationships in text corpora on the ba-
sis of mutual information, using for illustration the
problem of finding &amp;quot;prototypical&amp;quot; object classes for
verbs.
Let V = , v2, . , vil and N= {ni, n2, • • • nm}
be the sets of verbs and nouns in a vocabulary, and
C = fcic C N1 the set of noun classes; that is, the
power set of N. Since the relationship being inves-
tigated holds between verbs and classes of their ob-
jects, the elementary events of interest are members
of V x C. The joint probability of a verb and a class
is estimated as
</bodyText>
<equation confidence="0.812035125">
Ecount(v, n)
nEe
E E count(vi, ni)
viEV WEN&apos;
Given v E V, c E C, define the association score
P r()i:rvc
A(v, c) Pr(c1v) log p r(v
= Pr(c1v)I(v; c).
</equation>
<bodyText confidence="0.999795">
The association score takes the mutual information
between the verb and a class, and scales it according
</bodyText>
<equation confidence="0.680937">
Pr(v, c)
</equation>
<page confidence="0.974099">
327
</page>
<bodyText confidence="0.9969205">
to the likelihood that a member of that class will
actually appear as the object of the verb.&apos;
</bodyText>
<subsectionHeader confidence="0.999824">
3.2 Coherent Classes
</subsectionHeader>
<bodyText confidence="0.999878131578947">
A search among a verb&apos;s object nouns requires at
most I.Ail computations of the association score, and
can thus be done exhaustively. An exhaustive search
among object classes is impractical, however, since
the number of classes is exponential. Clearly some
way to constrain the search is needed. I propose re-
stricting the search by imposing a requirement of co-
herence upon the classes to be considered. For ex-
ample, among possible classes of objects for open,
the class {closet, locker, store} is more coherent than
{closet , locker, discourse} on intuitive grounds: ev-
ery noun in the former class describes a repository
of some kind, whereas the latter class has no such
obvious interpretation.
The WordNet lexical database [Miller, 1990] pro-
vides one way to structure the space of noun classes,
in order to make the search computationally feasi-
ble. WordNet is a lexical/conceptual database con-
structed on psycholinguistic principles by George
Miller and colleagues at Princeton University. Al-
though I cannot judge how well WordNet fares with
regard to its psycholinguistic aims, its noun taxon-
omy appears to have many of the qualities needed
if it is to provide basic taxonomic knowledge for the
purpose of corpus-based research in English, includ-
ing broad coverage and multiple word senses.
Given the WordNet noun hierarchy, the definition
of &amp;quot;coherent class&amp;quot; adopted here is straightforward.
Let words(w) be the set of nouns associated with a
WordNet class w.2
Definition. A noun class c E C is coher-
ent if there is a WordNet class w such
that words(w) n = c.
As a consequence of this definition, noun classes
that are &amp;quot;too small&amp;quot; or &amp;quot;too large&amp;quot; to be coherent are
excluded, and the problem of search through an ex-
ponentially large space of classes is reduced to search
within the WordNet hierarchy.3
</bodyText>
<tableCaption confidence="0.910710909090909">
&apos;Scaling mutual information in this fashion is often done;
see, e.g., [Rosenfeld and Huang, 19921.
2Strictly speaking, WordNet as described by [Miller,
1990] does not have classes, but rather lexical groupings
called synonym sets. By &amp;quot;WordNet class&amp;quot; I mean a pair
(word, synonym-set).
3A related possibility being investigated independently by
Paul Kogut (personal communication) is assign to each noun
and verb a vector of feature/value pairs based upon the word&apos;s
classification in the WordNet hierarchy, and to classify nouns
on the basis of their feature-value correspondences.
</tableCaption>
<table confidence="0.905725">
A(v,c) I verb I object class
3.58 drink (beverage, [beverage,...])
2.05 drink (intoxicant, [alcohol,...])
</table>
<tableCaption confidence="0.982709">
Table 2: Object classes for drink
</tableCaption>
<sectionHeader confidence="0.964177" genericHeader="method">
4 Preliminary Results
</sectionHeader>
<bodyText confidence="0.999930838709677">
An experiment was performed in order to discover the
&amp;quot;prototypical&amp;quot; object classes for a set of 115 common
English verbs. The counts of equation (2) were cal-
culated by collecting a sample of verb/object pairs
from the Brown corpus.4 Direct objects were iden-
tified using a set of heuristics to extract only the
surface object of the verb. Verb inflections were
mapped down to the base form and plural nouns
mapped down to singular.3 For example, the sen-
tence John ate two shiny red apples would yield the
pair (eat, apple). The sentence These are the apples
that John ate would not provide a pair for eat, since
apple does not appear as its surface object.
Given each verb, v, the &amp;quot;prototypical&amp;quot; object class
was found by conducting a best-first search upwards
in the WordNet noun hierarchy, starting with Word-
Net classes containing members that appeared as ob-
jects of the verb. Each WordNet class w consid-
ered was evaluated by calculating A(v, E gin E
words(w)1). Classes having too low a count (fewer
than five occurrences with the verb) were excluded
from consideration.
The results of this experiment are encouraging.
Table 2 shows the object classes discovered for the
verb drink (compare to Table 1), and Table 3 the
highest-scoring object classes for several other verbs.
Recall from the definition in Section 3.2 that each
WordNet class w in the tables appears as an ab-
breviation for { n E Hun E words(w)}; for example,
(intoxicant, (alcohol,...]) appears as an abbrevi-
ation for {whisky, cognac, wine, beer} .
</bodyText>
<sectionHeader confidence="0.750203" genericHeader="method">
5 Acquisition of Verb Properties
</sectionHeader>
<bodyText confidence="0.9997425">
More work is needed to improve the performance of
the technique proposed here. At the same time, the
ability to approximate a lexical/conceptual classifica-
tion of nouns opens up a number of possible applica-
tions in lexical acquisition. What such applications
have in common is the use of lexical associations as
a window into semantic relationships. The technique
described in this paper provides a new, hierarchical
</bodyText>
<footnote confidence="0.9909184">
4The version of the Brown corpus used was the tagged cor-
pus found as part of the Penn Treebank.
5Nouns outside the scope of WordNet that were tagged as
proper names were mapped to the token pname, a subclass of
classes (someone, [person] ) and (1 ocat ion , [location]).
</footnote>
<page confidence="0.988944">
328
</page>
<table confidence="0.999890294117647">
A(v,c) verb object class _I
1.94 ask quest ion , [question , . . .7)
,
0.16 call someone, [person, .. .3)
2.39 climb stair, [step,. • .3)
3.64 cook repast , [repast , ...])
0.27 draw cord, [cord,...))
3.58 drink (beverage, [beverage, ...])
1.76 eat (nutrient , [f ood, ...])
0.30 lose (sensory-faculty, [sense, ...])
1.28 play (part, [character , ...”
2.48 pour (liquid, [liquid, ...])
1.03 pull (cover, [covering,...])
(button, [button, ...])
1.23 push
1.18 read (writt en-mat erial, [writ ing, ...])
2.69 sing (music , [music, ...])
</table>
<tableCaption confidence="0.999933">
Table 3: Some &amp;quot;prototypical&amp;quot; object classes
</tableCaption>
<bodyText confidence="0.994562333333333">
source of semantic knowledge for statistical applica-
tions. This section briefly discusses one area where
this kind of knowledge might be exploited.
Diathesis alternations are variations in the way
that a verb syntactically expresses its arguments
[Levin, 1989]. For example, 1(a,b) shows an in-
stance of the indefinite object alternation, and 2(a,b)
shows an instance of the causative/inchoative alter-
nation.
</bodyText>
<listItem confidence="0.91315575">
1 a. John ate lunch.
b. John ate.
2 a. John opened the door.
b. The door opened.
</listItem>
<bodyText confidence="0.999738">
Such phenomena are of particular interest in the
study of how children learn the semantic and syn-
tactic properties of verbs, because they stand at the
border of syntax and lexical semantics. There are nu-
merous possible explanations for why verbs fall into
particular classes of alternations, ranging from shared
semantic properties of verbs within a class, to prag-
matic factors, to &amp;quot;lexical idiosyncracy.&amp;quot;
Statistical techniques like the one described in this
paper may be useful in investigating relationships be-
tween verbs and their arguments, with the goal of
contributing data to the study of diathesis alterna-
tions, and, ideally, in constructing a computational
model of verb acquisition. For example, in the experi-
ment described in Section 4, the verbs participating in
&amp;quot;implicit object&amp;quot; alternations&apos; appear to have higher
association scores with their &amp;quot;prototypical&amp;quot; object
classes than verbs for which implicit objects are dis-
allowed. Preliminary results, in fact, show a statis-
tically significant difference between the two groups.
</bodyText>
<footnote confidence="0.756795">
6The indefinite object alternation [Levin, 1989] and the
specified object alternation [Cote, 1992].
</footnote>
<bodyText confidence="0.999567642857143">
Might such shared information-theoretic properties of
verbs play a role in their acquisition, in the same way
that shared semantic properties might?
On a related topic, Grirnshaw has recently sug-
gested that the syntactic bootstrapping hypothe-
sis for verb acquisition [Gleitman, 1991] be ex-
tended in such a way that alternations such as the
causative/inchoative alternation (e.g. 2(a,b)) are
learned using class information about the observed
subjects and objects of the verb, in addition to sub-
categorization information.7 I hope to extend the
work on verb/object associations described here to
other arguments of the verb in order to explore this
suggestion.
</bodyText>
<sectionHeader confidence="0.999054" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999984866666667">
The technique proposed here provides a way to study
statistical associations beyond the level of individ-
ual words, using a broad-coverage lexical/conceptual
hierarchy to structure the space of possible noun
classes. Preliminary results, on the task of discover-
ing &amp;quot;prototypical&amp;quot; object classes for a set of common
English verbs, appear encouraging, and applications
in the study of verb argument structure are appar-
ent. In addition, assuming that the WordNet hier-
archy (or some similar knowledge base) proves ap-
propriately broad and consistent, the approach pro-
posed here may provide a model for importing basic
taxonomic knowledge into other corpus-based inves-
tigations, ranging from computational lexicography
to statistical language modelling.
</bodyText>
<sectionHeader confidence="0.995541" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991018590909091">
[Cote, 1992] Sharon Cote. Discourse functions of two
types of null objects in English. Presented at the 66th
Annual Meeting of the Linguistic Society of America,
Philadelphia, PA, January 1992.
[Gleitman, 1991] Lila Gleitman. The structural sources
of verb meanings. Language Acquisition, 1, 1991.
[Hindle, 1990] Donald Hindle. Noun classification from
predicate-argument structures. In Proceedings of the
28th Annual Meeting of the ACL, 1990.
[Levin, 1989] Beth Levin. Towards a lexical organization
of English verbs. Technical report, Dept. of Linguistics,
Northwestern University, November 1989.
[Miller, 1990] George Miller. Wordnet: An on-line lexical
database. International Journal of Lexicography, 4(3),
1990. (Special Issue).
[Rosenfeld and Huang, 1992] Ronald Rosenfeld and Xue-
dong Huang. Improvements in stochastic language
modelling. In Mitch Marcus, editor, Fifth DARPA
Workshop on Speech and Natural Language, February
1992. Arden House Conference Center, Harriman, NY.
7Jane Grimshaw, keynote address, Lexicon Acquisition
Workshop, University of Pennsylvania, January, 1992.
</reference>
<page confidence="0.999224">
329
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.999767">
<title confidence="0.999906">A CLASS-BASED APPROACH TO LEXICAL DISCOVERY</title>
<author confidence="0.999958">Philip Resnik</author>
<affiliation confidence="0.999961">Department of Computer and Information Science, University of Pennsylvania</affiliation>
<address confidence="0.999941">Philadelphia, Pennsylvania 19104, USA</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sharon Cote</author>
</authors>
<title>Discourse functions of two types of null objects</title>
<date>1992</date>
<booktitle>in English. Presented at the 66th Annual Meeting of the Linguistic Society of America,</booktitle>
<location>Philadelphia, PA,</location>
<marker>[Cote, 1992]</marker>
<rawString>Sharon Cote. Discourse functions of two types of null objects in English. Presented at the 66th Annual Meeting of the Linguistic Society of America, Philadelphia, PA, January 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lila Gleitman</author>
</authors>
<title>The structural sources of verb meanings.</title>
<date>1991</date>
<journal>Language Acquisition,</journal>
<volume>1</volume>
<marker>[Gleitman, 1991]</marker>
<rawString>Lila Gleitman. The structural sources of verb meanings. Language Acquisition, 1, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Noun classification from predicate-argument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the ACL,</booktitle>
<marker>[Hindle, 1990]</marker>
<rawString>Donald Hindle. Noun classification from predicate-argument structures. In Proceedings of the 28th Annual Meeting of the ACL, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>Towards a lexical organization of English verbs.</title>
<date>1989</date>
<tech>Technical report,</tech>
<institution>Dept. of Linguistics, Northwestern University,</institution>
<marker>[Levin, 1989]</marker>
<rawString>Beth Levin. Towards a lexical organization of English verbs. Technical report, Dept. of Linguistics, Northwestern University, November 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
</authors>
<title>Wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>4</volume>
<issue>3</issue>
<note>(Special Issue).</note>
<marker>[Miller, 1990]</marker>
<rawString>George Miller. Wordnet: An on-line lexical database. International Journal of Lexicography, 4(3), 1990. (Special Issue).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
<author>Xuedong Huang</author>
</authors>
<title>Improvements in stochastic language modelling.</title>
<date>1992</date>
<booktitle>Fifth DARPA Workshop on Speech and Natural Language,</booktitle>
<editor>In Mitch Marcus, editor,</editor>
<institution>University of Pennsylvania,</institution>
<marker>[Rosenfeld and Huang, 1992]</marker>
<rawString>Ronald Rosenfeld and Xuedong Huang. Improvements in stochastic language modelling. In Mitch Marcus, editor, Fifth DARPA Workshop on Speech and Natural Language, February 1992. Arden House Conference Center, Harriman, NY. 7Jane Grimshaw, keynote address, Lexicon Acquisition Workshop, University of Pennsylvania, January, 1992.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>