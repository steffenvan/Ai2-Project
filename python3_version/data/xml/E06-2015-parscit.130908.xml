<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.702972">
Semantic Role Labeling for Coreference Resolution
Simone Paolo Ponzetto and Michael Strube
EML Research gGmbH
</title>
<note confidence="0.8286355">
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
</note>
<email confidence="0.678366">
http://www.eml-research.de/nlp/
</email>
<sectionHeader confidence="0.987615" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9956358">
Extending a machine learning based coref-
erence resolution system with a feature
capturing automatically generated infor-
mation about semantic roles improves its
performance.
</bodyText>
<sectionHeader confidence="0.999266" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999722181818182">
The last years have seen a boost of work devoted
to the development of machine learning based
coreference resolution systems (Soon et al., 2001;
Ng &amp; Cardie, 2002; Kehler et al., 2004, inter alia).
Similarly, many researchers have explored tech-
niques for robust, broad coverage semantic pars-
ing in terms of semantic role labeling (Gildea &amp;
Jurafsky, 2002; Carreras &amp; M`arquez, 2005, SRL
henceforth).
This paper explores whether coreference reso-
lution can benefit from SRL, more specifically,
which phenomena are affected by such informa-
tion. The motivation comes from the fact that cur-
rent coreference resolution systems are mostly re-
lying on rather shallow features, such as the dis-
tance between the coreferent expressions, string
matching, and linguistic form. On the other hand,
the literature emphasizes since the very begin-
ning the relevance of world knowledge and infer-
ence (Charniak, 1973). As an example, consider
a sentence from the Automatic Content Extraction
(ACE) 2003 data.
</bodyText>
<subsectionHeader confidence="0.42349075">
(1) A state commission of inquiry into the sinking of the
Kursk will convene in Moscow on Wednesday, the
Interfax news agency reported. It said that the diving
operation will be completed by the end of next week.
</subsectionHeader>
<bodyText confidence="0.9999094">
It seems that in this example, knowing that the In-
terfax news agency is the AGENT of the report
predicate, and It being the AGENT of say, could
trigger the (semantic parallelism based) inference
required to correctly link the two expressions, in
contrast to anchoring the pronoun to Moscow.
SRL provides the semantic relationships that
constituents have with predicates, thus allowing
us to include document-level event descriptive in-
formation into the relations holding between re-
ferring expressions (REs). This layer of semantic
context abstracts from the specific lexical expres-
sions used, and therefore represents a higher level
of abstraction than predicate argument statistics
(Kehler et al., 2004) and Latent Semantic Analy-
sis used as a model of world knowledge (Klebanov
&amp; Wiemer-Hastings, 2002). In this respect, the
present work is closer in spirit to Ji et al. (2005),
who explore the employment of the ACE 2004 re-
lation ontology as a semantic filter.
</bodyText>
<sectionHeader confidence="0.918688" genericHeader="method">
2 Coreference Resolution Using SRL
</sectionHeader>
<subsectionHeader confidence="0.988678">
2.1 Corpora Used
</subsectionHeader>
<bodyText confidence="0.999986230769231">
The system was initially prototyped using the
MUC-6 and MUC-7 data sets (Chinchor &amp; Sund-
heim, 2003; Chinchor, 2001), using the standard
partitioning of 30 texts for training and 20-30 texts
for testing. Then, we developed and tested the
system with the ACE 2003 Training Data cor-
pus (Mitchell et al., 2003)1. Both the Newswire
(NWIRE) and Broadcast News (BNEWS) sections
where split into 60-20-20% document-based par-
titions for training, development, and testing, and
later per-partition merged (MERGED) for system
evaluation. The distribution of coreference chains
and referring expressions is given in Table 1.
</bodyText>
<subsectionHeader confidence="0.999627">
2.2 Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.999692">
For learning coreference decisions, we used a
Maximum Entropy (Berger et al., 1996) model.
Coreference resolution is viewed as a binary clas-
sification task: given a pair of REs, the classifier
has to decide whether they are coreferent or not.
First, a set of pre-processing components includ-
</bodyText>
<footnote confidence="0.8899195">
1We used the training data corpus only, as the availability
of the test data was restricted to ACE participants.
</footnote>
<page confidence="0.99937">
143
</page>
<tableCaption confidence="0.999713">
Table 1: Partitions of the ACE 2003 training data corpus
</tableCaption>
<figure confidence="0.973960789473684">
NWIRE
BNEWS
#pron. #comm. nouns #prop. names
#coref ch.
#coref ch.
1037 1210 2023
358 485 923
TRAIN.
587 876 572 980
904
DEVEL
201 315 163 465
399
329 484 712
TEST
228
354
#pron. #comm. nouns #prop. names
291 238 420
</figure>
<bodyText confidence="0.995994375">
ing a chunker and a named entity recognizer is
applied to the text in order to identify the noun
phrases, which are further taken as REs to be used
for instance generation. Instances are created fol-
lowing Soon et al. (2001). During testing the
classifier imposes a partitioning on the available
REs by clustering each set of expressions labeled
as coreferent into the same coreference chain.
</bodyText>
<subsectionHeader confidence="0.997603">
2.3 Baseline System Features
</subsectionHeader>
<bodyText confidence="0.9981792">
Following Ng &amp; Cardie (2002), our baseline sys-
tem reimplements the Soon et al. (2001) system.
The system uses 12 features. Given a pair of can-
didate referring expressions REZ and REQ the fea-
tures are computed as follows2.
</bodyText>
<figure confidence="0.9760262">
(a) Lexical features
STRING MATCH T if REZ and REQ have the
same spelling, else F.
ALIAS T if one RE is an alias of the other; else
F.
(b) Grammatical features
I PRONOUN T if REZ is a pronoun; else F.
J PRONOUN T if REQ is a pronoun; else F.
J DEF T if REQ starts with the; else F.
J DEM T if REQ starts with this, that, these, or
</figure>
<bodyText confidence="0.8574519">
those; else F.
NUMBER T if both REZ and REQ agree in num-
ber; else F.
GENDER U if REZ or REQ have an undefined
gender. Else if they are both defined and agree
T; else F.
PROPER NAME T if both REZ and REQ are
proper names; else F.
APPOSITIVE T if REQ is in apposition with
REZ; else F.
</bodyText>
<listItem confidence="0.294437222222222">
(c) Semantic features
WN CLASS U if REZ or REQ have an undefined
WordNet semantic class. Else if they both have
a defined one and it is the same T; else F.
2Possible values are U(nknown), T(rue) and F(alse). Note
that in contrast to Ng &amp; Cardie (2002) we classify ALIAS as
a lexical feature, as it solely relies on string comparison and
acronym string matching.
(d) Distance features
</listItem>
<bodyText confidence="0.683663">
DISTANCE how many sentences REZ and REQ
are apart.
</bodyText>
<subsectionHeader confidence="0.998216">
2.4 Semantic Role Features
</subsectionHeader>
<bodyText confidence="0.99250841025641">
The baseline system employs only a limited
amount of semantic knowledge. In particular, se-
mantic information is limited to WordNet seman-
tic class matching. Unfortunately, a simple Word-
Net semantic class lookup exhibits problems such
as coverage and sense disambiguation3, which
make the WN CLASS feature very noisy. As a
consequence, we propose in the following to en-
rich the semantic knowledge made available to the
classifier by using SRL information.
In our experiments we use the ASSERT
parser (Pradhan et al., 2004), an SVM based se-
mantic role tagger which uses a full syntactic
analysis to automatically identify all verb predi-
cates in a sentence together with their semantic
arguments, which are output as PropBank argu-
ments (Palmer et al., 2005). It is often the case
that the semantic arguments output by the parser
do not align with any of the previously identified
noun phrases. In this case, we pass a semantic role
label to a RE only in case the two phrases share the
same head. Labels have the form “ARG1 pred1 ...
ARG,,, pred,,,” for n semantic roles filled by a
constituent, where each semantic argument label
ARGZ is always defined with respect to a predicate
lemma predZ. Given such level of semantic infor-
mation available at the RE level, we introduce two
new features4.
I SEMROLE the semantic role argument-
predicate pairs of REZ.
3Following the system to be replicated, we simply
mapped each RE to the first WordNet sense of the head noun.
4During prototyping we experimented unpairing the ar-
guments from the predicates, which yielded worse results.
This is supported by the PropBank arguments always being
defined with respect to a target predicate. Binarizing the fea-
tures — i.e. do REi and REj have the same argument or
predicate label with respect to their closest predicate? — also
gave worse results.
</bodyText>
<page confidence="0.992953">
144
</page>
<table confidence="0.9990794">
MUC-6 MUC-7
original R P F1 R P F1
Soon et al. 58.6 67.3 62.3 56.1 65.5 60.4
duplicated 64.9 65.6 65.3 55.1 68.5 61.1
baseline
</table>
<tableCaption confidence="0.999166">
Table 2: Results on MUC
</tableCaption>
<bodyText confidence="0.8913228">
J SEMROLE the semantic role argument-
predicate pairs of REQ.
For the ACE 2003 data, 11,406 of 32,502 auto-
matically extracted noun phrases were tagged with
2,801 different argument-predicate pairs.
</bodyText>
<sectionHeader confidence="0.999891" genericHeader="evaluation">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998023">
3.1 Performance Metrics
</subsectionHeader>
<bodyText confidence="0.999912333333333">
We report in the following tables the MUC
score (Vilain et al., 1995). Scores in Table 2 are
computed for all noun phrases appearing in either
the key or the system response, whereas Tables 3
and 4 refer to scoring only those phrases which ap-
pear in both the key and the response. We discard
therefore those responses not present in the key,
as we are interested here in establishing the upper
limit of the improvements given by SRL.
We also report the accuracy score for all three
types of ACE mentions, namely pronouns, com-
mon nouns and proper names. Accuracy is the
percentage of REs of a given mention type cor-
rectly resolved divided by the total number of REs
of the same type given in the key. A RE is said
to be correctly resolved when both it and its direct
antecedent are in the same key coreference class.
In all experiments, the REs given to the clas-
sifier are noun phrases automatically extracted by
a pipeline of pre-processing components (i.e. PoS
tagger, NP chunker, Named Entity Recognizer).
</bodyText>
<subsectionHeader confidence="0.868829">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999652846153846">
Table 2 compares the results between our du-
plicated Soon baseline and the original system.
The systems show a similar performance w.r.t. F-
measure. We speculate that the result improve-
ments are due to the use of current pre-processing
components and another classifier.
Tables 3 and 4 show a comparison of the per-
formance between our baseline system and the
one incremented with SRL. Performance improve-
ments are highlighted in bold. The tables show
that SRL tends to improve system recall, rather
than acting as a ‘semantic filter’ improving pre-
cision. Semantic roles therefore seem to trigger a
</bodyText>
<table confidence="0.998473333333333">
R P F1 Av A.. AP.
baseline 54.5 88.0 67.3 34.7 20.4 53.1
+SRL 56.4 88.2 68.8 40.3 22.0 52.1
</table>
<tableCaption confidence="0.995421">
Table 4: Results ACE (merged BNEWS/NWIRE)
</tableCaption>
<table confidence="0.9999106">
Feature Chi-square
STR MATCH 1.0
J SEMROLE 0.2096
ALIAS 0.1852
I SEMROLE 0.1594
SEMCLASS 0.1474
DIST 0.1107
GENDER 0.1013
J PRONOUN 0.0982
NUMBER 0.0578
IPRONOUN 0.0489
APPOSITIVE 0.0397
PROPER NAME 0.0141
DEF NP 0.0016
DEM NP 0.0
</table>
<tableCaption confidence="0.999821">
Table 5: χ2 statistic for each feature
</tableCaption>
<bodyText confidence="0.998650545454545">
response in cases where more shallow features do
not seem to suffice (see example (1)).
The RE types which are most positively affected
by SRL are pronouns and common nouns. On the
other hand, SRL information has a limited or even
worsening effect on the performance on proper
names, where features such as string matching and
alias seem to suffice. This suggests that SRL plays
a role in pronoun and common noun resolution,
where surface features cannot account for complex
preferences and semantic knowledge is required.
</bodyText>
<subsectionHeader confidence="0.996786">
3.3 Feature Evaluation
</subsectionHeader>
<bodyText confidence="0.9999689">
We investigated the contribution of the different
features in the learning process. Table 5 shows
the chi-square statistic (normalized in the [0, 1] in-
terval) for each feature occurring in the training
data of the MERGED dataset. SRL features show
a high χ2 value, ranking immediately after string
matching and alias, which indicates a high corre-
lation of these features to the decision classes.
The importance of SRL is also indicated by the
analysis of the contribution of individual features
to the overall performance. Table 6 shows the per-
formance variations obtained by leaving out each
feature in turn. Again, it can be seen that remov-
ing both I and J SEMROLE induces a relatively
high performance degradation when compared to
other features. Their removal ranks 5th out of
12, following only essential features such as string
matching, alias, pronoun and number. Similarly
to Table 5, the semantic role of the anaphor ranks
higher than the one of the antecedent. This re-
</bodyText>
<page confidence="0.995837">
145
</page>
<table confidence="0.99771275">
BNEWS NWIRE
R P F1 Av A. Av. R P F1 Av A.. Av.
baseline 46.7 86.2 60.6 36.4 10.5 44.0 56.7 88.2 69.0 37.7 23.1 55.6
+SRL 50.9 86.1 64.0 36.8 14.3 45.7 58.3 86.9 69.8 38.0 25.8 55.8
</table>
<tableCaption confidence="0.999894">
Table 3: Results on the ACE 2003 data (BNEWS and NWIRE sections)
</tableCaption>
<figure confidence="0.921505892857143">
relations for learning in coreferential contexts.
Δ F1
Feature(s) removed
all features
68.8
STR MATCH
ALIAS
I/J PRONOUN
NUMBER
I/J SEMROLE
J SEMROLE
−21.02 Acknowledgements: This work has been funded
−2.96 −2.94 by the Klaus Tschira Foundation, Heidelberg, Ger-
−1.63 many. The first author has been supported by a
−1.50 KTF grant (09.003.2004).
−1.26
APPOSITIVE
GENDER
I SEMROLE
DIST
−1.20 References
−1.13
−0.74
−0.69
WN CLASS
DEF NP
DEM NP
PROPER NAME
</figure>
<tableCaption confidence="0.92693">
Table 6:
</tableCaption>
<bodyText confidence="0.6588405">
Fi fr
Δ
om feature removal
ce degradation.
</bodyText>
<sectionHeader confidence="0.998829" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999909466666667">
As it models the seman
tic relationship that a syn-
tactic constituent has with a predicate, it carries in-
directly syntactic preference information. In addi-
tion, when used as a feature it allows the classifier
to infer semantic role co-occurrence, thus induc-
ing deep representations of the predicate argument
lates to the improved performance on pronouns, as
it indicates that SRL helps for linking anaphoric
pronouns to preceding REs. Finally, it should
be noted that SRL provides much more solid and
noise-free semantic features when compared to the
WordNet class feature, whose removal induces al-
ways alower performan
In this paper we have investigated the effects
of using semantic role information within a ma-
chine learning based coreference resolution sys-
tem. Empirical results show that coreference res-
olution can benefit from SRL. The analysis of the
relevance of features, which had not been previ-
ously addressed, indicates that incorporating se-
mantic information as shallow event descriptions
improves the performance of the classifier. The
generated model is able to learn selection pref-
erences in cases where surface morpho-syntactic
features do not suffice, i.e. pronoun resolution.
We speculate that this contrasts with the disap-
pointing findings of Kehler et al. (2004) since SRL
provides a more fine grained level of information
when compared to predicate argument statistics.
</bodyText>
<reference confidence="0.99151538028169">
Berger, A., S. A. Della Pietra &amp; V. J. Della Pietra (1996). A
maximum entropy approach to natural language process-
ing. Computational Linguistics,
Carreras, X. &amp; L.
(2005). Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In
Proc. of CoNLL-05, pp.
Charniak, E. (1973). Jack and Janet in search of a theory
of knowledge. In Advance Papers from the Third Inter-
national Joint Conference on Artificial Intelligence, Stan-
ford, Cal., pp.
Chinchor, N. (2001). Message Understanding Conference
(MUC) 7.
Philadelphia, Penn: Linguistic
Data Consortium.
Chinchor, N. &amp; B. Sundheim (2003). Message Understand-
ing Conference (MUC) 6. LDC2003T13, Philadelphia,
Penn: Linguistic Data Consortium.
Gildea, D. &amp; D. Jurafsky (2002). Automatic labeling of se-
mantic roles. Computational Linguistics,
Ji, H., D. Westbrook &amp; R. Grishman (2005). Using semantic
relations to refine coreference decisions. In Proc. HLT-
EMNLP
pp.
Kehler, A., D. Appelt, L. Taylor &amp; A. Simma (2004). The
(non)utility of predicate-argument frequencies for pro-
noun interpretation. In Proc. of HLT-NAACL-04, pp.
Klebanov, B. &amp; P. Wiemer-Hastings (2002). The role of
wor(l)d knowledge in pronominal anaphora resolution. In
Proceedings of the International Symposium on Reference
Resolution for Natural Language Processing, Alicante,
Spain,
June, 2002, pp.
Mitchell, A., S. Strassel, M. Przybocki, J. Davis, G. Dod-
dington, R. Grishman, A. Meyers, A. Brunstain, L. Ferro
&amp; B. Sundheim (2003). TIDES Extraction (ACE) 2003
Multilingual Training Data. LDC2004T09, Philadelphia,
Penn.: Linguistic Data Consortium.
Ng, V. &amp; C. Cardie (2002). Improving machine learning ap-
proaches to coreference resolution. In Proc. of ACL-02,
pp.
Palmer, M., D. Gildea &amp; P. Kingsbury (2005). The proposi-
tion bank: An annotated corpus of semantic roles. Com-
putational Linguistics,
Pradhan, S., W. Ward, K. Hacioglu, J. H. Martin &amp; D. Juraf-
sky (2004). Shallow semantic parsing using support vector
machines. In Proc. of HLT-NAACL-04, pp.
Soon, W. M., H. T. Ng &amp; D. C. Y. Lim (2001). A ma-
chine learning approach to coreference resolution of noun
phrases. Computational Linguistics,
Vilain, M., J. Burger, J. Aberdeen, D. Connolly &amp;
L. Hirschman
22(1):39–71.
M`arquez
152–164.
337–343.
LDC2001T02,
28(3):245–288.
’05,
17–24.
289–
296.
3–4
1–8.
104–111.
31(1):71–105.
233–240.
27(4):521–544.
(1995). Amodel-theoretic coreference scor-
ing scheme. In Proceedings of the 6th Message Under-
standing Conference (MUC-6), pp. 45–52.
</reference>
<figure confidence="0.7930925">
−0.56
−0.57
−0.50
−0.49
</figure>
<page confidence="0.978814">
146
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.876050">
<title confidence="0.999657">Semantic Role Labeling for Coreference Resolution</title>
<author confidence="0.999947">Simone Paolo Ponzetto</author>
<author confidence="0.999947">Michael Strube</author>
<affiliation confidence="0.988647">EML Research gGmbH</affiliation>
<address confidence="0.9720405">Schloss-Wolfsbrunnenweg 33 69118 Heidelberg, Germany</address>
<web confidence="0.992203">http://www.eml-research.de/nlp/</web>
<abstract confidence="0.990058666666667">Extending a machine learning based coreference resolution system with a feature capturing automatically generated information about semantic roles improves its performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<contexts>
<context position="3327" citStr="Berger et al., 1996" startWordPosition="507" endWordPosition="510"> Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we developed and tested the system with the ACE 2003 Training Data corpus (Mitchell et al., 2003)1. Both the Newswire (NWIRE) and Broadcast News (BNEWS) sections where split into 60-20-20% document-based partitions for training, development, and testing, and later per-partition merged (MERGED) for system evaluation. The distribution of coreference chains and referring expressions is given in Table 1. 2.2 Learning Algorithm For learning coreference decisions, we used a Maximum Entropy (Berger et al., 1996) model. Coreference resolution is viewed as a binary classification task: given a pair of REs, the classifier has to decide whether they are coreferent or not. First, a set of pre-processing components includ1We used the training data corpus only, as the availability of the test data was restricted to ACE participants. 143 Table 1: Partitions of the ACE 2003 training data corpus NWIRE BNEWS #pron. #comm. nouns #prop. names #coref ch. #coref ch. 1037 1210 2023 358 485 923 TRAIN. 587 876 572 980 904 DEVEL 201 315 163 465 399 329 484 712 TEST 228 354 #pron. #comm. nouns #prop. names 291 238 420 i</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, A., S. A. Della Pietra &amp; V. J. Della Pietra (1996). A maximum entropy approach to natural language processing. Computational Linguistics,</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proc. of CoNLL-05,</booktitle>
<pages>pp.</pages>
<marker>Carreras, L, 2005</marker>
<rawString>Carreras, X. &amp; L. (2005). Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proc. of CoNLL-05, pp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Jack and Janet in search of a theory of knowledge.</title>
<date>1973</date>
<booktitle>In Advance Papers from the Third International Joint Conference on Artificial Intelligence,</booktitle>
<pages>pp.</pages>
<location>Stanford, Cal.,</location>
<contexts>
<context position="1288" citStr="Charniak, 1973" startWordPosition="186" endWordPosition="187">antic parsing in terms of semantic role labeling (Gildea &amp; Jurafsky, 2002; Carreras &amp; M`arquez, 2005, SRL henceforth). This paper explores whether coreference resolution can benefit from SRL, more specifically, which phenomena are affected by such information. The motivation comes from the fact that current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the coreferent expressions, string matching, and linguistic form. On the other hand, the literature emphasizes since the very beginning the relevance of world knowledge and inference (Charniak, 1973). As an example, consider a sentence from the Automatic Content Extraction (ACE) 2003 data. (1) A state commission of inquiry into the sinking of the Kursk will convene in Moscow on Wednesday, the Interfax news agency reported. It said that the diving operation will be completed by the end of next week. It seems that in this example, knowing that the Interfax news agency is the AGENT of the report predicate, and It being the AGENT of say, could trigger the (semantic parallelism based) inference required to correctly link the two expressions, in contrast to anchoring the pronoun to Moscow. SRL </context>
</contexts>
<marker>Charniak, 1973</marker>
<rawString>Charniak, E. (1973). Jack and Janet in search of a theory of knowledge. In Advance Papers from the Third International Joint Conference on Artificial Intelligence, Stanford, Cal., pp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chinchor</author>
</authors>
<date>2001</date>
<booktitle>Message Understanding Conference (MUC) 7.</booktitle>
<contexts>
<context position="2723" citStr="Chinchor, 2001" startWordPosition="418" endWordPosition="419">yer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than predicate argument statistics (Kehler et al., 2004) and Latent Semantic Analysis used as a model of world knowledge (Klebanov &amp; Wiemer-Hastings, 2002). In this respect, the present work is closer in spirit to Ji et al. (2005), who explore the employment of the ACE 2004 relation ontology as a semantic filter. 2 Coreference Resolution Using SRL 2.1 Corpora Used The system was initially prototyped using the MUC-6 and MUC-7 data sets (Chinchor &amp; Sundheim, 2003; Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we developed and tested the system with the ACE 2003 Training Data corpus (Mitchell et al., 2003)1. Both the Newswire (NWIRE) and Broadcast News (BNEWS) sections where split into 60-20-20% document-based partitions for training, development, and testing, and later per-partition merged (MERGED) for system evaluation. The distribution of coreference chains and referring expressions is given in Table 1. 2.2 Learning Algorithm For learning coreference decisions, we used a Maximum Entropy (Berger et al., 1</context>
</contexts>
<marker>Chinchor, 2001</marker>
<rawString>Chinchor, N. (2001). Message Understanding Conference (MUC) 7.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Penn Philadelphia</author>
</authors>
<title>Linguistic Data Consortium.</title>
<marker>Philadelphia, </marker>
<rawString>Philadelphia, Penn: Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chinchor</author>
<author>B Sundheim</author>
</authors>
<date>2003</date>
<booktitle>Message Understanding Conference (MUC) 6. LDC2003T13,</booktitle>
<institution>Linguistic Data Consortium.</institution>
<location>Philadelphia, Penn:</location>
<contexts>
<context position="2706" citStr="Chinchor &amp; Sundheim, 2003" startWordPosition="413" endWordPosition="417"> expressions (REs). This layer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than predicate argument statistics (Kehler et al., 2004) and Latent Semantic Analysis used as a model of world knowledge (Klebanov &amp; Wiemer-Hastings, 2002). In this respect, the present work is closer in spirit to Ji et al. (2005), who explore the employment of the ACE 2004 relation ontology as a semantic filter. 2 Coreference Resolution Using SRL 2.1 Corpora Used The system was initially prototyped using the MUC-6 and MUC-7 data sets (Chinchor &amp; Sundheim, 2003; Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we developed and tested the system with the ACE 2003 Training Data corpus (Mitchell et al., 2003)1. Both the Newswire (NWIRE) and Broadcast News (BNEWS) sections where split into 60-20-20% document-based partitions for training, development, and testing, and later per-partition merged (MERGED) for system evaluation. The distribution of coreference chains and referring expressions is given in Table 1. 2.2 Learning Algorithm For learning coreference decisions, we used a Maximum Entropy </context>
</contexts>
<marker>Chinchor, Sundheim, 2003</marker>
<rawString>Chinchor, N. &amp; B. Sundheim (2003). Message Understanding Conference (MUC) 6. LDC2003T13, Philadelphia, Penn: Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<contexts>
<context position="746" citStr="Gildea &amp; Jurafsky, 2002" startWordPosition="101" endWordPosition="104">brunnenweg 33 69118 Heidelberg, Germany http://www.eml-research.de/nlp/ Abstract Extending a machine learning based coreference resolution system with a feature capturing automatically generated information about semantic roles improves its performance. 1 Introduction The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (Soon et al., 2001; Ng &amp; Cardie, 2002; Kehler et al., 2004, inter alia). Similarly, many researchers have explored techniques for robust, broad coverage semantic parsing in terms of semantic role labeling (Gildea &amp; Jurafsky, 2002; Carreras &amp; M`arquez, 2005, SRL henceforth). This paper explores whether coreference resolution can benefit from SRL, more specifically, which phenomena are affected by such information. The motivation comes from the fact that current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the coreferent expressions, string matching, and linguistic form. On the other hand, the literature emphasizes since the very beginning the relevance of world knowledge and inference (Charniak, 1973). As an example, consider a sentence from the Automatic Co</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, D. &amp; D. Jurafsky (2002). Automatic labeling of semantic roles. Computational Linguistics,</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>D Westbrook</author>
<author>R Grishman</author>
</authors>
<title>Using semantic relations to refine coreference decisions.</title>
<date>2005</date>
<booktitle>In Proc. HLTEMNLP</booktitle>
<pages>pp.</pages>
<contexts>
<context position="2471" citStr="Ji et al. (2005)" startWordPosition="374" endWordPosition="377">oring the pronoun to Moscow. SRL provides the semantic relationships that constituents have with predicates, thus allowing us to include document-level event descriptive information into the relations holding between referring expressions (REs). This layer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than predicate argument statistics (Kehler et al., 2004) and Latent Semantic Analysis used as a model of world knowledge (Klebanov &amp; Wiemer-Hastings, 2002). In this respect, the present work is closer in spirit to Ji et al. (2005), who explore the employment of the ACE 2004 relation ontology as a semantic filter. 2 Coreference Resolution Using SRL 2.1 Corpora Used The system was initially prototyped using the MUC-6 and MUC-7 data sets (Chinchor &amp; Sundheim, 2003; Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we developed and tested the system with the ACE 2003 Training Data corpus (Mitchell et al., 2003)1. Both the Newswire (NWIRE) and Broadcast News (BNEWS) sections where split into 60-20-20% document-based partitions for training, development, and testing,</context>
</contexts>
<marker>Ji, Westbrook, Grishman, 2005</marker>
<rawString>Ji, H., D. Westbrook &amp; R. Grishman (2005). Using semantic relations to refine coreference decisions. In Proc. HLTEMNLP pp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
<author>D Appelt</author>
<author>L Taylor</author>
<author>A Simma</author>
</authors>
<title>The (non)utility of predicate-argument frequencies for pronoun interpretation.</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL-04,</booktitle>
<pages>pp.</pages>
<contexts>
<context position="2297" citStr="Kehler et al., 2004" startWordPosition="343" endWordPosition="346"> the report predicate, and It being the AGENT of say, could trigger the (semantic parallelism based) inference required to correctly link the two expressions, in contrast to anchoring the pronoun to Moscow. SRL provides the semantic relationships that constituents have with predicates, thus allowing us to include document-level event descriptive information into the relations holding between referring expressions (REs). This layer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than predicate argument statistics (Kehler et al., 2004) and Latent Semantic Analysis used as a model of world knowledge (Klebanov &amp; Wiemer-Hastings, 2002). In this respect, the present work is closer in spirit to Ji et al. (2005), who explore the employment of the ACE 2004 relation ontology as a semantic filter. 2 Coreference Resolution Using SRL 2.1 Corpora Used The system was initially prototyped using the MUC-6 and MUC-7 data sets (Chinchor &amp; Sundheim, 2003; Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we developed and tested the system with the ACE 2003 Training Data corpus (Mitch</context>
</contexts>
<marker>Kehler, Appelt, Taylor, Simma, 2004</marker>
<rawString>Kehler, A., D. Appelt, L. Taylor &amp; A. Simma (2004). The (non)utility of predicate-argument frequencies for pronoun interpretation. In Proc. of HLT-NAACL-04, pp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Klebanov</author>
<author>P Wiemer-Hastings</author>
</authors>
<title>The role of wor(l)d knowledge in pronominal anaphora resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Symposium on Reference Resolution for Natural Language Processing,</booktitle>
<location>Alicante, Spain,</location>
<contexts>
<context position="2396" citStr="Klebanov &amp; Wiemer-Hastings, 2002" startWordPosition="359" endWordPosition="362">llelism based) inference required to correctly link the two expressions, in contrast to anchoring the pronoun to Moscow. SRL provides the semantic relationships that constituents have with predicates, thus allowing us to include document-level event descriptive information into the relations holding between referring expressions (REs). This layer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than predicate argument statistics (Kehler et al., 2004) and Latent Semantic Analysis used as a model of world knowledge (Klebanov &amp; Wiemer-Hastings, 2002). In this respect, the present work is closer in spirit to Ji et al. (2005), who explore the employment of the ACE 2004 relation ontology as a semantic filter. 2 Coreference Resolution Using SRL 2.1 Corpora Used The system was initially prototyped using the MUC-6 and MUC-7 data sets (Chinchor &amp; Sundheim, 2003; Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we developed and tested the system with the ACE 2003 Training Data corpus (Mitchell et al., 2003)1. Both the Newswire (NWIRE) and Broadcast News (BNEWS) sections where split into </context>
</contexts>
<marker>Klebanov, Wiemer-Hastings, 2002</marker>
<rawString>Klebanov, B. &amp; P. Wiemer-Hastings (2002). The role of wor(l)d knowledge in pronominal anaphora resolution. In Proceedings of the International Symposium on Reference Resolution for Natural Language Processing, Alicante, Spain,</rawString>
</citation>
<citation valid="false">
<date>2002</date>
<pages>pp.</pages>
<contexts>
<context position="4375" citStr="(2002)" startWordPosition="692" endWordPosition="692">ef ch. 1037 1210 2023 358 485 923 TRAIN. 587 876 572 980 904 DEVEL 201 315 163 465 399 329 484 712 TEST 228 354 #pron. #comm. nouns #prop. names 291 238 420 ing a chunker and a named entity recognizer is applied to the text in order to identify the noun phrases, which are further taken as REs to be used for instance generation. Instances are created following Soon et al. (2001). During testing the classifier imposes a partitioning on the available REs by clustering each set of expressions labeled as coreferent into the same coreference chain. 2.3 Baseline System Features Following Ng &amp; Cardie (2002), our baseline system reimplements the Soon et al. (2001) system. The system uses 12 features. Given a pair of candidate referring expressions REZ and REQ the features are computed as follows2. (a) Lexical features STRING MATCH T if REZ and REQ have the same spelling, else F. ALIAS T if one RE is an alias of the other; else F. (b) Grammatical features I PRONOUN T if REZ is a pronoun; else F. J PRONOUN T if REQ is a pronoun; else F. J DEF T if REQ starts with the; else F. J DEM T if REQ starts with this, that, these, or those; else F. NUMBER T if both REZ and REQ agree in number; else F. GENDER</context>
</contexts>
<marker>2002</marker>
<rawString>June, 2002, pp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mitchell</author>
<author>S Strassel</author>
<author>M Przybocki</author>
<author>J Davis</author>
<author>G Doddington</author>
<author>R Grishman</author>
<author>A Meyers</author>
<author>A Brunstain</author>
<author>L Ferro</author>
<author>B Sundheim</author>
</authors>
<date>2003</date>
<booktitle>TIDES Extraction (ACE) 2003 Multilingual Training Data. LDC2004T09,</booktitle>
<institution>Linguistic Data Consortium.</institution>
<location>Philadelphia, Penn.:</location>
<contexts>
<context position="2914" citStr="Mitchell et al., 2003" startWordPosition="449" endWordPosition="452">2004) and Latent Semantic Analysis used as a model of world knowledge (Klebanov &amp; Wiemer-Hastings, 2002). In this respect, the present work is closer in spirit to Ji et al. (2005), who explore the employment of the ACE 2004 relation ontology as a semantic filter. 2 Coreference Resolution Using SRL 2.1 Corpora Used The system was initially prototyped using the MUC-6 and MUC-7 data sets (Chinchor &amp; Sundheim, 2003; Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we developed and tested the system with the ACE 2003 Training Data corpus (Mitchell et al., 2003)1. Both the Newswire (NWIRE) and Broadcast News (BNEWS) sections where split into 60-20-20% document-based partitions for training, development, and testing, and later per-partition merged (MERGED) for system evaluation. The distribution of coreference chains and referring expressions is given in Table 1. 2.2 Learning Algorithm For learning coreference decisions, we used a Maximum Entropy (Berger et al., 1996) model. Coreference resolution is viewed as a binary classification task: given a pair of REs, the classifier has to decide whether they are coreferent or not. First, a set of pre-process</context>
</contexts>
<marker>Mitchell, Strassel, Przybocki, Davis, Doddington, Grishman, Meyers, Brunstain, Ferro, Sundheim, 2003</marker>
<rawString>Mitchell, A., S. Strassel, M. Przybocki, J. Davis, G. Doddington, R. Grishman, A. Meyers, A. Brunstain, L. Ferro &amp; B. Sundheim (2003). TIDES Extraction (ACE) 2003 Multilingual Training Data. LDC2004T09, Philadelphia, Penn.: Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proc. of ACL-02,</booktitle>
<pages>pp.</pages>
<contexts>
<context position="4375" citStr="Ng &amp; Cardie (2002)" startWordPosition="689" endWordPosition="692">ref ch. #coref ch. 1037 1210 2023 358 485 923 TRAIN. 587 876 572 980 904 DEVEL 201 315 163 465 399 329 484 712 TEST 228 354 #pron. #comm. nouns #prop. names 291 238 420 ing a chunker and a named entity recognizer is applied to the text in order to identify the noun phrases, which are further taken as REs to be used for instance generation. Instances are created following Soon et al. (2001). During testing the classifier imposes a partitioning on the available REs by clustering each set of expressions labeled as coreferent into the same coreference chain. 2.3 Baseline System Features Following Ng &amp; Cardie (2002), our baseline system reimplements the Soon et al. (2001) system. The system uses 12 features. Given a pair of candidate referring expressions REZ and REQ the features are computed as follows2. (a) Lexical features STRING MATCH T if REZ and REQ have the same spelling, else F. ALIAS T if one RE is an alias of the other; else F. (b) Grammatical features I PRONOUN T if REZ is a pronoun; else F. J PRONOUN T if REQ is a pronoun; else F. J DEF T if REQ starts with the; else F. J DEM T if REQ starts with this, that, these, or those; else F. NUMBER T if both REZ and REQ agree in number; else F. GENDER</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Ng, V. &amp; C. Cardie (2002). Improving machine learning approaches to coreference resolution. In Proc. of ACL-02, pp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<contexts>
<context position="6399" citStr="Palmer et al., 2005" startWordPosition="1061" endWordPosition="1064">mantic class matching. Unfortunately, a simple WordNet semantic class lookup exhibits problems such as coverage and sense disambiguation3, which make the WN CLASS feature very noisy. As a consequence, we propose in the following to enrich the semantic knowledge made available to the classifier by using SRL information. In our experiments we use the ASSERT parser (Pradhan et al., 2004), an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments, which are output as PropBank arguments (Palmer et al., 2005). It is often the case that the semantic arguments output by the parser do not align with any of the previously identified noun phrases. In this case, we pass a semantic role label to a RE only in case the two phrases share the same head. Labels have the form “ARG1 pred1 ... ARG,,, pred,,,” for n semantic roles filled by a constituent, where each semantic argument label ARGZ is always defined with respect to a predicate lemma predZ. Given such level of semantic information available at the RE level, we introduce two new features4. I SEMROLE the semantic role argumentpredicate pairs of REZ. 3Fo</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, M., D. Gildea &amp; P. Kingsbury (2005). The proposition bank: An annotated corpus of semantic roles. Computational Linguistics,</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>W Ward</author>
<author>K Hacioglu</author>
<author>J H Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL-04,</booktitle>
<pages>pp.</pages>
<contexts>
<context position="6166" citStr="Pradhan et al., 2004" startWordPosition="1022" endWordPosition="1025"> (d) Distance features DISTANCE how many sentences REZ and REQ are apart. 2.4 Semantic Role Features The baseline system employs only a limited amount of semantic knowledge. In particular, semantic information is limited to WordNet semantic class matching. Unfortunately, a simple WordNet semantic class lookup exhibits problems such as coverage and sense disambiguation3, which make the WN CLASS feature very noisy. As a consequence, we propose in the following to enrich the semantic knowledge made available to the classifier by using SRL information. In our experiments we use the ASSERT parser (Pradhan et al., 2004), an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments, which are output as PropBank arguments (Palmer et al., 2005). It is often the case that the semantic arguments output by the parser do not align with any of the previously identified noun phrases. In this case, we pass a semantic role label to a RE only in case the two phrases share the same head. Labels have the form “ARG1 pred1 ... ARG,,, pred,,,” for n semantic roles filled by a constituent, where each semantic argument </context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Pradhan, S., W. Ward, K. Hacioglu, J. H. Martin &amp; D. Jurafsky (2004). Shallow semantic parsing using support vector machines. In Proc. of HLT-NAACL-04, pp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>D C Y Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<contexts>
<context position="4149" citStr="Soon et al. (2001)" startWordPosition="655" endWordPosition="658">clud1We used the training data corpus only, as the availability of the test data was restricted to ACE participants. 143 Table 1: Partitions of the ACE 2003 training data corpus NWIRE BNEWS #pron. #comm. nouns #prop. names #coref ch. #coref ch. 1037 1210 2023 358 485 923 TRAIN. 587 876 572 980 904 DEVEL 201 315 163 465 399 329 484 712 TEST 228 354 #pron. #comm. nouns #prop. names 291 238 420 ing a chunker and a named entity recognizer is applied to the text in order to identify the noun phrases, which are further taken as REs to be used for instance generation. Instances are created following Soon et al. (2001). During testing the classifier imposes a partitioning on the available REs by clustering each set of expressions labeled as coreferent into the same coreference chain. 2.3 Baseline System Features Following Ng &amp; Cardie (2002), our baseline system reimplements the Soon et al. (2001) system. The system uses 12 features. Given a pair of candidate referring expressions REZ and REQ the features are computed as follows2. (a) Lexical features STRING MATCH T if REZ and REQ have the same spelling, else F. ALIAS T if one RE is an alias of the other; else F. (b) Grammatical features I PRONOUN T if REZ i</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Soon, W. M., H. T. Ng &amp; D. C. Y. Lim (2001). A machine learning approach to coreference resolution of noun phrases. Computational Linguistics,</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, </marker>
<rawString>Vilain, M., J. Burger, J. Aberdeen, D. Connolly &amp; L. Hirschman 22(1):39–71.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M`arquez</author>
</authors>
<pages>337--343</pages>
<marker>M`arquez, </marker>
<rawString>M`arquez 152–164. 337–343.</rawString>
</citation>
<citation valid="false">
<volume>28</volume>
<issue>3</issue>
<pages>233--240</pages>
<marker></marker>
<rawString>LDC2001T02, 28(3):245–288. 17–24. 104–111. 233–240. 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<title>Amodel-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th Message Understanding Conference (MUC-6),</booktitle>
<pages>45--52</pages>
<marker>1995</marker>
<rawString>(1995). Amodel-theoretic coreference scoring scheme. In Proceedings of the 6th Message Understanding Conference (MUC-6), pp. 45–52.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>