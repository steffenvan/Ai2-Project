<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011032">
<title confidence="0.997777">
Ensemble Models for Dependency Parsing:
Cheap and Good?
</title>
<author confidence="0.993016">
Mihai Surdeanu and Christopher D. Manning
</author>
<affiliation confidence="0.7721065">
Computer Science Department
Stanford University, Stanford, CA 94305
</affiliation>
<email confidence="0.999567">
{mihais,manning}@stanford.edu
</email>
<sectionHeader confidence="0.997198" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.986039470588235">
Previous work on dependency parsing used
various kinds of combination models but a
systematic analysis and comparison of these
approaches is lacking. In this paper we imple-
mented such a study for English dependency
parsing and find several non-obvious facts: (a)
the diversity of base parsers is more important
than complex models for learning (e.g., stack-
ing, supervised meta-classification), (b) ap-
proximate, linear-time re-parsing algorithms
guarantee well-formed dependency trees with-
out significant performance loss, and (c) the
simplest scoring model for re-parsing (un-
weighted voting) performs essentially as well
as other more complex models. This study
proves that fast and accurate ensemble parsers
can be built with minimal effort.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9980525">
Several ensemble models have been proposed for
the parsing of syntactic dependencies. These ap-
proaches can generally be classified in two cate-
gories: models that integrate base parsers at learn-
ing time, e.g., using stacking (Nivre and McDon-
ald, 2008; Attardi and Dell’Orletta, 2009), and ap-
proaches that combine independently-trained mod-
els only at parsing time (Sagae and Lavie, 2006; Hall
et al., 2007; Attardi and Dell’Orletta, 2009). In the
latter case, the correctness of the final dependency
tree is ensured by: (a) selecting entire trees proposed
by the base parsers (Henderson and Brill, 1999); or
(b) re-parsing the pool of dependencies proposed by
the base models (Sagae and Lavie, 2006). The lat-
ter approach was shown to perform better for con-
stituent parsing (Henderson and Brill, 1999).
While all these models achieved good perfor-
mance, the previous work has left several questions
</bodyText>
<table confidence="0.998458666666667">
Devel In domain Out of domain
LAS LAS UAS LAS UAS
MST 85.36 87.07 89.95 80.48 86.08
Malt→AE 84.24 85.96 88.64 78.74 84.18
Malt→ CN 83.75 85.61 88.14 78.55 83.68
Malt→AS 83.74 85.36 88.06 77.23 82.39
Malt←AS 82.43 83.90 86.70 76.69 82.57
Malt←CN 81.75 83.53 86.17 77.29 83.02
Malt←AE 80.76 82.51 85.35 76.18 82.02
</table>
<tableCaption confidence="0.970004666666667">
Table 1: Labeled attachment scores (LAS) and unlabeled at-
tachment scores (UAS) for the base models. The parsers are
listed in descending order of LAS in the development partition.
</tableCaption>
<bodyText confidence="0.9653545">
unanswered. Here we answer the following ques-
tions, in the context of English dependency parsing:
</bodyText>
<listItem confidence="0.997585083333333">
1. When combining models at parsing time, what
is the best scoring model for candidate depen-
dencies during re-parsing? Can a meta classi-
fier improve over unsupervised voting?
2. Are (potentially-expensive) re-parsing strate-
gies justified for English? What percentage of
trees are not well-formed if one switches to a
light word-by-word voting scheme?
3. How important is the integration of base parsers
at learning time?
4. How do ensemble models compare against
state-of-the-art supervised parsers?
</listItem>
<sectionHeader confidence="0.976694" genericHeader="introduction">
2 Setup
</sectionHeader>
<bodyText confidence="0.998860857142857">
In our experiments we used the syntactic dependen-
cies from the CoNLL 2008 shared task corpus (Sur-
deanu et al., 2008).
We used seven base parsing models in this paper:
six are variants of the Malt parser1 and the seventh
is the projective version of MSTParser that uses only
first-order features2 (or MST for short). The six Malt
</bodyText>
<footnote confidence="0.99926">
1http://maltparser.org/
2http://sourceforge.net/projects/
mstparser/
</footnote>
<page confidence="0.885382">
649
</page>
<subsubsectionHeader confidence="0.417108">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 649–652,
</subsubsectionHeader>
<page confidence="0.195254">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</page>
<table confidence="0.9885255">
# of parsers Unweighted Weighted by Weighted by Weighted by Weighted by
POS of modifier label of dependency dependency length sentence length
LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS
3 86.03 89.44 86.02 89.43 85.53 88.97 85.85 89.23 86.03 89.45
4 86.79 90.14 86.68 90.07 86.38 89.78 86.46 89.79 86.84 90.18
5 86.98 90.33 86.95 90.30 86.60 90.06 86.87 90.22 86.86 90.22
6 87.14 90.51 87.17 90.50 86.74 90.22 86.91 90.23 87.04 90.37
7 86.81 90.21 86.82 90.21 86.50 90.01 86.71 90.08 86.80 90.19
</table>
<tableCaption confidence="0.9806245">
Table 2: Scores of unsupervised combination models using different voting strategies. The combined trees are assembled using a
word-by-word voting scheme.
</tableCaption>
<bodyText confidence="0.999798363636364">
parser variants are built by varying the parsing algo-
rithm (we used three parsing models: Nivre’s arc-
eager (AE), Nivre’s arc-standard (AS), and Coving-
ton’s non-projective model (CN)), and the parsing
direction (left to right (-*) or right to left (+-)), sim-
ilar to (Hall et al., 2007). The parameters of the Malt
models were set to the values reported in (Hall et
al., 2007). The MST parser was used with the de-
fault configuration. Table 1 shows the performance
of these models in the development and test parti-
tions.
</bodyText>
<sectionHeader confidence="0.999885" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999685">
3.1 On scoring models for parser combination
</subsectionHeader>
<bodyText confidence="0.99998188">
The most common approach for combining
independently-trained models at parsing time is to
assign each candidate dependency a score based
on the number of votes it received from the base
parsers. Considering that parsers specialize in
different phenomena, these votes can be weighted
by different criteria. To understand the importance
of such weighting strategies we compare several
voting approaches in Table 2: in the “unweighted”
strategy all votes have the same weight; in all other
strategies each vote is assigned a value equal to
the accuracy of the given parser in the particular
instance of the context considered, e.g., in the
“weighted by POS of modifier” model we use the
accuracies of the base models for each possible
part-of-speech (POS) tag of a modifier token. In
the table we show results as more base parsers are
added to the ensemble (we add parsers in the order
given by Table 1). The results in Table 2 indicate
that weighting strategies do not have an important
contribution to overall performance. The only
approach that outperforms the LAS score of the
unweighted voting model is the model that weighs
parsers by their accuracy for a given modifier POS
tag, but the improvement is marginal. On the other
</bodyText>
<table confidence="0.990947214285714">
POS(m) POS(m) x POS(h) length(s)
MST 38 56 26
Malt→ 0 6 6
AE 0 14 7
Malt→ 0 61 0
CN 0 0 3
Malt→ 0 9 0
AS 0 0 0
Malt←
AS
Malt←
CN
Malt←
AE
</table>
<tableCaption confidence="0.993915">
Table 3: Total number of minority dependencies with precision
larger than 50%, for different base parsers and most represen-
tative features (m - modifier, h - head, s - sentence). These
are counts of tokens, computed in the development corpus of
33,368 dependencies.
</tableCaption>
<bodyText confidence="0.979167344827586">
hand, the number of base parsers in the ensemble
pool is crucial: performance generally continues to
improve as more base parsers are considered. The
best ensemble uses 6 out of the 7 base parsers.3
It is often argued that the best way to re-score
candidate dependencies is not through voting but
rather through a meta-classifier that selects candi-
date dependencies based on their likelihood of be-
longing to the correct tree. Unlike voting, a meta-
classifier can combine evidence from multiple con-
texts (such as the ones listed in Table 2). However,
in our experiments such a meta-classifier4 did not
offer any gains over the much simpler unweighted
voting strategy. We explain these results as follows:
the meta-classifier can potentially help only when it
proposes dependencies that disagree with the major-
ity vote. We call such dependencies minority depen-
dencies.5 For a given parser and context instance
(e.g., a modifier POS), we define precision of mi-
nority dependencies as the ratio of minority depen-
dencies in this group that are correct. Obviously, a
3We drew similar conclusions when we replaced voting with
the re-parsing algorithms from the next sub-section.
4We implemented a L2-regularized logistic regression clas-
sifier using as features: identifiers of the base models, POS tags
of head and modifier, labels of dependencies, length of depen-
dencies, length of sentence, and combinations of the above.
5(Henderson and Brill, 1999) used a similar framework in
the context of constituent parsing and only three base parsers.
</bodyText>
<page confidence="0.990402">
650
</page>
<bodyText confidence="0.999924">
group of minority dependencies provides beneficial
signal only if its precision is larger than 50%. Ta-
ble 3 lists the total number of minority dependencies
in groups with precision larger than 50% for all our
base parsers and the most representative features.
The table shows that the number of minority depen-
dencies with useful signal is extremely low. All in
all, it accounts for less than 0.7% of all dependen-
cies in the development corpus.
</bodyText>
<subsectionHeader confidence="0.999692">
3.2 On re-parsing algorithms
</subsectionHeader>
<bodyText confidence="0.999879666666667">
To guarantee that the resulting dependency tree is
well-formed, most previous work used the dynamic
programming algorithm of Eisner (1996) for re-
parsing (Sagae and Lavie, 2006; Hall et al., 2007).6
However, it is not clear that this step is necessary.
In other words, how many sentences are not well-
formed if one uses a simple word-by-word voting
scheme? To answer this, we analyzed the output
of our best word-by-word voting scheme (six base
parsers weighted by the POS of the modifier). The
results for both in-domain and out-of-domain test-
ing corpora are listed in Table 4. The table shows
that the percentage of badly-formed trees is rela-
tively large: almost 10% out of domain. This in-
dicates that the focus on algorithms that guarantee
well-formed trees is justified.
However, it is not clear how the Eisner algo-
rithm, which has runtime complexity of O(n3) (n
– number of tokens per sentence), compares against
approximate re-parsing algorithms that have lower
runtime complexity. One such algorithm was pro-
posed by Attardi and Dell’Orletta (2009). The al-
gorithm, which has a runtime complexity of O(n),
builds dependency trees using a greedy top-down
strategy, i.e., it starts by selecting the highest-scoring
root node, then the highest-scoring children, etc. We
compare these algorithms against the word-by-word
voting scheme in Table 5.7 The results show that
both algorithms pay a small penalty for guaranteeing
well-formed trees. This performance drop is statis-
tically significant out of domain. On the other hand,
the difference between the Eisner and Attardi algo-
rithms is not statistically significant out of domain.
</bodyText>
<footnote confidence="0.9202036">
6We focus on projective parsing algorithms because 99.6%
of dependencies in our data are projective (Surdeanu et al.,
2008).
7Statistical significance was performed using Dan Bikel ran-
domized parsing evaluation comparator at 95% confidence.
</footnote>
<table confidence="0.9984372">
In domain Out of domain
Zero roots 0.83% 0.70%
Multiple roots 3.37% 6.11%
Cycles 4.29% 4.23%
Total 7.46% 9.64%
</table>
<tableCaption confidence="0.927884">
Table 4: Percentage of badly-formed dependency trees when
base parsers are combined using a word-by-word voting
scheme. The different error classes do not sum up to the listed
total because the errors are not mutually exclusive.
</tableCaption>
<table confidence="0.9988788">
In domain Out of domain
LAS UAS LAS UAS
Word by word 88.89 91.52 82.13* 87.51*
Eisner 88.83* 91.47* 81.99 87.32
Attardi 88.70 91.34 81.82 87.29
</table>
<tableCaption confidence="0.956836">
Table 5: Scores of different combination schemes. * indicates
that a model is significantly different than the next lower ranked
model.
</tableCaption>
<bodyText confidence="0.994188">
This experiment proves that approximate re-parsing
algorithms are a better choice for practical purposes,
i.e., ensemble parsing in domains different from the
training material of the base models.
</bodyText>
<subsectionHeader confidence="0.999906">
3.3 On parser integration at learning time
</subsectionHeader>
<bodyText confidence="0.999993964285714">
Recent work has shown that the combination of
base parsers at learning time, e.g., through stacking,
yields considerable benefits (Nivre and McDonald,
2008; Attardi and Dell’Orletta, 2009). However, it
is unclear how these approaches compare against the
simpler ensemble models, which combine parsers
only at runtime. To enable such a comparison, we
reimplemented the best stacking model from (Nivre
and McDonald, 2008) – MSTMalt – which trains a
variant of the MSTParser that uses additional fea-
tures extracted from the output of a Malt parser.
In Table 6, we compare this stacking approach
against four variants of our ensemble models. The
superscript in the ensemble name indicates the run-
time complexity of the model (O(n3) or O(n)). The
cubic-time models use all base parsers from Table 1
and the Eisner algorithm for re-parsing. The linear-
time models use only Malt-based parsers and the
Attardi algorithm for re-parsing. The subscript in
the model names indicates the percentage of avail-
able base parsers used, e.g., ensemble350% uses only
the first three parsers from Table 1. These re-
sults show that MSTMalt is statistically equivalent
to an ensemble that uses MST and two Malt vari-
ants, and both our ensemble100% models are signifi-
cantly better than MSTMalt. While this comparison
is somewhat unfair (MSTMalt uses two base models,
whereas our ensemble models use at least three) it
</bodyText>
<page confidence="0.996354">
651
</page>
<table confidence="0.999006875">
In domain Out of domain
LAS UAS LAS UAS
ensemble� 100% 88.83* 91.47* 81.99* 87.32*
1 88.01* 90.76* 80.78 86.55
ensemble 100% 87.45 90.17 81.12 86.62
ensemble50% 87.45* 90.22* 80.25* 85.90*
MSTMalt 86.74 89.62 79.44 85.54
ensemble1 50%
</table>
<tableCaption confidence="0.993891">
Table 6: Comparison of different combination strategies.
</tableCaption>
<table confidence="0.999292">
In domain Out of domain
LAS UAS LAS UAS
CoNLL 2008, #1 90.13* 92.45* 82.81* 88.19*
ensemblei00% 88.83* 91.47* 81.99* 87.32*
CoNLL 2008, #2 88.14 90.78 80.80 86.12
ensemblei00% 88.01 90.76 80.78 86.55
</table>
<tableCaption confidence="0.999986">
Table 7: Comparison with state of the art parsers.
</tableCaption>
<bodyText confidence="0.9996244">
does illustrate that the advantages gained from com-
bining parsers at learning time can be easily sur-
passed by runtime combination models that have ac-
cess to more base parsers. Considering that variants
of shift-reduce parsers can be generated with min-
imal effort (e.g., by varying the parsing direction,
learning algorithms, etc.) and combining models at
runtime is simpler than combining them at learning
time, we argue that runtime parser combination is a
more attractive approach.
</bodyText>
<subsectionHeader confidence="0.99794">
3.4 Comparison with the state of the art
</subsectionHeader>
<bodyText confidence="0.999969571428571">
In Table 7 we compare our best ensemble models
against the top two systems of the CoNLL-2008
shared task evaluation. The table indicates that our
best ensemble model ranks second, outperforming
significantly 19 other systems. The only model per-
forming better than our ensemble is a parser that
uses higher-order features and has a higher runtime
complexity (O(n4)) (Johansson and Nugues, 2008).
While this is certainly proof of the importance of
higher-order features, it also highlights a pragmatic
conclusion: in out-of-domain corpora, an ensemble
of models that use only first-order features achieves
performance that is within 1% LAS of much more
complex models.
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999855904761905">
This study unearthed several non-intuitive yet im-
portant observations about ensemble models for de-
pendency parsing. First, we showed that the diver-
sity of base parsers is more important than complex
learning models for parser combination, i.e., (a) en-
semble models that combine several base parsers at
runtime performs significantly better than a state-of-
the-art model that combines two parsers at learning
time, and (b) meta-classification does not outper-
form unsupervised voting schemes for the re-parsing
of candidate dependencies when six base models are
available. Second, we showed that well-formed de-
pendency trees can be guaranteed without signifi-
cant performance loss by linear-time approximate
re-parsing algorithms. And lastly, our analysis in-
dicates that unweighted voting performs as well as
weighted voting for the re-parsing of candidate de-
pendencies. Considering that different base models
are easy to generate, this work proves that ensemble
parsers that are both accurate and fast can be rapidly
developed with minimal effort.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.659968">
This material is based upon work supported by the Air
Force Research Laboratory (AFRL) under prime contract
no. FA8750-09-C-0181. Any opinions, findings, and
conclusion or recommendations expressed in this mate-
rial are those of the authors and do not necessarily reflect
the view of the Air Force Research Laboratory (AFRL).
We thank Johan Hall, Joakim Nivre, Ryan McDonald,
and Giuseppe Attardi for their help in understanding de-
tails of their models.
</bodyText>
<sectionHeader confidence="0.997718" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99953548">
G. Attardi and F. Dell’Orletta. 2009. Reverse revision
and linear tree combination for dependency parsing.
In Proc. of NAACL-HLT.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. of COL-
ING.
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi,
M. Nilsson, and M. Saers. 2007. Single malt or
blended? A study in multilingual parser optimization.
In Proc. of CoNLL Shared Task.
J. C. Henderson and E. Brill. 1999. Exploiting diversity
in natural language processing: Combining parsers. In
Proc. of EMNLP.
R. Johansson and P. Nugues. 2008. Dependency-based
syntactic semantic analysis with PropBank and Nom-
Bank. In Proc. of CoNLL Shared Task.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proc. of ACL.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of NAACL-HLT.
M. Surdeanu, R. Johansson, A. Meyers, L. Marquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on joint
parsing of syntactic and semantic dependencies. In
Proc. of CoNLL.
</reference>
<page confidence="0.998233">
652
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.844752">
<title confidence="0.988602">Ensemble Models for Dependency Cheap and Good?</title>
<author confidence="0.978661">Mihai Surdeanu</author>
<author confidence="0.978661">D Christopher</author>
<affiliation confidence="0.9433325">Computer Science Stanford University, Stanford, CA</affiliation>
<abstract confidence="0.999399166666667">Previous work on dependency parsing used various kinds of combination models but a systematic analysis and comparison of these approaches is lacking. In this paper we implemented such a study for English dependency parsing and find several non-obvious facts: (a) the diversity of base parsers is more important than complex models for learning (e.g., stacking, supervised meta-classification), (b) approximate, linear-time re-parsing algorithms guarantee well-formed dependency trees without significant performance loss, and (c) the simplest scoring model for re-parsing (unweighted voting) performs essentially as well as other more complex models. This study proves that fast and accurate ensemble parsers can be built with minimal effort.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Attardi</author>
<author>F Dell’Orletta</author>
</authors>
<title>Reverse revision and linear tree combination for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<marker>Attardi, Dell’Orletta, 2009</marker>
<rawString>G. Attardi and F. Dell’Orletta. 2009. Reverse revision and linear tree combination for dependency parsing. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="8565" citStr="Eisner (1996)" startWordPosition="1378" endWordPosition="1379">0 group of minority dependencies provides beneficial signal only if its precision is larger than 50%. Table 3 lists the total number of minority dependencies in groups with precision larger than 50% for all our base parsers and the most representative features. The table shows that the number of minority dependencies with useful signal is extremely low. All in all, it accounts for less than 0.7% of all dependencies in the development corpus. 3.2 On re-parsing algorithms To guarantee that the resulting dependency tree is well-formed, most previous work used the dynamic programming algorithm of Eisner (1996) for reparsing (Sagae and Lavie, 2006; Hall et al., 2007).6 However, it is not clear that this step is necessary. In other words, how many sentences are not wellformed if one uses a simple word-by-word voting scheme? To answer this, we analyzed the output of our best word-by-word voting scheme (six base parsers weighted by the POS of the modifier). The results for both in-domain and out-of-domain testing corpora are listed in Table 4. The table shows that the percentage of badly-formed trees is relatively large: almost 10% out of domain. This indicates that the focus on algorithms that guarant</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hall</author>
<author>J Nilsson</author>
<author>J Nivre</author>
<author>G Eryigit</author>
<author>B Megyesi</author>
<author>M Nilsson</author>
<author>M Saers</author>
</authors>
<title>Single malt or blended? A study in multilingual parser optimization.</title>
<date>2007</date>
<booktitle>In Proc. of CoNLL Shared Task.</booktitle>
<contexts>
<context position="1366" citStr="Hall et al., 2007" startWordPosition="195" endWordPosition="198">coring model for re-parsing (unweighted voting) performs essentially as well as other more complex models. This study proves that fast and accurate ensemble parsers can be built with minimal effort. 1 Introduction Several ensemble models have been proposed for the parsing of syntactic dependencies. These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009), and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; Hall et al., 2007; Attardi and Dell’Orletta, 2009). In the latter case, the correctness of the final dependency tree is ensured by: (a) selecting entire trees proposed by the base parsers (Henderson and Brill, 1999); or (b) re-parsing the pool of dependencies proposed by the base models (Sagae and Lavie, 2006). The latter approach was shown to perform better for constituent parsing (Henderson and Brill, 1999). While all these models achieved good performance, the previous work has left several questions Devel In domain Out of domain LAS LAS UAS LAS UAS MST 85.36 87.07 89.95 80.48 86.08 Malt→AE 84.24 85.96 88.6</context>
<context position="4488" citStr="Hall et al., 2007" startWordPosition="694" endWordPosition="697"> 90.33 86.95 90.30 86.60 90.06 86.87 90.22 86.86 90.22 6 87.14 90.51 87.17 90.50 86.74 90.22 86.91 90.23 87.04 90.37 7 86.81 90.21 86.82 90.21 86.50 90.01 86.71 90.08 86.80 90.19 Table 2: Scores of unsupervised combination models using different voting strategies. The combined trees are assembled using a word-by-word voting scheme. parser variants are built by varying the parsing algorithm (we used three parsing models: Nivre’s arceager (AE), Nivre’s arc-standard (AS), and Covington’s non-projective model (CN)), and the parsing direction (left to right (-*) or right to left (+-)), similar to (Hall et al., 2007). The parameters of the Malt models were set to the values reported in (Hall et al., 2007). The MST parser was used with the default configuration. Table 1 shows the performance of these models in the development and test partitions. 3 Experiments 3.1 On scoring models for parser combination The most common approach for combining independently-trained models at parsing time is to assign each candidate dependency a score based on the number of votes it received from the base parsers. Considering that parsers specialize in different phenomena, these votes can be weighted by different criteria. T</context>
<context position="8622" citStr="Hall et al., 2007" startWordPosition="1387" endWordPosition="1390"> signal only if its precision is larger than 50%. Table 3 lists the total number of minority dependencies in groups with precision larger than 50% for all our base parsers and the most representative features. The table shows that the number of minority dependencies with useful signal is extremely low. All in all, it accounts for less than 0.7% of all dependencies in the development corpus. 3.2 On re-parsing algorithms To guarantee that the resulting dependency tree is well-formed, most previous work used the dynamic programming algorithm of Eisner (1996) for reparsing (Sagae and Lavie, 2006; Hall et al., 2007).6 However, it is not clear that this step is necessary. In other words, how many sentences are not wellformed if one uses a simple word-by-word voting scheme? To answer this, we analyzed the output of our best word-by-word voting scheme (six base parsers weighted by the POS of the modifier). The results for both in-domain and out-of-domain testing corpora are listed in Table 4. The table shows that the percentage of badly-formed trees is relatively large: almost 10% out of domain. This indicates that the focus on algorithms that guarantee well-formed trees is justified. However, it is not cle</context>
</contexts>
<marker>Hall, Nilsson, Nivre, Eryigit, Megyesi, Nilsson, Saers, 2007</marker>
<rawString>J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M. Nilsson, and M. Saers. 2007. Single malt or blended? A study in multilingual parser optimization. In Proc. of CoNLL Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Henderson</author>
<author>E Brill</author>
</authors>
<title>Exploiting diversity in natural language processing: Combining parsers.</title>
<date>1999</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1564" citStr="Henderson and Brill, 1999" startWordPosition="226" endWordPosition="229">l effort. 1 Introduction Several ensemble models have been proposed for the parsing of syntactic dependencies. These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009), and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; Hall et al., 2007; Attardi and Dell’Orletta, 2009). In the latter case, the correctness of the final dependency tree is ensured by: (a) selecting entire trees proposed by the base parsers (Henderson and Brill, 1999); or (b) re-parsing the pool of dependencies proposed by the base models (Sagae and Lavie, 2006). The latter approach was shown to perform better for constituent parsing (Henderson and Brill, 1999). While all these models achieved good performance, the previous work has left several questions Devel In domain Out of domain LAS LAS UAS LAS UAS MST 85.36 87.07 89.95 80.48 86.08 Malt→AE 84.24 85.96 88.64 78.74 84.18 Malt→ CN 83.75 85.61 88.14 78.55 83.68 Malt→AS 83.74 85.36 88.06 77.23 82.39 Malt←AS 82.43 83.90 86.70 76.69 82.57 Malt←CN 81.75 83.53 86.17 77.29 83.02 Malt←AE 80.76 82.51 85.35 76.18</context>
<context position="7857" citStr="Henderson and Brill, 1999" startWordPosition="1261" endWordPosition="1264">We call such dependencies minority dependencies.5 For a given parser and context instance (e.g., a modifier POS), we define precision of minority dependencies as the ratio of minority dependencies in this group that are correct. Obviously, a 3We drew similar conclusions when we replaced voting with the re-parsing algorithms from the next sub-section. 4We implemented a L2-regularized logistic regression classifier using as features: identifiers of the base models, POS tags of head and modifier, labels of dependencies, length of dependencies, length of sentence, and combinations of the above. 5(Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers. 650 group of minority dependencies provides beneficial signal only if its precision is larger than 50%. Table 3 lists the total number of minority dependencies in groups with precision larger than 50% for all our base parsers and the most representative features. The table shows that the number of minority dependencies with useful signal is extremely low. All in all, it accounts for less than 0.7% of all dependencies in the development corpus. 3.2 On re-parsing algorithms To guarantee that the resultin</context>
</contexts>
<marker>Henderson, Brill, 1999</marker>
<rawString>J. C. Henderson and E. Brill. 1999. Exploiting diversity in natural language processing: Combining parsers. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Dependency-based syntactic semantic analysis with PropBank and NomBank.</title>
<date>2008</date>
<booktitle>In Proc. of CoNLL Shared Task.</booktitle>
<contexts>
<context position="14015" citStr="Johansson and Nugues, 2008" startWordPosition="2246" endWordPosition="2249">g direction, learning algorithms, etc.) and combining models at runtime is simpler than combining them at learning time, we argue that runtime parser combination is a more attractive approach. 3.4 Comparison with the state of the art In Table 7 we compare our best ensemble models against the top two systems of the CoNLL-2008 shared task evaluation. The table indicates that our best ensemble model ranks second, outperforming significantly 19 other systems. The only model performing better than our ensemble is a parser that uses higher-order features and has a higher runtime complexity (O(n4)) (Johansson and Nugues, 2008). While this is certainly proof of the importance of higher-order features, it also highlights a pragmatic conclusion: in out-of-domain corpora, an ensemble of models that use only first-order features achieves performance that is within 1% LAS of much more complex models. 4 Conclusions This study unearthed several non-intuitive yet important observations about ensemble models for dependency parsing. First, we showed that the diversity of base parsers is more important than complex learning models for parser combination, i.e., (a) ensemble models that combine several base parsers at runtime pe</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>R. Johansson and P. Nugues. 2008. Dependency-based syntactic semantic analysis with PropBank and NomBank. In Proc. of CoNLL Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>R McDonald</author>
</authors>
<title>Integrating graphbased and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proc. of ACL. K. Sagae</booktitle>
<contexts>
<context position="1212" citStr="Nivre and McDonald, 2008" startWordPosition="170" endWordPosition="174">fication), (b) approximate, linear-time re-parsing algorithms guarantee well-formed dependency trees without significant performance loss, and (c) the simplest scoring model for re-parsing (unweighted voting) performs essentially as well as other more complex models. This study proves that fast and accurate ensemble parsers can be built with minimal effort. 1 Introduction Several ensemble models have been proposed for the parsing of syntactic dependencies. These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009), and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; Hall et al., 2007; Attardi and Dell’Orletta, 2009). In the latter case, the correctness of the final dependency tree is ensured by: (a) selecting entire trees proposed by the base parsers (Henderson and Brill, 1999); or (b) re-parsing the pool of dependencies proposed by the base models (Sagae and Lavie, 2006). The latter approach was shown to perform better for constituent parsing (Henderson and Brill, 1999). While all these models achieved good performance,</context>
<context position="11314" citStr="Nivre and McDonald, 2008" startWordPosition="1811" endWordPosition="1814">d 88.89 91.52 82.13* 87.51* Eisner 88.83* 91.47* 81.99 87.32 Attardi 88.70 91.34 81.82 87.29 Table 5: Scores of different combination schemes. * indicates that a model is significantly different than the next lower ranked model. This experiment proves that approximate re-parsing algorithms are a better choice for practical purposes, i.e., ensemble parsing in domains different from the training material of the base models. 3.3 On parser integration at learning time Recent work has shown that the combination of base parsers at learning time, e.g., through stacking, yields considerable benefits (Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009). However, it is unclear how these approaches compare against the simpler ensemble models, which combine parsers only at runtime. To enable such a comparison, we reimplemented the best stacking model from (Nivre and McDonald, 2008) – MSTMalt – which trains a variant of the MSTParser that uses additional features extracted from the output of a Malt parser. In Table 6, we compare this stacking approach against four variants of our ensemble models. The superscript in the ensemble name indicates the runtime complexity of the model (O(n3) or O(n)). The cubic-time mo</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>J. Nivre and R. McDonald. 2008. Integrating graphbased and transition-based dependency parsers. In Proc. of ACL. K. Sagae and A. Lavie. 2006. Parser combination by reparsing. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>R Johansson</author>
<author>A Meyers</author>
<author>L Marquez</author>
<author>J Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="3073" citStr="Surdeanu et al., 2008" startWordPosition="469" endWordPosition="473"> When combining models at parsing time, what is the best scoring model for candidate dependencies during re-parsing? Can a meta classifier improve over unsupervised voting? 2. Are (potentially-expensive) re-parsing strategies justified for English? What percentage of trees are not well-formed if one switches to a light word-by-word voting scheme? 3. How important is the integration of base parsers at learning time? 4. How do ensemble models compare against state-of-the-art supervised parsers? 2 Setup In our experiments we used the syntactic dependencies from the CoNLL 2008 shared task corpus (Surdeanu et al., 2008). We used seven base parsing models in this paper: six are variants of the Malt parser1 and the seventh is the projective version of MSTParser that uses only first-order features2 (or MST for short). The six Malt 1http://maltparser.org/ 2http://sourceforge.net/projects/ mstparser/ 649 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 649–652, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics # of parsers Unweighted Weighted by Weighted by Weighted by Weighted by POS of modifier label of dependency dependen</context>
<context position="10181" citStr="Surdeanu et al., 2008" startWordPosition="1636" endWordPosition="1639"> using a greedy top-down strategy, i.e., it starts by selecting the highest-scoring root node, then the highest-scoring children, etc. We compare these algorithms against the word-by-word voting scheme in Table 5.7 The results show that both algorithms pay a small penalty for guaranteeing well-formed trees. This performance drop is statistically significant out of domain. On the other hand, the difference between the Eisner and Attardi algorithms is not statistically significant out of domain. 6We focus on projective parsing algorithms because 99.6% of dependencies in our data are projective (Surdeanu et al., 2008). 7Statistical significance was performed using Dan Bikel randomized parsing evaluation comparator at 95% confidence. In domain Out of domain Zero roots 0.83% 0.70% Multiple roots 3.37% 6.11% Cycles 4.29% 4.23% Total 7.46% 9.64% Table 4: Percentage of badly-formed dependency trees when base parsers are combined using a word-by-word voting scheme. The different error classes do not sum up to the listed total because the errors are not mutually exclusive. In domain Out of domain LAS UAS LAS UAS Word by word 88.89 91.52 82.13* 87.51* Eisner 88.83* 91.47* 81.99 87.32 Attardi 88.70 91.34 81.82 87.2</context>
</contexts>
<marker>Surdeanu, Johansson, Meyers, Marquez, Nivre, 2008</marker>
<rawString>M. Surdeanu, R. Johansson, A. Meyers, L. Marquez, and J. Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proc. of CoNLL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>