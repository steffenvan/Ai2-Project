<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001175">
<title confidence="0.977626">
Learning Better Rule Extraction with Translation Span Alignment
</title>
<author confidence="0.991537">
Jingbo Zhu Tong Xiao Chunliang Zhang
</author>
<affiliation confidence="0.8720495">
Natural Language Processing Laboratory
Northeastern University, Shenyang, China
</affiliation>
<email confidence="0.99167">
{zhujingbo,xiaotong,zhangcl}@mail.neu.edu.cn
</email>
<sectionHeader confidence="0.997299" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999774">
This paper presents an unsupervised ap-
proach to learning translation span align-
ments from parallel data that improves
syntactic rule extraction by deleting spuri-
ous word alignment links and adding new
valuable links based on bilingual transla-
tion span correspondences. Experiments on
Chinese-English translation demonstrate
improvements over standard methods for
tree-to-string and tree-to-tree translation.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99981176">
Most syntax-based statistical machine translation
(SMT) systems typically utilize word alignments
and parse trees on the source/target side to learn
syntactic transformation rules from parallel data.
The approach suffers from a practical problem that
even one spurious (word alignment) link can pre-
vent some desirable syntactic translation rules from
extraction, which can in turn affect the quality of
translation rules and translation performance (May
and Knight 2007; Fossum et al. 2008). To address
this challenge, a considerable amount of previous
research has been done to improve alignment qual-
ity by incorporating some statistics and linguistic
heuristics or syntactic information into word
alignments (Cherry and Lin 2006; DeNero and
Klein 2007; May and Knight 2007; Fossum et al.
2008; Hermjakob 2009; Liu et al. 2010).
Unlike their efforts, this paper presents a simple
approach that automatically builds the translation
span alignment (TSA) of a sentence pair by utiliz-
ing a phrase-based forced decoding technique, and
then improves syntactic rule extraction by deleting
spurious links and adding new valuable links based
on bilingual translation span correspondences. The
proposed approach has two promising properties.
</bodyText>
<figureCaption confidence="0.9964015">
Figure 1. A real example of Chinese-English sentence
pair with word alignment and both-side parse trees.
</figureCaption>
<table confidence="0.9225141">
Some blocked Tree-to-string Rules:
r1: AS(了) → have
r2: NN(进口) → the imports
r3: S (NN:x1 VP:x2) → x1 x2
Some blocked Tree-to-tree Rules:
r4: AS(了) → VBZ(have)
r5: NN(进口) → NP(DT(the) NNS(imports))
r6: S(NN:x1 VP:x2) → S(NP:x1 VP:x2)
ry: VP(AD:x1 VP(VV:x2 AS:x3))
→ VP(VBZ:x3 ADVP(RB:x1 VBN:x2))
</table>
<tableCaption confidence="0.9808745">
Table 1. Some useful syntactic rules are blocked due to
the spurious link between “了” and “the”.
</tableCaption>
<bodyText confidence="0.998892">
Firstly, The TSAs are constructed in an unsuper-
vised learning manner, and optimized by the trans-
lation model during the forced decoding process,
without using any statistics and linguistic heuristics
or syntactic constraints. Secondly, our approach is
independent of the word alignment-based algo-
rithm used to extract translation rules, and easy to
implement.
</bodyText>
<sectionHeader confidence="0.989602" genericHeader="method">
2 Translation Span Alignment Model
</sectionHeader>
<bodyText confidence="0.999747">
Different from word alignment, TSA is a process
of identifying span-to-span alignments between
parallel sentences. For each translation span pair,
</bodyText>
<figure confidence="0.997453555555556">
Word alignment
Frontier node
NP VP
ADVP
DT
S
NNS
VBZ
RB VBN
the
imports
have
drastically
fallen
进口
jinkou
了
le
VV AS
AD VP
NN
VP
S
大幅度
dafudu
减少
jianshao
</figure>
<page confidence="0.943665">
280
</page>
<note confidence="0.5839225">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 280–284,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<listItem confidence="0.985939">
1. Extract phrase translation rules R from the parallel
corpus with word alignment, and construct a phrase-
based translation model M.
2. Apply M to implement phrase-based forced decoding
on each training sentence pair (c, e), and output its
best derivation d* that can transform c into e.
3. Build a TSA of each sentence pair (c, e) from its best
derivation d*, in which each rule r in d* is used to
form a translation span pair {src(r)&lt;=&gt;tgt(r)}.
</listItem>
<figureCaption confidence="0.994558">
Figure 2. TSA generation algorithm. src(r) and tgt(r)
indicate the source and target side of rule r.
</figureCaption>
<bodyText confidence="0.8847226">
its source (or target) span is a sequence of source
(or target) words. Given a source sentence c=c1...cn,
a target sentence e=e1...em, and its word alignment
A, a translation span pair z is a pair of source span
(ci...cj) and target span (ep...eq)
</bodyText>
<equation confidence="0.986945">
τ=(ci ⇔eq)
p
</equation>
<bodyText confidence="0.9999674">
where z indicates that the source span (ci...cj) and
the target span (ep...eq) are translational equivalent.
We do not require that z must be consistent with
the associated word alignment A in a TSA model.
Figure 2 depicts the TSA generation algorithm
in which a phrase-based forced decoding tech-
nique is adopted to produce the TSA of each sen-
tence pair. In this work, we do not apply syntax-
based forced decoding (e.g., tree-to-string) because
phrase-based models can achieve the state-of-the-
art translation quality with a large amount of train-
ing data, and are not limited by any constituent
boundary based constraints for decoding.
Formally, given a sentence pair (c, e), the
phrase-based forced decoding technique aims to
search for the best derivation d* among all consis-
tent derivations that convert the given source sen-
tence c into the given target sentence e with respect
to the current translation model induced from the
training data, which can be expressed by
</bodyText>
<equation confidence="0.673645">
d * = arg max Prθ (TGT (d) j c) (1)
d∈ D(c,e)∧TGT (d)=e
</equation>
<bodyText confidence="0.99992425">
where D(c,e) is the set of candidate derivations that
transform c to e, and TGT(d) is a function that out-
puts the yield of a derivation d. B indicates parame-
ters of the phrase-based translation model learned
from the parallel corpus.
The best derivation d* produced by forced de-
coding can be viewed as a sequence of translation
steps (i.e., phrase translation rules), expressed by
</bodyText>
<equation confidence="0.715160636363636">
d*=r1⊕r2⊕ ... ⊕rk,
c = 进口 大幅度 减少 了
e = the imports have drastically fallen
The best derivation d* produced by forced decoding:
r1: 进口 → the imports
r2: 大幅度 减少 → drastically fallen
r3: 了 → have
Generating TSA from d*:
[进口]&lt;=&gt;[the imports]
[大幅度 减少]&lt;=&gt;[drastically fallen]
[了]&lt;=&gt;[have]
</equation>
<tableCaption confidence="0.9729195">
Table 2. Forced decoding based TSA generation on the
example sentence pair in Fig. 1.
</tableCaption>
<bodyText confidence="0.9998343125">
where ri indicates a phrase rule used to form d*.
⊕is a composition operation that combines rules
{r1...rk} together to produce the target translation.
As mentioned above, the best derivation d* re-
spects the input sentence pair (c, e). It means that
for each phrase translation rule ri used by d*, its
source (or target) side exactly matches a span of
the given source (or target) sentence. The source
side src(ri) and the target side tgt(ri) of each phrase
translation rule ri in d* form a translation span pair
{src(ri)&lt;=&gt;tgt(ri)} of (c,e). In other words, the
TSA of (c,e) is a set of translation span pairs gen-
erated from phrase translation rules used by the
best derivation d*. The forced decoding based TSA
generation on the example sentence pair in Figure
1 can be shown in Table 2.
</bodyText>
<sectionHeader confidence="0.980139" genericHeader="method">
3 Better Rule Extraction with TSAs
</sectionHeader>
<bodyText confidence="0.944617111111111">
To better understand the particular task that we
will address in this section, we first introduce a
definition of inconsistent with a translation span
alignment. Given a sentence pair (c, e) with the
word alignment A and the translation span align-
ment P, we call a link (ci, ej)∈A inconsistent with
P, if ci and ej are covered respectively by two dif-
ferent translation span pairs in P and vice versa.
(ci, ej)∈A inconsistent with P ⇔
</bodyText>
<equation confidence="0.998784818181818">
∈P : ci ∈ src(τ) ∧ e j ∉ tgt(τ)
OR ∃ ∈
τ
P:
src(
)
e
i∉
τ
∧
∈ tgt(τ) j
</equation>
<bodyText confidence="0.998162">
where src(z) and tgt(z) indicate the source and tar-
get span of a translation span pair z.
By this, we will say that a link
</bodyText>
<subsectionHeader confidence="0.552905">
is a
</subsectionHeader>
<bodyText confidence="0.996731666666667">
spurious link if it is inconsistent with the given
TSA. Table 3 shows that an original link
are covered by two different tran
</bodyText>
<equation confidence="0.734368">
(ci,ej)∈A
(4→1)
</equation>
<bodyText confidence="0.516953">
slation span pairs
∃τ
</bodyText>
<page confidence="0.953371">
281
</page>
<table confidence="0.999879222222222">
Source Target WA TSA
1: An 1: the 1–►2 [1,1]&lt;=&gt;[1,2]
2: )QVMfj 2: imports 2–►4 [2,3]&lt;=&gt;[4,5]
3: �� 3: have 3–►5 [4,4]&lt;=&gt;[3,3]
4: f 4: drastically 4–►1
5: fallen (null)–►3
Method Prec% Rec% F1% Del/Sent Add/Sent
Baseline 83.07 75.75 79.25 - -
TSA 84.01 75.46 79.51 1.5 1.1
</table>
<tableCaption confidence="0.9727134">
Table 4. Word alignment precision, recall and F1-score
of various methods on 200 sentence pairs of Chinese-
English data.
Table 3. A sentence pair with the original word align-
ment (WA) and the translation span alignment (TSA).
</tableCaption>
<bodyText confidence="0.999931666666667">
([4,4]&lt;=&gt;[3,3]) and ([1,1] &lt;=&gt;[1,2]), respectively.
In such a case, we think that this link (4–►1) is a
spurious link according to this TSA, and should be
removed for rule extraction.
Given a resulting TSA P, there are four different
types of translation span pairs, such as one-to-one,
one-to-many, many-to-one, and many-to-many
cases. For example, the TSA shown in Table 3
contains a one-to-one span pair ([4,4]&lt;=&gt;[3,3]), a
one-to-many span pair ([1,1]&lt;=&gt;[1,2]) and a
many-many span pair ([2,3]&lt;=&gt;[4,5]). In such a
case, we can learn a confident link from a one-to-
one translation span pair that is preferred by the
translation model in the forced decoding based
TSA generation approach. If such a confident link
does not exist in the original word alignment, we
consider it as a new valuable link.
Until now, a natural way is to use TSAs to di-
rectly improve word alignment quality by deleting
some spurious links and adding some new confi-
dent links, which in turn improves rule quality and
translation quality. In other words, if a desirable
translation rule was blocked due to some spurious
links, we will output this translation rule. Let’s
revisit the example in Figure 1 again. The blocked
tree-to-string r3 can be extracted successfully after
deleting the spurious link (f, the), and a new tree-
to-string rule r1 can be extracted after adding a new
confident link (f, have) that is inferred from a
one-to-one translation span pair [4,4]&lt;=&gt;[3,3].
</bodyText>
<sectionHeader confidence="0.99995" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996289">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999990970588235">
We utilized a state-of-the-art open-source SMT
system NiuTrans (Xiao et al. 2012) to implement
syntax-based models in the following experiments.
We begin with a training parallel corpus of Chi-
nese-English bitexts that consists of 8.8M Chinese
words and 10.1M English words in 350K sentence
pairs. The GIZA++ tool was used to perform the
bi-directional word alignment between the source
and the target sentences, referred to as the baseline
method. For syntactic translation rule extraction,
minimal GHKM (Galley et al., 2004) rules are first
extracted from the bilingual corpus whose source
and target sides are parsed using the Berkeley
parser (Petrov et al. 2006). The composed rules are
then generated by composing two or three minimal
rules. A 5-gram language model was trained on the
Xinhua portion of English Gigaword corpus. Beam
search and cube pruning techniques (Huang and
Chiang 2007) were used to prune the search space
for all the systems. The base feature set used for all
systems is similar to that used in (Marcu et al.
2006), including 14 base features in total such as 5-
gram language model, bidirectional lexical and
phrase-based translation probabilities. All features
were log-linearly combined and their weights were
optimized by performing minimum error rate train-
ing (MERT) (Och 2003). The development data set
used for weight training comes from NIST MT03
evaluation set, consisting of 326 sentence pairs of
less than 20 words in each Chinese sentence. Two
test sets are NIST MT04 (1788 sentence pairs) and
MT05 (1082 sentence pairs) evaluation sets. The
translation quality is evaluated in terms of the case-
insensitive IBM-BLEU4 metric.
</bodyText>
<subsectionHeader confidence="0.996426">
4.2 Effect on Word Alignment
</subsectionHeader>
<bodyText confidence="0.9999408">
To investigate the effect of the TSA method on
word alignment, we designed an experiment to
evaluate alignment quality against gold standard
annotations. There are 200 random chosen and
manually aligned Chinese-English sentence pairs
used to assert the word alignment quality. For
word alignment evaluation, we calculated precision,
recall and F1-score over gold word alignment.
Table 4 depicts word alignment performance of
the baseline and TSA methods. We apply the TSAs
to refine the baseline word alignments, involving
spurious link deletion and new link insertion op-
erations. Table 4 shows our method can yield im-
provements on precision and F1-score, only
causing a little negative effect on recall.
</bodyText>
<page confidence="0.990131">
282
</page>
<subsectionHeader confidence="0.653733">
4.3 Translation Quality
</subsectionHeader>
<table confidence="0.994568">
Method # of Rules MT03 MT04 MT05
Baseline (T2S) 33,769,071 34.10 32.55 30.15
TSA (T2S) 32,652,261 34.61+ 33.01+ 30.66+
(+0.51) (+0.46) (+0.51)
Baseline (T2T) 24,287,206 34.51 32.20 31.78
34.85 32.92* 32.22+
TSA (T2T) 24,119,719
(+0.34) (+0.72) (+0.44)
</table>
<tableCaption confidence="0.888505">
Table 5. Rule sizes and IBM-BLEU4 (%) scores of
baseline and our method (TSA) in tree-to-string (T2S)
and tree-to-tree (T2T) translation on Dev set (MT03)
and two test sets (MT04 and MT05). + and * indicate
significantly better on performance comparison at p&lt;.05
and p&lt;.01, respectively.
</tableCaption>
<bodyText confidence="0.9819155">
Table 5 depicts effectiveness of our TSA method
on translation quality in tree-to-string and tree-to-
tree translation tasks. Table 5 shows that our TSA
method can improve both syntax-based translation
systems. As mentioned before, the resulting TSAs
are essentially optimized by the translation model.
Based on such TSAs, experiments show that spuri-
ous link deletion and new valuable link insertion
can improve translation quality for tree-to-string
and tree-to-tree systems.
</bodyText>
<sectionHeader confidence="0.999922" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999938980769231">
Previous studies have made great efforts to incor-
porate statistics and linguistic heuristics or syntac-
tic information into word alignments (Ittycheriah
and Roukos 2005; Taskar et al. 2005; Moore et al.
2006; Cherry and Lin 2006; DeNero and Klein
2007; May and Knight 2007; Fossum et al. 2008;
Hermjakob 2009; Liu et al. 2010). For example,
Fossum et al. (2008) used a discriminatively
trained model to identify and delete incorrect links
from original word alignments to improve string-
to-tree transformation rule extraction, which incor-
porates four types of features such as lexical and
syntactic features. This paper presents an approach
to incorporating translation span alignments into
word alignments to delete spurious links and add
new valuable links.
Some previous work directly models the syntac-
tic correspondence in the training data for syntactic
rule extraction (Imamura 2001; Groves et al. 2004;
Tinsley et al. 2007; Sun et al. 2010a, 2010b; Pauls
et al. 2010). Some previous methods infer syntac-
tic correspondences between the source and the
target languages through word alignments and con-
stituent boundary based syntactic constraints. Such
a syntactic alignment method is sensitive to word
alignment behavior. To combat this, Pauls et al.
(2010) presented an unsupervised ITG alignment
model that directly aligns syntactic structures for
string-to-tree transformation rule extraction. One
major problem with syntactic structure alignment
is that syntactic divergence between languages can
prevent accurate syntactic alignments between the
source and target languages.
May and Knight (2007) presented a syntactic re-
alignment model for syntax-based MT that uses
syntactic constraints to re-align a parallel corpus
with word alignments. The motivation behind their
methods is similar to ours. Our work differs from
(May and Knight 2007) in two major respects.
First, the approach proposed by May and Knight
(2007) first utilizes the EM algorithm to obtain
Viterbi derivation trees from derivation forests of
each (tree, string) pair, and then produces Viterbi
alignments based on obtained derivation trees. Our
forced decoding based approach searches for the
best derivation to produce translation span align-
ments that are used to improve the extraction of
translation rules. Translation span alignments are
optimized by the translation model. Secondly, their
models are only applicable for syntax-based sys-
tems while our method can be applied to both
phrase-based and syntax-based translation tasks.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999971444444445">
This paper presents an unsupervised approach to
improving syntactic transformation rule extraction
by deleting spurious links and adding new valuable
links with the help of bilingual translation span
alignments that are built by using a phrase-based
forced decoding technique. In our future work, it is
worth studying how to combine the best of our ap-
proach and discriminative word alignment models
to improve rule extraction for SMT models.
</bodyText>
<sectionHeader confidence="0.998871" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9268715">
This research was supported in part by the National
Science Foundation of China (61073140), the Spe-
cialized Research Fund for the Doctoral Program
of Higher Education (20100042110031) and the
Fundamental Research Funds for the Central Uni-
versities in China.
</bodyText>
<page confidence="0.997779">
283
</page>
<sectionHeader confidence="0.996163" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998464130434783">
Colin Cherry and Dekang Lin. 2006. Soft syntactic con-
straints for word alignment through discriminative
training. In Proc. of ACL.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Proc.
of ACL.
Victoria Fossum, Kevin Knight and Steven Abney.
2008. Using syntax to improve word alignment pre-
cision for syntax-based machine translation. In Proc.
of the Third Workshop on Statistical Machine Trans-
lation, pages 44-52.
Michel Galley, Mark Hopkins, Kevin Knight and Daniel
Marcu. 2004. What&apos;s in a translation rule? In Proc. of
HLT-NAACL 2004, pp273-280.
Declan Groves, Mary Hearne and Andy Way. 2004.
Robust sub-sentential alignment of phrase-structure
trees. In Proc. of COLING, pp1072-1078.
Ulf Hermjakob. 2009. Improved word alignment with
statistics and linguistic heuristics. In Proc. of EMNLP,
pp229-237
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proc. of ACL, pp144-151.
Kenji Imamura. 2001. Hierarchical Phrase Alignment
Harmonized with Parsing. In Proc. of NLPRS,
pp377-384.
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for Arabic-English ma-
chine translation. In Proc. of HLT/EMNLP.
Yang Liu, Qun Liu and Shouxun Lin. 2010. Discrimina-
tive word alignment by linear modeling. Computa-
tional Linguistics, 36(3):303-339
Daniel Marcu, Wei Wang, Abdessamad Echihabi and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proc. of EMNLP, pp44-52.
Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In Proc. of
EMNLP-CoNLL.
Robert C. Moore, Wen-tau Yih and Andreas Bode. 2006.
Improved discriminative bilingual word alignment.
In Proc. of ACL
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL.
Adam Pauls, Dan Klein, David Chiang and Kevin
Knight. 2010. Unsupervised syntactic alignment with
inversion transduction grammars. In Proc. of NAACL,
pp118-126
Slav Petrov, Leon Barrett, Roman Thibaux and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of ACL, pp433-440.
Jun Sun, Min Zhang and Chew Lim Tan. 2010a. Explor-
ing Syntactic Structural Features for Sub-Tree
Alignment Using Bilingual Tree Kernels. In Proc. of
ACL, pp306-315.
Jun Sun, Min Zhang and Chew Lim Tan. 2010b. Dis-
criminative Induction of Sub-Tree Alignment using
Limited Labeled Data. In Proc. of COLING, pp1047-
1055.
Ben Taskar, Simon Lacoste-Julien and Dan Klein. 2005.
A discriminative matching approach to word align-
ment. In Proc. of HLT/EMNLP
John Tinsley, Ventsislav Zhechev, Mary Hearne and
Andy Way. 2007. Robust language pair-independent
sub-tree alignment. In Proc. of MT Summit XI.
Tong Xiao, Jingbo Zhu, Hao Zhang and Qiang Li. 2012.
NiuTrans: An Open Source Toolkit for Phrase-based
and Syntax-based Machine Translation. In Proceed-
ings of ACL, demonstration session
</reference>
<page confidence="0.998386">
284
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.398471">
<title confidence="0.843427333333333">Learning Better Rule Extraction with Translation Span Alignment Jingbo Zhu Tong Xiao Chunliang Natural Language Processing</title>
<address confidence="0.53937">Northeastern University, Shenyang, China</address>
<email confidence="0.969286">zhujingbo@mail.neu.edu.cn</email>
<email confidence="0.969286">xiaotong@mail.neu.edu.cn</email>
<email confidence="0.969286">zhangcl@mail.neu.edu.cn</email>
<abstract confidence="0.998386909090909">This paper presents an unsupervised apto learning span alignparallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences. Experiments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Soft syntactic constraints for word alignment through discriminative training.</title>
<date>2006</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1388" citStr="Cherry and Lin 2006" startWordPosition="182" endWordPosition="185">es on the source/target side to learn syntactic transformation rules from parallel data. The approach suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extraction by deleting spurious links and adding new valuable links based on bilingual translation span correspondences. The proposed approach has two promising properties. Figure 1. A real example of Chinese-English sentence pair with word alignment and both-side parse</context>
<context position="13150" citStr="Cherry and Lin 2006" startWordPosition="2122" endWordPosition="2125">totree translation tasks. Table 5 shows that our TSA method can improve both syntax-based translation systems. As mentioned before, the resulting TSAs are essentially optimized by the translation model. Based on such TSAs, experiments show that spurious link deletion and new valuable link insertion can improve translation quality for tree-to-string and tree-to-tree systems. 5 Related Work Previous studies have made great efforts to incorporate statistics and linguistic heuristics or syntactic information into word alignments (Ittycheriah and Roukos 2005; Taskar et al. 2005; Moore et al. 2006; Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). For example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in th</context>
</contexts>
<marker>Cherry, Lin, 2006</marker>
<rawString>Colin Cherry and Dekang Lin. 2006. Soft syntactic constraints for word alignment through discriminative training. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Tailoring word alignments to syntactic machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1411" citStr="DeNero and Klein 2007" startWordPosition="186" endWordPosition="189">et side to learn syntactic transformation rules from parallel data. The approach suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extraction by deleting spurious links and adding new valuable links based on bilingual translation span correspondences. The proposed approach has two promising properties. Figure 1. A real example of Chinese-English sentence pair with word alignment and both-side parse trees. Some blocked Tr</context>
<context position="13173" citStr="DeNero and Klein 2007" startWordPosition="2126" endWordPosition="2129">sks. Table 5 shows that our TSA method can improve both syntax-based translation systems. As mentioned before, the resulting TSAs are essentially optimized by the translation model. Based on such TSAs, experiments show that spurious link deletion and new valuable link insertion can improve translation quality for tree-to-string and tree-to-tree systems. 5 Related Work Previous studies have made great efforts to incorporate statistics and linguistic heuristics or syntactic information into word alignments (Ittycheriah and Roukos 2005; Taskar et al. 2005; Moore et al. 2006; Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). For example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syn</context>
</contexts>
<marker>DeNero, Klein, 2007</marker>
<rawString>John DeNero and Dan Klein. 2007. Tailoring word alignments to syntactic machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
<author>Steven Abney</author>
</authors>
<title>Using syntax to improve word alignment precision for syntax-based machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>44--52</pages>
<contexts>
<context position="1148" citStr="Fossum et al. 2008" startWordPosition="147" endWordPosition="150">glish translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation. 1 Introduction Most syntax-based statistical machine translation (SMT) systems typically utilize word alignments and parse trees on the source/target side to learn syntactic transformation rules from parallel data. The approach suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extraction by deleting spuriou</context>
<context position="13214" citStr="Fossum et al. 2008" startWordPosition="2134" endWordPosition="2137">mprove both syntax-based translation systems. As mentioned before, the resulting TSAs are essentially optimized by the translation model. Based on such TSAs, experiments show that spurious link deletion and new valuable link insertion can improve translation quality for tree-to-string and tree-to-tree systems. 5 Related Work Previous studies have made great efforts to incorporate statistics and linguistic heuristics or syntactic information into word alignments (Ittycheriah and Roukos 2005; Taskar et al. 2005; Moore et al. 2006; Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). For example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syntactic rule extraction (Imamura 2001; Gro</context>
</contexts>
<marker>Fossum, Knight, Abney, 2008</marker>
<rawString>Victoria Fossum, Kevin Knight and Steven Abney. 2008. Using syntax to improve word alignment precision for syntax-based machine translation. In Proc. of the Third Workshop on Statistical Machine Translation, pages 44-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What&apos;s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<pages>273--280</pages>
<contexts>
<context position="9992" citStr="Galley et al., 2004" startWordPosition="1630" endWordPosition="1633">hat is inferred from a one-to-one translation span pair [4,4]&lt;=&gt;[3,3]. 4 Experiments 4.1 Setup We utilized a state-of-the-art open-source SMT system NiuTrans (Xiao et al. 2012) to implement syntax-based models in the following experiments. We begin with a training parallel corpus of Chinese-English bitexts that consists of 8.8M Chinese words and 10.1M English words in 350K sentence pairs. The GIZA++ tool was used to perform the bi-directional word alignment between the source and the target sentences, referred to as the baseline method. For syntactic translation rule extraction, minimal GHKM (Galley et al., 2004) rules are first extracted from the bilingual corpus whose source and target sides are parsed using the Berkeley parser (Petrov et al. 2006). The composed rules are then generated by composing two or three minimal rules. A 5-gram language model was trained on the Xinhua portion of English Gigaword corpus. Beam search and cube pruning techniques (Huang and Chiang 2007) were used to prune the search space for all the systems. The base feature set used for all systems is similar to that used in (Marcu et al. 2006), including 14 base features in total such as 5- gram language model, bidirectional </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight and Daniel Marcu. 2004. What&apos;s in a translation rule? In Proc. of HLT-NAACL 2004, pp273-280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Declan Groves</author>
<author>Mary Hearne</author>
<author>Andy Way</author>
</authors>
<title>Robust sub-sentential alignment of phrase-structure trees.</title>
<date>2004</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>1072--1078</pages>
<contexts>
<context position="13829" citStr="Groves et al. 2004" startWordPosition="2226" endWordPosition="2229">008; Hermjakob 2009; Liu et al. 2010). For example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syntactic rule extraction (Imamura 2001; Groves et al. 2004; Tinsley et al. 2007; Sun et al. 2010a, 2010b; Pauls et al. 2010). Some previous methods infer syntactic correspondences between the source and the target languages through word alignments and constituent boundary based syntactic constraints. Such a syntactic alignment method is sensitive to word alignment behavior. To combat this, Pauls et al. (2010) presented an unsupervised ITG alignment model that directly aligns syntactic structures for string-to-tree transformation rule extraction. One major problem with syntactic structure alignment is that syntactic divergence between languages can pr</context>
</contexts>
<marker>Groves, Hearne, Way, 2004</marker>
<rawString>Declan Groves, Mary Hearne and Andy Way. 2004. Robust sub-sentential alignment of phrase-structure trees. In Proc. of COLING, pp1072-1078.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Hermjakob</author>
</authors>
<title>Improved word alignment with statistics and linguistic heuristics.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>229--237</pages>
<contexts>
<context position="1468" citStr="Hermjakob 2009" startWordPosition="198" endWordPosition="199">ta. The approach suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extraction by deleting spurious links and adding new valuable links based on bilingual translation span correspondences. The proposed approach has two promising properties. Figure 1. A real example of Chinese-English sentence pair with word alignment and both-side parse trees. Some blocked Tree-to-string Rules: r1: AS(了) → have r2: NN(进口) → the imp</context>
<context position="13230" citStr="Hermjakob 2009" startWordPosition="2138" endWordPosition="2139">ased translation systems. As mentioned before, the resulting TSAs are essentially optimized by the translation model. Based on such TSAs, experiments show that spurious link deletion and new valuable link insertion can improve translation quality for tree-to-string and tree-to-tree systems. 5 Related Work Previous studies have made great efforts to incorporate statistics and linguistic heuristics or syntactic information into word alignments (Ittycheriah and Roukos 2005; Taskar et al. 2005; Moore et al. 2006; Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). For example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syntactic rule extraction (Imamura 2001; Groves et al. 2004;</context>
</contexts>
<marker>Hermjakob, 2009</marker>
<rawString>Ulf Hermjakob. 2009. Improved word alignment with statistics and linguistic heuristics. In Proc. of EMNLP, pp229-237</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="10362" citStr="Huang and Chiang 2007" startWordPosition="1690" endWordPosition="1693">in 350K sentence pairs. The GIZA++ tool was used to perform the bi-directional word alignment between the source and the target sentences, referred to as the baseline method. For syntactic translation rule extraction, minimal GHKM (Galley et al., 2004) rules are first extracted from the bilingual corpus whose source and target sides are parsed using the Berkeley parser (Petrov et al. 2006). The composed rules are then generated by composing two or three minimal rules. A 5-gram language model was trained on the Xinhua portion of English Gigaword corpus. Beam search and cube pruning techniques (Huang and Chiang 2007) were used to prune the search space for all the systems. The base feature set used for all systems is similar to that used in (Marcu et al. 2006), including 14 base features in total such as 5- gram language model, bidirectional lexical and phrase-based translation probabilities. All features were log-linearly combined and their weights were optimized by performing minimum error rate training (MERT) (Och 2003). The development data set used for weight training comes from NIST MT03 evaluation set, consisting of 326 sentence pairs of less than 20 words in each Chinese sentence. Two test sets ar</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proc. of ACL, pp144-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Imamura</author>
</authors>
<title>Hierarchical Phrase Alignment Harmonized with Parsing.</title>
<date>2001</date>
<booktitle>In Proc. of NLPRS,</booktitle>
<pages>377--384</pages>
<contexts>
<context position="13809" citStr="Imamura 2001" startWordPosition="2224" endWordPosition="2225">ossum et al. 2008; Hermjakob 2009; Liu et al. 2010). For example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syntactic rule extraction (Imamura 2001; Groves et al. 2004; Tinsley et al. 2007; Sun et al. 2010a, 2010b; Pauls et al. 2010). Some previous methods infer syntactic correspondences between the source and the target languages through word alignments and constituent boundary based syntactic constraints. Such a syntactic alignment method is sensitive to word alignment behavior. To combat this, Pauls et al. (2010) presented an unsupervised ITG alignment model that directly aligns syntactic structures for string-to-tree transformation rule extraction. One major problem with syntactic structure alignment is that syntactic divergence betw</context>
</contexts>
<marker>Imamura, 2001</marker>
<rawString>Kenji Imamura. 2001. Hierarchical Phrase Alignment Harmonized with Parsing. In Proc. of NLPRS, pp377-384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy word aligner for Arabic-English machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP.</booktitle>
<contexts>
<context position="13090" citStr="Ittycheriah and Roukos 2005" startWordPosition="2110" endWordPosition="2113">of our TSA method on translation quality in tree-to-string and tree-totree translation tasks. Table 5 shows that our TSA method can improve both syntax-based translation systems. As mentioned before, the resulting TSAs are essentially optimized by the translation model. Based on such TSAs, experiments show that spurious link deletion and new valuable link insertion can improve translation quality for tree-to-string and tree-to-tree systems. 5 Related Work Previous studies have made great efforts to incorporate statistics and linguistic heuristics or syntactic information into word alignments (Ittycheriah and Roukos 2005; Taskar et al. 2005; Moore et al. 2006; Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). For example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some prev</context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2005. A maximum entropy word aligner for Arabic-English machine translation. In Proc. of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Discriminative word alignment by linear modeling.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<pages>36--3</pages>
<contexts>
<context position="1486" citStr="Liu et al. 2010" startWordPosition="200" endWordPosition="203"> suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extraction by deleting spurious links and adding new valuable links based on bilingual translation span correspondences. The proposed approach has two promising properties. Figure 1. A real example of Chinese-English sentence pair with word alignment and both-side parse trees. Some blocked Tree-to-string Rules: r1: AS(了) → have r2: NN(进口) → the imports r3: S (NN:x1 </context>
<context position="13248" citStr="Liu et al. 2010" startWordPosition="2140" endWordPosition="2143"> systems. As mentioned before, the resulting TSAs are essentially optimized by the translation model. Based on such TSAs, experiments show that spurious link deletion and new valuable link insertion can improve translation quality for tree-to-string and tree-to-tree systems. 5 Related Work Previous studies have made great efforts to incorporate statistics and linguistic heuristics or syntactic information into word alignments (Ittycheriah and Roukos 2005; Taskar et al. 2005; Moore et al. 2006; Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). For example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syntactic rule extraction (Imamura 2001; Groves et al. 2004; Tinsley et al. 20</context>
</contexts>
<marker>Liu, Liu, Lin, 2010</marker>
<rawString>Yang Liu, Qun Liu and Shouxun Lin. 2010. Discriminative word alignment by linear modeling. Computational Linguistics, 36(3):303-339</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>44--52</pages>
<contexts>
<context position="10508" citStr="Marcu et al. 2006" startWordPosition="1719" endWordPosition="1722"> as the baseline method. For syntactic translation rule extraction, minimal GHKM (Galley et al., 2004) rules are first extracted from the bilingual corpus whose source and target sides are parsed using the Berkeley parser (Petrov et al. 2006). The composed rules are then generated by composing two or three minimal rules. A 5-gram language model was trained on the Xinhua portion of English Gigaword corpus. Beam search and cube pruning techniques (Huang and Chiang 2007) were used to prune the search space for all the systems. The base feature set used for all systems is similar to that used in (Marcu et al. 2006), including 14 base features in total such as 5- gram language model, bidirectional lexical and phrase-based translation probabilities. All features were log-linearly combined and their weights were optimized by performing minimum error rate training (MERT) (Och 2003). The development data set used for weight training comes from NIST MT03 evaluation set, consisting of 326 sentence pairs of less than 20 words in each Chinese sentence. Two test sets are NIST MT04 (1788 sentence pairs) and MT05 (1082 sentence pairs) evaluation sets. The translation quality is evaluated in terms of the caseinsensi</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proc. of EMNLP, pp44-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
<author>Kevin Knight</author>
</authors>
<title>Syntactic realignment models for machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1127" citStr="May and Knight 2007" startWordPosition="143" endWordPosition="146">riments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation. 1 Introduction Most syntax-based statistical machine translation (SMT) systems typically utilize word alignments and parse trees on the source/target side to learn syntactic transformation rules from parallel data. The approach suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extractio</context>
<context position="13194" citStr="May and Knight 2007" startWordPosition="2130" endWordPosition="2133"> our TSA method can improve both syntax-based translation systems. As mentioned before, the resulting TSAs are essentially optimized by the translation model. Based on such TSAs, experiments show that spurious link deletion and new valuable link insertion can improve translation quality for tree-to-string and tree-to-tree systems. 5 Related Work Previous studies have made great efforts to incorporate statistics and linguistic heuristics or syntactic information into word alignments (Ittycheriah and Roukos 2005; Taskar et al. 2005; Moore et al. 2006; Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). For example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syntactic rule extractio</context>
<context position="14527" citStr="May and Knight (2007)" startWordPosition="2325" endWordPosition="2328">ious methods infer syntactic correspondences between the source and the target languages through word alignments and constituent boundary based syntactic constraints. Such a syntactic alignment method is sensitive to word alignment behavior. To combat this, Pauls et al. (2010) presented an unsupervised ITG alignment model that directly aligns syntactic structures for string-to-tree transformation rule extraction. One major problem with syntactic structure alignment is that syntactic divergence between languages can prevent accurate syntactic alignments between the source and target languages. May and Knight (2007) presented a syntactic realignment model for syntax-based MT that uses syntactic constraints to re-align a parallel corpus with word alignments. The motivation behind their methods is similar to ours. Our work differs from (May and Knight 2007) in two major respects. First, the approach proposed by May and Knight (2007) first utilizes the EM algorithm to obtain Viterbi derivation trees from derivation forests of each (tree, string) pair, and then produces Viterbi alignments based on obtained derivation trees. Our forced decoding based approach searches for the best derivation to produce transl</context>
</contexts>
<marker>May, Knight, 2007</marker>
<rawString>Jonathan May and Kevin Knight. 2007. Syntactic realignment models for machine translation. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Wen-tau Yih</author>
<author>Andreas Bode</author>
</authors>
<title>Improved discriminative bilingual word alignment.</title>
<date>2006</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="13129" citStr="Moore et al. 2006" startWordPosition="2118" endWordPosition="2121">to-string and tree-totree translation tasks. Table 5 shows that our TSA method can improve both syntax-based translation systems. As mentioned before, the resulting TSAs are essentially optimized by the translation model. Based on such TSAs, experiments show that spurious link deletion and new valuable link insertion can improve translation quality for tree-to-string and tree-to-tree systems. 5 Related Work Previous studies have made great efforts to incorporate statistics and linguistic heuristics or syntactic information into word alignments (Ittycheriah and Roukos 2005; Taskar et al. 2005; Moore et al. 2006; Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). For example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic</context>
</contexts>
<marker>Moore, Yih, Bode, 2006</marker>
<rawString>Robert C. Moore, Wen-tau Yih and Andreas Bode. 2006. Improved discriminative bilingual word alignment. In Proc. of ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="10776" citStr="Och 2003" startWordPosition="1759" endWordPosition="1760">ated by composing two or three minimal rules. A 5-gram language model was trained on the Xinhua portion of English Gigaword corpus. Beam search and cube pruning techniques (Huang and Chiang 2007) were used to prune the search space for all the systems. The base feature set used for all systems is similar to that used in (Marcu et al. 2006), including 14 base features in total such as 5- gram language model, bidirectional lexical and phrase-based translation probabilities. All features were log-linearly combined and their weights were optimized by performing minimum error rate training (MERT) (Och 2003). The development data set used for weight training comes from NIST MT03 evaluation set, consisting of 326 sentence pairs of less than 20 words in each Chinese sentence. Two test sets are NIST MT04 (1788 sentence pairs) and MT05 (1082 sentence pairs) evaluation sets. The translation quality is evaluated in terms of the caseinsensitive IBM-BLEU4 metric. 4.2 Effect on Word Alignment To investigate the effect of the TSA method on word alignment, we designed an experiment to evaluate alignment quality against gold standard annotations. There are 200 random chosen and manually aligned Chinese-Engli</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
<author>David Chiang</author>
<author>Kevin Knight</author>
</authors>
<title>Unsupervised syntactic alignment with inversion transduction grammars.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>118--126</pages>
<contexts>
<context position="13895" citStr="Pauls et al. 2010" startWordPosition="2239" endWordPosition="2242">(2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syntactic rule extraction (Imamura 2001; Groves et al. 2004; Tinsley et al. 2007; Sun et al. 2010a, 2010b; Pauls et al. 2010). Some previous methods infer syntactic correspondences between the source and the target languages through word alignments and constituent boundary based syntactic constraints. Such a syntactic alignment method is sensitive to word alignment behavior. To combat this, Pauls et al. (2010) presented an unsupervised ITG alignment model that directly aligns syntactic structures for string-to-tree transformation rule extraction. One major problem with syntactic structure alignment is that syntactic divergence between languages can prevent accurate syntactic alignments between the source and target </context>
</contexts>
<marker>Pauls, Klein, Chiang, Knight, 2010</marker>
<rawString>Adam Pauls, Dan Klein, David Chiang and Kevin Knight. 2010. Unsupervised syntactic alignment with inversion transduction grammars. In Proc. of NAACL, pp118-126</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Roman Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>433--440</pages>
<contexts>
<context position="10132" citStr="Petrov et al. 2006" startWordPosition="1653" endWordPosition="1656"> system NiuTrans (Xiao et al. 2012) to implement syntax-based models in the following experiments. We begin with a training parallel corpus of Chinese-English bitexts that consists of 8.8M Chinese words and 10.1M English words in 350K sentence pairs. The GIZA++ tool was used to perform the bi-directional word alignment between the source and the target sentences, referred to as the baseline method. For syntactic translation rule extraction, minimal GHKM (Galley et al., 2004) rules are first extracted from the bilingual corpus whose source and target sides are parsed using the Berkeley parser (Petrov et al. 2006). The composed rules are then generated by composing two or three minimal rules. A 5-gram language model was trained on the Xinhua portion of English Gigaword corpus. Beam search and cube pruning techniques (Huang and Chiang 2007) were used to prune the search space for all the systems. The base feature set used for all systems is similar to that used in (Marcu et al. 2006), including 14 base features in total such as 5- gram language model, bidirectional lexical and phrase-based translation probabilities. All features were log-linearly combined and their weights were optimized by performing m</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Roman Thibaux and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. of ACL, pp433-440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Sun</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
</authors>
<title>Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels.</title>
<date>2010</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>306--315</pages>
<contexts>
<context position="13867" citStr="Sun et al. 2010" startWordPosition="2234" endWordPosition="2237">r example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syntactic rule extraction (Imamura 2001; Groves et al. 2004; Tinsley et al. 2007; Sun et al. 2010a, 2010b; Pauls et al. 2010). Some previous methods infer syntactic correspondences between the source and the target languages through word alignments and constituent boundary based syntactic constraints. Such a syntactic alignment method is sensitive to word alignment behavior. To combat this, Pauls et al. (2010) presented an unsupervised ITG alignment model that directly aligns syntactic structures for string-to-tree transformation rule extraction. One major problem with syntactic structure alignment is that syntactic divergence between languages can prevent accurate syntactic alignments be</context>
</contexts>
<marker>Sun, Zhang, Tan, 2010</marker>
<rawString>Jun Sun, Min Zhang and Chew Lim Tan. 2010a. Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels. In Proc. of ACL, pp306-315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Sun</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
</authors>
<title>Discriminative Induction of Sub-Tree Alignment using Limited Labeled Data.</title>
<date>2010</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>1047--1055</pages>
<contexts>
<context position="13867" citStr="Sun et al. 2010" startWordPosition="2234" endWordPosition="2237">r example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syntactic rule extraction (Imamura 2001; Groves et al. 2004; Tinsley et al. 2007; Sun et al. 2010a, 2010b; Pauls et al. 2010). Some previous methods infer syntactic correspondences between the source and the target languages through word alignments and constituent boundary based syntactic constraints. Such a syntactic alignment method is sensitive to word alignment behavior. To combat this, Pauls et al. (2010) presented an unsupervised ITG alignment model that directly aligns syntactic structures for string-to-tree transformation rule extraction. One major problem with syntactic structure alignment is that syntactic divergence between languages can prevent accurate syntactic alignments be</context>
</contexts>
<marker>Sun, Zhang, Tan, 2010</marker>
<rawString>Jun Sun, Min Zhang and Chew Lim Tan. 2010b. Discriminative Induction of Sub-Tree Alignment using Limited Labeled Data. In Proc. of COLING, pp1047-1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simon Lacoste-Julien</author>
<author>Dan Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP</booktitle>
<contexts>
<context position="13110" citStr="Taskar et al. 2005" startWordPosition="2114" endWordPosition="2117">ion quality in tree-to-string and tree-totree translation tasks. Table 5 shows that our TSA method can improve both syntax-based translation systems. As mentioned before, the resulting TSAs are essentially optimized by the translation model. Based on such TSAs, experiments show that spurious link deletion and new valuable link insertion can improve translation quality for tree-to-string and tree-to-tree systems. 5 Related Work Previous studies have made great efforts to incorporate statistics and linguistic heuristics or syntactic information into word alignments (Ittycheriah and Roukos 2005; Taskar et al. 2005; Moore et al. 2006; Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). For example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly m</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>Ben Taskar, Simon Lacoste-Julien and Dan Klein. 2005. A discriminative matching approach to word alignment. In Proc. of HLT/EMNLP</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Tinsley</author>
<author>Ventsislav Zhechev</author>
<author>Mary Hearne</author>
<author>Andy Way</author>
</authors>
<title>Robust language pair-independent sub-tree alignment.</title>
<date>2007</date>
<booktitle>In Proc. of MT</booktitle>
<location>Summit XI.</location>
<contexts>
<context position="13850" citStr="Tinsley et al. 2007" startWordPosition="2230" endWordPosition="2233"> Liu et al. 2010). For example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syntactic rule extraction (Imamura 2001; Groves et al. 2004; Tinsley et al. 2007; Sun et al. 2010a, 2010b; Pauls et al. 2010). Some previous methods infer syntactic correspondences between the source and the target languages through word alignments and constituent boundary based syntactic constraints. Such a syntactic alignment method is sensitive to word alignment behavior. To combat this, Pauls et al. (2010) presented an unsupervised ITG alignment model that directly aligns syntactic structures for string-to-tree transformation rule extraction. One major problem with syntactic structure alignment is that syntactic divergence between languages can prevent accurate syntac</context>
</contexts>
<marker>Tinsley, Zhechev, Hearne, Way, 2007</marker>
<rawString>John Tinsley, Ventsislav Zhechev, Mary Hearne and Andy Way. 2007. Robust language pair-independent sub-tree alignment. In Proc. of MT Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Xiao</author>
<author>Jingbo Zhu</author>
<author>Hao Zhang</author>
<author>Qiang Li</author>
</authors>
<title>NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL, demonstration session</booktitle>
<contexts>
<context position="9548" citStr="Xiao et al. 2012" startWordPosition="1562" endWordPosition="1565">dent links, which in turn improves rule quality and translation quality. In other words, if a desirable translation rule was blocked due to some spurious links, we will output this translation rule. Let’s revisit the example in Figure 1 again. The blocked tree-to-string r3 can be extracted successfully after deleting the spurious link (f, the), and a new treeto-string rule r1 can be extracted after adding a new confident link (f, have) that is inferred from a one-to-one translation span pair [4,4]&lt;=&gt;[3,3]. 4 Experiments 4.1 Setup We utilized a state-of-the-art open-source SMT system NiuTrans (Xiao et al. 2012) to implement syntax-based models in the following experiments. We begin with a training parallel corpus of Chinese-English bitexts that consists of 8.8M Chinese words and 10.1M English words in 350K sentence pairs. The GIZA++ tool was used to perform the bi-directional word alignment between the source and the target sentences, referred to as the baseline method. For syntactic translation rule extraction, minimal GHKM (Galley et al., 2004) rules are first extracted from the bilingual corpus whose source and target sides are parsed using the Berkeley parser (Petrov et al. 2006). The composed r</context>
</contexts>
<marker>Xiao, Zhu, Zhang, Li, 2012</marker>
<rawString>Tong Xiao, Jingbo Zhu, Hao Zhang and Qiang Li. 2012. NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation. In Proceedings of ACL, demonstration session</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>