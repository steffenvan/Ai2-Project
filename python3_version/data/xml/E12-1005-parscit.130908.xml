<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007155">
<title confidence="0.998199">
Evaluating Distributional Models of Semantics for Syntactically
Invariant Inference
</title>
<author confidence="0.961391">
Jackie CK Cheung and Gerald Penn
</author>
<affiliation confidence="0.9980835">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.984811">
Toronto, ON, M5S 3G4, Canada
</address>
<email confidence="0.999587">
{jcheung,gpenn}@cs.toronto.edu
</email>
<sectionHeader confidence="0.993897" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999534">
A major focus of current work in distri-
butional models of semantics is to con-
struct phrase representations composition-
ally from word representations. However,
the syntactic contexts which are modelled
are usually severely limited, a fact which
is reflected in the lexical-level WSD-like
evaluation methods used. In this paper, we
broaden the scope of these models to build
sentence-level representations, and argue
that phrase representations are best eval-
uated in terms of the inference decisions
that they support, invariant to the partic-
ular syntactic constructions used to guide
composition. We propose two evaluation
methods in relation classification and QA
which reflect these goals, and apply several
recent compositional distributional models
to the tasks. We find that the models out-
perform a simple lemma overlap baseline
slightly, demonstrating that distributional
approaches can already be useful for tasks
requiring deeper inference.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983510204082">
A number of unsupervised semantic models
(Mitchell and Lapata, 2008, for example) have re-
cently been proposed which are inspired at least
in part by the distributional hypothesis (Harris,
1954)—that a word’s meaning can be character-
ized by the contexts in which it appears. Such
models represent word meaning as one or more
high-dimensional vectors which capture the lex-
ical and syntactic contexts of the word’s occur-
rences in a training corpus.
Much of the recent work in this area has, fol-
lowing Mitchell and Lapata (2008), focused on
the notion of compositionality as the litmus test of
a truly semantic model. Compositionality is a nat-
ural way to construct representations of linguistic
units larger than a word, and it has a long history
in Montagovian semantics for dealing with argu-
ment structure and assembling rich semantical ex-
pressions of the kind found in predicate logic.
While compositionality may thus provide a
convenient recipe for producing representations
of propositionally typed phrases, it is not a nec-
essary condition for a semantic representation.
Rather, that distinction still belongs to the crucial
ability to support inference. It is not the inten-
tion of this paper to argue for or against composi-
tionality in semantic representations. Rather, our
interest is in evaluating semantic models in order
to determine their suitability for inference tasks.
In particular, we contend that it is desirable and
arguably necessary for a compositional semantic
representation to support inference invariantly, in
the sense that the particular syntactic construction
that guided the composition should not matter rel-
ative to the representations of syntactically differ-
ent phrases with the same meanings. For example,
we can assert that John threw the ball and The ball
was thrown by John have the same meaning for
the purposes of inference, even though they differ
syntactically.
An analogy can be drawn to research in image
processing, in which it is widely regarded as im-
portant for the representations of images to be in-
variant to rotation and scaling. What we should
want is a representation of sentence meaning that
is invariant to diathesis, other regular syntactic al-
ternations in the assignment of argument struc-
ture, and, ideally, even invariant to other meaning-
preserving or near-preserving paraphrases.
</bodyText>
<page confidence="0.990407">
33
</page>
<note confidence="0.977042">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 33–43,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9993816">
Existing evaluations of distributional semantic
models fall short of measuring this. One evalua-
tion approach consists of lexical-level word sub-
stitution tasks which primarily evaluate a sys-
tem’s ability to disambiguate word senses within a
controlled syntactic environment (McCarthy and
Navigli, 2009, for example). Another approach is
to evaluate parsing accuracy (Socher et al., 2010,
for example), which is really a formalism-specific
approximation to argument structure analysis.
These evaluations may certainly be relevant to
specific components of, for example, machine
translation or natural language generation sys-
tems, but they tell us little about a semantic
model’s ability to support inference.
In this paper, we propose a general framework
for evaluating distributional semantic models that
build sentence representations, and suggest two
evaluation methods that test the notion of struc-
turally invariant inference directly. Both rely on
determining whether sentences express the same
semantic relation between entities, a crucial step
in solving a wide variety of inference tasks like
recognizing textual entailment, information re-
trieval, question answering, and summarization.
The first evaluation is a relation classification
task, where a semantic model is tested on its abil-
ity to recognize whether a pair of sentences both
contain a particular semantic relation, such as
Company X acquires Company Y. The second task
is a question answering task, the goal of which is
to locate the sentence in a document that contains
the answer. Here, the semantic model must match
the question, which expresses a proposition with a
missing argument, to the answer-bearing sentence
which contains the full proposition.
We apply these new evaluation protocols to
several recent distributional models, extending
several of them to build sentence representa-
tions. We find that the models outperform a sim-
ple lemma overlap model only slightly, but that
combining these models with the lemma overlap
model can improve performance. This result is
likely due to weaknesses in current models’ abil-
ity to deal with issues such as named entities,
coreference, and negation, which are not empha-
sized by existing evaluation methods, but it does
suggest that distributional models of semantics
can play a more central role in systems that re-
quire deep, precise inference.
</bodyText>
<sectionHeader confidence="0.795594" genericHeader="introduction">
2 Compositionality and Distributional
Semantics
</sectionHeader>
<bodyText confidence="0.942656765957447">
The idea of compositionality has been central to
understanding contemporary natural language se-
mantics from an historiographic perspective. The
idea is often credited to Frege, although in fact
Frege had very little to say about compositional-
ity that had not already been repeated since the
time of Aristotle (Hodges, 2005). Our modern
notion of compositionality took shape primarily
with the work of Tarski (1956), who was actu-
ally arguing that a central difference between for-
mal languages and natural languages is that nat-
ural language is not compositional. This in turn
was the “the contention that an important theo-
retical difference exists between formal and nat-
ural languages,” that Richard Montague so fa-
mously rejected (Montague, 1974). Composi-
tionality also features prominently in Fodor and
Pylyshyn’s (1988) rejection of early connection-
ist representations of natural language semantics,
which seems to have influenced Mitchell and La-
pata (2008) as well.
Logic-based forms of compositional semantics
have long strived for syntactic invariance in mean-
ing representations, which is known as the doc-
trine of the canonical form. The traditional justifi-
cation for canonical forms is that they allow easy
access to a knowledge base to retrieve some de-
sired information, which amounts to a form of in-
ference. Our work can be seen as an extension of
this notion to distributional semantic models with
a more general notion of representational similar-
ity and inference.
There are many regular alternations that seman-
tics models have tried to account for such as pas-
sive or dative alternations. There are also many
lexical paraphrases which can take drastically dif-
ferent syntactic forms. Take the following exam-
ple from Poon and Domingos (2009), in which the
same semantic relation can be expressed by a tran-
sitive verb or an attributive prepositional phrase:
(1) Utah borders Idaho.
Utah is next to Idaho.
In distributional semantics, the original sen-
tence similarity test proposed by Kintsch (2001)
served as the inspiration for the evaluation per-
formed by Mitchell and Lapata (2008) and most
later work in the area. Intransitive verbs are given
</bodyText>
<page confidence="0.998734">
34
</page>
<bodyText confidence="0.999930953488372">
in the context of their syntactic subject, and can-
didate synonyms are ranked for their appropri-
ateness. This method targets the fact that a syn-
onym is appropriate for only some of the verb’s
senses, and the intended verb sense depends on
the surrounding context. For example, burn and
beam are both synonyms of glow, but given a par-
ticular subject, one of the synonyms (called the
High similarity landmark) may be a more appro-
priate substitution than the other (the Low similar-
ity landmark). So, if the fire is the subject, glowed
is the High similarity landmark, and beamed the
Low similarity landmark.
Fundamentally, this method was designed as
a demonstration that compositionality in com-
puting phrasal semantic representations does not
interfere with the ability of a representation to
synthesize non-compositional collocation effects
that contribute to the disambiguation of homo-
graphs. Here, word-sense disambiguation is im-
plicitly viewed as a very restricted, highly lexi-
calized case of inference for selecting the appro-
priate disjunct in the representation of a word’s
meaning.
Kintsch (2001) was interested in sentence sim-
ilarity, but he only conducted his evaluation on
a few hand-selected examples. Mitchell and La-
pata (2008) conducted theirs on a much larger
scale, but chose to focus only on this single case
of syntactic combination, intransitive verbs and
their subjects, in order to “factor out inessential
degrees of freedom” to compare their various al-
ternative models more equitably. This was not
necessary—using the same, sufficiently large, un-
biased but syntactically heterogeneous sample of
evaluation sentences would have served as an ade-
quate control—and this decision furthermore pre-
vents the evaluation from testing the desired in-
variance of the semantic representation.
Other lexical evaluations suffer from the same
problem. One uses the WordSim-353 dataset
(Finkelstein et al., 2002), which contains hu-
man word pair similarity judgments that seman-
tic models should reproduce. However, the word
pairs are given without context, and homography
is unaddressed. Also, it is unclear how reliable
the similarity scores are, as different annotators
may interpret the integer scale of similarity scores
differently. Recent work uses this dataset mostly
for parameter tuning. Another is the lexical para-
phrase task of McCarthy and Navigli (2009), in
which words are given in the context of the sur-
rounding sentence, and the task is to rank a given
list of proposed substitutions for that word. The
list of substitutions as well as the correct rankings
are elicited from annotators. This task was origi-
nally conceived as an applied evaluation of WSD
systems, not an evaluation of phrase representa-
tions.
Parsing accuracy has been used as a prelimi-
nary evaluation of semantic models that produce
syntactic structure (Socher et al., 2010; Wu and
Schuler, 2011). However, syntax does not always
reflect semantic content, and we are specifically
interested in supporting syntactic invariance when
doing semantic inference. Also, this type of eval-
uation is tied to a particular grammar formalism.
The existing evaluations that are most similar in
spirit to what we propose are paraphrase detection
tasks that do not assume a restricted syntactic con-
text. Washtell (2011) collected human judgments
on the general meaning similarity of candidate
phrase pairs. Unfortunately, no additional guid-
ance on the definition of “most similar in mean-
ing” was provided, and it appears likely that sub-
jects conflated lexical, syntactic, and semantic re-
latedness. Dolan and Brockett (2005) define para-
phrase detection as identifying sentences that are
in a bidirectional entailment relation. While such
sentences do support exactly the same inferences,
we are also interested in the inferences that can
be made from similar sentences that are not para-
phrases according to this strict definition — a sit-
uation that is more often encountered in end ap-
plications. Thus, we adopt a less restricted notion
of paraphrasis.
</bodyText>
<sectionHeader confidence="0.960206" genericHeader="method">
3 An Evaluation Framework
</sectionHeader>
<bodyText confidence="0.999956214285714">
We now describe a simple, general framework
for evaluating semantic models. Our framework
consists of the following components: a seman-
tic model to be evaluated, pairs of sentences that
are considered to have high similarity, and pairs
of sentences that are considered to have low simi-
larity.
In particular, the semantic model is a binary
function, s = M(x, x′), which returns a real-
valued similarity score, s, given a pair of arbitrary
linguistic units (that is, words, phrases, sentences,
etc.), x and x′. Note that this formulation of the
semantic model is agnostic to whether the models
use compositionality to build a phrase represen-
</bodyText>
<page confidence="0.995944">
35
</page>
<bodyText confidence="0.999689">
tation from constituent representations, and even
to the actual representation used. The model is
tested by applying it to each element in the fol-
lowing two sets:
</bodyText>
<equation confidence="0.906426714285714">
H = {(h, h′)|h and h′ are linguistic units (2)
with high similarity}
L = {(l, l′)|l and l′ are linguistic units (3)
with low similarity}
The resulting sets of similarity scores are:
SH = JM(h, h′)|(h, h′) E H} (4)
SL = JM(l, l′)|(l, l′) E L} (5)
</equation>
<bodyText confidence="0.999974125">
The semantic model is evaluated according to
its ability to separate SH and SL. We will de-
fine specific measures of separation for the tasks
that we propose shortly. While the particular def-
initions of “high similarity” and “low similarity”
depend on the task, at the crux of both our evalu-
ations is that two sentences are similar if they ex-
press the same semantic relation between a given
entity pair, and dissimilar otherwise. This thresh-
old for similarity is closely tied to the argument
structure of the sentence, and allows considerable
flexibility in the other semantic content that may
be contained in the sentence, unlike the bidirec-
tional paraphrase detection task. Yet it ensures
that a consistent and useful distinction for infer-
ence is being detected, unlike unconstrained sim-
ilarity judgments.
Also, compared to word similarity assessments
or paraphrase elicitation, determining whether a
sentence expresses a semantic relation is a much
easier task cognitively for human judges. This bi-
nary judgment does not involve interpreting a nu-
merical scale or coming up with an open-ended
set of alternative paraphrases. It is thus easier to
get reliable annotated data.
Below, we present two tasks that instantiate
this evaluation framework and choice of similar-
ity threshold. They differ in that the first is tar-
geted towards recognizing declarative sentences
or phrases, while the second is targeted towards a
question answering scenario, where one argument
in the semantic relation is queried.
</bodyText>
<subsectionHeader confidence="0.998855">
3.1 Task 1: Relation Classification
</subsectionHeader>
<bodyText confidence="0.998948352941177">
The first task is a relation classification task. Rela-
tion extraction and recognition are central to a va-
riety of other tasks, such as information retrieval,
ontology construction, recognizing textual entail-
ment and question answering.
In this task, the high and the low similarity sen-
tence pairs are constructed in the following man-
ner. First, a target semantic relation, such as Com-
pany X acquires Company Y is chosen, and enti-
ties are chosen for each slot in the relation, such as
Company X=Pfizer and Company Y=Rinat Neu-
roscience. Then, sentences containing these enti-
ties are extracted and divided into two subsets. In
one of them, E, the entities are in the target se-
mantic relation, while in the other, NE, they are
not. The evaluation sets H and L are then con-
structed as follows:
</bodyText>
<equation confidence="0.9999965">
H = E x E \ {(e,e)|e E E} (6)
L = E x NE (7)
</equation>
<bodyText confidence="0.9999496875">
In other words, the high similarity sentence
pairs are all the pairs where both express the tar-
get semantic relation, except the pairs between a
sentence and itself, while the low similarity pairs
are all the pairs where exactly one of the two sen-
tences expresses the target relation.
Several sentences expressing the relation Pfizer
acquires Rinat Neuroscience are shown in Exam-
ples 8 to 10. These sentences illustrate the amount
of syntactic and lexical variation that the semantic
model must recognize as expressing the same se-
mantic relation. In particular, besides recognizing
synonymy or near-synonymy at the lexical level,
models must also account for subcategorization
differences, extra arguments or adjuncts, and part-
of-speech differences due to nominalization.
</bodyText>
<listItem confidence="0.997850363636363">
(8) Pfizer buys Rinat Neuroscience to extend
neuroscience research and in doing so
acquires a product candidate for OA.
(lexical difference)
(9) A month earlier, Pfizer paid an estimated
several hundred million dollars for biotech
firm Rinat Neuroscience. (extra argument,
subcategorization)
(10) Pfizer to Expand Neuroscience Research
With Acquisition of Biotech Company Rinat
Neuroscience (nominalization)
</listItem>
<bodyText confidence="0.991158">
Since our interest is to measure the models’
ability to separate SH and SL in an unsuper-
vised setting, standard supervised classification
accuracy is not applicable. Instead, we employ
</bodyText>
<page confidence="0.991661">
36
</page>
<bodyText confidence="0.999962461538461">
the area under a ROC curve (AUC), which does
not depend on choosing an arbitrary classification
threshold. A ROC curve is a plot of the true pos-
itive versus false positive rate of a binary classi-
fier as the classification threshold is varied. The
area under a ROC curve can thus be seen as the
performance of linear classifiers over the scores
produced by the semantic model. The AUC can
also be interpreted as the probability that a ran-
domly chosen positive instance will have a higher
similarity score than a randomly chosen negative
instance. A random classifier is expected to have
an AUC of 0.5.
</bodyText>
<subsectionHeader confidence="0.994255">
3.2 Task 2: Restricted QA
</subsectionHeader>
<bodyText confidence="0.9988095">
The second task that we propose is a restricted
form of question answering. In this task, the sys-
tem is given a question q and a document D con-
sisting of a list of sentences, in which one of the
sentences contains the answer to the question. We
define:
</bodyText>
<equation confidence="0.999347">
H = {(q, d)|d E D and d answers q} (11)
L = {(q, d)|d E D and d does not answer q}
(12)
</equation>
<bodyText confidence="0.999950166666667">
In other words, the sentences are divided into two
subsets; those that contain the answer to q should
be similar to q, while those that do not should be
dissimilar. We also assume that only one sentence
in each document contains the answer, so H con-
tains only one sentence.
Unrestricted question answering is a difficult
problem that forces a semantic representation to
deal sensibly with a number of other semantic is-
sues such as coreference and information aggre-
gation which still seem to be out of reach for
contemporary distributional models of meaning.
Since our focus in this work is on argument struc-
ture semantics, we restrict the question-answer
pairs to those that only require dealing with para-
phrases of this type.
To do so, we semi-automatically restrict the
question-answer pairs by using the output of an
unsupervised clustering semantic parser (Poon
and Domingos, 2009). The semantic parser clus-
ters semantic sub-expressions derived from a de-
pendency parse of the sentence, so that those sub-
expressions that express the same semantic re-
lations are clustered. The parser is used to an-
swer questions, and the output of the parser is
manually checked. We use only those cases that
have thus been determined to be correct question-
answer pairs. As a result of this restriction, this
task is rather more like Task 1 in how it tests a
model’s ability to recognize lexical and syntac-
tic paraphrases. This task also involves recog-
nizing voicing alternations, which were automati-
cally extracted by the semantic parser.
An example of a question-answer pair involv-
ing a voicing alternation that is used in this task is
presented in Example 13.
</bodyText>
<listItem confidence="0.935816">
(13) Q: What does il-2 activate?
A: PI3K
Sentence: Phosphatidyl inositol 3-kinase
(PI3K) is activated by IL-2.
</listItem>
<bodyText confidence="0.9986318">
Since there is only one element in H and hence
SH for each question and document, we measure
the separation between SH and SL using the rank
of the score of answer-bearing sentence among
the scores of all the sentences in the document.
We normalize the rank so that it is between 0
(ranked least similar) and 1 (ranked most simi-
lar). Where ties occur, the sentence is ranked as
if it were in the median position among the tied
sentences. If the question-answer pairs are zero-
indexed by i, answer(i) is the index of the sen-
tence containing the answer for the ith pair, and
length(i) is the number of sentences in the doc-
ument, then the mean normalized rank score of a
system is:
</bodyText>
<equation confidence="0.881612333333333">
r answer(i) ( )
1 − length(i) − 11
14
</equation>
<sectionHeader confidence="0.999284" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999804">
We drew a number of recent distributional seman-
tic models to compare in this paper. We first de-
scribe the models and our reimplementation of
them, before describing the tasks and the datasets
used in detail and the results.
</bodyText>
<subsectionHeader confidence="0.979992">
4.1 Distributional Semantic Models
</subsectionHeader>
<bodyText confidence="0.972845111111111">
We tested four recent distributional models and a
lemma overlap baseline, which we now describe.
We extended several of the models to compo-
sitionally construct phrase representations using
component-wise vector addition and multiplica-
tion, as we note below. Since the focus of this pa-
per is on evaluation methods for such models, we
did not experiment with other compositionality
norm rank = E
</bodyText>
<equation confidence="0.659469">
Z
</equation>
<page confidence="0.98493">
37
</page>
<bodyText confidence="0.9994505625">
operators. We do note, however, that component-
wise operators have been popular in recent liter-
ature, and have been applied across unrestricted
syntactic contexts (Mitchell and Lapata, 2009),
so there is value in evaluating the performance of
these operators in itself. The models were trained
on the Gigaword corpus (2nd ed., ~2.3B words).
All models use cosine similarity to measure the
similarity between representations, except for the
baseline model.
Lemma Overlap This baseline simply repre-
sents a sentence as the counts of each lemma
present in the sentence after removing stop
words. Let a sentence x consist of lemma-tokens
m1, ... , m|x|. The similarity between two sen-
tences is then defined as
</bodyText>
<equation confidence="0.999528666666667">
M(x, x′) = #In(x, x′) + #In(x′, x) (15)
#In(x, x′) = � |x |1x′(mi) (16)
i=1
</equation>
<bodyText confidence="0.999779448275862">
where 1x′(mi) is an indicator function that returns
1 if mi E x′, and 0 otherwise. This definition
accounts for multiple occurrences of a lemma.
M&amp;L Mitchell and Lapata (2008) propose a
framework for compositional distributional se-
mantics using a standard term-context vector
space word representation. A phrase is repre-
sented as a vector of context-word counts (actu-
ally, pmi-scaled values), which is derived compo-
sitionally by a function over constituent vectors,
such as component-wise addition or multiplica-
tion. This model ignores syntactic relations and
is insensitive to word-order.
E&amp;P Erk and Pad´o (2008) introduce a struc-
tured vector space model which uses syntactic de-
pendencies to model the selectional preferences
of words. The vector representation of a word in
context depends on the inverse selectional prefer-
ences of its dependents, and the selectional pref-
erences of its head. For example, suppose catch
occurs with a dependent ball in a direct object
relation. The vector for catch would then be in-
fluenced by the inverse direct object preferences
of ball (e.g. throw, organize), and the vector for
ball would be influenced by the selectional pref-
erences of catch (e.g. cold, drift). More formally,
given words a and b in a dependency relation r,
a distributional representation of a, va, the repre-
sentation of a in context, a′, is given by
</bodyText>
<equation confidence="0.967800333333333">
a′ = va O Rb(r−1) (17)
�Rb(r) = f(c, r, b) · vc, (18)
c:f(c,r,b)&gt;θ
</equation>
<bodyText confidence="0.9999075">
where Rb(r) is the vector describing the selec-
tional preference of word b in relation r, f(c, r, b)
is the frequency of this dependency triple, θ is a
frequency threshold to weed out uncommon de-
pendency triples (10 in our experiments), and O
is a vector combination operator, here component-
wise multiplication. We extend the model to com-
pute sentence representations from the contextu-
alized word vectors using component-wise addi-
tion and multiplication.
TFP Thater et al. (2010)’s model is also sensi-
tive to selectional preferences, but to two degrees.
For example, the vector for catch might contain
a dimension labelled (OBJ,OBJ-1,throw),
which indicates the strength of connection be-
tween the two verbs through all of the co-
occurring direct objects which they share. Unlike
E&amp;P, TFP’s model encodes the selectional prefer-
ences in a single vector using frequency counts.
We extend the model to the sentence level with
component-wise addition and multiplication, and
word vectors are contextualized by the depen-
dency neighbours. We use a frequency threshold
of 10 and a pmi threshold of 2 to prune infrequent
word and dependencies.
D&amp;L Dinu and Lapata (2010) (D&amp;L) assume
a global set of latent senses for all words, and
models each word as a mixture over these latent
senses. The vector for a word ti in the context of
a word cj is modelled by
</bodyText>
<equation confidence="0.998303">
v(ti,cj) = P(z1|ti,cj),...P(zK|ti,cj) (19)
</equation>
<bodyText confidence="0.999979">
where z1...K are the latent senses. By mak-
ing independence assumptions and decomposing
probabilities, training becomes a matter of esti-
mating the probability distributions P(zk|ti) and
P(cj|zk) from data. While Dinu and Lapata
(2010) describe two methods to do so, based
on non-negative matrix factorization and latent
Dirichlet allocation, the performances are similar,
so we tested only the latent Dirichlet allocation
method. Like the two previous models, we ex-
tend the model to build sentence representations
</bodyText>
<page confidence="0.99775">
38
</page>
<table confidence="0.98938375">
Pfizer/Rinat N. Yahoo/Inktomi Besson/Paris Antoinette/Vienna
Overlap 0.7393 0.6007 0.7395 0.8914
Models trained on the entire GigaWord
M&amp;L add 0.6196 0.5387 0.5259 0.7275
M&amp;L mult 0.9036 0.6099 0.6443 0.8467
D&amp;L add 0.9214 0.8168 0.6989 0.8932
D&amp;L mult 0.7732 0.6734 0.6527 0.7659
Models trained on the AFP section
E&amp;P add 0.7536 0.4933 0.2780 0.6408
E&amp;P mult 0.5268 0.5328 0.5252 0.8421
TFP add 0.4357 0.5325 0.8725 0.7183
TFP mult 0.5554 0.5524 0.7283 0.6917
M&amp;L add 0.5643 0.5504 0.4594 0.7640
M&amp;L mult 0.8679 0.6324 0.4356 0.8258
D&amp;L add 0.8143 0.9062 0.6373 0.8664
D&amp;L mult 0.8429 0.7461 0.645 0.5948
</table>
<figure confidence="0.954103785714286">
Average
0.7427
0.6029
0.7511
0.8326
0.7163
0.5414
0.6067
0.6398
0.6320
0.5845
0.6904
0.8061
0.7072
</figure>
<tableCaption confidence="0.996457">
Table 1: Task 1 results in AUC scores. The values in bold indicate the best performing model for a particular
training corpus. The expected random baseline performance is 0.5.
</tableCaption>
<table confidence="0.999827285714286">
Entities: {X, Y} + N
Relation: acquires
{Pfizer, Rinat Neuroscience} 41 50
{Yahoo, Inktomi} 115 433
Relation: was born in
{Luc Besson, Paris} 6 126
{Marie Antoinette, Vienna} 39 105
</table>
<tableCaption confidence="0.964015333333333">
Table 2: Task 1 dataset characteristics. N is the total
number of sentences. + is the number of sentences
that express the relation.
</tableCaption>
<bodyText confidence="0.936102666666667">
from the contextualized representations. We set
the number of latent senses to 1200, and train for
600 Gibbs sampling iterations.
</bodyText>
<subsectionHeader confidence="0.998276">
4.2 Training and Parameter Settings
</subsectionHeader>
<bodyText confidence="0.999994826086956">
We reimplemented these four models, following
the parameter settings described by previous work
where possible, though we also aimed for consis-
tency in parameter settings between models (for
example, in the number of context words). For the
non-baseline models, we followed previous work
and model only the 30000 most frequent lemmata.
Context vectors are constructed using a symmet-
ric window of 5 words, and their dimensions rep-
resent the 3000 most frequent lemmatized context
words excluding stop words. Due to resource lim-
itations, we trained the syntactic models over the
AFP subset of Gigaword (~338M words). We also
trained the other two models on just the AFP por-
tion for comparison. Note that the AFP portion
of Gigaword is three times larger than the BNC
corpus (~100M words), on which several previ-
ous syntactic models were trained. Because our
main goal is to test the general performance of the
models and to demonstrate the feasibility of our
evaluation methods, we did not further tune the
parameter settings to each of the tasks, as doing
so would likely only yield minor improvements.
</bodyText>
<subsectionHeader confidence="0.996309">
4.3 Task 1
</subsectionHeader>
<bodyText confidence="0.9999883">
We used the dataset by Bunescu and Mooney
(2007), which we selected because it contains
multiple realizations of an entity pair in a target
semantic relation, unlike similar datasets such as
the one by Roth and Yih (2002). Controlling for
the target entity pair in this manner makes the task
more difficult, because the semantic model cannot
make use of distributional information about the
entity pair in inference. The dataset is separated
into subsets depending on the target binary rela-
tion (Company X acquires Company Y or Person
X was born in Place Y) and the entity pair (e.g.,
Yahoo and Inktomi) (Table 2).
The dataset was constructed semi-
automatically using a Google search for the
two entities in order with up to seven content
words in between. Then, the extracted sentences
were hand-labelled with whether they express the
target relation. Because the order of the entities
has been fixed, passive alternations do not appear
</bodyText>
<page confidence="0.997474">
39
</page>
<table confidence="0.999930823529412">
Pure models Mixed models
All Subset All Subset
Overlap 0.8770 0.7291 0.8770 0.7291
Models trained on the entire GigaWord
M&amp;L add 0.7467 0.6106 0.8782 0.7523
M&amp;L mult 0.5331 0.5690 0.8841 0.7678
D&amp;L add 0.6552 0.5716 0.8791 0.7539
D&amp;L mult 0.5488 0.5255 0.8841 0.7466
Models trained on the AFP section
E&amp;P add 0.4589 0.4516 0.8748 0.7375
E&amp;P mult 0.5201 0.5584 0.8882 0.7719
TFP add 0.6887 0.6443 0.8940 0.7871
TFP mult 0.5210 0.5199 0.8785 0.7432
M&amp;L add 0.7588 0.6206 0.8710 0.7371
M&amp;L mult 0.5710 0.5540 0.8801 0.7540
D&amp;L add 0.6358 0.5402 0.8713 0.7305
D&amp;L mult 0.5647 0.5461 0.8856 0.7683
</table>
<tableCaption confidence="0.733149666666667">
Table 3: Task 2 results, in normalized rank scores.
Subset is the cases where lemma overlap does not
achieve a perfect score. The two columns on the right
indicate performance using the sum of the scores from
the lemma overlap and the semantic model. The ex-
pected random baseline performance is 0.5.
</tableCaption>
<bodyText confidence="0.9997750625">
in this dataset.
The results for Task 1 indicate that the D&amp;L ad-
dition model performs the best (Table 1), though
the lemma overlap model presents a surprisingly
strong baseline. The syntax-modulated E&amp;P and
TFP models perform poorly on this task, even
when compared to the other models trained on the
AFP subset. The M&amp;L multiplication model out-
performs the addition model, a result which cor-
roborates previous findings on the lexical substi-
tution task. The same does not hold in the D&amp;L
latent sense space. Overall, some of the datasets
(Yahoo and Antoinette) appear to be easier for the
models than others (Pfizer and Besson), but more
entity pairs and relations would be needed to in-
vestigate the models’ variance across datasets.
</bodyText>
<subsectionHeader confidence="0.981863">
4.4 Task 2
</subsectionHeader>
<bodyText confidence="0.999985466666667">
We used the question-answer pairs extracted by
the Poon and Domingos (2009) semantic parser
from the GENIA biomedical corpus that have
been manually checked to be correct (295 pairs).
Because our models were trained on newspaper
text, they required adaptation to this specialized
domain. Thus, we also trained the M&amp;L, E&amp;P
and TFP models on the GENIA corpus, back-
ing off to word vectors from the GENIA corpus
when a word vector could not be found in the
Gigaword-trained model. We could not do this
for the D&amp;L model, since the global latent senses
that are found by latent Dirichlet allocation train-
ing do not have any absolute meaning that holds
across multiple runs. Instead, we found the 5
words in the Gigaword-trained D&amp;L model that
were closest to each novel word in the GENIA
corpus according to cosine similarity over the co-
occurrence vectors of the words in the GENIA
corpus, and took their average latent sense distri-
butions as the vector for that word.
Unlike in Task 1, there is no control for the
named entities in a sentence, because one of the
entities in the semantic relation is missing. Also,
distributional models have problems in dealing
with named entities which are common in this
corpus, such as the names of genes and proteins.
To address these issues, we tested hybrid models
where the similarity score from a semantic model
is added to the similarity score from the lemma
overlap model.
The results are presented in Table 3. Lemma
overlap again presents a strong baseline, but the
hybridized models are able to outperform simple
lemma overlap. Unlike in Task 1, the E&amp;P and
TFP models are comparable to the D&amp;L model,
and the mixed TFP addition model achieves the
best result, likely due to the need to more pre-
cisely distinguish syntactic roles in this task. The
D&amp;L addition model, which achieved the best
performance in Task 1, does not perform as well
in this task. This could be due to the domain adap-
tation procedure for the D&amp;L model, which could
not be reasonably trained on such a small, special-
ized corpus.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999976333333333">
Turney and Pantel (2010) survey various types of
vector space models and applications thereof in
computational linguistics. We summarize below
a number of other word- or phrase-level distribu-
tional models.
Several approaches are specialized to deal with
homography. The top-down multi-prototype ap-
proach determines a number of senses for each
word, and then clusters the occurrences of the
word (Reisinger and Mooney, 2010) into these
senses. A prototype vector is created for each
of these sense clusters. When a new occurrence
</bodyText>
<page confidence="0.994413">
40
</page>
<bodyText confidence="0.999956744186047">
of a word is encountered, it is represented as a
combination of the prototype vectors, with the de-
gree of influence from each prototype determined
by the similarity of the new context to the exist-
ing sense contexts. In contrast, the bottom-up ex-
emplar-based approach assumes that each occur-
rence of a word expresses a different sense of the
word. The most similar senses of the word are ac-
tivated when a new occurrence of it is encountered
and combined, for example with a kNN algorithm
(Erk and Pad´o, 2010).
The models we compared and the above work
assume each dimension in the feature vector cor-
responds to a context word. In contrast, Washtell
(2011) uses potential paraphrases directly as di-
mensions in his expectation vectors. Unfortu-
nately, this approach does not outperform vari-
ous context word-based approaches in two phrase
similarity tasks.
In terms of the vector composition function,
component-wise addition and multiplication are
the most popular in recent work, but there ex-
ist a number of other operators such as tensor
product and convolution product, which are re-
viewed by Widdows (2008). Instead of vector
space representations, one could also use a matrix
space representation with its much more expres-
sive matrix operators (Rudolph and Giesbrecht,
2010). So far, however, this has only been ap-
plied to specific syntactic contexts (Baroni and
Zamparelli, 2010; Guevara, 2010; Grefenstette
and Sadrzadeh, 2011), or tasks (Yessenalina and
Cardie, 2011).
Neural networks have been used to learn both
phrase structure and representations. In Socher et
al. (2010), word representations learned by neu-
ral network models such as (Bengio et al., 2006;
Collobert and Weston, 2008) are fed as input into
a recursive neural network whose nodes represent
syntactic constituents. Each node models both the
probability of the input forming a constituent and
the phrase representation resulting from composi-
tion.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999986263157895">
We have proposed an evaluation framework for
distributional models of semantics which build
phrase- and sentence-level representations, and
instantiated two evaluation tasks which test for
the crucial ability to recognize whether sen-
tences express the same semantic relation. Our
results demonstrate that compositional distribu-
tional models of semantics already have some
utility in the context of more empirically complex
semantic tasks than WSD-like lexical substitution
tasks, in which compositional invariance is a req-
uisite property. Simply computing lemma over-
lap, however, is a very competitive baseline, due
to issues in these protocols with named entities
and domain adaptivity. The better performance
of the mixture models in Task 2 shows that such
weaknesses can be addressed by hybrid seman-
tic models. Future work should investigate more
refined versions of such hybridization, as well as
extend this idea to other semantic phenomena like
coreference, negation and modality.
We also observe that no single model or com-
position operator performs best for all tasks and
datasets. The latent sense mixture model of Dinu
and Lapata (2010) performs well in recognizing
semantic relations in general web text. Because
of the difficulty of adapting it to a specialized
domain, however, it does less well in biomedi-
cal question answering, where the syntax-based
model of Thater et al. (2010) performs the best.
A more thorough investigation of the factors that
can predict the performance and/or invariance of
a given composition operator is warranted.
In the future, we would like to evaluate other
models of compositional semantics that have been
recently proposed. We would also like to collect
more comprehensive test data, to increase the ex-
ternal validity of our evaluations.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999818571428571">
We would like to thank Georgiana Dinu and Ste-
fan Thater for help with reimplementing their
models. Saif Mohammad, Peter Turney, and
the anonymous reviewers provided valuable com-
ments on drafts of this paper. This project was
supported by the Natural Sciences and Engineer-
ing Research Council of Canada.
</bodyText>
<sectionHeader confidence="0.998971" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994593375">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183–1193.
Yoshua Bengio, Holger Schwenk, Jean-S´ebastien
Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain.
</reference>
<page confidence="0.995112">
41
</page>
<reference confidence="0.999650684210526">
2006. Neural probabilistic language models. In-
novations in Machine Learning, pages 137–186.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 576–583.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, page 160–167.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 1162–1172.
William B. Dolan and Chris Brockett. 2005. Auto-
matically constructing a corpus of sentential para-
phrases. In Proceedings of the Third International
Workshop on Paraphrasing, pages 9–16.
Katrin Erk and Sebastian Pad´o. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 897–
906.
Katrin Erk and Sebastian Pad´o. 2010. Exemplar-
based models for word meaning in context. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 92–97.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116–131.
Jerry A. Fodor and Zenon W. Pylyshyn. 1988. Con-
nectionism and cognitive architecture: A critical
analysis. Cognition, 28:3–71.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011. Experimental support for a categorical com-
positional distributional model of meaning. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
1394–1404.
Emiliano Guevara. 2010. A regression model
of adjective-noun compositionality in distributional
semantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, pages 33–37.
Zeller S. Harris. 1954. Distributional structure. Word,
10(23):146–162.
Wilfred Hodges. 2005. The interplay of fact and the-
ory in separating syntax from meaning. In Work-
shop on Empirical Challenges and Analytical Al-
ternatives to Strict Compositionality.
Walter Kintsch. 2001. Predication. Cognitive Sci-
ence, 25(2):173–202.
Diana McCarthy and Roberto Navigli. 2009. The en-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139–159.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236–244.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Pro-
ceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
430–439.
Richard Montague. 1974. English as a formal lan-
guage. Formal Philosophy, pages 188–221.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1–10.
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word
meaning. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
Dan Roth and Wen-tau Yih. 2002. Probabilistic rea-
soning for entity &amp; relation recognition. In Pro-
ceedings of the 19th International Conference on
Computational Linguistics, pages 835–841.
Sebastian Rudolph and Eugenie Giesbrecht. 2010.
Compositional matrix-space models of language.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
907–916.
Richard Socher, Christopher D. Manning, and An-
drew Y. Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. Proceedings of the Deep Learn-
ing and Unsupervised Feature Learning Workshop
ofNIPS 2010, pages 1–9.
Alfred Tarski. 1956. The concept of truth in formal-
ized languages. Logic, Semantics, Metamathemat-
ics, pages 152–278.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 948–957.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal ofArtificial Intelligence Research,
37:141–188.
Justin Washtell. 2011. Compositional expectation:
A purely distributional model of compositional se-
mantics. In Proceedings of the Ninth International
Conference on Computational Semantics (IWCS
2011), pages 285–294.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Second AAAI Sym-
posium on Quantum Interaction.
</reference>
<page confidence="0.981593">
42
</page>
<reference confidence="0.998481555555556">
Stephen Wu and William Schuler. 2011. Structured
composition of semantic vectors. In Proceedings
of the Ninth International Conference on Computa-
tional Semantics (IWCS 2011), pages 295–304.
Ainur Yessenalina and Claire Cardie. 2011. Com-
positional matrix-space models for sentiment analy-
sis. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing,
pages 172–182.
</reference>
<page confidence="0.999832">
43
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951898">
<title confidence="0.996351">Evaluating Distributional Models of Semantics for Invariant Inference</title>
<author confidence="0.999243">Jackie CK Cheung</author>
<author confidence="0.999243">Gerald</author>
<affiliation confidence="0.999743">Department of Computer University of</affiliation>
<address confidence="0.968895">Toronto, ON, M5S 3G4,</address>
<abstract confidence="0.999601166666667">A major focus of current work in distributional models of semantics is to construct phrase representations compositionally from word representations. However, the syntactic contexts which are modelled are usually severely limited, a fact which is reflected in the lexical-level WSD-like evaluation methods used. In this paper, we broaden the scope of these models to build sentence-level representations, and argue that phrase representations are best evaluated in terms of the inference decisions that they support, invariant to the particular syntactic constructions used to guide composition. We propose two evaluation methods in relation classification and QA which reflect these goals, and apply several recent compositional distributional models to the tasks. We find that the models outperform a simple lemma overlap baseline slightly, demonstrating that distributional approaches can already be useful for tasks requiring deeper inference.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<contexts>
<context position="34393" citStr="Baroni and Zamparelli, 2010" startWordPosition="5577" endWordPosition="5580"> approach does not outperform various context word-based approaches in two phrase similarity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase representation resulting from composition. 6 Conclusions We have proposed an evaluation framework for di</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yoshua Bengio</author>
<author>Holger Schwenk</author>
<author>Jean-S´ebastien Sen´ecal</author>
<author>Fr´ederic Morin</author>
<author>Jean-Luc Gauvain</author>
</authors>
<marker>Bengio, Schwenk, Sen´ecal, Morin, Gauvain, </marker>
<rawString>Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain.</rawString>
</citation>
<citation valid="true">
<title>Neural probabilistic language models.</title>
<date>2006</date>
<booktitle>Innovations in Machine Learning,</booktitle>
<pages>137--186</pages>
<marker>2006</marker>
<rawString>2006. Neural probabilistic language models. Innovations in Machine Learning, pages 137–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>576--583</pages>
<contexts>
<context position="27892" citStr="Bunescu and Mooney (2007)" startWordPosition="4489" endWordPosition="4492">we trained the syntactic models over the AFP subset of Gigaword (~338M words). We also trained the other two models on just the AFP portion for comparison. Note that the AFP portion of Gigaword is three times larger than the BNC corpus (~100M words), on which several previous syntactic models were trained. Because our main goal is to test the general performance of the models and to demonstrate the feasibility of our evaluation methods, we did not further tune the parameter settings to each of the tasks, as doing so would likely only yield minor improvements. 4.3 Task 1 We used the dataset by Bunescu and Mooney (2007), which we selected because it contains multiple realizations of an entity pair in a target semantic relation, unlike similar datasets such as the one by Roth and Yih (2002). Controlling for the target entity pair in this manner makes the task more difficult, because the semantic model cannot make use of distributional information about the entity pair in inference. The dataset is separated into subsets depending on the target binary relation (Company X acquires Company Y or Person X was born in Place Y) and the entity pair (e.g., Yahoo and Inktomi) (Table 2). The dataset was constructed semia</context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2007. Learning to extract relations from the web using minimal supervision. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 576–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="34705" citStr="Collobert and Weston, 2008" startWordPosition="5624" endWordPosition="5627">uct, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase representation resulting from composition. 6 Conclusions We have proposed an evaluation framework for distributional models of semantics which build phrase- and sentence-level representations, and instantiated two evaluation tasks which test for the crucial ability to recognize whether sentences express the same semantic relation. Our results demonstrate that compositional distributional models of semantics alrea</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, page 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Measuring distributional similarity in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1162--1172</pages>
<contexts>
<context position="24624" citStr="Dinu and Lapata (2010)" startWordPosition="3962" endWordPosition="3965">to two degrees. For example, the vector for catch might contain a dimension labelled (OBJ,OBJ-1,throw), which indicates the strength of connection between the two verbs through all of the cooccurring direct objects which they share. Unlike E&amp;P, TFP’s model encodes the selectional preferences in a single vector using frequency counts. We extend the model to the sentence level with component-wise addition and multiplication, and word vectors are contextualized by the dependency neighbours. We use a frequency threshold of 10 and a pmi threshold of 2 to prune infrequent word and dependencies. D&amp;L Dinu and Lapata (2010) (D&amp;L) assume a global set of latent senses for all words, and models each word as a mixture over these latent senses. The vector for a word ti in the context of a word cj is modelled by v(ti,cj) = P(z1|ti,cj),...P(zK|ti,cj) (19) where z1...K are the latent senses. By making independence assumptions and decomposing probabilities, training becomes a matter of estimating the probability distributions P(zk|ti) and P(cj|zk) from data. While Dinu and Lapata (2010) describe two methods to do so, based on non-negative matrix factorization and latent Dirichlet allocation, the performances are similar,</context>
<context position="36092" citStr="Dinu and Lapata (2010)" startWordPosition="5833" endWordPosition="5836">site property. Simply computing lemma overlap, however, is a very competitive baseline, due to issues in these protocols with named entities and domain adaptivity. The better performance of the mixture models in Task 2 shows that such weaknesses can be addressed by hybrid semantic models. Future work should investigate more refined versions of such hybridization, as well as extend this idea to other semantic phenomena like coreference, negation and modality. We also observe that no single model or composition operator performs best for all tasks and datasets. The latent sense mixture model of Dinu and Lapata (2010) performs well in recognizing semantic relations in general web text. Because of the difficulty of adapting it to a specialized domain, however, it does less well in biomedical question answering, where the syntax-based model of Thater et al. (2010) performs the best. A more thorough investigation of the factors that can predict the performance and/or invariance of a given composition operator is warranted. In the future, we would like to evaluate other models of compositional semantics that have been recently proposed. We would also like to collect more comprehensive test data, to increase th</context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Measuring distributional similarity in context. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162–1172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically constructing a corpus of sentential paraphrases.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Workshop on Paraphrasing,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="11897" citStr="Dolan and Brockett (2005)" startWordPosition="1832" endWordPosition="1835">terested in supporting syntactic invariance when doing semantic inference. Also, this type of evaluation is tied to a particular grammar formalism. The existing evaluations that are most similar in spirit to what we propose are paraphrase detection tasks that do not assume a restricted syntactic context. Washtell (2011) collected human judgments on the general meaning similarity of candidate phrase pairs. Unfortunately, no additional guidance on the definition of “most similar in meaning” was provided, and it appears likely that subjects conflated lexical, syntactic, and semantic relatedness. Dolan and Brockett (2005) define paraphrase detection as identifying sentences that are in a bidirectional entailment relation. While such sentences do support exactly the same inferences, we are also interested in the inferences that can be made from similar sentences that are not paraphrases according to this strict definition — a situation that is more often encountered in end applications. Thus, we adopt a less restricted notion of paraphrasis. 3 An Evaluation Framework We now describe a simple, general framework for evaluating semantic models. Our framework consists of the following components: a semantic model t</context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>897--906</pages>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 897– 906.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Exemplarbased models for word meaning in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>92--97</pages>
<marker>Erk, Pad´o, 2010</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2010. Exemplarbased models for word meaning in context. In Proceedings of the ACL 2010 Conference Short Papers, pages 92–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="10221" citStr="Finkelstein et al., 2002" startWordPosition="1568" endWordPosition="1571"> to focus only on this single case of syntactic combination, intransitive verbs and their subjects, in order to “factor out inessential degrees of freedom” to compare their various alternative models more equitably. This was not necessary—using the same, sufficiently large, unbiased but syntactically heterogeneous sample of evaluation sentences would have served as an adequate control—and this decision furthermore prevents the evaluation from testing the desired invariance of the semantic representation. Other lexical evaluations suffer from the same problem. One uses the WordSim-353 dataset (Finkelstein et al., 2002), which contains human word pair similarity judgments that semantic models should reproduce. However, the word pairs are given without context, and homography is unaddressed. Also, it is unclear how reliable the similarity scores are, as different annotators may interpret the integer scale of similarity scores differently. Recent work uses this dataset mostly for parameter tuning. Another is the lexical paraphrase task of McCarthy and Navigli (2009), in which words are given in the context of the surrounding sentence, and the task is to rank a given list of proposed substitutions for that word</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry A Fodor</author>
<author>Zenon W Pylyshyn</author>
</authors>
<title>Connectionism and cognitive architecture: A critical analysis.</title>
<date>1988</date>
<journal>Cognition,</journal>
<pages>28--3</pages>
<marker>Fodor, Pylyshyn, 1988</marker>
<rawString>Jerry A. Fodor and Zenon W. Pylyshyn. 1988. Connectionism and cognitive architecture: A critical analysis. Cognition, 28:3–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1394--1404</pages>
<contexts>
<context position="34443" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="5583" endWordPosition="5586">t word-based approaches in two phrase similarity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase representation resulting from composition. 6 Conclusions We have proposed an evaluation framework for distributional models of semantics which build phras</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394–1404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>33--37</pages>
<contexts>
<context position="34408" citStr="Guevara, 2010" startWordPosition="5581" endWordPosition="5582"> various context word-based approaches in two phrase similarity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase representation resulting from composition. 6 Conclusions We have proposed an evaluation framework for distributional mo</context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, pages 33–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zeller S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="1395" citStr="Harris, 1954" startWordPosition="198" endWordPosition="199">r syntactic constructions used to guide composition. We propose two evaluation methods in relation classification and QA which reflect these goals, and apply several recent compositional distributional models to the tasks. We find that the models outperform a simple lemma overlap baseline slightly, demonstrating that distributional approaches can already be useful for tasks requiring deeper inference. 1 Introduction A number of unsupervised semantic models (Mitchell and Lapata, 2008, for example) have recently been proposed which are inspired at least in part by the distributional hypothesis (Harris, 1954)—that a word’s meaning can be characterized by the contexts in which it appears. Such models represent word meaning as one or more high-dimensional vectors which capture the lexical and syntactic contexts of the word’s occurrences in a training corpus. Much of the recent work in this area has, following Mitchell and Lapata (2008), focused on the notion of compositionality as the litmus test of a truly semantic model. Compositionality is a natural way to construct representations of linguistic units larger than a word, and it has a long history in Montagovian semantics for dealing with argument</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zeller S. Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilfred Hodges</author>
</authors>
<title>The interplay of fact and theory in separating syntax from meaning.</title>
<date>2005</date>
<booktitle>In Workshop on Empirical Challenges and Analytical Alternatives to Strict Compositionality.</booktitle>
<contexts>
<context position="6478" citStr="Hodges, 2005" startWordPosition="979" endWordPosition="980"> as named entities, coreference, and negation, which are not emphasized by existing evaluation methods, but it does suggest that distributional models of semantics can play a more central role in systems that require deep, precise inference. 2 Compositionality and Distributional Semantics The idea of compositionality has been central to understanding contemporary natural language semantics from an historiographic perspective. The idea is often credited to Frege, although in fact Frege had very little to say about compositionality that had not already been repeated since the time of Aristotle (Hodges, 2005). Our modern notion of compositionality took shape primarily with the work of Tarski (1956), who was actually arguing that a central difference between formal languages and natural languages is that natural language is not compositional. This in turn was the “the contention that an important theoretical difference exists between formal and natural languages,” that Richard Montague so famously rejected (Montague, 1974). Compositionality also features prominently in Fodor and Pylyshyn’s (1988) rejection of early connectionist representations of natural language semantics, which seems to have inf</context>
</contexts>
<marker>Hodges, 2005</marker>
<rawString>Wilfred Hodges. 2005. The interplay of fact and theory in separating syntax from meaning. In Workshop on Empirical Challenges and Analytical Alternatives to Strict Compositionality.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Kintsch</author>
</authors>
<date>2001</date>
<journal>Predication. Cognitive Science,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="8158" citStr="Kintsch (2001)" startWordPosition="1247" endWordPosition="1248">tic models with a more general notion of representational similarity and inference. There are many regular alternations that semantics models have tried to account for such as passive or dative alternations. There are also many lexical paraphrases which can take drastically different syntactic forms. Take the following example from Poon and Domingos (2009), in which the same semantic relation can be expressed by a transitive verb or an attributive prepositional phrase: (1) Utah borders Idaho. Utah is next to Idaho. In distributional semantics, the original sentence similarity test proposed by Kintsch (2001) served as the inspiration for the evaluation performed by Mitchell and Lapata (2008) and most later work in the area. Intransitive verbs are given 34 in the context of their syntactic subject, and candidate synonyms are ranked for their appropriateness. This method targets the fact that a synonym is appropriate for only some of the verb’s senses, and the intended verb sense depends on the surrounding context. For example, burn and beam are both synonyms of glow, but given a particular subject, one of the synonyms (called the High similarity landmark) may be a more appropriate substitution tha</context>
<context position="9409" citStr="Kintsch (2001)" startWordPosition="1447" endWordPosition="1448">ark). So, if the fire is the subject, glowed is the High similarity landmark, and beamed the Low similarity landmark. Fundamentally, this method was designed as a demonstration that compositionality in computing phrasal semantic representations does not interfere with the ability of a representation to synthesize non-compositional collocation effects that contribute to the disambiguation of homographs. Here, word-sense disambiguation is implicitly viewed as a very restricted, highly lexicalized case of inference for selecting the appropriate disjunct in the representation of a word’s meaning. Kintsch (2001) was interested in sentence similarity, but he only conducted his evaluation on a few hand-selected examples. Mitchell and Lapata (2008) conducted theirs on a much larger scale, but chose to focus only on this single case of syntactic combination, intransitive verbs and their subjects, in order to “factor out inessential degrees of freedom” to compare their various alternative models more equitably. This was not necessary—using the same, sufficiently large, unbiased but syntactically heterogeneous sample of evaluation sentences would have served as an adequate control—and this decision further</context>
</contexts>
<marker>Kintsch, 2001</marker>
<rawString>Walter Kintsch. 2001. Predication. Cognitive Science, 25(2):173–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>The english lexical substitution task.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>2</issue>
<contexts>
<context position="4043" citStr="McCarthy and Navigli, 2009" startWordPosition="608" endWordPosition="611">ent of argument structure, and, ideally, even invariant to other meaningpreserving or near-preserving paraphrases. 33 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 33–43, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics Existing evaluations of distributional semantic models fall short of measuring this. One evaluation approach consists of lexical-level word substitution tasks which primarily evaluate a system’s ability to disambiguate word senses within a controlled syntactic environment (McCarthy and Navigli, 2009, for example). Another approach is to evaluate parsing accuracy (Socher et al., 2010, for example), which is really a formalism-specific approximation to argument structure analysis. These evaluations may certainly be relevant to specific components of, for example, machine translation or natural language generation systems, but they tell us little about a semantic model’s ability to support inference. In this paper, we propose a general framework for evaluating distributional semantic models that build sentence representations, and suggest two evaluation methods that test the notion of struc</context>
<context position="10674" citStr="McCarthy and Navigli (2009)" startWordPosition="1637" endWordPosition="1640">ing the desired invariance of the semantic representation. Other lexical evaluations suffer from the same problem. One uses the WordSim-353 dataset (Finkelstein et al., 2002), which contains human word pair similarity judgments that semantic models should reproduce. However, the word pairs are given without context, and homography is unaddressed. Also, it is unclear how reliable the similarity scores are, as different annotators may interpret the integer scale of similarity scores differently. Recent work uses this dataset mostly for parameter tuning. Another is the lexical paraphrase task of McCarthy and Navigli (2009), in which words are given in the context of the surrounding sentence, and the task is to rank a given list of proposed substitutions for that word. The list of substitutions as well as the correct rankings are elicited from annotators. This task was originally conceived as an applied evaluation of WSD systems, not an evaluation of phrase representations. Parsing accuracy has been used as a preliminary evaluation of semantic models that produce syntactic structure (Socher et al., 2010; Wu and Schuler, 2011). However, syntax does not always reflect semantic content, and we are specifically inte</context>
</contexts>
<marker>McCarthy, Navigli, 2009</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2009. The english lexical substitution task. Language Resources and Evaluation, 43(2):139–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="1269" citStr="Mitchell and Lapata, 2008" startWordPosition="176" endWordPosition="179">and argue that phrase representations are best evaluated in terms of the inference decisions that they support, invariant to the particular syntactic constructions used to guide composition. We propose two evaluation methods in relation classification and QA which reflect these goals, and apply several recent compositional distributional models to the tasks. We find that the models outperform a simple lemma overlap baseline slightly, demonstrating that distributional approaches can already be useful for tasks requiring deeper inference. 1 Introduction A number of unsupervised semantic models (Mitchell and Lapata, 2008, for example) have recently been proposed which are inspired at least in part by the distributional hypothesis (Harris, 1954)—that a word’s meaning can be characterized by the contexts in which it appears. Such models represent word meaning as one or more high-dimensional vectors which capture the lexical and syntactic contexts of the word’s occurrences in a training corpus. Much of the recent work in this area has, following Mitchell and Lapata (2008), focused on the notion of compositionality as the litmus test of a truly semantic model. Compositionality is a natural way to construct repres</context>
<context position="7112" citStr="Mitchell and Lapata (2008)" startWordPosition="1073" endWordPosition="1077">dern notion of compositionality took shape primarily with the work of Tarski (1956), who was actually arguing that a central difference between formal languages and natural languages is that natural language is not compositional. This in turn was the “the contention that an important theoretical difference exists between formal and natural languages,” that Richard Montague so famously rejected (Montague, 1974). Compositionality also features prominently in Fodor and Pylyshyn’s (1988) rejection of early connectionist representations of natural language semantics, which seems to have influenced Mitchell and Lapata (2008) as well. Logic-based forms of compositional semantics have long strived for syntactic invariance in meaning representations, which is known as the doctrine of the canonical form. The traditional justification for canonical forms is that they allow easy access to a knowledge base to retrieve some desired information, which amounts to a form of inference. Our work can be seen as an extension of this notion to distributional semantic models with a more general notion of representational similarity and inference. There are many regular alternations that semantics models have tried to account for </context>
<context position="9545" citStr="Mitchell and Lapata (2008)" startWordPosition="1466" endWordPosition="1470">entally, this method was designed as a demonstration that compositionality in computing phrasal semantic representations does not interfere with the ability of a representation to synthesize non-compositional collocation effects that contribute to the disambiguation of homographs. Here, word-sense disambiguation is implicitly viewed as a very restricted, highly lexicalized case of inference for selecting the appropriate disjunct in the representation of a word’s meaning. Kintsch (2001) was interested in sentence similarity, but he only conducted his evaluation on a few hand-selected examples. Mitchell and Lapata (2008) conducted theirs on a much larger scale, but chose to focus only on this single case of syntactic combination, intransitive verbs and their subjects, in order to “factor out inessential degrees of freedom” to compare their various alternative models more equitably. This was not necessary—using the same, sufficiently large, unbiased but syntactically heterogeneous sample of evaluation sentences would have served as an adequate control—and this decision furthermore prevents the evaluation from testing the desired invariance of the semantic representation. Other lexical evaluations suffer from t</context>
<context position="22212" citStr="Mitchell and Lapata (2008)" startWordPosition="3569" endWordPosition="3572">3B words). All models use cosine similarity to measure the similarity between representations, except for the baseline model. Lemma Overlap This baseline simply represents a sentence as the counts of each lemma present in the sentence after removing stop words. Let a sentence x consist of lemma-tokens m1, ... , m|x|. The similarity between two sentences is then defined as M(x, x′) = #In(x, x′) + #In(x′, x) (15) #In(x, x′) = � |x |1x′(mi) (16) i=1 where 1x′(mi) is an indicator function that returns 1 if mi E x′, and 0 otherwise. This definition accounts for multiple occurrences of a lemma. M&amp;L Mitchell and Lapata (2008) propose a framework for compositional distributional semantics using a standard term-context vector space word representation. A phrase is represented as a vector of context-word counts (actually, pmi-scaled values), which is derived compositionally by a function over constituent vectors, such as component-wise addition or multiplication. This model ignores syntactic relations and is insensitive to word-order. E&amp;P Erk and Pad´o (2008) introduce a structured vector space model which uses syntactic dependencies to model the selectional preferences of words. The vector representation of a word i</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Language models based on semantic composition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>430--439</pages>
<contexts>
<context position="21446" citStr="Mitchell and Lapata, 2009" startWordPosition="3437" endWordPosition="3440">s. 4.1 Distributional Semantic Models We tested four recent distributional models and a lemma overlap baseline, which we now describe. We extended several of the models to compositionally construct phrase representations using component-wise vector addition and multiplication, as we note below. Since the focus of this paper is on evaluation methods for such models, we did not experiment with other compositionality norm rank = E Z 37 operators. We do note, however, that componentwise operators have been popular in recent literature, and have been applied across unrestricted syntactic contexts (Mitchell and Lapata, 2009), so there is value in evaluating the performance of these operators in itself. The models were trained on the Gigaword corpus (2nd ed., ~2.3B words). All models use cosine similarity to measure the similarity between representations, except for the baseline model. Lemma Overlap This baseline simply represents a sentence as the counts of each lemma present in the sentence after removing stop words. Let a sentence x consist of lemma-tokens m1, ... , m|x|. The similarity between two sentences is then defined as M(x, x′) = #In(x, x′) + #In(x′, x) (15) #In(x, x′) = � |x |1x′(mi) (16) i=1 where 1x′</context>
</contexts>
<marker>Mitchell, Lapata, 2009</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2009. Language models based on semantic composition. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 430–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>English as a formal language. Formal Philosophy,</title>
<date>1974</date>
<pages>188--221</pages>
<contexts>
<context position="6899" citStr="Montague, 1974" startWordPosition="1046" endWordPosition="1047">rspective. The idea is often credited to Frege, although in fact Frege had very little to say about compositionality that had not already been repeated since the time of Aristotle (Hodges, 2005). Our modern notion of compositionality took shape primarily with the work of Tarski (1956), who was actually arguing that a central difference between formal languages and natural languages is that natural language is not compositional. This in turn was the “the contention that an important theoretical difference exists between formal and natural languages,” that Richard Montague so famously rejected (Montague, 1974). Compositionality also features prominently in Fodor and Pylyshyn’s (1988) rejection of early connectionist representations of natural language semantics, which seems to have influenced Mitchell and Lapata (2008) as well. Logic-based forms of compositional semantics have long strived for syntactic invariance in meaning representations, which is known as the doctrine of the canonical form. The traditional justification for canonical forms is that they allow easy access to a knowledge base to retrieve some desired information, which amounts to a form of inference. Our work can be seen as an ext</context>
</contexts>
<marker>Montague, 1974</marker>
<rawString>Richard Montague. 1974. English as a formal language. Formal Philosophy, pages 188–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="7902" citStr="Poon and Domingos (2009)" startWordPosition="1204" endWordPosition="1207">nonical form. The traditional justification for canonical forms is that they allow easy access to a knowledge base to retrieve some desired information, which amounts to a form of inference. Our work can be seen as an extension of this notion to distributional semantic models with a more general notion of representational similarity and inference. There are many regular alternations that semantics models have tried to account for such as passive or dative alternations. There are also many lexical paraphrases which can take drastically different syntactic forms. Take the following example from Poon and Domingos (2009), in which the same semantic relation can be expressed by a transitive verb or an attributive prepositional phrase: (1) Utah borders Idaho. Utah is next to Idaho. In distributional semantics, the original sentence similarity test proposed by Kintsch (2001) served as the inspiration for the evaluation performed by Mitchell and Lapata (2008) and most later work in the area. Intransitive verbs are given 34 in the context of their syntactic subject, and candidate synonyms are ranked for their appropriateness. This method targets the fact that a synonym is appropriate for only some of the verb’s se</context>
<context position="18995" citStr="Poon and Domingos, 2009" startWordPosition="3016" endWordPosition="3019">ne sentence. Unrestricted question answering is a difficult problem that forces a semantic representation to deal sensibly with a number of other semantic issues such as coreference and information aggregation which still seem to be out of reach for contemporary distributional models of meaning. Since our focus in this work is on argument structure semantics, we restrict the question-answer pairs to those that only require dealing with paraphrases of this type. To do so, we semi-automatically restrict the question-answer pairs by using the output of an unsupervised clustering semantic parser (Poon and Domingos, 2009). The semantic parser clusters semantic sub-expressions derived from a dependency parse of the sentence, so that those subexpressions that express the same semantic relations are clustered. The parser is used to answer questions, and the output of the parser is manually checked. We use only those cases that have thus been determined to be correct questionanswer pairs. As a result of this restriction, this task is rather more like Task 1 in how it tests a model’s ability to recognize lexical and syntactic paraphrases. This task also involves recognizing voicing alternations, which were automati</context>
<context position="30497" citStr="Poon and Domingos (2009)" startWordPosition="4923" endWordPosition="4926">ed E&amp;P and TFP models perform poorly on this task, even when compared to the other models trained on the AFP subset. The M&amp;L multiplication model outperforms the addition model, a result which corroborates previous findings on the lexical substitution task. The same does not hold in the D&amp;L latent sense space. Overall, some of the datasets (Yahoo and Antoinette) appear to be easier for the models than others (Pfizer and Besson), but more entity pairs and relations would be needed to investigate the models’ variance across datasets. 4.4 Task 2 We used the question-answer pairs extracted by the Poon and Domingos (2009) semantic parser from the GENIA biomedical corpus that have been manually checked to be correct (295 pairs). Because our models were trained on newspaper text, they required adaptation to this specialized domain. Thus, we also trained the M&amp;L, E&amp;P and TFP models on the GENIA corpus, backing off to word vectors from the GENIA corpus when a word vector could not be found in the Gigaword-trained model. We could not do this for the D&amp;L model, since the global latent senses that are found by latent Dirichlet allocation training do not have any absolute meaning that holds across multiple runs. Inste</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="32904" citStr="Reisinger and Mooney, 2010" startWordPosition="5332" endWordPosition="5335">1, does not perform as well in this task. This could be due to the domain adaptation procedure for the D&amp;L model, which could not be reasonably trained on such a small, specialized corpus. 5 Related Work Turney and Pantel (2010) survey various types of vector space models and applications thereof in computational linguistics. We summarize below a number of other word- or phrase-level distributional models. Several approaches are specialized to deal with homography. The top-down multi-prototype approach determines a number of senses for each word, and then clusters the occurrences of the word (Reisinger and Mooney, 2010) into these senses. A prototype vector is created for each of these sense clusters. When a new occurrence 40 of a word is encountered, it is represented as a combination of the prototype vectors, with the degree of influence from each prototype determined by the similarity of the new context to the existing sense contexts. In contrast, the bottom-up exemplar-based approach assumes that each occurrence of a word expresses a different sense of the word. The most similar senses of the word are activated when a new occurrence of it is encountered and combined, for example with a kNN algorithm (Erk</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J. Mooney. 2010. Multi-prototype vector-space models of word meaning. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>Probabilistic reasoning for entity &amp; relation recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>835--841</pages>
<contexts>
<context position="28065" citStr="Roth and Yih (2002)" startWordPosition="4518" endWordPosition="4521">on of Gigaword is three times larger than the BNC corpus (~100M words), on which several previous syntactic models were trained. Because our main goal is to test the general performance of the models and to demonstrate the feasibility of our evaluation methods, we did not further tune the parameter settings to each of the tasks, as doing so would likely only yield minor improvements. 4.3 Task 1 We used the dataset by Bunescu and Mooney (2007), which we selected because it contains multiple realizations of an entity pair in a target semantic relation, unlike similar datasets such as the one by Roth and Yih (2002). Controlling for the target entity pair in this manner makes the task more difficult, because the semantic model cannot make use of distributional information about the entity pair in inference. The dataset is separated into subsets depending on the target binary relation (Company X acquires Company Y or Person X was born in Place Y) and the entity pair (e.g., Yahoo and Inktomi) (Table 2). The dataset was constructed semiautomatically using a Google search for the two entities in order with up to seven content words in between. Then, the extracted sentences were hand-labelled with whether the</context>
</contexts>
<marker>Roth, Yih, 2002</marker>
<rawString>Dan Roth and Wen-tau Yih. 2002. Probabilistic reasoning for entity &amp; relation recognition. In Proceedings of the 19th International Conference on Computational Linguistics, pages 835–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Rudolph</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Compositional matrix-space models of language.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>907--916</pages>
<contexts>
<context position="34288" citStr="Rudolph and Giesbrecht, 2010" startWordPosition="5560" endWordPosition="5563">ll (2011) uses potential paraphrases directly as dimensions in his expectation vectors. Unfortunately, this approach does not outperform various context word-based approaches in two phrase similarity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase</context>
</contexts>
<marker>Rudolph, Giesbrecht, 2010</marker>
<rawString>Sebastian Rudolph and Eugenie Giesbrecht. 2010. Compositional matrix-space models of language. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 907–916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>Proceedings of the Deep Learning and Unsupervised Feature Learning Workshop ofNIPS 2010,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="4128" citStr="Socher et al., 2010" startWordPosition="621" endWordPosition="624">reserving paraphrases. 33 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 33–43, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics Existing evaluations of distributional semantic models fall short of measuring this. One evaluation approach consists of lexical-level word substitution tasks which primarily evaluate a system’s ability to disambiguate word senses within a controlled syntactic environment (McCarthy and Navigli, 2009, for example). Another approach is to evaluate parsing accuracy (Socher et al., 2010, for example), which is really a formalism-specific approximation to argument structure analysis. These evaluations may certainly be relevant to specific components of, for example, machine translation or natural language generation systems, but they tell us little about a semantic model’s ability to support inference. In this paper, we propose a general framework for evaluating distributional semantic models that build sentence representations, and suggest two evaluation methods that test the notion of structurally invariant inference directly. Both rely on determining whether sentences expr</context>
<context position="11163" citStr="Socher et al., 2010" startWordPosition="1720" endWordPosition="1723">. Recent work uses this dataset mostly for parameter tuning. Another is the lexical paraphrase task of McCarthy and Navigli (2009), in which words are given in the context of the surrounding sentence, and the task is to rank a given list of proposed substitutions for that word. The list of substitutions as well as the correct rankings are elicited from annotators. This task was originally conceived as an applied evaluation of WSD systems, not an evaluation of phrase representations. Parsing accuracy has been used as a preliminary evaluation of semantic models that produce syntactic structure (Socher et al., 2010; Wu and Schuler, 2011). However, syntax does not always reflect semantic content, and we are specifically interested in supporting syntactic invariance when doing semantic inference. Also, this type of evaluation is tied to a particular grammar formalism. The existing evaluations that are most similar in spirit to what we propose are paraphrase detection tasks that do not assume a restricted syntactic context. Washtell (2011) collected human judgments on the general meaning similarity of candidate phrase pairs. Unfortunately, no additional guidance on the definition of “most similar in meanin</context>
<context position="34592" citStr="Socher et al. (2010)" startWordPosition="5606" endWordPosition="5609">ar in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase representation resulting from composition. 6 Conclusions We have proposed an evaluation framework for distributional models of semantics which build phrase- and sentence-level representations, and instantiated two evaluation tasks which test for the crucial ability to recognize whether sentences expres</context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. Proceedings of the Deep Learning and Unsupervised Feature Learning Workshop ofNIPS 2010, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred Tarski</author>
</authors>
<title>The concept of truth in formalized languages.</title>
<date>1956</date>
<journal>Logic, Semantics, Metamathematics,</journal>
<pages>152--278</pages>
<contexts>
<context position="6569" citStr="Tarski (1956)" startWordPosition="993" endWordPosition="994">ion methods, but it does suggest that distributional models of semantics can play a more central role in systems that require deep, precise inference. 2 Compositionality and Distributional Semantics The idea of compositionality has been central to understanding contemporary natural language semantics from an historiographic perspective. The idea is often credited to Frege, although in fact Frege had very little to say about compositionality that had not already been repeated since the time of Aristotle (Hodges, 2005). Our modern notion of compositionality took shape primarily with the work of Tarski (1956), who was actually arguing that a central difference between formal languages and natural languages is that natural language is not compositional. This in turn was the “the contention that an important theoretical difference exists between formal and natural languages,” that Richard Montague so famously rejected (Montague, 1974). Compositionality also features prominently in Fodor and Pylyshyn’s (1988) rejection of early connectionist representations of natural language semantics, which seems to have influenced Mitchell and Lapata (2008) as well. Logic-based forms of compositional semantics ha</context>
</contexts>
<marker>Tarski, 1956</marker>
<rawString>Alfred Tarski. 1956. The concept of truth in formalized languages. Logic, Semantics, Metamathematics, pages 152–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>948--957</pages>
<marker>Thater, F¨urstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948–957.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="32505" citStr="Turney and Pantel (2010)" startWordPosition="5273" endWordPosition="5276">verlap again presents a strong baseline, but the hybridized models are able to outperform simple lemma overlap. Unlike in Task 1, the E&amp;P and TFP models are comparable to the D&amp;L model, and the mixed TFP addition model achieves the best result, likely due to the need to more precisely distinguish syntactic roles in this task. The D&amp;L addition model, which achieved the best performance in Task 1, does not perform as well in this task. This could be due to the domain adaptation procedure for the D&amp;L model, which could not be reasonably trained on such a small, specialized corpus. 5 Related Work Turney and Pantel (2010) survey various types of vector space models and applications thereof in computational linguistics. We summarize below a number of other word- or phrase-level distributional models. Several approaches are specialized to deal with homography. The top-down multi-prototype approach determines a number of senses for each word, and then clusters the occurrences of the word (Reisinger and Mooney, 2010) into these senses. A prototype vector is created for each of these sense clusters. When a new occurrence 40 of a word is encountered, it is represented as a combination of the prototype vectors, with </context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal ofArtificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Washtell</author>
</authors>
<title>Compositional expectation: A purely distributional model of compositional semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the Ninth International Conference on Computational Semantics (IWCS</booktitle>
<pages>285--294</pages>
<contexts>
<context position="11593" citStr="Washtell (2011)" startWordPosition="1788" endWordPosition="1789">systems, not an evaluation of phrase representations. Parsing accuracy has been used as a preliminary evaluation of semantic models that produce syntactic structure (Socher et al., 2010; Wu and Schuler, 2011). However, syntax does not always reflect semantic content, and we are specifically interested in supporting syntactic invariance when doing semantic inference. Also, this type of evaluation is tied to a particular grammar formalism. The existing evaluations that are most similar in spirit to what we propose are paraphrase detection tasks that do not assume a restricted syntactic context. Washtell (2011) collected human judgments on the general meaning similarity of candidate phrase pairs. Unfortunately, no additional guidance on the definition of “most similar in meaning” was provided, and it appears likely that subjects conflated lexical, syntactic, and semantic relatedness. Dolan and Brockett (2005) define paraphrase detection as identifying sentences that are in a bidirectional entailment relation. While such sentences do support exactly the same inferences, we are also interested in the inferences that can be made from similar sentences that are not paraphrases according to this strict d</context>
<context position="33668" citStr="Washtell (2011)" startWordPosition="5468" endWordPosition="5469">ted as a combination of the prototype vectors, with the degree of influence from each prototype determined by the similarity of the new context to the existing sense contexts. In contrast, the bottom-up exemplar-based approach assumes that each occurrence of a word expresses a different sense of the word. The most similar senses of the word are activated when a new occurrence of it is encountered and combined, for example with a kNN algorithm (Erk and Pad´o, 2010). The models we compared and the above work assume each dimension in the feature vector corresponds to a context word. In contrast, Washtell (2011) uses potential paraphrases directly as dimensions in his expectation vectors. Unfortunately, this approach does not outperform various context word-based approaches in two phrase similarity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph a</context>
</contexts>
<marker>Washtell, 2011</marker>
<rawString>Justin Washtell. 2011. Compositional expectation: A purely distributional model of compositional semantics. In Proceedings of the Ninth International Conference on Computational Semantics (IWCS 2011), pages 285–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Semantic vector products: Some initial investigations.</title>
<date>2008</date>
<booktitle>In Second AAAI Symposium on Quantum Interaction.</booktitle>
<contexts>
<context position="34119" citStr="Widdows (2008)" startWordPosition="5537" endWordPosition="5538">and Pad´o, 2010). The models we compared and the above work assume each dimension in the feature vector corresponds to a context word. In contrast, Washtell (2011) uses potential paraphrases directly as dimensions in his expectation vectors. Unfortunately, this approach does not outperform various context word-based approaches in two phrase similarity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as in</context>
</contexts>
<marker>Widdows, 2008</marker>
<rawString>Dominic Widdows. 2008. Semantic vector products: Some initial investigations. In Second AAAI Symposium on Quantum Interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wu</author>
<author>William Schuler</author>
</authors>
<title>Structured composition of semantic vectors.</title>
<date>2011</date>
<booktitle>In Proceedings of the Ninth International Conference on Computational Semantics (IWCS</booktitle>
<pages>295--304</pages>
<contexts>
<context position="11186" citStr="Wu and Schuler, 2011" startWordPosition="1724" endWordPosition="1727">is dataset mostly for parameter tuning. Another is the lexical paraphrase task of McCarthy and Navigli (2009), in which words are given in the context of the surrounding sentence, and the task is to rank a given list of proposed substitutions for that word. The list of substitutions as well as the correct rankings are elicited from annotators. This task was originally conceived as an applied evaluation of WSD systems, not an evaluation of phrase representations. Parsing accuracy has been used as a preliminary evaluation of semantic models that produce syntactic structure (Socher et al., 2010; Wu and Schuler, 2011). However, syntax does not always reflect semantic content, and we are specifically interested in supporting syntactic invariance when doing semantic inference. Also, this type of evaluation is tied to a particular grammar formalism. The existing evaluations that are most similar in spirit to what we propose are paraphrase detection tasks that do not assume a restricted syntactic context. Washtell (2011) collected human judgments on the general meaning similarity of candidate phrase pairs. Unfortunately, no additional guidance on the definition of “most similar in meaning” was provided, and it</context>
</contexts>
<marker>Wu, Schuler, 2011</marker>
<rawString>Stephen Wu and William Schuler. 2011. Structured composition of semantic vectors. In Proceedings of the Ninth International Conference on Computational Semantics (IWCS 2011), pages 295–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Claire Cardie</author>
</authors>
<title>Compositional matrix-space models for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>172--182</pages>
<contexts>
<context position="34484" citStr="Yessenalina and Cardie, 2011" startWordPosition="5589" endWordPosition="5592">ity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase representation resulting from composition. 6 Conclusions We have proposed an evaluation framework for distributional models of semantics which build phrase- and sentence-level representations, an</context>
</contexts>
<marker>Yessenalina, Cardie, 2011</marker>
<rawString>Ainur Yessenalina and Claire Cardie. 2011. Compositional matrix-space models for sentiment analysis. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 172–182.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>