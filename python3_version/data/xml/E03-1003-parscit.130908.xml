<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001099">
<title confidence="0.9980595">
Toward Evaluation of Writing Style:
Finding Overly Repetitive Word Use in Student Essays
</title>
<author confidence="0.958333">
Jill Burstein Magdalena Wolska
</author>
<affiliation confidence="0.696917">
Educational Testing Service Universitat des Saarlandes
Princeton, New Jersey 08541, USA Saarbticken, Germany
</affiliation>
<email confidence="0.967453">
jburstein@ets.org magda@coli.uni-sb.de
</email>
<bodyText confidence="0.999436303030303">
essay scoring systems have been made available
(PEG;Page 1966; e-rater®Burstein et al., 1998;
Intelligent Essay AssessorTm;Foltz, Kintsch, and
Landauer 1998; and, Intellimetric TM; Elliot, 2003).
In addition, based on the demands of users of the
automated scoring technology, tools have been
developed that perform more detailed evaluations
of student writing. One such application is
Critique Writing Analysis Tools. Critique and e-
rater are embedded in a broader writing instruction
application, Criterion SM Online Essay Evaluation
(see http://www.etstechnologies.com). Critique
performs a number of evaluations on a student
essay related to errors in grammar (Chodorow and
Leacock, 2000), usage, and mechanics, comments
on style, and analysis of essay-based discourse
(organization and development) (Burstein et al,
2001 and Burstein and Marcu, 2003, and Burstein,
Marcu and Knight, forthcoming).
Many of these capabilities use machine-
learning approaches to model each particular kind
of analysis. To develop such tools requires large
sets of human annotated data, where judges have
annotated information required to train a system to
evaluate a particular kind of essay characteristic.
For example, to build a capability to identify
sentence fragments, a corpus of essay data needs to
be annotated for this kind of ungrammatical
sentence. A capability exists that identifies essay-
based discourse elements in essays, for example,
thesis statements, and conclusions. To do this,
human judges annotated a corpus of essays for
these particular kinds of discourse elements.
</bodyText>
<sectionHeader confidence="0.64145" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996226916666667">
Automated essay scoring is now an
established capability used from
elementary school through graduate
school for purposes of instruction and
assessment. Newer applications provide
automated diagnostic feedback about
student writing. Feedback includes
errors in grammar, usage, and
mechanics, comments about writing
style, and evaluation of discourse
structure. This paper reports on a
system that evaluates a characteristic of
lower quality essay writing style:
repetitious word use. This capability is
embedded in a commercial writing
assessment application, Criterionsm
The system uses a machine-learning
approach with word-based features to
model repetitious word use in an essay.
System performance well exceeds
several baseline algorithms. Agreement
between the system and a single human
judge exceeds agreement between two
human judges.
</bodyText>
<sectionHeader confidence="0.98072" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.983801333333333">
Automated evaluation of student essay writing
is a rapidly growing field. Over the past few
years, at least four commercially automated
</bodyText>
<page confidence="0.998634">
35
</page>
<bodyText confidence="0.999449025">
The judges&apos; annotations were used to build an
essay-based discourse analysis system.
Annotation protocols are required for each
task. For identification of sentence fragments,
this is reasonably straightforward. In terms of
essay-based discourse analysis, it is fairly
clear-cut. Though there is a certain amount of
debate, annotators can be trained to have a
reasonable amount of agreement in classifying
essay-based discourse elements. Style, in
contrast to grammar usage and discourse
strategy, is tricky in terms of getting people to
agree. It is a strongly subjective measure.
We discuss a system that identifies a
specific characteristic of undesirable writing
style --- overly repetitious word usage.
Unlike identification of sentence fragments,
and essay-based discourse strategy, there are
no hard-and-fast rules that tell us how often a
word must be used in an essay to be considered
overly repetitious. The results reported in this
paper indicate that even for a subjective style
measure, human judges annotations can be
modeled. The system can label repetitive
words with precision, recall, and F-measures
upwards of 0.90. It clearly outperforms all
baseline methods described in the paper.
In earlier work with the writing instruction
application, &amp;quot;Writer&apos;s Workbench,&amp;quot; some
features associated with style were evaluated,
including: average word length, the
distribution of sentence lengths, grammatical
types of sentences (e g, simple and complex),
the percentage of passive voice verbs, and the
percentage of nouns that are nominalizations
(see MacDonald et al, 1982 for a complete
description of the Writer&apos;s Workbench). In
contrast to a subjective measure such as,
repetitive word usage, the stylistic features in
the Writer&apos;s Workbench are not subjective.
</bodyText>
<sectionHeader confidence="0.987962" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999312">
essays. The decision-based machine learning
algorithm, C5X11, was used to model the human
judgements.
</bodyText>
<subsectionHeader confidence="0.994389">
2.1 Human Annotation of
Repetitious Word Use
</subsectionHeader>
<bodyText confidence="0.9999896">
As noted in the Introduction, the identification of
good or bad writing style is highly subjective.
With regard to word overuse in an essay, what one
person may find irritating may not really bother
someone else. Our goal in developing this tool
was to indicate to students the cases in which word
overuse might affect the rating of the paper with
regard to its overall quality.
In the annotation protocol, the central
guideline for the two human judges was to label as
repetitious only those cases where the repetition of
a word interfered with the overall quality of the
essay. Both annotators were expert essay graders.
They used a PC-based graphical user interface to
label occurrences of repetitious words in a corpus
containing 296 essays2. These essay data were
randomly selected from a larger set of 5,000
essays. The final set contained essays from across
several populations (6th grade through college
freshman), and 11 test question topics.
</bodyText>
<subsectionHeader confidence="0.996199">
2.2 Decision-Based Approach
</subsectionHeader>
<bodyText confidence="0.9991781875">
We hypothesized, a priori, a number of features
that could reasonably be associated with word
overuse, such that the overuse interfered with a
smooth reading of the essay. Our hypotheses were
based on general discussions with the annotators
before the annotation process began. The
annotators are part of a team of experts who are
critical in the decision-making process with regard
to what kinds of feedback are helpful to students.
We have on-going discussions with them that
provide us with information about the kinds of
Since we want this system to model human
judgements about overly repetitious word use,
two human annotators labeled a corpus of
For details about this software, see
http://www.rulequest.com.
</bodyText>
<page confidence="0.704859">
2
</page>
<tableCaption confidence="0.561458">
Practical constraints (e.g., time and costs) did not
allow for additional annotation.
</tableCaption>
<page confidence="0.994088">
36
</page>
<bodyText confidence="0.999949857142857">
issues that they are concerned about in student
essay writing. Based on our hypotheses, we
found that 7 features could be used in
combination to reliably predict the word(s) in a
student&apos;s essay that should be labeled as
repetitious. These features are described
below in Figure 1.
For each lemmatized word token in an
essay, a vector was generated that contained
the values for the 7 features. A stoplist is used,
so that function words were excluded. A
decision-based machine learning algorithm,
C5.0, was used to model repetitious word use,
based on human judge annotations.
</bodyText>
<listItem confidence="0.990361705882353">
1) Absolute Count: Total number of occurrences.
2) Essay Ratio: Proportional occurrence of the
word in the essay (based on the total number of
words in the essay).
3) Paragraph Ratio: Average proportional
occurrence of the word in a paragraph (based
on the average number of words in all
paragraphs in the essay).
4) Highest Paragraph Ratio: Proportional
occurrence of the word in the paragraph where
it appears with the highest frequency (based on
the number of words in the paragraph where it
occurs most frequently).
5) Word Length: Total number of characters in a
word.
6) Is Pronoun: Is the word a pronoun?
7) Previous Occurrence Distance: The distance
</listItem>
<figureCaption confidence="0.789575">
between the word and its previous occurrence
(based on number of words.)
Figure 1: Word-Based Features
</figureCaption>
<sectionHeader confidence="0.999819" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.994761541666667">
repeated. Each judge annotated overly repetitious
word use in about 25% of the essays. In Table la,
&amp;quot;Ji with J2&amp;quot; agreement indicates that Judge 2
annotations were the basis for comparison; and,
&amp;quot;J2 with J1&amp;quot; agreement indicates that Judge 1
annotations were the basis for comparison. The
Kappa between the two judges was 0.5 based on
annotations for all words (i.e., repeated + non-
repeated). Kappa indicates the agreement between
judges with regard to chance agreement (Uebersax,
1982). Research in content analysis (Krippendorff,
1980) suggests that Kappa values higher than 0.8
reflect very high agreement, between 0.6 and 0.8
indicate good agreement, and values between 0.4
and 0.6 show lower agreement, but still greater
than chance.
Figures 2 and 3 in the Appendix show
annotated essays by each judge. These figures
illustrate the kinds of disagreement on repeated
words that exist between judges. The sample in
Figure 2 shows annotations made by Judge 1, but
not by Judge 2. Figure 3 shows an example where
Judge 2 annotated words as repeated, but Judge 1
did not.
</bodyText>
<table confidence="0.9998976875">
Precision Recall F-
measure
J1 with J23 70
essays
Repeated 1,315 0.55 0.56 0.56
words
Non-repeated 42,128 0.99 0.99 0.99
words
All words 43,443 0.97 0.97 0.97
J2 with J14 74
essays
Repeated 1,292 0.56 0.55 0.56
words
Non-repeated 42,151 0.99 0.99 0.99
words
All words 43,443 0.97 0.97 0.97
</table>
<subsectionHeader confidence="0.998317">
3.1 Human Performance
</subsectionHeader>
<bodyText confidence="0.998882666666667">
The results in Table la show agreement
between the two human judges based on essays
marked with repetition by one of the judges, at
the word level. So, this includes cases where
one judge annotated some repeated words and
the other judge annotated no words as
</bodyText>
<tableCaption confidence="0.9425945">
Table la: Precision, Recall, and F-measures Between
Judge 1 (JI) and Judge 2 (J2)
</tableCaption>
<listItem confidence="0.630645333333333">
3 Precision = Total number J1 + J2 agreements + total number J1
labels; Recall = Total number JI + J2 agreements +total number J2
labels; F-measure =2 * P R + (P + R).
4 Precision = Total number J1 + J2 agreements + total number J2
labels; Recall = Total number JI + J2 agreements +total number JI
labels; F-measure =2 * P * R + (P + R).
</listItem>
<page confidence="0.997751">
37
</page>
<bodyText confidence="0.998848575">
In Table la, agreement on &amp;quot;Repeated words&amp;quot;
between judges is somewhat low. How can we
build a system to reliably identify overly
repetitious words if judges cannot agree? If
we look in the total set of essays identified by
either judge as having some repetition, we find
an overlapping set of 40 essays where both
judges annotated the essay as having some sort
of repetition. We call this the agreement
subset.
Of the essays that Judge I annotated as
having repetition, approximately 57% (40/70)
agreed with Judge 2 as having some sort of
repetition; of the essays that Judge 2 annotated
with repetitious word use, about 54% (40/74)
agreed with Judge 1. If we look at the total
number of &amp;quot;Repeated words&amp;quot; labeled by each
judge for all essays in Table la, we find that
these 40 essays contain the majority of
&amp;quot;Repeated words&amp;quot; for each judge: 64%
(838/1315) for Judge 2, and 60% (767/1292)
for Judge 1.
It is possible that even for the essays where
judges both agree that there is some kind of
repetitive word use, they do not agree on what
the repetition is. Therefore, we want to answer
the following question: On the subset of essays
where judges agree that there is repetition, do
they agree on the same words as being
repetitious?
The core agreement with regard to
&amp;quot;Repeated words&amp;quot; appears to be in these 40
essays. Table lb shows high agreement
between the two judges for &amp;quot;Repeated words&amp;quot;
in the agreement subset. The Kappa between
the two judges for &amp;quot;All words&amp;quot; (repeated +
non-repeated) on this subset is 0.88. Figure 4
in the Appendix shows an example of an essay
where both judges annotated the same words
as repeated words.
</bodyText>
<table confidence="0.999938058823529">
Precision Recall F-measure
J1 with J2 40
essays
Repeated 838 0.87 0.95 0.91
words
Non- 4,977 0.99 0.98 0.98
repeated
words
All words 5,815 0.97 0.97 0.97
J2 with J1 40
essays
Repeated 767 0.95 0.87 0.90
words
Non- 5,048 0.98 0.99 0.98
repeated
words
All words 5,815 0.97 0.97 0.97
</table>
<tableCaption confidence="0.97834">
Table lb: Precision, Recall, and F-measure Between
Judge 1 (J1) and Judge 2 (J2): &amp;quot;Essay-Level Agreement
Subset&amp;quot;
</tableCaption>
<subsectionHeader confidence="0.999065">
3.2 System Performance
</subsectionHeader>
<bodyText confidence="0.99975264">
Table 2 shows agreement for repeated words
between several baseline systems, and each of the
two judges. Each baseline system uses one of the
7 word-based features used to select repetitious
words (see Figure 1). Baseline systems label all
occurrences of a word as repetitious if the criterion
value for the algorithm is met. After several
iterations using different values, the final criterion
value (V) is the one that yielded the highest
performance. The final criterion value is shown in
Table 2. Precision, Recall, and F-measures are
based on comparisons with the same sets of essays
and words from Table la. Comparisons between
Judge 1 with each baseline algorithm are based on
the 74 essays where Judge 1 annotated repetitious
words, and likewise, for Judge 2, on this judge&apos;s 70
essays annotated for repetitious words.
Using the baseline algorithms in Table 2, the
F-measures for non-repeated words range from
0.96 to 0.97, and from (193 to 0.94 for all words
(i.e., repeated + non-repeated words). The
exceptional case is for Highest Paragraph Ratio
Algorithm with Judge 2, where the F-measure for
non- repeated words is 0.89, and for all words is
0.82.
</bodyText>
<page confidence="0.998153">
38
</page>
<bodyText confidence="0.99867048">
To evaluate the system in comparison to
each of the human judges, for each feature
combination algorithm, a 10-fold cross-
validation was run on each set of annotations
for both judges. For each cross-validation run,
a unique nine-tenths of the data were used for
training, and the remaining one-tenth was used
for cross-validating that model. Based on this
evaluation, Table 3, shows agreement at the
word level between each judge and a system
that uses a different combination of features.
Agreement refers to the mean agreement
across the 10-fold cross-validation runs.
All systems clearly exceed the performance of
the 7 baseline algorithms in Table 2. The best
system is All Features, in which all 7 features are
used. These results are indicated in italicized
boldface in Table 3. It also indicates that building a
model using the annotated sample from human
judges 1 or 2 yielded indistinguishable results. For
this reason, we arbitrarily used the data from one
of the judges to build the final system.
When the All Features system is used, the F-
measure = 1.00 for non-repeated words, and for all
words for both &amp;quot;J1 with
</bodyText>
<table confidence="0.999878090909091">
Baseline Systems 5 V J1 with System J2 with System
Precision Recall F- Precision Recall F-
measure measure
Absolute Count 19 0.24 0.42 0.30 0.22 0.39 0.28
Essay Ratio 0.05 0.27 0.54 0.36 0.21 0.44 0.28
Paragraph Ratio 0.05 0.25 0.50 0.33 0.24 0.50 0.32
Highest Paragraph 0.05 0.25 0.50 0.33 0.11 0.76 0.19
Ratio
Word Length 8 0.05 0.14 0.07 0.06 0.16 0.08
Is Pronoun 1 0.04 0.06 0.04 0.02 0.03 0.02
Distance 3 0.01 0.11 0.01 0.01 0.10 0.01
</table>
<tableCaption confidence="0.930312">
Table 2: Precision, Recall, and F-measures Between Human Judges (J1 &amp; J2)
</tableCaption>
<table confidence="0.993267692307692">
&amp; Highest Baseline System Performance for Repeated Words
Feature Combination Algorithms 11 with System 12 with System
Precision Recall F-measure Precision Recall F-measure
Absolute Count + Essay Ratio + 0.95 0.72 0.82 0.91 0.69 0.78
Paragraph Ratio + Highest
Paragraph Ratio (Count
Features)
Count Features + Is Pronoun 0.93 0.78 0.85 0.91 0.75 0.82
Count Features + Word Length 0.95 0.89 0.92 0.95 0.88 0.91
Count Features + Distance 0.95 0.72 0.82 0.91 0.70 0.79
All Features: Count Features + Is 0.95 0.90 0.93 0.96 0.90 0.93
Pronoun + Word Length +
Distance
</table>
<tableCaption confidence="0.9421295">
Table 3: Precision, Recall, and F-measure Between Human Judges (JI &amp; J2)
&amp; 5 Feature Combination Systems for Predicting Repeated Words
</tableCaption>
<figure confidence="0.528403333333333">
5
Precision = Total judge+ system agreements ÷ total system labels;
Recall = Total judge + system agreements ÷ total judge labels; F-measure = 2 * P R (P + R).
</figure>
<page confidence="0.997404">
39
</page>
<bodyText confidence="0.999967555555556">
System&amp;quot; and &amp;quot;J2 with System.&amp;quot; Using A//
Features, agreement for repeated words more
closely resembles inter-judge agreement for the
agreement subset in Table lb. It seems that the
machine learning algorithm is capturing the
patterns of repetitious word use in that set of 40
essays. Perhaps, an additional explanation as to
why each judge has high agreement with the
system, is that each judge is internally consistent.
</bodyText>
<sectionHeader confidence="0.988574" genericHeader="method">
4 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.999969333333333">
Teachers would generally prefer that students try
to use synonyms in their writing, instead of the
same word, repeatedly. Feedback about word
overuse is helpful in terms of getting students to
refine the use of vocabulary in their writing.
Therefore, writing teachers would agree that it is
an important capability in an automated essay
evaluation system.
The evaluations presented in this paper show
that a reliable repetitive word detection system
can be built to model human annotations, even
though this is a highly subjective writing style
measure. An evaluation of our system indicates
that it outperforms all baseline systems. It also
has agreement with a single judge upward of
0.90 with regard to Precision, Recall and F-
measures.
As research continues in automated essay
scoring, it is standard to try to incorporate in a
scoring system, any new features of writing that
can be captured automatically. This new
capability to identify repetitious word usage is
currently being evaluated in terms of how it can
contribute to better accuracy in an automated
scoring system. Preliminary results indicate that
the ability to detect if a writer is overusing
certain vocabulary can contribute to the overall
accuracy of the score from an automated essay
scoring system. We are experimenting with the
information about repetitious word usage in
different discourse elements in an essay, e.g.,
thesis statements. In this case, the detection of
repetitious words in these elements could
contribute to a method for rating the overall
quality of a particular element.
The repetitious word detection system was
trained on annotated data across 11 test question
topics; however, informal evaluations indicate
that the system makes reasonable decisions on
any topic. Though more systematic testing still
needs to be done, the system appears to be topic-
independent.
</bodyText>
<sectionHeader confidence="0.999052" genericHeader="method">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999952">
The authors would like to thank Claudia Leacock
for advice on earlier versions of this paper. This
work was completed while both authors were
affiliated with ETS Technologies, Inc, formerly a
wholly-owned subsidiary of Educational Testing
Service. ETS Technologies is currently an
internal division of Educational Testing Service.
</bodyText>
<sectionHeader confidence="0.998533" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.998354782608695">
Burstein, Jill, Marcu, Daniel, and Knight, Kevin
(forthcoming). Finding the WRITE Stuff:
Automatic Identification of Discourse Structure in
Student Essays. Special Issue on Natural
Language Processing of IEEE Intelligent Systems,
January/February, 2003.
Burstein, J. and Marcu D. (2003). Developing
Technology for Automated Evaluation of
Discourse Structure in Student Essays. In M.
Shermis and J. Burstein (eds.), Automated essay
scoring: A cross-disciplinary perspective,
Hillsdale, NJ: Lawrence Erlbaum Associates, Inc.
Burstein, J., Marcu, D., Andreyev, S., and Chodorow,
M. (2001). Towards Automatic Classification of
Discourse Elements in Essays. In Proceedings of
the 30 Annual Meeting of the Association for
Computational Linguistics, Toulouse, France,
July, 2001.
Burstein, J., Kukich, K., Wolff, S., Lu, C., Chodorow,
M., Braden-Harder, L., and Harris M. D. 1998.
Automated Scoring Using A Hybrid Feature
Identification Technique. Proceedings of 36th
Annual Meeting of the Association for
</reference>
<page confidence="0.967635">
40
</page>
<reference confidence="0.999916111111111">
Computational Linguistics, 206-210. Montreal,
Canada.
Chodorow, Martin and Leacock, Claudia. 2000. An
unsupervised method for detecting grammatical
errors. In Proceedings of the 1st Annual Meeting
of the North American Chapter of the Association
for Computational Linguistics, 140-147.
Elliott, S. (2003). Intellimetric: From Here to
Validity. In M. Shermis and J. Burstein (eds.)
Automated essay scoring: A cross-disciplinary
perspective. Hillsdale, NJ: Lawrence Erlbaum
Associates.
Foltz, P. W., Kintsch, W., and Landauer, T. K. 1998.
Analysis of Text Coherence Using Latent
Semantic Analysis. Discourse Processes 25(2-
3):285-307.
Krippendorff K. (1980). Content Analysis: An
Introduction to Its Methodology. Sage Publishers.
MacDonald, N. H., Frase, L.T., Gingrich P.S., and
Keenan, S.A. (1982). The Writer&apos;s Workbench:
Computer Aids for Text Analysis. IEEE
Transactions on Communications. 30(1):105-110.
Page, E. B. 1966. The Imminence of Grading Essays
by Computer. Phi Delta Kappan, 48:238-243.
Uebersax, J.S. (1982) &amp;quot;A Generalized Kappa
Coefficient,&amp;quot; Educational and Psychological
Measurement, Vol. 42, pp. 181-183.
</reference>
<page confidence="0.996424">
41
</page>
<sectionHeader confidence="0.5297755" genericHeader="method">
Appendix: Sample Human Judge Annotations for Repeated Words,
In UPPER CASE BOLDFACE
</sectionHeader>
<bodyText confidence="0.427647666666667">
THE BEST PET
Did YOU ever have a pet that YOU thought was the best thing that YOU ever had.
I am going to tell YOU about a pet that I thought was the best.
The best pet I thought was the best was a pit bull. THEY are very easy to tran,
THEY are competetive. THEY are very strong, and good pets. Thet do not turn on you
if you fight them. THEY can protect things very well. THEY are alwas good to have.
</bodyText>
<figureCaption confidence="0.914967">
Figure 2: Sample Annotated Essay from Judge 1 Which Judge 2 Did Not Identify
</figureCaption>
<sectionHeader confidence="0.420465" genericHeader="method">
SHORTS
</sectionHeader>
<bodyText confidence="0.996723375">
The question here is what I think about, not being allwoed to wear SHORTS.
I think we should be allowed to wear SHORTS. Imean what is the big deal. I know
us girls can get our SHORTS pretty SHORT, but we can also get skirts pretty SHORT
too. So we should just have the same rules for skirts. Pretty soon we can&apos;t wear skirts.
Well this get&apos;s me on another thing. We can&apos;t wear capris! I know this isn&apos;t about capris, but
still they go down to your knees that dosn&apos;t make since.
Boys should be able to wear those long SHORTS that dosn&apos;t show anything. Well I don&apos;t
know. Maybe it&apos;s good we can&apos;t wear SHORTS. I don&apos;t know, Im just a teenager.
</bodyText>
<figureCaption confidence="0.991652">
Figure 3: Sample Annotated Essay from Judge 2 Which Judge 1 Did Not Identify
</figureCaption>
<bodyText confidence="0.9989024">
One major SCHOOL issue that we students face daily is the subject of SCHOOL safety. Many
SCHOOLS across the country have encountered SCHOOL VIOLENCE. I think that most
SCHOOL VIOLENCE starts with the SCHOOL and the community. Students who engage in
SCHOOL VIOLENCE are usually made fun of or are insecure about themselves. Some ways that
I think that we can stop SCHOOL follow. I think that in order to stop SCHOOL VIOLENCE
in and around our communities we have to get the community involved in sharing and making it
aware to other cities and towns that SCHOOL VIOLENCE is very real, and we face it everyday.
One way I think that we can cut down on SCHOOL VIOLENCE is to have striter disapline policies.
When students in a SCHOOL joke around or threaten other students about killing them, or bringing
weapons to SCHOOL, the staff of that SCHOOL needs to take action. When a student has thought
out a plan to kill others, they obviously need to be talked to. I hope that by reading these
ways to stop SCHOOL VIOLENCE we can all take action to make our SCHOOLS safer.
We can not stop SCHOOL VIOLENCE until we stop blaming others, and see that we too
have overlooked SCHOOL VIOLENCE. SCHOOL VIOLENCE is a major SCHOOL issue
that everyone can stop, if we all try to help.
</bodyText>
<figureCaption confidence="0.978396">
Figure 4: Sample Essay Where Both Judges Agree On Repeated Words
</figureCaption>
<page confidence="0.997778">
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.473052">
<title confidence="0.99953">Toward Evaluation of Writing Style: Finding Overly Repetitive Word Use in Student Essays</title>
<author confidence="0.993884">Jill Burstein Magdalena Wolska</author>
<affiliation confidence="0.61765">Testing Service Saarlandes</affiliation>
<address confidence="0.991515">Princeton, New Jersey 08541, USA Saarbticken, Germany</address>
<abstract confidence="0.995738237288135">jburstein@ets.org magda@coli.uni-sb.de essay scoring systems have been made available (PEG;Page 1966; e-rater®Burstein et al., 1998; Intelligent Essay AssessorTm;Foltz, Kintsch, and 1998; and, Intellimetric 2003). In addition, based on the demands of users of the automated scoring technology, tools have been developed that perform more detailed evaluations of student writing. One such application is Analysis Tools. eembedded in a broader writing instruction SMOnline Essay Evaluation http://www.etstechnologies.com). performs a number of evaluations on a student essay related to errors in grammar (Chodorow and Leacock, 2000), usage, and mechanics, comments on style, and analysis of essay-based discourse (organization and development) (Burstein et al, 2001 and Burstein and Marcu, 2003, and Burstein, Marcu and Knight, forthcoming). Many of these capabilities use machinelearning approaches to model each particular kind of analysis. To develop such tools requires large sets of human annotated data, where judges have annotated information required to train a system to evaluate a particular kind of essay characteristic. For example, to build a capability to identify sentence fragments, a corpus of essay data needs to be annotated for this kind of ungrammatical sentence. A capability exists that identifies essaybased discourse elements in essays, for example, statements, do this, human judges annotated a corpus of essays for these particular kinds of discourse elements. Abstract Automated essay scoring is now an established capability used from elementary school through graduate school for purposes of instruction and assessment. Newer applications provide automated diagnostic feedback about student writing. Feedback includes errors in grammar, usage, and mechanics, comments about writing style, and evaluation of discourse structure. This paper reports on a system that evaluates a characteristic of lower quality essay writing style: word use. capability is embedded in a commercial writing application, The system uses a machine-learning approach with word-based features to model repetitious word use in an essay. System performance well exceeds several baseline algorithms. Agreement between the system and a single human judge exceeds agreement between two human judges.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jill Burstein</author>
<author>Daniel Marcu</author>
<author>Kevin Knight</author>
</authors>
<title>Finding the WRITE Stuff: Automatic Identification</title>
<date>2003</date>
<booktitle>of Discourse Structure in Student Essays. Special Issue on Natural Language Processing of IEEE Intelligent Systems, January/February,</booktitle>
<marker>Burstein, Marcu, Knight, 2003</marker>
<rawString>Burstein, Jill, Marcu, Daniel, and Knight, Kevin (forthcoming). Finding the WRITE Stuff: Automatic Identification of Discourse Structure in Student Essays. Special Issue on Natural Language Processing of IEEE Intelligent Systems, January/February, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Burstein</author>
<author>D Marcu</author>
</authors>
<title>Developing Technology for Automated Evaluation of Discourse Structure in Student Essays.</title>
<date>2003</date>
<editor>In M. Shermis and J. Burstein (eds.),</editor>
<publisher>Lawrence Erlbaum Associates, Inc.</publisher>
<location>Hillsdale, NJ:</location>
<contexts>
<context position="1120" citStr="Burstein and Marcu, 2003" startWordPosition="147" endWordPosition="150">s of users of the automated scoring technology, tools have been developed that perform more detailed evaluations of student writing. One such application is Critique Writing Analysis Tools. Critique and erater are embedded in a broader writing instruction application, Criterion SM Online Essay Evaluation (see http://www.etstechnologies.com). Critique performs a number of evaluations on a student essay related to errors in grammar (Chodorow and Leacock, 2000), usage, and mechanics, comments on style, and analysis of essay-based discourse (organization and development) (Burstein et al, 2001 and Burstein and Marcu, 2003, and Burstein, Marcu and Knight, forthcoming). Many of these capabilities use machinelearning approaches to model each particular kind of analysis. To develop such tools requires large sets of human annotated data, where judges have annotated information required to train a system to evaluate a particular kind of essay characteristic. For example, to build a capability to identify sentence fragments, a corpus of essay data needs to be annotated for this kind of ungrammatical sentence. A capability exists that identifies essaybased discourse elements in essays, for example, thesis statements, </context>
</contexts>
<marker>Burstein, Marcu, 2003</marker>
<rawString>Burstein, J. and Marcu D. (2003). Developing Technology for Automated Evaluation of Discourse Structure in Student Essays. In M. Shermis and J. Burstein (eds.), Automated essay scoring: A cross-disciplinary perspective, Hillsdale, NJ: Lawrence Erlbaum Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Burstein</author>
<author>D Marcu</author>
<author>S Andreyev</author>
<author>M Chodorow</author>
</authors>
<title>Towards Automatic Classification of Discourse Elements in Essays.</title>
<date>2001</date>
<booktitle>In Proceedings of the 30 Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="1091" citStr="Burstein et al, 2001" startWordPosition="142" endWordPosition="145">tion, based on the demands of users of the automated scoring technology, tools have been developed that perform more detailed evaluations of student writing. One such application is Critique Writing Analysis Tools. Critique and erater are embedded in a broader writing instruction application, Criterion SM Online Essay Evaluation (see http://www.etstechnologies.com). Critique performs a number of evaluations on a student essay related to errors in grammar (Chodorow and Leacock, 2000), usage, and mechanics, comments on style, and analysis of essay-based discourse (organization and development) (Burstein et al, 2001 and Burstein and Marcu, 2003, and Burstein, Marcu and Knight, forthcoming). Many of these capabilities use machinelearning approaches to model each particular kind of analysis. To develop such tools requires large sets of human annotated data, where judges have annotated information required to train a system to evaluate a particular kind of essay characteristic. For example, to build a capability to identify sentence fragments, a corpus of essay data needs to be annotated for this kind of ungrammatical sentence. A capability exists that identifies essaybased discourse elements in essays, for</context>
</contexts>
<marker>Burstein, Marcu, Andreyev, Chodorow, 2001</marker>
<rawString>Burstein, J., Marcu, D., Andreyev, S., and Chodorow, M. (2001). Towards Automatic Classification of Discourse Elements in Essays. In Proceedings of the 30 Annual Meeting of the Association for Computational Linguistics, Toulouse, France, July, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Burstein</author>
<author>K Kukich</author>
<author>S Wolff</author>
<author>C Lu</author>
<author>M Chodorow</author>
<author>L Braden-Harder</author>
<author>M D Harris</author>
</authors>
<title>Automated Scoring Using A Hybrid Feature Identification Technique.</title>
<date>1998</date>
<booktitle>Proceedings of 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>206--210</pages>
<location>Montreal, Canada.</location>
<marker>Burstein, Kukich, Wolff, Lu, Chodorow, Braden-Harder, Harris, 1998</marker>
<rawString>Burstein, J., Kukich, K., Wolff, S., Lu, C., Chodorow, M., Braden-Harder, L., and Harris M. D. 1998. Automated Scoring Using A Hybrid Feature Identification Technique. Proceedings of 36th Annual Meeting of the Association for Computational Linguistics, 206-210. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>An unsupervised method for detecting grammatical errors.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>140--147</pages>
<contexts>
<context position="958" citStr="Chodorow and Leacock, 2000" startWordPosition="124" endWordPosition="127">e-rater®Burstein et al., 1998; Intelligent Essay AssessorTm;Foltz, Kintsch, and Landauer 1998; and, Intellimetric TM; Elliot, 2003). In addition, based on the demands of users of the automated scoring technology, tools have been developed that perform more detailed evaluations of student writing. One such application is Critique Writing Analysis Tools. Critique and erater are embedded in a broader writing instruction application, Criterion SM Online Essay Evaluation (see http://www.etstechnologies.com). Critique performs a number of evaluations on a student essay related to errors in grammar (Chodorow and Leacock, 2000), usage, and mechanics, comments on style, and analysis of essay-based discourse (organization and development) (Burstein et al, 2001 and Burstein and Marcu, 2003, and Burstein, Marcu and Knight, forthcoming). Many of these capabilities use machinelearning approaches to model each particular kind of analysis. To develop such tools requires large sets of human annotated data, where judges have annotated information required to train a system to evaluate a particular kind of essay characteristic. For example, to build a capability to identify sentence fragments, a corpus of essay data needs to b</context>
</contexts>
<marker>Chodorow, Leacock, 2000</marker>
<rawString>Chodorow, Martin and Leacock, Claudia. 2000. An unsupervised method for detecting grammatical errors. In Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics, 140-147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Elliott</author>
</authors>
<title>Intellimetric: From Here to Validity.</title>
<date>2003</date>
<editor>In M. Shermis and J. Burstein (eds.)</editor>
<marker>Elliott, 2003</marker>
<rawString>Elliott, S. (2003). Intellimetric: From Here to Validity. In M. Shermis and J. Burstein (eds.) Automated essay scoring: A cross-disciplinary perspective. Hillsdale, NJ: Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Foltz</author>
<author>W Kintsch</author>
<author>T K Landauer</author>
</authors>
<title>Analysis of Text Coherence Using Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes</booktitle>
<pages>25--2</pages>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Foltz, P. W., Kintsch, W., and Landauer, T. K. 1998. Analysis of Text Coherence Using Latent Semantic Analysis. Discourse Processes 25(2-3):285-307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology.</title>
<date>1980</date>
<publisher>Sage Publishers.</publisher>
<contexts>
<context position="8450" citStr="Krippendorff, 1980" startWordPosition="1279" endWordPosition="1280">ious occurrence (based on number of words.) Figure 1: Word-Based Features 3 Results repeated. Each judge annotated overly repetitious word use in about 25% of the essays. In Table la, &amp;quot;Ji with J2&amp;quot; agreement indicates that Judge 2 annotations were the basis for comparison; and, &amp;quot;J2 with J1&amp;quot; agreement indicates that Judge 1 annotations were the basis for comparison. The Kappa between the two judges was 0.5 based on annotations for all words (i.e., repeated + nonrepeated). Kappa indicates the agreement between judges with regard to chance agreement (Uebersax, 1982). Research in content analysis (Krippendorff, 1980) suggests that Kappa values higher than 0.8 reflect very high agreement, between 0.6 and 0.8 indicate good agreement, and values between 0.4 and 0.6 show lower agreement, but still greater than chance. Figures 2 and 3 in the Appendix show annotated essays by each judge. These figures illustrate the kinds of disagreement on repeated words that exist between judges. The sample in Figure 2 shows annotations made by Judge 1, but not by Judge 2. Figure 3 shows an example where Judge 2 annotated words as repeated, but Judge 1 did not. Precision Recall Fmeasure J1 with J23 70 essays Repeated 1,315 0.</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Krippendorff K. (1980). Content Analysis: An Introduction to Its Methodology. Sage Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N H MacDonald</author>
<author>L T Frase</author>
<author>P S Gingrich</author>
<author>S A Keenan</author>
</authors>
<title>The Writer&apos;s Workbench: Computer Aids for Text Analysis.</title>
<date>1982</date>
<journal>IEEE Transactions on Communications.</journal>
<pages>30--1</pages>
<contexts>
<context position="4428" citStr="MacDonald et al, 1982" startWordPosition="631" endWordPosition="634">at even for a subjective style measure, human judges annotations can be modeled. The system can label repetitive words with precision, recall, and F-measures upwards of 0.90. It clearly outperforms all baseline methods described in the paper. In earlier work with the writing instruction application, &amp;quot;Writer&apos;s Workbench,&amp;quot; some features associated with style were evaluated, including: average word length, the distribution of sentence lengths, grammatical types of sentences (e g, simple and complex), the percentage of passive voice verbs, and the percentage of nouns that are nominalizations (see MacDonald et al, 1982 for a complete description of the Writer&apos;s Workbench). In contrast to a subjective measure such as, repetitive word usage, the stylistic features in the Writer&apos;s Workbench are not subjective. 2 Approach essays. The decision-based machine learning algorithm, C5X11, was used to model the human judgements. 2.1 Human Annotation of Repetitious Word Use As noted in the Introduction, the identification of good or bad writing style is highly subjective. With regard to word overuse in an essay, what one person may find irritating may not really bother someone else. Our goal in developing this tool was</context>
</contexts>
<marker>MacDonald, Frase, Gingrich, Keenan, 1982</marker>
<rawString>MacDonald, N. H., Frase, L.T., Gingrich P.S., and Keenan, S.A. (1982). The Writer&apos;s Workbench: Computer Aids for Text Analysis. IEEE Transactions on Communications. 30(1):105-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E B Page</author>
</authors>
<date>1966</date>
<booktitle>The Imminence of Grading Essays by Computer. Phi Delta Kappan,</booktitle>
<pages>48--238</pages>
<marker>Page, 1966</marker>
<rawString>Page, E. B. 1966. The Imminence of Grading Essays by Computer. Phi Delta Kappan, 48:238-243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Uebersax</author>
</authors>
<title>A Generalized Kappa Coefficient,&amp;quot;</title>
<date>1982</date>
<journal>Educational and Psychological Measurement,</journal>
<volume>42</volume>
<pages>181--183</pages>
<contexts>
<context position="8399" citStr="Uebersax, 1982" startWordPosition="1273" endWordPosition="1274">nce: The distance between the word and its previous occurrence (based on number of words.) Figure 1: Word-Based Features 3 Results repeated. Each judge annotated overly repetitious word use in about 25% of the essays. In Table la, &amp;quot;Ji with J2&amp;quot; agreement indicates that Judge 2 annotations were the basis for comparison; and, &amp;quot;J2 with J1&amp;quot; agreement indicates that Judge 1 annotations were the basis for comparison. The Kappa between the two judges was 0.5 based on annotations for all words (i.e., repeated + nonrepeated). Kappa indicates the agreement between judges with regard to chance agreement (Uebersax, 1982). Research in content analysis (Krippendorff, 1980) suggests that Kappa values higher than 0.8 reflect very high agreement, between 0.6 and 0.8 indicate good agreement, and values between 0.4 and 0.6 show lower agreement, but still greater than chance. Figures 2 and 3 in the Appendix show annotated essays by each judge. These figures illustrate the kinds of disagreement on repeated words that exist between judges. The sample in Figure 2 shows annotations made by Judge 1, but not by Judge 2. Figure 3 shows an example where Judge 2 annotated words as repeated, but Judge 1 did not. Precision Reca</context>
</contexts>
<marker>Uebersax, 1982</marker>
<rawString>Uebersax, J.S. (1982) &amp;quot;A Generalized Kappa Coefficient,&amp;quot; Educational and Psychological Measurement, Vol. 42, pp. 181-183.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>