<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.127158">
<title confidence="0.998877">
A real-time multiple-choice question generation for language testing
– a preliminary study–
</title>
<author confidence="0.926031">
Ayako Hoshino Hiroshi Nakagawa
</author>
<affiliation confidence="0.9468975">
Interfaculty Initiative in Information Studies Information Technology Center
University of Tokyo University of Tokyo
</affiliation>
<address confidence="0.8065305">
7-3-1 Hongo, Bunkyo, Tokyo, 7-3-1 Hongo, Bunkyo, Tokyo,
113-0033, JAPAN 113-0033, JAPAN
</address>
<email confidence="0.997081">
qq36126@iii.u-tokyo.ac.jp nakagawa@dl.itc.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.995589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982133333333">
An automatic generation of multiple-
choice questions is one of the promising
examples of educational applications of
NLP techniques. A machine learning ap-
proach seems to be useful for this pur-
pose because some of the processes can
be done by classification. Using basic ma-
chine learning algorithms as Naive Bayes
and K-Nearest Neighbors, we have devel-
oped a real-time system which generates
questions on English grammar and vocab-
ulary from on-line news articles. This pa-
per describes the current version of our
system and discusses some of the issues
on constructing this kind of system.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999345766666667">
Multiple-choice question exams are widely used and
are effective to assess students’ knowledge, how-
ever it is costly to manually produce those questions.
Naturally, this kind of task should be done with a
help of computer.
Nevertheless, there have been very few attempts
to generate multiple-choice questions automatically.
Mitkov et al.(2003) generated questions for a lin-
guistics exam in a semi-automatic way and evalu-
ated that it exceeds manually made ones in cost and
is at least equivalent in quality. There are some
other researches that involve generating questions
with multiple alternatives (Dicheva and Dimitrova,
1998). But to the best of our knowledge, no attempt
has been made to generate this kind of questions in
a totally automatic way.
This paper presents a novel approach to generate
multiple-choice questions using machine learning
techniques. The questions generated are those of fill-
in-the-blank type, so it does not involve transform-
ing declarative sentences into question sentences as
in Mitkov’s work. This simplicity makes the method
to be language independent.
Although this application can be very versatile, in
that it can be used to test any kind of knowledge as in
history exams, as a purpose of this research we limit
ourselves to testing student’s proficiency in a foreign
language. One of the purposes of this research is to
automatically extract important words or phrases in
a text for a learner of the language.
</bodyText>
<sectionHeader confidence="0.972695" genericHeader="method">
2 System Design
</sectionHeader>
<bodyText confidence="0.999972">
The system we have implemented works in a sim-
ple pipelined manner; it takes an HTML file and
turns it into the one of quiz session. The process
of converting the input to multiple-choice questions
includes extracting features, deciding the blank po-
sitions, and choosing the wrong alternatives (which
are called distractors), which are all done in a mo-
ment when the user feeds the input. When the user
submits their answer, it shows the text with the cor-
rect answers as well as an overall feed back.
</bodyText>
<sectionHeader confidence="0.998377" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.973503">
The process of deciding blank positions in a given
text follows a standard machine learning framework,
which is first training a classifier on a training data
</bodyText>
<page confidence="0.992541">
17
</page>
<note confidence="0.745248">
Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 17–20, Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005
</note>
<tableCaption confidence="0.999805">
Table 1: the full list of test instances classified as true in test-on-train
</tableCaption>
<table confidence="0.99273325">
certainty a test instance (sentence with a blank) the answer
0.808 Joseph is preparing for tomorrow’s big [ ] to the president. presentation
0.751 Ms. Singh listened [ ] to the president’s announcement. carefully
0.744 The PR person is the one in charge of [ ] meetings and finding accommoda- scheduling
tions for our associates.
0.73 Ms. Havlat received a memo from the CEO [ ] the employees’ conduct. regarding
0.718 The amount of money in the budget decreased [ ] over the past year. significantly
0.692 Mr. Gomez is [ ] quickly; however it will be a log time before he gets used to learning
the job.
0.689 The boss can never get around to [ ] off his desk. cleaning
0.629 The interest rate has been increasingly [ ] higher. getting
0.628 Employees are [ ] to comply with the rules in the handbook. asked
0.62 The lawyer [ ] his case before the court. presented
0.59 The secretary was [ ] to correspond with the client immediately. supposed
0.576 The maintenance worker checked the machine before [ ] it on. turning
0.523 The [ ] manager’s office is across the corridor. assistant
</table>
<bodyText confidence="0.9997314">
(i.e. TOEIC questions), then applying it on an un-
seen test data, (i.e. the input text). In the current sys-
tem, the mechanism of choosing distractors is imple-
mented with the simplest algorithm, and its investi-
gation is left to future work.
</bodyText>
<subsectionHeader confidence="0.999564">
3.1 Preparing the Training Data
</subsectionHeader>
<bodyText confidence="0.99939125">
The training data is a collection of fill-in-the-blank
questions from a TOEIC preparation book (Matsuno
et al., 2000). As shown in the box below, a ques-
tion consists of a sentence with a missing word (or
words) and four alternatives one of among which
best fits into the blank.
Many people showed up early to [ ] for the posi-
tion that was open.
</bodyText>
<listItem confidence="0.654735">
1. apply 2. appliance 3. applies 4. application
</listItem>
<bodyText confidence="0.9999905">
The training instances are obtained from 100
questions by shifting the blank position. The orig-
inal position is labeled as true, while sentences with
a blank in a shifted position are at first labeled as
false. The instance shown above therefore yields in-
stances [ J people showed up early to apply for the
position that was open., Many [ J showed up early
to apply for the position that was open., and so on,
all of which are labeled as false except the original
blank position. 1962 (100 true and 1862 false) in-
stances were obtained.
The label true here is supposed to indicate that
it is possible to make a question with the sentence
with a blank in the specified position, while many
of the shifted positions which are labeled false can
also be good blanks. A semi-supervised learning
(Chakrabarti, 2003) 1 is conducted in the following
manner to retrieve the instances that are potentially
true among the ones initially classified as false.
We retrieved the 13 instances (shown in Table 1.)
which had initially been labeled as false and classi-
fied as true in a test-on-train result with a certainty 2
of more than 0.5 with a Naive Bayes classifier 3. The
labels of those instances were changed to true before
re-training the classifier. In this way, a training set
with 113 true instances was obtained.
</bodyText>
<subsectionHeader confidence="0.999233">
3.2 Deciding Blank Positions
</subsectionHeader>
<bodyText confidence="0.999801">
For the current system we use news articles from
BBC.com 4, which consist approximately 200-500
words. The test text goes through tagging and fea-
ture extraction in the same manner as the training
</bodyText>
<footnote confidence="0.8362975">
1Semi-supervised learning is a method to identify the class
of unclassified instances in the dataset where only some of the
instances are classified.
2The result of a classification of a instance is obtained along
with a certainty value between 0.0 to 1.0 for each class, which
indicates how certain it is that an instance belongs to the class.
3Seven features which are word, POS, POS of the previous
word, POS of the next word, position in the sentence, sentence
length, word length and were used.
4http://news.bbc.co.uk/
</footnote>
<page confidence="0.998375">
18
</page>
<bodyText confidence="0.9999212">
data, and the instances are classified into true or
false. The positions of the blanks are decided ac-
cording to the certainty of the classification so the
blanks (i.e. questions) are generated as many as the
user has specified.
</bodyText>
<subsectionHeader confidence="0.999605">
3.3 Choosing Distractors
</subsectionHeader>
<bodyText confidence="0.99989325">
In the current version of the system, the distractors
are chosen randomly from the same article exclud-
ing punctuations and the same word as the other al-
ternatives.
</bodyText>
<sectionHeader confidence="0.957317" genericHeader="method">
4 Current system
</sectionHeader>
<bodyText confidence="0.9956474">
The real-time system we are presenting is imple-
mented as a Java servlet, whose one of the main
screens is shown below. The tagger used here is the
Tree tagger (Schmid, 1994), which uses the Penn-
Treebank tagset.
</bodyText>
<figureCaption confidence="0.9260675">
Figure 1: a screen shot of the question session page
with an enlarged answer selector.
</figureCaption>
<bodyText confidence="0.999516894736842">
The current version of the system is avail-
able at http://www.iii.u-tokyo.ac.jp/
˜qq36126/mcwa1/. The interface of the system
consists of three sequenced web pages, namely 1)the
parameter selection page, 2)the quiz session page
and 3)the result page.
The parameter selection page shows the list of the
articles which are linked from the top page of the
BBC website, along with the option selectors for
number of blanks (5-30) and the classifier (Naive
Bayes or Nearest Neighbors).
The question session page is shown in Figure 1. It
displays the headline and the image from the chosen
article under the title and a brief instruction. The
alternatives are shown on option selectors, which are
placed in the article text.
The result page shows the text with the right an-
swers shown in green when the user’s choice is cor-
rect, red when it is wrong.
</bodyText>
<sectionHeader confidence="0.995639" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999936810810811">
To examine the quality of the questions generated
by the current system, we have evaluated the blank
positions determined by a Naive Bayes classifier and
a KNN classifier (K=3) with a certainty of more than
50 percent in 10 articles.
Among 3138 words in total, 361 blanks were
made and they were manually evaluated according
to their possibility of being a multiple-choice ques-
tion, with an assumption of having alternatives of
the same part of speech. The blank positions were
categorized into three groups, which are E (possible
to make a question), and D (difficult, but possible to
make a question), NG (not possible or not suitable
e.g. on a punctuation). The guideline for deciding
E or D was if a question is on a grammar rule, or it
requires more semantic understanding, for instance,
a background knowledge 5.
Table 2. shows the comparison of the number of
blank positions decided by the two classifiers, each
with a breakdown for each evaluation. The num-
ber in braces shows the proportion of the blanks
with a certain evaluation over the total number of
blanks made by the classifier. The rightmost column
I shows the number of the same blank positions se-
lected by both classifiers.
The KNN classifier tends to be more accurate and
seems to be more robust, although given the fact that
it produces less blanks. The fact that an instance-
based algorithm exceeds Naive Bayes, whose deci-
sion depends on the whole data, can be ascribed to
a mixed nature of the training data. For example,
blanks for grammar questions might have different
features from ones for vocabulary questions.
The result we sampled has exhibited another
problem of Naive Bayes algorithm. In two articles
among the data, it has shown the tendency to make a
blank on be-verbs. Naive Bayes tends to choose the
</bodyText>
<footnote confidence="0.962271">
5A blank on a verbs or a part of idioms (as [according] to)
was evaluated as E, most of the blanks on an adverbs, and (as
[now]) were D and a blank on a punctuation or a quotation mark
was NG.
</footnote>
<page confidence="0.999155">
19
</page>
<tableCaption confidence="0.999455">
Table 2: The evaluation on the blank positions decided by a Naive Bayes (NB) and a KNN classifier.
</tableCaption>
<table confidence="0.999491153846154">
NB KNN I
blanks E(%) D(%) NG(%) blanks E(%) D(%) NG(%) blanks
Article1 69 44(63.8) 21(30.4) 4(5.8) 33 20(60.6) 11(33.3) 2(6.1) 18
Article2 22 5(22.7) 3(13.6) 14(63.6) 8 5(62.5) 3(37.5) 0(0.0) 0
Article3 38 21(55.3) 15(39.5) 2(5.3) 18 12(66.7) 5(27.8) 1(5.6) 8
Article4 19 10(52.6) 9(47.4) 0(0.0) 9 7(77.8) 2(22.2) 0(0.0) 3
Article5 28 18(64.3) 10(35.7) 0(0.0) 14 10(71.4) 4(28.6) 0(0.0) 6
Article6 26 17(65.4) 8(30.8) 1(3.8) 11 6(54.5) 5(45.5) 0(0.0) 4
Article7 18 9(50.0) 5(27.8) 4(22.2) 6 3(50.0) 3(50.0) 0(0.0) 3
Article8 24 14(58.3) 9(37.5) 1(4.2) 5 3(60.0) 2(40.0) 0(0.0) 5
Article9 20 16(80.0) 4(20.0) 0(0.0) 6 2(33.3) 4(66.7) 0(0.0) 4
Article10 30 18(60.0) 12(40.0) 0(0.0) 14 11(78.6) 3(21.4) 0(0.0) 6
294 172(58.5) 96(32.7) 26(8.8) 124 79(63.7) 42(33.9) 3(2.4) 57
</table>
<bodyText confidence="0.999813">
same word as a blank position, therefore generates
many questions on the same word in one article.
Another general problem of these methods would
be that the blank positions are decided without con-
sideration of one another; the question will be some-
times too difficult when another blank is next to or
in the vicinity of the blank.
</bodyText>
<sectionHeader confidence="0.998621" genericHeader="evaluation">
6 Discussion and Future work
</sectionHeader>
<bodyText confidence="0.998035739130435">
From the problems of the current system, we can
conclude that the feature set we have used is not suf-
ficient. It is necessary that we use larger number
of features, possibly including semantic ones, so a
blank position would not depend on its superficial
aspects. Also, the training data should be examined
in more detail.
As it was thought to be a criteria of evaluating
generated questions, if a question requires simply a
grammatical knowledge or a farther knowledge (i.e.
background knowledge) can be a critical property of
a generated question. We should differentiate the
features from the ones which are used to generate,
for example, history questions, which require rather
background knowledge. Selecting suitable distrac-
tors, which is left to future work, would be a more
important process in generating a question. A se-
mantic distance between an alternative and the right
answer are suggested (Mitkov and Ha, 2003), to be
a good measure to evaluate an alternative. We are
investigating on a method of measuring those dis-
tances and a mechanism to retrieve best alternatives
automatically.
</bodyText>
<sectionHeader confidence="0.999212" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999995375">
We have presented a novel application of automat-
ically generating fill-in-the-blank, multiple-choice
questions using machine learning techniques, as
well as a real-time system implemented. Although
it is required to explore more feature settings for the
process of determining blank positions, and the pro-
cess of choosing distractors needs more elaboration,
the system has proved to be feasible.
</bodyText>
<sectionHeader confidence="0.999254" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999718722222222">
Soumen Chakrabarti. 2003. Mining the Web. Morgan
Kaufmann Publishers.
Darina Dicheva and Vania Dimitrova. 1998. An ap-
proach to representation and extraction of terminolog-
ical knowledge in icall. In Journal of Computing and
Information Technology, pages 39 – 52.
Shuhou Matsuno, Tomoko Miyahara, and Yoshi Aoki.
2000. STEP-UP Bunpo mondai TOEIC TEST. Kiri-
hara Publisher.
Ruslan Mitkov and Le An Ha. 2003. Computer-aided
generation of multiple-choice tests. In Proceedings of
the HLT-NAACL 2003 Workshop on Building Educa-
tional Applications Using Natural Language Process-
ing, pages 17 – 22, Edmonton, Canada, May.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing, Manchester, UK, September.
</reference>
<page confidence="0.994898">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.567615">
<title confidence="0.987489">A real-time multiple-choice question generation for language testing – a preliminary study–</title>
<author confidence="0.993378">Ayako Hoshino Hiroshi Nakagawa</author>
<affiliation confidence="0.9986545">Interfaculty Initiative in Information Studies Information Technology Center University of Tokyo University of Tokyo</affiliation>
<address confidence="0.9739245">7-3-1 Hongo, Bunkyo, Tokyo, 7-3-1 Hongo, Bunkyo, Tokyo, 113-0033, JAPAN 113-0033,</address>
<email confidence="0.608475">qq36126@iii.u-tokyo.ac.jpnakagawa@dl.itc.u-tokyo.ac.jp</email>
<abstract confidence="0.9996855">An automatic generation of multiplechoice questions is one of the promising examples of educational applications of NLP techniques. A machine learning approach seems to be useful for this purpose because some of the processes can be done by classification. Using basic machine learning algorithms as Naive Bayes and K-Nearest Neighbors, we have developed a real-time system which generates questions on English grammar and vocabulary from on-line news articles. This paper describes the current version of our system and discusses some of the issues on constructing this kind of system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Soumen Chakrabarti</author>
</authors>
<title>Mining the Web.</title>
<date>2003</date>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="5932" citStr="Chakrabarti, 2003" startWordPosition="984" endWordPosition="985">st labeled as false. The instance shown above therefore yields instances [ J people showed up early to apply for the position that was open., Many [ J showed up early to apply for the position that was open., and so on, all of which are labeled as false except the original blank position. 1962 (100 true and 1862 false) instances were obtained. The label true here is supposed to indicate that it is possible to make a question with the sentence with a blank in the specified position, while many of the shifted positions which are labeled false can also be good blanks. A semi-supervised learning (Chakrabarti, 2003) 1 is conducted in the following manner to retrieve the instances that are potentially true among the ones initially classified as false. We retrieved the 13 instances (shown in Table 1.) which had initially been labeled as false and classified as true in a test-on-train result with a certainty 2 of more than 0.5 with a Naive Bayes classifier 3. The labels of those instances were changed to true before re-training the classifier. In this way, a training set with 113 true instances was obtained. 3.2 Deciding Blank Positions For the current system we use news articles from BBC.com 4, which consi</context>
</contexts>
<marker>Chakrabarti, 2003</marker>
<rawString>Soumen Chakrabarti. 2003. Mining the Web. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darina Dicheva</author>
<author>Vania Dimitrova</author>
</authors>
<title>An approach to representation and extraction of terminological knowledge in icall.</title>
<date>1998</date>
<booktitle>In Journal of Computing and Information Technology,</booktitle>
<pages>39--52</pages>
<contexts>
<context position="1624" citStr="Dicheva and Dimitrova, 1998" startWordPosition="235" endWordPosition="238">ultiple-choice question exams are widely used and are effective to assess students’ knowledge, however it is costly to manually produce those questions. Naturally, this kind of task should be done with a help of computer. Nevertheless, there have been very few attempts to generate multiple-choice questions automatically. Mitkov et al.(2003) generated questions for a linguistics exam in a semi-automatic way and evaluated that it exceeds manually made ones in cost and is at least equivalent in quality. There are some other researches that involve generating questions with multiple alternatives (Dicheva and Dimitrova, 1998). But to the best of our knowledge, no attempt has been made to generate this kind of questions in a totally automatic way. This paper presents a novel approach to generate multiple-choice questions using machine learning techniques. The questions generated are those of fillin-the-blank type, so it does not involve transforming declarative sentences into question sentences as in Mitkov’s work. This simplicity makes the method to be language independent. Although this application can be very versatile, in that it can be used to test any kind of knowledge as in history exams, as a purpose of thi</context>
</contexts>
<marker>Dicheva, Dimitrova, 1998</marker>
<rawString>Darina Dicheva and Vania Dimitrova. 1998. An approach to representation and extraction of terminological knowledge in icall. In Journal of Computing and Information Technology, pages 39 – 52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuhou Matsuno</author>
<author>Tomoko Miyahara</author>
<author>Yoshi Aoki</author>
</authors>
<title>STEP-UP Bunpo mondai TOEIC TEST.</title>
<date>2000</date>
<publisher>Kirihara Publisher.</publisher>
<contexts>
<context position="4848" citStr="Matsuno et al., 2000" startWordPosition="785" endWordPosition="788">rt. presented 0.59 The secretary was [ ] to correspond with the client immediately. supposed 0.576 The maintenance worker checked the machine before [ ] it on. turning 0.523 The [ ] manager’s office is across the corridor. assistant (i.e. TOEIC questions), then applying it on an unseen test data, (i.e. the input text). In the current system, the mechanism of choosing distractors is implemented with the simplest algorithm, and its investigation is left to future work. 3.1 Preparing the Training Data The training data is a collection of fill-in-the-blank questions from a TOEIC preparation book (Matsuno et al., 2000). As shown in the box below, a question consists of a sentence with a missing word (or words) and four alternatives one of among which best fits into the blank. Many people showed up early to [ ] for the position that was open. 1. apply 2. appliance 3. applies 4. application The training instances are obtained from 100 questions by shifting the blank position. The original position is labeled as true, while sentences with a blank in a shifted position are at first labeled as false. The instance shown above therefore yields instances [ J people showed up early to apply for the position that was</context>
</contexts>
<marker>Matsuno, Miyahara, Aoki, 2000</marker>
<rawString>Shuhou Matsuno, Tomoko Miyahara, and Yoshi Aoki. 2000. STEP-UP Bunpo mondai TOEIC TEST. Kirihara Publisher.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
<author>Le An Ha</author>
</authors>
<title>Computer-aided generation of multiple-choice tests.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 2003 Workshop on Building Educational Applications Using Natural Language Processing, pages 17 – 22,</booktitle>
<location>Edmonton, Canada,</location>
<marker>Mitkov, Ha, 2003</marker>
<rawString>Ruslan Mitkov and Le An Ha. 2003. Computer-aided generation of multiple-choice tests. In Proceedings of the HLT-NAACL 2003 Workshop on Building Educational Applications Using Natural Language Processing, pages 17 – 22, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, UK,</location>
<contexts>
<context position="7791" citStr="Schmid, 1994" startWordPosition="1299" endWordPosition="1300">k/ 18 data, and the instances are classified into true or false. The positions of the blanks are decided according to the certainty of the classification so the blanks (i.e. questions) are generated as many as the user has specified. 3.3 Choosing Distractors In the current version of the system, the distractors are chosen randomly from the same article excluding punctuations and the same word as the other alternatives. 4 Current system The real-time system we are presenting is implemented as a Java servlet, whose one of the main screens is shown below. The tagger used here is the Tree tagger (Schmid, 1994), which uses the PennTreebank tagset. Figure 1: a screen shot of the question session page with an enlarged answer selector. The current version of the system is available at http://www.iii.u-tokyo.ac.jp/ ˜qq36126/mcwa1/. The interface of the system consists of three sequenced web pages, namely 1)the parameter selection page, 2)the quiz session page and 3)the result page. The parameter selection page shows the list of the articles which are linked from the top page of the BBC website, along with the option selectors for number of blanks (5-30) and the classifier (Naive Bayes or Nearest Neighbo</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing, Manchester, UK, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>