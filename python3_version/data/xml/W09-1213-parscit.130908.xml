<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000534">
<title confidence="0.988726">
Multilingual Semantic Role Labelling with Markov Logic
</title>
<author confidence="0.999299">
Ivan Meza-Ruiz* Sebastian Riedeltt
</author>
<affiliation confidence="0.983825">
*School of Informatics, University of Edinburgh, UK
bepartment of Computer Science, University of Tokyo, Japan
tDatabase Center for Life Science, Research Organization of Information and System, Japan
</affiliation>
<email confidence="0.995039">
*I.V.Meza-Ruiz@sms.ed.ac.uk tsebastian.riedel@gmail.com
</email>
<sectionHeader confidence="0.998577" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993869">
This paper presents our system for the CoNLL
2009 Shared Task on Syntactic and Semantic
Dependencies in Multiple Languages (Hajiˇc
et al., 2009). In this work we focus only on the
Semantic Role Labelling (SRL) task. We use
Markov Logic to define a joint SRL model and
achieve the third best average performance in
the closed Track for SRLOnly systems and the
sixth including for both SRLOnly and Joint
systems.
</bodyText>
<sectionHeader confidence="0.975266" genericHeader="keywords">
1 Markov Logic
</sectionHeader>
<bodyText confidence="0.99772795">
Markov Logic (ML, Richardson and Domingos,
2006) is a Statistical Relational Learning language
based on First Order Logic and Markov Networks.
It can be seen as a formalism that extends First Or-
der Logic to allow formulae that can be violated with
some penalty. From an alternative point of view, it is
an expressive template language that uses First Or-
der Logic formulae to instantiate Markov Networks
of repetitive structure.
In the ML framework, we model the SRL task
by first introducing a set of logical predicates1 such
as word(Token,Ortho) or role(Token,Token,Role). In
the case of word/2 the predicate represents a word
of a sentence, the type Token identifies the position
of the word and the type Ortho its orthography. In
the case of role/3, the predicate represents a seman-
tic role. The first token identifies the position of the
predicate, the second the syntactic head of the argu-
ment and finally the type Role signals the semantic
role label. We will refer to predicates such as word/2
</bodyText>
<footnote confidence="0.996039">
1In the cases were is not obvious whether we refer to SRL
or ML predicates we add the prefix SRL or ML, respectively.
</footnote>
<page confidence="0.998602">
85
</page>
<bodyText confidence="0.999927181818182">
as observed because they are known in advance. In
contrast, role/3 is hidden because we need to infer it
at test time.
With the ML predicates we specify a set of
weighted first order formulae that define a distribu-
tion over sets of ground atoms of these predicates (or
so-called possible worlds). A set of weighted formu-
lae is called a Markov Logic Network (MLN). For-
mally speaking, an MLN M is a set of pairs (0, w)
where 0 is a first order formula and w a real weight.
M assigns the probability
</bodyText>
<equation confidence="0.9936515">
p(y) = Z exp w (y) (1)
((φ,w)EM cECφ
</equation>
<bodyText confidence="0.972699708333333">
to the possible world y. Here
is the set of all
possible bindings of the free variables in 0 with the
constants of our domain.
is a feature function
that returns 1 if in the possible world y the ground
formula we get by replacing the free variables in 0
by the constants in c is true and 0 otherwise. Z
is a normalisation constant. Note that this distri-
bution corresponds to a Markov Network (the so-
called Ground Markov Network) where nodes repre-
sent ground atoms and factors represent ground for-
mulae.
In this work we use 1-best MIRA (Crammer and
Singer, 2003) Online Learning in order to train the
weights of an MLN. To find the SRL assignment
with maximal a posteriori probability according to
an MLN and observed sentence, we use Cutting
Plane Inference (CPI, Riedel, 2008) with ILP base
solver. This method is used during both test time
an
Cφ
fφc
d the MIRA online learning process.
</bodyText>
<subsubsectionHeader confidence="0.293973">
Proceedings of the Thirteenth Conference on Computational Natural Language Learn
</subsubsectionHeader>
<page confidence="0.277442">
ing (CoNLL): Shared Task, pages 85–90,
</page>
<note confidence="0.589789">
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.984914" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.999979163636364">
In order to model the SRL task in the ML frame-
work, we propose four hidden predicates. Consider
the example of the previous section:
argument/1 indicates the phrase for which its head
is a specific position is an SRL argument.
In our example argument(2) signals that the
phrase for which the word in position 2 is its
head is an argument (i.e., Ms. Haag).
hasRole/2 relates a SRL predicate to a SRL argu-
ment. For example, hasRole(3,2) relates the
predicate in position 3 (i.e., play) to the phrase
which head is in position 2 (i.e., Ms. Haag).
role/3 identifies the role for a predicate-argument
pair. For example, role(3,2,ARG0) denotes the
role ARG0 for the SRL predicate in the posi-
tion 2 and the SRL argument in position 3.
sense/2 denotes the sense of a predicate at a specific
position. For example, sense(3,02) signals that
the predicate in position 3 has the sense 02.
We also define three sets of observable predicates.
The first set represents information about each token
as provided in the shared task corpora for the closed
track: word for the word form (e.g. word(3,plays));
plemma/2 for the lemma; ppos/2 for the POS tag;
feat/3 for each feature-value pair; dependency/3 for
the head dependency and relation; predicate/1 for
tokens that are predicates according to the “FILL-
PRED” column. We will refer to these predicates as
the token predicates.
The second set extends the information provided
in the closed track corpus: cpos/2 is a coarse POS
tag (first letter of actual POS tag); possibleArg/1 is
true if the POS tag the token is a potential SRL argu-
ment POS tag (e.g., PUNC is not); voice/2 denotes
the voice for verbal tokens based on heuristics that
use syntactic information, or based on features in the
FEAT column of the data. We will refer to these
predicates as the extended predicates.
Finally, the third set represents dependency infor-
mation inspired by the features proposed by Xue and
Palmer (2004). There are two types of predicates
in this set: paths and frames. Paths capture the de-
pendency path between two tokens, and frames the
subcategorisation frame for a token or a pair of to-
kens. There are directed and undirected versions of
paths, and labelled (with dependency relations) and
unlabelled versions of paths and frames. Finally, we
have a frame predicate with the distance from the
predicate to its head. We will refer to the paths and
most of the frames predicates as the path predicates,
while we will consider the frame predicates for a
unique token part token predicates.
The ML predicates here presented are used within
the formulae of our MLN. We distinguish between
two types of formula: local and global.
</bodyText>
<subsectionHeader confidence="0.99665">
2.1 Local formulae
</subsectionHeader>
<bodyText confidence="0.9994035">
A formula is local if its groundings relate any num-
ber of observed ground atoms to exactly one hidden
ground atom. For example, a grounding of the local
formula
</bodyText>
<equation confidence="0.622963">
lemma(p, +l1)nlemma(a, +l2) ==&gt;. hasRole(p, a)
</equation>
<bodyText confidence="0.997839965517242">
connects a hidden hasRole/2 ground atom to two ob-
served plemma/2 ground atoms. This formula can be
interpreted as the feature for the predicate and argu-
ment lemmas in the argument identification stage of
a pipeline SRL system. Note that the “+” prefix indi-
cates that there is a different weight for each possible
pair of lemmas (l1, l2).
We divide our local formulae into four sets, one
for each hidden predicate. For instance, the set for
argument/1 only contains formulae in which the hid-
den predicate is argument/1.
The sets for argument/1 and sense/2 predicates
have similar formulae since each predicate only in-
volves one token at time: the SRL argument or the
SRL predicate token. The formulae in these sets are
defined using only token or extended observed pred-
icates.
There are two differences between the argument/1
and sense/2 formulae. First, the argument/1 for-
mulae use the possibleArg/1 predicate as precondi-
tion, while the sense formulae are conditioned on
the predicate/1 predicate. For instance, consider the
argument/1 formula based on word forms:
word(a,+w) n possibleArg(a) ==&gt;. argument(a),
and the equivalent version for the sense/2 predicate:
word(p,+w) n predicate(p) ==&gt;. sense(p,+s).
This means we only apply the argument/1 formulae
if the token is a potential SRL argument, and the
sense/2 formulae if the token is a SRL predicate.
</bodyText>
<page confidence="0.980018">
86
</page>
<bodyText confidence="0.998127">
The second difference is the fact that for the
sense/2 formulae we have different weights for each
possible sense (as indicated by the +s term in the
second formula above), while for the argument/1
formulae this is not the case. This follows naturally
from the fact that argument/1 do not explicitly con-
sider senses.
Table 1 presents templates for the local formuale
of argument/1 and sense/2. Templates allow us to
compactly describe the FOL clauses of a ML. The
template column shows the body of a clause. The
last two columns of the table indicate if there is a
clause with the given body and argument(i) (I) or
sense(i, +s) (S) head, respectively. For example,
consider the first row: since the last two columns
of the row are marked, this template expands into
two formulae: word(i,+w) ⇒ argument(i) and
word(i, +w) ⇒ sense(i, +s). Including the pre-
conditions for each hidden predicate we obtain the
following formulae:
</bodyText>
<equation confidence="0.879004">
possibleArg(i) ∧ word(i,+w) ⇒ argument(i)
and
predicate(i) ∧ word(i, +w) ⇒ sense(i, +s).
</equation>
<bodyText confidence="0.999884083333333">
In the case of the template marked with a “*”
sign, the parameters P and I, where P ∈
{ppos, plemma} and I ∈ {−2,−1,0,1,2}, have to
be replaced by any combination of possible values.
Since we generate argument and sense formulae
for this template, the row corresponds to 20 formu-
lae in total.
Table 2 shows the local formuale for hasRole/2
and role/3 predicates, for these formulae we use to-
ken, extended and path predicates. In this case,
these templates have as precondition the formula
predicate(p) ∧ possibleArg(a). This ensures that
the formulae are only applied for SRL predicates
and potential SRL arguments. In the table we in-
clude the values to replace the template parame-
ters with. Some of these formulae capture a no-
tion of distance between SRL predicate and SRL
argument and are implicitely conjoined with a
distance(p, a, +d) atom. If a formulae exists both
with and without distance atom, we write Both in
the “Dist” column; if it only exists with the distance
atom, we write Only, otherwise No.
Note that Tables 1 and 2 do not mention
the feature information provided in the cor-
</bodyText>
<table confidence="0.999304125">
Template I S
word(i, +w) X X
P(i+I,+v)* X X
cpos(i + 1, +c1) ∧ cpos(i − 1, +c2) X X
cpos(i + 1, +c1) ∧ cpos(i − 1, +c2) ∧ X X
cpos(i + 2, +c3) ∧ cpos(i − 2, +c4) X X
dep(i, , +d)
dep( , i, +d) X X
ppos(i, +o) ∧ dep(i, j, +d) X X
ppos(i, +o1) ∧ ppos(j, +o2) ∧ X X
dep(i, j, +d) X X
ppos(j, +o1) ∧ ppos(k, +o2) ∧ X X
dep(j, k, ) ∧ dep(k, i, +d)
plemma(i, +l) ∧ dep(j, i, +d)
frame(i, +f) X X
(Empty Body) X
</table>
<tableCaption confidence="0.740348">
Table 1: Templates of the local formulae for argument/1
and sense/2. I: head of clause is argument(i), S: head of
clause is sense(i, +s)
</tableCaption>
<bodyText confidence="0.9989128">
pora because this information was not avail-
able for every language. We therefore group
the formulae which consider the feature/3 pred-
icate into another a set we call feature formu-
lae. This is the summary of these formulae:
</bodyText>
<equation confidence="0.996384">
feat(p, +f, +v) ⇒ sense(p, +s)
feat(p, +f, +v) ⇒ argument(a)
feat(p, +f, +v1) ∧ feat(p, f, +v2) ⇒
hasRole(p, a)
feat(p, +f, +v1) ∧ feat(p, f, +v2) ⇒
role(p, a, +r)
</equation>
<bodyText confidence="0.997334">
Additionally, we define a set of language spe-
cific formulae. They are aimed to capture the re-
lations between argument and its siblings for the
hasRole/2 and role/3 predicates. In practice in
turned out that these formulae were only beneficial
for the Japanese language. This is a summary of
such formulae which we called argument siblings:
</bodyText>
<construct confidence="0.903245625">
dep(a, h, ) ∧ dep(h, c, ) ∧ ppos(a, +p1)∧
ppos(c, +p2) ⇒ hasRole(p, a)
dep(a, h, ) ∧ dep(h, c, ) ∧ ppos(a, +p1)∧
ppos(c, +p2) ⇒ role(p, a, +r)
dep(a, h, ) ∧ dep(h, c, ) ∧ plemma(a, +p1)∧
ppos(c, +p2) ⇒ hasRole(p, a)
dep(a, h, ) ∧ dep(h, c, ) ∧ plemma(a, +p1)∧
ppos(c, +p2) ⇒ role(p, a, +r)
</construct>
<bodyText confidence="0.999938">
With these sets of formulae we can build specific
MLNs for each language in the shared task. We
group the formulae into the modules: argument/1,
</bodyText>
<page confidence="0.998013">
87
</page>
<table confidence="0.996331851851852">
Template Parameters Dist. H R
P(p,+v) P E S1 Both X X
plemma(p, +l) n ppos(a, +o) No X
ppos(p, +o) n plemma(a, +l) No X
plemma(p, +l1) n plemma(a, +l2) Only X X
ppos(p, +o1) n ppos(a, +o2) Only X
ppos(p, +o1) n ppos(a + I, +o2) I E {−1, 0,1} Only X
plemma(p, +l) Only X
voice(p, +e) n lemma(a, +l) Only X
cpos(p, +c1) n cpos(p + I, +c2) n cpos(a, +c3) n cpos(a + J, c4) I, J E {−1,1}2 No X X
ppos(p, +v1) n ppos(a, IN) n dep(a, m, ) n P(m, +v2) P E S1 No X X
plemma(p,+v1) n ppos(a,IN) n dep(a,m, ) n ppos(m,+v2) No X X
P(p, a, +v) P E S2 No X X
P(p,a,+v) n plemma(p,+l) P E S3 No X X
P(p,a,+v) n plemma(p, +l1) n plemma(a,+l2) P E S4 No X X
pathFrame(p, a, +t) n plemma(p, +l) n voice(p, +e) No X X
pathFrameDist(p, a, +t) Only X X
pathFrameDist(p, a, +t) n voice(p, +e) Only X X
pathFrameDist(p, a, +t) n plemma(p, +l) Only X X
P(p, a, +v) n plemma(a, +l) P E S5 Only X X
P(p, a, +v) n ppos(p, +o) P E S5 Only X X
pathFrameDist(p, a, +t) n ppos(p, +o1) n ppos(a, +o2) Only X X
path(p, a, +t) n plemma(p, +l) n cpos(a, +c) Only X X
dep( , a, +d) Only X X
dep( , a, +) n voice(p, +e) Only X X
dep( , a, +d1) n dep( , p, +d2) Only X X
(EmptyBody) No X X
</table>
<tableCaption confidence="0.998003">
Table 2: Templates of the local formulae for hasRole/2 and role/3. H: head of clause is hasRole(p, a), R:
</tableCaption>
<bodyText confidence="0.945684923076923">
head of clause is role(p, a, +r) and S1 = {ppos, plemma}, S2 = {frame, unlabelFrame, path}, S3 =
{frame, pathFrame}, S4 = {frame, pathFrame, path}, S5 = {pathFrameDist, path}
hasRole/2, role/3, sense/3, feature and argument sib-
lings. Table 3 shows the different configurations of
such modules that we used for the individual lan-
guages. We omit to mention the argument/1, has-
Role/2 and role/3 modules because they are present
for all languages.
A more detailed description of the formulae can
be found in our MLN model files.2 They can be
used both as a reference and as input to our Markov
Logic Engine,3 and thus allow the reader to easily
reproduce our results.
</bodyText>
<subsectionHeader confidence="0.99385">
2.2 Global formulae
</subsectionHeader>
<bodyText confidence="0.999646">
Global formulae relate several hidden ground atoms.
We use them for two purposes: to ensure consis-
</bodyText>
<footnote confidence="0.999560333333333">
2http://thebeast.googlecode.com/svn/
mlns/conll09
3http://thebeast.googlecode.com
</footnote>
<table confidence="0.999815888888889">
Set Feature sense/2 Argument
siblings
Catalan Yes Yes No
Chinese No Yes No
Czech Yes No No
English No Yes No
German Yes Yes No
Japanese Yes No Yes
Spanish Yes Yes No
</table>
<tableCaption confidence="0.989792">
Table 3: Different configuration of the modules for the
formulae of the languages.
</tableCaption>
<page confidence="0.998828">
88
</page>
<bodyText confidence="0.999967">
tency between the decisions of all SRL stages and
to capture some of our intuition about the task. We
will refer to formulae that serve the first purpose
as structural constraints. For example, a structural
constraint is given by the (deterministic) formula
</bodyText>
<equation confidence="0.483499">
role(p, a, r) ⇒ hasRole(p, a)
</equation>
<bodyText confidence="0.9998575">
which ensures that, whenever the argument a is
given a label r with respect to the predicate p, this
argument must be an argument of a as denoted by
hasRole(p,a).
The global formulae that capture our intuition
about the task itself can be further divided into two
classes. The first one uses deterministic or hard con-
straints such as
</bodyText>
<equation confidence="0.601018">
role(p, a, r1) ∧ r1 =6 r2 ⇒ ¬role(p, a, r2)
</equation>
<bodyText confidence="0.999651">
which forbids cases where distinct arguments of a
predicate have the same role unless the role de-
scribes a modifier.
The second class of global formulae is soft or non-
deterministic. For instance, the formula
</bodyText>
<equation confidence="0.884999">
lemma(p, +l) ∧ ppos(a, +p)
∧hasRole(p, a) ⇒ sense(p, +f)
</equation>
<bodyText confidence="0.998974333333333">
is a soft global formula. It captures the observation
that the sense of a verb or noun depends on the type
of its arguments. Here the type of an argument token
is represented by its POS tag.
Table 4 presents the global formulae used in this
model.
</bodyText>
<sectionHeader confidence="0.999806" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.9989678">
For our experiments we use the corpora provided in
the SRLonly track of the shared task. Our MLN
is tested on the following languages: Catalan and
Spanish (Taul´e et al., 2008) , Chinese (Palmer and
Xue, 2009), Czech (Hajiˇc et al., 2006),4 English
(Surdeanu et al., 2008), German (Burchardt et al.,
2006), Japanese (Kawahara et al., 2002).
Table 5 presents the F1-scores and training/test
times for the development and in-domain corpora.
Clearly, our model does better for English. This is
</bodyText>
<footnote confidence="0.9962015">
4For training we use only sentences shorter than 40 words in
this corpus.
</footnote>
<table confidence="0.999766">
Structural constraints
hasRole(p, a) ⇒ argument(a)
role(p, a, r) ⇒ hasRole(p, a)
argument(a) ⇒ ∃p.hasRole(p, a)
hasRole(p, a) ⇒ ∃r.role(p, a, r)
Hard constraints
role(p, a, r1) ∧ r1 =6 r2 ⇒ ¬role(p, a, r2)
sense(p,s1) ∧ s1 =6 s2 ⇒ ¬sense(p,r2)
role (p, a1, r) ∧ ¬mod (r) ∧ a1 =6 a2 ⇒
¬role (p, a2, r)
Soft constraints
role (p, a1, r) ∧ ¬mod (r) ∧ a1 =6 a2 ⇒
¬role (p, a2, r)
plemma(p, +l)∧ppos(a, +p)∧hasRole(p, a) ⇒
sense(p, +f)
plemma(p, +l) ∧ role(p, a, +r) ⇒ sense(p, +f)
</table>
<tableCaption confidence="0.97677">
Table 4: Global formulae for ML model
</tableCaption>
<table confidence="0.9998834">
Language Devel Test Train Test
time time
Average 77.25% 77.46% 11h 29m 23m
Catalan 78.10% 78.00% 6h 11m 14m
Chinese 77.97% 77.73% 36h 30m 34m
Czech 75.98% 75.75% 14h 21m 1h 7m
English 82.28% 83.34% 12h 26m 16m
German 72.05% 73.52% 2h 28m 7m
Japanese 76.34% 76.00% 2h 17m 4m
Spanish 78.03% 77.91% 6h 9m 16m
</table>
<tableCaption confidence="0.97689">
Table 5: F-scores for in-domain in corpora for each lan-
guage.
</tableCaption>
<bodyText confidence="0.9999847">
in part because the original model was developed for
English.
To put these results into context: our SRL system
is the third best in the SRLOnly track of the Shared
Task, and it is the sixth best on both Joint and SR-
LOnly tracks. For five of the languages the differ-
ence to the F1 scores of the best system is 3%. How-
ever, for German it is 6.19% and for Czech 10.76%.
One possible explanation for the poor performance
on Czech data will be given below. Note that in com-
parison our system does slightly better in terms of
precision than in terms of recall (we have the fifth
best average precision and the eighth average recall).
Table 6 presents the F1 scores of our system for
the out of domain test corpora. We observe a similar
tendency: our system is the sixth best for both Joint
and SRLOnly tracks. We also observe similar large
differences between our scores and the best scores
for German and Czech (i.e., &gt; 7.5%), while for En-
glish the difference is relatively small (i.e., &lt; 3%).
</bodyText>
<page confidence="0.998463">
89
</page>
<table confidence="0.9746535">
Language Czech English German
F-score 77.34% 71.86% 62.37%
</table>
<tableCaption confidence="0.95307">
Table 6: F-scores for out-domain in corpora for each lan-
guage.
</tableCaption>
<bodyText confidence="0.999992166666667">
Finally, we evaluated the effect of the argument
siblings set of formulae introduced for the Japanese
MLN. Without this set the F-score is 69.52% for the
Japanese test set. Hence argument siblings formulae
improve performance by more than 6%.
We found that the MLN for Czech was the one
with the largest difference in performance when
compared to the best system. By inspecting our
results for the development set, we found that for
Czech many of the errors were of a rather techni-
cal nature. Our system would usually extract frame
IDs (such as “play.02”) by concatenating the lemma
of the token and outcome of the sense/2 prediction
(for the “02” part). However, in the case of Czech
some frame IDs are not based on the lemma of the
token, but on an abstract ID in a vocabulary (e.g.,
“v-w1757f1”). In these cases our heuristic failed,
leading to poor results for frame ID extraction.
</bodyText>
<sectionHeader confidence="0.999533" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999992785714286">
We presented a Markov Logic Network that per-
forms joint multi-lingual Semantic Role Labelling.
This network achieves the third best semantic F-
scores in the closed track among the SRLOnly sys-
tems of the CoNLL-09 Shared Task, and sixth best
semantic scores among SRLOnly and Joint systems
for the closed task.
We observed that the inclusion of features which
take into account information about the siblings of
the argument were beneficial for SRL performance
on the Japanese dataset. We also noticed that our
poor performance with Czech are caused by our
frame ID heuristic. Further work has to be done in
order to overcome this problem.
</bodyText>
<sectionHeader confidence="0.999645" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999863627450981">
Aljoscha Burchardt, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pad´o, and Manfred
Pinkal. The SALSA corpus: a German corpus
resource for lexical semantics. In Proceedings of
LREC-2006, Genoa, Italy, 2006.
Koby Crammer and Yoram Singer. Ultraconserva-
tive online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951–
991, 2003. ISSN 1533-7928.
Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr
Sgall, Petr Pajas, Jan ˇStˇep´anek, JiˇriHavelka,
Marie Mikulov´a, and Zdenˇek ˇZabokrtsk´y. Prague
dependency treebank 2.0, 2006.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Marti,
Lluis M`arquez, Adam Meyers, Joakim Nivre, Se-
bastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mi-
ahi Surdeanu, Nianwen Xue, and Yi Zhang. The
CoNLL-2009 shared task: Syntactic and semantic
dependencies in multiple languages. In Proceed-
ings of CoNLL-2009), Boulder, Colorado, USA,
2009.
Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti
Hasida. Construction of a Japanese relevance-
tagged corpus. In Proceedings of the LREC-2002,
pages 2008–2013, Las Palmas, Canary Islands,
2002.
Martha Palmer and Nianwen Xue. Adding semantic
roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143–172, 2009.
Matt Richardson and Pedro Domingos. Markov
logic networks. Machine Learning, 62:107–136,
2006.
Sebastian Riedel. Improving the accuracy and ef-
ficiency of map inference for markov logic. In
UAI ’08: Proceedings of the Annual Conference
on Uncertainty in AI, 2008.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluis M`arquez, and Joakim Nivre. The CoNLL-
2008 shared task on joint parsing of syntactic
and semantic dependencies. In Proceedings of
CoNLL-2008, 2008.
Mariona Taul´e, Maria Ant`onia Marti, and Marta
Recasens. AnCora: Multilevel Annotated Cor-
pora for Catalan and Spanish. In Proceedings of
LREC-2008, Marrakesh, Morroco, 2008.
Nianwen Xue and Martha Palmer. Calibrating fea-
tures for semantic role labeling. In EMNLP ’04:
Proceedings of the Annual Conference on Em-
pirical Methods in Natural Language Processing,
2004.
</reference>
<page confidence="0.998632">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999552">Multilingual Semantic Role Labelling with Markov Logic</title>
<author confidence="0.99556">Sebastian</author>
<affiliation confidence="0.981786333333333">of Informatics, University of Edinburgh, of Computer Science, University of Tokyo, Center for Life Science, Research Organization of Information and System,</affiliation>
<abstract confidence="0.994081323943662">This paper presents our system for the CoNLL 2009 Shared Task on Syntactic and Semantic Dependencies in Multiple Languages (Hajiˇc et al., 2009). In this work we focus only on the Semantic Role Labelling (SRL) task. We use Markov Logic to define a joint SRL model and achieve the third best average performance in the closed Track for SRLOnly systems and the sixth including for both SRLOnly and Joint systems. 1 Markov Logic Markov Logic (ML, Richardson and Domingos, 2006) is a Statistical Relational Learning language based on First Order Logic and Markov Networks. It can be seen as a formalism that extends First Order Logic to allow formulae that can be violated with some penalty. From an alternative point of view, it is an expressive template language that uses First Order Logic formulae to instantiate Markov Networks of repetitive structure. In the ML framework, we model the SRL task first introducing a set of logical such In case of predicate represents a word a sentence, the type the position the word and the type orthography. In case of the predicate represents a semantic role. The first token identifies the position of the predicate, the second the syntactic head of the argument and finally the type Role signals the semantic label. We will refer to predicates such as the cases were is not obvious whether we refer to SRL or ML predicates we add the prefix SRL or ML, respectively. 85 they are known in advance. In we need to infer it at test time. With the ML predicates we specify a set of weighted first order formulae that define a distribution over sets of ground atoms of these predicates (or A set of weighted formuis called a Logic Network Forspeaking, an MLN a set of pairs a first order formula and real weight. the probability = the possible world Here is the set of all bindings of the free variables in the constants of our domain. is a feature function returns 1 if in the possible world get by replacing the free variables in the constants in true and 0 otherwise. is a normalisation constant. Note that this distribution corresponds to a Markov Network (the so- Markov where nodes represent ground atoms and factors represent ground formulae. In this work we use 1-best MIRA (Crammer and Singer, 2003) Online Learning in order to train the weights of an MLN. To find the SRL assignment maximal posteriori according to an MLN and observed sentence, we use Cutting Plane Inference (CPI, Riedel, 2008) with ILP base solver. This method is used during both test time an Cφ fφc d the MIRA online learning process.</abstract>
<note confidence="0.765358">Proceedings of the Thirteenth Conference on Computational Natural Language Learn (CoNLL): Shared pages 85–90, Colorado, June 2009. Association for Computational Linguistics</note>
<abstract confidence="0.953130844827586">2 Model In order to model the SRL task in the ML framework, we propose four hidden predicates. Consider the example of the previous section: the phrase for which its head is a specific position is an SRL argument. our example that the for which the word in position its is an argument (i.e., a SRL predicate to a SRL argu- For example, the in position to the phrase head is in position the role for a predicate-argument For example, the the SRL predicate in the position 2 and the SRL argument in position 3. the sense of a predicate at a specific For example, that predicate in position the sense We also define three sets of observable predicates. The first set represents information about each token as provided in the shared task corpora for the closed the word form (e.g. the lemma; the POS tag; each feature-value pair; head dependency and relation; tokens that are predicates according to the “FILL- PRED” column. We will refer to these predicates as The second set extends the information provided the closed track corpus: a coarse POS (first letter of actual POS tag); true if the POS tag the token is a potential SRL argu- POS tag (e.g., PUNC is not); the voice for verbal tokens based on heuristics that use syntactic information, or based on features in the FEAT column of the data. We will refer to these as the Finally, the third set represents dependency information inspired by the features proposed by Xue and Palmer (2004). There are two types of predicates this set: Paths capture the dependency path between two tokens, and frames the subcategorisation frame for a token or a pair of tokens. There are directed and undirected versions of paths, and labelled (with dependency relations) and unlabelled versions of paths and frames. Finally, we have a frame predicate with the distance from the predicate to its head. We will refer to the paths and of the frames predicates as the we will consider the for a token part The ML predicates here presented are used within the formulae of our MLN. We distinguish between two types of formula: local and global. 2.1 Local formulae A formula is local if its groundings relate any number of observed ground atoms to exactly one hidden ground atom. For example, a grounding of the local formula a hidden atom to two obatoms. This formula can be interpreted as the feature for the predicate and argument lemmas in the argument identification stage of a pipeline SRL system. Note that the “+” prefix indicates that there is a different weight for each possible of lemmas We divide our local formulae into four sets, one for each hidden predicate. For instance, the set for contains formulae in which the hidpredicate is sets for have similar formulae since each predicate only involves one token at time: the SRL argument or the SRL predicate token. The formulae in these sets are using only predicates. are two differences between the First, the foruse the as precondition, while the sense formulae are conditioned on For instance, consider the based on word forms: the equivalent version for the means we only apply the if the token is a potential SRL argument, and the if the token is a SRL predicate. 86 The second difference is the fact that for the we have different weights for each sense (as indicated by the in the formula above), while for the formulae this is not the case. This follows naturally the fact that not explicitly consider senses. Table 1 presents templates for the local formuale Templates allow us to compactly describe the FOL clauses of a ML. The template column shows the body of a clause. The last two columns of the table indicate if there is a with the given body and or head, respectively. For example, consider the first row: since the last two columns of the row are marked, this template expands into formulae: Including the preconditions for each hidden predicate we obtain the following formulae: and In the case of the template marked with a “*” the parameters where have to be replaced by any combination of possible values. we generate this template, the row corresponds to formulae in total. 2 shows the local formuale for for these formulae we use to- In this case, these templates have as precondition the formula This ensures that the formulae are only applied for SRL predicates and potential SRL arguments. In the table we include the values to replace the template parameters with. Some of these formulae capture a notion of distance between SRL predicate and SRL argument and are implicitely conjoined with a a, If a formulae exists both and without we write “Dist” column; if it only exists with the we write otherwise Note that Tables 1 and 2 do not mention feature information provided in the cor- Template I S X X X X X X , X X X X i, X X j, X X X X j, k, i, i, X X (Empty Body) X 1: Templates of the local formulae for I: head of clause is S: head of is pora because this information was not available for every language. We therefore group formulae which consider the predinto another a set we call formulae. This is the summary of these formulae: f, f, a, Additionally, we define a set of language specific formulae. They are aimed to capture the relations between argument and its siblings for the In practice in turned out that these formulae were only beneficial for the Japanese language. This is a summary of formulae which we called h, c, h, c, a, h, c, h, c, a, With these sets of formulae we can build specific MLNs for each language in the shared task. We the formulae into the modules: 87</abstract>
<title confidence="0.596311346153846">Template Parameters Dist. H R Both X X No X No X Only X X Only X Only X Only X Only X No X X m, No X X No X X a, No X X No X X No X X a, No X X a, Only X X a, Only X X a, Only X X a, Only X X a, Only X X a, Only X X a, Only X X , Only X X , Only X X , , Only X X</title>
<author confidence="0.606038">X X No</author>
<abstract confidence="0.973300882352941">2: Templates of the local formulae for H: head of clause is R: of clause is a, plemma}, unlabelFrame, path}, pathFrame}, pathFrame, path}, path} sib- Table 3 shows the different configurations of such modules that we used for the individual lan- We omit to mention the hasbecause they are present for all languages. A more detailed description of the formulae can found in our MLN model They can be used both as a reference and as input to our Markov and thus allow the reader to easily reproduce our results. 2.2 Global formulae relate several hidden ground atoms. use them for two purposes: to ensure consis-</abstract>
<pubnum confidence="0.416956">mlns/conll09</pubnum>
<title confidence="0.544256">Set Feature sense/2 Argument siblings Catalan Yes Yes No Chinese No Yes No Czech Yes No No English No Yes No German Yes Yes No Japanese Yes No Yes</title>
<author confidence="0.669875">Spanish Yes Yes No</author>
<abstract confidence="0.885650495726496">Table 3: Different configuration of the modules for the formulae of the languages. 88 tency between the decisions of all SRL stages and to capture some of our intuition about the task. We will refer to formulae that serve the first purpose For example, a structural constraint is given by the (deterministic) formula a, ensures that, whenever the argument a label respect to the predicate this must be an argument of denoted by The global formulae that capture our intuition about the task itself can be further divided into two The first one uses deterministic or constraints such as a, a, which forbids cases where distinct arguments of a predicate have the same role unless the role describes a modifier. second class of global formulae is nondeterministic. For instance, the formula is a soft global formula. It captures the observation that the sense of a verb or noun depends on the type of its arguments. Here the type of an argument token is represented by its POS tag. Table 4 presents the global formulae used in this model. 3 Results For our experiments we use the corpora provided in the SRLonly track of the shared task. Our MLN is tested on the following languages: Catalan and Spanish (Taul´e et al., 2008) , Chinese (Palmer and 2009), Czech (Hajiˇc et al., English (Surdeanu et al., 2008), German (Burchardt et al., 2006), Japanese (Kawahara et al., 2002). Table 5 presents the F1-scores and training/test times for the development and in-domain corpora. Clearly, our model does better for English. This is training we use only sentences shorter than 40 words in this corpus. Structural constraints a, a, Hard constraints a, a, Soft constraints Table 4: Global formulae for ML model Language Devel Test time time Average 11h 29m 23m Catalan 6h 11m 14m Chinese 36h 30m 34m Czech 14h 21m 1h 7m English 12h 26m 16m German 2h 28m 7m Japanese 2h 17m 4m Spanish 6h 9m 16m Table 5: F-scores for in-domain in corpora for each language. in part because the original model was developed for English. To put these results into context: our SRL system the third best in the of the Shared and it is the sixth best on both SR- For five of the languages the difference to the F1 scores of the best system is 3%. Howfor German it is and for Czech One possible explanation for the poor performance on Czech data will be given below. Note that in comparison our system does slightly better in terms of precision than in terms of recall (we have the fifth best average precision and the eighth average recall). Table 6 presents the F1 scores of our system for the out of domain test corpora. We observe a similar our system is the sixth best for both We also observe similar large differences between our scores and the best scores German and Czech (i.e., while for Enthe difference is relatively small (i.e., 89 Language Czech English German F-score Table 6: F-scores for out-domain in corpora for each language. we evaluated the effect of the of formulae introduced for the Japanese Without this set the F-score is the test set. Hence siblings performance by more than We found that the MLN for Czech was the one with the largest difference in performance when compared to the best system. By inspecting our results for the development set, we found that for Czech many of the errors were of a rather technical nature. Our system would usually extract frame IDs (such as “play.02”) by concatenating the lemma the token and outcome of the (for the “02” part). However, in the case of Czech some frame IDs are not based on the lemma of the token, but on an abstract ID in a vocabulary (e.g., “v-w1757f1”). In these cases our heuristic failed, leading to poor results for frame ID extraction. 4 Conclusion We presented a Markov Logic Network that performs joint multi-lingual Semantic Role Labelling. This network achieves the third best semantic Fscores in the closed track among the SRLOnly systems of the CoNLL-09 Shared Task, and sixth best semantic scores among SRLOnly and Joint systems for the closed task. We observed that the inclusion of features which take into account information about the siblings of the argument were beneficial for SRL performance on the Japanese dataset. We also noticed that our poor performance with Czech are caused by our frame ID heuristic. Further work has to be done in order to overcome this problem.</abstract>
<note confidence="0.8515006">References Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pad´o, and Manfred Pinkal. The SALSA corpus: a German corpus for lexical semantics. In of Genoa, Italy, 2006. Crammer and Yoram Singer. Ultraconservative online algorithms for multiclass problems. of Machine Learning 3:951– 991, 2003. ISSN 1533-7928.</note>
<author confidence="0.8822845">Jan Hajiˇc</author>
<author confidence="0.8822845">Jarmila Panevov´a</author>
<author confidence="0.8822845">Eva Hajiˇcov´a</author>
<author confidence="0.8822845">Petr Petr Pajas</author>
<author confidence="0.8822845">Jan JiˇriHavelka</author>
<address confidence="0.542774">Mikulov´a, and Zdenˇek Prague</address>
<email confidence="0.62695">dependencytreebank2.0,2006.</email>
<author confidence="0.6592736">The</author>
<note confidence="0.779711363636364">CoNLL-2009 shared task: Syntactic and semantic in multiple languages. In Proceedof Boulder, Colorado, USA, 2009. Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida. Construction of a Japanese relevancecorpus. In of the pages 2008–2013, Las Palmas, Canary Islands, 2002. Martha Palmer and Nianwen Xue. Adding semantic to the Chinese Treebank. Language 15(1):143–172, 2009. Matt Richardson and Pedro Domingos. Markov networks. 62:107–136, 2006. Sebastian Riedel. Improving the accuracy and efficiency of map inference for markov logic. In UAI ’08: Proceedings of the Annual Conference Uncertainty in 2008. Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluis M`arquez, and Joakim Nivre. The CoNLL- 2008 shared task on joint parsing of syntactic semantic dependencies. In of 2008. Mariona Taul´e, Maria Ant`onia Marti, and Marta Recasens. AnCora: Multilevel Annotated Corfor Catalan and Spanish. In of Marrakesh, Morroco, 2008. Nianwen Xue and Martha Palmer. Calibrating feafor semantic role labeling. In ’04: Proceedings of the Annual Conference on Em- Methods in Natural Language 2004.</note>
<intro confidence="0.54882">90</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pad´o</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA corpus: a German corpus resource for lexical semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-2006,</booktitle>
<location>Genoa, Italy,</location>
<marker>Burchardt, Erk, Frank, Kowalski, Pad´o, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pad´o, and Manfred Pinkal. The SALSA corpus: a German corpus resource for lexical semantics. In Proceedings of LREC-2006, Genoa, Italy, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>1533--7928</pages>
<contexts>
<context position="2999" citStr="Crammer and Singer, 2003" startWordPosition="509" endWordPosition="512"> M assigns the probability p(y) = Z exp w (y) (1) ((φ,w)EM cECφ to the possible world y. Here is the set of all possible bindings of the free variables in 0 with the constants of our domain. is a feature function that returns 1 if in the possible world y the ground formula we get by replacing the free variables in 0 by the constants in c is true and 0 otherwise. Z is a normalisation constant. Note that this distribution corresponds to a Markov Network (the socalled Ground Markov Network) where nodes represent ground atoms and factors represent ground formulae. In this work we use 1-best MIRA (Crammer and Singer, 2003) Online Learning in order to train the weights of an MLN. To find the SRL assignment with maximal a posteriori probability according to an MLN and observed sentence, we use Cutting Plane Inference (CPI, Riedel, 2008) with ILP base solver. This method is used during both test time an Cφ fφc d the MIRA online learning process. Proceedings of the Thirteenth Conference on Computational Natural Language Learn ing (CoNLL): Shared Task, pages 85–90, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics 2 Model In order to model the SRL task in the ML framework, we propose fou</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951– 991, 2003. ISSN 1533-7928.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Jarmila Panevov´a</author>
<author>Eva Hajiˇcov´a</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
</authors>
<date></date>
<booktitle>Stˇep´anek, JiˇriHavelka, Marie Mikulov´a, and Zdenˇek ˇZabokrtsk´y. Prague dependency treebank 2.0,</booktitle>
<marker>Hajiˇc, Panevov´a, Hajiˇcov´a, Sgall, Pajas, </marker>
<rawString>Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr Sgall, Petr Pajas, Jan ˇStˇep´anek, JiˇriHavelka, Marie Mikulov´a, and Zdenˇek ˇZabokrtsk´y. Prague dependency treebank 2.0, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Marti</author>
<author>Lluis M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Miahi Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL-2009),</booktitle>
<location>Boulder, Colorado, USA,</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Marti, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Marti, Lluis M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Miahi Surdeanu, Nianwen Xue, and Yi Zhang. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of CoNLL-2009), Boulder, Colorado, USA, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
</authors>
<title>Sadao Kurohashi, and Kˆoiti Hasida. Construction of a Japanese relevancetagged corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of the LREC-2002,</booktitle>
<pages>2008--2013</pages>
<location>Las Palmas, Canary Islands,</location>
<marker>Kawahara, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida. Construction of a Japanese relevancetagged corpus. In Proceedings of the LREC-2002, pages 2008–2013, Las Palmas, Canary Islands, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Nianwen Xue</author>
</authors>
<title>Adding semantic roles to the Chinese Treebank.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="15362" citStr="Palmer and Xue, 2009" startWordPosition="2717" endWordPosition="2720">fier. The second class of global formulae is soft or nondeterministic. For instance, the formula lemma(p, +l) ∧ ppos(a, +p) ∧hasRole(p, a) ⇒ sense(p, +f) is a soft global formula. It captures the observation that the sense of a verb or noun depends on the type of its arguments. Here the type of an argument token is represented by its POS tag. Table 4 presents the global formulae used in this model. 3 Results For our experiments we use the corpora provided in the SRLonly track of the shared task. Our MLN is tested on the following languages: Catalan and Spanish (Taul´e et al., 2008) , Chinese (Palmer and Xue, 2009), Czech (Hajiˇc et al., 2006),4 English (Surdeanu et al., 2008), German (Burchardt et al., 2006), Japanese (Kawahara et al., 2002). Table 5 presents the F1-scores and training/test times for the development and in-domain corpora. Clearly, our model does better for English. This is 4For training we use only sentences shorter than 40 words in this corpus. Structural constraints hasRole(p, a) ⇒ argument(a) role(p, a, r) ⇒ hasRole(p, a) argument(a) ⇒ ∃p.hasRole(p, a) hasRole(p, a) ⇒ ∃r.role(p, a, r) Hard constraints role(p, a, r1) ∧ r1 =6 r2 ⇒ ¬role(p, a, r2) sense(p,s1) ∧ s1 =6 s2 ⇒ ¬sense(p,r2) </context>
</contexts>
<marker>Palmer, Xue, 2009</marker>
<rawString>Martha Palmer and Nianwen Xue. Adding semantic roles to the Chinese Treebank. Natural Language Engineering, 15(1):143–172, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--107</pages>
<contexts>
<context position="830" citStr="Richardson and Domingos, 2006" startWordPosition="117" endWordPosition="120"> tDatabase Center for Life Science, Research Organization of Information and System, Japan *I.V.Meza-Ruiz@sms.ed.ac.uk tsebastian.riedel@gmail.com Abstract This paper presents our system for the CoNLL 2009 Shared Task on Syntactic and Semantic Dependencies in Multiple Languages (Hajiˇc et al., 2009). In this work we focus only on the Semantic Role Labelling (SRL) task. We use Markov Logic to define a joint SRL model and achieve the third best average performance in the closed Track for SRLOnly systems and the sixth including for both SRLOnly and Joint systems. 1 Markov Logic Markov Logic (ML, Richardson and Domingos, 2006) is a Statistical Relational Learning language based on First Order Logic and Markov Networks. It can be seen as a formalism that extends First Order Logic to allow formulae that can be violated with some penalty. From an alternative point of view, it is an expressive template language that uses First Order Logic formulae to instantiate Markov Networks of repetitive structure. In the ML framework, we model the SRL task by first introducing a set of logical predicates1 such as word(Token,Ortho) or role(Token,Token,Role). In the case of word/2 the predicate represents a word of a sentence, the t</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matt Richardson and Pedro Domingos. Markov logic networks. Machine Learning, 62:107–136, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
</authors>
<title>Improving the accuracy and efficiency of map inference for markov logic.</title>
<date>2008</date>
<booktitle>In UAI ’08: Proceedings of the Annual Conference on Uncertainty in AI,</booktitle>
<contexts>
<context position="3215" citStr="Riedel, 2008" startWordPosition="547" endWordPosition="548">if in the possible world y the ground formula we get by replacing the free variables in 0 by the constants in c is true and 0 otherwise. Z is a normalisation constant. Note that this distribution corresponds to a Markov Network (the socalled Ground Markov Network) where nodes represent ground atoms and factors represent ground formulae. In this work we use 1-best MIRA (Crammer and Singer, 2003) Online Learning in order to train the weights of an MLN. To find the SRL assignment with maximal a posteriori probability according to an MLN and observed sentence, we use Cutting Plane Inference (CPI, Riedel, 2008) with ILP base solver. This method is used during both test time an Cφ fφc d the MIRA online learning process. Proceedings of the Thirteenth Conference on Computational Natural Language Learn ing (CoNLL): Shared Task, pages 85–90, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics 2 Model In order to model the SRL task in the ML framework, we propose four hidden predicates. Consider the example of the previous section: argument/1 indicates the phrase for which its head is a specific position is an SRL argument. In our example argument(2) signals that the phrase for </context>
</contexts>
<marker>Riedel, 2008</marker>
<rawString>Sebastian Riedel. Improving the accuracy and efficiency of map inference for markov logic. In UAI ’08: Proceedings of the Annual Conference on Uncertainty in AI, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluis M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL-2008,</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluis M`arquez, and Joakim Nivre. The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of CoNLL-2008, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taul´e</author>
<author>Maria Ant`onia Marti</author>
<author>Marta Recasens</author>
</authors>
<title>AnCora: Multilevel Annotated Corpora for Catalan and Spanish.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC-2008,</booktitle>
<location>Marrakesh, Morroco,</location>
<marker>Taul´e, Marti, Recasens, 2008</marker>
<rawString>Mariona Taul´e, Maria Ant`onia Marti, and Marta Recasens. AnCora: Multilevel Annotated Corpora for Catalan and Spanish. In Proceedings of LREC-2008, Marrakesh, Morroco, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In EMNLP ’04: Proceedings of the Annual Conference on Empirical Methods in Natural Language Processing,</booktitle>
<contexts>
<context position="5462" citStr="Xue and Palmer (2004)" startWordPosition="921" endWordPosition="924">ll refer to these predicates as the token predicates. The second set extends the information provided in the closed track corpus: cpos/2 is a coarse POS tag (first letter of actual POS tag); possibleArg/1 is true if the POS tag the token is a potential SRL argument POS tag (e.g., PUNC is not); voice/2 denotes the voice for verbal tokens based on heuristics that use syntactic information, or based on features in the FEAT column of the data. We will refer to these predicates as the extended predicates. Finally, the third set represents dependency information inspired by the features proposed by Xue and Palmer (2004). There are two types of predicates in this set: paths and frames. Paths capture the dependency path between two tokens, and frames the subcategorisation frame for a token or a pair of tokens. There are directed and undirected versions of paths, and labelled (with dependency relations) and unlabelled versions of paths and frames. Finally, we have a frame predicate with the distance from the predicate to its head. We will refer to the paths and most of the frames predicates as the path predicates, while we will consider the frame predicates for a unique token part token predicates. The ML predi</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. Calibrating features for semantic role labeling. In EMNLP ’04: Proceedings of the Annual Conference on Empirical Methods in Natural Language Processing, 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>