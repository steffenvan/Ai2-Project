<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.868298">
Linefeed Insertion into Japanese Spoken Monologue for Captioning
</title>
<email confidence="0.96789">
murata@el.itc.nagoya-u.ac.jp
</email>
<author confidence="0.955665">
Tomohiro Ohno
</author>
<affiliation confidence="0.940952">
Graduate School of
International Development,
Nagoya University, Japan
</affiliation>
<email confidence="0.784786">
ohno@nagoya-u.jp
</email>
<author confidence="0.957921">
Masalii Murata
</author>
<affiliation confidence="0.955549">
Graduate School of
Information Science,
Nagoya University, Japan
</affiliation>
<author confidence="0.974304">
Shigelii Matsubara
</author>
<affiliation confidence="0.982368">
Information Technology Center,
Nagoya University, Japan
</affiliation>
<email confidence="0.647097">
matubara@nagoya-u.jp
</email>
<sectionHeader confidence="0.993748" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999990789473684">
To support the real-time understanding of
spoken monologue such as lectures and
commentaries, the development of a cap-
tioning system is required. In monologues,
since a sentence tends to be long, each
sentence is often displayed in multi lines
on one screen, it is necessary to insert
linefeeds into a text so that the text be-
comes easy to read. This paper proposes
a technique for inserting linefeeds into a
Japanese spoken monologue text as an el-
emental technique to generate the read-
able captions. Our method appropriately
inserts linefeeds into a sentence by ma-
chine learning, based on the information
such as dependencies, clause boundaries,
pauses and line length. An experiment us-
ing Japanese speech data has shown the ef-
fectiveness of our technique.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.93904995">
Real-time captioning is a technique for support-
ing the speech understanding of deaf persons, el-
derly persons, or foreigners by displaying tran-
scribed texts of monologue speech such as lec-
tures. In recent years, there exist a lot of re-
searches about automatic captioning, and the tech-
niques of automatic speech recognition (ASR)
aimed for captioning have been developed (Bou-
lianne et al., 2006; Holter et al., 2000; Imai et
al., 2006; Munteanu et al., 2007; Saraclar et al.,
2002; Xue et al., 2006). However, in order to gen-
erate captions which is easy to read, it is important
not only to recognize speech with high recognition
rate but also to properly display the transcribed
text on a screen (Hoogenboom et al., 2008). Es-
pecially, in spoken monologue, since a sentence
tends to be long, each sentence is often displayed
as a multi-line text on a screen. Therefore, proper
linefeed insertion for the displayed text is desired
so that the text becomes easy to read.
Until now, there existed few researches about
how to display text on a screen in automatic cap-
tioning. As the research about linefeed insertion,
Monma et al. proposed a method based on pat-
terns of a sequence of morphemes (Monma et
al., 2003). However, the target of the research is
closed-captions of Japanese TV shows, in which
less than or equal to 2 lines text is displayed on
a screen and the text all switches to other text at
a time. In the work, the highest priority concept
on captioning is that one screen should be filled
with as much text as possible. Therefore, a se-
mantic boundary in a sentence is hardly taken into
account in linefeed insertion, and the readability
of the caption is hardly improved.
This paper proposes a technique for inserting
linefeeds into transcribed texts of Japanese mono-
logue speech as an elemental technique to gener-
ate readable captions. We assume that a screen for
displaying only multi-line caption is placed to pro-
vide the caption information to the audience on the
site of a lecture. In our method, the linefeeds are
inserted into only the boundaries between bunset-
sus1, and the linefeeds are appropriately inserted
into a sentence by machine learning, based on the
information such as morphemes, dependencies2,
clause boundaries, pauses and line length.
We conducted an experiment on inserting line-
feeds by using Japanese spoken monologue data.
As the results of inserting linefeeds for 1,714 sen-
tences, the recall and precision of our method were
82.66% and 80.24%, respectively. Our method
improved the performance dramatically compared
&apos;Bunsetsu is a linguistic unit in Japanese that roughly cor-
responds to a basic phrase in English. A bunsetsu consists of
one independent word and zero or more ancillary words.
2A dependency in Japanese is a modification relation in
which a modifier bunsetsu depends on a modified bunsetsu.
That is, the modifier bunsetsu and the modified bunsetsu work
as modifier and modifyee, respectively.
</bodyText>
<page confidence="0.934364">
531
</page>
<note confidence="0.999674">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 531–539,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.999984">
Figure 2: Caption of monologue speech
Figure 1: Caption display of spoken monologue
</figureCaption>
<bodyText confidence="0.999682555555556">
with four baseline methods, which we established
for comparative evaluation. The effectiveness of
our method has been confirmed.
This paper is organized as follows: The next
section describes our assumed caption and the pre-
liminary analysis. Section 3 presents our linefeed
insertion technique. An experiment and discussion
are reported in Sections 4 and 5, respectively. Fi-
nally, Section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.9446145" genericHeader="introduction">
2 Linefeed Insertion for Spoken
Monologue
</sectionHeader>
<bodyText confidence="0.99971152">
In our research, in an environment in which cap-
tions are displayed on the site of a lecture, we as-
sume that a screen for displaying only captions is
used. In the screen, multi lines are always dis-
played, being scrolled line by line. Figure 1 shows
our assumed environment in which captions are
displayed.
As shown in Figure 2, if the transcribed text of
speech is displayed in accordance with only the
width of a screen without considering the proper
points of linefeeds, the caption is not easy to read.
Especially, since the audience is forced to read
the caption in synchronization with the speaker’s
utterance speed, it is important that linefeeds are
properly inserted into the displayed text in consid-
eration of the readability as shown in Figure 3.
To investigate whether the line insertion facili-
tates the readability of the displayed texts, we con-
ducted an experiment using the transcribed text of
lecture speeches in the Simultaneous Interpreta-
tion Database (SIDB) (Matsubara et al., 2002). We
randomly selected 50 sentences from the data, and
then created the following two texts for each sen-
tence based on two different concepts about line-
feed insertion.
</bodyText>
<footnote confidence="0.5118495">
(1)Text into which linefeeds were forcibly in-
serted once every 20 characters
</footnote>
<figureCaption confidence="0.99541475">
Figure 3: Caption into which linefeeds are prop-
erly inserted
Figure 4: Result of investigation of effect of line-
feed insertion into transcription
</figureCaption>
<bodyText confidence="0.972839684210526">
(2)Text into which linefeeds were properly
inserted in consideration of readability by
hand3
Figure 2 and 3 show examples of the text (1) and
(2), respectively. 10 examinees decided which of
the two texts was more readable. Figure 4 shows
the result of the investigation. The ratio that each
examinee selected text (2) was 87.0% on average.
There was no sentence in the text group (1) which
was selected by more than 5 examinees. These
indicates that a text becomes more readable by
proper insertion of linefeeds.
Here, since a bunsetsu is the smallest seman-
tically meaningful language unit in Japanese, our
method adopts the bunsetsu boundaries as the can-
didates of points into which a linefeed is inserted.
In this paper, hereafter, we call a bunsetsu bound-
ary into which a linefeed is inserted a linefeed
point.
</bodyText>
<footnote confidence="0.6317515">
33 persons inserted linefeeds into the 50 sentences by dis-
cussing where to insert the linefeeds.
</footnote>
<page confidence="0.998058">
532
</page>
<tableCaption confidence="0.99983">
Table 1: Size of analysis data
</tableCaption>
<table confidence="0.9943006">
sentence 221
bunsetsu 2,891
character 13,899
linefeed 883
character per line 13.2
</table>
<sectionHeader confidence="0.893581" genericHeader="method">
3 Preliminary Analysis about Linefeed
Points
</sectionHeader>
<bodyText confidence="0.999923608695652">
In our research, the points into which linefeeds
should be inserted is detected by using machine
learning. To find the effective features, we investi-
gated the spoken language corpus. In our investi-
gation, we used Japanese monologue speech data
in the SIDB (Matsubara et al., 2002). The data
is annotated by hand with information on mor-
phological analysis, bunsetsu segmentation, de-
pendency analysis, clause boundary detection, and
linefeeds insertion. Table 1 shows the size of the
analysis data. Among 2,670 (= 2,891−221) bun-
setsu boundaries, which are candidates of linefeed
points, there existed 833 bunsetsu boundaries into
which linefeeds were inserted, that is, the ratio of
linefeed insertion was 31.2%.
The linefeeds were inserted by hand so that the
maximum number of characters per line is 20. We
set the number in consideration of the relation be-
tween readability and font size on the display. In
the analysis, we focused on the clause boundary,
dependency relation, line length, pause and mor-
pheme of line head, and investigated the relations
between them and linefeed points.
</bodyText>
<subsectionHeader confidence="0.999973">
3.1 Clause Boundary and Linefeed Point
</subsectionHeader>
<bodyText confidence="0.999988538461539">
Since a clause is one of semantically meaningful
language units, the clause boundary is considered
to be a strong candidate of a linefeed point. In the
analysis data, there existed 969 clause boundaries
except sentence breaks. Among them, 490 were
the points into which linefeeds were inserted, that
is, the ratio of linefeed insertion was 51.1%. This
ratio is higher than that of bunsetsu boundaries.
This indicates that linefeeds tend to be inserted
into clause boundaries.
We investigated the ratio of linefeed insertion
about 42 types of clause boundaries, which were
seen in the analysis data. Table 2 shows the top 10
</bodyText>
<footnote confidence="0.963468666666667">
4In our research, we used the types of clause boundaries
defined by the Clause Boundary Annotation Program (Kash-
ioka and Maruyama, 2004).
</footnote>
<tableCaption confidence="0.639504">
Table 2: Ratio of linefeed insertion for clause
boundary type
</tableCaption>
<bodyText confidence="0.9997547">
clause boundary types about the occurrence fre-
quency, and each ratio of linefeed insertion. In
the case of “compound clause-ga” and “compound
clause-keredomo,” the ratio of linefeed insertion
was 100%. On the other hand, in the case of “quo-
tational clause,” “adnominal clause” and “adnomi-
nal clause-toiu,” the ratio of linefeed insertion was
less than 30%. This means that the likelihood of
linefeed insertion is different according to the type
of the clause boundary.
</bodyText>
<subsectionHeader confidence="0.9927265">
3.2 Dependency Structure and Linefeed
Point
</subsectionHeader>
<bodyText confidence="0.999946954545455">
When a bunsetsu depends on the next bunsetsu, it
is thought that a linefeed is hard to be inserted into
the bunsetsu boundary between them because the
sequence of such bunsetsus constitutes a semanti-
cally meaningful unit. In the analysis data, there
existed 1,459 bunsetsus which depend on the next
bunsetsu. Among the bunsetsu boundaries right
after them, 192 were linefeed points, that is, the
ratio of linefeed insertions was 13.2%. This ra-
tio is less than half of that for all the bunsetsu
boundaries. On the other hand, when the bunsetsu
boundary right after the bunsetsu which does not
depend on the next bunsetsu, the ratio of linefeed
insertion was 52.7%.
Next, we focused on the type of the dependency
relation, by which the likelihood of linefeed inser-
tion is different. For example, when the bunsetsu
boundary right after a bunsetsu on which the final
bunsetsu of an adnominal clause depends, the ra-
tio of linefeed insertion was 43.1%. This ratio is
higher than that for all the bunsetsu boundaries.
In addition, we investigated the relation be-
</bodyText>
<figure confidence="0.996009583333333">
type of
clause boundary
ratio of linefeed
insertion (%)
topicalized element-wa
discourse marker
quotational clause
adnominal clause
compound clause-te
supplement clause
compound clause-ga
compound clause-keredomo
condition clause-to
adnominal clause-toiu
50.8
12.0
22.1
23.3
90.2
68.0
100.0
100.0
93.5
27.3
</figure>
<page confidence="0.863279">
533
</page>
<figureCaption confidence="0.946242">
Figure 5: Relation between dependency structure
and linefeed points
</figureCaption>
<bodyText confidence="0.998718">
tween a dependency structure and linefeed points,
that is, whether the dependency structure is closed
within a line or not. Here, a line whose depen-
dency structure is closed means that all bunsetsus,
except the final bunsetsu, in the line depend on one
of bunsetsus in the line. Since, in many of seman-
tically meaningful units, the dependency structure
is closed, the dependency structure of a line is con-
sidered to tend to be closed. In the analysis data,
among 883 lines, 599 lines’ dependency structures
were closed.
Figure 5 shows the relation between depen-
dency structure and linefeed points. In this exam-
ple, linefeeds are not inserted right after bunset-
sus which depend on the next bunsetsu (e.g. “11,
0) (my)” and “*-t- (car)”). Instead, a linefeed is
inserted right after a bunsetsu which does not de-
pend on the next bunsetsu (“AE� (a writer)”).
In addition, the dependency structure in each line
is closed.
</bodyText>
<subsectionHeader confidence="0.999824">
3.3 Line Length and Linefeed Point
</subsectionHeader>
<bodyText confidence="0.999994571428571">
An extremely-short line is considered to be hardly
generated because the readability goes down if the
length of each line is very different. In the analysis
data, a line whose length is less than or equal to 6
characters occupied only 7.59% of the total. This
indicates that linefeeds tend to be inserted into the
place where a line can maintain a certain length.
</bodyText>
<subsectionHeader confidence="0.981123">
3.4 Pause and Linefeed Point
</subsectionHeader>
<bodyText confidence="0.999984">
It is thought that a pause corresponds to a syn-
tactic boundary. Therefore, there are possibility
that a linefeed becomes more easily inserted into
a bunsetsu boundary at which a pause exists. In
our research, a pause is defined as a silent interval
equal to or longer than 200ms. In the analysis data,
among 748 bunsetsu boundaries at which a pause
exists, linefeeds were inserted into 471 bunsetsu
boundaries, that is, the ratio of linefeed insertion
was 62.97%. This ratio is higher than that for all
the bunsetsu boundaries, thus, we confirmed that
linefeeds tend to be inserted into bunsetsu bound-
aries at which a pause exists.
</bodyText>
<subsectionHeader confidence="0.995163">
3.5 Morpheme Located in the Start of a Line
</subsectionHeader>
<bodyText confidence="0.99979475">
There exist some morphemes which are unlikely
to become a line head. We investigated the ratio
that each leftmost morpheme of all the bunsetsus
appears at a line head. Here, we focused on the
basic form and part-of-speech of a morpheme. The
morphemes which appeared 20 times and of which
the ratio of appearance at a line head was less than
10% were as follows:
</bodyText>
<listItem confidence="0.955581">
• Basic form:
</listItem>
<equation confidence="0.595996">
“ T,5 (think) [2/70]”, “,349 (problem)
[0/42]”, “t6 (do) [3/33]”, “4�6 (become)
[2/32]”,“2Y W (necessary) [1/21]”
</equation>
<listItem confidence="0.970257">
• Part-of-speech:
</listItem>
<bodyText confidence="0.9746895">
noun-non independent-general [0/40],
noun-nai adjective stem [0/40],
noun-non independent-adverbial [(0/27]
If the leftmost morpheme of a bunsetsu is one of
these, it is thought that a linefeed is hardly inserted
right after the bunsetsu.
</bodyText>
<sectionHeader confidence="0.997973" genericHeader="method">
4 Linefeed Insertion Technique
</sectionHeader>
<bodyText confidence="0.985665095238095">
In our method, a sentence, on which morphologi-
cal analysis, bunsetsu segmentation, clause bound-
ary analysis and dependency analysis are per-
formed, is considered the input. Our method de-
cides whether or not to insert a linefeed into each
bunsetsu boundary in an input sentence. Under
the condition that the number of characters in each
line has to be less than or equal to the maximum
number of characters per line, our method identi-
fies the most appropriate combination among all
combinations of the points into which linefeeds
can be inserted, by using the probabilistic model.
In this paper, we describe an input sentence
which consists of n bunsetsus as B = bi • • • bn,
and the result of linefeeds insertion as R =
ri • • • rn. Here, ri is 1 if a linefeed is inserted right
after bunsetsu bi, and is 0 otherwise. We describe
a sequence of bunsetsus in the j-th line among the
m lines created by dividing an input sentence as
Lj = bj1 • • • bjnj(1 G j G m), and then, rjk = 0 if
k =� nj, and rj k = 1 otherwise.
</bodyText>
<page confidence="0.994683">
534
</page>
<sectionHeader confidence="0.80791" genericHeader="method">
4.1 Probabilistic Model for Linefeed
Insertion
</sectionHeader>
<bodyText confidence="0.9968175">
When an input sentence B is provided, our method
identifies the result of linefeeds insertion R, which
maximizes the conditional probability P(R|B).
Assuming that whether or not a linefeed is inserted
right after a bunsetsu is independent of other line-
feed points except the linefeed point of the start of
the line which contains the bunsetsu, P(R|B) can
be calculated as follows:
</bodyText>
<equation confidence="0.962319166666667">
P(R|B) (1)
1
= P(r11 = 0, ··· rn1 = 1, ··· ,rm1 = 0, ··· ,rmnm = 1|B)
�= P(r11 = 0|B) x P(r12 = 0|r11 = 0, B) x ···
xP(r1n1 = 1|r1n1−1 = 0, ··· ,r11 = 0, B) x ···
xP(rm1 = 0|rm−1
nm−1 = 1, B) x ···
xP(rmm = 1|rm nm−1 = 0, ··· ,rm1 = 0, rm−1
nm−1 = 1, B)
where P(rjk = 1|rjk−1 = 0, · · · , rj1 = 0, rj−1
nj−1 =
1, B) is the probability that a linefeed is inserted
</equation>
<bodyText confidence="0.998720666666667">
right after a bunsetsu bjk when the sequence of
bunsetsus B is provided and the linefeed point of
the start of the j-th line is identified. Similarly,
</bodyText>
<equation confidence="0.9998565">
P(rjk = 0|rjk−1 = 0, ··· , rj1 = 0, rj−1
nj−1 = 1, B)
</equation>
<bodyText confidence="0.995066285714286">
is the probability that a linefeed is not inserted
right after a bunsetsu bjk. These probabilities are
estimated by the maximum entropy method. The
result R which maximizes the conditional proba-
bility P(R|B) is regarded as the most appropriate
result of linefeed insertion, and calculated by dy-
namic programming.
</bodyText>
<subsectionHeader confidence="0.81889">
4.2 Features on Maximum Entropy Method
</subsectionHeader>
<equation confidence="0.916013">
To estimate P(rjk = 1|rjk−1 = 0, · · · , rj 1 =
0, rj−1
nj−1 = 1, B) and P (rj k = 0|rj k−1 =
0, · · · ,rj 1 = 0, rj−1
nj−1 = 1, B) by the maximum
</equation>
<bodyText confidence="0.849960333333333">
entropy method, we used the following features
based on the analysis described in Section 2.2.
Morphological information
</bodyText>
<listItem confidence="0.911054466666667">
• the rightmost independent morpheme (a part-
of-speech, an inflected form) and rightmost
morpheme (a part-of-speech) of a bunsetsu bjk
Clause boundary information
• whether or not a clause boundary exists right
after bjk
• a type of the clause boundary right after bjk (if
there exists a clause boundary)
Dependency information
• whether or not bjk depends on the next bun-
setsu
• whether or not bjk depends on the final bun-
setsu of a clause
• whether or not bjk depends on a bunsetsu to
which the number of characters from the start
of the line is less than or equal to the maxi-
mum number of characters
• whether or not bjk is depended on by the final
bunsetsu of an adnominal clause
• whether or not bjk is depended on by the bun-
setsu located right before it
• whether or not the dependency structure of
a sequence of bunsetsus between bjk and bj1,
which is the first bunsetsu of the line, is
closed
• whether or not there exists a bunsetsu which
depends on the modified bunsetsu of bjk,
among bunsetsus which are located after bjk
and to which the number of characters from
the start of the line is less than or equal to the
maximum number of characters
Line length
• any of the following is the class into which
the number of characters from the start of the
line to bjk is classified
– less than or equal to 2
– more than 2 and less than or equal to 6
– more than 6
Pause
• whether or not a pause exists right after bjk
Leftmost morpheme of a bunsetsu
• whether or not the basic form or part-of-
speech of the leftmost morpheme of the next
bunsetsu of bjk is one of the morphemes enu-
merated in Section 3.5.
</listItem>
<sectionHeader confidence="0.99693" genericHeader="method">
5 Experiment
</sectionHeader>
<bodyText confidence="0.999884666666667">
To evaluate the effectiveness of our method, we
conducted an experiment on inserting linefeeds by
using discourse speech data.
</bodyText>
<subsectionHeader confidence="0.915531">
5.1 Outline of Experiment
</subsectionHeader>
<bodyText confidence="0.999871571428572">
As the experimental data, we used the transcribed
data of Japanese discourse speech in the SIDB
(Matsubara et al., 2002). All the data are anno-
tated with information on morphological analysis,
clause boundary detection and dependency anal-
ysis by hand. We performed a cross-validation
experiment by using 16 discourses. That is, we
</bodyText>
<page confidence="0.996389">
535
</page>
<bodyText confidence="0.999939333333333">
repeated the experiment, in which we used one
discourse from among 16 discourses as the test
data and the others as the learning data, 16 times.
However, since we used 2 discourse among 16
discourses as the preliminary analysis data, we
evaluated the experimental result for the other 14
discourses (1,714 sentences, 20,707 bunsetsus).
Here, we used the maximum entropy method tool
(Zhang, 2008) with the default options except “-i
2000.”
In the evaluation, we obtained recall, precision
and the ratio of sentences into which all linefeed
points were correctly inserted (hereinafter called
sentence accuracy). The recall and precision are
respectively defined as follows.
</bodyText>
<equation confidence="0.7519124">
recall =
# of correctly inserted LFs
# of LFs in the correct data
# of correctly inserted LFs
precision =
</equation>
<bodyText confidence="0.860033333333333">
# of automatically inserted LFs
For comparison, we established the following
four baseline methods.
</bodyText>
<listItem confidence="0.948832764705882">
1. Linefeeds are inserted into the rightmost bun-
setsu boundaries among the bunsetsu bound-
aries into which linefeeds can be inserted so
that the length of the line does not exceed
the maximum number of characters (Line-
feed insertion based on bunsetsu bound-
aries).
2. Linefeeds are inserted into the all clause
boundaries (Linefeed insertion based on
clause boundaries).
3. Linefeeds are inserted between adjacent bun-
setsus which do not depend on each other
(Linefeed insertion based on dependency
relations).
4. Linefeeds are inserted into the all bunsetsu
boundaries in which a pause exists (Linefeed
insertion based on pauses).
</listItem>
<bodyText confidence="0.9999704">
In the baseline 2, 3 and 4, if each condition is not
fulfilled within the maximum number of charac-
ters, a linefeed is inserted into the rightmost bun-
setsu boundary as well as the baseline 1.
In the experiment, we defined the maximum
number of characters per line as 20. The cor-
rect data of linefeed insertion were created by ex-
perts who were familiar with displaying captions.
There existed 5,497 inserted linefeeds in the 14
discourses, which were used in the evaluation.
</bodyText>
<tableCaption confidence="0.995193">
Table 3: Experimental results
</tableCaption>
<table confidence="0.999534454545454">
recall (%) precision (%) F-measure
our method 82.66 80.24 81.43
(4,544/5,497) (4,544/5,663)
baseline 1 27.47 34.51 30.59
(1,510/5,497) (1,510/4,376)
baseline 2 69.34 48.65 57.19
(3,812/5,497) (3,812/7,834)
baseline 3 89.48 53.73 67.14
(4,919/5,497) (4,919/9,155)
baseline 4 69.84 55.60 61.91
(3,893/5,497) (3,893/6,905)
</table>
<subsectionHeader confidence="0.997466">
5.2 Experimental Result
</subsectionHeader>
<bodyText confidence="0.9993159375">
Table 3 shows the experimental results of the base-
lines and our method. The baseline 1 is very sim-
ple method which inserts linefeeds into the bun-
setsu boundaries so that the length of the line does
not exceed the maximum number of characters per
line. Therefore, the recall and precision were the
lowest.
In the result of baseline 2, the precision was
low. As described in the Section 3.1, the degree
in which linefeeds are inserted varies in differ-
ent types of clause boundaries. In the baseline
2, because linefeeds are also inserted into clause
boundaries which have the tendency that linefeeds
are hardly inserted, the unnecessary linefeeds are
considered to have been inserted.
The recall of baseline 3 was very high. This
is because, in the correct data, linefeeds were
hardly inserted between two neighboring bunset-
sus which are in a dependency relation. However,
the precision was low, because, in the baseline
3, linefeeds are invariably inserted between two
neighboring bunsetsus which are not in a depen-
dency relation.
In the baseline 4, both the recall and precision
were not good. The possible reason is that the bun-
setsu boundaries at which a pause exists do not
necessarily correspond to the linefeed points.
On the other hand, the F-measure and the sen-
tence accuracy of our method were 81.43 and
53.15%, respectively. Both of them were highest
among those of the four baseline, which showed
an effectiveness of our method.
</bodyText>
<subsectionHeader confidence="0.999759">
5.3 Causes of Incorrect Linefeed Insertion
</subsectionHeader>
<bodyText confidence="0.998976">
In this section, we discuss the causes of the in-
correct linefeed insertion occurred in our method.
Among 1,119 incorrectly inserted linefeeds, the
most frequent cause was that linefeeds were in-
</bodyText>
<page confidence="0.997721">
536
</page>
<figureCaption confidence="0.99873425">
Figure 6: Example of incorrect linefeed insertion
in “adnominal clause.”
Figure 7: Example of extra linefeed insertion
Figure 8: Result of subjective evaluation
</figureCaption>
<bodyText confidence="0.999938916666667">
serted into clause boundaries of a “adnominal
clause” type. The cause occupies 10.19% of the
total number of the incorrectly inserted linefeeds.
In the clause boundaries of the “adnominal clause”
type, linefeeds should rarely be inserted funda-
mentally. However, in the result of our method,
a lot of linefeeds were inserted into the “adnomi-
nal clause.” Figure 6 shows an example of those
results. In this example, a linefeed is inserted into
the “adnominal clause” boundary which is located
right after the bunsetsu “*lvZ6 (call).” The se-
mantic chunk “*lvZ6R+,MZ L I-,t (is the
period which I call)” is divided.
As another cause, there existed 291 linefeeds
which divide otherwise one line according to the
correct data into two lines. Figure 7 shows an ex-
ample of the extra linefeed insertion. Although, in
the example, a linefeed is inserted between “E�
I-,ZgLく�3AtLZ 6*, (about how detail I
can speak)” and “��Zta (I have a concern),”
the two lines are displayed in one line in the cor-
rect data. It is thought that, in our method, line-
feeds tend to be inserted even if a line has space to
spare.
</bodyText>
<sectionHeader confidence="0.9987" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99989">
In this section, we discuss the experimental results
described in Section 5 to verify the effectiveness
of our method in more detail.
</bodyText>
<subsectionHeader confidence="0.9866175">
6.1 Subjective Evaluation of Linefeed
Insertion Result
</subsectionHeader>
<bodyText confidence="0.99998237037037">
The purpose of our research is to improve the read-
ability of the spoken monologue text by our line-
feed insertion. Therefore, we conducted a subjec-
tive evaluation of the texts which were generated
by the above-mentioned experiment.
In the subjective evaluation, examinees looked
at the two texts placed side-by-side between which
the only difference is linefeed points, and then se-
lected the one which was felt more readable. Here,
we compared our method with the baseline 3, of
which F-measure was highest among four base-
lines described in Section 5.1. Ten examinees
evaluated 50 pairs of the results generated from
the same 50 randomly selected sentences.
Figure 8 shows the result of subjective evalua-
tion. This graph shows the number of each method
selected by each examinee. The ratio that our
method was selected was 94% in the highest case,
and 68% even in the lowest case. We confirmed
the effectiveness of our method for improving the
readability of the spoken monologue text.
On the other hand, there existed three sentences
for which more than 5 examinees judged that the
results of baseline 3 were more readable than those
of our method. From the analysis of the three sen-
tences, we found the following phenomena caused
text to be less readable
</bodyText>
<listItem confidence="0.8599436">
• Japanese syllabary characters (Hiragana) are
successionally displayed across a bunsetsu
boundary.
• The length of anteroposterior lines is ex-
tremely different each other.
</listItem>
<bodyText confidence="0.999157">
Each example of the two causes is shown in
Figure 9 and 10, respectively. In Figure 9, a
bunsetsu boundary existed between Japanese syl-
labary characters “��Zt� (I)” and “*,く&apos;�5
(if truth be told)” and these characters are succes-
sionally displayed in the same line. In these cases,
it becomes more difficult to identify the bunsetsu
boundary, therefore, the text is thought to become
difficult to read. In Figure 10, since the length of
the second line is extremely shorter than the first
line or third line, the text is thought to become dif-
ficult to read.
</bodyText>
<page confidence="0.997663">
537
</page>
<tableCaption confidence="0.9910865">
Table 5: Experimental results when information of
features are automatically provided
</tableCaption>
<figureCaption confidence="0.990548666666667">
Figure 10: Lines that have extremely different
length
Figure 9: Example of succession of hiragana
</figureCaption>
<table confidence="0.999897636363636">
recall (%) precision (%) F-measure
our method 77.37 75.04 76.18
(4,253/5,497) (4,253/5,668)
baseline 1 27.47 34.51 30.59
(1,510/5,497) (1,510/4,376)
baseline 2 69.51 48.63 57.23
(3,821/5,497) (3,821/7,857)
baseline 3 84.01 52.03 64.26
(4,618/5,497) (4,618/8,876)
baseline 4 69.84 55.60 61.91
(3,893/5,497) (3.893/6,905)
</table>
<tableCaption confidence="0.972336">
Table 4: Other annotator’s results
</tableCaption>
<table confidence="0.984306666666667">
recall (%) precision (%) F-measure
by human 89.82 (459/511) 89.82 (459/511) 89.82
our method 82.19 (420/511) 81.71 (420/514) 81.95
</table>
<subsectionHeader confidence="0.929887">
6.2 Comparison with Linefeeds Inserted by
Human
</subsectionHeader>
<bodyText confidence="0.999981357142857">
The concept of linefeed insertion for making the
caption be easy to read varies by the individual.
When multiple people insert linefeeds for the same
text, there is possibility that linefeeds are inserted
into different points.
Therefore, for one lecture data (128 sentences,
511 bunsetsus) in the experimental data, we con-
ducted an experiment on linefeed insertion by an
annotator who was not involved in the construc-
tion of the correct data. Table 4 shows the re-
call and the precision. The second line shows
the result of our method for the same lecture
data. In F-measure, our method achieved 91.24%
(81.95/89.82) of the result by the human annotator.
</bodyText>
<subsectionHeader confidence="0.8940285">
6.3 Performance of Linefeed Insertion Based
on Automatic Natural Language Analysis
</subsectionHeader>
<bodyText confidence="0.999986636363636">
In the experiment described in Section 5, we used
the linguistic information provided by human as
the features on the maximum entropy method.
However, compared with baseline 1, our method
uses a lot of linguistic information which should
be provided not by human but by natural language
analyzers under the real situation. Therefore, to
fairly evaluate our method and four baselines, we
conducted an experiment on linefeed insertion by
using the automatically provided information on
clause boundaries and dependency structuress.
</bodyText>
<footnote confidence="0.47705475">
5We used CBAP (Kashioka and Maruyama, 2004) as
a clause boundary analyzer and CaboCha (Kudo and Mat-
sumoto, 2002) with default learning data as a dependency
parser.
</footnote>
<bodyText confidence="0.999842714285714">
Table 5 shows the result. Compared with Table
3, it shows the decreasing rate of the performance
of our method was more than those of four base-
lines which use simply only basic linguistic infor-
mation. However, the F-measure of our method
was more than 10% higher than those of four base-
lines.
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999991695652174">
This paper proposed a method for inserting line-
feeds into discourse speech data. Our method can
insert linefeeds so that captions become easy to
read, by using machine learning techniques on fea-
tures such as morphemes, dependencies, clause
boundaries, pauses and line length. An experi-
ment by using transcribed data of Japanese dis-
course speech showed the recall and precision was
82.66% and 80.24%, respectively, and we con-
firmed the effectiveness of our method.
In applying the linefeed insertion technique to
practical real-time captioning, we have to consider
not only the readability but also the simultaneity.
Since the input of our method is a sentence which
tends to be long in spoken monologue, in the fu-
ture, we will develop more simultaneous a tech-
nique in which the input is shorter than a sentence.
In addition, we assumed the speech recognition
system with perfect performance. To demonstrate
practicality of our method for automatic speech
transcription, an experiment using a continuous
speech recognition system will be performed in
the future.
</bodyText>
<sectionHeader confidence="0.999385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9112486">
This research was partially supported by the
Grant-in-Aid for Scientific Research (B) (No.
20300058) and Young Scientists (B) (No.
21700157) of JSPS, and by The Asahi Glass
Foundation.
</bodyText>
<page confidence="0.996864">
538
</page>
<sectionHeader confidence="0.99834" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999209888888888">
G. Boulianne, J.-F. Beaumont, M. Boisvert,
J. Brousseau, P. Cardinal, C. Chapdelaine,
M. Comeau, P. Ouellet, and F. Osterrath. 2006.
Computer-assisted closed-captioning of live TV
broadcasts in French. In Proceedings of 9th Interna-
tional Conference on Spoken Language Processing,
pages 273–276.
T. Holter, E. Harborg, M. H. Johnsen, and T. Svendsen.
2000. ASR-based subtitling of live TV-programs for
the hearing impaired. In Proceedings of 6th Interna-
tional Conference on Spoken Language Processing,
volume 3, pages 570–573.
R. B. Hoogenboom, K. Uehara, T. Kanazawa,
S. Nakano, H. Kuroki, S. Ino, and T. Ifukube. 2008.
An application of real-time captioning system using
automatic speech recognition technology to college
efl education for deaf and hard-of-hearing students.
Gunma University Annual Research Reports, Cul-
tural Science Series, 57.
T. Imai, S. Sato, A. Kobayashi, K. Onoe, and
S. Homma. 2006. Online speech detection
and dual-gender speech recognition for captioning
broadcast news. In Proceedings of 9th International
Conference on Spoken Language Processing, pages
1602–1605.
H. Kashioka and T. Maruyama. 2004. Segmentation of
semantic units in Japanese monologues. In Proceed-
ings of ICSLY2004 and Oriental-COCOSDA2004,
pages 87–92.
T. Kudo and Y. Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Pro-
ceedings of 6th Conference on Computational Natu-
ral Language Learning, pages 63–69.
S. Matsubara, A. Takagi, N. Kawaguchi, and Y. Ina-
gaki. 2002. Bilingual spoken monologue corpus
for simultaneous machine interpretation research.
In Proceedings of 3rd International Conference on
Language Resources and Evaluation, pages 153–
159.
T. Monma, E. Sawamura, T. Fukushima, I. Maruyama,
T. Ehara, and K. Shirai. 2003. Automatic closed-
caption production system on TV programs for
hearing-impaired people. Systems and Computers
in Japan, 34(13):71–82.
C. Munteanu, G. Penn, and R. Baecker. 2007. Web-
based language modelling for automatic lecture tran-
scription. In Proceedings of 8th Annual Conference
of the International Speech Communication Associ-
ation, pages 2353–2356.
M. Saraclar, M. Riley, E. Bocchieri, and V. Gof-
fin. 2002. Towards automatic closed captioning:
Low latency real time broadcast news transcription.
In Proceedings of 7th International Conference on
Spoken Language Processing, pages 1741–1744.
J. Xue, R. Hu, and Y. Zhao. 2006. New improvements
in decoding speed and latency for automatic caption-
ing. In Proceedings of 9th International Conference
on Spoken Language Processing, pages 1630–1633.
L. Zhang. 2008. Maximum entropy mod-
eling toolkit for Python and C++. http:
//homepages.inf.ed.ac.uk/s0450736/
maxent toolkit.html. [Online; accessed
1-March-2008].
</reference>
<page confidence="0.998568">
539
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.388628">
<title confidence="0.8141925">Linefeed Insertion into Japanese Spoken Monologue for Captioning murata@el.itc.nagoya-u.ac.jp</title>
<author confidence="0.989068">Tomohiro Ohno</author>
<affiliation confidence="0.988641">Graduate School of International Development, Nagoya University, Japan</affiliation>
<email confidence="0.912374">ohno@nagoya-u.jp</email>
<author confidence="0.982393">Masalii Murata</author>
<affiliation confidence="0.958690666666667">Graduate School of Information Science, Nagoya University, Japan Shigelii Matsubara Information Technology Center, Nagoya University, Japan</affiliation>
<email confidence="0.956596">matubara@nagoya-u.jp</email>
<abstract confidence="0.99915075">To support the real-time understanding of spoken monologue such as lectures and commentaries, the development of a captioning system is required. In monologues, since a sentence tends to be long, each sentence is often displayed in multi lines on one screen, it is necessary to insert linefeeds into a text so that the text becomes easy to read. This paper proposes a technique for inserting linefeeds into a Japanese spoken monologue text as an elemental technique to generate the readable captions. Our method appropriately inserts linefeeds into a sentence by machine learning, based on the information such as dependencies, clause boundaries, pauses and line length. An experiment using Japanese speech data has shown the effectiveness of our technique.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Boulianne</author>
<author>J-F Beaumont</author>
<author>M Boisvert</author>
<author>J Brousseau</author>
<author>P Cardinal</author>
<author>C Chapdelaine</author>
<author>M Comeau</author>
<author>P Ouellet</author>
<author>F Osterrath</author>
</authors>
<title>Computer-assisted closed-captioning of live TV broadcasts in French.</title>
<date>2006</date>
<booktitle>In Proceedings of 9th International Conference on Spoken Language Processing,</booktitle>
<pages>273--276</pages>
<contexts>
<context position="1546" citStr="Boulianne et al., 2006" startWordPosition="226" endWordPosition="230"> into a sentence by machine learning, based on the information such as dependencies, clause boundaries, pauses and line length. An experiment using Japanese speech data has shown the effectiveness of our technique. 1 Introduction Real-time captioning is a technique for supporting the speech understanding of deaf persons, elderly persons, or foreigners by displaying transcribed texts of monologue speech such as lectures. In recent years, there exist a lot of researches about automatic captioning, and the techniques of automatic speech recognition (ASR) aimed for captioning have been developed (Boulianne et al., 2006; Holter et al., 2000; Imai et al., 2006; Munteanu et al., 2007; Saraclar et al., 2002; Xue et al., 2006). However, in order to generate captions which is easy to read, it is important not only to recognize speech with high recognition rate but also to properly display the transcribed text on a screen (Hoogenboom et al., 2008). Especially, in spoken monologue, since a sentence tends to be long, each sentence is often displayed as a multi-line text on a screen. Therefore, proper linefeed insertion for the displayed text is desired so that the text becomes easy to read. Until now, there existed </context>
</contexts>
<marker>Boulianne, Beaumont, Boisvert, Brousseau, Cardinal, Chapdelaine, Comeau, Ouellet, Osterrath, 2006</marker>
<rawString>G. Boulianne, J.-F. Beaumont, M. Boisvert, J. Brousseau, P. Cardinal, C. Chapdelaine, M. Comeau, P. Ouellet, and F. Osterrath. 2006. Computer-assisted closed-captioning of live TV broadcasts in French. In Proceedings of 9th International Conference on Spoken Language Processing, pages 273–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Holter</author>
<author>E Harborg</author>
<author>M H Johnsen</author>
<author>T Svendsen</author>
</authors>
<title>ASR-based subtitling of live TV-programs for the hearing impaired.</title>
<date>2000</date>
<booktitle>In Proceedings of 6th International Conference on Spoken Language Processing,</booktitle>
<volume>3</volume>
<pages>570--573</pages>
<contexts>
<context position="1567" citStr="Holter et al., 2000" startWordPosition="231" endWordPosition="234">ine learning, based on the information such as dependencies, clause boundaries, pauses and line length. An experiment using Japanese speech data has shown the effectiveness of our technique. 1 Introduction Real-time captioning is a technique for supporting the speech understanding of deaf persons, elderly persons, or foreigners by displaying transcribed texts of monologue speech such as lectures. In recent years, there exist a lot of researches about automatic captioning, and the techniques of automatic speech recognition (ASR) aimed for captioning have been developed (Boulianne et al., 2006; Holter et al., 2000; Imai et al., 2006; Munteanu et al., 2007; Saraclar et al., 2002; Xue et al., 2006). However, in order to generate captions which is easy to read, it is important not only to recognize speech with high recognition rate but also to properly display the transcribed text on a screen (Hoogenboom et al., 2008). Especially, in spoken monologue, since a sentence tends to be long, each sentence is often displayed as a multi-line text on a screen. Therefore, proper linefeed insertion for the displayed text is desired so that the text becomes easy to read. Until now, there existed few researches about </context>
</contexts>
<marker>Holter, Harborg, Johnsen, Svendsen, 2000</marker>
<rawString>T. Holter, E. Harborg, M. H. Johnsen, and T. Svendsen. 2000. ASR-based subtitling of live TV-programs for the hearing impaired. In Proceedings of 6th International Conference on Spoken Language Processing, volume 3, pages 570–573.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R B Hoogenboom</author>
<author>K Uehara</author>
<author>T Kanazawa</author>
<author>S Nakano</author>
<author>H Kuroki</author>
<author>S Ino</author>
<author>T Ifukube</author>
</authors>
<title>An application of real-time captioning system using automatic speech recognition technology to college efl education for deaf and hard-of-hearing students.</title>
<date>2008</date>
<journal>Gunma University Annual Research Reports, Cultural Science Series,</journal>
<volume>57</volume>
<contexts>
<context position="1874" citStr="Hoogenboom et al., 2008" startWordPosition="286" endWordPosition="289">erly persons, or foreigners by displaying transcribed texts of monologue speech such as lectures. In recent years, there exist a lot of researches about automatic captioning, and the techniques of automatic speech recognition (ASR) aimed for captioning have been developed (Boulianne et al., 2006; Holter et al., 2000; Imai et al., 2006; Munteanu et al., 2007; Saraclar et al., 2002; Xue et al., 2006). However, in order to generate captions which is easy to read, it is important not only to recognize speech with high recognition rate but also to properly display the transcribed text on a screen (Hoogenboom et al., 2008). Especially, in spoken monologue, since a sentence tends to be long, each sentence is often displayed as a multi-line text on a screen. Therefore, proper linefeed insertion for the displayed text is desired so that the text becomes easy to read. Until now, there existed few researches about how to display text on a screen in automatic captioning. As the research about linefeed insertion, Monma et al. proposed a method based on patterns of a sequence of morphemes (Monma et al., 2003). However, the target of the research is closed-captions of Japanese TV shows, in which less than or equal to 2 </context>
</contexts>
<marker>Hoogenboom, Uehara, Kanazawa, Nakano, Kuroki, Ino, Ifukube, 2008</marker>
<rawString>R. B. Hoogenboom, K. Uehara, T. Kanazawa, S. Nakano, H. Kuroki, S. Ino, and T. Ifukube. 2008. An application of real-time captioning system using automatic speech recognition technology to college efl education for deaf and hard-of-hearing students. Gunma University Annual Research Reports, Cultural Science Series, 57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Imai</author>
<author>S Sato</author>
<author>A Kobayashi</author>
<author>K Onoe</author>
<author>S Homma</author>
</authors>
<title>Online speech detection and dual-gender speech recognition for captioning broadcast news.</title>
<date>2006</date>
<booktitle>In Proceedings of 9th International Conference on Spoken Language Processing,</booktitle>
<pages>1602--1605</pages>
<contexts>
<context position="1586" citStr="Imai et al., 2006" startWordPosition="235" endWordPosition="238">n the information such as dependencies, clause boundaries, pauses and line length. An experiment using Japanese speech data has shown the effectiveness of our technique. 1 Introduction Real-time captioning is a technique for supporting the speech understanding of deaf persons, elderly persons, or foreigners by displaying transcribed texts of monologue speech such as lectures. In recent years, there exist a lot of researches about automatic captioning, and the techniques of automatic speech recognition (ASR) aimed for captioning have been developed (Boulianne et al., 2006; Holter et al., 2000; Imai et al., 2006; Munteanu et al., 2007; Saraclar et al., 2002; Xue et al., 2006). However, in order to generate captions which is easy to read, it is important not only to recognize speech with high recognition rate but also to properly display the transcribed text on a screen (Hoogenboom et al., 2008). Especially, in spoken monologue, since a sentence tends to be long, each sentence is often displayed as a multi-line text on a screen. Therefore, proper linefeed insertion for the displayed text is desired so that the text becomes easy to read. Until now, there existed few researches about how to display text</context>
</contexts>
<marker>Imai, Sato, Kobayashi, Onoe, Homma, 2006</marker>
<rawString>T. Imai, S. Sato, A. Kobayashi, K. Onoe, and S. Homma. 2006. Online speech detection and dual-gender speech recognition for captioning broadcast news. In Proceedings of 9th International Conference on Spoken Language Processing, pages 1602–1605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kashioka</author>
<author>T Maruyama</author>
</authors>
<title>Segmentation of semantic units in Japanese monologues.</title>
<date>2004</date>
<booktitle>In Proceedings of ICSLY2004 and Oriental-COCOSDA2004,</booktitle>
<pages>87--92</pages>
<contexts>
<context position="9148" citStr="Kashioka and Maruyama, 2004" startWordPosition="1476" endWordPosition="1480">efeed point. In the analysis data, there existed 969 clause boundaries except sentence breaks. Among them, 490 were the points into which linefeeds were inserted, that is, the ratio of linefeed insertion was 51.1%. This ratio is higher than that of bunsetsu boundaries. This indicates that linefeeds tend to be inserted into clause boundaries. We investigated the ratio of linefeed insertion about 42 types of clause boundaries, which were seen in the analysis data. Table 2 shows the top 10 4In our research, we used the types of clause boundaries defined by the Clause Boundary Annotation Program (Kashioka and Maruyama, 2004). Table 2: Ratio of linefeed insertion for clause boundary type clause boundary types about the occurrence frequency, and each ratio of linefeed insertion. In the case of “compound clause-ga” and “compound clause-keredomo,” the ratio of linefeed insertion was 100%. On the other hand, in the case of “quotational clause,” “adnominal clause” and “adnominal clause-toiu,” the ratio of linefeed insertion was less than 30%. This means that the likelihood of linefeed insertion is different according to the type of the clause boundary. 3.2 Dependency Structure and Linefeed Point When a bunsetsu depends</context>
<context position="28216" citStr="Kashioka and Maruyama, 2004" startWordPosition="4697" endWordPosition="4700">Insertion Based on Automatic Natural Language Analysis In the experiment described in Section 5, we used the linguistic information provided by human as the features on the maximum entropy method. However, compared with baseline 1, our method uses a lot of linguistic information which should be provided not by human but by natural language analyzers under the real situation. Therefore, to fairly evaluate our method and four baselines, we conducted an experiment on linefeed insertion by using the automatically provided information on clause boundaries and dependency structuress. 5We used CBAP (Kashioka and Maruyama, 2004) as a clause boundary analyzer and CaboCha (Kudo and Matsumoto, 2002) with default learning data as a dependency parser. Table 5 shows the result. Compared with Table 3, it shows the decreasing rate of the performance of our method was more than those of four baselines which use simply only basic linguistic information. However, the F-measure of our method was more than 10% higher than those of four baselines. 7 Conclusion This paper proposed a method for inserting linefeeds into discourse speech data. Our method can insert linefeeds so that captions become easy to read, by using machine learn</context>
</contexts>
<marker>Kashioka, Maruyama, 2004</marker>
<rawString>H. Kashioka and T. Maruyama. 2004. Segmentation of semantic units in Japanese monologues. In Proceedings of ICSLY2004 and Oriental-COCOSDA2004, pages 87–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In Proceedings of 6th Conference on Computational Natural Language Learning,</booktitle>
<pages>63--69</pages>
<contexts>
<context position="28285" citStr="Kudo and Matsumoto, 2002" startWordPosition="4708" endWordPosition="4712"> described in Section 5, we used the linguistic information provided by human as the features on the maximum entropy method. However, compared with baseline 1, our method uses a lot of linguistic information which should be provided not by human but by natural language analyzers under the real situation. Therefore, to fairly evaluate our method and four baselines, we conducted an experiment on linefeed insertion by using the automatically provided information on clause boundaries and dependency structuress. 5We used CBAP (Kashioka and Maruyama, 2004) as a clause boundary analyzer and CaboCha (Kudo and Matsumoto, 2002) with default learning data as a dependency parser. Table 5 shows the result. Compared with Table 3, it shows the decreasing rate of the performance of our method was more than those of four baselines which use simply only basic linguistic information. However, the F-measure of our method was more than 10% higher than those of four baselines. 7 Conclusion This paper proposed a method for inserting linefeeds into discourse speech data. Our method can insert linefeeds so that captions become easy to read, by using machine learning techniques on features such as morphemes, dependencies, clause bo</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>T. Kudo and Y. Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In Proceedings of 6th Conference on Computational Natural Language Learning, pages 63–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Matsubara</author>
<author>A Takagi</author>
<author>N Kawaguchi</author>
<author>Y Inagaki</author>
</authors>
<title>Bilingual spoken monologue corpus for simultaneous machine interpretation research.</title>
<date>2002</date>
<booktitle>In Proceedings of 3rd International Conference on Language Resources and Evaluation,</booktitle>
<pages>153--159</pages>
<contexts>
<context position="5795" citStr="Matsubara et al., 2002" startWordPosition="931" endWordPosition="934"> accordance with only the width of a screen without considering the proper points of linefeeds, the caption is not easy to read. Especially, since the audience is forced to read the caption in synchronization with the speaker’s utterance speed, it is important that linefeeds are properly inserted into the displayed text in consideration of the readability as shown in Figure 3. To investigate whether the line insertion facilitates the readability of the displayed texts, we conducted an experiment using the transcribed text of lecture speeches in the Simultaneous Interpretation Database (SIDB) (Matsubara et al., 2002). We randomly selected 50 sentences from the data, and then created the following two texts for each sentence based on two different concepts about linefeed insertion. (1)Text into which linefeeds were forcibly inserted once every 20 characters Figure 3: Caption into which linefeeds are properly inserted Figure 4: Result of investigation of effect of linefeed insertion into transcription (2)Text into which linefeeds were properly inserted in consideration of readability by hand3 Figure 2 and 3 show examples of the text (1) and (2), respectively. 10 examinees decided which of the two texts was </context>
<context position="7540" citStr="Matsubara et al., 2002" startWordPosition="1219" endWordPosition="1222">er, we call a bunsetsu boundary into which a linefeed is inserted a linefeed point. 33 persons inserted linefeeds into the 50 sentences by discussing where to insert the linefeeds. 532 Table 1: Size of analysis data sentence 221 bunsetsu 2,891 character 13,899 linefeed 883 character per line 13.2 3 Preliminary Analysis about Linefeed Points In our research, the points into which linefeeds should be inserted is detected by using machine learning. To find the effective features, we investigated the spoken language corpus. In our investigation, we used Japanese monologue speech data in the SIDB (Matsubara et al., 2002). The data is annotated by hand with information on morphological analysis, bunsetsu segmentation, dependency analysis, clause boundary detection, and linefeeds insertion. Table 1 shows the size of the analysis data. Among 2,670 (= 2,891−221) bunsetsu boundaries, which are candidates of linefeed points, there existed 833 bunsetsu boundaries into which linefeeds were inserted, that is, the ratio of linefeed insertion was 31.2%. The linefeeds were inserted by hand so that the maximum number of characters per line is 20. We set the number in consideration of the relation between readability and f</context>
<context position="18487" citStr="Matsubara et al., 2002" startWordPosition="3135" endWordPosition="3138">k is classified – less than or equal to 2 – more than 2 and less than or equal to 6 – more than 6 Pause • whether or not a pause exists right after bjk Leftmost morpheme of a bunsetsu • whether or not the basic form or part-ofspeech of the leftmost morpheme of the next bunsetsu of bjk is one of the morphemes enumerated in Section 3.5. 5 Experiment To evaluate the effectiveness of our method, we conducted an experiment on inserting linefeeds by using discourse speech data. 5.1 Outline of Experiment As the experimental data, we used the transcribed data of Japanese discourse speech in the SIDB (Matsubara et al., 2002). All the data are annotated with information on morphological analysis, clause boundary detection and dependency analysis by hand. We performed a cross-validation experiment by using 16 discourses. That is, we 535 repeated the experiment, in which we used one discourse from among 16 discourses as the test data and the others as the learning data, 16 times. However, since we used 2 discourse among 16 discourses as the preliminary analysis data, we evaluated the experimental result for the other 14 discourses (1,714 sentences, 20,707 bunsetsus). Here, we used the maximum entropy method tool (Zh</context>
</contexts>
<marker>Matsubara, Takagi, Kawaguchi, Inagaki, 2002</marker>
<rawString>S. Matsubara, A. Takagi, N. Kawaguchi, and Y. Inagaki. 2002. Bilingual spoken monologue corpus for simultaneous machine interpretation research. In Proceedings of 3rd International Conference on Language Resources and Evaluation, pages 153– 159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Monma</author>
<author>E Sawamura</author>
<author>T Fukushima</author>
<author>I Maruyama</author>
<author>T Ehara</author>
<author>K Shirai</author>
</authors>
<title>Automatic closedcaption production system on TV programs for hearing-impaired people.</title>
<date>2003</date>
<booktitle>Systems and Computers in Japan,</booktitle>
<pages>34--13</pages>
<contexts>
<context position="2362" citStr="Monma et al., 2003" startWordPosition="371" endWordPosition="374">recognize speech with high recognition rate but also to properly display the transcribed text on a screen (Hoogenboom et al., 2008). Especially, in spoken monologue, since a sentence tends to be long, each sentence is often displayed as a multi-line text on a screen. Therefore, proper linefeed insertion for the displayed text is desired so that the text becomes easy to read. Until now, there existed few researches about how to display text on a screen in automatic captioning. As the research about linefeed insertion, Monma et al. proposed a method based on patterns of a sequence of morphemes (Monma et al., 2003). However, the target of the research is closed-captions of Japanese TV shows, in which less than or equal to 2 lines text is displayed on a screen and the text all switches to other text at a time. In the work, the highest priority concept on captioning is that one screen should be filled with as much text as possible. Therefore, a semantic boundary in a sentence is hardly taken into account in linefeed insertion, and the readability of the caption is hardly improved. This paper proposes a technique for inserting linefeeds into transcribed texts of Japanese monologue speech as an elemental te</context>
</contexts>
<marker>Monma, Sawamura, Fukushima, Maruyama, Ehara, Shirai, 2003</marker>
<rawString>T. Monma, E. Sawamura, T. Fukushima, I. Maruyama, T. Ehara, and K. Shirai. 2003. Automatic closedcaption production system on TV programs for hearing-impaired people. Systems and Computers in Japan, 34(13):71–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Munteanu</author>
<author>G Penn</author>
<author>R Baecker</author>
</authors>
<title>Webbased language modelling for automatic lecture transcription.</title>
<date>2007</date>
<booktitle>In Proceedings of 8th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>2353--2356</pages>
<contexts>
<context position="1609" citStr="Munteanu et al., 2007" startWordPosition="239" endWordPosition="242">uch as dependencies, clause boundaries, pauses and line length. An experiment using Japanese speech data has shown the effectiveness of our technique. 1 Introduction Real-time captioning is a technique for supporting the speech understanding of deaf persons, elderly persons, or foreigners by displaying transcribed texts of monologue speech such as lectures. In recent years, there exist a lot of researches about automatic captioning, and the techniques of automatic speech recognition (ASR) aimed for captioning have been developed (Boulianne et al., 2006; Holter et al., 2000; Imai et al., 2006; Munteanu et al., 2007; Saraclar et al., 2002; Xue et al., 2006). However, in order to generate captions which is easy to read, it is important not only to recognize speech with high recognition rate but also to properly display the transcribed text on a screen (Hoogenboom et al., 2008). Especially, in spoken monologue, since a sentence tends to be long, each sentence is often displayed as a multi-line text on a screen. Therefore, proper linefeed insertion for the displayed text is desired so that the text becomes easy to read. Until now, there existed few researches about how to display text on a screen in automat</context>
</contexts>
<marker>Munteanu, Penn, Baecker, 2007</marker>
<rawString>C. Munteanu, G. Penn, and R. Baecker. 2007. Webbased language modelling for automatic lecture transcription. In Proceedings of 8th Annual Conference of the International Speech Communication Association, pages 2353–2356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Saraclar</author>
<author>M Riley</author>
<author>E Bocchieri</author>
<author>V Goffin</author>
</authors>
<title>Towards automatic closed captioning: Low latency real time broadcast news transcription.</title>
<date>2002</date>
<booktitle>In Proceedings of 7th International Conference on Spoken Language Processing,</booktitle>
<pages>1741--1744</pages>
<contexts>
<context position="1632" citStr="Saraclar et al., 2002" startWordPosition="243" endWordPosition="246">ause boundaries, pauses and line length. An experiment using Japanese speech data has shown the effectiveness of our technique. 1 Introduction Real-time captioning is a technique for supporting the speech understanding of deaf persons, elderly persons, or foreigners by displaying transcribed texts of monologue speech such as lectures. In recent years, there exist a lot of researches about automatic captioning, and the techniques of automatic speech recognition (ASR) aimed for captioning have been developed (Boulianne et al., 2006; Holter et al., 2000; Imai et al., 2006; Munteanu et al., 2007; Saraclar et al., 2002; Xue et al., 2006). However, in order to generate captions which is easy to read, it is important not only to recognize speech with high recognition rate but also to properly display the transcribed text on a screen (Hoogenboom et al., 2008). Especially, in spoken monologue, since a sentence tends to be long, each sentence is often displayed as a multi-line text on a screen. Therefore, proper linefeed insertion for the displayed text is desired so that the text becomes easy to read. Until now, there existed few researches about how to display text on a screen in automatic captioning. As the r</context>
</contexts>
<marker>Saraclar, Riley, Bocchieri, Goffin, 2002</marker>
<rawString>M. Saraclar, M. Riley, E. Bocchieri, and V. Goffin. 2002. Towards automatic closed captioning: Low latency real time broadcast news transcription. In Proceedings of 7th International Conference on Spoken Language Processing, pages 1741–1744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xue</author>
<author>R Hu</author>
<author>Y Zhao</author>
</authors>
<title>New improvements in decoding speed and latency for automatic captioning.</title>
<date>2006</date>
<booktitle>In Proceedings of 9th International Conference on Spoken Language Processing,</booktitle>
<pages>1630--1633</pages>
<contexts>
<context position="1651" citStr="Xue et al., 2006" startWordPosition="247" endWordPosition="250"> and line length. An experiment using Japanese speech data has shown the effectiveness of our technique. 1 Introduction Real-time captioning is a technique for supporting the speech understanding of deaf persons, elderly persons, or foreigners by displaying transcribed texts of monologue speech such as lectures. In recent years, there exist a lot of researches about automatic captioning, and the techniques of automatic speech recognition (ASR) aimed for captioning have been developed (Boulianne et al., 2006; Holter et al., 2000; Imai et al., 2006; Munteanu et al., 2007; Saraclar et al., 2002; Xue et al., 2006). However, in order to generate captions which is easy to read, it is important not only to recognize speech with high recognition rate but also to properly display the transcribed text on a screen (Hoogenboom et al., 2008). Especially, in spoken monologue, since a sentence tends to be long, each sentence is often displayed as a multi-line text on a screen. Therefore, proper linefeed insertion for the displayed text is desired so that the text becomes easy to read. Until now, there existed few researches about how to display text on a screen in automatic captioning. As the research about linef</context>
</contexts>
<marker>Xue, Hu, Zhao, 2006</marker>
<rawString>J. Xue, R. Hu, and Y. Zhao. 2006. New improvements in decoding speed and latency for automatic captioning. In Proceedings of 9th International Conference on Spoken Language Processing, pages 1630–1633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhang</author>
</authors>
<title>Maximum entropy modeling toolkit for Python and C++.</title>
<date>2008</date>
<note>http: //homepages.inf.ed.ac.uk/s0450736/</note>
<contexts>
<context position="19097" citStr="Zhang, 2008" startWordPosition="3234" endWordPosition="3235">2). All the data are annotated with information on morphological analysis, clause boundary detection and dependency analysis by hand. We performed a cross-validation experiment by using 16 discourses. That is, we 535 repeated the experiment, in which we used one discourse from among 16 discourses as the test data and the others as the learning data, 16 times. However, since we used 2 discourse among 16 discourses as the preliminary analysis data, we evaluated the experimental result for the other 14 discourses (1,714 sentences, 20,707 bunsetsus). Here, we used the maximum entropy method tool (Zhang, 2008) with the default options except “-i 2000.” In the evaluation, we obtained recall, precision and the ratio of sentences into which all linefeed points were correctly inserted (hereinafter called sentence accuracy). The recall and precision are respectively defined as follows. recall = # of correctly inserted LFs # of LFs in the correct data # of correctly inserted LFs precision = # of automatically inserted LFs For comparison, we established the following four baseline methods. 1. Linefeeds are inserted into the rightmost bunsetsu boundaries among the bunsetsu boundaries into which linefeeds c</context>
</contexts>
<marker>Zhang, 2008</marker>
<rawString>L. Zhang. 2008. Maximum entropy modeling toolkit for Python and C++. http: //homepages.inf.ed.ac.uk/s0450736/</rawString>
</citation>
<citation valid="false">
<authors>
<author>maxent toolkit html</author>
</authors>
<note>[Online; accessed 1-March-2008].</note>
<marker>html, </marker>
<rawString>maxent toolkit.html. [Online; accessed 1-March-2008].</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>