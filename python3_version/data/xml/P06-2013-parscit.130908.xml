<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002046">
<title confidence="0.995503">
An Empirical Study of Chinese Chunking
</title>
<author confidence="0.997923">
Wenliang Chen, Yujie Zhang, Hitoshi Isahara
</author>
<affiliation confidence="0.995242">
Computational Linguistics Group
National Institute of Information and Communications Technology
</affiliation>
<address confidence="0.939599">
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
</address>
<email confidence="0.992658">
{chenwl, yujie, isahara}@nict.go.jp
</email>
<sectionHeader confidence="0.997341" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999896714285714">
In this paper, we describe an empirical
study of Chinese chunking on a corpus,
which is extracted from UPENN Chinese
Treebank-4 (CTB4). First, we compare
the performance of the state-of-the-art ma-
chine learning models. Then we propose
two approaches in order to improve the
performance of Chinese chunking. 1) We
propose an approach to resolve the spe-
cial problems of Chinese chunking. This
approach extends the chunk tags for ev-
ery problem by a tag-extension function.
2) We propose two novel voting meth-
ods based on the characteristics of chunk-
ing task. Compared with traditional vot-
ing methods, the proposed voting methods
consider long distance information. The
experimental results show that the SVMs
model outperforms the other models and
that our proposed approaches can improve
performance significantly.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996655039215686">
Chunking identifies the non-recursive cores of
various types of phrases in text, possibly as a
precursor to full parsing or information extrac-
tion. Steven P. Abney was the first person
to introduce chunks for parsing(Abney, 1991).
Ramshaw and Marcus(Ramshaw and Marcus,
1995) first represented base noun phrase recog-
nition as a machine learning problem. In 2000,
CoNLL-2000 introduced a shared task to tag
many kinds of phrases besides noun phrases in
English(Sang and Buchholz, 2000). Addition-
ally, many machine learning approaches, such as
Support Vector Machines (SVMs)(Vapnik, 1995),
Conditional Random Fields (CRFs)(Lafferty et
al., 2001), Memory-based Learning (MBL)(Park
and Zhang, 2003), Transformation-based Learn-
ing (TBL)(Brill, 1995), and Hidden Markov Mod-
els (HMMs)(Zhou et al., 2000), have been applied
to text chunking(Sang and Buchholz, 2000; Ham-
merton et al., 2002).
Chinese chunking is a difficult task, and much
work has been done on this topic(Li et al., 2003a;
Tan et al., 2005; Wu et al., 2005; Zhao et al.,
2000). However, there are many different Chinese
chunk definitions, which are derived from differ-
ent data sets(Li et al., 2004; Zhang and Zhou,
2002). Therefore, comparing the performance of
previous studies in Chinese chunking is very dif-
ficult. Furthermore, compared with the other lan-
guages, there are some special problems for Chi-
nese chunking(Li et al., 2004).
In this paper, we extracted the chunking corpus
from UPENN Chinese Treebank-4(CTB4). We
presented an empirical study of Chinese chunk-
ing on this corpus. First, we made an evaluation
on the corpus to clarify the performance of state-
of-the-art models in Chinese chunking. Then we
proposed two approaches in order to improve the
performance of Chinese chunking. 1) We pro-
posed an approach to resolve the special prob-
lems of Chinese chunking. This approach ex-
tended the chunk tags for every problem by a tag-
extension function. 2) We proposed two novel vot-
ing methods based on the characteristics of chunk-
ing task. Compared with traditional voting meth-
ods, the proposed voting methods considered long
distance information. The experimental results
showed the proposed approaches can improve the
performance of Chinese chunking significantly.
The rest of this paper is as follows: Section 2
describes the definitions of Chinese chunks. Sec-
</bodyText>
<page confidence="0.995556">
97
</page>
<note confidence="0.7235245">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 97–104,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998045">
tion 3 simply introduces the models and features
for Chinese chunking. Section 4 proposes a tag-
extension method. Section 5 proposes two new
voting approaches. Section 6 explains the exper-
imental results. Finally, in section 7 we draw the
conclusions.
</bodyText>
<sectionHeader confidence="0.889959" genericHeader="method">
2 Definitions of Chinese Chunks
</sectionHeader>
<bodyText confidence="0.999044">
We defined the Chinese chunks based on the CTB4
dataset1. Many researchers have extracted the
chunks from different versions of CTB(Tan et al.,
2005; Li et al., 2003b). However, these studies did
not provide sufficient detail. We developed a tool2
to extract the corpus from CTB4 by modifying the
tool Chunklink3.
</bodyText>
<subsectionHeader confidence="0.989941">
2.1 Chunk Types
</subsectionHeader>
<bodyText confidence="0.99611225">
Here we define 12 types of chunks4: ADJP, ADVP,
CLP, DNP, DP, DVP, LCP, LST, NP, PP, QP,
VP(Xue et al., 2000). Table 1 provides definitions
of these chunks.
</bodyText>
<table confidence="0.982763153846154">
Type Definition
ADJP Adjective Phrase
ADVP Adverbial Phrase
CLP Classifier Phrase
DNP DEG Phrase
DP Determiner Phrase
DVP DEV phrase
LCP Localizer Phrase
LST List Marker
NP Noun Phrase
PP Prepositional Phrase
QP Quantifier Phrase
VP Verb Phrase
</table>
<tableCaption confidence="0.999573">
Table 1: Definition of Chunks
</tableCaption>
<subsectionHeader confidence="0.998743">
2.2 Data Representation
</subsectionHeader>
<bodyText confidence="0.999775166666667">
To represent the chunks clearly, we represent the
data with an IOB-based model as the CoNLL00
shared task did, in which every word is to be
tagged with a chunk type label extended with I
(inside a chunk), O (outside a chunk), and B (in-
side a chunk, but also the first word of the chunk).
</bodyText>
<footnote confidence="0.997862">
1More detailed information at
http://www.cis.upenn.edu/ chinese/.
2Tool is available at
http://www.nlplab.cn/chenwl/tools/chunklinkctb.txt.
3Tool is available at http://ilk.uvt.nl/software.html#chunklink.
4There are 15 types in the Upenn Chinese TreeBank. The
other chunk types are FRAG, PRN, and UCP.
</footnote>
<bodyText confidence="0.9952543">
Each chunk type could be extended with I or B
tags. For instance, NP could be represented as
two types of tags, B-NP or I-NP. Therefore, we
have 25 types of chunk tags based on the IOB-
based model. Every word in a sentence will be
tagged with one of these chunk tags. For in-
stance, the sentence (word segmented and Part-of-
Speech tagged) ”他-NR(He) /到达-VV(reached)
/北京-NR(Beijing) /机场-NN(airport) /。/” will
be tagged as follows:
</bodyText>
<equation confidence="0.633773333333333">
Example 1:
S1: [NP 他][VP 到达][NP 北京/机场][O 。]
S2: 他B-NP /到达B-VP /北京B-NP /机场I-NP /。O /
</equation>
<bodyText confidence="0.9991614">
Here S1 denotes that the sentence is tagged with
chunk types, and S2 denotes that the sentence is
tagged with chunk tags based on the IOB-based
model.
With data representation, the problem of Chi-
nese chunking can be regarded as a sequence tag-
ging task. That is to say, given a sequence of
tokens (words pairing with Part-of-Speech tags),
x = x1, x2, ..., xn, we need to generate a sequence
of chunk tags, y = y1, y2,..., yn.
</bodyText>
<subsectionHeader confidence="0.999427">
2.3 Data Set
</subsectionHeader>
<bodyText confidence="0.991568">
CTB4 dataset consists of 838 files. In the ex-
periments, we used the first 728 files (FID from
chtb 001.fid to chtb 899.fid) as training data, and
the other 110 files (FID from chtb 900.fid to
chtb 1078.fid) as testing data. In the following
sections, we use the CTB4 Corpus to refer to the
extracted data set. Table 2 lists details on the
CTB4 Corpus data used in this study.
</bodyText>
<table confidence="0.996805">
Training Test
Num of Files 728 110
Num of Sentences 9,878 5,290
Num of Words 238,906 165,862
Num of Phrases 141,426 101,449
</table>
<tableCaption confidence="0.999235">
Table 2: Information of the CTB4 Corpus
</tableCaption>
<sectionHeader confidence="0.99744" genericHeader="method">
3 Chinese Chunking
</sectionHeader>
<subsectionHeader confidence="0.998627">
3.1 Models for Chinese Chunking
</subsectionHeader>
<bodyText confidence="0.999815142857143">
In this paper, we applied four models, includ-
ing SVMs, CRFs, TBL, and MBL, which have
achieved good performance in other languages.
We only describe these models briefly since full
details are presented elsewhere(Kudo and Mat-
sumoto, 2001; Sha and Pereira, 2003; Ramshaw
and Marcus, 1995; Sang, 2002).
</bodyText>
<page confidence="0.995629">
98
</page>
<subsectionHeader confidence="0.804604">
3.1.1 SVMs
</subsectionHeader>
<bodyText confidence="0.999673764705882">
Support Vector Machines (SVMs) is a pow-
erful supervised learning paradigm based on the
Structured Risk Minimization principle from com-
putational learning theory(Vapnik, 1995). Kudo
and Matsumoto(Kudo and Matsumoto, 2000) ap-
plied SVMs to English chunking and achieved
the best performance in the CoNLL00 shared
task(Sang and Buchholz, 2000). They created 231
SVMs classifiers to predict the unique pairs of
chunk tags.The final decision was given by their
weighted voting. Then the label sequence was
chosen using a dynamic programming algorithm.
Tan et al. (Tan et al., 2004) applied SVMs to
Chinese chunking. They used sigmoid functions
to extract probabilities from SVMs outputs as the
post-processing of classification. In this paper, we
used Yamcha (V0.33)5 in our experiments.
</bodyText>
<subsectionHeader confidence="0.714143">
3.1.2 CRFs
</subsectionHeader>
<bodyText confidence="0.999728333333333">
Conditional Random Fields is a powerful se-
quence labeling model(Lafferty et al., 2001) that
combine the advantages of both the generative
model and the classification model. Sha and
Pereira(Sha and Pereira, 2003) showed that state-
of-the-art results can be achieved using CRFs in
English chunking. CRFs allow us to utilize a large
number of observation features as well as differ-
ent state sequence based features and other fea-
tures we want to add. Tan et al. (Tan et al., 2005)
applied CRFs to Chinese chunking and their ex-
perimental results showed that the CRFs approach
provided better performance than HMM. In this
paper, we used MALLET (V0.3.2)6(McCallum,
2002) to implement the CRF model.
</bodyText>
<subsectionHeader confidence="0.792781">
3.1.3 TBL
</subsectionHeader>
<bodyText confidence="0.999752777777778">
Transformation based learning(TBL), first in-
troduced by Eric Brill(Brill, 1995), is mainly
based on the idea of successively transforming the
data in order to correct the error. The transforma-
tion rules obtained are usually few , yet power-
ful. TBL was applied to Chinese chunking by Li
et al.(Li et al., 2004) and TBL provided good per-
formance on their corpus. In this paper, we used
fnTBL (V1.0)7 to implement the TBL model.
</bodyText>
<footnote confidence="0.998074666666667">
5Yamcha is available at
http://chasen.org/ taku/software/yamcha/
6MALLET is available at
http://mallet.cs.umass.edu/index.php/Main Page
7fnTBL is available at
http://nlp.cs.jhu.edu/ rflorian/fntbl/index.html
</footnote>
<subsectionHeader confidence="0.601194">
3.1.4 MBL
</subsectionHeader>
<bodyText confidence="0.999541769230769">
Memory-based Learning (also called instance
based learning) is a non-parametric inductive
learning paradigm that stores training instances in
a memory structure on which predictions of new
instances are based(Walter et al., 1999). The simi-
larity between the new instance X and example Y
in memory is computed using a distance metric.
Tjong Kim Sang(Sang, 2002) applied memory-
based learning(MBL) to English chunking. MBL
performs well for a variety of shallow parsing
tasks, often yielding good results. In this paper,
we used TiMBL8(Daelemans et al., 2004) to im-
plement the MBL model.
</bodyText>
<subsectionHeader confidence="0.985587">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.986345">
The observations are based on features that are
able to represent the difference between the two
events. We utilize both lexical and Part-Of-
Speech(POS) information as the features.
We use the lexical and POS information within
a fixed window. We also consider different combi-
nations of them. The features are listed as follows:
</bodyText>
<listItem confidence="0.997351666666667">
• WORD: uni-gram and bi-grams of words in
an n window.
• POS: uni-gram and bi-grams of POS in an n
window.
• WORD+POS: Both the features of WORD
and POS.
</listItem>
<bodyText confidence="0.941994090909091">
where n is a predefined number to denote window
size.
For instance, the WORD features at the 3rd
position (北京-NR) in Example 1 (set n as 2):
”他 L2 到 达 L1 北 京 0 机 场 R1 。 R2”(uni-
gram) and ”他 到达 LB1 到达 北京 B0 北京
场 RB1 机场 。 RB2”(bi-gram). Thus features
of WORD have 9 items(5 from uni-gram and
4 from bi-grams). In the similar way, fea-
tures of POS also have 9 items and features of
WORD+POS have 18 items(9+9).
</bodyText>
<sectionHeader confidence="0.982823" genericHeader="method">
4 Tag-Extension
</sectionHeader>
<bodyText confidence="0.999767333333333">
In Chinese chunking, there are some difficult prob-
lems, which are related to Special Terms, Noun-
Noun Compounds, Named Entities Tagging and
Coordination. In this section, we propose an ap-
proach to resolve these problems by extending the
chunk tags.
</bodyText>
<footnote confidence="0.993788">
8TiMBL is available at http://ilk.uvt.nl/timbl/
</footnote>
<page confidence="0.998423">
99
</page>
<bodyText confidence="0.999986">
In the current data representation, the chunk
tags are too generic to construct accurate models.
Therefore, we define a tag-extension function fs
in order to extend the chunk tags as follows:
</bodyText>
<equation confidence="0.945794">
Te = fs(T,Q) = T - Q (1)
</equation>
<bodyText confidence="0.9997031">
where, T denotes the original tag set, Q denotes
the problem set, and Te denotes the extended tag
set. For instance, we have an q problem(q E Q).
Then we extend the chunk tags with q. For NP
Recognition, we have two new tags: B-NP-q and
I-NP-q. Here we name this approach as Tag-
Extension.
In the following three cases study, we demon-
strate that how to use Tag-Extension to resolve the
difficult problems in NP Recognition.
</bodyText>
<listItem confidence="0.998697911764706">
1) Special Terms: this kind of noun phrases
is special terms such as ”『/ 生 命(Life)/ 禁
区(Forbidden Zone)/ 』/”, which are bracketed
with the punctuation ”『, 』, 「, 」, 《, 》”.
They are divided into two types: chunks with these
punctuation and chunks without these punctua-
tion. For instance, ”『/ 生命/ 禁区/ 』/” is an
NP chunk (『B-NP/ 生命I-NP/ 禁区I-NP/ 』I-
NP/) while ”『/永远(forever)/ 盛开(full-blown)/
的(DE)/ 紫荆花(Chinese Redbud)/ 』/” is tagged
as (『O/ 永 远O /盛 开O/ 的O/ 紫 荆 花B-NP/
』O/). We extend the tags with SPE for Special
Terms: B-NP-SPE and I-NP-SPE.
2) Coordination: These problems are related
to the conjunctions ”和(and), 与(and), 或(or),
暨(and)”. They can be divided into two types:
chunks with conjunctions and chunks without
conjunctions. For instance, ”香港(HongKong)/
和(and)/ 澳门(Macau)/” is an NP chunk (香港B-
NP/ 和I-NP/ 澳门I-NP/), while in ”最低(least)/
工 资(salary)/ 和(and)/ 生 活 费(living mainte-
nance)/” it is difficult to tell whether ”最低” is a
shared modifier or not, even for people. We extend
the tags with COO for Coordination: B-NP-COO
and I-NP-COO.
3) Named Entities Tagging: Named Enti-
ties(NE)(Sang and Meulder, 2003) are not dis-
tinguished in CTB4, and they are all tagged as
”NR”. However, they play different roles in
chunks, especial in noun phrases. For instance,
”澳门-NR(Macau)/ 机场-NN(Airport)” and ”香
港-NR(Hong Kong)/ 机场-NN(Airport)” vs ”邓小
平-NR(Deng Xiaoping)/ 先生-NN(Mr.)” and ”宋
卫 平-NR(Song Weiping) 主 席-NN(President)”.
</listItem>
<bodyText confidence="0.970603333333333">
Here ”澳门” and ”香港” are LOCATION, while
”邓小平” and ”宋卫平” are PERSON. To investi-
gate the effect of Named Entities, we use a LOCA-
TION dictionary, which is generated from the PFR
corpus9 of ICL, Peking University, to tag location
words in the CTB4 Corpus. Then we extend the
tags with LOC for this problem: B-NP-LOC and
I-NP-LOC.
From the above cases study, we know the steps
of Tag-Extension. Firstly, identifying a special
problem of chunking. Secondly, extending the
chunk tags via Equation (1). Finally, replacing the
tags of related tokens with new chunk tags. After
Tag-Extension, we use new added chunk tags to
describe some special problems.
</bodyText>
<sectionHeader confidence="0.999861" genericHeader="method">
5 Voting Methods
</sectionHeader>
<bodyText confidence="0.99981008">
Kudo and Matsumoto(Kudo and Matsumoto,
2001) reported that they achieved higher accuracy
by applying voting of systems that were trained
using different data representations. Tjong Kim
Sang et al.(Sang and Buchholz, 2000) reported
similar results by combining different systems.
In order to provide better results, we also ap-
ply the voting of basic systems, including SVMs,
CRFs, MBL and TBL. Depending on the charac-
teristics in the chunking task, we propose two new
voting methods. In these two voting methods, we
consider long distance information.
In the weighted voting method, we can assign
different weights to the results of the individ-
ual system(van Halteren et al., 1998). However,
it requires a larger amount of computational ca-
pacity as the training data is divided and is re-
peatedly used to obtain the voting weights. In
this paper, we give the same weight to all ba-
sic systems in our voting methods. Suppose, we
have K basic systems, the input sentence is x =
x1, x2, ..., xn, and the results of K basic systems
are tj = t1j, t2j, ..., tnj,1 G j G K. Then our
goal is to gain a new result y = y1, y2, ..., yn by
voting.
</bodyText>
<subsectionHeader confidence="0.993655">
5.1 Basic Voting
</subsectionHeader>
<bodyText confidence="0.999824142857143">
This is traditional voting method, which is the
same as Uniform Weight in (Kudo and Mat-
sumoto, 2001). Here we name it as Basic Voting.
For each position, we have K candidates from K
basic systems. After voting, we choose the candi-
date with the most votes as the final result for each
position.
</bodyText>
<footnote confidence="0.924198">
9More information at http://www.icl.pku.edu
</footnote>
<page confidence="0.981616">
100
</page>
<subsectionHeader confidence="0.993223">
5.2 Sent-based Voting
</subsectionHeader>
<bodyText confidence="0.999988384615385">
In this paper, we treat chunking as a sequence la-
beling task. Here we apply this idea in computing
the votes of one sentence instead of one word. We
name it as Sent-based Voting. For one sentence,
we have K candidates, which are the tagged se-
quences produced by K basic systems. First, we
vote on each position, as done in Basic Voting.
Then we compute the votes of every candidate by
accumulating the votes of each position. Finally,
we choose the candidate with the most votes as
the final result for the sentence. That is to say, we
make a decision based on the votes of the whole
sentence instead of each position.
</bodyText>
<subsectionHeader confidence="0.980513">
5.3 Phrase-based Voting
</subsectionHeader>
<bodyText confidence="0.999984">
In chunking, one phrase includes one or more
words, and the word tags in one phrase depend on
each other. Therefore, we propose a novel vot-
ing method based on phrases, and we compute the
votes of one phrase instead of one word or one sen-
tence. Here we name it as Phrase-based Voting.
There are two steps in the Phrase-based Voting
procedure. First, we segment one sentence into
pieces. Then we calculate the votes of the pieces.
Table 3 is the algorithm of Phrase-based Voting,
where F(tij, tik) is a binary function:
</bodyText>
<equation confidence="0.95058525">
�
1 : tij = tik
F(tij,tik) = (2)
0 : tij =� tik
</equation>
<bodyText confidence="0.999973470588235">
In the segmenting step, we seek the ”O” or ”B-
XP” (XP can be replaced by any type of phrase)
tags, in the results of basic systems. Then we get a
new piece if all K results have the ”O” or ”B-XP”
tags at the same position.
In the voting step, the goal is to choose a result
for each piece. For each piece, we have K candi-
dates. First, we vote on each position within the
piece, as done in Basic Voting. Then we accumu-
late the votes of each position for every candidate.
Finally, we pick the one, which has the most votes,
as the final result for the piece.
The difference in these three voting methods is
that we make the decisions in different ranges: Ba-
sic Voting is at one word; Phrase-based Voting is
in one piece; and Sent-based Voting is in one sen-
tence.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.983711">
In this section, we investigated the performance of
Chinese chunking on the CTB4 Corpus.
</bodyText>
<equation confidence="0.945639136363636">
Input:
Sequence: x = x1, ..., xn;
K results: tj = t1j, ..., tnj, 1 &lt; j &lt; K.
Output:
Voted results: y = y1, y2, ..., yn
Segmenting: Segment the sentence into pieces.
Pieces[]=null; begin = 1
For each i in (2, n){
For each j in (1,K)
if(tij is not ”O” and ”B-XP”) break;
if(j &gt; K){
add new piece: p = xbegin, ..., xi_1 into Pieces;
begin = i; }}
Voting: Choose the result with the most votes for each
piece: p = xbegin, ..., xend.
Votes[K] = 0;
For each k in (1,K)
F(tij,tik) (3)
begin&lt;i&lt;end,1&lt;j&lt;K
k. = argmax1&lt;k&lt;K(V otes[k]);
Choose tbegin,kmax, ..., tend,kmax as the result for
piece p.
</equation>
<tableCaption confidence="0.989812">
Table 3: Algorithm of Phrase-based Voting
</tableCaption>
<subsectionHeader confidence="0.995606">
6.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999989">
To investigate the chunker sensitivity to the size
of the training set, we generated different sizes of
training sets, including 1%, 2%, 5%, 10%, 20%,
50%, and 100% of the total training data.
In our experiments, we used all the default pa-
rameter settings of the packages. Our SVMs and
CRFs chunkers have a first-order Markov depen-
dency between chunk tags.
We evaluated the results as CONLL2000 share-
task did. The performance of the algorithm was
measured with two scores: precision P and recall
R. Precision measures how many chunks found by
the algorithm are correct and the recall rate con-
tains the percentage of chunks defined in the cor-
pus that were found by the chunking program. The
two rates can be combined in one measure:
</bodyText>
<equation confidence="0.972283666666667">
F1 = 2 x P x R
(4)
R + P
</equation>
<bodyText confidence="0.95847">
In this paper, we report the results with F1 score.
</bodyText>
<subsectionHeader confidence="0.991253">
6.2 Experimental Results
6.2.1 POS vs. WORD+POS
</subsectionHeader>
<bodyText confidence="0.9997675">
In this experiment, we compared the perfor-
mance of different feature representations, in-
</bodyText>
<equation confidence="0.91712">
Votes[k] =
</equation>
<page confidence="0.955321">
101
</page>
<figure confidence="0.8937605">
0.01 0.02 0.05 0.1 0.2 0.5 1
Size of Training data
</figure>
<figureCaption confidence="0.999901">
Figure 1: Results of different features
</figureCaption>
<bodyText confidence="0.999907047619048">
cluding POS and WORD+ POS(See section 3.2),
and set the window size as 2. We also inves-
tigated the effects of different sizes of training
data. The SVMs and CRFs approaches were used
in the experiments because they provided good
performance in chunking(Kudo and Matsumoto,
2001)(Sha and Pereira, 2003).
Figure 1 shows the experimental results, where
xtics denotes the size of the training data, ”WP”
refers to WORD+POS, ”P” refers to POS. We can
see from the figure that WORD+POS yielded bet-
ter performance than POS in the most cases. How-
ever, when the size of training data was small,
the performance was similar. With WORD+POS,
SVMs provided higher accuracy than CRFs in
all training sizes. However, with POS, CRFs
yielded better performance than SVMs in large
scale training sizes. Furthermore, we found SVMs
with WORD+POS provided 4.07% higher accu-
racy than with POS, while CRFs provided 2.73%
higher accuracy.
</bodyText>
<subsectionHeader confidence="0.996729">
6.2.2 Comparison of Models
</subsectionHeader>
<bodyText confidence="0.9995745">
In this experiment, we compared the perfor-
mance of the models, including SVMs, CRFs,
MBL, and TBL, in Chinese chunking. In the ex-
periments, we used the feature WORD+POS and
set the window size as 2 for the first two mod-
els. For MBL, WORD features were within a one-
window size, and POS features were within a two-
window size. We used the original data for TBL
without any reformatting.
Table 4 shows the comparative results of the
models. We found that the SVMs approach was
superior to the other ones. It yielded results that
were 0.72%, 1.51%, and 3.58% higher accuracy
than respective CRFs, TBL, and MBL approaches.
</bodyText>
<table confidence="0.999385285714286">
SVMs CRFs TBL MBL
ADJP 84.45 84.55 85.95 80.48
ADVP 83.12 82.74 81.98 77.95
CLP 5.26 0.00 0.00 3.70
DNP 99.65 99.64 99.65 99.61
DP 99.70 99.40 99.70 99.46
DVP 96.77 92.89 99.61 99.41
LCP 99.85 99.85 99.74 99.82
LST 68.75 68.25 56.72 64.75
NP 90.54 89.79 89.82 87.90
PP 99.67 99.66 99.67 99.59
QP 96.73 96.53 96.60 96.40
VP 89.74 88.50 85.75 82.51
+ 91.46 90.74 89.95 87.88
</table>
<tableCaption confidence="0.993706">
Table 4: Comparative Results of Models
</tableCaption>
<table confidence="0.999931">
Method Precision Recall Fi
CRFs 91.47 90.01 90.74
SVMs 92.03 90.91 91.46
V1 91.97 90.66 91.31
V2 92.32 90.93 91.62
V3 92.40 90.97 91.68
</table>
<tableCaption confidence="0.999259">
Table 5: Voting Results
</tableCaption>
<bodyText confidence="0.99969125">
Giving more details for each category, the SVMs
approach provided the best results in ten cate-
gories, the CRFs in one category, and the TBL in
five categories.
</bodyText>
<subsectionHeader confidence="0.983195">
6.2.3 Comparison of Voting Methods
</subsectionHeader>
<bodyText confidence="0.9998726">
In this section, we compared the performance of
the voting methods of four basic systems, which
were used in Section 6.2.2. Table 5 shows the
results of the voting systems, where V1 refers
to Basic Voting, V2 refers to Sent-based Voting,
and V3 refers to Phrase-based Voting. We found
that Basic Voting provided slightly worse results
than SVMs. However, by applying the Sent-
based Voting method, we achieved higher accu-
racy than any single system. Furthermore, we
were able to achieve more higher accuracy by ap-
plying Phrase-based Voting. Phrase-based Voting
provided 0.22% and 0.94% higher accuracy than
respective SVMs, CRFs approaches, the best two
single systems.
The results suggested that the Phrase-based Vot-
ing method is quite suitable for chunking task. The
Phrase-based Voting method considers one chunk
as a voting unit instead of one word or one sen-
tence.
</bodyText>
<figure confidence="0.9791128">
95
90
85
80
75
70
SVM_WP
SVM P
CRF WP
CRF P
</figure>
<page confidence="0.976104">
102
</page>
<table confidence="0.997938">
SVMs CRFs TBL MBL V3
NPR 90.62 89.72 89.89 87.77 90.92
COO 90.61 89.78 90.05 87.80 91.03
SPE 90.65 90.14 90.31 87.77 91.00
LOC 90.53 89.83 89.69 87.78 90.86
NPR* - - - - 91.13
</table>
<tableCaption confidence="0.9016185">
Table 6: Results of Tag-Extension in NP Recogni-
tion
</tableCaption>
<subsectionHeader confidence="0.875528">
6.2.4 Tag-Extension
</subsectionHeader>
<bodyText confidence="0.998960257142857">
NP is the most important phrase in Chinese
chunking and about 47% phrases in the CTB4 Cor-
pus are NPs. In this experiment, we presented the
results of Tag-Extension in NP Recognition.
Table 6 shows the experimental results of Tag-
Extension, where ”NPR” refers to chunking with-
out any extension, ”SPE” refers to chunking
with Special Terms Tag-Extension, ”COO” refers
to chunking with Coordination Tag-Extension,
”LOC” refers to chunking with LOCATION Tag-
Extension, ”NPR*” refers to voting of eight sys-
tems(four of SPE and four of COO), and ”V3”
refers to Phrase-based Voting method.
For NP Recognition, SVMs also yielded the
best results. But it was surprised that TBL pro-
vided 0.17% higher accuracy than CRFs. By ap-
plying Phrase-based Voting, we achieved better re-
sults, 0.30% higher accuracy than SVMs.
From the table, we can see that the Tag-
Extension approach can provide better results. In
COO, TBL got the most improvement with 0.16%.
And in SPE, TBL and CRFs got the same improve-
ment with 0.42%. We also found that Phrase-
based Voting can improve the performance signif-
icantly. NPR* provided 0.51% higher than SVMs,
the best single system.
For LOC, the voting method helped to improve
the performance, provided at least 0.33% higher
accuracy than any single system. But we also
found that CRFs and MBL provided better results
while SVMs and TBL yielded worse results. The
reason was that our NE tagging method was very
simple. We believe NE tagging can be effective
in Chinese chunking, if we use a highly accurate
Named Entity Recognition system.
</bodyText>
<sectionHeader confidence="0.999214" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999940869565217">
In this paper, we conducted an empirical study of
Chinese chunking. We compared the performance
of four models, SVMs, CRFs, MBL, and TBL.
We also investigated the effects of using different
sizes of training data. In order to provide higher
accuracy, we proposed two new voting methods
according to the characteristics of the chunking
task. We proposed the Tag-Extension approach to
resolve the special problems of Chinese chunking
by extending the chunk tags.
The experimental results showed that the SVMs
model was superior to the other three models.
We also found that part-of-speech tags played an
important role in Chinese chunking because the
gap of the performance between WORD+POS and
POS was very small.
We found that the proposed voting approaches
can provide higher accuracy than any single sys-
tem can. In particular, the Phrase-based Voting ap-
proach is more suitable for chunking task than the
other two voting approaches. Our experimental
results also indicated that the Tag-Extension ap-
proach can improve the performance significantly.
</bodyText>
<sectionHeader confidence="0.996665" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993577607142857">
Steven P. Abney. 1991. Parsing by chunks. In
Robert C. Berwick, Steven P. Abney, and Carol
Tenny, editors, Principle-Based Parsing: Computa-
tion and Psycholinguistics, pages 257–278. Kluwer,
Dordrecht.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational Lin-
guistics, 21(4):543–565.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2004. Timbl: Tilburg
memory-based learner v5.1.
James Hammerton, Miles Osborne, Susan Armstrong,
and Walter Daelemans. 2002. Introduction to spe-
cial issue on machine learning approaches to shallow
parsing. JMLR, 2(3):551–558.
Taku Kudo and Yuji Matsumoto. 2000. Use of sup-
port vector learning for chunk identification. In In
Proceedings of CoNLL-2000 and LLL-2000, pages
142–144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In In Proceedings of
NAACL01.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In International Conference on Ma-
chine Learning (ICML01).
</reference>
<page confidence="0.987928">
103
</page>
<reference confidence="0.999838573333334">
Heng Li, Jonathan J. Webster, Chunyu Kit, and Tian-
shun Yao. 2003a. Transductive hmm based chi-
nese text chunking. In Proceedings of IEEE NLP-
KE2003, pages 257–262, Beijing, China.
Sujian Li, Qun Liu, and Zhifeng Yang. 2003b. Chunk-
ing parsing with maximum entropy principle (in chi-
nese). Chinese Journal of Computers, 26(12):1722–
1727.
Hongqiao Li, Changning Huang, Jianfeng Gao, and Xi-
aozhong Fan. 2004. Chinese chunking with another
type of spec. In The Third SIGHAN Workshop on
Chinese Language Processing.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Seong-Bae Park and Byoung-Tak Zhang. 2003.
Text chunking by combining hand-crafted rules and
memory-based learning. In ACL, pages 497–504.
Lance Ramshaw and Mitch Marcus. 1995. Text
chunking using transformation-based learning. In
David Yarovsky and Kenneth Church, editors, Pro-
ceedings of the Third Workshop on Very Large Cor-
pora, pages 82–94, Somerset, New Jersey. Associa-
tion for Computational Linguistics.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: Chunk-
ing. In Proceedings of CoNLL-2000 and LLL2000,
pages 127–132, Lisbin, Portugal.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL-2003.
Erik F. Tjong Kim Sang. 2002. Memory-based shal-
low parsing. JMLR, 2(3):559–594.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT-NAACL03.
Yongmei Tan, Tianshun Yao, Qing Chen, and Jingbo
Zhu. 2004. Chinese chunk identification using svms
plus sigmoid. In IJCNLP, pages 527–536.
Yongmei Tan, Tianshun Yao, Qing Chen, and Jingbo
Zhu. 2005. Applying conditional random fields
to chinese shallow parsing. In Proceedings of
CICLing-2005, pages 167–176, Mexico City, Mex-
ico. Springer.
Hans van Halteren, Jakub Zavrel, and Walter Daele-
mans. 1998. Improving data driven wordclass tag-
ging by system combination. In COLING-ACL,
pages 491–497.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer-Verlag, New York.
Daelemans Walter, Sabine Buchholz, and Jorn Veen-
stra. 1999. Memory-based shallow parsing.
Shih-Hung Wu, Cheng-Wei Shih, Chia-Wei Wu,
Tzong-Han Tsai, and Wen-Lian Hsu. 2005. Ap-
plying maximum entropy to robust chinese shallow
parsing. In Proceedings of ROCLING2005.
Nianwen Xue, Fei Xia, Shizhe Huang, and Anthony
Kroch. 2000. The bracketing guidelines for the
penn chinese treebank. Technical report, University
of Pennsylvania.
Yuqi Zhang and Qiang Zhou. 2002. Chinese base-
phrases chunking. In Proceedings of The First
SIGHAN Workshop on Chinese Language Process-
ing.
Tiejun Zhao, Muyun Yang, Fang Liu, Jianmin Yao, and
Hao Yu. 2000. Statistics based hybrid approach to
chinese base phrase identification. In Proceedings
of Second Chinese Language Processing Workshop.
GuoDong Zhou, Jian Su, and TongGuan Tey. 2000.
Hybrid text chunking. In Claire Cardie, Walter
Daelemans, Claire N´edellec, and Erik Tjong Kim
Sang, editors, Proceedings of the CoNLL00, Lis-
bon, 2000, pages 163–165. Association for Compu-
tational Linguistics, Somerset, New Jersey.
</reference>
<page confidence="0.998788">
104
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.954965">
<title confidence="0.999761">An Empirical Study of Chinese Chunking</title>
<author confidence="0.985614">Wenliang Chen</author>
<author confidence="0.985614">Yujie Zhang</author>
<author confidence="0.985614">Hitoshi Isahara</author>
<affiliation confidence="0.999333">Computational Linguistics Group National Institute of Information and Communications Technology</affiliation>
<address confidence="0.987941">3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289</address>
<email confidence="0.991649">yujie,</email>
<abstract confidence="0.999536363636364">In this paper, we describe an empirical study of Chinese chunking on a corpus, which is extracted from UPENN Chinese Treebank-4 (CTB4). First, we compare the performance of the state-of-the-art machine learning models. Then we propose two approaches in order to improve the performance of Chinese chunking. 1) We propose an approach to resolve the special problems of Chinese chunking. This approach extends the chunk tags for every problem by a tag-extension function. 2) We propose two novel voting methods based on the characteristics of chunking task. Compared with traditional voting methods, the proposed voting methods consider long distance information. The experimental results show that the SVMs model outperforms the other models and that our proposed approaches can improve performance significantly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Parsing by chunks.</title>
<date>1991</date>
<booktitle>Principle-Based Parsing: Computation and Psycholinguistics,</booktitle>
<pages>257--278</pages>
<editor>In Robert C. Berwick, Steven P. Abney, and Carol Tenny, editors,</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="1343" citStr="Abney, 1991" startWordPosition="196" endWordPosition="197"> tag-extension function. 2) We propose two novel voting methods based on the characteristics of chunking task. Compared with traditional voting methods, the proposed voting methods consider long distance information. The experimental results show that the SVMs model outperforms the other models and that our proposed approaches can improve performance significantly. 1 Introduction Chunking identifies the non-recursive cores of various types of phrases in text, possibly as a precursor to full parsing or information extraction. Steven P. Abney was the first person to introduce chunks for parsing(Abney, 1991). Ramshaw and Marcus(Ramshaw and Marcus, 1995) first represented base noun phrase recognition as a machine learning problem. In 2000, CoNLL-2000 introduced a shared task to tag many kinds of phrases besides noun phrases in English(Sang and Buchholz, 2000). Additionally, many machine learning approaches, such as Support Vector Machines (SVMs)(Vapnik, 1995), Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunk</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Steven P. Abney. 1991. Parsing by chunks. In Robert C. Berwick, Steven P. Abney, and Carol Tenny, editors, Principle-Based Parsing: Computation and Psycholinguistics, pages 257–278. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="1858" citStr="Brill, 1995" startWordPosition="266" endWordPosition="267">ion extraction. Steven P. Abney was the first person to introduce chunks for parsing(Abney, 1991). Ramshaw and Marcus(Ramshaw and Marcus, 1995) first represented base noun phrase recognition as a machine learning problem. In 2000, CoNLL-2000 introduced a shared task to tag many kinds of phrases besides noun phrases in English(Sang and Buchholz, 2000). Additionally, many machine learning approaches, such as Support Vector Machines (SVMs)(Vapnik, 1995), Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Therefore, comparing the performance of previous studies in Chinese chunking is very difficult. Furthermore, compared with the other languages, there are some speci</context>
<context position="8683" citStr="Brill, 1995" startWordPosition="1382" endWordPosition="1383">el. Sha and Pereira(Sha and Pereira, 2003) showed that stateof-the-art results can be achieved using CRFs in English chunking. CRFs allow us to utilize a large number of observation features as well as different state sequence based features and other features we want to add. Tan et al. (Tan et al., 2005) applied CRFs to Chinese chunking and their experimental results showed that the CRFs approach provided better performance than HMM. In this paper, we used MALLET (V0.3.2)6(McCallum, 2002) to implement the CRF model. 3.1.3 TBL Transformation based learning(TBL), first introduced by Eric Brill(Brill, 1995), is mainly based on the idea of successively transforming the data in order to correct the error. The transformation rules obtained are usually few , yet powerful. TBL was applied to Chinese chunking by Li et al.(Li et al., 2004) and TBL provided good performance on their corpus. In this paper, we used fnTBL (V1.0)7 to implement the TBL model. 5Yamcha is available at http://chasen.org/ taku/software/yamcha/ 6MALLET is available at http://mallet.cs.umass.edu/index.php/Main Page 7fnTBL is available at http://nlp.cs.jhu.edu/ rflorian/fntbl/index.html 3.1.4 MBL Memory-based Learning (also called </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>Timbl: Tilburg memory-based learner v5.1.</title>
<date>2004</date>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2004</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2004. Timbl: Tilburg memory-based learner v5.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Hammerton</author>
<author>Miles Osborne</author>
<author>Susan Armstrong</author>
<author>Walter Daelemans</author>
</authors>
<title>Introduction to special issue on machine learning approaches to shallow parsing.</title>
<date>2002</date>
<journal>JMLR,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="1995" citStr="Hammerton et al., 2002" startWordPosition="286" endWordPosition="290">and Marcus, 1995) first represented base noun phrase recognition as a machine learning problem. In 2000, CoNLL-2000 introduced a shared task to tag many kinds of phrases besides noun phrases in English(Sang and Buchholz, 2000). Additionally, many machine learning approaches, such as Support Vector Machines (SVMs)(Vapnik, 1995), Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Therefore, comparing the performance of previous studies in Chinese chunking is very difficult. Furthermore, compared with the other languages, there are some special problems for Chinese chunking(Li et al., 2004). In this paper, we extracted the chunking corpus from UPENN Chinese Treebank-4(CTB4). W</context>
</contexts>
<marker>Hammerton, Osborne, Armstrong, Daelemans, 2002</marker>
<rawString>James Hammerton, Miles Osborne, Susan Armstrong, and Walter Daelemans. 2002. Introduction to special issue on machine learning approaches to shallow parsing. JMLR, 2(3):551–558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Use of support vector learning for chunk identification. In</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL-2000 and LLL-2000,</booktitle>
<pages>142--144</pages>
<contexts>
<context position="7328" citStr="Kudo and Matsumoto, 2000" startWordPosition="1165" endWordPosition="1168">able 2: Information of the CTB4 Corpus 3 Chinese Chunking 3.1 Models for Chinese Chunking In this paper, we applied four models, including SVMs, CRFs, TBL, and MBL, which have achieved good performance in other languages. We only describe these models briefly since full details are presented elsewhere(Kudo and Matsumoto, 2001; Sha and Pereira, 2003; Ramshaw and Marcus, 1995; Sang, 2002). 98 3.1.1 SVMs Support Vector Machines (SVMs) is a powerful supervised learning paradigm based on the Structured Risk Minimization principle from computational learning theory(Vapnik, 1995). Kudo and Matsumoto(Kudo and Matsumoto, 2000) applied SVMs to English chunking and achieved the best performance in the CoNLL00 shared task(Sang and Buchholz, 2000). They created 231 SVMs classifiers to predict the unique pairs of chunk tags.The final decision was given by their weighted voting. Then the label sequence was chosen using a dynamic programming algorithm. Tan et al. (Tan et al., 2004) applied SVMs to Chinese chunking. They used sigmoid functions to extract probabilities from SVMs outputs as the post-processing of classification. In this paper, we used Yamcha (V0.33)5 in our experiments. 3.1.2 CRFs Conditional Random Fields i</context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2000. Use of support vector learning for chunk identification. In In Proceedings of CoNLL-2000 and LLL-2000, pages 142–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines. In</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL01.</booktitle>
<contexts>
<context position="7030" citStr="Kudo and Matsumoto, 2001" startWordPosition="1121" endWordPosition="1125">d) as testing data. In the following sections, we use the CTB4 Corpus to refer to the extracted data set. Table 2 lists details on the CTB4 Corpus data used in this study. Training Test Num of Files 728 110 Num of Sentences 9,878 5,290 Num of Words 238,906 165,862 Num of Phrases 141,426 101,449 Table 2: Information of the CTB4 Corpus 3 Chinese Chunking 3.1 Models for Chinese Chunking In this paper, we applied four models, including SVMs, CRFs, TBL, and MBL, which have achieved good performance in other languages. We only describe these models briefly since full details are presented elsewhere(Kudo and Matsumoto, 2001; Sha and Pereira, 2003; Ramshaw and Marcus, 1995; Sang, 2002). 98 3.1.1 SVMs Support Vector Machines (SVMs) is a powerful supervised learning paradigm based on the Structured Risk Minimization principle from computational learning theory(Vapnik, 1995). Kudo and Matsumoto(Kudo and Matsumoto, 2000) applied SVMs to English chunking and achieved the best performance in the CoNLL00 shared task(Sang and Buchholz, 2000). They created 231 SVMs classifiers to predict the unique pairs of chunk tags.The final decision was given by their weighted voting. Then the label sequence was chosen using a dynamic</context>
<context position="13815" citStr="Kudo and Matsumoto, 2001" startWordPosition="2235" endWordPosition="2238"> the effect of Named Entities, we use a LOCATION dictionary, which is generated from the PFR corpus9 of ICL, Peking University, to tag location words in the CTB4 Corpus. Then we extend the tags with LOC for this problem: B-NP-LOC and I-NP-LOC. From the above cases study, we know the steps of Tag-Extension. Firstly, identifying a special problem of chunking. Secondly, extending the chunk tags via Equation (1). Finally, replacing the tags of related tokens with new chunk tags. After Tag-Extension, we use new added chunk tags to describe some special problems. 5 Voting Methods Kudo and Matsumoto(Kudo and Matsumoto, 2001) reported that they achieved higher accuracy by applying voting of systems that were trained using different data representations. Tjong Kim Sang et al.(Sang and Buchholz, 2000) reported similar results by combining different systems. In order to provide better results, we also apply the voting of basic systems, including SVMs, CRFs, MBL and TBL. Depending on the characteristics in the chunking task, we propose two new voting methods. In these two voting methods, we consider long distance information. In the weighted voting method, we can assign different weights to the results of the individu</context>
<context position="19340" citStr="Kudo and Matsumoto, 2001" startWordPosition="3241" endWordPosition="3244">be combined in one measure: F1 = 2 x P x R (4) R + P In this paper, we report the results with F1 score. 6.2 Experimental Results 6.2.1 POS vs. WORD+POS In this experiment, we compared the performance of different feature representations, inVotes[k] = 101 0.01 0.02 0.05 0.1 0.2 0.5 1 Size of Training data Figure 1: Results of different features cluding POS and WORD+ POS(See section 3.2), and set the window size as 2. We also investigated the effects of different sizes of training data. The SVMs and CRFs approaches were used in the experiments because they provided good performance in chunking(Kudo and Matsumoto, 2001)(Sha and Pereira, 2003). Figure 1 shows the experimental results, where xtics denotes the size of the training data, ”WP” refers to WORD+POS, ”P” refers to POS. We can see from the figure that WORD+POS yielded better performance than POS in the most cases. However, when the size of training data was small, the performance was similar. With WORD+POS, SVMs provided higher accuracy than CRFs in all training sizes. However, with POS, CRFs yielded better performance than SVMs in large scale training sizes. Furthermore, we found SVMs with WORD+POS provided 4.07% higher accuracy than with POS, while </context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In In Proceedings of NAACL01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In International Conference on Machine Learning (ICML01).</booktitle>
<contexts>
<context position="1757" citStr="Lafferty et al., 2001" startWordPosition="253" endWordPosition="256">he non-recursive cores of various types of phrases in text, possibly as a precursor to full parsing or information extraction. Steven P. Abney was the first person to introduce chunks for parsing(Abney, 1991). Ramshaw and Marcus(Ramshaw and Marcus, 1995) first represented base noun phrase recognition as a machine learning problem. In 2000, CoNLL-2000 introduced a shared task to tag many kinds of phrases besides noun phrases in English(Sang and Buchholz, 2000). Additionally, many machine learning approaches, such as Support Vector Machines (SVMs)(Vapnik, 1995), Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Therefore, comparing the performance of previous studies in Chi</context>
<context position="7987" citStr="Lafferty et al., 2001" startWordPosition="1268" endWordPosition="1271"> achieved the best performance in the CoNLL00 shared task(Sang and Buchholz, 2000). They created 231 SVMs classifiers to predict the unique pairs of chunk tags.The final decision was given by their weighted voting. Then the label sequence was chosen using a dynamic programming algorithm. Tan et al. (Tan et al., 2004) applied SVMs to Chinese chunking. They used sigmoid functions to extract probabilities from SVMs outputs as the post-processing of classification. In this paper, we used Yamcha (V0.33)5 in our experiments. 3.1.2 CRFs Conditional Random Fields is a powerful sequence labeling model(Lafferty et al., 2001) that combine the advantages of both the generative model and the classification model. Sha and Pereira(Sha and Pereira, 2003) showed that stateof-the-art results can be achieved using CRFs in English chunking. CRFs allow us to utilize a large number of observation features as well as different state sequence based features and other features we want to add. Tan et al. (Tan et al., 2005) applied CRFs to Chinese chunking and their experimental results showed that the CRFs approach provided better performance than HMM. In this paper, we used MALLET (V0.3.2)6(McCallum, 2002) to implement the CRF </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In International Conference on Machine Learning (ICML01).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Li</author>
<author>Jonathan J Webster</author>
<author>Chunyu Kit</author>
<author>Tianshun Yao</author>
</authors>
<title>Transductive hmm based chinese text chunking.</title>
<date>2003</date>
<booktitle>In Proceedings of IEEE NLPKE2003,</booktitle>
<pages>257--262</pages>
<location>Beijing, China.</location>
<contexts>
<context position="2092" citStr="Li et al., 2003" startWordPosition="305" endWordPosition="308">CoNLL-2000 introduced a shared task to tag many kinds of phrases besides noun phrases in English(Sang and Buchholz, 2000). Additionally, many machine learning approaches, such as Support Vector Machines (SVMs)(Vapnik, 1995), Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Therefore, comparing the performance of previous studies in Chinese chunking is very difficult. Furthermore, compared with the other languages, there are some special problems for Chinese chunking(Li et al., 2004). In this paper, we extracted the chunking corpus from UPENN Chinese Treebank-4(CTB4). We presented an empirical study of Chinese chunking on this corpus. First, we made an evaluation o</context>
<context position="4043" citStr="Li et al., 2003" startWordPosition="615" endWordPosition="618">se chunks. Sec97 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 97–104, Sydney, July 2006. c�2006 Association for Computational Linguistics tion 3 simply introduces the models and features for Chinese chunking. Section 4 proposes a tagextension method. Section 5 proposes two new voting approaches. Section 6 explains the experimental results. Finally, in section 7 we draw the conclusions. 2 Definitions of Chinese Chunks We defined the Chinese chunks based on the CTB4 dataset1. Many researchers have extracted the chunks from different versions of CTB(Tan et al., 2005; Li et al., 2003b). However, these studies did not provide sufficient detail. We developed a tool2 to extract the corpus from CTB4 by modifying the tool Chunklink3. 2.1 Chunk Types Here we define 12 types of chunks4: ADJP, ADVP, CLP, DNP, DP, DVP, LCP, LST, NP, PP, QP, VP(Xue et al., 2000). Table 1 provides definitions of these chunks. Type Definition ADJP Adjective Phrase ADVP Adverbial Phrase CLP Classifier Phrase DNP DEG Phrase DP Determiner Phrase DVP DEV phrase LCP Localizer Phrase LST List Marker NP Noun Phrase PP Prepositional Phrase QP Quantifier Phrase VP Verb Phrase Table 1: Definition of Chunks 2.2</context>
</contexts>
<marker>Li, Webster, Kit, Yao, 2003</marker>
<rawString>Heng Li, Jonathan J. Webster, Chunyu Kit, and Tianshun Yao. 2003a. Transductive hmm based chinese text chunking. In Proceedings of IEEE NLPKE2003, pages 257–262, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujian Li</author>
<author>Qun Liu</author>
<author>Zhifeng Yang</author>
</authors>
<title>Chunking parsing with maximum entropy principle (in chinese).</title>
<date>2003</date>
<journal>Chinese Journal of Computers,</journal>
<volume>26</volume>
<issue>12</issue>
<pages>1727</pages>
<contexts>
<context position="2092" citStr="Li et al., 2003" startWordPosition="305" endWordPosition="308">CoNLL-2000 introduced a shared task to tag many kinds of phrases besides noun phrases in English(Sang and Buchholz, 2000). Additionally, many machine learning approaches, such as Support Vector Machines (SVMs)(Vapnik, 1995), Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Therefore, comparing the performance of previous studies in Chinese chunking is very difficult. Furthermore, compared with the other languages, there are some special problems for Chinese chunking(Li et al., 2004). In this paper, we extracted the chunking corpus from UPENN Chinese Treebank-4(CTB4). We presented an empirical study of Chinese chunking on this corpus. First, we made an evaluation o</context>
<context position="4043" citStr="Li et al., 2003" startWordPosition="615" endWordPosition="618">se chunks. Sec97 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 97–104, Sydney, July 2006. c�2006 Association for Computational Linguistics tion 3 simply introduces the models and features for Chinese chunking. Section 4 proposes a tagextension method. Section 5 proposes two new voting approaches. Section 6 explains the experimental results. Finally, in section 7 we draw the conclusions. 2 Definitions of Chinese Chunks We defined the Chinese chunks based on the CTB4 dataset1. Many researchers have extracted the chunks from different versions of CTB(Tan et al., 2005; Li et al., 2003b). However, these studies did not provide sufficient detail. We developed a tool2 to extract the corpus from CTB4 by modifying the tool Chunklink3. 2.1 Chunk Types Here we define 12 types of chunks4: ADJP, ADVP, CLP, DNP, DP, DVP, LCP, LST, NP, PP, QP, VP(Xue et al., 2000). Table 1 provides definitions of these chunks. Type Definition ADJP Adjective Phrase ADVP Adverbial Phrase CLP Classifier Phrase DNP DEG Phrase DP Determiner Phrase DVP DEV phrase LCP Localizer Phrase LST List Marker NP Noun Phrase PP Prepositional Phrase QP Quantifier Phrase VP Verb Phrase Table 1: Definition of Chunks 2.2</context>
</contexts>
<marker>Li, Liu, Yang, 2003</marker>
<rawString>Sujian Li, Qun Liu, and Zhifeng Yang. 2003b. Chunking parsing with maximum entropy principle (in chinese). Chinese Journal of Computers, 26(12):1722– 1727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongqiao Li</author>
<author>Changning Huang</author>
<author>Jianfeng Gao</author>
<author>Xiaozhong Fan</author>
</authors>
<title>Chinese chunking with another type of spec.</title>
<date>2004</date>
<booktitle>In The Third SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="2269" citStr="Li et al., 2004" startWordPosition="336" endWordPosition="339">s Support Vector Machines (SVMs)(Vapnik, 1995), Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Therefore, comparing the performance of previous studies in Chinese chunking is very difficult. Furthermore, compared with the other languages, there are some special problems for Chinese chunking(Li et al., 2004). In this paper, we extracted the chunking corpus from UPENN Chinese Treebank-4(CTB4). We presented an empirical study of Chinese chunking on this corpus. First, we made an evaluation on the corpus to clarify the performance of stateof-the-art models in Chinese chunking. Then we proposed two approaches in order to improve the performance of Chinese chunking. 1</context>
<context position="8913" citStr="Li et al., 2004" startWordPosition="1422" endWordPosition="1425"> based features and other features we want to add. Tan et al. (Tan et al., 2005) applied CRFs to Chinese chunking and their experimental results showed that the CRFs approach provided better performance than HMM. In this paper, we used MALLET (V0.3.2)6(McCallum, 2002) to implement the CRF model. 3.1.3 TBL Transformation based learning(TBL), first introduced by Eric Brill(Brill, 1995), is mainly based on the idea of successively transforming the data in order to correct the error. The transformation rules obtained are usually few , yet powerful. TBL was applied to Chinese chunking by Li et al.(Li et al., 2004) and TBL provided good performance on their corpus. In this paper, we used fnTBL (V1.0)7 to implement the TBL model. 5Yamcha is available at http://chasen.org/ taku/software/yamcha/ 6MALLET is available at http://mallet.cs.umass.edu/index.php/Main Page 7fnTBL is available at http://nlp.cs.jhu.edu/ rflorian/fntbl/index.html 3.1.4 MBL Memory-based Learning (also called instance based learning) is a non-parametric inductive learning paradigm that stores training instances in a memory structure on which predictions of new instances are based(Walter et al., 1999). The similarity between the new ins</context>
</contexts>
<marker>Li, Huang, Gao, Fan, 2004</marker>
<rawString>Hongqiao Li, Changning Huang, Jianfeng Gao, and Xiaozhong Fan. 2004. Chinese chunking with another type of spec. In The Third SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="8565" citStr="McCallum, 2002" startWordPosition="1365" endWordPosition="1366">labeling model(Lafferty et al., 2001) that combine the advantages of both the generative model and the classification model. Sha and Pereira(Sha and Pereira, 2003) showed that stateof-the-art results can be achieved using CRFs in English chunking. CRFs allow us to utilize a large number of observation features as well as different state sequence based features and other features we want to add. Tan et al. (Tan et al., 2005) applied CRFs to Chinese chunking and their experimental results showed that the CRFs approach provided better performance than HMM. In this paper, we used MALLET (V0.3.2)6(McCallum, 2002) to implement the CRF model. 3.1.3 TBL Transformation based learning(TBL), first introduced by Eric Brill(Brill, 1995), is mainly based on the idea of successively transforming the data in order to correct the error. The transformation rules obtained are usually few , yet powerful. TBL was applied to Chinese chunking by Li et al.(Li et al., 2004) and TBL provided good performance on their corpus. In this paper, we used fnTBL (V1.0)7 to implement the TBL model. 5Yamcha is available at http://chasen.org/ taku/software/yamcha/ 6MALLET is available at http://mallet.cs.umass.edu/index.php/Main Page</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seong-Bae Park</author>
<author>Byoung-Tak Zhang</author>
</authors>
<title>Text chunking by combining hand-crafted rules and memory-based learning.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>497--504</pages>
<contexts>
<context position="1808" citStr="Park and Zhang, 2003" startWordPosition="259" endWordPosition="262">n text, possibly as a precursor to full parsing or information extraction. Steven P. Abney was the first person to introduce chunks for parsing(Abney, 1991). Ramshaw and Marcus(Ramshaw and Marcus, 1995) first represented base noun phrase recognition as a machine learning problem. In 2000, CoNLL-2000 introduced a shared task to tag many kinds of phrases besides noun phrases in English(Sang and Buchholz, 2000). Additionally, many machine learning approaches, such as Support Vector Machines (SVMs)(Vapnik, 1995), Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Therefore, comparing the performance of previous studies in Chinese chunking is very difficult. Furthermore, compa</context>
</contexts>
<marker>Park, Zhang, 2003</marker>
<rawString>Seong-Bae Park and Byoung-Tak Zhang. 2003. Text chunking by combining hand-crafted rules and memory-based learning. In ACL, pages 497–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance Ramshaw</author>
<author>Mitch Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<editor>In David Yarovsky and Kenneth Church, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="1389" citStr="Ramshaw and Marcus, 1995" startWordPosition="200" endWordPosition="203">opose two novel voting methods based on the characteristics of chunking task. Compared with traditional voting methods, the proposed voting methods consider long distance information. The experimental results show that the SVMs model outperforms the other models and that our proposed approaches can improve performance significantly. 1 Introduction Chunking identifies the non-recursive cores of various types of phrases in text, possibly as a precursor to full parsing or information extraction. Steven P. Abney was the first person to introduce chunks for parsing(Abney, 1991). Ramshaw and Marcus(Ramshaw and Marcus, 1995) first represented base noun phrase recognition as a machine learning problem. In 2000, CoNLL-2000 introduced a shared task to tag many kinds of phrases besides noun phrases in English(Sang and Buchholz, 2000). Additionally, many machine learning approaches, such as Support Vector Machines (SVMs)(Vapnik, 1995), Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al.,</context>
<context position="7079" citStr="Ramshaw and Marcus, 1995" startWordPosition="1130" endWordPosition="1133"> use the CTB4 Corpus to refer to the extracted data set. Table 2 lists details on the CTB4 Corpus data used in this study. Training Test Num of Files 728 110 Num of Sentences 9,878 5,290 Num of Words 238,906 165,862 Num of Phrases 141,426 101,449 Table 2: Information of the CTB4 Corpus 3 Chinese Chunking 3.1 Models for Chinese Chunking In this paper, we applied four models, including SVMs, CRFs, TBL, and MBL, which have achieved good performance in other languages. We only describe these models briefly since full details are presented elsewhere(Kudo and Matsumoto, 2001; Sha and Pereira, 2003; Ramshaw and Marcus, 1995; Sang, 2002). 98 3.1.1 SVMs Support Vector Machines (SVMs) is a powerful supervised learning paradigm based on the Structured Risk Minimization principle from computational learning theory(Vapnik, 1995). Kudo and Matsumoto(Kudo and Matsumoto, 2000) applied SVMs to English chunking and achieved the best performance in the CoNLL00 shared task(Sang and Buchholz, 2000). They created 231 SVMs classifiers to predict the unique pairs of chunk tags.The final decision was given by their weighted voting. Then the label sequence was chosen using a dynamic programming algorithm. Tan et al. (Tan et al., 2</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance Ramshaw and Mitch Marcus. 1995. Text chunking using transformation-based learning. In David Yarovsky and Kenneth Church, editors, Proceedings of the Third Workshop on Very Large Corpora, pages 82–94, Somerset, New Jersey. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the conll-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL-2000 and LLL2000,</booktitle>
<pages>127--132</pages>
<location>Lisbin, Portugal.</location>
<contexts>
<context position="1598" citStr="Sang and Buchholz, 2000" startWordPosition="233" endWordPosition="236">how that the SVMs model outperforms the other models and that our proposed approaches can improve performance significantly. 1 Introduction Chunking identifies the non-recursive cores of various types of phrases in text, possibly as a precursor to full parsing or information extraction. Steven P. Abney was the first person to introduce chunks for parsing(Abney, 1991). Ramshaw and Marcus(Ramshaw and Marcus, 1995) first represented base noun phrase recognition as a machine learning problem. In 2000, CoNLL-2000 introduced a shared task to tag many kinds of phrases besides noun phrases in English(Sang and Buchholz, 2000). Additionally, many machine learning approaches, such as Support Vector Machines (SVMs)(Vapnik, 1995), Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk </context>
<context position="7447" citStr="Sang and Buchholz, 2000" startWordPosition="1184" endWordPosition="1187"> models, including SVMs, CRFs, TBL, and MBL, which have achieved good performance in other languages. We only describe these models briefly since full details are presented elsewhere(Kudo and Matsumoto, 2001; Sha and Pereira, 2003; Ramshaw and Marcus, 1995; Sang, 2002). 98 3.1.1 SVMs Support Vector Machines (SVMs) is a powerful supervised learning paradigm based on the Structured Risk Minimization principle from computational learning theory(Vapnik, 1995). Kudo and Matsumoto(Kudo and Matsumoto, 2000) applied SVMs to English chunking and achieved the best performance in the CoNLL00 shared task(Sang and Buchholz, 2000). They created 231 SVMs classifiers to predict the unique pairs of chunk tags.The final decision was given by their weighted voting. Then the label sequence was chosen using a dynamic programming algorithm. Tan et al. (Tan et al., 2004) applied SVMs to Chinese chunking. They used sigmoid functions to extract probabilities from SVMs outputs as the post-processing of classification. In this paper, we used Yamcha (V0.33)5 in our experiments. 3.1.2 CRFs Conditional Random Fields is a powerful sequence labeling model(Lafferty et al., 2001) that combine the advantages of both the generative model an</context>
<context position="13992" citStr="Sang and Buchholz, 2000" startWordPosition="2261" endWordPosition="2264">extend the tags with LOC for this problem: B-NP-LOC and I-NP-LOC. From the above cases study, we know the steps of Tag-Extension. Firstly, identifying a special problem of chunking. Secondly, extending the chunk tags via Equation (1). Finally, replacing the tags of related tokens with new chunk tags. After Tag-Extension, we use new added chunk tags to describe some special problems. 5 Voting Methods Kudo and Matsumoto(Kudo and Matsumoto, 2001) reported that they achieved higher accuracy by applying voting of systems that were trained using different data representations. Tjong Kim Sang et al.(Sang and Buchholz, 2000) reported similar results by combining different systems. In order to provide better results, we also apply the voting of basic systems, including SVMs, CRFs, MBL and TBL. Depending on the characteristics in the chunking task, we propose two new voting methods. In these two voting methods, we consider long distance information. In the weighted voting method, we can assign different weights to the results of the individual system(van Halteren et al., 1998). However, it requires a larger amount of computational capacity as the training data is divided and is repeatedly used to obtain the voting </context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the conll-2000 shared task: Chunking. In Proceedings of CoNLL-2000 and LLL2000, pages 127–132, Lisbin, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003.</booktitle>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Proceedings of CoNLL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Memory-based shallow parsing.</title>
<date>2002</date>
<journal>JMLR,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="7092" citStr="Sang, 2002" startWordPosition="1134" endWordPosition="1135">fer to the extracted data set. Table 2 lists details on the CTB4 Corpus data used in this study. Training Test Num of Files 728 110 Num of Sentences 9,878 5,290 Num of Words 238,906 165,862 Num of Phrases 141,426 101,449 Table 2: Information of the CTB4 Corpus 3 Chinese Chunking 3.1 Models for Chinese Chunking In this paper, we applied four models, including SVMs, CRFs, TBL, and MBL, which have achieved good performance in other languages. We only describe these models briefly since full details are presented elsewhere(Kudo and Matsumoto, 2001; Sha and Pereira, 2003; Ramshaw and Marcus, 1995; Sang, 2002). 98 3.1.1 SVMs Support Vector Machines (SVMs) is a powerful supervised learning paradigm based on the Structured Risk Minimization principle from computational learning theory(Vapnik, 1995). Kudo and Matsumoto(Kudo and Matsumoto, 2000) applied SVMs to English chunking and achieved the best performance in the CoNLL00 shared task(Sang and Buchholz, 2000). They created 231 SVMs classifiers to predict the unique pairs of chunk tags.The final decision was given by their weighted voting. Then the label sequence was chosen using a dynamic programming algorithm. Tan et al. (Tan et al., 2004) applied </context>
<context position="9608" citStr="Sang, 2002" startWordPosition="1520" endWordPosition="1521">.0)7 to implement the TBL model. 5Yamcha is available at http://chasen.org/ taku/software/yamcha/ 6MALLET is available at http://mallet.cs.umass.edu/index.php/Main Page 7fnTBL is available at http://nlp.cs.jhu.edu/ rflorian/fntbl/index.html 3.1.4 MBL Memory-based Learning (also called instance based learning) is a non-parametric inductive learning paradigm that stores training instances in a memory structure on which predictions of new instances are based(Walter et al., 1999). The similarity between the new instance X and example Y in memory is computed using a distance metric. Tjong Kim Sang(Sang, 2002) applied memorybased learning(MBL) to English chunking. MBL performs well for a variety of shallow parsing tasks, often yielding good results. In this paper, we used TiMBL8(Daelemans et al., 2004) to implement the MBL model. 3.2 Features The observations are based on features that are able to represent the difference between the two events. We utilize both lexical and Part-OfSpeech(POS) information as the features. We use the lexical and POS information within a fixed window. We also consider different combinations of them. The features are listed as follows: • WORD: uni-gram and bi-grams of w</context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>Erik F. Tjong Kim Sang. 2002. Memory-based shallow parsing. JMLR, 2(3):559–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL03.</booktitle>
<contexts>
<context position="7053" citStr="Sha and Pereira, 2003" startWordPosition="1126" endWordPosition="1129"> following sections, we use the CTB4 Corpus to refer to the extracted data set. Table 2 lists details on the CTB4 Corpus data used in this study. Training Test Num of Files 728 110 Num of Sentences 9,878 5,290 Num of Words 238,906 165,862 Num of Phrases 141,426 101,449 Table 2: Information of the CTB4 Corpus 3 Chinese Chunking 3.1 Models for Chinese Chunking In this paper, we applied four models, including SVMs, CRFs, TBL, and MBL, which have achieved good performance in other languages. We only describe these models briefly since full details are presented elsewhere(Kudo and Matsumoto, 2001; Sha and Pereira, 2003; Ramshaw and Marcus, 1995; Sang, 2002). 98 3.1.1 SVMs Support Vector Machines (SVMs) is a powerful supervised learning paradigm based on the Structured Risk Minimization principle from computational learning theory(Vapnik, 1995). Kudo and Matsumoto(Kudo and Matsumoto, 2000) applied SVMs to English chunking and achieved the best performance in the CoNLL00 shared task(Sang and Buchholz, 2000). They created 231 SVMs classifiers to predict the unique pairs of chunk tags.The final decision was given by their weighted voting. Then the label sequence was chosen using a dynamic programming algorithm.</context>
<context position="19363" citStr="Sha and Pereira, 2003" startWordPosition="3244" endWordPosition="3247">: F1 = 2 x P x R (4) R + P In this paper, we report the results with F1 score. 6.2 Experimental Results 6.2.1 POS vs. WORD+POS In this experiment, we compared the performance of different feature representations, inVotes[k] = 101 0.01 0.02 0.05 0.1 0.2 0.5 1 Size of Training data Figure 1: Results of different features cluding POS and WORD+ POS(See section 3.2), and set the window size as 2. We also investigated the effects of different sizes of training data. The SVMs and CRFs approaches were used in the experiments because they provided good performance in chunking(Kudo and Matsumoto, 2001)(Sha and Pereira, 2003). Figure 1 shows the experimental results, where xtics denotes the size of the training data, ”WP” refers to WORD+POS, ”P” refers to POS. We can see from the figure that WORD+POS yielded better performance than POS in the most cases. However, when the size of training data was small, the performance was similar. With WORD+POS, SVMs provided higher accuracy than CRFs in all training sizes. However, with POS, CRFs yielded better performance than SVMs in large scale training sizes. Furthermore, we found SVMs with WORD+POS provided 4.07% higher accuracy than with POS, while CRFs provided 2.73% hig</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of HLT-NAACL03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongmei Tan</author>
<author>Tianshun Yao</author>
<author>Qing Chen</author>
<author>Jingbo Zhu</author>
</authors>
<title>Chinese chunk identification using svms plus sigmoid.</title>
<date>2004</date>
<booktitle>In IJCNLP,</booktitle>
<pages>527--536</pages>
<contexts>
<context position="7683" citStr="Tan et al., 2004" startWordPosition="1223" endWordPosition="1226"> Marcus, 1995; Sang, 2002). 98 3.1.1 SVMs Support Vector Machines (SVMs) is a powerful supervised learning paradigm based on the Structured Risk Minimization principle from computational learning theory(Vapnik, 1995). Kudo and Matsumoto(Kudo and Matsumoto, 2000) applied SVMs to English chunking and achieved the best performance in the CoNLL00 shared task(Sang and Buchholz, 2000). They created 231 SVMs classifiers to predict the unique pairs of chunk tags.The final decision was given by their weighted voting. Then the label sequence was chosen using a dynamic programming algorithm. Tan et al. (Tan et al., 2004) applied SVMs to Chinese chunking. They used sigmoid functions to extract probabilities from SVMs outputs as the post-processing of classification. In this paper, we used Yamcha (V0.33)5 in our experiments. 3.1.2 CRFs Conditional Random Fields is a powerful sequence labeling model(Lafferty et al., 2001) that combine the advantages of both the generative model and the classification model. Sha and Pereira(Sha and Pereira, 2003) showed that stateof-the-art results can be achieved using CRFs in English chunking. CRFs allow us to utilize a large number of observation features as well as different </context>
</contexts>
<marker>Tan, Yao, Chen, Zhu, 2004</marker>
<rawString>Yongmei Tan, Tianshun Yao, Qing Chen, and Jingbo Zhu. 2004. Chinese chunk identification using svms plus sigmoid. In IJCNLP, pages 527–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongmei Tan</author>
<author>Tianshun Yao</author>
<author>Qing Chen</author>
<author>Jingbo Zhu</author>
</authors>
<title>Applying conditional random fields to chinese shallow parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of CICLing-2005,</booktitle>
<pages>167--176</pages>
<publisher>Springer.</publisher>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="2111" citStr="Tan et al., 2005" startWordPosition="309" endWordPosition="312">ced a shared task to tag many kinds of phrases besides noun phrases in English(Sang and Buchholz, 2000). Additionally, many machine learning approaches, such as Support Vector Machines (SVMs)(Vapnik, 1995), Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Therefore, comparing the performance of previous studies in Chinese chunking is very difficult. Furthermore, compared with the other languages, there are some special problems for Chinese chunking(Li et al., 2004). In this paper, we extracted the chunking corpus from UPENN Chinese Treebank-4(CTB4). We presented an empirical study of Chinese chunking on this corpus. First, we made an evaluation on the corpus to cla</context>
<context position="4026" citStr="Tan et al., 2005" startWordPosition="611" endWordPosition="614">finitions of Chinese chunks. Sec97 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 97–104, Sydney, July 2006. c�2006 Association for Computational Linguistics tion 3 simply introduces the models and features for Chinese chunking. Section 4 proposes a tagextension method. Section 5 proposes two new voting approaches. Section 6 explains the experimental results. Finally, in section 7 we draw the conclusions. 2 Definitions of Chinese Chunks We defined the Chinese chunks based on the CTB4 dataset1. Many researchers have extracted the chunks from different versions of CTB(Tan et al., 2005; Li et al., 2003b). However, these studies did not provide sufficient detail. We developed a tool2 to extract the corpus from CTB4 by modifying the tool Chunklink3. 2.1 Chunk Types Here we define 12 types of chunks4: ADJP, ADVP, CLP, DNP, DP, DVP, LCP, LST, NP, PP, QP, VP(Xue et al., 2000). Table 1 provides definitions of these chunks. Type Definition ADJP Adjective Phrase ADVP Adverbial Phrase CLP Classifier Phrase DNP DEG Phrase DP Determiner Phrase DVP DEV phrase LCP Localizer Phrase LST List Marker NP Noun Phrase PP Prepositional Phrase QP Quantifier Phrase VP Verb Phrase Table 1: Definit</context>
<context position="8377" citStr="Tan et al., 2005" startWordPosition="1335" endWordPosition="1338">babilities from SVMs outputs as the post-processing of classification. In this paper, we used Yamcha (V0.33)5 in our experiments. 3.1.2 CRFs Conditional Random Fields is a powerful sequence labeling model(Lafferty et al., 2001) that combine the advantages of both the generative model and the classification model. Sha and Pereira(Sha and Pereira, 2003) showed that stateof-the-art results can be achieved using CRFs in English chunking. CRFs allow us to utilize a large number of observation features as well as different state sequence based features and other features we want to add. Tan et al. (Tan et al., 2005) applied CRFs to Chinese chunking and their experimental results showed that the CRFs approach provided better performance than HMM. In this paper, we used MALLET (V0.3.2)6(McCallum, 2002) to implement the CRF model. 3.1.3 TBL Transformation based learning(TBL), first introduced by Eric Brill(Brill, 1995), is mainly based on the idea of successively transforming the data in order to correct the error. The transformation rules obtained are usually few , yet powerful. TBL was applied to Chinese chunking by Li et al.(Li et al., 2004) and TBL provided good performance on their corpus. In this pape</context>
</contexts>
<marker>Tan, Yao, Chen, Zhu, 2005</marker>
<rawString>Yongmei Tan, Tianshun Yao, Qing Chen, and Jingbo Zhu. 2005. Applying conditional random fields to chinese shallow parsing. In Proceedings of CICLing-2005, pages 167–176, Mexico City, Mexico. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
</authors>
<title>Improving data driven wordclass tagging by system combination.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>491--497</pages>
<marker>van Halteren, Zavrel, Daelemans, 1998</marker>
<rawString>Hans van Halteren, Jakub Zavrel, and Walter Daelemans. 1998. Improving data driven wordclass tagging by system combination. In COLING-ACL, pages 491–497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<contexts>
<context position="1700" citStr="Vapnik, 1995" startWordPosition="248" endWordPosition="249">nificantly. 1 Introduction Chunking identifies the non-recursive cores of various types of phrases in text, possibly as a precursor to full parsing or information extraction. Steven P. Abney was the first person to introduce chunks for parsing(Abney, 1991). Ramshaw and Marcus(Ramshaw and Marcus, 1995) first represented base noun phrase recognition as a machine learning problem. In 2000, CoNLL-2000 introduced a shared task to tag many kinds of phrases besides noun phrases in English(Sang and Buchholz, 2000). Additionally, many machine learning approaches, such as Support Vector Machines (SVMs)(Vapnik, 1995), Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Theref</context>
<context position="7282" citStr="Vapnik, 1995" startWordPosition="1161" endWordPosition="1162">2 Num of Phrases 141,426 101,449 Table 2: Information of the CTB4 Corpus 3 Chinese Chunking 3.1 Models for Chinese Chunking In this paper, we applied four models, including SVMs, CRFs, TBL, and MBL, which have achieved good performance in other languages. We only describe these models briefly since full details are presented elsewhere(Kudo and Matsumoto, 2001; Sha and Pereira, 2003; Ramshaw and Marcus, 1995; Sang, 2002). 98 3.1.1 SVMs Support Vector Machines (SVMs) is a powerful supervised learning paradigm based on the Structured Risk Minimization principle from computational learning theory(Vapnik, 1995). Kudo and Matsumoto(Kudo and Matsumoto, 2000) applied SVMs to English chunking and achieved the best performance in the CoNLL00 shared task(Sang and Buchholz, 2000). They created 231 SVMs classifiers to predict the unique pairs of chunk tags.The final decision was given by their weighted voting. Then the label sequence was chosen using a dynamic programming algorithm. Tan et al. (Tan et al., 2004) applied SVMs to Chinese chunking. They used sigmoid functions to extract probabilities from SVMs outputs as the post-processing of classification. In this paper, we used Yamcha (V0.33)5 in our exper</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daelemans Walter</author>
<author>Sabine Buchholz</author>
<author>Jorn Veenstra</author>
</authors>
<date>1999</date>
<note>Memory-based shallow parsing.</note>
<contexts>
<context position="9477" citStr="Walter et al., 1999" startWordPosition="1495" endWordPosition="1498">pplied to Chinese chunking by Li et al.(Li et al., 2004) and TBL provided good performance on their corpus. In this paper, we used fnTBL (V1.0)7 to implement the TBL model. 5Yamcha is available at http://chasen.org/ taku/software/yamcha/ 6MALLET is available at http://mallet.cs.umass.edu/index.php/Main Page 7fnTBL is available at http://nlp.cs.jhu.edu/ rflorian/fntbl/index.html 3.1.4 MBL Memory-based Learning (also called instance based learning) is a non-parametric inductive learning paradigm that stores training instances in a memory structure on which predictions of new instances are based(Walter et al., 1999). The similarity between the new instance X and example Y in memory is computed using a distance metric. Tjong Kim Sang(Sang, 2002) applied memorybased learning(MBL) to English chunking. MBL performs well for a variety of shallow parsing tasks, often yielding good results. In this paper, we used TiMBL8(Daelemans et al., 2004) to implement the MBL model. 3.2 Features The observations are based on features that are able to represent the difference between the two events. We utilize both lexical and Part-OfSpeech(POS) information as the features. We use the lexical and POS information within a fi</context>
</contexts>
<marker>Walter, Buchholz, Veenstra, 1999</marker>
<rawString>Daelemans Walter, Sabine Buchholz, and Jorn Veenstra. 1999. Memory-based shallow parsing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shih-Hung Wu</author>
<author>Cheng-Wei Shih</author>
<author>Chia-Wei Wu</author>
<author>Tzong-Han Tsai</author>
<author>Wen-Lian Hsu</author>
</authors>
<title>Applying maximum entropy to robust chinese shallow parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of ROCLING2005.</booktitle>
<contexts>
<context position="2128" citStr="Wu et al., 2005" startWordPosition="313" endWordPosition="316">to tag many kinds of phrases besides noun phrases in English(Sang and Buchholz, 2000). Additionally, many machine learning approaches, such as Support Vector Machines (SVMs)(Vapnik, 1995), Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Therefore, comparing the performance of previous studies in Chinese chunking is very difficult. Furthermore, compared with the other languages, there are some special problems for Chinese chunking(Li et al., 2004). In this paper, we extracted the chunking corpus from UPENN Chinese Treebank-4(CTB4). We presented an empirical study of Chinese chunking on this corpus. First, we made an evaluation on the corpus to clarify the performa</context>
</contexts>
<marker>Wu, Shih, Wu, Tsai, Hsu, 2005</marker>
<rawString>Shih-Hung Wu, Cheng-Wei Shih, Chia-Wei Wu, Tzong-Han Tsai, and Wen-Lian Hsu. 2005. Applying maximum entropy to robust chinese shallow parsing. In Proceedings of ROCLING2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Shizhe Huang</author>
<author>Anthony Kroch</author>
</authors>
<title>The bracketing guidelines for the penn chinese treebank.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="4317" citStr="Xue et al., 2000" startWordPosition="663" endWordPosition="666">method. Section 5 proposes two new voting approaches. Section 6 explains the experimental results. Finally, in section 7 we draw the conclusions. 2 Definitions of Chinese Chunks We defined the Chinese chunks based on the CTB4 dataset1. Many researchers have extracted the chunks from different versions of CTB(Tan et al., 2005; Li et al., 2003b). However, these studies did not provide sufficient detail. We developed a tool2 to extract the corpus from CTB4 by modifying the tool Chunklink3. 2.1 Chunk Types Here we define 12 types of chunks4: ADJP, ADVP, CLP, DNP, DP, DVP, LCP, LST, NP, PP, QP, VP(Xue et al., 2000). Table 1 provides definitions of these chunks. Type Definition ADJP Adjective Phrase ADVP Adverbial Phrase CLP Classifier Phrase DNP DEG Phrase DP Determiner Phrase DVP DEV phrase LCP Localizer Phrase LST List Marker NP Noun Phrase PP Prepositional Phrase QP Quantifier Phrase VP Verb Phrase Table 1: Definition of Chunks 2.2 Data Representation To represent the chunks clearly, we represent the data with an IOB-based model as the CoNLL00 shared task did, in which every word is to be tagged with a chunk type label extended with I (inside a chunk), O (outside a chunk), and B (inside a chunk, but </context>
</contexts>
<marker>Xue, Xia, Huang, Kroch, 2000</marker>
<rawString>Nianwen Xue, Fei Xia, Shizhe Huang, and Anthony Kroch. 2000. The bracketing guidelines for the penn chinese treebank. Technical report, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqi Zhang</author>
<author>Qiang Zhou</author>
</authors>
<title>Chinese basephrases chunking.</title>
<date>2002</date>
<booktitle>In Proceedings of The First SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="2292" citStr="Zhang and Zhou, 2002" startWordPosition="340" endWordPosition="343">Machines (SVMs)(Vapnik, 1995), Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Therefore, comparing the performance of previous studies in Chinese chunking is very difficult. Furthermore, compared with the other languages, there are some special problems for Chinese chunking(Li et al., 2004). In this paper, we extracted the chunking corpus from UPENN Chinese Treebank-4(CTB4). We presented an empirical study of Chinese chunking on this corpus. First, we made an evaluation on the corpus to clarify the performance of stateof-the-art models in Chinese chunking. Then we proposed two approaches in order to improve the performance of Chinese chunking. 1) We proposed an approa</context>
</contexts>
<marker>Zhang, Zhou, 2002</marker>
<rawString>Yuqi Zhang and Qiang Zhou. 2002. Chinese basephrases chunking. In Proceedings of The First SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiejun Zhao</author>
<author>Muyun Yang</author>
<author>Fang Liu</author>
<author>Jianmin Yao</author>
<author>Hao Yu</author>
</authors>
<title>Statistics based hybrid approach to chinese base phrase identification.</title>
<date>2000</date>
<booktitle>In Proceedings of Second Chinese Language Processing Workshop.</booktitle>
<contexts>
<context position="2148" citStr="Zhao et al., 2000" startWordPosition="317" endWordPosition="320"> of phrases besides noun phrases in English(Sang and Buchholz, 2000). Additionally, many machine learning approaches, such as Support Vector Machines (SVMs)(Vapnik, 1995), Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Therefore, comparing the performance of previous studies in Chinese chunking is very difficult. Furthermore, compared with the other languages, there are some special problems for Chinese chunking(Li et al., 2004). In this paper, we extracted the chunking corpus from UPENN Chinese Treebank-4(CTB4). We presented an empirical study of Chinese chunking on this corpus. First, we made an evaluation on the corpus to clarify the performance of stateof-the-a</context>
</contexts>
<marker>Zhao, Yang, Liu, Yao, Yu, 2000</marker>
<rawString>Tiejun Zhao, Muyun Yang, Fang Liu, Jianmin Yao, and Hao Yu. 2000. Statistics based hybrid approach to chinese base phrase identification. In Proceedings of Second Chinese Language Processing Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
<author>Jian Su</author>
<author>TongGuan Tey</author>
</authors>
<title>Hybrid text chunking.</title>
<date>2000</date>
<booktitle>Proceedings of the CoNLL00,</booktitle>
<pages>163--165</pages>
<editor>In Claire Cardie, Walter Daelemans, Claire N´edellec, and Erik Tjong Kim Sang, editors,</editor>
<location>Lisbon,</location>
<contexts>
<context position="1910" citStr="Zhou et al., 2000" startWordPosition="273" endWordPosition="276">person to introduce chunks for parsing(Abney, 1991). Ramshaw and Marcus(Ramshaw and Marcus, 1995) first represented base noun phrase recognition as a machine learning problem. In 2000, CoNLL-2000 introduced a shared task to tag many kinds of phrases besides noun phrases in English(Sang and Buchholz, 2000). Additionally, many machine learning approaches, such as Support Vector Machines (SVMs)(Vapnik, 1995), Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Therefore, comparing the performance of previous studies in Chinese chunking is very difficult. Furthermore, compared with the other languages, there are some special problems for Chinese chunking(Li et al., 2004). I</context>
</contexts>
<marker>Zhou, Su, Tey, 2000</marker>
<rawString>GuoDong Zhou, Jian Su, and TongGuan Tey. 2000. Hybrid text chunking. In Claire Cardie, Walter Daelemans, Claire N´edellec, and Erik Tjong Kim Sang, editors, Proceedings of the CoNLL00, Lisbon, 2000, pages 163–165. Association for Computational Linguistics, Somerset, New Jersey.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>