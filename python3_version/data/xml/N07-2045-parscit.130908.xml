<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002311">
<title confidence="0.995259">
Language Modeling for Determiner Selection
</title>
<author confidence="0.987888">
Jenine Turner and Eugene Charniak
</author>
<affiliation confidence="0.978714">
Department of Computer Science
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
</affiliation>
<address confidence="0.955296">
Providence, RI 02912
</address>
<email confidence="0.999653">
{jenine|ec}@cs.brown.edu
</email>
<sectionHeader confidence="0.995654" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999925285714286">
We present a method for automatic deter-
miner selection, based on an existing lan-
guage model. We train on the Penn Tree-
bank and also use additional data from the
North American News Text Corpus. Our
results are a significant improvement over
previous best.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999694619047619">
Determiner placement (choosing if a noun phrase
needs a determiner, and if so, which one) is a
non-trivial problem in several language processing
tasks. While context beyond that of the current sen-
tence can sometimes be necessary, native speakers
of languages with determiners can select determin-
ers quite well for most NPs. Native speakers of lan-
guages without determiners have a much more diffi-
cult time.
Automating determiner selection is helpful in sev-
eral applications. A determiner selection program
can aid in Machine Translation of determiner-free
languages (by adding determiners after the text has
been translated), correct English text written by non-
native speakers (Lee, 2004), and choose determiners
for text generation programs.
Early work on determiner selection focuses on
rule-based systems (Gawronska, 1990; Murata and
Nagao, 1993; Bond and Ogura, 1994; Heine, 1998).
Knight and Chander (1994) use decision trees to
choose between the and a/an, ignoring NPs with no
determiner, and achieve 78% accuracy on their Wall
Street Journal corpus. (Deciding between a and an
is a trivial postprocessing step.)
Minnen et al. (2000) use a memory-based learner
(Daelemans et al., 2000) to choose determiners of
base noun phrases. They choose between no deter-
miner (hencefore null), the, and a/an. They use syn-
tactic features (head of the NP, part-of-speech tag of
the head of the NP, functional tag of the head of the
NP, category of the constituent embedding the NP,
and functional tag of the constituent embedding the
NP), whether the head is a mass or count noun and
semantic classes of the head of the NP (Ikehara et
al., 1991). They report 83.58% accuracy.
In this paper, we use the Charniak language model
(Charniak, 2001) for determiner selection. Our ap-
proach significantly improves upon the work of Min-
nen et al. (2000). We also use additional automat-
ically parsed data from the North American News
Text Corpus (Graff, 1995), further improving our re-
sults.
</bodyText>
<sectionHeader confidence="0.936801" genericHeader="method">
2 The Immediate-Head Parsing Model
</sectionHeader>
<bodyText confidence="0.998475333333333">
The language model we use is described in (Char-
niak, 2001). It is based upon a parser that, for a
sentence s, tries to find the parse 7r defined as:
</bodyText>
<equation confidence="0.808817">
arg max7rp(7r  |s) = arg max7rp(7r, s) (1)
</equation>
<bodyText confidence="0.98809125">
The parser can be turned into a language model p(s)
describing the probability distribution over all pos-
sible strings s in the language, by considering all
parses 7r of s:
</bodyText>
<equation confidence="0.997407">
p(s) = � p(7r, s) (2)
</equation>
<footnote confidence="0.407364">
7r
</footnote>
<page confidence="0.929257">
177
</page>
<note confidence="0.464868">
Proceedings of NAACL HLT 2007, Companion Volume, pages 177–180,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.998433285714286">
Here p(7r, s) is zero if the yield of 7r =6 s.
The parsing model assigns a probability to a parse
7r by a top-down process. For each constituent c in
7r it first guesses the pre-terminal of c, t(c) (t for
“tag”), then the lexical head of c, h(c), and then the
expansion of c into further constituents e(c). Thus
the probability of a parse is given by the equation
</bodyText>
<equation confidence="0.99902075">
�p(7r) = p(t(c)  |l(c), H(c))
cE7r
· p(h(c)  |t(c),l(c), H(c))
· p(e(c)  |l(c), t(c), h(c), H(c))
</equation>
<bodyText confidence="0.999979230769231">
where l(c) is the label of c (e.g., whether it is a noun
phrase NP, verb phrase, etc.) and H(c) is the rel-
evant history of c — information outside c deemed
important in determining the probability in question.
H(c) approximately consists of the label, head, and
head-part-of-speech for the parent of c: m(c), i(c),
and u(c) respectively and also a secondary head
(e.g., in “Monday Night Football” Monday would
be conditioned on both the head of the noun-phrase
“Football” and the secondary head “Night”).
It is usually clear to which constituent we are re-
ferring and we omit the (c) in, e.g., h(c). In this no-
tation the above equation takes the following form:
</bodyText>
<equation confidence="0.974482666666667">
�p(7r) = p(t  |l, m, u, i) · p(h  |t,l, m, u, i)
cE7r
· p(e  |l, t, h, m, u). (3)
</equation>
<bodyText confidence="0.999544588235294">
Next we describe how we assign a probability to
the expansion e of a constituent. We break up a tra-
ditional probabilistic context-free grammar (PCFG)
rule into a left-hand side with a label l(c) drawn
from the non-terminal symbols of our grammar, and
a right-hand side that is a sequence of one or more
such symbols. For each expansion we distinguish
one of the right-hand side labels as the “middle” or
“head” symbol M(c). M(c) is the constituent from
which the head lexical item h is obtained according
to deterministic rules that pick the head of a con-
stituent from among the heads of its children. To the
left of M is a sequence of one or more left labels
LZ(c) including the special termination symbol △,
which indicates that there are no more symbols to
the left. We do the same for the labels to the right,
RZ(c). Thus, an expansion e(c) looks like:
</bodyText>
<equation confidence="0.779179">
l → △L,,t...L1MR1...R,,,△. (4)
</equation>
<bodyText confidence="0.999619818181818">
The expansion is generated first by guessing M,
then in order L1 through L,,t+1 (= △), and then, R1
through R.+1.
Let us turn to how this works in the case of de-
terminer recovery. Consider a noun-phrase, which,
missing a possible determiner, is simply “FBI.” The
language model is interested in the probability of the
strings “the FBI,” “a/an FBI” and “FBI.” The ver-
sion with the highest probability will dictate the de-
terminer, or lack thereof. So, consider (most of) the
probability calculation for the answer “the FBI:”
</bodyText>
<equation confidence="0.977577">
p(NNP  |H) · p(FBI  |NNP, H)
· p(det  |FBI, NNP, H)
· p(△  |det, FBI, NNP, H)
· p(the  |det, FBI, NNP, H) (5)
</equation>
<bodyText confidence="0.999649">
Of these, the first two terms, the probability that
the head will be an NNP (a singular proper noun)
and the probability that it will be “FBI”, are shared
by all three competitors, null, the, and a/an. These
terms can therefore be ignored when we only wish to
identify the competitor with the highest probability.
The next two probabilities state that the noun-phrase
contains a determiner to the left of “FBI” and that
the determiner is the last constituent of the left-hand
side. The last of the probabilities states that the de-
terminer in question is the. Ignoring the first two
probabilities, the critical probabilities for “the FBI”
are:
</bodyText>
<equation confidence="0.850107666666667">
p(det  |FBI, NNP, H)
· p(△  |det, FBI, NNP, H)
· p(the  |det, FBI, NNP, H) (6)
</equation>
<bodyText confidence="0.915888">
Conversely, to evaluate the probability of the noun-
phrase “FBI” — i.e., no determiner, we evaluate:
</bodyText>
<equation confidence="0.924729">
p(△  |FBI, NNP, H) (7)
</equation>
<bodyText confidence="0.994081333333333">
We ask the probability of the NP stopping immedi-
ately to the left of “FBI.” For “a/an FBI” we evalu-
ate:
</bodyText>
<table confidence="0.742769846153846">
p(det  |FBI, NNP, H)
· p(△  |det, FBI, NNP, H) (8)
· (p(a  |det, FBI, NNP, H) +
p(an  |det, FBI, NNP, H))
178
Test Data Method Accuracy
leave-one-out Minnen et al. 83.58%
Language Model (LM) 86.74%
tenfold on development LM 84.72%
LM trained on WSJ + 3 million words of NANC 85.83%
LM trained on WSJ + 10 million words of NANC 86.36%
LM trained on WSJ + 20 million words of NANC 86.64%
tenfold on test LM trained on WSJ + 20 million words of NANC 86.63%
</table>
<tableCaption confidence="0.999819">
Table 1: Results of classification
</tableCaption>
<bodyText confidence="0.999866857142857">
This equation is very similar to Equation 6 (the
equation for “the FBI”, except the term for the prob-
ability of the is replaced by the sum of the probabil-
ities for a and an.
To choose between null, the, or a/an, the language
model in effect constructs Equations 6, 7 and 8 and
we pick the one that has the highest probability.
</bodyText>
<subsectionHeader confidence="0.972071">
2.1 Training the model
</subsectionHeader>
<bodyText confidence="0.999984">
As with (Minnen et al., 2000), we train the lan-
guage model on the Penn Treebank (Marcus et al.,
1993). As far as we know, language modeling
always improves with additional training data, so
we add data from the North American News Text
Corpus (NANC) (Graff, 1995) automatically parsed
with the Charniak parser (McClosky et al., 2006) to
train our language model on up to 20 million addi-
tional words.
</bodyText>
<sectionHeader confidence="0.999671" genericHeader="evaluation">
3 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999975153846154">
The best results of Minnen et al. (2000) are using
leave-one-out cross-validation. We also test our lan-
guage model using leave-one-out cross-validation
on the Penn Treebank (Marcus et al., 1993) (WSJ),
giving us 86.74% accuracy (see Table 1).
Leave-one-out cross-validation does not make
sense in this case. When choosing determiners, we
can train a language model on similar data, but not
on other NPs in the article. Therefore, for the rest
of our tests, we use tenfold cross-validation. The
difference between leave-one-out and tenfold cross-
validation is due to the co-occurrence of NPs within
an article. Church (2000) shows that a word appears
with much higher probability when seen elsewhere
in an article. Thus, a rare NP might be unseen in
tenfold cross-validation, but seen in leave-one-out.
For each of our sets in tenfold cross validation,
we use 80% of the Penn Treebank for training, 10%
for development, and 10% for testing. The divisions
occur at article boundaries. On our development set
with tenfold cross-validation, we get 84.72% accu-
racy using the language model (Table 1).
As expected, we achieve significant improvement
when adding NANC data over training on data from
the Penn Treebank alone (Table 1). With 20 mil-
lion additional words, we seem to be approaching
an upper bound on the language model features. We
obtain improvement despite the fact that the parses
were automatic, but there may have been errors in
determiner selection due to parsing error.
Table 2 gives “error” examples. Some errors are
wrong (either grammatically or yielding a signifi-
cantly different interpretation), but some “incorrect”
answers are reasonable possibilities. Furthermore,
even all the text of the article is not enough for clas-
sification at times. In particular note Example 5,
where unless you know whether IBM was the world
leader or simply one of the world leaders at the time
of the article, no additional context would help.
</bodyText>
<sectionHeader confidence="0.992597" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.998956545454545">
With the Charniak (Charniak, 2001) language
model, our results exceed those of the previous best
(Minnen et al., 2000) on the determiner selection
task. This shows the benefits of the language model
features in determining the most grammatical deter-
miner to use in a noun phrase. Such a language
model looks at much of the structure in individual
sentences, but there may be additional features that
could improve performance. There is a high rate of
ambiguity for many of the misclassified sentences.
The success of using a state-of-the-art language
</bodyText>
<page confidence="0.990178">
179
</page>
<table confidence="0.334891615384615">
Guess Correct Sentence
the null (1) The computers were crude by today’s standards.
null the (2) In addition, the Apple II was an affordable $1,298.
(3) Highway officials insist the ornamental railings on older bridges aren’t strong enough
to prevent vehicles from crashing through.
a/an the (4) The new carrier can tote as many as four cups at once.
(5) IBM, the world leader in computers, didn’t offer its first PC
until August 1981 as many other companies entered the market.
the a/an (6) In addition, the Apple II was an affordable $1,298.
(7) “The primary purpose of a railing is to contain a vehicle and not to provide
a scenic view,” says Jack White, a planner with the Indiana Highway Department.
a/an null (8) Crude as they were, these early PCs triggered explosive product development in
desktop models for the home and office.
</table>
<tableCaption confidence="0.975822">
Table 2: Examples of “errors”
</tableCaption>
<bodyText confidence="0.99927375">
model in determiner selection also suggests that one
would be helpful in making other decisions in the
surface realization stage of text generation. This is
an avenue worth exploring.
</bodyText>
<sectionHeader confidence="0.994823" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9770245">
This work was supported by NSF PIRE grant OISE-0530118.
We would also like to thank the BLLIP team for their comments.
</bodyText>
<sectionHeader confidence="0.998543" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999685145454545">
Francis Bond and Kentaro Ogura. 1994. Countability
and number in Japanese-to-English machine transla-
tion. In 15th International Conference on Computa-
tional Linguistics, pages 32–38.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting ofthe Association for Computational Linguis-
tics. The Association for Computational Linguistics.
Kenneth Church. 2000. Empirical estimates of adap-
tation: The chance of Two Noriegas is closer to p/2
than p2. In Proceedings of COLING-2000.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2000. TiMBL: Tilburg memory
based learner, version 3.0, reference guide. ILK Tech-
nical Report ILK-0001, ILK, Tilburg University, The
Netherlands.
Barbara Gawronska. 1990. Translation great problem.
In Proceedings of the 13th International Conference
on Computational Linguistics.
David Graff. 1995. North American News Text Corpus.
Linguistic Data Consortium. LDC95T21.
Julia E. Heine. 1998. Definiteness predictions for
Japanese noun phrases. In Proceedings of the 36th
Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference
on Computational Linguistics, pages 519–525.
Satoru Ikehara, Satoship Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-
editing - effects of new methods in ALT-J/E. In Third
Machine Translation Summit, pages 101–106.
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In Proceedings of the
Twelfth National Conference on Artificial Intelligence,
pages 779–784.
John Lee. 2004. Automatic article restoration. In Pro-
ceedings of the 2004 NAACL Conference Student Re-
search Workshop, pages 195–200.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings ofHLT-NAACL 2006.
Guido Minnen, Francis Bond, and Ann Copestake. 2000.
Memory-based learning for article generation. In Pro-
ceedings of the Fourth Conference on Computational
Natural Language Learning and of the Second Learn-
ing Language in Logic Workshop, pages 43–48.
Masaki Murata and Makoto Nagao. 1993. Determina-
tion of referential property and number of nouns in
Japanese sentences for machine translation into En-
glish. In Fifth International Conference on Theoret-
ical and Methodological Issues in Machine Transla-
tion, pages 218–225.
</reference>
<page confidence="0.997761">
180
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.818933">
<title confidence="0.999611">Language Modeling for Determiner Selection</title>
<author confidence="0.997601">Jenine Turner</author>
<author confidence="0.997601">Eugene</author>
<affiliation confidence="0.963782">Department of Computer Brown Laboratory for Linguistic Information Processing Brown</affiliation>
<address confidence="0.938405">Providence, RI</address>
<abstract confidence="0.993647375">We present a method for automatic determiner selection, based on an existing language model. We train on the Penn Treebank and also use additional data from the North American News Text Corpus. Our results are a significant improvement over previous best.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Kentaro Ogura</author>
</authors>
<title>Countability and number in Japanese-to-English machine translation.</title>
<date>1994</date>
<booktitle>In 15th International Conference on Computational Linguistics,</booktitle>
<pages>32--38</pages>
<contexts>
<context position="1384" citStr="Bond and Ogura, 1994" startWordPosition="202" endWordPosition="205">languages with determiners can select determiners quite well for most NPs. Native speakers of languages without determiners have a much more difficult time. Automating determiner selection is helpful in several applications. A determiner selection program can aid in Machine Translation of determiner-free languages (by adding determiners after the text has been translated), correct English text written by nonnative speakers (Lee, 2004), and choose determiners for text generation programs. Early work on determiner selection focuses on rule-based systems (Gawronska, 1990; Murata and Nagao, 1993; Bond and Ogura, 1994; Heine, 1998). Knight and Chander (1994) use decision trees to choose between the and a/an, ignoring NPs with no determiner, and achieve 78% accuracy on their Wall Street Journal corpus. (Deciding between a and an is a trivial postprocessing step.) Minnen et al. (2000) use a memory-based learner (Daelemans et al., 2000) to choose determiners of base noun phrases. They choose between no determiner (hencefore null), the, and a/an. They use syntactic features (head of the NP, part-of-speech tag of the head of the NP, functional tag of the head of the NP, category of the constituent embedding the</context>
</contexts>
<marker>Bond, Ogura, 1994</marker>
<rawString>Francis Bond and Kentaro Ogura. 1994. Countability and number in Japanese-to-English machine translation. In 15th International Conference on Computational Linguistics, pages 32–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting ofthe Association for Computational Linguistics. The Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2249" citStr="Charniak, 2001" startWordPosition="352" endWordPosition="353">Minnen et al. (2000) use a memory-based learner (Daelemans et al., 2000) to choose determiners of base noun phrases. They choose between no determiner (hencefore null), the, and a/an. They use syntactic features (head of the NP, part-of-speech tag of the head of the NP, functional tag of the head of the NP, category of the constituent embedding the NP, and functional tag of the constituent embedding the NP), whether the head is a mass or count noun and semantic classes of the head of the NP (Ikehara et al., 1991). They report 83.58% accuracy. In this paper, we use the Charniak language model (Charniak, 2001) for determiner selection. Our approach significantly improves upon the work of Minnen et al. (2000). We also use additional automatically parsed data from the North American News Text Corpus (Graff, 1995), further improving our results. 2 The Immediate-Head Parsing Model The language model we use is described in (Charniak, 2001). It is based upon a parser that, for a sentence s, tries to find the parse 7r defined as: arg max7rp(7r |s) = arg max7rp(7r, s) (1) The parser can be turned into a language model p(s) describing the probability distribution over all possible strings s in the language,</context>
<context position="9981" citStr="Charniak, 2001" startWordPosition="1714" endWordPosition="1715">e automatic, but there may have been errors in determiner selection due to parsing error. Table 2 gives “error” examples. Some errors are wrong (either grammatically or yielding a significantly different interpretation), but some “incorrect” answers are reasonable possibilities. Furthermore, even all the text of the article is not enough for classification at times. In particular note Example 5, where unless you know whether IBM was the world leader or simply one of the world leaders at the time of the article, no additional context would help. 4 Conclusions and Future Work With the Charniak (Charniak, 2001) language model, our results exceed those of the previous best (Minnen et al., 2000) on the determiner selection task. This shows the benefits of the language model features in determining the most grammatical determiner to use in a noun phrase. Such a language model looks at much of the structure in individual sentences, but there may be additional features that could improve performance. There is a high rate of ambiguity for many of the misclassified sentences. The success of using a state-of-the-art language 179 Guess Correct Sentence the null (1) The computers were crude by today’s standar</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proceedings of the 39th Annual Meeting ofthe Association for Computational Linguistics. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>Empirical estimates of adaptation: The chance of Two Noriegas is closer to p/2 than p2.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING-2000.</booktitle>
<contexts>
<context position="8596" citStr="Church (2000)" startWordPosition="1488" endWordPosition="1489">results of Minnen et al. (2000) are using leave-one-out cross-validation. We also test our language model using leave-one-out cross-validation on the Penn Treebank (Marcus et al., 1993) (WSJ), giving us 86.74% accuracy (see Table 1). Leave-one-out cross-validation does not make sense in this case. When choosing determiners, we can train a language model on similar data, but not on other NPs in the article. Therefore, for the rest of our tests, we use tenfold cross-validation. The difference between leave-one-out and tenfold crossvalidation is due to the co-occurrence of NPs within an article. Church (2000) shows that a word appears with much higher probability when seen elsewhere in an article. Thus, a rare NP might be unseen in tenfold cross-validation, but seen in leave-one-out. For each of our sets in tenfold cross validation, we use 80% of the Penn Treebank for training, 10% for development, and 10% for testing. The divisions occur at article boundaries. On our development set with tenfold cross-validation, we get 84.72% accuracy using the language model (Table 1). As expected, we achieve significant improvement when adding NANC data over training on data from the Penn Treebank alone (Table</context>
</contexts>
<marker>Church, 2000</marker>
<rawString>Kenneth Church. 2000. Empirical estimates of adaptation: The chance of Two Noriegas is closer to p/2 than p2. In Proceedings of COLING-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>TiMBL: Tilburg memory based learner, version 3.0, reference guide.</title>
<date>2000</date>
<tech>ILK Technical Report ILK-0001,</tech>
<institution>ILK, Tilburg University, The Netherlands.</institution>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2000</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2000. TiMBL: Tilburg memory based learner, version 3.0, reference guide. ILK Technical Report ILK-0001, ILK, Tilburg University, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Gawronska</author>
</authors>
<title>Translation great problem.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="1338" citStr="Gawronska, 1990" startWordPosition="196" endWordPosition="197">metimes be necessary, native speakers of languages with determiners can select determiners quite well for most NPs. Native speakers of languages without determiners have a much more difficult time. Automating determiner selection is helpful in several applications. A determiner selection program can aid in Machine Translation of determiner-free languages (by adding determiners after the text has been translated), correct English text written by nonnative speakers (Lee, 2004), and choose determiners for text generation programs. Early work on determiner selection focuses on rule-based systems (Gawronska, 1990; Murata and Nagao, 1993; Bond and Ogura, 1994; Heine, 1998). Knight and Chander (1994) use decision trees to choose between the and a/an, ignoring NPs with no determiner, and achieve 78% accuracy on their Wall Street Journal corpus. (Deciding between a and an is a trivial postprocessing step.) Minnen et al. (2000) use a memory-based learner (Daelemans et al., 2000) to choose determiners of base noun phrases. They choose between no determiner (hencefore null), the, and a/an. They use syntactic features (head of the NP, part-of-speech tag of the head of the NP, functional tag of the head of the</context>
</contexts>
<marker>Gawronska, 1990</marker>
<rawString>Barbara Gawronska. 1990. Translation great problem. In Proceedings of the 13th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>North American News Text Corpus. Linguistic Data Consortium.</title>
<date>1995</date>
<tech>LDC95T21.</tech>
<contexts>
<context position="2454" citStr="Graff, 1995" startWordPosition="386" endWordPosition="387">res (head of the NP, part-of-speech tag of the head of the NP, functional tag of the head of the NP, category of the constituent embedding the NP, and functional tag of the constituent embedding the NP), whether the head is a mass or count noun and semantic classes of the head of the NP (Ikehara et al., 1991). They report 83.58% accuracy. In this paper, we use the Charniak language model (Charniak, 2001) for determiner selection. Our approach significantly improves upon the work of Minnen et al. (2000). We also use additional automatically parsed data from the North American News Text Corpus (Graff, 1995), further improving our results. 2 The Immediate-Head Parsing Model The language model we use is described in (Charniak, 2001). It is based upon a parser that, for a sentence s, tries to find the parse 7r defined as: arg max7rp(7r |s) = arg max7rp(7r, s) (1) The parser can be turned into a language model p(s) describing the probability distribution over all possible strings s in the language, by considering all parses 7r of s: p(s) = � p(7r, s) (2) 7r 177 Proceedings of NAACL HLT 2007, Companion Volume, pages 177–180, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics </context>
<context position="7812" citStr="Graff, 1995" startWordPosition="1363" endWordPosition="1364">his equation is very similar to Equation 6 (the equation for “the FBI”, except the term for the probability of the is replaced by the sum of the probabilities for a and an. To choose between null, the, or a/an, the language model in effect constructs Equations 6, 7 and 8 and we pick the one that has the highest probability. 2.1 Training the model As with (Minnen et al., 2000), we train the language model on the Penn Treebank (Marcus et al., 1993). As far as we know, language modeling always improves with additional training data, so we add data from the North American News Text Corpus (NANC) (Graff, 1995) automatically parsed with the Charniak parser (McClosky et al., 2006) to train our language model on up to 20 million additional words. 3 Results and Discussion The best results of Minnen et al. (2000) are using leave-one-out cross-validation. We also test our language model using leave-one-out cross-validation on the Penn Treebank (Marcus et al., 1993) (WSJ), giving us 86.74% accuracy (see Table 1). Leave-one-out cross-validation does not make sense in this case. When choosing determiners, we can train a language model on similar data, but not on other NPs in the article. Therefore, for the </context>
</contexts>
<marker>Graff, 1995</marker>
<rawString>David Graff. 1995. North American News Text Corpus. Linguistic Data Consortium. LDC95T21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia E Heine</author>
</authors>
<title>Definiteness predictions for Japanese noun phrases.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>519--525</pages>
<contexts>
<context position="1398" citStr="Heine, 1998" startWordPosition="206" endWordPosition="207">ners can select determiners quite well for most NPs. Native speakers of languages without determiners have a much more difficult time. Automating determiner selection is helpful in several applications. A determiner selection program can aid in Machine Translation of determiner-free languages (by adding determiners after the text has been translated), correct English text written by nonnative speakers (Lee, 2004), and choose determiners for text generation programs. Early work on determiner selection focuses on rule-based systems (Gawronska, 1990; Murata and Nagao, 1993; Bond and Ogura, 1994; Heine, 1998). Knight and Chander (1994) use decision trees to choose between the and a/an, ignoring NPs with no determiner, and achieve 78% accuracy on their Wall Street Journal corpus. (Deciding between a and an is a trivial postprocessing step.) Minnen et al. (2000) use a memory-based learner (Daelemans et al., 2000) to choose determiners of base noun phrases. They choose between no determiner (hencefore null), the, and a/an. They use syntactic features (head of the NP, part-of-speech tag of the head of the NP, functional tag of the head of the NP, category of the constituent embedding the NP, and funct</context>
</contexts>
<marker>Heine, 1998</marker>
<rawString>Julia E. Heine. 1998. Definiteness predictions for Japanese noun phrases. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 519–525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Satoship Shirai</author>
<author>Akio Yokoo</author>
<author>Hiromi Nakaiwa</author>
</authors>
<title>Toward an MT system without preediting - effects of new methods in ALT-J/E.</title>
<date>1991</date>
<booktitle>In Third Machine Translation Summit,</booktitle>
<pages>101--106</pages>
<contexts>
<context position="2152" citStr="Ikehara et al., 1991" startWordPosition="335" endWordPosition="338">acy on their Wall Street Journal corpus. (Deciding between a and an is a trivial postprocessing step.) Minnen et al. (2000) use a memory-based learner (Daelemans et al., 2000) to choose determiners of base noun phrases. They choose between no determiner (hencefore null), the, and a/an. They use syntactic features (head of the NP, part-of-speech tag of the head of the NP, functional tag of the head of the NP, category of the constituent embedding the NP, and functional tag of the constituent embedding the NP), whether the head is a mass or count noun and semantic classes of the head of the NP (Ikehara et al., 1991). They report 83.58% accuracy. In this paper, we use the Charniak language model (Charniak, 2001) for determiner selection. Our approach significantly improves upon the work of Minnen et al. (2000). We also use additional automatically parsed data from the North American News Text Corpus (Graff, 1995), further improving our results. 2 The Immediate-Head Parsing Model The language model we use is described in (Charniak, 2001). It is based upon a parser that, for a sentence s, tries to find the parse 7r defined as: arg max7rp(7r |s) = arg max7rp(7r, s) (1) The parser can be turned into a languag</context>
</contexts>
<marker>Ikehara, Shirai, Yokoo, Nakaiwa, 1991</marker>
<rawString>Satoru Ikehara, Satoship Shirai, Akio Yokoo, and Hiromi Nakaiwa. 1991. Toward an MT system without preediting - effects of new methods in ALT-J/E. In Third Machine Translation Summit, pages 101–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Ishwar Chander</author>
</authors>
<title>Automated postediting of documents.</title>
<date>1994</date>
<booktitle>In Proceedings of the Twelfth National Conference on Artificial Intelligence,</booktitle>
<pages>779--784</pages>
<contexts>
<context position="1425" citStr="Knight and Chander (1994)" startWordPosition="208" endWordPosition="211">t determiners quite well for most NPs. Native speakers of languages without determiners have a much more difficult time. Automating determiner selection is helpful in several applications. A determiner selection program can aid in Machine Translation of determiner-free languages (by adding determiners after the text has been translated), correct English text written by nonnative speakers (Lee, 2004), and choose determiners for text generation programs. Early work on determiner selection focuses on rule-based systems (Gawronska, 1990; Murata and Nagao, 1993; Bond and Ogura, 1994; Heine, 1998). Knight and Chander (1994) use decision trees to choose between the and a/an, ignoring NPs with no determiner, and achieve 78% accuracy on their Wall Street Journal corpus. (Deciding between a and an is a trivial postprocessing step.) Minnen et al. (2000) use a memory-based learner (Daelemans et al., 2000) to choose determiners of base noun phrases. They choose between no determiner (hencefore null), the, and a/an. They use syntactic features (head of the NP, part-of-speech tag of the head of the NP, functional tag of the head of the NP, category of the constituent embedding the NP, and functional tag of the constituen</context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>Kevin Knight and Ishwar Chander. 1994. Automated postediting of documents. In Proceedings of the Twelfth National Conference on Artificial Intelligence, pages 779–784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
</authors>
<title>Automatic article restoration.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 NAACL Conference Student Research Workshop,</booktitle>
<pages>195--200</pages>
<contexts>
<context position="1202" citStr="Lee, 2004" startWordPosition="178" endWordPosition="179"> which one) is a non-trivial problem in several language processing tasks. While context beyond that of the current sentence can sometimes be necessary, native speakers of languages with determiners can select determiners quite well for most NPs. Native speakers of languages without determiners have a much more difficult time. Automating determiner selection is helpful in several applications. A determiner selection program can aid in Machine Translation of determiner-free languages (by adding determiners after the text has been translated), correct English text written by nonnative speakers (Lee, 2004), and choose determiners for text generation programs. Early work on determiner selection focuses on rule-based systems (Gawronska, 1990; Murata and Nagao, 1993; Bond and Ogura, 1994; Heine, 1998). Knight and Chander (1994) use decision trees to choose between the and a/an, ignoring NPs with no determiner, and achieve 78% accuracy on their Wall Street Journal corpus. (Deciding between a and an is a trivial postprocessing step.) Minnen et al. (2000) use a memory-based learner (Daelemans et al., 2000) to choose determiners of base noun phrases. They choose between no determiner (hencefore null),</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>John Lee. 2004. Automatic article restoration. In Proceedings of the 2004 NAACL Conference Student Research Workshop, pages 195–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="7650" citStr="Marcus et al., 1993" startWordPosition="1334" endWordPosition="1337">f NANC 86.36% LM trained on WSJ + 20 million words of NANC 86.64% tenfold on test LM trained on WSJ + 20 million words of NANC 86.63% Table 1: Results of classification This equation is very similar to Equation 6 (the equation for “the FBI”, except the term for the probability of the is replaced by the sum of the probabilities for a and an. To choose between null, the, or a/an, the language model in effect constructs Equations 6, 7 and 8 and we pick the one that has the highest probability. 2.1 Training the model As with (Minnen et al., 2000), we train the language model on the Penn Treebank (Marcus et al., 1993). As far as we know, language modeling always improves with additional training data, so we add data from the North American News Text Corpus (NANC) (Graff, 1995) automatically parsed with the Charniak parser (McClosky et al., 2006) to train our language model on up to 20 million additional words. 3 Results and Discussion The best results of Minnen et al. (2000) are using leave-one-out cross-validation. We also test our language model using leave-one-out cross-validation on the Penn Treebank (Marcus et al., 1993) (WSJ), giving us 86.74% accuracy (see Table 1). Leave-one-out cross-validation do</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Michell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings ofHLT-NAACL</booktitle>
<contexts>
<context position="7882" citStr="McClosky et al., 2006" startWordPosition="1371" endWordPosition="1374"> “the FBI”, except the term for the probability of the is replaced by the sum of the probabilities for a and an. To choose between null, the, or a/an, the language model in effect constructs Equations 6, 7 and 8 and we pick the one that has the highest probability. 2.1 Training the model As with (Minnen et al., 2000), we train the language model on the Penn Treebank (Marcus et al., 1993). As far as we know, language modeling always improves with additional training data, so we add data from the North American News Text Corpus (NANC) (Graff, 1995) automatically parsed with the Charniak parser (McClosky et al., 2006) to train our language model on up to 20 million additional words. 3 Results and Discussion The best results of Minnen et al. (2000) are using leave-one-out cross-validation. We also test our language model using leave-one-out cross-validation on the Penn Treebank (Marcus et al., 1993) (WSJ), giving us 86.74% accuracy (see Table 1). Leave-one-out cross-validation does not make sense in this case. When choosing determiners, we can train a language model on similar data, but not on other NPs in the article. Therefore, for the rest of our tests, we use tenfold cross-validation. The difference bet</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings ofHLT-NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>Francis Bond</author>
<author>Ann Copestake</author>
</authors>
<title>Memory-based learning for article generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fourth Conference on Computational Natural Language Learning and of the Second Learning Language in Logic Workshop,</booktitle>
<pages>43--48</pages>
<contexts>
<context position="1654" citStr="Minnen et al. (2000)" startWordPosition="246" endWordPosition="249">hine Translation of determiner-free languages (by adding determiners after the text has been translated), correct English text written by nonnative speakers (Lee, 2004), and choose determiners for text generation programs. Early work on determiner selection focuses on rule-based systems (Gawronska, 1990; Murata and Nagao, 1993; Bond and Ogura, 1994; Heine, 1998). Knight and Chander (1994) use decision trees to choose between the and a/an, ignoring NPs with no determiner, and achieve 78% accuracy on their Wall Street Journal corpus. (Deciding between a and an is a trivial postprocessing step.) Minnen et al. (2000) use a memory-based learner (Daelemans et al., 2000) to choose determiners of base noun phrases. They choose between no determiner (hencefore null), the, and a/an. They use syntactic features (head of the NP, part-of-speech tag of the head of the NP, functional tag of the head of the NP, category of the constituent embedding the NP, and functional tag of the constituent embedding the NP), whether the head is a mass or count noun and semantic classes of the head of the NP (Ikehara et al., 1991). They report 83.58% accuracy. In this paper, we use the Charniak language model (Charniak, 2001) for </context>
<context position="7578" citStr="Minnen et al., 2000" startWordPosition="1320" endWordPosition="1323"> + 3 million words of NANC 85.83% LM trained on WSJ + 10 million words of NANC 86.36% LM trained on WSJ + 20 million words of NANC 86.64% tenfold on test LM trained on WSJ + 20 million words of NANC 86.63% Table 1: Results of classification This equation is very similar to Equation 6 (the equation for “the FBI”, except the term for the probability of the is replaced by the sum of the probabilities for a and an. To choose between null, the, or a/an, the language model in effect constructs Equations 6, 7 and 8 and we pick the one that has the highest probability. 2.1 Training the model As with (Minnen et al., 2000), we train the language model on the Penn Treebank (Marcus et al., 1993). As far as we know, language modeling always improves with additional training data, so we add data from the North American News Text Corpus (NANC) (Graff, 1995) automatically parsed with the Charniak parser (McClosky et al., 2006) to train our language model on up to 20 million additional words. 3 Results and Discussion The best results of Minnen et al. (2000) are using leave-one-out cross-validation. We also test our language model using leave-one-out cross-validation on the Penn Treebank (Marcus et al., 1993) (WSJ), gi</context>
<context position="10065" citStr="Minnen et al., 2000" startWordPosition="1726" endWordPosition="1729">ing error. Table 2 gives “error” examples. Some errors are wrong (either grammatically or yielding a significantly different interpretation), but some “incorrect” answers are reasonable possibilities. Furthermore, even all the text of the article is not enough for classification at times. In particular note Example 5, where unless you know whether IBM was the world leader or simply one of the world leaders at the time of the article, no additional context would help. 4 Conclusions and Future Work With the Charniak (Charniak, 2001) language model, our results exceed those of the previous best (Minnen et al., 2000) on the determiner selection task. This shows the benefits of the language model features in determining the most grammatical determiner to use in a noun phrase. Such a language model looks at much of the structure in individual sentences, but there may be additional features that could improve performance. There is a high rate of ambiguity for many of the misclassified sentences. The success of using a state-of-the-art language 179 Guess Correct Sentence the null (1) The computers were crude by today’s standards. null the (2) In addition, the Apple II was an affordable $1,298. (3) Highway off</context>
</contexts>
<marker>Minnen, Bond, Copestake, 2000</marker>
<rawString>Guido Minnen, Francis Bond, and Ann Copestake. 2000. Memory-based learning for article generation. In Proceedings of the Fourth Conference on Computational Natural Language Learning and of the Second Learning Language in Logic Workshop, pages 43–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Makoto Nagao</author>
</authors>
<title>Determination of referential property and number of nouns in Japanese sentences for machine translation into English. In</title>
<date>1993</date>
<booktitle>Fifth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>218--225</pages>
<contexts>
<context position="1362" citStr="Murata and Nagao, 1993" startWordPosition="198" endWordPosition="201">ary, native speakers of languages with determiners can select determiners quite well for most NPs. Native speakers of languages without determiners have a much more difficult time. Automating determiner selection is helpful in several applications. A determiner selection program can aid in Machine Translation of determiner-free languages (by adding determiners after the text has been translated), correct English text written by nonnative speakers (Lee, 2004), and choose determiners for text generation programs. Early work on determiner selection focuses on rule-based systems (Gawronska, 1990; Murata and Nagao, 1993; Bond and Ogura, 1994; Heine, 1998). Knight and Chander (1994) use decision trees to choose between the and a/an, ignoring NPs with no determiner, and achieve 78% accuracy on their Wall Street Journal corpus. (Deciding between a and an is a trivial postprocessing step.) Minnen et al. (2000) use a memory-based learner (Daelemans et al., 2000) to choose determiners of base noun phrases. They choose between no determiner (hencefore null), the, and a/an. They use syntactic features (head of the NP, part-of-speech tag of the head of the NP, functional tag of the head of the NP, category of the con</context>
</contexts>
<marker>Murata, Nagao, 1993</marker>
<rawString>Masaki Murata and Makoto Nagao. 1993. Determination of referential property and number of nouns in Japanese sentences for machine translation into English. In Fifth International Conference on Theoretical and Methodological Issues in Machine Translation, pages 218–225.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>