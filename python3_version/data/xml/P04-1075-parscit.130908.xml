<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995986">
Multi-Criteria-based Active Learning for Named Entity Recognition
</title>
<author confidence="0.935411">
Dan Shen†‡1 Jie Zhang†‡ Jian Su† Guodong Zhou† Chew-Lim Tan‡
</author>
<affiliation confidence="0.831597">
† Institute for Infocomm Technology
</affiliation>
<address confidence="0.6031005">
21 Heng Mui Keng Terrace
Singapore 119613
</address>
<affiliation confidence="0.858426333333333">
‡ Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
</affiliation>
<email confidence="0.935534">
{shendan,zhangjie,sujian,zhougd}@i2r.a-star.edu.sg
{shendan,zhangjie,tancl}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.993718" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975611111111">
In this paper, we propose a multi-criteria-
based active learning approach and effec-
tively apply it to named entity recognition.
Active learning targets to minimize the
human annotation efforts by selecting ex-
amples for labeling. To maximize the con-
tribution of the selected examples, we
consider the multiple criteria: informative-
ness, representativeness and diversity and
propose measures to quantify them. More
comprehensively, we incorporate all the
criteria using two selection strategies, both
of which result in less labeling cost than
single-criterion-based method. The results
of the named entity recognition in both
MUC-6 and GENIA show that the labeling
cost can be reduced by at least 80% with-
out degrading the performance.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998945423728814">
In the machine learning approaches of natural lan-
guage processing (NLP), models are generally
trained on large annotated corpus. However, anno-
tating such corpus is expensive and time-
consuming, which makes it difficult to adapt an
existing model to a new domain. In order to over-
come this difficulty, active learning (sample selec-
tion) has been studied in more and more NLP
applications such as POS tagging (Engelson and
Dagan 1999), information extraction (Thompson et
al. 1999), text classification (Lewis and Catlett
1994; McCallum and Nigam 1998; Schohn and
Cohn 2000; Tong and Koller 2000; Brinker 2003),
statistical parsing (Thompson et al. 1999; Tang et
al. 2002; Steedman et al. 2003), noun phrase
chunking (Ngai and Yarowsky 2000), etc.
Active learning is based on the assumption that
a small number of annotated examples and a large
number of unannotated examples are available.
This assumption is valid in most NLP tasks. Dif-
ferent from supervised learning in which the entire
corpus are labeled manually, active learning is to
select the most useful example for labeling and add
the labeled example to training set to retrain model.
This procedure is repeated until the model achieves
a certain level of performance. Practically, a batch
of examples are selected at a time, called batched-
based sample selection (Lewis and Catlett 1994)
since it is time consuming to retrain the model if
only one new example is added to the training set.
Many existing work in the area focus on two ap-
proaches: certainty-based methods (Thompson et
al. 1999; Tang et al. 2002; Schohn and Cohn 2000;
Tong and Koller 2000; Brinker 2003) and commit-
tee-based methods (McCallum and Nigam 1998;
Engelson and Dagan 1999; Ngai and Yarowsky
2000) to select the most informative examples for
which the current model are most uncertain.
Being the first piece of work on active learning
for name entity recognition (NER) task, we target
to minimize the human annotation efforts yet still
reaching the same level of performance as a super-
vised learning approach. For this purpose, we
make a more comprehensive consideration on the
contribution of individual examples, and more im-
portantly maximizing the contribution of a batch
based on three criteria: informativeness, represen-
tativeness and diversity.
First, we propose three scoring functions to
quantify the informativeness of an example, which
can be used to select the most uncertain examples.
Second, the representativeness measure is further
proposed to choose the examples representing the
majority. Third, we propose two diversity consid-
erations (global and local) to avoid repetition
among the examples of a batch. Finally, two com-
bination strategies with the above three criteria are
proposed to reach the maximum effectiveness on
active learning for NER.
</bodyText>
<footnote confidence="0.342941">
1 Current address of the first author: Universität des Saarlandes, Computational Linguistics Dept., 66041 Saarbrücken, Germany
dshen@coli.uni-sb.de
</footnote>
<bodyText confidence="0.999960533333333">
We build our NER model using Support Vec-
tor Machines (SVM). The experiment shows that
our active learning methods achieve a promising
result in this NER task. The results in both MUC-
6 and GENIA show that the amount of the labeled
training data can be reduced by at least 80% with-
out degrading the quality of the named entity rec-
ognizer. The contributions not only come from the
above measures, but also the two sample selection
strategies which effectively incorporate informa-
tiveness, representativeness and diversity criteria.
To our knowledge, it is the first work on consider-
ing the three criteria all together for active learning.
Furthermore, such measures and strategies can be
easily adapted to other active learning tasks as well.
</bodyText>
<sectionHeader confidence="0.992728" genericHeader="method">
2 Multi-criteria for NER Active Learning
</sectionHeader>
<bodyText confidence="0.998016487179488">
Support Vector Machines (SVM) is a powerful
machine learning method, which has been applied
successfully in NER tasks, such as (Kazama et al.
2002; Lee et al. 2003). In this paper, we apply ac-
tive learning methods to a simple and effective
SVM model to recognize one class of names at a
time, such as protein names, person names, etc. In
NER, SVM is to classify a word into positive class
“1” indicating that the word is a part of an entity,
or negative class “-1” indicating that the word is
not a part of an entity. Each word in SVM is rep-
resented as a high-dimensional feature vector in-
cluding surface word information, orthographic
features, POS feature and semantic trigger features
(Shen et al. 2003). The semantic trigger features
consist of some special head nouns for an entity
class which is supplied by users. Furthermore, a
window (size = 7), which represents the local con-
text of the target word w, is also used to classify w.
However, for active learning in NER, it is not
reasonable to select a single word without context
for human to label. Even if we require human to
label a single word, he has to make an addition
effort to refer to the context of the word. In our
active learning process, we select a word sequence
which consists of a machine-annotated named en-
tity and its context rather than a single word.
Therefore, all of the measures we propose for ac-
tive learning should be applied to the machine-
annotated named entities and we have to further
study how to extend the measures for words to
named entities. Thus, the active learning in SVM-
based NER will be more complex than that in sim-
ple classification tasks, such as text classification
on which most SVM active learning works are
conducted (Schohn and Cohn 2000; Tong and
Koller 2000; Brinker 2003). In the next part, we
will introduce informativeness, representativeness
and diversity measures for the SVM-based NER.
</bodyText>
<subsectionHeader confidence="0.820501">
2.1 Informativeness
</subsectionHeader>
<bodyText confidence="0.999936">
The basic idea of informativeness criterion is simi-
lar to certainty-based sample selection methods,
which have been used in many previous works. In
our task, we use a distance-based measure to
evaluate the informativeness of a word and extend
it to the measure of an entity using three scoring
functions. We prefer the examples with high in-
formative degree for which the current model are
most uncertain.
</bodyText>
<subsectionHeader confidence="0.45802">
2.1.1 Informativeness Measure for Word
</subsectionHeader>
<bodyText confidence="0.999940137931035">
In the simplest linear form, training SVM is to find
a hyperplane that can separate the positive and
negative examples in training set with maximum
margin. The margin is defined by the distance of
the hyperplane to the nearest of the positive and
negative examples. The training examples which
are closest to the hyperplane are called support
vectors. In SVM, only the support vectors are use-
ful for the classification, which is different from
statistical models. SVM training is to get these
support vectors and their weights from training set
by solving quadratic programming problem. The
support vectors can later be used to classify the test
data.
Intuitively, we consider the informativeness of
an example as how it can make effect on the sup-
port vectors by adding it to training set. An exam-
ple may be informative for the learner if the
distance of its feature vector to the hyperplane is
less than that of the support vectors to the hyper-
plane (equal to 1). This intuition is also justified
by (Schohn and Cohn 2000; Tong and Koller 2000)
based on a version space analysis. They state that
labeling an example that lies on or close to the hy-
perplane is guaranteed to have an effect on the so-
lution. In our task, we use the distance to measure
the informativeness of an example.
The distance of a word’s feature vector to the
hyperplane is computed as follows:
</bodyText>
<equation confidence="0.895865666666667">
N
aiyik(si,w)+
b
</equation>
<bodyText confidence="0.9984675">
where w is the feature vector of the word, ai, yi, si
corresponds to the weight, the class and the feature
vector of the ith support vector respectively. N is
the number of the support vectors in current model.
We select the example with minimal Dist,
which indicates that it comes closest to the hyper-
plane in feature space. This example is considered
most informative for current model.
</bodyText>
<footnote confidence="0.55332">
2.1.2 Informativeness Measure for Named
Entity
</footnote>
<equation confidence="0.9520772">
= ∑
Dist(w)
i
=
1
</equation>
<bodyText confidence="0.994961">
Based on the above informativeness measure for a
word, we compute the overall informativeness de-
gree of a named entity NE. In this paper, we pro-
pose three scoring functions as follows. Let NE =
w1...wN in which wi is the feature vector of the ith
word of NE.
</bodyText>
<listItem confidence="0.942176333333333">
• Info_Avg: The informativeness of NE is
scored by the average distance of the words in
NE to the hyperplane.
</listItem>
<equation confidence="0.996156">
Info(NE) 1
= − ∑ Dist(w
i)
w
i∈ NE
</equation>
<bodyText confidence="0.9767955">
where, wi is the feature vector of the ith word in
NE.
</bodyText>
<listItem confidence="0.820848818181818">
• Info_Min: The informativeness of NE is
scored by the minimal distance of the words in
NE.
• Info_S/N: If the distance of a word to the hy-
perplane is less than a threshold a (= 1 in our
task), the word is considered with short dis-
tance. Then, we compute the proportion of the
number of words with short distance to the to-
tal number of words in the named entity and
use this proportion to quantify the informa-
tiveness of the named entity.
</listItem>
<equation confidence="0.900578">
NUM (Dist (wi) &lt; a)
NE
N
</equation>
<bodyText confidence="0.9924825">
In Section 4.3, we will evaluate the effective-
ness of these scoring functions.
</bodyText>
<subsectionHeader confidence="0.996506">
2.2 Representativeness
</subsectionHeader>
<bodyText confidence="0.999997">
In addition to the most informative example, we
also prefer the most representative example. The
representativeness of an example can be evaluated
based on how many examples there are similar or
near to it. So, the examples with high representa-
tive degree are less likely to be an outlier. Adding
them to the training set will have effect on a large
number of unlabeled examples. There are only a
few works considering this selection criterion
(McCallum and Nigam 1998; Tang et al. 2002) and
both of them are specific to their tasks, viz. text
classification and statistical parsing. In this section,
we compute the similarity between words using a
general vector-based measure, extend this measure
to named entity level using dynamic time warping
algorithm and quantify the representativeness of a
named entity by its density.
</bodyText>
<subsubsectionHeader confidence="0.803225">
2.2.1 Similarity Measure between Words
</subsubsectionHeader>
<bodyText confidence="0.986375894736842">
In general vector space model, the similarity be-
tween two vectors may be measured by computing
the cosine value of the angle between them. The
smaller the angle is, the more similar between the
vectors are. This measure, called cosine-similarity
measure, has been widely used in information re-
trieval tasks (Baeza-Yates and Ribeiro-Neto 1999).
In our task, we also use it to quantify the similarity
between two words. Particularly, the calculation in
SVM need be projected to a higher dimensional
space by using a certain kernel function K ( w i , w j ) .
Therefore, we adapt the cosine-similarity measure
to SVM as follows:
(wi,wj)=k (wi , wi)k(wj , wj )
where, wi and wj are the feature vectors of the
words i and j. This calculation is also supported by
(Brinker 2003)’s work. Furthermore, if we use the
linear kernel k(wi, wj) = wi ⋅ w j , the measure is
the same as the traditional cosine similarity meas-
</bodyText>
<equation confidence="0.878231">
ure cos 0 = wi⋅wj
w w
⋅
i j
</equation>
<bodyText confidence="0.915809">
general vector-based similarity measure.
</bodyText>
<subsubsectionHeader confidence="0.707642">
2.2.2 Similarity Measure between Named En-
</subsubsectionHeader>
<bodyText confidence="0.977221272727273">
tities
In this part, we compute the similarity between two
machine-annotated named entities given the simi-
larities between words. Regarding an entity as a
word sequence, this work is analogous to the
alignment of two sequences. We employ the dy-
namic time warping (DTW) algorithm (Rabiner et
al. 1978) to find an optimal alignment between the
words in the sequences which maximize the accu-
mulated similarity degree between the sequences.
Here, we adapt it to our task. A sketch of the
modified algorithm is as follows.
Let NE1 = w11w12...w1n...w1N, (n = 1,..., N) and
NE2 = w21w22...w2m...w2M, (m = 1,..., M) denote two
word sequences to be matched. NE1 and NE2 con-
sist of M and N words respectively. NE1(n) = w1n
and NE2(m) = w2m. A similarity value Sim(w1n ,w2m)
has been known for every pair of words (w1n,w2m)
within NE1 and NE2. The goal of DTW is to find a
path, m = map(n), which map n onto the corre-
sponding m such that the accumulated similarity
Sim* along the path is maximized.
</bodyText>
<equation confidence="0.99444975">
N
Sim = ∑
* Max { Sim (N E1 (n), N E2 (map(n))}
{ ( )} 1
</equation>
<bodyText confidence="0.8940186">
map n n=
A dynamic programming method is used to deter-
mine the optimum path map(n). The accumulated
similarity SimA to any grid point (n, m) can be re-
cursively calculated as
</bodyText>
<equation confidence="0.8182015">
SimA (n,m) = Sim (w1n,w2m) + MaxSimA(n−1,q)
q m
≤
Finally, Sim* = Sim A(N,M )
</equation>
<bodyText confidence="0.999859">
Certainly, the overall similarity measure Sim*
has to be normalized as longer sequences normally
give higher similarity value. So, the similarity be-
tween two sequences NE1 and NE2 is calculated as
</bodyText>
<equation confidence="0.898005125">
Info(NE) 1
= − Min{Dist(w
i)}
∈ NE
wi
Info (NE) = w i∈
Sim
k(wi, wj)
</equation>
<bodyText confidence="0.514068">
and may be regarded as a
</bodyText>
<subsubsectionHeader confidence="0.569781">
2.2.3 Representativeness Measure for Named
</subsubsectionHeader>
<bodyText confidence="0.935299090909091">
Entity
Given a set of machine-annotated named entities
NESet = {NE1, . .., NEN}, the representativeness of
a named entity NEi in NESet is quantified by its
density. The density of NEi is defined as the aver-
age similarity between NEi and all the other enti-
ties NEj in NESet as follows.
If NEi has the largest density among all the entities
in NESet, it can be regarded as the centroid of NE-
Set and also the most representative examples in
NESet.
</bodyText>
<subsectionHeader confidence="0.973649">
2.3 Diversity
</subsectionHeader>
<bodyText confidence="0.999941222222222">
Diversity criterion is to maximize the training util-
ity of a batch. We prefer the batch in which the
examples have high variance to each other. For
example, given the batch size 5, we try not to se-
lect five repetitious examples at a time. To our
knowledge, there is only one work (Brinker 2003)
exploring this criterion. In our task, we propose
two methods: local and global, to make the exam-
ples diverse enough in a batch.
</bodyText>
<subsubsectionHeader confidence="0.667114">
2.3.1 Global Consideration
</subsubsectionHeader>
<bodyText confidence="0.996422625">
For a global consideration, we cluster all named
entities in NESet based on the similarity measure
proposed in Section 2.2.2. The named entities in
the same cluster may be considered similar to each
other, so we will select the named entities from
different clusters at one time. We employ a K-
means clustering algorithm (Jelinek 1997), which
is shown in Figure 1.
</bodyText>
<equation confidence="0.427824">
Given:
NESet = {NE1, . .., NEN}
Suppose:
</equation>
<bodyText confidence="0.636535166666667">
The number of clusters is K
Initialization:
Randomly equally partition {NE1, ..., NEN} into K
initial clusters Cj (j = 1, ... , K).
Loop until the number of changes for the centroids of
all clusters is less than a threshold
</bodyText>
<listItem confidence="0.869047833333333">
• Find the centroid of each cluster Cj (j = 1, ..., K).
NECent j = arg max( ∑ Sim(NEi, NE))
NE∈ Cj NEi∈ Cj
• Repartition {NE1, ..., NEN} into K clusters. NEi
will be assigned to Cluster Cj if
Sim(NEi,NECentj)≥ Sim(NEi,NECentw),w ≠ j
</listItem>
<figureCaption confidence="0.975275">
Figure 1: Global Consideration for Diversity: K-
Means Clustering algorithm
</figureCaption>
<bodyText confidence="0.999938285714286">
In each round, we need to compute the pair-
wise similarities within each cluster to get the cen-
troid of the cluster. And then, we need to compute
the similarities between each example and all cen-
troids to repartition the examples. So, the algo-
rithm is time-consuming. Based on the assumption
that N examples are uniformly distributed between
the K clusters, the time complexity of the algo-
rithm is about O(N2/K+NK) (Tang et al. 2002). In
one of our experiments, the size of the NESet (N) is
around 17000 and K is equal to 50, so the time
complexity is about O(106). For efficiency, we
may filter the entities in NESet before clustering
them, which will be further discussed in Section 3.
</bodyText>
<subsubsectionHeader confidence="0.621488">
2.3.2 Local Consideration
</subsubsectionHeader>
<bodyText confidence="0.9999515">
When selecting a machine-annotated named entity,
we compare it with all previously selected named
entities in the current batch. If the similarity be-
tween them is above a threshold 8, this example
cannot be allowed to add into the batch. The order
of selecting examples is based on some measure,
such as informativeness measure, representative-
ness measure or their combination. This local se-
lection method is shown in Figure 2. In this way,
we avoid selecting too similar examples (similarity
value ≥ 8) in a batch. The threshold 8 may be the
average similarity between the examples in NESet.
</bodyText>
<figure confidence="0.5814465">
Given:
NESet = {NE1, . .., NEN}
BatchSet with the maximal size K.
Initialization:
BatchSet = empty
Loop until BatchSet is full
</figure>
<listItem confidence="0.853631111111111">
• Select NEi based on some measure from NESet.
• RepeatFlag = false;
• Loop from j = 1 to CurrentSize(BatchSet)
If Sim (NEi,NEj) ≥ R Then
RepeatFlag = true;
Stop the Loop;
• If RepeatFlag == false Then
add NEi into BatchSet
• remove NEi from NESet
</listItem>
<figureCaption confidence="0.990753">
Figure 2: Local Consideration for Diversity
</figureCaption>
<bodyText confidence="0.9986072">
This consideration only requires O(NK+K2)
computational time. In one of our experiments (N
˜ 17000 and K = 50), the time complexity is about
O(105). It is more efficient than clustering algo-
rithm described in Section 2.3.1.
</bodyText>
<sectionHeader confidence="0.980227" genericHeader="method">
3 Sample Selection strategies
</sectionHeader>
<bodyText confidence="0.998744333333333">
In this section, we will study how to combine and
strike a proper balance between these criteria, viz.
informativeness, representativeness and diversity,
</bodyText>
<equation confidence="0.981869">
∑
Sim
(NEi ,NEj )
N−1
Density
(NEi) = j≠i
Sim *
Sim NE NE
( , ) =
1 2
Max N M
( , )
</equation>
<bodyText confidence="0.9999442">
to reach the maximum effectiveness on NER active
learning. We build two strategies to combine the
measures proposed above. These strategies are
based on the varying priorities of the criteria and
the varying degrees to satisfy the criteria.
</bodyText>
<listItem confidence="0.862348">
• Strategy 1: We first consider the informative-
ness criterion. We choose m examples with the
</listItem>
<bodyText confidence="0.970103384615385">
most informativeness score from NESet to an in-
termediate set called INTERSet. By this pre-
selecting, we make the selection process faster in
the later steps since the size of INTERSet is much
smaller than that of NESet. Then we cluster the
examples in INTERSet and choose the centroid of
each cluster into a batch called BatchSet. The cen-
troid of a cluster is the most representative exam-
ple in that cluster since it has the largest density.
Furthermore, the examples in different clusters
may be considered diverse to each other. By this
means, we consider representativeness and diver-
sity criteria at the same time. This strategy is
shown in Figure 3. One limitation of this strategy
is that clustering result may not reflect the distribu-
tion of whole sample space since we only cluster
on INTERSet for efficiency. The other is that since
the representativeness of an example is only evalu-
ated on a cluster. If the cluster size is too small,
the most representative example in this cluster may
not be representative in the whole sample space.
Given:
NESet = {NE1, . .., NEN}
BatchSet with the maximal size K.
INTERSet with the maximal size M
Steps:
</bodyText>
<listItem confidence="0.9935975">
• BatchSet = 0
• INTERSet = 0
• Select M entities with most Info score from NESet
to INTERSet.
• Cluster the entities in INTERSet into K clusters
• Add the centroid entity of each cluster to BatchSet
Figure 3: Sample Selection Strategy 1
• Strategy 2: (Figure 4) We combine the infor-
mativeness and representativeness criteria using
the functio lInfo(NEi) + (1− l)Density(NEi) , in
</listItem>
<bodyText confidence="0.872658076923077">
which the Info and Density value of NEi are nor-
malized first. The individual importance of each
criterion in this function is adjusted by the trade-
off parameter l ( 0 ≤ l ≤1 ) (set to 0.6 in our
experiment). First, we select a candidate example
NEi with the maximum value of this function from
NESet. Second, we consider diversity criterion
using the local method in Section 3.3.2. We add
the candidate example NEi to a batch only if NEi is
different enough from any previously selected ex-
ample in the batch. The threshold ß is set to the
average pair-wise similarity of the entities in NE-
Set.
</bodyText>
<figure confidence="0.8096365">
Given:
NESet = {NE1, . .., NEN}
BatchSet with the maximal size K.
Initialization:
BatchSet = 0
Loop until BatchSet is full
</figure>
<listItem confidence="0.770855">
• Select NEi which have the maximum value for the
combination function between Info score and Den-
sity socre from NESet.
</listItem>
<equation confidence="0.905917666666667">
NEi = argMax(lInfo(NEi)+(1−l)Density(NEi))
NE NESet
i∈
</equation>
<listItem confidence="0.859711375">
• RepeatFlag = false;
• Loop from j = 1 to CurrentSize(BatchSet)
If Sim (NEi, NEj) ≥ b Then
RepeatFlag = true;
Stop the Loop;
• If RepeatFlag == false Then
add NEi into BatchSet
• remove NEi from NESet
</listItem>
<figureCaption confidence="0.992464">
Figure 4: Sample Selection Strategy 2
</figureCaption>
<sectionHeader confidence="0.863188" genericHeader="method">
4 Experimental Results and Analysis
</sectionHeader>
<subsectionHeader confidence="0.995698">
4.1 Experiment Settings
</subsectionHeader>
<bodyText confidence="0.974111869565218">
In order to evaluate the effectiveness of our selec-
tion strategies, we apply them to recognize protein
(PRT) names in biomedical domain using GENIA
corpus V1.1 (Ohta et al. 2002) and person (PER),
location (LOC), organization (ORG) names in
newswire domain using MUC-6 corpus. First, we
randomly split the whole corpus into three parts: an
initial training set to build an initial model, a test
set to evaluate the performance of the model and
an unlabeled set to select examples. The size of
each data set is shown in Table 1. Then, iteratively,
we select a batch of examples following the selec-
tion strategies proposed, require human experts to
label them and add them into the training set. The
batch size K = 50 in GENIA and 10 in MUC-6.
Each example is defined as a machine-recognized
named entity and its context words (previous 3
words and next 3 words).
Domain Class Corpus Initial Training Set Test Set Unlabeled Set
Biomedical PRT GENIA1.1 10 sent. (277 words) 900 sent. (26K words) 8004 sent. (223K words)
Newswire PER MUC-6 5 sent. (131 words) 602 sent. (14K words) 7809 sent. (157K words)
LOC 5 sent. (130 words) 7809 sent. (157K words)
ORG 5 sent. (113 words) 7809 sent. (157K words)
</bodyText>
<tableCaption confidence="0.998776">
Table 1: Experiment settings for active learning using GENIA1.1(PRT) and MUC-6(PER,LOC,ORG)
</tableCaption>
<bodyText confidence="0.9997996">
The goal of our work is to minimize the human
annotation effort to learn a named entity recognizer
with the same performance level as supervised
learning. The performance of our model is evalu-
ated using “precision/recall/F-measure”.
</bodyText>
<subsectionHeader confidence="0.998421">
4.2 Overall Result in GENIA and MUC-6
</subsectionHeader>
<bodyText confidence="0.998474375">
In this section, we evaluate our selection strategies
by comparing them with a random selection
method, in which a batch of examples is randomly
selected iteratively, on GENIA and MUC-6 corpus.
Table 2 shows the amount of training data needed
to achieve the performance of supervised learning
using various selection methods, viz. Random,
Strategy1 and Strategy2. In GENIA, we find:
</bodyText>
<listItem confidence="0.996245125">
• The model achieves 63.3 F-measure using 223K
words in the supervised learning.
• The best performer is Strategy2 (31K words),
requiring less than 40% of the training data that
Random (83K words) does and 14% of the train-
ing data that the supervised learning does.
• Strategy1 (40K words) performs slightly worse
than Strategy2, requiring 9K more words. It is
probably because Strategy1 cannot avoid select-
ing outliers if a cluster is too small.
• Random (83K words) requires about 37% of the
training data that the supervised learning does. It
indicates that only the words in and around a
named entity are useful for classification and the
words far from the named entity may not be
helpful.
</listItem>
<table confidence="0.9994514">
Class Supervised Random Strategy1 Strategy2
PRT 223K (F=63.3) 83K 40K 31K
PER 157K (F=90.4) 11.5K 4.2K 3.5K
LOC 157K (F=73.5) 13.6K 3.5K 2.1K
ORG 157K (F=86.0) 20.2K 9.5K 7.8K
</table>
<tableCaption confidence="0.999295">
Table 2: Overall Result in GENIA and MUC-6
</tableCaption>
<bodyText confidence="0.999986153846154">
Furthermore, when we apply our model to news-
wire domain (MUC-6) to recognize person, loca-
tion and organization names, Strategy1 and
Strategy2 show a more promising result by com-
paring with the supervised learning and Random,
as shown in Table 2. On average, about 95% of
the data can be reduced to achieve the same per-
formance with the supervised learning in MUC-6.
It is probably because NER in the newswire do-
main is much simpler than that in the biomedical
domain (Shen et al. 2003) and named entities are
less and distributed much sparser in the newswire
texts than in the biomedical texts.
</bodyText>
<subsectionHeader confidence="0.999861">
4.3 Effectiveness of Informativeness-based
Selection Method
</subsectionHeader>
<bodyText confidence="0.998980941176471">
In this section, we investigate the effectiveness of
informativeness criterion in NER task. Figure 5
shows a plot of training data size versus F-measure
achieved by the informativeness-based measures in
Section 3.1.2: Info_Avg, Info_Min and Info_S/N as
well as Random. We make the comparisons in
GENIA corpus. In Figure 5, the horizontal line is
the performance level (63.3 F-measure) achieved
by supervised learning (223K words). We find
that the three informativeness-based measures per-
form similarly and each of them outperforms Ran-
dom. Table 3 highlights the various data sizes to
achieve the peak performance using these selection
methods. We find that Random (83K words) on
average requires over 1.5 times as much as data to
achieve the same performance as the informative-
ness-based selection methods (52K words).
</bodyText>
<figureCaption confidence="0.91779">
Figure 5: Active learning curves: effectiveness of the three in-
formativeness-criterion-based selections comparing with the
Random selection.
</figureCaption>
<table confidence="0.869157">
Supervised Random Info_Avg Info_Min Info_ S/N
223K 83K 52.0K 51.9K 52.3K
</table>
<tableCaption confidence="0.997033">
Table 3: Training data sizes for various selection methods to
achieve the same performance level as the supervised learning
</tableCaption>
<subsectionHeader confidence="0.9915455">
4.4 Effectiveness of Two Sample Selection
Strategies
</subsectionHeader>
<bodyText confidence="0.999968409090909">
In addition to the informativeness criterion, we
further incorporate representativeness and diversity
criteria into active learning using two strategies
described in Section 3. Comparing the two strate-
gies with the best result of the single-criterion-
based selection methods Info_Min, we are to jus-
tify that representativeness and diversity are also
important factors for active learning. Figure 6
shows the learning curves for the various methods:
Strategy1, Strategy2 and Info_Min. In the begin-
ning iterations (F-measure &lt; 60), the three methods
performed similarly. But with the larger training
set, the efficiencies of Stratety1 and Strategy2 be-
gin to be evident. Table 4 highlights the final re-
sult of the three methods. In order to reach the
performance of supervised learning, Strategy1
(40K words) and Strategyy2 (31K words) require
about 80% and 60% of the data that Info_Min
(51.9K) does. So we believe the effective combi-
nations of informativeness, representativeness and
diversity will help to learn the NER model more
quickly and cost less in annotation.
</bodyText>
<figure confidence="0.999971666666667">
0 20 40 60 K words 80
0.65
0.55
F
0.5
0.6
Supervised
Random
Info_Min
Info_S/N
Info_Avg
0 20 40 60 K words
</figure>
<figureCaption confidence="0.922925666666667">
Figure 6: Active learning curves: effectiveness of the two
multi-criteria-based selection strategies comparing with the
informativeness-criterion-based selection (Info_Min).
</figureCaption>
<tableCaption confidence="0.757841333333333">
Info_Min Strategy1 Strategy2
51.9K 40K 31K
Table 4: Comparisons of training data sizes for the multi-
criteria-based selection strategies and the informativeness-
criterion-based selection (Info_Min) to achieve the same per-
formance level as the supervised learning.
</tableCaption>
<sectionHeader confidence="0.999882" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999958777777778">
Since there is no study on active learning for NER
task previously, we only introduce general active
learning methods here. Many existing active learn-
ing methods are to select the most uncertain exam-
ples using various measures (Thompson et al. 1999;
Schohn and Cohn 2000; Tong and Koller 2000;
Engelson and Dagan 1999; Ngai and Yarowsky
2000). Our informativeness-based measure is
similar to these works. However these works just
follow a single criterion. (McCallum and Nigam
1998; Tang et al. 2002) are the only two works
considering the representativeness criterion in ac-
tive learning. (Tang et al. 2002) use the density
information to weight the selected examples while
we use it to select examples. Moreover, the repre-
sentativeness measure we use is relatively general
and easy to adapt to other tasks, in which the ex-
ample selected is a sequence of words, such as text
chunking, POS tagging, etc. On the other hand,
(Brinker 2003) first incorporate diversity in active
learning for text classification. Their work is simi-
lar to our local consideration in Section 2.3.2.
However, he didn’t further explore how to avoid
selecting outliers to a batch. So far, we haven’t
found any previous work integrating the informa-
tiveness, representativeness and diversity all to-
gether.
</bodyText>
<sectionHeader confidence="0.990568" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999964107142857">
In this paper, we study the active learning in a
more complex NLP task, named entity recognition.
We propose a multi-criteria-based approach to se-
lect examples based on their informativeness, rep-
resentativeness and diversity, which are
incorporated all together by two strategies (local
and global). Experiments show that, in both MUC-
6 and GENIA, both of the two strategies combin-
ing the three criteria outperform the single criterion
(informativeness). The labeling cost can be sig-
nificantly reduced by at least 80% comparing with
the supervised learning. To our best knowledge,
this is not only the first work to report the empiri-
cal results of active learning for NER, but also the
first work to incorporate the three criteria all to-
gether for selecting examples.
Although the current experiment results are
very promising, some parameters in our experi-
ment, such as the batch size K and the X in the
function of strategy 2, are decided by our experi-
ence in the domain. In practical application, the
optimal value of these parameters should be de-
cided automatically based on the training process.
Furthermore, we will study how to overcome the
limitation of the strategy 1 discussed in Section 3
by using more effective clustering algorithm. An-
other interesting work is to study when to stop ac-
tive learning.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999761">
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Mod-
ern Information Retrieval. ISBN 0-201-39829-X.
K. Brinker. 2003. Incorporating Diversity in Ac-
tive Learning with Support Vector Machines. In
Proceedings of ICML, 2003.
S. A. Engelson and I. Dagan. 1999. Committee-
Based Sample Selection for Probabilistic Classi-
fiers. Journal of Artifical Intelligence Research.
F. Jelinek. 1997. Statistical Methods for Speech
Recognition. MIT Press.
J. Kazama, T. Makino, Y. Ohta and J. Tsujii. 2002.
Tuning Support Vector Machines for Biomedi-
cal Named Entity Recognition. In Proceedings
of the ACL2002 Workshop on NLP in Biomedi-
cine.
K. J. Lee, Y. S. Hwang and H. C. Rim. 2003. Two-
Phase Biomedical NE Recognition based on
SVMs. In Proceedings of the ACL2003 Work-
shop on NLP in Biomedicine.
D. D. Lewis and J. Catlett. 1994. Heterogeneous
Uncertainty Sampling for Supervised Learning.
In Proceedings of ICML, 1994.
A. McCallum and K. Nigam. 1998. Employing EM
in Pool-Based Active Learning for Text Classi-
fication. In Proceedings of ICML, 1998.
G. Ngai and D. Yarowsky. 2000. Rule Writing or
Annotation: Cost-efficient Resource Usage for
Base Noun Phrase Chunking. In Proceedings of
ACL, 2000.
</reference>
<figure confidence="0.996848111111111">
0.65
0.55
F
0.6
0.5
Supervised
Info_Min
Strategy1
Strategy2
</figure>
<reference confidence="0.999649108108108">
T. Ohta, Y. Tateisi, J. Kim, H. Mima and J. Tsujii.
2002. The GENIA corpus: An annotated re-
search abstract corpus in molecular biology do-
main. In Proceedings of HLT 2002.
L. R. Rabiner, A. E. Rosenberg and S. E. Levinson.
1978. Considerations in Dynamic Time Warping
Algorithms for Discrete Word Recognition. In
Proceedings of IEEE Transactions on acoustics,
speech and signal processing. Vol. ASSP-26,
NO.6.
D. Schohn and D. Cohn. 2000. Less is More: Ac-
tive Learning with Support Vector Machines. In
Proceedings of the 17th International Confer-
ence on Machine Learning.
D. Shen, J. Zhang, G. D. Zhou, J. Su and C. L. Tan.
2003. Effective Adaptation of a Hidden Markov
Model-based Named Entity Recognizer for Bio-
medical Domain. In Proceedings of the
ACL2003 Workshop on NLP in Biomedicine.
M. Steedman, R. Hwa, S. Clark, M. Osborne, A.
Sarkar, J. Hockenmaier, P. Ruhlen, S. Baker and
J. Crim. 2003. Example Selection for Bootstrap-
ping Statistical Parsers. In Proceedings of HLT-
NAACL, 2003.
M. Tang, X. Luo and S. Roukos. 2002. Active
Learning for Statistical Natural Language Pars-
ing. In Proceedings of the ACL 2002.
C. A. Thompson, M. E. Califf and R. J. Mooney.
1999. Active Learning for Natural Language
Parsing and Information Extraction. In Proceed-
ings of ICML 1999.
S. Tong and D. Koller. 2000. Support Vector Ma-
chine Active Learning with Applications to Text
Classification. Journal of Machine Learning Re-
search.
V. Vapnik. 1998. Statistical learning theory.
N.Y.:John Wiley.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.293488">
<title confidence="0.999451">Multi-Criteria-based Active Learning for Named Entity Recognition</title>
<author confidence="0.89124">Jie Jian Guodong Chew-Lim</author>
<affiliation confidence="0.956776">for Infocomm Technology</affiliation>
<address confidence="0.912005">21 Heng Mui Keng Terrace Singapore 119613</address>
<affiliation confidence="0.867863">of Computer Science National University of Singapore 3 Science Drive 2, Singapore 117543</affiliation>
<email confidence="0.8221605">shendan@comp.nus.edu.sg</email>
<email confidence="0.8221605">zhangjie@comp.nus.edu.sg</email>
<email confidence="0.8221605">sujian@comp.nus.edu.sg</email>
<email confidence="0.8221605">zhougd}@i2r.a-star.edu.sg{shendan@comp.nus.edu.sg</email>
<email confidence="0.8221605">zhangjie@comp.nus.edu.sg</email>
<email confidence="0.8221605">tancl@comp.nus.edu.sg</email>
<abstract confidence="0.999076388888889">In this paper, we propose a multi-criteriabased active learning approach and effectively apply it to named entity recognition. Active learning targets to minimize the human annotation efforts by selecting examples for labeling. To maximize the contribution of the selected examples, we the multiple criteria: informativepropose measures to quantify them. More comprehensively, we incorporate all the criteria using two selection strategies, both of which result in less labeling cost than single-criterion-based method. The results of the named entity recognition in both MUC-6 and GENIA show that the labeling cost can be reduced by at least 80% without degrading the performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Baeza-Yates</author>
<author>B Ribeiro-Neto</author>
</authors>
<date>1999</date>
<booktitle>Modern Information Retrieval. ISBN</booktitle>
<pages>0--201</pages>
<contexts>
<context position="11319" citStr="Baeza-Yates and Ribeiro-Neto 1999" startWordPosition="1870" endWordPosition="1873">parsing. In this section, we compute the similarity between words using a general vector-based measure, extend this measure to named entity level using dynamic time warping algorithm and quantify the representativeness of a named entity by its density. 2.2.1 Similarity Measure between Words In general vector space model, the similarity between two vectors may be measured by computing the cosine value of the angle between them. The smaller the angle is, the more similar between the vectors are. This measure, called cosine-similarity measure, has been widely used in information retrieval tasks (Baeza-Yates and Ribeiro-Neto 1999). In our task, we also use it to quantify the similarity between two words. Particularly, the calculation in SVM need be projected to a higher dimensional space by using a certain kernel function K ( w i , w j ) . Therefore, we adapt the cosine-similarity measure to SVM as follows: (wi,wj)=k (wi , wi)k(wj , wj ) where, wi and wj are the feature vectors of the words i and j. This calculation is also supported by (Brinker 2003)’s work. Furthermore, if we use the linear kernel k(wi, wj) = wi ⋅ w j , the measure is the same as the traditional cosine similarity measure cos 0 = wi⋅wj w w ⋅ i j gener</context>
</contexts>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern Information Retrieval. ISBN 0-201-39829-X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Brinker</author>
</authors>
<title>Incorporating Diversity in Active Learning with Support Vector Machines.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML,</booktitle>
<contexts>
<context position="1764" citStr="Brinker 2003" startWordPosition="255" endWordPosition="256">ction In the machine learning approaches of natural language processing (NLP), models are generally trained on large annotated corpus. However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain. In order to overcome this difficulty, active learning (sample selection) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classification (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al. 1999; Tang et al. 2002; Steedman et al. 2003), noun phrase chunking (Ngai and Yarowsky 2000), etc. Active learning is based on the assumption that a small number of annotated examples and a large number of unannotated examples are available. This assumption is valid in most NLP tasks. Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labeling and add the labeled example to training set to retrain model. This procedure is repeated until the model achieves a certain l</context>
<context position="6663" citStr="Brinker 2003" startWordPosition="1068" endWordPosition="1069">f the word. In our active learning process, we select a word sequence which consists of a machine-annotated named entity and its context rather than a single word. Therefore, all of the measures we propose for active learning should be applied to the machineannotated named entities and we have to further study how to extend the measures for words to named entities. Thus, the active learning in SVMbased NER will be more complex than that in simple classification tasks, such as text classification on which most SVM active learning works are conducted (Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003). In the next part, we will introduce informativeness, representativeness and diversity measures for the SVM-based NER. 2.1 Informativeness The basic idea of informativeness criterion is similar to certainty-based sample selection methods, which have been used in many previous works. In our task, we use a distance-based measure to evaluate the informativeness of a word and extend it to the measure of an entity using three scoring functions. We prefer the examples with high informative degree for which the current model are most uncertain. 2.1.1 Informativeness Measure for Word In the simplest </context>
<context position="11748" citStr="Brinker 2003" startWordPosition="1953" endWordPosition="1954">le is, the more similar between the vectors are. This measure, called cosine-similarity measure, has been widely used in information retrieval tasks (Baeza-Yates and Ribeiro-Neto 1999). In our task, we also use it to quantify the similarity between two words. Particularly, the calculation in SVM need be projected to a higher dimensional space by using a certain kernel function K ( w i , w j ) . Therefore, we adapt the cosine-similarity measure to SVM as follows: (wi,wj)=k (wi , wi)k(wj , wj ) where, wi and wj are the feature vectors of the words i and j. This calculation is also supported by (Brinker 2003)’s work. Furthermore, if we use the linear kernel k(wi, wj) = wi ⋅ w j , the measure is the same as the traditional cosine similarity measure cos 0 = wi⋅wj w w ⋅ i j general vector-based similarity measure. 2.2.2 Similarity Measure between Named Entities In this part, we compute the similarity between two machine-annotated named entities given the similarities between words. Regarding an entity as a word sequence, this work is analogous to the alignment of two sequences. We employ the dynamic time warping (DTW) algorithm (Rabiner et al. 1978) to find an optimal alignment between the words in t</context>
<context position="14380" citStr="Brinker 2003" startWordPosition="2440" endWordPosition="2441">antified by its density. The density of NEi is defined as the average similarity between NEi and all the other entities NEj in NESet as follows. If NEi has the largest density among all the entities in NESet, it can be regarded as the centroid of NESet and also the most representative examples in NESet. 2.3 Diversity Diversity criterion is to maximize the training utility of a batch. We prefer the batch in which the examples have high variance to each other. For example, given the batch size 5, we try not to select five repetitious examples at a time. To our knowledge, there is only one work (Brinker 2003) exploring this criterion. In our task, we propose two methods: local and global, to make the examples diverse enough in a batch. 2.3.1 Global Consideration For a global consideration, we cluster all named entities in NESet based on the similarity measure proposed in Section 2.2.2. The named entities in the same cluster may be considered similar to each other, so we will select the named entities from different clusters at one time. We employ a Kmeans clustering algorithm (Jelinek 1997), which is shown in Figure 1. Given: NESet = {NE1, . .., NEN} Suppose: The number of clusters is K Initializa</context>
<context position="28031" citStr="Brinker 2003" startWordPosition="4722" endWordPosition="4723"> and Yarowsky 2000). Our informativeness-based measure is similar to these works. However these works just follow a single criterion. (McCallum and Nigam 1998; Tang et al. 2002) are the only two works considering the representativeness criterion in active learning. (Tang et al. 2002) use the density information to weight the selected examples while we use it to select examples. Moreover, the representativeness measure we use is relatively general and easy to adapt to other tasks, in which the example selected is a sequence of words, such as text chunking, POS tagging, etc. On the other hand, (Brinker 2003) first incorporate diversity in active learning for text classification. Their work is similar to our local consideration in Section 2.3.2. However, he didn’t further explore how to avoid selecting outliers to a batch. So far, we haven’t found any previous work integrating the informativeness, representativeness and diversity all together. 6 Conclusion and Future Work In this paper, we study the active learning in a more complex NLP task, named entity recognition. We propose a multi-criteria-based approach to select examples based on their informativeness, representativeness and diversity, whi</context>
</contexts>
<marker>Brinker, 2003</marker>
<rawString>K. Brinker. 2003. Incorporating Diversity in Active Learning with Support Vector Machines. In Proceedings of ICML, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Engelson</author>
<author>I Dagan</author>
</authors>
<title>CommitteeBased Sample Selection for Probabilistic Classifiers.</title>
<date>1999</date>
<journal>Journal of Artifical Intelligence Research.</journal>
<contexts>
<context position="1588" citStr="Engelson and Dagan 1999" startWordPosition="227" endWordPosition="230">n-based method. The results of the named entity recognition in both MUC-6 and GENIA show that the labeling cost can be reduced by at least 80% without degrading the performance. 1 Introduction In the machine learning approaches of natural language processing (NLP), models are generally trained on large annotated corpus. However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain. In order to overcome this difficulty, active learning (sample selection) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classification (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al. 1999; Tang et al. 2002; Steedman et al. 2003), noun phrase chunking (Ngai and Yarowsky 2000), etc. Active learning is based on the assumption that a small number of annotated examples and a large number of unannotated examples are available. This assumption is valid in most NLP tasks. Different from supervised learning in which the entire corpus are labeled manually, active learning</context>
<context position="2862" citStr="Engelson and Dagan 1999" startWordPosition="436" endWordPosition="439">and add the labeled example to training set to retrain model. This procedure is repeated until the model achieves a certain level of performance. Practically, a batch of examples are selected at a time, called batchedbased sample selection (Lewis and Catlett 1994) since it is time consuming to retrain the model if only one new example is added to the training set. Many existing work in the area focus on two approaches: certainty-based methods (Thompson et al. 1999; Tang et al. 2002; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003) and committee-based methods (McCallum and Nigam 1998; Engelson and Dagan 1999; Ngai and Yarowsky 2000) to select the most informative examples for which the current model are most uncertain. Being the first piece of work on active learning for name entity recognition (NER) task, we target to minimize the human annotation efforts yet still reaching the same level of performance as a supervised learning approach. For this purpose, we make a more comprehensive consideration on the contribution of individual examples, and more importantly maximizing the contribution of a batch based on three criteria: informativeness, representativeness and diversity. First, we propose thr</context>
<context position="27412" citStr="Engelson and Dagan 1999" startWordPosition="4618" endWordPosition="4621">-based selection (Info_Min). Info_Min Strategy1 Strategy2 51.9K 40K 31K Table 4: Comparisons of training data sizes for the multicriteria-based selection strategies and the informativenesscriterion-based selection (Info_Min) to achieve the same performance level as the supervised learning. 5 Related Work Since there is no study on active learning for NER task previously, we only introduce general active learning methods here. Many existing active learning methods are to select the most uncertain examples using various measures (Thompson et al. 1999; Schohn and Cohn 2000; Tong and Koller 2000; Engelson and Dagan 1999; Ngai and Yarowsky 2000). Our informativeness-based measure is similar to these works. However these works just follow a single criterion. (McCallum and Nigam 1998; Tang et al. 2002) are the only two works considering the representativeness criterion in active learning. (Tang et al. 2002) use the density information to weight the selected examples while we use it to select examples. Moreover, the representativeness measure we use is relatively general and easy to adapt to other tasks, in which the example selected is a sequence of words, such as text chunking, POS tagging, etc. On the other h</context>
</contexts>
<marker>Engelson, Dagan, 1999</marker>
<rawString>S. A. Engelson and I. Dagan. 1999. CommitteeBased Sample Selection for Probabilistic Classifiers. Journal of Artifical Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="14871" citStr="Jelinek 1997" startWordPosition="2522" endWordPosition="2523"> size 5, we try not to select five repetitious examples at a time. To our knowledge, there is only one work (Brinker 2003) exploring this criterion. In our task, we propose two methods: local and global, to make the examples diverse enough in a batch. 2.3.1 Global Consideration For a global consideration, we cluster all named entities in NESet based on the similarity measure proposed in Section 2.2.2. The named entities in the same cluster may be considered similar to each other, so we will select the named entities from different clusters at one time. We employ a Kmeans clustering algorithm (Jelinek 1997), which is shown in Figure 1. Given: NESet = {NE1, . .., NEN} Suppose: The number of clusters is K Initialization: Randomly equally partition {NE1, ..., NEN} into K initial clusters Cj (j = 1, ... , K). Loop until the number of changes for the centroids of all clusters is less than a threshold • Find the centroid of each cluster Cj (j = 1, ..., K). NECent j = arg max( ∑ Sim(NEi, NE)) NE∈ Cj NEi∈ Cj • Repartition {NE1, ..., NEN} into K clusters. NEi will be assigned to Cluster Cj if Sim(NEi,NECentj)≥ Sim(NEi,NECentw),w ≠ j Figure 1: Global Consideration for Diversity: KMeans Clustering algorith</context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>F. Jelinek. 1997. Statistical Methods for Speech Recognition. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>T Makino</author>
<author>Y Ohta</author>
<author>J Tsujii</author>
</authors>
<title>Tuning Support Vector Machines for Biomedical Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL2002 Workshop on NLP in Biomedicine.</booktitle>
<contexts>
<context position="5030" citStr="Kazama et al. 2002" startWordPosition="772" endWordPosition="775">he named entity recognizer. The contributions not only come from the above measures, but also the two sample selection strategies which effectively incorporate informativeness, representativeness and diversity criteria. To our knowledge, it is the first work on considering the three criteria all together for active learning. Furthermore, such measures and strategies can be easily adapted to other active learning tasks as well. 2 Multi-criteria for NER Active Learning Support Vector Machines (SVM) is a powerful machine learning method, which has been applied successfully in NER tasks, such as (Kazama et al. 2002; Lee et al. 2003). In this paper, we apply active learning methods to a simple and effective SVM model to recognize one class of names at a time, such as protein names, person names, etc. In NER, SVM is to classify a word into positive class “1” indicating that the word is a part of an entity, or negative class “-1” indicating that the word is not a part of an entity. Each word in SVM is represented as a high-dimensional feature vector including surface word information, orthographic features, POS feature and semantic trigger features (Shen et al. 2003). The semantic trigger features consist </context>
</contexts>
<marker>Kazama, Makino, Ohta, Tsujii, 2002</marker>
<rawString>J. Kazama, T. Makino, Y. Ohta and J. Tsujii. 2002. Tuning Support Vector Machines for Biomedical Named Entity Recognition. In Proceedings of the ACL2002 Workshop on NLP in Biomedicine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K J Lee</author>
<author>Y S Hwang</author>
<author>H C Rim</author>
</authors>
<title>TwoPhase Biomedical NE Recognition based on SVMs.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL2003 Workshop on NLP in Biomedicine.</booktitle>
<contexts>
<context position="5048" citStr="Lee et al. 2003" startWordPosition="776" endWordPosition="779">gnizer. The contributions not only come from the above measures, but also the two sample selection strategies which effectively incorporate informativeness, representativeness and diversity criteria. To our knowledge, it is the first work on considering the three criteria all together for active learning. Furthermore, such measures and strategies can be easily adapted to other active learning tasks as well. 2 Multi-criteria for NER Active Learning Support Vector Machines (SVM) is a powerful machine learning method, which has been applied successfully in NER tasks, such as (Kazama et al. 2002; Lee et al. 2003). In this paper, we apply active learning methods to a simple and effective SVM model to recognize one class of names at a time, such as protein names, person names, etc. In NER, SVM is to classify a word into positive class “1” indicating that the word is a part of an entity, or negative class “-1” indicating that the word is not a part of an entity. Each word in SVM is represented as a high-dimensional feature vector including surface word information, orthographic features, POS feature and semantic trigger features (Shen et al. 2003). The semantic trigger features consist of some special he</context>
</contexts>
<marker>Lee, Hwang, Rim, 2003</marker>
<rawString>K. J. Lee, Y. S. Hwang and H. C. Rim. 2003. TwoPhase Biomedical NE Recognition based on SVMs. In Proceedings of the ACL2003 Workshop on NLP in Biomedicine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>J Catlett</author>
</authors>
<title>Heterogeneous Uncertainty Sampling for Supervised Learning.</title>
<date>1994</date>
<booktitle>In Proceedings of ICML,</booktitle>
<contexts>
<context position="1680" citStr="Lewis and Catlett 1994" startWordPosition="239" endWordPosition="242">the labeling cost can be reduced by at least 80% without degrading the performance. 1 Introduction In the machine learning approaches of natural language processing (NLP), models are generally trained on large annotated corpus. However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain. In order to overcome this difficulty, active learning (sample selection) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classification (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al. 1999; Tang et al. 2002; Steedman et al. 2003), noun phrase chunking (Ngai and Yarowsky 2000), etc. Active learning is based on the assumption that a small number of annotated examples and a large number of unannotated examples are available. This assumption is valid in most NLP tasks. Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labeling and add the labeled example to training s</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>D. D. Lewis and J. Catlett. 1994. Heterogeneous Uncertainty Sampling for Supervised Learning. In Proceedings of ICML, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Nigam</author>
</authors>
<title>Employing EM in Pool-Based Active Learning for Text Classification.</title>
<date>1998</date>
<booktitle>In Proceedings of ICML,</booktitle>
<contexts>
<context position="1705" citStr="McCallum and Nigam 1998" startWordPosition="243" endWordPosition="246"> reduced by at least 80% without degrading the performance. 1 Introduction In the machine learning approaches of natural language processing (NLP), models are generally trained on large annotated corpus. However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain. In order to overcome this difficulty, active learning (sample selection) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classification (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al. 1999; Tang et al. 2002; Steedman et al. 2003), noun phrase chunking (Ngai and Yarowsky 2000), etc. Active learning is based on the assumption that a small number of annotated examples and a large number of unannotated examples are available. This assumption is valid in most NLP tasks. Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labeling and add the labeled example to training set to retrain model. This</context>
<context position="10578" citStr="McCallum and Nigam 1998" startWordPosition="1755" endWordPosition="1758">veness of the named entity. NUM (Dist (wi) &lt; a) NE N In Section 4.3, we will evaluate the effectiveness of these scoring functions. 2.2 Representativeness In addition to the most informative example, we also prefer the most representative example. The representativeness of an example can be evaluated based on how many examples there are similar or near to it. So, the examples with high representative degree are less likely to be an outlier. Adding them to the training set will have effect on a large number of unlabeled examples. There are only a few works considering this selection criterion (McCallum and Nigam 1998; Tang et al. 2002) and both of them are specific to their tasks, viz. text classification and statistical parsing. In this section, we compute the similarity between words using a general vector-based measure, extend this measure to named entity level using dynamic time warping algorithm and quantify the representativeness of a named entity by its density. 2.2.1 Similarity Measure between Words In general vector space model, the similarity between two vectors may be measured by computing the cosine value of the angle between them. The smaller the angle is, the more similar between the vectors</context>
<context position="27576" citStr="McCallum and Nigam 1998" startWordPosition="4642" endWordPosition="4645"> and the informativenesscriterion-based selection (Info_Min) to achieve the same performance level as the supervised learning. 5 Related Work Since there is no study on active learning for NER task previously, we only introduce general active learning methods here. Many existing active learning methods are to select the most uncertain examples using various measures (Thompson et al. 1999; Schohn and Cohn 2000; Tong and Koller 2000; Engelson and Dagan 1999; Ngai and Yarowsky 2000). Our informativeness-based measure is similar to these works. However these works just follow a single criterion. (McCallum and Nigam 1998; Tang et al. 2002) are the only two works considering the representativeness criterion in active learning. (Tang et al. 2002) use the density information to weight the selected examples while we use it to select examples. Moreover, the representativeness measure we use is relatively general and easy to adapt to other tasks, in which the example selected is a sequence of words, such as text chunking, POS tagging, etc. On the other hand, (Brinker 2003) first incorporate diversity in active learning for text classification. Their work is similar to our local consideration in Section 2.3.2. Howev</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>A. McCallum and K. Nigam. 1998. Employing EM in Pool-Based Active Learning for Text Classification. In Proceedings of ICML, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ngai</author>
<author>D Yarowsky</author>
</authors>
<title>Rule Writing or Annotation: Cost-efficient Resource Usage for Base Noun Phrase Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL,</booktitle>
<contexts>
<context position="1895" citStr="Ngai and Yarowsky 2000" startWordPosition="274" endWordPosition="277">tated corpus. However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain. In order to overcome this difficulty, active learning (sample selection) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classification (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al. 1999; Tang et al. 2002; Steedman et al. 2003), noun phrase chunking (Ngai and Yarowsky 2000), etc. Active learning is based on the assumption that a small number of annotated examples and a large number of unannotated examples are available. This assumption is valid in most NLP tasks. Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labeling and add the labeled example to training set to retrain model. This procedure is repeated until the model achieves a certain level of performance. Practically, a batch of examples are selected at a time, called batchedbased sample selection (Lewis and Catle</context>
<context position="27437" citStr="Ngai and Yarowsky 2000" startWordPosition="4622" endWordPosition="4625">n). Info_Min Strategy1 Strategy2 51.9K 40K 31K Table 4: Comparisons of training data sizes for the multicriteria-based selection strategies and the informativenesscriterion-based selection (Info_Min) to achieve the same performance level as the supervised learning. 5 Related Work Since there is no study on active learning for NER task previously, we only introduce general active learning methods here. Many existing active learning methods are to select the most uncertain examples using various measures (Thompson et al. 1999; Schohn and Cohn 2000; Tong and Koller 2000; Engelson and Dagan 1999; Ngai and Yarowsky 2000). Our informativeness-based measure is similar to these works. However these works just follow a single criterion. (McCallum and Nigam 1998; Tang et al. 2002) are the only two works considering the representativeness criterion in active learning. (Tang et al. 2002) use the density information to weight the selected examples while we use it to select examples. Moreover, the representativeness measure we use is relatively general and easy to adapt to other tasks, in which the example selected is a sequence of words, such as text chunking, POS tagging, etc. On the other hand, (Brinker 2003) first</context>
</contexts>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>G. Ngai and D. Yarowsky. 2000. Rule Writing or Annotation: Cost-efficient Resource Usage for Base Noun Phrase Chunking. In Proceedings of ACL, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ohta</author>
<author>Y Tateisi</author>
<author>J Kim</author>
<author>H Mima</author>
<author>J Tsujii</author>
</authors>
<title>The GENIA corpus: An annotated research abstract corpus in molecular biology domain.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT</booktitle>
<contexts>
<context position="20925" citStr="Ohta et al. 2002" startWordPosition="3582" endWordPosition="3585">value for the combination function between Info score and Density socre from NESet. NEi = argMax(lInfo(NEi)+(1−l)Density(NEi)) NE NESet i∈ • RepeatFlag = false; • Loop from j = 1 to CurrentSize(BatchSet) If Sim (NEi, NEj) ≥ b Then RepeatFlag = true; Stop the Loop; • If RepeatFlag == false Then add NEi into BatchSet • remove NEi from NESet Figure 4: Sample Selection Strategy 2 4 Experimental Results and Analysis 4.1 Experiment Settings In order to evaluate the effectiveness of our selection strategies, we apply them to recognize protein (PRT) names in biomedical domain using GENIA corpus V1.1 (Ohta et al. 2002) and person (PER), location (LOC), organization (ORG) names in newswire domain using MUC-6 corpus. First, we randomly split the whole corpus into three parts: an initial training set to build an initial model, a test set to evaluate the performance of the model and an unlabeled set to select examples. The size of each data set is shown in Table 1. Then, iteratively, we select a batch of examples following the selection strategies proposed, require human experts to label them and add them into the training set. The batch size K = 50 in GENIA and 10 in MUC-6. Each example is defined as a machine</context>
</contexts>
<marker>Ohta, Tateisi, Kim, Mima, Tsujii, 2002</marker>
<rawString>T. Ohta, Y. Tateisi, J. Kim, H. Mima and J. Tsujii. 2002. The GENIA corpus: An annotated research abstract corpus in molecular biology domain. In Proceedings of HLT 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
<author>A E Rosenberg</author>
<author>S E Levinson</author>
</authors>
<title>Considerations in Dynamic Time Warping Algorithms for Discrete Word Recognition.</title>
<date>1978</date>
<booktitle>In Proceedings of IEEE Transactions on acoustics, speech and signal processing. Vol. ASSP-26,</booktitle>
<pages>6</pages>
<contexts>
<context position="12296" citStr="Rabiner et al. 1978" startWordPosition="2047" endWordPosition="2050"> the words i and j. This calculation is also supported by (Brinker 2003)’s work. Furthermore, if we use the linear kernel k(wi, wj) = wi ⋅ w j , the measure is the same as the traditional cosine similarity measure cos 0 = wi⋅wj w w ⋅ i j general vector-based similarity measure. 2.2.2 Similarity Measure between Named Entities In this part, we compute the similarity between two machine-annotated named entities given the similarities between words. Regarding an entity as a word sequence, this work is analogous to the alignment of two sequences. We employ the dynamic time warping (DTW) algorithm (Rabiner et al. 1978) to find an optimal alignment between the words in the sequences which maximize the accumulated similarity degree between the sequences. Here, we adapt it to our task. A sketch of the modified algorithm is as follows. Let NE1 = w11w12...w1n...w1N, (n = 1,..., N) and NE2 = w21w22...w2m...w2M, (m = 1,..., M) denote two word sequences to be matched. NE1 and NE2 consist of M and N words respectively. NE1(n) = w1n and NE2(m) = w2m. A similarity value Sim(w1n ,w2m) has been known for every pair of words (w1n,w2m) within NE1 and NE2. The goal of DTW is to find a path, m = map(n), which map n onto the</context>
</contexts>
<marker>Rabiner, Rosenberg, Levinson, 1978</marker>
<rawString>L. R. Rabiner, A. E. Rosenberg and S. E. Levinson. 1978. Considerations in Dynamic Time Warping Algorithms for Discrete Word Recognition. In Proceedings of IEEE Transactions on acoustics, speech and signal processing. Vol. ASSP-26, NO.6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Schohn</author>
<author>D Cohn</author>
</authors>
<title>Less is More: Active Learning with Support Vector Machines.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="1727" citStr="Schohn and Cohn 2000" startWordPosition="247" endWordPosition="250">without degrading the performance. 1 Introduction In the machine learning approaches of natural language processing (NLP), models are generally trained on large annotated corpus. However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain. In order to overcome this difficulty, active learning (sample selection) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classification (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al. 1999; Tang et al. 2002; Steedman et al. 2003), noun phrase chunking (Ngai and Yarowsky 2000), etc. Active learning is based on the assumption that a small number of annotated examples and a large number of unannotated examples are available. This assumption is valid in most NLP tasks. Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labeling and add the labeled example to training set to retrain model. This procedure is repeated</context>
<context position="6626" citStr="Schohn and Cohn 2000" startWordPosition="1060" endWordPosition="1063">an addition effort to refer to the context of the word. In our active learning process, we select a word sequence which consists of a machine-annotated named entity and its context rather than a single word. Therefore, all of the measures we propose for active learning should be applied to the machineannotated named entities and we have to further study how to extend the measures for words to named entities. Thus, the active learning in SVMbased NER will be more complex than that in simple classification tasks, such as text classification on which most SVM active learning works are conducted (Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003). In the next part, we will introduce informativeness, representativeness and diversity measures for the SVM-based NER. 2.1 Informativeness The basic idea of informativeness criterion is similar to certainty-based sample selection methods, which have been used in many previous works. In our task, we use a distance-based measure to evaluate the informativeness of a word and extend it to the measure of an entity using three scoring functions. We prefer the examples with high informative degree for which the current model are most uncertain. 2.1.1 Informativen</context>
<context position="8269" citStr="Schohn and Cohn 2000" startWordPosition="1332" endWordPosition="1335">e classification, which is different from statistical models. SVM training is to get these support vectors and their weights from training set by solving quadratic programming problem. The support vectors can later be used to classify the test data. Intuitively, we consider the informativeness of an example as how it can make effect on the support vectors by adding it to training set. An example may be informative for the learner if the distance of its feature vector to the hyperplane is less than that of the support vectors to the hyperplane (equal to 1). This intuition is also justified by (Schohn and Cohn 2000; Tong and Koller 2000) based on a version space analysis. They state that labeling an example that lies on or close to the hyperplane is guaranteed to have an effect on the solution. In our task, we use the distance to measure the informativeness of an example. The distance of a word’s feature vector to the hyperplane is computed as follows: N aiyik(si,w)+ b where w is the feature vector of the word, ai, yi, si corresponds to the weight, the class and the feature vector of the ith support vector respectively. N is the number of the support vectors in current model. We select the example with </context>
<context position="27365" citStr="Schohn and Cohn 2000" startWordPosition="4610" endWordPosition="4613">comparing with the informativeness-criterion-based selection (Info_Min). Info_Min Strategy1 Strategy2 51.9K 40K 31K Table 4: Comparisons of training data sizes for the multicriteria-based selection strategies and the informativenesscriterion-based selection (Info_Min) to achieve the same performance level as the supervised learning. 5 Related Work Since there is no study on active learning for NER task previously, we only introduce general active learning methods here. Many existing active learning methods are to select the most uncertain examples using various measures (Thompson et al. 1999; Schohn and Cohn 2000; Tong and Koller 2000; Engelson and Dagan 1999; Ngai and Yarowsky 2000). Our informativeness-based measure is similar to these works. However these works just follow a single criterion. (McCallum and Nigam 1998; Tang et al. 2002) are the only two works considering the representativeness criterion in active learning. (Tang et al. 2002) use the density information to weight the selected examples while we use it to select examples. Moreover, the representativeness measure we use is relatively general and easy to adapt to other tasks, in which the example selected is a sequence of words, such as </context>
</contexts>
<marker>Schohn, Cohn, 2000</marker>
<rawString>D. Schohn and D. Cohn. 2000. Less is More: Active Learning with Support Vector Machines. In Proceedings of the 17th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>J Zhang</author>
<author>G D Zhou</author>
<author>J Su</author>
<author>C L Tan</author>
</authors>
<title>Effective Adaptation of a Hidden Markov Model-based Named Entity Recognizer for Biomedical Domain.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL2003 Workshop on NLP in Biomedicine.</booktitle>
<contexts>
<context position="5590" citStr="Shen et al. 2003" startWordPosition="874" endWordPosition="877">ed successfully in NER tasks, such as (Kazama et al. 2002; Lee et al. 2003). In this paper, we apply active learning methods to a simple and effective SVM model to recognize one class of names at a time, such as protein names, person names, etc. In NER, SVM is to classify a word into positive class “1” indicating that the word is a part of an entity, or negative class “-1” indicating that the word is not a part of an entity. Each word in SVM is represented as a high-dimensional feature vector including surface word information, orthographic features, POS feature and semantic trigger features (Shen et al. 2003). The semantic trigger features consist of some special head nouns for an entity class which is supplied by users. Furthermore, a window (size = 7), which represents the local context of the target word w, is also used to classify w. However, for active learning in NER, it is not reasonable to select a single word without context for human to label. Even if we require human to label a single word, he has to make an addition effort to refer to the context of the word. In our active learning process, we select a word sequence which consists of a machine-annotated named entity and its context rat</context>
<context position="24090" citStr="Shen et al. 2003" startWordPosition="4116" endWordPosition="4119">0.4) 11.5K 4.2K 3.5K LOC 157K (F=73.5) 13.6K 3.5K 2.1K ORG 157K (F=86.0) 20.2K 9.5K 7.8K Table 2: Overall Result in GENIA and MUC-6 Furthermore, when we apply our model to newswire domain (MUC-6) to recognize person, location and organization names, Strategy1 and Strategy2 show a more promising result by comparing with the supervised learning and Random, as shown in Table 2. On average, about 95% of the data can be reduced to achieve the same performance with the supervised learning in MUC-6. It is probably because NER in the newswire domain is much simpler than that in the biomedical domain (Shen et al. 2003) and named entities are less and distributed much sparser in the newswire texts than in the biomedical texts. 4.3 Effectiveness of Informativeness-based Selection Method In this section, we investigate the effectiveness of informativeness criterion in NER task. Figure 5 shows a plot of training data size versus F-measure achieved by the informativeness-based measures in Section 3.1.2: Info_Avg, Info_Min and Info_S/N as well as Random. We make the comparisons in GENIA corpus. In Figure 5, the horizontal line is the performance level (63.3 F-measure) achieved by supervised learning (223K words).</context>
</contexts>
<marker>Shen, Zhang, Zhou, Su, Tan, 2003</marker>
<rawString>D. Shen, J. Zhang, G. D. Zhou, J. Su and C. L. Tan. 2003. Effective Adaptation of a Hidden Markov Model-based Named Entity Recognizer for Biomedical Domain. In Proceedings of the ACL2003 Workshop on NLP in Biomedicine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
<author>R Hwa</author>
<author>S Clark</author>
<author>M Osborne</author>
<author>A Sarkar</author>
<author>J Hockenmaier</author>
<author>P Ruhlen</author>
<author>S Baker</author>
<author>J Crim</author>
</authors>
<title>Example Selection for Bootstrapping Statistical Parsers.</title>
<date>2003</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<contexts>
<context position="1848" citStr="Steedman et al. 2003" startWordPosition="267" endWordPosition="270">), models are generally trained on large annotated corpus. However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain. In order to overcome this difficulty, active learning (sample selection) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classification (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al. 1999; Tang et al. 2002; Steedman et al. 2003), noun phrase chunking (Ngai and Yarowsky 2000), etc. Active learning is based on the assumption that a small number of annotated examples and a large number of unannotated examples are available. This assumption is valid in most NLP tasks. Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labeling and add the labeled example to training set to retrain model. This procedure is repeated until the model achieves a certain level of performance. Practically, a batch of examples are selected at a time, called</context>
</contexts>
<marker>Steedman, Hwa, Clark, Osborne, Sarkar, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>M. Steedman, R. Hwa, S. Clark, M. Osborne, A. Sarkar, J. Hockenmaier, P. Ruhlen, S. Baker and J. Crim. 2003. Example Selection for Bootstrapping Statistical Parsers. In Proceedings of HLTNAACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tang</author>
<author>X Luo</author>
<author>S Roukos</author>
</authors>
<title>Active Learning for Statistical Natural Language Parsing.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL</booktitle>
<contexts>
<context position="1825" citStr="Tang et al. 2002" startWordPosition="263" endWordPosition="266">ge processing (NLP), models are generally trained on large annotated corpus. However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain. In order to overcome this difficulty, active learning (sample selection) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classification (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al. 1999; Tang et al. 2002; Steedman et al. 2003), noun phrase chunking (Ngai and Yarowsky 2000), etc. Active learning is based on the assumption that a small number of annotated examples and a large number of unannotated examples are available. This assumption is valid in most NLP tasks. Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labeling and add the labeled example to training set to retrain model. This procedure is repeated until the model achieves a certain level of performance. Practically, a batch of examples are sel</context>
<context position="10597" citStr="Tang et al. 2002" startWordPosition="1759" endWordPosition="1762">y. NUM (Dist (wi) &lt; a) NE N In Section 4.3, we will evaluate the effectiveness of these scoring functions. 2.2 Representativeness In addition to the most informative example, we also prefer the most representative example. The representativeness of an example can be evaluated based on how many examples there are similar or near to it. So, the examples with high representative degree are less likely to be an outlier. Adding them to the training set will have effect on a large number of unlabeled examples. There are only a few works considering this selection criterion (McCallum and Nigam 1998; Tang et al. 2002) and both of them are specific to their tasks, viz. text classification and statistical parsing. In this section, we compute the similarity between words using a general vector-based measure, extend this measure to named entity level using dynamic time warping algorithm and quantify the representativeness of a named entity by its density. 2.2.1 Similarity Measure between Words In general vector space model, the similarity between two vectors may be measured by computing the cosine value of the angle between them. The smaller the angle is, the more similar between the vectors are. This measure,</context>
<context position="15905" citStr="Tang et al. 2002" startWordPosition="2705" endWordPosition="2708">NE1, ..., NEN} into K clusters. NEi will be assigned to Cluster Cj if Sim(NEi,NECentj)≥ Sim(NEi,NECentw),w ≠ j Figure 1: Global Consideration for Diversity: KMeans Clustering algorithm In each round, we need to compute the pairwise similarities within each cluster to get the centroid of the cluster. And then, we need to compute the similarities between each example and all centroids to repartition the examples. So, the algorithm is time-consuming. Based on the assumption that N examples are uniformly distributed between the K clusters, the time complexity of the algorithm is about O(N2/K+NK) (Tang et al. 2002). In one of our experiments, the size of the NESet (N) is around 17000 and K is equal to 50, so the time complexity is about O(106). For efficiency, we may filter the entities in NESet before clustering them, which will be further discussed in Section 3. 2.3.2 Local Consideration When selecting a machine-annotated named entity, we compare it with all previously selected named entities in the current batch. If the similarity between them is above a threshold 8, this example cannot be allowed to add into the batch. The order of selecting examples is based on some measure, such as informativeness</context>
<context position="27595" citStr="Tang et al. 2002" startWordPosition="4646" endWordPosition="4649">riterion-based selection (Info_Min) to achieve the same performance level as the supervised learning. 5 Related Work Since there is no study on active learning for NER task previously, we only introduce general active learning methods here. Many existing active learning methods are to select the most uncertain examples using various measures (Thompson et al. 1999; Schohn and Cohn 2000; Tong and Koller 2000; Engelson and Dagan 1999; Ngai and Yarowsky 2000). Our informativeness-based measure is similar to these works. However these works just follow a single criterion. (McCallum and Nigam 1998; Tang et al. 2002) are the only two works considering the representativeness criterion in active learning. (Tang et al. 2002) use the density information to weight the selected examples while we use it to select examples. Moreover, the representativeness measure we use is relatively general and easy to adapt to other tasks, in which the example selected is a sequence of words, such as text chunking, POS tagging, etc. On the other hand, (Brinker 2003) first incorporate diversity in active learning for text classification. Their work is similar to our local consideration in Section 2.3.2. However, he didn’t furth</context>
</contexts>
<marker>Tang, Luo, Roukos, 2002</marker>
<rawString>M. Tang, X. Luo and S. Roukos. 2002. Active Learning for Statistical Natural Language Parsing. In Proceedings of the ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C A Thompson</author>
<author>M E Califf</author>
<author>R J Mooney</author>
</authors>
<title>Active Learning for Natural Language Parsing and Information Extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="1635" citStr="Thompson et al. 1999" startWordPosition="233" endWordPosition="236">cognition in both MUC-6 and GENIA show that the labeling cost can be reduced by at least 80% without degrading the performance. 1 Introduction In the machine learning approaches of natural language processing (NLP), models are generally trained on large annotated corpus. However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain. In order to overcome this difficulty, active learning (sample selection) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classification (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al. 1999; Tang et al. 2002; Steedman et al. 2003), noun phrase chunking (Ngai and Yarowsky 2000), etc. Active learning is based on the assumption that a small number of annotated examples and a large number of unannotated examples are available. This assumption is valid in most NLP tasks. Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for label</context>
<context position="27343" citStr="Thompson et al. 1999" startWordPosition="4606" endWordPosition="4609"> selection strategies comparing with the informativeness-criterion-based selection (Info_Min). Info_Min Strategy1 Strategy2 51.9K 40K 31K Table 4: Comparisons of training data sizes for the multicriteria-based selection strategies and the informativenesscriterion-based selection (Info_Min) to achieve the same performance level as the supervised learning. 5 Related Work Since there is no study on active learning for NER task previously, we only introduce general active learning methods here. Many existing active learning methods are to select the most uncertain examples using various measures (Thompson et al. 1999; Schohn and Cohn 2000; Tong and Koller 2000; Engelson and Dagan 1999; Ngai and Yarowsky 2000). Our informativeness-based measure is similar to these works. However these works just follow a single criterion. (McCallum and Nigam 1998; Tang et al. 2002) are the only two works considering the representativeness criterion in active learning. (Tang et al. 2002) use the density information to weight the selected examples while we use it to select examples. Moreover, the representativeness measure we use is relatively general and easy to adapt to other tasks, in which the example selected is a seque</context>
</contexts>
<marker>Thompson, Califf, Mooney, 1999</marker>
<rawString>C. A. Thompson, M. E. Califf and R. J. Mooney. 1999. Active Learning for Natural Language Parsing and Information Extraction. In Proceedings of ICML 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tong</author>
<author>D Koller</author>
</authors>
<title>Support Vector Machine Active Learning with Applications to Text Classification.</title>
<date>2000</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="1749" citStr="Tong and Koller 2000" startWordPosition="251" endWordPosition="254">performance. 1 Introduction In the machine learning approaches of natural language processing (NLP), models are generally trained on large annotated corpus. However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain. In order to overcome this difficulty, active learning (sample selection) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classification (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al. 1999; Tang et al. 2002; Steedman et al. 2003), noun phrase chunking (Ngai and Yarowsky 2000), etc. Active learning is based on the assumption that a small number of annotated examples and a large number of unannotated examples are available. This assumption is valid in most NLP tasks. Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labeling and add the labeled example to training set to retrain model. This procedure is repeated until the model achie</context>
<context position="6648" citStr="Tong and Koller 2000" startWordPosition="1064" endWordPosition="1067">refer to the context of the word. In our active learning process, we select a word sequence which consists of a machine-annotated named entity and its context rather than a single word. Therefore, all of the measures we propose for active learning should be applied to the machineannotated named entities and we have to further study how to extend the measures for words to named entities. Thus, the active learning in SVMbased NER will be more complex than that in simple classification tasks, such as text classification on which most SVM active learning works are conducted (Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003). In the next part, we will introduce informativeness, representativeness and diversity measures for the SVM-based NER. 2.1 Informativeness The basic idea of informativeness criterion is similar to certainty-based sample selection methods, which have been used in many previous works. In our task, we use a distance-based measure to evaluate the informativeness of a word and extend it to the measure of an entity using three scoring functions. We prefer the examples with high informative degree for which the current model are most uncertain. 2.1.1 Informativeness Measure for Word I</context>
<context position="8292" citStr="Tong and Koller 2000" startWordPosition="1336" endWordPosition="1339">h is different from statistical models. SVM training is to get these support vectors and their weights from training set by solving quadratic programming problem. The support vectors can later be used to classify the test data. Intuitively, we consider the informativeness of an example as how it can make effect on the support vectors by adding it to training set. An example may be informative for the learner if the distance of its feature vector to the hyperplane is less than that of the support vectors to the hyperplane (equal to 1). This intuition is also justified by (Schohn and Cohn 2000; Tong and Koller 2000) based on a version space analysis. They state that labeling an example that lies on or close to the hyperplane is guaranteed to have an effect on the solution. In our task, we use the distance to measure the informativeness of an example. The distance of a word’s feature vector to the hyperplane is computed as follows: N aiyik(si,w)+ b where w is the feature vector of the word, ai, yi, si corresponds to the weight, the class and the feature vector of the ith support vector respectively. N is the number of the support vectors in current model. We select the example with minimal Dist, which ind</context>
<context position="27387" citStr="Tong and Koller 2000" startWordPosition="4614" endWordPosition="4617">ormativeness-criterion-based selection (Info_Min). Info_Min Strategy1 Strategy2 51.9K 40K 31K Table 4: Comparisons of training data sizes for the multicriteria-based selection strategies and the informativenesscriterion-based selection (Info_Min) to achieve the same performance level as the supervised learning. 5 Related Work Since there is no study on active learning for NER task previously, we only introduce general active learning methods here. Many existing active learning methods are to select the most uncertain examples using various measures (Thompson et al. 1999; Schohn and Cohn 2000; Tong and Koller 2000; Engelson and Dagan 1999; Ngai and Yarowsky 2000). Our informativeness-based measure is similar to these works. However these works just follow a single criterion. (McCallum and Nigam 1998; Tang et al. 2002) are the only two works considering the representativeness criterion in active learning. (Tang et al. 2002) use the density information to weight the selected examples while we use it to select examples. Moreover, the representativeness measure we use is relatively general and easy to adapt to other tasks, in which the example selected is a sequence of words, such as text chunking, POS tag</context>
</contexts>
<marker>Tong, Koller, 2000</marker>
<rawString>S. Tong and D. Koller. 2000. Support Vector Machine Active Learning with Applications to Text Classification. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>Statistical learning theory.</title>
<date>1998</date>
<publisher>N.Y.:John Wiley.</publisher>
<marker>Vapnik, 1998</marker>
<rawString>V. Vapnik. 1998. Statistical learning theory. N.Y.:John Wiley.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>