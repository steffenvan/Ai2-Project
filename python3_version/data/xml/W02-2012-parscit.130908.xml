<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9988815">
GraSp: Grammar learning
from unlabelled speech corpora
</title>
<author confidence="0.957779">
Peter Juel Henrichsen
</author>
<affiliation confidence="0.8664144">
CMOL
Center for Computational Modelling of Language
c/o Dept. of Computational Linguistics
Copenhagen Business School
Frederiksberg, Denmark
</affiliation>
<email confidence="0.917875">
pjuel@id.cbs.dk
</email>
<bodyText confidence="0.999953315789474">
greatly reduced given a structure of primitive
linguistic constraints (&amp;quot;a highly restrictive
schematism&amp;quot;, ibid.). It has however been very
hard to establish independently the
psychological reality of such a structure, and the
question of innateness is still far from settled.
While a decisive experiment may never be
conceived, the issue could be addressed
indirectly, e.g. by asking: Are innate principles
and parameters necessary preconditions for
grammar acquisition? Or rephrased in the spirit
of constructive logic: Can a learning algorithm
be devised that learns what the infant learns
without incorporating specific linguistic axioms?
The presentation of such an algorithm would
certainly undermine arguments referring to the
&apos;poverty of the stimulus&apos;, showing the innateness
hypothesis to be dispensable.
This paper presents our first try.
</bodyText>
<sectionHeader confidence="0.666911" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999971">
This paper presents the ongoing project
Computational Models of First Language
Acquisition, together with its current
product, the learning algorithm GraSp.
GraSp is designed specifically for
inducing grammars from large, unlabelled
corpora of spontaneous (i.e. unscripted)
speech. The learning algorithm does not
assume a predefined grammatical
taxonomy; rather the determination of
categories and their relations is considered
as part of the learning task. While GraSp
learning can be used for a range of
practical tasks, the long-term goal of the
project is to contribute to the debate of
innate linguistic knowledge – under the
hypothesis that there is no such.
</bodyText>
<sectionHeader confidence="0.960902" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999904333333334">
Most current models of grammar learning
assume a set of primitive linguistic categories
and constraints, the learning process being
modelled as category filling and rule
instantiation – rather than category formation
and rule creation. Arguably, distributing
linguistic data over predefined categories and
templates does not qualify as grammar &apos;learning&apos;
in the strictest sense, but is better described as
&apos;adjustment&apos; or &apos;adaptation&apos;. Indeed, Chomsky,
the prime advocate of the hypothesis of innate
linguistic principles, has claimed that &amp;quot;in certain
fundamental respects we do not really learn
language&amp;quot; (Chomsky 1980: 134). As Chomsky
points out, the complexity of the learning task is
</bodyText>
<sectionHeader confidence="0.974251" genericHeader="method">
1 The essential algorithm
</sectionHeader>
<subsectionHeader confidence="0.961024">
1.1 Psycho-linguistic preconditions
</subsectionHeader>
<bodyText confidence="0.973051923076923">
Typical spontaneous speech is anything but
syntactically &apos;well-formed&apos; in the Chomskyan
sense of the word.
right well let&apos;s er --= let&apos;s look at the applications
- erm - let me just ask initially this -- I discussed
it with er Reith er but we&apos;ll = have to go into it a
bit further - is it is it within our erm er = are we
free er to er draw up a rather = exiguous list - of
people to interview
(sample from the London-Lund corpus)
Yet informal speech is not perceived as being
disorderly (certainly not by the language
learning infant), suggesting that its organizing
principles differ from those of the written
language. So, arguably, a speech grammar
inducing algorithm should avoid referring to the
usual categories of text based linguistics –
&apos;sentence&apos;, &apos;determiner phrase&apos;, etc.1
Instead we allow a large, indefinite number
of (indistinguishable) basic categories – and then
leave it to the learner to shape them, fill them
up, and combine them. For this task, the learner
needs a built-in concept of constituency. This
kind of innateness is not in conflict with our
main hypothesis, we believe, since constituency
as such is not specific to linguistic structure.
</bodyText>
<subsectionHeader confidence="0.995278">
1.2 Logical preliminaries
</subsectionHeader>
<bodyText confidence="0.944636904761905">
For the reasons explained, we want the learning
algorithm to be strictly data-driven. This puts
special demands on our parser which must be
robust enough to accept input strings with little
or no hints of syntactic structure (for the early
stages of a learning session), while at the same
time retaining the discriminating powers of a
standard context free parser (for the later stages).
Our solution is a sequent calculus, a variant
of the Gentzen-Lambek categorial grammar
formalism (L) enhanced with non-classical rules
for isolating a residue of uninterpretable sequent
elements. The classical part is identical to L
(except that antecedents may be empty).
1 Hoekstra (2000) and Nivre (2001) discuss the
annotation of spoken corpora with traditional tags.
These seven rules capture the input parts that
can be interpreted as syntactic constituents
(examples below). For the remaining parts, we
include two non-classical rules (6L and 6R).2
Non-classical part
</bodyText>
<equation confidence="0.992797666666667">
6+ A1 A2 =&gt; C 6
6L –––––– 6R
A1 6 A2 =&gt; C =&gt; 6
</equation>
<listItem confidence="0.808172">
6 is a basic category. Ax are (possibly empty)
strings of categories. Superscripts + – denote
polarity of residual elements.
</listItem>
<bodyText confidence="0.871909333333333">
By way of an example, consider the input string
right well let&apos;s er let&apos;s look at the applications
as analyzed in an early stage of a learning
session. Since no lexical structure has developed
yet, the input is mapped onto a sequent of basic
(dummy) categories:3
</bodyText>
<equation confidence="0.89229">
c29 c22 c81 c5 c81 c215 c10 c1 c891 =&gt; c0
</equation>
<bodyText confidence="0.9996906">
Using 6L recursively, each category of the
antecedent (the part to the left of =&gt;) is removed
from the main sequent. As the procedure is fairly
simple, we just show a fragment of the proof.
Notice that proofs read most easily bottom-up.
</bodyText>
<equation confidence="0.991777">
c0
––––– 6R
c81 + c10 + c1 + c891 + =&gt; c0 6L
...
6L
+
c215 c81 c10 c1 c891 =&gt; c0
6L
+
c5 c81 c215 c10 c1 c891 =&gt; c0
6L
... c5 c81 c215 c10 c1 c891 =&gt; c0
</equation>
<bodyText confidence="0.999441666666667">
In this proof there are no links, meaning that no
grammatical structure was found. Later, when
the lexicon has developed, the parser may
</bodyText>
<footnote confidence="0.903185285714286">
2 The calculus presented here is slightly simplified.
Two rules are missing, and so is the reserved
category T (&apos;noise&apos;) used e.g. for consequents (in
place of c0 of the example). Cf. Henrichsen (2000).
3 By convention the indexing of category names
reflects the frequency distribution: If word W has
rank n in the training corpus, it is initialized as W:cn .
</footnote>
<figure confidence="0.861007088235294">
AB =&gt; B A1 A A2 =&gt; C
A1 A/B AB A2 =&gt; C
A0 B =&gt; A
/R
A0 =&gt; A/B
/L
AB =&gt; B A1 A A2 =&gt; C
A1 AB B\A A2 =&gt; C
B A0 =&gt; A
\R
A0 =&gt; B\A
\L
*L
*R
A1 A B A2 =&gt; C
A1 A*B A2 =&gt; C
A1 =&gt; A A2 =&gt; B
A1 A2 =&gt; A*B
Classical part
link
6 =&gt; 6
A, B, C are categories; Ax are (possibly empty)
strings of categories.
recognize more structure in the same input: [an utterance delimited by e.g. turntakes and
pauses]. A corpus is a bag of soli [a transcript of
l l a conversation].
c10=&gt;c10 c891=&gt;c891
*R
c10 c891 =&gt; c10*c891 c81 c215 =&gt; c0
––––––l /L
c1 =&gt; c1 c81 c215/(c10*c891) c10 c891 =&gt; c0
\L
... c81 c215/(c10*c891) c10 c1 c1\c891 =&gt; c0
... let&apos;s look at the applications
</figure>
<bodyText confidence="0.999915">
This proof tree has three links, meaning that the
disorder of the input string (wrt. the new
lexicon) has dropped by three degrees. More on
disorder shortly.
</bodyText>
<subsectionHeader confidence="0.996452">
1.3 The algorithm in outline
</subsectionHeader>
<bodyText confidence="0.9971848">
Having presented the sequent parser, we now
show its embedding in the learning algorithm
GraSp (Grammar of Speech).
For reasons mentioned earlier, the common
inventory of categories (S, NP, CN, etc) is
avoided. Instead each lexeme initially inhabits
its own proto-category. If a training corpus has,
say, 12,345 word types the initial lexicon maps
them onto as many different categories. A
learning session, then, is a sequence of lexical
changes, introducing, removing, and
manipulating the operators /, \, and * as guided
by a well-defined measure of structural disorder.
We prefer formal terms without a linguistic
bias (&amp;quot;no innate linguistic constraints&amp;quot;).
Suggestive linguistic interpretations are
provided in square brackets.
A-F summarize the learning algorithm.
A) There are categories. Complex categories are
built from basic categories using /, \, and *:
</bodyText>
<equation confidence="0.82097925">
Basic categories
c1, c2, c3, ... , c12345 , ...
Complex categories
c1\c12345, c2/c3, c4*c5, c2/(c3\(c4*c5))
</equation>
<listItem confidence="0.8433068">
B) A lexicon is a mapping of lexemes [word
types represented in phonetic or enriched-
orthographic encoding] onto categories.
C) An input segment is an instance of a lexeme
[an input word]. A solo is a string of segments
</listItem>
<bodyText confidence="0.973495909090909">
D) Applying an update L:C1&apos;C2 in lexicon Lex
means changing the mapping of L in Lex from
C1 to C2. Valid changes are minimal, i.e. C2 is
construed from C1 by adding or removing 1
basic category (using \, /, or *).
E) The learning process is guided by a measure
of disorder. The disorder function Dis takes a
sequent E [the lexical mapping of an utterance]
returning the number of uninterpretable atoms in
E, i.e. 6+s and 6–s in a (maximally linked) proof.
Dis(E)=0 iff E is Lambek valid. Examples:
</bodyText>
<table confidence="0.5563652">
Dis( ca/cb cb =&gt; ca ) = 0
Dis( ca/cb cb =&gt; cc ) = 2
Dis( cb ca/cb =&gt; cc ) = 4
Dis( ca/cb cc cb =&gt; ca ) = 1
Dis( ca/cc cb ca\cc =&gt; ca ) = 2
</table>
<bodyText confidence="0.996973">
DIS(Lex,K) is the total amount of disorder in
training corpus K wrt. lexicon Lex, i.e. the sum
of Dis-values for all soli in K as mapped by Lex.
F) A learning session is an iterative process. In
each iteration i a suitable update Ui is applied in
the lexicon Lexi–1 producing Lexi . Quantifying
over all possible updates, Ui is picked so as to
maximize the drop in disorder (DisDrop):
</bodyText>
<equation confidence="0.926313">
DisDrop = DIS(Lexi–1,K) – DIS(Lexi,K)
</equation>
<bodyText confidence="0.9966712">
The session terminates when no suitable update
remains.
It is possible to GraSp efficiently and yet
preserve logical completeness. See Henrichsen
(2000) for discussion and demonstrations.
</bodyText>
<subsectionHeader confidence="0.995306">
1.4 A staged learning session
</subsectionHeader>
<bodyText confidence="0.982506">
Given this tiny corpus of four soli (&apos;utterances&apos;)
</bodyText>
<construct confidence="0.9971995">
if you must you can
if you must you must and if we must we must
if you must you can and if you can you must
if we must you must and if you must you must
</construct>
<table confidence="0.938166333333333">
, GraSp produces the lexicon below.
Lexeme Initial Final Textbook
Category Category4 Category
must c1 c2\c1 NP\S
you c2 c2 NP
if c3 (c3/c1)/c1 (S/S)/S
and c4 (c3\c4)/c3 (S\S)/S
can c5 c2\c1 NP\S
we c6 c2 NP
</table>
<bodyText confidence="0.9968802">
As shown, training corpora can be manufactured
so as to produce lexical structure fairly similar to
what is found in CG textbooks. Such close
similarity is however not typical of &apos;naturalistic&apos;
learning sessions – as will be clear in section 2.
</bodyText>
<subsectionHeader confidence="0.986316">
1.5 Why categorial grammar?
</subsectionHeader>
<bodyText confidence="0.999964642857143">
In CG, all structural information is located in the
lexicon. Grammar rules (e.g. VP → Vt N) and
parts of speech (e.g. &apos;transitive verb&apos;, &apos;common
noun&apos;) are treated as variants of the same formal
kind. This reduces the dimensionality of the
logical learning space, since a CG-based learner
needs to induce just a single kind of structure.
Besides its formal elegance, the CG basis
accomodates a particular kind of cognitive
models, viz. those that reject the idea of separate
mental modules for lexical and grammatical
processing (e.g. Bates 1997). As we see it, our
formal approach allows us the luxury of not
taking sides in the heated debate of modularity.5
</bodyText>
<sectionHeader confidence="0.903427" genericHeader="method">
2 Learning from spoken language
</sectionHeader>
<bodyText confidence="0.9992985">
The current GraSp implementation completes a
learning session in about one hour when fed
with our main corpus.6 Such a session spans
2500-4000 iterations and delivers a lexicon rich
</bodyText>
<footnote confidence="0.538871461538462">
4 For perspicuity, two of the GraSped categories –
viz. &apos;can&apos;:(c2\c5)*(c5\c1) and &apos;we&apos;:(c2/c6)*c6 – are
replaced in the table by functional equivalents.
5 A caveat: Even if we do share some tools with other
CG-based NL learning programmes, our goals are
distinct, and our results do not compare easily with
e.g. Kanazawa (1994), Watkinson (2000). In terms of
philosophy, GraSp seems closer to connectionist
approaches to NLL.
6 The Danish corpus BySoc (person interviews). Size:
1.0 mio. words. Duration: 100 hours. Style: Labovian
interviews. Transcription: Enriched orthography.
Tagging: none. Ref.: http://www.cphling.dk/BySoc
</footnote>
<bodyText confidence="0.9999538">
in microparadigms and microstructure. Lexical
structure develops mainly around content words
while most function words retain their initial
category. The structure grown is almost fractal
in character with lots of inter-connected
categories, while the traditional large open
classes − nouns, verbs, prepositions, etc. − are
absent as such. The following sections present
some samples from the main corpus session
(Henrichsen 2000 has a detailed description).
</bodyText>
<subsectionHeader confidence="0.858">
2.1 Microparadigms
</subsectionHeader>
<bodyText confidence="0.547236">
{ &amp;quot;Den Franske&amp;quot;, &amp;quot;Nyboder&amp;quot;,
</bodyText>
<equation confidence="0.585978">
&amp;quot;Sølvgades&amp;quot;, &amp;quot;Krebses&amp;quot; }
</equation>
<bodyText confidence="0.984290666666667">
These four lexemes – or rather lexeme clusters –
chose to co-categorize. The collection does not
resemble a traditional syntactic paradigm, yet
the connection is quite clear: all four items
appeared in the training corpus as names of
primary schools.
</bodyText>
<table confidence="0.9981005">
Lexeme Initial Final
Category Category
Den c882 c882
Franske c1588 ((c882\c97)/c1588)*c1588
Nyboder c97 c97
Sølvgades c5351 (c97/c5351)*c5351
Krebses c3865 (c3865/c288)*c97
Skole c288 c97\c288
</table>
<bodyText confidence="0.995142076923077">
The final categories are superficially different,
but are easily seen to be functionally equivalent.
The same session delivered several other
microparadigms: a collection of family members
(in English translation: brother, grandfather,
younger-brother, stepfather, sister-in-law, etc.),
a class of negative polarity items, a class of mass
terms, a class of disjunctive operators, etc.
(Henrichsen 2000 6.4.2).
GraSp-paradigms are usually small and
almost always intuitively &apos;natural&apos; (not unlike the
small categories of L1 learners reported by e.g.
Lucariello 1985).
</bodyText>
<subsectionHeader confidence="0.991246">
2.2 Microgrammars
</subsectionHeader>
<bodyText confidence="0.99791775">
GraSp&apos;ed grammar rules are generally not of the
kind studied within traditional phrase structure
grammar. Still PSG-like &apos;islands&apos; do occur, in the
form of isolated networks of connected lexemes.
</bodyText>
<table confidence="0.999109842105263">
Lexeme Initial Final Con-
Category Category nection
Sankt c620 c620 +
c620
Sct. c4713 (c620/c4713)*c4713
Skt. c3301 (c620/c3301)*c3301
Annæ c3074 c620\(c22\c3074) c620 –
Josef c2921 c620\c2921
Joseph c3564 c620\c3564
Knuds c6122 c620\c6122
Pauls c1218 c620\c1218
Paulsgade c2927 c620\c2927
Pouls c2180 c620\c2180
Poulsgade c4707 c620\c4707
Pauls c1218 c620\c1218 +
c1218
Gade c3849 c1218\(c9\c3849) –
c1218
Plads c1263 c1218\(c22\c1263)
</table>
<bodyText confidence="0.996457666666667">
Centred around lexeme &apos;Pauls&apos;, a microgrammar
(of street names) has evolved almost directly
translatable into rewrite rules:7
</bodyText>
<equation confidence="0.983112375">
PP &apos;i&apos; N1 &apos;Gade&apos;
PP &apos;på&apos; N1 &apos;Plads&apos;
PP &apos;på&apos; N2
N1 X &apos;Pauls&apos;
N2 X &apos;Annæ&apos;
Nx X Y
X &apos;Sankt&apos;  |&apos;Skt.&apos;  |&apos;Sct.&apos;
Y &apos;Pauls&apos;  |&apos;Josef&apos;  |&apos;Joseph&apos;  |&apos;Knuds&apos;  |...
</equation>
<subsectionHeader confidence="0.978256">
2.3 Idioms and locutions
</subsectionHeader>
<bodyText confidence="0.987629090909091">
Consider the five utterances of the main corpus
containing the word &apos;rafle&apos; (cast-diceINF):8
det gør den der er ikke noget at rafle om der
der er ikke så meget at rafle om
der er ikke noget og rafle om
sætte sig ned og rafle lidt med fyrene der
at rafle om der
On most of its occurrences, &apos;rafle&apos; takes part in
the idiom &amp;quot;der er ikke noget/meget og/at rafle
om&amp;quot;, often followed by a resumptive &apos;der&apos;
(literally: there is not anything/much and/to
</bodyText>
<footnote confidence="0.959036857142857">
7 Lexemes &apos;Sankt&apos;, &apos;Sct.&apos;, and &apos;Skt.&apos; have in effect
cocategorized, since it holds that (x/y)*y =&gt; x. This
cocategorization is quite neat considering that GraSp
is blind to the interior of lexemes. c9 and c22 are the
categories of &apos;i&apos; (in) and &apos;på&apos; (on).
8 In writing, only two out of five would probably
qualify as syntactically well-formed sentences.
</footnote>
<bodyText confidence="0.8692705">
cast-diceINF about (there), meaning: this is not a
subject of negotiations). Lexeme &apos;ikke&apos; (category
c8) occurs in the left context of &apos;rafle&apos; more often
than not, and this fact is reflected in the final
category of &apos;rafle&apos;:
rafle: ((c12\(C8X(c5\(c7\c5808))))/c7)/c42
Similarly for the lexemes &apos;der&apos; (c7), &apos;er&apos; (c5), &apos;at&apos;
(c12), and &apos;om&apos; (c42) which are also present in the
argument structure of the category, while the top
functor is the initial &apos;rafle&apos; category (c5808).
The minimal context motivating the full
rafle category is:
... der ... er ... ikke ... at ... rafle ... om ... der ...
(&amp;quot;...&amp;quot; means that any amount and kind of
material may intervene). This template is a quite
accurate description of an acknowledged Danish
idiom.
Such idioms have a specific categorial
signature in the GraSped lexicon: a rich, but flat
argument structure (i.e. analyzed solely by 6R)
centered around a single low-frequency functor
(analyzed by 6L). Further examples with the
same signature:
... det ... kan ... man ... ikke ... fortænke ... i ...
... det ... vil ... blæse ... på ...
... ikke ... en ... kinamands ... chance ...
– all well-known Danish locutions.9
There are of course plenty of simpler and
faster algorithms available for extracting idioms.
Most such algorithms however include specific
knowledge about idioms (topological and
morphological patterns, concepts of mutual
information, heuristic and statistical rules, etc.).
Our algorithm has no such inclination: it does
not search for idioms, but merely finds them.
Observe also that GraSp may induce idiom
templates like the ones shown even from corpora
without a single verbatim occurrence.
9 For entry rafle, Danish-Danish dictionary Politiken
has this paradigmatic example: &amp;quot;Der er ikke noget at
rafle om&amp;quot;. Also fortænke, blæse, kinamands have
examples near-identical with the learned templates.
</bodyText>
<sectionHeader confidence="0.671372" genericHeader="method">
3 Learning from exotic corpora
</sectionHeader>
<bodyText confidence="0.999978166666667">
In order to test GraSp as a general purpose
learner we have used the algorithm on a range of
non-verbal data. We have had GraSp study
melodic patterns in musical scores and prosodic
patterns in spontaneous speech (and even dna-
structure of the banana fly). Results are not yet
conclusive, but encouraging (Henrichsen 2002).
When fed with HTML-formatted text,
GraSp delivers a lexical patchwork of linguistic
structure and HTML-structure. GraSp&apos;s
uncritical appetite for context-free structure
makes it a candidate for intelligent web-
crawling. We are preparing an experiment with a
large number of cloned learners to be let loose in
the internet, reporting back on the structure of
the documents they see. Since GraSp produces
formatting definitions as output (rather than
requiring it as input), the algorithm could save
the www-programmer the troubles of preparing
his web-crawler for this-and-that format.
Of course such experiments are side-issues.
However, as discussed in the next section,
learning from non-verbal sources may serve as
an inspiration in the L1 learning domain also.
</bodyText>
<sectionHeader confidence="0.80818" genericHeader="method">
4 Towards a model of L1 acquisition
</sectionHeader>
<subsectionHeader confidence="0.94907">
4.1 Artificial language learning
</subsectionHeader>
<bodyText confidence="0.998372724137931">
Training infants in language tasks within
artificial (i.e. semantically empty) languages is
an established psycho-linguistic method. Infants
have been shown able to extract structural
information – e.g. rules of phonemic
segmentation, prosodic contour, and even
abstract grammar (Cutler 1994, Gomez 1999,
Ellefson 2000) – from streams of carefully
designed nonsense. Such results are an important
source of inspiration for us, since the
experimental conditions are relatively easy to
simulate. We are conducting a series of &apos;retakes&apos;
with the GraSp learner in the subject&apos;s role.
Below we present an example.
In an often-quoted experiment, psychologist
Jenny Saffran and her team had eight-months-
old infants listening to continuous streams of
nonsense syllables: ti, do, pa, bu, la, go, etc.
Some streams were organized in three-syllable
&apos;words&apos; like padoti and golabu (repeated in
random order) while others consisted of the
same syllables in random order. After just two
minutes of listening, the subjects were able to
distinguish the two kinds of streams.
Conclusion: Infants can learn to identify
compound words on the basis of structural clues
alone, in a semantic vacuum.
Presented with similar streams of syllables,
the GraSp learner too discovers word-hood.
</bodyText>
<table confidence="0.996264666666667">
Lexeme Initial Final
Category Category10
pa c2 c2
do c1 (c2\c1)/c3
ti c3 c3
go c5 c5
la c6 c6
bu c4 c6\(c5\c4)
... ... ...
</table>
<bodyText confidence="0.998679">
It may be objected that such streams of
presegmented syllables do not represent the
experimental conditions faithfully, leaping over
the difficult task of segmentation. While we do
not yet have a definitive answer to this
objection, we observe that replacing &amp;quot;pa do ti go
la bu (..)&amp;quot; by &amp;quot;p a d o t i g o l a b u (..)&amp;quot; has the
GraSp learner discover syllable-hood and word-
hood on a par.11
</bodyText>
<subsectionHeader confidence="0.975608">
4.2 Naturalistic language learning
</subsectionHeader>
<bodyText confidence="0.999964111111111">
Even if human learners can demonstrably learn
structural rules without access to semantic and
pragmatic cues, this is certainly not the typical
L1 acquisition scenario. Our current learning
model fails to reflect the natural conditions in a
number of ways, being a purely syntactic
calculus working on symbolic input organized in
well-delimited strings. Natural learning, in
contrast, draws on far richer input sources:
</bodyText>
<listItem confidence="0.9994415">
• continuous (unsegmented) input streams
• suprasegmental (prosodic) information
• sensory data
• background knowledge
</listItem>
<bodyText confidence="0.939799894736842">
10 As seen, padoti has selected do for its functional
head, and golabu, bu. These choices are arbitrary.
11 The very influential Eimas (1971) showed one-
month-old infants to be able to distinguish /p/ and /b/.
Many follow-ups have established that phonemic
segmentation develops very early and may be innate.
Any model of first language acquisition must be
prepared to integrate such information sources.
Among these, the extra-linguistic sources are
perhaps the most challenging, since they
introduce a syntactic-semantic interface in the
model. As it seems, the formal simplicity of one-
dimensional learning (cf. sect. 1.5) is at stake.
If, however, semantic information (such as
sensory data) could be &apos;syntactified&apos; and included
in the lexical structure in a principled way,
single stratum learning could be regained. We
are currently working on a formal upgrading of
the calculus using a framework of constructive
type theory (Coquant 1988, Ranta 1994). In
CTT, the radical lexicalism of categorial
grammar is taken even a step further,
representing semantic information in the same
data structure as grammatical and lexical
information. This formal upgrading takes a
substantial refinement of the Dis function (cf.
sect. 1.3 E) as the determination of &apos;structural
disorder&apos; must now include contextual reasoning
(cf. Henrichsen 1998). We are pursuing a design
with s+ and s– as instructions to respectively
insert and search for information in a CTT-style
context.
These formal considerations are reflections
of our cognitive hypotheses. Our aim is to study
learning as a radically data-driven process
drawing on linguistic and extra-linguistic
information sources on a par – and we should
like our formal system to fit like a glove.
</bodyText>
<sectionHeader confidence="0.988809" genericHeader="conclusions">
5 Concluding remarks
</sectionHeader>
<bodyText confidence="0.979417227272727">
As far as we know, GraSp is the first published
algorithm for extracting grammatical taxonomy
out of untagged corpora of spoken language.12
This in an uneasy situation, since if our findings
are not comparable to those of other approaches
to grammar learning, how could our results be
judged − or falsified? Important issues wide
open to discussion are: validation of results,
psycho-linguistic relevance of the experimental
setup, principled ways of surpassing the context-
free limitations of Lambek grammar (inherited
in GraSp), just to mention a few.
On the other hand, already the spin-offs of
our project (the collection of non-linguistic
learners) do inspire confidence in our tenets, we
12 The learning experiment sketched in Moortgat
(2001) shares some of GraSp&apos;s features.
think – even if the big issue of psychological
realism has so far only just been touched.
The GraSp implementation referred to in this
paper is available for test runs at
http://www.id.cbs.dk/~pjuel/GraSp
</bodyText>
<sectionHeader confidence="0.99864" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999343975609756">
Bates, E.; J.C. Goodman (1997) On the Inseparability
of Grammar and the Lexicon: Evidence From
Acquisition, Aphasia, and Real-time Processing;
Language and Cognitive Processes 12, 507-584
Chomsky, N. (1980) Rules and Representations;
Columbia Univ. Press
Coquant, T.; G. Huet (1988) The Calculus of
Constructions; Info. &amp; Computation 76, 95-120
Cutler, A. (1994) Segmentation Problems, Rhythmic
Solutions; Lingua 92, 81-104
Eimas, P.D.; E.D. Siqueland; P.W. Jusczyk (1971)
Speech Perception in Infants; Science 171 303-306
Ellefson, M.R.; M.H.Christiansen (2000) Subjacency
Constraints Without Universal Grammar:
Evidence from Artificial Language Learning and
Connectionist Modelling; 22nd Ann. Conference of
the Cognitive Science Society, Erlbaum, 645-650
Gomez, R.L.; L.A. Gerken (1999) Artificial Gram-
mar Learning by 1-year-olds Leads to Specific and
Abstract Knowledge; Cognition 70 109-135
Henrichsen, P.J. (1998) Does the Sentence Exist? Do
We Need It?; in K. Korta et al. (eds) Discourse,
Interaction, and Communication; Kluwer Acad.
Henrichsen, P.J. (2000) Learning Within Grasp −
Interactive Investigations into the Grammar of
Speech; Ph.D., http://www.id.cbs.dk/~pjuel/GraSp
Henrichsen, P.J. (2002) GraSp: Grammar Learning
With a Healthy Appetite (in prep.)
Hoekstra, H. et al. (2000) Syntactic Annotation for
the Spoken Dutch Corpus Project; CLIN2000
Kanazawa (1994) Learnable Classes of CG; Ph.D.
Moortgat, M. (2001) Structural Equations in
Language Learning; 4th LACL2001 1-16
Nivre, J.; L. Grönqvist (2001) Tagging a Corpus of
Spoken Swedish: Int. Jn. of Corpus Ling. 6:1 47-78
Ranta, A. (1994) Type-Theoretical Grammar; Oxford
Saffran, J.R. et al. (1996) Statistical Learning By 8-
Months-Old Infants; Science 274 1926-1928
Watkinson S.; S. Manandhar (2000) Unsupervised
Lexical Learning with CG; in Cussens J. et al. (eds)
Learning Language in Logic; Springer
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.346745">
<title confidence="0.944285">GraSp: Grammar from unlabelled speech corpora</title>
<author confidence="0.999437">Peter Juel</author>
<affiliation confidence="0.997124666666667">Center for Computational Modelling of c/o Dept. of Computational Copenhagen Business</affiliation>
<address confidence="0.548038">Frederiksberg,</address>
<email confidence="0.663019">pjuel@id.cbs.dk</email>
<abstract confidence="0.995023027027027">greatly reduced given a structure of primitive linguistic constraints (&amp;quot;a highly restrictive schematism&amp;quot;, ibid.). It has however been very hard to establish independently the psychological reality of such a structure, and the question of innateness is still far from settled. While a decisive experiment may never be conceived, the issue could be addressed indirectly, e.g. by asking: Are innate principles parameters for grammar acquisition? Or rephrased in the spirit of constructive logic: Can a learning algorithm be devised that learns what the infant learns without incorporating specific linguistic axioms? The presentation of such an algorithm would certainly undermine arguments referring to the &apos;poverty of the stimulus&apos;, showing the innateness hypothesis to be dispensable. This paper presents our first try. Abstract This paper presents the ongoing project Computational Models of First Language Acquisition, together with its current the learning algorithm GraSp is designed specifically for inducing grammars from large, unlabelled corpora of spontaneous (i.e. unscripted) speech. The learning algorithm does not assume a predefined grammatical taxonomy; rather the determination of categories and their relations is considered as part of the learning task. While GraSp learning can be used for a range of practical tasks, the long-term goal of the project is to contribute to the debate of innate linguistic knowledge – under the hypothesis that there is no such.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Bates</author>
<author>J C Goodman</author>
</authors>
<title>On the Inseparability of Grammar and the Lexicon: Evidence From Acquisition, Aphasia, and Real-time Processing;</title>
<date>1997</date>
<journal>Language and Cognitive Processes</journal>
<volume>12</volume>
<pages>507--584</pages>
<marker>Bates, Goodman, 1997</marker>
<rawString>Bates, E.; J.C. Goodman (1997) On the Inseparability of Grammar and the Lexicon: Evidence From Acquisition, Aphasia, and Real-time Processing; Language and Cognitive Processes 12, 507-584</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Rules and Representations; Columbia Univ.</title>
<date>1980</date>
<publisher>Press</publisher>
<contexts>
<context position="2389" citStr="Chomsky 1980" startWordPosition="338" endWordPosition="339">rrent models of grammar learning assume a set of primitive linguistic categories and constraints, the learning process being modelled as category filling and rule instantiation – rather than category formation and rule creation. Arguably, distributing linguistic data over predefined categories and templates does not qualify as grammar &apos;learning&apos; in the strictest sense, but is better described as &apos;adjustment&apos; or &apos;adaptation&apos;. Indeed, Chomsky, the prime advocate of the hypothesis of innate linguistic principles, has claimed that &amp;quot;in certain fundamental respects we do not really learn language&amp;quot; (Chomsky 1980: 134). As Chomsky points out, the complexity of the learning task is 1 The essential algorithm 1.1 Psycho-linguistic preconditions Typical spontaneous speech is anything but syntactically &apos;well-formed&apos; in the Chomskyan sense of the word. right well let&apos;s er --= let&apos;s look at the applications - erm - let me just ask initially this -- I discussed it with er Reith er but we&apos;ll = have to go into it a bit further - is it is it within our erm er = are we free er to er draw up a rather = exiguous list - of people to interview (sample from the London-Lund corpus) Yet informal speech is not perceived </context>
</contexts>
<marker>Chomsky, 1980</marker>
<rawString>Chomsky, N. (1980) Rules and Representations; Columbia Univ. Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Coquant</author>
<author>G Huet</author>
</authors>
<title>The Calculus of Constructions;</title>
<date>1988</date>
<journal>Info. &amp; Computation</journal>
<volume>76</volume>
<pages>95--120</pages>
<marker>Coquant, Huet, 1988</marker>
<rawString>Coquant, T.; G. Huet (1988) The Calculus of Constructions; Info. &amp; Computation 76, 95-120</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cutler</author>
</authors>
<title>Segmentation Problems, Rhythmic Solutions;</title>
<date>1994</date>
<journal>Lingua</journal>
<volume>92</volume>
<pages>81--104</pages>
<contexts>
<context position="18167" citStr="Cutler 1994" startWordPosition="2956" endWordPosition="2957">er the troubles of preparing his web-crawler for this-and-that format. Of course such experiments are side-issues. However, as discussed in the next section, learning from non-verbal sources may serve as an inspiration in the L1 learning domain also. 4 Towards a model of L1 acquisition 4.1 Artificial language learning Training infants in language tasks within artificial (i.e. semantically empty) languages is an established psycho-linguistic method. Infants have been shown able to extract structural information – e.g. rules of phonemic segmentation, prosodic contour, and even abstract grammar (Cutler 1994, Gomez 1999, Ellefson 2000) – from streams of carefully designed nonsense. Such results are an important source of inspiration for us, since the experimental conditions are relatively easy to simulate. We are conducting a series of &apos;retakes&apos; with the GraSp learner in the subject&apos;s role. Below we present an example. In an often-quoted experiment, psychologist Jenny Saffran and her team had eight-monthsold infants listening to continuous streams of nonsense syllables: ti, do, pa, bu, la, go, etc. Some streams were organized in three-syllable &apos;words&apos; like padoti and golabu (repeated in random or</context>
</contexts>
<marker>Cutler, 1994</marker>
<rawString>Cutler, A. (1994) Segmentation Problems, Rhythmic Solutions; Lingua 92, 81-104</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Eimas</author>
<author>E D Siqueland</author>
<author>P W Jusczyk</author>
</authors>
<title>Speech Perception in Infants;</title>
<date>1971</date>
<journal>Science</journal>
<volume>171</volume>
<pages>303--306</pages>
<marker>Eimas, Siqueland, Jusczyk, 1971</marker>
<rawString>Eimas, P.D.; E.D. Siqueland; P.W. Jusczyk (1971) Speech Perception in Infants; Science 171 303-306</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Ellefson</author>
<author>M H Christiansen</author>
</authors>
<title>Subjacency Constraints Without Universal Grammar: Evidence from</title>
<date>2000</date>
<booktitle>Artificial Language Learning and Connectionist Modelling; 22nd Ann. Conference of the Cognitive Science Society, Erlbaum,</booktitle>
<pages>645--650</pages>
<marker>Ellefson, Christiansen, 2000</marker>
<rawString>Ellefson, M.R.; M.H.Christiansen (2000) Subjacency Constraints Without Universal Grammar: Evidence from Artificial Language Learning and Connectionist Modelling; 22nd Ann. Conference of the Cognitive Science Society, Erlbaum, 645-650</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L Gomez</author>
<author>L A Gerken</author>
</authors>
<title>Artificial Grammar Learning by 1-year-olds Leads to Specific and Abstract Knowledge;</title>
<date>1999</date>
<journal>Cognition</journal>
<volume>70</volume>
<pages>109--135</pages>
<marker>Gomez, Gerken, 1999</marker>
<rawString>Gomez, R.L.; L.A. Gerken (1999) Artificial Grammar Learning by 1-year-olds Leads to Specific and Abstract Knowledge; Cognition 70 109-135</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Henrichsen</author>
</authors>
<title>Does the Sentence Exist? Do We Need It?; in K. Korta et al. (eds) Discourse, Interaction, and Communication;</title>
<date>1998</date>
<publisher>Kluwer Acad.</publisher>
<contexts>
<context position="21564" citStr="Henrichsen 1998" startWordPosition="3492" endWordPosition="3493"> included in the lexical structure in a principled way, single stratum learning could be regained. We are currently working on a formal upgrading of the calculus using a framework of constructive type theory (Coquant 1988, Ranta 1994). In CTT, the radical lexicalism of categorial grammar is taken even a step further, representing semantic information in the same data structure as grammatical and lexical information. This formal upgrading takes a substantial refinement of the Dis function (cf. sect. 1.3 E) as the determination of &apos;structural disorder&apos; must now include contextual reasoning (cf. Henrichsen 1998). We are pursuing a design with s+ and s– as instructions to respectively insert and search for information in a CTT-style context. These formal considerations are reflections of our cognitive hypotheses. Our aim is to study learning as a radically data-driven process drawing on linguistic and extra-linguistic information sources on a par – and we should like our formal system to fit like a glove. 5 Concluding remarks As far as we know, GraSp is the first published algorithm for extracting grammatical taxonomy out of untagged corpora of spoken language.12 This in an uneasy situation, since if </context>
</contexts>
<marker>Henrichsen, 1998</marker>
<rawString>Henrichsen, P.J. (1998) Does the Sentence Exist? Do We Need It?; in K. Korta et al. (eds) Discourse, Interaction, and Communication; Kluwer Acad.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Henrichsen</author>
</authors>
<title>Learning Within Grasp − Interactive Investigations into the Grammar of Speech; Ph.D.,</title>
<date>2000</date>
<location>http://www.id.cbs.dk/~pjuel/GraSp</location>
<contexts>
<context position="5885" citStr="Henrichsen (2000)" startWordPosition="945" endWordPosition="946">uent. As the procedure is fairly simple, we just show a fragment of the proof. Notice that proofs read most easily bottom-up. c0 ––––– 6R c81 + c10 + c1 + c891 + =&gt; c0 6L ... 6L + c215 c81 c10 c1 c891 =&gt; c0 6L + c5 c81 c215 c10 c1 c891 =&gt; c0 6L ... c5 c81 c215 c10 c1 c891 =&gt; c0 In this proof there are no links, meaning that no grammatical structure was found. Later, when the lexicon has developed, the parser may 2 The calculus presented here is slightly simplified. Two rules are missing, and so is the reserved category T (&apos;noise&apos;) used e.g. for consequents (in place of c0 of the example). Cf. Henrichsen (2000). 3 By convention the indexing of category names reflects the frequency distribution: If word W has rank n in the training corpus, it is initialized as W:cn . AB =&gt; B A1 A A2 =&gt; C A1 A/B AB A2 =&gt; C A0 B =&gt; A /R A0 =&gt; A/B /L AB =&gt; B A1 A A2 =&gt; C A1 AB B\A A2 =&gt; C B A0 =&gt; A \R A0 =&gt; B\A \L *L *R A1 A B A2 =&gt; C A1 A*B A2 =&gt; C A1 =&gt; A A2 =&gt; B A1 A2 =&gt; A*B Classical part link 6 =&gt; 6 A, B, C are categories; Ax are (possibly empty) strings of categories. recognize more structure in the same input: [an utterance delimited by e.g. turntakes and pauses]. A corpus is a bag of soli [a transcript of l l a </context>
<context position="9288" citStr="Henrichsen (2000)" startWordPosition="1573" endWordPosition="1574"> ca ) = 1 Dis( ca/cc cb ca\cc =&gt; ca ) = 2 DIS(Lex,K) is the total amount of disorder in training corpus K wrt. lexicon Lex, i.e. the sum of Dis-values for all soli in K as mapped by Lex. F) A learning session is an iterative process. In each iteration i a suitable update Ui is applied in the lexicon Lexi–1 producing Lexi . Quantifying over all possible updates, Ui is picked so as to maximize the drop in disorder (DisDrop): DisDrop = DIS(Lexi–1,K) – DIS(Lexi,K) The session terminates when no suitable update remains. It is possible to GraSp efficiently and yet preserve logical completeness. See Henrichsen (2000) for discussion and demonstrations. 1.4 A staged learning session Given this tiny corpus of four soli (&apos;utterances&apos;) if you must you can if you must you must and if we must we must if you must you can and if you can you must if we must you must and if you must you must , GraSp produces the lexicon below. Lexeme Initial Final Textbook Category Category4 Category must c1 c2\c1 NP\S you c2 c2 NP if c3 (c3/c1)/c1 (S/S)/S and c4 (c3\c4)/c3 (S\S)/S can c5 c2\c1 NP\S we c6 c2 NP As shown, training corpora can be manufactured so as to produce lexical structure fairly similar to what is found in CG tex</context>
<context position="11971" citStr="Henrichsen 2000" startWordPosition="2006" endWordPosition="2007">s BySoc (person interviews). Size: 1.0 mio. words. Duration: 100 hours. Style: Labovian interviews. Transcription: Enriched orthography. Tagging: none. Ref.: http://www.cphling.dk/BySoc in microparadigms and microstructure. Lexical structure develops mainly around content words while most function words retain their initial category. The structure grown is almost fractal in character with lots of inter-connected categories, while the traditional large open classes − nouns, verbs, prepositions, etc. − are absent as such. The following sections present some samples from the main corpus session (Henrichsen 2000 has a detailed description). 2.1 Microparadigms { &amp;quot;Den Franske&amp;quot;, &amp;quot;Nyboder&amp;quot;, &amp;quot;Sølvgades&amp;quot;, &amp;quot;Krebses&amp;quot; } These four lexemes – or rather lexeme clusters – chose to co-categorize. The collection does not resemble a traditional syntactic paradigm, yet the connection is quite clear: all four items appeared in the training corpus as names of primary schools. Lexeme Initial Final Category Category Den c882 c882 Franske c1588 ((c882\c97)/c1588)*c1588 Nyboder c97 c97 Sølvgades c5351 (c97/c5351)*c5351 Krebses c3865 (c3865/c288)*c97 Skole c288 c97\c288 The final categories are superficially different, but </context>
</contexts>
<marker>Henrichsen, 2000</marker>
<rawString>Henrichsen, P.J. (2000) Learning Within Grasp − Interactive Investigations into the Grammar of Speech; Ph.D., http://www.id.cbs.dk/~pjuel/GraSp</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Henrichsen</author>
</authors>
<title>GraSp: Grammar Learning With a Healthy Appetite (in prep.)</title>
<date>2002</date>
<contexts>
<context position="17041" citStr="Henrichsen 2002" startWordPosition="2792" endWordPosition="2793"> even from corpora without a single verbatim occurrence. 9 For entry rafle, Danish-Danish dictionary Politiken has this paradigmatic example: &amp;quot;Der er ikke noget at rafle om&amp;quot;. Also fortænke, blæse, kinamands have examples near-identical with the learned templates. 3 Learning from exotic corpora In order to test GraSp as a general purpose learner we have used the algorithm on a range of non-verbal data. We have had GraSp study melodic patterns in musical scores and prosodic patterns in spontaneous speech (and even dnastructure of the banana fly). Results are not yet conclusive, but encouraging (Henrichsen 2002). When fed with HTML-formatted text, GraSp delivers a lexical patchwork of linguistic structure and HTML-structure. GraSp&apos;s uncritical appetite for context-free structure makes it a candidate for intelligent webcrawling. We are preparing an experiment with a large number of cloned learners to be let loose in the internet, reporting back on the structure of the documents they see. Since GraSp produces formatting definitions as output (rather than requiring it as input), the algorithm could save the www-programmer the troubles of preparing his web-crawler for this-and-that format. Of course such</context>
</contexts>
<marker>Henrichsen, 2002</marker>
<rawString>Henrichsen, P.J. (2002) GraSp: Grammar Learning With a Healthy Appetite (in prep.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hoekstra</author>
</authors>
<title>Syntactic Annotation for the Spoken Dutch Corpus Project;</title>
<date>2000</date>
<booktitle>CLIN2000</booktitle>
<publisher>Ph.D.</publisher>
<location>Kanazawa</location>
<contexts>
<context position="4395" citStr="Hoekstra (2000)" startWordPosition="667" endWordPosition="668"> strictly data-driven. This puts special demands on our parser which must be robust enough to accept input strings with little or no hints of syntactic structure (for the early stages of a learning session), while at the same time retaining the discriminating powers of a standard context free parser (for the later stages). Our solution is a sequent calculus, a variant of the Gentzen-Lambek categorial grammar formalism (L) enhanced with non-classical rules for isolating a residue of uninterpretable sequent elements. The classical part is identical to L (except that antecedents may be empty). 1 Hoekstra (2000) and Nivre (2001) discuss the annotation of spoken corpora with traditional tags. These seven rules capture the input parts that can be interpreted as syntactic constituents (examples below). For the remaining parts, we include two non-classical rules (6L and 6R).2 Non-classical part 6+ A1 A2 =&gt; C 6 6L –––––– 6R A1 6 A2 =&gt; C =&gt; 6 6 is a basic category. Ax are (possibly empty) strings of categories. Superscripts + – denote polarity of residual elements. By way of an example, consider the input string right well let&apos;s er let&apos;s look at the applications as analyzed in an early stage of a learning </context>
</contexts>
<marker>Hoekstra, 2000</marker>
<rawString>Hoekstra, H. et al. (2000) Syntactic Annotation for the Spoken Dutch Corpus Project; CLIN2000 Kanazawa (1994) Learnable Classes of CG; Ph.D.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Moortgat</author>
</authors>
<date>2001</date>
<booktitle>Structural Equations in Language Learning; 4th LACL2001</booktitle>
<pages>1--16</pages>
<marker>Moortgat, 2001</marker>
<rawString>Moortgat, M. (2001) Structural Equations in Language Learning; 4th LACL2001 1-16</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>L Grönqvist</author>
</authors>
<title>Tagging a Corpus of Spoken Swedish:</title>
<date>2001</date>
<journal>Int. Jn. of Corpus Ling.</journal>
<volume>6</volume>
<pages>47--78</pages>
<marker>Nivre, Grönqvist, 2001</marker>
<rawString>Nivre, J.; L. Grönqvist (2001) Tagging a Corpus of Spoken Swedish: Int. Jn. of Corpus Ling. 6:1 47-78</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ranta</author>
</authors>
<title>Type-Theoretical Grammar;</title>
<date>1994</date>
<journal>Science</journal>
<volume>274</volume>
<pages>1926--1928</pages>
<location>Oxford</location>
<contexts>
<context position="21182" citStr="Ranta 1994" startWordPosition="3436" endWordPosition="3437">ion must be prepared to integrate such information sources. Among these, the extra-linguistic sources are perhaps the most challenging, since they introduce a syntactic-semantic interface in the model. As it seems, the formal simplicity of onedimensional learning (cf. sect. 1.5) is at stake. If, however, semantic information (such as sensory data) could be &apos;syntactified&apos; and included in the lexical structure in a principled way, single stratum learning could be regained. We are currently working on a formal upgrading of the calculus using a framework of constructive type theory (Coquant 1988, Ranta 1994). In CTT, the radical lexicalism of categorial grammar is taken even a step further, representing semantic information in the same data structure as grammatical and lexical information. This formal upgrading takes a substantial refinement of the Dis function (cf. sect. 1.3 E) as the determination of &apos;structural disorder&apos; must now include contextual reasoning (cf. Henrichsen 1998). We are pursuing a design with s+ and s– as instructions to respectively insert and search for information in a CTT-style context. These formal considerations are reflections of our cognitive hypotheses. Our aim is to</context>
</contexts>
<marker>Ranta, 1994</marker>
<rawString>Ranta, A. (1994) Type-Theoretical Grammar; Oxford Saffran, J.R. et al. (1996) Statistical Learning By 8-Months-Old Infants; Science 274 1926-1928</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Watkinson</author>
<author>S Manandhar</author>
</authors>
<title>Unsupervised Lexical Learning with CG; in Cussens J. et al. (eds) Learning Language in Logic;</title>
<date>2000</date>
<publisher>Springer</publisher>
<marker>Watkinson, Manandhar, 2000</marker>
<rawString>Watkinson S.; S. Manandhar (2000) Unsupervised Lexical Learning with CG; in Cussens J. et al. (eds) Learning Language in Logic; Springer</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>