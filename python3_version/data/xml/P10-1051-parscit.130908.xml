<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.996833">
Minimized models and grammar-informed initialization
for supertagging with highly ambiguous lexicons
</title>
<author confidence="0.999094">
Sujith Ravi&apos; Jason Baldridge2 Kevin Knight&apos;
</author>
<affiliation confidence="0.8843">
&apos;University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
</affiliation>
<email confidence="0.998876">
{sravi,knight}@isi.edu
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999701875">
We combine two complementary ideas
for learning supertaggers from highly am-
biguous lexicons: grammar-informed tag
transitions and models minimized via in-
teger programming. Each strategy on its
own greatly improves performance over
basic expectation-maximization training
with a bitag Hidden Markov Model, which
we show on the CCGbank and CCG-TUT
corpora. The strategies provide further er-
ror reductions when combined. We de-
scribe a new two-stage integer program-
ming strategy that efficiently deals with
the high degree of ambiguity on these
datasets while obtaining the full effect of
model minimization.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999781590909091">
Creating accurate part-of-speech (POS) taggers
using a tag dictionary and unlabeled data is an
interesting task with practical applications. It
has been explored at length in the literature since
Merialdo (1994), though the task setting as usu-
ally defined in such experiments is somewhat arti-
ficial since the tag dictionaries are derived from
tagged corpora. Nonetheless, the methods pro-
posed apply to realistic scenarios in which one
has an electronic part-of-speech tag dictionary or
a hand-crafted grammar with limited coverage.
Most work has focused on POS-tagging for
English using the Penn Treebank (Marcus et al.,
1993), such as (Banko and Moore, 2004; Gold-
water and Griffiths, 2007; Toutanova and John-
son, 2008; Goldberg et al., 2008; Ravi and Knight,
2009). This generally involves working with the
standard set of 45 POS-tags employed in the Penn
Treebank. The most ambiguous word has 7 dif-
ferent POS tags associated with it. Most methods
have employed some variant of Expectation Max-
imization (EM) to learn parameters for a bigram
</bodyText>
<affiliation confidence="0.981657">
2Department of Linguistics
The University of Texas at Austin
</affiliation>
<address confidence="0.55459">
Austin, Texas 78712
</address>
<email confidence="0.99366">
jbaldrid@mail.utexas.edu
</email>
<bodyText confidence="0.999780763157895">
or trigram Hidden Markov Model (HMM). Ravi
and Knight (2009) achieved the best results thus
far (92.3% word token accuracy) via a Minimum
Description Length approach using an integer pro-
gram (IP) that finds a minimal bigram grammar
that obeys the tag dictionary constraints and cov-
ers the observed data.
A more challenging task is learning supertag-
gers for lexicalized grammar formalisms such as
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000). For example, CCGbank (Hocken-
maier and Steedman, 2007) contains 1241 dis-
tinct supertags (lexical categories) and the most
ambiguous word has 126 supertags. This pro-
vides a much more challenging starting point
for the semi-supervised methods typically ap-
plied to the task. Yet, this is an important task
since creating grammars and resources for CCG
parsers for new domains and languages is highly
labor- and knowledge-intensive. Baldridge (2008)
uses grammar-informed initialization for HMM
tag transitions based on the universal combinatory
rules of the CCG formalism to obtain 56.1% accu-
racy on ambiguous word tokens, a large improve-
ment over the 33.0% accuracy obtained with uni-
form initialization for tag transitions.
The strategies employed in Ravi and Knight
(2009) and Baldridge (2008) are complementary.
The former reduces the model size globally given
a data set, while the latter biases bitag transitions
toward those which are more likely based on a uni-
versal grammar without reference to any data. In
this paper, we show how these strategies may be
combined straightforwardly to produce improve-
ments on the task of learning supertaggers from
lexicons that have not been filtered in any way.1
We demonstrate their cross-lingual effectiveness
on CCGbank (English) and the Italian CCG-TUT
</bodyText>
<footnote confidence="0.997049666666667">
1See Banko and Moore (2004) for a description of how
many early POS-tagging papers in fact used a number of
heuristic cutoffs that greatly simplify the problem.
</footnote>
<page confidence="0.955933">
495
</page>
<note confidence="0.9487285">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 495–503,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9888304">
corpus (Bos et al., 2009). We find a consistent im-
proved performance by using each of the methods
compared to basic EM, and further improvements
by using them in combination.
Applying the approach of Ravi and Knight
(2009) naively to CCG supertagging is intractable
due to the high level of ambiguity. We deal with
this by defining a new two-stage integer program-
ming formulation that identifies minimal gram-
mars efficiently and effectively.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.997574878787879">
CCGbank. CCGbank was created by semi-
automatically converting the Penn Treebank to
CCG derivations (Hockenmaier and Steedman,
2007). We use the standard splits of the data
used in semi-supervised tagging experiments (e.g.
Banko and Moore (2004)): sections 0-18 for train-
ing, 19-21 for development, and 22-24 for test.
CCG-TUT. CCG-TUT was created by semi-
automatically converting dependencies in the Ital-
ian Turin University Treebank to CCG deriva-
tions (Bos et al., 2009). It is much smaller than
CCGbank, with only 1837 sentences. It is split
into three sections: newspaper texts (NPAPER),
civil code texts (CIVIL), and European law texts
from the JRC-Acquis Multilingual Parallel Corpus
(JRC). For test sets, we use the first 400 sentences
of NPAPER, the first 400 of CIVIL, and all of JRC.
This leaves 409 and 498 sentences from NPAPER
and CIVIL, respectively, for training (to acquire a
lexicon and run EM). For evaluation, we use two
different settings of train/test splits:
TEST 1 Evaluate on the NPAPER section of test
using a lexicon extracted only from NPAPER
section of train.
TEST 2 Evaluate on the entire test using lexi-
cons extracted from (a) NPAPER + CIVIL,
(b) NPAPER, and (c) CIVIL.
Table 1 shows statistics for supertag ambiguity
in CCGbank and CCG-TUT. As a comparison, the
POS word token ambiguity in CCGbank is 2.2: the
corresponding value of 18.71 for supertags is in-
dicative of the (challenging) fact that supertag am-
biguity is greatest for the most frequent words.
</bodyText>
<sectionHeader confidence="0.976792" genericHeader="method">
3 Grammar informed initialization for
supertagging
</sectionHeader>
<bodyText confidence="0.9357645">
Part-of-speech tags are atomic labels that in and of
themselves encode no internal structure. In con-
</bodyText>
<table confidence="0.998084333333333">
Data Distinct Max Type ambig Tok ambig
CCGbank 1241 126 1.69 18.71
CCG-TUT
NPAPER+CIVIL 849 64 1.48 11.76
NPAPER 644 48 1.42 12.17
CIVIL 486 39 1.52 11.33
</table>
<tableCaption confidence="0.995859">
Table 1: Statistics for the training data used to ex-
</tableCaption>
<bodyText confidence="0.952766392857143">
tract lexicons for CCGbank and CCG-TUT. Dis-
tinct: # of distinct lexical categories; Max: # of
categories for the most ambiguous word; Type
ambig: per word type category ambiguity; Tok
ambig: per word token category ambiguity.
trast, supertags are detailed, structured labels; a
universal set of grammatical rules defines how cat-
egories may combine with one another to project
syntactic structure.2 Because of this, properties of
the CCG formalism itself can be used to constrain
learning—prior to considering any particular lan-
guage, grammar or data set. Baldridge (2008) uses
this observation to create grammar-informed tag
transitions for a bitag HMM supertagger based on
two main properties. First, categories differ in
their complexity and less complex categories tend
to be used more frequently. For example, two cat-
egories for buy in CCGbank are (S[dcl]\NP)/NP
and ((((S[b]\NP)/PP)/PP)/(S[adj]\NP))/NP; the
former occurs 33 times, the latter once. Second,
categories indicate the form of categories found
adjacent to them; for example, the category for
sentential complement verbs ((S\NP)/S) expects
an NP to its left and an S to its right.
Categories combine via rules such as applica-
tion and composition (see Steedman (2000) for de-
tails). Given a lexicon containing the categories
for each word, these allow derivations like:
</bodyText>
<equation confidence="0.963462833333333">
Ed might see a cat
NP (S\NP)/(S\NP) (S\NP)/NP NP/N N
&gt;B &gt;
(S\NP)/NP NP
&gt;
&gt;
</equation>
<bodyText confidence="0.996835">
Other derivations are possible. In fact, every pair
of adjacent words above may be combined di-
rectly. For example, see and a may combine
through forward composition to produce the cate-
gory (S\NP)/N, and Ed’s category may type-raise
to S/(S\NP) and compose with might’s category.
Baldridge uses these properties to define tag
</bodyText>
<footnote confidence="0.99841475">
2Note that supertags can be lexical categories of CCG
(Steedman, 2000), elementary trees of Tree-adjoining Gram-
mar (Joshi, 1988), or types in a feature hierarchy as in Head-
driven Phrase Structure Grammar (Pollard and Sag, 1994).
</footnote>
<equation confidence="0.658687">
S\NP
S
</equation>
<page confidence="0.991034">
496
</page>
<bodyText confidence="0.999898789473684">
transition distributions that have higher likeli-
hood for simpler categories that are able to
combine. For example, for the distribution
p(ti|ti_1=NP), (S\NP)\NP is more likely than
((S\NP)/(N/N))\NP because both categories may
combine with a preceding NP but the former is
simpler. In turn, the latter is more likely than NP: it
is more complex but can combine with the preced-
ing NP. Finally, NP is more likely than (S/NP)/NP
since neither can combine, but NP is simpler.
By starting EM with these tag transition dis-
tributions and an unfiltered lexicon (word-to-
supertag dictionary), Baldridge obtains a tagging
accuracy of 56.1% on ambiguous words—a large
improvement over the accuracy of 33.0% obtained
by starting with uniform transition distributions.
We refer to a model learned from basic EM (uni-
formly initialized) as EM, and to a model with
grammar-informed initialization as EMGI.
</bodyText>
<sectionHeader confidence="0.99445" genericHeader="method">
4 Minimized models for supertagging
</sectionHeader>
<bodyText confidence="0.99997146875">
The idea of searching for minimized models is
related to classic Minimum Description Length
(MDL) (Barron et al., 1998), which seeks to se-
lect a small model that captures the most regularity
in the observed data. This modeling strategy has
been shown to produce good results for many nat-
ural language tasks (Goldsmith, 2001; Creutz and
Lagus, 2002; Ravi and Knight, 2009). For tagging,
the idea has been implemented using Bayesian
models with priors that indirectly induce sparsity
in the learned models (Goldwater and Griffiths,
2007); however, Ravi and Knight (2009) show a
better approach is to directly minimize the model
using an integer programming (IP) formulation.
Here, we build on this idea for supertagging.
There are many challenges involved in using IP
minimization for supertagging. The 1241 distinct
supertags in the tagset result in 1.5 million tag bi-
gram entries in the model and the dictionary con-
tains almost 3.5 million word/tag pairs that are rel-
evant to the test data. The set of 45 POS tags for
the same data yields 2025 tag bigrams and 8910
dictionary entries. We also wish to scale our meth-
ods to larger data settings than the 24k word tokens
in the test data used in the POS tagging task.
Our objective is to find the smallest supertag
grammar (of tag bigram types) that explains the
entire text while obeying the lexicon’s constraints.
However, the original IP method of Ravi and
Knight (2009) is intractable for supertagging, so
we propose a new two-stage method that scales to
the larger tagsets and data involved.
</bodyText>
<subsectionHeader confidence="0.963523">
4.1 IP method for supertagging
</subsectionHeader>
<bodyText confidence="0.999785511111111">
Our goal for supertagging is to build a minimized
model with the following objective:
IPoriginal: Find the smallest supertag gram-
mar (i.e., tag bigrams) that can explain the en-
tire text (the test word token sequence).
Using the full grammar and lexicon to perform
model minimization results in a very large, diffi-
cult to solve integer program involving billions of
variables and constraints. This renders the mini-
mization objective IPoriginal intractable. One way
of combating this is to use a reduced grammar
and lexicon as input to the integer program. We
do this without further supervision by using the
HMM model trained using basic EM: entries are
pruned based on the tag sequence it predicts on
the test data. This produces an observed grammar
of distinct tag bigrams (Gobs) and lexicon of ob-
served lexical assignments (Lobs). For CCGbank,
Gobs and Lobs have 12,363 and 18,869 entries,
respectively—far less than the millions of entries
in the full grammar and lexicon.
Even though EM minimizes the model some-
what, many bad entries remain in the grammar.
We prune further by supplying Gobs and Lobs as
input (G, L) to the IP-minimization procedure.
However, even with the EM-reduced grammar and
lexicon, the IP-minimization is still very hard to
solve. We thus split it into two stages. The first
stage (Minimization 1) finds the smallest grammar
Gmin1 C G that explains the set of word bigram
types observed in the data rather than the word
sequence itself, and the second (Minimization 2)
finds the smallest augmentation of Gmin1 that ex-
plains the full word sequence.
Minimization 1 (MIN1). We begin with a sim-
pler minimization problem than the original one
(IPoriginal), with the following objective:
IPmin 1: Find the smallest set of tag bigrams
Gmin1 C G, such that there is at least one
tagging assignment possible for every word bi-
gram type observed in the data.
We formulate this as an integer program, creat-
ing binary variables gvari for every tag bigram
gi = titk in G. Binary link variables connect tag
bigrams with word bigrams; these are restricted
</bodyText>
<page confidence="0.997284">
497
</page>
<figureCaption confidence="0.999544">
Figure 1: Two-stage IP method for selecting minimized models for supertagging.
</figureCaption>
<figure confidence="0.97766203125">
IP Minimization 1
word bigrams: Input w1 w2 Grammar (G) word bigrams: Input Grammar (G)
: w1 w2 :
w2 w3 :
w2 w3 :
: ti tj
: ti tj
: : MIN 1 : :
wi wj : wi wj
:
:
:
IP Minimization 2
tag bigrams chosen in first minimization step (Gmin1l tag bigrams chosen in second minimization step (Gmin2l
(does not explain the word sequence)
word sequence: w1 w2 w3 w4 w5
t1
t2
t3
:
:
tk
supertags
MIN 2
supertags
word sequence: w1 w2 w3 w4 w5
t1
t2
t3
:
:
tk
</figure>
<bodyText confidence="0.9991354375">
to the set of links that respect the lexicon L pro-
vided as input, i.e., there exists a link variable
linkjklm connecting tag bigram tjtk with word bi-
gram wlwm only if the word/tag pairs (wl, tj) and
(wm, tk) are present in L. The entire integer pro-
gramming formulation is shown Figure 2.
The IP solver3 solves the above integer program
and we extract the set of tag bigrams Gmin1 based
on the activated grammar variables. For the CCG-
bank test data, MIN1 yields 2530 tag bigrams.
However, a second stage is needed since there is
no guarantee that Gmin1 can explain the test data:
it contains tags for all word bigram types, but it
cannot necessarily tag the full word sequence. Fig-
ure 1 illustrates this. Using only tag bigrams from
MIN1 (shown in blue), there is no fully-linked tag
path through the network. There are missing links
between words w2 and w3 and between words w3
and w4 in the word sequence. The next stage fills
in these missing links.
Minimization 2 (MIN2). This stage uses the
original minimization formulation for the su-
pertagging problem IPGrional, again using an in-
teger programming method similar to that pro-
posed by Ravi and Knight (2009). If applied to
the observed grammar GGbs, the resulting integer
program is hard to solve.4 However, by using the
partial solution Gmin1 obtained in MIN1 the IP
optimization speeds up considerably. We imple-
ment this by fixing the values of all binary gram-
mar variables present in Gmin1 to 1 before opti-
mization. This reduces the search space signifi-
</bodyText>
<footnote confidence="0.997926">
3We use the commercial CPLEX solver.
4The solver runs for days without returning a solution.
</footnote>
<equation confidence="0.51025">
Minimize:gvari
Vgi EG
</equation>
<subsectionHeader confidence="0.550976">
Subject to constraints:
</subsectionHeader>
<listItem confidence="0.732118">
1. For every word bigram wlwm, there exists at least
one tagging that respects the lexicon L.
</listItem>
<equation confidence="0.508936">
EV tjEL(wl), tkEL(w_) linkjklm &gt; 1
</equation>
<bodyText confidence="0.789891875">
where L(wl) and L(wm) represent the set of tags seen
in the lexicon for words wl and wm respectively.
2. The link variable assignments are constrained to re-
spect the grammar variables chosen by the integer pro-
gram.
linkjklm &lt; gvari
where gvari is the binary variable corresponding to tag
bigram tjtk in the grammar G.
</bodyText>
<figureCaption confidence="0.992686">
Figure 2: IP formulation for Minimization 1.
</figureCaption>
<bodyText confidence="0.980009066666667">
cantly, and CPLEX finishes in just a few hours.
The details of this method are described below.
We instantiate binary variables gvari and lvari
for every tag bigram (in G) and lexicon entry (in
L). We then create a network of possible taggings
for the word token sequence w1w2....wn in the
corpus and assign a binary variable to each link
in the network. We name these variables linkpjk,
where c indicates the column of the link’s source
in the network, and j and k represent the link’s
source and destination (i.e., linkpjk corresponds to
tag bigram tjtk in column c). Next, we formulate
the integer program given in Figure 3.
Figure 1 illustrates how MIN2 augments the
grammar Gmin1 (links shown in blue) with addi-
</bodyText>
<page confidence="0.991592">
498
</page>
<sectionHeader confidence="0.29428" genericHeader="method">
Minimize: EvgiEG gvari
</sectionHeader>
<subsectionHeader confidence="0.460519">
Subject to constraints:
</subsectionHeader>
<listItem confidence="0.819541">
1. Chosen link variables form a left-to-right path
through the tagging network.
Vc_1..n−2Vk Ej linkcjk = Ej link�c+1)kj
2. Link variable assignments should respect the chosen
grammar variables.
</listItem>
<bodyText confidence="0.986263">
for every link: linkcjk &lt; gvari
where gvari corresponds to tag bigram tjtk
</bodyText>
<listItem confidence="0.975211">
3. Link variable assignments should respect the chosen
lexicon variables.
</listItem>
<bodyText confidence="0.846572428571429">
for every link: linkcjk &lt; lvarwctj
for every link: linkcjk &lt; lvarwc+1tk
where wc is the cth word in the word sequence w1...wn,
and lvarwctj is the binary variable corresponding to the
word/tag pair wc/tj in the lexicon L.
4. The final solution should produce at least one com-
plete tagging path through the network.
</bodyText>
<figure confidence="0.68452">
E
vj,k link1jk &gt; 1
5. Provide minimized grammar from MIN1as partial
solution to the integer program.
VgiEGmin1 gvari = 1
</figure>
<figureCaption confidence="0.996214">
Figure 3: IP formulation for Minimization 2.
</figureCaption>
<bodyText confidence="0.999951066666667">
tional tag bigrams (shown in red) to form a com-
plete tag path through the network. The minimized
grammar set in the final solution Gmin2 contains
only 2810 entries, significantly fewer than the
original grammar Gobs’s 12,363 tag bigrams.
We note that the two-stage minimization pro-
cedure proposed here is not guaranteed to yield
the optimal solution to our original objective
IPoriginal. On the simpler task of unsupervised
POS tagging with a dictionary, we compared
our method versus directly solving IPoriginal and
found that the minimization (in terms of grammar
size) achieved by our method is close to the opti-
mal solution for the original objective and yields
the same tagging accuracy far more efficiently.
Fitting the minimized model. The IP-
minimization procedure gives us a minimal
grammar, but does not fit the model to the data.
In order to estimate probabilities for the HMM
model for supertagging, we use the EM algorithm
but with certain restrictions. We build the transi-
tion model using only entries from the minimized
grammar set Gmin2, and instantiate an emission
model using the word/tag pairs seen in L (pro-
vided as input to the minimization procedure). All
the parameters in the HMM model are initialized
with uniform probabilities, and we run EM for 40
iterations. The trained model is used to find the
Viterbi tag sequence for the corpus. We refer to
this model (where the EM output (Gobs, Lobs) was
provided to the IP-minimization as initial input)
as EM+IP.
Bootstrapped minimization. The quality of the
observed grammar and lexicon improves consid-
erably at the end of a single EM+IP run. Ravi
and Knight (2009) exploited this to iteratively im-
prove their POS tag model: since the first mini-
mization procedure is seeded with a noisy gram-
mar and tag dictionary, iterating the IP procedure
with progressively better grammars further im-
proves the model. We do likewise, bootstrapping a
new EM+IP run using as input, the observed gram-
mar Gobs and lexicon Lobs from the last tagging
output of the previous iteration. We run this until
the chosen grammar set Gmin2 does not change.5
</bodyText>
<subsectionHeader confidence="0.855575">
4.2 Minimization with grammar-informed
initialization
</subsectionHeader>
<bodyText confidence="0.999052833333333">
There are two complementary ways to use
grammar-informed initialization with the IP-
minimization approach: (1) using EMGI output
as the starting grammar/lexicon and (2) using the
tag transitions directly in the IP objective function.
The first takes advantage of the earlier observation
that the quality of the grammar and lexicon pro-
vided as initial input to the minimization proce-
dure can affect the quality of the final supertagging
output. For the second, we modify the objective
function used in the two IP-minimization steps to
be:
</bodyText>
<equation confidence="0.9509395">
�Minimize: wi · gvari (1)
dgiEG
</equation>
<bodyText confidence="0.9998484">
where, G is the set of tag bigrams provided as in-
put to IP, gvari is a binary variable in the integer
program corresponding to tag bigram (ti_1, ti) E
G, and wi is negative logarithm of pgii(ti ti_1)
as given by Baldridge (2008).6 All other parts of
</bodyText>
<footnote confidence="0.996386">
5In our experiments, we run three bootstrap iterations.
6Other numeric weights associated with the tag bi-
grams could be considered, such as 0/1 for uncombin-
</footnote>
<page confidence="0.999084">
499
</page>
<bodyText confidence="0.999954727272727">
the integer program including the constraints re-
main unchanged, and, we acquire a final tagger in
the same manner as described in the previous sec-
tion. In this way, we combine the minimization
and GI strategies into a single objective function
that finds a minimal grammar set while keeping
the more likely tag bigrams in the chosen solution.
EMGI+IPGI is used to refer to the method that
uses GI information in both ways: EMGI output
as the starting grammar/lexicon and GI weights in
the IP-minimization objective.
</bodyText>
<sectionHeader confidence="0.999608" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.998480586206896">
We compare the four strategies described in Sec-
tions 3 and 4, summarized below:
EM HMM uniformly initialized, EM training.
EM+IP IP minimization using initial grammar
provided by EM.
EMGI HMM with grammar-informed initializa-
tion, EM training.
EMGI+IPGI IP minimization using initial gram-
mar/lexicon provided by EMGI and addi-
tional grammar-informed IP objective.
For EM+IP and EMGI+IPGI, the minimization
and EM training processes are iterated until the
resulting grammar and lexicon remain unchanged.
Forty EM iterations are used for all cases.
We also include a baseline which randomly
chooses a tag from those associated with each
word in the lexicon, averaged over three runs.
Accuracy on ambiguous word tokens. We
evaluate the performance in terms of tagging accu-
racy with respect to gold tags for ambiguous words
in held-out test sets for English and Italian. We
consider results with and without punctuation.7
Recall that unlike much previous work, we do
not collect the lexicon (tag dictionary) from the
test set: this means the model must handle un-
known words and the possibility of having missing
lexical entries for covering the test set.
Precision and recall of grammar and lexicon.
In addition to accuracy, we measure precision and
</bodyText>
<footnote confidence="0.965106428571429">
able/combinable bigrams.
7The reason for this is that the “categories” for punctua-
tion in CCGbank are for the most part not actual categories;
for example, the period “.” has the categories “.” and “S”.
As such, these supertags are outside of the categorial system:
their use in derivations requires phrase structure rules that are
not derivable from the CCG combinatory rules.
</footnote>
<table confidence="0.994575">
Model ambig ambig all all
-punc -punc
Random 17.9 16.2 27.4 21.9
EM 38.7 35.6 45.6 39.8
EM+IP 52.1 51.0 57.3 53.9
EMGI 56.3 59.4 61.0 61.7
EMGI+IPGI 59.6 62.3 63.8 64.3
</table>
<tableCaption confidence="0.979157333333333">
Table 2: Supertagging accuracy for CCGbank sec-
tions 22-24. Accuracies are reported for four
settings—(1) ambiguous word tokens in the test
corpus, (2) ambiguous word tokens, ignoring
punctuation, (3) all word tokens, and (4) all word
tokens except punctuation.
</tableCaption>
<bodyText confidence="0.998907142857143">
recall for each model on the observed bitag gram-
mar and observed lexicon on the test set. We cal-
culate them as follows, for an observed grammar
or lexicon X:
This provides a measure of model performance on
bitag types for the grammar and lexical entry types
for the lexicon, rather than tokens.
</bodyText>
<subsectionHeader confidence="0.984418">
5.1 English CCGbank results
</subsectionHeader>
<bodyText confidence="0.999953913043478">
Accuracy on ambiguous tokens. Table 2 gives
performance on the CCGbank test sections. All
models are well above the random baseline, and
both of the strategies individually boost perfor-
mance over basic EM by a large margin. For the
models using GI, accuracy ignoring punctuation is
higher than for all almost entirely due to the fact
that “.” has the supertags “.” and S, and the GI
gives a preference to S since it can in fact combine
with other categories, unlike “.”—the effect is that
nearly every sentence-final period (˜5.5k tokens) is
tagged S rather than “.”.
EMGI is more effective than EM+IP; however,
it should be kept in mind that IP-minimization
is a general technique that can be applied to
any sequence prediction task, whereas grammar-
informed initialization may be used only with
tasks in which the interactions of adjacent labels
may be derived from the labels themselves. In-
terestingly, the gap between the two approaches
is greater when punctuation is ignored (51.0 vs.
59.4)—this is unsurprising because, as noted al-
ready, punctuation supertags are not actual cate-
</bodyText>
<equation confidence="0.99418925">
Precision = |{X} ∩ {Observedgold}|
|{X}|
Recall = |{X} ∩ {Observedgold}|
|{Observedgold}|
</equation>
<page confidence="0.980337">
500
</page>
<table confidence="0.999768142857143">
EM EM+IP EMGI EMGI+IPGI
Grammar
Precision 7.5 32.9 52.6 68.1
Recall 26.9 13.2 34.0 19.8
Lexicon
Precision 58.4 63.0 78.0 80.6
Recall 50.9 56.0 71.5 67.6
</table>
<tableCaption confidence="0.999255">
Table 3: Comparison of grammar/lexicon ob-
</tableCaption>
<bodyText confidence="0.976835184210526">
served in the model tagging vs. gold tagging
in terms of precision and recall measures for su-
pertagging on CCGbank data.
gories, so EMGI is unable to model their distribu-
tion. Most importantly, the complementary effects
of the two approaches can be seen in the improved
results for EMGI+IPGI, which obtains about 3%
better accuracy than EMGI.
Accuracy on all tokens. Table 2 also gives per-
formance when taking all tokens into account. The
HMM when using full supervision obtains 87.6%
accuracy (Baldridge, 2008),8 so the accuracy of
63.8% achieved by EMGI+IPGI nearly halves the
gap between the supervised model and the 45.6%
obtained by basic EM semi-supervised model.
Effect of GI information in EM and/or IP-
minimization stages. We can also consider the
effect of GI information in either EM training or
IP-minimization to see whether it can be effec-
tively exploited in both. The latter, EM+IPGI,
obtains 53.2/51.1 for all/no-punc—a small gain
compared to EM+IP’s 52.1/51.0. The former,
EMGI+IP, obtains 58.9/61.6—a much larger gain.
Thus, the better starting point provided by EMGI
has more impact than the integer program that in-
cludes GI in its objective function. However, we
note that it should be possible to exploit the GI
information more effectively in the integer pro-
gram than we have here. Also, our best model,
EMGI+IPGI, uses GI information in both stages
to obtain our best accuracy of 59.6/62.3.
P/R for grammars and lexicons. We can ob-
tain a more-fine grained understanding of how the
models differ by considering the precision and re-
call values for the grammars and lexicons of the
different models, given in Table 3. The basic EM
model has very low precision for the grammar, in-
dicating it proposes many unnecessary bitags; it
</bodyText>
<footnote confidence="0.839989">
8A state-of-the-art, fully-supervised maximum entropy
tagger (Clark and Curran, 2007) (which also uses part-of-
speech labels) obtains 91.4% on the same train/test split.
</footnote>
<bodyText confidence="0.999912044444444">
achieves better recall because of the sheer num-
ber of bitags it proposes (12,363). EM+IP prunes
that set of bitags considerably, leading to better
precision at the cost of recall. EMGI’s higher re-
call and precision indicate the tag transition dis-
tributions do capture general patterns of linkage
between adjacent CCG categories, while EM en-
sures that the data filters out combinable, but un-
necessary, bitags. With EMGI+IPGI, we again
see that IP-minimization prunes even more entries,
improving precision at the loss of some recall.
Similar trends are seen for precision and recall
on the lexicon. IP-minimization’s pruning of inap-
propriate taggings means more common words are
not assigned highly infrequent supertags (boosting
precision) while unknown words are generally as-
signed more sensible supertags (boosting recall).
EMGI again focuses taggings on combinable con-
texts, boosting precision and recall similarly to
EM+IP, but in greater measure. EMGI+IPGI then
prunes some of the spurious entries, boosting pre-
cision at some loss of recall.
Tag frequencies predicted on the test set. Ta-
ble 4 compares gold tags to tags generated by
all four methods for the frequent and highly am-
biguous words the and in. Basic EM wanders
far away from the gold assignments; it has little
guidance in the very large search space available
to it. IP-minimization identifies a smaller set of
tags that better matches the gold tags; this emerges
because other determiners and prepositions evoke
similar, but not identical, supertags, and the gram-
mar minimization pushes (but does not force)
them to rely on the same supertags wherever pos-
sible. However, the proportions are incorrect;
for example, the tag assigned most frequently to
in is ((S\NP)\(S\NP))/NP though (NP\NP)/NP
is more frequent in the test set. EMGI’s tags
correct that balance and find better proportions,
but also some less common categories, such as
(((N/N)\(N/N))\((N/N)\(N/N)))/N, sneak in be-
cause they combine with frequent categories like
N/N and N. Bringing the two strategies together
with EMGI+IPGI filters out the unwanted cate-
gories while getting better overall proportions.
</bodyText>
<subsectionHeader confidence="0.983476">
5.2 Italian CCG-TUT results
</subsectionHeader>
<bodyText confidence="0.9998435">
To demonstrate that both methods and their com-
bination are language independent, we apply them
to the Italian CCG-TUT corpus. We wanted
to evaluate performance out-of-the-box because
</bodyText>
<page confidence="0.989402">
501
</page>
<table confidence="0.999776785714286">
Lexicon Gold EM EM+IP EMGI EMGI+IPGI
the → (41 distinct tags in Ltrain) (14 tags) (18 tags) (9 tags) (25 tags) (12 tags)
NP[nb]/N 5742 0 4544 4176 4666
((S\NP)\(S\NP))/N 14 5 642 122 107
(((N/N)\(N/N))\((N/N)\(N/N)))/N 0 0 0 698 0
((S/S)/S[dcl])/(S[adj]\NP) 0 733 0 0 0
PP/N 0 1755 0 3 1
: : : : : :
in → (76 distinct tags in Ltrain) (35 tags) (20 tags) (17 tags) (37 tags) (14 tags)
(NP\NP)/NP 883 0 649 708 904
((S\NP)\(S\NP))/NP 793 0 911 320 424
PP/NP 177 1 33 12 82
((S[adj]\NP)/(S[adj]\NP))/NP 0 215 0 0 0
: : : : : :
</table>
<tableCaption confidence="0.998366">
Table 4: Comparison of tag assignments from the gold tags versus model tags obtained on the test set.
</tableCaption>
<bodyText confidence="0.495499">
The table shows tag assignments (and their counts for each method) for the and in in the CCGbank test
sections. The number of distinct tags assigned by each method is given in parentheses. Ltrain is the
lexicon obtained from sections 0-18 of CCGbank that is used as the basis for EM training.
</bodyText>
<table confidence="0.999242">
Model TEST 1 TEST 2 (using lexicon from:)
NPAPER+CIVIL NPAPER CIVIL
Random 9.6 9.7 8.4 9.6
EM 26.4 26.8 27.2 29.3
EM+IP 34.8 32.4 34.8 34.6
EMGI 43.1 43.9 44.0 40.3
EMGI+IPGI 45.8 43.6 47.5 40.9
</table>
<tableCaption confidence="0.759689666666667">
Table 5: Comparison of supertagging results for
CCG-TUT. Accuracies are for ambiguous word
tokens in the test corpus, ignoring punctuation.
</tableCaption>
<bodyText confidence="0.999903">
bootstrapping a supertagger for a new language is
one of the main use scenarios we envision: in such
a scenario, there is no development data for chang-
ing settings and parameters. Thus, we determined
a train/test split beforehand and ran the methods
exactly as we had for CCGbank.
The results, given in Table 5, demonstrate the
same trends as for English: basic EM is far more
accurate than random, EM+IP adds another 8-10%
absolute accuracy, and EMGI adds an additional 8-
10% again. The combination of the methods gen-
erally improves over EMGI, except when the lex-
icon is extracted from NPAPER+CIVIL. Table 6
gives precision and recall for the grammars and
lexicons for CCG-TUT—the values are lower than
for CCGbank (in line with the lower baseline), but
exhibit the same trends.
</bodyText>
<sectionHeader confidence="0.999595" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999183">
We have shown how two complementary
strategies—grammar-informed tag transitions and
IP-minimization—for learning of supertaggers
from highly ambiguous lexicons can be straight-
</bodyText>
<table confidence="0.997766285714286">
EM EM+IP EMGI EMGI+IPGI
Grammar
Precision 23.1 26.4 44.9 46.7
Recall 18.4 15.9 24.9 22.7
Lexicon
Precision 51.2 52.0 54.8 55.1
Recall 43.6 42.8 46.0 44.9
</table>
<tableCaption confidence="0.988551">
Table 6: Comparison of grammar/lexicon ob-
</tableCaption>
<bodyText confidence="0.997862333333333">
served in the model tagging vs. gold tagging
in terms of precision and recall measures for su-
pertagging on CCG-TUT.
forwardly integrated. We verify the benefits of
both cross-lingually, on English and Italian data.
We also provide a new two-stage integer program-
ming setup that allows model minimization to be
tractable for supertagging without sacrificing the
quality of the search for minimal bitag grammars.
The experiments in this paper use large lexi-
cons, but the methodology will be particularly use-
ful in the context of bootstrapping from smaller
ones. This brings further challenges; in particular,
it will be necessary to identify novel entries con-
sisting of seen word and seen category and to pre-
dict unseen, but valid, categories which are needed
to explain the data. For this, it will be necessary
to forgo the assumption that the provided lexicon
is always obeyed. The methods we introduce here
should help maintain good accuracy while open-
ing up these degrees of freedom. Because the lexi-
con is the grammar in CCG, learning new word-
category associations is grammar generalization
and is of interest for grammar acquisition.
</bodyText>
<page confidence="0.989215">
502
</page>
<bodyText confidence="0.999951071428571">
Finally, such lexicon refinement and generaliza-
tion is directly relevant for using CCG in syntax-
based machine translation models (Hassan et al.,
2009). Such models are currently limited to lan-
guages for which corpora annotated with CCG
derivations are available. Clark and Curran (2006)
show that CCG parsers can be learned from sen-
tences labeled with just supertags—without full
derivations—with little loss in accuracy. The im-
provements we show here for learning supertag-
gers from lexicons without labeled data may be
able to help create annotated resources more ef-
ficiently, or enable CCG parsers to be learned with
less human-coded knowledge.
</bodyText>
<sectionHeader confidence="0.999024" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998546625">
The authors would like to thank Johan Bos, Joey
Frazee, Taesun Moon, the members of the UT-
NLL reading group, and the anonymous review-
ers. Ravi and Knight acknowledge the support
of the NSF (grant IIS-0904684) for this work.
Baldridge acknowledges the support of a grant
from the Morris Memorial Trust Fund of the New
York Community Trust.
</bodyText>
<sectionHeader confidence="0.999361" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999908371794872">
J. Baldridge. 2008. Weakly supervised supertagging
with grammar-informed initialization. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 57–64,
Manchester, UK, August.
M. Banko and R. C. Moore. 2004. Part of speech
tagging in context. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING), page 556, Morristown, NJ, USA.
A. R. Barron, J. Rissanen, and B. Yu. 1998. The
minimum description length principle in coding and
modeling. IEEE Transactions on Information The-
ory, 44(6):2743–2760.
J. Bos, C. Bosco, and A. Mazzei. 2009. Converting a
dependency treebank to a categorial grammar tree-
bank for Italian. In Proceedings of the Eighth In-
ternational Workshop on Treebanks and Linguistic
Theories (TLT8), pages 27–38, Milan, Italy.
S. Clark and J. Curran. 2006. Partial training for
a lexicalized-grammar parser. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 144–151, New
York City, USA, June.
S. Clark and J. Curran. 2007. Wide-coverage efficient
statistical parsing with CCG and log-linear models.
Computational Linguistics, 33(4).
M. Creutz and K. Lagus. 2002. Unsupervised discov-
ery of morphemes. In Proceedings of the ACL Work-
shop on Morphological and Phonological Learning,
pages 21–30, Morristown, NJ, USA.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM can
find pretty good HMM POS-taggers (when given a
good start). In Proceedings of the ACL, pages 746–
754, Columbus, Ohio, June.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2):153–198.
S. Goldwater and T. L. Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the ACL, pages 744–751,
Prague, Czech Republic, June.
H. Hassan, K. Sima’an, and A. Way. 2009. A syntac-
tified direct translation model with linear-time de-
coding. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1182–1191, Singapore, August.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
A corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355–396.
A. Joshi. 1988. Tree Adjoining Grammars. In David
Dowty, Lauri Karttunen, and Arnold Zwicky, ed-
itors, Natural Language Parsing, pages 206–250.
Cambridge University Press, Cambridge.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2).
B. Merialdo. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155–171.
C. Pollard and I. Sag. 1994. Head Driven Phrase
Structure Grammar. CSLI/Chicago University
Press, Chicago.
S. Ravi and K. Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 504–512, Suntec, Singapore, August.
M. Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In Proceedings of the Ad-
vances in Neural Information Processing Systems
(NIPS), pages 1521–1528, Cambridge, MA. MIT
Press.
</reference>
<page confidence="0.998859">
503
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.969239">
<title confidence="0.9987845">Minimized models and grammar-informed initialization for supertagging with highly ambiguous lexicons</title>
<author confidence="0.998979">Kevin</author>
<affiliation confidence="0.993986">of Southern California Information Sciences Institute</affiliation>
<address confidence="0.998271">Marina del Rey, California 90292</address>
<abstract confidence="0.999152588235294">We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Baldridge</author>
</authors>
<title>Weakly supervised supertagging with grammar-informed initialization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>57--64</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2939" citStr="Baldridge (2008)" startWordPosition="438" endWordPosition="439">overs the observed data. A more challenging task is learning supertaggers for lexicalized grammar formalisms such as Combinatory Categorial Grammar (CCG) (Steedman, 2000). For example, CCGbank (Hockenmaier and Steedman, 2007) contains 1241 distinct supertags (lexical categories) and the most ambiguous word has 126 supertags. This provides a much more challenging starting point for the semi-supervised methods typically applied to the task. Yet, this is an important task since creating grammars and resources for CCG parsers for new domains and languages is highly labor- and knowledge-intensive. Baldridge (2008) uses grammar-informed initialization for HMM tag transitions based on the universal combinatory rules of the CCG formalism to obtain 56.1% accuracy on ambiguous word tokens, a large improvement over the 33.0% accuracy obtained with uniform initialization for tag transitions. The strategies employed in Ravi and Knight (2009) and Baldridge (2008) are complementary. The former reduces the model size globally given a data set, while the latter biases bitag transitions toward those which are more likely based on a universal grammar without reference to any data. In this paper, we show how these st</context>
<context position="7008" citStr="Baldridge (2008)" startWordPosition="1089" endWordPosition="1090">: Statistics for the training data used to extract lexicons for CCGbank and CCG-TUT. Distinct: # of distinct lexical categories; Max: # of categories for the most ambiguous word; Type ambig: per word type category ambiguity; Tok ambig: per word token category ambiguity. trast, supertags are detailed, structured labels; a universal set of grammatical rules defines how categories may combine with one another to project syntactic structure.2 Because of this, properties of the CCG formalism itself can be used to constrain learning—prior to considering any particular language, grammar or data set. Baldridge (2008) uses this observation to create grammar-informed tag transitions for a bitag HMM supertagger based on two main properties. First, categories differ in their complexity and less complex categories tend to be used more frequently. For example, two categories for buy in CCGbank are (S[dcl]\NP)/NP and ((((S[b]\NP)/PP)/PP)/(S[adj]\NP))/NP; the former occurs 33 times, the latter once. Second, categories indicate the form of categories found adjacent to them; for example, the category for sentential complement verbs ((S\NP)/S) expects an NP to its left and an S to its right. Categories combine via r</context>
<context position="20192" citStr="Baldridge (2008)" startWordPosition="3309" endWordPosition="3310"> transitions directly in the IP objective function. The first takes advantage of the earlier observation that the quality of the grammar and lexicon provided as initial input to the minimization procedure can affect the quality of the final supertagging output. For the second, we modify the objective function used in the two IP-minimization steps to be: �Minimize: wi · gvari (1) dgiEG where, G is the set of tag bigrams provided as input to IP, gvari is a binary variable in the integer program corresponding to tag bigram (ti_1, ti) E G, and wi is negative logarithm of pgii(ti ti_1) as given by Baldridge (2008).6 All other parts of 5In our experiments, we run three bootstrap iterations. 6Other numeric weights associated with the tag bigrams could be considered, such as 0/1 for uncombin499 the integer program including the constraints remain unchanged, and, we acquire a final tagger in the same manner as described in the previous section. In this way, we combine the minimization and GI strategies into a single objective function that finds a minimal grammar set while keeping the more likely tag bigrams in the chosen solution. EMGI+IPGI is used to refer to the method that uses GI information in both w</context>
<context position="25162" citStr="Baldridge, 2008" startWordPosition="4122" endWordPosition="4123">exicon Precision 58.4 63.0 78.0 80.6 Recall 50.9 56.0 71.5 67.6 Table 3: Comparison of grammar/lexicon observed in the model tagging vs. gold tagging in terms of precision and recall measures for supertagging on CCGbank data. gories, so EMGI is unable to model their distribution. Most importantly, the complementary effects of the two approaches can be seen in the improved results for EMGI+IPGI, which obtains about 3% better accuracy than EMGI. Accuracy on all tokens. Table 2 also gives performance when taking all tokens into account. The HMM when using full supervision obtains 87.6% accuracy (Baldridge, 2008),8 so the accuracy of 63.8% achieved by EMGI+IPGI nearly halves the gap between the supervised model and the 45.6% obtained by basic EM semi-supervised model. Effect of GI information in EM and/or IPminimization stages. We can also consider the effect of GI information in either EM training or IP-minimization to see whether it can be effectively exploited in both. The latter, EM+IPGI, obtains 53.2/51.1 for all/no-punc—a small gain compared to EM+IP’s 52.1/51.0. The former, EMGI+IP, obtains 58.9/61.6—a much larger gain. Thus, the better starting point provided by EMGI has more impact than the i</context>
</contexts>
<marker>Baldridge, 2008</marker>
<rawString>J. Baldridge. 2008. Weakly supervised supertagging with grammar-informed initialization. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 57–64, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>R C Moore</author>
</authors>
<title>Part of speech tagging in context.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING),</booktitle>
<pages>556</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1554" citStr="Banko and Moore, 2004" startWordPosition="220" endWordPosition="223">) taggers using a tag dictionary and unlabeled data is an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 2Department of Linguistics The University of Texas at Austin Austin, Texas 78712 jbaldrid@mail.utexas.edu or trigram Hidden Markov Model (HMM). Ravi and Knight (2009) achieved the best results thus far (92.3% word to</context>
<context position="3819" citStr="Banko and Moore (2004)" startWordPosition="574" endWordPosition="577">r tag transitions. The strategies employed in Ravi and Knight (2009) and Baldridge (2008) are complementary. The former reduces the model size globally given a data set, while the latter biases bitag transitions toward those which are more likely based on a universal grammar without reference to any data. In this paper, we show how these strategies may be combined straightforwardly to produce improvements on the task of learning supertaggers from lexicons that have not been filtered in any way.1 We demonstrate their cross-lingual effectiveness on CCGbank (English) and the Italian CCG-TUT 1See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 495–503, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics corpus (Bos et al., 2009). We find a consistent improved performance by using each of the methods compared to basic EM, and further improvements by using them in combination. Applying the approach of Ravi and Knight (2009) naively to CCG supertagging is intractable due to th</context>
</contexts>
<marker>Banko, Moore, 2004</marker>
<rawString>M. Banko and R. C. Moore. 2004. Part of speech tagging in context. In Proceedings of the International Conference on Computational Linguistics (COLING), page 556, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Barron</author>
<author>J Rissanen</author>
<author>B Yu</author>
</authors>
<title>The minimum description length principle in coding and modeling.</title>
<date>1998</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>44</volume>
<issue>6</issue>
<contexts>
<context position="9455" citStr="Barron et al., 1998" startWordPosition="1474" endWordPosition="1477">P since neither can combine, but NP is simpler. By starting EM with these tag transition distributions and an unfiltered lexicon (word-tosupertag dictionary), Baldridge obtains a tagging accuracy of 56.1% on ambiguous words—a large improvement over the accuracy of 33.0% obtained by starting with uniform transition distributions. We refer to a model learned from basic EM (uniformly initialized) as EM, and to a model with grammar-informed initialization as EMGI. 4 Minimized models for supertagging The idea of searching for minimized models is related to classic Minimum Description Length (MDL) (Barron et al., 1998), which seeks to select a small model that captures the most regularity in the observed data. This modeling strategy has been shown to produce good results for many natural language tasks (Goldsmith, 2001; Creutz and Lagus, 2002; Ravi and Knight, 2009). For tagging, the idea has been implemented using Bayesian models with priors that indirectly induce sparsity in the learned models (Goldwater and Griffiths, 2007); however, Ravi and Knight (2009) show a better approach is to directly minimize the model using an integer programming (IP) formulation. Here, we build on this idea for supertagging. </context>
</contexts>
<marker>Barron, Rissanen, Yu, 1998</marker>
<rawString>A. R. Barron, J. Rissanen, and B. Yu. 1998. The minimum description length principle in coding and modeling. IEEE Transactions on Information Theory, 44(6):2743–2760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
<author>C Bosco</author>
<author>A Mazzei</author>
</authors>
<title>Converting a dependency treebank to a categorial grammar treebank for Italian.</title>
<date>2009</date>
<booktitle>In Proceedings of the Eighth International Workshop on Treebanks and Linguistic Theories (TLT8),</booktitle>
<pages>27--38</pages>
<location>Milan, Italy.</location>
<contexts>
<context position="4169" citStr="Bos et al., 2009" startWordPosition="626" endWordPosition="629">be combined straightforwardly to produce improvements on the task of learning supertaggers from lexicons that have not been filtered in any way.1 We demonstrate their cross-lingual effectiveness on CCGbank (English) and the Italian CCG-TUT 1See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 495–503, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics corpus (Bos et al., 2009). We find a consistent improved performance by using each of the methods compared to basic EM, and further improvements by using them in combination. Applying the approach of Ravi and Knight (2009) naively to CCG supertagging is intractable due to the high level of ambiguity. We deal with this by defining a new two-stage integer programming formulation that identifies minimal grammars efficiently and effectively. 2 Data CCGbank. CCGbank was created by semiautomatically converting the Penn Treebank to CCG derivations (Hockenmaier and Steedman, 2007). We use the standard splits of the data used </context>
</contexts>
<marker>Bos, Bosco, Mazzei, 2009</marker>
<rawString>J. Bos, C. Bosco, and A. Mazzei. 2009. Converting a dependency treebank to a categorial grammar treebank for Italian. In Proceedings of the Eighth International Workshop on Treebanks and Linguistic Theories (TLT8), pages 27–38, Milan, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J Curran</author>
</authors>
<title>Partial training for a lexicalized-grammar parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>144--151</pages>
<location>New York City, USA,</location>
<contexts>
<context position="32767" citStr="Clark and Curran (2006)" startWordPosition="5363" endWordPosition="5366">sary to forgo the assumption that the provided lexicon is always obeyed. The methods we introduce here should help maintain good accuracy while opening up these degrees of freedom. Because the lexicon is the grammar in CCG, learning new wordcategory associations is grammar generalization and is of interest for grammar acquisition. 502 Finally, such lexicon refinement and generalization is directly relevant for using CCG in syntaxbased machine translation models (Hassan et al., 2009). Such models are currently limited to languages for which corpora annotated with CCG derivations are available. Clark and Curran (2006) show that CCG parsers can be learned from sentences labeled with just supertags—without full derivations—with little loss in accuracy. The improvements we show here for learning supertaggers from lexicons without labeled data may be able to help create annotated resources more efficiently, or enable CCG parsers to be learned with less human-coded knowledge. Acknowledgements The authors would like to thank Johan Bos, Joey Frazee, Taesun Moon, the members of the UTNLL reading group, and the anonymous reviewers. Ravi and Knight acknowledge the support of the NSF (grant IIS-0904684) for this work</context>
</contexts>
<marker>Clark, Curran, 2006</marker>
<rawString>S. Clark and J. Curran. 2006. Partial training for a lexicalized-grammar parser. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 144–151, New York City, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J Curran</author>
</authors>
<title>Wide-coverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="26483" citStr="Clark and Curran, 2007" startWordPosition="4335" endWordPosition="4338">ible to exploit the GI information more effectively in the integer program than we have here. Also, our best model, EMGI+IPGI, uses GI information in both stages to obtain our best accuracy of 59.6/62.3. P/R for grammars and lexicons. We can obtain a more-fine grained understanding of how the models differ by considering the precision and recall values for the grammars and lexicons of the different models, given in Table 3. The basic EM model has very low precision for the grammar, indicating it proposes many unnecessary bitags; it 8A state-of-the-art, fully-supervised maximum entropy tagger (Clark and Curran, 2007) (which also uses part-ofspeech labels) obtains 91.4% on the same train/test split. achieves better recall because of the sheer number of bitags it proposes (12,363). EM+IP prunes that set of bitags considerably, leading to better precision at the cost of recall. EMGI’s higher recall and precision indicate the tag transition distributions do capture general patterns of linkage between adjacent CCG categories, while EM ensures that the data filters out combinable, but unnecessary, bitags. With EMGI+IPGI, we again see that IP-minimization prunes even more entries, improving precision at the loss</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>S. Clark and J. Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Creutz</author>
<author>K Lagus</author>
</authors>
<title>Unsupervised discovery of morphemes.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Morphological and Phonological Learning,</booktitle>
<pages>21--30</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9683" citStr="Creutz and Lagus, 2002" startWordPosition="1513" endWordPosition="1516">large improvement over the accuracy of 33.0% obtained by starting with uniform transition distributions. We refer to a model learned from basic EM (uniformly initialized) as EM, and to a model with grammar-informed initialization as EMGI. 4 Minimized models for supertagging The idea of searching for minimized models is related to classic Minimum Description Length (MDL) (Barron et al., 1998), which seeks to select a small model that captures the most regularity in the observed data. This modeling strategy has been shown to produce good results for many natural language tasks (Goldsmith, 2001; Creutz and Lagus, 2002; Ravi and Knight, 2009). For tagging, the idea has been implemented using Bayesian models with priors that indirectly induce sparsity in the learned models (Goldwater and Griffiths, 2007); however, Ravi and Knight (2009) show a better approach is to directly minimize the model using an integer programming (IP) formulation. Here, we build on this idea for supertagging. There are many challenges involved in using IP minimization for supertagging. The 1241 distinct supertags in the tagset result in 1.5 million tag bigram entries in the model and the dictionary contains almost 3.5 million word/ta</context>
</contexts>
<marker>Creutz, Lagus, 2002</marker>
<rawString>M. Creutz and K. Lagus. 2002. Unsupervised discovery of morphemes. In Proceedings of the ACL Workshop on Morphological and Phonological Learning, pages 21–30, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Goldberg</author>
<author>M Adler</author>
<author>M Elhadad</author>
</authors>
<title>EM can find pretty good HMM POS-taggers (when given a good start).</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>746--754</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1637" citStr="Goldberg et al., 2008" startWordPosition="234" endWordPosition="237">ctical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 2Department of Linguistics The University of Texas at Austin Austin, Texas 78712 jbaldrid@mail.utexas.edu or trigram Hidden Markov Model (HMM). Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (I</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM can find pretty good HMM POS-taggers (when given a good start). In Proceedings of the ACL, pages 746– 754, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="9659" citStr="Goldsmith, 2001" startWordPosition="1511" endWordPosition="1512">mbiguous words—a large improvement over the accuracy of 33.0% obtained by starting with uniform transition distributions. We refer to a model learned from basic EM (uniformly initialized) as EM, and to a model with grammar-informed initialization as EMGI. 4 Minimized models for supertagging The idea of searching for minimized models is related to classic Minimum Description Length (MDL) (Barron et al., 1998), which seeks to select a small model that captures the most regularity in the observed data. This modeling strategy has been shown to produce good results for many natural language tasks (Goldsmith, 2001; Creutz and Lagus, 2002; Ravi and Knight, 2009). For tagging, the idea has been implemented using Bayesian models with priors that indirectly induce sparsity in the learned models (Goldwater and Griffiths, 2007); however, Ravi and Knight (2009) show a better approach is to directly minimize the model using an integer programming (IP) formulation. Here, we build on this idea for supertagging. There are many challenges involved in using IP minimization for supertagging. The 1241 distinct supertags in the tagset result in 1.5 million tag bigram entries in the model and the dictionary contains al</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>J. Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T L Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>744--751</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1585" citStr="Goldwater and Griffiths, 2007" startWordPosition="224" endWordPosition="228">ictionary and unlabeled data is an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 2Department of Linguistics The University of Texas at Austin Austin, Texas 78712 jbaldrid@mail.utexas.edu or trigram Hidden Markov Model (HMM). Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Des</context>
<context position="9871" citStr="Goldwater and Griffiths, 2007" startWordPosition="1541" endWordPosition="1544">d to a model with grammar-informed initialization as EMGI. 4 Minimized models for supertagging The idea of searching for minimized models is related to classic Minimum Description Length (MDL) (Barron et al., 1998), which seeks to select a small model that captures the most regularity in the observed data. This modeling strategy has been shown to produce good results for many natural language tasks (Goldsmith, 2001; Creutz and Lagus, 2002; Ravi and Knight, 2009). For tagging, the idea has been implemented using Bayesian models with priors that indirectly induce sparsity in the learned models (Goldwater and Griffiths, 2007); however, Ravi and Knight (2009) show a better approach is to directly minimize the model using an integer programming (IP) formulation. Here, we build on this idea for supertagging. There are many challenges involved in using IP minimization for supertagging. The 1241 distinct supertags in the tagset result in 1.5 million tag bigram entries in the model and the dictionary contains almost 3.5 million word/tag pairs that are relevant to the test data. The set of 45 POS tags for the same data yields 2025 tag bigrams and 8910 dictionary entries. We also wish to scale our methods to larger data s</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech tagging. In Proceedings of the ACL, pages 744–751, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hassan</author>
<author>K Sima’an</author>
<author>A Way</author>
</authors>
<title>A syntactified direct translation model with linear-time decoding.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1182--1191</pages>
<location>Singapore,</location>
<marker>Hassan, Sima’an, Way, 2009</marker>
<rawString>H. Hassan, K. Sima’an, and A. Way. 2009. A syntactified direct translation model with linear-time decoding. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1182–1191, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hockenmaier</author>
<author>M Steedman</author>
</authors>
<title>CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="2548" citStr="Hockenmaier and Steedman, 2007" startWordPosition="375" endWordPosition="379">ameters for a bigram 2Department of Linguistics The University of Texas at Austin Austin, Texas 78712 jbaldrid@mail.utexas.edu or trigram Hidden Markov Model (HMM). Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data. A more challenging task is learning supertaggers for lexicalized grammar formalisms such as Combinatory Categorial Grammar (CCG) (Steedman, 2000). For example, CCGbank (Hockenmaier and Steedman, 2007) contains 1241 distinct supertags (lexical categories) and the most ambiguous word has 126 supertags. This provides a much more challenging starting point for the semi-supervised methods typically applied to the task. Yet, this is an important task since creating grammars and resources for CCG parsers for new domains and languages is highly labor- and knowledge-intensive. Baldridge (2008) uses grammar-informed initialization for HMM tag transitions based on the universal combinatory rules of the CCG formalism to obtain 56.1% accuracy on ambiguous word tokens, a large improvement over the 33.0%</context>
<context position="4723" citStr="Hockenmaier and Steedman, 2007" startWordPosition="713" endWordPosition="716">0. c�2010 Association for Computational Linguistics corpus (Bos et al., 2009). We find a consistent improved performance by using each of the methods compared to basic EM, and further improvements by using them in combination. Applying the approach of Ravi and Knight (2009) naively to CCG supertagging is intractable due to the high level of ambiguity. We deal with this by defining a new two-stage integer programming formulation that identifies minimal grammars efficiently and effectively. 2 Data CCGbank. CCGbank was created by semiautomatically converting the Penn Treebank to CCG derivations (Hockenmaier and Steedman, 2007). We use the standard splits of the data used in semi-supervised tagging experiments (e.g. Banko and Moore (2004)): sections 0-18 for training, 19-21 for development, and 22-24 for test. CCG-TUT. CCG-TUT was created by semiautomatically converting dependencies in the Italian Turin University Treebank to CCG derivations (Bos et al., 2009). It is much smaller than CCGbank, with only 1837 sentences. It is split into three sections: newspaper texts (NPAPER), civil code texts (CIVIL), and European law texts from the JRC-Acquis Multilingual Parallel Corpus (JRC). For test sets, we use the first 400 </context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>J. Hockenmaier and M. Steedman. 2007. CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
</authors>
<title>Tree Adjoining Grammars. In</title>
<date>1988</date>
<booktitle>Natural Language Parsing,</booktitle>
<pages>206--250</pages>
<editor>David Dowty, Lauri Karttunen, and Arnold Zwicky, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="8299" citStr="Joshi, 1988" startWordPosition="1292" endWordPosition="1293">en a lexicon containing the categories for each word, these allow derivations like: Ed might see a cat NP (S\NP)/(S\NP) (S\NP)/NP NP/N N &gt;B &gt; (S\NP)/NP NP &gt; &gt; Other derivations are possible. In fact, every pair of adjacent words above may be combined directly. For example, see and a may combine through forward composition to produce the category (S\NP)/N, and Ed’s category may type-raise to S/(S\NP) and compose with might’s category. Baldridge uses these properties to define tag 2Note that supertags can be lexical categories of CCG (Steedman, 2000), elementary trees of Tree-adjoining Grammar (Joshi, 1988), or types in a feature hierarchy as in Headdriven Phrase Structure Grammar (Pollard and Sag, 1994). S\NP S 496 transition distributions that have higher likelihood for simpler categories that are able to combine. For example, for the distribution p(ti|ti_1=NP), (S\NP)\NP is more likely than ((S\NP)/(N/N))\NP because both categories may combine with a preceding NP but the former is simpler. In turn, the latter is more likely than NP: it is more complex but can combine with the preceding NP. Finally, NP is more likely than (S/NP)/NP since neither can combine, but NP is simpler. By starting EM w</context>
</contexts>
<marker>Joshi, 1988</marker>
<rawString>A. Joshi. 1988. Tree Adjoining Grammars. In David Dowty, Lauri Karttunen, and Arnold Zwicky, editors, Natural Language Parsing, pages 206–250. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1522" citStr="Marcus et al., 1993" startWordPosition="214" endWordPosition="217">ng accurate part-of-speech (POS) taggers using a tag dictionary and unlabeled data is an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 2Department of Linguistics The University of Texas at Austin Austin, Texas 78712 jbaldrid@mail.utexas.edu or trigram Hidden Markov Model (HMM). Ravi and Knight (2009) achieved the best</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="1107" citStr="Merialdo (1994)" startWordPosition="151" endWordPosition="152">rformance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization. 1 Introduction Creating accurate part-of-speech (POS) taggers using a tag dictionary and unlabeled data is an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the sta</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>B. Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<title>Head Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>CSLI/Chicago University Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="8398" citStr="Pollard and Sag, 1994" startWordPosition="1307" endWordPosition="1310">ght see a cat NP (S\NP)/(S\NP) (S\NP)/NP NP/N N &gt;B &gt; (S\NP)/NP NP &gt; &gt; Other derivations are possible. In fact, every pair of adjacent words above may be combined directly. For example, see and a may combine through forward composition to produce the category (S\NP)/N, and Ed’s category may type-raise to S/(S\NP) and compose with might’s category. Baldridge uses these properties to define tag 2Note that supertags can be lexical categories of CCG (Steedman, 2000), elementary trees of Tree-adjoining Grammar (Joshi, 1988), or types in a feature hierarchy as in Headdriven Phrase Structure Grammar (Pollard and Sag, 1994). S\NP S 496 transition distributions that have higher likelihood for simpler categories that are able to combine. For example, for the distribution p(ti|ti_1=NP), (S\NP)\NP is more likely than ((S\NP)/(N/N))\NP because both categories may combine with a preceding NP but the former is simpler. In turn, the latter is more likely than NP: it is more complex but can combine with the preceding NP. Finally, NP is more likely than (S/NP)/NP since neither can combine, but NP is simpler. By starting EM with these tag transition distributions and an unfiltered lexicon (word-tosupertag dictionary), Bald</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>C. Pollard and I. Sag. 1994. Head Driven Phrase Structure Grammar. CSLI/Chicago University Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ravi</author>
<author>K Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>504--512</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1661" citStr="Ravi and Knight, 2009" startWordPosition="238" endWordPosition="241"> has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 2Department of Linguistics The University of Texas at Austin Austin, Texas 78712 jbaldrid@mail.utexas.edu or trigram Hidden Markov Model (HMM). Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal </context>
<context position="3265" citStr="Ravi and Knight (2009)" startWordPosition="486" endWordPosition="489">rtags. This provides a much more challenging starting point for the semi-supervised methods typically applied to the task. Yet, this is an important task since creating grammars and resources for CCG parsers for new domains and languages is highly labor- and knowledge-intensive. Baldridge (2008) uses grammar-informed initialization for HMM tag transitions based on the universal combinatory rules of the CCG formalism to obtain 56.1% accuracy on ambiguous word tokens, a large improvement over the 33.0% accuracy obtained with uniform initialization for tag transitions. The strategies employed in Ravi and Knight (2009) and Baldridge (2008) are complementary. The former reduces the model size globally given a data set, while the latter biases bitag transitions toward those which are more likely based on a universal grammar without reference to any data. In this paper, we show how these strategies may be combined straightforwardly to produce improvements on the task of learning supertaggers from lexicons that have not been filtered in any way.1 We demonstrate their cross-lingual effectiveness on CCGbank (English) and the Italian CCG-TUT 1See Banko and Moore (2004) for a description of how many early POS-taggi</context>
<context position="9707" citStr="Ravi and Knight, 2009" startWordPosition="1517" endWordPosition="1520">he accuracy of 33.0% obtained by starting with uniform transition distributions. We refer to a model learned from basic EM (uniformly initialized) as EM, and to a model with grammar-informed initialization as EMGI. 4 Minimized models for supertagging The idea of searching for minimized models is related to classic Minimum Description Length (MDL) (Barron et al., 1998), which seeks to select a small model that captures the most regularity in the observed data. This modeling strategy has been shown to produce good results for many natural language tasks (Goldsmith, 2001; Creutz and Lagus, 2002; Ravi and Knight, 2009). For tagging, the idea has been implemented using Bayesian models with priors that indirectly induce sparsity in the learned models (Goldwater and Griffiths, 2007); however, Ravi and Knight (2009) show a better approach is to directly minimize the model using an integer programming (IP) formulation. Here, we build on this idea for supertagging. There are many challenges involved in using IP minimization for supertagging. The 1241 distinct supertags in the tagset result in 1.5 million tag bigram entries in the model and the dictionary contains almost 3.5 million word/tag pairs that are relevan</context>
<context position="14669" citStr="Ravi and Knight (2009)" startWordPosition="2385" endWordPosition="2388">e that Gmin1 can explain the test data: it contains tags for all word bigram types, but it cannot necessarily tag the full word sequence. Figure 1 illustrates this. Using only tag bigrams from MIN1 (shown in blue), there is no fully-linked tag path through the network. There are missing links between words w2 and w3 and between words w3 and w4 in the word sequence. The next stage fills in these missing links. Minimization 2 (MIN2). This stage uses the original minimization formulation for the supertagging problem IPGrional, again using an integer programming method similar to that proposed by Ravi and Knight (2009). If applied to the observed grammar GGbs, the resulting integer program is hard to solve.4 However, by using the partial solution Gmin1 obtained in MIN1 the IP optimization speeds up considerably. We implement this by fixing the values of all binary grammar variables present in Gmin1 to 1 before optimization. This reduces the search space signifi3We use the commercial CPLEX solver. 4The solver runs for days without returning a solution. Minimize:gvari Vgi EG Subject to constraints: 1. For every word bigram wlwm, there exists at least one tagging that respects the lexicon L. EV tjEL(wl), tkEL(</context>
<context position="18875" citStr="Ravi and Knight (2009)" startWordPosition="3088" endWordPosition="3091">s from the minimized grammar set Gmin2, and instantiate an emission model using the word/tag pairs seen in L (provided as input to the minimization procedure). All the parameters in the HMM model are initialized with uniform probabilities, and we run EM for 40 iterations. The trained model is used to find the Viterbi tag sequence for the corpus. We refer to this model (where the EM output (Gobs, Lobs) was provided to the IP-minimization as initial input) as EM+IP. Bootstrapped minimization. The quality of the observed grammar and lexicon improves considerably at the end of a single EM+IP run. Ravi and Knight (2009) exploited this to iteratively improve their POS tag model: since the first minimization procedure is seeded with a noisy grammar and tag dictionary, iterating the IP procedure with progressively better grammars further improves the model. We do likewise, bootstrapping a new EM+IP run using as input, the observed grammar Gobs and lexicon Lobs from the last tagging output of the previous iteration. We run this until the chosen grammar set Gmin2 does not change.5 4.2 Minimization with grammar-informed initialization There are two complementary ways to use grammar-informed initialization with the</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>S. Ravi and K. Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 504–512, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2493" citStr="Steedman, 2000" startWordPosition="369" endWordPosition="371">ectation Maximization (EM) to learn parameters for a bigram 2Department of Linguistics The University of Texas at Austin Austin, Texas 78712 jbaldrid@mail.utexas.edu or trigram Hidden Markov Model (HMM). Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data. A more challenging task is learning supertaggers for lexicalized grammar formalisms such as Combinatory Categorial Grammar (CCG) (Steedman, 2000). For example, CCGbank (Hockenmaier and Steedman, 2007) contains 1241 distinct supertags (lexical categories) and the most ambiguous word has 126 supertags. This provides a much more challenging starting point for the semi-supervised methods typically applied to the task. Yet, this is an important task since creating grammars and resources for CCG parsers for new domains and languages is highly labor- and knowledge-intensive. Baldridge (2008) uses grammar-informed initialization for HMM tag transitions based on the universal combinatory rules of the CCG formalism to obtain 56.1% accuracy on am</context>
<context position="7669" citStr="Steedman (2000)" startWordPosition="1189" endWordPosition="1190">med tag transitions for a bitag HMM supertagger based on two main properties. First, categories differ in their complexity and less complex categories tend to be used more frequently. For example, two categories for buy in CCGbank are (S[dcl]\NP)/NP and ((((S[b]\NP)/PP)/PP)/(S[adj]\NP))/NP; the former occurs 33 times, the latter once. Second, categories indicate the form of categories found adjacent to them; for example, the category for sentential complement verbs ((S\NP)/S) expects an NP to its left and an S to its right. Categories combine via rules such as application and composition (see Steedman (2000) for details). Given a lexicon containing the categories for each word, these allow derivations like: Ed might see a cat NP (S\NP)/(S\NP) (S\NP)/NP NP/N N &gt;B &gt; (S\NP)/NP NP &gt; &gt; Other derivations are possible. In fact, every pair of adjacent words above may be combined directly. For example, see and a may combine through forward composition to produce the category (S\NP)/N, and Ed’s category may type-raise to S/(S\NP) and compose with might’s category. Baldridge uses these properties to define tag 2Note that supertags can be lexical categories of CCG (Steedman, 2000), elementary trees of Tree-a</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>M. Steedman. 2000. The Syntactic Process. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian LDA-based model for semi-supervised part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of the Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>1521--1528</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1614" citStr="Toutanova and Johnson, 2008" startWordPosition="229" endWordPosition="233"> an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 2Department of Linguistics The University of Texas at Austin Austin, Texas 78712 jbaldrid@mail.utexas.edu or trigram Hidden Markov Model (HMM). Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach usin</context>
</contexts>
<marker>Toutanova, Johnson, 2008</marker>
<rawString>Kristina Toutanova and Mark Johnson. 2008. A Bayesian LDA-based model for semi-supervised part-of-speech tagging. In Proceedings of the Advances in Neural Information Processing Systems (NIPS), pages 1521–1528, Cambridge, MA. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>