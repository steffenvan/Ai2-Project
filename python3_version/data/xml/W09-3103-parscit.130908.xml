<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020341">
<title confidence="0.995203">
An Extensible Crosslinguistic Readability Framework
</title>
<author confidence="0.996469">
Jesse Saba Kirchner
</author>
<affiliation confidence="0.995722">
Department of Linguistics
</affiliation>
<address confidence="0.960119333333333">
UC Santa Cruz
1156 High Street
Santa Cruz, CA 95064
</address>
<email confidence="0.999475">
kirchner@ucsc.edu
</email>
<author confidence="0.995015">
Justin Nuger
</author>
<affiliation confidence="0.994976">
Department of Linguistics
</affiliation>
<address confidence="0.959900333333333">
UC Santa Cruz
1156 High Street
Santa Cruz, CA 95064
</address>
<email confidence="0.999395">
jnuger@ucsc.edu
</email>
<author confidence="0.898133">
Yi Zhang
</author>
<affiliation confidence="0.834276">
Baskin School of Engineering
</affiliation>
<address confidence="0.910456333333333">
UC Santa Cruz
1156 High Street, SOE 3
Santa Cruz, CA 95064
</address>
<email confidence="0.999575">
yiz@soe.ucsc.edu
</email>
<sectionHeader confidence="0.993909" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999880961538462">
Automatic assessment of the readability
level (i.e., the relative linguistic complex-
ity) of documents in a large number of
languages is an important problem that
can be applied to many real-world appli-
cations, such as retrieving age-appropriate
search engine results for kids, construct-
ing automatic tutoring systems, and so on.
Unfortunately, existing readability label-
ing techniques have only been applied to
a very small number of languages. In this
paper, we present an extensible crosslin-
guistic readability framework based on the
use of parallel corpora to quickly create
readability software for thousands of lan-
guages, including languages for which no
linguists are available to define readability
rules or for which documents with read-
ability labels are lacking to train readabil-
ity models. To demonstrate our idea, we
developed a system based on the proposed
framework. This paper discusses the theo-
retical and practical issues involved in de-
signing such a system and presents the re-
sults of an experiment conducted with the
system.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956595744681">
Automatically labeling the reading difficulty of an
arbitrary document is an important problem in sev-
eral human language technology applications. It
can, for example, be used in the next generation of
personalized information retrieval systems to find
documents tailored to children at different grade
levels. In a tutoring system, it can be used to find
online reading materials of the appropriate diffi-
culty level for students (Heilman et al., 2006).
Of the world’s more than 6,000 languages
(Grimes, 2005), readability classification software
exists for a striking few, and it is limited in cover-
age to languages spoken in countries with promi-
nent standing in global economics and politics.
A substantial number of the remaining languages
nevertheless have a sufficient corpus of digital
documents — a number which may already be
in the hundreds and soon in the thousands (Pao-
lillo et al., 2005). A natural idea is to create
software to automatically predict readability lev-
els (henceforth “RLs”) for these documents. Such
software has significant potential for applications
in different areas of research, such as creating web
search engines for kids speaking languages not
covered by existing readability software, as de-
scribed above.
There is much research on assessing the read-
ing difficulties of texts in a particular language,
and the existing work can be roughly classified as
falling under two approaches. The first approach
uses manually or semi-automatically crafted rules
designed by computational linguists who are fa-
miliar with the language in question (Anderson,
1981). The second approach learns readability
models for a particular language based on labeled
data (Collins-Thompson and Callan, 2004).
Unfortunately, existing approaches cannot be
easily extended to handle thousands of different
languages. The first approach, using rules de-
vised by computational linguists familiar with the
languages, is impractical because for many lan-
guages, especially minority or understudied lan-
guages, there are relatively few linguists suffi-
ciently familiar with the language to design such
software. Even if these linguists exist, it is un-
likely that a search engine company that wanted to
serve the whole world would have the resources to
</bodyText>
<page confidence="0.993801">
11
</page>
<note confidence="0.9637425">
Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 11–18,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.99970496969697">
hire all of them. The second approach, using ma-
chine learning techniques on labeled data, is very
expensive because it requires the support of edu-
cated speakers of each language to provide read-
ability labels for documents in the language. The
availability of such speakers cannot always be as-
sumed. Again, recruiting annotators for thousands
of different languages is not economically feasible
or practical for a company. An alternative strategy
that can scale to thousands of different languages
is needed.
In this paper, we propose a general framework
to solve this problem based on a parallel corpus
crawled from the web. To illustrate the idea, we
developed an Extensible Crosslinguistic Readabil-
ity system (henceforth “ECR system”), which uses
a Cross-Lingual Information Retrieval (henceforth
“CLIR”) system that we call EXCLAIM. The ECR
system functions to create RL classification soft-
ware in any language with sufficient coverage in
the CLIR system. We also report the promising —
though very preliminary — results of an experi-
ment that tests a real-world application of this sys-
tem. Investigation of the basic assumptions and
generalization of parameters and evaluation met-
rics are left for future work.
The rest of this paper is organized as follows.
The problem setting is described in Section 2. The
architecture of our ECR system is explained in
Section 3. Our experimental design is laid out in
Section 4, followed by experimental result analy-
sis in Section 5. Section 6 gives an overview of
related work, and section 7 concludes.
</bodyText>
<sectionHeader confidence="0.867026" genericHeader="introduction">
2 Problem and Proposed Methodology
</sectionHeader>
<subsectionHeader confidence="0.891295">
2.1 Existing Approaches to Readability
Classification
</subsectionHeader>
<bodyText confidence="0.999987333333333">
In traditional approaches to computational read-
ability classification, there is a variety of language-
specific system requirements needed in order to
perform the RL classification task. For some
languages, this task is relatively well-studied.
For example, the simple and widely-used Laes-
barhedsindex (henceforth “LIX”) calculates RLs
for texts written in Western European languages1
with the following LIX formula:
</bodyText>
<equation confidence="0.878776">
words
RLD =
+
sentences
</equation>
<footnote confidence="0.991092">
1In practice, LIX may be substituted with other metrics,
such as Flesch-Kincaid.
</footnote>
<bodyText confidence="0.999978608695652">
where IID is a document written in an unfamiliar
language, and RLD is the readability score of the
document D.
The above formula relies on specific parame-
ters which have been tuned to a certain set of lan-
guages. These include the total number of words
in IID (words), the total number of sentences in IID
(sentences), and the total number of words in IID
with more than six characters (words,h,&gt;s ).
Although this formula may be successful in
RL classification for languages like English and
French (Bj¨ornsson and H˚ard af Segerstad(1979),
Anderson (1981)), it remains essentially parochial
in the context of other languages because the pa-
rameters overfit the data from the Western Euor-
pean languages for which it was designed. Since
the LIX formula depends on measuring the num-
ber of characters in a word to find words greater
than 6, it is ineffective in determining the readabil-
ity of documents written in languages with differ-
ent writing systems, such as Chinese. This is due
to the fact that some languages, like Chinese, are
written with characters based on semantic mean-
ing rather than phonemes, as in English, and a
large number of Chinese words consist of just one
or two characters, regardless of semantic complex-
ity (Li and Thompson, 1981). In a similar vein,
many languages of the world (even some that use
phonemically-based writing systems) do not ad-
here to the implicit assumption of the LIX formula
that semantically “complex” words are longer than
simpler words (Greenberg, 1954). In these lan-
guages, then, the same metric cannot be used as a
valid measure of RL difficulty of documents, since
word length does not correlate with semantic com-
plexity.
One recent alternative approach has been devel-
oped for readability labeling that uses multiple sta-
tistical language models (Collins-Thompson and
Callan, 2004). The idea is to train statistical lan-
guage models for each grade level automatically
from manually labeled training documents. How-
ever, even an approach like this is not scalable to
handle thousands of languages, since it is hard to
recruit annotators of all of these languages to man-
ually label the training data.
</bodyText>
<subsectionHeader confidence="0.997653">
2.2 Proposed Solution
</subsectionHeader>
<bodyText confidence="0.7593162">
We propose a scalable solution to the problem of
labeling the readability of documents in many lan-
guages. The general idea is to combine CLIR
100 × words,h,&gt;s
words
</bodyText>
<page confidence="0.987817">
12
</page>
<bodyText confidence="0.99997974509804">
technology with off-the-shelf readability software
for at least one well-studied language, such as
English. First, off-the-shelf readability software
is used to assign RLs to a set of documents in
the source language, e.g. English, which serve as
training data. Second, a set of key terms is se-
lected from each group of documents correspond-
ing to a particular RL to construct a readability
model for that RL. Third, for each of these sets
of terms, the cross-lingual query-expansion com-
ponent of the CLIR system returns a semantically
relevant set of terms in the target language. Fi-
nally, these target-language term sets are used to
build the target-language RL models, which can
be used to assign RLs to documents in the tar-
get language, even if language-specific readability
classification software does not exist for that lan-
guage. This solution plausibly extends to any of
the languages covered by the CLIR system. It is
possible to create a CLIR system by crawling the
internet for parallel corpora, which exist for many
language pairs. As a result, the proposed solution
already has the potential to cover many different
languages.
The success of this method relies on the as-
sumption that readability levels remain fairly con-
stant across syntactically and semantically paral-
lel documents in the two languages in question,
or simply across documents typified by equivalent
key terms. This does not seem unreasonable: if the
same information is represented in two different
languages in semantically and structurally compa-
rable ways, it is likely that the reading difficulty of
the two texts should not differ much, if at all. If
this assumption is true, generation of readability
software really depends only on the availability of
a solid CLIR system, and the problem of requir-
ing trained computational linguists and native lan-
guage speakers to design the system is mitigated.
Figure 1 shows a simple process model of a sys-
tem for generating RL classifiers for various lan-
guages. A set of training documents from a source
language (i.e., the “L1” in Figure 1) is assigned
RLs by the off-the-shelf RL classification soft-
ware R(L1). Using the source langauge files and
the RLs produced by R(L1), the ECR system pro-
duces a source language (L1) readability model.
Through the system interface, the CLIR system
(EXCLAIM) uses the L1 readability model to pro-
duce a target language (L2) readability model. The
system uses the L2 readability model to produce a
</bodyText>
<figureCaption confidence="0.999314">
Figure 1: ECR Domain
</figureCaption>
<bodyText confidence="0.999447666666667">
new RL classifier R(L2) for the target language.
The newly developed classifier R(L2) can then be
used to classify documents in the L2.
</bodyText>
<sectionHeader confidence="0.969567" genericHeader="method">
3 System Architecture
</sectionHeader>
<bodyText confidence="0.9999885">
To address any theoretical or empirical concerns
and questions about the proposed solution, includ-
ing those relating to the assumption that key term
equivalence correlates with RL equivalence, we
have developed an ECR system compatible with
an existing CLIR system and have proposed eval-
uation metrics for this system. We developed
the ECR system to meet the needs of two differ-
ent kinds of users. First, higher-level intermedi-
ate users can build RL classification software for
a given target language. Second, end users can
use the software to classify documents in that lan-
guage. In this section, we give a developer’s-eye
view of the system architecture (shown in Figure
2), making specific reference to the points at which
intermediate and end users may interact with the
system. For presentational clarity, we periodically
adopt the arbitrary assumption that the source lan-
guage is English, as this is the source language of
our experiment described in the following section.
The ECR system has three primary tasks. The
first task is to enable intermediate users to develop
RL classification model for the source language.
The second task is to provide the intermediate user
with a toolkit to construct language-specific soft-
ware that automatically tags documents in the tar-
get language with the appropriate RLs. The final
task is to provide an interface module for the end
</bodyText>
<page confidence="0.998231">
13
</page>
<figureCaption confidence="0.99981">
Figure 2: ECR System design
</figureCaption>
<bodyText confidence="0.997621590909091">
user to utilize this software.
In order to approach the first task, one needs a
set of documents in a source language for which
off-the-shelf readability software is available. This
set of documents functions as a training data set;
if a user is trying to assign RLs to documents
in a particular domain — e.g., forestry, medical,
leisure, etc. — then (s)he can already help shape
the results of the system by providing domain-
relevant source langauge data at this stage. To
aid the intermediate user in obtaining RLs for this
set of data, the ECR system has a number of pa-
rameters that may be selected, based on different
models of RL-tagging — for example, we selected
English as the source language and the aforemen-
tioned LIX formula due to its simplicity. The doc-
uments are then organized according to the gener-
ated RLs and separated into different RL groups.
At this point, the K most salient words are
extracted from each source language RL groups
(RLS) based on the following tf*idf term weight-
ing:2
</bodyText>
<figure confidence="0.874287666666667">
× lo
a _ / 0.5 freqi,j 1 g N
w ,7 10.5 + maxl freql,j ) ni
</figure>
<footnote confidence="0.96777">
2In principle, this choice is arbitrary and any other appro-
priate term-weighting formula could also be used.
</footnote>
<bodyText confidence="0.99884911627907">
The selected words RLS = {f1, f2, ...fK }
form the basis for constructing an RL classifica-
tion model for an unknown target language.
In order to construct a target language RL clas-
sification model, the cross-lingual query expan-
sion component of a CLIR system is necessary
to select semantically comparable and semanti-
cally related words in the target language. The
CLIR system we developed is called EXCLAIM,
or the EXtensible Cross-Linguistic Automatic
Information Machine. We constructed EXCLAIM
from a semantically (though not structurally) par-
allel corpus crawled from Wikipedia (Wikime-
dia Foundation, 1999). All Wikipedia articles with
both source and target language versions collec-
tively function as data to construct the CLIR com-
ponent. Due to Wikipedia’s coverage of a large
amount of languages (English being the language
with the largest collection of articles at the time
of writing), CLIR components for English paired
with a wide number of target languages was cre-
ated for EXCLAIM.
For each RLS, the query-expansion component
of EXCLAIM determines a set of corresponding
words for the target language RLT. Initially, each
word in RLS is matched with the source language
document in EXCLAIM for which it has the highest
tf*idf term weight. The M most salient terms in
the corresponding target language document (cal-
culated once again using the tf*idf formula) are
then added to RLT. Therefore, RLT contains no
more than K ∗ M terms. The total set of RLTs
form the base of the target language readability
classification model.
Using this model, the system generates target
language readability classification software on the
fly, which plugs into the system’s existing inter-
face module for end users. Through the module,
the end user can use the newly generated software
to determine RLs for a set of target language doc-
uments without requiring any specialized knowl-
edge of the languages or the software development
process.
</bodyText>
<sectionHeader confidence="0.999118" genericHeader="method">
4 Experimental Design
</sectionHeader>
<bodyText confidence="0.9998075">
We conducted an experiment to demonstrate this
idea and to test our ECR system. Without loss
of generality, we chose English as our source lan-
guage and Chinese as our target language. While
Chinese is a major language for which it would
be relatively easy to find linguistic experts to write
</bodyText>
<page confidence="0.9955">
14
</page>
<bodyText confidence="0.999773590163935">
readability rules and native speakers to label doc-
ument readability for training, our goal is not to
demonstrate that the proposed solution is the best
solution to build readability software for Chinese.
Instead, we chose these languages for the follow-
ing reasons. First, we are capable of reading both
languages and are thus able to judge the quality of
the ECR system. Second, publicly available En-
glish readability labeling software exists, and we
are not aware of such software for Chinese. Third,
we had access to a parallel set of documents that
could be used for the evaluation of our experiment.
Fourth, the many differences between English and
Chinese might demonstrate the applicability of our
system for a diverse set of languages. However,
the features that made Chinese a desirable target
language for us are not essential for the proposed
solution, and do not affect the extensibility of the
approach.
We created a test set using a collection of
Chinese-English parallel documents from the
medical domain (Chinese Community Health Re-
source Center, 2004). The set comprised 65 docu-
ments in English and their human-translated Chi-
nese translations. Although a typical user does
not need to have access to sets of bilingual doc-
uments for the system to run successfully, we cir-
cumvented both the lack of off-the-shelf Chinese
readability labeling software and the lack of la-
beled Chinese documents for the evaluation of the
results of our system by using a high quality trans-
lated parallel document set. Since RLs are rough
measures of semantic and structural complexity,
we assume they should be approximately if not
exactly the same for a given document and its
translation in a different language, an extension of
the ideas in Collins-Thompson and Callan (2004).
Based on this assumption, we can accurately com-
pare the RLs of the translated CCHRC Chinese
medical documents to the RLs of the original En-
glish documents, which we call the “true RLs” of
the testing documents.
LIX-based RLs can be roughly mapped to grade
levels, e.g., a text that is classified with an RL of
8 is appropriate for the average 8th grade reader.
Since we can assign RLs to the English versions
of the 65 CCHRC documents, these RLs can serve
as targets to match when generating RLs for the
corresponding Chinese versions of the same docu-
ments.
An advantage of our system arises from a com-
plete vertical integration which allows a user with
knowledge of the eventual goal to help shape the
development of the target language RL classifica-
tion model and software. In our case, the target
language (Chinese) test set was from the medical
domain, so we selected the OHSU87 medical ab-
stract corpus as an English data set. We automati-
cally classified the OHSU87 documents using the
LIX mapping schema assigned by the UNIX Dic-
tion and Style tools,3 given in the following Table.
</bodyText>
<table confidence="0.998264666666667">
LIX Index RL LIX Index RL
Under 34.0 4 48.0-50.9 9
34.0-37.9 5 51.0-53.9 10
38.0-40.9 6 54.0-56.9 11
41.0-43.9 7 57.0 and over 12
44.0-47.9 8
</table>
<tableCaption confidence="0.986541">
Table 1: Mapping of LIX Index scores to RLs as
assigned by Diction
</tableCaption>
<bodyText confidence="0.999838785714286">
Then, we concatenated the English OHSU87 doc-
uments in each RL group. The tf*idf formula was
used to select the K English words most represen-
tative of each RL group.
Next, we automatically selected a set of Chi-
nese words for each RL class to create a corre-
sponding Chinese readability model by passing
each English word through the CLIR system, EX-
CLAIM, to retrieve the most relevant English doc-
ument in the Wikipedia corpus, where relevance
is measured using the tf*idf vector space model.
The top M Chinese words from the corresponding
Chinese document in the parallel Wikipedia cor-
pus were added to RLT. By repeating this pro-
cess for each word of each RL class, the Chinese
readability model was constructed. In our exper-
iment, we set K = 50 and M = 10 arbitrarily.
The ECR system then automatically generated the
subsequent RL classification software for Chinese.
Finally, we assigned a RL to each document in
the test set. At this point the procedure is essen-
tially similar to document retrieval task. Each RL
group’s set of words RLT was treated as a docu-
ment (dj ), and each test document to be labeled
was treated as a query (q). RLs were ranked based
on the cosine similarity between RLT and q. Fi-
nally, the top-ranked RL was assigned to each test
document.
</bodyText>
<footnote confidence="0.915245">
3Available online at http://www.gnu.org/software
/diction/diction.html.
</footnote>
<page confidence="0.998039">
15
</page>
<sectionHeader confidence="0.989727" genericHeader="method">
5 Empirical Results
</sectionHeader>
<bodyText confidence="0.972058105263158">
The results are presented below in Table 2. The
RL assigned to each Chinese document is com-
pared to the “true RL” of the English document, on
the assumption that translation does not affect the
readability level. Although only 7.8% of the RLs
were predicted accurately (i.e., the highest ranked
RL for the Chinese document corresponded iden-
tically to the RL of the translated English docu-
ment), over 50% were either perfectly accurate or
off by only one RL.
Correctly predicted RL 7.8%
RL off by 1 grade level 43.1%
RL off by 2 grade levels 18.4%
RL off by 3 grade levels 18.4%
RL off by 4 grade levels 6.1%
RL off by 5 grade levels 3.1%
RL off by 6 grade levels 0%
RL off by 7 grade levels 3.1%
RL off by 8 grade levels 0%
</bodyText>
<tableCaption confidence="0.6434205">
Table 2: Distribution of RLs as predicted by our
ECR system
</tableCaption>
<bodyText confidence="0.999967294117647">
This table motivates us to represent the results
in a more comprehensive fashion. Intuitively, the
system tends to succeed at assigning RLs near the
correct level, though not necessarily at the exact
level. To quantify this intuition, we used Root
Mean Squared Error (RMSE) to evaluate the ex-
perimental results. We compared our results to
two kinds of baseline RL assignments. The first
method was to randomly assign RLs 1000 times
and take the average of the RMSE obtained in
each assignment; this yielded an average RMSE
of 3.05. The second method used a fixed equal
distribution of the nine RLs, applying each RL to
each document an equal number of times, and tak-
ing the average of these results. This baseline re-
turned an average RMSE of 3.65. The average
RMSE of our ECR system’s performance on the
CCHRC Chinese documents is 2.48. This number
compares favorably against both of the baseline al-
gorithms.
Recall that the actual RL-tagging procedure has
been treated as a document retrieval task, using
Vector Space Cosine similarity. As such, RLs are
not simply “picked out” for each document: each
document receives a cosine similarity score for
each RL, calculated on the basis of its similarity to
the language model word set constructed for each
RL. For the results above, only the top ranked RL
was considered, as this would be the RL yielded if
the user wanted a discrete numeric value to assign
to the text. If we allow for enough flexibility to se-
lect the better of the two top-ranked RLs assigned
to each document by our ECR system, the results
are as given in Table 3.
</bodyText>
<table confidence="0.678482666666667">
Correctly predicted RL 10.8%
RL off by 1 grade level 49.2%
RL off by 2 grade levels 27.7%
RL off by 3 grade levels 7.7%
RL off by 4 grade levels 1.5%
RL off by 5 grade levels 0%
RL off by 6 grade levels 3.1%
RL off by 7 grade levels 0%
RL off by 8 grade levels 0%
</table>
<tableCaption confidence="0.9732075">
Table 3: RL Distribution (Best of Two Top-
Ranked RLs)
</tableCaption>
<bodyText confidence="0.998356444444444">
While this extra selection is certain to improve
the RMSE, what is surprising is the extent to
which the RMSE improves. Once again, RMSE
can be calculated in the following way. The two
top-ranked RLs for each document are taken into
consideration, and of these two RLs, the RL near-
est to the true RL is selected. Selecting the best of
the two top-ranked RLs causes the RMSE to drop
to 1.91.
</bodyText>
<sectionHeader confidence="0.9999" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.998111842105263">
The method described above builds on recent work
that has exploited the web and parallel corpora to
develop language technologies for minority lan-
guages (Trosterud (2002), inter alia).
Yarowsky et al. (2001) describe a system and
a set of algorithms for automatically deriving au-
tonomous monolingual POS-taggers, base noun-
phrase bracketers, named-entity taggers, and mor-
phological analyzers for an arbitrary target lan-
guage. Bilingual text corpora are treated with
existing text analysis tools for English, and their
output is projected onto the target language via
statistically derived word alignments. Their ap-
proach is especially interesting insofar as the sys-
tem does not require hand-annotation of target-
language training data or virtually any target-
language-specific knowledge or resources.
Martin et al. (2003) present an English-Inuktitut
aligned parallel corpus, demonstrating superior
</bodyText>
<page confidence="0.990274">
16
</page>
<bodyText confidence="0.999975111111111">
sentence alignment via Pointwise Mutual Informa-
tion (PMI). Their approach provides broad cov-
erage of cross-linguistic morphology, which has
implications for dictionary expansion tasks; prob-
lems encountered in dealing with the agglutina-
tive morphology of Inuktitut are suggestive of the
myriad issues arising from cross-language com-
parisons.
Rogati et al. (2003) present an unsupervised
learning approach to building an Arabic stemmer,
modeled on statistical machine translation. The
authors use an English stemmer and a small par-
allel corpus as training resources, with no parallel
text necessary after the training phase. Additional
monolingual texts can be incorporated to improve
the stemmer by allowing it to adapt to a specific
domain.
While Yarowsky et al. (2001), Martin et
al. (2003) and Rogati et al. (2003) all focus
on aligned parallel corpora, our approach dif-
fers in that we use comparable documents from
Wikipedia are linked thematically on the basis
of semantic content alone: there is no presumed
structural or lexical alignment between parallel
documents. We have adapted the methods used
in conjunction with aligned parallel corpora for
use with non-aligned parallel corpora to handle
the task pursued by Collins-Thompson and Callan
(2004), which presents a new approach to predict-
ing the RLs of a document by evaluating readabil-
ity in terms of statistical language modeling. Their
approach employs multiple language models to es-
timate the most likely RL for each document.
This approach contrasts with other previous
monolingual methods of calculating readability,
such as Chall and Dale (1995), which assesses
the readability of texts by calculating the percent-
age of terms that do not appear on a 3,000 word
list that 80% of tested fourth-grade students were
able to read. Similarly, Stenner et al. (1988) use
the word frequency information from a 5-million-
word corpus.
While our work has drawn from several tech-
niques employed in prior research, we have mainly
hybridized the technique of using parallel cor-
pus employed by Yarowsky (2001) and the lan-
guage modeling approach employed by Collins-
Thompson and Callan (2004). Our approach relies
on parallel corpora to build a readability classi-
fier for one language based on readability software
for another language. Rather than focusing on
language-specific readability classification based
on training data drawn from the same language
as the testing data (Collins-Thompson and Callan,
2004), we have constructed a radically extensible
tool that can easily create readability classifiers
for an arbitrary target language using training data
from a source language such as English. The result
is a system capable of allowing a user to construct
readability software for languages like Indonesian,
for example, even if that user does not speak In-
donesian — this is possible due to the large paral-
lel English-Indonesian corpus on Wikipedia.
</bodyText>
<sectionHeader confidence="0.999046" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999936342857143">
We have proposed a general framework to quickly
construct a standalone readability classifier for
an arbitrary (and possibly unfamiliar) language
using statistical language models based both on
monolingual and non-aligned parallel corpora. To
demonstrate the proposed idea, we developed an
Extensible Crosslingual Readability system. We
evaluated the system on the task of predicting
readability level of a set of Chinese medical docu-
ments. The experimental results show that the pre-
dicted RLs were correct or nearly correct for over
50% of the documents. This research is important
because it is the only technique we are aware of
that is capable of straightforwardly creating read-
ability labels for hundreds, or theoretically even
thousands, of different languages.
Although the general framework and architec-
ture of the proposed system are straightforward,
the details of implementation of the system mod-
ules could be further improved to achieve bet-
ter performance. For example, all target lan-
guage words are selected from a single “best-
matching document” using EXCLAIM in this pa-
per. Further experimentation might discover a
better word selection module. Future work may
also reveal delineation points for over- and under-
specialized sets of training data. The OHSU87
data set was selected on the basis of its medical
domain coverage, however it may not have pro-
vided broad enough coverage of the appropriate
domain-independent vocabulary in the CCHRC
documents. And finally, we conducted the ex-
periment using our own CLIR system, EXCLAIM,
while other CLIR systems might yield better re-
sults.
</bodyText>
<page confidence="0.998689">
17
</page>
<sectionHeader confidence="0.997378" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999067875">
The research reported here was partly supported
by NSF Grant #BCS-0846979 and the Institute of
Education Sciences, US Department of Education,
through Grant R305A00596 to the University of
California, Santa Cruz. Any opinions, findings,
conclusions or recommendations expressed in this
paper are the authors’, and do not necessarily re-
flect those of the sponsors.
</bodyText>
<sectionHeader confidence="0.999188" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999760078125">
Jonathan Anderson. 1981. Analysing the readability
of English and non-English texts in the classroom
with Lix. Paper presented at the Annual Meeting of
the Australian Reading Association.
C. H. Bj¨ornsson and Birgit H˚ard af Segerstad. 1979.
Lix p˚a Franska och tio andra spr˚ak. Pedagogiskt
centrum, Stockholms skolf¨orvaltning.
Jeanne S. Chall and Edgar Dale. 1995. Readability Re-
visited: The New Dale-Chall Readability Formula.
Brookline, Cambridge, Mass.
Chinese Community Health Resource Center. 2004.
CCHRC Medical Documents. Retrieved Decem-
ber 9, 2006, from http://www.cchphmo.com/
cchrchealth/index E.html.
Kevyn Collins-Thompson and Jamie Callan. 2004.
A language modeling approach to predicting read-
ing difficulty. In Proceedings ofHLT/NAACL 2004.
ACL.
Joseph H. Greenberg. 1954. A quantitative approach
to the morphological typology of language. In
Method and Perspective in Anthropology: Papers
in Honor of Wilson D. Wallis, pages 192–220, Min-
neapolis. University of Minnesota Press.
Barbara Grimes. 2005. Ethnologue: Languages of the
World, 15th ed. Summer Institute of Linguistics.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2006. Classroom
success of an intelligent tutoring system for lexical
practice and reading comprehension. In Proceed-
ings of the Ninth International Conference on Spo-
ken Language Processing.
Charles N. Li and Sandra Thompson. 1981. Mandarin
Chinese: A Functional Reference Grammar. Uni-
versity of California Press.
Joel Martin, Howard Johnson, Benoit Farley, and Anna
Maclachlan. 2003. Aligning and using an English-
Inuktitut parallel corpus. In Proceedings of the HLT-
NAACL 2003 workshop on building and using par-
allel texts: Data driven machine translation and be-
yond. ACL.
John Paolillo, Daniel Pimienta, and Daniel Prado.
2005. Measuring Linguistic Diversity on the Inter-
net. UNESCO, France.
Monica Rogati, Scott McCarley, and Yiming Yang.
2003. Unsupervised learning of arabic stemming us-
ing a parallel corpus. In Proceedings of the 41st an-
nual meeting of the Association for Computational
Linguistics. ACL.
A.J. Stenner, I. Horabin, D.R. Smith, and M. Smith.
1988. The Lexile Framework. Metametrics,
Durham, NC.
Trond Trosterud. 2002. Parallel corpora as tools for
investigating and developing minority languages. In
Parallel corpora, parallel worlds, pages 111–122.
Rodopi.
Wikimedia Foundation. 1999. Wikipedia, the
free encyclopedia. Retrieved May 8, 2006, from
http://en.wikipedia.org/.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the First International Conference
on Human Language Technology Research, pages
161–168.
</reference>
<page confidence="0.999281">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.728869">
<title confidence="0.999849">An Extensible Crosslinguistic Readability Framework</title>
<author confidence="0.999986">Jesse Saba</author>
<affiliation confidence="0.9983835">Department of UC Santa</affiliation>
<address confidence="0.9928795">1156 High Santa Cruz, CA</address>
<email confidence="0.999359">kirchner@ucsc.edu</email>
<author confidence="0.982184">Justin</author>
<affiliation confidence="0.998445">Department of UC Santa</affiliation>
<address confidence="0.9944495">1156 High Santa Cruz, CA</address>
<email confidence="0.999195">jnuger@ucsc.edu</email>
<author confidence="0.971147">Yi</author>
<affiliation confidence="0.9103315">Baskin School of UC Santa</affiliation>
<address confidence="0.986787">1156 High Street, SOE Santa Cruz, CA</address>
<email confidence="0.999144">yiz@soe.ucsc.edu</email>
<abstract confidence="0.999499703703704">Automatic assessment of the readability level (i.e., the relative linguistic complexity) of documents in a large number of languages is an important problem that can be applied to many real-world applications, such as retrieving age-appropriate search engine results for kids, constructing automatic tutoring systems, and so on. Unfortunately, existing readability labeling techniques have only been applied to a very small number of languages. In this paper, we present an extensible crosslinguistic readability framework based on the use of parallel corpora to quickly create readability software for thousands of languages, including languages for which no linguists are available to define readability rules or for which documents with readability labels are lacking to train readability models. To demonstrate our idea, we developed a system based on the proposed framework. This paper discusses the theoretical and practical issues involved in designing such a system and presents the results of an experiment conducted with the system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jonathan Anderson</author>
</authors>
<title>Analysing the readability of English and non-English texts in the classroom with Lix. Paper presented at the Annual Meeting of the Australian Reading Association.</title>
<date>1981</date>
<contexts>
<context position="3041" citStr="Anderson, 1981" startWordPosition="467" endWordPosition="468">ity levels (henceforth “RLs”) for these documents. Such software has significant potential for applications in different areas of research, such as creating web search engines for kids speaking languages not covered by existing readability software, as described above. There is much research on assessing the reading difficulties of texts in a particular language, and the existing work can be roughly classified as falling under two approaches. The first approach uses manually or semi-automatically crafted rules designed by computational linguists who are familiar with the language in question (Anderson, 1981). The second approach learns readability models for a particular language based on labeled data (Collins-Thompson and Callan, 2004). Unfortunately, existing approaches cannot be easily extended to handle thousands of different languages. The first approach, using rules devised by computational linguists familiar with the languages, is impractical because for many languages, especially minority or understudied languages, there are relatively few linguists sufficiently familiar with the language to design such software. Even if these linguists exist, it is unlikely that a search engine company t</context>
<context position="6578" citStr="Anderson (1981)" startWordPosition="1020" endWordPosition="1021">be substituted with other metrics, such as Flesch-Kincaid. where IID is a document written in an unfamiliar language, and RLD is the readability score of the document D. The above formula relies on specific parameters which have been tuned to a certain set of languages. These include the total number of words in IID (words), the total number of sentences in IID (sentences), and the total number of words in IID with more than six characters (words,h,&gt;s ). Although this formula may be successful in RL classification for languages like English and French (Bj¨ornsson and H˚ard af Segerstad(1979), Anderson (1981)), it remains essentially parochial in the context of other languages because the parameters overfit the data from the Western Euorpean languages for which it was designed. Since the LIX formula depends on measuring the number of characters in a word to find words greater than 6, it is ineffective in determining the readability of documents written in languages with different writing systems, such as Chinese. This is due to the fact that some languages, like Chinese, are written with characters based on semantic meaning rather than phonemes, as in English, and a large number of Chinese words c</context>
</contexts>
<marker>Anderson, 1981</marker>
<rawString>Jonathan Anderson. 1981. Analysing the readability of English and non-English texts in the classroom with Lix. Paper presented at the Annual Meeting of the Australian Reading Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H</author>
</authors>
<title>Bj¨ornsson and Birgit H˚ard af Segerstad.</title>
<date>1979</date>
<marker>H, 1979</marker>
<rawString>C. H. Bj¨ornsson and Birgit H˚ard af Segerstad. 1979. Lix p˚a Franska och tio andra spr˚ak. Pedagogiskt centrum, Stockholms skolf¨orvaltning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeanne S Chall</author>
<author>Edgar Dale</author>
</authors>
<title>Readability Revisited: The New Dale-Chall Readability Formula.</title>
<date>1995</date>
<location>Brookline, Cambridge, Mass.</location>
<contexts>
<context position="25864" citStr="Chall and Dale (1995)" startWordPosition="4256" endWordPosition="4259">there is no presumed structural or lexical alignment between parallel documents. We have adapted the methods used in conjunction with aligned parallel corpora for use with non-aligned parallel corpora to handle the task pursued by Collins-Thompson and Callan (2004), which presents a new approach to predicting the RLs of a document by evaluating readability in terms of statistical language modeling. Their approach employs multiple language models to estimate the most likely RL for each document. This approach contrasts with other previous monolingual methods of calculating readability, such as Chall and Dale (1995), which assesses the readability of texts by calculating the percentage of terms that do not appear on a 3,000 word list that 80% of tested fourth-grade students were able to read. Similarly, Stenner et al. (1988) use the word frequency information from a 5-millionword corpus. While our work has drawn from several techniques employed in prior research, we have mainly hybridized the technique of using parallel corpus employed by Yarowsky (2001) and the language modeling approach employed by CollinsThompson and Callan (2004). Our approach relies on parallel corpora to build a readability classif</context>
</contexts>
<marker>Chall, Dale, 1995</marker>
<rawString>Jeanne S. Chall and Edgar Dale. 1995. Readability Revisited: The New Dale-Chall Readability Formula. Brookline, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<title>Chinese Community Health Resource Center.</title>
<date>2004</date>
<journal>CCHRC Medical Documents. Retrieved</journal>
<note>from http://www.cchphmo.com/ cchrchealth/index E.html.</note>
<contexts>
<context position="17604" citStr="(2004)" startWordPosition="2847" endWordPosition="2847">h a typical user does not need to have access to sets of bilingual documents for the system to run successfully, we circumvented both the lack of off-the-shelf Chinese readability labeling software and the lack of labeled Chinese documents for the evaluation of the results of our system by using a high quality translated parallel document set. Since RLs are rough measures of semantic and structural complexity, we assume they should be approximately if not exactly the same for a given document and its translation in a different language, an extension of the ideas in Collins-Thompson and Callan (2004). Based on this assumption, we can accurately compare the RLs of the translated CCHRC Chinese medical documents to the RLs of the original English documents, which we call the “true RLs” of the testing documents. LIX-based RLs can be roughly mapped to grade levels, e.g., a text that is classified with an RL of 8 is appropriate for the average 8th grade reader. Since we can assign RLs to the English versions of the 65 CCHRC documents, these RLs can serve as targets to match when generating RLs for the corresponding Chinese versions of the same documents. An advantage of our system arises from a</context>
<context position="25508" citStr="(2004)" startWordPosition="4203" endWordPosition="4203"> can be incorporated to improve the stemmer by allowing it to adapt to a specific domain. While Yarowsky et al. (2001), Martin et al. (2003) and Rogati et al. (2003) all focus on aligned parallel corpora, our approach differs in that we use comparable documents from Wikipedia are linked thematically on the basis of semantic content alone: there is no presumed structural or lexical alignment between parallel documents. We have adapted the methods used in conjunction with aligned parallel corpora for use with non-aligned parallel corpora to handle the task pursued by Collins-Thompson and Callan (2004), which presents a new approach to predicting the RLs of a document by evaluating readability in terms of statistical language modeling. Their approach employs multiple language models to estimate the most likely RL for each document. This approach contrasts with other previous monolingual methods of calculating readability, such as Chall and Dale (1995), which assesses the readability of texts by calculating the percentage of terms that do not appear on a 3,000 word list that 80% of tested fourth-grade students were able to read. Similarly, Stenner et al. (1988) use the word frequency informa</context>
</contexts>
<marker>2004</marker>
<rawString>Chinese Community Health Resource Center. 2004. CCHRC Medical Documents. Retrieved December 9, 2006, from http://www.cchphmo.com/ cchrchealth/index E.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevyn Collins-Thompson</author>
<author>Jamie Callan</author>
</authors>
<title>A language modeling approach to predicting reading difficulty.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT/NAACL</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="3172" citStr="Collins-Thompson and Callan, 2004" startWordPosition="483" endWordPosition="486">ifferent areas of research, such as creating web search engines for kids speaking languages not covered by existing readability software, as described above. There is much research on assessing the reading difficulties of texts in a particular language, and the existing work can be roughly classified as falling under two approaches. The first approach uses manually or semi-automatically crafted rules designed by computational linguists who are familiar with the language in question (Anderson, 1981). The second approach learns readability models for a particular language based on labeled data (Collins-Thompson and Callan, 2004). Unfortunately, existing approaches cannot be easily extended to handle thousands of different languages. The first approach, using rules devised by computational linguists familiar with the languages, is impractical because for many languages, especially minority or understudied languages, there are relatively few linguists sufficiently familiar with the language to design such software. Even if these linguists exist, it is unlikely that a search engine company that wanted to serve the whole world would have the resources to 11 Proceedings of the 2nd Workshop on Building and Using Comparable</context>
<context position="7849" citStr="Collins-Thompson and Callan, 2004" startWordPosition="1228" endWordPosition="1231"> regardless of semantic complexity (Li and Thompson, 1981). In a similar vein, many languages of the world (even some that use phonemically-based writing systems) do not adhere to the implicit assumption of the LIX formula that semantically “complex” words are longer than simpler words (Greenberg, 1954). In these languages, then, the same metric cannot be used as a valid measure of RL difficulty of documents, since word length does not correlate with semantic complexity. One recent alternative approach has been developed for readability labeling that uses multiple statistical language models (Collins-Thompson and Callan, 2004). The idea is to train statistical language models for each grade level automatically from manually labeled training documents. However, even an approach like this is not scalable to handle thousands of languages, since it is hard to recruit annotators of all of these languages to manually label the training data. 2.2 Proposed Solution We propose a scalable solution to the problem of labeling the readability of documents in many languages. The general idea is to combine CLIR 100 × words,h,&gt;s words 12 technology with off-the-shelf readability software for at least one well-studied language, suc</context>
<context position="17604" citStr="Collins-Thompson and Callan (2004)" startWordPosition="2844" endWordPosition="2847">hinese translations. Although a typical user does not need to have access to sets of bilingual documents for the system to run successfully, we circumvented both the lack of off-the-shelf Chinese readability labeling software and the lack of labeled Chinese documents for the evaluation of the results of our system by using a high quality translated parallel document set. Since RLs are rough measures of semantic and structural complexity, we assume they should be approximately if not exactly the same for a given document and its translation in a different language, an extension of the ideas in Collins-Thompson and Callan (2004). Based on this assumption, we can accurately compare the RLs of the translated CCHRC Chinese medical documents to the RLs of the original English documents, which we call the “true RLs” of the testing documents. LIX-based RLs can be roughly mapped to grade levels, e.g., a text that is classified with an RL of 8 is appropriate for the average 8th grade reader. Since we can assign RLs to the English versions of the 65 CCHRC documents, these RLs can serve as targets to match when generating RLs for the corresponding Chinese versions of the same documents. An advantage of our system arises from a</context>
<context position="25508" citStr="Collins-Thompson and Callan (2004)" startWordPosition="4200" endWordPosition="4203">Additional monolingual texts can be incorporated to improve the stemmer by allowing it to adapt to a specific domain. While Yarowsky et al. (2001), Martin et al. (2003) and Rogati et al. (2003) all focus on aligned parallel corpora, our approach differs in that we use comparable documents from Wikipedia are linked thematically on the basis of semantic content alone: there is no presumed structural or lexical alignment between parallel documents. We have adapted the methods used in conjunction with aligned parallel corpora for use with non-aligned parallel corpora to handle the task pursued by Collins-Thompson and Callan (2004), which presents a new approach to predicting the RLs of a document by evaluating readability in terms of statistical language modeling. Their approach employs multiple language models to estimate the most likely RL for each document. This approach contrasts with other previous monolingual methods of calculating readability, such as Chall and Dale (1995), which assesses the readability of texts by calculating the percentage of terms that do not appear on a 3,000 word list that 80% of tested fourth-grade students were able to read. Similarly, Stenner et al. (1988) use the word frequency informa</context>
</contexts>
<marker>Collins-Thompson, Callan, 2004</marker>
<rawString>Kevyn Collins-Thompson and Jamie Callan. 2004. A language modeling approach to predicting reading difficulty. In Proceedings ofHLT/NAACL 2004. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph H Greenberg</author>
</authors>
<title>A quantitative approach to the morphological typology of language.</title>
<date>1954</date>
<booktitle>In Method and Perspective in Anthropology: Papers in Honor of</booktitle>
<pages>192--220</pages>
<publisher>University of Minnesota Press.</publisher>
<location>Minneapolis.</location>
<contexts>
<context position="7519" citStr="Greenberg, 1954" startWordPosition="1178" endWordPosition="1179">lity of documents written in languages with different writing systems, such as Chinese. This is due to the fact that some languages, like Chinese, are written with characters based on semantic meaning rather than phonemes, as in English, and a large number of Chinese words consist of just one or two characters, regardless of semantic complexity (Li and Thompson, 1981). In a similar vein, many languages of the world (even some that use phonemically-based writing systems) do not adhere to the implicit assumption of the LIX formula that semantically “complex” words are longer than simpler words (Greenberg, 1954). In these languages, then, the same metric cannot be used as a valid measure of RL difficulty of documents, since word length does not correlate with semantic complexity. One recent alternative approach has been developed for readability labeling that uses multiple statistical language models (Collins-Thompson and Callan, 2004). The idea is to train statistical language models for each grade level automatically from manually labeled training documents. However, even an approach like this is not scalable to handle thousands of languages, since it is hard to recruit annotators of all of these l</context>
</contexts>
<marker>Greenberg, 1954</marker>
<rawString>Joseph H. Greenberg. 1954. A quantitative approach to the morphological typology of language. In Method and Perspective in Anthropology: Papers in Honor of Wilson D. Wallis, pages 192–220, Minneapolis. University of Minnesota Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grimes</author>
</authors>
<date>2005</date>
<booktitle>Ethnologue: Languages of the World, 15th ed. Summer Institute of Linguistics.</booktitle>
<contexts>
<context position="1964" citStr="Grimes, 2005" startWordPosition="300" endWordPosition="301">igning such a system and presents the results of an experiment conducted with the system. 1 Introduction Automatically labeling the reading difficulty of an arbitrary document is an important problem in several human language technology applications. It can, for example, be used in the next generation of personalized information retrieval systems to find documents tailored to children at different grade levels. In a tutoring system, it can be used to find online reading materials of the appropriate difficulty level for students (Heilman et al., 2006). Of the world’s more than 6,000 languages (Grimes, 2005), readability classification software exists for a striking few, and it is limited in coverage to languages spoken in countries with prominent standing in global economics and politics. A substantial number of the remaining languages nevertheless have a sufficient corpus of digital documents — a number which may already be in the hundreds and soon in the thousands (Paolillo et al., 2005). A natural idea is to create software to automatically predict readability levels (henceforth “RLs”) for these documents. Such software has significant potential for applications in different areas of research</context>
</contexts>
<marker>Grimes, 2005</marker>
<rawString>Barbara Grimes. 2005. Ethnologue: Languages of the World, 15th ed. Summer Institute of Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Kevyn Collins-Thompson</author>
<author>Jamie Callan</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Classroom success of an intelligent tutoring system for lexical practice and reading comprehension.</title>
<date>2006</date>
<booktitle>In Proceedings of the Ninth International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="1907" citStr="Heilman et al., 2006" startWordPosition="289" endWordPosition="292">er discusses the theoretical and practical issues involved in designing such a system and presents the results of an experiment conducted with the system. 1 Introduction Automatically labeling the reading difficulty of an arbitrary document is an important problem in several human language technology applications. It can, for example, be used in the next generation of personalized information retrieval systems to find documents tailored to children at different grade levels. In a tutoring system, it can be used to find online reading materials of the appropriate difficulty level for students (Heilman et al., 2006). Of the world’s more than 6,000 languages (Grimes, 2005), readability classification software exists for a striking few, and it is limited in coverage to languages spoken in countries with prominent standing in global economics and politics. A substantial number of the remaining languages nevertheless have a sufficient corpus of digital documents — a number which may already be in the hundreds and soon in the thousands (Paolillo et al., 2005). A natural idea is to create software to automatically predict readability levels (henceforth “RLs”) for these documents. Such software has significant </context>
</contexts>
<marker>Heilman, Collins-Thompson, Callan, Eskenazi, 2006</marker>
<rawString>Michael Heilman, Kevyn Collins-Thompson, Jamie Callan, and Maxine Eskenazi. 2006. Classroom success of an intelligent tutoring system for lexical practice and reading comprehension. In Proceedings of the Ninth International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles N Li</author>
<author>Sandra Thompson</author>
</authors>
<title>Mandarin Chinese: A Functional Reference Grammar.</title>
<date>1981</date>
<publisher>University of California Press.</publisher>
<contexts>
<context position="7273" citStr="Li and Thompson, 1981" startWordPosition="1137" endWordPosition="1140">ause the parameters overfit the data from the Western Euorpean languages for which it was designed. Since the LIX formula depends on measuring the number of characters in a word to find words greater than 6, it is ineffective in determining the readability of documents written in languages with different writing systems, such as Chinese. This is due to the fact that some languages, like Chinese, are written with characters based on semantic meaning rather than phonemes, as in English, and a large number of Chinese words consist of just one or two characters, regardless of semantic complexity (Li and Thompson, 1981). In a similar vein, many languages of the world (even some that use phonemically-based writing systems) do not adhere to the implicit assumption of the LIX formula that semantically “complex” words are longer than simpler words (Greenberg, 1954). In these languages, then, the same metric cannot be used as a valid measure of RL difficulty of documents, since word length does not correlate with semantic complexity. One recent alternative approach has been developed for readability labeling that uses multiple statistical language models (Collins-Thompson and Callan, 2004). The idea is to train s</context>
</contexts>
<marker>Li, Thompson, 1981</marker>
<rawString>Charles N. Li and Sandra Thompson. 1981. Mandarin Chinese: A Functional Reference Grammar. University of California Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Martin</author>
<author>Howard Johnson</author>
<author>Benoit Farley</author>
<author>Anna Maclachlan</author>
</authors>
<title>Aligning and using an EnglishInuktitut parallel corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLTNAACL</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="24170" citStr="Martin et al. (2003)" startWordPosition="4001" endWordPosition="4004">l. (2001) describe a system and a set of algorithms for automatically deriving autonomous monolingual POS-taggers, base nounphrase bracketers, named-entity taggers, and morphological analyzers for an arbitrary target language. Bilingual text corpora are treated with existing text analysis tools for English, and their output is projected onto the target language via statistically derived word alignments. Their approach is especially interesting insofar as the system does not require hand-annotation of targetlanguage training data or virtually any targetlanguage-specific knowledge or resources. Martin et al. (2003) present an English-Inuktitut aligned parallel corpus, demonstrating superior 16 sentence alignment via Pointwise Mutual Information (PMI). Their approach provides broad coverage of cross-linguistic morphology, which has implications for dictionary expansion tasks; problems encountered in dealing with the agglutinative morphology of Inuktitut are suggestive of the myriad issues arising from cross-language comparisons. Rogati et al. (2003) present an unsupervised learning approach to building an Arabic stemmer, modeled on statistical machine translation. The authors use an English stemmer and a</context>
</contexts>
<marker>Martin, Johnson, Farley, Maclachlan, 2003</marker>
<rawString>Joel Martin, Howard Johnson, Benoit Farley, and Anna Maclachlan. 2003. Aligning and using an EnglishInuktitut parallel corpus. In Proceedings of the HLTNAACL 2003 workshop on building and using parallel texts: Data driven machine translation and beyond. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Paolillo</author>
<author>Daniel Pimienta</author>
<author>Daniel Prado</author>
</authors>
<date>2005</date>
<booktitle>Measuring Linguistic Diversity on the Internet. UNESCO,</booktitle>
<contexts>
<context position="2354" citStr="Paolillo et al., 2005" startWordPosition="361" endWordPosition="365">ildren at different grade levels. In a tutoring system, it can be used to find online reading materials of the appropriate difficulty level for students (Heilman et al., 2006). Of the world’s more than 6,000 languages (Grimes, 2005), readability classification software exists for a striking few, and it is limited in coverage to languages spoken in countries with prominent standing in global economics and politics. A substantial number of the remaining languages nevertheless have a sufficient corpus of digital documents — a number which may already be in the hundreds and soon in the thousands (Paolillo et al., 2005). A natural idea is to create software to automatically predict readability levels (henceforth “RLs”) for these documents. Such software has significant potential for applications in different areas of research, such as creating web search engines for kids speaking languages not covered by existing readability software, as described above. There is much research on assessing the reading difficulties of texts in a particular language, and the existing work can be roughly classified as falling under two approaches. The first approach uses manually or semi-automatically crafted rules designed by </context>
</contexts>
<marker>Paolillo, Pimienta, Prado, 2005</marker>
<rawString>John Paolillo, Daniel Pimienta, and Daniel Prado. 2005. Measuring Linguistic Diversity on the Internet. UNESCO, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monica Rogati</author>
<author>Scott McCarley</author>
<author>Yiming Yang</author>
</authors>
<title>Unsupervised learning of arabic stemming using a parallel corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st annual meeting of the Association for Computational Linguistics. ACL.</booktitle>
<contexts>
<context position="24612" citStr="Rogati et al. (2003)" startWordPosition="4061" endWordPosition="4064">sting insofar as the system does not require hand-annotation of targetlanguage training data or virtually any targetlanguage-specific knowledge or resources. Martin et al. (2003) present an English-Inuktitut aligned parallel corpus, demonstrating superior 16 sentence alignment via Pointwise Mutual Information (PMI). Their approach provides broad coverage of cross-linguistic morphology, which has implications for dictionary expansion tasks; problems encountered in dealing with the agglutinative morphology of Inuktitut are suggestive of the myriad issues arising from cross-language comparisons. Rogati et al. (2003) present an unsupervised learning approach to building an Arabic stemmer, modeled on statistical machine translation. The authors use an English stemmer and a small parallel corpus as training resources, with no parallel text necessary after the training phase. Additional monolingual texts can be incorporated to improve the stemmer by allowing it to adapt to a specific domain. While Yarowsky et al. (2001), Martin et al. (2003) and Rogati et al. (2003) all focus on aligned parallel corpora, our approach differs in that we use comparable documents from Wikipedia are linked thematically on the ba</context>
</contexts>
<marker>Rogati, McCarley, Yang, 2003</marker>
<rawString>Monica Rogati, Scott McCarley, and Yiming Yang. 2003. Unsupervised learning of arabic stemming using a parallel corpus. In Proceedings of the 41st annual meeting of the Association for Computational Linguistics. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Stenner</author>
<author>I Horabin</author>
<author>D R Smith</author>
<author>M Smith</author>
</authors>
<title>The Lexile Framework. Metametrics,</title>
<date>1988</date>
<location>Durham, NC.</location>
<contexts>
<context position="26077" citStr="Stenner et al. (1988)" startWordPosition="4293" endWordPosition="4296">e task pursued by Collins-Thompson and Callan (2004), which presents a new approach to predicting the RLs of a document by evaluating readability in terms of statistical language modeling. Their approach employs multiple language models to estimate the most likely RL for each document. This approach contrasts with other previous monolingual methods of calculating readability, such as Chall and Dale (1995), which assesses the readability of texts by calculating the percentage of terms that do not appear on a 3,000 word list that 80% of tested fourth-grade students were able to read. Similarly, Stenner et al. (1988) use the word frequency information from a 5-millionword corpus. While our work has drawn from several techniques employed in prior research, we have mainly hybridized the technique of using parallel corpus employed by Yarowsky (2001) and the language modeling approach employed by CollinsThompson and Callan (2004). Our approach relies on parallel corpora to build a readability classifier for one language based on readability software for another language. Rather than focusing on language-specific readability classification based on training data drawn from the same language as the testing data</context>
</contexts>
<marker>Stenner, Horabin, Smith, Smith, 1988</marker>
<rawString>A.J. Stenner, I. Horabin, D.R. Smith, and M. Smith. 1988. The Lexile Framework. Metametrics, Durham, NC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trond Trosterud</author>
</authors>
<title>Parallel corpora as tools for investigating and developing minority languages. In Parallel corpora, parallel worlds,</title>
<date>2002</date>
<pages>111--122</pages>
<publisher>Rodopi.</publisher>
<contexts>
<context position="23522" citStr="Trosterud (2002)" startWordPosition="3908" endWordPosition="3909">istribution (Best of Two TopRanked RLs) While this extra selection is certain to improve the RMSE, what is surprising is the extent to which the RMSE improves. Once again, RMSE can be calculated in the following way. The two top-ranked RLs for each document are taken into consideration, and of these two RLs, the RL nearest to the true RL is selected. Selecting the best of the two top-ranked RLs causes the RMSE to drop to 1.91. 6 Related Work The method described above builds on recent work that has exploited the web and parallel corpora to develop language technologies for minority languages (Trosterud (2002), inter alia). Yarowsky et al. (2001) describe a system and a set of algorithms for automatically deriving autonomous monolingual POS-taggers, base nounphrase bracketers, named-entity taggers, and morphological analyzers for an arbitrary target language. Bilingual text corpora are treated with existing text analysis tools for English, and their output is projected onto the target language via statistically derived word alignments. Their approach is especially interesting insofar as the system does not require hand-annotation of targetlanguage training data or virtually any targetlanguage-speci</context>
</contexts>
<marker>Trosterud, 2002</marker>
<rawString>Trond Trosterud. 2002. Parallel corpora as tools for investigating and developing minority languages. In Parallel corpora, parallel worlds, pages 111–122. Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wikimedia Foundation</author>
</authors>
<title>Wikipedia, the free encyclopedia. Retrieved</title>
<date>1999</date>
<note>from http://en.wikipedia.org/.</note>
<contexts>
<context position="14188" citStr="Foundation, 1999" startWordPosition="2284" endWordPosition="2285">used. The selected words RLS = {f1, f2, ...fK } form the basis for constructing an RL classification model for an unknown target language. In order to construct a target language RL classification model, the cross-lingual query expansion component of a CLIR system is necessary to select semantically comparable and semantically related words in the target language. The CLIR system we developed is called EXCLAIM, or the EXtensible Cross-Linguistic Automatic Information Machine. We constructed EXCLAIM from a semantically (though not structurally) parallel corpus crawled from Wikipedia (Wikimedia Foundation, 1999). All Wikipedia articles with both source and target language versions collectively function as data to construct the CLIR component. Due to Wikipedia’s coverage of a large amount of languages (English being the language with the largest collection of articles at the time of writing), CLIR components for English paired with a wide number of target languages was created for EXCLAIM. For each RLS, the query-expansion component of EXCLAIM determines a set of corresponding words for the target language RLT. Initially, each word in RLS is matched with the source language document in EXCLAIM for whi</context>
</contexts>
<marker>Foundation, 1999</marker>
<rawString>Wikimedia Foundation. 1999. Wikipedia, the free encyclopedia. Retrieved May 8, 2006, from http://en.wikipedia.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the First International Conference on Human Language Technology Research,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="23559" citStr="Yarowsky et al. (2001)" startWordPosition="3912" endWordPosition="3915">ked RLs) While this extra selection is certain to improve the RMSE, what is surprising is the extent to which the RMSE improves. Once again, RMSE can be calculated in the following way. The two top-ranked RLs for each document are taken into consideration, and of these two RLs, the RL nearest to the true RL is selected. Selecting the best of the two top-ranked RLs causes the RMSE to drop to 1.91. 6 Related Work The method described above builds on recent work that has exploited the web and parallel corpora to develop language technologies for minority languages (Trosterud (2002), inter alia). Yarowsky et al. (2001) describe a system and a set of algorithms for automatically deriving autonomous monolingual POS-taggers, base nounphrase bracketers, named-entity taggers, and morphological analyzers for an arbitrary target language. Bilingual text corpora are treated with existing text analysis tools for English, and their output is projected onto the target language via statistically derived word alignments. Their approach is especially interesting insofar as the system does not require hand-annotation of targetlanguage training data or virtually any targetlanguage-specific knowledge or resources. Martin et</context>
<context position="25020" citStr="Yarowsky et al. (2001)" startWordPosition="4124" endWordPosition="4127">tions for dictionary expansion tasks; problems encountered in dealing with the agglutinative morphology of Inuktitut are suggestive of the myriad issues arising from cross-language comparisons. Rogati et al. (2003) present an unsupervised learning approach to building an Arabic stemmer, modeled on statistical machine translation. The authors use an English stemmer and a small parallel corpus as training resources, with no parallel text necessary after the training phase. Additional monolingual texts can be incorporated to improve the stemmer by allowing it to adapt to a specific domain. While Yarowsky et al. (2001), Martin et al. (2003) and Rogati et al. (2003) all focus on aligned parallel corpora, our approach differs in that we use comparable documents from Wikipedia are linked thematically on the basis of semantic content alone: there is no presumed structural or lexical alignment between parallel documents. We have adapted the methods used in conjunction with aligned parallel corpora for use with non-aligned parallel corpora to handle the task pursued by Collins-Thompson and Callan (2004), which presents a new approach to predicting the RLs of a document by evaluating readability in terms of statis</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of the First International Conference on Human Language Technology Research, pages 161–168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>