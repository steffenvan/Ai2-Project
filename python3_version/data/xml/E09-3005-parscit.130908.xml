<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003221">
<title confidence="0.995467">
Structural Correspondence Learning for Parse Disambiguation
</title>
<author confidence="0.949329">
Barbara Plank
</author>
<affiliation confidence="0.931072">
Alfa-informatica
University of Groningen, The Netherlands
</affiliation>
<email confidence="0.986712">
b.plank@rug.nl
</email>
<sectionHeader confidence="0.993652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999869764705882">
The paper presents an application of
Structural Correspondence Learning
(SCL) (Blitzer et al., 2006) for domain
adaptation of a stochastic attribute-value
grammar (SAVG). So far, SCL has
been applied successfully in NLP for
Part-of-Speech tagging and Sentiment
Analysis (Blitzer et al., 2006; Blitzer
et al., 2007). An attempt was made
in the CoNLL 2007 shared task to ap-
ply SCL to non-projective dependency
parsing (Shimizu and Nakagawa, 2007),
however, without any clear conclusions.
We report on our exploration of applying
SCL to adapt a syntactic disambiguation
model and show promising initial results
on Wikipedia domains.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999715942307693">
Many current, effective natural language process-
ing systems are based on supervised Machine
Learning techniques. The parameters of such sys-
tems are estimated to best reflect the character-
istics of the training data, at the cost of porta-
bility: a system will be successful only as long
as the training material resembles the input that
the model gets. Therefore, whenever we have ac-
cess to a large amount of labeled data from some
“source” (out-of-domain), but we would like a
model that performs well on some new “target”
domain (Gildea, 2001; Daum´e III, 2007), we face
the problem of domain adaptation.
The need for domain adaptation arises in many
NLP tasks: Part-of-Speech tagging, Sentiment
Analysis, Semantic Role Labeling or Statistical
Parsing, to name but a few. For example, the per-
formance of a statistical parsing system drops in
an appalling way when a model trained on the Wall
Street Journal is applied to the more varied Brown
corpus (Gildea, 2001).
The problem itself has started to get attention
only recently (Roark and Bacchiani, 2003; Hara et
al., 2005; Daum´e III and Marcu, 2006; Daum´e III,
2007; Blitzer et al., 2006; McClosky et al., 2006;
Dredze et al., 2007). We distinguish two main ap-
proaches to domain adaptation that have been ad-
dressed in the literature (Daum´e III, 2007): super-
vised and semi-supervised.
In supervised domain adaptation (Gildea, 2001;
Roark and Bacchiani, 2003; Hara et al., 2005;
Daum´e III, 2007), besides the labeled source data,
we have access to a comparably small, but labeled
amount of target data. In contrast, semi-supervised
domain adaptation (Blitzer et al., 2006; McClosky
et al., 2006; Dredze et al., 2007) is the scenario in
which, in addition to the labeled source data, we
only have unlabeled and no labeled target domain
data. Semi-supervised adaptation is a much more
realistic situation, while at the same time also con-
siderably more difficult.
Studies on the supervised task have shown that
straightforward baselines (e.g. models based on
source only, target only, or the union of the data)
achieve a relatively high performance level and are
“surprisingly difficult to beat” (Daum´e III, 2007).
Thus, one conclusion from that line of work is that
as soon as there is a reasonable (often even small)
amount of labeled target data, it is often more fruit-
ful to either just use that, or to apply simple adap-
tation techniques (Daum´e III, 2007; Plank and van
Noord, 2008).
</bodyText>
<sectionHeader confidence="0.915715" genericHeader="introduction">
2 Motivation and Prior Work
</sectionHeader>
<bodyText confidence="0.99995675">
While several authors have looked at the super-
vised adaptation case, there are less (and espe-
cially less successful) studies on semi-supervised
domain adaptation (McClosky et al., 2006; Blitzer
et al., 2006; Dredze et al., 2007). Of these, Mc-
Closky et al. (2006) deal specifically with self-
training for data-driven statistical parsing. They
show that together with a re-ranker, improvements
</bodyText>
<note confidence="0.959829">
Proceedings of the EACL 2009 Student Research Workshop, pages 37–45,
Athens, Greece, 2 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998069">
37
</page>
<note confidence="0.512162333333333">
are obtained. Similarly, Structural Correspon-
dence Learning (Blitzer et al., 2006; Blitzer et
al., 2007; Blitzer, 2008) has proven to be suc-
</note>
<bodyText confidence="0.965685406779662">
cessful for the two tasks examined, PoS tagging
and Sentiment Classification. In contrast, Dredze
et al. (2007) report on “frustrating” results on
the CoNLL 2007 semi-supervised adaptation task
for dependency parsing, i.e. ”no team was able
to improve target domain performance substan-
tially over a state of the art baseline”. In the
same shared task, an attempt was made to ap-
ply SCL to domain adaptation for data-driven de-
pendency parsing (Shimizu and Nakagawa, 2007).
The system just ended up at rank 7 out of 8 teams.
However, based on annotation differences in the
datasets (Dredze et al., 2007) and a bug in their
system (Shimizu and Nakagawa, 2007), their re-
sults are inconclusive.1 Thus, the effectiveness of
SCL is rather unexplored for parsing.
So far, most previous work on domain adapta-
tion for parsing has focused on data-driven sys-
tems (Gildea, 2001; Roark and Bacchiani, 2003;
McClosky et al., 2006; Shimizu and Nakagawa,
2007), i.e. systems employing (constituent or de-
pendency based) treebank grammars (Charniak,
1996). Parse selection constitutes an important
part of many parsing systems (Johnson et al.,
1999; Hara et al., 2005; van Noord and Malouf,
2005; McClosky et al., 2006). Yet, the adaptation
of parse selection models to novel domains is a far
less studied area. This may be motivated by the
fact that potential gains for this task are inherently
bounded by the underlying grammar. The few
studies on adapting disambiguation models (Hara
et al., 2005; Plank and van Noord, 2008) have fo-
cused exclusively on the supervised scenario.
Therefore, the direction we explore in this
study is semi-supervised domain adaptation for
parse disambiguation. We examine the effec-
tiveness of Structural Correspondence Learning
(SCL) (Blitzer et al., 2006) for this task, a re-
cently proposed adaptation technique shown to be
effective for PoS tagging and Sentiment Analy-
sis. The system used in this study is Alpino, a
wide-coverage Stochastic Attribute Value Gram-
mar (SAVG) for Dutch (van Noord and Malouf,
2005; van Noord, 2006). For our empirical eval-
1As shown in Dredze et al. (2007), the biggest problem
for the shared task was that the provided datasets were an-
notated with different annotation guidelines, thus the gen-
eral conclusion was that the task was ill-defined (Nobuyuki
Shimizu, personal communication).
uation we explore Wikipedia as primary test and
training collection.
In the sequel, we first introduce the parsing sys-
tem. Section 4 reviews Structural Correspondence
Learning and shows our application of SCL to
parse selection, including all our design choices.
In Section 5 we present the datasets, introduce the
process of constructing target domain data from
Wikipedia, and discuss interesting initial empiri-
cal results of this ongoing study.
</bodyText>
<sectionHeader confidence="0.974636" genericHeader="method">
3 Background: Alpino parser
</sectionHeader>
<bodyText confidence="0.999398857142857">
Alpino (van Noord and Malouf, 2005; van Noord,
2006) is a robust computational analyzer for Dutch
that implements the conceptual two-stage parsing
approach. The system consists of approximately
800 grammar rules in the tradition of HPSG, and
a large hand-crafted lexicon, that together with a
left-corner parser constitutes the generation com-
ponent. For parse selection, Alpino employs a dis-
criminative approach based on Maximum Entropy
(MaxEnt). The output of the parser is dependency
structure based on the guidelines of CGN (Oost-
dijk, 2000).
The Maximum Entropy model (Berger et al.,
1996; Ratnaparkhi, 1997; Abney, 1997) is a con-
ditional model that assigns a probability to every
possible parse w for a given sentence s. The model
consists of a set of m feature functions fj(w) that
describe properties of parses, together with their
associated weights Bj. The denominator is a nor-
malization term where Y (s) is the set of parses
with yield s:
</bodyText>
<equation confidence="0.997612">
Ey∈Y (s) exp(Emj=1 Bjfj(y))) (1)
</equation>
<bodyText confidence="0.98505375">
The parameters (weights) Bj can be estimated
efficiently by maximizing the regularized condi-
tional likelihood of a training corpus (Johnson et
al., 1999; van Noord and Malouf, 2005):
</bodyText>
<equation confidence="0.9894615">
Em j=1
Bˆ = arg mθ log L(B) − 2σ2 B2j (2)
</equation>
<bodyText confidence="0.999969571428571">
where L(B) is the likelihood of the training data.
The second term is a regularization term (Gaus-
sian prior on the feature weights with mean zero
and variance σ). The estimated weights determine
the contribution of each feature. Features appear-
ing in correct parses are given increasing (posi-
tive) weight, while features in incorrect parses are
</bodyText>
<equation confidence="0.996853">
pθ(w|s; B) =
exp(Em j=1 Bjfj(w))
</equation>
<page confidence="0.984598">
38
</page>
<bodyText confidence="0.9999685">
given decreasing (negative) weight. Once a model
is trained, it can be applied to choose the parse
with the highest sum of feature weights.
The MaxEnt model consists of a large set of
features, corresponding to instantiations of feature
templates that model various properties of parses.
For instance, Part-of-Speech tags, dependency re-
lations, grammar rule applications, etc. The cur-
rent standard model uses about 11,000 features.
We will refer to this set of features as original fea-
tures. They are used to train the baseline model on
the given labeled source data.
</bodyText>
<sectionHeader confidence="0.982384" genericHeader="method">
4 Structural Correspondence Learning
</sectionHeader>
<bodyText confidence="0.991939951219512">
SCL (Structural Correspondence Learn-
ing) (Blitzer et al., 2006; Blitzer et al., 2007;
Blitzer, 2008) is a recently proposed domain
adaptation technique which uses unlabeled data
from both source and target domain to learn
correspondences between features from different
domains.
Before describing the algorithm in detail, let us
illustrate the intuition behind SCL with an exam-
ple, borrowed from Blitzer et al. (2007). Suppose
we have a Sentiment Analysis system trained on
book reviews (domain A), and we would like to
adapt it to kitchen appliances (domain B). Fea-
tures such as “boring” and “repetitive” are com-
mon ways to express negative sentiment in A,
while “not working” or “defective” are specific to
B. If there are features across the domains, e.g.
“don’t buy”, with which the domain specific fea-
tures are highly correlated with, then we might
tentatively align those features.
Therefore, the key idea of SCL is to identify au-
tomatically correspondences among features from
different domains by modeling their correlations
with pivot features. Pivots are features occur-
ring frequently and behaving similarly in both do-
mains (Blitzer et al., 2006). They are inspired by
auxiliary problems from Ando and Zhang (2005).
Non-pivot features that correspond with many of
the same pivot-features are assumed to corre-
spond. Intuitively, if we are able to find good cor-
respondences among features, then the augmented
labeled source domain data should transfer better
to a target domain (where no labeled data is avail-
able) (Blitzer et al., 2006).
The outline of the algorithm is given in Figure 1.
The first step is to identify m pivot features oc-
curring frequently in the unlabeled data of both
Input: -labeled source data {(xs, ys)Ns
s=1}
- unlabeled data from both source and
target domain xul = xs, xt
</bodyText>
<listItem confidence="0.999128">
1. Select m pivot features
2. Train m binary classifiers (pivot predictors)
3. Create matrix Wnxm of binary predictor
weight vectors W = [w1, .., wm], where n
is the number of nonpivot features in xul
4. Apply SVD to W: Wnxm =
UnxnDnxmVTmxm where 0 = UT[1:h,:]
are the h top left singular vectors of W.
5. Apply projection xs0 and train a predictor
on the original and new features obtained
through the projection.
</listItem>
<figureCaption confidence="0.998248">
Figure 1: SCL algorithm (Blitzer et al., 2006).
</figureCaption>
<bodyText confidence="0.999202483870968">
domains. Then, a binary classifier is trained for
each pivot feature (pivot predictor) of the form:
“Does pivot feature l occur in this instance?”. The
pivots are masked in the unlabeled data and the
aim is to predict them using non-pivot features.
In this way, we obtain a weight vector w for each
pivot predictor. Positive entries in the weight vec-
tor indicate that a non-pivot is highly correlated
with the respective pivot feature. Step 3 is to ar-
range the m weight vectors in a matrix W, where
a column corresponds to a pivot predictor weight
vector. Applying the projection WTx (where x
is a training instance) would give us m new fea-
tures, however, for “both computational and sta-
tistical reasons” (Blitzer et al., 2006; Ando and
Zhang, 2005) a low-dimensional approximation of
the original feature space is computed by applying
Singular Value Decomposition (SVD) on W (step
4). Let 0 = UThxn be the top h left singular vec-
tors of W (with h a dimension parameter and n
the number of non-pivot features). The resulting 0
is a projection onto a lower dimensional space ][8h,
parameterized by h.
The final step of SCL is to train a linear predic-
tor on the augmented labeled source data (x, 0x).
In more detail, the original feature space x is aug-
mented with h new features obtained by apply-
ing the projection 0x. In this way, we can learn
weights for domain-specific features, which oth-
erwise would not have been observed. If 0 con-
tains meaningful correspondences, then the pre-
</bodyText>
<page confidence="0.996576">
39
</page>
<bodyText confidence="0.9985334">
dictor trained on the augmented data should trans-
fer well to the new domain.
t = 5000. In this way we obtained on average 360
pivot features, on the datasets described in Sec-
tion 5.
</bodyText>
<subsectionHeader confidence="0.912501">
4.1 SCL for Parse Disambiguation
</subsectionHeader>
<bodyText confidence="0.999972791044776">
A property of the pivot predictors is that they can
be trained from unlabeled data, as they represent
properties of the input. So far, pivot features on the
word level were used (Blitzer et al., 2006; Blitzer
et al., 2007; Blitzer, 2008), e.g. “Does the bigram
not buy occur in this document?” (Blitzer, 2008).
Pivot features are the key ingredient for SCL,
and they should align well with the NLP task. For
PoS tagging and Sentiment Analysis, features on
the word level are intuitively well-related to the
problem at hand. For the task of parse disambigua-
tion based on a conditional model this is not the
case.
Hence, we actually introduce an additional and
new layer of abstraction, which, we hypothesize,
aligns well with the task of parse disambiguation:
we first parse the unlabeled data. In this way we
obtain full parses for given sentences as produced
by the grammar, allowing access to more abstract
representations of the underlying pivot predictor
training data (for reasons of efficiency, we here use
only the first generated parse as training data for
the pivot predictors, rather than n-best).
Thus, instead of using word-level features, our
features correspond to properties of the gener-
ated parses: application of grammar rules (r1,r2
features), dependency relations (dep), PoS tags
(f1,f2), syntactic features (s1), precedence (mf),
bilexical preferences (z), apposition (appos) and
further features for unknown words, temporal
phrases, coordination (h,in year and p1, respec-
tively). This allows us to get a possibly noisy,
but more abstract representation of the underlying
data. The set of features used in Alpino is further
described in van Noord and Malouf (2005).
Selection of pivot features As pivot features
should be common across domains, here we re-
strict our pivots to be of the type r1,p1,s1 (the most
frequently occurring feature types). In more de-
tail, r1 indicates which grammar rule applied, p1
whether coordination conjuncts are parallel, and
s1 whether topicalization or long-distance depen-
dencies occurred. We count how often each fea-
ture appears in the parsed source and target do-
main data, and select those r1,p1,s1 features as
pivot features, whose count is &gt; t, where t is a
specified threshold. In all our experiments, we set
Predictive features As pointed out by Blitzer et
al. (2006), each instance will actually contain fea-
tures which are totally predictive of the pivot fea-
tures (i.e. the pivot itself). In our case, we ad-
ditionally have to pay attention to ’more specific’
features, e.g. r2 is a feature that extends r1, in the
sense that it incorporates more information than
its parent (i.e. which grammar rules applied in the
construction of daughter nodes). It is crucial to re-
move these predictive features when creating the
training data for the pivot predictors.
Matrix and SVD Following Blitzer et al. (2006)
(which follow Ando and Zhang (2005)), we only
use positive entries in the pivot predictors weight
vectors to compute the SVD. Thus, when con-
structing the matrix W, we disregard all nega-
tive entries in W and compute the SVD (W =
UDVT) on the resulting non-negative sparse ma-
trix. This sparse representation saves both time
and space.
</bodyText>
<subsectionHeader confidence="0.989161">
4.2 Further practical issues of SCL
</subsectionHeader>
<bodyText confidence="0.99147152">
In practice, there are more free parameters and
model choices (Ando and Zhang, 2005; Ando,
2006; Blitzer et al., 2006; Blitzer, 2008) besides
the ones discussed above.
Feature normalization and feature scaling.
Blitzer et al. (2006) found it necessary to normal-
ize and scale the new features obtained by the pro-
jection O, in order to “allow them to receive more
weight from a regularized discriminative learner”.
For each of the features, they centered them by
subtracting out the mean and normalized them to
unit variance (i.e. x − mean/sd). They then
rescaled the features by a factor α found on held-
out data: αOx.
Restricted Regularization. When training the
supervised model on the augmented feature space
(x, Ox), Blitzer et al. (2006) only regularize the
weight vector of the original features, but not
the one for the new low-dimensional features.
This was done to encourage the model to use
the new low-dimensional representation rather
than the higher-dimensional original representa-
tion (Blitzer, 2008).
Dimensionality reduction by feature type. An
extension suggested in Ando and Zhang (2005) is
</bodyText>
<page confidence="0.992446">
40
</page>
<bodyText confidence="0.89222875">
to compute separate SVDs for blocks of the matrix
W corresponding to feature types (as illustrated in
Figure 2), and then to apply separate projection
for every type. Due to the positive results in Ando
(2006), Blitzer et al. (2006) include this in their
standard setting of SCL and report results using
block SVDs only.
Figure 2: Illustration of dimensionality reduction
by feature type (Ando and Zhang, 2005). The grey
area corresponds to a feature type (submatrix of
W) on which the SVD is computed (block SVD);
the white area is regarded as fixed to zero matrices.
</bodyText>
<sectionHeader confidence="0.997455" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.923285">
5.1 Experimental design
</subsectionHeader>
<bodyText confidence="0.999871434782609">
The base (source domain) disambiguation model
is trained on the Alpino Treebank (van Noord,
2006) (newspaper text), which consists of ap-
proximately 7,000 sentences and 145,000 tokens.
For parameter estimation of the disambiguation
model, in all reported experiments we use the
TADM2 toolkit (toolkit for advanced discrimina-
tive training), with a Gaussian prior (U2=1000)
and the (default) limited memory variable metric
estimation technique (Malouf, 2002).
For training the binary pivot predictors, we use
the MegaM3 Optimization Package with the so-
called ”bernoulli implicit” input format. To com-
pute the SVD, we use SVDLIBC.4
The output of the parser is dependency struc-
ture. A standard evaluation metric is to measure
the amount of generated dependencies that are
identical to the stored dependencies (correct la-
beled dependencies), expressed as f-score. An al-
ternative measure is concept accuracy (CA), which
is similar to f-score, but allows possible discrep-
ancy between the number of returned dependen-
cies (van Noord, 2006; Plank and van Noord,
</bodyText>
<footnote confidence="0.999949333333333">
2http://tadm.sourceforge.net/
3http://www.cs.utah.edu/-hal/megam/
4http://tedlab.mit.edu/-dr/svdlibc/
</footnote>
<bodyText confidence="0.987243833333333">
2008). CA is usually slightly lower than f-score.
Let DZp be the number of dependencies produced
by the parser for sentence i. DZg is the number of
dependencies in the treebank parse, and DZo is the
number of correct dependencies produced by the
parser. Then,
</bodyText>
<equation confidence="0.773007">
Do
CA = EZ max(DZ�, DZp)
</equation>
<bodyText confidence="0.999919">
If we want to compare the performance of dis-
ambiguation models, we can employ the 0 mea-
sure (van Noord and Malouf, 2005; van Noord,
2007). Intuitively, it tells us how much of the dis-
ambiguation problem has been solved.
</bodyText>
<equation confidence="0.964577">
CA − base
0 =
× 100
</equation>
<bodyText confidence="0.984886076923077">
oracle − base
In more detail, the 0 measure incorporates an up-
per and lower bound: base measures the accu-
racy of a model that simply selects the first parse
for each sentence; oracle represents the accuracy
achieved by a model that always selects the best
parse from the set of potential parses (within the
coverage of the parser). In addition, we also re-
port relative error reduction (rel.er), which is the
relative difference in 0 scores for two models.
As target domain, we consider the Dutch part
of Wikipedia as data collection, described in the
following.
</bodyText>
<subsectionHeader confidence="0.996907">
5.2 Wikipedia as resource
</subsectionHeader>
<bodyText confidence="0.998218894736842">
In our experiments, we exploit Wikipedia both as
testset and as unlabeled data source. We assume
that in order to parse data from a very specific do-
main, say about the artist Prince, then data related
to that domain, like information about the New
Power Generation, the Purple rain movie, or other
American singers and artists, should be of help.
Thus, we exploit Wikipedia and its category sys-
tem to gather domain-specific target data.
Construction of target domain data In more
detail, we use the Dutch part of Wikipedia pro-
vided by WikiXML,5 a collection of Wikipedia ar-
ticles converted to XML format. As the corpus is
encoded in XML, we can exploit general purpose
XML Query Languages, such as XQuery, Xslt and
XPath, to extract relevant information from the
Wikipedia corpus.
Given a wikipage p, with c E categories(p),
we can identify pages related to p of various
</bodyText>
<footnote confidence="0.975673">
5http://ilps.science.uva.nl/WikiXML/
</footnote>
<page confidence="0.99949">
41
</page>
<bodyText confidence="0.994757111111111">
types of ’relatedness’: directly related pages (those
that share a category, i.e. all p′ where Ic′ E
categories(p′) such that c = c′), or alterna-
tively, pages that share a sub- or supercategory
of p, i.e. p′ where c′ E categories(p′) and c′ E
sub categories(p) or c′ E super categories(p).
For example, Figure 3 shows the categories ex-
tracted for the Wikipedia article about pope Jo-
hannes Paulus II.
</bodyText>
<figure confidence="0.980837818181818">
&lt;wikipage id=&amp;quot;6677&amp;quot;&gt;
&lt;cat t=&amp;quot;direct&amp;quot; n=&amp;quot;Categorie:Paus&amp;quot;/&gt;
&lt;cat t=&amp;quot;direct&amp;quot; n=&amp;quot;Categorie:Pools_theoloog&amp;quot;/&gt;
&lt;cat t=&amp;quot;super&amp;quot; n=&amp;quot;Categorie:Religieus leider&amp;quot;/&gt;
&lt;cat t=&amp;quot;super&amp;quot; n=&amp;quot;Categorie:Rooms-katholiek persoon&amp;quot;/&gt;
&lt;cat t=&amp;quot;super&amp;quot; n=&amp;quot;Categorie:Vaticaanstad&amp;quot;/&gt;
&lt;cat t=&amp;quot;super&amp;quot; n=&amp;quot;Categorie:Bisschop&amp;quot;/&gt;
&lt;cat t=&amp;quot;super&amp;quot; n=&amp;quot;Categorie:Kerkgeschiedenis&amp;quot;/&gt;
&lt;cat t=&amp;quot;sub&amp;quot; n=&amp;quot;Categorie:Tegenpaus&amp;quot;/&gt;
&lt;cat t=&amp;quot;super&amp;quot; n=&amp;quot;Categorie:Pools persoon&amp;quot;/&gt;
&lt;/wikipage&gt;
</figure>
<figureCaption confidence="0.919805">
Figure 3: Example of extracted Wikipedia cate-
gories for a given article (direct, sup- and subcats).
</figureCaption>
<bodyText confidence="0.992911">
To create the set of related pages for a given ar-
ticle p, we proceed as follows:
</bodyText>
<listItem confidence="0.99943375">
1. Find sub- and supercategories of p
2. Extract all pages that are related to p (through
sharing a direct, sub or super category)
3. Optionally, filter out certain pages
</listItem>
<bodyText confidence="0.999203619047619">
In our empirical setup, we followed Blitzer et al.
(2006) and tried to balance the size of source and
target data. Thus, depending on the size of the re-
sulting target domain dataset, and the “broadness”
of the categories involved in creating it, we might
wish to filter out certain pages. We implemented
a filter mechanism that excludes pages of a cer-
tain category (e.g. a supercategory that is hypoth-
esized to be “too broad”). Alternatively, we might
have used a filter mechanism that excludes certain
pages directly.
In our experiments, we always included pages
that are directly related to a page of inter-
est, and those that shared a subcategory. Of
course, the page itself is not included in that
dataset. With regard to supercategories, we usu-
ally included all pages having a category c E
super categories(p), unless stated otherwise.
Test collection Our testset consists of a selection
of Wikipedia articles that have been manually cor-
rected in the course of the D-Coi/LASSY project.6
</bodyText>
<footnote confidence="0.942987">
6Ongoing project, see http://www.let.rug.nl/
∼vannoord/Lassy/
</footnote>
<bodyText confidence="0.9905175">
An overview of the testset including size indica-
tions is given in Table 1. Table 2 provides infor-
mation on the target domain datasets constructed
from Wikipedia.
</bodyText>
<table confidence="0.9982665">
Wiki/DCOIID Title Sents
6677/026563 Prince (musician) 358
6729/036834 Paus Johannes Paulus II 232
182654/041235 Augustus De Morgan 259
</table>
<tableCaption confidence="0.997432">
Table 1: Size of test datasets.
</tableCaption>
<table confidence="0.999353">
Related to Articles Sents Tokens Relationship
Prince 290 9,772 145,504 filtered super
Paus 445 8,832 134,451 all
De Morgan 394 8,466 132,948 all
</table>
<tableCaption confidence="0.991361666666667">
Table 2: Size of related unlabeled data; relation-
ship indicates whether all related pages are used
or some are filtered out (see section 5.2).
</tableCaption>
<subsectionHeader confidence="0.997512">
5.3 Empirical Results
</subsectionHeader>
<bodyText confidence="0.998274875">
For all reported results, we randomly select n =
200 maximum number of parses per sentence for
evaluation.
Baseline accuracies Table 3 shows the baseline
performance (of the standard Alpino model) on the
various Wikipedia testsets (CA, f-score). The third
and fourth column indicate the upper- and lower
bound measures (defined in section 5.1).
</bodyText>
<table confidence="0.99767875">
Title CA f-score base oracle
Prince (musician) 85.03 85.38 71.95 88.70
Paus Johannes Paulus II 85.72 86.32 74.30 89.09
Augustus De Morgan 80.09 80.61 70.08 83.52
</table>
<tableCaption confidence="0.999618">
Table 3: Baseline results.
</tableCaption>
<bodyText confidence="0.999860214285714">
While the parser normally operates on an accu-
racy level of roughly 88-89% (van Noord, 2007)
on its own domain (newspaper text), the accu-
racy on these subdomains drops to around 85%.
The biggest performance decrease (to 80%) was
on the article about the British logician and math-
ematician De Morgan. This confirms the intu-
ition that this specific subdomain is the “hardest”,
given that mathematical expressions might emerge
in the data (e.g. “Wet der distributiviteit : a(b+c)
= ab+ac” - distributivity law).
SCL results Table 4 shows the results of our in-
stantiation of SCL for parse disambiguation, with
varying h parameter (dimensionality parameter;
</bodyText>
<page confidence="0.997913">
42
</page>
<bodyText confidence="0.99029058">
h = 25 means that applying the projection x� re-
sulted in adding 25 new features to every source
domain instance).
CA f-score φ rel.er.
baseline Prince 85.03 85.38 78.06 0.00
SCL[+/-], h = 25 85.12 85.46 78.64 2.64
SCL[+/-], h = 50 85.29 85.63 79.66 7.29
SCL[+/-], h = 100 85.19 85.53 79.04 4.47
SCL[+/-], h = 200 85.21 85.54 79.18 5.10
baseline Paus 85.72 86.32 77.23 0.00
SCL[+/-], h = 25 85.87 86.48 78.26 4.52
SCL[+/-], h = 50 85.82 86.43 77.87 2.81
SCL[+/-], h = 100 85.87 86.49 78.26 4.52
SCL[+/-], h = 200 85.87 86.48 78.26 4.52
baseline DeMorgan 80.09 80.61 74.44 0.00
SCL[+/-], h = 25 80.15 80.67 74.92 1.88
SCL[+/-], h = 50 80.12 80.64 74.68 0.94
SCL[+/-], h = 100 80.12 80.64 74.68 0.94
SCL[+/-], h = 200 80.15 80.67 74.91 1.88
Table 4: Results of our instantiation of SCL (with
varying h parameter and no feature normaliza-
tion).
The results show a (sometimes) small but con-
sistent increase in absolute performance on all
testsets over the baseline system (up to +0.26
absolute CA score), as well as an increase in 0
measure (absolute error reduction). This corre-
sponds to a relative error reduction of up to 7.29%.
Thus, our first instantiation of SCL for parse dis-
ambiguation indeed shows promising results.
We can confirm that changing the dimensional-
ity parameter h has rather little effect (Table 4),
which is in line with previous findings (Ando and
Zhang, 2005; Blitzer et al., 2006). Thus we might
fix the parameter and prefer smaller dimensionali-
ties, which saves space and time.
Note that these results were obtained without
any of the additional normalization, rescaling,
feature-specific regularization, or block SVD is-
sues, etc. (discussed in section 4.2). We used the
same Gaussian regularization term (U2=1000) for
all features (original and new features), and did
not perform any feature normalization or rescal-
ing. This means our current instantiation of SCL
is an actually simplified version of the original
SCL algorithm, applied to parse disambiguation.
Of course, our results are preliminary and, rather
than warranting many definite conclusions, en-
courage further exploration of SCL and related
semi-supervised adaptation techniques.
</bodyText>
<subsectionHeader confidence="0.998175">
5.4 Additional Empirical Results
</subsectionHeader>
<bodyText confidence="0.99911124">
In the following, we describe additional results ob-
tained by extensions and/or refinements of our cur-
rent SCL instantiation.
Feature normalization. We also tested fea-
ture normalization (as described in Section 4.2).
While Blitzer et al. (2006) found it necessary to
normalize (and scale) the projection features, we
did not observe any improvement by normalizing
them (actually, it slightly degraded performance in
our case). Thus, we found this step unnecessary,
and currently did not look at this issue any further.
A look at 0 To gain some insight of which kind
of correspondences SCL learned in our case, we
started to examine the rows of 0. Recall that ap-
plying a row of the projection matrix BZ to a train-
ing instance x gives us a new real-valued fea-
ture. If features from different domains have sim-
ilar entries (scores) in the projection row, they
are assumed to correspond (Blitzer, 2008). Fig-
ure 4 shows example of correspondences that SCL
found in the Prince dataset. The first column rep-
resents the score of a feature. The labels wiki
and alp indicate the domain of the features, re-
spectively. For readability, we here grouped the
features obtaining similar scores.
</bodyText>
<figure confidence="0.801811666666667">
0.000217698|dep34(tournee,noun,hd/su,verb)|1|wiki
0.000223301|dep34(regisseur,noun,hd/su,verb)|15|wiki
0.000224517|dep34(voorsprong,noun,hd/su,verb)|2|alp
0.000224684|dep34(wetenschap,noun,hd/su,verb)|2|alp
0.000226617|dep34(pop_rock,noun,hd/su,verb)|1|wiki
0.000228918|dep34(plan,noun,hd/su,verb)|9|alp
</figure>
<figureCaption confidence="0.999097">
Figure 4: Example projection from 0 (row 2).
</figureCaption>
<bodyText confidence="0.999944454545454">
SCL clustered information about ’Chaka Khan’,
an ’Afro-Amerikaanse’ ’zangeres’ (afro-american
singer) whose real name is ’Yvette Marie
Stevens’. She had close connections to Prince,
who even wrote one of her singles. These features
got aligned to the Alpino feature ’leraar’ (teacher).
Moreover, SCL finds that ’tournee’, ’regisseur’
and ’pop rock’ in the Prince domain behave like
’voorsprong’ (advance), ’wetenschap’ (research)
and ’plan’ as possible heads in a subject relation
in the newspaper domain. Similarly, correspon-
</bodyText>
<figure confidence="0.99836525">
0.00010248|dep35(’Chaka Khan’,name(’PER’),hd/su,verb,ben)|wiki
0.00010248|dep35(de,det,hd/det,adj,’Afro-Amerikaanse’)|wiki
0.00010248|dep35(’Yvette Marie Stevens’,name(’PER’),hd/app,
noun,zangeres)|wiki
0.000102772|dep34(leraar,noun,hd/su,verb)|alp
0.000161095|dep34(commissie,noun,hd/obj1,prep)|16|alp
0.00016113|dep34(’Confessions Tour’,name,hd/obj1,prep)|2|wiki
0.000161241|dep34(orgel,noun,hd/obj1,prep)|1|wiki
</figure>
<page confidence="0.999757">
43
</page>
<bodyText confidence="0.999307375">
dences between the direct object features ’Con-
fessions Tour’ and ’orgel’ (pipe organ) to ’com-
missie’ (commission) are discovered.
More unlabeled data In the experiments so far,
we balanced the amount of source and target data.
We started to examine the effect of more unla-
beled target domain data. For the Prince dataset,
we included all supercategories in constructing
the related target domain data. The so obtained
dataset contains: 859 articles, 29,186 sentences
and 385,289 tokens; hence, the size approximately
tripled (w.r.t. Table 2). Table 5 shows the effect of
using this larger dataset for SCL with h = 25. The
accuracy increases (from 85.12 to 85.25). Thus,
there seems to be a positive effect (to be investi-
gated further).
</bodyText>
<table confidence="0.959846666666667">
CA f-score φ rel.er.
baseline Prince 85.03 85.38 78.06 0.00
SCL[+/-], h = 25, all 85.25 85.58 79.42 6.20
</table>
<tableCaption confidence="0.993119">
Table 5: First result on increasing unlabeled data.
</tableCaption>
<bodyText confidence="0.99983068">
Dimensionality reduction by feature type We
have started to implement the extension discussed
in section 4.2, i.e. perform separate dimension-
ality reductions based on blocks of nonpivot fea-
tures. We clustered nonpivots (see section 4.1 for
a description) into 9 types (ordered in terms of
decreasing cluster size): dep, f1/f2 (pos), r1/r2
(rules), appos person, mf, z, h1, in year, dist. For
each type, a separate SVD was computed on sub-
matrix Wt (illustrated in Figure 2). Then, sepa-
rate projections were applied to every training in-
stance.
The results of these experiments on the Prince
dataset are shown in Figure 5. Applying SCL with
dimensionality reduction by feature type (SCL
block) results in a model that performs better (CA
85.27, φ 79.52, rel.er. 6.65%) than the model with
no feature split (no block SVDs), thus obtaining a
relative error reduction of 6.65% over the baseline.
The same figure also shows what happens if we
remove a specific feature type at a time; the appo-
sition features contribute the most on this Prince
domain. As a fact, one third of the sentences in
the Prince testset contain constructions with appo-
sitions (e.g. about film-, album- and song titles).
</bodyText>
<sectionHeader confidence="0.995618" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.993857871794872">
The paper presents an application of Structural
Correspondence Learning (SCL) to parse disam-
Figure 5: Results of dimensionality reduction by
feature type, h = 25; block SVD included all 9
feature types; the right part shows the accuracy
when one feature type was removed.
biguation. While SCL has been successfully
applied to PoS tagging and Sentiment Analy-
sis (Blitzer et al., 2006; Blitzer et al., 2007), its
effectiveness for parsing was rather unexplored.
The empirical results show that our instantiation
of SCL to parse disambiguation gives promising
initial results, even without the many additional
extensions on the feature level as done in Blitzer
et al. (2006). We exploited Wikipedia as pri-
mary resource, both for collecting unlabeled tar-
get domain data, as well as test suite for empirical
evaluation. On the three examined datasets, SCL
slightly but constantly outperformed the baseline.
Applying SCL involves many design choices and
practical issues, which we tried to depict here in
detail. A novelty in our application is that we
first actually parse the unlabeled data from both
domains. This allows us to get a possibly noisy,
but more abstract representation of the underlying
data on which the pivot predictors are trained.
In the near future, we plan to extend the work on
semi-supervised domain adaptation for parse dis-
ambiguation, viz. (1) further explore/refine SCL
(block SVDs, varying amount of target domain
data, other testsets, etc.), and (2) examine self-
training. Studies on the latter have focused mainly
on generative, constituent based, i.e. data-driven
parsing systems. Furthermore, from a machine
learning point of view, it would be interesting to
know a measure of corpus similarity to estimate
the success of porting an NLP system from one do-
main to another. This relates to the general ques-
tion of what is meant by domain.
</bodyText>
<page confidence="0.998914">
44
</page>
<sectionHeader confidence="0.995899" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998985">
Steven P. Abney. 1997. Stochastic attribute-value
grammars. Computational Linguistics, 23:597–618.
Rie Kubota Ando and Tong Zhang. 2005. A frame-
work for learning predictive structures from multi-
ple tasks and unlabeled data. Journal of Machine
Learning Research, 6:1817–1853.
Rie Kubota Ando. 2006. Applying alternating struc-
ture optimization to word sense disambiguation. In
Proceedings of the 10th Conference on Computa-
tional Natural Language Learning (CoNLL).
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguis-
tics, 22(1):39–72.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Conference on Empirical Meth-
ods in Natural Language Processing, Sydney, Aus-
tralia.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Association for Computational Linguis-
tics, Prague, Czech Republic.
John Blitzer. 2008. Domain Adaptation of Natural
Language Processing Systems. Ph.D. thesis, Uni-
versity of Pennsylvania.
Eugene Charniak. 1996. Tree-bank grammars. In In
Proceedings of the Thirteenth National Conference
on Artificial Intelligence, pages 1031–1036.
Hal Daum´e III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101–126.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Conference of the Association for Compu-
tational Linguistics (ACL), Prague, Czech Republic.
Mark Dredze, John Blitzer, Pratha Pratim Taluk-
dar, Kuzman Ganchev, Joao Graca, and Fernando
Pereira. 2007. Frustratingly hard domain adap-
tation for parsing. In Proceedings of the CoNLL
Shared Task Session - Conference on Natural Lan-
guage Learning, Prague, Czech Republic.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Tadayoshi Hara, Miyao Yusuke, and Jun’ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an hpsg parser to a new domain. In Pro-
ceedings of the International Joint Conference on
Natural Language Processing.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic “unification-based” grammars. In Pro-
ceedings of the 37th Annual Meeting of the ACL.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Conference on Natural Lan-
guage Learning (CoNLL-2002), Taipei.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152–159, New York City, USA, June. Association
for Computational Linguistics.
Nelleke Oostdijk. 2000. The Spoken Dutch Corpus:
Overview and first evaluation. In Proceedings of
Second International Conference on Language Re-
sources and Evaluation (LREC), pages 887–894.
Barbara Plank and Gertjan van Noord. 2008. Ex-
ploring an auxiliary distribution based approach to
domain adaptation of a syntactic disambiguation
model. In Proceedings of the Workshop on Cross-
Framework and Cross-Domain Parser Evaluation
(PE), Manchester, August.
A. Ratnaparkhi. 1997. A simple introduction to max-
imum entropy models for natural language process-
ing. Technical report, Institute for Research in Cog-
nitive Science, University of Pennsylvania.
Brian Roark and Michiel Bacchiani. 2003. Supervised
and unsupervised pcfg adaptation to novel domains.
In In Proceedings of the Human Language Technol-
ogy Conference and Meeting of the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL).
Nobuyuki Shimizu and Hiroshi Nakagawa. 2007.
Structural correspondence learning for dependency
parsing. In Proceedings of the CoNLL Shared Task
Session ofEMNLP-CoNLL 2007.
Gertjan van Noord and Robert Malouf. 2005.
Wide coverage parsing with stochastic at-
tribute value grammars. Draft available from
http://www.let.rug.nl/˜vannoord. A preliminary ver-
sion of this paper was published in the Proceedings
of the IJCNLP workshop Beyond Shallow Analyses,
Hainan China, 2004.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina,
Actes De La 13e Conference sur Le Traitement
Automatique des Langues naturelles, pages 20–42,
Leuven.
Gertjan van Noord. 2007. Using self-trained bilexi-
cal preferences to improve disambiguation accuracy.
In Proceedings of the Tenth International Confer-
ence on Parsing Technologies. IWPT 2007, Prague.,
pages 1–10, Prague.
</reference>
<page confidence="0.999391">
45
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.636607">
<title confidence="0.999928">Structural Correspondence Learning for Parse Disambiguation</title>
<author confidence="0.99824">Barbara Plank</author>
<affiliation confidence="0.821353">Alfa-informatica University of Groningen, The Netherlands</affiliation>
<email confidence="0.9952">b.plank@rug.nl</email>
<abstract confidence="0.997348444444444">The paper presents an application of Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for domain adaptation of a stochastic attribute-value grammar (SAVG). So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007). An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions. We report on our exploration of applying SCL to adapt a syntactic disambiguation model and show promising initial results on Wikipedia domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Stochastic attribute-value grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--597</pages>
<contexts>
<context position="7382" citStr="Abney, 1997" startWordPosition="1164" endWordPosition="1165">Noord and Malouf, 2005; van Noord, 2006) is a robust computational analyzer for Dutch that implements the conceptual two-stage parsing approach. The system consists of approximately 800 grammar rules in the tradition of HPSG, and a large hand-crafted lexicon, that together with a left-corner parser constitutes the generation component. For parse selection, Alpino employs a discriminative approach based on Maximum Entropy (MaxEnt). The output of the parser is dependency structure based on the guidelines of CGN (Oostdijk, 2000). The Maximum Entropy model (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997) is a conditional model that assigns a probability to every possible parse w for a given sentence s. The model consists of a set of m feature functions fj(w) that describe properties of parses, together with their associated weights Bj. The denominator is a normalization term where Y (s) is the set of parses with yield s: Ey∈Y (s) exp(Emj=1 Bjfj(y))) (1) The parameters (weights) Bj can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus (Johnson et al., 1999; van Noord and Malouf, 2005): Em j=1 Bˆ = arg mθ log L(B) − 2σ2 B2j (2) where L(B) is the </context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Steven P. Abney. 1997. Stochastic attribute-value grammars. Computational Linguistics, 23:597–618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--1817</pages>
<contexts>
<context position="10174" citStr="Ando and Zhang (2005)" startWordPosition="1619" endWordPosition="1622">e” are common ways to express negative sentiment in A, while “not working” or “defective” are specific to B. If there are features across the domains, e.g. “don’t buy”, with which the domain specific features are highly correlated with, then we might tentatively align those features. Therefore, the key idea of SCL is to identify automatically correspondences among features from different domains by modeling their correlations with pivot features. Pivots are features occurring frequently and behaving similarly in both domains (Blitzer et al., 2006). They are inspired by auxiliary problems from Ando and Zhang (2005). Non-pivot features that correspond with many of the same pivot-features are assumed to correspond. Intuitively, if we are able to find good correspondences among features, then the augmented labeled source domain data should transfer better to a target domain (where no labeled data is available) (Blitzer et al., 2006). The outline of the algorithm is given in Figure 1. The first step is to identify m pivot features occurring frequently in the unlabeled data of both Input: -labeled source data {(xs, ys)Ns s=1} - unlabeled data from both source and target domain xul = xs, xt 1. Select m pivot </context>
<context position="11968" citStr="Ando and Zhang, 2005" startWordPosition="1933" endWordPosition="1936">in this instance?”. The pivots are masked in the unlabeled data and the aim is to predict them using non-pivot features. In this way, we obtain a weight vector w for each pivot predictor. Positive entries in the weight vector indicate that a non-pivot is highly correlated with the respective pivot feature. Step 3 is to arrange the m weight vectors in a matrix W, where a column corresponds to a pivot predictor weight vector. Applying the projection WTx (where x is a training instance) would give us m new features, however, for “both computational and statistical reasons” (Blitzer et al., 2006; Ando and Zhang, 2005) a low-dimensional approximation of the original feature space is computed by applying Singular Value Decomposition (SVD) on W (step 4). Let 0 = UThxn be the top h left singular vectors of W (with h a dimension parameter and n the number of non-pivot features). The resulting 0 is a projection onto a lower dimensional space ][8h, parameterized by h. The final step of SCL is to train a linear predictor on the augmented labeled source data (x, 0x). In more detail, the original feature space x is augmented with h new features obtained by applying the projection 0x. In this way, we can learn weight</context>
<context position="15815" citStr="Ando and Zhang (2005)" startWordPosition="2577" endWordPosition="2580"> features As pointed out by Blitzer et al. (2006), each instance will actually contain features which are totally predictive of the pivot features (i.e. the pivot itself). In our case, we additionally have to pay attention to ’more specific’ features, e.g. r2 is a feature that extends r1, in the sense that it incorporates more information than its parent (i.e. which grammar rules applied in the construction of daughter nodes). It is crucial to remove these predictive features when creating the training data for the pivot predictors. Matrix and SVD Following Blitzer et al. (2006) (which follow Ando and Zhang (2005)), we only use positive entries in the pivot predictors weight vectors to compute the SVD. Thus, when constructing the matrix W, we disregard all negative entries in W and compute the SVD (W = UDVT) on the resulting non-negative sparse matrix. This sparse representation saves both time and space. 4.2 Further practical issues of SCL In practice, there are more free parameters and model choices (Ando and Zhang, 2005; Ando, 2006; Blitzer et al., 2006; Blitzer, 2008) besides the ones discussed above. Feature normalization and feature scaling. Blitzer et al. (2006) found it necessary to normalize a</context>
<context position="17252" citStr="Ando and Zhang (2005)" startWordPosition="2811" endWordPosition="2814">ean and normalized them to unit variance (i.e. x − mean/sd). They then rescaled the features by a factor α found on heldout data: αOx. Restricted Regularization. When training the supervised model on the augmented feature space (x, Ox), Blitzer et al. (2006) only regularize the weight vector of the original features, but not the one for the new low-dimensional features. This was done to encourage the model to use the new low-dimensional representation rather than the higher-dimensional original representation (Blitzer, 2008). Dimensionality reduction by feature type. An extension suggested in Ando and Zhang (2005) is 40 to compute separate SVDs for blocks of the matrix W corresponding to feature types (as illustrated in Figure 2), and then to apply separate projection for every type. Due to the positive results in Ando (2006), Blitzer et al. (2006) include this in their standard setting of SCL and report results using block SVDs only. Figure 2: Illustration of dimensionality reduction by feature type (Ando and Zhang, 2005). The grey area corresponds to a feature type (submatrix of W) on which the SVD is computed (block SVD); the white area is regarded as fixed to zero matrices. 5 Experiments and Result</context>
<context position="26501" citStr="Ando and Zhang, 2005" startWordPosition="4304" endWordPosition="4307"> our instantiation of SCL (with varying h parameter and no feature normalization). The results show a (sometimes) small but consistent increase in absolute performance on all testsets over the baseline system (up to +0.26 absolute CA score), as well as an increase in 0 measure (absolute error reduction). This corresponds to a relative error reduction of up to 7.29%. Thus, our first instantiation of SCL for parse disambiguation indeed shows promising results. We can confirm that changing the dimensionality parameter h has rather little effect (Table 4), which is in line with previous findings (Ando and Zhang, 2005; Blitzer et al., 2006). Thus we might fix the parameter and prefer smaller dimensionalities, which saves space and time. Note that these results were obtained without any of the additional normalization, rescaling, feature-specific regularization, or block SVD issues, etc. (discussed in section 4.2). We used the same Gaussian regularization term (U2=1000) for all features (original and new features), and did not perform any feature normalization or rescaling. This means our current instantiation of SCL is an actually simplified version of the original SCL algorithm, applied to parse disambigu</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6:1817–1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
</authors>
<title>Applying alternating structure optimization to word sense disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="16244" citStr="Ando, 2006" startWordPosition="2653" endWordPosition="2654">cial to remove these predictive features when creating the training data for the pivot predictors. Matrix and SVD Following Blitzer et al. (2006) (which follow Ando and Zhang (2005)), we only use positive entries in the pivot predictors weight vectors to compute the SVD. Thus, when constructing the matrix W, we disregard all negative entries in W and compute the SVD (W = UDVT) on the resulting non-negative sparse matrix. This sparse representation saves both time and space. 4.2 Further practical issues of SCL In practice, there are more free parameters and model choices (Ando and Zhang, 2005; Ando, 2006; Blitzer et al., 2006; Blitzer, 2008) besides the ones discussed above. Feature normalization and feature scaling. Blitzer et al. (2006) found it necessary to normalize and scale the new features obtained by the projection O, in order to “allow them to receive more weight from a regularized discriminative learner”. For each of the features, they centered them by subtracting out the mean and normalized them to unit variance (i.e. x − mean/sd). They then rescaled the features by a factor α found on heldout data: αOx. Restricted Regularization. When training the supervised model on the augmented</context>
<context position="17468" citStr="Ando (2006)" startWordPosition="2851" endWordPosition="2852">e (x, Ox), Blitzer et al. (2006) only regularize the weight vector of the original features, but not the one for the new low-dimensional features. This was done to encourage the model to use the new low-dimensional representation rather than the higher-dimensional original representation (Blitzer, 2008). Dimensionality reduction by feature type. An extension suggested in Ando and Zhang (2005) is 40 to compute separate SVDs for blocks of the matrix W corresponding to feature types (as illustrated in Figure 2), and then to apply separate projection for every type. Due to the positive results in Ando (2006), Blitzer et al. (2006) include this in their standard setting of SCL and report results using block SVDs only. Figure 2: Illustration of dimensionality reduction by feature type (Ando and Zhang, 2005). The grey area corresponds to a feature type (submatrix of W) on which the SVD is computed (block SVD); the white area is regarded as fixed to zero matrices. 5 Experiments and Results 5.1 Experimental design The base (source domain) disambiguation model is trained on the Alpino Treebank (van Noord, 2006) (newspaper text), which consists of approximately 7,000 sentences and 145,000 tokens. For pa</context>
</contexts>
<marker>Ando, 2006</marker>
<rawString>Rie Kubota Ando. 2006. Applying alternating structure optimization to word sense disambiguation. In Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="7349" citStr="Berger et al., 1996" startWordPosition="1158" endWordPosition="1161">3 Background: Alpino parser Alpino (van Noord and Malouf, 2005; van Noord, 2006) is a robust computational analyzer for Dutch that implements the conceptual two-stage parsing approach. The system consists of approximately 800 grammar rules in the tradition of HPSG, and a large hand-crafted lexicon, that together with a left-corner parser constitutes the generation component. For parse selection, Alpino employs a discriminative approach based on Maximum Entropy (MaxEnt). The output of the parser is dependency structure based on the guidelines of CGN (Oostdijk, 2000). The Maximum Entropy model (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997) is a conditional model that assigns a probability to every possible parse w for a given sentence s. The model consists of a set of m feature functions fj(w) that describe properties of parses, together with their associated weights Bj. The denominator is a normalization term where Y (s) is the set of parses with yield s: Ey∈Y (s) exp(Emj=1 Bjfj(y))) (1) The parameters (weights) Bj can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus (Johnson et al., 1999; van Noord and Malouf, 2005): Em j=1 Bˆ = arg mθ log L(B)</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="1942" citStr="Blitzer et al., 2006" startWordPosition="299" endWordPosition="302"> domain (Gildea, 2001; Daum´e III, 2007), we face the problem of domain adaptation. The need for domain adaptation arises in many NLP tasks: Part-of-Speech tagging, Sentiment Analysis, Semantic Role Labeling or Statistical Parsing, to name but a few. For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001). The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daum´e III and Marcu, 2006; Daum´e III, 2007; Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007). We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daum´e III, 2007): supervised and semi-supervised. In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daum´e III, 2007), besides the labeled source data, we have access to a comparably small, but labeled amount of target data. In contrast, semi-supervised domain adaptation (Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007) is the scenario in which, in addition to the labeled source data, we o</context>
<context position="3476" citStr="Blitzer et al., 2006" startWordPosition="550" endWordPosition="553"> data) achieve a relatively high performance level and are “surprisingly difficult to beat” (Daum´e III, 2007). Thus, one conclusion from that line of work is that as soon as there is a reasonable (often even small) amount of labeled target data, it is often more fruitful to either just use that, or to apply simple adaptation techniques (Daum´e III, 2007; Plank and van Noord, 2008). 2 Motivation and Prior Work While several authors have looked at the supervised adaptation case, there are less (and especially less successful) studies on semi-supervised domain adaptation (McClosky et al., 2006; Blitzer et al., 2006; Dredze et al., 2007). Of these, McClosky et al. (2006) deal specifically with selftraining for data-driven statistical parsing. They show that together with a re-ranker, improvements Proceedings of the EACL 2009 Student Research Workshop, pages 37–45, Athens, Greece, 2 April 2009. c�2009 Association for Computational Linguistics 37 are obtained. Similarly, Structural Correspondence Learning (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification. In contrast, Dredze et al. (2007) report on “f</context>
<context position="5721" citStr="Blitzer et al., 2006" startWordPosition="904" endWordPosition="907"> van Noord and Malouf, 2005; McClosky et al., 2006). Yet, the adaptation of parse selection models to novel domains is a far less studied area. This may be motivated by the fact that potential gains for this task are inherently bounded by the underlying grammar. The few studies on adapting disambiguation models (Hara et al., 2005; Plank and van Noord, 2008) have focused exclusively on the supervised scenario. Therefore, the direction we explore in this study is semi-supervised domain adaptation for parse disambiguation. We examine the effectiveness of Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for this task, a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis. The system used in this study is Alpino, a wide-coverage Stochastic Attribute Value Grammar (SAVG) for Dutch (van Noord and Malouf, 2005; van Noord, 2006). For our empirical eval1As shown in Dredze et al. (2007), the biggest problem for the shared task was that the provided datasets were annotated with different annotation guidelines, thus the general conclusion was that the task was ill-defined (Nobuyuki Shimizu, personal communication). uation we explore Wikipedia as primary</context>
<context position="9012" citStr="Blitzer et al., 2006" startWordPosition="1434" endWordPosition="1437">l is trained, it can be applied to choose the parse with the highest sum of feature weights. The MaxEnt model consists of a large set of features, corresponding to instantiations of feature templates that model various properties of parses. For instance, Part-of-Speech tags, dependency relations, grammar rule applications, etc. The current standard model uses about 11,000 features. We will refer to this set of features as original features. They are used to train the baseline model on the given labeled source data. 4 Structural Correspondence Learning SCL (Structural Correspondence Learning) (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains. Before describing the algorithm in detail, let us illustrate the intuition behind SCL with an example, borrowed from Blitzer et al. (2007). Suppose we have a Sentiment Analysis system trained on book reviews (domain A), and we would like to adapt it to kitchen appliances (domain B). Features such as “boring” and “repetitive” are common ways to express negative sentiment in A, whil</context>
<context position="10495" citStr="Blitzer et al., 2006" startWordPosition="1671" endWordPosition="1674"> identify automatically correspondences among features from different domains by modeling their correlations with pivot features. Pivots are features occurring frequently and behaving similarly in both domains (Blitzer et al., 2006). They are inspired by auxiliary problems from Ando and Zhang (2005). Non-pivot features that correspond with many of the same pivot-features are assumed to correspond. Intuitively, if we are able to find good correspondences among features, then the augmented labeled source domain data should transfer better to a target domain (where no labeled data is available) (Blitzer et al., 2006). The outline of the algorithm is given in Figure 1. The first step is to identify m pivot features occurring frequently in the unlabeled data of both Input: -labeled source data {(xs, ys)Ns s=1} - unlabeled data from both source and target domain xul = xs, xt 1. Select m pivot features 2. Train m binary classifiers (pivot predictors) 3. Create matrix Wnxm of binary predictor weight vectors W = [w1, .., wm], where n is the number of nonpivot features in xul 4. Apply SVD to W: Wnxm = UnxnDnxmVTmxm where 0 = UT[1:h,:] are the h top left singular vectors of W. 5. Apply projection xs0 and train a </context>
<context position="11945" citStr="Blitzer et al., 2006" startWordPosition="1929" endWordPosition="1932">pivot feature l occur in this instance?”. The pivots are masked in the unlabeled data and the aim is to predict them using non-pivot features. In this way, we obtain a weight vector w for each pivot predictor. Positive entries in the weight vector indicate that a non-pivot is highly correlated with the respective pivot feature. Step 3 is to arrange the m weight vectors in a matrix W, where a column corresponds to a pivot predictor weight vector. Applying the projection WTx (where x is a training instance) would give us m new features, however, for “both computational and statistical reasons” (Blitzer et al., 2006; Ando and Zhang, 2005) a low-dimensional approximation of the original feature space is computed by applying Singular Value Decomposition (SVD) on W (step 4). Let 0 = UThxn be the top h left singular vectors of W (with h a dimension parameter and n the number of non-pivot features). The resulting 0 is a projection onto a lower dimensional space ][8h, parameterized by h. The final step of SCL is to train a linear predictor on the augmented labeled source data (x, 0x). In more detail, the original feature space x is augmented with h new features obtained by applying the projection 0x. In this w</context>
<context position="15243" citStr="Blitzer et al. (2006)" startWordPosition="2481" endWordPosition="2484"> features As pivot features should be common across domains, here we restrict our pivots to be of the type r1,p1,s1 (the most frequently occurring feature types). In more detail, r1 indicates which grammar rule applied, p1 whether coordination conjuncts are parallel, and s1 whether topicalization or long-distance dependencies occurred. We count how often each feature appears in the parsed source and target domain data, and select those r1,p1,s1 features as pivot features, whose count is &gt; t, where t is a specified threshold. In all our experiments, we set Predictive features As pointed out by Blitzer et al. (2006), each instance will actually contain features which are totally predictive of the pivot features (i.e. the pivot itself). In our case, we additionally have to pay attention to ’more specific’ features, e.g. r2 is a feature that extends r1, in the sense that it incorporates more information than its parent (i.e. which grammar rules applied in the construction of daughter nodes). It is crucial to remove these predictive features when creating the training data for the pivot predictors. Matrix and SVD Following Blitzer et al. (2006) (which follow Ando and Zhang (2005)), we only use positive entr</context>
<context position="16889" citStr="Blitzer et al. (2006)" startWordPosition="2758" endWordPosition="2761">Blitzer, 2008) besides the ones discussed above. Feature normalization and feature scaling. Blitzer et al. (2006) found it necessary to normalize and scale the new features obtained by the projection O, in order to “allow them to receive more weight from a regularized discriminative learner”. For each of the features, they centered them by subtracting out the mean and normalized them to unit variance (i.e. x − mean/sd). They then rescaled the features by a factor α found on heldout data: αOx. Restricted Regularization. When training the supervised model on the augmented feature space (x, Ox), Blitzer et al. (2006) only regularize the weight vector of the original features, but not the one for the new low-dimensional features. This was done to encourage the model to use the new low-dimensional representation rather than the higher-dimensional original representation (Blitzer, 2008). Dimensionality reduction by feature type. An extension suggested in Ando and Zhang (2005) is 40 to compute separate SVDs for blocks of the matrix W corresponding to feature types (as illustrated in Figure 2), and then to apply separate projection for every type. Due to the positive results in Ando (2006), Blitzer et al. (200</context>
<context position="22300" citStr="Blitzer et al. (2006)" startWordPosition="3606" endWordPosition="3609">=&amp;quot;Categorie:Vaticaanstad&amp;quot;/&gt; &lt;cat t=&amp;quot;super&amp;quot; n=&amp;quot;Categorie:Bisschop&amp;quot;/&gt; &lt;cat t=&amp;quot;super&amp;quot; n=&amp;quot;Categorie:Kerkgeschiedenis&amp;quot;/&gt; &lt;cat t=&amp;quot;sub&amp;quot; n=&amp;quot;Categorie:Tegenpaus&amp;quot;/&gt; &lt;cat t=&amp;quot;super&amp;quot; n=&amp;quot;Categorie:Pools persoon&amp;quot;/&gt; &lt;/wikipage&gt; Figure 3: Example of extracted Wikipedia categories for a given article (direct, sup- and subcats). To create the set of related pages for a given article p, we proceed as follows: 1. Find sub- and supercategories of p 2. Extract all pages that are related to p (through sharing a direct, sub or super category) 3. Optionally, filter out certain pages In our empirical setup, we followed Blitzer et al. (2006) and tried to balance the size of source and target data. Thus, depending on the size of the resulting target domain dataset, and the “broadness” of the categories involved in creating it, we might wish to filter out certain pages. We implemented a filter mechanism that excludes pages of a certain category (e.g. a supercategory that is hypothesized to be “too broad”). Alternatively, we might have used a filter mechanism that excludes certain pages directly. In our experiments, we always included pages that are directly related to a page of interest, and those that shared a subcategory. Of cour</context>
<context position="26524" citStr="Blitzer et al., 2006" startWordPosition="4308" endWordPosition="4311">SCL (with varying h parameter and no feature normalization). The results show a (sometimes) small but consistent increase in absolute performance on all testsets over the baseline system (up to +0.26 absolute CA score), as well as an increase in 0 measure (absolute error reduction). This corresponds to a relative error reduction of up to 7.29%. Thus, our first instantiation of SCL for parse disambiguation indeed shows promising results. We can confirm that changing the dimensionality parameter h has rather little effect (Table 4), which is in line with previous findings (Ando and Zhang, 2005; Blitzer et al., 2006). Thus we might fix the parameter and prefer smaller dimensionalities, which saves space and time. Note that these results were obtained without any of the additional normalization, rescaling, feature-specific regularization, or block SVD issues, etc. (discussed in section 4.2). We used the same Gaussian regularization term (U2=1000) for all features (original and new features), and did not perform any feature normalization or rescaling. This means our current instantiation of SCL is an actually simplified version of the original SCL algorithm, applied to parse disambiguation. Of course, our r</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Conference on Empirical Methods in Natural Language Processing, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3915" citStr="Blitzer et al., 2007" startWordPosition="614" endWordPosition="617">e looked at the supervised adaptation case, there are less (and especially less successful) studies on semi-supervised domain adaptation (McClosky et al., 2006; Blitzer et al., 2006; Dredze et al., 2007). Of these, McClosky et al. (2006) deal specifically with selftraining for data-driven statistical parsing. They show that together with a re-ranker, improvements Proceedings of the EACL 2009 Student Research Workshop, pages 37–45, Athens, Greece, 2 April 2009. c�2009 Association for Computational Linguistics 37 are obtained. Similarly, Structural Correspondence Learning (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification. In contrast, Dredze et al. (2007) report on “frustrating” results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. ”no team was able to improve target domain performance substantially over a state of the art baseline”. In the same shared task, an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa, 2007). The system just ended up at rank 7 out of 8 teams. However, based on annotation differences </context>
<context position="9034" citStr="Blitzer et al., 2007" startWordPosition="1438" endWordPosition="1441">e applied to choose the parse with the highest sum of feature weights. The MaxEnt model consists of a large set of features, corresponding to instantiations of feature templates that model various properties of parses. For instance, Part-of-Speech tags, dependency relations, grammar rule applications, etc. The current standard model uses about 11,000 features. We will refer to this set of features as original features. They are used to train the baseline model on the given labeled source data. 4 Structural Correspondence Learning SCL (Structural Correspondence Learning) (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains. Before describing the algorithm in detail, let us illustrate the intuition behind SCL with an example, borrowed from Blitzer et al. (2007). Suppose we have a Sentiment Analysis system trained on book reviews (domain A), and we would like to adapt it to kitchen appliances (domain B). Features such as “boring” and “repetitive” are common ways to express negative sentiment in A, while “not working” or “de</context>
<context position="13139" citStr="Blitzer et al., 2007" startWordPosition="2144" endWordPosition="2147">e projection 0x. In this way, we can learn weights for domain-specific features, which otherwise would not have been observed. If 0 contains meaningful correspondences, then the pre39 dictor trained on the augmented data should transfer well to the new domain. t = 5000. In this way we obtained on average 360 pivot features, on the datasets described in Section 5. 4.1 SCL for Parse Disambiguation A property of the pivot predictors is that they can be trained from unlabeled data, as they represent properties of the input. So far, pivot features on the word level were used (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008), e.g. “Does the bigram not buy occur in this document?” (Blitzer, 2008). Pivot features are the key ingredient for SCL, and they should align well with the NLP task. For PoS tagging and Sentiment Analysis, features on the word level are intuitively well-related to the problem at hand. For the task of parse disambiguation based on a conditional model this is not the case. Hence, we actually introduce an additional and new layer of abstraction, which, we hypothesize, aligns well with the task of parse disambiguation: we first parse the unlabeled data. In this way we obtain full </context>
<context position="32309" citStr="Blitzer et al., 2007" startWordPosition="5139" endWordPosition="5142">contribute the most on this Prince domain. As a fact, one third of the sentences in the Prince testset contain constructions with appositions (e.g. about film-, album- and song titles). 6 Conclusions and Future Work The paper presents an application of Structural Correspondence Learning (SCL) to parse disamFigure 5: Results of dimensionality reduction by feature type, h = 25; block SVD included all 9 feature types; the right part shows the accuracy when one feature type was removed. biguation. While SCL has been successfully applied to PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), its effectiveness for parsing was rather unexplored. The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results, even without the many additional extensions on the feature level as done in Blitzer et al. (2006). We exploited Wikipedia as primary resource, both for collecting unlabeled target domain data, as well as test suite for empirical evaluation. On the three examined datasets, SCL slightly but constantly outperformed the baseline. Applying SCL involves many design choices and practical issues, which we tried to depict here in detail</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Association for Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
</authors>
<title>Domain Adaptation of Natural Language Processing Systems.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="3931" citStr="Blitzer, 2008" startWordPosition="618" endWordPosition="619">ised adaptation case, there are less (and especially less successful) studies on semi-supervised domain adaptation (McClosky et al., 2006; Blitzer et al., 2006; Dredze et al., 2007). Of these, McClosky et al. (2006) deal specifically with selftraining for data-driven statistical parsing. They show that together with a re-ranker, improvements Proceedings of the EACL 2009 Student Research Workshop, pages 37–45, Athens, Greece, 2 April 2009. c�2009 Association for Computational Linguistics 37 are obtained. Similarly, Structural Correspondence Learning (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification. In contrast, Dredze et al. (2007) report on “frustrating” results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. ”no team was able to improve target domain performance substantially over a state of the art baseline”. In the same shared task, an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa, 2007). The system just ended up at rank 7 out of 8 teams. However, based on annotation differences in the datasets </context>
<context position="9050" citStr="Blitzer, 2008" startWordPosition="1442" endWordPosition="1443">e parse with the highest sum of feature weights. The MaxEnt model consists of a large set of features, corresponding to instantiations of feature templates that model various properties of parses. For instance, Part-of-Speech tags, dependency relations, grammar rule applications, etc. The current standard model uses about 11,000 features. We will refer to this set of features as original features. They are used to train the baseline model on the given labeled source data. 4 Structural Correspondence Learning SCL (Structural Correspondence Learning) (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains. Before describing the algorithm in detail, let us illustrate the intuition behind SCL with an example, borrowed from Blitzer et al. (2007). Suppose we have a Sentiment Analysis system trained on book reviews (domain A), and we would like to adapt it to kitchen appliances (domain B). Features such as “boring” and “repetitive” are common ways to express negative sentiment in A, while “not working” or “defective” are spe</context>
<context position="13155" citStr="Blitzer, 2008" startWordPosition="2148" endWordPosition="2149">is way, we can learn weights for domain-specific features, which otherwise would not have been observed. If 0 contains meaningful correspondences, then the pre39 dictor trained on the augmented data should transfer well to the new domain. t = 5000. In this way we obtained on average 360 pivot features, on the datasets described in Section 5. 4.1 SCL for Parse Disambiguation A property of the pivot predictors is that they can be trained from unlabeled data, as they represent properties of the input. So far, pivot features on the word level were used (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008), e.g. “Does the bigram not buy occur in this document?” (Blitzer, 2008). Pivot features are the key ingredient for SCL, and they should align well with the NLP task. For PoS tagging and Sentiment Analysis, features on the word level are intuitively well-related to the problem at hand. For the task of parse disambiguation based on a conditional model this is not the case. Hence, we actually introduce an additional and new layer of abstraction, which, we hypothesize, aligns well with the task of parse disambiguation: we first parse the unlabeled data. In this way we obtain full parses for given</context>
<context position="16282" citStr="Blitzer, 2008" startWordPosition="2659" endWordPosition="2660">atures when creating the training data for the pivot predictors. Matrix and SVD Following Blitzer et al. (2006) (which follow Ando and Zhang (2005)), we only use positive entries in the pivot predictors weight vectors to compute the SVD. Thus, when constructing the matrix W, we disregard all negative entries in W and compute the SVD (W = UDVT) on the resulting non-negative sparse matrix. This sparse representation saves both time and space. 4.2 Further practical issues of SCL In practice, there are more free parameters and model choices (Ando and Zhang, 2005; Ando, 2006; Blitzer et al., 2006; Blitzer, 2008) besides the ones discussed above. Feature normalization and feature scaling. Blitzer et al. (2006) found it necessary to normalize and scale the new features obtained by the projection O, in order to “allow them to receive more weight from a regularized discriminative learner”. For each of the features, they centered them by subtracting out the mean and normalized them to unit variance (i.e. x − mean/sd). They then rescaled the features by a factor α found on heldout data: αOx. Restricted Regularization. When training the supervised model on the augmented feature space (x, Ox), Blitzer et al.</context>
<context position="28219" citStr="Blitzer, 2008" startWordPosition="4576" endWordPosition="4577">cale) the projection features, we did not observe any improvement by normalizing them (actually, it slightly degraded performance in our case). Thus, we found this step unnecessary, and currently did not look at this issue any further. A look at 0 To gain some insight of which kind of correspondences SCL learned in our case, we started to examine the rows of 0. Recall that applying a row of the projection matrix BZ to a training instance x gives us a new real-valued feature. If features from different domains have similar entries (scores) in the projection row, they are assumed to correspond (Blitzer, 2008). Figure 4 shows example of correspondences that SCL found in the Prince dataset. The first column represents the score of a feature. The labels wiki and alp indicate the domain of the features, respectively. For readability, we here grouped the features obtaining similar scores. 0.000217698|dep34(tournee,noun,hd/su,verb)|1|wiki 0.000223301|dep34(regisseur,noun,hd/su,verb)|15|wiki 0.000224517|dep34(voorsprong,noun,hd/su,verb)|2|alp 0.000224684|dep34(wetenschap,noun,hd/su,verb)|2|alp 0.000226617|dep34(pop_rock,noun,hd/su,verb)|1|wiki 0.000228918|dep34(plan,noun,hd/su,verb)|9|alp Figure 4: Examp</context>
</contexts>
<marker>Blitzer, 2008</marker>
<rawString>John Blitzer. 2008. Domain Adaptation of Natural Language Processing Systems. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Tree-bank grammars. In</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence,</booktitle>
<pages>1031--1036</pages>
<contexts>
<context position="4987" citStr="Charniak, 1996" startWordPosition="790" endWordPosition="791">n dependency parsing (Shimizu and Nakagawa, 2007). The system just ended up at rank 7 out of 8 teams. However, based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive.1 Thus, the effectiveness of SCL is rather unexplored for parsing. So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007), i.e. systems employing (constituent or dependency based) treebank grammars (Charniak, 1996). Parse selection constitutes an important part of many parsing systems (Johnson et al., 1999; Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006). Yet, the adaptation of parse selection models to novel domains is a far less studied area. This may be motivated by the fact that potential gains for this task are inherently bounded by the underlying grammar. The few studies on adapting disambiguation models (Hara et al., 2005; Plank and van Noord, 2008) have focused exclusively on the supervised scenario. Therefore, the direction we explore in this study is semi-supervised domai</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>Eugene Charniak. 1996. Tree-bank grammars. In In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 1031–1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>26--101</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelligence Research, 26:101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Conference of the Association for Computational Linguistics (ACL),</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Conference of the Association for Computational Linguistics (ACL), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>John Blitzer</author>
</authors>
<title>Pratha Pratim Talukdar, Kuzman Ganchev, Joao Graca, and Fernando Pereira.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session - Conference on Natural Language Learning,</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Dredze, Blitzer, 2007</marker>
<rawString>Mark Dredze, John Blitzer, Pratha Pratim Talukdar, Kuzman Ganchev, Joao Graca, and Fernando Pereira. 2007. Frustratingly hard domain adaptation for parsing. In Proceedings of the CoNLL Shared Task Session - Conference on Natural Language Learning, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1343" citStr="Gildea, 2001" startWordPosition="202" endWordPosition="203">d show promising initial results on Wikipedia domains. 1 Introduction Many current, effective natural language processing systems are based on supervised Machine Learning techniques. The parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of portability: a system will be successful only as long as the training material resembles the input that the model gets. Therefore, whenever we have access to a large amount of labeled data from some “source” (out-of-domain), but we would like a model that performs well on some new “target” domain (Gildea, 2001; Daum´e III, 2007), we face the problem of domain adaptation. The need for domain adaptation arises in many NLP tasks: Part-of-Speech tagging, Sentiment Analysis, Semantic Role Labeling or Statistical Parsing, to name but a few. For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001). The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daum´e III and Marcu, 2006; Daum´e III, 2007; Blitzer et al., 2006;</context>
<context position="4815" citStr="Gildea, 2001" startWordPosition="766" endWordPosition="767"> target domain performance substantially over a state of the art baseline”. In the same shared task, an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa, 2007). The system just ended up at rank 7 out of 8 teams. However, based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive.1 Thus, the effectiveness of SCL is rather unexplored for parsing. So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007), i.e. systems employing (constituent or dependency based) treebank grammars (Charniak, 1996). Parse selection constitutes an important part of many parsing systems (Johnson et al., 1999; Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006). Yet, the adaptation of parse selection models to novel domains is a far less studied area. This may be motivated by the fact that potential gains for this task are inherently bounded by the underlying grammar. The few studies on adapting disambiguation models (H</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadayoshi Hara</author>
<author>Miyao Yusuke</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Adapting a probabilistic disambiguation model of an hpsg parser to a new domain.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="1874" citStr="Hara et al., 2005" startWordPosition="287" endWordPosition="290">but we would like a model that performs well on some new “target” domain (Gildea, 2001; Daum´e III, 2007), we face the problem of domain adaptation. The need for domain adaptation arises in many NLP tasks: Part-of-Speech tagging, Sentiment Analysis, Semantic Role Labeling or Statistical Parsing, to name but a few. For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001). The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daum´e III and Marcu, 2006; Daum´e III, 2007; Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007). We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daum´e III, 2007): supervised and semi-supervised. In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daum´e III, 2007), besides the labeled source data, we have access to a comparably small, but labeled amount of target data. In contrast, semi-supervised domain adaptation (Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007) is</context>
<context position="5099" citStr="Hara et al., 2005" startWordPosition="806" endWordPosition="809"> based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive.1 Thus, the effectiveness of SCL is rather unexplored for parsing. So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007), i.e. systems employing (constituent or dependency based) treebank grammars (Charniak, 1996). Parse selection constitutes an important part of many parsing systems (Johnson et al., 1999; Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006). Yet, the adaptation of parse selection models to novel domains is a far less studied area. This may be motivated by the fact that potential gains for this task are inherently bounded by the underlying grammar. The few studies on adapting disambiguation models (Hara et al., 2005; Plank and van Noord, 2008) have focused exclusively on the supervised scenario. Therefore, the direction we explore in this study is semi-supervised domain adaptation for parse disambiguation. We examine the effectiveness of Structural Correspondence Learning (SCL) </context>
</contexts>
<marker>Hara, Yusuke, Tsujii, 2005</marker>
<rawString>Tadayoshi Hara, Miyao Yusuke, and Jun’ichi Tsujii. 2005. Adapting a probabilistic disambiguation model of an hpsg parser to a new domain. In Proceedings of the International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unification-based” grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="5080" citStr="Johnson et al., 1999" startWordPosition="802" endWordPosition="805">t of 8 teams. However, based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive.1 Thus, the effectiveness of SCL is rather unexplored for parsing. So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007), i.e. systems employing (constituent or dependency based) treebank grammars (Charniak, 1996). Parse selection constitutes an important part of many parsing systems (Johnson et al., 1999; Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006). Yet, the adaptation of parse selection models to novel domains is a far less studied area. This may be motivated by the fact that potential gains for this task are inherently bounded by the underlying grammar. The few studies on adapting disambiguation models (Hara et al., 2005; Plank and van Noord, 2008) have focused exclusively on the supervised scenario. Therefore, the direction we explore in this study is semi-supervised domain adaptation for parse disambiguation. We examine the effectiveness of Structural Corresponde</context>
<context position="7891" citStr="Johnson et al., 1999" startWordPosition="1249" endWordPosition="1252">elines of CGN (Oostdijk, 2000). The Maximum Entropy model (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997) is a conditional model that assigns a probability to every possible parse w for a given sentence s. The model consists of a set of m feature functions fj(w) that describe properties of parses, together with their associated weights Bj. The denominator is a normalization term where Y (s) is the set of parses with yield s: Ey∈Y (s) exp(Emj=1 Bjfj(y))) (1) The parameters (weights) Bj can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus (Johnson et al., 1999; van Noord and Malouf, 2005): Em j=1 Bˆ = arg mθ log L(B) − 2σ2 B2j (2) where L(B) is the likelihood of the training data. The second term is a regularization term (Gaussian prior on the feature weights with mean zero and variance σ). The estimated weights determine the contribution of each feature. Features appearing in correct parses are given increasing (positive) weight, while features in incorrect parses are pθ(w|s; B) = exp(Em j=1 Bjfj(w)) 38 given decreasing (negative) weight. Once a model is trained, it can be applied to choose the parse with the highest sum of feature weights. The Ma</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unification-based” grammars. In Proceedings of the 37th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Sixth Conference on Natural Language Learning (CoNLL-2002),</booktitle>
<location>Taipei.</location>
<contexts>
<context position="18333" citStr="Malouf, 2002" startWordPosition="2984" endWordPosition="2985">atrix of W) on which the SVD is computed (block SVD); the white area is regarded as fixed to zero matrices. 5 Experiments and Results 5.1 Experimental design The base (source domain) disambiguation model is trained on the Alpino Treebank (van Noord, 2006) (newspaper text), which consists of approximately 7,000 sentences and 145,000 tokens. For parameter estimation of the disambiguation model, in all reported experiments we use the TADM2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior (U2=1000) and the (default) limited memory variable metric estimation technique (Malouf, 2002). For training the binary pivot predictors, we use the MegaM3 Optimization Package with the socalled ”bernoulli implicit” input format. To compute the SVD, we use SVDLIBC.4 The output of the parser is dependency structure. A standard evaluation metric is to measure the amount of generated dependencies that are identical to the stored dependencies (correct labeled dependencies), expressed as f-score. An alternative measure is concept accuracy (CA), which is similar to f-score, but allows possible discrepancy between the number of returned dependencies (van Noord, 2006; Plank and van Noord, 2htt</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of the Sixth Conference on Natural Language Learning (CoNLL-2002), Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>152--159</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="1965" citStr="McClosky et al., 2006" startWordPosition="303" endWordPosition="306"> Daum´e III, 2007), we face the problem of domain adaptation. The need for domain adaptation arises in many NLP tasks: Part-of-Speech tagging, Sentiment Analysis, Semantic Role Labeling or Statistical Parsing, to name but a few. For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001). The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daum´e III and Marcu, 2006; Daum´e III, 2007; Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007). We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daum´e III, 2007): supervised and semi-supervised. In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daum´e III, 2007), besides the labeled source data, we have access to a comparably small, but labeled amount of target data. In contrast, semi-supervised domain adaptation (Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007) is the scenario in which, in addition to the labeled source data, we only have unlabeled and </context>
<context position="3454" citStr="McClosky et al., 2006" startWordPosition="546" endWordPosition="549">ly, or the union of the data) achieve a relatively high performance level and are “surprisingly difficult to beat” (Daum´e III, 2007). Thus, one conclusion from that line of work is that as soon as there is a reasonable (often even small) amount of labeled target data, it is often more fruitful to either just use that, or to apply simple adaptation techniques (Daum´e III, 2007; Plank and van Noord, 2008). 2 Motivation and Prior Work While several authors have looked at the supervised adaptation case, there are less (and especially less successful) studies on semi-supervised domain adaptation (McClosky et al., 2006; Blitzer et al., 2006; Dredze et al., 2007). Of these, McClosky et al. (2006) deal specifically with selftraining for data-driven statistical parsing. They show that together with a re-ranker, improvements Proceedings of the EACL 2009 Student Research Workshop, pages 37–45, Athens, Greece, 2 April 2009. c�2009 Association for Computational Linguistics 37 are obtained. Similarly, Structural Correspondence Learning (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification. In contrast, Dredze et a</context>
<context position="4865" citStr="McClosky et al., 2006" startWordPosition="772" endWordPosition="775">over a state of the art baseline”. In the same shared task, an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa, 2007). The system just ended up at rank 7 out of 8 teams. However, based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive.1 Thus, the effectiveness of SCL is rather unexplored for parsing. So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007), i.e. systems employing (constituent or dependency based) treebank grammars (Charniak, 1996). Parse selection constitutes an important part of many parsing systems (Johnson et al., 1999; Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006). Yet, the adaptation of parse selection models to novel domains is a far less studied area. This may be motivated by the fact that potential gains for this task are inherently bounded by the underlying grammar. The few studies on adapting disambiguation models (Hara et al., 2005; Plank and van Noord, 2008) have </context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nelleke Oostdijk</author>
</authors>
<title>The Spoken Dutch Corpus: Overview and first evaluation.</title>
<date>2000</date>
<booktitle>In Proceedings of Second International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>887--894</pages>
<contexts>
<context position="7301" citStr="Oostdijk, 2000" startWordPosition="1151" endWordPosition="1153">al empirical results of this ongoing study. 3 Background: Alpino parser Alpino (van Noord and Malouf, 2005; van Noord, 2006) is a robust computational analyzer for Dutch that implements the conceptual two-stage parsing approach. The system consists of approximately 800 grammar rules in the tradition of HPSG, and a large hand-crafted lexicon, that together with a left-corner parser constitutes the generation component. For parse selection, Alpino employs a discriminative approach based on Maximum Entropy (MaxEnt). The output of the parser is dependency structure based on the guidelines of CGN (Oostdijk, 2000). The Maximum Entropy model (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997) is a conditional model that assigns a probability to every possible parse w for a given sentence s. The model consists of a set of m feature functions fj(w) that describe properties of parses, together with their associated weights Bj. The denominator is a normalization term where Y (s) is the set of parses with yield s: Ey∈Y (s) exp(Emj=1 Bjfj(y))) (1) The parameters (weights) Bj can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus (Johnson et al., 1999; van Noor</context>
</contexts>
<marker>Oostdijk, 2000</marker>
<rawString>Nelleke Oostdijk. 2000. The Spoken Dutch Corpus: Overview and first evaluation. In Proceedings of Second International Conference on Language Resources and Evaluation (LREC), pages 887–894.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Gertjan van Noord</author>
</authors>
<title>Exploring an auxiliary distribution based approach to domain adaptation of a syntactic disambiguation model.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on CrossFramework and Cross-Domain Parser Evaluation (PE),</booktitle>
<location>Manchester,</location>
<marker>Plank, van Noord, 2008</marker>
<rawString>Barbara Plank and Gertjan van Noord. 2008. Exploring an auxiliary distribution based approach to domain adaptation of a syntactic disambiguation model. In Proceedings of the Workshop on CrossFramework and Cross-Domain Parser Evaluation (PE), Manchester, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A simple introduction to maximum entropy models for natural language processing.</title>
<date>1997</date>
<tech>Technical report,</tech>
<institution>Institute for Research in Cognitive Science, University of Pennsylvania.</institution>
<contexts>
<context position="7368" citStr="Ratnaparkhi, 1997" startWordPosition="1162" endWordPosition="1163">parser Alpino (van Noord and Malouf, 2005; van Noord, 2006) is a robust computational analyzer for Dutch that implements the conceptual two-stage parsing approach. The system consists of approximately 800 grammar rules in the tradition of HPSG, and a large hand-crafted lexicon, that together with a left-corner parser constitutes the generation component. For parse selection, Alpino employs a discriminative approach based on Maximum Entropy (MaxEnt). The output of the parser is dependency structure based on the guidelines of CGN (Oostdijk, 2000). The Maximum Entropy model (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997) is a conditional model that assigns a probability to every possible parse w for a given sentence s. The model consists of a set of m feature functions fj(w) that describe properties of parses, together with their associated weights Bj. The denominator is a normalization term where Y (s) is the set of parses with yield s: Ey∈Y (s) exp(Emj=1 Bjfj(y))) (1) The parameters (weights) Bj can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus (Johnson et al., 1999; van Noord and Malouf, 2005): Em j=1 Bˆ = arg mθ log L(B) − 2σ2 B2j (2) wher</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>A. Ratnaparkhi. 1997. A simple introduction to maximum entropy models for natural language processing. Technical report, Institute for Research in Cognitive Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Michiel Bacchiani</author>
</authors>
<title>Supervised and unsupervised pcfg adaptation to novel domains. In</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL).</booktitle>
<contexts>
<context position="1855" citStr="Roark and Bacchiani, 2003" startWordPosition="283" endWordPosition="286"> “source” (out-of-domain), but we would like a model that performs well on some new “target” domain (Gildea, 2001; Daum´e III, 2007), we face the problem of domain adaptation. The need for domain adaptation arises in many NLP tasks: Part-of-Speech tagging, Sentiment Analysis, Semantic Role Labeling or Statistical Parsing, to name but a few. For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001). The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daum´e III and Marcu, 2006; Daum´e III, 2007; Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007). We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daum´e III, 2007): supervised and semi-supervised. In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daum´e III, 2007), besides the labeled source data, we have access to a comparably small, but labeled amount of target data. In contrast, semi-supervised domain adaptation (Blitzer et al., 2006; McClosky et al., 2006; Dred</context>
<context position="4842" citStr="Roark and Bacchiani, 2003" startWordPosition="768" endWordPosition="771"> performance substantially over a state of the art baseline”. In the same shared task, an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa, 2007). The system just ended up at rank 7 out of 8 teams. However, based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive.1 Thus, the effectiveness of SCL is rather unexplored for parsing. So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007), i.e. systems employing (constituent or dependency based) treebank grammars (Charniak, 1996). Parse selection constitutes an important part of many parsing systems (Johnson et al., 1999; Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006). Yet, the adaptation of parse selection models to novel domains is a far less studied area. This may be motivated by the fact that potential gains for this task are inherently bounded by the underlying grammar. The few studies on adapting disambiguation models (Hara et al., 2005; Plank and</context>
</contexts>
<marker>Roark, Bacchiani, 2003</marker>
<rawString>Brian Roark and Michiel Bacchiani. 2003. Supervised and unsupervised pcfg adaptation to novel domains. In In Proceedings of the Human Language Technology Conference and Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuyuki Shimizu</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Structural correspondence learning for dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL</booktitle>
<contexts>
<context position="4421" citStr="Shimizu and Nakagawa, 2007" startWordPosition="696" endWordPosition="699">al Linguistics 37 are obtained. Similarly, Structural Correspondence Learning (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification. In contrast, Dredze et al. (2007) report on “frustrating” results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. ”no team was able to improve target domain performance substantially over a state of the art baseline”. In the same shared task, an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa, 2007). The system just ended up at rank 7 out of 8 teams. However, based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive.1 Thus, the effectiveness of SCL is rather unexplored for parsing. So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007), i.e. systems employing (constituent or dependency based) treebank grammars (Charniak, 1996). Parse selection constitutes an i</context>
</contexts>
<marker>Shimizu, Nakagawa, 2007</marker>
<rawString>Nobuyuki Shimizu and Hiroshi Nakagawa. 2007. Structural correspondence learning for dependency parsing. In Proceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
<author>Robert Malouf</author>
</authors>
<title>Wide coverage parsing with stochastic attribute value grammars. Draft available from http://www.let.rug.nl/˜vannoord. A preliminary version of this paper was published</title>
<date>2005</date>
<booktitle>in the Proceedings of the IJCNLP workshop Beyond Shallow Analyses,</booktitle>
<location>Hainan</location>
<marker>van Noord, Malouf, 2005</marker>
<rawString>Gertjan van Noord and Robert Malouf. 2005. Wide coverage parsing with stochastic attribute value grammars. Draft available from http://www.let.rug.nl/˜vannoord. A preliminary version of this paper was published in the Proceedings of the IJCNLP workshop Beyond Shallow Analyses, Hainan China, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>At Last Parsing Is Now Operational.</title>
<date>2006</date>
<booktitle>In TALN 2006 Verbum Ex Machina, Actes De La 13e Conference sur Le Traitement Automatique des Langues naturelles,</booktitle>
<pages>20--42</pages>
<location>Leuven.</location>
<marker>van Noord, 2006</marker>
<rawString>Gertjan van Noord. 2006. At Last Parsing Is Now Operational. In TALN 2006 Verbum Ex Machina, Actes De La 13e Conference sur Le Traitement Automatique des Langues naturelles, pages 20–42, Leuven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Using self-trained bilexical preferences to improve disambiguation accuracy.</title>
<date>2007</date>
<booktitle>In Proceedings of the Tenth International Conference on Parsing Technologies. IWPT 2007, Prague.,</booktitle>
<pages>1--10</pages>
<location>Prague.</location>
<marker>van Noord, 2007</marker>
<rawString>Gertjan van Noord. 2007. Using self-trained bilexical preferences to improve disambiguation accuracy. In Proceedings of the Tenth International Conference on Parsing Technologies. IWPT 2007, Prague., pages 1–10, Prague.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>