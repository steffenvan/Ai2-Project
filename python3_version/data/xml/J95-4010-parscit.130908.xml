<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001927">
<note confidence="0.266017">
Book Reviews
</note>
<title confidence="0.858031">
Computational lexical semantics
</title>
<author confidence="0.736499">
Patrick Saint-Dizier and Evelyne Viegas (editors)
</author>
<bodyText confidence="0.7200535">
(Institut de Recherche en Informatique de Toulouse, CNRS and Brandeis University)
Cambridge University Press (Studies in
natural language processing, edited by
Branimir K. Boguraev), 1995, ix +
447 pp.
Hardbound, ISBN 0-521-44410-1, $69.95
</bodyText>
<figure confidence="0.983332333333333">
Reviewed by
Paul Deane
Dataware Technologies
</figure>
<sectionHeader confidence="0.564998" genericHeader="abstract">
1. Overview
</sectionHeader>
<bodyText confidence="0.999908272727273">
The last decade has seen a striking expansion of interest in the lexicon and in lexical
semantics. Within theoretical linguistics, this trend can be measured by the increasing
interest generative grammarians have displayed toward such issues as lexical con-
ceptual structure and argument structure and by the increased appeal of cognitive
semantics. Within computational linguistics, the same period has seen a burgeoning
of interest in the construction of semantically realistic lexicons and their integration
with larger Natural Language Processing (NLP) systems. Computational lexical semantics
supplies a fascinating snapshot of the state-of-the-art in the early 1990s. The articles
date from late 1991 or early 1992, reflecting the immediate impact of Pustejovsky&apos;s the-
ory of the generative lexicon (1991), but lacking references to George Miller&apos;s WordNet
system (1990). The book is organized into six sections:
</bodyText>
<listItem confidence="0.942166833333333">
I. Psycholinguistics for lexical semantics;
II. Foundational issues in lexical semantics;
III. Lexical databases;
IV. Lexical semantics and artificial intelligence;
V. Applications;
VI. Computer models for lexical semantics.
</listItem>
<bodyText confidence="0.999787666666667">
The book provides wide coverage. A variety of problems are addressed, though the
focus is upon polysemy, disambiguation, and co-composition—that is, processes by
which word meanings are dynamically derived and contextually modulated. There
are gaps in coverage, however, which appear to reflect the early state of work in the
field. Polysemy generally falls into one of three categories: metonymy (associations
between concepts in the same domain), metaphor (mappings across conceptual do-
mains), and various patterns of semantic variation that are often lumped together and
labeled as prototype effects. But this book (and, it would appear, computational ap-
proaches generally) concentrates almost exclusively on explicating metonymy (which
</bodyText>
<page confidence="0.996252">
593
</page>
<note confidence="0.465579">
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.999924166666667">
is, in fact, the least problematic of the three and thus the easiest to formalize). Similarly,
the articles in the book are strongest when dealing with logical aspects of meaning.
Very little attention is devoted to dealing with less easily formalized aspects of lexical
knowledge, such as the role of attention and frequency of use in disambiguation pro-
cesses. These, of course, are not criticisms of the book, but a reflection of its focus on
problems that seem tractable using well-understood tools and techniques.
</bodyText>
<sectionHeader confidence="0.536396" genericHeader="categories and subject descriptors">
2. Contents of the book
</sectionHeader>
<bodyText confidence="0.9894227">
The articles may be divided under headings somewhat different from those used by
the editors. While the editors organized the information by field and task, the actual
research is heavily weighted toward a few key issues, in particular polysemy and
lexical disambiguation.
General articles. &apos;An introduction to lexical semantics from a linguistic and a psycholin-
guistic perspective&apos;, by Patrick Saint-Dizier and Evelyne Viegas; &apos;Polysemy and related
phenomena from a cognitive linguistic viewpoint&apos;, by D. A. Cruse; &apos;Mental lexicon and
machine lexicon: Which properties are shared by machine and mental word represen-
tations? Which are not?&apos; by Jean-Francois Le Ny.
Saint-Dizier and Viegas provide a brief review of lexical semantics covering classic
concepts in several key frameworks, including Jackendoff&apos;s lexical conceptual struc-
ture, Pustejovsky&apos;s generative lexicon, and Menuk&apos;s Explanatory Combinatory Dic-
tionary (1988). Cruse focuses on providing a linguistic explication of the complexities
that make polysemy difficult to handle, including its partial productivity lack of clear
boundaries, and context-sensitivity. Le Ny provides a useful checklist of the properties
of the mental lexicon that are and are not paralleled in current NLP lexicons. Criti-
cally, Le Ny notes, such basic psycholinguistic properties as activation (and the related
properties of activability and pre-activation) are not built into most NLP models of
the lexicon. The concerns raised by Le Ny and Cruse are very important, particularly
since they do not seem yet to have been fully addressed in current NLP work.
Polysemy in general. &apos;Word meaning between lexical and conceptual structure&apos;, by Pe-
ter Gerstl; &apos;Lexical semantics and terminological knowledge representation&apos;, by Gerrit
Burkert; &apos;A preliminary lexical and conceptual analysis of break: A computational per-
spective&apos;, by Martha Palmer and Alain Polguere. Of closely related interest: &apos;Inheriting
polysemy&apos;, by Adam Kilgariff.
Computational lexical semantics implies the construction of very large lexical
databases that go beyond traditional NLP lexicons (or most Al knowledge bases for
that matter) by supporting highly flexible, dynamic interpretative processing. Gerstl&apos;s
article explores what information is required to support dynamic lexical interpreta-
tion. After a detailed literature review, he argues programmatically for an analysis in
which word meanings are composed of a set of interacting &apos;factors&apos;, some of which con-
strain and others of which expand the potential for interpretation. Burkert&apos;s (equally
programmatic) article focuses on aspects of lexical meaning amenable to traditional
knowledge representation formalisms (i.e., term subsumption languages). By contrast,
Palmer and Polguere&apos;s article is extremely data-oriented. They focus upon the word
break, arguing that it is best analyzed as a hierarchy of sense components, each of
which entails specific constraints on the word&apos;s overall syntactic and semantic struc-
ture. Adam Kilgariff&apos;s article is extremely interesting because it shows that at least
some types of variations in word meaning can be stored in a semantic net and in-
herited. It should be noted, however, that Kilgariff&apos;s article focuses on metonymy:
</bodyText>
<page confidence="0.991176">
594
</page>
<subsectionHeader confidence="0.815145">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999033204081633">
other types of polysemy are left largely unaddressed. This emphasis on accounting
for metonymy is typical of the current state of NLP work on polysemy, as may be
seen in the articles discussed below.
Metonymy, the generative lexicon and related issues. &apos;Linguistic constraints on type coer-
cion&apos;, by James Pustejovsky; &apos;From lexical semantics to text analysis&apos;, by Sabine Bergler;
&apos;Lexical functions, generative lexicons, and the world&apos;, by Dirk Heylen; &apos;Semantic fea-
tures in a generic lexicon&apos;, by Gabriel G. Bes and Alain LeComte. A closely related
application of generative lexicon theory: &apos;The representation of group denoting nouns
in a lexical knowledge base&apos;, by Ann Copestake. Of related interest: &apos;A lexical semantic
solution to the divergence problem in machine translation&apos;, by Bonnie J. Dorr.
Much of the semantic variability of natural language is due to the influence of
logical metonymy—where word meanings shift (following conceptually natural asso-
ciations) during the process of semantic composition. Pustejovsky&apos;s theory of the gen-
erative lexicon integrates a linguistically motivated view of metonymy with a rigorous
formal semantics through the mechanism of type coercion to make the lexicon into
a generative mechanism capable of deriving contextually appropriate meanings &apos;on
the fly&apos;. Pustejovsky&apos;s article outlines the basic theory; Sabine Bergler discusses its ap-
plications to text comprehension. Dirk Heylen draws parallels between Pustejovsky&apos;s
generative lexicon and Mel&apos;aik&apos;s Explanatory Combinatory Dictionary, pointing out
close parallels between the two systems. Bes and LeComte seek to define a metalan-
guage for describing proposals about word meaning, proposing mechanisms parallel
to Pustejovsky&apos;s, though purposely more generic.
Copestake&apos;s article is quite interesting, as it has a strong theoretical base but ad-
dresses many of the practical problems involved in constructing a large lexical seman-
tic database. Copestake&apos;s system implements a version of Pustejovsky&apos;s generative
lexicon in a unification-based lexical knowledge representation. She develops a type-
coercion analysis of the polysemy and related syntactic properties of group nouns,
and illustrates how the relevant lexical entries can be automatically acquired from a
machine-readable dictionary. But, as might be expected, there are significant difficul-
ties, including problems in defining default inheritance relationships and the absence
of fully consistent cues for group noun membership in the underlying dictionary data.
Dorr&apos;s article focuses on machine translation but shares with the articles listed
above a deep concern for providing an adequate linguistic underpinning for NLP
work. She argues that use of Jackendoff&apos;s lexical conceptual structures provides a
useful solution to the problem of divergence in machine translation. The argument is
simple: since lexical conceptual structures (a) are closely tied to syntactic structure, and
(b) are nonetheless deep semantic representations, they are well suited to provide an
interlingua. Independently needed, language-specific mapping mechanisms can then
be exploited to account for different divergent syntactic expressions of the common
interlingua.
Theoretically motivated work using Jackendoff&apos;s conceptual semantics and/or
Pustejovsky&apos;s generative lexicon forms the cutting edge of current NLP work. The ar-
ticles listed above provide a good picture of early advances along this line of research.
It may be too early, however, to judge how far the generative lexicon approach can be
taken, until large-scale lexicons based upon its principles have been constructed. But
there can be no doubt that these approaches mark a significant advance over earlier
computational models of the lexicon.
Disambiguation, defaults, and logical approaches to NLP. &apos;Large neural networks for the res-
olution of lexical ambiguity&apos;, by Jean Veronis and Nancy Ide; &apos;Blocking&apos;, by Ted Briscoe,
</bodyText>
<page confidence="0.995964">
595
</page>
<note confidence="0.75327">
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.952024222222223">
Ann Copestake, and Alex Lascarides; &apos;A non-monotonic approach to lexical seman-
tics&apos; by Daniel Kayser and Flocine Abir. Of related interest: &apos;Introducing Lexlog&apos;, by
Jacques Jayez; &apos;Constraint propagation techniques for lexical semantics descriptions&apos;,
by Patrick Saint-Dizier.
Word-sense disambiguation is a fundamental and potentially intractable problem.
Veronis and Ide argue that it can be accomplished by a neural network without prior
linguistic analysis. They construct a network in which words are associated with the
words in their (machine-readable dictionary) definition. In the data sets upon which
they report, they achieve excellent disambiguation, but their method would seem
to have severe limitations: it is not at all clear that it would scale up well, both for
computational reasons (the size of the required neural network) and theoretical reasons
(the potential for interference from crosstalk among definitions as the number of words
in the network increases.)
Default logic is an obvious alternative to connectionist techniques of lexical disam-
biguation. Briscoe et al.&apos;s article deals with the important issue of lexical blocking—the
prevention of one form or meaning from occurring because a competing element al-
ready exists. In principle, the resolution principles that Briscoe et al. employ apply
equally well to formal and semantic ambiguity. Kayser and Abir argue that disam-
biguation can be formally modeled by using default logic to set preferences for one
meaning over another. The articles by Jayez and Saint-Dizier concern implementations
of logic programming-based NLP systems; while not directly concerned with default
logic, many of the same technical issues arise, such as the propagation of inherited
information and the resolution of multiple constraints in a complex knowledge base.
These articles address one of the thorniest problem types in lexical semantics.
However, default logic has an apparent weakness when applied to language: its fail-
ure to account for the effects of analogy, habituation, and other essentially cognitive
factors. For example, Kayser and Abir are forced to postulate a &apos;strength&apos; factor (in
effect, an impressionistic measure of relative psychological dominance, or capacity to
attract attention in a neutral context) to induce the default logic to choose a single
(likely) word meaning from the choices that remain after clearly inappropriate mean-
ings have been eliminated. It remains to be seen whether an account can be developed
that maintains the strengths of a formal logic while incorporating realistic theories of
dominance, attention, and other extra-logical aspects of human cognition.
Structural analysis. &apos;Lexical semantics: Dictionary or encyclopedia&apos;, by Marc Cavazza
and Pierre Zweigenbaum; &apos;Lexical functions of the Explanatory Combinatorial Dictio-
nary for lexicalization in text generation&apos;, by Margarita Alonso Ramos, Agnes Tutin,
and Guy LaPalme.
Cavazza and Zweigenbaum describe a classically structuralist technique for an-
alyzing word meaning into contrasting semantic components. Since they apply the
technique to medical texts, where the key words tend to be highly terminologized, the
results are quite good. Keeping a strict separation between domain prototypes and
word definitions, they focus on demonstrating how an NLP system can infer proto-
types from partial, phrasal descriptions. Ramos et al. describe the use of Menuk&apos;s
lexical functions to generate anaphor matches, paraphrases, and the like in a speech
generation system.
</bodyText>
<sectionHeader confidence="0.988638" genericHeader="conclusions">
3. Conclusions
</sectionHeader>
<bodyText confidence="0.985485">
One of the striking facts about this book is the way in which it illustrates a general
trend: the growing recognition (throughout linguistics and its allied fields) of the im-
</bodyText>
<page confidence="0.994102">
596
</page>
<subsectionHeader confidence="0.837012">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.9999335">
portance of the lexicon. Computational lexical semantics provides a valuable overview
of NLP work instantiating this trend. The articles are generally of very high quality,
and provide a fairly balanced view of NLP approaches, with approximately equal
representation of research originating in North America and Europe.
</bodyText>
<sectionHeader confidence="0.9924" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997321272727273">
Miller, George A.; Beckwith, Richard;
Fellbaum, Christiane; Gross, Derek; and
Miller, Katherine (1990). &amp;quot;Introduction to
WordNet: An on-line lexical database.&amp;quot;
International Journal of Lexicography, 3(4),
235-244.
Menuk, Igor and Zholkovsky, Alexander
(1988). &amp;quot;The Explanatory Combinatorial
Dictionary.&amp;quot; In Evens, Martha W. (editor),
Relational Models of the Lexicon, 41-74.
Cambridge: Cambridge University Press.
Pustejovsky, James (1991). &amp;quot;The generative
lexicon.&amp;quot; Computational Linguistics, 17(4),
409-441.
Paul Deane wrote his Ph.D. thesis at Chicago on Semantic Theory and the Problem of Polysemy
(1987). His research focuses on lexical semantics, polysemy, and metaphor, with articles ap-
pearing in such journals as Lingua, Cognitive Linguistics, Metaphor and Symbolic Activity, and
Journal of Pragmatics. In addition, he has published a monograph on cognitive syntax, Grammar
in mind and brain (Mouton de Gruyter, 1993). He is currently employed as linguist and lexi-
cal semanticist for a project in cognitive modalities in information retrieval. Deane&apos;s address
is: Dataware Technologies, One Antares Drive, Suite 200, Nepean, Ontario, Canada K2E 8C4;
e-mail: PDeane@dataware.com.
</reference>
<page confidence="0.997495">
597
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.280034">
<title confidence="0.9716905">Book Reviews Computational lexical semantics</title>
<author confidence="0.992684">Patrick Saint-Dizier</author>
<author confidence="0.992684">Evelyne Viegas</author>
<affiliation confidence="0.913563">(Institut de Recherche en Informatique de Toulouse, CNRS and Brandeis University) Cambridge University Press (Studies in</affiliation>
<note confidence="0.9099592">natural language processing, edited by Branimir K. Boguraev), 1995, ix + 447 pp. Hardbound, ISBN 0-521-44410-1, $69.95 Reviewed by</note>
<author confidence="0.98556">Paul Deane</author>
<affiliation confidence="0.602808">Dataware Technologies</affiliation>
<abstract confidence="0.99402585">1. Overview The last decade has seen a striking expansion of interest in the lexicon and in lexical semantics. Within theoretical linguistics, this trend can be measured by the increasing interest generative grammarians have displayed toward such issues as lexical conceptual structure and argument structure and by the increased appeal of cognitive semantics. Within computational linguistics, the same period has seen a burgeoning of interest in the construction of semantically realistic lexicons and their integration larger Natural Language Processing (NLP) systems. lexical semantics supplies a fascinating snapshot of the state-of-the-art in the early 1990s. The articles date from late 1991 or early 1992, reflecting the immediate impact of Pustejovsky&apos;s theory of the generative lexicon (1991), but lacking references to George Miller&apos;s WordNet system (1990). The book is organized into six sections: I. Psycholinguistics for lexical semantics; II. Foundational issues in lexical semantics; III. Lexical databases; IV. Lexical semantics and artificial intelligence; V. Applications; VI. Computer models for lexical semantics. The book provides wide coverage. A variety of problems are addressed, though the focus is upon polysemy, disambiguation, and co-composition—that is, processes by which word meanings are dynamically derived and contextually modulated. There are gaps in coverage, however, which appear to reflect the early state of work in the field. Polysemy generally falls into one of three categories: metonymy (associations between concepts in the same domain), metaphor (mappings across conceptual domains), and various patterns of semantic variation that are often lumped together and labeled as prototype effects. But this book (and, it would appear, computational approaches generally) concentrates almost exclusively on explicating metonymy (which 593 Computational Linguistics Volume 21, Number 4 is, in fact, the least problematic of the three and thus the easiest to formalize). Similarly, the articles in the book are strongest when dealing with logical aspects of meaning. Very little attention is devoted to dealing with less easily formalized aspects of lexical knowledge, such as the role of attention and frequency of use in disambiguation processes. These, of course, are not criticisms of the book, but a reflection of its focus on problems that seem tractable using well-understood tools and techniques. 2. Contents of the book The articles may be divided under headings somewhat different from those used by the editors. While the editors organized the information by field and task, the actual research is heavily weighted toward a few key issues, in particular polysemy and lexical disambiguation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.&amp;quot;</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>235--244</pages>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Miller, George A.; Beckwith, Richard; Fellbaum, Christiane; Gross, Derek; and Miller, Katherine (1990). &amp;quot;Introduction to WordNet: An on-line lexical database.&amp;quot; International Journal of Lexicography, 3(4), 235-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Menuk</author>
<author>Alexander Zholkovsky</author>
</authors>
<title>The Explanatory Combinatorial Dictionary.&amp;quot;</title>
<date>1988</date>
<booktitle>Relational Models of the Lexicon,</booktitle>
<pages>41--74</pages>
<editor>In Evens, Martha W. (editor),</editor>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge:</location>
<marker>Menuk, Zholkovsky, 1988</marker>
<rawString>Menuk, Igor and Zholkovsky, Alexander (1988). &amp;quot;The Explanatory Combinatorial Dictionary.&amp;quot; In Evens, Martha W. (editor), Relational Models of the Lexicon, 41-74. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>The generative lexicon.&amp;quot;</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>4</issue>
<pages>409--441</pages>
<marker>Pustejovsky, 1991</marker>
<rawString>Pustejovsky, James (1991). &amp;quot;The generative lexicon.&amp;quot; Computational Linguistics, 17(4), 409-441.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Paul Deane</author>
</authors>
<title>wrote his Ph.D. thesis at Chicago on Semantic Theory and the Problem of Polysemy</title>
<date>1987</date>
<booktitle>Grammar in mind and brain (Mouton de Gruyter,</booktitle>
<location>Nepean, Ontario, Canada</location>
<note>K2E 8C4; e-mail: PDeane@dataware.com.</note>
<marker>Deane, 1987</marker>
<rawString>Paul Deane wrote his Ph.D. thesis at Chicago on Semantic Theory and the Problem of Polysemy (1987). His research focuses on lexical semantics, polysemy, and metaphor, with articles appearing in such journals as Lingua, Cognitive Linguistics, Metaphor and Symbolic Activity, and Journal of Pragmatics. In addition, he has published a monograph on cognitive syntax, Grammar in mind and brain (Mouton de Gruyter, 1993). He is currently employed as linguist and lexical semanticist for a project in cognitive modalities in information retrieval. Deane&apos;s address is: Dataware Technologies, One Antares Drive, Suite 200, Nepean, Ontario, Canada K2E 8C4; e-mail: PDeane@dataware.com.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>