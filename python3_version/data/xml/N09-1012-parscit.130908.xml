<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.999504">
Improving Unsupervised Dependency Parsing
with Richer Contexts and Smoothing
</title>
<author confidence="0.977637">
William P. Headden III, Mark Johnson, David McClosky
</author>
<affiliation confidence="0.9643145">
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
</affiliation>
<address confidence="0.955071">
Providence, RI 02912
</address>
<email confidence="0.999809">
{headdenw,mj,dmcc}@cs.brown.edu
</email>
<sectionHeader confidence="0.997404" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999633642857143">
Unsupervised grammar induction models tend
to employ relatively simple models of syntax
when compared to their supervised counter-
parts. Traditionally, the unsupervised mod-
els have been kept simple due to tractabil-
ity and data sparsity concerns. In this paper,
we introduce basic valence frames and lexi-
cal information into an unsupervised depen-
dency grammar inducer and show how this
additional information can be leveraged via
smoothing. Our model produces state-of-the-
art results on the task of unsupervised gram-
mar induction, improving over the best previ-
ous work by almost 10 percentage points.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98940025">
The last decade has seen great strides in statisti-
cal natural language parsing. Supervised and semi-
supervised methods now provide highly accurate
parsers for a number of languages, but require train-
ing from corpora hand-annotated with parse trees.
Unfortunately, manually annotating corpora with
parse trees is expensive and time consuming so for
languages and domains with minimal resources it is
valuable to study methods for parsing without re-
quiring annotated sentences.
In this work, we focus on unsupervised depen-
dency parsing. Our goal is to produce a directed
graph of dependency relations (e.g. Figure 1) where
each edge indicates a head-argument relation. Since
the task is unsupervised, we are not given any ex-
amples of correct dependency graphs and only take
words and their parts of speech as input. Most
of the recent work in this area (Smith, 2006; Co-
hen et al., 2008) has focused on variants of the
The big dog barks
</bodyText>
<figureCaption confidence="0.999506">
Figure 1: Example dependency parse.
</figureCaption>
<bodyText confidence="0.99955940625">
Dependency Model with Valence (DMV) by Klein
and Manning (2004). DMV was the first unsu-
pervised dependency grammar induction system to
achieve accuracy above a right-branching baseline.
However, DMV is not able to capture some of the
more complex aspects of language. Borrowing some
ideas from the supervised parsing literature, we
present two new models: Extended Valence Gram-
mar (EVG) and its lexicalized extension (L-EVG).
The primary difference between EVG and DMV is
that DMV uses valence information to determine the
number of arguments a head takes but not their cat-
egories. In contrast, EVG allows different distri-
butions over arguments for different valence slots.
L-EVG extends EVG by conditioning on lexical in-
formation as well. This allows L-EVG to potentially
capture subcategorizations. The downside of adding
additional conditioning events is that we introduce
data sparsity problems. Incorporating more valence
and lexical information increases the number of pa-
rameters to estimate. A common solution to data
sparsity in supervised parsing is to add smoothing.
We show that smoothing can be employed in an un-
supervised fashion as well, and show that mixing
DMV, EVG, and L-EVG together produces state-of-
the-art results on this task. To our knowledge, this is
the first time that grammars with differing levels of
detail have been successfully combined for unsuper-
vised dependency parsing.
A brief overview of the paper follows. In Section
2, we discuss the relevant background. Section 3
presents how we will extend DMV with additional
</bodyText>
<page confidence="0.98457">
101
</page>
<note confidence="0.8933425">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 101–109,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.9843288">
that satisfies the following properties:
features. We describe smoothing in an unsupervised
context in Section 4. In Section 5, we discuss search
issues. We present our experiments in Section 6 and
conclude in Section 7.
</bodyText>
<listItem confidence="0.866379">
1. Tied rules have the same probability.
2. Rules expanding the same nonterminal are
never tied.
</listItem>
<sectionHeader confidence="0.981284" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.981767022222223">
In this paper, the observed variables will be a corpus
of n sentences of text s = s1 ... sn, and for each
word sij an associated part-of-speech τij. We denote
the set of all words as Vw and the set of all parts-of-
speech as Vτ. The hidden variables are parse trees
t = t1 ... tn and parameters θ¯ which specify a dis-
tribution over t. A dependency tree ti is a directed
acyclic graph whose nodes are the words in si. The
graph has a single incoming edge for each word in
each sentence, except one called the root of ti. An
edge from word i to word j means that word j is
an argument of word i or alternatively, word i is the
head of word j. Note that each word token may be
the argument of at most one head, but a head may
have several arguments.
If parse tree ti can be drawn on a plane above the
sentence with no crossing edges, it is called projec-
tive. Otherwise it is nonprojective. As in previous
work, we restrict ourselves to projective dependency
trees. The dependency models in this paper will be
formulated as a particular kind of Probabilistic Con-
text Free Grammar (PCFG), described below.
3. If N1 —* β1 and N2 —* β2 are tied then the ty-
ing relation defines a one-to-one mapping be-
tween rules in RN, and RN2, and we say that
N1 and N2 are tied nonterminals.
As we see below, we can estimate tied PCFGs using
standard techniques. Clearly, the tying relation also
defines an equivalence class over nonterminals. The
tying relation allows us to formulate the distribu-
tions over trees in terms of rule equivalence classes
and nonterminal equivalence classes. Suppose R¯ is
the set of rule equivalence classes and JV¯ is the set
of nonterminal equivalence classes. Since all rules
in an equivalence class r¯ have the same probability
(condition 1), and since all the nonterminals in an
equivalence class N¯ E JV¯ have the same distribu-
tion over rule equivalence classes (condition 1 and
3), we can define the set of rule equivalence classes
R¯N¯ associated with a nonterminal equivalence class
¯N, and a vector θ¯ of probabilities, indexed by rule
equivalence classes r¯ E R¯ . θ¯N¯ refers to the sub-
vector of θ¯ associated with nonterminal equivalence
class ¯N, indexed by r¯ E R¯¯N. Since rules in the
same equivalence class have the same probability,
</bodyText>
<subsectionHeader confidence="0.926154">
2.1 Tied Probabilistic Context Free Grammars we have that for each r E ¯r, θr = ¯θ¯r.
</subsectionHeader>
<bodyText confidence="0.99999055">
In order to perform smoothing, we will find useful a
class of PCFGs in which the probabilities of certain
rules are required to be the same. This will allow
us to make independence assumptions for smooth-
ing purposes without losing information, by giving
analogous rules the same probability.
Let G = (JV, T , S, R, θ) be a Probabilistic Con-
text Free Grammar with nonterminal symbols JV,
terminal symbols T, start symbol S E JV, set of
productions R of the form N —* β, N E JV, β E
(JV U T )*. Let RN indicate the subset of R whose
left-hand sides are N. θ is a vector of length JRJ, in-
dexed by productions N —* β E R. θN�β specifies
the probability that N rewrites to β. We will let θN
indicate the subvector of θ corresponding to RN.
A tied PCFG constrains a PCFG G with a tying
relation, which is an equivalence relation over rules
Let f(t, r) denote the number of times rule r ap-
pears in tree t, and let f(t, ¯r) = ErE¯r f(t, r). We
see that the complete data likelihood is
</bodyText>
<equation confidence="0.998303">
θf (t,r) = 11
r
¯rE R¯
</equation>
<bodyText confidence="0.999834636363636">
That is, the likelihood is a product of multinomi-
als, one for each nonterminal equivalence class, and
there are no constraints placed on the parameters of
these multinomials besides being positive and sum-
ming to one. This means that all the standard es-
timation methods (e.g. Expectation Maximization,
Variational Bayes) extend directly to tied PCFGs.
Maximum likelihood estimation provides a point
estimate of ¯θ. However, often we want to incorpo-
rate information about θ¯ by modeling its prior distri-
bution. As a prior, for each N¯ E JV¯ we will specify a
</bodyText>
<equation confidence="0.9974925">
P(s,tJθ) = 11 H
¯rE R¯ rE¯r
¯θf(t,¯r)
r¯
</equation>
<page confidence="0.980625">
102
</page>
<bodyText confidence="0.9627005">
Dirichlet distribution over θN¯ with hyperparameters
α ¯N. The Dirichlet has the density function:
</bodyText>
<equation confidence="0.9899725">
7r(P¯r∈ R¯N¯ α¯r) ¯θα¯r−1r¯,
11T∈ R¯N¯ Γ(α¯r) ¯r∈ R¯N¯
</equation>
<bodyText confidence="0.999106428571429">
Thus the prior over θ is a product of Dirichlets,which
is conjugate to the PCFG likelihood function (John-
son et al., 2007). That is, the posterior P(¯θ|s, t, α)
is also a product of Dirichlets, also factoring into a
Dirichlet for each nonterminal ¯N, where the param-
eters α¯r are augmented by the number of times rule
r¯ is observed in tree t:
</bodyText>
<equation confidence="0.997863">
P( θ|s,t,α) ∝ P(s,t |θ)P( θ|α)
¯θf(t,¯r)+α¯r−1
r¯
</equation>
<bodyText confidence="0.987234137931034">
We can see that α¯r acts as a pseudocount of the num-
ber of times r¯ is observed prior to t.
To make use of this prior, we use the Variational
Bayes (VB) technique for PCFGs with Dirichlet Pri-
ors presented by Kurihara and Sato (2004). VB es-
timates a distribution over ¯θ. In contrast, Expec-
tation Maximization estimates merely a point esti-
mate of ¯θ. In VB, one estimates Q(t, ¯θ), called
the variational distribution, which approximates the
posterior distribution P(t, ¯θ|s, α) by minimizing the
KL divergence of P from Q. Minimizing the KL
divergence, it turns out, is equivalent to maximiz-
ing a lower bound F of the log marginal likelihood
log P(s|α).
The negative of the lower bound, −F, is sometimes
called the free energy.
As is typical in variational approaches, Kuri-
hara and Sato (2004) make certain independence as-
sumptions about the hidden variables in the vari-
ational posterior, which will make estimating it
simpler. It factors Q(t, θ) = Q(t)Q( ¯θ) =
Qn i=1 Qi(ti)Q¯N∈N¯Q( θ ¯N). The goal is to recover
θ), the estimate of the posterior distribution over
parameters and Q(t), the estimate of the posterior
distribution over trees. Finding a local maximum of
F is done via an alternating maximization of Q(¯θ)
and Q(t). Kurihara and Sato (2004) show that each
θ ¯N) is a Dirichlet distribution with parameters
ˆαr = αr + EQ(t)f(t, r).
</bodyText>
<subsectionHeader confidence="0.999007">
2.2 Split-head Bilexical CFGs
</subsectionHeader>
<bodyText confidence="0.999991409090909">
In the sections that follow, we frame various de-
pendency models as a particular variety of CFGs
known as split-head bilexical CFGs (Eisner and
Satta, 1999). These allow us to use the fast Eisner
and Satta (1999) parsing algorithm to compute the
expectations required by VB in O(m3) time (Eis-
ner and Blatz, 2007; Johnson, 2007) where m is the
length of the sentence.1
In the split-head bilexical CFG framework, each
nonterminal in the grammar is annotated with a ter-
minal symbol. For dependency grammars, these
annotations correspond to words and/or parts-of-
speech. Additionally, split-head bilexical CFGs re-
quire that each word sij in sentence si is represented
in a split form by two terminals called its left part
sijL and right part sijR. The set of these parts con-
stitutes the terminal symbols of the grammar. This
split-head property relates to a particular type of de-
pendency grammar in which the left and right depen-
dents of a head are generated independently. Note
that like CFGs, split-head bilexical CFGs can be
made probabilistic.
</bodyText>
<subsectionHeader confidence="0.877172">
2.3 Dependency Model with Valence
</subsectionHeader>
<bodyText confidence="0.929317">
The most successful recent work on dependency
induction has focused on the Dependency Model
with Valence (DMV) by Klein and Manning (2004).
DMV is a generative model in which the head of
the sentence is generated and then each head recur-
sively generates its left and right dependents. The
arguments of head H in direction d are generated
by repeatedly deciding whether to generate another
new argument or to stop and then generating the
argument if required. The probability of deciding
whether to generate another argument is conditioned
on H, d and whether this would be the first argument
(this is the sense in which it models valence). When
DMV generates an argument, the part-of-speech of
that argument A is generated given H and d.
1Efficiently parsable versions of split-head bilexical CFGs
for the models described in this paper can be derived using the
fold-unfold grammar transform (Eisner and Blatz, 2007; John-
son, 2007).
</bodyText>
<equation confidence="0.999047153846154">
P( θ ¯N|α¯N) =
Y∝
¯r∈ R¯
X
log P(s|α) ≥
J Q(t, θ) log P(s, t,
t B Q(t,
t
θ|α)
θ)
= F
Q(
Q(
</equation>
<page confidence="0.995409">
103
</page>
<table confidence="0.5391755">
Rule Description
S → YH Select H as root
YH → LH RH Move to split-head representation
LH → HL STOP  |dir = L, head = H, val = 0
LH → L1 CONT  |dir = L, head = H, val = 0
H
L′ H → HL STOP  |dir = L, head = H, val = 1
L′H → L1 CONT  |dir = L, head = H, val = 1
H
Ll H → YA L′H Arg A  |dir = L,head = H
</table>
<figureCaption confidence="0.999449666666667">
Figure 2: Rule schema for DMV. For brevity, we omit
the portion of the grammar that handles the right argu-
ments since they are symmetric to the left (all rules are
the same except for the attachment rule where the RHS is
reversed). val E 10, 11 indicates whether we have made
any attachments.
</figureCaption>
<figure confidence="0.999910964285714">
S
Ybarks
Lbarks Rbarks
Lbarks barksR
Ydog
Ldog
Ldog
L′
barks
barksL
Rdog
dogR
LThe
TheL
RThe
TheR
Yb%g
Lb%g
bigL
L′
dog
Ldog
L′
dog
dogL
Rb%g
bigR
YThe
</figure>
<figureCaption confidence="0.945296">
The grammar schema for this model is shown in
Figure 2. The first rule generates the root of the sen-
</figureCaption>
<bodyText confidence="0.911065666666667">
tence. Note that these rules are for VH, A E VT so
there is an instance of the first schema rule for each
part-of-speech. YH splits words into their left and
right components. LH encodes the stopping deci-
sion given that we have not generated any arguments
so far. L′H encodes the same decision after generat-
ing one or more arguments. L1H represents the distri-
bution over left attachments. To extract dependency
relations from these parse trees, we scan for attach-
ment rules (e.g., L1H —* YA L′H) and record that
A depends on H. The schema omits the rules for
right arguments since they are symmetric. We show
a parse of “The big dog barks” in Figure 3.2
Much of the extensions to this work have fo-
cused on estimation procedures. Klein and Manning
(2004) use Expectation Maximization to estimate
the model parameters. Smith and Eisner (2005) and
Smith (2006) investigate using Contrastive Estima-
tion to estimate DMV. Contrastive Estimation max-
imizes the conditional probability of the observed
sentences given a neighborhood of similar unseen
sequences. The results of this approach vary widely
based on regularization and neighborhood, but often
outperforms EM.
</bodyText>
<footnote confidence="0.97105">
2Note that our examples use words as leaf nodes but in our
unlexicalized models, the leaf nodes are in fact parts-of-speech.
</footnote>
<figureCaption confidence="0.996126">
Figure 3: DMV split-head bilexical CFG parse of “The
big dog barks.”
</figureCaption>
<bodyText confidence="0.999069884615384">
Smith (2006) also investigates two techniques for
maximizing likelihood while incorporating the lo-
cality bias encoded in the harmonic initializer for
DMV. One technique, skewed deterministic anneal-
ing, ameliorates the local maximum problem by flat-
tening the likelihood and adding a bias towards the
Klein and Manning initializer, which is decreased
during learning. The second technique is structural
annealing (Smith and Eisner, 2006; Smith, 2006)
which penalizes long dependencies initially, grad-
ually weakening the penalty during estimation. If
hand-annotated dependencies on a held-out set are
available for parameter selection, this performs far
better than EM; however, performing parameter se-
lection on a held-out set without the use of gold de-
pendencies does not perform as well.
Cohen et al. (2008) investigate using Bayesian
Priors with DMV. The two priors they use are the
Dirichlet (which we use here) and the Logistic Nor-
mal prior, which allows the model to capture correla-
tions between different distributions. They initialize
using the harmonic initializer of Klein and Manning
(2004). They find that the Logistic Normal distri-
bution performs much better than the Dirichlet with
this initialization scheme.
Cohen and Smith (2009), investigate (concur-
</bodyText>
<page confidence="0.981465">
104
</page>
<table confidence="0.9969705">
Rule Description
S → YH Select H as root
YH → LH RH Move to split-head representation
LH → HL STOP  |dir = L, head = H, val = 0
LH → L′ CONT  |dir = L, head = H, val = 0
H
L′H → L1 STOP  |dir = L, head = H, val = 1
H
L′H → L2 CONT  |dir = L, head = H, val = 1
H
L2H → YA L′H Arg A  |dir = L, head = H, val = 1
L1H → YA HL Arg A  |dir = L, head = H, val = 0
</table>
<figureCaption confidence="0.95594425">
Figure 4: Extended Valence Grammar schema. As be-
fore, we omit rules involving the right parts of words. In
this case, val E 10, 11 indicates whether we are generat-
ing the nearest argument (0) or not (1).
</figureCaption>
<bodyText confidence="0.999953666666667">
rently with our work) an extension of this, the
Shared Logistic Normal prior, which allows differ-
ent PCFG rule distributions to share components.
They use this machinery to investigate smoothing
the attachment distributions for (nouns/verbs), and
for learning using multiple languages.
</bodyText>
<sectionHeader confidence="0.999671" genericHeader="method">
3 Enriched Contexts
</sectionHeader>
<bodyText confidence="0.9990355">
DMV models the distribution over arguments iden-
tically without regard to their order. Instead, we
propose to distinguish the distribution over the argu-
ment nearest the head from the distribution of sub-
sequent arguments. 3
Consider the following changes to the DMV
grammar (results shown in Figure 4). First, we will
introduce the rule L2H —* YA L′H to denote the deci-
sion of what argument to generate for positions not
nearest to the head. Next, instead of having L′H ex-
pand to HL or L1H, we will expand it to L1H (attach
to nearest argument and stop) or L2H (attach to non-
nearest argument and continue). We call this the Ex-
tended Valence Grammar (EVG).
As a concrete example, consider the phrase “the
big hungry dog” (Figure 5). We would expect that
distribution over the nearest left argument for “dog”
to be different than farther left arguments. The fig-
</bodyText>
<footnote confidence="0.9187515">
3McClosky (2008) explores this idea further in an un-
smoothed grammar.
</footnote>
<figure confidence="0.387326666666667">
.. ..
. .
Ldog Ldog
L1 L′
dog dog
bigL bigR
</figure>
<figureCaption confidence="0.9984966">
Figure 5: An example of moving from DMV to EVG
for a fragment of “The big dog.” Boxed nodes indicate
changes. The key difference is that EVG distinguishes
between the distributions over the argument nearest the
head (big) from arguments farther away (The).
</figureCaption>
<bodyText confidence="0.9998385">
ure shows that EVG allows these two distributions to
be different (nonterminals L2dog and L1dog) whereas
DMV forces them to be equivalent (both use L1dog as
the nonterminal).
</bodyText>
<subsectionHeader confidence="0.996486">
3.1 Lexicalization
</subsectionHeader>
<bodyText confidence="0.999305916666667">
All of the probabilistic models discussed thus far
have incorporated only part-of-speech information
(see Footnote 2). In supervised parsing of both de-
pendencies and constituency, lexical information is
critical (Collins, 1999). We incorporate lexical in-
formation into EVG (henceforth L-EVG) by extend-
ing the distributions over argument parts-of-speech
A to condition on the head word h in addition to the
head part-of-speech H, direction d and argument po-
sition v. The argument word a distribution is merely
conditioned on part-of-speech A; we leave refining
this model to future work.
In order to incorporate lexicalization, we extend
the EVG CFG to allow the nonterminals to be anno-
tated with both the word and part-of-speech of the
head. We first remove the old rules YH —* LH RH
for each H E VT. Then we mark each nonter-
minal which is annotated with a part-of-speech as
also annotated with its head, with a single excep-
tion: YH. We add a new nonterminal YH,h for each
H E VT, h E Vw, and the rules YH —* YH,h and
YH,h —* LH,h RH,h. The rule YH —* YH,h cor-
responds to selecting the word, given its part-of-
speech.
</bodyText>
<figure confidence="0.999296">
L′
dog
YThe
TheL TheR
L′
dog
L1
dog
dogL
Yb%g
dogL
2
Ldog
TheL TheR
Yb%g
YThe
L′
dog
L1
dog
bigL bigR
</figure>
<page confidence="0.994994">
105
</page>
<sectionHeader confidence="0.996871" genericHeader="method">
4 Smoothing
</sectionHeader>
<bodyText confidence="0.999797047619048">
In supervised estimation one common smoothing
technique is linear interpolation, (Jelinek, 1997).
This section explains how linear interpolation can
be represented using a PCFG with tied rule proba-
bilities, and how one might estimate smoothing pa-
rameters in an unsupervised framework.
In many probabilistic models it is common to esti-
mate the distribution of some event x conditioned on
some set of context information P(x|N(1) ... N(k))
by smoothing it with less complicated condi-
tional distributions. Using linear interpolation
we model P(x|N(1) ... N(k)) as a weighted aver-
age of two distributions λ1P1(x|N(1), ... , N(k)) +
λ2P2(x|N(1), ... , N(k−1)), where the distribution
P2 makes an independence assumption by dropping
the conditioning event N(k).
In a PCFG a nonterminal N can encode a collec-
tion of conditioning events N(1) ... N(k), and BN de-
termines a distribution conditioned on N(1) ... N(k)
over events represented by the rules r ∈ RN. For
example, in EVG the nonterminal L1 encodes
</bodyText>
<equation confidence="0.370196">
NN
</equation>
<bodyText confidence="0.97585725">
three separate pieces of conditioning information:
the direction d = left, the head part-of-speech
H = NN, and the argument position v = 0;
BL1→YJJ NNL represents the probability of gener-
</bodyText>
<equation confidence="0.36893">
NN
</equation>
<bodyText confidence="0.9867028">
ating JJ as the first left argument of NN. Sup-
pose in EVG we are interested in smoothing P(A |
d, H, v) with a component that excludes the head
conditioning event. Using linear interpolation, this
would be:
</bodyText>
<equation confidence="0.661166">
P(A  |d, H, v) = λ1P1(A  |d, H, v)+λ2P2(A  |d, v)
</equation>
<bodyText confidence="0.992569672131148">
We will estimate PCFG rules with linearly interpo-
lated probabilities by creating a tied PCFG which
is extended by adding rules that select between the
main distribution P1 and the backoff distribution P2,
and also rules that correspond to draws from those
distributions. We will make use of tied rule proba-
bilities to make the independence assumption in the
backoff distribution.
We still use the original grammar to parse the sen-
tence. However, we estimate the parameters in the
extended grammar and then translate them back into
the original grammar for parsing.
More formally, suppose B ⊆ N is a set of non-
terminals (called the backoff set) with conditioning
events N(1) ... N(k−1) in common (differing in a
conditioning event N(k)), and with rule sets of the
same cardinality. If G is our model’s PCFG, we can
define a new tied PCFG G′ = (N′, T , S, R′, O),
where N′ = N ∪ {Nbℓ  |N ∈ B, E ∈ {1, 2}},
meaning for each nonterminal N in the backoff
set we add two nonterminals Nb1, Nb2 represent-
ing each distribution P1 and P2. The new rule
set R′ = (∪N∈N′R′N) where for all N ∈ B
rule set R′N = IN → Nbℓ  |E ∈ {1, 2}}, mean-
ing at N in G′ we decide which distribution P1, P2
to use; and for N ∈ B and E ∈ {1, 2} ,
R′Nbℓ = {Nbℓ → Q  |N → Q ∈ RN} indicating a
draw from distribution Pℓ. For nonterminals N ∈6 B,
R′N = RN. Finally, for each N, M ∈ B we
specify a tying relation between the rules inR′Nb2
and R′Mb2 , grouping together analogous rules. This
has the effect of making an independence assump-
tion about P2, namely that it ignores the condition-
ing event N(k), drawing from a common distribution
each time a nonterminal Nb2 is rewritten.
For example, in EVG to smooth P(A = DT |
d = left, H = NN, v = 0) with P2(A = DT |
d = left, v = 0) we define the backoff set to
be {LH  |H ∈ Vτ}. In the extended grammar we
define the tying relation to form rule equivalence
classes by the argument they generate, i.e. for each
argument A ∈ Vτ, we have a rule equivalence class
{LFb2 → YA HL  |H ∈ Vτ}.
We can see that in grammar G′ each N ∈ B even-
tually ends up rewriting to one of N’s expansions Q
in G. There are two indirect paths, one through Nb1
and one through Nb2. Thus this defines the proba-
bility of N → Q in G, BN→β, as the probability of
rewriting N as Q in G′ via Nb1 and Nb2. That is:
BN→β = ON→Nb1 ONb1→β + ON→Nb2 ONb2→β
The example in Figure 6 shows the probability that
L1 rewrites to Ybig dogL in grammar G.
dog
Typically when smoothing we need to incorporate
the prior knowledge that conditioning events that
have been seen fewer times should be more strongly
smoothed. We accomplish this by setting the Dirich-
let hyperparameters for each N → Nb1, N → Nb2
decision to (K, 2K), where K = |RNb1  |is the num-
ber of rewrite rules for A. This ensures that the
model will only start to ignore the backoff distribu-
</bodyText>
<page confidence="0.997382">
106
</page>
<figureCaption confidence="0.981880333333333">
Figure 6: Using linear interpolation to smooth L&apos;dog →
Ybig dogL: The first component represents the distri-
bution fully conditioned on head dog, while the second
component represents the distribution ignoring the head
conditioning event. This later is accomplished by tying
the rule L&apos;b2
</figureCaption>
<figure confidence="0.8420275">
dog → Ybig dogL to, for instance, L&apos;b2cat
→
Ybig catL, L&apos;b2
fish → Ybig fishL etc.
</figure>
<figureCaption confidence="0.3301785">
tion after having seen a sufficiently large number of
training examples. 4
</figureCaption>
<subsectionHeader confidence="0.995398">
4.1 Smoothed Dependency Models
</subsectionHeader>
<bodyText confidence="0.999984652173913">
Our first experiments examine smoothing the dis-
tributions over an argument in the DMV and EVG
models. In DMV we smooth the probability of argu-
ment A given head part-of-speech H and direction d
with a distribution that ignores H. In EVG, which
conditions on H, d and argument position v we back
off two ways. The first is to ignore v and use back-
off conditioning event H, d. This yields a backoff
distribution with the same conditioning information
as the argument distribution from DMV. We call this
EVG smoothed-skip-val.
The second possibility is to have the backoff
distribution ignore the head part-of-speech H and
use backoff conditioning event v, d. This assumes
that arguments share a common distribution across
heads. We call this EVG smoothed-skip-head. As
we see below, backing off by ignoring the part-of-
speech of the head H worked better than ignoring
the argument position v.
For L-EVG we smooth the argument part-of-
speech distribution (conditioned on the head word)
with the unlexicalized EVG smoothed-skip-head
model.
</bodyText>
<sectionHeader confidence="0.975799" genericHeader="method">
5 Initialization and Search issues
</sectionHeader>
<bodyText confidence="0.999557833333333">
Klein and Manning (2004) strongly emphasize the
importance of smart initialization in getting good
performance from DMV. The likelihood function is
full of local maxima and different initial parameter
values yield vastly different quality solutions. They
offer what they call a “harmonic initializer” which
</bodyText>
<footnote confidence="0.470116">
4We set the other Dirichlet hyperparameters to 1.
</footnote>
<bodyText confidence="0.999934607142857">
initializes the attachment probabilities to favor ar-
guments that appear more closely in the data. This
starts EM in a state preferring shorter attachments.
Since our goal is to expand the model to incor-
porate lexical information, we want an initializa-
tion scheme which does not depend on the details
of DMV. The method we use is to create M sets of
B random initial settings and to run VB some small
number of iterations (40 in all our experiments) for
each initial setting. For each of the M sets, the
model with the best free energy of the B runs is
then run out until convergence (as measured by like-
lihood of a held-out data set); the other models are
pruned away. In this paper we use B = 20 and
M = 50.
For the bth setting, we draw a random sample
from the prior ¯θ(b). We set the initial Q(t) =
P(t|s,¯θ(b)) which can be calculated using the
Expectation-Maximization E-Step. Q(¯θ) is then ini-
tialized using the standard VB M-step.
For the Lexicalized-EVG, we modify this proce-
dure slightly, by first running MB smoothed EVG
models for 40 iterations each and selecting the best
model in each cohort as before; each L-EVG dis-
tribution is initialized from its corresponding EVG
distribution. The new P(A|h, H, d, v) distributions
are set initially to their corresponding P(A|H, d, v)
values.
</bodyText>
<sectionHeader confidence="0.999985" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.994309823529412">
We trained on the standard Penn Treebank WSJ cor-
pus (Marcus et al., 1993). Following Klein and Man-
ning (2002), sentences longer than 10 words after
removing punctuation are ignored. We refer to this
variant as WSJ10. Following Cohen et al. (2008),
we train on sections 2-21, used 22 as a held-out de-
velopment corpus, and present results evaluated on
section 23. The models were all trained using Varia-
tional Bayes, and initialized as described in Section
5. To evaluate, we follow Cohen et al. (2008) in us-
ing the mean of the variational posterior Dirichlets
as a point estimate ¯θ′. For the unsmoothed models
we decode by selecting the Viterbi parse given ¯θ′, or
argmaxtP (t|s, ¯θ′).
For the smoothed models we find the Viterbi parse
of the unsmoothed CFG, but use the smoothed prob-
abilities. We evaluate against the gold standard
</bodyText>
<figure confidence="0.997368535714286">
L1
dog
Ybig dogL
Ybig dogL
1
Ldog
L1b2
dog
Ybig dogL
1
ACCCCCCC
0
B B @
PG
0
B B B B B B B @
= PG′
1
ACC
1
ACCCCCCC
+ PG′
0
B B B B B B B @
1
Ldog
L1b1
dog
</figure>
<page confidence="0.98018">
107
</page>
<table confidence="0.9998545">
Model Variant Dir. Acc.
DMV harmonic init 46.9*
DMV random init 55.7 (8.0)
DMV log normal-families 59.4*
DMV shared log normal-families 62.4†
DMV smoothed 61.2 (1.2)
EVG random init 53.3 (7.1)
EVG smoothed-skip-val 62.1 (1.9)
EVG smoothed-skip-head 65.0 (5.7)
L-EVG smoothed 68.8 (4.5)
</table>
<tableCaption confidence="0.9931745">
Table 1: Directed accuracy (DA) for WSJ10, section 23.
*,† indicate results reported by Cohen et al. (2008), Co-
hen and Smith (2009) respectively. Standard deviations
over 10 runs are given in parentheses
</tableCaption>
<bodyText confidence="0.999714148148148">
dependencies for section 23, which were extracted
from the phrase structure trees using the standard
rules by Yamada and Matsumoto (2003). We mea-
sure the percent accuracy of the directed dependency
edges. For the lexicalized model, we replaced all
words that were seen fewer than 100 times with
“UNK.” We ran each of our systems 10 times, and
report the average directed accuracy achieved. The
results are shown in Table 1. We compare to work
by Cohen et al. (2008) and Cohen and Smith (2009).
Looking at Table 1, we can first of all see the
benefit of randomized initialization over the har-
monic initializer for DMV. We can also see a large
gain by adding smoothing to DMV, topping even
the logistic normal prior. The unsmoothed EVG ac-
tually performs worse than unsmoothed DMV, but
both smoothed versions improve even on smoothed
DMV. Adding lexical information (L-EVG) yields a
moderate further improvement.
As the greatest improvement comes from moving
to model EVG smoothed-skip-head, we show in Ta-
ble 2 the most probable arguments for each val, dir,
using the mean of the appropriate variational Dirich-
let. For d = right, v = 1, P(A|v, d) largely seems
to acts as a way of grouping together various verb
types, while for d = left, v = 0 the model finds
that nouns tend to act as the closest left argument.
</bodyText>
<table confidence="0.99951">
Dir,Val Arg Prob Dir,Val Arg Prob
left, 0 NN 0.65 right, 0 NN 0.26
NNP 0.18 RB 0.23
DT 0.12 NNS 0.12
IN 0.11
left, 1 CC 0.35 right, 1 IN 0.78
RB 0.27
IN 0.18
</table>
<tableCaption confidence="0.993911">
Table 2: Most likely arguments given valence and direc-
</tableCaption>
<bodyText confidence="0.870367666666667">
tion, according to smoothing distribution P(arg|dir, val)
in EVG smoothed-skip-head model with lowest free en-
ergy.
</bodyText>
<sectionHeader confidence="0.999595" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999995083333333">
We present a smoothing technique for unsupervised
PCFG estimation which allows us to explore more
sophisticated dependency grammars. Our method
combines linear interpolation with a Bayesian prior
that ensures the backoff distribution receives proba-
bility mass. Estimating the smoothed model requires
running the standard Variational Bayes on an ex-
tended PCFG. We used this technique to estimate a
series of dependency grammars which extend DMV
with additional valence and lexical information. We
found that both were helpful in learning English de-
pendency grammars. Our L-EVG model gives the
best reported accuracy to date on the WSJ10 corpus.
Future work includes using lexical information
more deeply in the model by conditioning argument
words and valence on the lexical head. We suspect
that successfully doing so will require using much
larger datasets. We would also like to explore us-
ing our smoothing technique in other models such
as HMMs. For instance, we could do unsupervised
HMM part-of-speech induction by smooth a tritag
model with a bitag model. Finally, we would like to
learn the parts-of-speech in our dependency model
from text and not rely on the gold-standard tags.
</bodyText>
<sectionHeader confidence="0.999286" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.4732418">
This research is based upon work supported by
National Science Foundation grants 0544127 and
0631667 and DARPA GALE contract HR0011-06-
2-0001. We thank members of BLLIP for their feed-
back.
</reference>
<page confidence="0.999041">
108
</page>
<sectionHeader confidence="0.998335" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99971462295082">
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
NAACL-HLT 2009.
Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.
2008. Logistic normal priors for unsupervised prob-
abilistic grammar induction. In Advances in Neural
Information Processing Systems 21.
Michael Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, The Uni-
versity of Pennsylvania.
Jason Eisner and John Blatz. 2007. Program transforma-
tions for optimization of parsing algorithms and other
weighted logic programs. In Proceedings of the 11th
Conference on Formal Grammar.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head-
automaton grammars. In Proceedings ofACL 1999.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge, Mas-
sachusetts.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Proceedings ofNAACL 2007.
Mark Johnson. 2007. Transforming projective bilexical
dependency grammars into efficiently-parsable CFGs
with unfold-fold. In Proceedings ofACL 2007.
Dan Klein and Christopher Manning. 2002. A genera-
tive constituent-context model for improved grammar
induction. In Proceedings ofACL 2002.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL
2004, July.
Kenichi Kurihara and Taisuke Sato. 2004. An applica-
tion of the variational bayesian approach to probabilis-
tics context-free grammars. In IJCNLP 2004 Work-
shop Beyond Shallow Analyses.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
David McClosky. 2008. Modeling valence effects in un-
supervised grammar induction. Technical Report CS-
09-01, Brown University, Providence, RI, USA.
Noah A. Smith and Jason Eisner. 2005. Guiding unsu-
pervised grammar induction using contrastive estima-
tion. In International Joint Conference on Artificial
Intelligence Workshop on Grammatical Inference Ap-
plications.
Noah A. Smith and Jason Eisner. 2006. Annealing struc-
tural bias in multilingual weighted grammar induction.
In Proceedings of COLING-ACL 2006.
Noah A. Smith. 2006. Novel Estimation Methods for
Unsupervised Discovery ofLatent Structure in Natural
Language Text. Ph.D. thesis, Department of Computer
Science, Johns Hopkins University.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
In Proceedings of the International Workshop on Pars-
ing Technologies.
</reference>
<page confidence="0.998941">
109
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.661572">
<title confidence="0.9991325">Improving Unsupervised Dependency with Richer Contexts and Smoothing</title>
<author confidence="0.999282">William P Headden Mark Johnson</author>
<author confidence="0.999282">David</author>
<affiliation confidence="0.94506">Brown Laboratory for Linguistic Information Processing Brown</affiliation>
<address confidence="0.93797">Providence, RI</address>
<abstract confidence="0.982655466666667">Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts. Traditionally, the unsupervised models have been kept simple due to tractability and data sparsity concerns. In this paper, we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing. Our model produces state-of-theart results on the task of unsupervised grammar induction, improving over the best previous work by almost 10 percentage points.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This research is based upon work supported by National Science Foundation grants 0544127 and 0631667 and DARPA GALE contract HR0011-06-2-0001. We thank members of BLLIP for their feedback.</title>
<marker></marker>
<rawString>This research is based upon work supported by National Science Foundation grants 0544127 and 0631667 and DARPA GALE contract HR0011-06-2-0001. We thank members of BLLIP for their feedback.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<contexts>
<context position="15348" citStr="Cohen and Smith (2009)" startWordPosition="2630" endWordPosition="2633">selection, this performs far better than EM; however, performing parameter selection on a held-out set without the use of gold dependencies does not perform as well. Cohen et al. (2008) investigate using Bayesian Priors with DMV. The two priors they use are the Dirichlet (which we use here) and the Logistic Normal prior, which allows the model to capture correlations between different distributions. They initialize using the harmonic initializer of Klein and Manning (2004). They find that the Logistic Normal distribution performs much better than the Dirichlet with this initialization scheme. Cohen and Smith (2009), investigate (concur104 Rule Description S → YH Select H as root YH → LH RH Move to split-head representation LH → HL STOP |dir = L, head = H, val = 0 LH → L′ CONT |dir = L, head = H, val = 0 H L′H → L1 STOP |dir = L, head = H, val = 1 H L′H → L2 CONT |dir = L, head = H, val = 1 H L2H → YA L′H Arg A |dir = L, head = H, val = 1 L1H → YA HL Arg A |dir = L, head = H, val = 0 Figure 4: Extended Valence Grammar schema. As before, we omit rules involving the right parts of words. In this case, val E 10, 11 indicates whether we are generating the nearest argument (0) or not (1). rently with our work</context>
<context position="27773" citStr="Cohen and Smith (2009)" startWordPosition="4879" endWordPosition="4883">ities. We evaluate against the gold standard L1 dog Ybig dogL Ybig dogL 1 Ldog L1b2 dog Ybig dogL 1 ACCCCCCC 0 B B @ PG 0 B B B B B B B @ = PG′ 1 ACC 1 ACCCCCCC + PG′ 0 B B B B B B B @ 1 Ldog L1b1 dog 107 Model Variant Dir. Acc. DMV harmonic init 46.9* DMV random init 55.7 (8.0) DMV log normal-families 59.4* DMV shared log normal-families 62.4† DMV smoothed 61.2 (1.2) EVG random init 53.3 (7.1) EVG smoothed-skip-val 62.1 (1.9) EVG smoothed-skip-head 65.0 (5.7) L-EVG smoothed 68.8 (4.5) Table 1: Directed accuracy (DA) for WSJ10, section 23. *,† indicate results reported by Cohen et al. (2008), Cohen and Smith (2009) respectively. Standard deviations over 10 runs are given in parentheses dependencies for section 23, which were extracted from the phrase structure trees using the standard rules by Yamada and Matsumoto (2003). We measure the percent accuracy of the directed dependency edges. For the lexicalized model, we replaced all words that were seen fewer than 100 times with “UNK.” We ran each of our systems 10 times, and report the average directed accuracy achieved. The results are shown in Table 1. We compare to work by Cohen et al. (2008) and Cohen and Smith (2009). Looking at Table 1, we can first </context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>Shay B. Cohen and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proceedings of NAACL-HLT 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Logistic normal priors for unsupervised probabilistic grammar induction.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 21.</booktitle>
<contexts>
<context position="1769" citStr="Cohen et al., 2008" startWordPosition="264" endWordPosition="268">anually annotating corpora with parse trees is expensive and time consuming so for languages and domains with minimal resources it is valuable to study methods for parsing without requiring annotated sentences. In this work, we focus on unsupervised dependency parsing. Our goal is to produce a directed graph of dependency relations (e.g. Figure 1) where each edge indicates a head-argument relation. Since the task is unsupervised, we are not given any examples of correct dependency graphs and only take words and their parts of speech as input. Most of the recent work in this area (Smith, 2006; Cohen et al., 2008) has focused on variants of the The big dog barks Figure 1: Example dependency parse. Dependency Model with Valence (DMV) by Klein and Manning (2004). DMV was the first unsupervised dependency grammar induction system to achieve accuracy above a right-branching baseline. However, DMV is not able to capture some of the more complex aspects of language. Borrowing some ideas from the supervised parsing literature, we present two new models: Extended Valence Grammar (EVG) and its lexicalized extension (L-EVG). The primary difference between EVG and DMV is that DMV uses valence information to deter</context>
<context position="14911" citStr="Cohen et al. (2008)" startWordPosition="2562" endWordPosition="2565">ic annealing, ameliorates the local maximum problem by flattening the likelihood and adding a bias towards the Klein and Manning initializer, which is decreased during learning. The second technique is structural annealing (Smith and Eisner, 2006; Smith, 2006) which penalizes long dependencies initially, gradually weakening the penalty during estimation. If hand-annotated dependencies on a held-out set are available for parameter selection, this performs far better than EM; however, performing parameter selection on a held-out set without the use of gold dependencies does not perform as well. Cohen et al. (2008) investigate using Bayesian Priors with DMV. The two priors they use are the Dirichlet (which we use here) and the Logistic Normal prior, which allows the model to capture correlations between different distributions. They initialize using the harmonic initializer of Klein and Manning (2004). They find that the Logistic Normal distribution performs much better than the Dirichlet with this initialization scheme. Cohen and Smith (2009), investigate (concur104 Rule Description S → YH Select H as root YH → LH RH Move to split-head representation LH → HL STOP |dir = L, head = H, val = 0 LH → L′ CON</context>
<context position="26609" citStr="Cohen et al. (2008)" startWordPosition="4663" endWordPosition="4666">tep. For the Lexicalized-EVG, we modify this procedure slightly, by first running MB smoothed EVG models for 40 iterations each and selecting the best model in each cohort as before; each L-EVG distribution is initialized from its corresponding EVG distribution. The new P(A|h, H, d, v) distributions are set initially to their corresponding P(A|H, d, v) values. 6 Results We trained on the standard Penn Treebank WSJ corpus (Marcus et al., 1993). Following Klein and Manning (2002), sentences longer than 10 words after removing punctuation are ignored. We refer to this variant as WSJ10. Following Cohen et al. (2008), we train on sections 2-21, used 22 as a held-out development corpus, and present results evaluated on section 23. The models were all trained using Variational Bayes, and initialized as described in Section 5. To evaluate, we follow Cohen et al. (2008) in using the mean of the variational posterior Dirichlets as a point estimate ¯θ′. For the unsmoothed models we decode by selecting the Viterbi parse given ¯θ′, or argmaxtP (t|s, ¯θ′). For the smoothed models we find the Viterbi parse of the unsmoothed CFG, but use the smoothed probabilities. We evaluate against the gold standard L1 dog Ybig d</context>
<context position="28311" citStr="Cohen et al. (2008)" startWordPosition="4970" endWordPosition="4973"> 23. *,† indicate results reported by Cohen et al. (2008), Cohen and Smith (2009) respectively. Standard deviations over 10 runs are given in parentheses dependencies for section 23, which were extracted from the phrase structure trees using the standard rules by Yamada and Matsumoto (2003). We measure the percent accuracy of the directed dependency edges. For the lexicalized model, we replaced all words that were seen fewer than 100 times with “UNK.” We ran each of our systems 10 times, and report the average directed accuracy achieved. The results are shown in Table 1. We compare to work by Cohen et al. (2008) and Cohen and Smith (2009). Looking at Table 1, we can first of all see the benefit of randomized initialization over the harmonic initializer for DMV. We can also see a large gain by adding smoothing to DMV, topping even the logistic normal prior. The unsmoothed EVG actually performs worse than unsmoothed DMV, but both smoothed versions improve even on smoothed DMV. Adding lexical information (L-EVG) yields a moderate further improvement. As the greatest improvement comes from moving to model EVG smoothed-skip-head, we show in Table 2 the most probable arguments for each val, dir, using the </context>
</contexts>
<marker>Cohen, Gimpel, Smith, 2008</marker>
<rawString>Shay B. Cohen, Kevin Gimpel, and Noah A. Smith. 2008. Logistic normal priors for unsupervised probabilistic grammar induction. In Advances in Neural Information Processing Systems 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>The University of Pennsylvania.</institution>
<contexts>
<context position="17883" citStr="Collins, 1999" startWordPosition="3096" endWordPosition="3097"> “The big dog.” Boxed nodes indicate changes. The key difference is that EVG distinguishes between the distributions over the argument nearest the head (big) from arguments farther away (The). ure shows that EVG allows these two distributions to be different (nonterminals L2dog and L1dog) whereas DMV forces them to be equivalent (both use L1dog as the nonterminal). 3.1 Lexicalization All of the probabilistic models discussed thus far have incorporated only part-of-speech information (see Footnote 2). In supervised parsing of both dependencies and constituency, lexical information is critical (Collins, 1999). We incorporate lexical information into EVG (henceforth L-EVG) by extending the distributions over argument parts-of-speech A to condition on the head word h in addition to the head part-of-speech H, direction d and argument position v. The argument word a distribution is merely conditioned on part-of-speech A; we leave refining this model to future work. In order to incorporate lexicalization, we extend the EVG CFG to allow the nonterminals to be annotated with both the word and part-of-speech of the head. We first remove the old rules YH —* LH RH for each H E VT. Then we mark each nontermi</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, The University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>John Blatz</author>
</authors>
<title>Program transformations for optimization of parsing algorithms and other weighted logic programs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th Conference on Formal Grammar.</booktitle>
<contexts>
<context position="10114" citStr="Eisner and Blatz, 2007" startWordPosition="1720" endWordPosition="1724">er parameters and Q(t), the estimate of the posterior distribution over trees. Finding a local maximum of F is done via an alternating maximization of Q(¯θ) and Q(t). Kurihara and Sato (2004) show that each θ ¯N) is a Dirichlet distribution with parameters ˆαr = αr + EQ(t)f(t, r). 2.2 Split-head Bilexical CFGs In the sections that follow, we frame various dependency models as a particular variety of CFGs known as split-head bilexical CFGs (Eisner and Satta, 1999). These allow us to use the fast Eisner and Satta (1999) parsing algorithm to compute the expectations required by VB in O(m3) time (Eisner and Blatz, 2007; Johnson, 2007) where m is the length of the sentence.1 In the split-head bilexical CFG framework, each nonterminal in the grammar is annotated with a terminal symbol. For dependency grammars, these annotations correspond to words and/or parts-ofspeech. Additionally, split-head bilexical CFGs require that each word sij in sentence si is represented in a split form by two terminals called its left part sijL and right part sijR. The set of these parts constitutes the terminal symbols of the grammar. This split-head property relates to a particular type of dependency grammar in which the left an</context>
<context position="11795" citStr="Eisner and Blatz, 2007" startWordPosition="1997" endWordPosition="2000">ments of head H in direction d are generated by repeatedly deciding whether to generate another new argument or to stop and then generating the argument if required. The probability of deciding whether to generate another argument is conditioned on H, d and whether this would be the first argument (this is the sense in which it models valence). When DMV generates an argument, the part-of-speech of that argument A is generated given H and d. 1Efficiently parsable versions of split-head bilexical CFGs for the models described in this paper can be derived using the fold-unfold grammar transform (Eisner and Blatz, 2007; Johnson, 2007). P( θ ¯N|α¯N) = Y∝ ¯r∈ R¯ X log P(s|α) ≥ J Q(t, θ) log P(s, t, t B Q(t, t θ|α) θ) = F Q( Q( 103 Rule Description S → YH Select H as root YH → LH RH Move to split-head representation LH → HL STOP |dir = L, head = H, val = 0 LH → L1 CONT |dir = L, head = H, val = 0 H L′ H → HL STOP |dir = L, head = H, val = 1 L′H → L1 CONT |dir = L, head = H, val = 1 H Ll H → YA L′H Arg A |dir = L,head = H Figure 2: Rule schema for DMV. For brevity, we omit the portion of the grammar that handles the right arguments since they are symmetric to the left (all rules are the same except for the atta</context>
</contexts>
<marker>Eisner, Blatz, 2007</marker>
<rawString>Jason Eisner and John Blatz. 2007. Program transformations for optimization of parsing algorithms and other weighted logic programs. In Proceedings of the 11th Conference on Formal Grammar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and headautomaton grammars.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="9959" citStr="Eisner and Satta, 1999" startWordPosition="1693" endWordPosition="1696">timating it simpler. It factors Q(t, θ) = Q(t)Q( ¯θ) = Qn i=1 Qi(ti)Q¯N∈N¯Q( θ ¯N). The goal is to recover θ), the estimate of the posterior distribution over parameters and Q(t), the estimate of the posterior distribution over trees. Finding a local maximum of F is done via an alternating maximization of Q(¯θ) and Q(t). Kurihara and Sato (2004) show that each θ ¯N) is a Dirichlet distribution with parameters ˆαr = αr + EQ(t)f(t, r). 2.2 Split-head Bilexical CFGs In the sections that follow, we frame various dependency models as a particular variety of CFGs known as split-head bilexical CFGs (Eisner and Satta, 1999). These allow us to use the fast Eisner and Satta (1999) parsing algorithm to compute the expectations required by VB in O(m3) time (Eisner and Blatz, 2007; Johnson, 2007) where m is the length of the sentence.1 In the split-head bilexical CFG framework, each nonterminal in the grammar is annotated with a terminal symbol. For dependency grammars, these annotations correspond to words and/or parts-ofspeech. Additionally, split-head bilexical CFGs require that each word sij in sentence si is represented in a split form by two terminals called its left part sijL and right part sijR. The set of th</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Jason Eisner and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and headautomaton grammars. In Proceedings ofACL 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1997</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="18987" citStr="Jelinek, 1997" startWordPosition="3299" endWordPosition="3300">t-of-speech of the head. We first remove the old rules YH —* LH RH for each H E VT. Then we mark each nonterminal which is annotated with a part-of-speech as also annotated with its head, with a single exception: YH. We add a new nonterminal YH,h for each H E VT, h E Vw, and the rules YH —* YH,h and YH,h —* LH,h RH,h. The rule YH —* YH,h corresponds to selecting the word, given its part-ofspeech. L′ dog YThe TheL TheR L′ dog L1 dog dogL Yb%g dogL 2 Ldog TheL TheR Yb%g YThe L′ dog L1 dog bigL bigR 105 4 Smoothing In supervised estimation one common smoothing technique is linear interpolation, (Jelinek, 1997). This section explains how linear interpolation can be represented using a PCFG with tied rule probabilities, and how one might estimate smoothing parameters in an unsupervised framework. In many probabilistic models it is common to estimate the distribution of some event x conditioned on some set of context information P(x|N(1) ... N(k)) by smoothing it with less complicated conditional distributions. Using linear interpolation we model P(x|N(1) ... N(k)) as a weighted average of two distributions λ1P1(x|N(1), ... , N(k)) + λ2P2(x|N(1), ... , N(k−1)), where the distribution P2 makes an indep</context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>Frederick Jelinek. 1997. Statistical Methods for Speech Recognition. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Proceedings ofNAACL</booktitle>
<contexts>
<context position="8157" citStr="Johnson et al., 2007" startWordPosition="1379" endWordPosition="1383">ion methods (e.g. Expectation Maximization, Variational Bayes) extend directly to tied PCFGs. Maximum likelihood estimation provides a point estimate of ¯θ. However, often we want to incorporate information about θ¯ by modeling its prior distribution. As a prior, for each N¯ E JV¯ we will specify a P(s,tJθ) = 11 H ¯rE R¯ rE¯r ¯θf(t,¯r) r¯ 102 Dirichlet distribution over θN¯ with hyperparameters α ¯N. The Dirichlet has the density function: 7r(P¯r∈ R¯N¯ α¯r) ¯θα¯r−1r¯, 11T∈ R¯N¯ Γ(α¯r) ¯r∈ R¯N¯ Thus the prior over θ is a product of Dirichlets,which is conjugate to the PCFG likelihood function (Johnson et al., 2007). That is, the posterior P(¯θ|s, t, α) is also a product of Dirichlets, also factoring into a Dirichlet for each nonterminal ¯N, where the parameters α¯r are augmented by the number of times rule r¯ is observed in tree t: P( θ|s,t,α) ∝ P(s,t |θ)P( θ|α) ¯θf(t,¯r)+α¯r−1 r¯ We can see that α¯r acts as a pseudocount of the number of times r¯ is observed prior to t. To make use of this prior, we use the Variational Bayes (VB) technique for PCFGs with Dirichlet Priors presented by Kurihara and Sato (2004). VB estimates a distribution over ¯θ. In contrast, Expectation Maximization estimates merely a </context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proceedings ofNAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Transforming projective bilexical dependency grammars into efficiently-parsable CFGs with unfold-fold.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="10130" citStr="Johnson, 2007" startWordPosition="1725" endWordPosition="1726">the estimate of the posterior distribution over trees. Finding a local maximum of F is done via an alternating maximization of Q(¯θ) and Q(t). Kurihara and Sato (2004) show that each θ ¯N) is a Dirichlet distribution with parameters ˆαr = αr + EQ(t)f(t, r). 2.2 Split-head Bilexical CFGs In the sections that follow, we frame various dependency models as a particular variety of CFGs known as split-head bilexical CFGs (Eisner and Satta, 1999). These allow us to use the fast Eisner and Satta (1999) parsing algorithm to compute the expectations required by VB in O(m3) time (Eisner and Blatz, 2007; Johnson, 2007) where m is the length of the sentence.1 In the split-head bilexical CFG framework, each nonterminal in the grammar is annotated with a terminal symbol. For dependency grammars, these annotations correspond to words and/or parts-ofspeech. Additionally, split-head bilexical CFGs require that each word sij in sentence si is represented in a split form by two terminals called its left part sijL and right part sijR. The set of these parts constitutes the terminal symbols of the grammar. This split-head property relates to a particular type of dependency grammar in which the left and right dependen</context>
<context position="11811" citStr="Johnson, 2007" startWordPosition="2001" endWordPosition="2003">tion d are generated by repeatedly deciding whether to generate another new argument or to stop and then generating the argument if required. The probability of deciding whether to generate another argument is conditioned on H, d and whether this would be the first argument (this is the sense in which it models valence). When DMV generates an argument, the part-of-speech of that argument A is generated given H and d. 1Efficiently parsable versions of split-head bilexical CFGs for the models described in this paper can be derived using the fold-unfold grammar transform (Eisner and Blatz, 2007; Johnson, 2007). P( θ ¯N|α¯N) = Y∝ ¯r∈ R¯ X log P(s|α) ≥ J Q(t, θ) log P(s, t, t B Q(t, t θ|α) θ) = F Q( Q( 103 Rule Description S → YH Select H as root YH → LH RH Move to split-head representation LH → HL STOP |dir = L, head = H, val = 0 LH → L1 CONT |dir = L, head = H, val = 0 H L′ H → HL STOP |dir = L, head = H, val = 1 L′H → L1 CONT |dir = L, head = H, val = 1 H Ll H → YA L′H Arg A |dir = L,head = H Figure 2: Rule schema for DMV. For brevity, we omit the portion of the grammar that handles the right arguments since they are symmetric to the left (all rules are the same except for the attachment rule wher</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Transforming projective bilexical dependency grammars into efficiently-parsable CFGs with unfold-fold. In Proceedings ofACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>A generative constituent-context model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="26472" citStr="Klein and Manning (2002)" startWordPosition="4640" endWordPosition="4644">ial Q(t) = P(t|s,¯θ(b)) which can be calculated using the Expectation-Maximization E-Step. Q(¯θ) is then initialized using the standard VB M-step. For the Lexicalized-EVG, we modify this procedure slightly, by first running MB smoothed EVG models for 40 iterations each and selecting the best model in each cohort as before; each L-EVG distribution is initialized from its corresponding EVG distribution. The new P(A|h, H, d, v) distributions are set initially to their corresponding P(A|H, d, v) values. 6 Results We trained on the standard Penn Treebank WSJ corpus (Marcus et al., 1993). Following Klein and Manning (2002), sentences longer than 10 words after removing punctuation are ignored. We refer to this variant as WSJ10. Following Cohen et al. (2008), we train on sections 2-21, used 22 as a held-out development corpus, and present results evaluated on section 23. The models were all trained using Variational Bayes, and initialized as described in Section 5. To evaluate, we follow Cohen et al. (2008) in using the mean of the variational posterior Dirichlets as a point estimate ¯θ′. For the unsmoothed models we decode by selecting the Viterbi parse given ¯θ′, or argmaxtP (t|s, ¯θ′). For the smoothed models</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher Manning. 2002. A generative constituent-context model for improved grammar induction. In Proceedings ofACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1918" citStr="Klein and Manning (2004)" startWordPosition="290" endWordPosition="293">o study methods for parsing without requiring annotated sentences. In this work, we focus on unsupervised dependency parsing. Our goal is to produce a directed graph of dependency relations (e.g. Figure 1) where each edge indicates a head-argument relation. Since the task is unsupervised, we are not given any examples of correct dependency graphs and only take words and their parts of speech as input. Most of the recent work in this area (Smith, 2006; Cohen et al., 2008) has focused on variants of the The big dog barks Figure 1: Example dependency parse. Dependency Model with Valence (DMV) by Klein and Manning (2004). DMV was the first unsupervised dependency grammar induction system to achieve accuracy above a right-branching baseline. However, DMV is not able to capture some of the more complex aspects of language. Borrowing some ideas from the supervised parsing literature, we present two new models: Extended Valence Grammar (EVG) and its lexicalized extension (L-EVG). The primary difference between EVG and DMV is that DMV uses valence information to determine the number of arguments a head takes but not their categories. In contrast, EVG allows different distributions over arguments for different vale</context>
<context position="11018" citStr="Klein and Manning (2004)" startWordPosition="1868" endWordPosition="1871">bilexical CFGs require that each word sij in sentence si is represented in a split form by two terminals called its left part sijL and right part sijR. The set of these parts constitutes the terminal symbols of the grammar. This split-head property relates to a particular type of dependency grammar in which the left and right dependents of a head are generated independently. Note that like CFGs, split-head bilexical CFGs can be made probabilistic. 2.3 Dependency Model with Valence The most successful recent work on dependency induction has focused on the Dependency Model with Valence (DMV) by Klein and Manning (2004). DMV is a generative model in which the head of the sentence is generated and then each head recursively generates its left and right dependents. The arguments of head H in direction d are generated by repeatedly deciding whether to generate another new argument or to stop and then generating the argument if required. The probability of deciding whether to generate another argument is conditioned on H, d and whether this would be the first argument (this is the sense in which it models valence). When DMV generates an argument, the part-of-speech of that argument A is generated given H and d. </context>
<context position="13501" citStr="Klein and Manning (2004)" startWordPosition="2352" endWordPosition="2355">ts words into their left and right components. LH encodes the stopping decision given that we have not generated any arguments so far. L′H encodes the same decision after generating one or more arguments. L1H represents the distribution over left attachments. To extract dependency relations from these parse trees, we scan for attachment rules (e.g., L1H —* YA L′H) and record that A depends on H. The schema omits the rules for right arguments since they are symmetric. We show a parse of “The big dog barks” in Figure 3.2 Much of the extensions to this work have focused on estimation procedures. Klein and Manning (2004) use Expectation Maximization to estimate the model parameters. Smith and Eisner (2005) and Smith (2006) investigate using Contrastive Estimation to estimate DMV. Contrastive Estimation maximizes the conditional probability of the observed sentences given a neighborhood of similar unseen sequences. The results of this approach vary widely based on regularization and neighborhood, but often outperforms EM. 2Note that our examples use words as leaf nodes but in our unlexicalized models, the leaf nodes are in fact parts-of-speech. Figure 3: DMV split-head bilexical CFG parse of “The big dog barks</context>
<context position="15203" citStr="Klein and Manning (2004)" startWordPosition="2608" endWordPosition="2611">dencies initially, gradually weakening the penalty during estimation. If hand-annotated dependencies on a held-out set are available for parameter selection, this performs far better than EM; however, performing parameter selection on a held-out set without the use of gold dependencies does not perform as well. Cohen et al. (2008) investigate using Bayesian Priors with DMV. The two priors they use are the Dirichlet (which we use here) and the Logistic Normal prior, which allows the model to capture correlations between different distributions. They initialize using the harmonic initializer of Klein and Manning (2004). They find that the Logistic Normal distribution performs much better than the Dirichlet with this initialization scheme. Cohen and Smith (2009), investigate (concur104 Rule Description S → YH Select H as root YH → LH RH Move to split-head representation LH → HL STOP |dir = L, head = H, val = 0 LH → L′ CONT |dir = L, head = H, val = 0 H L′H → L1 STOP |dir = L, head = H, val = 1 H L′H → L2 CONT |dir = L, head = H, val = 1 H L2H → YA L′H Arg A |dir = L, head = H, val = 1 L1H → YA HL Arg A |dir = L, head = H, val = 0 Figure 4: Extended Valence Grammar schema. As before, we omit rules involving t</context>
<context position="24724" citStr="Klein and Manning (2004)" startWordPosition="4340" endWordPosition="4343">ribution from DMV. We call this EVG smoothed-skip-val. The second possibility is to have the backoff distribution ignore the head part-of-speech H and use backoff conditioning event v, d. This assumes that arguments share a common distribution across heads. We call this EVG smoothed-skip-head. As we see below, backing off by ignoring the part-ofspeech of the head H worked better than ignoring the argument position v. For L-EVG we smooth the argument part-ofspeech distribution (conditioned on the head word) with the unlexicalized EVG smoothed-skip-head model. 5 Initialization and Search issues Klein and Manning (2004) strongly emphasize the importance of smart initialization in getting good performance from DMV. The likelihood function is full of local maxima and different initial parameter values yield vastly different quality solutions. They offer what they call a “harmonic initializer” which 4We set the other Dirichlet hyperparameters to 1. initializes the attachment probabilities to favor arguments that appear more closely in the data. This starts EM in a state preferring shorter attachments. Since our goal is to expand the model to incorporate lexical information, we want an initialization scheme whic</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Proceedings of ACL 2004, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenichi Kurihara</author>
<author>Taisuke Sato</author>
</authors>
<title>An application of the variational bayesian approach to probabilistics context-free grammars.</title>
<date>2004</date>
<booktitle>In IJCNLP</booktitle>
<contexts>
<context position="8661" citStr="Kurihara and Sato (2004)" startWordPosition="1475" endWordPosition="1478">us the prior over θ is a product of Dirichlets,which is conjugate to the PCFG likelihood function (Johnson et al., 2007). That is, the posterior P(¯θ|s, t, α) is also a product of Dirichlets, also factoring into a Dirichlet for each nonterminal ¯N, where the parameters α¯r are augmented by the number of times rule r¯ is observed in tree t: P( θ|s,t,α) ∝ P(s,t |θ)P( θ|α) ¯θf(t,¯r)+α¯r−1 r¯ We can see that α¯r acts as a pseudocount of the number of times r¯ is observed prior to t. To make use of this prior, we use the Variational Bayes (VB) technique for PCFGs with Dirichlet Priors presented by Kurihara and Sato (2004). VB estimates a distribution over ¯θ. In contrast, Expectation Maximization estimates merely a point estimate of ¯θ. In VB, one estimates Q(t, ¯θ), called the variational distribution, which approximates the posterior distribution P(t, ¯θ|s, α) by minimizing the KL divergence of P from Q. Minimizing the KL divergence, it turns out, is equivalent to maximizing a lower bound F of the log marginal likelihood log P(s|α). The negative of the lower bound, −F, is sometimes called the free energy. As is typical in variational approaches, Kurihara and Sato (2004) make certain independence assumptions </context>
</contexts>
<marker>Kurihara, Sato, 2004</marker>
<rawString>Kenichi Kurihara and Taisuke Sato. 2004. An application of the variational bayesian approach to probabilistics context-free grammars. In IJCNLP 2004 Workshop Beyond Shallow Analyses.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="26436" citStr="Marcus et al., 1993" startWordPosition="4635" endWordPosition="4638">the prior ¯θ(b). We set the initial Q(t) = P(t|s,¯θ(b)) which can be calculated using the Expectation-Maximization E-Step. Q(¯θ) is then initialized using the standard VB M-step. For the Lexicalized-EVG, we modify this procedure slightly, by first running MB smoothed EVG models for 40 iterations each and selecting the best model in each cohort as before; each L-EVG distribution is initialized from its corresponding EVG distribution. The new P(A|h, H, d, v) distributions are set initially to their corresponding P(A|H, d, v) values. 6 Results We trained on the standard Penn Treebank WSJ corpus (Marcus et al., 1993). Following Klein and Manning (2002), sentences longer than 10 words after removing punctuation are ignored. We refer to this variant as WSJ10. Following Cohen et al. (2008), we train on sections 2-21, used 22 as a held-out development corpus, and present results evaluated on section 23. The models were all trained using Variational Bayes, and initialized as described in Section 5. To evaluate, we follow Cohen et al. (2008) in using the mean of the variational posterior Dirichlets as a point estimate ¯θ′. For the unsmoothed models we decode by selecting the Viterbi parse given ¯θ′, or argmaxtP</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
</authors>
<title>Modeling valence effects in unsupervised grammar induction.</title>
<date>2008</date>
<tech>Technical Report CS09-01,</tech>
<institution>Brown University,</institution>
<location>Providence, RI, USA.</location>
<contexts>
<context position="17107" citStr="McClosky (2008)" startWordPosition="2970" endWordPosition="2972">e DMV grammar (results shown in Figure 4). First, we will introduce the rule L2H —* YA L′H to denote the decision of what argument to generate for positions not nearest to the head. Next, instead of having L′H expand to HL or L1H, we will expand it to L1H (attach to nearest argument and stop) or L2H (attach to nonnearest argument and continue). We call this the Extended Valence Grammar (EVG). As a concrete example, consider the phrase “the big hungry dog” (Figure 5). We would expect that distribution over the nearest left argument for “dog” to be different than farther left arguments. The fig3McClosky (2008) explores this idea further in an unsmoothed grammar. .. .. . . Ldog Ldog L1 L′ dog dog bigL bigR Figure 5: An example of moving from DMV to EVG for a fragment of “The big dog.” Boxed nodes indicate changes. The key difference is that EVG distinguishes between the distributions over the argument nearest the head (big) from arguments farther away (The). ure shows that EVG allows these two distributions to be different (nonterminals L2dog and L1dog) whereas DMV forces them to be equivalent (both use L1dog as the nonterminal). 3.1 Lexicalization All of the probabilistic models discussed thus far </context>
</contexts>
<marker>McClosky, 2008</marker>
<rawString>David McClosky. 2008. Modeling valence effects in unsupervised grammar induction. Technical Report CS09-01, Brown University, Providence, RI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Guiding unsupervised grammar induction using contrastive estimation.</title>
<date>2005</date>
<booktitle>In International Joint Conference on Artificial Intelligence Workshop on Grammatical Inference Applications.</booktitle>
<contexts>
<context position="13588" citStr="Smith and Eisner (2005)" startWordPosition="2364" endWordPosition="2367">at we have not generated any arguments so far. L′H encodes the same decision after generating one or more arguments. L1H represents the distribution over left attachments. To extract dependency relations from these parse trees, we scan for attachment rules (e.g., L1H —* YA L′H) and record that A depends on H. The schema omits the rules for right arguments since they are symmetric. We show a parse of “The big dog barks” in Figure 3.2 Much of the extensions to this work have focused on estimation procedures. Klein and Manning (2004) use Expectation Maximization to estimate the model parameters. Smith and Eisner (2005) and Smith (2006) investigate using Contrastive Estimation to estimate DMV. Contrastive Estimation maximizes the conditional probability of the observed sentences given a neighborhood of similar unseen sequences. The results of this approach vary widely based on regularization and neighborhood, but often outperforms EM. 2Note that our examples use words as leaf nodes but in our unlexicalized models, the leaf nodes are in fact parts-of-speech. Figure 3: DMV split-head bilexical CFG parse of “The big dog barks.” Smith (2006) also investigates two techniques for maximizing likelihood while incorp</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Guiding unsupervised grammar induction using contrastive estimation. In International Joint Conference on Artificial Intelligence Workshop on Grammatical Inference Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Annealing structural bias in multilingual weighted grammar induction.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<contexts>
<context position="14538" citStr="Smith and Eisner, 2006" startWordPosition="2505" endWordPosition="2508">that our examples use words as leaf nodes but in our unlexicalized models, the leaf nodes are in fact parts-of-speech. Figure 3: DMV split-head bilexical CFG parse of “The big dog barks.” Smith (2006) also investigates two techniques for maximizing likelihood while incorporating the locality bias encoded in the harmonic initializer for DMV. One technique, skewed deterministic annealing, ameliorates the local maximum problem by flattening the likelihood and adding a bias towards the Klein and Manning initializer, which is decreased during learning. The second technique is structural annealing (Smith and Eisner, 2006; Smith, 2006) which penalizes long dependencies initially, gradually weakening the penalty during estimation. If hand-annotated dependencies on a held-out set are available for parameter selection, this performs far better than EM; however, performing parameter selection on a held-out set without the use of gold dependencies does not perform as well. Cohen et al. (2008) investigate using Bayesian Priors with DMV. The two priors they use are the Dirichlet (which we use here) and the Logistic Normal prior, which allows the model to capture correlations between different distributions. They init</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>Noah A. Smith and Jason Eisner. 2006. Annealing structural bias in multilingual weighted grammar induction. In Proceedings of COLING-ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
</authors>
<title>Novel Estimation Methods for Unsupervised Discovery ofLatent Structure in Natural Language Text.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Johns Hopkins University.</institution>
<contexts>
<context position="1748" citStr="Smith, 2006" startWordPosition="262" endWordPosition="263">ortunately, manually annotating corpora with parse trees is expensive and time consuming so for languages and domains with minimal resources it is valuable to study methods for parsing without requiring annotated sentences. In this work, we focus on unsupervised dependency parsing. Our goal is to produce a directed graph of dependency relations (e.g. Figure 1) where each edge indicates a head-argument relation. Since the task is unsupervised, we are not given any examples of correct dependency graphs and only take words and their parts of speech as input. Most of the recent work in this area (Smith, 2006; Cohen et al., 2008) has focused on variants of the The big dog barks Figure 1: Example dependency parse. Dependency Model with Valence (DMV) by Klein and Manning (2004). DMV was the first unsupervised dependency grammar induction system to achieve accuracy above a right-branching baseline. However, DMV is not able to capture some of the more complex aspects of language. Borrowing some ideas from the supervised parsing literature, we present two new models: Extended Valence Grammar (EVG) and its lexicalized extension (L-EVG). The primary difference between EVG and DMV is that DMV uses valence</context>
<context position="13605" citStr="Smith (2006)" startWordPosition="2369" endWordPosition="2370"> arguments so far. L′H encodes the same decision after generating one or more arguments. L1H represents the distribution over left attachments. To extract dependency relations from these parse trees, we scan for attachment rules (e.g., L1H —* YA L′H) and record that A depends on H. The schema omits the rules for right arguments since they are symmetric. We show a parse of “The big dog barks” in Figure 3.2 Much of the extensions to this work have focused on estimation procedures. Klein and Manning (2004) use Expectation Maximization to estimate the model parameters. Smith and Eisner (2005) and Smith (2006) investigate using Contrastive Estimation to estimate DMV. Contrastive Estimation maximizes the conditional probability of the observed sentences given a neighborhood of similar unseen sequences. The results of this approach vary widely based on regularization and neighborhood, but often outperforms EM. 2Note that our examples use words as leaf nodes but in our unlexicalized models, the leaf nodes are in fact parts-of-speech. Figure 3: DMV split-head bilexical CFG parse of “The big dog barks.” Smith (2006) also investigates two techniques for maximizing likelihood while incorporating the local</context>
</contexts>
<marker>Smith, 2006</marker>
<rawString>Noah A. Smith. 2006. Novel Estimation Methods for Unsupervised Discovery ofLatent Structure in Natural Language Text. Ph.D. thesis, Department of Computer Science, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines. In</title>
<date>2003</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="27983" citStr="Yamada and Matsumoto (2003)" startWordPosition="4911" endWordPosition="4914">Model Variant Dir. Acc. DMV harmonic init 46.9* DMV random init 55.7 (8.0) DMV log normal-families 59.4* DMV shared log normal-families 62.4† DMV smoothed 61.2 (1.2) EVG random init 53.3 (7.1) EVG smoothed-skip-val 62.1 (1.9) EVG smoothed-skip-head 65.0 (5.7) L-EVG smoothed 68.8 (4.5) Table 1: Directed accuracy (DA) for WSJ10, section 23. *,† indicate results reported by Cohen et al. (2008), Cohen and Smith (2009) respectively. Standard deviations over 10 runs are given in parentheses dependencies for section 23, which were extracted from the phrase structure trees using the standard rules by Yamada and Matsumoto (2003). We measure the percent accuracy of the directed dependency edges. For the lexicalized model, we replaced all words that were seen fewer than 100 times with “UNK.” We ran each of our systems 10 times, and report the average directed accuracy achieved. The results are shown in Table 1. We compare to work by Cohen et al. (2008) and Cohen and Smith (2009). Looking at Table 1, we can first of all see the benefit of randomized initialization over the harmonic initializer for DMV. We can also see a large gain by adding smoothing to DMV, topping even the logistic normal prior. The unsmoothed EVG act</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In In Proceedings of the International Workshop on Parsing Technologies.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>