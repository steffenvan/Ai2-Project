<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000188">
<title confidence="0.982832">
Boosting automatic lexical acquisition with morphological information
</title>
<author confidence="0.995727">
Massimiliano Ciaramita
</author>
<affiliation confidence="0.983478">
Department of Cognitive and Linguistic Sciences
Brown University
</affiliation>
<address confidence="0.982178">
Providence, RI, USA 02912
</address>
<email confidence="0.998849">
massimiliano ciaramita@brown.edu
</email>
<sectionHeader confidence="0.995648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999461153846154">
In this paper we investigate the impact of
morphological features on the task of au-
tomatically extending a dictionary. We
approach the problem as a pattern clas-
sification task and compare the perfor-
mance of several models in classifying
nouns that are unknown to a broad cov-
erage dictionary. We used a boosting clas-
sifier to compare the performance of mod-
els that use different sets of features. We
show how adding simple morphological
features to a model greatly improves the
classification performance.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999737083333333">
The incompleteness of the available lexical re-
sources is a major bottleneck in natural language
processing (NLP). The development of methods for
the automatic extension of these resources might af-
fect many NLP tasks. Further, from a more general
computational perspective, modeling lexical mean-
ing is a necessary step toward semantic modeling of
larger linguistic units.
We approach the problem of lexical acquisition
as a classification task. The goal of the classifier is
to insert new words into an existing dictionary. A
dictionary1 in this context simply associates lexical
</bodyText>
<footnote confidence="0.894816666666667">
I✁ would like to thank for their input everybody in the Brown
Laboratory for Linguistic Information Processing (BLLIP) and
Information Retrieval and Machine Learning Group at Brown
(IRML), and particularly Mark Johnson and Thomas Hofmann.
I also thank Brian Roark and Jesse Hochstadt.
1Or lexicon, we use the two terms interchangeably.
</footnote>
<bodyText confidence="0.992788966666667">
forms with class labels; e.g., ,
where the arrow can be interpreted as the ISA rela-
tion. In this study we use a simplified version of
Wordnet as our base lexicon and we ignore other
relevant semantic relations (like hyponymy) and the
problem of word sense ambiguity. We focus on
finding features that are useful for associating un-
known words with class labels from the dictionary.
In this paper we report the following preliminary
findings. First of all we found that the task is dif-
ficult. We developed several models, based on near-
est neighbor (NN), naive Bayes (NB) and boosting
classifiers. Unfortunately, the error rate of these
models is much higher than what is found in text
categorization tasks2 with comparable numbers of
classes. Secondly, it seems obvious that informa-
tion that is potentially useful for word classifica-
tion can be of very diverse types, e.g., semantic
and syntactic, morphological and topical. There-
fore methods that allow flexible feature combination
and selection are desirable. We experimented with a
multiclass boosting algorithm (Schapire and Singer,
2000), which proved successful in this respect. In
this context boosting combines two sources of in-
formation: words co-occurring near the new word,
which we refer to as collocations, and morpholog-
ical properties of the new word. This classifier
shows improved performance over models that use
only collocations. In particular, we found that even
rudimentary morphological information greatly im-
</bodyText>
<footnote confidence="0.8809216">
2Text categorization is the task of associating documents
with topic labels (POLITICS, SPORT, ...) and it bears simi-
larities with semantic classification tasks such as word sense
disambiguation, information extraction and acquisition.
Unsupervised Lexical Acquisition: Proceedings of the Workshop of the
</footnote>
<note confidence="0.799051">
ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,
July 2002, pp. 17-25. Association for Computational Linguistics.
SHAPE TRAIT QUALITY PROPERTY OTHER ATTR SOCIAL REL SPATIAL REL OTHER REL TIME OTHER ABS
</note>
<figure confidence="0.887912">
RELATION
ATTRIBUTE
ABSTRACTION
MEASURE
</figure>
<figureCaption confidence="0.999972">
Figure 1: A few classes under the root class ABSTRACTION in MiniWordnet.
</figureCaption>
<bodyText confidence="0.982833454545455">
proves classification performance and should there-
fore be part of any word classification model.
The outline of the paper is as follows. In section
2 we introduce the dictionary we used for our tests,
a simplified version of Wordnet. In section 3 we de-
scribe more formally the task, a few simple mod-
els, and the test methods. In section 4 we describe
the boosting model and the set of morphological fea-
tures. In section 5 we summarize the results of our
experiments. In section 6 we describe related work,
and then in section 7 we present our conclusions.
</bodyText>
<sectionHeader confidence="0.993816" genericHeader="introduction">
2 MiniWordnet
</sectionHeader>
<bodyText confidence="0.999563055555556">
Ideally the lexicon we would like to extend is a
broad coverage machine readable dictionary like
Wordnet (Miller et al., 1990; Fellbaum, 1998). The
problem with trying to directly use Wordnet is that it
contains too many classes (synsets), around 70 thou-
sand. Learning in such a huge class space can be
extremely problematic, and intuitively it is not the
best way to start on a task that hasn’t been much ex-
plored3. Instead, we manually developed a smaller
lexicon dubbed MiniWordnet, which is derived from
Wordnet version 1.6. The reduced lexicon has the
same coverage (about 95 thousand noun types) but
only a fraction of the classes. In this paper we con-
sidered only nouns and the noun database. The goal
was to reduce the number of classes to about one
hundred4 of roughly comparable taxonomical gen-
erality and consistency, while maintaining a little bit
of hierarchical structure.
</bodyText>
<footnote confidence="0.988791714285714">
3Preliminary experiments confirmed this; classification is
computationally expensive, performance is low, and it is very
hard to obtain even very small improvements when the full
database is used.
4A magnitude comparable to the class space of well stud-
ied text categorization data sets like the Reuters-21578 (Yang,
1999).
</footnote>
<bodyText confidence="0.994635813953488">
The output of the manual coding is a set of 106
classes that are the result of merging hundreds of
synsets. A few random examples of these classes
are PERSON, PLANT, FLUID, LOCATION, AC-
TION, and BUSINESS. One way to look at this set
of classes is from the perspective of named-entity
recognition tasks, where there are a few classes of
a similar level of generality, e.g, PERSON, LOCA-
TION, ORGANIZATION, OTHER. The difference
here is that the classes are intended to capture all
possible taxonomic distinctions collapsed into the
OTHER class above. In addition to the 106 leaves
we also kept a set of superordinate levels. We
maintained the 9 root classes in Wordnet plus 18
intermediate ones. Examples of these intermedi-
ate classes are ANIMAL, NATURAL OBJECT, AR-
TIFACT, PROCESS, and ORGANIZATION. The rea-
son for keeping some of the superordinate structure
is that hierarchical information might be important
in word classification; this is something we will in-
vestigate in the future. For example, there might not
be enough information to classify the noun ostrich
in the BIRD class but enough to label it as ANIMAL.
The superordinates are the original Wordnet synsets.
The database has a maximum depth of 5.
We acknowledge that the methodology and results
of reducing Wordnet in this way are highly subjec-
tive and noisy. However, we also think that go-
ing through an intermediary step with the reduced
database has been useful for our purposes and it
might also be so for other researchers5. Figure 1 de-
picts the hierarchy below the root class ABSTRAC-
TION. The classes that are lined up at the bottom
of the figure are leaves. As in Wordnet, some sub-
5More information about MiniWordnet and the
database itself are available at www.cog.brown.edu/
massi/research.
hierarchies are more densely populated than others.
For example, the ABSTRACTION sub-hierarchy is
more populated (11 leaves) than that of EVENT (3
leaves). The most populated and structured class is
ENTITY, with almost half of the leaves (45) and sev-
eral superordinate classes (10).
</bodyText>
<sectionHeader confidence="0.993726" genericHeader="method">
3 Automatic lexical acquisition
</sectionHeader>
<subsectionHeader confidence="0.999916">
3.1 Word classification
</subsectionHeader>
<bodyText confidence="0.999906689655172">
We frame the task of inserting new words into the
dictionary as a classification problem: is the set
of classes defined by the dictionary. Given a vector
of features we want to find functions
of the form . In particular we are interested
in learning functions from data, i.e., a training set of
pairs and , such that there will
be a small probability of error when we apply the
classifier to unknown pairs (new nouns).
Each class is described by a vector of features. A
class of features that intuitively carry semantic in-
formation are collocations, i.e., words that co-occur
with the nouns of interest in a corpus. Collocations
have been widely used for tasks such as word sense
disambiguation (WSD) (Yarowsky, 1995), informa-
tion extraction (IE) (Riloff, 1996), and named-entity
recognition (Collins and Singer, 1999). The choice
of collocations can be conditioned in many ways:
according to syntactic relations with the target word,
syntactic category, distance from the target, and so
on.
We use a very simple set of collocations: each
word that appears within positions from a
nounis a feature. Each occurrence, or token,
of, , is then characterized by a vector of fea-
ture counts . The vector representation of the noun
typeis the sum of all the vectors representing the
contexts in which it occurs. Overall the vector repre-
sentation for each class in the dictionary is the sum
of the vectors of all nouns that are members of the
class
while the vector representation of an unknown noun
is the sum of the feature vectors of the contexts in
which it occurred
The corpus that we used to collect the statistics
about collocations is the set of articles from the 1989
Wall Street Journal (about 4 million words) in the
BLLIP’99 corpus.
We performed the following tokenization steps.
We used the Wordnet ”morph” functions to mor-
phologically simplify nouns, verbs and adjectives.
We excluded only punctuation; we did no filtering
for part of speech (POS). Each word was actually
a word-POS pair; i.e., we distinguished between
plant:NN and plant:VB. We collapsed sequences of
NNs that appeared in Wordnet as one noun; so we
have one entry for the noun car company:NN. We
also collapsed sequences of NNPs, possibly inter-
leaved by the symbol ”&amp;”, e.g., George Bush:NNP
and Procter &amp; Gamble:NNP. To reduce the number
of features a little we changed all NNPs beginning
with Mr. or Ms. to MISS X:NNP, all NNPs ending in
CORP. or CO. to COMPANY X:NNP, and all words
with POS CD, i.e., numbers, starting with a digit to
NUMBER X:CD. For training and testing we con-
sidered only nouns that are not ambiguous accord-
ing to the dictionary, and we used only features that
occurred at least 10 times in the corpus.
</bodyText>
<subsectionHeader confidence="0.999703">
3.2 Simple models
</subsectionHeader>
<bodyText confidence="0.997704833333333">
We developed several simple classifiers. In particu-
lar we focused on nearest neighbor ( ) and naive
Bayes ( ) methods. Both are very simple and
powerful classification techniques. For NN we used
cosine as a measure of distance between two vectors,
and the classifier is thus
</bodyText>
<equation confidence="0.948397">
(1)
</equation>
<bodyText confidence="0.985978739130435">
Since we used aggregate vectors for classes and
noun types, we only used the best class; i.e., we
always used 1-nearest-neighbor classifiers. Thus
in this paper refers only to the size of the win-
dow around the target noun and never to number of
neighbors consulted in-nearest-neighbor classifi-
cation. We found that using TFIDF weights instead
of simple counts greatly improved performance of
the NN classifiers, and we mainly report results rel-
ative to the TFIDF NN classifiers ( ). A
document in this context is the context, delimited by
the window size, in which each each noun occurs.
TFIDF basically filters out the impact of closed class
level
words and re-weights features by their informative-
ness, thus making a stop list or other feature manip-
ulations unnecessary. The naive Bayes classifiers is
also very simple
(2)
The parameters of the prior and class-conditional
distributions are easily estimated using maximum
likelihood. We smoothed all counts by a factor of
.5.
</bodyText>
<subsectionHeader confidence="0.999927">
3.3 Testing procedure
</subsectionHeader>
<bodyText confidence="0.999554142857143">
We tested each model on an increasing numbers of
classes or level. At level 1 the dictionary maps nouns
only to the nine Wordnet roots; i.e., there is a very
coarse distinction among noun categories at the level
of ENTITY, STATE, ACT,.... At level 2 the dictionary
maps nouns to all the classes that have a level-1 par-
ent; thus each class can be either a leaf or an inter-
mediate (level 2) class. In general, at levelnouns
are only mapped to classes that have a level ( ),
or smaller, parent. There are 34 level-2 classes, 69
level-3 classes and 95 level-4 ones. Finally, at level
5, nouns are mapped to all 106 leaves. We compared
the boosting models and the NN and NB classifiers
over a fixed size forof 4.
For each level we extracted all unambiguous in-
stances from the BLLIP’99 data. The data ranged
from 200 thousand instances at level 5, to almost 400
thousand at level 1. As the number of classes grows
there are less unambiguous words. We randomly se-
lected a fixed number of noun types for each level:
200 types at levels 4 and 5, 300 at level 3, 350 at
level 2 and 400 at level 1. Test was limited to com-
mon nouns with frequency between 10 and 300 on
the total data. No instance of the noun types present
in the test set ever appeared in the training data. The
test data was between 5 and 10% of the training data;
10 thousand instances at level 5, 16 thousand at level
1, with intermediate figures for the other levels. We
used exactly the same partition of the data for all ex-
periments, across all models.
Figure 2 shows the error rate of several simple
models at level 1 for increasing values of. The
error keeps dropping untilreaches a value around
4 and then starts rising. Testing for all values of
confirmed this pattern. This result sug-
gests that the most useful contextual information is
that close to the noun, which should be syntactic-
semantic in nature, e.g., predicate-argument prefer-
ences. As the window widens, the bag of features
becomes more noisy. This fact is not too surprising.
If we made the window as wide as the whole docu-
ment, every noun token in the document would have
the same set of features. As expected, as the num-
ber of classes increases, the task becomes harder and
the error of the classifiers increases. Nonetheless the
same general pattern of performance with respect to
holds. As the figure shows greatly im-
proves over the simpler classifier that only uses
counts. outperforms both.
</bodyText>
<sectionHeader confidence="0.999436" genericHeader="method">
4 Boosting for word classification
</sectionHeader>
<subsectionHeader confidence="0.999881">
4.1 AdaBoost.MH with abstaining
</subsectionHeader>
<bodyText confidence="0.9999674">
Boosting is an iterative method for combining the
output of many weak classifiers or learners6 to
produce an accurate ensemble of classifiers. The
method starts with a training setand trains the first
classifier. At each successive iterationa new clas-
sifier is trained on a new training set , which is
obtained by re-weighting the training data used at
so that the examples that were misclassified
at are given more weight while less weight is
given to the correctly classified examples. At each
</bodyText>
<footnote confidence="0.963336333333333">
6The learner is called weak because it is required to clas-
sify examples better than at random only by an arbitrarily small
quantity.
</footnote>
<figure confidence="0.99629125">
1 2 3 4 5 6 7 8 9 10
45
80
75
70
65
60
55
50
NNfreq
NNtfidf
NB
</figure>
<figureCaption confidence="0.993498">
Figure 2: Error of the ,and
</figureCaption>
<bodyText confidence="0.9305256">
models for .at level 1
error
iteration a weak learner is trained and added
to the ensemble with weight . The final ensemble
has the form
</bodyText>
<equation confidence="0.577718">
(3)
</equation>
<bodyText confidence="0.998452035714286">
In the most popular version of a boosting algorithm,
AdaBoost (Schapire and Singer, 1998), at each it-
eration a classifier is trained to minimize the expo-
nential loss on the weighted training set. The ex-
ponential loss is an upper bound on the zero-one
loss. AdaBoost minimizes the exponential loss on
the training set so that incorrect classification and
disagreement between members of the ensemble are
penalized.
Boosting has been successfully applied to sev-
eral problems. Among these is text categoriza-
tion (Schapire and Singer, 2000), which bears
similarities with word classification. For our
experiments we used AdaBoost.MH with real-
valued predictions and abstaining, a version of
boosting for multiclass classification described
in Schapire and Singer (2000). This version of Ad-
aBoost minimizes a loss function that is an upper
bound on the Hamming distance between the weak
learners’ predictions and the real labels, i.e., the
number of label mismatches (Schapire and Singer,
1998). This upper bound is the product . The
function is 1 ifis the correct label for the train-
ing exampleand is -1 otherwise; is the
total number of classes; and is the number
of training examples. We explain what the term for
the weak learner means in the next section.
Then
</bodyText>
<figure confidence="0.858075428571428">
(4)
AdaBoost.MH looks schematically as follows:
ADABOOST.MH
1 uniform initialization
2 for to
3 do
4
</figure>
<bodyText confidence="0.930883153846154">
is the weight assigned to the instance-label
pair (). In the first round each pair is assigned
the same weight. At the end of each round the re-
weightedis normalized so that it forms a distri-
bution; i.e.,is a normalizing factor. The algo-
rithm outputs the final hypotheses for an instance
with respect to class label
(5)
since we are interested in classifying noun types the
final score for each unknown noun is
(6)
where with instanceis a token of noun
type.
</bodyText>
<subsectionHeader confidence="0.982324">
4.2 Weak learners
</subsectionHeader>
<bodyText confidence="0.954530884615385">
In this version of AdaBoost weak learners are ex-
tremely simple. Each feature, e.g., one particular
collocation, is a weak classifier. At each round one
feature is selected. Each feature makes a real-
valued prediction with respect to each class
. If is positive then featuremakes a pos-
itive prediction about class; if negative, it makes
a negative prediction about class. The magnitude
of the prediction is interpreted as a mea-
sure of the confidence in the prediction. Then for
each training instance a simple check for the pres-
ence or absence of this feature is performed. For
example, a possible collocation feature is eat:VB,
and the corresponding prediction is “if eat:VB ap-
pears in the context of a noun, predict that the noun
belongs to the class FOOD and doesn’t belong to
classes PLANT, BUSINESS,...”. A weak learner is
defined as follows:
if (7)
if
The prediction is computed as follows:
(8)
( ) is the sum of the weights of noun-label
pairs, from the distribution, where the feature ap-
pears and the label is correct (wrong); is a
smoothing factor. In Schapire and Singer (1998) it
</bodyText>
<equation confidence="0.99827225">
W=August; PL=0; MU=1; CO=’:POS; CO=passenger:NN; CO=traffic:NN; ...
W=punishment; PL=1; MU=0; MS=ment; MS=ishment; CO=in:IN; CO=to:TO; ...
W=vice president; PL=0; MU=0; MSHH=president; CO=say:VB; CO=chief:JJ; ...
W=newsletter; PL=0; MU=0; MS=er; MSSH=letter; CO=yield:NN; CO=seven-day:JJ; ...
</equation>
<figureCaption confidence="0.8679275">
Figure 3: Sample input to the classifiers, only has access to morphological information. CO stands
for the attribute “collocation”.
</figureCaption>
<bodyText confidence="0.933559333333333">
is shown thatis minimized for a particular fea-
ture by choosing its predictions as described in
equation (8). The weight usually associated with
the weak classifier (see equation (2)) here is simply
set to 1.
If the value in (8) is plugged into (4),becomes
</bodyText>
<equation confidence="0.741971">
(9)
</equation>
<bodyText confidence="0.9999108">
Therefore to minimizeat each round we choose
the feature for which this value is the smallest.
Updating these scores is what takes most of the com-
putation, Collins (2000) describes an efficient ver-
sion of this algorithm.
</bodyText>
<subsectionHeader confidence="0.998781">
4.3 Morphological features
</subsectionHeader>
<bodyText confidence="0.999184075">
We investigated two boosting models: ,
which uses only collocations as features, and
, which uses also a very simple set of mor-
phological features. In we used the colloca-
tions within a window of , which seemed
to be a good value for both the nearest neighbor
and the naive Bayes model. However, we didn’t fo-
cus on any method for choosing, since we believe
that the collocational features we used only approx-
imate more complex ones that need specific investi-
gation. Our main goal was to compare models with
and without morphological information. To spec-
ify the morphological properties of the nouns being
classified, we used the following set of features:
plural (PL): if the token occurs in the plural
form, PL=1; otherwise PL=0
upper case (MU): if the token’s first character
is upper-cased MU=1; otherwise MU=0
suffixes (MS): each token can have 0, 1, or
more of a given set of suffixes, e.g., -er, -
ishment, -ity, -ism, -esse, ...
prefixes (MP): each token can have 0, 1 or more
prefixes, e.g., pro-, re-, di-, tri-, ...
Words that have complex morphology share the
morphological head word if this is a noun in
Wordnet. There are two cases, depending on
whether the word is hyphenated (MSHH) or the
head word is a suffix (MSSH)
– hyphenated (MSHH): drinking age and
age share the same head-word age
– non-hyphenated (MSSH): chairman and
man share the same suffix head word,
man. We limited the use of this feature
to the case in which the remaining prefix
(chair) also is a noun in Wordnet.
We manually encoded two lists of 61 suffixes and
26 prefixes7. Figure 3 shows a few examples of the
input to the models. Each line is a training instance;
the attribute W refers to the lexical form of the noun
and was ignored by the classifier.
</bodyText>
<subsectionHeader confidence="0.999235">
4.4 Stopping criterion
</subsectionHeader>
<bodyText confidence="0.999989785714286">
One issue when using iterative procedures is decid-
ing when to stop. We used the simplest procedure of
fixing in advance the number of iterations. We no-
ticed that the test error drops until it reaches a point
at which it seems not to improve anymore. Then
the error oscillates around the same value even for
thousands of iterations, without apparent overtrain-
ing. A similar behavior is observable in some of the
results on text categorization presented in (Schapire
and Singer, 2000). We cannot say that overtrain-
ing is not a potential danger in multiclass boosting
models. However, for our experiments, in which the
main goal is to investigate the impact of a particu-
lar class of features, we could limit the number of
</bodyText>
<footnote confidence="0.9886845">
7The feature lists are available together with the MiniWord-
net files.
</footnote>
<note confidence="0.324963">
Training error
</note>
<figureCaption confidence="0.998016">
Figure 4: Training error at level 4.
</figureCaption>
<figure confidence="0.992849666666667">
Test error
BoostS
BoostM
75
0 500 1000 1500 2000 2500 3000 3500
t
</figure>
<figureCaption confidence="0.999918">
Figure 5: Test error at level 4.
</figureCaption>
<bodyText confidence="0.999773333333333">
iterations to a fixed value for all models. We chose
this maximum number of iterations to be 3500; this
allowed us to perform the experiments in a reason-
able time. Figure 4 and Figure 5 plot training and
test error for and at level 4 (per
instance). As the figures show, the error rate, on
both training and testing, is still dropping after the
fixed number of iterations. For the simplest model,
at level 1, the situation is slightly different:
the model converges on its final test error rate after
roughly 200 iterations and then remains stable. In
general, as the number of classes grows, the model
takes more iterations to converge and then the test
error remains stable while the training error keeps
slowly decreasing.
</bodyText>
<sectionHeader confidence="0.994989" genericHeader="evaluation">
5 Results and discussion
</sectionHeader>
<bodyText confidence="0.9844415">
The following table summarizes the different
models we tested:
</bodyText>
<table confidence="0.9440276">
MODEL FEATURES
Boost s TFIDF weights for collocations
Boost m collocation counts
collocations (binary)
collocations (binary)+morphology
</table>
<bodyText confidence="0.994668578947368">
Figure 6 plots the results across the five different
subsets of the reduced lexicon. The error rate is
the error on types. We also plot the results of a
baseline (BASE), which always chooses the most
frequent class and the error rate for random choice
(RAND). The baseline strategy is quite successful
on the first sets of classes, because the hierarchy un-
der the root is by far the most populated.
At level 1 it performs worse only than . As
the size of the model increases, the distribution of
classes becomes more uniform and the task becomes
harder for the baseline. As the figure shows the im-
pact of morphological features is quite impressive.
The average decrease in type error of over
is more than 17%, notice also the difference
in test and training error, per instance, in Figures 4
and 5.
In general, we observed that it is harder for all
classifiers to classify nouns that don’t belong to the
ENTITY class, i.e., maybe not surprisingly, it is
harder to classify nouns that refer to abstract con-
cepts such as groups, acts, or psychological fea-
tures. Usually most of the correct guesses regard
members of the ENTITY class or its descendants,
which are also typically the classes for which there
is more training data. really improves on
in this respect. guesses correctly
several nouns to which morphological features ap-
ply like spending, enforcement, participation, com-
petitiveness, credibility or consulting firm. It makes
also many mistakes, for example on conversation,
controversy and insurance company. One prob-
lem that we noticed is that there are several cases
of nouns that have intuitively meaningful suffixes
or prefixes that are not present in our hand-coded
lists. A possible solution to his problem might be
the use of more general morphological rules like
those used in part-of-speech tagging models (e.g.,
</bodyText>
<figure confidence="0.995483272727273">
t
error
90
80
70
60
50
40
30
20
0
500 1000 1500
2000 2500 3000 3500
BoostS
BoostM
100
95
90
error
85
80
level
</figure>
<figureCaption confidence="0.999974">
Figure 6: Comparison of all models for .
</figureCaption>
<bodyText confidence="0.999757906976744">
Ratnaparkhi (1996)), where all suffixes up to a cer-
tain length are included. We observed also cases of
recurrent confusion between classes. For example
between ACT and ABSTRACTION (or their subor-
dinates), e.g., for the noun modernization, possibly
because the suffix is common in both cases.
Another measure of the importance of morpho-
logical features is the ratio of their use with respect
to that of collocations. In the first 100 rounds of
, at level 5, 77% of the features selected
are morphological, 69% in the first 200 rounds. As
Figures 4 and 5 show these early rounds are usually
the ones in which most of the error is reduced. The
first ten features selected at level 5 by
were the following: PL=0, MU=0, PL=1, MU=0,
PL=1, MU=1, MS=ing, PL=0, MS=tion, and finally
CO=NUMBER X:CD. One final characteristic
of morphology that is worth mentioning is that
it is independent from frequency. Morphological
features are properties of the type and not just of
the token. A model that includes morphological
information should therefore suffer less from sparse
data problems.
From a more general perspective, Figure 6 shows
that even if the simpler boosting model’s perfor-
mance degrades more than the competitors after
level 3, performs better than all the other
classifiers until level 5 when the TFIDF nearest
neighbor and the naive Bayes classifiers catch up.
It should be noted though that, as Figures 4 and 5
showed, boosting was still improving at the end of
the fixed number of iterations at level 4 (but also
5). It might quite well improve significantly after
more iterations. However, determining absolute per-
formance was beyond the scope of this paper. It
is also fair to say that both and are very
competitive methods, and much simpler to imple-
ment efficiently than boosting. The main advantage
with boosting algorithms is the flexibility in manag-
ing features of very different nature. Feature combi-
nation can be performed naturally with probabilistic
models too but it is more complicated. However, this
is something worth investigating.
</bodyText>
<sectionHeader confidence="0.999973" genericHeader="evaluation">
6 Related work
</sectionHeader>
<bodyText confidence="0.99997803125">
Automatic lexical acquisition is a classic problem
in AI. It was originally approached in the con-
text of story understanding with the aim of en-
abling systems to deal with unknown words while
processing text or spoken input. These systems
would typically rely heavily on script-based knowl-
edge resources. FOUL-UP (Granger, 1977) is one
of these early models that tries to deterministically
maximize the expectations built into its knowledge
base. Jacobs and Zernik (1988) introduced the idea
of using morphological information, together with
other sources, to guess the meaning of unknown
words. Hastings and Lytinen (1994) investigated at-
tacking the lexical acquisition problem with a sys-
tem that relies mainly on taxonomic information.
In the last decade or so research on lexical seman-
tics has focused more on sub-problems like word
sense disambiguation (Yarowsky, 1995; Stevenson
and Wilks, 2001), named entity recognition (Collins
and Singer, 1999), and vocabulary construction for
information extraction (Riloff, 1996). All of these
can be seen as sub-tasks, because the space of pos-
sible classes for each word is restricted. In WSD the
possible classes for a word are its possible senses;
in named entity recognition or IE the number of
classes is limited to the fixed (usually small) num-
ber the task focuses on. Other kinds of models that
have been studied in the context of lexical acquisi-
tion are those based on lexico-syntactic patterns of
the kind ”X, Y and other Zs”, as in the phrase ”blue-
jays, robins and other birds”. These types of mod-
els have been used for hyponym discovery (Hearst,
</bodyText>
<figure confidence="0.9944824375">
100
90
80
70
60
50
40
RAND
BASE
Boost S
NNS
NB
Boost_M
1 2 3 4 5
30
error
</figure>
<bodyText confidence="0.958225666666667">
1992; Roark and Charniak, 1998), meronym dis-
covery (Berland and Charniak, 1999), and hierar-
chy building (Caraballo, 1999). These methods are
very interesting but of limited applicability, because
nouns that do not appear in known lexico-syntactic
patterns cannot be learned.
</bodyText>
<sectionHeader confidence="0.998808" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999989933333333">
All the approaches cited above focus on some aspect
of the problem of lexical acquisition. What we learn
from them is that information about the meaning of
words comes in very different forms. One thing that
needs to be investigated is the design of better sets
of features that encode the information that has been
found useful in these studies. For example, it is
known from work in word sense disambiguation that
conditioning on distance and syntactic relations can
be very helpful. For a model for lexical acquisition
to be successful it must be able to combine as many
sources of information as possible. We found that
boosting is a viable method in this respect. In par-
ticular, in this paper we showed that morphology is
one very useful source of information, independent
of frequency, that can be easily encoded in simple
features.
A more general finding was that inserting new
words into a dictionary is a hard task. For these
classifiers to become useful in practice, much bet-
ter accuracy is needed. This raises the question of
the scalability of machine learning methods to mul-
ticlass classification for very large lexicons. Our im-
pression on this is that directly attempting classifi-
cation on tens of thousands of classes is not a viable
approach. However, there is a great deal of informa-
tion in the structure of a lexicon like Wordnet. Our
guess is that the ability to make use of structural in-
formation will be key in successful approaches to
this problem.
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999409333333333">
M. Berland and E. Charniak. 1999. Finding parts in very large
corpora. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics.
S. Caraballo. 1999. Automatic acquisition of a hypernym-
labeled noun hierarchy from text. In Proceedings of the 37th
Annual Meeting of the Association for Computational Lin-
guistics.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proceedings of the Joint SIG-
DAT Conference on Empirical Methods in Natural Language
Processing and Very Large Corpora.
M. Collins. 2000. Discriminative reranking for natural lan-
guage parsing. In Proceedings of the 17th ICML.
C. Fellbaum. 1998. WordNet: An Electronic Lexical Database.
MIT Press, Cambridge, MA.
R. Granger. 1977. Foul-up: A program that figures out mean-
ings of words from context. In Proceedings of the Fifth In-
ternational Joint Conference on Artificial Intelligence.
P.M. Hastings and S.L. Lytinen. 1994. The ups and downs of
lexical acquisition. In AAAI-94.
M. Hearst. 1992. Automatic acquisition of hyponyms from
large text corpora. In Proceedings of the 14th International
Conference on Computational Linguistics.
P. Jacobs and U. Zernik. 1988. Acquiring lexical knowledge
from text: A case study. In AAAI-88.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller.
1990. Introduction to Wordnet: An on-line lexical database.
International Journal ofLexicography, 3(4).
A. Ratnaparkhi. 1996. A maximum entropy model for part-of-
speech tagging. In Proceedings of the First Empirical Meth-
ods in Natural Language Processing Conference.
E. Riloff. 1996. An empirical study of automated dictionary
construction for information extraction in three domains. Ar-
tificialIntelligence, 85.
B. Roark and E. Charniak. 1998. Noun-phrase co-occurrence
statistics for semi-automatic semantic lexicon construction.
In Proceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th International
Conference on Computational Linguistics.
R. E. Schapire and Y. Singer. 1998. Improved boosting algo-
rithms using confidence-rated predictions. In Proceedings of
the Eleventh Annual Conference on Computational Learning
Theory.
R. E. Schapire and Y. Singer. 2000. Boostexter: A boosting-
based system for text categorization. Machine Learning, 39.
M. Stevenson and Y. Wilks. 2001. The interaction of knowl-
edge sources in word sense disambiguation. Computational
Linguistics, 27.
Y. Yang. 1999. An evaluation of statistical approaches to text
categorization. Information Retrieval, 1.
D. Yarowsky. 1995. Unsupervised word sense disambiguation
rivaling supervised methods. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational Linguis-
tics.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.916505">
<title confidence="0.999606">Boosting automatic lexical acquisition with morphological information</title>
<author confidence="0.997977">Massimiliano</author>
<affiliation confidence="0.987854">Department of Cognitive and Linguistic Brown</affiliation>
<address confidence="0.986455">Providence, RI, USA</address>
<email confidence="0.969464">massimilianociaramita@brown.edu</email>
<abstract confidence="0.9987745">In this paper we investigate the impact of morphological features on the task of automatically extending a dictionary. We approach the problem as a pattern classification task and compare the performance of several models in classifying nouns that are unknown to a broad coverage dictionary. We used a boosting classifier to compare the performance of models that use different sets of features. We show how adding simple morphological features to a model greatly improves the classification performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Berland</author>
<author>E Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="28323" citStr="Berland and Charniak, 1999" startWordPosition="4739" endWordPosition="4742">ed. In WSD the possible classes for a word are its possible senses; in named entity recognition or IE the number of classes is limited to the fixed (usually small) number the task focuses on. Other kinds of models that have been studied in the context of lexical acquisition are those based on lexico-syntactic patterns of the kind ”X, Y and other Zs”, as in the phrase ”bluejays, robins and other birds”. These types of models have been used for hyponym discovery (Hearst, 100 90 80 70 60 50 40 RAND BASE Boost S NNS NB Boost_M 1 2 3 4 5 30 error 1992; Roark and Charniak, 1998), meronym discovery (Berland and Charniak, 1999), and hierarchy building (Caraballo, 1999). These methods are very interesting but of limited applicability, because nouns that do not appear in known lexico-syntactic patterns cannot be learned. 7 Conclusion All the approaches cited above focus on some aspect of the problem of lexical acquisition. What we learn from them is that information about the meaning of words comes in very different forms. One thing that needs to be investigated is the design of better sets of features that encode the information that has been found useful in these studies. For example, it is known from work in word s</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>M. Berland and E. Charniak. 1999. Finding parts in very large corpora. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Caraballo</author>
</authors>
<title>Automatic acquisition of a hypernymlabeled noun hierarchy from text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="28365" citStr="Caraballo, 1999" startWordPosition="4747" endWordPosition="4748">ssible senses; in named entity recognition or IE the number of classes is limited to the fixed (usually small) number the task focuses on. Other kinds of models that have been studied in the context of lexical acquisition are those based on lexico-syntactic patterns of the kind ”X, Y and other Zs”, as in the phrase ”bluejays, robins and other birds”. These types of models have been used for hyponym discovery (Hearst, 100 90 80 70 60 50 40 RAND BASE Boost S NNS NB Boost_M 1 2 3 4 5 30 error 1992; Roark and Charniak, 1998), meronym discovery (Berland and Charniak, 1999), and hierarchy building (Caraballo, 1999). These methods are very interesting but of limited applicability, because nouns that do not appear in known lexico-syntactic patterns cannot be learned. 7 Conclusion All the approaches cited above focus on some aspect of the problem of lexical acquisition. What we learn from them is that information about the meaning of words comes in very different forms. One thing that needs to be investigated is the design of better sets of features that encode the information that has been found useful in these studies. For example, it is known from work in word sense disambiguation that conditioning on d</context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>S. Caraballo. 1999. Automatic acquisition of a hypernymlabeled noun hierarchy from text. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<contexts>
<context position="8475" citStr="Collins and Singer, 1999" startWordPosition="1350" endWordPosition="1353">f the form . In particular we are interested in learning functions from data, i.e., a training set of pairs and , such that there will be a small probability of error when we apply the classifier to unknown pairs (new nouns). Each class is described by a vector of features. A class of features that intuitively carry semantic information are collocations, i.e., words that co-occur with the nouns of interest in a corpus. Collocations have been widely used for tasks such as word sense disambiguation (WSD) (Yarowsky, 1995), information extraction (IE) (Riloff, 1996), and named-entity recognition (Collins and Singer, 1999). The choice of collocations can be conditioned in many ways: according to syntactic relations with the target word, syntactic category, distance from the target, and so on. We use a very simple set of collocations: each word that appears within positions from a nounis a feature. Each occurrence, or token, of, , is then characterized by a vector of feature counts . The vector representation of the noun typeis the sum of all the vectors representing the contexts in which it occurs. Overall the vector representation for each class in the dictionary is the sum of the vectors of all nouns that are</context>
<context position="27521" citStr="Collins and Singer, 1999" startWordPosition="4592" endWordPosition="4595"> is one of these early models that tries to deterministically maximize the expectations built into its knowledge base. Jacobs and Zernik (1988) introduced the idea of using morphological information, together with other sources, to guess the meaning of unknown words. Hastings and Lytinen (1994) investigated attacking the lexical acquisition problem with a system that relies mainly on taxonomic information. In the last decade or so research on lexical semantics has focused more on sub-problems like word sense disambiguation (Yarowsky, 1995; Stevenson and Wilks, 2001), named entity recognition (Collins and Singer, 1999), and vocabulary construction for information extraction (Riloff, 1996). All of these can be seen as sub-tasks, because the space of possible classes for each word is restricted. In WSD the possible classes for a word are its possible senses; in named entity recognition or IE the number of classes is limited to the fixed (usually small) number the task focuses on. Other kinds of models that have been studied in the context of lexical acquisition are those based on lexico-syntactic patterns of the kind ”X, Y and other Zs”, as in the phrase ”bluejays, robins and other birds”. These types of mode</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th ICML. C. Fellbaum.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="18789" citStr="Collins (2000)" startWordPosition="3120" endWordPosition="3121">=er; MSSH=letter; CO=yield:NN; CO=seven-day:JJ; ... Figure 3: Sample input to the classifiers, only has access to morphological information. CO stands for the attribute “collocation”. is shown thatis minimized for a particular feature by choosing its predictions as described in equation (8). The weight usually associated with the weak classifier (see equation (2)) here is simply set to 1. If the value in (8) is plugged into (4),becomes (9) Therefore to minimizeat each round we choose the feature for which this value is the smallest. Updating these scores is what takes most of the computation, Collins (2000) describes an efficient version of this algorithm. 4.3 Morphological features We investigated two boosting models: , which uses only collocations as features, and , which uses also a very simple set of morphological features. In we used the collocations within a window of , which seemed to be a good value for both the nearest neighbor and the naive Bayes model. However, we didn’t focus on any method for choosing, since we believe that the collocational features we used only approximate more complex ones that need specific investigation. Our main goal was to compare models with and without morp</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>M. Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of the 17th ICML. C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Granger</author>
</authors>
<title>Foul-up: A program that figures out meanings of words from context.</title>
<date>1977</date>
<booktitle>In Proceedings of the Fifth International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="26896" citStr="Granger, 1977" startWordPosition="4500" endWordPosition="4501">oosting. The main advantage with boosting algorithms is the flexibility in managing features of very different nature. Feature combination can be performed naturally with probabilistic models too but it is more complicated. However, this is something worth investigating. 6 Related work Automatic lexical acquisition is a classic problem in AI. It was originally approached in the context of story understanding with the aim of enabling systems to deal with unknown words while processing text or spoken input. These systems would typically rely heavily on script-based knowledge resources. FOUL-UP (Granger, 1977) is one of these early models that tries to deterministically maximize the expectations built into its knowledge base. Jacobs and Zernik (1988) introduced the idea of using morphological information, together with other sources, to guess the meaning of unknown words. Hastings and Lytinen (1994) investigated attacking the lexical acquisition problem with a system that relies mainly on taxonomic information. In the last decade or so research on lexical semantics has focused more on sub-problems like word sense disambiguation (Yarowsky, 1995; Stevenson and Wilks, 2001), named entity recognition (</context>
</contexts>
<marker>Granger, 1977</marker>
<rawString>R. Granger. 1977. Foul-up: A program that figures out meanings of words from context. In Proceedings of the Fifth International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P M Hastings</author>
<author>S L Lytinen</author>
</authors>
<title>The ups and downs of lexical acquisition.</title>
<date>1994</date>
<booktitle>In AAAI-94.</booktitle>
<contexts>
<context position="27191" citStr="Hastings and Lytinen (1994)" startWordPosition="4542" endWordPosition="4545">work Automatic lexical acquisition is a classic problem in AI. It was originally approached in the context of story understanding with the aim of enabling systems to deal with unknown words while processing text or spoken input. These systems would typically rely heavily on script-based knowledge resources. FOUL-UP (Granger, 1977) is one of these early models that tries to deterministically maximize the expectations built into its knowledge base. Jacobs and Zernik (1988) introduced the idea of using morphological information, together with other sources, to guess the meaning of unknown words. Hastings and Lytinen (1994) investigated attacking the lexical acquisition problem with a system that relies mainly on taxonomic information. In the last decade or so research on lexical semantics has focused more on sub-problems like word sense disambiguation (Yarowsky, 1995; Stevenson and Wilks, 2001), named entity recognition (Collins and Singer, 1999), and vocabulary construction for information extraction (Riloff, 1996). All of these can be seen as sub-tasks, because the space of possible classes for each word is restricted. In WSD the possible classes for a word are its possible senses; in named entity recognition</context>
</contexts>
<marker>Hastings, Lytinen, 1994</marker>
<rawString>P.M. Hastings and S.L. Lytinen. 1994. The ups and downs of lexical acquisition. In AAAI-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics.</booktitle>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jacobs</author>
<author>U Zernik</author>
</authors>
<title>Acquiring lexical knowledge from text: A case study.</title>
<date>1988</date>
<booktitle>In AAAI-88.</booktitle>
<contexts>
<context position="27039" citStr="Jacobs and Zernik (1988)" startWordPosition="4520" endWordPosition="4523">ation can be performed naturally with probabilistic models too but it is more complicated. However, this is something worth investigating. 6 Related work Automatic lexical acquisition is a classic problem in AI. It was originally approached in the context of story understanding with the aim of enabling systems to deal with unknown words while processing text or spoken input. These systems would typically rely heavily on script-based knowledge resources. FOUL-UP (Granger, 1977) is one of these early models that tries to deterministically maximize the expectations built into its knowledge base. Jacobs and Zernik (1988) introduced the idea of using morphological information, together with other sources, to guess the meaning of unknown words. Hastings and Lytinen (1994) investigated attacking the lexical acquisition problem with a system that relies mainly on taxonomic information. In the last decade or so research on lexical semantics has focused more on sub-problems like word sense disambiguation (Yarowsky, 1995; Stevenson and Wilks, 2001), named entity recognition (Collins and Singer, 1999), and vocabulary construction for information extraction (Riloff, 1996). All of these can be seen as sub-tasks, becaus</context>
</contexts>
<marker>Jacobs, Zernik, 1988</marker>
<rawString>P. Jacobs and U. Zernik. 1988. Acquiring lexical knowledge from text: A case study. In AAAI-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Introduction to Wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal ofLexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="4470" citStr="Miller et al., 1990" startWordPosition="690" endWordPosition="693">lassification model. The outline of the paper is as follows. In section 2 we introduce the dictionary we used for our tests, a simplified version of Wordnet. In section 3 we describe more formally the task, a few simple models, and the test methods. In section 4 we describe the boosting model and the set of morphological features. In section 5 we summarize the results of our experiments. In section 6 we describe related work, and then in section 7 we present our conclusions. 2 MiniWordnet Ideally the lexicon we would like to extend is a broad coverage machine readable dictionary like Wordnet (Miller et al., 1990; Fellbaum, 1998). The problem with trying to directly use Wordnet is that it contains too many classes (synsets), around 70 thousand. Learning in such a huge class space can be extremely problematic, and intuitively it is not the best way to start on a task that hasn’t been much explored3. Instead, we manually developed a smaller lexicon dubbed MiniWordnet, which is derived from Wordnet version 1.6. The reduced lexicon has the same coverage (about 95 thousand noun types) but only a fraction of the classes. In this paper we considered only nouns and the noun database. The goal was to reduce th</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1990. Introduction to Wordnet: An on-line lexical database. International Journal ofLexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-ofspeech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the First Empirical Methods in Natural Language Processing Conference.</booktitle>
<contexts>
<context position="24520" citStr="Ratnaparkhi (1996)" startWordPosition="4108" endWordPosition="4109">itiveness, credibility or consulting firm. It makes also many mistakes, for example on conversation, controversy and insurance company. One problem that we noticed is that there are several cases of nouns that have intuitively meaningful suffixes or prefixes that are not present in our hand-coded lists. A possible solution to his problem might be the use of more general morphological rules like those used in part-of-speech tagging models (e.g., t error 90 80 70 60 50 40 30 20 0 500 1000 1500 2000 2500 3000 3500 BoostS BoostM 100 95 90 error 85 80 level Figure 6: Comparison of all models for . Ratnaparkhi (1996)), where all suffixes up to a certain length are included. We observed also cases of recurrent confusion between classes. For example between ACT and ABSTRACTION (or their subordinates), e.g., for the noun modernization, possibly because the suffix is common in both cases. Another measure of the importance of morphological features is the ratio of their use with respect to that of collocations. In the first 100 rounds of , at level 5, 77% of the features selected are morphological, 69% in the first 200 rounds. As Figures 4 and 5 show these early rounds are usually the ones in which most of the</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-ofspeech tagging. In Proceedings of the First Empirical Methods in Natural Language Processing Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>An empirical study of automated dictionary construction for information extraction in three domains.</title>
<date>1996</date>
<journal>ArtificialIntelligence,</journal>
<volume>85</volume>
<contexts>
<context position="8418" citStr="Riloff, 1996" startWordPosition="1345" endWordPosition="1346">ector of features we want to find functions of the form . In particular we are interested in learning functions from data, i.e., a training set of pairs and , such that there will be a small probability of error when we apply the classifier to unknown pairs (new nouns). Each class is described by a vector of features. A class of features that intuitively carry semantic information are collocations, i.e., words that co-occur with the nouns of interest in a corpus. Collocations have been widely used for tasks such as word sense disambiguation (WSD) (Yarowsky, 1995), information extraction (IE) (Riloff, 1996), and named-entity recognition (Collins and Singer, 1999). The choice of collocations can be conditioned in many ways: according to syntactic relations with the target word, syntactic category, distance from the target, and so on. We use a very simple set of collocations: each word that appears within positions from a nounis a feature. Each occurrence, or token, of, , is then characterized by a vector of feature counts . The vector representation of the noun typeis the sum of all the vectors representing the contexts in which it occurs. Overall the vector representation for each class in the d</context>
<context position="27592" citStr="Riloff, 1996" startWordPosition="4602" endWordPosition="4603">ions built into its knowledge base. Jacobs and Zernik (1988) introduced the idea of using morphological information, together with other sources, to guess the meaning of unknown words. Hastings and Lytinen (1994) investigated attacking the lexical acquisition problem with a system that relies mainly on taxonomic information. In the last decade or so research on lexical semantics has focused more on sub-problems like word sense disambiguation (Yarowsky, 1995; Stevenson and Wilks, 2001), named entity recognition (Collins and Singer, 1999), and vocabulary construction for information extraction (Riloff, 1996). All of these can be seen as sub-tasks, because the space of possible classes for each word is restricted. In WSD the possible classes for a word are its possible senses; in named entity recognition or IE the number of classes is limited to the fixed (usually small) number the task focuses on. Other kinds of models that have been studied in the context of lexical acquisition are those based on lexico-syntactic patterns of the kind ”X, Y and other Zs”, as in the phrase ”bluejays, robins and other birds”. These types of models have been used for hyponym discovery (Hearst, 100 90 80 70 60 50 40 </context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. An empirical study of automated dictionary construction for information extraction in three domains. ArtificialIntelligence, 85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>E Charniak</author>
</authors>
<title>Noun-phrase co-occurrence statistics for semi-automatic semantic lexicon construction.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="28275" citStr="Roark and Charniak, 1998" startWordPosition="4732" endWordPosition="4735"> of possible classes for each word is restricted. In WSD the possible classes for a word are its possible senses; in named entity recognition or IE the number of classes is limited to the fixed (usually small) number the task focuses on. Other kinds of models that have been studied in the context of lexical acquisition are those based on lexico-syntactic patterns of the kind ”X, Y and other Zs”, as in the phrase ”bluejays, robins and other birds”. These types of models have been used for hyponym discovery (Hearst, 100 90 80 70 60 50 40 RAND BASE Boost S NNS NB Boost_M 1 2 3 4 5 30 error 1992; Roark and Charniak, 1998), meronym discovery (Berland and Charniak, 1999), and hierarchy building (Caraballo, 1999). These methods are very interesting but of limited applicability, because nouns that do not appear in known lexico-syntactic patterns cannot be learned. 7 Conclusion All the approaches cited above focus on some aspect of the problem of lexical acquisition. What we learn from them is that information about the meaning of words comes in very different forms. One thing that needs to be investigated is the design of better sets of features that encode the information that has been found useful in these studi</context>
</contexts>
<marker>Roark, Charniak, 1998</marker>
<rawString>B. Roark and E. Charniak. 1998. Noun-phrase co-occurrence statistics for semi-automatic semantic lexicon construction. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Improved boosting algorithms using confidence-rated predictions.</title>
<date>1998</date>
<booktitle>In Proceedings of the Eleventh Annual Conference on Computational Learning Theory.</booktitle>
<contexts>
<context position="15097" citStr="Schapire and Singer, 1998" startWordPosition="2514" endWordPosition="2517">ng the training data used at so that the examples that were misclassified at are given more weight while less weight is given to the correctly classified examples. At each 6The learner is called weak because it is required to classify examples better than at random only by an arbitrarily small quantity. 1 2 3 4 5 6 7 8 9 10 45 80 75 70 65 60 55 50 NNfreq NNtfidf NB Figure 2: Error of the ,and models for .at level 1 error iteration a weak learner is trained and added to the ensemble with weight . The final ensemble has the form (3) In the most popular version of a boosting algorithm, AdaBoost (Schapire and Singer, 1998), at each iteration a classifier is trained to minimize the exponential loss on the weighted training set. The exponential loss is an upper bound on the zero-one loss. AdaBoost minimizes the exponential loss on the training set so that incorrect classification and disagreement between members of the ensemble are penalized. Boosting has been successfully applied to several problems. Among these is text categorization (Schapire and Singer, 2000), which bears similarities with word classification. For our experiments we used AdaBoost.MH with realvalued predictions and abstaining, a version of boo</context>
<context position="17930" citStr="Schapire and Singer (1998)" startWordPosition="2993" endWordPosition="2996">iction. Then for each training instance a simple check for the presence or absence of this feature is performed. For example, a possible collocation feature is eat:VB, and the corresponding prediction is “if eat:VB appears in the context of a noun, predict that the noun belongs to the class FOOD and doesn’t belong to classes PLANT, BUSINESS,...”. A weak learner is defined as follows: if (7) if The prediction is computed as follows: (8) ( ) is the sum of the weights of noun-label pairs, from the distribution, where the feature appears and the label is correct (wrong); is a smoothing factor. In Schapire and Singer (1998) it W=August; PL=0; MU=1; CO=’:POS; CO=passenger:NN; CO=traffic:NN; ... W=punishment; PL=1; MU=0; MS=ment; MS=ishment; CO=in:IN; CO=to:TO; ... W=vice president; PL=0; MU=0; MSHH=president; CO=say:VB; CO=chief:JJ; ... W=newsletter; PL=0; MU=0; MS=er; MSSH=letter; CO=yield:NN; CO=seven-day:JJ; ... Figure 3: Sample input to the classifiers, only has access to morphological information. CO stands for the attribute “collocation”. is shown thatis minimized for a particular feature by choosing its predictions as described in equation (8). The weight usually associated with the weak classifier (see eq</context>
</contexts>
<marker>Schapire, Singer, 1998</marker>
<rawString>R. E. Schapire and Y. Singer. 1998. Improved boosting algorithms using confidence-rated predictions. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Boostexter: A boostingbased system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<volume>39</volume>
<contexts>
<context position="2751" citStr="Schapire and Singer, 2000" startWordPosition="421" endWordPosition="424">hat the task is difficult. We developed several models, based on nearest neighbor (NN), naive Bayes (NB) and boosting classifiers. Unfortunately, the error rate of these models is much higher than what is found in text categorization tasks2 with comparable numbers of classes. Secondly, it seems obvious that information that is potentially useful for word classification can be of very diverse types, e.g., semantic and syntactic, morphological and topical. Therefore methods that allow flexible feature combination and selection are desirable. We experimented with a multiclass boosting algorithm (Schapire and Singer, 2000), which proved successful in this respect. In this context boosting combines two sources of information: words co-occurring near the new word, which we refer to as collocations, and morphological properties of the new word. This classifier shows improved performance over models that use only collocations. In particular, we found that even rudimentary morphological information greatly im2Text categorization is the task of associating documents with topic labels (POLITICS, SPORT, ...) and it bears similarities with semantic classification tasks such as word sense disambiguation, information extr</context>
<context position="15544" citStr="Schapire and Singer, 2000" startWordPosition="2586" endWordPosition="2589">er is trained and added to the ensemble with weight . The final ensemble has the form (3) In the most popular version of a boosting algorithm, AdaBoost (Schapire and Singer, 1998), at each iteration a classifier is trained to minimize the exponential loss on the weighted training set. The exponential loss is an upper bound on the zero-one loss. AdaBoost minimizes the exponential loss on the training set so that incorrect classification and disagreement between members of the ensemble are penalized. Boosting has been successfully applied to several problems. Among these is text categorization (Schapire and Singer, 2000), which bears similarities with word classification. For our experiments we used AdaBoost.MH with realvalued predictions and abstaining, a version of boosting for multiclass classification described in Schapire and Singer (2000). This version of AdaBoost minimizes a loss function that is an upper bound on the Hamming distance between the weak learners’ predictions and the real labels, i.e., the number of label mismatches (Schapire and Singer, 1998). This upper bound is the product . The function is 1 ifis the correct label for the training exampleand is -1 otherwise; is the total number of cla</context>
<context position="21112" citStr="Schapire and Singer, 2000" startWordPosition="3524" endWordPosition="3527">h line is a training instance; the attribute W refers to the lexical form of the noun and was ignored by the classifier. 4.4 Stopping criterion One issue when using iterative procedures is deciding when to stop. We used the simplest procedure of fixing in advance the number of iterations. We noticed that the test error drops until it reaches a point at which it seems not to improve anymore. Then the error oscillates around the same value even for thousands of iterations, without apparent overtraining. A similar behavior is observable in some of the results on text categorization presented in (Schapire and Singer, 2000). We cannot say that overtraining is not a potential danger in multiclass boosting models. However, for our experiments, in which the main goal is to investigate the impact of a particular class of features, we could limit the number of 7The feature lists are available together with the MiniWordnet files. Training error Figure 4: Training error at level 4. Test error BoostS BoostM 75 0 500 1000 1500 2000 2500 3000 3500 t Figure 5: Test error at level 4. iterations to a fixed value for all models. We chose this maximum number of iterations to be 3500; this allowed us to perform the experiments </context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>R. E. Schapire and Y. Singer. 2000. Boostexter: A boostingbased system for text categorization. Machine Learning, 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>Y Wilks</author>
</authors>
<title>The interaction of knowledge sources in word sense disambiguation.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<contexts>
<context position="27468" citStr="Stevenson and Wilks, 2001" startWordPosition="4585" endWordPosition="4588">ipt-based knowledge resources. FOUL-UP (Granger, 1977) is one of these early models that tries to deterministically maximize the expectations built into its knowledge base. Jacobs and Zernik (1988) introduced the idea of using morphological information, together with other sources, to guess the meaning of unknown words. Hastings and Lytinen (1994) investigated attacking the lexical acquisition problem with a system that relies mainly on taxonomic information. In the last decade or so research on lexical semantics has focused more on sub-problems like word sense disambiguation (Yarowsky, 1995; Stevenson and Wilks, 2001), named entity recognition (Collins and Singer, 1999), and vocabulary construction for information extraction (Riloff, 1996). All of these can be seen as sub-tasks, because the space of possible classes for each word is restricted. In WSD the possible classes for a word are its possible senses; in named entity recognition or IE the number of classes is limited to the fixed (usually small) number the task focuses on. Other kinds of models that have been studied in the context of lexical acquisition are those based on lexico-syntactic patterns of the kind ”X, Y and other Zs”, as in the phrase ”b</context>
</contexts>
<marker>Stevenson, Wilks, 2001</marker>
<rawString>M. Stevenson and Y. Wilks. 2001. The interaction of knowledge sources in word sense disambiguation. Computational Linguistics, 27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
</authors>
<title>An evaluation of statistical approaches to text categorization.</title>
<date>1999</date>
<journal>Information Retrieval,</journal>
<volume>1</volume>
<contexts>
<context position="5553" citStr="Yang, 1999" startWordPosition="868" endWordPosition="869">pes) but only a fraction of the classes. In this paper we considered only nouns and the noun database. The goal was to reduce the number of classes to about one hundred4 of roughly comparable taxonomical generality and consistency, while maintaining a little bit of hierarchical structure. 3Preliminary experiments confirmed this; classification is computationally expensive, performance is low, and it is very hard to obtain even very small improvements when the full database is used. 4A magnitude comparable to the class space of well studied text categorization data sets like the Reuters-21578 (Yang, 1999). The output of the manual coding is a set of 106 classes that are the result of merging hundreds of synsets. A few random examples of these classes are PERSON, PLANT, FLUID, LOCATION, ACTION, and BUSINESS. One way to look at this set of classes is from the perspective of named-entity recognition tasks, where there are a few classes of a similar level of generality, e.g, PERSON, LOCATION, ORGANIZATION, OTHER. The difference here is that the classes are intended to capture all possible taxonomic distinctions collapsed into the OTHER class above. In addition to the 106 leaves we also kept a set </context>
</contexts>
<marker>Yang, 1999</marker>
<rawString>Y. Yang. 1999. An evaluation of statistical approaches to text categorization. Information Retrieval, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8374" citStr="Yarowsky, 1995" startWordPosition="1339" endWordPosition="1340">f classes defined by the dictionary. Given a vector of features we want to find functions of the form . In particular we are interested in learning functions from data, i.e., a training set of pairs and , such that there will be a small probability of error when we apply the classifier to unknown pairs (new nouns). Each class is described by a vector of features. A class of features that intuitively carry semantic information are collocations, i.e., words that co-occur with the nouns of interest in a corpus. Collocations have been widely used for tasks such as word sense disambiguation (WSD) (Yarowsky, 1995), information extraction (IE) (Riloff, 1996), and named-entity recognition (Collins and Singer, 1999). The choice of collocations can be conditioned in many ways: according to syntactic relations with the target word, syntactic category, distance from the target, and so on. We use a very simple set of collocations: each word that appears within positions from a nounis a feature. Each occurrence, or token, of, , is then characterized by a vector of feature counts . The vector representation of the noun typeis the sum of all the vectors representing the contexts in which it occurs. Overall the v</context>
<context position="27440" citStr="Yarowsky, 1995" startWordPosition="4583" endWordPosition="4584">y heavily on script-based knowledge resources. FOUL-UP (Granger, 1977) is one of these early models that tries to deterministically maximize the expectations built into its knowledge base. Jacobs and Zernik (1988) introduced the idea of using morphological information, together with other sources, to guess the meaning of unknown words. Hastings and Lytinen (1994) investigated attacking the lexical acquisition problem with a system that relies mainly on taxonomic information. In the last decade or so research on lexical semantics has focused more on sub-problems like word sense disambiguation (Yarowsky, 1995; Stevenson and Wilks, 2001), named entity recognition (Collins and Singer, 1999), and vocabulary construction for information extraction (Riloff, 1996). All of these can be seen as sub-tasks, because the space of possible classes for each word is restricted. In WSD the possible classes for a word are its possible senses; in named entity recognition or IE the number of classes is limited to the fixed (usually small) number the task focuses on. Other kinds of models that have been studied in the context of lexical acquisition are those based on lexico-syntactic patterns of the kind ”X, Y and ot</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>