<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9988035">
Lattice-based Minimum Error Rate Training
for Statistical Machine Translation
</title>
<author confidence="0.998566">
Wolfgang Macherey Franz Josef Och Ignacio Thayer Jakob Uszkoreit
</author>
<affiliation confidence="0.97307">
Google Inc.
</affiliation>
<address confidence="0.7442515">
1600 Amphitheatre Parkway
Mountain View, CA 94043, USA
</address>
<email confidence="0.998612">
{wmach,och,thayer,uszkoreit}@google.com
</email>
<sectionHeader confidence="0.998529" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998413366666667">
Minimum Error Rate Training (MERT) is an
effective means to estimate the feature func-
tion weights of a linear model such that an
automated evaluation criterion for measuring
system performance can directly be optimized
in training. To accomplish this, the training
procedure determines for each feature func-
tion its exact error surface on a given set of
candidate translations. The feature function
weights are then adjusted by traversing the
error surface combined over all sentences and
picking those values for which the resulting
error count reaches a minimum. Typically,
candidates in MERT are represented as N-
best lists which contain the N most probable
translation hypotheses produced by a decoder.
In this paper, we present a novel algorithm that
allows for efficiently constructing and repre-
senting the exact error surface of all trans-
lations that are encoded in a phrase lattice.
Compared to N-best MERT, the number of
candidate translations thus taken into account
increases by several orders of magnitudes.
The proposed method is used to train the
feature function weights of a phrase-based
statistical machine translation system. Experi-
ments conducted on the NIST 2008 translation
tasks show significant runtime improvements
and moderate BLEU score gains over N-best
MERT.
</bodyText>
<sectionHeader confidence="0.999624" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.97516803125">
Many statistical methods in natural language pro-
cessing aim at minimizing the probability of sen-
tence errors. In practice, however, system quality
is often measured based on error metrics that assign
non-uniform costs to classification errors and thus
go far beyond counting the number of wrong de-
cisions. Examples are the mean average precision
for ranked retrieval, the F-measure for parsing, and
the BLEU score for statistical machine transla-
tion (SMT). A class of training criteria that provides
a tighter connection between the decision rule and
the final error metric is known as Minimum Error
Rate Training (MERT) and has been suggested for
SMT in (Och, 2003).
MERT aims at estimating the model parameters
such that the decision under the zero-one loss func-
tion maximizes some end-to-end performance mea-
sure on a development corpus. In combination with
log-linear models, the training procedure allows for
a direct optimization of the unsmoothed error count.
The criterion can be derived from Bayes’ decision
rule as follows: Let ff1, ..., fi denote a source
sentence (’French’) which is to be translated into a
target sentence (’English’) ee1, ..., eI. Under
the zero-one loss function, the translation which
maximizes the a posteriori probability is chosen:
earg max Prpe|fq( (1)
e
Since the true posterior distribution is unknown,
Prpe|fqis modeled via a log-linear translation model
which combines some feature functions hmpe, fq )
with feature function weights am, m~1, ..., M:
</bodyText>
<equation confidence="0.803226666666667">
Prpe|fpxm �e|f
exp
e1exp m1 amhmpe1, f
</equation>
<bodyText confidence="0.999923333333333">
The feature function weights are the parameters of
the model, and the objective of the MERT criterion
is to find a parameter set aM that minimizes the error
count on a representative set of training sentences.
More precisely, let f denote the source sentences
of a training corpus with given reference translations
</bodyText>
<equation confidence="0.4772585">
� m~1 amhmpe, f
(2)
</equation>
<page confidence="0.975226">
725
</page>
<note confidence="0.834852666666667">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 725–734,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
rS
</note>
<bodyText confidence="0.9475862">
vidual sentences, i.e., EprS
1 , and let Cs�tes,1, ..., es,Kudenote a set of K
candidate translations. Assuming that the corpus-
based error count for some translations eS 1 is addi-
tively decomposable into the error counts of the indi-
</bodyText>
<equation confidence="0.988059666666667">
1q�°S
1 , eS s~1 Eprs, esq,
the MERT criterion is given as:
S¸
Ers, 6fs; λMi (3)�
s1
K¸
k�1 Eprs, es,kqδ��epfs; λM 1q, es,k�+
with
M¸
λmhmpe,fsq+(4)
M1
</equation>
<bodyText confidence="0.999938291666666">
In (Och, 2003), it was shown that linear models can
effectively be trained under the MERT criterion us-
ing a special line optimization algorithm. This line
optimization determines for each feature function
hm and sentence fs the exact error surface on a set
of candidate translations Cs. The feature function
weights are then adjusted by traversing the error
surface combined over all sentences in the training
corpus and moving the weights to a point where the
resulting error reaches a minimum.
Candidate translations in MERT are typically rep-
resented as N-best lists which contain the N most
probable translation hypotheses. A downside of this
approach is, however, that N-best lists can only
capture a very small fraction of the search space.
As a consequence, the line optimization algorithm
needs to repeatedly translate the development corpus
and enlarge the candidate repositories with newly
found hypotheses in order to avoid overfitting on Cs
and preventing the optimization procedure from
stopping in a poor local optimum.
In this paper, we present a novel algorithm that
allows for efficiently constructing and representing
the unsmoothed error surface for all translations
that are encoded in a phrase lattice. The number
of candidate translations thus taken into account
increases by several orders of magnitudes compared
to N-best MERT. Lattice MERT is shown to yield
significantly faster convergence rates while it ex-
plores a much larger space of candidate translations
which is exponential in the lattice size. Despite
this vast search space, we show that the suggested
algorithm is always efficient in both running time
and memory.
The remainder of this paper is organized as fol-
lows. Section 2 briefly reviews N-best MERT and
introduces some basic concepts that are used in
order to develop the line optimization algorithm for
phrase lattices in Section 3. Section 4 presents an
upper bound on the complexity of the unsmoothed
error surface for the translation hypotheses repre-
sented in a phrase lattice. This upper bound is
used to prove the space and runtime efficiency of
the suggested algorithm. Section 5 lists some best
practices for MERT. Section 6 discusses related
work. Section 7 reports on experiments conducted
on the NIST 2008 translation tasks. The paper
concludes with a summary in Section 8.
</bodyText>
<sectionHeader confidence="0.832405" genericHeader="introduction">
2 Minimum Error Rate Training on
N-best Lists
</sectionHeader>
<bodyText confidence="0.9976094">
The goal of MERT is to find a weights set that
minimizes the unsmoothed error count on a rep-
resentative training corpus (cf. Eq. (3)). This
can be accomplished through a sequence of line
minimizations along some vector directionstdM
</bodyText>
<equation confidence="0.540818">
candidate translations Cs�te1, ..., eKualong the
1u.
</equation>
<bodyText confidence="0.967128">
Starting from an initial point λM 1 , computing the
most probable sentence hypothesis out of a set of K
line λM1rydM1 results in the following optimiza-
tion problem (Och, 2003):
</bodyText>
<equation confidence="0.96382575">
efs; ryarg max
ePCs!p λM1,ydM1qJ�hM1pe, fsq)�
arg max tE
ePCs&amp;quot;¸
m
ape,fsq� bpe,fsq��
arg max
ePCs ape, fs-ybpe, fs } (5)
</equation>
<bodyText confidence="0.99665375">
Hence, the total scorep~qfor any candidate trans-
lation corresponds to a line in the plane with γ as
the independent variable. For any particular choice
of γ, the decoder seeks that translation which yields
the largest score and therefore corresponds to the
topmost line segment.
Overall, the candidate repository Cs defines K
lines where each line may be divided into at most
K line segments due to possible intersections with
the other K1 lines. The sequence of the topmost
line segments constitute the upper envelope which
is the pointwise maximum over all lines induced by
Cs. The upper envelope is a convex hull and can
be inscribed with a convex polygon whose edges
are the segments of a piecewise linear function in γ
(Papineni, 1999; Och, 2003):
</bodyText>
<equation confidence="0.447541642857143">
Envpfq~max
ePC ape, fq-ybpe, fq: γPR((6)
�
�λM1�arg min
λM
S¸
�
arg min
λM
s1
6fs; λM1q�arg max
e#
λmhmpe, fs ry dmhmpe, fs
m
</equation>
<page confidence="0.624958">
726
</page>
<figure confidence="0.779858">
e8
0 Y
</figure>
<figureCaption confidence="0.9916704">
Figure 1: The upper envelope (bold, red curve) for a set
of lines is the convex hull which consists of the topmost
line segments. Each line corresponds to a candidate
translation and is thus related to a certain error count.
Envelopes can efficiently be computed with Algorithm 1.
</figureCaption>
<bodyText confidence="0.999834620689655">
The importance of the upper envelope is that it pro-
vides a compact encoding of all possible outcomes
that a rescoring of Cs may yield if the parameter
set λM1 is moved along the chosen direction. Once
the upper envelope has been determined, we can
project its constituent line segments onto the error
counts of the corresponding candidate translations
(cf. Figure 1). This projection is independent of
how the envelope is generated and can therefore be
applied to any set of line segments1.
An effective means to compute the upper enve-
lope is a sweep line algorithm which is often used in
computational geometry to determine the intersec-
tion points of a sequence of lines or line segments
(Bentley and Ottmann, 1979). The idea is to shift
(“sweep”) a vertical ray from~8to8over the
plane while keeping track of those points where two
or more lines intersect. Since the upper envelope
is fully specified by the topmost line segments, it
suffices to store the following components for each
line object ℓ: the x-intercept ℓ.x with the left-
adjacent line, the slope ℓ.m, and the y-intercept ℓ.y;
a fourth component, ℓ.t, is used to store the candi-
date translation. Algorithm 1 shows the pseudo code
for a sweep line algorithm which reduces an input
array a[0..K-1] consisting of the K line objects
of the candidate repository Cs to its upper envelope.
By construction, the upper envelope consists of at
most K line segments. The endpoints of each line
</bodyText>
<footnote confidence="0.945569666666667">
1 For lattice MERT, it will therefore suffice to find an
efficient way to compute the upper envelope over all translations
that are encoded in a phrase graph.
</footnote>
<construct confidence="0.414056333333333">
Algorithm 1 SweepLine
input: array a[0..K-1] containing lines
output: upper envelope of a
</construct>
<equation confidence="0.9818278">
sort(a:m);
j = 0; K = size(a);
for (i = 0; i &lt; K; ++i) {
$ = a[i];
$.x = -8;
</equation>
<construct confidence="0.812215666666667">
if (0 &lt; j) {
if (a[j-1].m == $.m) {
if ($.y &lt;= a[j-1].y) continue;
</construct>
<equation confidence="0.962412928571428">
--j;
}
while (0 &lt; j) {
$.x = ($.y - a[j-1].y)/
(a[j-1].m - $.m);
if (a[j-1].x &lt; $.x) break;
--j;
}
if (0 == j) $.x = -8;
a[j++] = $;
} else a[j++] = $;
}
a.resize(j);
return a;
</equation>
<bodyText confidence="0.999885965517241">
segment define the interval boundaries at which the
decision made by the decoder will change. Hence,
as γ increases from~8to8, we will see that
the most probable translation hypothesis will change
whenever γ passes an intersection point.
Let γis &lt; γ2s &lt; ... &lt; γfsN denote the sequence of
interval boundaries and let AEfs1, AE2s, ..., AEfsN
denote the corresponding sequence of changes in the
error count where AEns is the amount by which the
error count will change if γ is moved from a point in
[Ns1, γns) to a point inr [γns, γns1q). Both sequences
together provide an exhaustive representation of the
unsmoothed error surface for the sentence fs along
the line λM + ry • dM. The error surface for the
whole training corpus is obtained by merging the
interval boundaries (and their corresponding error
counts) over all sentences in the training corpus.
The optimal γ can then be found by traversing the
merged error surface and choosing a point from the
interval where the total error reaches its minimum.
After the parameter update, λM = λM +γopt~ &apos; dM1
the decoder may find new translation hypotheses
which are merged into the candidate repositories if
they are ranked among the top N candidates. The
relation K = N holds therefore only in the first
iteration. From the second iteration on, K is usually
larger than N. The sequence of line optimizations
and decodings is repeated until (1) the candidate
repositories remain unchanged and (2) γopt~0.
</bodyText>
<figure confidence="0.997238157894737">
e6
e7
ep
e5
e6
1
ep
e8
0
e
Score
Error
count
e5
ea
1
e
ey
Y
</figure>
<page confidence="0.956379">
727
</page>
<sectionHeader confidence="0.895476" genericHeader="method">
3 Minimum Error Rate Training on
Lattices
</sectionHeader>
<bodyText confidence="0.999618823529412">
In this section, the algorithm for computing the
upper envelope on N-best lists is extended to phrase
lattices. For a description on how to generate
lattices, see (Ueffing et al., 2002).
Formally, a phrase lattice for a source sentence f
is defined as a connected, directed acyclic graph
Gf~p = (Vf, Efq) with vertice set Vf, unique source and
sink nodes s, tP c Vf, and a set of arcs Ef€ c Vf~ x Vf.
Each arc is labeled with a phrase ϕij = ei1, ..., eij
and the (local) feature function values hM(ϕij, fq).
A path 7r = (v0, E0, v1, E1, ..., En— 1, vnq) in Gf (with
EiP c Ef and vi, vz+1P c Vf as the tail and head of
Ei, 0¤ &lt;, i &lt; n) defines a partial translation eπ of f
which is the concatenation of all phrases along this
path. The corresponding feature function values are
obtained by summing over the arc-specific feature
function values:
</bodyText>
<equation confidence="0.9145081">
ϕ
1pϕn�1,n, fqvn
n�1,nÝÑ
hM
ϕ.. _ ϕ o ...~ o ϕ
z3 0,1 n—1,n
h1pM
(ϕij, fq
i,j �
viÑvjPπ
</equation>
<bodyText confidence="0.936877210526316">
In the following, we use the notation inp (vq) and outp(vq)
to refer to the set of incoming and outgoing arcs for
a node vP c Vf. Similarly, headp(E) and tailp(E) denote
the head and tail of E c Ef.
To develop the algorithm for computing the up-
per envelope of all translation hypotheses that are
encoded in a phrase lattice, we first consider a node
vPVf with some incoming and outgoing arcs:
Each path that starts at the source node s and ends in
v defines a partial translation hypothesis which can
be represented as a line (cf. Eq. (5)). We now assume
that the upper envelope for these partial translation
hypotheses is known. The lines that constitute this
envelope shall be denoted by f1, ..., fN. Next we
consider continuations of these partial translation
candidates by following one of the outgoing arcs
Algorithm 2 Lattice Envelope
input: a phrase lattice 9f~pVf, Efq
output: upper envelope of 9f
</bodyText>
<equation confidence="0.866816388888889">
a =H;
L=H;
TopSort(9f);
for v = s to t do {
a = SweepLine(” L[E]);
εPinpvq
foreach (Einpvq)
L.delete(E);
foreach (Eoutpvq) {
L[E] = a;
for (i = 0; i &lt; a.size(); ++i) {
L[E][i].m = a[i].m +°m dmhmpE, fq;
L[E][i].y = a[i].y +°m AmhmpE, fq;
L[E][i].p = a[i]. p~(Pv,headpεq;
}
}
}
return a;
</equation>
<bodyText confidence="0.999474541666667">
Eoutpvq. Each such arc defines another line
denoted by gpE. If we add the slope and y-intercept
of gpEto each line in the setf1, ..., fNu, then the
upper envelope will be constituted by segments of
fi + gp(E), ..., fN + gp(E). This operation neither
changes the number of line segments nor their rela-
tive order in the envelope, and therefore it preserves
the structure of the convex hull. As a consequence,
we can propagate the resulting envelope over an
outgoing arc E to a successor node v1~&apos; = headp(E).
Other incoming arcs for v1may be associated with
different upper envelopes, and all that remains is
to merge these envelopes into a single combined
envelope. This is, however, easy to accomplish
since the combined envelope is simply the convex
hull of the union over the line sets which constitute
the individual envelopes. Thus, by merging the
arrays that store the line segments for the incoming
arcs and applying Algorithm 1 to the resulting array
we obtain the combined upper envelope for all
partial translation candidates that are associated with
paths starting at the source node s and ending in
v1. The correctness of this procedure is based on
the following two observations:
</bodyText>
<listItem confidence="0.800952">
(1) A single translation hypothesis cannot consti-
tute multiple line segments of the same envelope.
This is because translations associated with different
line segments are path-disjoint.
(2) Once a partial translation has been discarded
from an envelope because its associated line f is
completely covered by the topmost line segments
of the convex hull, there is no path continuation
that could bring back f into the upper envelope
</listItem>
<equation confidence="0.880887777777778">
v
E
ϕ0 ,l ϕ1 ,2
7r :•
vp h1pM (ϕ0 1, fq) v1 h1pM (ϕ1 2, fq~)
e,rQ
i,j �
viÑvjPπ
h1pM (eπ, fq~¸
</equation>
<page confidence="0.972992">
728
</page>
<bodyText confidence="0.991317253968254">
again. Proof: Suppose that such a continuation
exists, then this continuation can be represented as
a line g, and since f~ has been discarded from the
envelope, the path associated with g must also be a
valid continuation for the line segments f1, ..., fN
that constitute the envelope. Thus it follows that
maxpfig, ..., fNgq~maxpf1, ..., fNqg &lt;
~fg for some -yR. This, however, is in contra-
~
diction with the premise that
for all -y c R.
To keep track of the phrase expansions when
propagating an envelope over an outgoing arc e c
tailp (vq), the phrase label ϕv hwdp(E) has to be appended
from the right to all partial translation hypotheses in
the envelope. The complete algorithm then works
as follows: First, all nodes in the phrase lattice
are sorted in topological order. Starting with the
source node, we combine for each node v the upper
envelopes that are associated with v’s incoming arcs
by merging their respective line arrays and reducing
the merged array into a combined upper envelope
using Algorithm 1. The combined envelope is then
propagated over the outgoing arcs by associating
each e c outp(vq) with a copy of the combined
envelope. This copy is modified by adding the
parameters (slope and y-intercept) of the line gp(e)
to the envelope’s constituent line segments. The
envelopes of the incoming arcs are no longer needed
and can be deleted in order to release memory. The
envelope computed at the sink node is by construc-
tion the convex hull over all translation hypotheses
represented in the lattice, and it compactly encodes
those candidates which maximize the decision rule
Eq. (1) for any point along the line λM-ydM .
Algorithm 2 shows the pseudo code. Note that
the component E.x does not change and therefore
requires no update.
It remains to verify that the suggested algorithm
is efficient in both running time and memory. For
this purpose, we first analyze the complexity of
Algorithm 1 and derive from it the running time of
Algorithm 2.
After sorting, each line object in Algorithm 1 is
visited at most three times. The first time is when
it is picked by the outer loop. The second time is
when it either gets discarded or when it terminates
the inner loop. Whenever a line object is visited
for the third time, it is irrevocably removed from
the envelope. The runtime complexity is therefore
dominated by the initial sorting and amounts to
Op (K log Kq)
Topological sort on a phrase lattice _ (V, Eq)
can be performed in time O (I V I + 1E|q 1) . As will be
shown in Section 4, the size of the upper envelope
for G can never exceed the size of the arc set E. The
same holds for any subgraph Gr[,,v] of G which is
induced by the paths that connect the source node
s with vPV. Since the envelopes propagated from
the source to the sink node can only increase linearly
in the number of previously processed arcs, the total
running time amounts to a worst case complexity of
Op|VE|log|E|q.
</bodyText>
<sectionHeader confidence="0.985242" genericHeader="method">
4 Upper Bound for Size of Envelopes
</sectionHeader>
<bodyText confidence="0.998276291666667">
The memory efficiency of the suggested algorithm
results from the following theorem which provides
a novel upper bound for the number of cost mini-
mizing paths in a directed acyclic graph with arc-
specific affine cost functions. The bound is not only
meaningful for proving the space efficiency of lattice
MERT, but it also provides deeper insight into the
structure and complexity of the unsmoothed error
surface induced by log-linear models. Since we are
examining a special class of shortest paths problems,
we will invert the sign of each local feature function
value in order to turn the feature scores into cor-
responding costs. Hence, the objective of finding
the best translation hypotheses in a phrase lattice
becomes the problem of finding all cost-minimizing
paths in a graph with affine cost functions.
Theorem: Let !g = (V, Eq) be a connected directed
acyclic graph with vertex set V, unique source and
sink nodes s, tP c V, and an arc set E€ c V x V in
which each arc e c E is associated with an affine
cost function cE (-y) = aE • -y + bE, aE, bE c R.
Counting ties only once, the cardinality of the union
over the sets of all cost-minimizing paths for all
-y c R is then upper-bounded by |1E |1:
</bodyText>
<equation confidence="0.9979995">
7r : 7r = 7r(G; -y) is a cost-minimizing
-yR path in G given -y } &lt; 1E|1 (7)
</equation>
<bodyText confidence="0.999728333333333">
Proof: The proposition holds for the empty graph
as well as for the case that V = Is, tu} with all
arcs e c E joining the source and sink node. Let
G therefore be a larger graph. Then we perform
an s-t cut and split G into two subgraphs G1 (left
subgraph) and G2 (right subgraph). Arcs spanning
the section boundary are duplicated (with the costs
of the copied arcs in G2 being set to zero) and
connected with a newly added head or tail node:
</bodyText>
<page confidence="0.869153">
c4
c4
</page>
<figure confidence="0.487510666666667">
G: c1 G1: c1 G2:
c3
0
</figure>
<page confidence="0.851846333333333">
c3
c2
c2
</page>
<figure confidence="0.257208">
fmaxpf1, ..., fNq
</figure>
<page confidence="0.993909">
729
</page>
<bodyText confidence="0.999658671875">
The zero-cost arcs in G2 that emerged from the
duplication process are contracted, which can be
done without loss of generality because zero-cost
arcs do not affect the total costs of paths in the
lattice. The contraction essentially amounts to a
removal of arcs and is required in order to ensure
that the sum of edges in both subgraphs does not
exceed the number of edges in G. All nodes in
G1 with out-degree zero are then combined into a
single sink node t1. Similarly, nodes in G2 whose
in-degree is zero are combined into a single source
node s2. Let N1 and N2 denote the number of
arcs in G1 and G2, respectively. By construction,
NiN2E|. Both subgraphs are smaller
than G and thus, due to the induction hypothesis,
their lower envelopes consist of at most N1 and N2
line segments, respectively. We further notice that
either envelope is a convex hull whose constituent
line segments inscribe a convex polygon, in the
following denoted by P1 and P2. Now, we combine
both subgraphs into a single graph G1by merging
the sink node t1 in G1 with the source node s2
in G2. The merged node is an articulation point
whose removal would disconnect both subgraphs,
and hence, all paths in G1that start at the source
node s and stop in the sink node t lead through this
articulation point. The graph G1has at least as many
cost minimizing paths as G, although these paths
as well as their associated costs might be different
from those in G. The additivity of the cost function
and the articulation point allow us to split the costs
for any path from s to t into two portions: the first
portion can be attributed to G1 and must be a line
inside P1; the remainder can be attributed to G2
and must therefore be a line inside P2. Hence, the
total costs for any path in G1can be bounded by
the convex hull of the superposition of P1 and P2.
This convex hull is again a convex polygon which
consists of at most N1 + N2 edges, and therefore,
the number of cost minimizing paths in G1&apos; (and thus
also in G) is upper bounded by NiN2.� ❑
Corollary: The upper envelope for a phrase lattice
Gf~p = (Vf, Efq) consists of at most |IEf |I line segments.
This bound can even be refined and one obtains
(proof omitted) |19 1 — I V I + 2. Both bounds are tight.
This result may seem somewhat surprising as it
states that, independent of the choice of the direction
along which the line optimization is performed, the
structure of the error surface is far less complex
than one might expect based on the huge number
of alternative translation candidates that are rep-
resented in the lattice and thus contribute to the
error surface. In fact, this result is a consequence
of using a log-linear model which constrains how
costs (or scores, respectively) can evolve due to
hypothesis expansion. If instead quadratic cost
functions were used, the size of the envelopes could
not be limited in the same way. The above theorem
does not, however, provide any additional guidance
that would help to choose more promising directions
in the line optimization algorithm to find better local
optima. To alleviate this problem, the following
section lists some best practices that we found to be
useful in the context of MERT.
</bodyText>
<sectionHeader confidence="0.993019" genericHeader="method">
5 Practical Aspects
</sectionHeader>
<bodyText confidence="0.964576">
This section addresses some techniques that we
found to be beneficial in order to improve the
performance of MERT.
</bodyText>
<listItem confidence="0.948235285714286">
(1) Random Starting Points: To prevent the line
optimization algorithm from stopping in a poor local
optimum, MERT explores additional starting points
that are randomly chosen by sampling the parameter
space.
(2) Constrained Optimization: This technique
allows for limiting the range of some or all feature
</listItem>
<bodyText confidence="0.999583125">
function weights by defining weights restrictions.
The weight restriction for a feature function hm is
specified as an interval Rm~r = [lm, rms], lm, rmP E
lR, u {—oo, +oo} which defines the admissible region
from which the feature function weight λm can be
chosen. If the line optimization is performed under
the presence of weights restrictions, γ needs to be
chosen such that the following constraint holds:
</bodyText>
<equation confidence="0.682211">
lm + Y &apos; dM rM (8)
</equation>
<listItem confidence="0.85616">
(3) Weight Priors: Weight priors give a small (pos-
</listItem>
<bodyText confidence="0.921818333333333">
itive or negative) boost ω on the objective function
if the new weight is chosen such that it matches a
certain target value A*
</bodyText>
<equation confidence="0.670435666666667">
m:
γopt~arg min
m
</equation>
<bodyText confidence="0.999880333333333">
A zero-weights prior (Am~0) provides a means of
doing feature selection since the weight of a feature
function which is not discriminative will be set to
zero. An initial-weights prior (Am~λm) can
be used to confine changes in the parameter update
with the consequence that the new parameter may
be closer to the initial weights set. Initial weights
priors are useful in cases where the starting weights
already yield a decent baseline.
</bodyText>
<equation confidence="0.977035666666667">
Ers, efsi�
s
δpλm1&apos;dm, Amq~ω)(9)
</equation>
<page confidence="0.972913">
730
</page>
<bodyText confidence="0.565181142857143">
(4) Interval Merging: The intervalr [ryzs,ryZs1q) of
a translation hypothesis can be merged with the
intervalr [ryz s1, ryz s) of its left-adjacent translation
hypothesis if the corresponding change in the error
count DEzs = 0. The resulting intervalr 1ry41s1, ryZs1q )
has a larger range, and the choice of ryopt may be
more reliable.
</bodyText>
<listItem confidence="0.896342666666667">
(5) Random Directions: If the directions chosen in
the line optimization algorithm are the coordinate
axes of the M-dimensional parameter space, each
</listItem>
<bodyText confidence="0.972834684210526">
iteration will result in the update of a single feature
function only. While this update scheme provides
a ranking of the feature functions according to their
discriminative power (each iteration picks the fea-
ture function for which changing the corresponding
weight yields the highest gain), it does not take
possible correlations between the feature functions
into account. As a consequence, the optimization
procedure may stop in a poor local optimum. On
the other hand, it is difficult to compute a direction
that decorrelates two or more correlated feature
functions. This problem can be alleviated by ex-
ploring a large number of random directions which
update many feature weights simultaneously. The
random directions are chosen as the lines which
connect some randomly distributed points on the
surface of an M-dimensional hypersphere with the
hypersphere’s center. The center of the hypersphere
is defined as the initial parameter set.
</bodyText>
<sectionHeader confidence="0.999935" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999264857142857">
As suggested in (Och, 2003), an alternative method
for the optimization of the unsmoothed error count is
Powell’s algorithm combined with a grid-based line
optimization (Press et al., 2007, p. 509). In (Zens
et al., 2007), the MERT criterion is optimized on
N-best lists using the Downhill Simplex algorithm
(Press et al., 2007, p. 503). The optimization proce-
dure allows for optimizing other objective function
as, e.g., the expected BLEU score. A weakness
of the Downhill Simplex algorithm is, however, its
decreasing robustness for optimization problems in
more than 10 dimensions. A different approach
to minimize the expected BLEU score is suggested
in (Smith and Eisner, 2006) who use deterministic
annealing to gradually turn the objective function
from a convex entropy surface into the more com-
plex risk surface. A large variety of different search
strategies for MERT are investigated in (Cer et al.,
2008), which provides many fruitful insights into
the optimization process. In (Duh and Kirchhoff,
2008), MERT is used to boost the BLEU score on
</bodyText>
<tableCaption confidence="0.5978654">
Table 1: Corpus statistics for three text translation sets:
Arabic-to-English (aren), Chinese-to-English (zhen),
and English-to-Chinese (enzh). Development and test
data are compiled from evaluation data used in past
NIST Machine Translation Evaluations.
</tableCaption>
<table confidence="0.9951688">
data set collection # of sentences enzh
aren zhen
dev1 nist02 1043 878 –
dev2 nist04 1353 1788 –
blind nist08 1360 1357 1859
</table>
<bodyText confidence="0.99749275">
N-best re-ranking tasks. The incorporation of a
large number of sparse feature functions is described
in (Watanabe et al., 2007). The paper investigates a
perceptron-like online large-margin training for sta-
tistical machine translation. The described approach
is reported to yield significant improvements on top
of a baseline system which employs a small number
of feature functions whose weights are optimized
under the MERT criterion. A study which is comple-
mentary to the upper bound on the size of envelopes
derived in Section 4 is provided in (Elizalde and
Woods, 2006) which shows that the number of
inference functions of any graphical model as, for
instance, Bayesian networks and Markov random
fields is polynomial in the size of the model if the
number of parameters is fixed.
</bodyText>
<sectionHeader confidence="0.998716" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999768333333333">
Experiments were conducted on the NIST 2008
translation tasks under the conditions of the con-
strained data track for the language pairs Arabic-
to-English (aren), English-to-Chinese (enzh), and
Chinese-to-English (zhen). The development cor-
pora were compiled from test data used in the
2002 and 2004 NIST evaluations. Each corpus set
provides 4 reference translations per source sen-
tence. Table 1 summarizes some corpus statistics.
</bodyText>
<tableCaption confidence="0.994149333333333">
Table 2: BLEU score results on the NIST-08 test set
obtained after 25 iterations using N-best MERT or 5
iterations using lattice MERT, respectively.
</tableCaption>
<table confidence="0.99967675">
task loss dev1+dev2 lattice blind lattice
N-best N-best
aren MBR 56.6 57.4 42.9 43.9
0-1 56.7 57.4 42.8 43.7
enzh MBR 39.7 39.6 36.5 38.8
0-1 40.4 40.5 35.1 37.6
zhen MBR 39.5 39.7 27.5 28.2
0-1 39.6 39.6 27.0 27.6
</table>
<page confidence="0.992175">
731
</page>
<figure confidence="0.996026333333333">
BLEU[%]
0 5 10 15 20 25
iteration
</figure>
<figureCaption confidence="0.96942625">
Figure 2: BLEU scores for N-best MERT and lattice
MERT after each decoding step on the zhen-dev1 corpus.
The grey shaded subfigure shows the complete graph
including the bottom partfor N-best MERT.
</figureCaption>
<bodyText confidence="0.999944142857143">
Translation results were evaluated using the mixed-
case BLEU score metric in the implementation as
suggested by (Papineni et al., 2001).
Translation results were produced with a state-of-
the-art phrase-based SMT system which uses EM-
trained word alignment models (IBM1, HMM) and
a 5-gram language model built from the Web-1T
collection2. Translation hypotheses produced on the
blind test data were reranked using the Minimum-
Bayes Risk (MBR) decision rule (Kumar and Byrne,
2004; Tromble et al., 2008). Each system uses a log-
linear combination of 20 to 30 feature functions.
In a first experiment, we investigated the conver-
gence speed of lattice MERT and N-best MERT.
</bodyText>
<equation confidence="0.5299205">
2http://www.ldc.upenn.edu, catalog entry: LDC2006T13
-15 -10 -5 0 5 10 15
</equation>
<page confidence="0.629814">
γ
</page>
<figureCaption confidence="0.9990815">
Figure 3: Error surface ofthe phrase penaltyfeature after
the first iteration on the zhen-dev1 corpus.
</figureCaption>
<figure confidence="0.9867545">
0 1 2 3 4 5
iteration
</figure>
<figureCaption confidence="0.988627">
Figure 4: BLEU scores on the zhen-dev1 corpus for
lattice MERT with additional directions.
</figureCaption>
<bodyText confidence="0.997736611111111">
Figure 2 shows the evolution of the BLEU score
in the course of the iteration index on the zhen-
dev1 corpus for either method. In each iteration,
the training procedure translates the development
corpus using the most recent weights set and merges
the top ranked candidate translations (either repre-
sented as phrase lattices or N-best lists) into the
candidate repositories before the line optimization
is performed. For N-best MERT, we used N~50
which yielded the best results. In contrast to lattice
MERT, N-best MERT optimizes all dimensions in
each iteration and, in addition, it also explores a
large number of random starting points before it
re-decodes and expands the hypothesis set. As is
typical for N-best MERT, the first iteration causes
a dramatic performance loss caused by overadapting
the candidate repositories, which amounts to more
than 27.3 BLEU points. Although this performance
loss is recouped after the 5th iteration, the initial
decline makes the line optimization under N-best
MERT more fragile since the optimum found at the
end of the training procedure is affected by the initial
performance drop rather than by the choice of the
initial start weights. Lattice MERT on the other hand
results in a significantly faster convergence speed
and reaches its optimum already in the 5th iteration.
For lattice MERT, we used a graph density of 40
arcs per phrase which corresponds to an N-best size
of more than two octillionp2~1027qentries. This
huge number of alternative candidate translations
makes updating the weights under lattice MERT
more reliable and robust and, compared to N-best
MERT, it becomes less likely that the same feature
weight needs to be picked again and adjusted in
subsequent iterations. Figure 4 shows the evolution
of the BLEU score on the zhen-dev1 corpus using
</bodyText>
<figure confidence="0.994288226415094">
40
39.5
39
38.5
38
37.5
37
36.5
36
35.5
lattice MERT
N-best MERT
45
35
25
15
5
0 5 10 15 20 25
BLEU[%]
36.8
36.6
36.4
36.2
35.8
35.6
37
36
36.5
35.5
37
36
35
-40 -20 0 20 40
36.34
36.32
36.28
-0.2 -0.1 0 0.1 0.2
36.3
lattice
50-best
lattice MERT: 1000 directions
1 direction
40
39.5
39
38.5
38
BLEU[%]
37.5
37
36.5
36
35.5
</figure>
<page confidence="0.984825">
732
</page>
<tableCaption confidence="0.9809815">
Table 3: BLEU score results on the NIST-08 tests set
obtained after 5 iterations using lattice MERT with
different numbers of random directions in addition to the
optimization along the coordinate axes.
</tableCaption>
<table confidence="0.999379166666667">
# random dev1+dev2 blind
task directions 0-1 MBR 0-1 MBR
aren – 57.4 57.4 43.7 43.9
1000 57.6 57.7 43.9 44.5
zhen – 39.6 39.7 27.6 28.2
500 39.5 39.9 27.9 28.3
</table>
<bodyText confidence="0.999820925">
lattice MERT with 5 weights updates per iteration.
The performance drop in iteration 1 is also attributed
to overfitting the candidate repository. The decline
of less than 0.5% in terms of BLEU is, however,
almost negligible compared to the performance drop
of more than 27% in case of N-best MERT. The
vast number of alternative translation hypotheses
represented in a lattice also increases the number
of phase transitions in the error surface, and thus
prevents MERT from selecting a low performing
feature weights set at early stages in the optimization
procedure. This is illustrated in Figure 3, where
lattice MERT and N-best MERT find different op-
tima for the weight of the phrase penalty feature
function after the first iteration. Table 2 shows the
BLEU score results on the NIST 2008 blind test
using the combined dev1+dev2 corpus as training
data. While only the aren task shows improvements
on the development data, lattice MERT provides
consistent gains over N-best MERT on all three
blind test sets. The reduced performance for N-best
MERT is a consequence of the performance drop in
the first iteration which causes the final weights to
be far off from the initial parameter set. This can
impair the ability of N-best MERT to generalize to
unseen data if the initial weights are already capable
of producing a decent baseline. Lattice MERT on
the other hand can produce weights sets which are
closer to the initial weights and thus more likely to
retain the ability to generalize to unseen data. It
could therefore be worthwhile to investigate whether
a more elaborated version of an initial-weights prior
allows for alleviating this effect in case of N-
best MERT. Table 3 shows the effect of optimizing
the feature function weights along some randomly
chosen directions in addition to the coordinate axes.
The different local optima found on the development
set by using random directions result in additional
gains on the blind test sets and range from 0.1% to
0.6% absolute in terms of BLEU.
</bodyText>
<sectionHeader confidence="0.998123" genericHeader="conclusions">
8 Summary
</sectionHeader>
<bodyText confidence="0.999981517241379">
We presented a novel algorithm that allows for
efficiently constructing and representing the un-
smoothed error surface over all sentence hypotheses
that are represented in a phrase lattice. The proposed
algorithm was used to train the feature function
weights of a log-linear model for a statistical ma-
chine translation system under the Minimum Error
Rate Training (MERT) criterion. Lattice MERT was
shown analytically and experimentally to be supe-
rior over N-best MERT, resulting in significantly
faster convergence speed and a reduced number of
decoding steps. While the approach was used to
optimize the model parameters of a single machine
translation system, there are many other applications
in which this framework can be useful, too. One
possible usecase is the computation of consensus
translations from the outputs of multiple machine
translation systems where this framework allows us
to estimate the system prior weights directly on con-
fusion networks (Rosti et al., 2007; Macherey and
Och, 2007). It is also straightforward to extend the
suggested method to hypergraphs and forests as they
are used, e.g., in hierarchical and syntax-augmented
systems (Chiang, 2005; Zollmann and Venugopal,
2006). Our future work will therefore focus on how
much system combination and syntax-augmented
machine translation can benefit from lattice MERT
and to what extent feature function weights can
robustly be estimated using the suggested method.
</bodyText>
<sectionHeader confidence="0.999554" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99332605">
J. L. Bentley and T. A. Ottmann. 1979. Algorithms for
reporting and counting geometric intersections. IEEE
Trans. on Computers, C-28(9):643–647.
D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regular-
ization and Search for Minimum Error Rate Training.
In Proceedings of the Third Workshop on Statistical
Machine Translation, 46th Annual Meeting of the
Association of Computational Linguistics: Human
Language Technologies (ACL-2008 HLT), pages 26–
34, Columbus, OH, USA, June.
D. Chiang. 2005. A Hierarchical Phrase-based Model
for Statistical Machine Translation. In ACL-2005,
pages 263–270, Ann Arbor, MI, USA, June.
K. Duh and K. Kirchhoff. 2008. Beyond Log-Linear
Models: Boosted Minimum Error Rate Training for
N-best Re-ranking. In Proceedings of the Third
Workshop on Statistical Machine Translation, 46th
Annual Meeting of the Association of Computational
Linguistics: Human Language Technologies (ACL-
2008 HLT), pages 37–40, Columbus, OH, USA, June.
</reference>
<page confidence="0.987847">
733
</page>
<reference confidence="0.99992336">
S. Elizalde and K. Woods. 2006. Bounds on the Number
of Inference Functions of a Graphical Model, October.
arXiv:math/0610233v1.
S. Kumar and W. Byrne. 2004. Minimum Bayes-
Risk Decoding for Statistical Machine Translation.
In Proc. HLT-NAACL, pages 196–176, Boston, MA,
USA, May.
W. Macherey and F. J. Och. 2007. An Empirical Study
on Computing Consensus Translations from Multiple
Machine Translation Systems. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 986–995, Prague, Czech Republic,
June.
F. J. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In 41st Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 160–167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
2001. BLEU: a Method for Automatic Evaluation
of Machine Translation. Technical Report RC22176
(W0109-022), IBM Research Division, Thomas J.
Watson Research Center, Yorktown Heights, NY,
USA.
K. A. Papineni. 1999. Discriminative training via
linear programming. In IEEE Int. Conf. on Acoustics,
Speech, and Signal Processing, volume 2, pages 561–
564, Phoenix, AZ, March.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P.
Flannery. 2007. Numerical Recipes: The Art of
Scientific Computing. Cambridge University Press,
Cambridge, UK, third edition.
A. V. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas,
R. Schwartz, and B. Dorr. 2007. Combining outputs
from multiple machine translation systems. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 228–235, Rochester, New York,
April. Association for Computational Linguistics.
D. A. Smith and J. Eisner. 2006. Minimum Risk
Annealing for Training Log-linear Models. In 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics (Coling/ACL-2006), pages
787–794, Sydney, Australia, July.
R. Tromble, S. Kumar, F. J. Och, and W. Macherey. 2008.
Lattice minimum bayes-risk decoding for statistical
machine translation. In Conference on Empirical
Methods in Natural Language Processing (EMNLP),
page 10, Waikiki, Honolulu, Hawaii, USA, October.
N. Ueffing, F. J. Och, and H. Ney. 2002. Generation
of word graphs in statistical machine translation. In
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, pages 156–
163, Philadelphia, PE, July.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 764–773,
Prague, Czech Republic.
R. Zens, S. Hasan, and H. Ney. 2007. A Systematic
Comparison of Training Criteria for Statistical Ma-
chine Translation. In Proceedings of the 2007 Con-
ference on Empirical Methods in Natural Language
Processing, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In
NAACL ’06: Proceedings of the 2006 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 138–141, New York, NY, June.
Association for Computational Linguistics.
</reference>
<page confidence="0.998394">
734
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.500304">
<title confidence="0.996406">Lattice-based Minimum Error Rate for Statistical Machine Translation</title>
<author confidence="0.949361">Wolfgang Macherey Franz Josef Och Ignacio Thayer Jakob Uszkoreit</author>
<affiliation confidence="0.742853">Google</affiliation>
<address confidence="0.9854195">1600 Amphitheatre Mountain View, CA 94043,</address>
<abstract confidence="0.989673096774193">Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, in MERT are represented as lists which contain the probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and reprethe exact error surface of translations that are encoded in a phrase lattice. to MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements moderate BLEU score gains over MERT.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J L Bentley</author>
<author>T A Ottmann</author>
</authors>
<title>Algorithms for reporting and counting geometric intersections.</title>
<date>1979</date>
<journal>IEEE Trans. on Computers,</journal>
<volume>28</volume>
<issue>9</issue>
<contexts>
<context position="8833" citStr="Bentley and Ottmann, 1979" startWordPosition="1423" endWordPosition="1426">e outcomes that a rescoring of Cs may yield if the parameter set λM1 is moved along the chosen direction. Once the upper envelope has been determined, we can project its constituent line segments onto the error counts of the corresponding candidate translations (cf. Figure 1). This projection is independent of how the envelope is generated and can therefore be applied to any set of line segments1. An effective means to compute the upper envelope is a sweep line algorithm which is often used in computational geometry to determine the intersection points of a sequence of lines or line segments (Bentley and Ottmann, 1979). The idea is to shift (“sweep”) a vertical ray from~8to8over the plane while keeping track of those points where two or more lines intersect. Since the upper envelope is fully specified by the topmost line segments, it suffices to store the following components for each line object ℓ: the x-intercept ℓ.x with the leftadjacent line, the slope ℓ.m, and the y-intercept ℓ.y; a fourth component, ℓ.t, is used to store the candidate translation. Algorithm 1 shows the pseudo code for a sweep line algorithm which reduces an input array a[0..K-1] consisting of the K line objects of the candidate reposi</context>
</contexts>
<marker>Bentley, Ottmann, 1979</marker>
<rawString>J. L. Bentley and T. A. Ottmann. 1979. Algorithms for reporting and counting geometric intersections. IEEE Trans. on Computers, C-28(9):643–647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cer</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Regularization and Search for Minimum Error Rate Training.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation, 46th Annual Meeting of the Association of Computational Linguistics: Human Language Technologies (ACL-2008 HLT),</booktitle>
<pages>26--34</pages>
<location>Columbus, OH, USA,</location>
<contexts>
<context position="27387" citStr="Cer et al., 2008" startWordPosition="4705" endWordPosition="4708">lgorithm (Press et al., 2007, p. 503). The optimization procedure allows for optimizing other objective function as, e.g., the expected BLEU score. A weakness of the Downhill Simplex algorithm is, however, its decreasing robustness for optimization problems in more than 10 dimensions. A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. A large variety of different search strategies for MERT are investigated in (Cer et al., 2008), which provides many fruitful insights into the optimization process. In (Duh and Kirchhoff, 2008), MERT is used to boost the BLEU score on Table 1: Corpus statistics for three text translation sets: Arabic-to-English (aren), Chinese-to-English (zhen), and English-to-Chinese (enzh). Development and test data are compiled from evaluation data used in past NIST Machine Translation Evaluations. data set collection # of sentences enzh aren zhen dev1 nist02 1043 878 – dev2 nist04 1353 1788 – blind nist08 1360 1357 1859 N-best re-ranking tasks. The incorporation of a large number of sparse feature </context>
</contexts>
<marker>Cer, Jurafsky, Manning, 2008</marker>
<rawString>D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regularization and Search for Minimum Error Rate Training. In Proceedings of the Third Workshop on Statistical Machine Translation, 46th Annual Meeting of the Association of Computational Linguistics: Human Language Technologies (ACL-2008 HLT), pages 26– 34, Columbus, OH, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A Hierarchical Phrase-based Model for Statistical Machine Translation. In</title>
<date>2005</date>
<booktitle>ACL-2005,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI, USA,</location>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A Hierarchical Phrase-based Model for Statistical Machine Translation. In ACL-2005, pages 263–270, Ann Arbor, MI, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Duh</author>
<author>K Kirchhoff</author>
</authors>
<title>Beyond Log-Linear Models: Boosted Minimum Error Rate Training for N-best Re-ranking.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation, 46th Annual Meeting of the Association of Computational Linguistics: Human Language Technologies (ACL2008 HLT),</booktitle>
<pages>37--40</pages>
<location>Columbus, OH, USA,</location>
<contexts>
<context position="27486" citStr="Duh and Kirchhoff, 2008" startWordPosition="4719" endWordPosition="4722">r objective function as, e.g., the expected BLEU score. A weakness of the Downhill Simplex algorithm is, however, its decreasing robustness for optimization problems in more than 10 dimensions. A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. A large variety of different search strategies for MERT are investigated in (Cer et al., 2008), which provides many fruitful insights into the optimization process. In (Duh and Kirchhoff, 2008), MERT is used to boost the BLEU score on Table 1: Corpus statistics for three text translation sets: Arabic-to-English (aren), Chinese-to-English (zhen), and English-to-Chinese (enzh). Development and test data are compiled from evaluation data used in past NIST Machine Translation Evaluations. data set collection # of sentences enzh aren zhen dev1 nist02 1043 878 – dev2 nist04 1353 1788 – blind nist08 1360 1357 1859 N-best re-ranking tasks. The incorporation of a large number of sparse feature functions is described in (Watanabe et al., 2007). The paper investigates a perceptron-like online </context>
</contexts>
<marker>Duh, Kirchhoff, 2008</marker>
<rawString>K. Duh and K. Kirchhoff. 2008. Beyond Log-Linear Models: Boosted Minimum Error Rate Training for N-best Re-ranking. In Proceedings of the Third Workshop on Statistical Machine Translation, 46th Annual Meeting of the Association of Computational Linguistics: Human Language Technologies (ACL2008 HLT), pages 37–40, Columbus, OH, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Elizalde</author>
<author>K Woods</author>
</authors>
<title>Bounds on the Number of Inference Functions of a Graphical Model,</title>
<date>2006</date>
<contexts>
<context position="28483" citStr="Elizalde and Woods, 2006" startWordPosition="4874" endWordPosition="4877">v2 nist04 1353 1788 – blind nist08 1360 1357 1859 N-best re-ranking tasks. The incorporation of a large number of sparse feature functions is described in (Watanabe et al., 2007). The paper investigates a perceptron-like online large-margin training for statistical machine translation. The described approach is reported to yield significant improvements on top of a baseline system which employs a small number of feature functions whose weights are optimized under the MERT criterion. A study which is complementary to the upper bound on the size of envelopes derived in Section 4 is provided in (Elizalde and Woods, 2006) which shows that the number of inference functions of any graphical model as, for instance, Bayesian networks and Markov random fields is polynomial in the size of the model if the number of parameters is fixed. 7 Experiments Experiments were conducted on the NIST 2008 translation tasks under the conditions of the constrained data track for the language pairs Arabicto-English (aren), English-to-Chinese (enzh), and Chinese-to-English (zhen). The development corpora were compiled from test data used in the 2002 and 2004 NIST evaluations. Each corpus set provides 4 reference translations per sou</context>
</contexts>
<marker>Elizalde, Woods, 2006</marker>
<rawString>S. Elizalde and K. Woods. 2006. Bounds on the Number of Inference Functions of a Graphical Model, October. arXiv:math/0610233v1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum BayesRisk Decoding for Statistical Machine Translation. In</title>
<date>2004</date>
<booktitle>Proc. HLT-NAACL,</booktitle>
<pages>196--176</pages>
<location>Boston, MA, USA,</location>
<contexts>
<context position="30214" citStr="Kumar and Byrne, 2004" startWordPosition="5155" endWordPosition="5158">ERT after each decoding step on the zhen-dev1 corpus. The grey shaded subfigure shows the complete graph including the bottom partfor N-best MERT. Translation results were evaluated using the mixedcase BLEU score metric in the implementation as suggested by (Papineni et al., 2001). Translation results were produced with a state-ofthe-art phrase-based SMT system which uses EMtrained word alignment models (IBM1, HMM) and a 5-gram language model built from the Web-1T collection2. Translation hypotheses produced on the blind test data were reranked using the MinimumBayes Risk (MBR) decision rule (Kumar and Byrne, 2004; Tromble et al., 2008). Each system uses a loglinear combination of 20 to 30 feature functions. In a first experiment, we investigated the convergence speed of lattice MERT and N-best MERT. 2http://www.ldc.upenn.edu, catalog entry: LDC2006T13 -15 -10 -5 0 5 10 15 γ Figure 3: Error surface ofthe phrase penaltyfeature after the first iteration on the zhen-dev1 corpus. 0 1 2 3 4 5 iteration Figure 4: BLEU scores on the zhen-dev1 corpus for lattice MERT with additional directions. Figure 2 shows the evolution of the BLEU score in the course of the iteration index on the zhendev1 corpus for either</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>S. Kumar and W. Byrne. 2004. Minimum BayesRisk Decoding for Statistical Machine Translation. In Proc. HLT-NAACL, pages 196–176, Boston, MA, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Macherey</author>
<author>F J Och</author>
</authors>
<title>An Empirical Study on Computing Consensus Translations from Multiple Machine Translation Systems.</title>
<date>2007</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>986--995</pages>
<location>Prague, Czech Republic,</location>
<marker>Macherey, Och, 2007</marker>
<rawString>W. Macherey and F. J. Och. 2007. An Empirical Study on Computing Consensus Translations from Multiple Machine Translation Systems. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 986–995, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="2226" citStr="Och, 2003" startWordPosition="336" endWordPosition="337">aim at minimizing the probability of sentence errors. In practice, however, system quality is often measured based on error metrics that assign non-uniform costs to classification errors and thus go far beyond counting the number of wrong decisions. Examples are the mean average precision for ranked retrieval, the F-measure for parsing, and the BLEU score for statistical machine translation (SMT). A class of training criteria that provides a tighter connection between the decision rule and the final error metric is known as Minimum Error Rate Training (MERT) and has been suggested for SMT in (Och, 2003). MERT aims at estimating the model parameters such that the decision under the zero-one loss function maximizes some end-to-end performance measure on a development corpus. In combination with log-linear models, the training procedure allows for a direct optimization of the unsmoothed error count. The criterion can be derived from Bayes’ decision rule as follows: Let ff1, ..., fi denote a source sentence (’French’) which is to be translated into a target sentence (’English’) ee1, ..., eI. Under the zero-one loss function, the translation which maximizes the a posteriori probability is chosen:</context>
<context position="4004" citStr="Och, 2003" startWordPosition="624" endWordPosition="625">nslations � m~1 amhmpe, f (2) 725 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 725–734, Honolulu, October 2008.c�2008 Association for Computational Linguistics rS vidual sentences, i.e., EprS 1 , and let Cs�tes,1, ..., es,Kudenote a set of K candidate translations. Assuming that the corpusbased error count for some translations eS 1 is additively decomposable into the error counts of the indi1q�°S 1 , eS s~1 Eprs, esq, the MERT criterion is given as: S¸ Ers, 6fs; λMi (3)� s1 K¸ k�1 Eprs, es,kqδ��epfs; λM 1q, es,k�+ with M¸ λmhmpe,fsq+(4) M1 In (Och, 2003), it was shown that linear models can effectively be trained under the MERT criterion using a special line optimization algorithm. This line optimization determines for each feature function hm and sentence fs the exact error surface on a set of candidate translations Cs. The feature function weights are then adjusted by traversing the error surface combined over all sentences in the training corpus and moving the weights to a point where the resulting error reaches a minimum. Candidate translations in MERT are typically represented as N-best lists which contain the N most probable translation</context>
<context position="6823" citStr="Och, 2003" startWordPosition="1077" endWordPosition="1078">riments conducted on the NIST 2008 translation tasks. The paper concludes with a summary in Section 8. 2 Minimum Error Rate Training on N-best Lists The goal of MERT is to find a weights set that minimizes the unsmoothed error count on a representative training corpus (cf. Eq. (3)). This can be accomplished through a sequence of line minimizations along some vector directionstdM candidate translations Cs�te1, ..., eKualong the 1u. Starting from an initial point λM 1 , computing the most probable sentence hypothesis out of a set of K line λM1rydM1 results in the following optimization problem (Och, 2003): efs; ryarg max ePCs!p λM1,ydM1qJ�hM1pe, fsq)� arg max tE ePCs&amp;quot;¸ m ape,fsq� bpe,fsq�� arg max ePCs ape, fs-ybpe, fs } (5) Hence, the total scorep~qfor any candidate translation corresponds to a line in the plane with γ as the independent variable. For any particular choice of γ, the decoder seeks that translation which yields the largest score and therefore corresponds to the topmost line segment. Overall, the candidate repository Cs defines K lines where each line may be divided into at most K line segments due to possible intersections with the other K1 lines. The sequence of the topmost li</context>
<context position="26498" citStr="Och, 2003" startWordPosition="4567" endWordPosition="4568">s a consequence, the optimization procedure may stop in a poor local optimum. On the other hand, it is difficult to compute a direction that decorrelates two or more correlated feature functions. This problem can be alleviated by exploring a large number of random directions which update many feature weights simultaneously. The random directions are chosen as the lines which connect some randomly distributed points on the surface of an M-dimensional hypersphere with the hypersphere’s center. The center of the hypersphere is defined as the initial parameter set. 6 Related Work As suggested in (Och, 2003), an alternative method for the optimization of the unsmoothed error count is Powell’s algorithm combined with a grid-based line optimization (Press et al., 2007, p. 509). In (Zens et al., 2007), the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503). The optimization procedure allows for optimizing other objective function as, e.g., the expected BLEU score. A weakness of the Downhill Simplex algorithm is, however, its decreasing robustness for optimization problems in more than 10 dimensions. A different approach to minimize the expec</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<journal>IBM Research Division, Thomas J. Watson Research</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<location>Center, Yorktown Heights, NY, USA.</location>
<contexts>
<context position="29874" citStr="Papineni et al., 2001" startWordPosition="5103" endWordPosition="5106">ns using lattice MERT, respectively. task loss dev1+dev2 lattice blind lattice N-best N-best aren MBR 56.6 57.4 42.9 43.9 0-1 56.7 57.4 42.8 43.7 enzh MBR 39.7 39.6 36.5 38.8 0-1 40.4 40.5 35.1 37.6 zhen MBR 39.5 39.7 27.5 28.2 0-1 39.6 39.6 27.0 27.6 731 BLEU[%] 0 5 10 15 20 25 iteration Figure 2: BLEU scores for N-best MERT and lattice MERT after each decoding step on the zhen-dev1 corpus. The grey shaded subfigure shows the complete graph including the bottom partfor N-best MERT. Translation results were evaluated using the mixedcase BLEU score metric in the implementation as suggested by (Papineni et al., 2001). Translation results were produced with a state-ofthe-art phrase-based SMT system which uses EMtrained word alignment models (IBM1, HMM) and a 5-gram language model built from the Web-1T collection2. Translation hypotheses produced on the blind test data were reranked using the MinimumBayes Risk (MBR) decision rule (Kumar and Byrne, 2004; Tromble et al., 2008). Each system uses a loglinear combination of 20 to 30 feature functions. In a first experiment, we investigated the convergence speed of lattice MERT and N-best MERT. 2http://www.ldc.upenn.edu, catalog entry: LDC2006T13 -15 -10 -5 0 5 1</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001. BLEU: a Method for Automatic Evaluation of Machine Translation. Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
</authors>
<title>Discriminative training via linear programming.</title>
<date>1999</date>
<booktitle>In IEEE Int. Conf. on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>2</volume>
<pages>561--564</pages>
<location>Phoenix, AZ,</location>
<contexts>
<context position="7685" citStr="Papineni, 1999" startWordPosition="1223" endWordPosition="1224"> For any particular choice of γ, the decoder seeks that translation which yields the largest score and therefore corresponds to the topmost line segment. Overall, the candidate repository Cs defines K lines where each line may be divided into at most K line segments due to possible intersections with the other K1 lines. The sequence of the topmost line segments constitute the upper envelope which is the pointwise maximum over all lines induced by Cs. The upper envelope is a convex hull and can be inscribed with a convex polygon whose edges are the segments of a piecewise linear function in γ (Papineni, 1999; Och, 2003): Envpfq~max ePC ape, fq-ybpe, fq: γPR((6) � �λM1�arg min λM S¸ � arg min λM s1 6fs; λM1q�arg max e# λmhmpe, fs ry dmhmpe, fs m 726 e8 0 Y Figure 1: The upper envelope (bold, red curve) for a set of lines is the convex hull which consists of the topmost line segments. Each line corresponds to a candidate translation and is thus related to a certain error count. Envelopes can efficiently be computed with Algorithm 1. The importance of the upper envelope is that it provides a compact encoding of all possible outcomes that a rescoring of Cs may yield if the parameter set λM1 is moved </context>
</contexts>
<marker>Papineni, 1999</marker>
<rawString>K. A. Papineni. 1999. Discriminative training via linear programming. In IEEE Int. Conf. on Acoustics, Speech, and Signal Processing, volume 2, pages 561– 564, Phoenix, AZ, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H Press</author>
<author>S A Teukolsky</author>
<author>W T Vetterling</author>
<author>B P Flannery</author>
</authors>
<title>Numerical Recipes: The Art of Scientific Computing.</title>
<date>2007</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK,</location>
<note>third edition.</note>
<contexts>
<context position="26659" citStr="Press et al., 2007" startWordPosition="4589" endWordPosition="4592"> two or more correlated feature functions. This problem can be alleviated by exploring a large number of random directions which update many feature weights simultaneously. The random directions are chosen as the lines which connect some randomly distributed points on the surface of an M-dimensional hypersphere with the hypersphere’s center. The center of the hypersphere is defined as the initial parameter set. 6 Related Work As suggested in (Och, 2003), an alternative method for the optimization of the unsmoothed error count is Powell’s algorithm combined with a grid-based line optimization (Press et al., 2007, p. 509). In (Zens et al., 2007), the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503). The optimization procedure allows for optimizing other objective function as, e.g., the expected BLEU score. A weakness of the Downhill Simplex algorithm is, however, its decreasing robustness for optimization problems in more than 10 dimensions. A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface in</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 2007</marker>
<rawString>W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. 2007. Numerical Recipes: The Art of Scientific Computing. Cambridge University Press, Cambridge, UK, third edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Rosti</author>
<author>N F Ayan</author>
<author>B Xiang</author>
<author>S Matsoukas</author>
<author>R Schwartz</author>
<author>B Dorr</author>
</authors>
<title>Combining outputs from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>228--235</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<marker>Rosti, Ayan, Xiang, Matsoukas, Schwartz, Dorr, 2007</marker>
<rawString>A. V. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas, R. Schwartz, and B. Dorr. 2007. Combining outputs from multiple machine translation systems. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 228–235, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Minimum Risk Annealing for Training Log-linear Models.</title>
<date>2006</date>
<booktitle>In 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (Coling/ACL-2006),</booktitle>
<pages>787--794</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="27153" citStr="Smith and Eisner, 2006" startWordPosition="4667" endWordPosition="4670">optimization of the unsmoothed error count is Powell’s algorithm combined with a grid-based line optimization (Press et al., 2007, p. 509). In (Zens et al., 2007), the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503). The optimization procedure allows for optimizing other objective function as, e.g., the expected BLEU score. A weakness of the Downhill Simplex algorithm is, however, its decreasing robustness for optimization problems in more than 10 dimensions. A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. A large variety of different search strategies for MERT are investigated in (Cer et al., 2008), which provides many fruitful insights into the optimization process. In (Duh and Kirchhoff, 2008), MERT is used to boost the BLEU score on Table 1: Corpus statistics for three text translation sets: Arabic-to-English (aren), Chinese-to-English (zhen), and English-to-Chinese (enzh). Development and test data are compiled from evaluation data used in past NIST Mac</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>D. A. Smith and J. Eisner. 2006. Minimum Risk Annealing for Training Log-linear Models. In 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (Coling/ACL-2006), pages 787–794, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tromble</author>
<author>S Kumar</author>
<author>F J Och</author>
<author>W Macherey</author>
</authors>
<title>Lattice minimum bayes-risk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>10</pages>
<location>Waikiki, Honolulu, Hawaii, USA,</location>
<contexts>
<context position="30237" citStr="Tromble et al., 2008" startWordPosition="5159" endWordPosition="5162"> step on the zhen-dev1 corpus. The grey shaded subfigure shows the complete graph including the bottom partfor N-best MERT. Translation results were evaluated using the mixedcase BLEU score metric in the implementation as suggested by (Papineni et al., 2001). Translation results were produced with a state-ofthe-art phrase-based SMT system which uses EMtrained word alignment models (IBM1, HMM) and a 5-gram language model built from the Web-1T collection2. Translation hypotheses produced on the blind test data were reranked using the MinimumBayes Risk (MBR) decision rule (Kumar and Byrne, 2004; Tromble et al., 2008). Each system uses a loglinear combination of 20 to 30 feature functions. In a first experiment, we investigated the convergence speed of lattice MERT and N-best MERT. 2http://www.ldc.upenn.edu, catalog entry: LDC2006T13 -15 -10 -5 0 5 10 15 γ Figure 3: Error surface ofthe phrase penaltyfeature after the first iteration on the zhen-dev1 corpus. 0 1 2 3 4 5 iteration Figure 4: BLEU scores on the zhen-dev1 corpus for lattice MERT with additional directions. Figure 2 shows the evolution of the BLEU score in the course of the iteration index on the zhendev1 corpus for either method. In each iterat</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>R. Tromble, S. Kumar, F. J. Och, and W. Macherey. 2008. Lattice minimum bayes-risk decoding for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP), page 10, Waikiki, Honolulu, Hawaii, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Generation of word graphs in statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>156--163</pages>
<location>Philadelphia, PE,</location>
<contexts>
<context position="11879" citStr="Ueffing et al., 2002" startWordPosition="1972" endWordPosition="1975">to the candidate repositories if they are ranked among the top N candidates. The relation K = N holds therefore only in the first iteration. From the second iteration on, K is usually larger than N. The sequence of line optimizations and decodings is repeated until (1) the candidate repositories remain unchanged and (2) γopt~0. e6 e7 ep e5 e6 1 ep e8 0 e Score Error count e5 ea 1 e ey Y 727 3 Minimum Error Rate Training on Lattices In this section, the algorithm for computing the upper envelope on N-best lists is extended to phrase lattices. For a description on how to generate lattices, see (Ueffing et al., 2002). Formally, a phrase lattice for a source sentence f is defined as a connected, directed acyclic graph Gf~p = (Vf, Efq) with vertice set Vf, unique source and sink nodes s, tP c Vf, and a set of arcs Ef€ c Vf~ x Vf. Each arc is labeled with a phrase ϕij = ei1, ..., eij and the (local) feature function values hM(ϕij, fq). A path 7r = (v0, E0, v1, E1, ..., En— 1, vnq) in Gf (with EiP c Ef and vi, vz+1P c Vf as the tail and head of Ei, 0¤ &lt;, i &lt; n) defines a partial translation eπ of f which is the concatenation of all phrases along this path. The corresponding feature function values are obtaine</context>
</contexts>
<marker>Ueffing, Och, Ney, 2002</marker>
<rawString>N. Ueffing, F. J. Och, and H. Ney. 2002. Generation of word graphs in statistical machine translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 156– 163, Philadelphia, PE, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Watanabe</author>
<author>J Suzuki</author>
<author>H Tsukada</author>
<author>H Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>764--773</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="28036" citStr="Watanabe et al., 2007" startWordPosition="4804" endWordPosition="4807">ful insights into the optimization process. In (Duh and Kirchhoff, 2008), MERT is used to boost the BLEU score on Table 1: Corpus statistics for three text translation sets: Arabic-to-English (aren), Chinese-to-English (zhen), and English-to-Chinese (enzh). Development and test data are compiled from evaluation data used in past NIST Machine Translation Evaluations. data set collection # of sentences enzh aren zhen dev1 nist02 1043 878 – dev2 nist04 1353 1788 – blind nist08 1360 1357 1859 N-best re-ranking tasks. The incorporation of a large number of sparse feature functions is described in (Watanabe et al., 2007). The paper investigates a perceptron-like online large-margin training for statistical machine translation. The described approach is reported to yield significant improvements on top of a baseline system which employs a small number of feature functions whose weights are optimized under the MERT criterion. A study which is complementary to the upper bound on the size of envelopes derived in Section 4 is provided in (Elizalde and Woods, 2006) which shows that the number of inference functions of any graphical model as, for instance, Bayesian networks and Markov random fields is polynomial in </context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki. 2007. Online large-margin training for statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 764–773, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>S Hasan</author>
<author>H Ney</author>
</authors>
<title>A Systematic Comparison of Training Criteria for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="26692" citStr="Zens et al., 2007" startWordPosition="4596" endWordPosition="4599">unctions. This problem can be alleviated by exploring a large number of random directions which update many feature weights simultaneously. The random directions are chosen as the lines which connect some randomly distributed points on the surface of an M-dimensional hypersphere with the hypersphere’s center. The center of the hypersphere is defined as the initial parameter set. 6 Related Work As suggested in (Och, 2003), an alternative method for the optimization of the unsmoothed error count is Powell’s algorithm combined with a grid-based line optimization (Press et al., 2007, p. 509). In (Zens et al., 2007), the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503). The optimization procedure allows for optimizing other objective function as, e.g., the expected BLEU score. A weakness of the Downhill Simplex algorithm is, however, its decreasing robustness for optimization problems in more than 10 dimensions. A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface.</context>
</contexts>
<marker>Zens, Hasan, Ney, 2007</marker>
<rawString>R. Zens, S. Hasan, and H. Ney. 2007. A Systematic Comparison of Training Criteria for Statistical Machine Translation. In Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zollmann</author>
<author>A Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In NAACL ’06: Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>138--141</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York, NY,</location>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>A. Zollmann and A. Venugopal. 2006. Syntax augmented machine translation via chart parsing. In NAACL ’06: Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 138–141, New York, NY, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>