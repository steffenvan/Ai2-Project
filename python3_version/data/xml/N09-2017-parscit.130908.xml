<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.070827">
<title confidence="0.9989295">
Evaluation of a System for Noun Concepts Acquisition from Utterances
about Images (SINCA) Using Daily Conversation Data
</title>
<author confidence="0.992359">
Yuzu UCHIDA
</author>
<affiliation confidence="0.997975333333333">
Graduate School of
Information Science and Technology
Hokkaido University
</affiliation>
<address confidence="0.663499">
Sapporo, 060-0814, Japan
</address>
<email confidence="0.995863">
yuzu@media.eng.hokudai.ac.jp
</email>
<author confidence="0.928287">
Kenji ARAKI
</author>
<affiliation confidence="0.993426666666667">
Graduate School of
Information Science and Technology
Hokkaido University
</affiliation>
<address confidence="0.815113">
Sapporo, 060-0814, Japan
</address>
<email confidence="0.999216">
araki@media.eng.hokudai.ac.jp
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995195">
For a robot working in an open environment,
a task-oriented language capability will not
be sufficient. In order to adapt to the en-
vironment, such a robot will have to learn
language dynamically. We developed a Sys-
tem for Noun Concepts Acquisition from ut-
terances about Images, SINCA in short. It is
a language acquisition system without knowl-
edge of grammar and vocabulary, which learns
noun concepts from user utterances. We
recorded a video of a child’s daily life to
collect dialogue data that was spoken to and
around him. The child is a member of a fam-
ily consisting of the parents and his sister. We
evaluated the performance of SINCA using
the collected data. In this paper, we describe
the algorithms of SINCA and an evaluation
experiment. We work on Japanese language
acquisition, however our method can easily be
adapted to other languages.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999865923076923">
There are several other studies about language ac-
quisition systems. Rogers et al. (1997) proposed
”Babbette”, which learns language rules from pro-
vided examples. Levinson et al. (2005) describe
their research with a robot which acquires language
from interaction with the real world. Kobayashi et
al. (2002) proposed a model for child vocabulary ac-
quisition based on an inductive logic programming
framework. Thompson (1995) presented a lexical
acquisition system that learns a mapping of words
to their semantic representation from training exam-
ples consisting of sentences paired with their seman-
tic representations.
As mentioned above, researchers are interested in
making a robot learn language. Most studies seem
to be lacking in the ability to adapt to the real world.
In addition, they should be more independent from
language rules. We believe that it is necessary to
simulate human language ability in order to create a
complete natural language understanding system.
As the first step in our research, we devel-
oped a System for Noun Concepts Acquisition from
utterances about Images, called SINCA in short
(which means ”evolution” in Japanese) (Uchida et
al., 2007). It is a language acquisition system with-
out knowledge of grammar and vocabulary, which
learns noun concepts from a user’s input. SINCA
uses images as a meaning representation in order to
eliminate ambiguity of language. SINCA can only
acquire concrete nouns.
Currently, SINCA is for Japanese only. The lan-
guage acquisition method of this system is very gen-
eral and it is independent of language rules. SINCA
is expected to work successfully using any language.
In this paper, we describe the algorithms of
SINCA and an experiment to test what kind of input
would be appropriate for our system. We would em-
phasize that we prepared a large video data of daily
life of a family with young children.
</bodyText>
<sectionHeader confidence="0.985559" genericHeader="method">
2 The Algorithms of SINCA
</sectionHeader>
<bodyText confidence="0.99536025">
Figure 1 shows the SINCA user interface. The situ-
ation shown in Fig.1 is that the affection of SINCA
is directed to an eraser by the user, and after the
recognition process, SINCA asks ”KESHIGOMU?
</bodyText>
<page confidence="0.995794">
65
</page>
<subsubsectionHeader confidence="0.80526">
Proceedings of NAACL HLT 2009: Short Papers, pages 65–68,
</subsubsectionHeader>
<bodyText confidence="0.956978666666667">
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
Kore-ha KAPPU-tte iu-n-da-yo.
(This is a thing called a cup.)
KAPPU-ni gyunyu ireyoka.
(Let’s pour some milk into the cup.)
Strings indicated by boldface are labels.
</bodyText>
<figureCaption confidence="0.949965">
Figure 1: The SINCA Interface recognizing an eraser Figure 2: Examples of input data
</figureCaption>
<bodyText confidence="0.931903">
(Eraser?).”
We describe SINCA’s process in detail in the fol-
lowing subsections.
</bodyText>
<subsectionHeader confidence="0.732062">
2.1 Input
</subsectionHeader>
<bodyText confidence="0.996386476190476">
A user input consists of an image paired with a spo-
ken utterance.
First, a user chooses an object O which he or she
likes and captures an image of it with a web camera
with 300,000 pixels effective sensor resolution. The
user has to try to capture the whole object O in the
image.
Next, a user imagines an utterance that an infant
might be exposed to when listening to caregivers
while gazing at the object O in the environment.
The user enters the utterance on the keyboard as a
linguistic input. The linguistic input is written in
Hiragana, which are Japanese phonetic characters,
to avoid the linguistic input containing some direct
meanings as in the case of Chinese Kanji ideograms.
This is also intended to standardize the transcrip-
tion. SINCA does not carry out morphological anal-
ysis of the linguistic input, because we believe that
infant capability for word segmentation is not per-
fect (Jusczyk et al., 1999).
Figure 2 shows some example inputs. 1
</bodyText>
<subsectionHeader confidence="0.99644">
2.2 Image Processing
</subsectionHeader>
<bodyText confidence="0.999923">
The ERSP 3.1 Software Development Kit 2 provides
cutting edge technologies for vision, navigation, and
</bodyText>
<footnote confidence="0.98742925">
1The Japanese words are written in italics in all following
figures.
2Evolution Robotics, Inc.:ERSP 3.1 Robotic Development
Platform OEM Software by Evolution Robotics
</footnote>
<bodyText confidence="0.996331857142857">
system development. ERSP Vision included in the
ERSP enables a robot or device to recognize 2D and
3D objects in real world settings where lighting and
placement are not controlled. We use the ERSP vi-
sion for image processing. ERSP Vision informs the
system whether the object in the present input image
appears in the previously input images or not.
</bodyText>
<subsectionHeader confidence="0.996834">
2.3 Common Parts
</subsectionHeader>
<bodyText confidence="0.999650857142857">
When a user inputs an image of an object O and
an utterance, the system extracts all sections of the
string matching section of previously input utter-
ances accompanied by the image of the same object
O. We call these strings common parts. After this
process, the system deals with them as candidates
for a label for the object O.
The system provides every common part with a
”basic score”. The basic score is based on frequency
of appearance and the number of characters, and in-
dicates how appropriate as a label the common part
is. The higher the score, the more appropriate the
common part is. The basic score is defined as fol-
lows:
</bodyText>
<equation confidence="0.997814">
F √
SCORE = α × PN × L
(1)
</equation>
<bodyText confidence="0.9997365">
where, α is a coefficient which reduces the basic
score if the common part has appeared with other
objects than O, F is frequency of appearance of the
common part with the images of O, PN is the num-
ber of use inputs with images of O, and L is the num-
ber of characters of the common part.
</bodyText>
<page confidence="0.948011">
66
</page>
<subsectionHeader confidence="0.775585">
2.4 Output
</subsectionHeader>
<bodyText confidence="0.998209">
If the system finds a common part whose basic score
exceeds a threshold, it outputs it as text. The reason
for doing this is the assumption that there is a high
possibility that such common parts are appropriate
as labels.
A user evaluates an output by choosing one of the
following keywords:
</bodyText>
<listItem confidence="0.999921">
• Good : It is appropriate as a label.
• Almost : It makes some sense but is not
proper for the label.
• Bad : It makes no sense.
</listItem>
<bodyText confidence="0.999786">
Infants cannot understand these keywords com-
pletely, but they can get a sense of some meanings
from the tone of an adult’s voice or facial expres-
sions. In our research, we use the keywords as a
substitute for such information. The system recalcu-
lates the basic score based on the keyword chosen by
the user. Specifically, the system multiplies the basic
score by the coefficient β dependent on the keyword.
</bodyText>
<subsectionHeader confidence="0.996502">
2.5 Acquisition of the Noun Concepts
</subsectionHeader>
<bodyText confidence="0.99999525">
After repeating these processes, if there is a com-
mon part whose score is more than 30.0 and which
has been rated as ”Good”, the system acquires the
common part as the label for O.
</bodyText>
<subsectionHeader confidence="0.992402">
2.6 Label Acquisition Rules
</subsectionHeader>
<bodyText confidence="0.999811777777778">
Humans can use their newfound knowledge to learn
their native language effectively. This system imi-
tates humans’ way with ”label acquisition rules”.
A label acquisition rule is like a template, which
enables recursive learning for acquisition of noun
concepts. The system generates label acquisition
rules after acquisition of a label. When the system
acquires a string S as a label for an object, the system
picks up the previous linguistic inputs with the im-
ages of the object which contain the string S. Then,
the system replaces the string S in the linguistic in-
puts with a variable ”&apos;y”. These abstracted sentences
are called label acquisition rules. An example of the
label acquisition rules is shown in Fig.3.
If the rules match other parts of previoiusly input
strings, the parts corresponding to the ”&apos;y” variable
are extracted. The scores of these extracted strings
are then increased.
</bodyText>
<figure confidence="0.894020833333333">
Acquired Label : WAN-CHAN (a doggy)
Previous Input : Acchi-ni WAN-CHAN-ga iru-yo.
(There is a doggy over there.)
Label Acquisition Rule : Acchi-ni γ1-ga iru-yo.
(There is γ1 over there.)
Strings indicated by boldface are labels.
</figure>
<figureCaption confidence="0.996534">
Figure 3: An example of a label acquisition rule
</figureCaption>
<sectionHeader confidence="0.993718" genericHeader="method">
3 Evaluation Experiment
</sectionHeader>
<bodyText confidence="0.999872666666667">
We carried out an experiment to test what kinds of
input would be appropriate for SINCA. This section
describes the experiment.
</bodyText>
<subsectionHeader confidence="0.993371">
3.1 Experimental Procedure
</subsectionHeader>
<bodyText confidence="0.9999564">
Two types of linguistic input data were collected
in two different ways: a questionnaire and a video
recording. We had SINCA acquire labels for 10 im-
ages using the linguistic input data. The following
are the details about the data collection methods.
</bodyText>
<subsectionHeader confidence="0.895403">
3.1.1 Questionnaire
</subsectionHeader>
<bodyText confidence="0.999970375">
10 images were printed on the questionnaire, and
it asked ”What would you say to a young child if
he or she pays attention to these objects?”. The re-
spondents are allowed to answer with whatever they
come up with. 31 people responded to this question-
naire, and 13 of them have children of their own.
We collected 324 sentences, and the average mora
length of them was 11.0.
</bodyText>
<subsectionHeader confidence="0.888985">
3.1.2 Video recording
</subsectionHeader>
<bodyText confidence="0.999937571428571">
We recorded a video of a child’s daily life to col-
lect dialogue data that was spoken to and around
him. The child is a member of a family consisting
of his parents and his sister.
The recordings are intended to collect daily con-
versation, therefore we did not set any tasks. The
total recording period comprised 125 days and we
recorded about 82 hours of video data. The first au-
thor watched about 26 hours of the video data, and
wrote parents’ dictation in Hiragana. We selected
353 sentences for linguistic input data that were spo-
ken when joint attention interactions between a par-
ent and a child were recognized. On average, their
mora length was 9.8.
</bodyText>
<page confidence="0.998506">
67
</page>
<subsectionHeader confidence="0.997854">
3.2 Experimental Result
</subsectionHeader>
<bodyText confidence="0.999965583333333">
We input sentences from the collected inputs one at
a time until SINCA acquired a noun concept for an
image. SINCA was able to acquire labels for 10 im-
ages, with each type of linguistic input. When we
used the questionnaire data, SINCA needed on aver-
age 6.2 inputs to acquire one label, and SINCA ac-
quired 52 rules through the experiment. They cover
83.8% of the total number of inputs. When we used
the video data, SINCA needed on average 5.3 inputs
to acquire one label, and SINCA acquired 44 rules
through the experiment. They cover 83.0% of the
total number of inputs.
</bodyText>
<subsectionHeader confidence="0.986146">
3.3 Considerations
</subsectionHeader>
<bodyText confidence="0.999990464285714">
The experimental results indicate that using video
data makes the acquisition of labels more efficient.
There are 3 factors that contribute to this.
The first factor is the number of one-word sen-
tences. There are 66 one-word sentences in the
video data (18.6% of the total). Therefore, the length
of the sentences from the video data tends to be
short.
The second factor is the lack of particles. The re-
spondents of the questionnaire hardly ever omit par-
ticles. By contrast, of the 53 sentences which were
input, 23 sentences lack particles (42.6% of the to-
tal) in video data. Spoken language is more likely
to have omitted particles compared with written lan-
guage.
The third factor is the variety of words. We ran-
domly selected 100 sentences from both sets of lin-
guistic input data and checked the words adjacent to
a label. Table 1 shows the number of different words
that occur adjacent to a label. Because the respon-
dents of the questionnaire all try to explain some-
thing in an image, they use similar expressions.
When SINCA uses the video data, it can extract
labels more easily than using the questionnaire data
because of the factors listed above. This means that
SINCA is well suited for spoken language. If we
assume one application of SINCA is for communi-
cation robots, this result is promising.
</bodyText>
<sectionHeader confidence="0.998135" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9581405">
In this paper, we described the algorithms of
SINCA. SINCA can acquire labels for images with-
</bodyText>
<tableCaption confidence="0.979614">
Table 1: Variety of words
</tableCaption>
<equation confidence="0.956653">
Previous(WA) following(WB)
Video 19 42
Questionnaire 15 22
Sentence: W1 W2 ... WA label WB ... .
</equation>
<bodyText confidence="0.99991875">
out ready-made linguistic resources, lexical infor-
mation, or syntactic rules. Additionally, it targets
images of real world objects.
We collected linguistic input data in two ways.
One method is videos of a family’s daily life. The
other method is a questionnaire. We had SINCA ac-
quire noun concepts using both video and question-
naire data. As a result, we have showed that spoken
language is well suited to SINCA’s algorithm for ac-
quiring noun concepts.
In the next step, we will focus on acquisition of
adjectives.
</bodyText>
<sectionHeader confidence="0.999415" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999609291666667">
Jusczyk, P. W. Houston, D. M. and Newsome, M. 1999.
The beginnings of word segmentation in english-
learning infants. Cognitive Psychology. 39. pp.159–
207.
Kobayashi, I. Furukawa, K. Ozaki, T. and Imai, M.
2002. A Computational Model for Children’s Lan-
guage Acquisition Using Inductive Logic Program-
ming. Progress in Discovery Science. 2281 pp.140–
155.
Levinson S. E. Squire, K. Lin, R. S. and McClain, M.
2005. Automatic language acquisition by an au-
tonomous robot. AAAI Spring Symposium on Devel-
opmental Robotics.
Rogers, P. A. P. and Lefley, M. 1997. The baby project.
Machine Conversations. ed. Wilks, Y. Kluwer Aca-
demic Publishers.
Thompson, C, A. 1997. Acquisition of a Lexicon from
Semantic Representations of Sentences. Proceedings
of the 33rd Annual Meeting of the Association for
Computational Linguistics. pp.335–337.
Uchida, Y. and Araki, K. 2007. A System for Acquisition
of Noun Concepts from Utterances for Images Using
the Label Acquisition Rules. Springer-Verlag Lecture
Notes in Artificial Intelligence (LNAI). pp.798–802.
</reference>
<page confidence="0.999445">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.475491">
<title confidence="0.981456">Evaluation of a System for Noun Concepts Acquisition from about Images (SINCA) Using Daily Conversation Data</title>
<author confidence="0.915824">Yuzu UCHIDA</author>
<affiliation confidence="0.980387333333333">Graduate School of Information Science and Technology Hokkaido University</affiliation>
<address confidence="0.999565">Sapporo, 060-0814, Japan</address>
<email confidence="0.976741">yuzu@media.eng.hokudai.ac.jp</email>
<author confidence="0.942806">Kenji ARAKI</author>
<affiliation confidence="0.863293">Graduate School Information Science and Hokkaido</affiliation>
<address confidence="0.997208">Sapporo, 060-0814, Japan</address>
<email confidence="0.978701">araki@media.eng.hokudai.ac.jp</email>
<abstract confidence="0.998937476190476">For a robot working in an open environment, a task-oriented language capability will not be sufficient. In order to adapt to the environment, such a robot will have to learn language dynamically. We developed a System for Noun Concepts Acquisition from utterances about Images, SINCA in short. It is a language acquisition system without knowledge of grammar and vocabulary, which learns noun concepts from user utterances. We recorded a video of a child’s daily life to collect dialogue data that was spoken to and around him. The child is a member of a family consisting of the parents and his sister. We evaluated the performance of SINCA using the collected data. In this paper, we describe the algorithms of SINCA and an evaluation experiment. We work on Japanese language acquisition, however our method can easily be adapted to other languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P W Houston Jusczyk</author>
<author>D M</author>
<author>M Newsome</author>
</authors>
<title>The beginnings of word segmentation in englishlearning infants.</title>
<date>1999</date>
<journal>Cognitive Psychology.</journal>
<volume>39</volume>
<pages>159--207</pages>
<contexts>
<context position="4772" citStr="Jusczyk et al., 1999" startWordPosition="766" endWordPosition="769">gines an utterance that an infant might be exposed to when listening to caregivers while gazing at the object O in the environment. The user enters the utterance on the keyboard as a linguistic input. The linguistic input is written in Hiragana, which are Japanese phonetic characters, to avoid the linguistic input containing some direct meanings as in the case of Chinese Kanji ideograms. This is also intended to standardize the transcription. SINCA does not carry out morphological analysis of the linguistic input, because we believe that infant capability for word segmentation is not perfect (Jusczyk et al., 1999). Figure 2 shows some example inputs. 1 2.2 Image Processing The ERSP 3.1 Software Development Kit 2 provides cutting edge technologies for vision, navigation, and 1The Japanese words are written in italics in all following figures. 2Evolution Robotics, Inc.:ERSP 3.1 Robotic Development Platform OEM Software by Evolution Robotics system development. ERSP Vision included in the ERSP enables a robot or device to recognize 2D and 3D objects in real world settings where lighting and placement are not controlled. We use the ERSP vision for image processing. ERSP Vision informs the system whether th</context>
</contexts>
<marker>Jusczyk, M, Newsome, 1999</marker>
<rawString>Jusczyk, P. W. Houston, D. M. and Newsome, M. 1999. The beginnings of word segmentation in englishlearning infants. Cognitive Psychology. 39. pp.159– 207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Furukawa Kobayashi</author>
<author>K Ozaki</author>
<author>T</author>
<author>M Imai</author>
</authors>
<title>A Computational Model for Children’s Language Acquisition Using Inductive Logic Programming.</title>
<date>2002</date>
<booktitle>Progress in Discovery Science.</booktitle>
<pages>2281--140</pages>
<contexts>
<context position="1584" citStr="Kobayashi et al. (2002)" startWordPosition="238" endWordPosition="241">f a family consisting of the parents and his sister. We evaluated the performance of SINCA using the collected data. In this paper, we describe the algorithms of SINCA and an evaluation experiment. We work on Japanese language acquisition, however our method can easily be adapted to other languages. 1 Introduction There are several other studies about language acquisition systems. Rogers et al. (1997) proposed ”Babbette”, which learns language rules from provided examples. Levinson et al. (2005) describe their research with a robot which acquires language from interaction with the real world. Kobayashi et al. (2002) proposed a model for child vocabulary acquisition based on an inductive logic programming framework. Thompson (1995) presented a lexical acquisition system that learns a mapping of words to their semantic representation from training examples consisting of sentences paired with their semantic representations. As mentioned above, researchers are interested in making a robot learn language. Most studies seem to be lacking in the ability to adapt to the real world. In addition, they should be more independent from language rules. We believe that it is necessary to simulate human language ability</context>
</contexts>
<marker>Kobayashi, Ozaki, T, Imai, 2002</marker>
<rawString>Kobayashi, I. Furukawa, K. Ozaki, T. and Imai, M. 2002. A Computational Model for Children’s Language Acquisition Using Inductive Logic Programming. Progress in Discovery Science. 2281 pp.140– 155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Levinson S E Squire</author>
<author>K Lin</author>
<author>R S</author>
<author>M McClain</author>
</authors>
<title>Automatic language acquisition by an autonomous robot. AAAI Spring Symposium on Developmental Robotics.</title>
<date>2005</date>
<marker>Squire, Lin, S, McClain, 2005</marker>
<rawString>Levinson S. E. Squire, K. Lin, R. S. and McClain, M. 2005. Automatic language acquisition by an autonomous robot. AAAI Spring Symposium on Developmental Robotics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A P Rogers</author>
<author>M Lefley</author>
</authors>
<title>The baby project. Machine Conversations.</title>
<date>1997</date>
<editor>ed. Wilks, Y.</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Rogers, Lefley, 1997</marker>
<rawString>Rogers, P. A. P. and Lefley, M. 1997. The baby project. Machine Conversations. ed. Wilks, Y. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Thompson</author>
<author>A</author>
</authors>
<title>Acquisition of a Lexicon from Semantic Representations of Sentences.</title>
<date>1997</date>
<booktitle>Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics. pp.335–337.</booktitle>
<marker>Thompson, A, 1997</marker>
<rawString>Thompson, C, A. 1997. Acquisition of a Lexicon from Semantic Representations of Sentences. Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics. pp.335–337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Uchida</author>
<author>K Araki</author>
</authors>
<title>A System for Acquisition of Noun Concepts from Utterances for Images Using the Label Acquisition Rules.</title>
<date>2007</date>
<booktitle>Springer-Verlag Lecture Notes in Artificial Intelligence (LNAI).</booktitle>
<pages>798--802</pages>
<marker>Uchida, Araki, 2007</marker>
<rawString>Uchida, Y. and Araki, K. 2007. A System for Acquisition of Noun Concepts from Utterances for Images Using the Label Acquisition Rules. Springer-Verlag Lecture Notes in Artificial Intelligence (LNAI). pp.798–802.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>