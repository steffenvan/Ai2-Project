<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.026823">
<note confidence="0.932809">
Proceedings of HLT-NAACL 2003
Student Research Workshop , pp. 1-6
Edmonton, May-June 2003
</note>
<title confidence="0.9993745">
Semantic Language Models for
Topic Detection and Tracking
</title>
<author confidence="0.991121">
Ramesh Nallapati
</author>
<affiliation confidence="0.992123666666667">
Center for Intelligent Information Retrieval,
Department of Computer Science,
University of Massachusetts,
</affiliation>
<address confidence="0.812831">
Amherst, MA 01003.
</address>
<email confidence="0.996562">
nmramesh@cs.umass.edu
</email>
<sectionHeader confidence="0.996633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947666666667">
In this work, we present a new semantic lan-
guage modeling approach to model news sto-
ries in the Topic Detection and Tracking (TDT)
task. In the new approach, we build a unigram
language model for each semantic class in a
news story. We also cast the link detection sub-
task of TDT as a two-class classification prob-
lem in which the features of each sample con-
sist of the generative log-likelihood ratios from
each semantic class. We then compute a lin-
ear discriminant classifier using the perceptron
learning algorithm on the training set. Results
on the test set show a marginal improvement
over the unigram performance, but are not very
encouraging on the whole.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999862466666667">
TDT is a research program investigating methods for au-
tomatically organizing news stories by the events that
they discuss (Allan, 2002a). The goal of TDT consists of
breaking the stream of news into individual news stories,
to monitor the stories for events that have not been seen
before and to gather stories into groups that each discuss
a single topic.
Several approaches have been explored for compar-
ing news stories in TDT. The traditional vector space ap-
proach (Yang et al., 1999) using cosine similarity has by
far been the most consistently successful approach across
different tasks and several data sets.
In the recent past, a new probabilistic approach called
Language Modeling (Ponte and Croft, 1998) has proven
to be very effective in several information retrieval tasks.
One of the attractive features of language models is that
they are firmly rooted in the theory of probability thereby
allowing a researcher to explore more sophisticated mod-
els guided by the theoretical framework.
Allan et al (Allan et al., 1999) applied language mod-
els to the first story detection task of TDT and found that
its performance is on par with the traditional vector space
models, if not better. In the language modeling approach
to TDT, we measure the similarity of a news story D to
a topic by the probability of its generation from the topic
model M. Using the unigram assumption of indepen-
dence of terms, one can compute the probability of gen-
eration of a news story as the product of probabilities of
generation of the terms in the story, as shown in the fol-
lowing equation:
</bodyText>
<equation confidence="0.998741">
P(DIM) = � IDI P(wi|M) (1)
i=1
</equation>
<bodyText confidence="0.999982965517242">
where wi is the i-th term in the story. The topic model M
is typically evaluated from the statistics of a set of stories
that are known to be on the topic in consideration.
One potential drawback of the unigram language
model is that it treats all terms on an equal footing and
seems to ignore semantic information of the terms. We
believe that such information could be useful in determin-
ing the relative importance of a term to the topic of the
story. For example, terms that belong to the named-entity
type such as person, location, organization may convey
more information about the topic of the story than other
entity types. Likewise, one might expect that nouns and
verbs play a more important role than adjectives, adverbs
or propositions in determining the topic of the story.
The present work is an attempt to extend the language
modeling framework to incorporate a model of the rela-
tive importance of terms according to the semantic class
they belong to.
The remainder of the report is organized as follows.
Section 2 summarizes attempts made in the past in cap-
turing semantic-class information in information retrieval
related tasks. We present the methodology of the new se-
mantic language modeling approach in section 3. In sec-
tion 4, we present details of the link detection task and
its evaluation. Section 5 describes the experiments per-
formed and presents the results obtained. In section 6
we analyze the performance of the new model. Section
7 ends the discussion with a few observations and lays
down the path to future work.
</bodyText>
<sectionHeader confidence="0.998095" genericHeader="method">
2 Past work
</sectionHeader>
<bodyText confidence="0.999823764705882">
Traditionally NLP techniques have not met with much
success in the IR domain. However, after several ad-
vances in tasks such as automatic tagging of text with
high level semantics such as parts-of-speech (Ratna-
parkhi, 1996), named-entities (Bikel et al., 1999),
sentence-parsing (Charniak, 1997), etc., there is increas-
ing hope that one could leverage this information into IR
techniques. Traditional vector space models (Salton et
al., 1975) and the more recent language models (Ponte
and Croft, 1998) tend to ignore any semantic information
and consider only word-tokens or word-stems as basic
features.
We know of no prior work in the language model-
ing framework that tries to incorporate semantic informa-
tion into IR models. However, in vector space modeling
framework, there have been a few attempts. For example,
Allan, et al (Allan et al., 1999) use an ad-hoc weight-
ing scheme to weight named-entities higher than other
tokens in their vector space models for the new event de-
tection task of TDT. They do not report any significant
improvements in their results. Additionally, the weight-
ing scheme is empirical and they present no principled
approach to compute the weights.
In the field of ad-hoc retrieval, emerging research on
integrating NLP tools into retrieval models seems encour-
aging. Mihalcea and Mihalcea (Mihalcea and Mihalcea,
2001) show that retrieval effectiveness can be improved
by indexing words with their semantic classes such as
parts-of-speech, named-entity-type, WordNet synonyms,
hypernyms, hyponyms, etc.
In this work, we present a principled approach to inte-
grating semantic information into the language modeling
framework and show how to compute the relative impor-
tance of various semantic classes automatically.
</bodyText>
<sectionHeader confidence="0.98571" genericHeader="method">
3 Semantic language models
</sectionHeader>
<bodyText confidence="0.999970228571429">
Recall that our task involves analyzing and comparing the
content of news stories by the topics that they discuss.
The topic of a news story is typically characterized by an
event, one or more key players which may include per-
sons or organizations (the who? of the event), a location
to which the event is associated (the where? of the event),
a time of occurrence of the event (the when? of the event)
and a description of the event (the what? of the event).
Hence, when comparing news stories, it makes sense to
compare those features between the stories that answer
the above mentioned four ‘wh’ questions (Allan et al.,
2002b). However, extracting these features may not be
a trivial task. It may need a deep understanding of the
semantics of the story.
As a first step towards that end, we can leverage
the ability of statistical taggers that can recognize au-
tomatically all instances of named-entities such as per-
sons, locations, organizations, and parts-of-speech such
as nouns, verbs, adjectives, etc., in a news story. As an
approximation to our exact answers to the four ‘wh’ ques-
tions, we will assume that the set of tokens labeled as per-
sons and organizations by the taggers correspond to an
answer to the who? question, the set of dates correspond
to the when? question, the set of locations to the where?
question and lastly, the set of nouns, verbs and adjectives
to the what? question. Our hope is that these categories
of named-entities and parts-of-speech help us capture the
semantics of the news story. Hence we will address these
categories as semantic classes in this work and our model
as semantic language model. Our model is a two-stage
process in which the first stage involves computing class-
specific likelihood ratios while the second stage consists
of combining the ratios using a weighted perceptron. The
ensuing discussion presents the mathematical description
of the two stage process.
</bodyText>
<subsectionHeader confidence="0.999318">
3.1 Class-specific likelihood ratio
</subsectionHeader>
<bodyText confidence="0.9998408">
Let C = {Cl, .., C|C|} be the set of semantic classes.
Let C(w) be a relation that maps a given occurrence of
a word w to its semantic class C E C. Then, for any
story D, we define the list of features Fi(D) that belong
to class Ci as follows:
</bodyText>
<equation confidence="0.925714">
�
Fi(D) = {w1,..,wn  |�n j=1 (wj E D) (C(wj) = Ci)}
</equation>
<bodyText confidence="0.9803538">
(2)
where n = |Fi(D)|. In other words, Fi(D) represents
the list of all tokens in the story D that fall into the cate-
gory Ci. Thus, each story is now represented as a set of
feature-lists of all the semantic-classes as shown below:
</bodyText>
<equation confidence="0.992157">
D =— {F1(D), ..., F|C|(D)} (3)
</equation>
<bodyText confidence="0.982267333333333">
For each semantic class Ci and story D, we define the
class-specific semantic language model Mi(D) as fol-
lows:
</bodyText>
<equation confidence="0.7685695">
P(w|Mi(D)) = f(w, Fi(D))
|Fi(D) |+(1−)f(w, Fi(GE))
|Fi(GE)|
(4)
</equation>
<bodyText confidence="0.988979294117647">
where f(w, Fi(D)) is the number of occurrences of a
word w in a story D in the class Ci and GE is a gen-
eral English collection, while  is a smoothing parameter
that lies between 0 and 1. Thus, the class-specific se-
mantic language model Mi(D) is a smoothed probability
distribution of words in class Ci of story D. This is anal-
ogous to the standard document language models used by
IR researchers.
Given two stories D1 and D2, the semantic class spe-
cific likelihood of D2 with respect to D1 is given by:
(5)
where n = |Fi(D2)|. We compute the log-
likelihood ratio instead of just the generative probability
P(D2|M(D1)) to overcome the tendency of the genera-
tive probability to favor shorter stories.
The generative semantic-class-specific general English
model is given by:
</bodyText>
<equation confidence="0.869454">
P(w|Mi(GE)) = f(w,Fi(GE)) (6)
|Fi(GE)|
</equation>
<subsectionHeader confidence="0.99821">
3.2 Weighted Perceptron approach
</subsectionHeader>
<bodyText confidence="0.999916461538462">
Now, all that remains to be done is to com-
bine the semantic class-specific log-likelihood scores
[L1(D2|D1),..,L|C|(D2|D1)]T in a principled way to
obtain the overall similarity score of D1 with respect to
D2. Towards that end, we cast the link detection task as
a two-class classification problem, the two classes being
‘on-topic’ and ‘off-topic’. In other words, each story-
pair (D1, D2) is a sample and the classification task in-
volves assigning the label ‘on-topic’ or ‘off-topic’ to the
story pair. We compute the semantic-class-specific log-
likelihood scores for all classes and treat them as com-
ponents of the feature vector x of the sample as shown
below:
</bodyText>
<equation confidence="0.994493">
xi(D1, D2) = Li(D2|D1) (7)
</equation>
<bodyText confidence="0.999965">
We use a linear discriminant function that is a linear
combination of the components of x for classification as
shown in the following equation:
</bodyText>
<equation confidence="0.996132">
g(y) = wTy (8)
</equation>
<bodyText confidence="0.9988447">
where y is the augmented feature vector given by y =
[1, x]T, w = [w0, w1,.., w|C|]T is the weight vector. In
particular w0 is called the bias or threshold weight. For
a discriminant function of the form of equation 8, a two-
class classifier implements the following decision rule:
Decide ‘on-topic’ if g(y) &gt; 0 and ‘off-topic’ otherwise.
The linear discriminant function clearly constitutes a per-
ceptron. Figure 1 shows a graphical representation of
the perceptron that takes the semantic-class-specific log-
likelihood scores as input.
</bodyText>
<figureCaption confidence="0.925028">
Figure 1: A graphical representation of the semantic lan-
guage model
</figureCaption>
<bodyText confidence="0.999803761904762">
As the figure indicates, for each story pair
(D1, D2), we build semantic-class-specific models
[M1(D1),..,M|C|(D1)] from story D1 as given by
equation 4. We also construct the semantic class-specific
feature lists {F1(D2),..,F|C|(D2)} from story D2 as
defined in equation 2 and then compute the feature vector
x = [L1(D2|D1), .., L|C|(D2|D1)]T where each com-
ponent likelihood ratio is computed as given in equation
5. We then perform an inner product of the augmented
feature vector y and the weight vector w of the percep-
tron and the resulting score is output as shown in the
figure.
The standard perceptron learns the optimal weight vec-
tor w by minimizing its misclassification rate on a train-
ing set (Duda et al., 2000). However, in TDT, misses
(classification of an on-topic story pair as off-topic) are
penalized 10 times more strongly than false alarms (clas-
sification of an off-topic story pair as on-topic) (Allan,
2002a). We have therefore incorporated these penalties
into the criterion function to force the perceptron learn
the optimal classification based on TDT’s cost function.
</bodyText>
<sectionHeader confidence="0.986149" genericHeader="method">
4 Link Detection task and Evaluation
</sectionHeader>
<bodyText confidence="0.999718533333333">
In this section, we describe one of the tasks called link de-
tection, on which we performed the experiments reported
in this work.
Link detection requires determining whether or not
two randomly selected stories (D1, D2) discuss the same
topic. The evaluation methodology of a link detection
system requires the system to output a score for each
story pair that represents the system’s confidence that
both stories in the pair discuss the same topic. The
system’s performance is then evaluated using a topic-
weighted Detection Error Trade-off (DET) curve (Mar-
tin et al., 1997) that plots miss rate against false alarm
over a large number of story pairs, at different values of
decision-threshold. A Link Detection cost function Clink
is then used to combine the miss and false alarm proba-
</bodyText>
<figure confidence="0.975867140350877">
Li(D2|D1) = ln( P(Fi(D2))|Mi(D1)
P (Fi (D2)) |Mi (GE) )
= ln( �n  |f
j=1 (P(wj |Mi(GE))) (wj ,Fi(D2)))
D
2
0
F
1
(D2 )
w
F
i
(D 2 )
:
wiLi
w1
(D 2
L
1
(D 2
|D
1
)
|D
1
)
E
:
:
w n
(D 2
|D
Ln
Fn
(D
2 )
)
1
D
1
)
M
(D
1
1
:
:
:
)
(D
1
Mn
)
(D
Mi
1
</figure>
<bodyText confidence="0.995054333333333">
bilities at each value of threshold into a single normalized
evaluation score (Allan, 2002a). We use the minimum
value of Clink as the primary measure of effectiveness
and show DET curves to illustrate the error trade-offs. It
may be useful for the reader to remember that, since the
DET curve is an error-tradeoff plot, the closer the curve
is to the origin, the better is the performance, unlike the
standard precision-recall curve familiar to the IR commu-
nity.
</bodyText>
<sectionHeader confidence="0.995482" genericHeader="method">
5 Experiments and results
</sectionHeader>
<bodyText confidence="0.9999839">
We have used Identifinder (Bikel et al., 1999) and Jtag
(Xu et al., 1994) respectively, to tag each term by its
named-entity type and its part of speech category. Addi-
tionally, we have used a list of 423 most frequent words
to remove stop words from stories. Stemming is done us-
ing the Porter stemmer (Porter, 1980) while the model is
implemented using Java.
As a training set, we have used a subset of TDT3
corpus that consists of news stories from eight English
sources collected roughly from October through Decem-
ber 1998. We have used manual transcriptions of stories
when the source is audio/video. The training set consists
of 7200 story pairs. For the general English model for this
set, we have used the same TDT3 natural English manu-
ally transcribed set consisting of 37,526 news stories.
For the test set, we have used a randomly chosen sub-
set of natural English, manually transcribed stories from
TDT2 corpus. It consists of 6,363 story pairs and the gen-
eral English statistics are derived from 40,851 stories.
In the unigram language modeling approach to link
detection, which we have used as baseline in our ex-
periments, we build a topic model M(D1) from one of
the stories D1 in the pair. We then compute the log-
likelihood ratio L(D21D1) of the second story D2 with
respect to M(D1) similar to equation 5 but considering
the entire document as a single feature list. The semantic
language model score, on the other hand, is computed as
described in section 3.
Sometimes we may use a symmetrized version of the
formula, as shown below:
</bodyText>
<equation confidence="0.9969155">
1
score(D1,D2) = 2(L(D2|D1) + L(D1|D2)) (9)
</equation>
<bodyText confidence="0.999551090909091">
However, in this work, we have considered only the
asymmetric version of the formula to maintain simplic-
ity of the scoring function. For fair comparison, we have
used an asymmetric version of the baseline unigram lan-
guage model too.
We have considered the categories in figure 2 as our
semantic classes. Note that only terms that are not classi-
fied as persons, organizations or locations are considered
as candidates for nouns. The numbers in the table indi-
cate the weight assigned by the perceptron to each class.
We have trained the perceptron using the 7200 labeled
story-pairs of the training set.
The class All corresponds to the unigram model and
consists of all the terms of the story. Note that some of
the classes are defined as the union of two or more sub-
classes. We have done this to nullify the labeling error of
the named-entity and parts-of-speech taggers. For exam-
ple, we have noticed that Identifinder mislabels Persons
as Organizations and vice versa quite frequently. Our
hope is that creating a new class that is a union of both
Persons and Organizations will offset such tagging er-
rors.
</bodyText>
<figure confidence="0.741330307692308">
Class Perceptron weight
Persons(P) 0.034998
Organizations(O) 0.0258486
Locations(L) 0.0374133
Nouns(N) 0.134969
Verbs(V) -7.99771e-05
Adjectives(A) 0.017435
Adverbs(Ad) -0.0010557
PUO 0.0647334
PUOUL 0.106417
N U V 0.138696
NUVUA 0.16157
All 0.279056
</figure>
<figureCaption confidence="0.995061">
Figure 2: Semantic classes and their weights
</figureCaption>
<bodyText confidence="0.98385552">
The optimum class-weights as learnt by the Percep-
tron offer some interesting insights. First we note that
the class All receives the highest weight and this seems
quite intuitive since this class contains all the informa-
tion in the story. However, somewhat surprisingly, the
class N U V U A receives higher weight than the class
P U O U L indicating that the former class contains more
topical information than the latter. Also, note that Per-
sons are more important than Locations which are in turn
more important than Organizations which seems to agree
with common sense.
Next we trained the unigram model on the training set
and found the optimum value of the smoothing parameter
A to be 0.2. We have used the same value for the smooth-
ing parameter in all the classes of the class-specific lan-
guage models and combined the class-specific likelihood
scores using the perceptron weights. A comparison of the
performance of semantic language model and unigram
model on the training set is shown in the DET curve of
figure 3. Quite disappointingly, the results indicate that
the overall performance as indicated by the minimum cost
in the DET curve has only worsened.
Figure 4 presents a comparison between unigram and
semantic language models on the test set. The smoothing
parameters and the perceptron weights are set to the val-
</bodyText>
<figureCaption confidence="0.957857125">
Figure 3: Comparison of semantic LM and unigram per-
formance on training set
ues learnt on the training set. This time, however, we note
that the minimum cost of the semantic language model is
slightly lower than that of the unigram model, but the im-
provement is very insignificant.
Figure 4: Comparison of semantic LM and unigram per-
formance on test set
</figureCaption>
<sectionHeader confidence="0.998082" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999870854166667">
In this section, we first briefly touch upon the variations
in the model we considered and the various experiments
we performed, but could not report in detail owing to
space constraints. Secondly we discuss why we think the
model’s performance is unsatisfactory.
We have considered a simple mixture model to start
with, wherein each class-specific model Mi(D1) gener-
ates a list of features in its class Fi(D2) but the model
itself is sampled with a prior probability of P (Mi(D1))
which we made dependent on |Fi(D1)|. This model’s
performance is found to be far below that of the uni-
gram approach and hence we abandoned it to favor the
perceptron-weighted likelihood ratios.
In terms of experiments done, we started out with the
basic semantic classes of P, O, L, N, V, A and Ad with-
out considering unions of the classes. We found that tak-
ing unions improved performance and we report the list
of classes whose combination performed the best.
Coming to the second part of our discussion, we are
yet to perform exploratory data analysis to understand the
reasons behind the unsatisfactory performance of the new
approach, but we believe the reasons could be three-fold:
Firstly, it is possible that we are operating the semantic
language model at a sub-optimal level. For example, we
have used the same value of the smoothing parameter that
we have learnt for the unigram model in all the classes of
the semantic language model. It is possible that different
classes may require different levels of smoothing for op-
timum performance. We believe one could use a gradient
descent algorithm on TDT’s cost function to learn from
the training set the optimum values of the smoothing pa-
rameters for different classes.
Secondly, a linear discriminant function that a percep-
tron implements is an overly simplistic classifier and may
not be doing a good job on separating the on-topic pairs
from the off-topic ones. A non-linear classifier such as an
SVM (Burges, 1998) could help improve our accuracy.
Lastly, it is possible that the unigram model is already
capturing the relative importance of terms that we are try-
ing to model using our semantic language models. The
likelihood ratio score we use in the unigram approach be-
haves similar to the tf-idf weights, which we know are
a powerful statistic to capture the relative importance of
terms. If this were true, then the semantic language model
may be rendered redundant.
The real reasons will only be revealed by an analysis
of the data and we hope to do this as part of our future
work.
</bodyText>
<sectionHeader confidence="0.998003" genericHeader="conclusions">
7 Conclusions and Future work
</sectionHeader>
<bodyText confidence="0.999914578947368">
In this work, we have presented a novel approach for link
detection task of TDT. The new approach has three key
ideas, namely modeling the relative importance of terms
by their semantic classes through a new semantic lan-
guage modeling approach, casting the link detection task
as a two-class classification problem and learning the op-
timum linear discriminant function using the perceptron
learning algorithm. We believe this is one of the earli-
est works that attempts incorporating semantic informa-
tion into the language modeling framework. Although
we have built the model specifically for the link detec-
tion task, it is general enough to be extended to the other
tasks of TDT such as Tracking, New Event Detection and
Clustering.
The results on train and test sets indicate that there is a
little or no improvement in the performance from the new
model as compared to the unigram approach.
As part of our future work, we would like to under-
stand the reasons behind the unsatisfactory performance
</bodyText>
<figure confidence="0.995536166666667">
Lambda = 0.2
Miss probability (in %)
90
40
20
10
5
2
1
.01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90
False Alarms probability (in %)
Random Performance
Perceptron weighted class-specific LM
Perceptron weighted class-specific LM TW Min DET Norm(Cost) = 0.1200
Unigram
Unigram TW Min DET Norm(Cost) = 0.1136
80
60
lambda = 0.2
Miss probability (in %)
90
40
20
10
5
2
1
.01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90
False Alarms probability (in %)
Random Performance
Perceptron weighted class-specific LM
Perceptron weighted class-specific LM TW Min DET Norm(Cost) = 0.1248
Unigram
Unigram TW Min DET Norm(Cost) = 0.1260
80
60
</figure>
<bodyText confidence="0.999972275862069">
of the new model and try out a few improvements sug-
gested in section 6. The possible improvements could
consist of finding the optimal smoothing parameters for
each semantic class and using better non-linear classifiers
like SVM. Another possible area of improvement is to
consider more semantic classes such as dates, numbers,
etc.
We would also like to build systems for other tasks in
TDT based on semantic language models and test their
performance. We believe that semantic information is
more critical in tasks such as New Event Detection which
involves identifying the first story that discusses a par-
ticular event. New events are typically characterized by
mentions of new persons, locations or actions and our se-
mantic models are capable of capturing exactly such in-
formation.
Additionally, it has been suggested that statistical mod-
els such as the aspect model (Hoffman, 1999) and the
latent Dirichlet allocation (Blei et al., 2001) which gen-
erate words from a mixture of aspect-models can be ex-
ploited by modeling semantic classes as the aspects. We
will be studying the applicability of these ideas to the cur-
rent task as part of our future work.
We believe the main contribution of our work lies in
our attempt at incorporating semantic information in the
language modeling framework and combining scores in
a principled way. We believe we have only taken a first
step in this direction and much remains to be done as part
of future work.
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999974727272727">
I would like to thank Prof. James Allan for motivating the
idea of exploiting semantic information for TDT and Vic-
tor Lavrenko for his valuable comments. I am also grate-
ful to the anonymous reviewers for their very insightful
comments and suggestions. This work was supported
in part by the Center for Intelligent Information Re-
trieval and in part by SPAWARSYSCEN-SD grant num-
bers N66001-99-1-8912 and N66001-02-1-8903. Any
opinions, findings and conclusions or recommendations
expressed in this material are the author’s and do not nec-
essarily reflect those of the sponsor.
</bodyText>
<sectionHeader confidence="0.999646" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999791107142857">
Allan, J., Jin, H., Rajman, M., Wayne, C., Gildea, D.,
Lavrenko, V., Hoberman, R., Caputo, D., Topic-Based
Novelty Detection, Summer Workshop Final Report,
Center for Language and Speech Processing, Johns
Hopkins University, 1999.
Allan, J., Introduction to Topic Detection and Tracking,
Topic Detection and Tracking: Event-based Informa-
tion Organization, James Allan, Editor, Kluwer Aca-
demic Publishers, 1-16, 2002a.
Allan, J., Lavrenko, V. and Nallapati, R. UMass at
TDT 2002, Topic Detection and Tracking: Workshop,
2002b.
Bikel, D. M., Schwartz, R. L. and Weischedel, R. M., An
Algorithm that Learns What’s in a Name, Machine
Learning, Vol. 34(1-3), p 211-231, 1999.
Blei, D. M., and Ng, A. Y., and Jordan, M. I., Latent
Dirichlet Allocation, Neural Information Processing
Systems 14, 2001.
Burges, C. J. C., A Tutorial on Support Vector Machines
for Pattern Recognition, Data Mining and Knowledge
Discovery, vol. 2(2), p 121-167, 1998.
Charniak, E., Statistical Parsing with a Context-Free
Grammar and Word Statistics, AAAI, p 598-603, 1997.
Duda, R. O., Hart, P. E. and Stork, D. G., Pattern Classi-
fication, Wiley-Interscience, 2nd edition, 2000.
Hoffman, T., Probabilistic Latent Semantic Analysis,
Proc. of Uncertainty in Artificial Intelligence, 1999.
Martin, A., Doddington, G., Kamm, T. and Ordowski,
M., The DET curve in assessment of detection task
performance, EuroSpeech, 1895–1898, 1997.
Mihalcea R. and Mihalcea, S., Word Semantics for In-
formation Retrieval: Moving One Step Closer to the
Semantic Web, International Conference on Tools
with Artificial Intelligence, 280-287, 2001.
Ponte, J. M. and Croft, W. B., A Language Modeling
Approach to Information Retrieval, ACM SIGIR, 275-
281, 1998.
Porter, M. F., An algorithm for suffix stripping, Pro-
gram, 14(3):130-137, 1980.
Ratnaparkhi, A., A maximum entropy part-of-speech
tagger, In Proceedings of the Empirical Methods in
Natural Language Processing Conference, University
of Pennsylvania, May 1996.
Salton, G., Yang, C., and Wong, A., A vector-space
model for information retrieval, Comm. of the ACM,
18, 1975.
Xu, J., Broglio, J. and Croft, W. B., The design and im-
plementation of a part of speech tagger for English,
Technical Report IR-52, Center for Intelligent Informa-
tion Retrieval, University of Massachusetts, Amherst,
1994.
Yang, Y., Carbonell, J., Brown, R., Pierce, T., Archibald,
B.T. and Liu, X., Learning Approaches for Detecting
and Tracking News Events. IEEE Intelligent Systems:
Special Issue on Applications of Intelligent Informa-
tion Retrieval, Vol 14(4), 32-43, 1999.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.792370">
<note confidence="0.950901333333333">Proceedings of HLT-NAACL 2003 Student Research Workshop , pp. 1-6 Edmonton, May-June 2003</note>
<title confidence="0.9967585">Semantic Language Models for Topic Detection and Tracking</title>
<author confidence="0.981298">Ramesh</author>
<affiliation confidence="0.999521333333333">Center for Intelligent Information Department of Computer University of</affiliation>
<address confidence="0.975437">Amherst, MA</address>
<email confidence="0.999058">nmramesh@cs.umass.edu</email>
<abstract confidence="0.9984546875">In this work, we present a new semantic language modeling approach to model news stories in the Topic Detection and Tracking (TDT) task. In the new approach, we build a unigram language model for each semantic class in a story. We also cast the detection subtask of TDT as a two-class classification problem in which the features of each sample consist of the generative log-likelihood ratios from each semantic class. We then compute a linear discriminant classifier using the perceptron learning algorithm on the training set. Results on the test set show a marginal improvement over the unigram performance, but are not very encouraging on the whole.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>H Jin</author>
<author>M Rajman</author>
<author>C Wayne</author>
<author>D Gildea</author>
<author>V Lavrenko</author>
<author>R Hoberman</author>
<author>D Caputo</author>
</authors>
<date>1999</date>
<institution>Topic-Based Novelty Detection, Summer Workshop Final Report, Center for Language and Speech Processing, Johns Hopkins University,</institution>
<contexts>
<context position="2033" citStr="Allan et al., 1999" startWordPosition="321" endWordPosition="324"> stories in TDT. The traditional vector space approach (Yang et al., 1999) using cosine similarity has by far been the most consistently successful approach across different tasks and several data sets. In the recent past, a new probabilistic approach called Language Modeling (Ponte and Croft, 1998) has proven to be very effective in several information retrieval tasks. One of the attractive features of language models is that they are firmly rooted in the theory of probability thereby allowing a researcher to explore more sophisticated models guided by the theoretical framework. Allan et al (Allan et al., 1999) applied language models to the first story detection task of TDT and found that its performance is on par with the traditional vector space models, if not better. In the language modeling approach to TDT, we measure the similarity of a news story D to a topic by the probability of its generation from the topic model M. Using the unigram assumption of independence of terms, one can compute the probability of generation of a news story as the product of probabilities of generation of the terms in the story, as shown in the following equation: P(DIM) = � IDI P(wi|M) (1) i=1 where wi is the i-th </context>
<context position="5021" citStr="Allan et al., 1999" startWordPosition="831" endWordPosition="834">tities (Bikel et al., 1999), sentence-parsing (Charniak, 1997), etc., there is increasing hope that one could leverage this information into IR techniques. Traditional vector space models (Salton et al., 1975) and the more recent language models (Ponte and Croft, 1998) tend to ignore any semantic information and consider only word-tokens or word-stems as basic features. We know of no prior work in the language modeling framework that tries to incorporate semantic information into IR models. However, in vector space modeling framework, there have been a few attempts. For example, Allan, et al (Allan et al., 1999) use an ad-hoc weighting scheme to weight named-entities higher than other tokens in their vector space models for the new event detection task of TDT. They do not report any significant improvements in their results. Additionally, the weighting scheme is empirical and they present no principled approach to compute the weights. In the field of ad-hoc retrieval, emerging research on integrating NLP tools into retrieval models seems encouraging. Mihalcea and Mihalcea (Mihalcea and Mihalcea, 2001) show that retrieval effectiveness can be improved by indexing words with their semantic classes such</context>
</contexts>
<marker>Allan, Jin, Rajman, Wayne, Gildea, Lavrenko, Hoberman, Caputo, 1999</marker>
<rawString>Allan, J., Jin, H., Rajman, M., Wayne, C., Gildea, D., Lavrenko, V., Hoberman, R., Caputo, D., Topic-Based Novelty Detection, Summer Workshop Final Report, Center for Language and Speech Processing, Johns Hopkins University, 1999.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Allan</author>
</authors>
<title>Introduction to Topic Detection and Tracking, Topic Detection and Tracking: Event-based Information Organization, James Allan, Editor,</title>
<pages>1--16</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<marker>Allan, </marker>
<rawString>Allan, J., Introduction to Topic Detection and Tracking, Topic Detection and Tracking: Event-based Information Organization, James Allan, Editor, Kluwer Academic Publishers, 1-16, 2002a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>V Lavrenko</author>
<author>R Nallapati</author>
</authors>
<title>UMass at TDT 2002, Topic Detection and Tracking: Workshop,</title>
<date>2002</date>
<contexts>
<context position="6571" citStr="Allan et al., 2002" startWordPosition="1080" endWordPosition="1083">t our task involves analyzing and comparing the content of news stories by the topics that they discuss. The topic of a news story is typically characterized by an event, one or more key players which may include persons or organizations (the who? of the event), a location to which the event is associated (the where? of the event), a time of occurrence of the event (the when? of the event) and a description of the event (the what? of the event). Hence, when comparing news stories, it makes sense to compare those features between the stories that answer the above mentioned four ‘wh’ questions (Allan et al., 2002b). However, extracting these features may not be a trivial task. It may need a deep understanding of the semantics of the story. As a first step towards that end, we can leverage the ability of statistical taggers that can recognize automatically all instances of named-entities such as persons, locations, organizations, and parts-of-speech such as nouns, verbs, adjectives, etc., in a news story. As an approximation to our exact answers to the four ‘wh’ questions, we will assume that the set of tokens labeled as persons and organizations by the taggers correspond to an answer to the who? quest</context>
</contexts>
<marker>Allan, Lavrenko, Nallapati, 2002</marker>
<rawString>Allan, J., Lavrenko, V. and Nallapati, R. UMass at TDT 2002, Topic Detection and Tracking: Workshop, 2002b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>R L Schwartz</author>
<author>R M Weischedel</author>
</authors>
<title>An Algorithm that Learns What’s in a Name,</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>34</volume>
<issue>1</issue>
<pages>211--231</pages>
<contexts>
<context position="4429" citStr="Bikel et al., 1999" startWordPosition="737" endWordPosition="740">age modeling approach in section 3. In section 4, we present details of the link detection task and its evaluation. Section 5 describes the experiments performed and presents the results obtained. In section 6 we analyze the performance of the new model. Section 7 ends the discussion with a few observations and lays down the path to future work. 2 Past work Traditionally NLP techniques have not met with much success in the IR domain. However, after several advances in tasks such as automatic tagging of text with high level semantics such as parts-of-speech (Ratnaparkhi, 1996), named-entities (Bikel et al., 1999), sentence-parsing (Charniak, 1997), etc., there is increasing hope that one could leverage this information into IR techniques. Traditional vector space models (Salton et al., 1975) and the more recent language models (Ponte and Croft, 1998) tend to ignore any semantic information and consider only word-tokens or word-stems as basic features. We know of no prior work in the language modeling framework that tries to incorporate semantic information into IR models. However, in vector space modeling framework, there have been a few attempts. For example, Allan, et al (Allan et al., 1999) use an </context>
<context position="13626" citStr="Bikel et al., 1999" startWordPosition="2303" endWordPosition="2306">|D 1 ) E : : w n (D 2 |D Ln Fn (D 2 ) ) 1 D 1 ) M (D 1 1 : : : ) (D 1 Mn ) (D Mi 1 bilities at each value of threshold into a single normalized evaluation score (Allan, 2002a). We use the minimum value of Clink as the primary measure of effectiveness and show DET curves to illustrate the error trade-offs. It may be useful for the reader to remember that, since the DET curve is an error-tradeoff plot, the closer the curve is to the origin, the better is the performance, unlike the standard precision-recall curve familiar to the IR community. 5 Experiments and results We have used Identifinder (Bikel et al., 1999) and Jtag (Xu et al., 1994) respectively, to tag each term by its named-entity type and its part of speech category. Additionally, we have used a list of 423 most frequent words to remove stop words from stories. Stemming is done using the Porter stemmer (Porter, 1980) while the model is implemented using Java. As a training set, we have used a subset of TDT3 corpus that consists of news stories from eight English sources collected roughly from October through December 1998. We have used manual transcriptions of stories when the source is audio/video. The training set consists of 7200 story pa</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Bikel, D. M., Schwartz, R. L. and Weischedel, R. M., An Algorithm that Learns What’s in a Name, Machine Learning, Vol. 34(1-3), p 211-231, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<date>2001</date>
<booktitle>Latent Dirichlet Allocation, Neural Information Processing Systems 14,</booktitle>
<contexts>
<context position="23401" citStr="Blei et al., 2001" startWordPosition="3962" endWordPosition="3965">rs, etc. We would also like to build systems for other tasks in TDT based on semantic language models and test their performance. We believe that semantic information is more critical in tasks such as New Event Detection which involves identifying the first story that discusses a particular event. New events are typically characterized by mentions of new persons, locations or actions and our semantic models are capable of capturing exactly such information. Additionally, it has been suggested that statistical models such as the aspect model (Hoffman, 1999) and the latent Dirichlet allocation (Blei et al., 2001) which generate words from a mixture of aspect-models can be exploited by modeling semantic classes as the aspects. We will be studying the applicability of these ideas to the current task as part of our future work. We believe the main contribution of our work lies in our attempt at incorporating semantic information in the language modeling framework and combining scores in a principled way. We believe we have only taken a first step in this direction and much remains to be done as part of future work. Acknowledgments I would like to thank Prof. James Allan for motivating the idea of exploit</context>
</contexts>
<marker>Blei, Ng, Jordan, 2001</marker>
<rawString>Blei, D. M., and Ng, A. Y., and Jordan, M. I., Latent Dirichlet Allocation, Neural Information Processing Systems 14, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J C Burges</author>
</authors>
<title>A Tutorial on Support Vector Machines for Pattern Recognition,</title>
<date>1998</date>
<journal>Data Mining and Knowledge Discovery,</journal>
<volume>2</volume>
<issue>2</issue>
<pages>121--167</pages>
<contexts>
<context position="20233" citStr="Burges, 1998" startWordPosition="3419" endWordPosition="3420">e learnt for the unigram model in all the classes of the semantic language model. It is possible that different classes may require different levels of smoothing for optimum performance. We believe one could use a gradient descent algorithm on TDT’s cost function to learn from the training set the optimum values of the smoothing parameters for different classes. Secondly, a linear discriminant function that a perceptron implements is an overly simplistic classifier and may not be doing a good job on separating the on-topic pairs from the off-topic ones. A non-linear classifier such as an SVM (Burges, 1998) could help improve our accuracy. Lastly, it is possible that the unigram model is already capturing the relative importance of terms that we are trying to model using our semantic language models. The likelihood ratio score we use in the unigram approach behaves similar to the tf-idf weights, which we know are a powerful statistic to capture the relative importance of terms. If this were true, then the semantic language model may be rendered redundant. The real reasons will only be revealed by an analysis of the data and we hope to do this as part of our future work. 7 Conclusions and Future </context>
</contexts>
<marker>Burges, 1998</marker>
<rawString>Burges, C. J. C., A Tutorial on Support Vector Machines for Pattern Recognition, Data Mining and Knowledge Discovery, vol. 2(2), p 121-167, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical Parsing with a Context-Free Grammar and Word Statistics, AAAI,</title>
<date>1997</date>
<pages>598--603</pages>
<contexts>
<context position="4464" citStr="Charniak, 1997" startWordPosition="742" endWordPosition="743">section 4, we present details of the link detection task and its evaluation. Section 5 describes the experiments performed and presents the results obtained. In section 6 we analyze the performance of the new model. Section 7 ends the discussion with a few observations and lays down the path to future work. 2 Past work Traditionally NLP techniques have not met with much success in the IR domain. However, after several advances in tasks such as automatic tagging of text with high level semantics such as parts-of-speech (Ratnaparkhi, 1996), named-entities (Bikel et al., 1999), sentence-parsing (Charniak, 1997), etc., there is increasing hope that one could leverage this information into IR techniques. Traditional vector space models (Salton et al., 1975) and the more recent language models (Ponte and Croft, 1998) tend to ignore any semantic information and consider only word-tokens or word-stems as basic features. We know of no prior work in the language modeling framework that tries to incorporate semantic information into IR models. However, in vector space modeling framework, there have been a few attempts. For example, Allan, et al (Allan et al., 1999) use an ad-hoc weighting scheme to weight n</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Charniak, E., Statistical Parsing with a Context-Free Grammar and Word Statistics, AAAI, p 598-603, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R O Duda</author>
<author>P E Hart</author>
<author>D G Stork</author>
</authors>
<title>Pattern Classification, Wiley-Interscience, 2nd edition,</title>
<date>2000</date>
<contexts>
<context position="11651" citStr="Duda et al., 2000" startWordPosition="1942" endWordPosition="1945">.,M|C|(D1)] from story D1 as given by equation 4. We also construct the semantic class-specific feature lists {F1(D2),..,F|C|(D2)} from story D2 as defined in equation 2 and then compute the feature vector x = [L1(D2|D1), .., L|C|(D2|D1)]T where each component likelihood ratio is computed as given in equation 5. We then perform an inner product of the augmented feature vector y and the weight vector w of the perceptron and the resulting score is output as shown in the figure. The standard perceptron learns the optimal weight vector w by minimizing its misclassification rate on a training set (Duda et al., 2000). However, in TDT, misses (classification of an on-topic story pair as off-topic) are penalized 10 times more strongly than false alarms (classification of an off-topic story pair as on-topic) (Allan, 2002a). We have therefore incorporated these penalties into the criterion function to force the perceptron learn the optimal classification based on TDT’s cost function. 4 Link Detection task and Evaluation In this section, we describe one of the tasks called link detection, on which we performed the experiments reported in this work. Link detection requires determining whether or not two randoml</context>
</contexts>
<marker>Duda, Hart, Stork, 2000</marker>
<rawString>Duda, R. O., Hart, P. E. and Stork, D. G., Pattern Classification, Wiley-Interscience, 2nd edition, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hoffman</author>
</authors>
<title>Probabilistic Latent Semantic Analysis,</title>
<date>1999</date>
<booktitle>Proc. of Uncertainty in Artificial Intelligence,</booktitle>
<contexts>
<context position="23345" citStr="Hoffman, 1999" startWordPosition="3955" endWordPosition="3956"> consider more semantic classes such as dates, numbers, etc. We would also like to build systems for other tasks in TDT based on semantic language models and test their performance. We believe that semantic information is more critical in tasks such as New Event Detection which involves identifying the first story that discusses a particular event. New events are typically characterized by mentions of new persons, locations or actions and our semantic models are capable of capturing exactly such information. Additionally, it has been suggested that statistical models such as the aspect model (Hoffman, 1999) and the latent Dirichlet allocation (Blei et al., 2001) which generate words from a mixture of aspect-models can be exploited by modeling semantic classes as the aspects. We will be studying the applicability of these ideas to the current task as part of our future work. We believe the main contribution of our work lies in our attempt at incorporating semantic information in the language modeling framework and combining scores in a principled way. We believe we have only taken a first step in this direction and much remains to be done as part of future work. Acknowledgments I would like to th</context>
</contexts>
<marker>Hoffman, 1999</marker>
<rawString>Hoffman, T., Probabilistic Latent Semantic Analysis, Proc. of Uncertainty in Artificial Intelligence, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Martin</author>
<author>G Doddington</author>
<author>T Kamm</author>
<author>M Ordowski</author>
</authors>
<title>The DET curve in assessment of detection task performance,</title>
<date>1997</date>
<location>EuroSpeech, 1895–1898,</location>
<contexts>
<context position="12633" citStr="Martin et al., 1997" startWordPosition="2096" endWordPosition="2100">unction. 4 Link Detection task and Evaluation In this section, we describe one of the tasks called link detection, on which we performed the experiments reported in this work. Link detection requires determining whether or not two randomly selected stories (D1, D2) discuss the same topic. The evaluation methodology of a link detection system requires the system to output a score for each story pair that represents the system’s confidence that both stories in the pair discuss the same topic. The system’s performance is then evaluated using a topicweighted Detection Error Trade-off (DET) curve (Martin et al., 1997) that plots miss rate against false alarm over a large number of story pairs, at different values of decision-threshold. A Link Detection cost function Clink is then used to combine the miss and false alarm probaLi(D2|D1) = ln( P(Fi(D2))|Mi(D1) P (Fi (D2)) |Mi (GE) ) = ln( �n |f j=1 (P(wj |Mi(GE))) (wj ,Fi(D2))) D 2 0 F 1 (D2 ) w F i (D 2 ) : wiLi w1 (D 2 L 1 (D 2 |D 1 ) |D 1 ) E : : w n (D 2 |D Ln Fn (D 2 ) ) 1 D 1 ) M (D 1 1 : : : ) (D 1 Mn ) (D Mi 1 bilities at each value of threshold into a single normalized evaluation score (Allan, 2002a). We use the minimum value of Clink as the primary </context>
</contexts>
<marker>Martin, Doddington, Kamm, Ordowski, 1997</marker>
<rawString>Martin, A., Doddington, G., Kamm, T. and Ordowski, M., The DET curve in assessment of detection task performance, EuroSpeech, 1895–1898, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>S Mihalcea</author>
</authors>
<title>Word Semantics for Information Retrieval: Moving One Step Closer to the Semantic Web,</title>
<date>2001</date>
<booktitle>International Conference on Tools with Artificial Intelligence,</booktitle>
<pages>280--287</pages>
<contexts>
<context position="5520" citStr="Mihalcea and Mihalcea, 2001" startWordPosition="910" endWordPosition="913">dels. However, in vector space modeling framework, there have been a few attempts. For example, Allan, et al (Allan et al., 1999) use an ad-hoc weighting scheme to weight named-entities higher than other tokens in their vector space models for the new event detection task of TDT. They do not report any significant improvements in their results. Additionally, the weighting scheme is empirical and they present no principled approach to compute the weights. In the field of ad-hoc retrieval, emerging research on integrating NLP tools into retrieval models seems encouraging. Mihalcea and Mihalcea (Mihalcea and Mihalcea, 2001) show that retrieval effectiveness can be improved by indexing words with their semantic classes such as parts-of-speech, named-entity-type, WordNet synonyms, hypernyms, hyponyms, etc. In this work, we present a principled approach to integrating semantic information into the language modeling framework and show how to compute the relative importance of various semantic classes automatically. 3 Semantic language models Recall that our task involves analyzing and comparing the content of news stories by the topics that they discuss. The topic of a news story is typically characterized by an eve</context>
</contexts>
<marker>Mihalcea, Mihalcea, 2001</marker>
<rawString>Mihalcea R. and Mihalcea, S., Word Semantics for Information Retrieval: Moving One Step Closer to the Semantic Web, International Conference on Tools with Artificial Intelligence, 280-287, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ponte</author>
<author>W B Croft</author>
</authors>
<title>A Language Modeling Approach to Information Retrieval,</title>
<date>1998</date>
<journal>ACM SIGIR,</journal>
<pages>275--281</pages>
<contexts>
<context position="1714" citStr="Ponte and Croft, 1998" startWordPosition="269" endWordPosition="272">y the events that they discuss (Allan, 2002a). The goal of TDT consists of breaking the stream of news into individual news stories, to monitor the stories for events that have not been seen before and to gather stories into groups that each discuss a single topic. Several approaches have been explored for comparing news stories in TDT. The traditional vector space approach (Yang et al., 1999) using cosine similarity has by far been the most consistently successful approach across different tasks and several data sets. In the recent past, a new probabilistic approach called Language Modeling (Ponte and Croft, 1998) has proven to be very effective in several information retrieval tasks. One of the attractive features of language models is that they are firmly rooted in the theory of probability thereby allowing a researcher to explore more sophisticated models guided by the theoretical framework. Allan et al (Allan et al., 1999) applied language models to the first story detection task of TDT and found that its performance is on par with the traditional vector space models, if not better. In the language modeling approach to TDT, we measure the similarity of a news story D to a topic by the probability o</context>
<context position="4671" citStr="Ponte and Croft, 1998" startWordPosition="773" endWordPosition="776"> the new model. Section 7 ends the discussion with a few observations and lays down the path to future work. 2 Past work Traditionally NLP techniques have not met with much success in the IR domain. However, after several advances in tasks such as automatic tagging of text with high level semantics such as parts-of-speech (Ratnaparkhi, 1996), named-entities (Bikel et al., 1999), sentence-parsing (Charniak, 1997), etc., there is increasing hope that one could leverage this information into IR techniques. Traditional vector space models (Salton et al., 1975) and the more recent language models (Ponte and Croft, 1998) tend to ignore any semantic information and consider only word-tokens or word-stems as basic features. We know of no prior work in the language modeling framework that tries to incorporate semantic information into IR models. However, in vector space modeling framework, there have been a few attempts. For example, Allan, et al (Allan et al., 1999) use an ad-hoc weighting scheme to weight named-entities higher than other tokens in their vector space models for the new event detection task of TDT. They do not report any significant improvements in their results. Additionally, the weighting sche</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>Ponte, J. M. and Croft, W. B., A Language Modeling Approach to Information Retrieval, ACM SIGIR, 275-281, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping, Program,</title>
<date>1980</date>
<pages>14--3</pages>
<contexts>
<context position="13895" citStr="Porter, 1980" startWordPosition="2354" endWordPosition="2355">llustrate the error trade-offs. It may be useful for the reader to remember that, since the DET curve is an error-tradeoff plot, the closer the curve is to the origin, the better is the performance, unlike the standard precision-recall curve familiar to the IR community. 5 Experiments and results We have used Identifinder (Bikel et al., 1999) and Jtag (Xu et al., 1994) respectively, to tag each term by its named-entity type and its part of speech category. Additionally, we have used a list of 423 most frequent words to remove stop words from stories. Stemming is done using the Porter stemmer (Porter, 1980) while the model is implemented using Java. As a training set, we have used a subset of TDT3 corpus that consists of news stories from eight English sources collected roughly from October through December 1998. We have used manual transcriptions of stories when the source is audio/video. The training set consists of 7200 story pairs. For the general English model for this set, we have used the same TDT3 natural English manually transcribed set consisting of 37,526 news stories. For the test set, we have used a randomly chosen subset of natural English, manually transcribed stories from TDT2 co</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Porter, M. F., An algorithm for suffix stripping, Program, 14(3):130-137, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-of-speech tagger,</title>
<date>1996</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference,</booktitle>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="4392" citStr="Ratnaparkhi, 1996" startWordPosition="733" endWordPosition="735">ethodology of the new semantic language modeling approach in section 3. In section 4, we present details of the link detection task and its evaluation. Section 5 describes the experiments performed and presents the results obtained. In section 6 we analyze the performance of the new model. Section 7 ends the discussion with a few observations and lays down the path to future work. 2 Past work Traditionally NLP techniques have not met with much success in the IR domain. However, after several advances in tasks such as automatic tagging of text with high level semantics such as parts-of-speech (Ratnaparkhi, 1996), named-entities (Bikel et al., 1999), sentence-parsing (Charniak, 1997), etc., there is increasing hope that one could leverage this information into IR techniques. Traditional vector space models (Salton et al., 1975) and the more recent language models (Ponte and Croft, 1998) tend to ignore any semantic information and consider only word-tokens or word-stems as basic features. We know of no prior work in the language modeling framework that tries to incorporate semantic information into IR models. However, in vector space modeling framework, there have been a few attempts. For example, Alla</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Ratnaparkhi, A., A maximum entropy part-of-speech tagger, In Proceedings of the Empirical Methods in Natural Language Processing Conference, University of Pennsylvania, May 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Yang</author>
<author>A Wong</author>
</authors>
<title>A vector-space model for information retrieval,</title>
<date>1975</date>
<journal>Comm. of the ACM,</journal>
<volume>18</volume>
<contexts>
<context position="4611" citStr="Salton et al., 1975" startWordPosition="763" endWordPosition="766">sults obtained. In section 6 we analyze the performance of the new model. Section 7 ends the discussion with a few observations and lays down the path to future work. 2 Past work Traditionally NLP techniques have not met with much success in the IR domain. However, after several advances in tasks such as automatic tagging of text with high level semantics such as parts-of-speech (Ratnaparkhi, 1996), named-entities (Bikel et al., 1999), sentence-parsing (Charniak, 1997), etc., there is increasing hope that one could leverage this information into IR techniques. Traditional vector space models (Salton et al., 1975) and the more recent language models (Ponte and Croft, 1998) tend to ignore any semantic information and consider only word-tokens or word-stems as basic features. We know of no prior work in the language modeling framework that tries to incorporate semantic information into IR models. However, in vector space modeling framework, there have been a few attempts. For example, Allan, et al (Allan et al., 1999) use an ad-hoc weighting scheme to weight named-entities higher than other tokens in their vector space models for the new event detection task of TDT. They do not report any significant imp</context>
</contexts>
<marker>Salton, Yang, Wong, 1975</marker>
<rawString>Salton, G., Yang, C., and Wong, A., A vector-space model for information retrieval, Comm. of the ACM, 18, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>J Broglio</author>
<author>W B Croft</author>
</authors>
<title>The design and implementation of a part of speech tagger for English,</title>
<date>1994</date>
<tech>Technical Report IR-52,</tech>
<institution>Center for Intelligent Information Retrieval, University of Massachusetts,</institution>
<location>Amherst,</location>
<contexts>
<context position="13653" citStr="Xu et al., 1994" startWordPosition="2309" endWordPosition="2312"> (D 2 ) ) 1 D 1 ) M (D 1 1 : : : ) (D 1 Mn ) (D Mi 1 bilities at each value of threshold into a single normalized evaluation score (Allan, 2002a). We use the minimum value of Clink as the primary measure of effectiveness and show DET curves to illustrate the error trade-offs. It may be useful for the reader to remember that, since the DET curve is an error-tradeoff plot, the closer the curve is to the origin, the better is the performance, unlike the standard precision-recall curve familiar to the IR community. 5 Experiments and results We have used Identifinder (Bikel et al., 1999) and Jtag (Xu et al., 1994) respectively, to tag each term by its named-entity type and its part of speech category. Additionally, we have used a list of 423 most frequent words to remove stop words from stories. Stemming is done using the Porter stemmer (Porter, 1980) while the model is implemented using Java. As a training set, we have used a subset of TDT3 corpus that consists of news stories from eight English sources collected roughly from October through December 1998. We have used manual transcriptions of stories when the source is audio/video. The training set consists of 7200 story pairs. For the general Englis</context>
</contexts>
<marker>Xu, Broglio, Croft, 1994</marker>
<rawString>Xu, J., Broglio, J. and Croft, W. B., The design and implementation of a part of speech tagger for English, Technical Report IR-52, Center for Intelligent Information Retrieval, University of Massachusetts, Amherst, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>J Carbonell</author>
<author>R Brown</author>
<author>T Pierce</author>
<author>B T Archibald</author>
<author>X Liu</author>
</authors>
<title>Learning Approaches for Detecting and Tracking News Events.</title>
<date>1999</date>
<journal>IEEE Intelligent Systems: Special Issue on Applications of Intelligent Information Retrieval, Vol</journal>
<volume>14</volume>
<issue>4</issue>
<pages>32--43</pages>
<contexts>
<context position="1488" citStr="Yang et al., 1999" startWordPosition="235" endWordPosition="238">n the test set show a marginal improvement over the unigram performance, but are not very encouraging on the whole. 1 Introduction TDT is a research program investigating methods for automatically organizing news stories by the events that they discuss (Allan, 2002a). The goal of TDT consists of breaking the stream of news into individual news stories, to monitor the stories for events that have not been seen before and to gather stories into groups that each discuss a single topic. Several approaches have been explored for comparing news stories in TDT. The traditional vector space approach (Yang et al., 1999) using cosine similarity has by far been the most consistently successful approach across different tasks and several data sets. In the recent past, a new probabilistic approach called Language Modeling (Ponte and Croft, 1998) has proven to be very effective in several information retrieval tasks. One of the attractive features of language models is that they are firmly rooted in the theory of probability thereby allowing a researcher to explore more sophisticated models guided by the theoretical framework. Allan et al (Allan et al., 1999) applied language models to the first story detection t</context>
</contexts>
<marker>Yang, Carbonell, Brown, Pierce, Archibald, Liu, 1999</marker>
<rawString>Yang, Y., Carbonell, J., Brown, R., Pierce, T., Archibald, B.T. and Liu, X., Learning Approaches for Detecting and Tracking News Events. IEEE Intelligent Systems: Special Issue on Applications of Intelligent Information Retrieval, Vol 14(4), 32-43, 1999.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>