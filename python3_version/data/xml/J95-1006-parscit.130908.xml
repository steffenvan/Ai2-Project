<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.947769">
Book Reviews
Statistical Language Learning
</title>
<author confidence="0.941878">
Eugene Charniak
</author>
<bodyText confidence="0.395834">
(Brown University)
Cambridge, MA: The MIT Press (A
Bradford book), 1993, xx + 170 pp.
Hardbound, ISBN 0-262-03216-3, $25.00
</bodyText>
<note confidence="0.175716333333333">
Reviewed by
David M. Magerman
Bolt Beranek and Newman Inc.
</note>
<sectionHeader confidence="0.994921" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999796266666667">
The $64,000 question in computational linguistics these days is: &amp;quot;What should I read
to learn about statistical natural language processing?&amp;quot; I have been asked this question
over and over, and each time I have given basically the same reply: there is no text
that addresses this topic directly; the best one can do is find a good probability-theory
textbook and a good information-theory textbook and supplement those texts with an
assortment of conference papers and journal articles. Understanding the disappoint-
ment this answer provoked, I was delighted to hear that someone had finally written
a book directly addressing this topic. However, after reading Eugene Charniak&apos;s Sta-
tistical Language Learning, I have very mixed feelings about the impact this book might
have on the ever-growing field of statistical NLP.
The book begins with a very brief description of the classic artificial intelligence
approach to NLP (Chapter 1), including morphology, syntax, semantics, and prag-
matics. It presents a few definitions from probability theory and information theory
(Chapter 2), then proceeds to introduce hidden Markov models (Chapters 3 and 4)
and probabilistic context-free grammars (Chapters 5 and 6). The book concludes with
a few chapters discussing advanced topics in statistical language learning, such as
grammar induction (Chapter 7), syntactic disambiguation (Chapter 8), word cluster-
ing (Chapter 9), and word sense disambiguation (Chapter 10).
To its credit, the book serves as an interesting popular discussion of statistical
modeling in NLP. It is well written and entertaining and very accessible to the reader
with a limited mathematical background. It presents a good selection of statistical
NLP topics to introduce the reader to the field. And the descriptions of the forward—
backward algorithm for hidden Markov models and the inside—outside algorithm for
probabilistic context-free grammars are intuitive and easy to follow.
However, as a resource for someone interested in entering this area of research,
this book falls far short of its author&apos;s goals. These goals are clearly stated in the
preface:
This book aims to acquaint the reader with both the current state of
the art in statistical language processing ... and the mathematical
background necessary to understand the field.... Because the book
</bodyText>
<note confidence="0.545097">
Computational Linguistics Volume 21, Number 1
</note>
<bodyText confidence="0.999742464285714">
is relatively self-contained, it could be used at the intermediate or
advanced undergraduate level. However, because of its narrow focus,
and because it skirts so close to the research edge in this area, it would
probably be better as a beginning graduate-level course. (p. xvii)
The book is written at a level that is perhaps appropriate for an undergraduate student.
But it is too brief (170 pages) for a book that claims to be a self-contained summary of
the current state of the art in this field. In particular, far too little attention is paid to the
mathematical fields of study upon which the statistical methods are based: probability
theory and information theory. The book contains too few references to the literature
(44 bibliography entries) to be useful as a springboard to other sources for a researcher.
And it is neither self-contained nor an adequate presentation of the state of the art,
even as it existed in 1993, when the book was written.
One of the mixed blessings of this book is the tone of the technical discussions.
Charniak is trying to appeal to the widest possible audience in his book, and he is to be
applauded for his effort. However, in avoiding mathematical formalism and concrete
proofs in favor of intuitive explanations and simple examples, he withholds from
the reader the critical building blocks of this field. Although the technical material is
generally accurate, the intuitive discussions are frequently off the mark. If the reader
were offered the mathematical proofs, he or she could evaluate the discussions on
their merits. However, without the technical details, the reader must place faith in the
author&apos;s intuition.
My main concern about this book is that it will be taken as the definitive presen-
tation of statistical NLP. It is the first book on the topic and the only one currently
available. Given the exposure it is likely to receive as a result, it might be worthwhile
to explore some of the shortcomings of the book in more detail. In particular, I will
address three main points: (1) the limited mathematical background provided in the
book, (2) the gaps in the book&apos;s coverage of the literature, and (3) some significant
oversimplifications made in the book.
</bodyText>
<sectionHeader confidence="0.879596" genericHeader="method">
2. &amp;quot;A Fragment of Probability Theory&amp;quot;
</sectionHeader>
<bodyText confidence="0.999982">
One of the book&apos;s aims is to acquaint the reader with &amp;quot;the mathematical background
necessary to understand the field.&amp;quot; This mathematical background is no less than
probability theory and information theory, and each of these topics demands at least a
chapter, if not an entire textbook. In a book about statistical modeling, more discussion
of probability theory is warranted than the three-page section entitled &amp;quot;A fragment
of probability theory,&amp;quot; which is all this book devotes to the topic. Also, a formal pre-
sentation of the concepts of information theory and their relationships to one another
would be much more informative than the casual treatment they are given here. Read-
ers may appreciate and enjoy the chatty tone of the examples given in the chapter, but
it is necessary to understand these concepts at a formal mathematical level in order
to comprehend the topics discussed later in the book, as well as other topics that are
not mentioned but are relevant to the field.
In particular, the book omits a number of important definitions. Significantly, there
is no formal definition of what constitutes a probability model! Nowhere in the text
does it say that a probability function is a mapping onto the closed set of real numbers
between 0 and 1. Nor does it state that, in the discrete case, the probabilities of all
possibilities must sum to 1. This latter detail, however, is necessary to solve at least
one of the exercises. The book also uses notation like P(X -=- x,Y =- y) without men-
tioning the term joint probability or defining the concept it represents. And, whereas
</bodyText>
<page confidence="0.997904">
104
</page>
<subsectionHeader confidence="0.949362">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.9999585">
information-theoretic concepts such as entropy and cross entropy are defined in some
detail early in the book, the definitions of other important concepts, such as conditional
entropy, relative entropy, and mutual information, are scattered throughout the text,
without a clear description of the relationships among them. Many of the measures
in information theory can be derived from combinations of other measures, and it is
very instructive to see these derivations.
One of the consequences of the lack of mathematical background presented in
the book is that it makes it difficult for students to identify careless mistakes in the
literature, such as typographical errors in formulas and mathematical errors in deriva-
tions. For instance, in Chapter 2, the application of the source-channel model to the
speech-recognition problem is introduced, where the probability of the transcription
of an utterance, w, given the speech signal is estimated using Bayes&apos;s rule:
</bodyText>
<subsectionHeader confidence="0.480577666666667">
P(wi&apos;n)P(speech signal I w1,n)
P(wi,n I speech signal) = .
P(speech signal)
</subsectionHeader>
<bodyText confidence="0.950567">
Later, in a discussion of an example of the speech-recognition algorithm, the text states
that recognizers search for the sequence of words that maximizes the quantity:1
</bodyText>
<equation confidence="0.471576666666667">
P (a2, b1, c4)P(a2, b1, c4 I speech signal).
The correct formula is actually
P(a2,b1, c4)P(speech signal I a, b1, c4).
</equation>
<bodyText confidence="0.999891833333333">
This is most certainly a typographical error, although it is repeated twice. But the
reader, without confidence in his or her understanding of probability theory, is not
likely to identify this as an error.
One might suggest that the author is of course assuming that the student will
augment his or her reading with an appropriate probability-theory text and a good
introduction to information theory. However, he fails to recommend a good resource
for these fields of study, of which there are many (e.g., DeGroot 1986, or Bickel and
Doksum 1977, for probability theory, and Cover and Thomas 1991, or Ash 1965, for
information theory), or to include any such texts in his bibliography. It seems as though
he feels his presentation is sufficient to understand the material, or at least that further
discussion of these topics is unnecessary.
This treatment of the mathematical foundations of statistical modeling is common
in the statistical NLP literature, and it is motivated by a misinterpretation of the role
statistical methods play in this field of research. Statistical methods are not simply well-
understood tools that are used to learn about language; they are one of the central
concerns of the research. Improvements in language modeling have not been achieved
by viewing a trigram model as a black box and trying to work around its deficiencies.
Over the last five years, language models have been improved by discovering precisely
why n-gram models are estimating the probabilities of certain events poorly (generally
because of sparse data) and finding alternative techniques that are more accurate.
There are many different approaches to this problem, but all of them require a detailed
understanding of the fundamentals of at least probability theory, if not information
theory as well. A textbook that suggests that this understanding is unnecessary, or
that it can be achieved by reading a few pages, is misleading at best.
</bodyText>
<footnote confidence="0.8358135">
1 The denominator drops out of the equation, since P(speech signal) remains constant when the word
sequence is varied.
</footnote>
<page confidence="0.992165">
105
</page>
<note confidence="0.470613">
Computational Linguistics Volume 21, Number 1
</note>
<subsectionHeader confidence="0.461705">
3. Surveying the State of the Art
</subsectionHeader>
<bodyText confidence="0.999914081081081">
Evaluating and presenting the state of the art in statistical NLP is more difficult than
in more established fields because there are so few concentrated sources of material to
point to and summarize. In the absence of books on the subject, the best places to look
are journals, conference proceedings, and published workshop proceedings. Another
good resource is recent doctoral theses, although these tend to be more verbose and
overly technical, and they are frequently summarized later in journals.
Conveniently, much of the foundational work in statistical NLP has been published
in the proceedings of the Speech and Natural Language Workshop (later called the
Human Language Technologies Workshop), sponsored by DARPA (later called ARPA)
(DARPA 1989a, 1989b, 1990, 1991, 1992; ARPA, 1993, 1994). However, the book&apos;s bib-
liography fails to cite any papers from any of these workshops, many of which were
important in the development of statistical NLP (e.g., Church et al. 1989; Brill et al.
1990; Chitrao and Grishman 1990; Gale and Church 1990; Magerman and Marcus
1991; Black et al. 1992a, 1992b; Brill 1992; Lau, Rosenfeld, and Roukos 1993). Of the 44
bibliography entries, only two papers from Computational Linguistics are mentioned,
omitting papers such as Brown et al. 1990, Seneff 1992, and Hindle and Rooth 1993,2
among others. And from the major computational linguistics conferences, whose par-
ticipants have recently complained about the overwhelming number of papers on
statistical methods, only eight papers are cited.
There are a number of papers cited from the working notes of two AAAI work-
shops: the 1992 AAAI Fall Symposium Series on Probabilistic Approaches to Natural
Language and the 1992 Workshop on Statistically-Based NLP Techniques. These pa-
pers may be interesting and worthy of discussion, but they are of less value to the
reader than papers from published proceedings and journals, since working notes are
much more difficult to access for those who did not attend the workshops.
The omissions of important papers and published resources are a problem in
the book; but they are symptomatic of a general lack of coverage of the mainstream
literature throughout. Some of the important and foundational papers are discussed
in the book, but many others are ignored and replaced by discussions of papers that
are more on the periphery of the field. And many of the omitted papers represent the
state of the art in the fields of language modeling, part-of-speech tagging, parsing,
and general statistical language learning.
Some recently published work relevant to the study of these areas include the
following: Lafferty, Sleator, and Temperly 1992, Lau, Rosenfeld, and Roukos 1993, and
Della Pietra et al. 1994, for language modeling; Brill 1993 and Merialdo 1994 for part-
of-speech tagging; Bod 1993 and Magerman 1994 for parsing; and Resnik 1993, Miller
et al. 1994, and Yarowsky 1994, for various topics in statistical language learning.
</bodyText>
<sectionHeader confidence="0.936942" genericHeader="method">
4. Oversimplifications
</sectionHeader>
<bodyText confidence="0.998638">
In an introductory text, it is advisable to simplify some concepts for the reader to avoid
confusion. Charniak employs this technique to great advantage. Occasionally, however,
he crosses the line from simplification to oversimplification, leading the reader to draw
inappropriate conclusions.
</bodyText>
<footnote confidence="0.691146">
2 Hindle and Rooth 1993 is an expanded, journal-length version of a conference paper the same title,
which is cited in the book.
</footnote>
<page confidence="0.992384">
106
</page>
<subsectionHeader confidence="0.893004">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.995713035714286">
One example is the book&apos;s treatment of smoothing. Smoothing probability models
in the face of sparse data is a hot topic in the field, and it is worthy of far more discus-
sion than it is given in this book. One technique, deleted interpolation, is discussed at
length.&apos; But after this technique is introduced, explained, and discussed in Chapter 3,
it is ignored in future discussions. Consider the following passage:
It is also useful to consider some not-so-good language models to
see why they are less desirable.... [Consider a model in which] the
next part of speech is conditioned on not just the previous part of
speech, but the previous word as well. In the abstract, including this
dependence might or might not help the word model. In actuality, it
is almost certainly a bad idea because of its effect on the sparse-data
problem. (p. 48)
This is an excellent opportunity to suggest that smoothing this distribution with
deleted interpolation would alleviate the sparse data problem, but no mention of this
alternative is made.
Later, in discussing the prospects of estimating a 4-gram model for modeling
prepositional phrase attachment,
P(attachment I verb, noun, preposition, object of preposition),
the text states plainly, &amp;quot;Unfortunately, this is still far from being something for which
we can collect statistics&amp;quot; (p. 120). Again, no mention is made of the potential ap-
plication of deleted interpolation or any other smoothing techniques. The only pro-
posed solutions involve reducing the number of parameters in the model in ways that
make the parameters estimable directly from frequency counts from a corpus. These
reductions yield much weaker models and are unnecessary given current modeling
techniques.
Another example of oversimplification occurs in the text&apos;s presentation of deleted
interpolation. The definition of deleted interpolation applied to smoothing trigram
word models is technically correct:
</bodyText>
<equation confidence="0.984645333333333">
P(Wn I Wn—lW0-2) = AlPe(Wn) +
A2Pe(Wn I wn-1) +
A3Pe (Wn 1 Wn-1Wn —2) -
</equation>
<bodyText confidence="0.993558666666667">
Here, the A parameters are simply scalar values. However, in Brown et al. 1992 (also
in Bahl, Jelinek, and Mercer 1983), where deleted interpolation is defined, these pa-
rameters represent functions of the history, A( ,wn-iwn-2) :
</bodyText>
<equation confidence="0.693961333333333">
P(Wn I Wn—lWn-2) Ai(Wn—lWn-2)Pe(Wn) ±
A2(Wn-1Wn-2)Pe(Wn I wn-1) ±
A3(Wn-1Wn-2)Pe(Wn I Wn—lWn-2)•
</equation>
<bodyText confidence="0.65581">
This may seem like a minor point, but the effectiveness of deleted interpolation de-
pends on this distinction, and the repercussions of this oversimplification are quite
</bodyText>
<footnote confidence="0.9968605">
3 Actually, the book never uses the term deleted interpolation when describing the technique. This
omission makes it difficult for the reader to investigate this technique further in the literature.
</footnote>
<page confidence="0.984221">
107
</page>
<note confidence="0.654175">
Computational Linguistics Volume 21, Number 1
</note>
<bodyText confidence="0.9998764">
apparent when evaluating the performance of language models using each of these
definitions.
To illustrate this, consider the example used in the text, trigram language model-
ing. A trigram language model trained on a large corpus using the scalar parameters
would yield the result that on average the empirical estimates of the trigram model
are more reliable than those of the bigram model and that both are more reliable on
average than the unigram model. But there are many bigram contexts, or histories,
(wn-iwn_2) that are infrequent, and in these cases the trigram model is unreliable.
Since the trigram model cannot be relied upon in all cases, the bigram and unigram
model parameters (A2 and Ai, respectively) will never be close to zero. For the sake
of discussion, let&apos;s use the author&apos;s guesses of these parameters, since they are quite
reasonable: A3 = 0.6, A2 = 0.3, and A1 = 0.1.
For the most frequent bigram histories, the empirical trigram model is the best
predictor of the next word. For instance, the probability of the word that follows the bi-
gram in the can be accurately estimated by an empirical trigram model. In contrast, the
probability of the word following volcanologic astrobiology is better estimated using
a unigram model. The formulation of the deleted interpolation parameters of Brown
et al. (1992) is designed to allow high-frequency histories to depend on the trigram
counts while deferring to the bigram and unigram counts for the low-frequency histo-
ries. So, A3 (in the) will be very close to 1. Using the book&apos;s formulation, P(the 1 in the)
is significantly overestimated because the model derives 10% of its estimate from the
unigram model, even though the direct estimate from corpus frequencies is more ac-
curate. Deleted interpolation as described by Brown et al. (1992) yields a far better
language model than the simpler formulation used in the book, in terms of both en-
tropy and performance.
This oversimplification of deleted interpolation occurs frequently in the literature.
In fact, it probably occurs more frequently than can be determined, since details about
smoothing algorithms are often omitted from conference papers. However, it is es-
pecially important that this concept be spelled out clearly in a textbook. Charniak
does allude to the full definition of deleted interpolation in Exercise 3.2, in which he
suggests the possibility of interpreting the A values of a trigram model as functions
of the word bigram history. However, this exercise is never answered or discussed in
the text, and the topic is never brought up again. This point is far too critical to be
relegated to an unsolved exercise. Glossing over this issue only serves to perpetuate
the misuse of deleted interpolation in published research.
</bodyText>
<sectionHeader confidence="0.99076" genericHeader="method">
5. Summary
</sectionHeader>
<bodyText confidence="0.999465153846154">
I cannot recommend this book without strong qualifications. It is the only book avail-
able that discusses this field at any length, and it is one of the few presentations of this
material that is both substantive and accessible to the non-expert. However, it fails to
accomplish even its own stated goals, much less satisfy the needs of the community
at large.
For the casual reader interested in a snapshot of this field for the sake of personal
knowledge, this book is quite adequate. It is not necessarily the snapshot I would
have taken, but it certainly represents some of the work going on in the field in the
past decade. One might place undergraduates in this category, in which case this book
could be used in an elective NLP course for computer science majors.
But many will attempt to use this book for other purposes. Some will teach
graduate-level courses in statistical NLP using this book as a primary text. Others
will offer this book to their students who are interested in exploring statistical NLP
</bodyText>
<page confidence="0.997586">
108
</page>
<subsectionHeader confidence="0.851047">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.99996125">
for their thesis work. And some researchers from different branches of NLP will read
this book on their own, hoping to learn enough about statistical NLP to begin research
in the field themselves.
To all of these potential readers, I offer the following advice: Start by reading the
first few chapters of a probability-theory textbook and the first few chapters of an
information-theory textbook, in both cases focusing on the discussions of discrete (as
opposed to continuous) models. You might read the middle chapters (3-6) of Statistical
Language Learning for an initial introduction to topics such as hidden Markov models,
the forward—backward algorithm, and the inside—outside algorithm, in order to make
the original sources more accessible. And, finally, you should study some of the articles
and papers cited in this review for a better understanding of the applications of these
techniques.
The overriding concern should be to learn (and teach) the mathematical under-
pinnings of the statistical techniques used in this field. The field of statistical NLP
is very young, and the foundations are still being laid. Deep knowledge of the basic
machinery is far more valuable than the details of the most recent unproven ideas.
</bodyText>
<sectionHeader confidence="0.82562" genericHeader="references">
References
</sectionHeader>
<figureCaption confidence="0.846131863636364">
ARPA (1993). Human Language Technology:
Proceedings of a Workshop Held at Plainsboro,
New Jersey. Sponsored by Advanced
Research Projects Agency, Information
Science and Technology Office. San
Francisco, California: Morgan Kaufmann
Publishers, Inc.
ARPA (1994). Human Language Technology:
Proceedings of a Workshop Held at Plainsboro,
New Jersey. Sponsored by Advanced
Research Projects Agency, Information
Science and Technology Office. San
Francisco, California: Morgan Kaufmann
Publishers, Inc.
Ash, Robert B. (1965). Information Theory.
New York: Dover Publications, Inc.
Bahl, Lalit R.; Jelinek, Fred; and Mercer,
Robert L. (1983). &amp;quot;A maximum likelihood
approach to continuous speech
recognition.&amp;quot; IEEE Transactions on Pattern
Analysis and Machine Intelligence,
PAMI-5(2):179-190.
</figureCaption>
<reference confidence="0.986140566037736">
Bickel, Peter J., and Doksum, Kjell A. (1977).
Mathematical Statistics: Basic Ideas and
Selected Topics. Oakland, California: Holden
Day, Inc.
Black, Ezra; Jelinek, Fred; Lafferty, John D.;
Magerman, David M.; Mercer, Robert L.;
and Roukos, Salim (1992a). &amp;quot;Towards
history-based grammars: Using richer
models for probabilistic parsing.&amp;quot; In Speech
and Natural Language: Proceedings of a
Workshop Held at Harriman, New York.
134-139. San Francisco, California: Morgan
Kaufmann Publishers, Inc.
Black, Ezra; Jelinek, Fred; Lafferty, John D.;
Mercer, Robert L.; and Roukos, Salim
(1992b). &amp;quot;Decision tree models applied to
the labeling of text with parts of speech.&amp;quot;
In Speech and Natural Language: Proceedings
of a Workshop Held at Harriman, New York.
117-121. San Francisco, California:
Morgan Kaufmann Publishers, Inc.
Bod, Rens (1993). &amp;quot;Using an annotated
corpus as a stochastic grammar.&amp;quot; In Sixth
Conference of the European Chapter of the
Association for Computational Linguistics.
37-44. Utrecht, The Netherlands.
Brill, Eric (1992). &amp;quot;A simple rule-based part
of speech tagger.&amp;quot; In Speech and Natural
Language: Proceedings of a Workshop Held at
Harriman, New York. 112-116. San
Francisco, California: Morgan Kaufmann
Publishers, Inc.
Brill, Eric (1993). A corpus-based approach to
language learning. Doctoral dissertation,
Department of Computer and
Information Science, University of
Pennsylvania. Philadelphia, Pennsylvania.
Brill, Eric; Magerman, David M.; Marcus,
Mitchell P.; and Santorini, Beatrice (1990).
&amp;quot;Deducing linguistic structure from the
statistics of large corpora.&amp;quot; In Speech and
Natural Language: Proceedings of a Workshop
Held at Hidden Valley, Pennsylvania.
275-282. San Francisco, California:
Morgan Kaufmann Publishers, Inc.
Brown, Peter F.; Cocke, John; Della Pietra,
Stephen A.; Della Pietra, Vincent J.;
Jelinek, Fred; Lafferty, John D.; Mercer,
Robert L.; and Roossin, Paul S. (1990). &amp;quot;A
statistical approach to machine
translation.&amp;quot; Computational Linguistics
16(2): 79-85.
Brown, Peter F.; Della Pietra, Vincent J.;
</reference>
<page confidence="0.997316">
109
</page>
<note confidence="0.512965">
Computational Linguistics Volume 21, Number 1
</note>
<reference confidence="0.998883139344263">
deSouza, Peter V.; Lai, Jennifer C.; and
Mercer, Robert L. (1992). &amp;quot;Class-based
n-gram models of natural language.&amp;quot;
Computational Linguistics 18(4):467-479.
Chitrao, Mahesh, and Grishman, Ralph
(1990). &amp;quot;Statistical parsing of messages.&amp;quot; In
Speech and Natural Language: Proceedings of a
Workshop Held at Hidden Valley, Pennsylvania.
263-266. San Francisco, California: Morgan
Kaufmann Publishers, Inc.
Church, Kenneth W.; Gale, William; Hanks,
Peter; and Hindle, Donald (1989).
&amp;quot;Parsing, word associations, and typical
predicate-argument relations.&amp;quot; In Speech
and Natural Language: Proceedings of a
Workshop Held at Cape Cod, Massachusetts.
75-81. San Francisco, California: Morgan
Kaufmann Publishers, Inc.
Cover, Thomas M., and Thomas, Joy A.
(1991). Elements of Information Theory. New
York: John Wiley and Sons, Inc.
DARPA (1989a). Speech and Natural Language:
Proceedings of a Workshop Held at Philadelphia,
Pennsylvania (1989). Sponsored by Defense
Advanced Research Projects Agency,
Information Science and Technology
Office. San Francisco, California: Morgan
Kaufmann Publishers, Inc.
DARPA (1989b). Speech and Natural Language:
Proceedings of a Workshop Held at Cape Cod,
Massachusetts (1989). Sponsored by
Defense Advanced Research Projects
Agency, Information Science and
Technology Office. San Francisco,
California: Morgan Kaufmann Publishers,
Inc.
DARPA (1990). Speech and Natural Language:
Proceedings of a Workshop Held at Hidden
Valley, Pennsylvania (1990). Sponsored by
Defense Advanced Research Projects
Agency, Information Science and
Technology Office. San Francisco,
California: Morgan Kaufmann Publishers,
Inc.
DARPA (1991). Speech and Natural Language:
Proceedings of a Workshop Held at Pacific
Grove, California (1991). Sponsored by
Defense Advanced Research Projects
Agency, Information Science and
Technology Office. San Francisco,
California: Morgan Kaufmann Publishers,
Inc.
DARPA (1992). Speech and Natural Language:
Proceedings of a Workshop Held at Harriman,
New York (1992). Sponsored by Defense
Advanced Research Projects Agency,
Information Science and Technology
Office. San Francisco, California: Morgan
Kaufmann Publishers, Inc.
DeGroot, M. H. (1986). Probability and
Statistics, Second Edition. Reading,
Massachusetts: Addison-Wesley.
Della Pietra, Stephen A.; Della Pietra,
Vincent J.; Gillett, John; Lafferty, John D.;
Printz, Harry; Ure§, Lubos (1994).
&amp;quot;Inference and estimation of a long-range
trigram model.&amp;quot; In Lecture Notes in
Artificial Intelligence 862, Proceedings of the
Second International Colloquium on
Grammatical Inference and Applications.
78-92. New York: Springer-Verlag. Also
available as Technical Report
CMU-CS-94-188, Department of
Computer Science, Carnegie Mellon
University, Pittsburgh, Pennsylvania.
Gale, William, and Church, Kenneth W.
(1990). &amp;quot;Poor estimates of context are
worse than none.&amp;quot; In Speech and Natural
Language: Proceedings of a Workshop Held at
Hidden Valley, Pennsylvania. 283-287. San
Francisco, California: Morgan Kaufmann
Publishers, Inc.
Hindle, Donald, and Rooth, Mats (1993).
&amp;quot;Structural ambiguity and lexical
relations.&amp;quot; Computational Linguistics,
Special Issue on Using Large Corpora: I
19(1):103-120.
Lafferty, John D.; Sleator, Daniel; and
Temperly, Davy (1992). &amp;quot;Grammatical
trigrams: A probabilistic link grammar.&amp;quot;
In AAAI Fall Symposium on Probabilistic
Approaches to Natural Language.
Lau, Raymond; Rosenfeld, Roni; and
Roukos, Salim (1993). &amp;quot;Adaptive language
modeling using the maximum entropy
principle.&amp;quot; In Human Language Technology:
Proceedings of a Workshop Held at Plainsboro,
New Jersey. 108-113. San Francisco,
California: Morgan Kaufmann Publishers,
Inc.
Magerman, David M. (1994). Natural
language parsing as statistical pattern
recognition. Doctoral dissertation,
Computer Science Department, Stanford
University, Stanford, California.
Magerman, David M., and Marcus, Mitchell
P. (1991). &amp;quot;Parsing the voyager domain
using Pearl.&amp;quot; In Speech and Natural
Language: Proceedings of a Workshop Held at
Pacific Grove, California. 231-236. San
Francisco, California: Morgan Kaufmann
Publishers, Inc.
Merialdo, Bernard (1994). &amp;quot;Tagging English
text with a probabilistic model.&amp;quot;
Computational Linguistics 20(2):155-172.
Miller, Scott Z.; Bobrow, Robert; Ingria,
Robert; and Schwartz, Richard (1994).
&amp;quot;Hidden understanding models of
natural language.&amp;quot; In Proceedings, 32nd
Annual Meeting of the Association for
Computational Linguistics. 25-32. Las
Cruces, New Mexico.
</reference>
<page confidence="0.984268">
110
</page>
<reference confidence="0.987958529411765">
Book Reviews
Resnik, Philip (1993). Selection and 18(1):61-86.
information: A class-based approach to lexical Yarowsky, David (1994). &amp;quot;Decision lists for
relationships. Doctoral dissertation, lexical ambiguity resolution: Application
Department of Computer and Information to accent restoration in Spanish and
Science, University of Pennsylvania, French.&amp;quot; In Proceedings, 32nd Annual
Philadelphia, Pennsylvania. Meeting of the Association for Computational
Seneff, Stephanie (1992). &amp;quot;TINA: A natural Linguistics. 88-95. Las Cruces, New
language system for spoken language Mexico.
applications.&amp;quot; Computational Linguistics
David Magerman is a research scientist in the Speech and Language Processing Department
at Bolt Beranek and Newman Inc. He was a member of the IBM Speech Recognition group
under Frederick Jelinek from 1991 to 1994, where he completed his doctoral thesis on statistical
grammar acquisition. He also worked with Mitchell Marcus at the University of Pennsylvania
from 1989 to 1991 on self-organized grammar learning and statistical parsing. Magerman&apos;s
address is Bolt Beranek and Newman Inc., Room 15/148, 70 Fawcett Street, Cambridge, MA
02138. E-mail: magerman@bbn.com
</reference>
<page confidence="0.9988">
111
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.698467">
<title confidence="0.9980785">Book Reviews Statistical Language Learning</title>
<author confidence="0.995248">Eugene Charniak</author>
<affiliation confidence="0.999745">(Brown University)</affiliation>
<address confidence="0.994803">Cambridge, MA: The MIT Press (A</address>
<note confidence="0.975022666666667">Bradford book), 1993, xx + 170 pp. Hardbound, ISBN 0-262-03216-3, $25.00 Reviewed by</note>
<author confidence="0.994101">David M Magerman</author>
<affiliation confidence="0.762034">Bolt Beranek and Newman Inc.</affiliation>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter J Bickel</author>
<author>Kjell A Doksum</author>
</authors>
<title>Mathematical Statistics: Basic Ideas and Selected Topics.</title>
<date>1977</date>
<publisher>Holden Day, Inc.</publisher>
<location>Oakland, California:</location>
<contexts>
<context position="8384" citStr="Bickel and Doksum 1977" startWordPosition="1343" endWordPosition="1346"> The correct formula is actually P(a2,b1, c4)P(speech signal I a, b1, c4). This is most certainly a typographical error, although it is repeated twice. But the reader, without confidence in his or her understanding of probability theory, is not likely to identify this as an error. One might suggest that the author is of course assuming that the student will augment his or her reading with an appropriate probability-theory text and a good introduction to information theory. However, he fails to recommend a good resource for these fields of study, of which there are many (e.g., DeGroot 1986, or Bickel and Doksum 1977, for probability theory, and Cover and Thomas 1991, or Ash 1965, for information theory), or to include any such texts in his bibliography. It seems as though he feels his presentation is sufficient to understand the material, or at least that further discussion of these topics is unnecessary. This treatment of the mathematical foundations of statistical modeling is common in the statistical NLP literature, and it is motivated by a misinterpretation of the role statistical methods play in this field of research. Statistical methods are not simply wellunderstood tools that are used to learn ab</context>
</contexts>
<marker>Bickel, Doksum, 1977</marker>
<rawString>Bickel, Peter J., and Doksum, Kjell A. (1977). Mathematical Statistics: Basic Ideas and Selected Topics. Oakland, California: Holden Day, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>Fred Jelinek</author>
<author>John D Lafferty</author>
<author>David M Magerman</author>
<author>Robert L Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Towards history-based grammars: Using richer models for probabilistic parsing.&amp;quot;</title>
<date>1992</date>
<booktitle>In Speech and Natural Language: Proceedings of a Workshop Held at Harriman,</booktitle>
<pages>134--139</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>New York.</location>
<contexts>
<context position="11076" citStr="Black et al. 1992" startWordPosition="1772" endWordPosition="1775">arized later in journals. Conveniently, much of the foundational work in statistical NLP has been published in the proceedings of the Speech and Natural Language Workshop (later called the Human Language Technologies Workshop), sponsored by DARPA (later called ARPA) (DARPA 1989a, 1989b, 1990, 1991, 1992; ARPA, 1993, 1994). However, the book&apos;s bibliography fails to cite any papers from any of these workshops, many of which were important in the development of statistical NLP (e.g., Church et al. 1989; Brill et al. 1990; Chitrao and Grishman 1990; Gale and Church 1990; Magerman and Marcus 1991; Black et al. 1992a, 1992b; Brill 1992; Lau, Rosenfeld, and Roukos 1993). Of the 44 bibliography entries, only two papers from Computational Linguistics are mentioned, omitting papers such as Brown et al. 1990, Seneff 1992, and Hindle and Rooth 1993,2 among others. And from the major computational linguistics conferences, whose participants have recently complained about the overwhelming number of papers on statistical methods, only eight papers are cited. There are a number of papers cited from the working notes of two AAAI workshops: the 1992 AAAI Fall Symposium Series on Probabilistic Approaches to Natural L</context>
</contexts>
<marker>Black, Jelinek, Lafferty, Magerman, Mercer, Roukos, 1992</marker>
<rawString>Black, Ezra; Jelinek, Fred; Lafferty, John D.; Magerman, David M.; Mercer, Robert L.; and Roukos, Salim (1992a). &amp;quot;Towards history-based grammars: Using richer models for probabilistic parsing.&amp;quot; In Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York. 134-139. San Francisco, California: Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>Fred Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Decision tree models applied to the labeling of text with parts of speech.&amp;quot;</title>
<date>1992</date>
<booktitle>In Speech and Natural Language: Proceedings of a Workshop Held at Harriman,</booktitle>
<pages>117--121</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>New York.</location>
<contexts>
<context position="11076" citStr="Black et al. 1992" startWordPosition="1772" endWordPosition="1775">arized later in journals. Conveniently, much of the foundational work in statistical NLP has been published in the proceedings of the Speech and Natural Language Workshop (later called the Human Language Technologies Workshop), sponsored by DARPA (later called ARPA) (DARPA 1989a, 1989b, 1990, 1991, 1992; ARPA, 1993, 1994). However, the book&apos;s bibliography fails to cite any papers from any of these workshops, many of which were important in the development of statistical NLP (e.g., Church et al. 1989; Brill et al. 1990; Chitrao and Grishman 1990; Gale and Church 1990; Magerman and Marcus 1991; Black et al. 1992a, 1992b; Brill 1992; Lau, Rosenfeld, and Roukos 1993). Of the 44 bibliography entries, only two papers from Computational Linguistics are mentioned, omitting papers such as Brown et al. 1990, Seneff 1992, and Hindle and Rooth 1993,2 among others. And from the major computational linguistics conferences, whose participants have recently complained about the overwhelming number of papers on statistical methods, only eight papers are cited. There are a number of papers cited from the working notes of two AAAI workshops: the 1992 AAAI Fall Symposium Series on Probabilistic Approaches to Natural L</context>
</contexts>
<marker>Black, Jelinek, Lafferty, Mercer, Roukos, 1992</marker>
<rawString>Black, Ezra; Jelinek, Fred; Lafferty, John D.; Mercer, Robert L.; and Roukos, Salim (1992b). &amp;quot;Decision tree models applied to the labeling of text with parts of speech.&amp;quot; In Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York. 117-121. San Francisco, California: Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Using an annotated corpus as a stochastic grammar.&amp;quot;</title>
<date>1993</date>
<booktitle>In Sixth Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<pages>37--44</pages>
<location>Utrecht, The Netherlands.</location>
<contexts>
<context position="12811" citStr="Bod 1993" startWordPosition="2049" endWordPosition="2050">oundational papers are discussed in the book, but many others are ignored and replaced by discussions of papers that are more on the periphery of the field. And many of the omitted papers represent the state of the art in the fields of language modeling, part-of-speech tagging, parsing, and general statistical language learning. Some recently published work relevant to the study of these areas include the following: Lafferty, Sleator, and Temperly 1992, Lau, Rosenfeld, and Roukos 1993, and Della Pietra et al. 1994, for language modeling; Brill 1993 and Merialdo 1994 for partof-speech tagging; Bod 1993 and Magerman 1994 for parsing; and Resnik 1993, Miller et al. 1994, and Yarowsky 1994, for various topics in statistical language learning. 4. Oversimplifications In an introductory text, it is advisable to simplify some concepts for the reader to avoid confusion. Charniak employs this technique to great advantage. Occasionally, however, he crosses the line from simplification to oversimplification, leading the reader to draw inappropriate conclusions. 2 Hindle and Rooth 1993 is an expanded, journal-length version of a conference paper the same title, which is cited in the book. 106 Book Revi</context>
</contexts>
<marker>Bod, 1993</marker>
<rawString>Bod, Rens (1993). &amp;quot;Using an annotated corpus as a stochastic grammar.&amp;quot; In Sixth Conference of the European Chapter of the Association for Computational Linguistics. 37-44. Utrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.&amp;quot;</title>
<date>1992</date>
<booktitle>In Speech and Natural Language: Proceedings of a Workshop Held at</booktitle>
<pages>112--116</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>Harriman, New York.</location>
<contexts>
<context position="11096" citStr="Brill 1992" startWordPosition="1777" endWordPosition="1778">onveniently, much of the foundational work in statistical NLP has been published in the proceedings of the Speech and Natural Language Workshop (later called the Human Language Technologies Workshop), sponsored by DARPA (later called ARPA) (DARPA 1989a, 1989b, 1990, 1991, 1992; ARPA, 1993, 1994). However, the book&apos;s bibliography fails to cite any papers from any of these workshops, many of which were important in the development of statistical NLP (e.g., Church et al. 1989; Brill et al. 1990; Chitrao and Grishman 1990; Gale and Church 1990; Magerman and Marcus 1991; Black et al. 1992a, 1992b; Brill 1992; Lau, Rosenfeld, and Roukos 1993). Of the 44 bibliography entries, only two papers from Computational Linguistics are mentioned, omitting papers such as Brown et al. 1990, Seneff 1992, and Hindle and Rooth 1993,2 among others. And from the major computational linguistics conferences, whose participants have recently complained about the overwhelming number of papers on statistical methods, only eight papers are cited. There are a number of papers cited from the working notes of two AAAI workshops: the 1992 AAAI Fall Symposium Series on Probabilistic Approaches to Natural Language and the 1992</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>Brill, Eric (1992). &amp;quot;A simple rule-based part of speech tagger.&amp;quot; In Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York. 112-116. San Francisco, California: Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A corpus-based approach to language learning.</title>
<date>1993</date>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<location>Philadelphia, Pennsylvania.</location>
<note>Doctoral dissertation,</note>
<contexts>
<context position="12757" citStr="Brill 1993" startWordPosition="2040" endWordPosition="2041">tream literature throughout. Some of the important and foundational papers are discussed in the book, but many others are ignored and replaced by discussions of papers that are more on the periphery of the field. And many of the omitted papers represent the state of the art in the fields of language modeling, part-of-speech tagging, parsing, and general statistical language learning. Some recently published work relevant to the study of these areas include the following: Lafferty, Sleator, and Temperly 1992, Lau, Rosenfeld, and Roukos 1993, and Della Pietra et al. 1994, for language modeling; Brill 1993 and Merialdo 1994 for partof-speech tagging; Bod 1993 and Magerman 1994 for parsing; and Resnik 1993, Miller et al. 1994, and Yarowsky 1994, for various topics in statistical language learning. 4. Oversimplifications In an introductory text, it is advisable to simplify some concepts for the reader to avoid confusion. Charniak employs this technique to great advantage. Occasionally, however, he crosses the line from simplification to oversimplification, leading the reader to draw inappropriate conclusions. 2 Hindle and Rooth 1993 is an expanded, journal-length version of a conference paper the</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>Brill, Eric (1993). A corpus-based approach to language learning. Doctoral dissertation, Department of Computer and Information Science, University of Pennsylvania. Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>David M Magerman</author>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
</authors>
<title>Deducing linguistic structure from the statistics of large corpora.&amp;quot;</title>
<date>1990</date>
<booktitle>In Speech and Natural Language: Proceedings of a Workshop Held at Hidden</booktitle>
<pages>275--282</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>Valley, Pennsylvania.</location>
<contexts>
<context position="10982" citStr="Brill et al. 1990" startWordPosition="1756" endWordPosition="1759">ses, although these tend to be more verbose and overly technical, and they are frequently summarized later in journals. Conveniently, much of the foundational work in statistical NLP has been published in the proceedings of the Speech and Natural Language Workshop (later called the Human Language Technologies Workshop), sponsored by DARPA (later called ARPA) (DARPA 1989a, 1989b, 1990, 1991, 1992; ARPA, 1993, 1994). However, the book&apos;s bibliography fails to cite any papers from any of these workshops, many of which were important in the development of statistical NLP (e.g., Church et al. 1989; Brill et al. 1990; Chitrao and Grishman 1990; Gale and Church 1990; Magerman and Marcus 1991; Black et al. 1992a, 1992b; Brill 1992; Lau, Rosenfeld, and Roukos 1993). Of the 44 bibliography entries, only two papers from Computational Linguistics are mentioned, omitting papers such as Brown et al. 1990, Seneff 1992, and Hindle and Rooth 1993,2 among others. And from the major computational linguistics conferences, whose participants have recently complained about the overwhelming number of papers on statistical methods, only eight papers are cited. There are a number of papers cited from the working notes of tw</context>
</contexts>
<marker>Brill, Magerman, Marcus, Santorini, 1990</marker>
<rawString>Brill, Eric; Magerman, David M.; Marcus, Mitchell P.; and Santorini, Beatrice (1990). &amp;quot;Deducing linguistic structure from the statistics of large corpora.&amp;quot; In Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania. 275-282. San Francisco, California: Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Della Pietra</author>
<author>A Stephen</author>
<author>Della Pietra</author>
<author>J Vincent</author>
<author>Fred Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.&amp;quot;</title>
<date>1990</date>
<journal>Computational Linguistics</journal>
<volume>16</volume>
<issue>2</issue>
<pages>79--85</pages>
<contexts>
<context position="11267" citStr="Brown et al. 1990" startWordPosition="1801" endWordPosition="1804">man Language Technologies Workshop), sponsored by DARPA (later called ARPA) (DARPA 1989a, 1989b, 1990, 1991, 1992; ARPA, 1993, 1994). However, the book&apos;s bibliography fails to cite any papers from any of these workshops, many of which were important in the development of statistical NLP (e.g., Church et al. 1989; Brill et al. 1990; Chitrao and Grishman 1990; Gale and Church 1990; Magerman and Marcus 1991; Black et al. 1992a, 1992b; Brill 1992; Lau, Rosenfeld, and Roukos 1993). Of the 44 bibliography entries, only two papers from Computational Linguistics are mentioned, omitting papers such as Brown et al. 1990, Seneff 1992, and Hindle and Rooth 1993,2 among others. And from the major computational linguistics conferences, whose participants have recently complained about the overwhelming number of papers on statistical methods, only eight papers are cited. There are a number of papers cited from the working notes of two AAAI workshops: the 1992 AAAI Fall Symposium Series on Probabilistic Approaches to Natural Language and the 1992 Workshop on Statistically-Based NLP Techniques. These papers may be interesting and worthy of discussion, but they are of less value to the reader than papers from publis</context>
</contexts>
<marker>Brown, Cocke, Pietra, Stephen, Pietra, Vincent, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Brown, Peter F.; Cocke, John; Della Pietra, Stephen A.; Della Pietra, Vincent J.; Jelinek, Fred; Lafferty, John D.; Mercer, Robert L.; and Roossin, Paul S. (1990). &amp;quot;A statistical approach to machine translation.&amp;quot; Computational Linguistics 16(2): 79-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Della Pietra</author>
<author>J Vincent</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.&amp;quot;</title>
<date>1992</date>
<journal>Computational Linguistics</journal>
<pages>18--4</pages>
<contexts>
<context position="15489" citStr="Brown et al. 1992" startWordPosition="2470" endWordPosition="2473">nly proposed solutions involve reducing the number of parameters in the model in ways that make the parameters estimable directly from frequency counts from a corpus. These reductions yield much weaker models and are unnecessary given current modeling techniques. Another example of oversimplification occurs in the text&apos;s presentation of deleted interpolation. The definition of deleted interpolation applied to smoothing trigram word models is technically correct: P(Wn I Wn—lW0-2) = AlPe(Wn) + A2Pe(Wn I wn-1) + A3Pe (Wn 1 Wn-1Wn —2) - Here, the A parameters are simply scalar values. However, in Brown et al. 1992 (also in Bahl, Jelinek, and Mercer 1983), where deleted interpolation is defined, these parameters represent functions of the history, A( ,wn-iwn-2) : P(Wn I Wn—lWn-2) Ai(Wn—lWn-2)Pe(Wn) ± A2(Wn-1Wn-2)Pe(Wn I wn-1) ± A3(Wn-1Wn-2)Pe(Wn I Wn—lWn-2)• This may seem like a minor point, but the effectiveness of deleted interpolation depends on this distinction, and the repercussions of this oversimplification are quite 3 Actually, the book never uses the term deleted interpolation when describing the technique. This omission makes it difficult for the reader to investigate this technique further in</context>
<context position="17488" citStr="Brown et al. (1992)" startWordPosition="2789" endWordPosition="2792">ely) will never be close to zero. For the sake of discussion, let&apos;s use the author&apos;s guesses of these parameters, since they are quite reasonable: A3 = 0.6, A2 = 0.3, and A1 = 0.1. For the most frequent bigram histories, the empirical trigram model is the best predictor of the next word. For instance, the probability of the word that follows the bigram in the can be accurately estimated by an empirical trigram model. In contrast, the probability of the word following volcanologic astrobiology is better estimated using a unigram model. The formulation of the deleted interpolation parameters of Brown et al. (1992) is designed to allow high-frequency histories to depend on the trigram counts while deferring to the bigram and unigram counts for the low-frequency histories. So, A3 (in the) will be very close to 1. Using the book&apos;s formulation, P(the 1 in the) is significantly overestimated because the model derives 10% of its estimate from the unigram model, even though the direct estimate from corpus frequencies is more accurate. Deleted interpolation as described by Brown et al. (1992) yields a far better language model than the simpler formulation used in the book, in terms of both entropy and performa</context>
</contexts>
<marker>Brown, Pietra, Vincent, deSouza, Lai, Mercer, 1992</marker>
<rawString>Brown, Peter F.; Della Pietra, Vincent J.; deSouza, Peter V.; Lai, Jennifer C.; and Mercer, Robert L. (1992). &amp;quot;Class-based n-gram models of natural language.&amp;quot; Computational Linguistics 18(4):467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahesh Chitrao</author>
<author>Ralph Grishman</author>
</authors>
<title>Statistical parsing of messages.&amp;quot;</title>
<date>1990</date>
<booktitle>In Speech and Natural Language: Proceedings of a Workshop Held at Hidden</booktitle>
<pages>263--266</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>Valley, Pennsylvania.</location>
<contexts>
<context position="11009" citStr="Chitrao and Grishman 1990" startWordPosition="1760" endWordPosition="1763"> tend to be more verbose and overly technical, and they are frequently summarized later in journals. Conveniently, much of the foundational work in statistical NLP has been published in the proceedings of the Speech and Natural Language Workshop (later called the Human Language Technologies Workshop), sponsored by DARPA (later called ARPA) (DARPA 1989a, 1989b, 1990, 1991, 1992; ARPA, 1993, 1994). However, the book&apos;s bibliography fails to cite any papers from any of these workshops, many of which were important in the development of statistical NLP (e.g., Church et al. 1989; Brill et al. 1990; Chitrao and Grishman 1990; Gale and Church 1990; Magerman and Marcus 1991; Black et al. 1992a, 1992b; Brill 1992; Lau, Rosenfeld, and Roukos 1993). Of the 44 bibliography entries, only two papers from Computational Linguistics are mentioned, omitting papers such as Brown et al. 1990, Seneff 1992, and Hindle and Rooth 1993,2 among others. And from the major computational linguistics conferences, whose participants have recently complained about the overwhelming number of papers on statistical methods, only eight papers are cited. There are a number of papers cited from the working notes of two AAAI workshops: the 1992 </context>
</contexts>
<marker>Chitrao, Grishman, 1990</marker>
<rawString>Chitrao, Mahesh, and Grishman, Ralph (1990). &amp;quot;Statistical parsing of messages.&amp;quot; In Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania. 263-266. San Francisco, California: Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>William Gale</author>
<author>Peter Hanks</author>
<author>Donald Hindle</author>
</authors>
<date>1989</date>
<contexts>
<context position="10963" citStr="Church et al. 1989" startWordPosition="1752" endWordPosition="1755"> recent doctoral theses, although these tend to be more verbose and overly technical, and they are frequently summarized later in journals. Conveniently, much of the foundational work in statistical NLP has been published in the proceedings of the Speech and Natural Language Workshop (later called the Human Language Technologies Workshop), sponsored by DARPA (later called ARPA) (DARPA 1989a, 1989b, 1990, 1991, 1992; ARPA, 1993, 1994). However, the book&apos;s bibliography fails to cite any papers from any of these workshops, many of which were important in the development of statistical NLP (e.g., Church et al. 1989; Brill et al. 1990; Chitrao and Grishman 1990; Gale and Church 1990; Magerman and Marcus 1991; Black et al. 1992a, 1992b; Brill 1992; Lau, Rosenfeld, and Roukos 1993). Of the 44 bibliography entries, only two papers from Computational Linguistics are mentioned, omitting papers such as Brown et al. 1990, Seneff 1992, and Hindle and Rooth 1993,2 among others. And from the major computational linguistics conferences, whose participants have recently complained about the overwhelming number of papers on statistical methods, only eight papers are cited. There are a number of papers cited from the </context>
</contexts>
<marker>Church, Gale, Hanks, Hindle, 1989</marker>
<rawString>Church, Kenneth W.; Gale, William; Hanks, Peter; and Hindle, Donald (1989).</rawString>
</citation>
<citation valid="false">
<authors>
<author>word Parsing</author>
</authors>
<title>associations, and typical predicate-argument relations.&amp;quot; In Speech and Natural Language:</title>
<booktitle>Proceedings of a Workshop Held at Cape Cod,</booktitle>
<pages>75--81</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>Massachusetts.</location>
<marker>Parsing, </marker>
<rawString>&amp;quot;Parsing, word associations, and typical predicate-argument relations.&amp;quot; In Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts. 75-81. San Francisco, California: Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley and Sons, Inc.</publisher>
<location>New York:</location>
<contexts>
<context position="8435" citStr="Cover and Thomas 1991" startWordPosition="1351" endWordPosition="1354">h signal I a, b1, c4). This is most certainly a typographical error, although it is repeated twice. But the reader, without confidence in his or her understanding of probability theory, is not likely to identify this as an error. One might suggest that the author is of course assuming that the student will augment his or her reading with an appropriate probability-theory text and a good introduction to information theory. However, he fails to recommend a good resource for these fields of study, of which there are many (e.g., DeGroot 1986, or Bickel and Doksum 1977, for probability theory, and Cover and Thomas 1991, or Ash 1965, for information theory), or to include any such texts in his bibliography. It seems as though he feels his presentation is sufficient to understand the material, or at least that further discussion of these topics is unnecessary. This treatment of the mathematical foundations of statistical modeling is common in the statistical NLP literature, and it is motivated by a misinterpretation of the role statistical methods play in this field of research. Statistical methods are not simply wellunderstood tools that are used to learn about language; they are one of the central concerns </context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, Thomas M., and Thomas, Joy A. (1991). Elements of Information Theory. New York: John Wiley and Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<title>Speech and Natural Language: Proceedings of a Workshop Held at</title>
<date>1989</date>
<booktitle>Sponsored by Defense Advanced Research Projects Agency, Information Science and Technology Office.</booktitle>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>Philadelphia, Pennsylvania</location>
<contexts>
<context position="10737" citStr="DARPA 1989" startWordPosition="1716" endWordPosition="1717">entrated sources of material to point to and summarize. In the absence of books on the subject, the best places to look are journals, conference proceedings, and published workshop proceedings. Another good resource is recent doctoral theses, although these tend to be more verbose and overly technical, and they are frequently summarized later in journals. Conveniently, much of the foundational work in statistical NLP has been published in the proceedings of the Speech and Natural Language Workshop (later called the Human Language Technologies Workshop), sponsored by DARPA (later called ARPA) (DARPA 1989a, 1989b, 1990, 1991, 1992; ARPA, 1993, 1994). However, the book&apos;s bibliography fails to cite any papers from any of these workshops, many of which were important in the development of statistical NLP (e.g., Church et al. 1989; Brill et al. 1990; Chitrao and Grishman 1990; Gale and Church 1990; Magerman and Marcus 1991; Black et al. 1992a, 1992b; Brill 1992; Lau, Rosenfeld, and Roukos 1993). Of the 44 bibliography entries, only two papers from Computational Linguistics are mentioned, omitting papers such as Brown et al. 1990, Seneff 1992, and Hindle and Rooth 1993,2 among others. And from the </context>
</contexts>
<marker>DARPA, 1989</marker>
<rawString>DARPA (1989a). Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, Pennsylvania (1989). Sponsored by Defense Advanced Research Projects Agency, Information Science and Technology Office. San Francisco, California: Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<title>Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod,</title>
<date>1989</date>
<booktitle>Sponsored by Defense Advanced Research Projects Agency, Information Science and Technology Office.</booktitle>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>Massachusetts</location>
<contexts>
<context position="10737" citStr="DARPA 1989" startWordPosition="1716" endWordPosition="1717">entrated sources of material to point to and summarize. In the absence of books on the subject, the best places to look are journals, conference proceedings, and published workshop proceedings. Another good resource is recent doctoral theses, although these tend to be more verbose and overly technical, and they are frequently summarized later in journals. Conveniently, much of the foundational work in statistical NLP has been published in the proceedings of the Speech and Natural Language Workshop (later called the Human Language Technologies Workshop), sponsored by DARPA (later called ARPA) (DARPA 1989a, 1989b, 1990, 1991, 1992; ARPA, 1993, 1994). However, the book&apos;s bibliography fails to cite any papers from any of these workshops, many of which were important in the development of statistical NLP (e.g., Church et al. 1989; Brill et al. 1990; Chitrao and Grishman 1990; Gale and Church 1990; Magerman and Marcus 1991; Black et al. 1992a, 1992b; Brill 1992; Lau, Rosenfeld, and Roukos 1993). Of the 44 bibliography entries, only two papers from Computational Linguistics are mentioned, omitting papers such as Brown et al. 1990, Seneff 1992, and Hindle and Rooth 1993,2 among others. And from the </context>
</contexts>
<marker>DARPA, 1989</marker>
<rawString>DARPA (1989b). Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts (1989). Sponsored by Defense Advanced Research Projects Agency, Information Science and Technology Office. San Francisco, California: Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<title>Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley,</title>
<date>1990</date>
<booktitle>Sponsored by Defense Advanced Research Projects Agency, Information Science and Technology Office.</booktitle>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>Pennsylvania</location>
<marker>DARPA, 1990</marker>
<rawString>DARPA (1990). Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania (1990). Sponsored by Defense Advanced Research Projects Agency, Information Science and Technology Office. San Francisco, California: Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<title>Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove,</title>
<date>1991</date>
<booktitle>Sponsored by Defense Advanced Research Projects Agency, Information Science and Technology Office.</booktitle>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>California</location>
<marker>DARPA, 1991</marker>
<rawString>DARPA (1991). Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California (1991). Sponsored by Defense Advanced Research Projects Agency, Information Science and Technology Office. San Francisco, California: Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<title>Speech and Natural Language: Proceedings of a Workshop Held at Harriman,</title>
<date>1992</date>
<booktitle>Sponsored by Defense Advanced Research Projects Agency, Information Science and Technology Office.</booktitle>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>New York</location>
<marker>DARPA, 1992</marker>
<rawString>DARPA (1992). Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York (1992). Sponsored by Defense Advanced Research Projects Agency, Information Science and Technology Office. San Francisco, California: Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M H DeGroot</author>
</authors>
<title>Probability and Statistics, Second Edition.</title>
<date>1986</date>
<publisher>Addison-Wesley.</publisher>
<location>Reading, Massachusetts:</location>
<contexts>
<context position="8357" citStr="DeGroot 1986" startWordPosition="1340" endWordPosition="1341">I speech signal). The correct formula is actually P(a2,b1, c4)P(speech signal I a, b1, c4). This is most certainly a typographical error, although it is repeated twice. But the reader, without confidence in his or her understanding of probability theory, is not likely to identify this as an error. One might suggest that the author is of course assuming that the student will augment his or her reading with an appropriate probability-theory text and a good introduction to information theory. However, he fails to recommend a good resource for these fields of study, of which there are many (e.g., DeGroot 1986, or Bickel and Doksum 1977, for probability theory, and Cover and Thomas 1991, or Ash 1965, for information theory), or to include any such texts in his bibliography. It seems as though he feels his presentation is sufficient to understand the material, or at least that further discussion of these topics is unnecessary. This treatment of the mathematical foundations of statistical modeling is common in the statistical NLP literature, and it is motivated by a misinterpretation of the role statistical methods play in this field of research. Statistical methods are not simply wellunderstood tool</context>
</contexts>
<marker>DeGroot, 1986</marker>
<rawString>DeGroot, M. H. (1986). Probability and Statistics, Second Edition. Reading, Massachusetts: Addison-Wesley.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Della Pietra</author>
<author>A Stephen</author>
<author>Della Pietra</author>
<author>J Vincent</author>
<author>John Gillett</author>
<author>John D Lafferty</author>
<author>Harry Printz</author>
<author>Ure§</author>
</authors>
<title>Inference and estimation of a long-range trigram model.&amp;quot;</title>
<date>1994</date>
<booktitle>In Lecture Notes in Artificial Intelligence 862, Proceedings of the Second International Colloquium on Grammatical Inference and Applications.</booktitle>
<tech>Technical Report CMU-CS-94-188,</tech>
<pages>78--92</pages>
<institution>Department of Computer Science, Carnegie Mellon University,</institution>
<location>Lubos</location>
<marker>Pietra, Stephen, Pietra, Vincent, Gillett, Lafferty, Printz, Ure§, 1994</marker>
<rawString>Della Pietra, Stephen A.; Della Pietra, Vincent J.; Gillett, John; Lafferty, John D.; Printz, Harry; Ure§, Lubos (1994). &amp;quot;Inference and estimation of a long-range trigram model.&amp;quot; In Lecture Notes in Artificial Intelligence 862, Proceedings of the Second International Colloquium on Grammatical Inference and Applications. 78-92. New York: Springer-Verlag. Also available as Technical Report CMU-CS-94-188, Department of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>Poor estimates of context are worse than none.&amp;quot;</title>
<date>1990</date>
<booktitle>In Speech and Natural Language: Proceedings of a Workshop Held at Hidden</booktitle>
<pages>283--287</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>Valley, Pennsylvania.</location>
<contexts>
<context position="11031" citStr="Gale and Church 1990" startWordPosition="1764" endWordPosition="1767">d overly technical, and they are frequently summarized later in journals. Conveniently, much of the foundational work in statistical NLP has been published in the proceedings of the Speech and Natural Language Workshop (later called the Human Language Technologies Workshop), sponsored by DARPA (later called ARPA) (DARPA 1989a, 1989b, 1990, 1991, 1992; ARPA, 1993, 1994). However, the book&apos;s bibliography fails to cite any papers from any of these workshops, many of which were important in the development of statistical NLP (e.g., Church et al. 1989; Brill et al. 1990; Chitrao and Grishman 1990; Gale and Church 1990; Magerman and Marcus 1991; Black et al. 1992a, 1992b; Brill 1992; Lau, Rosenfeld, and Roukos 1993). Of the 44 bibliography entries, only two papers from Computational Linguistics are mentioned, omitting papers such as Brown et al. 1990, Seneff 1992, and Hindle and Rooth 1993,2 among others. And from the major computational linguistics conferences, whose participants have recently complained about the overwhelming number of papers on statistical methods, only eight papers are cited. There are a number of papers cited from the working notes of two AAAI workshops: the 1992 AAAI Fall Symposium Se</context>
</contexts>
<marker>Gale, Church, 1990</marker>
<rawString>Gale, William, and Church, Kenneth W. (1990). &amp;quot;Poor estimates of context are worse than none.&amp;quot; In Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania. 283-287. San Francisco, California: Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics, Special Issue on Using Large Corpora: I</journal>
<pages>19--1</pages>
<contexts>
<context position="11307" citStr="Hindle and Rooth 1993" startWordPosition="1808" endWordPosition="1811"> sponsored by DARPA (later called ARPA) (DARPA 1989a, 1989b, 1990, 1991, 1992; ARPA, 1993, 1994). However, the book&apos;s bibliography fails to cite any papers from any of these workshops, many of which were important in the development of statistical NLP (e.g., Church et al. 1989; Brill et al. 1990; Chitrao and Grishman 1990; Gale and Church 1990; Magerman and Marcus 1991; Black et al. 1992a, 1992b; Brill 1992; Lau, Rosenfeld, and Roukos 1993). Of the 44 bibliography entries, only two papers from Computational Linguistics are mentioned, omitting papers such as Brown et al. 1990, Seneff 1992, and Hindle and Rooth 1993,2 among others. And from the major computational linguistics conferences, whose participants have recently complained about the overwhelming number of papers on statistical methods, only eight papers are cited. There are a number of papers cited from the working notes of two AAAI workshops: the 1992 AAAI Fall Symposium Series on Probabilistic Approaches to Natural Language and the 1992 Workshop on Statistically-Based NLP Techniques. These papers may be interesting and worthy of discussion, but they are of less value to the reader than papers from published proceedings and journals, since work</context>
<context position="13292" citStr="Hindle and Rooth 1993" startWordPosition="2117" endWordPosition="2120">feld, and Roukos 1993, and Della Pietra et al. 1994, for language modeling; Brill 1993 and Merialdo 1994 for partof-speech tagging; Bod 1993 and Magerman 1994 for parsing; and Resnik 1993, Miller et al. 1994, and Yarowsky 1994, for various topics in statistical language learning. 4. Oversimplifications In an introductory text, it is advisable to simplify some concepts for the reader to avoid confusion. Charniak employs this technique to great advantage. Occasionally, however, he crosses the line from simplification to oversimplification, leading the reader to draw inappropriate conclusions. 2 Hindle and Rooth 1993 is an expanded, journal-length version of a conference paper the same title, which is cited in the book. 106 Book Reviews One example is the book&apos;s treatment of smoothing. Smoothing probability models in the face of sparse data is a hot topic in the field, and it is worthy of far more discussion than it is given in this book. One technique, deleted interpolation, is discussed at length.&apos; But after this technique is introduced, explained, and discussed in Chapter 3, it is ignored in future discussions. Consider the following passage: It is also useful to consider some not-so-good language mode</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Hindle, Donald, and Rooth, Mats (1993). &amp;quot;Structural ambiguity and lexical relations.&amp;quot; Computational Linguistics, Special Issue on Using Large Corpora: I 19(1):103-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Daniel Sleator</author>
<author>Davy Temperly</author>
</authors>
<title>Grammatical trigrams: A probabilistic link grammar.&amp;quot;</title>
<date>1992</date>
<booktitle>In AAAI Fall Symposium on Probabilistic Approaches to Natural Language.</booktitle>
<marker>Lafferty, Sleator, Temperly, 1992</marker>
<rawString>Lafferty, John D.; Sleator, Daniel; and Temperly, Davy (1992). &amp;quot;Grammatical trigrams: A probabilistic link grammar.&amp;quot; In AAAI Fall Symposium on Probabilistic Approaches to Natural Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond Lau</author>
<author>Roni Rosenfeld</author>
<author>Salim Roukos</author>
</authors>
<title>Adaptive language modeling using the maximum entropy principle.&amp;quot; In Human Language Technology:</title>
<date>1993</date>
<booktitle>Proceedings of a Workshop Held at Plainsboro,</booktitle>
<pages>108--113</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>New Jersey.</location>
<marker>Lau, Rosenfeld, Roukos, 1993</marker>
<rawString>Lau, Raymond; Rosenfeld, Roni; and Roukos, Salim (1993). &amp;quot;Adaptive language modeling using the maximum entropy principle.&amp;quot; In Human Language Technology: Proceedings of a Workshop Held at Plainsboro, New Jersey. 108-113. San Francisco, California: Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Natural language parsing as statistical pattern recognition.</title>
<date>1994</date>
<tech>Doctoral dissertation,</tech>
<institution>Computer Science Department, Stanford University,</institution>
<location>Stanford, California.</location>
<contexts>
<context position="12829" citStr="Magerman 1994" startWordPosition="2052" endWordPosition="2053">apers are discussed in the book, but many others are ignored and replaced by discussions of papers that are more on the periphery of the field. And many of the omitted papers represent the state of the art in the fields of language modeling, part-of-speech tagging, parsing, and general statistical language learning. Some recently published work relevant to the study of these areas include the following: Lafferty, Sleator, and Temperly 1992, Lau, Rosenfeld, and Roukos 1993, and Della Pietra et al. 1994, for language modeling; Brill 1993 and Merialdo 1994 for partof-speech tagging; Bod 1993 and Magerman 1994 for parsing; and Resnik 1993, Miller et al. 1994, and Yarowsky 1994, for various topics in statistical language learning. 4. Oversimplifications In an introductory text, it is advisable to simplify some concepts for the reader to avoid confusion. Charniak employs this technique to great advantage. Occasionally, however, he crosses the line from simplification to oversimplification, leading the reader to draw inappropriate conclusions. 2 Hindle and Rooth 1993 is an expanded, journal-length version of a conference paper the same title, which is cited in the book. 106 Book Reviews One example is</context>
</contexts>
<marker>Magerman, 1994</marker>
<rawString>Magerman, David M. (1994). Natural language parsing as statistical pattern recognition. Doctoral dissertation, Computer Science Department, Stanford University, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Parsing the voyager domain using Pearl.&amp;quot; In Speech and Natural Language:</title>
<date>1991</date>
<booktitle>Proceedings of a Workshop Held at</booktitle>
<pages>231--236</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>Pacific Grove, California.</location>
<contexts>
<context position="11057" citStr="Magerman and Marcus 1991" startWordPosition="1768" endWordPosition="1771">d they are frequently summarized later in journals. Conveniently, much of the foundational work in statistical NLP has been published in the proceedings of the Speech and Natural Language Workshop (later called the Human Language Technologies Workshop), sponsored by DARPA (later called ARPA) (DARPA 1989a, 1989b, 1990, 1991, 1992; ARPA, 1993, 1994). However, the book&apos;s bibliography fails to cite any papers from any of these workshops, many of which were important in the development of statistical NLP (e.g., Church et al. 1989; Brill et al. 1990; Chitrao and Grishman 1990; Gale and Church 1990; Magerman and Marcus 1991; Black et al. 1992a, 1992b; Brill 1992; Lau, Rosenfeld, and Roukos 1993). Of the 44 bibliography entries, only two papers from Computational Linguistics are mentioned, omitting papers such as Brown et al. 1990, Seneff 1992, and Hindle and Rooth 1993,2 among others. And from the major computational linguistics conferences, whose participants have recently complained about the overwhelming number of papers on statistical methods, only eight papers are cited. There are a number of papers cited from the working notes of two AAAI workshops: the 1992 AAAI Fall Symposium Series on Probabilistic Appr</context>
</contexts>
<marker>Magerman, Marcus, 1991</marker>
<rawString>Magerman, David M., and Marcus, Mitchell P. (1991). &amp;quot;Parsing the voyager domain using Pearl.&amp;quot; In Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California. 231-236. San Francisco, California: Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.&amp;quot;</title>
<date>1994</date>
<journal>Computational Linguistics</journal>
<pages>20--2</pages>
<contexts>
<context position="12775" citStr="Merialdo 1994" startWordPosition="2043" endWordPosition="2044">e throughout. Some of the important and foundational papers are discussed in the book, but many others are ignored and replaced by discussions of papers that are more on the periphery of the field. And many of the omitted papers represent the state of the art in the fields of language modeling, part-of-speech tagging, parsing, and general statistical language learning. Some recently published work relevant to the study of these areas include the following: Lafferty, Sleator, and Temperly 1992, Lau, Rosenfeld, and Roukos 1993, and Della Pietra et al. 1994, for language modeling; Brill 1993 and Merialdo 1994 for partof-speech tagging; Bod 1993 and Magerman 1994 for parsing; and Resnik 1993, Miller et al. 1994, and Yarowsky 1994, for various topics in statistical language learning. 4. Oversimplifications In an introductory text, it is advisable to simplify some concepts for the reader to avoid confusion. Charniak employs this technique to great advantage. Occasionally, however, he crosses the line from simplification to oversimplification, leading the reader to draw inappropriate conclusions. 2 Hindle and Rooth 1993 is an expanded, journal-length version of a conference paper the same title, which</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Merialdo, Bernard (1994). &amp;quot;Tagging English text with a probabilistic model.&amp;quot; Computational Linguistics 20(2):155-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Z Miller</author>
<author>Robert Bobrow</author>
<author>Robert Ingria</author>
<author>Richard Schwartz</author>
</authors>
<title>Hidden understanding models of natural language.&amp;quot;</title>
<date>1994</date>
<booktitle>In Proceedings, 32nd Annual Meeting of the Association for Computational Linguistics. 25-32. Las</booktitle>
<location>Cruces, New Mexico.</location>
<contexts>
<context position="12878" citStr="Miller et al. 1994" startWordPosition="2059" endWordPosition="2062">hers are ignored and replaced by discussions of papers that are more on the periphery of the field. And many of the omitted papers represent the state of the art in the fields of language modeling, part-of-speech tagging, parsing, and general statistical language learning. Some recently published work relevant to the study of these areas include the following: Lafferty, Sleator, and Temperly 1992, Lau, Rosenfeld, and Roukos 1993, and Della Pietra et al. 1994, for language modeling; Brill 1993 and Merialdo 1994 for partof-speech tagging; Bod 1993 and Magerman 1994 for parsing; and Resnik 1993, Miller et al. 1994, and Yarowsky 1994, for various topics in statistical language learning. 4. Oversimplifications In an introductory text, it is advisable to simplify some concepts for the reader to avoid confusion. Charniak employs this technique to great advantage. Occasionally, however, he crosses the line from simplification to oversimplification, leading the reader to draw inappropriate conclusions. 2 Hindle and Rooth 1993 is an expanded, journal-length version of a conference paper the same title, which is cited in the book. 106 Book Reviews One example is the book&apos;s treatment of smoothing. Smoothing pro</context>
</contexts>
<marker>Miller, Bobrow, Ingria, Schwartz, 1994</marker>
<rawString>Miller, Scott Z.; Bobrow, Robert; Ingria, Robert; and Schwartz, Richard (1994). &amp;quot;Hidden understanding models of natural language.&amp;quot; In Proceedings, 32nd Annual Meeting of the Association for Computational Linguistics. 25-32. Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="false">
<institution>Book Reviews</institution>
<marker></marker>
<rawString>Book Reviews</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selection and information: A class-based approach to lexical relationships. Doctoral dissertation,</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<booktitle>In Proceedings, 32nd Annual Meeting of the Association for Computational Linguistics. 88-95. Las</booktitle>
<pages>18--1</pages>
<institution>Department of Computer and Information Science, University of Pennsylvania,</institution>
<location>Philadelphia, Pennsylvania. Seneff, Stephanie</location>
<contexts>
<context position="12858" citStr="Resnik 1993" startWordPosition="2057" endWordPosition="2058">, but many others are ignored and replaced by discussions of papers that are more on the periphery of the field. And many of the omitted papers represent the state of the art in the fields of language modeling, part-of-speech tagging, parsing, and general statistical language learning. Some recently published work relevant to the study of these areas include the following: Lafferty, Sleator, and Temperly 1992, Lau, Rosenfeld, and Roukos 1993, and Della Pietra et al. 1994, for language modeling; Brill 1993 and Merialdo 1994 for partof-speech tagging; Bod 1993 and Magerman 1994 for parsing; and Resnik 1993, Miller et al. 1994, and Yarowsky 1994, for various topics in statistical language learning. 4. Oversimplifications In an introductory text, it is advisable to simplify some concepts for the reader to avoid confusion. Charniak employs this technique to great advantage. Occasionally, however, he crosses the line from simplification to oversimplification, leading the reader to draw inappropriate conclusions. 2 Hindle and Rooth 1993 is an expanded, journal-length version of a conference paper the same title, which is cited in the book. 106 Book Reviews One example is the book&apos;s treatment of smoo</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Resnik, Philip (1993). Selection and information: A class-based approach to lexical relationships. Doctoral dissertation, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, Pennsylvania. Seneff, Stephanie (1992). &amp;quot;TINA: A natural language system for spoken language applications.&amp;quot; Computational Linguistics 18(1):61-86. Yarowsky, David (1994). &amp;quot;Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French.&amp;quot; In Proceedings, 32nd Annual Meeting of the Association for Computational Linguistics. 88-95. Las Cruces, New Mexico.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>