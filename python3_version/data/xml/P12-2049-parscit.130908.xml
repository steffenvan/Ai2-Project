<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.198839">
<title confidence="0.996988">
A Corpus of Textual Revisions in Second Language Writing
</title>
<author confidence="0.994696">
John Lee and Jonathan Webster
</author>
<affiliation confidence="0.995308333333333">
The Halliday Centre for Intelligent Applications of Language Studies
Department of Chinese, Translation and Linguistics
City University of Hong Kong
</affiliation>
<email confidence="0.998617">
{jsylee,ctjjw}@cityu.edu.hk
</email>
<sectionHeader confidence="0.998597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998317083333333">
This paper describes the creation of the first
large-scale corpus containing drafts and fi-
nal versions of essays written by non-native
speakers, with the sentences aligned across
different versions. Furthermore, the sentences
in the drafts are annotated with comments
from teachers. The corpus is intended to sup-
port research on textual revision by language
learners, and how it is influenced by feedback.
This corpus has been converted into an XML
format conforming to the standards of the Text
Encoding Initiative (TEI).
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926736842105">
Learner corpora have been playing an increasingly
important role in both Second Language Acquisition
and Foreign Language Teaching research (Granger,
2004; Nesi et al., 2004). These corpora contain
texts written by non-native speakers of the lan-
guage (Granger et al., 2009); many also annotate
text segments where there are errors, and the cor-
responding error categories (Nagata et al., 2011). In
addition, some learner corpora contain pairs of sen-
tences: a sentence written by a learner of English
as a second language (ESL), paired with its correct
version produced by a native speaker (Dahlmeier
and Ng, 2011). These datasets are intended to sup-
port the training of automatic text correction sys-
tems (Dale and Kilgarriff, 2011).
Less attention has been paid to how a language
learner produces a text. Writing is often an iterative
and interactive process, with cycles of textual revi-
sion, guided by comments from language teachers.
</bodyText>
<table confidence="0.998461933333333">
Discipline # drafts
Applied Physics 988
Asian and International Studies 410
Biology 2310
Building Science and Technology 705
Business 1754
Computer Science 466
Creative Media 118
Electronic Engineering 1532
General Education 651
Law 31
Linguistics 2165
Management Sciences 1278
Social Studies 912
Total 13320
</table>
<tableCaption confidence="0.899534875">
Table 1: Draft essays are collected from courses in vari-
ous disciplines at City University of Hong Kong. These
drafts include lab reports, data analysis, argumentative
essays, and article summaries. There are 3760 distinct
essays, most of which consist of two to four successive
drafts. Each draft has on average 44.2 sentences, and the
average length of a sentence is 13.3 words. In total, the
corpus contains 7.9 million words.
</tableCaption>
<bodyText confidence="0.99976625">
Understanding the dynamics of this process would
benefit not only language teachers, but also the de-
sign of writing assistance tools that provide auto-
matic feedback (Burstein and Chodorow, 2004).
This paper presents the first large-scale corpus
that will enable research in this direction. After a re-
view of previous work (§2), we describe the design
and a preliminary analysis of our corpus (§3).
</bodyText>
<page confidence="0.958749">
248
</page>
<note confidence="0.9027345">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 248–252,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.974001">
Figure 1: On top is a typical draft essay, interleaved with comments from a tutor (§3.2): two-digit codes from the
Comment Bank are enclosed in angled brackets, while open-ended comments are enclosed in angled brackets. On the
bottom is the same essay in TEI format, the output of the process described in §3.3.
</figureCaption>
<sectionHeader confidence="0.945338" genericHeader="method">
2 Previous Research
</sectionHeader>
<bodyText confidence="0.999841">
In this section, we summarize previous research on
feedback in language teaching, and on the nature of
the revision process by language learners.
</bodyText>
<subsectionHeader confidence="0.984531">
2.1 Feedback in Language Learning
</subsectionHeader>
<bodyText confidence="0.999968705882353">
Receiving feedback is a crucial element in language
learning. While most agree that both the form and
content of feedback plays an important role, there
is no consensus on their effects. Regarding form,
some argue that direct feedback (providing correc-
tions) are more effective in improving the quality of
writing than indirect feedback (pointing out an er-
ror but not providing corrections) (Sugita, 2006), but
others reached opposite conclusions (Ferris, 2006;
Lee, 2008).
Regarding content, it has been observed that
teachers spend a disproportionate amount of time
on identifying word-level errors, at the expense of
those at higher levels, such as coherence (Furneaux
et al., 2007; Zamel, 1985). There has been no large-
scale empirical study, however, on the effectiveness
of feedback at the paragraph or discourse levels.
</bodyText>
<subsectionHeader confidence="0.970258">
2.2 Revision Process
</subsectionHeader>
<bodyText confidence="0.999977">
While text editing in general has been ana-
lyzed (Mahlow and Piotrowski, 2008), the nature
of revisions by language learners — for example,
whether learners mostly focus on correcting me-
chanical, word-level errors, or also substantially re-
organize paragraph or essay structures — has hardly
been investigated. One reason for this gap in the
literature is the lack of corpus data: none of the ex-
isting learner corpora (Izumi et al., 2004; Granger
et al., 2009; Nagata et al., 2011; Dahlmeier and Ng,
2011) contains drafts written by non-native speakers
that led to the “final version”. Recently, two cor-
pora with text revision information have been com-
piled (Xue and Hwa, 2010; Mizumoto et al., 2011),
but neither contain feedback from language teach-
ers. Our corpus will allow researchers to not only
examine the revision process, but also investigate
any correlation with the amount and type of feed-
back.
</bodyText>
<sectionHeader confidence="0.983514" genericHeader="method">
3 Corpus Description
</sectionHeader>
<bodyText confidence="0.999847333333333">
We first introduce the context in which our data was
collected (§3.1), then describe the kinds of com-
ments in the drafts (§3.2). We then outline the
conversion process of the corpus into XML format
(§3.3), followed by an evaluation (§3.4) and an anal-
ysis (§3.5).
</bodyText>
<subsectionHeader confidence="0.99667">
3.1 Background
</subsectionHeader>
<bodyText confidence="0.99979775">
Between 2007 and 2010, City University of Hong
Kong hosted a language learning project where
English-language tutors reviewed and provided
feedback on academic essays written by students,
</bodyText>
<page confidence="0.996028">
249
</page>
<table confidence="0.999747857142857">
Paragraph level Sentence level Word level
Coherence: more 680 Conjunction missing 1554 Article missing 10586
elaboration is needed
Paragraph: new paragraph 522 Sentence: new sentence 1389 Delete this 9224
Coherence: sign posting 322 Conjunction: wrong use 923 Noun: countable 7316
Coherence: missing 222 Sentence: fragment 775 Subject-verb 4008
topic sentence agreement
</table>
<tableCaption confidence="0.999855">
Table 2: The most frequent error categories from the Comment Bank, aimed at errors at different levels.
</tableCaption>
<bodyText confidence="0.999886230769231">
most of whom were native speakers of Chi-
nese (Webster et al., 2011). More than 300 TESOL
students served as language tutors, and over 4,200
students from a wide range of disciplines (see Ta-
ble 1) took part in the project.
For each essay, a student posted a first draft1 as
a blog on an e-learning environment called Black-
board Academic Suite; a language tutor then directly
added comments on the blog. Figure 1 shows an ex-
ample of such a draft. The student then revised his or
her draft and may re-post it to receive further com-
ments. Most essays underwent two revision cycles
before the student submitted the final version.
</bodyText>
<subsectionHeader confidence="0.992905">
3.2 Comments
</subsectionHeader>
<bodyText confidence="0.991402176470588">
Comments in the draft can take one of three forms:
Code The tutor may insert a two-digit code, repre-
senting one of the 60 common error categories
in our “Comment Bank”, adopted from the
XWiLL project (Wible et al., 2001). These cat-
egories address issues ranging from the word
level to paragraph level (see Table 2), with
a mix of direct (e.g., “new paragraph”) and
indirect feedback (e.g., “more elaboration is
needed”).
Open-ended comment The tutor may also provide
personally tailored comments.
Hybrid Both a code and an open-ended comment.
For every comment2, the tutor highlights the prob-
lematic words or sentences at which it is aimed.
Sometimes, general comments about the draft as a
whole are also inserted at the beginning or the end.
</bodyText>
<footnote confidence="0.984604333333333">
1In the rest of the paper, these drafts will be referred to “ver-
sion 1”, “version 2”, and so on.
2Except those comments indicating that a word is missing.
</footnote>
<subsectionHeader confidence="0.997851">
3.3 Conversion to XML Format
</subsectionHeader>
<bodyText confidence="0.999731882352941">
The data format for the essays and comments was
not originally conceived for computational analysis.
The drafts, downloaded from the blog entries, are in
HTML format, with comments interspersed in them;
the final versions are Microsoft Word documents.
Our first task, therefore, is to convert them into a
machine-actionable, XML format conforming to the
standards of the Text Encoding Initiative (TEI). This
conversion consists of the following steps:
Comment extraction After repairing irregularities
in the HTML tags, we eliminated attributes that
are irrelevant to comment extraction, such as
font and style. We then identified the Comment
Bank codes and open-ended comments.
Comment-to-text alignment Each comment is
aimed at a particular text segment. The text
segment is usually indicated by highlighting
the relevant words or changing their back-
ground color. After consolidating the tags for
highlighting and colors, our algorithm looks
for the nearest, preceding text segment with a
color different from that of the comment.
Title and metadata extraction From the top of the
essay, our algorithm scans for short lines with
metadata such as the student and tutor IDs,
semester and course codes, and assignment and
version numbers. The first sentence in the es-
say proper is taken to be the title.
Sentence segmentation Off-the-shelf sentence
segmentators tend to be trained on newswire
texts (Reynar and Ratnaparkhi, 1997), which
significantly differ from the noisy text in our
corpus. We found it adequate to use a stop-list,
supplemented with a few regular expressions
</bodyText>
<page confidence="0.992552">
250
</page>
<table confidence="0.999916">
Evaluation Precision Recall
Comment extraction
- code 94.7% 100%
- open-ended 61.8% 78.3%
Comment-to-text alignment 86.0% 85.2%
Sentence segmentation 94.8% 91.3%
</table>
<tableCaption confidence="0.84592725">
Table 3: Evaluation results of the conversion process de-
scribed in §3.3. Precision and recall are calculated on
correct detection of the start and end points of comments
and boundaries.
</tableCaption>
<bodyText confidence="0.998759909090909">
that detect exceptions, such as abbreviations
and digits.
Sentence alignment Sentences in consecutive ver-
sions of an essay are aligned using cosine simi-
larity score. To allow dynamic programming,
alignments are limited to one-to-one, one-to-
two, two-to-one, or two-to-two3. Below a cer-
tain threshold4, a sentence is no longer aligned,
but is rather considered inserted or deleted. The
alignment results are stored in the XCES for-
mat (Ide et al., 2002).
</bodyText>
<subsectionHeader confidence="0.9884">
3.4 Conversion Evaluation
</subsectionHeader>
<bodyText confidence="0.99818705">
To evaluate the performance of the conversion algo-
rithm described in §3.3, we asked a human to manu-
ally construct the TEI XML files for 14 pairs of draft
versions. These gold files are then compared to the
output of our algorithm. The results are shown in
Table 3.
In comment extraction, codes can be reliably
identified. Among the open-ended comments, how-
ever, those at the beginning and end of the drafts
severely affected the precision, since they are of-
ten not quoted in brackets and are therefore indistin-
guishable from the text proper. In comment-to-text
alignment, most errors were caused by inconsistent
or missing highlighting and background colors.
The accuracy of sentence alignment is 89.8%,
measured from the perspective of sentences in Ver-
sion 1. It is sometimes difficult to decide whether a
sentence has simply been edited (and should there-
fore be aligned), or has been deleted with a new sen-
tence inserted in the next draft.
</bodyText>
<footnote confidence="0.9979">
3That is, the order of two sentences is flipped.
4Tuned to 0.5 based on a random subset of sentence pairs.
</footnote>
<subsectionHeader confidence="0.98895">
3.5 Preliminary Analysis
</subsectionHeader>
<bodyText confidence="0.999809666666667">
As shown in Table 4, the tutors were much more
likely to use codes than to provide open-ended com-
ments. Among the codes, they overwhelmingly em-
phasized word-level issues, echoing previous find-
ings (§2.1). Table 2 lists the most frequent codes.
Missing articles, noun number and subject-verb
agreement round out the top errors at the word level,
similar to the trend for Japanese speakers (Lee and
Seneff, 2008). At the sentence level, conjunctions
turn out to be challenging; at the paragraph level,
paragraph organization, sign posting, and topic sen-
tence receive the most comments.
In a first attempt to gauge the utility of the com-
ments, we measured their density across versions.
Among Version 1 drafts, a code appears on aver-
age every 40.8 words, while an open-ended com-
ment appears every 84.7 words. The respective fig-
ures for Version 2 drafts are 65.9 words and 105.0
words. The lowered densities suggest that students
were able to improve the quality of their writing af-
ter receiving feedback.
</bodyText>
<table confidence="0.998597571428571">
Comment Form Frequency
Open-ended 47072
Hybrid 1993
Code 88370
- Paragraph level 3.2%
- Sentence level 6.0%
- Word level 90.8%
</table>
<tableCaption confidence="0.802891333333333">
Table 4: Distribution of the three kinds of comments
(§3.2), with the Comment Bank codes further subdivided
into different levels (See Table 2).
</tableCaption>
<sectionHeader confidence="0.9878" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999987909090909">
We have presented the first large-scale learner cor-
pus which contains not only texts written by non-
native speakers, but also the successive drafts lead-
ing to the final essay, as well as teachers’ comments
on the drafts. The corpus has been converted into an
XML format conforming to TEI standards.
We plan to port the corpus to a platform for text
visualization and search, and release it to the re-
search community. It is expected to support stud-
ies on textual revision of language learners, and the
effects of different types of feedback.
</bodyText>
<page confidence="0.996221">
251
</page>
<sectionHeader confidence="0.999211" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9978556">
We thank Shun-shing Tsang for his assistance with
implementing the conversion and performing the
evaluation. This project was partially funded by a
Strategic Research Grant (#7008065) from City Uni-
versity of Hong Kong.
</bodyText>
<sectionHeader confidence="0.998943" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999774305882353">
Jill Burstein and Martin Chodorow. 2004. Automated
Essay Evaluation: The Criterion online writing ser-
vice. AI Magazine.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical Error Correction with Alternating Structure Opti-
mization. Proc. ACL.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 Pilot Shared Task. Proc. Eu-
ropean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Dana Ferris. 2006. Does Error Feedback Help Student
Writers? New Evidence on the Short- and Long-Term
Effects of Written Error Correction. In Feedback in
Second Language Writing: Contexts and Issues, Ken
Hyland and Fiona Hyland (eds). Cambridge Univer-
sity Press.
Clare Furneaux, Amos Paran, and Beverly Fairfax. 2007.
Teacher Stance as Reflected in Feedback on Student
Writing: An Empirical Study of Secondary School
Teachers in Five Countries. International Review of
Applied Linguistics in Language Teaching 45(1): 69-
94.
Sylviane Granger. 2004. Computer Learner Corpus Re-
search: Current Status and Future Prospect. Language
and Computers 23:123–145.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier, and
Magali Paquot. 2009. International Corpus of Learner
English v2. Presses universitaires de Louvain, Bel-
gium.
Nancy Ide, Patrice Bonhomme, and Laurent Romary.
2000. XCES: An XML-based Encoding Standard for
Linguistic Corpora. Proc. LREC.
Emi Izumi, Kiyotaka Uchimoto, and Hitoshi Isahara.
2004. The NICT JLE Corpus: Exploiting the Lan-
guage Learners’ Speech Database for Research and
Education. International Journal of the Computer, the
Internet and Management 12(2):119–125.
Icy Lee. 2008. Student Reactions to Teacher Feedback
in Two Hong Kong Secondary Classrooms. Journal of
Second Language Writing 17(3):144-164.
John Lee and Stephanie Seneff. 2008. An Analysis of
Grammatical Errors in Nonnative Speech in English.
Proc. IEEE Workshop on Spoken Language Technol-
ogy.
Cerstin Mahlow and Michael Piotrowski. 2008. Linguis-
tic Support for Revising and Editing. Proc. Interna-
tional Conference on Computational Linguistics and
Intelligent Text Processing.
Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata,
and Yuji Matsumoto. 2011. Mining Revision Log of
Language Learning SNS for Automated Japanese Er-
ror Correction of Second Language Learners. Proc.
IJCNLP.
Ryo Nagata, Edward Whittaker, and Vera Sheinman.
2011. Creating a Manually Error-tagged and Shallow-
parsed Learner Corpus. Proc. ACL.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
Maximum Entropy Approach to Identifying Sentence
Boundaries. Proc. 5th Conference on Applied Natural
Language Processing, Washington DC.
Yoshihito Sugita. 2006. The Impact of Teachers’ Com-
ment Types on Students’ Revision. ELT Journal
60(1):34–41.
Hilary Nesi, Gerard Sharpling, and Lisa Ganobcsik-
Williams. 2004. Student Papers Across the Cur-
riculum: Designing and Developing a Corpus of
British Student Writing. Computers and Composition
21(4):439–450.
Frank Tuzi. 2004. The Impact of E-Feedback on the Re-
visions of L2 Writers in an Academic Writing Course.
Computers and Composition 21(2):217-235.
Jonathan Webster, Angela Chan, and John Lee. 2011.
Online Language Learning for Addressing Hong Kong
Tertiary Students’ Needs in Academic Writing. Asia
Pacific World 2(2):44–65.
David Wible, Chin-Hwa Kuo, Feng-Li Chien, Anne Liu,
and Nai-Lung Tsao. 2001. A Web-Based EFL Writ-
ing Environment: Integrating Information for Learn-
ers, Teachers, and Researchers. Computers and Edu-
cation 37(34):297-315.
Huichao Xue and Rebecca Hwa. 2010. Syntax-Driven
Machine Translation as a Model of ESL Revision.
Proc. COLING.
Vivian Zamel. 1985. Responding to Student Writing.
TESOL Quarterly 19(1):79-101.
</reference>
<page confidence="0.99743">
252
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.291628">
<title confidence="0.999702">A Corpus of Textual Revisions in Second Language Writing</title>
<author confidence="0.987862">John Lee</author>
<author confidence="0.987862">Jonathan</author>
<affiliation confidence="0.903751">The Halliday Centre for Intelligent Applications of Language Department of Chinese, Translation and City University of Hong</affiliation>
<abstract confidence="0.9992642">This paper describes the creation of the first large-scale corpus containing drafts and final versions of essays written by non-native speakers, with the sentences aligned across different versions. Furthermore, the sentences in the drafts are annotated with comments from teachers. The corpus is intended to support research on textual revision by language learners, and how it is influenced by feedback.</abstract>
<note confidence="0.478192333333333">This corpus has been converted into an XML format conforming to the standards of the Text Encoding Initiative (TEI).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jill Burstein</author>
<author>Martin Chodorow</author>
</authors>
<title>Automated Essay Evaluation: The Criterion online writing service.</title>
<date>2004</date>
<journal>AI Magazine.</journal>
<contexts>
<context position="2679" citStr="Burstein and Chodorow, 2004" startWordPosition="408" endWordPosition="411">320 Table 1: Draft essays are collected from courses in various disciplines at City University of Hong Kong. These drafts include lab reports, data analysis, argumentative essays, and article summaries. There are 3760 distinct essays, most of which consist of two to four successive drafts. Each draft has on average 44.2 sentences, and the average length of a sentence is 13.3 words. In total, the corpus contains 7.9 million words. Understanding the dynamics of this process would benefit not only language teachers, but also the design of writing assistance tools that provide automatic feedback (Burstein and Chodorow, 2004). This paper presents the first large-scale corpus that will enable research in this direction. After a review of previous work (§2), we describe the design and a preliminary analysis of our corpus (§3). 248 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 248–252, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: On top is a typical draft essay, interleaved with comments from a tutor (§3.2): two-digit codes from the Comment Bank are enclosed in angled brackets, while open-ended comments are enclose</context>
</contexts>
<marker>Burstein, Chodorow, 2004</marker>
<rawString>Jill Burstein and Martin Chodorow. 2004. Automated Essay Evaluation: The Criterion online writing service. AI Magazine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Grammatical Error Correction with Alternating Structure Optimization.</title>
<date>2011</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="1423" citStr="Dahlmeier and Ng, 2011" startWordPosition="212" endWordPosition="215">troduction Learner corpora have been playing an increasingly important role in both Second Language Acquisition and Foreign Language Teaching research (Granger, 2004; Nesi et al., 2004). These corpora contain texts written by non-native speakers of the language (Granger et al., 2009); many also annotate text segments where there are errors, and the corresponding error categories (Nagata et al., 2011). In addition, some learner corpora contain pairs of sentences: a sentence written by a learner of English as a second language (ESL), paired with its correct version produced by a native speaker (Dahlmeier and Ng, 2011). These datasets are intended to support the training of automatic text correction systems (Dale and Kilgarriff, 2011). Less attention has been paid to how a language learner produces a text. Writing is often an iterative and interactive process, with cycles of textual revision, guided by comments from language teachers. Discipline # drafts Applied Physics 988 Asian and International Studies 410 Biology 2310 Building Science and Technology 705 Business 1754 Computer Science 466 Creative Media 118 Electronic Engineering 1532 General Education 651 Law 31 Linguistics 2165 Management Sciences 1278</context>
<context position="4943" citStr="Dahlmeier and Ng, 2011" startWordPosition="768" endWordPosition="771">e empirical study, however, on the effectiveness of feedback at the paragraph or discourse levels. 2.2 Revision Process While text editing in general has been analyzed (Mahlow and Piotrowski, 2008), the nature of revisions by language learners — for example, whether learners mostly focus on correcting mechanical, word-level errors, or also substantially reorganize paragraph or essay structures — has hardly been investigated. One reason for this gap in the literature is the lack of corpus data: none of the existing learner corpora (Izumi et al., 2004; Granger et al., 2009; Nagata et al., 2011; Dahlmeier and Ng, 2011) contains drafts written by non-native speakers that led to the “final version”. Recently, two corpora with text revision information have been compiled (Xue and Hwa, 2010; Mizumoto et al., 2011), but neither contain feedback from language teachers. Our corpus will allow researchers to not only examine the revision process, but also investigate any correlation with the amount and type of feedback. 3 Corpus Description We first introduce the context in which our data was collected (§3.1), then describe the kinds of comments in the drafts (§3.2). We then outline the conversion process of the cor</context>
</contexts>
<marker>Dahlmeier, Ng, 2011</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammatical Error Correction with Alternating Structure Optimization. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Helping Our Own: The HOO 2011 Pilot Shared Task.</title>
<date>2011</date>
<booktitle>Proc. European Workshop on Natural Language Generation (ENLG),</booktitle>
<location>Nancy, France.</location>
<contexts>
<context position="1541" citStr="Dale and Kilgarriff, 2011" startWordPosition="231" endWordPosition="234"> Foreign Language Teaching research (Granger, 2004; Nesi et al., 2004). These corpora contain texts written by non-native speakers of the language (Granger et al., 2009); many also annotate text segments where there are errors, and the corresponding error categories (Nagata et al., 2011). In addition, some learner corpora contain pairs of sentences: a sentence written by a learner of English as a second language (ESL), paired with its correct version produced by a native speaker (Dahlmeier and Ng, 2011). These datasets are intended to support the training of automatic text correction systems (Dale and Kilgarriff, 2011). Less attention has been paid to how a language learner produces a text. Writing is often an iterative and interactive process, with cycles of textual revision, guided by comments from language teachers. Discipline # drafts Applied Physics 988 Asian and International Studies 410 Biology 2310 Building Science and Technology 705 Business 1754 Computer Science 466 Creative Media 118 Electronic Engineering 1532 General Education 651 Law 31 Linguistics 2165 Management Sciences 1278 Social Studies 912 Total 13320 Table 1: Draft essays are collected from courses in various disciplines at City Univer</context>
</contexts>
<marker>Dale, Kilgarriff, 2011</marker>
<rawString>Robert Dale and Adam Kilgarriff. 2011. Helping Our Own: The HOO 2011 Pilot Shared Task. Proc. European Workshop on Natural Language Generation (ENLG), Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dana Ferris</author>
</authors>
<title>Does Error Feedback Help Student Writers? New Evidence on the Short- and Long-Term Effects of Written Error Correction.</title>
<date>2006</date>
<booktitle>In Feedback in Second Language Writing: Contexts and Issues, Ken Hyland and Fiona Hyland (eds).</booktitle>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4053" citStr="Ferris, 2006" startWordPosition="626" endWordPosition="627">e previous research on feedback in language teaching, and on the nature of the revision process by language learners. 2.1 Feedback in Language Learning Receiving feedback is a crucial element in language learning. While most agree that both the form and content of feedback plays an important role, there is no consensus on their effects. Regarding form, some argue that direct feedback (providing corrections) are more effective in improving the quality of writing than indirect feedback (pointing out an error but not providing corrections) (Sugita, 2006), but others reached opposite conclusions (Ferris, 2006; Lee, 2008). Regarding content, it has been observed that teachers spend a disproportionate amount of time on identifying word-level errors, at the expense of those at higher levels, such as coherence (Furneaux et al., 2007; Zamel, 1985). There has been no largescale empirical study, however, on the effectiveness of feedback at the paragraph or discourse levels. 2.2 Revision Process While text editing in general has been analyzed (Mahlow and Piotrowski, 2008), the nature of revisions by language learners — for example, whether learners mostly focus on correcting mechanical, word-level errors,</context>
</contexts>
<marker>Ferris, 2006</marker>
<rawString>Dana Ferris. 2006. Does Error Feedback Help Student Writers? New Evidence on the Short- and Long-Term Effects of Written Error Correction. In Feedback in Second Language Writing: Contexts and Issues, Ken Hyland and Fiona Hyland (eds). Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clare Furneaux</author>
<author>Amos Paran</author>
<author>Beverly Fairfax</author>
</authors>
<title>Teacher Stance as Reflected in Feedback on Student Writing: An Empirical Study of Secondary School Teachers in Five Countries.</title>
<date>2007</date>
<journal>International Review of Applied Linguistics in Language Teaching</journal>
<volume>45</volume>
<issue>1</issue>
<pages>69--94</pages>
<contexts>
<context position="4277" citStr="Furneaux et al., 2007" startWordPosition="659" endWordPosition="662">hile most agree that both the form and content of feedback plays an important role, there is no consensus on their effects. Regarding form, some argue that direct feedback (providing corrections) are more effective in improving the quality of writing than indirect feedback (pointing out an error but not providing corrections) (Sugita, 2006), but others reached opposite conclusions (Ferris, 2006; Lee, 2008). Regarding content, it has been observed that teachers spend a disproportionate amount of time on identifying word-level errors, at the expense of those at higher levels, such as coherence (Furneaux et al., 2007; Zamel, 1985). There has been no largescale empirical study, however, on the effectiveness of feedback at the paragraph or discourse levels. 2.2 Revision Process While text editing in general has been analyzed (Mahlow and Piotrowski, 2008), the nature of revisions by language learners — for example, whether learners mostly focus on correcting mechanical, word-level errors, or also substantially reorganize paragraph or essay structures — has hardly been investigated. One reason for this gap in the literature is the lack of corpus data: none of the existing learner corpora (Izumi et al., 2004; </context>
</contexts>
<marker>Furneaux, Paran, Fairfax, 2007</marker>
<rawString>Clare Furneaux, Amos Paran, and Beverly Fairfax. 2007. Teacher Stance as Reflected in Feedback on Student Writing: An Empirical Study of Secondary School Teachers in Five Countries. International Review of Applied Linguistics in Language Teaching 45(1): 69-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylviane Granger</author>
</authors>
<title>Computer Learner Corpus Research: Current Status and Future Prospect.</title>
<date>2004</date>
<journal>Language and Computers</journal>
<pages>23--123</pages>
<contexts>
<context position="965" citStr="Granger, 2004" startWordPosition="138" endWordPosition="139">afts and final versions of essays written by non-native speakers, with the sentences aligned across different versions. Furthermore, the sentences in the drafts are annotated with comments from teachers. The corpus is intended to support research on textual revision by language learners, and how it is influenced by feedback. This corpus has been converted into an XML format conforming to the standards of the Text Encoding Initiative (TEI). 1 Introduction Learner corpora have been playing an increasingly important role in both Second Language Acquisition and Foreign Language Teaching research (Granger, 2004; Nesi et al., 2004). These corpora contain texts written by non-native speakers of the language (Granger et al., 2009); many also annotate text segments where there are errors, and the corresponding error categories (Nagata et al., 2011). In addition, some learner corpora contain pairs of sentences: a sentence written by a learner of English as a second language (ESL), paired with its correct version produced by a native speaker (Dahlmeier and Ng, 2011). These datasets are intended to support the training of automatic text correction systems (Dale and Kilgarriff, 2011). Less attention has bee</context>
</contexts>
<marker>Granger, 2004</marker>
<rawString>Sylviane Granger. 2004. Computer Learner Corpus Research: Current Status and Future Prospect. Language and Computers 23:123–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylviane Granger</author>
<author>Estelle Dagneaux</author>
<author>Fanny Meunier</author>
<author>Magali Paquot</author>
</authors>
<date>2009</date>
<booktitle>International Corpus of Learner English v2. Presses universitaires de Louvain,</booktitle>
<location>Belgium.</location>
<contexts>
<context position="1084" citStr="Granger et al., 2009" startWordPosition="156" endWordPosition="159">ersions. Furthermore, the sentences in the drafts are annotated with comments from teachers. The corpus is intended to support research on textual revision by language learners, and how it is influenced by feedback. This corpus has been converted into an XML format conforming to the standards of the Text Encoding Initiative (TEI). 1 Introduction Learner corpora have been playing an increasingly important role in both Second Language Acquisition and Foreign Language Teaching research (Granger, 2004; Nesi et al., 2004). These corpora contain texts written by non-native speakers of the language (Granger et al., 2009); many also annotate text segments where there are errors, and the corresponding error categories (Nagata et al., 2011). In addition, some learner corpora contain pairs of sentences: a sentence written by a learner of English as a second language (ESL), paired with its correct version produced by a native speaker (Dahlmeier and Ng, 2011). These datasets are intended to support the training of automatic text correction systems (Dale and Kilgarriff, 2011). Less attention has been paid to how a language learner produces a text. Writing is often an iterative and interactive process, with cycles of</context>
<context position="4897" citStr="Granger et al., 2009" startWordPosition="760" endWordPosition="763">; Zamel, 1985). There has been no largescale empirical study, however, on the effectiveness of feedback at the paragraph or discourse levels. 2.2 Revision Process While text editing in general has been analyzed (Mahlow and Piotrowski, 2008), the nature of revisions by language learners — for example, whether learners mostly focus on correcting mechanical, word-level errors, or also substantially reorganize paragraph or essay structures — has hardly been investigated. One reason for this gap in the literature is the lack of corpus data: none of the existing learner corpora (Izumi et al., 2004; Granger et al., 2009; Nagata et al., 2011; Dahlmeier and Ng, 2011) contains drafts written by non-native speakers that led to the “final version”. Recently, two corpora with text revision information have been compiled (Xue and Hwa, 2010; Mizumoto et al., 2011), but neither contain feedback from language teachers. Our corpus will allow researchers to not only examine the revision process, but also investigate any correlation with the amount and type of feedback. 3 Corpus Description We first introduce the context in which our data was collected (§3.1), then describe the kinds of comments in the drafts (§3.2). We </context>
</contexts>
<marker>Granger, Dagneaux, Meunier, Paquot, 2009</marker>
<rawString>Sylviane Granger, Estelle Dagneaux, Fanny Meunier, and Magali Paquot. 2009. International Corpus of Learner English v2. Presses universitaires de Louvain, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Patrice Bonhomme</author>
<author>Laurent Romary</author>
</authors>
<title>XCES: An XML-based Encoding Standard for Linguistic Corpora.</title>
<date>2000</date>
<booktitle>Proc. LREC.</booktitle>
<marker>Ide, Bonhomme, Romary, 2000</marker>
<rawString>Nancy Ide, Patrice Bonhomme, and Laurent Romary. 2000. XCES: An XML-based Encoding Standard for Linguistic Corpora. Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emi Izumi</author>
<author>Kiyotaka Uchimoto</author>
<author>Hitoshi Isahara</author>
</authors>
<title>The NICT JLE Corpus: Exploiting the Language Learners’ Speech Database for Research and Education.</title>
<date>2004</date>
<journal>International Journal of the Computer, the Internet and Management</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="4875" citStr="Izumi et al., 2004" startWordPosition="756" endWordPosition="759">urneaux et al., 2007; Zamel, 1985). There has been no largescale empirical study, however, on the effectiveness of feedback at the paragraph or discourse levels. 2.2 Revision Process While text editing in general has been analyzed (Mahlow and Piotrowski, 2008), the nature of revisions by language learners — for example, whether learners mostly focus on correcting mechanical, word-level errors, or also substantially reorganize paragraph or essay structures — has hardly been investigated. One reason for this gap in the literature is the lack of corpus data: none of the existing learner corpora (Izumi et al., 2004; Granger et al., 2009; Nagata et al., 2011; Dahlmeier and Ng, 2011) contains drafts written by non-native speakers that led to the “final version”. Recently, two corpora with text revision information have been compiled (Xue and Hwa, 2010; Mizumoto et al., 2011), but neither contain feedback from language teachers. Our corpus will allow researchers to not only examine the revision process, but also investigate any correlation with the amount and type of feedback. 3 Corpus Description We first introduce the context in which our data was collected (§3.1), then describe the kinds of comments in </context>
</contexts>
<marker>Izumi, Uchimoto, Isahara, 2004</marker>
<rawString>Emi Izumi, Kiyotaka Uchimoto, and Hitoshi Isahara. 2004. The NICT JLE Corpus: Exploiting the Language Learners’ Speech Database for Research and Education. International Journal of the Computer, the Internet and Management 12(2):119–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Icy Lee</author>
</authors>
<title>Student Reactions to Teacher Feedback in Two Hong Kong Secondary Classrooms.</title>
<date>2008</date>
<journal>Journal of Second Language Writing</journal>
<pages>17--3</pages>
<contexts>
<context position="4065" citStr="Lee, 2008" startWordPosition="628" endWordPosition="629">earch on feedback in language teaching, and on the nature of the revision process by language learners. 2.1 Feedback in Language Learning Receiving feedback is a crucial element in language learning. While most agree that both the form and content of feedback plays an important role, there is no consensus on their effects. Regarding form, some argue that direct feedback (providing corrections) are more effective in improving the quality of writing than indirect feedback (pointing out an error but not providing corrections) (Sugita, 2006), but others reached opposite conclusions (Ferris, 2006; Lee, 2008). Regarding content, it has been observed that teachers spend a disproportionate amount of time on identifying word-level errors, at the expense of those at higher levels, such as coherence (Furneaux et al., 2007; Zamel, 1985). There has been no largescale empirical study, however, on the effectiveness of feedback at the paragraph or discourse levels. 2.2 Revision Process While text editing in general has been analyzed (Mahlow and Piotrowski, 2008), the nature of revisions by language learners — for example, whether learners mostly focus on correcting mechanical, word-level errors, or also sub</context>
</contexts>
<marker>Lee, 2008</marker>
<rawString>Icy Lee. 2008. Student Reactions to Teacher Feedback in Two Hong Kong Secondary Classrooms. Journal of Second Language Writing 17(3):144-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Stephanie Seneff</author>
</authors>
<title>An Analysis of Grammatical Errors in Nonnative Speech in English.</title>
<date>2008</date>
<booktitle>Proc. IEEE Workshop on Spoken Language Technology.</booktitle>
<contexts>
<context position="11765" citStr="Lee and Seneff, 2008" startWordPosition="1872" endWordPosition="1875"> aligned), or has been deleted with a new sentence inserted in the next draft. 3That is, the order of two sentences is flipped. 4Tuned to 0.5 based on a random subset of sentence pairs. 3.5 Preliminary Analysis As shown in Table 4, the tutors were much more likely to use codes than to provide open-ended comments. Among the codes, they overwhelmingly emphasized word-level issues, echoing previous findings (§2.1). Table 2 lists the most frequent codes. Missing articles, noun number and subject-verb agreement round out the top errors at the word level, similar to the trend for Japanese speakers (Lee and Seneff, 2008). At the sentence level, conjunctions turn out to be challenging; at the paragraph level, paragraph organization, sign posting, and topic sentence receive the most comments. In a first attempt to gauge the utility of the comments, we measured their density across versions. Among Version 1 drafts, a code appears on average every 40.8 words, while an open-ended comment appears every 84.7 words. The respective figures for Version 2 drafts are 65.9 words and 105.0 words. The lowered densities suggest that students were able to improve the quality of their writing after receiving feedback. Comment </context>
</contexts>
<marker>Lee, Seneff, 2008</marker>
<rawString>John Lee and Stephanie Seneff. 2008. An Analysis of Grammatical Errors in Nonnative Speech in English. Proc. IEEE Workshop on Spoken Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cerstin Mahlow</author>
<author>Michael Piotrowski</author>
</authors>
<title>Linguistic Support for Revising and Editing.</title>
<date>2008</date>
<booktitle>Proc. International Conference on Computational Linguistics and Intelligent Text Processing.</booktitle>
<contexts>
<context position="4517" citStr="Mahlow and Piotrowski, 2008" startWordPosition="697" endWordPosition="700">uality of writing than indirect feedback (pointing out an error but not providing corrections) (Sugita, 2006), but others reached opposite conclusions (Ferris, 2006; Lee, 2008). Regarding content, it has been observed that teachers spend a disproportionate amount of time on identifying word-level errors, at the expense of those at higher levels, such as coherence (Furneaux et al., 2007; Zamel, 1985). There has been no largescale empirical study, however, on the effectiveness of feedback at the paragraph or discourse levels. 2.2 Revision Process While text editing in general has been analyzed (Mahlow and Piotrowski, 2008), the nature of revisions by language learners — for example, whether learners mostly focus on correcting mechanical, word-level errors, or also substantially reorganize paragraph or essay structures — has hardly been investigated. One reason for this gap in the literature is the lack of corpus data: none of the existing learner corpora (Izumi et al., 2004; Granger et al., 2009; Nagata et al., 2011; Dahlmeier and Ng, 2011) contains drafts written by non-native speakers that led to the “final version”. Recently, two corpora with text revision information have been compiled (Xue and Hwa, 2010; M</context>
</contexts>
<marker>Mahlow, Piotrowski, 2008</marker>
<rawString>Cerstin Mahlow and Michael Piotrowski. 2008. Linguistic Support for Revising and Editing. Proc. International Conference on Computational Linguistics and Intelligent Text Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoya Mizumoto</author>
<author>Mamoru Komachi</author>
<author>Masaaki Nagata</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Mining Revision Log of Language Learning SNS for Automated Japanese Error Correction of Second Language Learners.</title>
<date>2011</date>
<booktitle>Proc. IJCNLP.</booktitle>
<contexts>
<context position="5138" citStr="Mizumoto et al., 2011" startWordPosition="800" endWordPosition="803">), the nature of revisions by language learners — for example, whether learners mostly focus on correcting mechanical, word-level errors, or also substantially reorganize paragraph or essay structures — has hardly been investigated. One reason for this gap in the literature is the lack of corpus data: none of the existing learner corpora (Izumi et al., 2004; Granger et al., 2009; Nagata et al., 2011; Dahlmeier and Ng, 2011) contains drafts written by non-native speakers that led to the “final version”. Recently, two corpora with text revision information have been compiled (Xue and Hwa, 2010; Mizumoto et al., 2011), but neither contain feedback from language teachers. Our corpus will allow researchers to not only examine the revision process, but also investigate any correlation with the amount and type of feedback. 3 Corpus Description We first introduce the context in which our data was collected (§3.1), then describe the kinds of comments in the drafts (§3.2). We then outline the conversion process of the corpus into XML format (§3.3), followed by an evaluation (§3.4) and an analysis (§3.5). 3.1 Background Between 2007 and 2010, City University of Hong Kong hosted a language learning project where En</context>
</contexts>
<marker>Mizumoto, Komachi, Nagata, Matsumoto, 2011</marker>
<rawString>Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata, and Yuji Matsumoto. 2011. Mining Revision Log of Language Learning SNS for Automated Japanese Error Correction of Second Language Learners. Proc. IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryo Nagata</author>
<author>Edward Whittaker</author>
<author>Vera Sheinman</author>
</authors>
<title>Creating a Manually Error-tagged and Shallowparsed Learner Corpus.</title>
<date>2011</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="1203" citStr="Nagata et al., 2011" startWordPosition="175" endWordPosition="178">upport research on textual revision by language learners, and how it is influenced by feedback. This corpus has been converted into an XML format conforming to the standards of the Text Encoding Initiative (TEI). 1 Introduction Learner corpora have been playing an increasingly important role in both Second Language Acquisition and Foreign Language Teaching research (Granger, 2004; Nesi et al., 2004). These corpora contain texts written by non-native speakers of the language (Granger et al., 2009); many also annotate text segments where there are errors, and the corresponding error categories (Nagata et al., 2011). In addition, some learner corpora contain pairs of sentences: a sentence written by a learner of English as a second language (ESL), paired with its correct version produced by a native speaker (Dahlmeier and Ng, 2011). These datasets are intended to support the training of automatic text correction systems (Dale and Kilgarriff, 2011). Less attention has been paid to how a language learner produces a text. Writing is often an iterative and interactive process, with cycles of textual revision, guided by comments from language teachers. Discipline # drafts Applied Physics 988 Asian and Interna</context>
<context position="4918" citStr="Nagata et al., 2011" startWordPosition="764" endWordPosition="767">has been no largescale empirical study, however, on the effectiveness of feedback at the paragraph or discourse levels. 2.2 Revision Process While text editing in general has been analyzed (Mahlow and Piotrowski, 2008), the nature of revisions by language learners — for example, whether learners mostly focus on correcting mechanical, word-level errors, or also substantially reorganize paragraph or essay structures — has hardly been investigated. One reason for this gap in the literature is the lack of corpus data: none of the existing learner corpora (Izumi et al., 2004; Granger et al., 2009; Nagata et al., 2011; Dahlmeier and Ng, 2011) contains drafts written by non-native speakers that led to the “final version”. Recently, two corpora with text revision information have been compiled (Xue and Hwa, 2010; Mizumoto et al., 2011), but neither contain feedback from language teachers. Our corpus will allow researchers to not only examine the revision process, but also investigate any correlation with the amount and type of feedback. 3 Corpus Description We first introduce the context in which our data was collected (§3.1), then describe the kinds of comments in the drafts (§3.2). We then outline the conv</context>
</contexts>
<marker>Nagata, Whittaker, Sheinman, 2011</marker>
<rawString>Ryo Nagata, Edward Whittaker, and Vera Sheinman. 2011. Creating a Manually Error-tagged and Shallowparsed Learner Corpus. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Approach to Identifying Sentence Boundaries.</title>
<date>1997</date>
<booktitle>Proc. 5th Conference on Applied Natural Language Processing,</booktitle>
<location>Washington DC.</location>
<contexts>
<context position="9302" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="1472" endWordPosition="1475">ighlighting the relevant words or changing their background color. After consolidating the tags for highlighting and colors, our algorithm looks for the nearest, preceding text segment with a color different from that of the comment. Title and metadata extraction From the top of the essay, our algorithm scans for short lines with metadata such as the student and tutor IDs, semester and course codes, and assignment and version numbers. The first sentence in the essay proper is taken to be the title. Sentence segmentation Off-the-shelf sentence segmentators tend to be trained on newswire texts (Reynar and Ratnaparkhi, 1997), which significantly differ from the noisy text in our corpus. We found it adequate to use a stop-list, supplemented with a few regular expressions 250 Evaluation Precision Recall Comment extraction - code 94.7% 100% - open-ended 61.8% 78.3% Comment-to-text alignment 86.0% 85.2% Sentence segmentation 94.8% 91.3% Table 3: Evaluation results of the conversion process described in §3.3. Precision and recall are calculated on correct detection of the start and end points of comments and boundaries. that detect exceptions, such as abbreviations and digits. Sentence alignment Sentences in consecuti</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A Maximum Entropy Approach to Identifying Sentence Boundaries. Proc. 5th Conference on Applied Natural Language Processing, Washington DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshihito Sugita</author>
</authors>
<date>2006</date>
<journal>The Impact of Teachers’ Comment Types on Students’ Revision. ELT Journal</journal>
<volume>60</volume>
<issue>1</issue>
<contexts>
<context position="3998" citStr="Sugita, 2006" startWordPosition="619" endWordPosition="620">n §3.3. 2 Previous Research In this section, we summarize previous research on feedback in language teaching, and on the nature of the revision process by language learners. 2.1 Feedback in Language Learning Receiving feedback is a crucial element in language learning. While most agree that both the form and content of feedback plays an important role, there is no consensus on their effects. Regarding form, some argue that direct feedback (providing corrections) are more effective in improving the quality of writing than indirect feedback (pointing out an error but not providing corrections) (Sugita, 2006), but others reached opposite conclusions (Ferris, 2006; Lee, 2008). Regarding content, it has been observed that teachers spend a disproportionate amount of time on identifying word-level errors, at the expense of those at higher levels, such as coherence (Furneaux et al., 2007; Zamel, 1985). There has been no largescale empirical study, however, on the effectiveness of feedback at the paragraph or discourse levels. 2.2 Revision Process While text editing in general has been analyzed (Mahlow and Piotrowski, 2008), the nature of revisions by language learners — for example, whether learners mo</context>
</contexts>
<marker>Sugita, 2006</marker>
<rawString>Yoshihito Sugita. 2006. The Impact of Teachers’ Comment Types on Students’ Revision. ELT Journal 60(1):34–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hilary Nesi</author>
<author>Gerard Sharpling</author>
<author>Lisa GanobcsikWilliams</author>
</authors>
<title>Student Papers Across the Curriculum: Designing and Developing a</title>
<date>2004</date>
<journal>Corpus of British Student Writing. Computers and Composition</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="985" citStr="Nesi et al., 2004" startWordPosition="140" endWordPosition="143">versions of essays written by non-native speakers, with the sentences aligned across different versions. Furthermore, the sentences in the drafts are annotated with comments from teachers. The corpus is intended to support research on textual revision by language learners, and how it is influenced by feedback. This corpus has been converted into an XML format conforming to the standards of the Text Encoding Initiative (TEI). 1 Introduction Learner corpora have been playing an increasingly important role in both Second Language Acquisition and Foreign Language Teaching research (Granger, 2004; Nesi et al., 2004). These corpora contain texts written by non-native speakers of the language (Granger et al., 2009); many also annotate text segments where there are errors, and the corresponding error categories (Nagata et al., 2011). In addition, some learner corpora contain pairs of sentences: a sentence written by a learner of English as a second language (ESL), paired with its correct version produced by a native speaker (Dahlmeier and Ng, 2011). These datasets are intended to support the training of automatic text correction systems (Dale and Kilgarriff, 2011). Less attention has been paid to how a lang</context>
</contexts>
<marker>Nesi, Sharpling, GanobcsikWilliams, 2004</marker>
<rawString>Hilary Nesi, Gerard Sharpling, and Lisa GanobcsikWilliams. 2004. Student Papers Across the Curriculum: Designing and Developing a Corpus of British Student Writing. Computers and Composition 21(4):439–450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Tuzi</author>
</authors>
<title>The Impact of E-Feedback on the Revisions of L2 Writers in an Academic Writing Course.</title>
<date>2004</date>
<journal>Computers and Composition</journal>
<pages>21--2</pages>
<marker>Tuzi, 2004</marker>
<rawString>Frank Tuzi. 2004. The Impact of E-Feedback on the Revisions of L2 Writers in an Academic Writing Course. Computers and Composition 21(2):217-235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Webster</author>
<author>Angela Chan</author>
<author>John Lee</author>
</authors>
<title>Online Language Learning for Addressing Hong Kong Tertiary Students’ Needs in Academic Writing. Asia Pacific World 2(2):44–65.</title>
<date>2011</date>
<contexts>
<context position="6376" citStr="Webster et al., 2011" startWordPosition="993" endWordPosition="996">tors reviewed and provided feedback on academic essays written by students, 249 Paragraph level Sentence level Word level Coherence: more 680 Conjunction missing 1554 Article missing 10586 elaboration is needed Paragraph: new paragraph 522 Sentence: new sentence 1389 Delete this 9224 Coherence: sign posting 322 Conjunction: wrong use 923 Noun: countable 7316 Coherence: missing 222 Sentence: fragment 775 Subject-verb 4008 topic sentence agreement Table 2: The most frequent error categories from the Comment Bank, aimed at errors at different levels. most of whom were native speakers of Chinese (Webster et al., 2011). More than 300 TESOL students served as language tutors, and over 4,200 students from a wide range of disciplines (see Table 1) took part in the project. For each essay, a student posted a first draft1 as a blog on an e-learning environment called Blackboard Academic Suite; a language tutor then directly added comments on the blog. Figure 1 shows an example of such a draft. The student then revised his or her draft and may re-post it to receive further comments. Most essays underwent two revision cycles before the student submitted the final version. 3.2 Comments Comments in the draft can tak</context>
</contexts>
<marker>Webster, Chan, Lee, 2011</marker>
<rawString>Jonathan Webster, Angela Chan, and John Lee. 2011. Online Language Learning for Addressing Hong Kong Tertiary Students’ Needs in Academic Writing. Asia Pacific World 2(2):44–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Wible</author>
<author>Chin-Hwa Kuo</author>
<author>Feng-Li Chien</author>
<author>Anne Liu</author>
<author>Nai-Lung Tsao</author>
</authors>
<title>A Web-Based EFL Writing Environment: Integrating Information for Learners, Teachers, and Researchers.</title>
<date>2001</date>
<journal>Computers and Education</journal>
<pages>37--34</pages>
<contexts>
<context position="7167" citStr="Wible et al., 2001" startWordPosition="1134" endWordPosition="1137"> student posted a first draft1 as a blog on an e-learning environment called Blackboard Academic Suite; a language tutor then directly added comments on the blog. Figure 1 shows an example of such a draft. The student then revised his or her draft and may re-post it to receive further comments. Most essays underwent two revision cycles before the student submitted the final version. 3.2 Comments Comments in the draft can take one of three forms: Code The tutor may insert a two-digit code, representing one of the 60 common error categories in our “Comment Bank”, adopted from the XWiLL project (Wible et al., 2001). These categories address issues ranging from the word level to paragraph level (see Table 2), with a mix of direct (e.g., “new paragraph”) and indirect feedback (e.g., “more elaboration is needed”). Open-ended comment The tutor may also provide personally tailored comments. Hybrid Both a code and an open-ended comment. For every comment2, the tutor highlights the problematic words or sentences at which it is aimed. Sometimes, general comments about the draft as a whole are also inserted at the beginning or the end. 1In the rest of the paper, these drafts will be referred to “version 1”, “ver</context>
</contexts>
<marker>Wible, Kuo, Chien, Liu, Tsao, 2001</marker>
<rawString>David Wible, Chin-Hwa Kuo, Feng-Li Chien, Anne Liu, and Nai-Lung Tsao. 2001. A Web-Based EFL Writing Environment: Integrating Information for Learners, Teachers, and Researchers. Computers and Education 37(34):297-315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huichao Xue</author>
<author>Rebecca Hwa</author>
</authors>
<title>Syntax-Driven Machine Translation as a Model of ESL Revision.</title>
<date>2010</date>
<booktitle>Proc. COLING.</booktitle>
<contexts>
<context position="5114" citStr="Xue and Hwa, 2010" startWordPosition="796" endWordPosition="799">nd Piotrowski, 2008), the nature of revisions by language learners — for example, whether learners mostly focus on correcting mechanical, word-level errors, or also substantially reorganize paragraph or essay structures — has hardly been investigated. One reason for this gap in the literature is the lack of corpus data: none of the existing learner corpora (Izumi et al., 2004; Granger et al., 2009; Nagata et al., 2011; Dahlmeier and Ng, 2011) contains drafts written by non-native speakers that led to the “final version”. Recently, two corpora with text revision information have been compiled (Xue and Hwa, 2010; Mizumoto et al., 2011), but neither contain feedback from language teachers. Our corpus will allow researchers to not only examine the revision process, but also investigate any correlation with the amount and type of feedback. 3 Corpus Description We first introduce the context in which our data was collected (§3.1), then describe the kinds of comments in the drafts (§3.2). We then outline the conversion process of the corpus into XML format (§3.3), followed by an evaluation (§3.4) and an analysis (§3.5). 3.1 Background Between 2007 and 2010, City University of Hong Kong hosted a language l</context>
</contexts>
<marker>Xue, Hwa, 2010</marker>
<rawString>Huichao Xue and Rebecca Hwa. 2010. Syntax-Driven Machine Translation as a Model of ESL Revision. Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivian Zamel</author>
</authors>
<title>Responding to Student Writing.</title>
<date>1985</date>
<journal>TESOL Quarterly</journal>
<pages>19--1</pages>
<contexts>
<context position="4291" citStr="Zamel, 1985" startWordPosition="663" endWordPosition="664">th the form and content of feedback plays an important role, there is no consensus on their effects. Regarding form, some argue that direct feedback (providing corrections) are more effective in improving the quality of writing than indirect feedback (pointing out an error but not providing corrections) (Sugita, 2006), but others reached opposite conclusions (Ferris, 2006; Lee, 2008). Regarding content, it has been observed that teachers spend a disproportionate amount of time on identifying word-level errors, at the expense of those at higher levels, such as coherence (Furneaux et al., 2007; Zamel, 1985). There has been no largescale empirical study, however, on the effectiveness of feedback at the paragraph or discourse levels. 2.2 Revision Process While text editing in general has been analyzed (Mahlow and Piotrowski, 2008), the nature of revisions by language learners — for example, whether learners mostly focus on correcting mechanical, word-level errors, or also substantially reorganize paragraph or essay structures — has hardly been investigated. One reason for this gap in the literature is the lack of corpus data: none of the existing learner corpora (Izumi et al., 2004; Granger et al.</context>
</contexts>
<marker>Zamel, 1985</marker>
<rawString>Vivian Zamel. 1985. Responding to Student Writing. TESOL Quarterly 19(1):79-101.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>