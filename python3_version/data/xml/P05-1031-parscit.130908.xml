<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000473">
<title confidence="0.9981725">
Towards Finding and Fixing Fragments: Using ML to Identify
Non-Sentential Utterances and their Antecedents in Multi-Party Dialogue
</title>
<author confidence="0.997654">
David Schlangen
</author>
<affiliation confidence="0.9973355">
Department of Linguistics
University of Potsdam
</affiliation>
<address confidence="0.9589785">
P.O. Box 601553
D-14415 Potsdam — Germany
</address>
<email confidence="0.996444">
das@ling.uni-potsdam.de
</email>
<sectionHeader confidence="0.995586" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999879444444444">
Non-sentential utterances (e.g., short-
answers as in “Who came to the party?”—
“Peter.”) are pervasive in dialogue. As
with other forms of ellipsis, the elided ma-
terial is typically present in the context
(e.g., the question that a short answer an-
swers). We present a machine learning
approach to the novel task of identifying
fragments and their antecedents in multi-
party dialogue. We compare the perfor-
mance of several learning algorithms, us-
ing a mixture of structural and lexical fea-
tures, and show that the task of identifying
antecedents given a fragment can be learnt
successfully (f(0.5) _ .76); we discuss
why the task of identifying fragments is
harder (f(0.5) _ .41) and finally report
on a combined task (f(0.5) _ .38).
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997184">
Non-sentential utterances (NSUs) as in (1) are per-
vasive in dialogue: recent studies put the proportion
of such utterances at around 10% across different
types of dialogue (Fern´andez and Ginzburg, 2002;
Schlangen and Lascarides, 2003).
</bodyText>
<listItem confidence="0.942699857142857">
(1) a. A: Who came to the party?
B: Peter. (= Peter came to the party.)
b. A: I talked to Peter.
B: Peter Miller? (= Was it Peter Miller
you talked to?)
c. A: Who was this? Peter Miller? (= Was
this Peter Miller?
</listItem>
<bodyText confidence="0.953248387096774">
Such utterances pose an obvious problem for natural
language processing applications, namely that the
intended information (in (1-a)-B a proposition) has
to be recovered from the uttered information (here,
an NP meaning) with the help of information from
the context.
While some systems that automatically resolve
such fragments have recently been developed
(Schlangen and Lascarides, 2002; Fern´andez et al.,
2004a), they have the drawback that they require
“deep” linguistic processing (full parses, and also in-
formation about discourse structure) and hence are
not very robust. We have defined a well-defined
subtask of this problem, namely identifying frag-
ments (certain kinds of NSUs, see below) and their
antecedents (in multi-party dialogue, in our case),
and present a novel machine learning approach to it,
which we hypothesise will be useful for tasks such
as automatic meeting summarisation.1
The remainder of this paper is structured as fol-
lows. In the next section we further specify the task
and different possible approaches to it. We then de-
scribe the corpus we used, some of its characteris-
tics with respect to fragments, and the features we
extracted from it for machine learning. Section 4
describes our experimental settings and reports the
results. After a comparison to related work in Sec-
tion 5, we close with a conclusion and some further
1(Zechner and Lavie, 2001) describe a related task, linking
questions and answers, and evaluate its usefulness in the context
of automatic summarisation; see Section 5.
</bodyText>
<page confidence="0.952461">
247
</page>
<note confidence="0.822301333333333">
Proceedings of the 43rd Annual Meeting of the ACL, pages 247–254,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
work that is planned.
</note>
<sectionHeader confidence="0.858892" genericHeader="method">
2 The Tasks
</sectionHeader>
<bodyText confidence="0.999630846153846">
As we said in the introduction, the main task we
want to tackle is to align (certain kinds of) NSUs
and their antecedents. Now, what characterises this
kind of NSU, and what are their antecedents?
In the examples from the introduction, the NSUs
can be resolved simply by looking at the previous
utterance, which provides the material that is elided
in them. In reality, however, the situation is not that
simple, for three reasons: First, it is of course not
always the previous utterance that provides this ma-
terial (as illustrated by (2), where utterance 7 is re-
solved by utterance 1); in our data the average dis-
tance in fact is 2.5 utterances (see below).
</bodyText>
<listItem confidence="0.9636513">
(2) 1 B: [... ] What else should be done ?
2 C: More intelligence.
3 More good intelligence.
4 Right.
5 D: Intelligent intelligence.
6 B: Better application of face and voice
recognition .
7 C: More [... ] intermingling of the
agencies, you know.
[ from NSI 20011115 ]
</listItem>
<bodyText confidence="0.999461230769231">
Second, it’s not even necessarily a single utter-
ance that does this–it might very well be a span
of utterances, or something that has to be inferred
from such spans (parallel to the situation with pro-
nouns, as discussed empirically e.g. in (Strube and
M¨uller, 2003)). (3) shows an example where a new
topic is broached by using an NSU. It is possible to
analyse this as an answer to the question under dis-
cussion “what shall we organise for the party?”, as
(Fern´andez et al., 2004a) would do; a question, how-
ever, which is only implicitly posed by the previous
discourse, and hence this is an example of an NSU
that does not have an overt antecedent.
</bodyText>
<equation confidence="0.73872325">
(3) [after discussing a number of different topics]
1 D: So, equipment.
2 I can bring [...]
[ from NSI 20011211 ]
</equation>
<bodyText confidence="0.999483783783784">
Lastly, not all NSUs should be analysed as being the
result of ellipsis: backchannels for example (like the
“Right” in utterance 4 in (2) above) seem to directly
fulfil their discourse function without any need for
reconstruction.2
To keep matters simple, we concentrate in this pa-
per on NSUs of a certain kind, namely those that a)
do not predominantly have a discourse-management
function (like for example backchannels), but rather
convey messages (i.e., propositions, questions or
requests)—this is what distinguishes fragments from
other NSUs—and b) have individual utterances as
antecedents. In the terminology of (Schlangen and
Lascarides, 2003), fragments of the latter type are
resolution-via-identity-fragments, where the elided
information can be identified in the context and
need not be inferred (as opposed to resolution-via-
inference-fragments). Choosing only this special
kind of NSUs poses the question whether this sub-
group is distinguished from the general group of
fragments by criteria that can be learnt; we will re-
turn to this below when we analyse the errors made
by the classifier.
We have defined two approaches to this task. One
is to split the task into two sub-tasks: identifying
fragments in a corpus, and identifying antecedents
for fragments. These steps are naturally performed
sequentially to handle our main task, but they also
allow the fragment classification decision to come
from another source—a language-model used in an
automatic speech recognition system, for example—
and to use only the antecedent-classifier. The other
approach is to do both at the same time, i.e. to clas-
sify pairs of utterances into those that combine a
fragment and its antecedent and those that don’t. We
report the results of our experiments with these tasks
below, after describing the data we used.
</bodyText>
<sectionHeader confidence="0.706853" genericHeader="method">
3 Corpus, Features, and Data Creation
</sectionHeader>
<subsectionHeader confidence="0.996937">
3.1 Corpus
</subsectionHeader>
<bodyText confidence="0.9893">
As material we have used six transcripts from the
“NIST Meeting Room Pilot Corpus” (Garofolo et al.,
2004), a corpus of recordings and transcriptions of
multi-party meetings.3 Those six transcripts con-
</bodyText>
<footnote confidence="0.996647555555556">
2The boundaries are fuzzy here, however, as backchan-
nels can also be fragmental repetitions of previous material,
and sometimes it is not clear how to classify a given utter-
ance. A similar problem of classifying fragments is discussed
in (Schlangen, 2003) and we will not go further into this here.
3We have chosen a multi-party setting because we are ulti-
mately interested in automatic summarisation of meetings. In
this paper here, however, we view our task as a “stand-alone
task”. Some of the problems resulting in the presence of many
</footnote>
<page confidence="0.986572">
248
</page>
<table confidence="0.9610247">
average distance α – ,Q
(utterances): 2.5
α declarative 159 (52%)
α interrogative 140 (46%)
α unclassfd. 8 (2%)
,Q declarative 235 (76%)
,Q interrogative (23%)
,Q unclassfd. 2 (0.7%)
α being last in their turn 142 (46%)
,Q being first in their turn 159 (52%)
</table>
<tableCaption confidence="0.918087">
Table 1: Some distributional characteristics. (α de-
notes antecedent, β fragment.)
</tableCaption>
<bodyText confidence="0.999929272727273">
sist of 5,999 utterances, among which we identified
307 fragment–antecedent pairs.4,5 With 5.1% this is
a lower rate than that reported for NSUs in other cor-
pora (see above); but note that as explained above,
we are actually only looking at a sub-class of all
NSUs here.
For these pairs we also annotated some more at-
tributes, which are summarised in Table 1. Note
that the average distance is slightly higher than that
reported in (Schlangen and Lascarides, 2003) for
(2-party) dialogue (1.8); this is presumably due to
the presence of more speakers who are able to re-
ply to an utterance. Finally, we automatically an-
notated all utterances with part-of-speech tags, us-
ing TreeTagger (Schmid, 1994), which we’ve
trained on the switchboard corpus of spoken lan-
guage (Godfrey et al., 1992), because it contains,
just like our corpus, speech disfluencies.6
We now describe the creation of the data we used
for training. We first describe the data-sets for the
different tasks, and then the features used to repre-
sent the events that are to be classified.
</bodyText>
<subsectionHeader confidence="0.999699">
3.2 Data Sets
</subsectionHeader>
<bodyText confidence="0.91690725">
Data creation for the fragment-identification task
(henceforth simply fragment-task) was straightfor-
speakers are discussed below.
4We have used the MMAX tool (M¨uller and Strube, 2001))
for the annotation.
5To test the reliability of the annotation scheme, we had a
subset of the data annotated by two annotators and found a sat-
isfactory r,-agreement (Carletta, 1996) of r, = 0.81.
</bodyText>
<footnote confidence="0.922023">
6The tagger is available free for academic research from
http://www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger/DecisionTreeTagger.html.
</footnote>
<bodyText confidence="0.999971375">
ward: for each utterance, a number of features was
derived automatically (see next section) and the cor-
rect class (fragment / other) was added. (Note
that none of the manually annotated attributes were
used.) This resulted in a file with 5,999 data points
for classification. Given that there were 307 frag-
ments, this means that in this data-set there is a ratio
positives (fragments) vs. negatives (non-fragments)
for the classifier of 1:20. To address this imbalance,
we also ran the experiments with balanced data-sets
with a ratio of 1:5.
The other tasks, antecedent-identification
(antecedent-task) and antecedent-fragment-
identification (combined-task) required the creation
of data-sets containing pairs. For this we created
an “accessibility window” going back from each
utterance. Specifically, we included for each
utterance a) all previous utterances of the same
speaker from the same turn; and b) the three last
utterances of every speaker, but only until one
speaker took the turn again and up to a maximum
of 6 previous utterances. To illustrate this method,
given example (2) it would form pairs with utterance
7 as fragment-candidate and all of utterances 6–2,
but not 1, because that violates condition b) (it is the
second turn of speaker B).
In the case of (2), this exclusion would be a wrong
decision, since 1 is in fact the antecedent for 7. In
general, however, this dynamic method proved good
at capturing as many antecedents as possible while
keeping the number of data points manageable. It
captured 269 antecedent-fragment pairs, which had
an average distance of 1.84 utterances. The remain-
ing 38 pairs which it missed had an average distance
of 7.27 utterances, which means that to capture those
we would have had to widen the window consid-
erably. E.g., considering all previous 8 utterances
would capture an additional 25 pairs, but at the cost
of doubling the number of data points. We hence
chose the approach described here, being aware of
the introduction of a certain bias.
As we have said, we are trying to link utterances,
one a fragment, the other its antecedent. The no-
tion of utterance is however less well-defined than
one might expect, and the segmentation of contin-
uous speech into utterances is a veritable research
problem on its own (see e.g. (Traum and Heeman,
1997)). Often it is arguable whether a prepositional
</bodyText>
<page confidence="0.996559">
249
</page>
<table confidence="0.942532045454545">
Structuralfeatures
dis distance α – β, in utterances
sspk same speaker yes/no
nspk number speaker changes (= # turns)
iqu number of intervening questions
alt α last utterance in its turn?
bft β first utterance in its turn?
Lexical / Utterance-based features
bvb (tensed) verb present in β?
bds disfluency present in β?
aqm α contains question mark
awh α contains wh word
bpr ratio of polar particles (yes, no, maybe, etc..)
/ other in β
apr ratio of polar particles in α
lal length of α
lbe length of β
nra ratio nouns / non-nouns in α
nra ratio nouns / non-nouns in β
rab ratio nouns in β that also occur in α
rap ratio words in β that also occur in α
god google similarity (see text)
</table>
<tableCaption confidence="0.99735">
Table 2: The Features
</tableCaption>
<bodyText confidence="0.999990368421053">
phrase for example should be analysed as an adjunct
(and hence as not being an utterance on its own) or
as a fragment. In our experiments, we have followed
the decision made by the transcribers of the origi-
nal corpus, since they had information (e.g. about
pauses) which was not available to us.
For the antecedent-task, we include only pairs
where Q (the second utterance in the pair) is a
fragment—since the task is to identify an antecedent
for already identified fragments. This results in a
data-set with 1318 data points (i.e., we created on
average 4 pairs per fragment). This data-set is suf-
ficiently balanced between positives and negatives,
and so we did not create another version of it. The
data for the combined-task, however, is much big-
ger, as it contains pairs for all utterances. It consists
of 26,340 pairs, i.e. a ratio of roughly 1:90. For this
reason we also used balanced data-sets for training,
where the ratio was adjusted to 1:25.
</bodyText>
<subsectionHeader confidence="0.896801">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.999927217391304">
Table 2 lists the features we have used to represent
the utterances. (In this table, and in this section, we
denote the candidate for being a fragment with Q and
the candidate for being Q’s antecedent with α.)
We have defined a number of structural fea-
tures, which give information about the (discourse-
)structural relation between α and Q. The rationale
behind choosing them should be clear; iqu for ex-
ample indicates in a weak way whether there might
have been a topic change, and high nspk should
presumably make an antecedent relation between α
and Q less likely.
We have also used some lexical or utterance-
based features, which describe lexical properties of
the individual utterances and lexical relations be-
tween them which could be relevant for the tasks.
For example, the presence of a verb in Q is presum-
ably predictive for its being a fragment or not, as
is the length. To capture a possible semantic rela-
tionship between the utterances, we defined two fea-
tures. The more direct one, rab, looks at verbatim
re-occurrences of nouns from α in Q, which occur
for example in check-questions as in (4) below.
</bodyText>
<listItem confidence="0.958706">
(4) A: I saw Peter.
B: Peter? (= Who is this Peter you saw?)
</listItem>
<bodyText confidence="0.995794428571429">
Less direct semantic relations are intended to be
captured by god, the second semantic feature we
use.7 It is computed as follows: for each pair (x, y)
of nouns from α and Q, Google is called (via the
Google API) with a query for x, for y, and for x and
y together. The similarity then is the average ratio of
pair vs. individual term:
</bodyText>
<equation confidence="0.9923855">
Google Similarity(x, y) = (hits(x, y) + hits(x, y) )∗ 1
hits(x) hits(y) 2
</equation>
<bodyText confidence="0.999758">
We now describe the experiments we performed
and their results.
</bodyText>
<sectionHeader confidence="0.998347" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.930191">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.998697">
For the learning experiments, we used three classi-
fiers on all data-sets for the the three tasks:
</bodyText>
<listItem confidence="0.9985266">
• SLIPPER (Simple Learner with Iterative Prun-
ing to Produce Error Reduction), (Cohen and Singer,
1999), which is a rule learner which combines
the separate-and-conquer approach with confidence-
rated boosting. It is unique among the classifiers that
</listItem>
<footnote confidence="0.999040333333333">
7The name is short for google distance, which indicates its
relatedness to the feature used by (Poesio et al., 2004); it is how-
ever a measure of similarity, not distance, as described above.
</footnote>
<page confidence="0.996988">
250
</page>
<bodyText confidence="0.99959175">
we have used in that it can make use of “set-valued”
features, e.g. strings; we have run this learner both
with only the features listed above and with the ut-
terances (and POS-tags) as an additional feature.
</bodyText>
<listItem confidence="0.789536">
• TIMBL (Tilburg Memory-Based Learner),
(Daelemans et al., 2003), which implements a
</listItem>
<bodyText confidence="0.959007051282051">
memory-based learning algorithm (IB1) which pre-
dicts the class of a test data point by looking at its
distance to all examples from the training data, us-
ing some distance metric. In our experiments, we
have used the weighted-overlap method, which as-
signs weights to all features.
• MAXENT, Zhang Le’s C++ implementation8 of
maximum entropy modelling (Berger et al., 1996).
In our experiments, we used L-BFGS parameter es-
timation.
We also implemented a naive bayes classifier and
ran it on the fragment-task, with a data-set consisting
only of the strings and POS-tags.
To determine the contribution of all features, we
used an iterative process similar to the one described
in (Kohavi and John, 1997; Strube and M¨uller,
2003): we start with training a model using a base-
line set of features, and then add each remaining
feature individually, recording the gain (w.r.t. the f-
measure (f(0.5), to be precise)), and choosing the
best-performing feature, incrementally until no fur-
ther gain is recorded. All individual training- and
evaluation-steps are performed using 8-fold cross-
validation (given the small number of positive in-
stances, more folds would have made the number of
instances in the test set set too small).
The baselines were as follows: for the fragment-
task, we used bvb and lbe as baseline, i.e. we let
the classifier know the length of the candidate and
whether the candidate contains a verb or not. For
the antecedent-task we tested a very simple baseline,
containing only of one feature, the distance between
α and Q (dis). The baseline for the combined-
task, finally, was a combination of those two base-
lines, i.e. bvb+lbe+dis. The full feature-set for
the fragment-task was lbe, bvb, bpr, nrb,
bft, bds (since for this task there was no α to
compute features of), for the two other tasks it was
the complete set shown in Table 2.
</bodyText>
<footnote confidence="0.99775">
8Available from http://homepages.inf.ed.ac.uk/
s0450736/maxent toolkit.html.
</footnote>
<sectionHeader confidence="0.610956" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999993478260869">
The Tables 3–5 show the results of the experiments.
The entries are roughly sorted by performance of the
classifier used; for most of the classifiers and data-
sets for each task we show the performance for base-
line, intermediate feature set(s), and full feature-set,
for the rest we only show the best-performing set-
ting. We also indicate whether a balanced or unbal-
anced data set was used. I.e., the first three lines
in Table 3 report on MaxEnt on a balanced data set
for the fragment-task, giving results for the baseline,
baseline+nrb+bft, and the full feature-set.
We begin with discussing the fragment task. As
Table 3 shows, the three main classifiers perform
roughly equivalently. Re-balancing the data, as ex-
pected, boosts recall at the cost of precision. For all
settings (i.e., combinations of data-sets, feature-sets
and classifier), except re-balanced maxent, the base-
line (verb in Q yes/no, and length of Q) already has
some success in identifying fragments, but adding
the remaining features still boosts the performance.
Having available the string (condition s.s; slipper
with set valued features) interestingly does not help
SLIPPER much.
Overall the performance on this task is not great.
Why is that? An analysis of the errors made shows
two problems. Among the false negatives, there is a
high number of fragments like “yeah” and “mhm”,
which in their particular context were answers to
questions, but that however occur much more of-
ten as backchannels (true negatives). The classifier,
without having information about the context, can of
course not distinguish between these cases, and goes
for the majority decision. Among the false positives,
we find utterances that are indeed non-sentential,
but for which no antecedent was marked (as in (3)
above), i.e., which are not fragments in our narrow
sense. It seems, thus, that the required distinctions
are not ones that can be reliably learnt from looking
at the fragments alone.
The antecedent-task was handled more satisfac-
torily, as Table 4 shows. For this task, a naive base-
line (“always take previous utterance”) preforms rel-
atively well already; however, all classifiers were
able to improve on this, with a slight advantage for
the maxent model (f(0.5) = 0.76). As the entry
for MaxEnt shows, adding to the baseline-features
</bodyText>
<page confidence="0.993065">
251
</page>
<table confidence="0.9999364375">
Data Set Cl. Recall Precision f(0.5) f(1.0) f(2.0)
B; bl m 0.00 0.00 0.00 0.00 0.00
B; bl+nrb+bft m 36.39 31.16 0.31 0.33 0.35
B; all m 40.61 44.10 0.43 0.42 0.41
UB; all m 22.13 65.06 0.47 0.33 0.25
B; bl t 31.77 21.20 0.22 0.24 0.28
B; bl+nrb+bpr+bds t 42.18 41.26 0.41 0.42 0.42
B; all t 44.54 32.74 0.34 0.37 0.41
UB; bl+nrb t 26.22 59.05 0.47 0.36 0.29
B; bl s 21.07 16.95 0.17 0.18 0.20
B; bl+nrb+bft+bds s 36.37 49.28 0.46 0.41 0.38
B; all s 36.67 43.31 0.42 0.40 0.38
UB; bl+nrb s 28.28 57.88 0.48 0.38 0.31
B s.s 32.57 42.96 0.40 0.36 0.34
B b 55.62 19.75 0.23 0.29 0.41
UB b 66.50 20.00 0.23 0.31 0.45
</table>
<tableCaption confidence="0.998111">
Table 3: Results for the fragment task. (Cl. = classifier used, where s = slipper, s.s = slipper + set-valued
features, t = timbl, m = maxent, b = naive bayes; UB/B = (un)balanced training data.)
</tableCaption>
<table confidence="0.9999724">
Data Set Cl. Recall Precision f(0.5) f(1.0) f(2.0)
dis=1 - 44.95 44.81 0.45 0.45 0.45
UB; bl m 0 0 0.0 0.0 0.0
UB; bl+awh m 43.21 52.90 0.50 0.47 0.45
UB; bl+awh+god m 36.98 75.31 0.62 0.50 0.41
UB; bl+awh+god+lbe+lal+iqu+nra+buh m 64.26 80.39 0.76 0.71 0.67
UB; all m 58.16 73.57 0.69 0.64 0.60
UB; bl s 0.00 0.00 0.00 0.00 0.00
UB; bl+aqm s 36.65 78.44 0.63 0.49 0.41
UB; bl+aqm+rab+iqu+lal s 49.72 79.75 0.71 0.61 0.54
UB; all s 49.43 72.57 0.66 0.58 0.52
UB; bl t 0 0 0.0 0.0 0.0
UB; bl+aqm t 36.98 73.58 0.61 0.49 0.41
UB; bl+aqm+awh+rab+iqu t 46.41 77.65 0.68 0.58 0.50
UB; all t 60.57 58.74 0.59 0.60 0.60
</table>
<tableCaption confidence="0.943416">
Table 4: Results for the antecedent task.
</tableCaption>
<table confidence="0.999945111111111">
Data Set Cl. Recall Precision f(0.5) f(1.0) f(2.0)
B; bl m 0.00 0.00 0.00 0.00 0.00
B; bl+rap m 5.83 40.91 0.18 0.10 0.07
B; bl+rap+god m 7.95 55.83 0.25 0.14 0.10
B; bl+rap+god+nspk m 11.70 49.15 0.30 0.19 0.14
B; bl+rap+god+nspk+alt+awh+nra+lal m 20.27 50.02 0.38 0.28 0.23
B; all m 23.29 43.79 0.36 0.30 0.25
UB; bl+rap+god+nspk+iqu+nra+bds+rab+awh m 13.01 54.87 0.33 0.21 0.15
B; bl s 0.00 0.00 0.00 0.00 0.00
B; bl+god s 11.80 35.60 0.25 0.17 0.13
B; bl+god+bds s 14.44 46.98 0.32 0.22 0.17
B; all s 17.78 41.96 0.32 0.24 0.20
UB; bl+alt+bds+god+sspk+rap s 11.37 56.34 0.31 0.19 0.13
B; bl t 0.00 0.00 0.00 0.00 0.00
B; bl+god t 17.20 29.09 0.25 0.21 0.19
B; all t 17.87 19.97 0.19 0.19 0.18
UB; bl+god+iqu+rab t 14.24 41.63 0.29 0.21 0.16
B; bl+rab+buh s.s 8.63 54.20 0.26 0.15 0.10
</table>
<tableCaption confidence="0.996228">
Table 5: Results for the combined task.
</tableCaption>
<page confidence="0.995613">
252
</page>
<bodyText confidence="0.999994658536586">
information about whether α is a question or not al-
ready boost the performance considerably. An anal-
ysis of the predictions of this model then indeed
shows that it already captures cases of question and
answer pairs quite well. Adding the similarity fea-
ture god then gives the model information about
semantic relatedness, which, as hypothesised, cap-
tures elaboration-type relations (as in (1-b) and (1-c)
above). Structural information (iqu) further im-
proves the model; however, the remaining features
only seem to add interfering information, for perfor-
mance using the full feature-set is worse.
If one of the problems of the fragment-task was
that information about the context is required to dis-
tinguish fragments and backchannels, then the hope
could be that in the combined-task the classifier
would able to capture these cases. However, the per-
formance of all classifiers on this task is not satis-
factory, as Table 5 shows; in fact, it is even slightly
worse than the performance on the fragment task
alone. We speculate that instead of of cancelling out
mistakes in the other part of the task, the two goals
(let β be a fragment, and α a typical antecedent) in-
terfere during optimisation of the rules.
To summarise, we have shown that the task of
identifying the antecedent of a given fragment is
learnable, using a feature-set that combines struc-
tural and lexical features; in particular, the inclusion
of a measure of semantic relatedness, which was
computed via queries to an internet search engine,
proved helpful. The task of identifying (resolution-
via-identity) fragments, however, is hindered by the
high number of non-sentential utterances which can
be confused with the kinds of fragments we are in-
terested in. Here it could be helpful to have a method
that identifies and filters out backchannels, presum-
ably using a much more local mechanism (as for ex-
ample proposed in (Traum, 1994)). Similarly, the
performance on the combined task is low, also due
to a high number of confusions of backchannels and
fragments. We discuss an alternative set-up below.
</bodyText>
<sectionHeader confidence="0.999977" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999963657142857">
To our knowledge, the tasks presented here have so
far not been studied with a machine learning ap-
proach. The closest to our problem is (Fern´andez et
al., 2004b), which discusses classifying certain types
of fragments, namely questions of the type “Who?”,
“When?”, etc. (sluices). However, that paper does
not address the task of identifying those in a cor-
pus (which in any case should be easier than our
fragment-task, since those fragments cannot be con-
fused with backchannels).
Overlapping from another direction is the work
presented in (Zechner and Lavie, 2001), where the
task of aligning questions and answers is tackled.
This subsumes the task of identifying question-
antecedents for short-answers, but again is presum-
ably somewhat simpler than our general task, be-
cause questions are easier to identify. The authors
also evaluate the use of the alignment of questions
and answers in a summarisation system, and report
an increase in summary fluency, without a compro-
mise in informativeness. This is something we hope
to be able to show for our tasks as well.
There are also similarities, especially of the an-
tecedent task, to the pronoun resolution task (see
e.g. (Strube and M¨uller, 2003; Poesio et al., 2004)).
Interestingly, our results for the antecedent task are
close to those reported for that task. The problem of
identifying the units in need of an antecedent, how-
ever, is harder for us, due to the problem of there
being a large number of non-sentential utterances
that cannot be linked to a single utterance as an-
tecedent. In general, this seems to be the main differ-
ence between our task and the ones mentioned here,
which concentrate on more easily identified mark-
ables (questions, sluices, and pronouns).
</bodyText>
<sectionHeader confidence="0.996888" genericHeader="conclusions">
6 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.999961285714286">
We have presented a machine learning approach
to the task of identifying fragments and their an-
tecedents in multi-party dialogue. This represents a
well-defined subtask of computing discourse struc-
ture, which to our knowledge has not been studied so
far. We have shown that the task of identifying the
antecedent of a given fragment is learnable, using
features that provide information about the structure
of the discourse between antecedent and fragment,
and about semantic closeness.
The other tasks, identifying fragments and the
combined tasks, however, did not perform as well,
mainly because of a high rate of confusions be-
tween general non-sentential utterances and frag-
</bodyText>
<page confidence="0.994094">
253
</page>
<bodyText confidence="0.999828166666667">
ments (in our sense). In future work, we will try
a modified approach, where the detection of frag-
ments is integrated with a classification of utterances
as backchannels, fragments, or full sentences, and
where the antecedent task only ranks pairs, leaving
open the possibility of excluding a supposed frag-
ment by using contextual information. Lastly, we
are planning to integrate our classifier into a pro-
cessing pipeline after the pronoun resolution step,
to see whether this would improve both our perfor-
mance and the quality of automatic meeting sum-
marisations.9
</bodyText>
<sectionHeader confidence="0.998922" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999752846153846">
Adam L. Berger, Stephen Della Pietra, and Vincent J. Della
Pietra. 1996. A maximum entropy approach to natural lan-
guage processing. Computational Linguistics, 22(1):39–71.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Linguistics,
22(2):249–254.
William Cohen and Yoram Singer. 1999. A simple, fast, and
effective rule learner. In Proceedings of the Sixteenth Na-
tional Conference on Artificial Intelligence (AAAI-99), Or-
lando, Florida, July. AAAI.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2003. TiMBL: Tilburg
memory based learner, version 5.0, reference guide.
ILC Technical Report 03-10, Induction of Linguis-
tic Knowledge; Tilburg University. Available from
http://ilk.uvt.nl/downloads/pub/...
papers/ilk0310.pdf.
Raquel Fern´andez and Jonathan Ginzburg. 2002. Non-
sentential utterances in dialogue: A corpus-based study. In
Kristiina Jokinen and Susan McRoy, editors, Proceedings
of the Third SIGdial Workshop on Discourse and Dialogue,
pages 15–26, Philadelphia, USA, July. ACL Special Interest
Group on Dialog.
Raquel Fern´andez, Jonathan Ginzburg, Howard Gregory, and
Shalom Lappin. 2004a. Shards: Fragment resolution in
dialogue. In H. Bunt and R. Muskens, editors, Computing
Meaning, volume 3. Kluwer.
Raquel Fern´andez, Jonathan Ginzburg, and Shalom Lappin.
2004b. Classifying ellipsis in dialogue: A machine learn-
ing approach. In Proceedings of COLING 2004, Geneva,
Switzerland, August.
John S. Garofolo, Christophe D. Laprun, Martial Michel, Vin-
cent M. Stanford, and Elham Tabassi. 2004. The NITS
9Acknowledgements: We would like to acknowledge help-
ful discussions with Jason Baldridge and Michael Strube during
the early stages of the project, and helpful comments from the
anonymous reviewers.
meeting room pilot corpus. In Proceedings of the Interna-
tional Language Resources Conference (LREC04), Lisbon,
Portugal, May.
J.J. Godfrey, E. C. Holliman, and J. McDaniel. 1992. SWITCH-
BOARD: Telephone speech corpus for research and devlop-
ment. In Proceedings of the IEEE Conference on Acoustics,
Speech, and Signal Processing, pages 517–520, San Fran-
cisco, USA, March.
Ron Kohavi and George H. John. 1997. Wrappers for feature
selection. Artificial Intelligence Journal, 97(1–2):273–324.
Christoph M¨uller and Michael Strube. 2001. MMAX: A Tool
for the Annotation of Multi-modal Corpora. In Proceedings
of the 2nd IJCAI Workshop on Knowledge and Reasoning
in Practical Dialogue Systems, pages 45–50, Seattle, USA,
August.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and Janet
Hitzeman. 2004. Learning to resolve bridging refer-
ences. In Proceedings of the 42nd annual meeting of the
Association for Computational Linguistics, pages 144–151,
Barcelona, Spain, July.
David Schlangen and Alex Lascarides. 2002. Resolving
fragments using discourse information. In Johan Bos,
Mary Ellen Foster, and Colin Matheson, editors, Proceed-
ings of the 6th International Workshop on Formal Semantics
and Pragmatics of Dialogue (EDILOG 2002), pages 161–
168, Edinburgh, September.
David Schlangen and Alex Lascarides. 2003. The interpreta-
tion of non-sentential utterances in dialogue. In Alexander
Rudnicky, editor, Proceedings of the 4th SIGdial workshop
on Discourse and Dialogue, Sapporo, Japan, July.
David Schlangen. 2003. A Coherence-Based Approach to
the Interpretation ofNon-Sentential Utterances in Dialogue.
Ph.D. thesis, School of Informatics, University of Edin-
burgh, Edinburgh, UK.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging us-
ing decision trees. In Proceedings of the International Con-
ference on New Methods in Language Processing, Manch-
ester, UK.
Michael Strube and Christoph M¨uller. 2003. A machine learn-
ing approach to pronoun resolution in spoken dialogue. In
Proceedings of the 41st Annual Meeting of the Association
for Computational Lingustics, Sapporo, Japan.
D. Traum and P. Heeman. 1997. Utterance units in spoken
dialogue. In E. Maier, M. Mast, and S. LuperFoy, editors,
Dialogue Processing in Spoken Language Systems, Lecture
Notes in Artificial Intelligence. Springer-Verlag.
David R. Traum. 1994. A Computational Theory of Grounding
in Natural Language Conversation. Ph.D. thesis, Computer
Science, University of Rochester, Rochester, USA, Decem-
ber.
Klaus Zechner and Anton Lavie. 2001. Increasing the coher-
ence of spoken dialogue summaries by cross-speaker infor-
mation linking. In Proceedings of the NAAACL Workshop
on Automatic Summarisation, Pittsburgh, USA, June.
</reference>
<page confidence="0.998644">
254
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.668320">
<title confidence="0.998685">Towards Finding and Fixing Fragments: Using ML to Identify Non-Sentential Utterances and their Antecedents in Multi-Party Dialogue</title>
<author confidence="0.999965">David Schlangen</author>
<affiliation confidence="0.9982355">Department of Linguistics University of Potsdam</affiliation>
<address confidence="0.9995905">P.O. Box 601553 D-14415 Potsdam — Germany</address>
<email confidence="0.999653">das@ling.uni-potsdam.de</email>
<abstract confidence="0.982350157894737">Non-sentential utterances (e.g., shortanswers as in “Who came to the party?”— “Peter.”) are pervasive in dialogue. As with other forms of ellipsis, the elided material is typically present in the context (e.g., the question that a short answer answers). We present a machine learning approach to the novel task of identifying fragments and their antecedents in multiparty dialogue. We compare the performance of several learning algorithms, using a mixture of structural and lexical features, and show that the task of identifying antecedents given a fragment can be learnt _ we discuss why the task of identifying fragments is _ and finally report a combined task _</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="16356" citStr="Berger et al., 1996" startWordPosition="2720" endWordPosition="2723">-valued” features, e.g. strings; we have run this learner both with only the features listed above and with the utterances (and POS-tags) as an additional feature. • TIMBL (Tilburg Memory-Based Learner), (Daelemans et al., 2003), which implements a memory-based learning algorithm (IB1) which predicts the class of a test data point by looking at its distance to all examples from the training data, using some distance metric. In our experiments, we have used the weighted-overlap method, which assigns weights to all features. • MAXENT, Zhang Le’s C++ implementation8 of maximum entropy modelling (Berger et al., 1996). In our experiments, we used L-BFGS parameter estimation. We also implemented a naive bayes classifier and ran it on the fragment-task, with a data-set consisting only of the strings and POS-tags. To determine the contribution of all features, we used an iterative process similar to the one described in (Kohavi and John, 1997; Strube and M¨uller, 2003): we start with training a model using a baseline set of features, and then add each remaining feature individually, recording the gain (w.r.t. the fmeasure (f(0.5), to be precise)), and choosing the best-performing feature, incrementally until </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="9255" citStr="Carletta, 1996" startWordPosition="1517" endWordPosition="1518">e our corpus, speech disfluencies.6 We now describe the creation of the data we used for training. We first describe the data-sets for the different tasks, and then the features used to represent the events that are to be classified. 3.2 Data Sets Data creation for the fragment-identification task (henceforth simply fragment-task) was straightforspeakers are discussed below. 4We have used the MMAX tool (M¨uller and Strube, 2001)) for the annotation. 5To test the reliability of the annotation scheme, we had a subset of the data annotated by two annotators and found a satisfactory r,-agreement (Carletta, 1996) of r, = 0.81. 6The tagger is available free for academic research from http://www.ims.uni-stuttgart.de/projekte/ corplex/TreeTagger/DecisionTreeTagger.html. ward: for each utterance, a number of features was derived automatically (see next section) and the correct class (fragment / other) was added. (Note that none of the manually annotated attributes were used.) This resulted in a file with 5,999 data points for classification. Given that there were 307 fragments, this means that in this data-set there is a ratio positives (fragments) vs. negatives (non-fragments) for the classifier of 1:20.</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Cohen</author>
<author>Yoram Singer</author>
</authors>
<title>A simple, fast, and effective rule learner.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intelligence (AAAI-99),</booktitle>
<publisher>AAAI.</publisher>
<location>Orlando, Florida,</location>
<contexts>
<context position="15351" citStr="Cohen and Singer, 1999" startWordPosition="2555" endWordPosition="2558">e.7 It is computed as follows: for each pair (x, y) of nouns from α and Q, Google is called (via the Google API) with a query for x, for y, and for x and y together. The similarity then is the average ratio of pair vs. individual term: Google Similarity(x, y) = (hits(x, y) + hits(x, y) )∗ 1 hits(x) hits(y) 2 We now describe the experiments we performed and their results. 4 Experiments and Results 4.1 Experimental Setup For the learning experiments, we used three classifiers on all data-sets for the the three tasks: • SLIPPER (Simple Learner with Iterative Pruning to Produce Error Reduction), (Cohen and Singer, 1999), which is a rule learner which combines the separate-and-conquer approach with confidencerated boosting. It is unique among the classifiers that 7The name is short for google distance, which indicates its relatedness to the feature used by (Poesio et al., 2004); it is however a measure of similarity, not distance, as described above. 250 we have used in that it can make use of “set-valued” features, e.g. strings; we have run this learner both with only the features listed above and with the utterances (and POS-tags) as an additional feature. • TIMBL (Tilburg Memory-Based Learner), (Daelemans </context>
</contexts>
<marker>Cohen, Singer, 1999</marker>
<rawString>William Cohen and Yoram Singer. 1999. A simple, fast, and effective rule learner. In Proceedings of the Sixteenth National Conference on Artificial Intelligence (AAAI-99), Orlando, Florida, July. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>TiMBL: Tilburg memory based learner, version 5.0, reference guide. ILC</title>
<date>2003</date>
<tech>Technical Report 03-10,</tech>
<institution>Induction of Linguistic Knowledge; Tilburg University.</institution>
<note>Available from http://ilk.uvt.nl/downloads/pub/... papers/ilk0310.pdf.</note>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2003</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2003. TiMBL: Tilburg memory based learner, version 5.0, reference guide. ILC Technical Report 03-10, Induction of Linguistic Knowledge; Tilburg University. Available from http://ilk.uvt.nl/downloads/pub/... papers/ilk0310.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Fern´andez</author>
<author>Jonathan Ginzburg</author>
</authors>
<title>Nonsentential utterances in dialogue: A corpus-based study.</title>
<date>2002</date>
<journal>ACL Special Interest Group on Dialog.</journal>
<booktitle>Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>15--26</pages>
<editor>In Kristiina Jokinen and Susan McRoy, editors,</editor>
<location>Philadelphia, USA,</location>
<marker>Fern´andez, Ginzburg, 2002</marker>
<rawString>Raquel Fern´andez and Jonathan Ginzburg. 2002. Nonsentential utterances in dialogue: A corpus-based study. In Kristiina Jokinen and Susan McRoy, editors, Proceedings of the Third SIGdial Workshop on Discourse and Dialogue, pages 15–26, Philadelphia, USA, July. ACL Special Interest Group on Dialog.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Fern´andez</author>
<author>Jonathan Ginzburg</author>
<author>Howard Gregory</author>
<author>Shalom Lappin</author>
</authors>
<title>Shards: Fragment resolution in dialogue.</title>
<date>2004</date>
<booktitle>Computing Meaning,</booktitle>
<volume>3</volume>
<editor>In H. Bunt and R. Muskens, editors,</editor>
<publisher>Kluwer.</publisher>
<marker>Fern´andez, Ginzburg, Gregory, Lappin, 2004</marker>
<rawString>Raquel Fern´andez, Jonathan Ginzburg, Howard Gregory, and Shalom Lappin. 2004a. Shards: Fragment resolution in dialogue. In H. Bunt and R. Muskens, editors, Computing Meaning, volume 3. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Fern´andez</author>
<author>Jonathan Ginzburg</author>
<author>Shalom Lappin</author>
</authors>
<title>Classifying ellipsis in dialogue: A machine learning approach.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING 2004,</booktitle>
<location>Geneva, Switzerland,</location>
<marker>Fern´andez, Ginzburg, Lappin, 2004</marker>
<rawString>Raquel Fern´andez, Jonathan Ginzburg, and Shalom Lappin. 2004b. Classifying ellipsis in dialogue: A machine learning approach. In Proceedings of COLING 2004, Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Garofolo</author>
<author>Christophe D Laprun</author>
<author>Martial Michel</author>
<author>Vincent M Stanford</author>
<author>Elham Tabassi</author>
</authors>
<title>The NITS 9Acknowledgements: We would like to acknowledge helpful discussions with Jason Baldridge and Michael Strube during the early stages of the project, and helpful comments from the anonymous reviewers.</title>
<date>2004</date>
<contexts>
<context position="6842" citStr="Garofolo et al., 2004" startWordPosition="1119" endWordPosition="1122"> they also allow the fragment classification decision to come from another source—a language-model used in an automatic speech recognition system, for example— and to use only the antecedent-classifier. The other approach is to do both at the same time, i.e. to classify pairs of utterances into those that combine a fragment and its antecedent and those that don’t. We report the results of our experiments with these tasks below, after describing the data we used. 3 Corpus, Features, and Data Creation 3.1 Corpus As material we have used six transcripts from the “NIST Meeting Room Pilot Corpus” (Garofolo et al., 2004), a corpus of recordings and transcriptions of multi-party meetings.3 Those six transcripts con2The boundaries are fuzzy here, however, as backchannels can also be fragmental repetitions of previous material, and sometimes it is not clear how to classify a given utterance. A similar problem of classifying fragments is discussed in (Schlangen, 2003) and we will not go further into this here. 3We have chosen a multi-party setting because we are ultimately interested in automatic summarisation of meetings. In this paper here, however, we view our task as a “stand-alone task”. Some of the problems</context>
</contexts>
<marker>Garofolo, Laprun, Michel, Stanford, Tabassi, 2004</marker>
<rawString>John S. Garofolo, Christophe D. Laprun, Martial Michel, Vincent M. Stanford, and Elham Tabassi. 2004. The NITS 9Acknowledgements: We would like to acknowledge helpful discussions with Jason Baldridge and Michael Strube during the early stages of the project, and helpful comments from the anonymous reviewers.</rawString>
</citation>
<citation valid="true">
<title>meeting room pilot corpus.</title>
<date></date>
<booktitle>In Proceedings of the International Language Resources Conference (LREC04),</booktitle>
<location>Lisbon, Portugal,</location>
<marker></marker>
<rawString>meeting room pilot corpus. In Proceedings of the International Language Resources Conference (LREC04), Lisbon, Portugal, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Godfrey</author>
<author>E C Holliman</author>
<author>J McDaniel</author>
</authors>
<title>SWITCHBOARD: Telephone speech corpus for research and devlopment.</title>
<date>1992</date>
<booktitle>In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>517--520</pages>
<location>San Francisco, USA,</location>
<contexts>
<context position="8609" citStr="Godfrey et al., 1992" startWordPosition="1412" endWordPosition="1415"> above); but note that as explained above, we are actually only looking at a sub-class of all NSUs here. For these pairs we also annotated some more attributes, which are summarised in Table 1. Note that the average distance is slightly higher than that reported in (Schlangen and Lascarides, 2003) for (2-party) dialogue (1.8); this is presumably due to the presence of more speakers who are able to reply to an utterance. Finally, we automatically annotated all utterances with part-of-speech tags, using TreeTagger (Schmid, 1994), which we’ve trained on the switchboard corpus of spoken language (Godfrey et al., 1992), because it contains, just like our corpus, speech disfluencies.6 We now describe the creation of the data we used for training. We first describe the data-sets for the different tasks, and then the features used to represent the events that are to be classified. 3.2 Data Sets Data creation for the fragment-identification task (henceforth simply fragment-task) was straightforspeakers are discussed below. 4We have used the MMAX tool (M¨uller and Strube, 2001)) for the annotation. 5To test the reliability of the annotation scheme, we had a subset of the data annotated by two annotators and foun</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>J.J. Godfrey, E. C. Holliman, and J. McDaniel. 1992. SWITCHBOARD: Telephone speech corpus for research and devlopment. In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing, pages 517–520, San Francisco, USA, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Kohavi</author>
<author>George H John</author>
</authors>
<title>Wrappers for feature selection.</title>
<date>1997</date>
<journal>Artificial Intelligence Journal,</journal>
<pages>97--1</pages>
<contexts>
<context position="16684" citStr="Kohavi and John, 1997" startWordPosition="2774" endWordPosition="2777">point by looking at its distance to all examples from the training data, using some distance metric. In our experiments, we have used the weighted-overlap method, which assigns weights to all features. • MAXENT, Zhang Le’s C++ implementation8 of maximum entropy modelling (Berger et al., 1996). In our experiments, we used L-BFGS parameter estimation. We also implemented a naive bayes classifier and ran it on the fragment-task, with a data-set consisting only of the strings and POS-tags. To determine the contribution of all features, we used an iterative process similar to the one described in (Kohavi and John, 1997; Strube and M¨uller, 2003): we start with training a model using a baseline set of features, and then add each remaining feature individually, recording the gain (w.r.t. the fmeasure (f(0.5), to be precise)), and choosing the best-performing feature, incrementally until no further gain is recorded. All individual training- and evaluation-steps are performed using 8-fold crossvalidation (given the small number of positive instances, more folds would have made the number of instances in the test set set too small). The baselines were as follows: for the fragmenttask, we used bvb and lbe as base</context>
</contexts>
<marker>Kohavi, John, 1997</marker>
<rawString>Ron Kohavi and George H. John. 1997. Wrappers for feature selection. Artificial Intelligence Journal, 97(1–2):273–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph M¨uller</author>
<author>Michael Strube</author>
</authors>
<title>MMAX: A Tool for the Annotation of Multi-modal Corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd IJCAI Workshop on Knowledge and Reasoning in Practical Dialogue Systems,</booktitle>
<pages>45--50</pages>
<location>Seattle, USA,</location>
<marker>M¨uller, Strube, 2001</marker>
<rawString>Christoph M¨uller and Michael Strube. 2001. MMAX: A Tool for the Annotation of Multi-modal Corpora. In Proceedings of the 2nd IJCAI Workshop on Knowledge and Reasoning in Practical Dialogue Systems, pages 45–50, Seattle, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Rahul Mehta</author>
<author>Axel Maroudas</author>
<author>Janet Hitzeman</author>
</authors>
<title>Learning to resolve bridging references.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>144--151</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="15613" citStr="Poesio et al., 2004" startWordPosition="2596" endWordPosition="2599">ts(x, y) + hits(x, y) )∗ 1 hits(x) hits(y) 2 We now describe the experiments we performed and their results. 4 Experiments and Results 4.1 Experimental Setup For the learning experiments, we used three classifiers on all data-sets for the the three tasks: • SLIPPER (Simple Learner with Iterative Pruning to Produce Error Reduction), (Cohen and Singer, 1999), which is a rule learner which combines the separate-and-conquer approach with confidencerated boosting. It is unique among the classifiers that 7The name is short for google distance, which indicates its relatedness to the feature used by (Poesio et al., 2004); it is however a measure of similarity, not distance, as described above. 250 we have used in that it can make use of “set-valued” features, e.g. strings; we have run this learner both with only the features listed above and with the utterances (and POS-tags) as an additional feature. • TIMBL (Tilburg Memory-Based Learner), (Daelemans et al., 2003), which implements a memory-based learning algorithm (IB1) which predicts the class of a test data point by looking at its distance to all examples from the training data, using some distance metric. In our experiments, we have used the weighted-ove</context>
<context position="25833" citStr="Poesio et al., 2004" startWordPosition="4332" endWordPosition="4335">tions and answers is tackled. This subsumes the task of identifying questionantecedents for short-answers, but again is presumably somewhat simpler than our general task, because questions are easier to identify. The authors also evaluate the use of the alignment of questions and answers in a summarisation system, and report an increase in summary fluency, without a compromise in informativeness. This is something we hope to be able to show for our tasks as well. There are also similarities, especially of the antecedent task, to the pronoun resolution task (see e.g. (Strube and M¨uller, 2003; Poesio et al., 2004)). Interestingly, our results for the antecedent task are close to those reported for that task. The problem of identifying the units in need of an antecedent, however, is harder for us, due to the problem of there being a large number of non-sentential utterances that cannot be linked to a single utterance as antecedent. In general, this seems to be the main difference between our task and the ones mentioned here, which concentrate on more easily identified markables (questions, sluices, and pronouns). 6 Conclusions and Further Work We have presented a machine learning approach to the task of</context>
</contexts>
<marker>Poesio, Mehta, Maroudas, Hitzeman, 2004</marker>
<rawString>Massimo Poesio, Rahul Mehta, Axel Maroudas, and Janet Hitzeman. 2004. Learning to resolve bridging references. In Proceedings of the 42nd annual meeting of the Association for Computational Linguistics, pages 144–151, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Alex Lascarides</author>
</authors>
<title>Resolving fragments using discourse information.</title>
<date>2002</date>
<booktitle>Proceedings of the 6th International Workshop on Formal Semantics and Pragmatics of Dialogue (EDILOG 2002),</booktitle>
<pages>161--168</pages>
<editor>In Johan Bos, Mary Ellen Foster, and Colin Matheson, editors,</editor>
<location>Edinburgh,</location>
<contexts>
<context position="1855" citStr="Schlangen and Lascarides, 2002" startWordPosition="290" endWordPosition="293">ngen and Lascarides, 2003). (1) a. A: Who came to the party? B: Peter. (= Peter came to the party.) b. A: I talked to Peter. B: Peter Miller? (= Was it Peter Miller you talked to?) c. A: Who was this? Peter Miller? (= Was this Peter Miller? Such utterances pose an obvious problem for natural language processing applications, namely that the intended information (in (1-a)-B a proposition) has to be recovered from the uttered information (here, an NP meaning) with the help of information from the context. While some systems that automatically resolve such fragments have recently been developed (Schlangen and Lascarides, 2002; Fern´andez et al., 2004a), they have the drawback that they require “deep” linguistic processing (full parses, and also information about discourse structure) and hence are not very robust. We have defined a well-defined subtask of this problem, namely identifying fragments (certain kinds of NSUs, see below) and their antecedents (in multi-party dialogue, in our case), and present a novel machine learning approach to it, which we hypothesise will be useful for tasks such as automatic meeting summarisation.1 The remainder of this paper is structured as follows. In the next section we further </context>
</contexts>
<marker>Schlangen, Lascarides, 2002</marker>
<rawString>David Schlangen and Alex Lascarides. 2002. Resolving fragments using discourse information. In Johan Bos, Mary Ellen Foster, and Colin Matheson, editors, Proceedings of the 6th International Workshop on Formal Semantics and Pragmatics of Dialogue (EDILOG 2002), pages 161– 168, Edinburgh, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Alex Lascarides</author>
</authors>
<title>The interpretation of non-sentential utterances in dialogue.</title>
<date>2003</date>
<booktitle>Proceedings of the 4th SIGdial workshop on Discourse and Dialogue,</booktitle>
<editor>In Alexander Rudnicky, editor,</editor>
<location>Sapporo, Japan,</location>
<contexts>
<context position="1251" citStr="Schlangen and Lascarides, 2003" startWordPosition="189" endWordPosition="192">cedents in multiparty dialogue. We compare the performance of several learning algorithms, using a mixture of structural and lexical features, and show that the task of identifying antecedents given a fragment can be learnt successfully (f(0.5) _ .76); we discuss why the task of identifying fragments is harder (f(0.5) _ .41) and finally report on a combined task (f(0.5) _ .38). 1 Introduction Non-sentential utterances (NSUs) as in (1) are pervasive in dialogue: recent studies put the proportion of such utterances at around 10% across different types of dialogue (Fern´andez and Ginzburg, 2002; Schlangen and Lascarides, 2003). (1) a. A: Who came to the party? B: Peter. (= Peter came to the party.) b. A: I talked to Peter. B: Peter Miller? (= Was it Peter Miller you talked to?) c. A: Who was this? Peter Miller? (= Was this Peter Miller? Such utterances pose an obvious problem for natural language processing applications, namely that the intended information (in (1-a)-B a proposition) has to be recovered from the uttered information (here, an NP meaning) with the help of information from the context. While some systems that automatically resolve such fragments have recently been developed (Schlangen and Lascarides, </context>
<context position="5521" citStr="Schlangen and Lascarides, 2003" startWordPosition="907" endWordPosition="910">should be analysed as being the result of ellipsis: backchannels for example (like the “Right” in utterance 4 in (2) above) seem to directly fulfil their discourse function without any need for reconstruction.2 To keep matters simple, we concentrate in this paper on NSUs of a certain kind, namely those that a) do not predominantly have a discourse-management function (like for example backchannels), but rather convey messages (i.e., propositions, questions or requests)—this is what distinguishes fragments from other NSUs—and b) have individual utterances as antecedents. In the terminology of (Schlangen and Lascarides, 2003), fragments of the latter type are resolution-via-identity-fragments, where the elided information can be identified in the context and need not be inferred (as opposed to resolution-viainference-fragments). Choosing only this special kind of NSUs poses the question whether this subgroup is distinguished from the general group of fragments by criteria that can be learnt; we will return to this below when we analyse the errors made by the classifier. We have defined two approaches to this task. One is to split the task into two sub-tasks: identifying fragments in a corpus, and identifying antec</context>
<context position="8286" citStr="Schlangen and Lascarides, 2003" startWordPosition="1359" endWordPosition="1362">2 (0.7%) α being last in their turn 142 (46%) ,Q being first in their turn 159 (52%) Table 1: Some distributional characteristics. (α denotes antecedent, β fragment.) sist of 5,999 utterances, among which we identified 307 fragment–antecedent pairs.4,5 With 5.1% this is a lower rate than that reported for NSUs in other corpora (see above); but note that as explained above, we are actually only looking at a sub-class of all NSUs here. For these pairs we also annotated some more attributes, which are summarised in Table 1. Note that the average distance is slightly higher than that reported in (Schlangen and Lascarides, 2003) for (2-party) dialogue (1.8); this is presumably due to the presence of more speakers who are able to reply to an utterance. Finally, we automatically annotated all utterances with part-of-speech tags, using TreeTagger (Schmid, 1994), which we’ve trained on the switchboard corpus of spoken language (Godfrey et al., 1992), because it contains, just like our corpus, speech disfluencies.6 We now describe the creation of the data we used for training. We first describe the data-sets for the different tasks, and then the features used to represent the events that are to be classified. 3.2 Data Set</context>
</contexts>
<marker>Schlangen, Lascarides, 2003</marker>
<rawString>David Schlangen and Alex Lascarides. 2003. The interpretation of non-sentential utterances in dialogue. In Alexander Rudnicky, editor, Proceedings of the 4th SIGdial workshop on Discourse and Dialogue, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
</authors>
<title>A Coherence-Based Approach to the Interpretation ofNon-Sentential Utterances in Dialogue.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Informatics, University of Edinburgh,</institution>
<location>Edinburgh, UK.</location>
<contexts>
<context position="7192" citStr="Schlangen, 2003" startWordPosition="1175" endWordPosition="1176">hat don’t. We report the results of our experiments with these tasks below, after describing the data we used. 3 Corpus, Features, and Data Creation 3.1 Corpus As material we have used six transcripts from the “NIST Meeting Room Pilot Corpus” (Garofolo et al., 2004), a corpus of recordings and transcriptions of multi-party meetings.3 Those six transcripts con2The boundaries are fuzzy here, however, as backchannels can also be fragmental repetitions of previous material, and sometimes it is not clear how to classify a given utterance. A similar problem of classifying fragments is discussed in (Schlangen, 2003) and we will not go further into this here. 3We have chosen a multi-party setting because we are ultimately interested in automatic summarisation of meetings. In this paper here, however, we view our task as a “stand-alone task”. Some of the problems resulting in the presence of many 248 average distance α – ,Q (utterances): 2.5 α declarative 159 (52%) α interrogative 140 (46%) α unclassfd. 8 (2%) ,Q declarative 235 (76%) ,Q interrogative (23%) ,Q unclassfd. 2 (0.7%) α being last in their turn 142 (46%) ,Q being first in their turn 159 (52%) Table 1: Some distributional characteristics. (α den</context>
</contexts>
<marker>Schlangen, 2003</marker>
<rawString>David Schlangen. 2003. A Coherence-Based Approach to the Interpretation ofNon-Sentential Utterances in Dialogue. Ph.D. thesis, School of Informatics, University of Edinburgh, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="8520" citStr="Schmid, 1994" startWordPosition="1399" endWordPosition="1400"> With 5.1% this is a lower rate than that reported for NSUs in other corpora (see above); but note that as explained above, we are actually only looking at a sub-class of all NSUs here. For these pairs we also annotated some more attributes, which are summarised in Table 1. Note that the average distance is slightly higher than that reported in (Schlangen and Lascarides, 2003) for (2-party) dialogue (1.8); this is presumably due to the presence of more speakers who are able to reply to an utterance. Finally, we automatically annotated all utterances with part-of-speech tags, using TreeTagger (Schmid, 1994), which we’ve trained on the switchboard corpus of spoken language (Godfrey et al., 1992), because it contains, just like our corpus, speech disfluencies.6 We now describe the creation of the data we used for training. We first describe the data-sets for the different tasks, and then the features used to represent the events that are to be classified. 3.2 Data Sets Data creation for the fragment-identification task (henceforth simply fragment-task) was straightforspeakers are discussed below. 4We have used the MMAX tool (M¨uller and Strube, 2001)) for the annotation. 5To test the reliability o</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Christoph M¨uller</author>
</authors>
<title>A machine learning approach to pronoun resolution in spoken dialogue.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Lingustics,</booktitle>
<location>Sapporo, Japan.</location>
<marker>Strube, M¨uller, 2003</marker>
<rawString>Michael Strube and Christoph M¨uller. 2003. A machine learning approach to pronoun resolution in spoken dialogue. In Proceedings of the 41st Annual Meeting of the Association for Computational Lingustics, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Traum</author>
<author>P Heeman</author>
</authors>
<title>Utterance units in spoken dialogue.</title>
<date>1997</date>
<booktitle>Dialogue Processing in Spoken Language Systems, Lecture Notes in Artificial Intelligence.</booktitle>
<editor>In E. Maier, M. Mast, and S. LuperFoy, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="11724" citStr="Traum and Heeman, 1997" startWordPosition="1909" endWordPosition="1912"> which means that to capture those we would have had to widen the window considerably. E.g., considering all previous 8 utterances would capture an additional 25 pairs, but at the cost of doubling the number of data points. We hence chose the approach described here, being aware of the introduction of a certain bias. As we have said, we are trying to link utterances, one a fragment, the other its antecedent. The notion of utterance is however less well-defined than one might expect, and the segmentation of continuous speech into utterances is a veritable research problem on its own (see e.g. (Traum and Heeman, 1997)). Often it is arguable whether a prepositional 249 Structuralfeatures dis distance α – β, in utterances sspk same speaker yes/no nspk number speaker changes (= # turns) iqu number of intervening questions alt α last utterance in its turn? bft β first utterance in its turn? Lexical / Utterance-based features bvb (tensed) verb present in β? bds disfluency present in β? aqm α contains question mark awh α contains wh word bpr ratio of polar particles (yes, no, maybe, etc..) / other in β apr ratio of polar particles in α lal length of α lbe length of β nra ratio nouns / non-nouns in α nra ratio no</context>
</contexts>
<marker>Traum, Heeman, 1997</marker>
<rawString>D. Traum and P. Heeman. 1997. Utterance units in spoken dialogue. In E. Maier, M. Mast, and S. LuperFoy, editors, Dialogue Processing in Spoken Language Systems, Lecture Notes in Artificial Intelligence. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
</authors>
<title>A Computational Theory of Grounding in Natural Language Conversation.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science, University of Rochester,</institution>
<location>Rochester, USA,</location>
<contexts>
<context position="24428" citStr="Traum, 1994" startWordPosition="4104" endWordPosition="4105">en fragment is learnable, using a feature-set that combines structural and lexical features; in particular, the inclusion of a measure of semantic relatedness, which was computed via queries to an internet search engine, proved helpful. The task of identifying (resolutionvia-identity) fragments, however, is hindered by the high number of non-sentential utterances which can be confused with the kinds of fragments we are interested in. Here it could be helpful to have a method that identifies and filters out backchannels, presumably using a much more local mechanism (as for example proposed in (Traum, 1994)). Similarly, the performance on the combined task is low, also due to a high number of confusions of backchannels and fragments. We discuss an alternative set-up below. 5 Related Work To our knowledge, the tasks presented here have so far not been studied with a machine learning approach. The closest to our problem is (Fern´andez et al., 2004b), which discusses classifying certain types of fragments, namely questions of the type “Who?”, “When?”, etc. (sluices). However, that paper does not address the task of identifying those in a corpus (which in any case should be easier than our fragment-</context>
</contexts>
<marker>Traum, 1994</marker>
<rawString>David R. Traum. 1994. A Computational Theory of Grounding in Natural Language Conversation. Ph.D. thesis, Computer Science, University of Rochester, Rochester, USA, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
<author>Anton Lavie</author>
</authors>
<title>Increasing the coherence of spoken dialogue summaries by cross-speaker information linking.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAAACL Workshop on Automatic Summarisation,</booktitle>
<location>Pittsburgh, USA,</location>
<contexts>
<context position="2855" citStr="Zechner and Lavie, 2001" startWordPosition="453" endWordPosition="456">d present a novel machine learning approach to it, which we hypothesise will be useful for tasks such as automatic meeting summarisation.1 The remainder of this paper is structured as follows. In the next section we further specify the task and different possible approaches to it. We then describe the corpus we used, some of its characteristics with respect to fragments, and the features we extracted from it for machine learning. Section 4 describes our experimental settings and reports the results. After a comparison to related work in Section 5, we close with a conclusion and some further 1(Zechner and Lavie, 2001) describe a related task, linking questions and answers, and evaluate its usefulness in the context of automatic summarisation; see Section 5. 247 Proceedings of the 43rd Annual Meeting of the ACL, pages 247–254, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics work that is planned. 2 The Tasks As we said in the introduction, the main task we want to tackle is to align (certain kinds of) NSUs and their antecedents. Now, what characterises this kind of NSU, and what are their antecedents? In the examples from the introduction, the NSUs can be resolved simply by looking at </context>
<context position="25180" citStr="Zechner and Lavie, 2001" startWordPosition="4224" endWordPosition="4227">. We discuss an alternative set-up below. 5 Related Work To our knowledge, the tasks presented here have so far not been studied with a machine learning approach. The closest to our problem is (Fern´andez et al., 2004b), which discusses classifying certain types of fragments, namely questions of the type “Who?”, “When?”, etc. (sluices). However, that paper does not address the task of identifying those in a corpus (which in any case should be easier than our fragment-task, since those fragments cannot be confused with backchannels). Overlapping from another direction is the work presented in (Zechner and Lavie, 2001), where the task of aligning questions and answers is tackled. This subsumes the task of identifying questionantecedents for short-answers, but again is presumably somewhat simpler than our general task, because questions are easier to identify. The authors also evaluate the use of the alignment of questions and answers in a summarisation system, and report an increase in summary fluency, without a compromise in informativeness. This is something we hope to be able to show for our tasks as well. There are also similarities, especially of the antecedent task, to the pronoun resolution task (see</context>
</contexts>
<marker>Zechner, Lavie, 2001</marker>
<rawString>Klaus Zechner and Anton Lavie. 2001. Increasing the coherence of spoken dialogue summaries by cross-speaker information linking. In Proceedings of the NAAACL Workshop on Automatic Summarisation, Pittsburgh, USA, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>