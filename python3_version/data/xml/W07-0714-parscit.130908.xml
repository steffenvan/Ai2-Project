<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006936">
<title confidence="0.997939">
Labelled Dependencies in Machine Translation Evaluation
</title>
<author confidence="0.9761">
Karolina Owczarzak Josef van Genabith Andy Wa
</author>
<affiliation confidence="0.9793945">
National Centre for Language Technology
School of Computing, Dublin City University
</affiliation>
<address confidence="0.796794">
Dublin 9, Ireland
</address>
<email confidence="0.99449">
{owczarzak,josef,away}@computing.dcu.ie
</email>
<sectionHeader confidence="0.997325" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999944466666667">
We present a method for evaluating the
quality of Machine Translation (MT)
output, using labelled dependencies
produced by a Lexical-Functional
Grammar (LFG) parser. *ur dependency-
based method, in contrast to most popular
string-based evaluation metrics, does not
unfairly penalize perfectly valid syntactic
variations in the translation, and the
addition of WordNet provides a way to
accommodate lexical variation. In
comparison with other metrics on 16,800
sentences of Chinese-English newswire
text, our method reaches high correlation
with human scores.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959677966102">
Since the creation of BLEU (Papineni et al., 2002)
and NIST (Doddington, 2002), the subject of
automatic evaluation metrics for MT has been
given quite a lot of attention. Although widely
popular thanks to their speed and efficiency, both
BLEU and NIST have been criticized for
inadequate accuracy of evaluation at the segment
level (Callison-Burch et al., 2006). As string
based-metrics, they are limited to superficial
comparison of word sequences between a
translated sentence and one or more reference
sentences, and are unable to accommodate any
legitimate grammatical variation when it comes to
lexical choices or syntactic structure of the
translation, beyond what can be found in the
multiple references. A natural next step in the field
of evaluation was to introduce metrics that would
better reflect our human judgement by accepting
synonyms in the translated sentence or evaluating
the translation on the basis of what syntactic
features it shares with the reference.
*ur method follows and substantially extends
the earlier work of Liu and Gildea (2005), who use
syntactic features and unlabelled dependencies to
evaluate MT quality, outperforming BLEU on
segment-level correlation with human judgement.
Dependencies abstract away from the particulars of
the surface string (and syntactic tree) realization
and provide a &amp;quot;normalized&amp;quot; representation of
(some) syntactic variants of a given sentence.
While Liu and Gildea (2005) calculate n-gram
matches on non-labelled head-modifier sequences
derived by head-extraction rules from syntactic
trees, we automatically evaluate the quality of
translation by calculating an f-score on labelled
dependency structures produced by a Lexical-
Functional Grammar (LFG) parser. These
dependencies differ from those used by Liu and
Gildea (2005), in that they are extracted according
to the rules of the LFG grammar and they are
labelled with a type of grammatical relation that
connects the head and the modifier, such as
subject, determiner, etc. The presence of
grammatical relation labels adds another layer of
important linguistic information into the
comparison and allows us to account for partial
matches, for example when a lexical item finds
itself in a correct relation but with an incorrect
partner. Moreover, we use a number of best parses
for the translation and the reference, which serves
to decrease the amount of noise that can be
introduced by the process of parsing and extracting
dependency information.
The translation and reference files are
analyzed by a treebank-based, probabilistic LFG
parser (Cahill et al., 2004), which produces a set of
dependency triples for each input. The translation
set is compared to the reference set, and the
number of matches is calculated, giving the
</bodyText>
<page confidence="0.977899">
104
</page>
<note confidence="0.8927755">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 104–111,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99947209375">
precision, recall, and f-score for each particular
translation.
In addition, to allow for the possibility of valid
lexical differences between the translation and the
references, we follow Kauchak and Barzilay
(2006) in adding a number of synonyms in the
process of evaluation to raise the number of
matches between the translation and the reference,
leading to a higher score.
In an experiment on 16,800 sentences of
Chinese-English newswire text with segment-level
human evaluation from the Linguistic Data
Consortium&apos;s (LDC) Multiple Translation project,
we compare the LFG-based evaluation method
with other popular metrics like BLEU, NIST,
General Text Matcher (GTM) (Turian et al., 2003),
Translation Error Rate (TER) (Snover et al.,
2006)1, and METE*R (Banerjee and Lavie, 2005),
and we show that combining dependency
representations with synonyms leads to a more
accurate evaluation that correlates better with
human judgment. Although evaluated on a
different test set, our method also outperforms the
correlation with human scores reported in Liu and
Gildea (2005).
The remainder of this paper is organized as
follows: Section 2 gives a basic introduction to
LFG; Section 3 describes related work; Section 4
describes our method and gives results of the
experiment on the Multiple Translation data;
Section 5 discusses ongoing work; Section 6
concludes.
</bodyText>
<sectionHeader confidence="0.9878" genericHeader="method">
2 Lexical-Functional Grammar
</sectionHeader>
<bodyText confidence="0.987773">
In Lexical-Functional Grammar (Kaplan and
Bresnan, 1982; Bresnan, 2001) sentence structure
is represented in terms of c(onstituent)-structure
and f(unctional)-structure. C-structure represents
the word order of the surface string and the
hierarchical organisation of phrases in terms of
CFG trees. F-structures are recursive feature (or
attribute-value) structures, representing abstract
grammatical relations, such as subj(ect), obj(ect),
obl(ique), adj(unct), etc., approximating to
predicate-argument structure or simple logical
forms. C-structure and f-structure are related in
1 e omit HTER (Human-Targeted Translation Error
Rate), as it is not fully automatic and requires human
input.
terms of functional annotations (attribute-value
structure equations) in c-structure trees, describing
f-structures.
While c-structure is sensitive to surface
rearrangement of constituents, f-structure abstracts
away from the particulars of the surface
realization. The sentences John resigned yesterday
and Yesterday, John resigned will receive different
tree representations, but identical f-structures,
shown in (1).
</bodyText>
<equation confidence="0.9818225">
(1) C-structure: F-structure:
V NP-TMP
I I
resigned yesterday
</equation>
<bodyText confidence="0.9851315">
Note that if these sentences were a translation-
reference pair, they would receive a less-than-
perfect score from string-based metrics. For
example, BLEU with add-one smoothing2 gives
this pair a score of barely 0.3781. This is because,
although all three unigrams from the &amp;quot;translation&amp;quot;
(John; resigned; yesterday) are present in the
reference, which contains four items including the
comma (Yesterday; ,; John; resigned), the
&amp;quot;translation&amp;quot; contains only one bigram (John
resigned) that matches the &amp;quot;reference&amp;quot; (Yesterday
,; , John; John resigned), and no matching
trigrams.
The f-structure can also be described in terms
of a flat set of triples. In triples format, the f-
structure in (1) is represented as follows:
{subj(resign, john), pers(john, 3), num(john, sg),
tense(resign, past), adj(resign, yesterday),
pers(yesterday, 3), num(yesterday, sg)}.
2 e use smoothing because the original BLEU metric
gives zero points to sentences with fewer than one four-
gram.
</bodyText>
<figure confidence="0.999437954545454">
S
NP VP
I
John
SUBJ PRED john
NUM sg
PERS 3
PRED resign
TENSE past
ADJ {[PRED yesterday])
S
NP NP VP
I I I
Yesterday John V
I
resigned
SUBJ PRED john
NUM sg
PERS 3
PRED resign
TENSE past
ADJ {[PRED yesterday])
</figure>
<page confidence="0.993189">
105
</page>
<bodyText confidence="0.99991455">
Cahill et al. (2004) presents a set of Penn-II
Treebank-based LFG parsing resources. Their
approach distinguishes 32 types of dependencies,
including grammatical functions and
morphological information. This set can be divided
into two major groups: a group of predicate-only
dependencies and non-predicate dependencies.
Predicate-only dependencies are those whose path
ends in a predicate-value pair, describing
grammatical relations. For example, for the f-
structure in (1), predicate-only dependencies would
include: {subj(resign, john), adj(resign,
yesterday)}.
*ther predicate-only dependencies include:
apposition, complement, open complement,
coordination, determiner, object, second object,
oblique, second oblique, oblique agent, possessive,
quantifier, relative clause, topic, and relative
clause pronoun. The remaining non-predicate
dependencies are: adjectival degree, coordination
surface form, focus, complementizer forms: if,
whether, and that, modal, number, verbal particle,
participle, passive, person, pronoun surface form,
tense, and infinitival clause.
In parser evaluation, the quality of the f-
structures produced automatically can be checked
against a set of gold standard sentences annotated
with f-structures by a linguist. The evaluation is
conducted by calculating the precision and recall
between the set of dependencies produced by the
parser, and the set of dependencies derived from
the human-created f-structure. Usually, two
versions of f-score are calculated: one for all the
dependencies for a given input, and a separate one
for the subset of predicate-only dependencies.
In this paper, we use the parser developed by
Cahill et al. (2004), which automatically annotates
input text with c-structure trees and f-structure
dependencies, obtaining high precision and recall
rates. 3
</bodyText>
<sectionHeader confidence="0.999991" genericHeader="method">
3 Related work
</sectionHeader>
<subsectionHeader confidence="0.998198">
3.1 String-based metrics
</subsectionHeader>
<bodyText confidence="0.9742268">
The insensitivity of BLEU and NIST to perfectly
legitimate syntactic and lexical variation has been
raised, among others, in Callison-Burch et al.
(2006), but the criticism is widespread. Even the
3 A demo of the parser can be found at http://lfg-
demo.computing.dcu.ie/lfgparser.html
creators of BLEU point out that it may not
correlate particularly well with human judgment at
the sentence level (Papineni et al., 2002).
Recently a number of attempts to remedy these
shortcomings have led to the development of other
automatic MT evaluation metrics. Some of them
concentrate mainly on word order, like General
Text Matcher (Turian et al., 2003), which
calculates precision and recall for translation-
reference pairs, weighting contiguous matches
more than non-sequential matches, or Translation
Error Rate (Snover et al., 2006), which computes
the number of substitutions, insertions, deletions,
and shifts necessary to transform the translation
text to match the reference. *thers try to
accommodate both syntactic and lexical
differences between the candidate translation and
the reference, like CDER (Leusch et al., 2006),
which employs a version of edit distance for word
substitution and reordering; or METE*R
(Banerjee and Lavie, 2005), which uses stemming
and WordNet synonymy. Kauchak and Barzilay
(2006) and *wczarzak et al. (2006) use
paraphrases during BLEU and NIST evaluation to
increase the number of matches between the
translation and the reference; the paraphrases are
either taken from WordNet4 in Kauchak and
Barzilay (2006) or derived from the test set itself
through automatic word and phrase alignment in
*wczarzak et al. (2006). Another metric making
use of synonyms is the linear regression model
developed by Russo-Lassner et al. (2005), which
makes use of stemming, WordNet synonymy, verb
class synonymy, matching noun phrase heads, and
proper name matching. Kulesza and Shieber
(2004), on the other hand, train a Support Vector
Machine using features such as proportion of n-
gram matches and word error rate to judge a given
translation&apos;s distance from human-level quality.
</bodyText>
<subsectionHeader confidence="0.979513">
3.2 Dependency-based metric
</subsectionHeader>
<bodyText confidence="0.999794285714286">
The metrics described above use only string-based
comparisons, even while taking into consideration
reordering. By contrast, Liu and Gildea (2005)
present three metrics that use syntactic and
unlabelled dependency information. Two of these
metrics are based on matching syntactic subtrees
between the translation and the reference, and one
</bodyText>
<footnote confidence="0.886107">
4 http://wordnet.princeton.edu/
</footnote>
<page confidence="0.9952">
106
</page>
<bodyText confidence="0.999969695652174">
is based on matching headword chains, i.e.
sequences of words that correspond to a path in the
unlabelled dependency tree of the sentence.
Dependency trees are created by extracting a
headword for each node of the syntactic tree,
according to the rules used by the parser of Collins
(1999), where every subtree represents the
modifier information for its root headword. The
dependency trees for the translation and the
reference are converted into flat headword chains,
and the number of overlapping n-grams between
the translation and the reference chains is
calculated. *ur method, extending this line of
research with the use of labelled LFG
dependencies, partial matching, and n-best parses,
allows us to considerably outperform Liu and
Gildea&apos;s (2005) highest correlations with human
judgement (they report 0.144 for the correlation
with human fluency judgement, 0.202 for the
correlation with human overall judgement),
although it has to be kept in mind that such
comparison is only tentative, as their correlation is
calculated on a different test set.
</bodyText>
<sectionHeader confidence="0.985764" genericHeader="method">
4 LFG f-structure in MT evaluation
</sectionHeader>
<bodyText confidence="0.999971">
LFG-based automatic MT evaluation reflects the
same process that underlies the evaluation of
parser-produced f-structure quality against a gold
standard: we parse the translation and the
reference, and then, for each sentence, we check
the set of labelled translation dependencies against
the set of labelled reference dependencies,
counting the number of matches. As a result, we
obtain the precision and recall scores for the
translation, and we calculate the f-score for the
given pair.
</bodyText>
<subsectionHeader confidence="0.999356">
4.1 Determining parser noise
</subsectionHeader>
<bodyText confidence="0.9979756">
Because we are comparing two outputs that were
produced automatically, there is a possibility that
the result will not be noise-free, even if the parser
fails to provide a parse only in 0.1% of cases.
To assess the amount of noise that the parser
introduces, *wczarzak et al. (2006) conducted an
experiment where 100 English sentences were
hand-modified so that the position of adjuncts was
changed, but the sentence remained grammatical
and the meaning was not influenced. This way, an
ideal parser should give both the source and the
modified sentence the same f-structure, similarly to
the example presented in (1). The modified
sentences were treated like a translation file, and
the original sentences played the part of the
reference. Each set was run through the parser, and
the dependency triples obtained from the
&amp;quot;translation&amp;quot; were compared against the
dependency triples for the &amp;quot;reference&amp;quot;, calculating
the f-score. Additionally, the same &amp;quot;translation-
reference&amp;quot; set was scored with other metrics (TER,
METE*R, BLEU, NIST, and GTM). The results,
including the distinction between f-scores for all
dependencies and predicate-only dependencies,
appear in Table 1.
</bodyText>
<table confidence="0.999171">
baseline modified
TER 0.0 6.417
METEOR 1.0 0.9970
BLEU 1.0000 0.8725
NIST 11.5232 11.1704 (96.94%)
GTM 100 99.18
dep f-score 100 96.56
dep_preds f-score 100 94.13
</table>
<tableCaption confidence="0.998773">
Table 1. Scores for sentences with reordered adjuncts
</tableCaption>
<bodyText confidence="0.997304583333333">
The baseline column shows the upper bound for a
given metric: the score which a perfect translation,
word-for-word identical to the reference, would
obtain.5 The other column lists the scores that the
metrics gave to the &amp;quot;translation&amp;quot; containing
reordered adjunct. As can be seen, the dependency
and predicate-only dependency scores are lower
than the perfect 100, reflecting the noise
introduced by the parser.
We propose that the problem of parser
noise can be alleviated by introducing a number of
best parses into the comparison between the
translation and the reference. Table 2 shows how
increasing the number of parses available for
comparison brings our method closer to an ideal
noise-free parser.
5 Two things have to be noted here: (1) in the case of
NIST the perfect score differs from text to text, which is
why the percentage points are provided along the
numerical score, and (2) in the case of TER the lower
the score, the better the translation, so the perfect
translation will receive 0, and there is no upper bound
on the score, which makes this particular metric
extremely difficult to directly compare with others.
</bodyText>
<page confidence="0.995227">
107
</page>
<table confidence="0.997617333333333">
dependency f-score
1 best 96.56
2 best 97.31
5 best 97.90
10 best 98.31
20 best 98.59
30 best 98.74
50 best 98.79
baseline 100
</table>
<tableCaption confidence="0.9122355">
Table 2. Dependency f-scores for sentences with reordered
adjuncts with n-best parses available
</tableCaption>
<bodyText confidence="0.999835142857143">
It has to be noted, however, that increasing the
number of parses beyond a certain threshold does
little to further improve results, and at the same
time it considerably decreases the efficiency of the
method, so it is important to find the right balance
between these two factors. In our opinion, the
optimal value would be 10-best parses.
</bodyText>
<subsectionHeader confidence="0.9662735">
4.2 Correlation with human judgement —
MultiTrans
</subsectionHeader>
<subsubsectionHeader confidence="0.71247">
4.2.1 Experimental design
</subsubsectionHeader>
<bodyText confidence="0.99995605882353">
To evaluate the correlation with human
assessment, we used the data from the Linguistic
Data Consortium Multiple Translation Chinese
(MTC) Parts 2 and 4, which consists of multiple
translations of Chinese newswire text, four human-
produced references, and segment-level human
scores for a subset of the translation-reference
pairs. Although a single translated segment was
always evaluated by more than one judge, the
judges used a different reference every time, which
is why we treated each translation-reference-
human score triple as a separate segment. In effect,
the test set created from this data contained 16,800
segments. As in the previous experiment, the
translation was scored using BLEU, NIST, GTM,
TER, METE*R, and our labelled dependency-
based method.
</bodyText>
<subsubsectionHeader confidence="0.535028">
4.2.2 Labelled dependency-based method
</subsubsectionHeader>
<bodyText confidence="0.999967935483871">
We examined a number of modifications of the
dependency-based method in order to find out
which one gives the highest correlation with
human scores. The correlation differences between
immediate neighbours in the ranking were often
too small to be statistically significant; however,
there is a clear overall trend towards improvement.
Besides the plain version of the dependency f-
score, we also looked at the f-score calculated on
predicate dependencies only (ignoring &amp;quot;atomic&amp;quot;
features such as person, number, tense, etc.), which
turned out not to correlate well with human
judgements.
Another addition was the use of 2-, 10-, or 50-
best parses of the translation and reference
sentences, which partially neutralized parser noise
and resulted in increased correlations.
We also created a version where predicate
dependencies of the type subj(resign,John) are split
into two parts, each time replacing one of the
elements participating in the relation with a
variable, giving in effect subj(resign,x) and
subj(y,John). This lets us score partial matches,
where one correct lexical object happens to find
itself in the correct relation, but with an incorrect
&amp;quot;partner&amp;quot;.
Lastly, we added WordNet synonyms into the
matching process to accommodate lexical
variation, and to compare our WordNet-enhanced
method with the WordNet-enhanced version of
METE*R.
</bodyText>
<sectionHeader confidence="0.710891" genericHeader="evaluation">
4.2.3 Results
</sectionHeader>
<bodyText confidence="0.99998496">
We calculated Pearson&apos;s correlation coefficient for
segment-level scores that were given by each
metric and by human judges. The results of the
correlation are shown in Table 3. Note that the
correlation for TER is negative, because in TER
zero is the perfect score, in contrast to other
metrics where zero is the worst possible score;
however, this time the absolute values can be
easily compared to each other. Rows are ordered
by the highest value of the (absolute) correlation
with the human score.
First, it seems like none of the metrics is very
good at reflecting human fluency judgments; the
correlation values in the first column are
significantly lower than the correlation with
accuracy. This finding has been previously
reported, among others, in Liu and Gildea (2005).
However, the dependency-based method in almost
all its versions has decidedly the highest
correlation in this area. This can be explained by
the method&apos;s sensitivity to the grammatical
structure of the sentence: a more grammatical
translation is also a translation that is more fluent.
As to the correlation with human evaluation of
translation accuracy, our method currently falls
</bodyText>
<page confidence="0.998184">
108
</page>
<bodyText confidence="0.999941318181818">
short of METE*R. This is caused by the fact that
METE*R assign relatively little importance to the
position of a specific word in a sentence, therefore
rewarding the translation for content rather than
linguistic form. Interestingly, while METE*R,
with or without WordNet, considerably
outperforms all other metrics when it comes to the
correlation with human judgements of translation
accuracy, it falls well behind most versions of our
dependency-based method in correlation with
human scores of translation fluency.
Surprisingly, adding partial matching to the
dependency-based method resulted in the greatest
increase in correlation levels, to the extent that the
partial-match versions consistently outperformed
versions with a larger number of parses available
but without the partial match. The most interesting
effect was that the partial-match versions (even
those with just a single parse) offered results
comparable to or higher than the addition of
WordNet to the matching process when it comes to
accuracy and overall judgement.
</bodyText>
<sectionHeader confidence="0.901738" genericHeader="evaluation">
5 Current and future work
</sectionHeader>
<bodyText confidence="0.99948232">
Fluency and accuracy are two very different
aspects of translation quality, each with its own set
of conditions along which the input is evaluated.
Therefore, it seems unfair to expect a single
automatic metric to correlate highly with human
judgements of both at the same time. This pattern
is very noticeable in Table 3: if a metric is
(relatively) good at correlating with fluency, its
accuracy correlation suffers (GTM might serve as
an example here), and the opposite holds as well
(see METE*R&apos;s scores). It does not mean that any
improvement that increases the method&apos;s
correlation with one aspect will result in a decrease
in the correlation with the other aspect; but it does
suggest that a possible way of development would
be to target these correlations separately, if we
want our automated metrics to reflect human
scores better. At the same time, string-based
metrics might have already exhausted their
potential when it comes to increasing their
correlation with human evaluation; as has been
pointed out before, these metrics can only tell us
that two strings differ, but they cannot distinguish
legitimate grammatical variance from
ungrammatical variance. As the quality of MT
</bodyText>
<table confidence="0.999138444444444">
fluency accuracy average
d_50+WN 0.177 M+WN 0.294 M+WN 0.255
d+WN 0.175 M 0.278 d_50_var 0.252
d_50_var 0.174 d_50_var 0.273 d_50+WN 0.250
GTM 0.172 NIST 0.273 d_10_var 0.250
d_10_var 0.172 d_10_var 0.273 d_2_var 0.247
d_50 0.171 d_2_var 0.270 d+WN 0.244
d_2_var 0.168 d_50+WN 0.269 d_50 0.243
d_10 0.168 d_var 0.266 d_var 0.243
d_var 0.165 d_50 0.262 M 0.242
d_2 0.164 d_10 0.262 d_10 0.242
d 0.161 d+WN 0.260 NIST 0.238
BLEU 0.155 d_2 0.257 d_2 0.237
M+WN 0.153 d 0.256 d 0.235
M 0.149 d_pr 0.240 d_pr 0.216
NIST 0.146 GTM 0.203 GTM 0.208
d_pr 0.143 BLEU 0.199 BLEU 0.197
TER -0.133 TER -0.192 TER -0.182
</table>
<tableCaption confidence="0.94772625">
Table 3. Pearson&apos;s correlation between human scores and
evaluation metrics. Legend: d = dependency f-score, _pr =
predicate-only f-score, 2, 10, 50 = n-best parses; var =
partial-match version; M = METEOR, WN = WordNet6
</tableCaption>
<bodyText confidence="0.994068615384615">
improves, the community will need metrics that are
more sensitive in this respect. After all, the true
quality of MT depends on producing grammatical
output which describes the same concept as the
source utterance, and the string identity with a
reference is only a very selective approximation of
this goal.
6 In general terms, an increase of 0.022 or more between
any two scores in the same column is significant with a
95% confidence interval. The statistical significance of
correlation differences was calculated using Fisher&apos;s z&apos;
transformation and the general formula for confidence
interval.
</bodyText>
<page confidence="0.997421">
109
</page>
<bodyText confidence="0.9999816875">
In order to maximize the correlation with
human scores of fluency, we plan to look more
closely at the parser output, and implement some
basic transformations which would allow an even
deeper logical analysis of input (e.g. passive to
active voice transformation).
Additionally, we want to take advantage of
the fact that the score produced by the dependency-
based method is the proportional average of
matches for a group of up to 32 (but usually far
fewer) different dependency types. We plan to
implement a set of weights, one for each
dependency type, trained in such a way as to
maximize the correlation of the final dependency f-
score with human evaluation. In a preliminary
experiment, for example, assigning a low weight to
the topic dependency increases our correlations
slightly (this particular case can also be seen as a
transformation into a more basic logical form by
removing non-elementary dependency types).
In a similar direction, we want to
experiment more with the f-score calculations.
Initial check shows that assigning a higher weight
to recall than to precision improves results.
To improve the correlation with accuracy
judgements, we would like to experiment using a
paraphrase set derived from a large parallel corpus,
as described in *wczarzak et al. (2006). While
retaining the advantage of having a similar size to
a corresponding set of WordNet synonyms, this set
will also capture low-level syntactic variations,
which can increase the number of matches.
</bodyText>
<sectionHeader confidence="0.999696" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.997651041666667">
In this paper we present a linguistically-
motivated method for automatically evaluating the
output of Machine Translation. Most currently
used popular metrics rely on comparing translation
and reference on a string level. Even given
reordering, stemming, and synonyms for individual
words, current methods are still far from reaching
human ability to assess the quality of translation,
and there exists a need in the community to
develop more dependable metrics. *ur method
explores one such direction of development,
comparing the sentences on the level of their
grammatical structure, as exemplified by their f-
structure labelled dependency triples produced by
an LFG parser. In our experiments we showed that
the dependency-based method correlates higher
than any other metric with human evaluation of
translation fluency, and shows high correlation
with the average human score. The use of
dependencies in MT evaluation has not been
extensively researched before (one exception here
would be Liu and Gildea (2005)), and requires
more research to improve it, but the method shows
potential to become an accurate evaluation metric.
</bodyText>
<sectionHeader confidence="0.996402" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9994976">
This work was partly funded by Microsoft Ireland
PhD studentship 2006-8 for the first author of the
paper. We would also like to thank our reviewers
and Dan Melamed for their insightful comments.
All remaining errors are our own.
</bodyText>
<sectionHeader confidence="0.998081" genericHeader="references">
References
</sectionHeader>
<subsectionHeader confidence="0.739950666666667">
Satanjeev Banerjee and Alon Lavie. 2005. METE*R:
An Automatic Metric for MT Evaluation with
Improved Correlation with Human Judgments.
Proceedings of the Workshop on Intrinsic and
Extrinsic Evaluation Measures for MT and/or
Summarization at the Association for Computational
</subsectionHeader>
<reference confidence="0.99112928">
Linguistics Conference 2005: 65-73. Ann Arbor,
Michigan.
Joan Bresnan. 2001. Lexical-Functional Syntax,
Blackwell, *xford.
Aoife Cahill, Michael Burke, Ruth *&apos;Donovan, Josef
van Genabith, and Andy Way. 2004. Long-Distance
Dependency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approximations,
In Proceedings of Association for Computational
Linguistics 2004: 320-327. Barcelona, Spain.
Chris Callison-Burch, Miles *sborne and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in
Machine Translation Research. Proceedings of the
European Chapter of the Association for
Computational Linguistics 2006: 249-256. *slo,
Norway.
Michael J. Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
George Doddington. 2002. Automatic Evaluation of MT
Quality using N-gram Co-occurrence Statistics.
Proceedings of Human Language Technology
Conference 2002: 138-145. San Diego, California.
Kaplan, R. M., and J. Bresnan. 1982. Lexical-functional
Grammar: A Formal System for Grammatical
</reference>
<page confidence="0.982662">
110
</page>
<reference confidence="0.999859069444445">
Representation. In J. Bresnan (ed.), The Mental
Representation of Grammatical Relations. MIT
Press, Cambridge.
David Kauchak and Regina Barzilay. 2006.
Paraphrasing for Automatic Evaluation. Proceedings
of Human Language Technology — North American
Chapter of the Association for Computational
Linguistics Conference 2006: 45-462. New York,
New York.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation
models. Proceedings of the Workshop on Machine
Translation: From real users to research at the
Association for Machine Translation in the Americas
Conference 2004: 115-124. Washington, DC.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. Proceedings of MT
Summit 2005: 79-86. Phuket, Thailand.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the Conference on Theoretical and
Methodological Issues in Machine Translation 2004:
75-84. Baltimore, Maryland.
Gregor Leusch, Nicola Ueffing and Hermann Ney.
2006. CDER: Efficient MT Evaluation Using Block
Movements. Proceedings of European Chapter of the
Association for Computational Linguistics
Conference 2006: 241-248. Trento, Italy.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In
Proceedings of the Workshop on Intrinsic and
Extrinsic Evaluation Measures for Machine
Translation and/or Summarization at the Association
for Computational Linguistics Conference 2005. Ann
Arbor, Michigan.
Franz Josef *ch and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Modes.
Computational Linguistics, 29:19-51.
Karolina *wczarzak, Declan Groves, Josef van
Genabith, and Andy Way. 2006. Contextual Bitext-
Derived Paraphrases in Automatic MT Evaluation.
Proceedings of the Workshop on Statistical Machine
Translation at the Human Language Technology —
North American Chapter of the Association for
Computational Linguistics Conference 2006: 86-93.
New York, New York.
Kishore Papineni, Salim Roukos, Todd Ward, and
WeiJing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
Association for Computational Linguistics
Conference 2002: 311-318. Philadelphia,
Pennsylvania.
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.
2005. A Paraphrase-based Approach to Machine
Translation Evaluation. Technical Report LAMP-TR-
125/CS-TR-4754/UMIACS-TR-2005-57, University
of Maryland, College Park, Maryland.
Mathew Snover, Bonnie Dorr, Richard Schwartz, John
Makhoul, Linnea Micciula. 2006. A Study of
Translation Error Rate with Targeted Human
Annotation. Proceedings of the Association for
Machine Translation in the Americas Conference
2006: 223-231. Boston, Massachusetts.
Joseph P. Turian, Luke Shen, and I. Dan Melamed.
2003. Evaluation of Machine Translation and Its
Evaluation. Proceedings of MT Summit 2003: 386-
393. New *rleans, Luisiana.
Ying Zhang and Stephan Vogel. 2004. Measuring
confidence intervals for the machine translation
evaluation metrics. Proceedings of Conference on
Theoretical and Methodological Issues in Machine
Translation 2004: 85-94. Baltimore, Maryland.
</reference>
<page confidence="0.998798">
111
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968538">
<title confidence="0.999166">Labelled Dependencies in Machine Translation Evaluation</title>
<author confidence="0.999893">Karolina Owczarzak Josef van_Genabith Andy Wa</author>
<affiliation confidence="0.994501">National Centre for Language Technology School of Computing, Dublin City University</affiliation>
<address confidence="0.998122">Dublin 9, Ireland</address>
<email confidence="0.989361">owczarzak@computing.dcu.ie</email>
<email confidence="0.989361">josef@computing.dcu.ie</email>
<email confidence="0.989361">away@computing.dcu.ie</email>
<abstract confidence="0.999446625">We present a method for evaluating the quality of Machine Translation (MT) output, using labelled dependencies produced by a Lexical-Functional Grammar (LFG) parser. *ur dependencybased method, in contrast to most popular string-based evaluation metrics, does not unfairly penalize perfectly valid syntactic variations in the translation, and the addition of WordNet provides a way to accommodate lexical variation. In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<date>2005</date>
<pages>65--73</pages>
<institution>Linguistics Conference</institution>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1892" citStr="(2005)" startWordPosition="275" endWordPosition="275">translated sentence and one or more reference sentences, and are unable to accommodate any legitimate grammatical variation when it comes to lexical choices or syntactic structure of the translation, beyond what can be found in the multiple references. A natural next step in the field of evaluation was to introduce metrics that would better reflect our human judgement by accepting synonyms in the translated sentence or evaluating the translation on the basis of what syntactic features it shares with the reference. *ur method follows and substantially extends the earlier work of Liu and Gildea (2005), who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement. Dependencies abstract away from the particulars of the surface string (and syntactic tree) realization and provide a &amp;quot;normalized&amp;quot; representation of (some) syntactic variants of a given sentence. While Liu and Gildea (2005) calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency stru</context>
<context position="4814" citStr="(2005)" startWordPosition="710" endWordPosition="710">uman evaluation from the Linguistic Data Consortium&apos;s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METE*R (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment. Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of the experiment on the Multiple Translation data; Section 5 discusses ongoing work; Section 6 concludes. 2 Lexical-Functional Grammar In Lexical-Functional Grammar (Kaplan and Bresnan, 1982; Bresnan, 2001) sentence structure is represented in terms of c(onstituent)-structure and f(unctional)-structure. C-structure represents the word order of the surface string and the hierarchical organisation of phrases in ter</context>
<context position="11099" citStr="(2005)" startWordPosition="1615" endWordPosition="1615">it distance for word substitution and reordering; or METE*R (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. Kauchak and Barzilay (2006) and *wczarzak et al. (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet4 in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in *wczarzak et al. (2006). Another metric making use of synonyms is the linear regression model developed by Russo-Lassner et al. (2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. Kulesza and Shieber (2004), on the other hand, train a Support Vector Machine using features such as proportion of ngram matches and word error rate to judge a given translation&apos;s distance from human-level quality. 3.2 Dependency-based metric The metrics described above use only string-based comparisons, even while taking into consideration reordering. By contrast, Liu and Gildea (2005) present three metrics that use syntactic and unlabelled dependency information. Two of</context>
<context position="12598" citStr="(2005)" startWordPosition="1840" endWordPosition="1840">trees are created by extracting a headword for each node of the syntactic tree, according to the rules used by the parser of Collins (1999), where every subtree represents the modifier information for its root headword. The dependency trees for the translation and the reference are converted into flat headword chains, and the number of overlapping n-grams between the translation and the reference chains is calculated. *ur method, extending this line of research with the use of labelled LFG dependencies, partial matching, and n-best parses, allows us to considerably outperform Liu and Gildea&apos;s (2005) highest correlations with human judgement (they report 0.144 for the correlation with human fluency judgement, 0.202 for the correlation with human overall judgement), although it has to be kept in mind that such comparison is only tentative, as their correlation is calculated on a different test set. 4 LFG f-structure in MT evaluation LFG-based automatic MT evaluation reflects the same process that underlies the evaluation of parser-produced f-structure quality against a gold standard: we parse the translation and the reference, and then, for each sentence, we check the set of labelled trans</context>
<context position="19576" citStr="(2005)" startWordPosition="2934" endWordPosition="2934"> 3. Note that the correlation for TER is negative, because in TER zero is the perfect score, in contrast to other metrics where zero is the worst possible score; however, this time the absolute values can be easily compared to each other. Rows are ordered by the highest value of the (absolute) correlation with the human score. First, it seems like none of the metrics is very good at reflecting human fluency judgments; the correlation values in the first column are significantly lower than the correlation with accuracy. This finding has been previously reported, among others, in Liu and Gildea (2005). However, the dependency-based method in almost all its versions has decidedly the highest correlation in this area. This can be explained by the method&apos;s sensitivity to the grammatical structure of the sentence: a more grammatical translation is also a translation that is more fluent. As to the correlation with human evaluation of translation accuracy, our method currently falls 108 short of METE*R. This is caused by the fact that METE*R assign relatively little importance to the position of a specific word in a sentence, therefore rewarding the translation for content rather than linguistic</context>
<context position="26170" citStr="(2005)" startWordPosition="3973" endWordPosition="3973">in the community to develop more dependable metrics. *ur method explores one such direction of development, comparing the sentences on the level of their grammatical structure, as exemplified by their fstructure labelled dependency triples produced by an LFG parser. In our experiments we showed that the dependency-based method correlates higher than any other metric with human evaluation of translation fluency, and shows high correlation with the average human score. The use of dependencies in MT evaluation has not been extensively researched before (one exception here would be Liu and Gildea (2005)), and requires more research to improve it, but the method shows potential to become an accurate evaluation metric. Acknowledgements This work was partly funded by Microsoft Ireland PhD studentship 2006-8 for the first author of the paper. We would also like to thank our reviewers and Dan Melamed for their insightful comments. All remaining errors are our own. References Satanjeev Banerjee and Alon Lavie. 2005. METE*R: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or S</context>
</contexts>
<marker>2005</marker>
<rawString>Linguistics Conference 2005: 65-73. Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
</authors>
<title>Lexical-Functional Syntax,</title>
<date>2001</date>
<location>Blackwell, *xford.</location>
<contexts>
<context position="5204" citStr="Bresnan, 2001" startWordPosition="767" endWordPosition="768">nyms leads to a more accurate evaluation that correlates better with human judgment. Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of the experiment on the Multiple Translation data; Section 5 discusses ongoing work; Section 6 concludes. 2 Lexical-Functional Grammar In Lexical-Functional Grammar (Kaplan and Bresnan, 1982; Bresnan, 2001) sentence structure is represented in terms of c(onstituent)-structure and f(unctional)-structure. C-structure represents the word order of the surface string and the hierarchical organisation of phrases in terms of CFG trees. F-structures are recursive feature (or attribute-value) structures, representing abstract grammatical relations, such as subj(ect), obj(ect), obl(ique), adj(unct), etc., approximating to predicate-argument structure or simple logical forms. C-structure and f-structure are related in 1 e omit HTER (Human-Targeted Translation Error Rate), as it is not fully automatic and r</context>
</contexts>
<marker>Bresnan, 2001</marker>
<rawString>Joan Bresnan. 2001. Lexical-Functional Syntax, Blackwell, *xford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Michael Burke</author>
<author>Ruth &apos;Donovan</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Long-Distance Dependency Resolution in Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations,</title>
<date>2004</date>
<booktitle>In Proceedings of Association for Computational Linguistics</booktitle>
<pages>320--327</pages>
<location>Barcelona,</location>
<marker>Cahill, Burke, &apos;Donovan, van Genabith, Way, 2004</marker>
<rawString>Aoife Cahill, Michael Burke, Ruth *&apos;Donovan, Josef van Genabith, and Andy Way. 2004. Long-Distance Dependency Resolution in Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations, In Proceedings of Association for Computational Linguistics 2004: 320-327. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles sborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the role of BLEU</title>
<date>2006</date>
<booktitle>in Machine Translation Research. Proceedings of the European Chapter of the Association for Computational Linguistics</booktitle>
<pages>249--256</pages>
<contexts>
<context position="1188" citStr="Callison-Burch et al., 2006" startWordPosition="163" endWordPosition="166"> variations in the translation, and the addition of WordNet provides a way to accommodate lexical variation. In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores. 1 Introduction Since the creation of BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), the subject of automatic evaluation metrics for MT has been given quite a lot of attention. Although widely popular thanks to their speed and efficiency, both BLEU and NIST have been criticized for inadequate accuracy of evaluation at the segment level (Callison-Burch et al., 2006). As string based-metrics, they are limited to superficial comparison of word sequences between a translated sentence and one or more reference sentences, and are unable to accommodate any legitimate grammatical variation when it comes to lexical choices or syntactic structure of the translation, beyond what can be found in the multiple references. A natural next step in the field of evaluation was to introduce metrics that would better reflect our human judgement by accepting synonyms in the translated sentence or evaluating the translation on the basis of what syntactic features it shares wi</context>
<context position="9492" citStr="Callison-Burch et al. (2006)" startWordPosition="1369" endWordPosition="1372"> set of dependencies derived from the human-created f-structure. Usually, two versions of f-score are calculated: one for all the dependencies for a given input, and a separate one for the subset of predicate-only dependencies. In this paper, we use the parser developed by Cahill et al. (2004), which automatically annotates input text with c-structure trees and f-structure dependencies, obtaining high precision and recall rates. 3 3 Related work 3.1 String-based metrics The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al. (2006), but the criticism is widespread. Even the 3 A demo of the parser can be found at http://lfgdemo.computing.dcu.ie/lfgparser.html creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level (Papineni et al., 2002). Recently a number of attempts to remedy these shortcomings have led to the development of other automatic MT evaluation metrics. Some of them concentrate mainly on word order, like General Text Matcher (Turian et al., 2003), which calculates precision and recall for translationreference pairs, weighting contiguous matches more tha</context>
</contexts>
<marker>Callison-Burch, sborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles *sborne and Philipp Koehn. 2006. Re-evaluating the role of BLEU in Machine Translation Research. Proceedings of the European Chapter of the Association for Computational Linguistics 2006: 249-256. *slo, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="12131" citStr="Collins (1999)" startWordPosition="1770" endWordPosition="1771">omparisons, even while taking into consideration reordering. By contrast, Liu and Gildea (2005) present three metrics that use syntactic and unlabelled dependency information. Two of these metrics are based on matching syntactic subtrees between the translation and the reference, and one 4 http://wordnet.princeton.edu/ 106 is based on matching headword chains, i.e. sequences of words that correspond to a path in the unlabelled dependency tree of the sentence. Dependency trees are created by extracting a headword for each node of the syntactic tree, according to the rules used by the parser of Collins (1999), where every subtree represents the modifier information for its root headword. The dependency trees for the translation and the reference are converted into flat headword chains, and the number of overlapping n-grams between the translation and the reference chains is calculated. *ur method, extending this line of research with the use of labelled LFG dependencies, partial matching, and n-best parses, allows us to considerably outperform Liu and Gildea&apos;s (2005) highest correlations with human judgement (they report 0.144 for the correlation with human fluency judgement, 0.202 for the correla</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael J. Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic Evaluation of MT Quality using N-gram Co-occurrence Statistics.</title>
<date>2002</date>
<booktitle>Proceedings of Human Language Technology Conference</booktitle>
<pages>138--145</pages>
<location>San Diego, California.</location>
<contexts>
<context position="904" citStr="Doddington, 2002" startWordPosition="120" endWordPosition="121"> the quality of Machine Translation (MT) output, using labelled dependencies produced by a Lexical-Functional Grammar (LFG) parser. *ur dependencybased method, in contrast to most popular string-based evaluation metrics, does not unfairly penalize perfectly valid syntactic variations in the translation, and the addition of WordNet provides a way to accommodate lexical variation. In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores. 1 Introduction Since the creation of BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), the subject of automatic evaluation metrics for MT has been given quite a lot of attention. Although widely popular thanks to their speed and efficiency, both BLEU and NIST have been criticized for inadequate accuracy of evaluation at the segment level (Callison-Burch et al., 2006). As string based-metrics, they are limited to superficial comparison of word sequences between a translated sentence and one or more reference sentences, and are unable to accommodate any legitimate grammatical variation when it comes to lexical choices or syntactic structure of the translation, beyond what can be</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic Evaluation of MT Quality using N-gram Co-occurrence Statistics. Proceedings of Human Language Technology Conference 2002: 138-145. San Diego, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>J Bresnan</author>
</authors>
<title>Lexical-functional Grammar: A Formal System for Grammatical Representation.</title>
<date>1982</date>
<editor>In J. Bresnan (ed.),</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="5188" citStr="Kaplan and Bresnan, 1982" startWordPosition="763" endWordPosition="766"> representations with synonyms leads to a more accurate evaluation that correlates better with human judgment. Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of the experiment on the Multiple Translation data; Section 5 discusses ongoing work; Section 6 concludes. 2 Lexical-Functional Grammar In Lexical-Functional Grammar (Kaplan and Bresnan, 1982; Bresnan, 2001) sentence structure is represented in terms of c(onstituent)-structure and f(unctional)-structure. C-structure represents the word order of the surface string and the hierarchical organisation of phrases in terms of CFG trees. F-structures are recursive feature (or attribute-value) structures, representing abstract grammatical relations, such as subj(ect), obj(ect), obl(ique), adj(unct), etc., approximating to predicate-argument structure or simple logical forms. C-structure and f-structure are related in 1 e omit HTER (Human-Targeted Translation Error Rate), as it is not fully</context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>Kaplan, R. M., and J. Bresnan. 1982. Lexical-functional Grammar: A Formal System for Grammatical Representation. In J. Bresnan (ed.), The Mental Representation of Grammatical Relations. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for Automatic Evaluation.</title>
<date>2006</date>
<booktitle>Proceedings of Human Language Technology — North American Chapter of the Association for Computational Linguistics Conference</booktitle>
<pages>45--462</pages>
<location>New York, New York.</location>
<contexts>
<context position="3956" citStr="Kauchak and Barzilay (2006)" startWordPosition="578" endWordPosition="581">files are analyzed by a treebank-based, probabilistic LFG parser (Cahill et al., 2004), which produces a set of dependency triples for each input. The translation set is compared to the reference set, and the number of matches is calculated, giving the 104 Proceedings of the Second Workshop on Statistical Machine Translation, pages 104–111, Prague, June 2007. c�2007 Association for Computational Linguistics precision, recall, and f-score for each particular translation. In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium&apos;s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METE*R (Banerjee and Lavie, 2005), and we show that combining de</context>
<context position="10650" citStr="Kauchak and Barzilay (2006)" startWordPosition="1541" endWordPosition="1544">for translationreference pairs, weighting contiguous matches more than non-sequential matches, or Translation Error Rate (Snover et al., 2006), which computes the number of substitutions, insertions, deletions, and shifts necessary to transform the translation text to match the reference. *thers try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al., 2006), which employs a version of edit distance for word substitution and reordering; or METE*R (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. Kauchak and Barzilay (2006) and *wczarzak et al. (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet4 in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in *wczarzak et al. (2006). Another metric making use of synonyms is the linear regression model developed by Russo-Lassner et al. (2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. Kulesza and Shieber (2004),</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for Automatic Evaluation. Proceedings of Human Language Technology — North American Chapter of the Association for Computational Linguistics Conference 2006: 45-462. New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>Proceedings of the Workshop on Machine Translation: From real users to research at the Association for Machine Translation in the Americas Conference</booktitle>
<pages>115--124</pages>
<location>Washington, DC.</location>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. Proceedings of the Workshop on Machine Translation: From real users to research at the Association for Machine Translation in the Americas Conference 2004: 115-124. Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>Proceedings of MT Summit</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand.</location>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. Proceedings of MT Summit 2005: 79-86. Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Kulesza</author>
<author>Stuart M Shieber</author>
</authors>
<title>A learning approach to improving sentence-level MT evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Theoretical and Methodological Issues in Machine Translation</booktitle>
<pages>75--84</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="11249" citStr="Kulesza and Shieber (2004)" startWordPosition="1634" endWordPosition="1637">Kauchak and Barzilay (2006) and *wczarzak et al. (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet4 in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in *wczarzak et al. (2006). Another metric making use of synonyms is the linear regression model developed by Russo-Lassner et al. (2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. Kulesza and Shieber (2004), on the other hand, train a Support Vector Machine using features such as proportion of ngram matches and word error rate to judge a given translation&apos;s distance from human-level quality. 3.2 Dependency-based metric The metrics described above use only string-based comparisons, even while taking into consideration reordering. By contrast, Liu and Gildea (2005) present three metrics that use syntactic and unlabelled dependency information. Two of these metrics are based on matching syntactic subtrees between the translation and the reference, and one 4 http://wordnet.princeton.edu/ 106 is base</context>
</contexts>
<marker>Kulesza, Shieber, 2004</marker>
<rawString>Alex Kulesza and Stuart M. Shieber. 2004. A learning approach to improving sentence-level MT evaluation. In Proceedings of the Conference on Theoretical and Methodological Issues in Machine Translation 2004: 75-84. Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>CDER: Efficient MT Evaluation Using Block Movements.</title>
<date>2006</date>
<booktitle>Proceedings of European Chapter of the Association for Computational Linguistics Conference</booktitle>
<pages>241--248</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="10462" citStr="Leusch et al., 2006" startWordPosition="1513" endWordPosition="1516">ment of other automatic MT evaluation metrics. Some of them concentrate mainly on word order, like General Text Matcher (Turian et al., 2003), which calculates precision and recall for translationreference pairs, weighting contiguous matches more than non-sequential matches, or Translation Error Rate (Snover et al., 2006), which computes the number of substitutions, insertions, deletions, and shifts necessary to transform the translation text to match the reference. *thers try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al., 2006), which employs a version of edit distance for word substitution and reordering; or METE*R (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. Kauchak and Barzilay (2006) and *wczarzak et al. (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet4 in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in *wczarzak et al. (2006). Another metric making use of synonyms is the linear regression model dev</context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2006</marker>
<rawString>Gregor Leusch, Nicola Ueffing and Hermann Ney. 2006. CDER: Efficient MT Evaluation Using Block Movements. Proceedings of European Chapter of the Association for Computational Linguistics Conference 2006: 241-248. Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic Features for Evaluation of Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization at the Association for Computational Linguistics Conference</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1892" citStr="Liu and Gildea (2005)" startWordPosition="272" endWordPosition="275">nces between a translated sentence and one or more reference sentences, and are unable to accommodate any legitimate grammatical variation when it comes to lexical choices or syntactic structure of the translation, beyond what can be found in the multiple references. A natural next step in the field of evaluation was to introduce metrics that would better reflect our human judgement by accepting synonyms in the translated sentence or evaluating the translation on the basis of what syntactic features it shares with the reference. *ur method follows and substantially extends the earlier work of Liu and Gildea (2005), who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement. Dependencies abstract away from the particulars of the surface string (and syntactic tree) realization and provide a &amp;quot;normalized&amp;quot; representation of (some) syntactic variants of a given sentence. While Liu and Gildea (2005) calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency stru</context>
<context position="4814" citStr="Liu and Gildea (2005)" startWordPosition="707" endWordPosition="710">segment-level human evaluation from the Linguistic Data Consortium&apos;s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METE*R (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment. Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of the experiment on the Multiple Translation data; Section 5 discusses ongoing work; Section 6 concludes. 2 Lexical-Functional Grammar In Lexical-Functional Grammar (Kaplan and Bresnan, 1982; Bresnan, 2001) sentence structure is represented in terms of c(onstituent)-structure and f(unctional)-structure. C-structure represents the word order of the surface string and the hierarchical organisation of phrases in ter</context>
<context position="11612" citStr="Liu and Gildea (2005)" startWordPosition="1688" endWordPosition="1691">nother metric making use of synonyms is the linear regression model developed by Russo-Lassner et al. (2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. Kulesza and Shieber (2004), on the other hand, train a Support Vector Machine using features such as proportion of ngram matches and word error rate to judge a given translation&apos;s distance from human-level quality. 3.2 Dependency-based metric The metrics described above use only string-based comparisons, even while taking into consideration reordering. By contrast, Liu and Gildea (2005) present three metrics that use syntactic and unlabelled dependency information. Two of these metrics are based on matching syntactic subtrees between the translation and the reference, and one 4 http://wordnet.princeton.edu/ 106 is based on matching headword chains, i.e. sequences of words that correspond to a path in the unlabelled dependency tree of the sentence. Dependency trees are created by extracting a headword for each node of the syntactic tree, according to the rules used by the parser of Collins (1999), where every subtree represents the modifier information for its root headword. </context>
<context position="19576" citStr="Liu and Gildea (2005)" startWordPosition="2931" endWordPosition="2934"> shown in Table 3. Note that the correlation for TER is negative, because in TER zero is the perfect score, in contrast to other metrics where zero is the worst possible score; however, this time the absolute values can be easily compared to each other. Rows are ordered by the highest value of the (absolute) correlation with the human score. First, it seems like none of the metrics is very good at reflecting human fluency judgments; the correlation values in the first column are significantly lower than the correlation with accuracy. This finding has been previously reported, among others, in Liu and Gildea (2005). However, the dependency-based method in almost all its versions has decidedly the highest correlation in this area. This can be explained by the method&apos;s sensitivity to the grammatical structure of the sentence: a more grammatical translation is also a translation that is more fluent. As to the correlation with human evaluation of translation accuracy, our method currently falls 108 short of METE*R. This is caused by the fact that METE*R assign relatively little importance to the position of a specific word in a sentence, therefore rewarding the translation for content rather than linguistic</context>
<context position="26170" citStr="Liu and Gildea (2005)" startWordPosition="3970" endWordPosition="3973"> exists a need in the community to develop more dependable metrics. *ur method explores one such direction of development, comparing the sentences on the level of their grammatical structure, as exemplified by their fstructure labelled dependency triples produced by an LFG parser. In our experiments we showed that the dependency-based method correlates higher than any other metric with human evaluation of translation fluency, and shows high correlation with the average human score. The use of dependencies in MT evaluation has not been extensively researched before (one exception here would be Liu and Gildea (2005)), and requires more research to improve it, but the method shows potential to become an accurate evaluation metric. Acknowledgements This work was partly funded by Microsoft Ireland PhD studentship 2006-8 for the first author of the paper. We would also like to thank our reviewers and Dan Melamed for their insightful comments. All remaining errors are our own. References Satanjeev Banerjee and Alon Lavie. 2005. METE*R: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or S</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic Features for Evaluation of Machine Translation. In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization at the Association for Computational Linguistics Conference 2005. Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef ch</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Modes.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--19</pages>
<marker>ch, Ney, 2003</marker>
<rawString>Franz Josef *ch and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Modes. Computational Linguistics, 29:19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina wczarzak</author>
<author>Declan Groves</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Contextual BitextDerived Paraphrases in Automatic MT Evaluation.</title>
<date>2006</date>
<booktitle>Proceedings of the Workshop on Statistical Machine Translation at the Human Language Technology — North American Chapter of the Association for Computational Linguistics Conference</booktitle>
<pages>86--93</pages>
<location>New York, New York.</location>
<marker>wczarzak, Groves, van Genabith, Way, 2006</marker>
<rawString>Karolina *wczarzak, Declan Groves, Josef van Genabith, and Andy Way. 2006. Contextual BitextDerived Paraphrases in Automatic MT Evaluation. Proceedings of the Workshop on Statistical Machine Translation at the Human Language Technology — North American Chapter of the Association for Computational Linguistics Conference 2006: 86-93. New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of Association for Computational Linguistics Conference</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="876" citStr="Papineni et al., 2002" startWordPosition="114" endWordPosition="117">e present a method for evaluating the quality of Machine Translation (MT) output, using labelled dependencies produced by a Lexical-Functional Grammar (LFG) parser. *ur dependencybased method, in contrast to most popular string-based evaluation metrics, does not unfairly penalize perfectly valid syntactic variations in the translation, and the addition of WordNet provides a way to accommodate lexical variation. In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores. 1 Introduction Since the creation of BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), the subject of automatic evaluation metrics for MT has been given quite a lot of attention. Although widely popular thanks to their speed and efficiency, both BLEU and NIST have been criticized for inadequate accuracy of evaluation at the segment level (Callison-Burch et al., 2006). As string based-metrics, they are limited to superficial comparison of word sequences between a translated sentence and one or more reference sentences, and are unable to accommodate any legitimate grammatical variation when it comes to lexical choices or syntactic structure of the tra</context>
<context position="9758" citStr="Papineni et al., 2002" startWordPosition="1411" endWordPosition="1414">y Cahill et al. (2004), which automatically annotates input text with c-structure trees and f-structure dependencies, obtaining high precision and recall rates. 3 3 Related work 3.1 String-based metrics The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al. (2006), but the criticism is widespread. Even the 3 A demo of the parser can be found at http://lfgdemo.computing.dcu.ie/lfgparser.html creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level (Papineni et al., 2002). Recently a number of attempts to remedy these shortcomings have led to the development of other automatic MT evaluation metrics. Some of them concentrate mainly on word order, like General Text Matcher (Turian et al., 2003), which calculates precision and recall for translationreference pairs, weighting contiguous matches more than non-sequential matches, or Translation Error Rate (Snover et al., 2006), which computes the number of substitutions, insertions, deletions, and shifts necessary to transform the translation text to match the reference. *thers try to accommodate both syntactic and </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of Association for Computational Linguistics Conference 2002: 311-318. Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grazia Russo-Lassner</author>
<author>Jimmy Lin</author>
<author>Philip Resnik</author>
</authors>
<title>A Paraphrase-based Approach to Machine Translation Evaluation.</title>
<date>2005</date>
<tech>Technical Report LAMP-TR125/CS-TR-4754/UMIACS-TR-2005-57,</tech>
<institution>University of Maryland, College Park,</institution>
<location>Maryland.</location>
<contexts>
<context position="11099" citStr="Russo-Lassner et al. (2005)" startWordPosition="1612" endWordPosition="1615">ploys a version of edit distance for word substitution and reordering; or METE*R (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. Kauchak and Barzilay (2006) and *wczarzak et al. (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet4 in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in *wczarzak et al. (2006). Another metric making use of synonyms is the linear regression model developed by Russo-Lassner et al. (2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. Kulesza and Shieber (2004), on the other hand, train a Support Vector Machine using features such as proportion of ngram matches and word error rate to judge a given translation&apos;s distance from human-level quality. 3.2 Dependency-based metric The metrics described above use only string-based comparisons, even while taking into consideration reordering. By contrast, Liu and Gildea (2005) present three metrics that use syntactic and unlabelled dependency information. Two of</context>
</contexts>
<marker>Russo-Lassner, Lin, Resnik, 2005</marker>
<rawString>Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik. 2005. A Paraphrase-based Approach to Machine Translation Evaluation. Technical Report LAMP-TR125/CS-TR-4754/UMIACS-TR-2005-57, University of Maryland, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
<author>Linnea Micciula</author>
</authors>
<title>A Study of Translation Error Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>Proceedings of the Association for Machine Translation in the Americas Conference</booktitle>
<pages>223--231</pages>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="4485" citStr="Snover et al., 2006" startWordPosition="658" endWordPosition="661">fferences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium&apos;s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METE*R (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment. Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of the experiment on the Multiple Translation data; Section 5 discusses ongoing work; Se</context>
<context position="10165" citStr="Snover et al., 2006" startWordPosition="1471" endWordPosition="1474">of the parser can be found at http://lfgdemo.computing.dcu.ie/lfgparser.html creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level (Papineni et al., 2002). Recently a number of attempts to remedy these shortcomings have led to the development of other automatic MT evaluation metrics. Some of them concentrate mainly on word order, like General Text Matcher (Turian et al., 2003), which calculates precision and recall for translationreference pairs, weighting contiguous matches more than non-sequential matches, or Translation Error Rate (Snover et al., 2006), which computes the number of substitutions, insertions, deletions, and shifts necessary to transform the translation text to match the reference. *thers try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al., 2006), which employs a version of edit distance for word substitution and reordering; or METE*R (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. Kauchak and Barzilay (2006) and *wczarzak et al. (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches betw</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Makhoul, Micciula, 2006</marker>
<rawString>Mathew Snover, Bonnie Dorr, Richard Schwartz, John Makhoul, Linnea Micciula. 2006. A Study of Translation Error Rate with Targeted Human Annotation. Proceedings of the Association for Machine Translation in the Americas Conference 2006: 223-231. Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph P Turian</author>
<author>Luke Shen</author>
<author>I Dan Melamed</author>
</authors>
<title>Evaluation of Machine Translation and Its Evaluation.</title>
<date>2003</date>
<booktitle>Proceedings of MT Summit</booktitle>
<pages>386--393</pages>
<location>New *rleans, Luisiana.</location>
<contexts>
<context position="4433" citStr="Turian et al., 2003" startWordPosition="650" endWordPosition="653">on, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium&apos;s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METE*R (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment. Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of the experiment on the Multiple Tr</context>
<context position="9983" citStr="Turian et al., 2003" startWordPosition="1447" endWordPosition="1450">U and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al. (2006), but the criticism is widespread. Even the 3 A demo of the parser can be found at http://lfgdemo.computing.dcu.ie/lfgparser.html creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level (Papineni et al., 2002). Recently a number of attempts to remedy these shortcomings have led to the development of other automatic MT evaluation metrics. Some of them concentrate mainly on word order, like General Text Matcher (Turian et al., 2003), which calculates precision and recall for translationreference pairs, weighting contiguous matches more than non-sequential matches, or Translation Error Rate (Snover et al., 2006), which computes the number of substitutions, insertions, deletions, and shifts necessary to transform the translation text to match the reference. *thers try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al., 2006), which employs a version of edit distance for word substitution and reordering; or METE*R (Banerjee and Lavie, 2005), wh</context>
</contexts>
<marker>Turian, Shen, Melamed, 2003</marker>
<rawString>Joseph P. Turian, Luke Shen, and I. Dan Melamed. 2003. Evaluation of Machine Translation and Its Evaluation. Proceedings of MT Summit 2003: 386-393. New *rleans, Luisiana.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
</authors>
<title>Measuring confidence intervals for the machine translation evaluation metrics.</title>
<date>2004</date>
<booktitle>Proceedings of Conference on Theoretical and Methodological Issues in Machine Translation</booktitle>
<pages>85--94</pages>
<location>Baltimore, Maryland.</location>
<marker>Zhang, Vogel, 2004</marker>
<rawString>Ying Zhang and Stephan Vogel. 2004. Measuring confidence intervals for the machine translation evaluation metrics. Proceedings of Conference on Theoretical and Methodological Issues in Machine Translation 2004: 85-94. Baltimore, Maryland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>