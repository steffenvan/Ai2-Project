<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.024042">
<title confidence="0.995811">
Natural Language Models for Predicting Programming Comments
</title>
<author confidence="0.998785">
Dana Movshovitz-Attias William W. Cohen
</author>
<affiliation confidence="0.9940895">
Computer Science Department Computer Science Department
Carnegie Mellon University Carnegie Mellon University
</affiliation>
<email confidence="0.996612">
dma@cs.cmu.edu wcohen@cs.cmu.edu
</email>
<sectionHeader confidence="0.993832" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999892631578947">
Statistical language models have success-
fully been used to describe and analyze
natural language documents. Recent work
applying language models to program-
ming languages is focused on the task
of predicting code, while mainly ignoring
the prediction of programmer comments.
In this work, we predict comments from
JAVA source files of open source projects,
using topic models and n-grams, and we
analyze the performance of the models
given varying amounts of background data
on the project being predicted. We evalu-
ate models on their comment-completion
capability in a setting similar to code-
completion tools built into standard code
editors, and show that using a comment
completion tool can save up to 47% of the
comment typing.
</bodyText>
<sectionHeader confidence="0.977662" genericHeader="categories and subject descriptors">
1 Introduction and Related Work
</sectionHeader>
<bodyText confidence="0.999969288135593">
Statistical language models have traditionally
been used to describe and analyze natural lan-
guage documents. Recently, software engineer-
ing researchers have adopted the use of language
models for modeling software code. Hindle et al.
(2012) observe that, as code is created by humans
it is likely to be repetitive and predictable, similar
to natural language. NLP models have thus been
used for a variety of software development tasks
such as code token completion (Han et al., 2009;
Jacob and Tairas, 2010), analysis of names in code
(Lawrie et al., 2006; Binkley et al., 2011) and min-
ing software repositories (Gabel and Su, 2008).
An important part of software programming and
maintenance lies in documentation, which may
come in the form of tutorials describing the code,
or inline comments provided by the programmer.
The documentation provides a high level descrip-
tion of the task performed by the code, and may
include examples of use-cases for specific code
segments or identifiers such as classes, methods
and variables. Well documented code is easier to
read and maintain in the long-run but writing com-
ments is a laborious task that is often overlooked
or at least postponed by many programmers.
Code commenting not only provides a summa-
rization of the conceptual idea behind the code
(Sridhara et al., 2010), but can also be viewed as a
form of document expansion where the comment
contains significant terms relevant to the described
code. Accurately predicted comment words can
therefore be used for a variety of linguistic uses
including improved search over code bases using
natural language queries, code categorization, and
locating parts of the code that are relevant to a spe-
cific topic or idea (Tseng and Juang, 2003; Wan et
al., 2007; Kumar and Carterette, 2013; Shepherd
et al., 2007; Rastkar et al., 2011). A related and
well studied NLP task is that of predicting natural
language caption and commentary for images and
videos (Blei and Jordan, 2003; Feng and Lapata,
2010; Feng and Lapata, 2013; Wu and Li, 2011).
In this work, our goal is to apply statistical lan-
guage models for predicting class comments. We
show that n-gram models are extremely success-
ful in this task, and can lead to a saving of up
to 47% in comment typing. This is expected as
n-grams have been shown as a strong model for
language and speech prediction that is hard to im-
prove upon (Rosenfeld, 2000). In some cases how-
ever, for example in a document expansion task,
we wish to extract important terms relevant to the
code regardless of local syntactic dependencies.
We hence also evaluate the use of LDA (Blei et al.,
2003) and link-LDA (Erosheva et al., 2004) topic
models, which are more relevant for the term ex-
traction scenario. We find that the topic model per-
formance can be improved by distinguishing code
and text tokens in the code.
</bodyText>
<page confidence="0.992774">
35
</page>
<note confidence="0.527856">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 35–40,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.929474" genericHeader="method">
2 Method
</sectionHeader>
<subsectionHeader confidence="0.977629">
2.1 Models
</subsectionHeader>
<bodyText confidence="0.973908137931035">
We train n-gram models (n = 1, 2,3) over source
code documents containing sequences of com-
bined code and text tokens from multiple training
datasets (described below). We use the Berkeley
Language Model package (Pauls and Klein, 2011)
with absolute discounting (Kneser-Ney smooth-
ing; (1995)) which includes a backoff strategy to
lower-order n-grams. Next, we use LDA topic
models (Blei et al., 2003) trained on the same data,
with 1, 5, 10 and 20 topics. The joint distribution
of a topic mixture θ, and a set of N topics z, for
a single source code document with N observed
word tokens, d = {wi}N i=1, given the Dirichlet pa-
rameters α and β, is therefore
Under the models described so far, there is no dis-
tinction between text and code tokens.
Finally, we consider documents as having a
mixed membership of two entity types, code and
text tokens, d = ({wZode}Cn1, {wtext
i }Tn
i=1), where
the text words are tokens from comment and
string literals, and the code words include the pro-
gramming language syntax tokens (e.g., public,
private, for, etc’ ) and all identifiers. In this
case, we train link-LDA models (Erosheva et al.,
2004) with 1, 5, 10 and 20 topics. Under the link-
LDA model, the mixed-membership joint distribu-
tion of a topic mixture, words and topics is then
</bodyText>
<equation confidence="0.883703">
p(θ, z, w|α, β) = p(θ|α)· (2)
</equation>
<subsectionHeader confidence="0.999433">
2.2 Testing Methodology
</subsectionHeader>
<bodyText confidence="0.9999785">
Our goal is to predict the tokens of the JAVA class
comment (the one preceding the class definition)
in each of the test files. Each of the models de-
scribed above assigns a probability to the next
comment token. In the case of n-grams, the prob-
ability of a token word wi is given by considering
previous words p(wi|wi−1, ... , w0). This proba-
bility is estimated given the previous n − 1 tokens
as p(wi|wi−1, ... , wi−(n−1)).
For the topic models, we separate the docu-
ment tokens into the class definition and the com-
ment we wish to predict. The set of tokens of
the class comment wc, are all considered as text
tokens. The rest of the tokens in the document
wr, are considered to be the class definition, and
they may contain both code and text tokens (from
string literals and other comments in the source
file). We then compute the posterior probability
of document topics by solving the following infer-
ence problem conditioned on the wr tokens
</bodyText>
<equation confidence="0.993542">
p(θ, zr|wr, α, β) = p(θ, zr, wr|α, β) (3)
p(wr|α,β)
</equation>
<bodyText confidence="0.995474666666667">
This gives us an estimate of the document distri-
bution, θ, with which we infer the probability of
the comment tokens as
</bodyText>
<equation confidence="0.9365355">
p(wc|θ, β) = E p(wc|z, β)p(z|θ) (4)
z
</equation>
<bodyText confidence="0.999945125">
Following Blei et al. (2003), for the case
of a single entity LDA, the inference problem
from equation (3) can be solved by considering
p(θ, z, w|α,β), as in equation (1), and by taking
the marginal distribution of the document tokens
as a continuous mixture distribution for the set
w = wr, by integrating over θ and summing over
the set of topics z
</bodyText>
<equation confidence="0.994008">
p(θ, z, w|α,β) = (1)
p(θ|α) ri p(z|θ)p(w|z,β)
w
ri p(ztext|θ)p(wtext|ztext, β)· fp(w|α,β) = p(θ|α)· (5)
wtext
ri p(zcode|θ)p(wcode|zcode, β) riE �p(z|θ)p(w|z, β) dθ
wcode w z
</equation>
<bodyText confidence="0.999988833333333">
where θ is the joint topic distribution, w is the set
of observed document words, ztext is a topic asso-
ciated with a text word, and zcode a topic associ-
ated with a code word.
The LDA and link-LDA models use Gibbs sam-
pling (Griffiths and Steyvers, 2004) for topic infer-
ence, based on the implementation of Balasubra-
manyan and Cohen (2011) with single or multiple
entities per document, respectively.
For the case of link-LDA where the document is
comprised of two entities, in our case code to-
kens and text tokens, we can consider the mixed-
membership joint distribution θ, as in equation (2),
and similarly the marginal distribution p(w|α, β)
over both code and text tokens from wr. Since
comment words in wc are all considered as text
tokens they are sampled using text topics, namely
ztext, in equation (4).
</bodyText>
<page confidence="0.982377">
36
</page>
<listItem confidence="0.514589333333333">
3 Experimental Settings the Apache Tika toolkit1 and then tokenized with
the Mallet package. We considered as raw code
3.1 Data and Training Methodology tokens anything labeled using a &lt;code&gt; markup
</listItem>
<bodyText confidence="0.999405520833334">
(as indicated by the SO users who wrote the post).
We use source code from nine open source JAVA
projects: Ant, Cassandra, Log4j, Maven, Minor-
Third, Batik, Lucene, Xalan and Xerces. For each
project, we divide the source files into a training
and testing dataset. Then, for each project in turn,
we consider the following three main training sce-
narios, leading to using three training datasets.
To emulate a scenario in which we are predict-
ing comments in the middle of project develop-
ment, we can use data (documented code) from the
same project. In this case, we use the in-project
training dataset (IN). Alternatively, if we train a
comment prediction model at the beginning of the
development, we need to use source files from
other, possibly related projects. To analyze this
scenario, for each of the projects above we train
models using an out-of-project dataset (OUT) con-
taining data from the other eight projects.
Typically, source code files contain a greater
amount of code versus comment text. Since we are
interested in predicting comments, we consider a
third training data source which contains more En-
glish text as well as some code segments. We use
data from the popular Q&amp;A website StackOver-
flow (SO) where users ask and answer technical
questions about software development, tools, al-
gorithms, etc’. We downloaded a dataset of all ac-
tions performed on the site since it was launched in
August 2008 until August 2012. The data includes
3,453,742 questions and 6,858,133 answers posted
by 1,295,620 users. We used only posts that are
tagged as JAVA related questions and answers.
All the models for each project are then tested
on the testing set of that project. We report results
averaged over all projects in Table 1.
Source files were tokenized using the Eclipse
JDT compiler tools, separating code tokens and
identifiers. Identifier names (of classes, methods
and variables), were further tokenized by camel
case notation (e.g., ’minMargin’ was converted to
’min margin’). Non alpha-numeric tokens (e.g.,
dot, semicolon) were discarded from the code, as
well as numeric and single character literals. Text
from comments or any string literals within the
code were further tokenized with the Mallet sta-
tistical natural language processing package (Mc-
Callum, 2002). Posts from SO were parsed using
</bodyText>
<subsectionHeader confidence="0.991702">
3.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.99999612">
Since our models are trained using various data
sources the vocabularies used by each of them are
different, making the comment likelihood given by
each model incomparable due to different sets of
out-of-vocabulary tokens. We thus evaluate mod-
els using a character saving metric which aims at
quantifying the percentage of characters that can
be saved by using the model in a word-completion
settings, similar to standard code completion tools
built into code editors. For a comment word with
n characters, w = wi, ... , wn, we predict the two
most likely words given each model filtered by the
first 0, ... , n characters of w. Let k be the minimal
ki for which w is in the top two predicted word to-
kens where tokens are filtered by the first ki char-
acters. Then, the number of saved characters for w
is n − k. In Table 1 we report the average percent-
age of saved characters per comment using each of
the above models. The final results are also aver-
aged over the nine input projects. As an example,
in the predicted comment shown in Table 2, taken
from the project Minor-Third, the token entity is
the most likely token according to the model SO
trigram, out of tokens starting with the prefix ’en’.
The saved characters in this case are ’tity’.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9996165625">
Table 1 displays the average percentage of char-
acters saved per class comment using each of the
models. Models trained on in-project data (IN)
perform significantly better than those trained on
another data source, regardless of the model type,
with an average saving of 47.1% characters using
a trigram model. This is expected, as files from
the same project are likely to contain similar com-
ments, and identifier names that appear in the com-
ment of one class may appear in the code of an-
other class in the same project. Clearly, in-project
data should be used when available as it improves
comment prediction leading to an average increase
of between 6% for the worst model (26.6 for OUT
unigram versus 33.05 for IN) and 14% for the best
(32.96 for OUT trigram versus 47.1 for IN).
</bodyText>
<footnote confidence="0.989304">
1http://tika.apache.org/
</footnote>
<page confidence="0.998427">
37
</page>
<table confidence="0.999641888888889">
Model n-gram LDA Link-LDA
n / topics
1 2 3 20 10 5 1 20 10 5 1
IN 33.05 43.27 47.1 34.20 33.93 33.63 33.05 35.76 35.81 35.37 34.59
(3.62) (5.79) (6.87) (3.63) (3.67) (3.67) (3.62) (3.95) (4.12) (3.98) (3.92)
OUT 26.6 31.52 32.96 26.79 26.8 26.86 26.6 28.03 28 28 27.82
(3.37) (4.17) (4.33) (3.26) (3.36) (3.44) (3.37) (3.60) (3.56) (3.67) (3.62)
SO 27.8 33.29 34.56 27.25 27.22 27.34 27.8 28.08 28.12 27.94 27.9
(3.51) (4.40) (4.78) (3.67) (3.44) (3.55) (3.51) (3.48) (3.58) (3.56) (3.45)
</table>
<tableCaption confidence="0.960975333333333">
Table 1: Average percentage of characters saved per comment using n-gram, LDA and link-LDA models
trained on three training sets: IN, OUT, and SO. The results are averaged over nine JAVA projects (with
standard deviations in parenthesis).
</tableCaption>
<table confidence="0.9972732">
Model Predicted Comment Dataset n-gram link-LDA
IN trigram “Train a named-entity extractor“ IN 2778.35 574.34
IN link-LDA “Train a named-entity extractor“ OUT 1865.67 670.34
OUT trigram “Train a named-entity extractor“ SO 1898.43 638.55
SO trigram “Train a named-entity extractor“
</table>
<tableCaption confidence="0.9999525">
Table 2: Sample comment from the Minor-Third
project predicted using IN, OUT and SO based
models. Saved characters are underlined.
Table 3: Average words per project for which each
</tableCaption>
<bodyText confidence="0.994355377358491">
tested model completes the word better than the
other. This indicates that each of the models is bet-
ter at predicting a different set of comment words.
Of the out-of-project data sources, models us-
ing a greater amount of text (SO) mostly out-
performed models based on more code (OUT).
This increase in performance, however, comes at
a cost of greater run-time due to the larger word
dictionary associated with the SO data. Note that
in the scope of this work we did not investigate the
contribution of each of the background projects
used in OUT, and how their relevance to the tar-
get prediction project effects their performance.
The trigram model shows the best performance
across all training data sources (47% for IN, 32%
for OUT and 34% for SO). Amongst the tested
topic models, link-LDA models which distinguish
code and text tokens perform consistently better
than simple LDA models in which all tokens are
considered as text. We did not however find a
correlation between the number of latent topics
learned by a topic model and its performance. In
fact, for each of the data sources, a different num-
ber of topics gave the optimal character saving re-
sults.
Note that in this work, all topic models are
based on unigram tokens, therefore their results
are most comparable with that of the unigram in
Table 1, which does not benefit from the back-
off strategy used by the bigram and trigram mod-
els. By this comparison, the link-LDA topic model
proves more successful in the comment prediction
task than the simpler models which do not distin-
guish code and text tokens. Using n-grams without
backoff leads to results significantly worse than
any of the presented models (not shown).
Table 2 shows a sample comment segment for
which words were predicted using trigram models
from all training sources and an in-project link-
LDA. The comment is taken from the TrainEx-
tractor class in the Minor-Third project, a ma-
chine learning library for annotating and catego-
rizing text. Both IN models show a clear advan-
tage in completing the project-specific word Train,
compared to models based on out-of-project data
(OUT and SO). Interestingly, in this example the
trigram is better at completing the term named-
entity given the prefix named. However, the topic
model is better at completing the word extractor
which refers to the target class. This example indi-
cates that each model type may be more successful
in predicting different comment words, and that
combining multiple models may be advantageous.
</bodyText>
<page confidence="0.997966">
38
</page>
<bodyText confidence="0.999700375">
This can also be seen by the analysis in Table 3
where we compare the average number of words
completed better by either the best n-gram or topic
model given each training dataset. Again, while
n-grams generally complete more words better, a
considerable portion of the words is better com-
pleted using a topic model, further motivating a
hybrid solution.
</bodyText>
<sectionHeader confidence="0.995959" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999808">
We analyze the use of language models for pre-
dicting class comments for source file documents
containing a mixture of code and text tokens. Our
experiments demonstrate the effectiveness of us-
ing language models for comment completion,
showing a saving of up to 47% of the comment
characters. When available, using in-project train-
ing data proves significantly more successful than
using out-of-project data. However, we find that
when using out-of-project data, a dataset based on
more words than code performs consistently bet-
ter. The results also show that different models
are better at predicting different comment words,
which motivates a hybrid solution combining the
advantages of multiple models.
</bodyText>
<sectionHeader confidence="0.997559" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9548995">
This research was supported by the NSF under
grant CCF-1247088.
</bodyText>
<sectionHeader confidence="0.998419" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997959794520548">
Ramnath Balasubramanyan and William W Cohen.
2011. Block-lda: Jointly modeling entity-annotated
text and entity-entity links. In Proceedings of the 7th
SIAM International Conference on Data Mining.
Dave Binkley, Matthew Hearn, and Dawn Lawrie.
2011. Improving identifier informativeness using
part of speech information. In Proc. of the Working
Conference on Mining Software Repositories. ACM.
David M Blei and Michael I Jordan. 2003. Modeling
annotated data. In Proceedings of the 26th annual
international ACM SIGIR conference on Research
and development in informaion retrieval. ACM.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research.
Elena Erosheva, Stephen Fienberg, and John Lafferty.
2004. Mixed-membership models of scientific pub-
lications. Proceedings of the National Academy of
Sciences of the United States of America.
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? automatic caption gener-
ation for news images. In Proc. of the 48th Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics.
Yansong Feng and Mirella Lapata. 2013. Automatic
caption generation for news images. IEEE transac-
tions on pattern analysis and machine intelligence.
Mark Gabel and Zhendong Su. 2008. Javert: fully au-
tomatic mining of general temporal properties from
dynamic traces. In Proceedings of the 16th ACM
SIGSOFT International Symposium on Foundations
of software engineering, pages 339–349. ACM.
Thomas L Griffiths and Mark Steyvers. 2004. Finding
scientific topics. Proc. of the National Academy of
Sciences of the United States ofAmerica.
Sangmok Han, David R Wallace, and Robert C Miller.
2009. Code completion from abbreviated input.
In Automated Software Engineering, 2009. ASE’09.
24th IEEE/ACM International Conference on, pages
332–343. IEEE.
Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel,
and Premkumar Devanbu. 2012. On the naturalness
of software. In Software Engineering (ICSE), 2012
34th International Conference on. IEEE.
Ferosh Jacob and Robert Tairas. 2010. Code template
inference using language models. In Proceedings
of the 48th Annual Southeast Regional Conference.
ACM.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., volume 1, pages 181–184. IEEE.
Naveen Kumar and Benjamin Carterette. 2013. Time
based feedback and query expansion for twitter
search. In Advances in Information Retrieval, pages
734–737. Springer.
Dawn Lawrie, Christopher Morrell, Henry Feild, and
David Binkley. 2006. Whats in a name? a study
of identifiers. In Program Comprehension, 2006.
ICPC 2006. 14th IEEE International Conference on,
pages 3–12. IEEE.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the
49th annual meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, volume 1, pages 258–267.
Sarah Rastkar, Gail C Murphy, and Alexander WJ
Bradley. 2011. Generating natural language sum-
maries for crosscutting source code concerns. In
Software Maintenance (ICSM), 2011 27th IEEE In-
ternational Conference on, pages 103–112. IEEE.
</reference>
<page confidence="0.988261">
39
</page>
<reference confidence="0.999726172413793">
Ronald Rosenfeld. 2000. Two decades of statistical
language modeling: Where do we go from here?
Proceedings of the IEEE, 88(8):1270–1278.
David Shepherd, Zachary P Fry, Emily Hill, Lori Pol-
lock, and K Vijay-Shanker. 2007. Using natu-
ral language program analysis to locate and under-
stand action-oriented concerns. In Proceedings of
the 6th international conference on Aspect-oriented
software development, pages 212–224. ACM.
Giriprasad Sridhara, Emily Hill, Divya Muppaneni,
Lori Pollock, and K Vijay-Shanker. 2010. To-
wards automatically generating summary comments
for java methods. In Proceedings of the IEEE/ACM
international conference on Automated software en-
gineering, pages 43–52. ACM.
Yuen-Hsien Tseng and Da-Wei Juang. 2003.
Document-self expansion for text categorization. In
Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in
informaion retrieval, pages 399–400. ACM.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007.
Single document summarization with document ex-
pansion. In Proc. of the National Conference on
Artificial Intelligence. Menlo Park, CA; Cambridge,
MA; London; AAAI Press; MIT Press; 1999.
Roung-Shiunn Wu and Po-Chun Li. 2011. Video
annotation using hierarchical dirichlet process mix-
ture model. Expert Systems with Applications,
38(4):3040–3048.
</reference>
<page confidence="0.998624">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.987258">
<title confidence="0.999888">Natural Language Models for Predicting Programming Comments</title>
<author confidence="0.999985">Dana Movshovitz-Attias William W Cohen</author>
<affiliation confidence="0.9999095">Computer Science Department Computer Science Department Carnegie Mellon University Carnegie Mellon University</affiliation>
<email confidence="0.999133">dma@cs.cmu.eduwcohen@cs.cmu.edu</email>
<abstract confidence="0.9993539">Statistical language models have successfully been used to describe and analyze natural language documents. Recent work applying language models to programming languages is focused on the task of predicting code, while mainly ignoring the prediction of programmer comments. In this work, we predict comments from JAVA source files of open source projects, using topic models and n-grams, and we analyze the performance of the models given varying amounts of background data on the project being predicted. We evaluate models on their comment-completion capability in a setting similar to codecompletion tools built into standard code editors, and show that using a comment completion tool can save up to 47% of the comment typing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ramnath Balasubramanyan</author>
<author>William W Cohen</author>
</authors>
<title>Block-lda: Jointly modeling entity-annotated text and entity-entity links.</title>
<date>2011</date>
<booktitle>In Proceedings of the 7th SIAM International Conference on Data Mining.</booktitle>
<contexts>
<context position="7414" citStr="Balasubramanyan and Cohen (2011)" startWordPosition="1250" endWordPosition="1254">ens as a continuous mixture distribution for the set w = wr, by integrating over θ and summing over the set of topics z p(θ, z, w|α,β) = (1) p(θ|α) ri p(z|θ)p(w|z,β) w ri p(ztext|θ)p(wtext|ztext, β)· fp(w|α,β) = p(θ|α)· (5) wtext ri p(zcode|θ)p(wcode|zcode, β) riE �p(z|θ)p(w|z, β) dθ wcode w z where θ is the joint topic distribution, w is the set of observed document words, ztext is a topic associated with a text word, and zcode a topic associated with a code word. The LDA and link-LDA models use Gibbs sampling (Griffiths and Steyvers, 2004) for topic inference, based on the implementation of Balasubramanyan and Cohen (2011) with single or multiple entities per document, respectively. For the case of link-LDA where the document is comprised of two entities, in our case code tokens and text tokens, we can consider the mixedmembership joint distribution θ, as in equation (2), and similarly the marginal distribution p(w|α, β) over both code and text tokens from wr. Since comment words in wc are all considered as text tokens they are sampled using text topics, namely ztext, in equation (4). 36 3 Experimental Settings the Apache Tika toolkit1 and then tokenized with the Mallet package. We considered as raw code 3.1 Da</context>
</contexts>
<marker>Balasubramanyan, Cohen, 2011</marker>
<rawString>Ramnath Balasubramanyan and William W Cohen. 2011. Block-lda: Jointly modeling entity-annotated text and entity-entity links. In Proceedings of the 7th SIAM International Conference on Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dave Binkley</author>
<author>Matthew Hearn</author>
<author>Dawn Lawrie</author>
</authors>
<title>Improving identifier informativeness using part of speech information.</title>
<date>2011</date>
<booktitle>In Proc. of the Working Conference on Mining Software Repositories.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="1593" citStr="Binkley et al., 2011" startWordPosition="238" endWordPosition="241">nt typing. 1 Introduction and Related Work Statistical language models have traditionally been used to describe and analyze natural language documents. Recently, software engineering researchers have adopted the use of language models for modeling software code. Hindle et al. (2012) observe that, as code is created by humans it is likely to be repetitive and predictable, similar to natural language. NLP models have thus been used for a variety of software development tasks such as code token completion (Han et al., 2009; Jacob and Tairas, 2010), analysis of names in code (Lawrie et al., 2006; Binkley et al., 2011) and mining software repositories (Gabel and Su, 2008). An important part of software programming and maintenance lies in documentation, which may come in the form of tutorials describing the code, or inline comments provided by the programmer. The documentation provides a high level description of the task performed by the code, and may include examples of use-cases for specific code segments or identifiers such as classes, methods and variables. Well documented code is easier to read and maintain in the long-run but writing comments is a laborious task that is often overlooked or at least po</context>
</contexts>
<marker>Binkley, Hearn, Lawrie, 2011</marker>
<rawString>Dave Binkley, Matthew Hearn, and Dawn Lawrie. 2011. Improving identifier informativeness using part of speech information. In Proc. of the Working Conference on Mining Software Repositories. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Michael I Jordan</author>
</authors>
<title>Modeling annotated data.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="2988" citStr="Blei and Jordan, 2003" startWordPosition="467" endWordPosition="470">m of document expansion where the comment contains significant terms relevant to the described code. Accurately predicted comment words can therefore be used for a variety of linguistic uses including improved search over code bases using natural language queries, code categorization, and locating parts of the code that are relevant to a specific topic or idea (Tseng and Juang, 2003; Wan et al., 2007; Kumar and Carterette, 2013; Shepherd et al., 2007; Rastkar et al., 2011). A related and well studied NLP task is that of predicting natural language caption and commentary for images and videos (Blei and Jordan, 2003; Feng and Lapata, 2010; Feng and Lapata, 2013; Wu and Li, 2011). In this work, our goal is to apply statistical language models for predicting class comments. We show that n-gram models are extremely successful in this task, and can lead to a saving of up to 47% in comment typing. This is expected as n-grams have been shown as a strong model for language and speech prediction that is hard to improve upon (Rosenfeld, 2000). In some cases however, for example in a document expansion task, we wish to extract important terms relevant to the code regardless of local syntactic dependencies. We henc</context>
</contexts>
<marker>Blei, Jordan, 2003</marker>
<rawString>David M Blei and Michael I Jordan. 2003. Modeling annotated data. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="3638" citStr="Blei et al., 2003" startWordPosition="585" endWordPosition="588">d Lapata, 2013; Wu and Li, 2011). In this work, our goal is to apply statistical language models for predicting class comments. We show that n-gram models are extremely successful in this task, and can lead to a saving of up to 47% in comment typing. This is expected as n-grams have been shown as a strong model for language and speech prediction that is hard to improve upon (Rosenfeld, 2000). In some cases however, for example in a document expansion task, we wish to extract important terms relevant to the code regardless of local syntactic dependencies. We hence also evaluate the use of LDA (Blei et al., 2003) and link-LDA (Erosheva et al., 2004) topic models, which are more relevant for the term extraction scenario. We find that the topic model performance can be improved by distinguishing code and text tokens in the code. 35 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 35–40, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Method 2.1 Models We train n-gram models (n = 1, 2,3) over source code documents containing sequences of combined code and text tokens from multiple training datasets (described below). We us</context>
<context position="6579" citStr="Blei et al. (2003)" startWordPosition="1103" endWordPosition="1106">lass comment wc, are all considered as text tokens. The rest of the tokens in the document wr, are considered to be the class definition, and they may contain both code and text tokens (from string literals and other comments in the source file). We then compute the posterior probability of document topics by solving the following inference problem conditioned on the wr tokens p(θ, zr|wr, α, β) = p(θ, zr, wr|α, β) (3) p(wr|α,β) This gives us an estimate of the document distribution, θ, with which we infer the probability of the comment tokens as p(wc|θ, β) = E p(wc|z, β)p(z|θ) (4) z Following Blei et al. (2003), for the case of a single entity LDA, the inference problem from equation (3) can be solved by considering p(θ, z, w|α,β), as in equation (1), and by taking the marginal distribution of the document tokens as a continuous mixture distribution for the set w = wr, by integrating over θ and summing over the set of topics z p(θ, z, w|α,β) = (1) p(θ|α) ri p(z|θ)p(w|z,β) w ri p(ztext|θ)p(wtext|ztext, β)· fp(w|α,β) = p(θ|α)· (5) wtext ri p(zcode|θ)p(wcode|zcode, β) riE �p(z|θ)p(w|z, β) dθ wcode w z where θ is the joint topic distribution, w is the set of observed document words, ztext is a topic ass</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Erosheva</author>
<author>Stephen Fienberg</author>
<author>John Lafferty</author>
</authors>
<title>Mixed-membership models of scientific publications.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America.</booktitle>
<contexts>
<context position="3675" citStr="Erosheva et al., 2004" startWordPosition="591" endWordPosition="594"> In this work, our goal is to apply statistical language models for predicting class comments. We show that n-gram models are extremely successful in this task, and can lead to a saving of up to 47% in comment typing. This is expected as n-grams have been shown as a strong model for language and speech prediction that is hard to improve upon (Rosenfeld, 2000). In some cases however, for example in a document expansion task, we wish to extract important terms relevant to the code regardless of local syntactic dependencies. We hence also evaluate the use of LDA (Blei et al., 2003) and link-LDA (Erosheva et al., 2004) topic models, which are more relevant for the term extraction scenario. We find that the topic model performance can be improved by distinguishing code and text tokens in the code. 35 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 35–40, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Method 2.1 Models We train n-gram models (n = 1, 2,3) over source code documents containing sequences of combined code and text tokens from multiple training datasets (described below). We use the Berkeley Language Model package</context>
<context position="5200" citStr="Erosheva et al., 2004" startWordPosition="849" endWordPosition="852">N topics z, for a single source code document with N observed word tokens, d = {wi}N i=1, given the Dirichlet parameters α and β, is therefore Under the models described so far, there is no distinction between text and code tokens. Finally, we consider documents as having a mixed membership of two entity types, code and text tokens, d = ({wZode}Cn1, {wtext i }Tn i=1), where the text words are tokens from comment and string literals, and the code words include the programming language syntax tokens (e.g., public, private, for, etc’ ) and all identifiers. In this case, we train link-LDA models (Erosheva et al., 2004) with 1, 5, 10 and 20 topics. Under the linkLDA model, the mixed-membership joint distribution of a topic mixture, words and topics is then p(θ, z, w|α, β) = p(θ|α)· (2) 2.2 Testing Methodology Our goal is to predict the tokens of the JAVA class comment (the one preceding the class definition) in each of the test files. Each of the models described above assigns a probability to the next comment token. In the case of n-grams, the probability of a token word wi is given by considering previous words p(wi|wi−1, ... , w0). This probability is estimated given the previous n − 1 tokens as p(wi|wi−1</context>
</contexts>
<marker>Erosheva, Fienberg, Lafferty, 2004</marker>
<rawString>Elena Erosheva, Stephen Fienberg, and John Lafferty. 2004. Mixed-membership models of scientific publications. Proceedings of the National Academy of Sciences of the United States of America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>How many words is a picture worth? automatic caption generation for news images.</title>
<date>2010</date>
<booktitle>In Proc. of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3011" citStr="Feng and Lapata, 2010" startWordPosition="471" endWordPosition="474"> where the comment contains significant terms relevant to the described code. Accurately predicted comment words can therefore be used for a variety of linguistic uses including improved search over code bases using natural language queries, code categorization, and locating parts of the code that are relevant to a specific topic or idea (Tseng and Juang, 2003; Wan et al., 2007; Kumar and Carterette, 2013; Shepherd et al., 2007; Rastkar et al., 2011). A related and well studied NLP task is that of predicting natural language caption and commentary for images and videos (Blei and Jordan, 2003; Feng and Lapata, 2010; Feng and Lapata, 2013; Wu and Li, 2011). In this work, our goal is to apply statistical language models for predicting class comments. We show that n-gram models are extremely successful in this task, and can lead to a saving of up to 47% in comment typing. This is expected as n-grams have been shown as a strong model for language and speech prediction that is hard to improve upon (Rosenfeld, 2000). In some cases however, for example in a document expansion task, we wish to extract important terms relevant to the code regardless of local syntactic dependencies. We hence also evaluate the use</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. How many words is a picture worth? automatic caption generation for news images. In Proc. of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Automatic caption generation for news images. IEEE transactions on pattern analysis and machine intelligence.</title>
<date>2013</date>
<contexts>
<context position="3034" citStr="Feng and Lapata, 2013" startWordPosition="475" endWordPosition="478">ains significant terms relevant to the described code. Accurately predicted comment words can therefore be used for a variety of linguistic uses including improved search over code bases using natural language queries, code categorization, and locating parts of the code that are relevant to a specific topic or idea (Tseng and Juang, 2003; Wan et al., 2007; Kumar and Carterette, 2013; Shepherd et al., 2007; Rastkar et al., 2011). A related and well studied NLP task is that of predicting natural language caption and commentary for images and videos (Blei and Jordan, 2003; Feng and Lapata, 2010; Feng and Lapata, 2013; Wu and Li, 2011). In this work, our goal is to apply statistical language models for predicting class comments. We show that n-gram models are extremely successful in this task, and can lead to a saving of up to 47% in comment typing. This is expected as n-grams have been shown as a strong model for language and speech prediction that is hard to improve upon (Rosenfeld, 2000). In some cases however, for example in a document expansion task, we wish to extract important terms relevant to the code regardless of local syntactic dependencies. We hence also evaluate the use of LDA (Blei et al., 2</context>
</contexts>
<marker>Feng, Lapata, 2013</marker>
<rawString>Yansong Feng and Mirella Lapata. 2013. Automatic caption generation for news images. IEEE transactions on pattern analysis and machine intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Gabel</author>
<author>Zhendong Su</author>
</authors>
<title>Javert: fully automatic mining of general temporal properties from dynamic traces.</title>
<date>2008</date>
<booktitle>In Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of software engineering,</booktitle>
<pages>339--349</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1647" citStr="Gabel and Su, 2008" startWordPosition="247" endWordPosition="250">anguage models have traditionally been used to describe and analyze natural language documents. Recently, software engineering researchers have adopted the use of language models for modeling software code. Hindle et al. (2012) observe that, as code is created by humans it is likely to be repetitive and predictable, similar to natural language. NLP models have thus been used for a variety of software development tasks such as code token completion (Han et al., 2009; Jacob and Tairas, 2010), analysis of names in code (Lawrie et al., 2006; Binkley et al., 2011) and mining software repositories (Gabel and Su, 2008). An important part of software programming and maintenance lies in documentation, which may come in the form of tutorials describing the code, or inline comments provided by the programmer. The documentation provides a high level description of the task performed by the code, and may include examples of use-cases for specific code segments or identifiers such as classes, methods and variables. Well documented code is easier to read and maintain in the long-run but writing comments is a laborious task that is often overlooked or at least postponed by many programmers. Code commenting not only </context>
</contexts>
<marker>Gabel, Su, 2008</marker>
<rawString>Mark Gabel and Zhendong Su. 2008. Javert: fully automatic mining of general temporal properties from dynamic traces. In Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of software engineering, pages 339–349. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proc. of the National Academy of Sciences of the United States ofAmerica.</booktitle>
<contexts>
<context position="7329" citStr="Griffiths and Steyvers, 2004" startWordPosition="1237" endWordPosition="1240">), as in equation (1), and by taking the marginal distribution of the document tokens as a continuous mixture distribution for the set w = wr, by integrating over θ and summing over the set of topics z p(θ, z, w|α,β) = (1) p(θ|α) ri p(z|θ)p(w|z,β) w ri p(ztext|θ)p(wtext|ztext, β)· fp(w|α,β) = p(θ|α)· (5) wtext ri p(zcode|θ)p(wcode|zcode, β) riE �p(z|θ)p(w|z, β) dθ wcode w z where θ is the joint topic distribution, w is the set of observed document words, ztext is a topic associated with a text word, and zcode a topic associated with a code word. The LDA and link-LDA models use Gibbs sampling (Griffiths and Steyvers, 2004) for topic inference, based on the implementation of Balasubramanyan and Cohen (2011) with single or multiple entities per document, respectively. For the case of link-LDA where the document is comprised of two entities, in our case code tokens and text tokens, we can consider the mixedmembership joint distribution θ, as in equation (2), and similarly the marginal distribution p(w|α, β) over both code and text tokens from wr. Since comment words in wc are all considered as text tokens they are sampled using text topics, namely ztext, in equation (4). 36 3 Experimental Settings the Apache Tika </context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proc. of the National Academy of Sciences of the United States ofAmerica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sangmok Han</author>
<author>David R Wallace</author>
<author>Robert C Miller</author>
</authors>
<title>Code completion from abbreviated input.</title>
<date>2009</date>
<booktitle>In Automated Software Engineering, 2009. ASE’09. 24th IEEE/ACM International Conference on,</booktitle>
<pages>332--343</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1497" citStr="Han et al., 2009" startWordPosition="221" endWordPosition="224">code editors, and show that using a comment completion tool can save up to 47% of the comment typing. 1 Introduction and Related Work Statistical language models have traditionally been used to describe and analyze natural language documents. Recently, software engineering researchers have adopted the use of language models for modeling software code. Hindle et al. (2012) observe that, as code is created by humans it is likely to be repetitive and predictable, similar to natural language. NLP models have thus been used for a variety of software development tasks such as code token completion (Han et al., 2009; Jacob and Tairas, 2010), analysis of names in code (Lawrie et al., 2006; Binkley et al., 2011) and mining software repositories (Gabel and Su, 2008). An important part of software programming and maintenance lies in documentation, which may come in the form of tutorials describing the code, or inline comments provided by the programmer. The documentation provides a high level description of the task performed by the code, and may include examples of use-cases for specific code segments or identifiers such as classes, methods and variables. Well documented code is easier to read and maintain </context>
</contexts>
<marker>Han, Wallace, Miller, 2009</marker>
<rawString>Sangmok Han, David R Wallace, and Robert C Miller. 2009. Code completion from abbreviated input. In Automated Software Engineering, 2009. ASE’09. 24th IEEE/ACM International Conference on, pages 332–343. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abram Hindle</author>
<author>Earl T Barr</author>
<author>Zhendong Su</author>
<author>Mark Gabel</author>
<author>Premkumar Devanbu</author>
</authors>
<title>On the naturalness of software.</title>
<date>2012</date>
<booktitle>In Software Engineering (ICSE), 2012 34th International Conference on. IEEE.</booktitle>
<contexts>
<context position="1255" citStr="Hindle et al. (2012)" startWordPosition="179" endWordPosition="182">ams, and we analyze the performance of the models given varying amounts of background data on the project being predicted. We evaluate models on their comment-completion capability in a setting similar to codecompletion tools built into standard code editors, and show that using a comment completion tool can save up to 47% of the comment typing. 1 Introduction and Related Work Statistical language models have traditionally been used to describe and analyze natural language documents. Recently, software engineering researchers have adopted the use of language models for modeling software code. Hindle et al. (2012) observe that, as code is created by humans it is likely to be repetitive and predictable, similar to natural language. NLP models have thus been used for a variety of software development tasks such as code token completion (Han et al., 2009; Jacob and Tairas, 2010), analysis of names in code (Lawrie et al., 2006; Binkley et al., 2011) and mining software repositories (Gabel and Su, 2008). An important part of software programming and maintenance lies in documentation, which may come in the form of tutorials describing the code, or inline comments provided by the programmer. The documentation</context>
</contexts>
<marker>Hindle, Barr, Su, Gabel, Devanbu, 2012</marker>
<rawString>Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. 2012. On the naturalness of software. In Software Engineering (ICSE), 2012 34th International Conference on. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferosh Jacob</author>
<author>Robert Tairas</author>
</authors>
<title>Code template inference using language models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Southeast Regional Conference.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="1522" citStr="Jacob and Tairas, 2010" startWordPosition="225" endWordPosition="228">show that using a comment completion tool can save up to 47% of the comment typing. 1 Introduction and Related Work Statistical language models have traditionally been used to describe and analyze natural language documents. Recently, software engineering researchers have adopted the use of language models for modeling software code. Hindle et al. (2012) observe that, as code is created by humans it is likely to be repetitive and predictable, similar to natural language. NLP models have thus been used for a variety of software development tasks such as code token completion (Han et al., 2009; Jacob and Tairas, 2010), analysis of names in code (Lawrie et al., 2006; Binkley et al., 2011) and mining software repositories (Gabel and Su, 2008). An important part of software programming and maintenance lies in documentation, which may come in the form of tutorials describing the code, or inline comments provided by the programmer. The documentation provides a high level description of the task performed by the code, and may include examples of use-cases for specific code segments or identifiers such as classes, methods and variables. Well documented code is easier to read and maintain in the long-run but writi</context>
</contexts>
<marker>Jacob, Tairas, 2010</marker>
<rawString>Ferosh Jacob and Robert Tairas. 2010. Code template inference using language models. In Proceedings of the 48th Annual Southeast Regional Conference. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>95</volume>
<pages>181--184</pages>
<publisher>IEEE.</publisher>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., volume 1, pages 181–184. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naveen Kumar</author>
<author>Benjamin Carterette</author>
</authors>
<title>Time based feedback and query expansion for twitter search.</title>
<date>2013</date>
<booktitle>In Advances in Information Retrieval,</booktitle>
<pages>734--737</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2798" citStr="Kumar and Carterette, 2013" startWordPosition="435" endWordPosition="438">looked or at least postponed by many programmers. Code commenting not only provides a summarization of the conceptual idea behind the code (Sridhara et al., 2010), but can also be viewed as a form of document expansion where the comment contains significant terms relevant to the described code. Accurately predicted comment words can therefore be used for a variety of linguistic uses including improved search over code bases using natural language queries, code categorization, and locating parts of the code that are relevant to a specific topic or idea (Tseng and Juang, 2003; Wan et al., 2007; Kumar and Carterette, 2013; Shepherd et al., 2007; Rastkar et al., 2011). A related and well studied NLP task is that of predicting natural language caption and commentary for images and videos (Blei and Jordan, 2003; Feng and Lapata, 2010; Feng and Lapata, 2013; Wu and Li, 2011). In this work, our goal is to apply statistical language models for predicting class comments. We show that n-gram models are extremely successful in this task, and can lead to a saving of up to 47% in comment typing. This is expected as n-grams have been shown as a strong model for language and speech prediction that is hard to improve upon (</context>
</contexts>
<marker>Kumar, Carterette, 2013</marker>
<rawString>Naveen Kumar and Benjamin Carterette. 2013. Time based feedback and query expansion for twitter search. In Advances in Information Retrieval, pages 734–737. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dawn Lawrie</author>
<author>Christopher Morrell</author>
<author>Henry Feild</author>
<author>David Binkley</author>
</authors>
<title>Whats in a name? a study of identifiers.</title>
<date>2006</date>
<booktitle>In Program Comprehension,</booktitle>
<pages>3--12</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1570" citStr="Lawrie et al., 2006" startWordPosition="234" endWordPosition="237">p to 47% of the comment typing. 1 Introduction and Related Work Statistical language models have traditionally been used to describe and analyze natural language documents. Recently, software engineering researchers have adopted the use of language models for modeling software code. Hindle et al. (2012) observe that, as code is created by humans it is likely to be repetitive and predictable, similar to natural language. NLP models have thus been used for a variety of software development tasks such as code token completion (Han et al., 2009; Jacob and Tairas, 2010), analysis of names in code (Lawrie et al., 2006; Binkley et al., 2011) and mining software repositories (Gabel and Su, 2008). An important part of software programming and maintenance lies in documentation, which may come in the form of tutorials describing the code, or inline comments provided by the programmer. The documentation provides a high level description of the task performed by the code, and may include examples of use-cases for specific code segments or identifiers such as classes, methods and variables. Well documented code is easier to read and maintain in the long-run but writing comments is a laborious task that is often ov</context>
</contexts>
<marker>Lawrie, Morrell, Feild, Binkley, 2006</marker>
<rawString>Dawn Lawrie, Christopher Morrell, Henry Feild, and David Binkley. 2006. Whats in a name? a study of identifiers. In Program Comprehension, 2006. ICPC 2006. 14th IEEE International Conference on, pages 3–12. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<contexts>
<context position="10372" citStr="McCallum, 2002" startWordPosition="1736" endWordPosition="1738">at project. We report results averaged over all projects in Table 1. Source files were tokenized using the Eclipse JDT compiler tools, separating code tokens and identifiers. Identifier names (of classes, methods and variables), were further tokenized by camel case notation (e.g., ’minMargin’ was converted to ’min margin’). Non alpha-numeric tokens (e.g., dot, semicolon) were discarded from the code, as well as numeric and single character literals. Text from comments or any string literals within the code were further tokenized with the Mallet statistical natural language processing package (McCallum, 2002). Posts from SO were parsed using 3.2 Evaluation Since our models are trained using various data sources the vocabularies used by each of them are different, making the comment likelihood given by each model incomparable due to different sets of out-of-vocabulary tokens. We thus evaluate models using a character saving metric which aims at quantifying the percentage of characters that can be saved by using the model in a word-completion settings, similar to standard code completion tools built into code editors. For a comment word with n characters, w = wi, ... , wn, we predict the two most li</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Faster and smaller n-gram language models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th annual meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<volume>1</volume>
<pages>258--267</pages>
<contexts>
<context position="4299" citStr="Pauls and Klein, 2011" startWordPosition="690" endWordPosition="693">opic models, which are more relevant for the term extraction scenario. We find that the topic model performance can be improved by distinguishing code and text tokens in the code. 35 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 35–40, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Method 2.1 Models We train n-gram models (n = 1, 2,3) over source code documents containing sequences of combined code and text tokens from multiple training datasets (described below). We use the Berkeley Language Model package (Pauls and Klein, 2011) with absolute discounting (Kneser-Ney smoothing; (1995)) which includes a backoff strategy to lower-order n-grams. Next, we use LDA topic models (Blei et al., 2003) trained on the same data, with 1, 5, 10 and 20 topics. The joint distribution of a topic mixture θ, and a set of N topics z, for a single source code document with N observed word tokens, d = {wi}N i=1, given the Dirichlet parameters α and β, is therefore Under the models described so far, there is no distinction between text and code tokens. Finally, we consider documents as having a mixed membership of two entity types, code and</context>
</contexts>
<marker>Pauls, Klein, 2011</marker>
<rawString>Adam Pauls and Dan Klein. 2011. Faster and smaller n-gram language models. In Proceedings of the 49th annual meeting of the Association for Computational Linguistics: Human Language Technologies, volume 1, pages 258–267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Rastkar</author>
<author>Gail C Murphy</author>
<author>Alexander WJ Bradley</author>
</authors>
<title>Generating natural language summaries for crosscutting source code concerns.</title>
<date>2011</date>
<booktitle>In Software Maintenance (ICSM), 2011 27th IEEE International Conference on,</booktitle>
<pages>103--112</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="2844" citStr="Rastkar et al., 2011" startWordPosition="443" endWordPosition="446">ode commenting not only provides a summarization of the conceptual idea behind the code (Sridhara et al., 2010), but can also be viewed as a form of document expansion where the comment contains significant terms relevant to the described code. Accurately predicted comment words can therefore be used for a variety of linguistic uses including improved search over code bases using natural language queries, code categorization, and locating parts of the code that are relevant to a specific topic or idea (Tseng and Juang, 2003; Wan et al., 2007; Kumar and Carterette, 2013; Shepherd et al., 2007; Rastkar et al., 2011). A related and well studied NLP task is that of predicting natural language caption and commentary for images and videos (Blei and Jordan, 2003; Feng and Lapata, 2010; Feng and Lapata, 2013; Wu and Li, 2011). In this work, our goal is to apply statistical language models for predicting class comments. We show that n-gram models are extremely successful in this task, and can lead to a saving of up to 47% in comment typing. This is expected as n-grams have been shown as a strong model for language and speech prediction that is hard to improve upon (Rosenfeld, 2000). In some cases however, for e</context>
</contexts>
<marker>Rastkar, Murphy, Bradley, 2011</marker>
<rawString>Sarah Rastkar, Gail C Murphy, and Alexander WJ Bradley. 2011. Generating natural language summaries for crosscutting source code concerns. In Software Maintenance (ICSM), 2011 27th IEEE International Conference on, pages 103–112. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>Two decades of statistical language modeling: Where do we go from here?</title>
<date>2000</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>88</volume>
<issue>8</issue>
<contexts>
<context position="3414" citStr="Rosenfeld, 2000" startWordPosition="548" endWordPosition="549">; Shepherd et al., 2007; Rastkar et al., 2011). A related and well studied NLP task is that of predicting natural language caption and commentary for images and videos (Blei and Jordan, 2003; Feng and Lapata, 2010; Feng and Lapata, 2013; Wu and Li, 2011). In this work, our goal is to apply statistical language models for predicting class comments. We show that n-gram models are extremely successful in this task, and can lead to a saving of up to 47% in comment typing. This is expected as n-grams have been shown as a strong model for language and speech prediction that is hard to improve upon (Rosenfeld, 2000). In some cases however, for example in a document expansion task, we wish to extract important terms relevant to the code regardless of local syntactic dependencies. We hence also evaluate the use of LDA (Blei et al., 2003) and link-LDA (Erosheva et al., 2004) topic models, which are more relevant for the term extraction scenario. We find that the topic model performance can be improved by distinguishing code and text tokens in the code. 35 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 35–40, Sofia, Bulgaria, August 4-9 2013. c�2013 Association</context>
</contexts>
<marker>Rosenfeld, 2000</marker>
<rawString>Ronald Rosenfeld. 2000. Two decades of statistical language modeling: Where do we go from here? Proceedings of the IEEE, 88(8):1270–1278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Shepherd</author>
<author>Zachary P Fry</author>
<author>Emily Hill</author>
<author>Lori Pollock</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Using natural language program analysis to locate and understand action-oriented concerns.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th international conference on Aspect-oriented software development,</booktitle>
<pages>212--224</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2821" citStr="Shepherd et al., 2007" startWordPosition="439" endWordPosition="442"> by many programmers. Code commenting not only provides a summarization of the conceptual idea behind the code (Sridhara et al., 2010), but can also be viewed as a form of document expansion where the comment contains significant terms relevant to the described code. Accurately predicted comment words can therefore be used for a variety of linguistic uses including improved search over code bases using natural language queries, code categorization, and locating parts of the code that are relevant to a specific topic or idea (Tseng and Juang, 2003; Wan et al., 2007; Kumar and Carterette, 2013; Shepherd et al., 2007; Rastkar et al., 2011). A related and well studied NLP task is that of predicting natural language caption and commentary for images and videos (Blei and Jordan, 2003; Feng and Lapata, 2010; Feng and Lapata, 2013; Wu and Li, 2011). In this work, our goal is to apply statistical language models for predicting class comments. We show that n-gram models are extremely successful in this task, and can lead to a saving of up to 47% in comment typing. This is expected as n-grams have been shown as a strong model for language and speech prediction that is hard to improve upon (Rosenfeld, 2000). In so</context>
</contexts>
<marker>Shepherd, Fry, Hill, Pollock, Vijay-Shanker, 2007</marker>
<rawString>David Shepherd, Zachary P Fry, Emily Hill, Lori Pollock, and K Vijay-Shanker. 2007. Using natural language program analysis to locate and understand action-oriented concerns. In Proceedings of the 6th international conference on Aspect-oriented software development, pages 212–224. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giriprasad Sridhara</author>
<author>Emily Hill</author>
<author>Divya Muppaneni</author>
<author>Lori Pollock</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Towards automatically generating summary comments for java methods.</title>
<date>2010</date>
<booktitle>In Proceedings of the IEEE/ACM international conference on Automated software engineering,</booktitle>
<pages>43--52</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2334" citStr="Sridhara et al., 2010" startWordPosition="359" endWordPosition="362">in documentation, which may come in the form of tutorials describing the code, or inline comments provided by the programmer. The documentation provides a high level description of the task performed by the code, and may include examples of use-cases for specific code segments or identifiers such as classes, methods and variables. Well documented code is easier to read and maintain in the long-run but writing comments is a laborious task that is often overlooked or at least postponed by many programmers. Code commenting not only provides a summarization of the conceptual idea behind the code (Sridhara et al., 2010), but can also be viewed as a form of document expansion where the comment contains significant terms relevant to the described code. Accurately predicted comment words can therefore be used for a variety of linguistic uses including improved search over code bases using natural language queries, code categorization, and locating parts of the code that are relevant to a specific topic or idea (Tseng and Juang, 2003; Wan et al., 2007; Kumar and Carterette, 2013; Shepherd et al., 2007; Rastkar et al., 2011). A related and well studied NLP task is that of predicting natural language caption and c</context>
</contexts>
<marker>Sridhara, Hill, Muppaneni, Pollock, Vijay-Shanker, 2010</marker>
<rawString>Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, and K Vijay-Shanker. 2010. Towards automatically generating summary comments for java methods. In Proceedings of the IEEE/ACM international conference on Automated software engineering, pages 43–52. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuen-Hsien Tseng</author>
<author>Da-Wei Juang</author>
</authors>
<title>Document-self expansion for text categorization.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,</booktitle>
<pages>399--400</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2752" citStr="Tseng and Juang, 2003" startWordPosition="427" endWordPosition="430">ts is a laborious task that is often overlooked or at least postponed by many programmers. Code commenting not only provides a summarization of the conceptual idea behind the code (Sridhara et al., 2010), but can also be viewed as a form of document expansion where the comment contains significant terms relevant to the described code. Accurately predicted comment words can therefore be used for a variety of linguistic uses including improved search over code bases using natural language queries, code categorization, and locating parts of the code that are relevant to a specific topic or idea (Tseng and Juang, 2003; Wan et al., 2007; Kumar and Carterette, 2013; Shepherd et al., 2007; Rastkar et al., 2011). A related and well studied NLP task is that of predicting natural language caption and commentary for images and videos (Blei and Jordan, 2003; Feng and Lapata, 2010; Feng and Lapata, 2013; Wu and Li, 2011). In this work, our goal is to apply statistical language models for predicting class comments. We show that n-gram models are extremely successful in this task, and can lead to a saving of up to 47% in comment typing. This is expected as n-grams have been shown as a strong model for language and sp</context>
</contexts>
<marker>Tseng, Juang, 2003</marker>
<rawString>Yuen-Hsien Tseng and Da-Wei Juang. 2003. Document-self expansion for text categorization. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 399–400. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
<author>Jianguo Xiao</author>
</authors>
<title>Single document summarization with document expansion.</title>
<date>2007</date>
<booktitle>In Proc. of the National Conference on Artificial Intelligence. Menlo</booktitle>
<publisher>AAAI Press; MIT Press;</publisher>
<location>Park, CA; Cambridge, MA; London;</location>
<contexts>
<context position="2770" citStr="Wan et al., 2007" startWordPosition="431" endWordPosition="434">that is often overlooked or at least postponed by many programmers. Code commenting not only provides a summarization of the conceptual idea behind the code (Sridhara et al., 2010), but can also be viewed as a form of document expansion where the comment contains significant terms relevant to the described code. Accurately predicted comment words can therefore be used for a variety of linguistic uses including improved search over code bases using natural language queries, code categorization, and locating parts of the code that are relevant to a specific topic or idea (Tseng and Juang, 2003; Wan et al., 2007; Kumar and Carterette, 2013; Shepherd et al., 2007; Rastkar et al., 2011). A related and well studied NLP task is that of predicting natural language caption and commentary for images and videos (Blei and Jordan, 2003; Feng and Lapata, 2010; Feng and Lapata, 2013; Wu and Li, 2011). In this work, our goal is to apply statistical language models for predicting class comments. We show that n-gram models are extremely successful in this task, and can lead to a saving of up to 47% in comment typing. This is expected as n-grams have been shown as a strong model for language and speech prediction th</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. Single document summarization with document expansion. In Proc. of the National Conference on Artificial Intelligence. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roung-Shiunn Wu</author>
<author>Po-Chun Li</author>
</authors>
<title>Video annotation using hierarchical dirichlet process mixture model.</title>
<date>2011</date>
<journal>Expert Systems with Applications,</journal>
<volume>38</volume>
<issue>4</issue>
<contexts>
<context position="3052" citStr="Wu and Li, 2011" startWordPosition="479" endWordPosition="482">relevant to the described code. Accurately predicted comment words can therefore be used for a variety of linguistic uses including improved search over code bases using natural language queries, code categorization, and locating parts of the code that are relevant to a specific topic or idea (Tseng and Juang, 2003; Wan et al., 2007; Kumar and Carterette, 2013; Shepherd et al., 2007; Rastkar et al., 2011). A related and well studied NLP task is that of predicting natural language caption and commentary for images and videos (Blei and Jordan, 2003; Feng and Lapata, 2010; Feng and Lapata, 2013; Wu and Li, 2011). In this work, our goal is to apply statistical language models for predicting class comments. We show that n-gram models are extremely successful in this task, and can lead to a saving of up to 47% in comment typing. This is expected as n-grams have been shown as a strong model for language and speech prediction that is hard to improve upon (Rosenfeld, 2000). In some cases however, for example in a document expansion task, we wish to extract important terms relevant to the code regardless of local syntactic dependencies. We hence also evaluate the use of LDA (Blei et al., 2003) and link-LDA </context>
</contexts>
<marker>Wu, Li, 2011</marker>
<rawString>Roung-Shiunn Wu and Po-Chun Li. 2011. Video annotation using hierarchical dirichlet process mixture model. Expert Systems with Applications, 38(4):3040–3048.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>