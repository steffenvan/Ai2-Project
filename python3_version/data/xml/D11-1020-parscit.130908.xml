<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995223">
A Novel Dependency-to-String Model for Statistical Machine Translation
</title>
<author confidence="0.982305">
Jun Xie, Haitao Mi and Qun Liu
</author>
<affiliation confidence="0.979402">
Key Laboratory of Intelligent Information Processiong
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.876779">
P.O. Box 2704, Beijing 100190, China
</address>
<email confidence="0.999445">
{junxie,htmi,liuqun}@ict.ac.cn
</email>
<sectionHeader confidence="0.996668" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996128296296296">
Dependency structure, as a first step towards
semantics, is believed to be helpful to improve
translation quality. However, previous works
on dependency structure based models typi-
cally resort to insertion operations to complete
translations, which make it difficult to spec-
ify ordering information in translation rules.
In our model of this paper, we handle this
problem by directly specifying the ordering
information in head-dependents rules which
represent the source side as head-dependents
relations and the target side as strings. The
head-dependents rules require only substitu-
tion operation, thus our model requires no
heuristics or separate ordering models of the
previous works to control the word order of
translations. Large-scale experiments show
that our model performs well on long dis-
tance reordering, and outperforms the state-
of-the-art constituency-to-string model (+1.47
BLEU on average) and hierarchical phrase-
based model (+0.46 BLEU on average) on two
Chinese-English NIST test sets without resort
to phrases or parse forest. For the first time,
a source dependency structure based model
catches up with and surpasses the state-of-the-
art translation models.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936974358974">
Dependency structure represents the grammatical
relations that hold between the words in a sentence.
It encodes semantic relations directly, and has the
best inter-lingual phrasal cohesion properties (Fox,
2002). Those attractive characteristics make it pos-
sible to improve translation quality by using depen-
dency structures.
Some researchers pay more attention to use de-
pendency structure on the target side. (Shen et al.,
2008) presents a string-to-dependency model, which
restricts the target side of each hierarchical rule to be
a well-formed dependency tree fragment, and em-
ploys a dependency language model to make the out-
put more grammatically. This model significantly
outperforms the state-of-the-art hierarchical phrase-
based model (Chiang, 2005). However, those string-
to-tree systems run slowly in cubic time (Huang et
al., 2006).
Using dependency structure on the source side
is also a promising way, as tree-based systems run
much faster (linear time vs. cubic time, see (Huang
et al., 2006)). Conventional dependency structure
based models (Lin, 2004; Quirk et al., 2005; Ding
and Palmer, 2005; Xiong et al., 2007) typically
employ both substitution and insertion operation to
complete translations, which make it difficult to
specify ordering information directly in the transla-
tion rules. As a result, they have to resort to either
heuristics (Lin, 2004; Xiong et al., 2007) or sepa-
rate ordering models (Quirk et al., 2005; Ding and
Palmer, 2005) to control the word order of transla-
tions.
In this paper, we handle this problem by di-
rectly specifying the ordering information in head-
dependents rules that represent the source side as
head-dependents relations and the target side as
string. The head-dependents rules have only one
substitution operation, thus we don’t face the prob-
lems appeared in previous work and get rid of the
</bodyText>
<page confidence="0.985291">
216
</page>
<note confidence="0.9578885">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 216–226,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999539555555556">
heuristics and ordering model. To alleviate data
sparseness problem, we generalize the lexicalized
words in head-dependents relations with their cor-
responding categories.
In the following parts, we first describe the moti-
vation of using head-dependents relations (Section
2). Then we formalize our grammar (Section 3),
present our rule acquisition algorithm (Section 4),
our model (Section 5) and decoding algorithm (Sec-
tion 6). Finally, large-scale experiments (Section 7)
show that our model exhibits good performance on
long distance reordering, and outperforms the state-
of-the-art tree-to-string model (+1.47 BLEU on av-
erage) and hierarchical phrase-based model (+0.46
BLEU on average) on two Chinese-English NIST
test sets. For the first time, a source dependency tree
based model catches up with and surpasses the state-
of-the-art translation models.
</bodyText>
<sectionHeader confidence="0.998356" genericHeader="introduction">
2 Dependency Structure and
Head-Dependents Relation
</sectionHeader>
<subsectionHeader confidence="0.998979">
2.1 Dependency Sturcture
</subsectionHeader>
<bodyText confidence="0.937805933333333">
A dependency structure for a sentence is a directed
acyclic graph with words as nodes and modification
relations as edges. Each edge direct from a head to
a dependent. Figure 1 (a) shows an example depen-
dency structure of a Chinese sentence.
2010年 FIFA 世界杯 在 南非 成功 举行
2010 FIFA [World Cup] in/at [South Africa]
successfully hold
Each node is annotated with the part-of-speech
(POS) of the related word.
For convenience, we use the lexicon dependency
grammar (Hellwig, 2006) which adopts a bracket
representation to express a projective dependency
structure. The dependency structure of Figure 1 (a)
can be expressed as:
</bodyText>
<equation confidence="0.897918">
((2010年) (FIFA) 世界杯) (在(南非)) (成功) 举行
</equation>
<bodyText confidence="0.996788666666667">
where the lexicon in brackets represents the depen-
dents, while the lexicon out the brackets is the head.
To construct the dependency structure of a sen-
tence, the most important thing is to establish de-
pendency relations and distinguish the head from the
dependent. Here are some criteria (Zwicky, 1985;
</bodyText>
<figure confidence="0.86989975">
Ѯ㹼/VV
2010ᒤ/NT I ই䶎/NR
FIFA/NR
Ѯ㹼/VV
</figure>
<figureCaption confidence="0.937968125">
Figure 1: Examples of dependency structure (a), head-
dependents relation (b), head-dependents rule (r1 of Fig-
ure 2) and head rule (d). Where “x1:世 界 杯” and
“x2:在” indicate substitution sites which can be replaced
by a subtree rooted at “世界杯” and “在” respectively.
“x3:AD”indicates a substitution site that can be replaced
by a subtree whose root has part-of-speech “AD”. The
underline denotes a leaf node.
</figureCaption>
<bodyText confidence="0.764776666666667">
Hudson, 1990) for identifying a syntactic relation
between a head and a dependent between a head-
dependent pair:
</bodyText>
<listItem confidence="0.9846975">
1. head determines the syntactic category of C,
and can often replace C;
2. head determines the semantic category of C;
dependent gives semantic specification.
</listItem>
<subsectionHeader confidence="0.997231">
2.2 Head-Dependents Relation
</subsectionHeader>
<bodyText confidence="0.999562714285714">
A head-dependents relation is composed of a head
and all its dependents as shown in Figure 1(b).
Since all the head-dependent pairs satisfy crite-
ria 1 and 2, we can deduce that a head-dependents
relation L holds the property that the head deter-
mines the syntactic and semantic categories of L,
and can often replace L. Therefore, we can recur-
</bodyText>
<equation confidence="0.99107725">
ц⭼ᶟ/NR ൘/P
ᡀ࣏/AD
1:x1:ц⭼ᶟ :x2:൘ :x3:AD
ᡀ࣏ l
successfully
ц⭼ᶟ/NR ൘ /P ᡀ࣏/AD
1x1 was lheld x3 x2
Ѯ㹼
</equation>
<page confidence="0.99528">
217
</page>
<bodyText confidence="0.9998415">
sively replace the bottom level head-dependent re-
lations of a dependency structure with their heads
until the root. This implies an representation of the
generation of a dependency structure on the basis of
head-dependents relation.
Inspired by this, we represent the translation rules
of our dependency-to-string model on the founda-
tion of head-dependents relations.
</bodyText>
<sectionHeader confidence="0.976169" genericHeader="method">
3 Dependency-to-String Grammar
</sectionHeader>
<bodyText confidence="0.976553708333333">
Figure 1 (c) and (d) show two examples of the trans-
lation rules used in our dependency-to-string model.
The former is an example of head-dependent rules
that represent the source side as head-dependents re-
lations and act as both translation rules and reorder-
ing rules. The latter is an example of head rules
which are used for translating words.
Formally, a dependency-to-string grammar is de-
fined as a tuple (E, N, ∆, R), where E is a set of
source language terminals, N is a set of categories
for the terminals in E , ∆ is a set of target language
terminals, and R is a set of translation rules. A rule
r in R is a tuple (t, s, 0), where:
- t is a node labeled by terminal from E; or a
head-dependents relation of the source depen-
dency structures, with each node labeled by a
terminal from E or a variable from a set X =
{x1, x2, ...1 constrained by a terminal from E
or a category from N;
- s E (X U ∆)∗ is the target side string;
- 0 is a one-to-one mapping from nonterminals
in t to variables in s.
For example, the head-dependents rule shown in
Figure 1 (c) can be formalized as:
</bodyText>
<equation confidence="0.999973666666667">
t = ((x1:世界杯) (x2:在) (x3:AD) 举行)
s = x1 was �hpelld x3 x2
0 = {x1:世7t&apos;杯 H x1, x2:在 H x2, x3:AD H x31
</equation>
<bodyText confidence="0.999541857142857">
where the underline indicates a leaf node, and
xi:letters indicates a pair of variable and its con-
straint.
A derivation is informally defined as a sequence
of steps converting a source dependency structure
into a target language string, with each step apply-
ing one translation rule. As an example, Figure 2
</bodyText>
<figureCaption confidence="0.988407666666667">
Figure 2: An example derivation of dependency-to-string
translation. The dash lines indicate the reordering when
employing a head-dependents rule.
</figureCaption>
<bodyText confidence="0.992826">
shows the derivation for translating a Chinese (CH)
sentence into an English (EN) string.
</bodyText>
<figure confidence="0.964871680851064">
CH 2010年 FIFA 世界杯 在 南非 成功 举行
EN 2010 FIFA World Cup was held successfully in
South Africa
(a)
2010ᒤ IFIFA ц⭼ᶟ ൘ ই䶎 ᡀ࣏ Ѯ㹼
parser
Ѯ㹼/VV
ц⭼ᶟ/NR
was lheld ll
successfully
2010ᒤ/NTIFIFA/N R
ই䶎/NR
r3: (2010ᒤ) (FIFA) ц⭼ᶟ
42010 FIFA World Cup
൘/P
2010ᒤ/NTIFIFA/NR
r2: ᡀ࣏4succssfully
e
ই䶎/NR
2010 IFIFA [l[World Cup] was lheld ll
successfully
൘/P
ই䶎/NR
2010 IFIFA lWorld Cup was lheld ll
successfully iin
r5: ই䶎4South Africa
2010 I FIFA l World Cup was l held ll
successfully i in [ [South i] Africa]
(b)
(c)
2010ᒤ/NT I FIFA/R
N
ц⭼ᶟ/NR
ц⭼ᶟ/NR൘/P
r1: (x1:ц⭼ᶟ)(x2 :൘)(x3:AD)Ѯ㹼
4x1 was held x3 x2
was lheld
ই䶎/ NR
ᡀ࣏/AD
ᡀ࣏/AD
൘/P
(d)
(e)
(f)
(g)
r4: ൘ (x2:NR)4in x2
ই䶎
</figure>
<page confidence="0.997737">
218
</page>
<bodyText confidence="0.999975631578947">
The Chinese sentence (a) is first parsed into a de-
pendency structure (b), which is converted into an
English string in five steps. First, at the root node,
we apply head-dependents rule r1 shown in Figure
1(c) to translate the top level head-dependents rela-
tion and result in three unfinished substructures and
target string in (c). The rule is particular interesting
since it captures the fact: in Chinese prepositional
phrases and adverbs typically modify verbs on the
left, whereas in English prepositional phrases and
adverbs typically modify verbs on the right. Second,
we use head rule r2 translating “成功” into “success-
fully” and reach situation (d). Third, we apply head-
dependents rule r3 translating the head-dependents
relation rooted at “世界杯” and yield (e). Fourth,
head-dependents rules r5 partially translate the sub-
tree rooted at “在” and arrive situation in (f). Finally,
we apply head rule r5 translating the residual node
“南非” and obtain the final translation in (g).
</bodyText>
<sectionHeader confidence="0.979152" genericHeader="method">
4 Rule Acquisition
</sectionHeader>
<bodyText confidence="0.999993363636364">
The rule acquisition begins with a word-aligned cor-
pus: a set of triples (T, S, A), where T is a source
dependency structure, S is a target side sentence,
and A is an alignment relation between T and S.
We extract from each triple (T, S, A) head rules that
are consistent with the word alignments and head-
dependents rules that satisfy the intuition that syn-
tactically close items tend to stay close across lan-
guages. We accomplish the rule acquisition through
three steps: tree annotation, head-dependents frag-
ments identification and rule induction.
</bodyText>
<subsectionHeader confidence="0.965838">
4.1 Tree Annotation
</subsectionHeader>
<figureCaption confidence="0.688295666666667">
Figure 3: An annotated dependency structure. Each node
is annotated with two spans, the former is head span and
the latter dependency span. The nodes in acceptable head
set are displayed in gray, and the nodes in acceptable de-
pendent set are denoted by boxes. The triangle denotes
the only acceptable head-dependents fragment.
</figureCaption>
<bodyText confidence="0.798170583333333">
For example, hsp(南非) is consistent, while
hsp(2010年) is not consistent since hsp(2010年) n
hsp(在) = 5.
Definition 3. Given a head span hsp(n), its closure
cloz(hsp(n)) is the smallest contiguous head span
that is a superset of hsp(n).
For example, cloz(hsp(2010年)) = {1, 2, 3, 4, 51,
which corresponds to the target side word sequence
“2010 FIFA World Cup was”. For simplicity, we use
{1-5} to denotes the contiguous span {1, 2, 3, 4, 5}.
Definition 4. Given a subtree T′ rooted at n, the
dependency span dsp(n) of n is defined as:
</bodyText>
<figure confidence="0.996098">
2010 I FIFA l Cup was l ll iin South i
1 2 World 4 5 held successfully 8 9 Africa
3 6 7 10
2010�/NT
,
{1,5}{}
FIFA/NR
{2,2}{2,2}
IkPr*f/NR
{3,4}{2-4}
�/P
,,
{5,8}{9,10}
dd /
� NR
{9,0,10}
1 }{9
AT)J/AD
{7}{7}
*Tr/VV
{6}{2-10}
</figure>
<bodyText confidence="0.99126525">
Given a triple (T, S, A) as shown in Figure 3, we ∪ hsp(n′)).
first annotate each node n of T with two attributes: dsp(n) = cloz(
head span and dependency span, which are defined n′∈T′
as follows. hsp(n′) is consistent
</bodyText>
<construct confidence="0.919815166666667">
Definition 1. Given a node n, its head span hsp(n)
is a set of index of the target words aligned to n.
For example, hsp(2010年)={1, 51, which corre-
sponds to the target words “2010” and “was”.
Definition 2. A head span hsp(n) is consistent if it
satisfies the following property:
</construct>
<equation confidence="0.496751">
bn′̸�nhsp(n′) n hsp(n) = 0.
</equation>
<bodyText confidence="0.912243222222222">
If the head spans of all the nodes of T′ is not consis-
tent, dsp(n) = 0.
For example, since hsp(在) is not consistent,
dsp(在)=dsp(南非)={9,101, which corresponds to
the target words “South” and “Africa”.
The tree annotation can be accomplished by a sin-
gle postorder transversal of T. The extraction of
head rules from each node can be readily achieved
with the same criteria as (Och and Ney, 2004). In
</bodyText>
<page confidence="0.993892">
219
</page>
<bodyText confidence="0.9965405">
the following, we focus on head-dependents rules
acquisition.
</bodyText>
<subsectionHeader confidence="0.998302">
4.2 Head-Dependents Fragments Identification
</subsectionHeader>
<bodyText confidence="0.911552916666667">
We then identify the head-dependents fragments that
are suitable for rule induction from the annotated de-
pendency structure.
To facilitate the identification process, we first de-
fine two sets of dependency structure related to head
spans and dependency spans.
Definition 5. A acceptable head set ahs(T) of a de-
pendency structure T is a set of nodes, each of which
has a consistent head span.
For example, the elements of the acceptable head
set of the dependency structure in Figure 3 are dis-
played in gray.
</bodyText>
<construct confidence="0.978840666666667">
Definition 6. A acceptable dependent set adt(T) of
a dependency structure T is a set of nodes, each of
which satisfies: dep(n) ≠ ∅.
</construct>
<bodyText confidence="0.980002333333333">
For example, the elements of the acceptable de-
pendent set of the dependency structure in Figure 3
are denoted by boxes.
</bodyText>
<construct confidence="0.9167725">
Definition 7. We say a head-dependents fragments
is acceptable if it satisfies the following properties:
</construct>
<listItem confidence="0.9990645">
1. the root falls into acceptable head set;
2. all the sinks fall into acceptable dependent set.
</listItem>
<bodyText confidence="0.999947631578947">
An acceptable head-dependents fragment holds
the property that the head span of the root and the de-
pendency spans of the sinks do not overlap with each
other, which enables us to determine the reordering
in the target side.
The identification of acceptable head-dependents
fragments can be achieved by a single preorder
transversal of the annotated dependency structure.
For each accessed internal node n, we check
whether the head-dependents fragment f rooted at
n is acceptable. If f is acceptable, we output an
acceptable head-dependents fragment; otherwise we
access the next node.
Typically, each acceptable head-dependents frag-
ment has three types of nodes: internal nodes, inter-
nal nodes of the dependency structure; leaf nodes,
leaf nodes of the dependency structure; head node, a
special internal node acting as the head of the related
head-dependents relation.
</bodyText>
<figureCaption confidence="0.999064666666667">
Figure 4: A lexicalized head-dependents rule (b) induced
from the only acceptable head-dependents fragment (a)
of Figure 3.
</figureCaption>
<subsectionHeader confidence="0.991473">
4.3 Rule Induction
</subsectionHeader>
<bodyText confidence="0.99898">
From each acceptable head-dependents fragment,
we induce a set of lexicalized and unlexicalized
head-dependents rules.
</bodyText>
<subsectionHeader confidence="0.472461">
4.3.1 Lexicalized Rule
</subsectionHeader>
<bodyText confidence="0.997972333333333">
We induce a lexicalized head-dependents rule
from an acceptable head-dependents fragment by
the following procedure:
</bodyText>
<listItem confidence="0.970824333333333">
1. extract the head-dependents relation and mark
the internal nodes as substitution sites. This
forms the input of a head-dependents rule;
2. place the nodes in order according to the head
span of the root and the dependency spans of
the sinks, then replace the internal nodes with
variables and the other nodes with the target
words covered by their head spans. This forms
the output of a head-dependents rule.
</listItem>
<figureCaption confidence="0.5541805">
Figure 4 shows an acceptable head-dependents
fragment and a lexicalized head-dependents rule in-
</figureCaption>
<figure confidence="0.999204952380952">
1: x1:ц⭼ᶟ
: x2:൘
ᡀ࣏
Output:
1 x1 l held fll
successfully x2
Ѯ㹼/VV
{6}{2-10}
[I[FIFA lWorld ]Cup] lheld fl
successfully [ South fi]Africa]
ц⭼ᶟ/NR
{3,4}{2-4}
൘/P
,,
{5,8}{9,10}
ᡀ࣏/AD
{7}{7}
Input:
Ѯ㹼
(x1:ц⭼ᶟ)(x2:൘)(ᡀ࣏) Ѯ㹼
4 x1 held successfully x2
</figure>
<page confidence="0.952776">
220
</page>
<bodyText confidence="0.837737">
duced from it.
</bodyText>
<subsectionHeader confidence="0.490563">
4.3.2 Unlexicalized Rules
</subsectionHeader>
<bodyText confidence="0.999964695652174">
Since head-dependents relations with verbs as
heads typically consist of more than four nodes, em-
ploying only lexicalized head-dependents rules will
result in severe sparseness problem. To alleviate
this problem, we generalize the lexicalized head-
dependents rules and induce rules with unlexicalized
nodes.
As we know, the modification relation of a head-
dependents relation is determined by the edges.
Therefore, we can replace the lexical word of each
node with its categories (i.e. POS) and obtain new
head-dependents relations with unlexicalized nodes
holding the same modification relation. Here we call
the lexicalized and unlexicalized head-dependents
relations as instances of the modification relation.
For a head-dependents relation with m node, we can
produce 2&apos; − 1 instances with unlexicalized nodes.
Each instance represents the modification relation
with a different specification.
Based on this observation, from each lexical-
ized head-dependent rule, we generate new head-
dependents rules with unlexicalized nodes according
to the following principles:
</bodyText>
<listItem confidence="0.9313048">
1. change the aligned part of the target string into
a new variable when turning a head node or a
leaf node into its category;
2. keep the target side unchanged when turning a
internal node into its category.
</listItem>
<bodyText confidence="0.996703714285714">
Restrictions: Since head-dependents relations
with verbs as heads typically consists of more than
four nodes, enumerating all the instances will re-
sult in a massive grammar with too many kinds of
rules and inflexibility in decoding. To alleviate these
problems, we filter the grammar with the following
principles:
</bodyText>
<listItem confidence="0.983007875">
1. nodes of the same type turn into their categories
simultaneously.
2. as for leaf nodes, only those with open class
words can be turned into their categories.
In our experiments of this paper, we only
turn those dependents with POS tag in the
set of {CD,DT,OD,JJ,NN,NR,NT,AD,FW,PN}
into their categories.
</listItem>
<figureCaption confidence="0.967915625">
Figure 5: An illustration of rule generalization. Where
“x1:世 界 杯” and “x2:在” indicate substitution sites
which can be replaced by a subtree rooted at “世界杯”
and “在” respectively. “x3:AD”indicates a substitution
site that can be replaced by a subtree whose root has part-
of-speech “AD”. The underline denotes a leaf node. The
box indicates the starting lexicalized head-dependents
rule.
</figureCaption>
<bodyText confidence="0.599127">
Figure 5 illustrates the rule generalization process
under these restrictions.
</bodyText>
<subsectionHeader confidence="0.468325">
4.3.3 Unaligned Words
</subsectionHeader>
<bodyText confidence="0.99991">
We handle the unaligned words of the target side
by extending the head spans of the lexicalized head
and leaf nodes on both left and right directions.
This procedure is similar with the method of (Och
and Ney, 2004) except that we might extend several
</bodyText>
<figure confidence="0.755278807692308">
(x1:ц⭼ᶟ)(x2:൘)(x3:AD) Ѯ㹼
4 x1 held x3 x2
: x1:ц⭼ᶟ : x2:൘ : x3: AD
:x4:VV
x1 x4 x3 x2
(x1:ц⭼ᶟ)(x2:൘)(x3:AD) x4:VV
4 x1 x4 x3 x2
generalize leaf generalize leaf
generalize internal
: x1:NR :x2:P ᡀ࣏
x1 lheld ll
successfully 2x2
(x1:NR)(x2:P)(ᡀ࣏) Ѯ㹼
4 x1 held successfully x2
:x4:VV
head
:x1:ц⭼ᶟ 2:x2:൘ ᡀ࣏
x1 x4 ll
successfully x2
(x1:ц⭼ᶟ)(x2:൘)(ᡀ࣏) x4:VV
4 x1 x4 successfully x2
generalize internal
: x4:VV
: x1:NR :x2:P ᡀ࣏
x1 x4 ll
successfully x2
(x1:NR)(x2:P)(ᡀ࣏) x4:VV
4 x1 x4 successfully x2
Ѯ㹼
:x1:ц⭼ᶟ :x2:൘ ᡀ࣏
x1 lheld ll
successfully x2
(x1:ц⭼ᶟ)(x2:൘)(ᡀ࣏) Ѯ㹼
4 x1 held successfully x2
Ѯ㹼
generalize
1:x1:NR :x2:P :x3:AD
x1 lheldl x3 x2
(x1:NR)(x2:P)(x3:AD) Ѯ㹼
4 x1 held x3 x2
generalize leaf generalize leaf
Ѯ㹼
1:x1:NR :x2:P :x3:AD
: x4:VV
x1 x4 x3 2x2
(x1:NR)(x2:P)(x3:AD) x4:VV
4 x1 x4 x3 x2
Ѯ㹼
:x1:ц⭼ᶟ
x1 lheld x3 x2
2:x2:൘
: x3: AD
</figure>
<page confidence="0.872703">
221
</page>
<table confidence="0.7172776">
Algorithm 1: Algorithm for Rule Acquisition
Input: Source dependency structure T, target string S, alignment A
Output: Translation rule set R
1 HSet ACCEPTABLE HEAD(T,S,A)
2 DSet ACCEPTABLE DEPENDENT(T,S,A)
</table>
<figure confidence="0.9045886">
3 for each node n E HSet do
4 extract head rules
5 append the extracted rules to R
6 if bn′ E child(n) n′ E DSet
7 then
8 obtain a head-dependent fragment f
9 induce lexicalized and unlexicalized head-dependents rules from f
10 append the induced rules to R
11 end
12 end
</figure>
<bodyText confidence="0.9836798">
spans simultaneously. In this process, we might ob-
tain m(m &gt; 1) head-dependents rules from a head-
dependent fragment in handling unaligned words.
Each of these rules is assigned with a fractional
count 1/m.
</bodyText>
<subsectionHeader confidence="0.94147">
4.4 Algorithm for Rule Acquisition
</subsectionHeader>
<bodyText confidence="0.9999294">
The rule acquisition is a three-step process, which is
summarized in Algorithm 1.
We take the extracted rule set as observed data and
make use of relative frequency estimator to obtain
the translation probabilities P(t|s) and P(s|t).
</bodyText>
<sectionHeader confidence="0.994073" genericHeader="method">
5 The model
</sectionHeader>
<bodyText confidence="0.98911475">
Following (Och and Ney, 2002), we adopt a general
log-linear model. Let d be a derivation that convert
a source dependency structure T into a target string
e. The probability of d is defined as:
</bodyText>
<equation confidence="0.9872395">
P (d) a ∏ Oi(d)λz (1)
i
</equation>
<bodyText confidence="0.989809">
where Oi are features defined on derivations and Xi
are feature weights. In our experiments of this paper,
we used seven features as follows:
- translation probabilities P(t|s) and P(s|t);
- lexical translation probabilities Plex(t|s) and
Plex(s|t);
- rule penalty exp(−1);
- language model Plm(e);
- word penalty exp(|e|).
</bodyText>
<sectionHeader confidence="0.995115" genericHeader="method">
6 Decoding
</sectionHeader>
<bodyText confidence="0.99979225">
Our decoder is based on bottom up chart parsing.
It finds the best derivation d* that convert the input
dependency structure into a target string among all
possible derivations D:
</bodyText>
<equation confidence="0.995124">
d* = argmaxdEDP(D) (2)
</equation>
<bodyText confidence="0.9994893">
Given a source dependency structure T, the decoder
transverses T in post-order. For each accessed in-
ternal node n, it enumerates all instances of the re-
lated modification relation of the head-dependents
relation rooted at n, and checks the rule set for
matched translation rules. If there is no matched
rule, we construct a pseudo translation rule accord-
ing to the word order of the head-dependents rela-
tion. For example, suppose that we can not find
any translation rule about to “(2010年) (FIFA) 世
界杯”, we will construct a pseudo translation rule
“(x1:2010年) (x2:FIFA) x3:世界杯 -+ x1 x2 x3”.
A larger translation is generated by substituting the
variables in the target side of a translation rule with
the translations of the corresponding dependents.
We make use of cube pruning (Chiang, 2007; Huang
and Chiang, 2007) to find the k-best items with inte-
grated language model for each node.
To balance performance and speed, we prune the
search space in several ways. First, beam thresh-
</bodyText>
<page confidence="0.994143">
222
</page>
<bodyText confidence="0.999975555555556">
old 0 , items with a score worse than 0 times of the
best score in the same cell will be discarded; sec-
ond, beam size b, items with a score worse than the
bth best item in the same cell will be discarded. The
item consist of the necessary information used in de-
coding. Each cell contains all the items standing for
the subtree rooted at it. For our experiments, we set
0 = 10−3 and b = 300. Additionally, we also prune
rules that have the same source side (b = 100).
</bodyText>
<sectionHeader confidence="0.998945" genericHeader="method">
7 Experiments
</sectionHeader>
<bodyText confidence="0.99997225">
We evaluated the performance of our dependency-
to-string model by comparison with replications of
the hierarchical phrase-based model and the tree-to-
string models on Chinese-English translation.
</bodyText>
<subsectionHeader confidence="0.995245">
7.1 Data preparation
</subsectionHeader>
<bodyText confidence="0.999930076923077">
Our training corpus consists of 1.5M sentence
pairs from LDC data, including LDC2002E18,
LDC2003E07, LDC2003E14, Hansards portion of
LDC2004T07, LDC2004T08 and LDC2005T06.
We parse the source sentences with Stanford
Parser (Klein and Manning, 2003) into projective
dependency structures, whose nodes are annotated
by POS tags and edges by typed dependencies. In
our implementation of this paper, we make use of
the POS tags only.
We obtain the word alignments by running
GIZA++ (Och and Ney, 2003) on the corpus in
both directions and applying “grow-diag-and” re-
finement(Koehn et al., 2003).
We apply SRI Language Modeling Toolkit (Stol-
cke, 2002) to train a 4-gram language model with
modified Kneser-Ney smoothing on the Xinhua por-
tion of the Gigaword corpus.
We use NIST MT Evaluation test set 2002 as our
development set, NIST MT Evaluation test set 2004
(MT04) and 2005 (MT05) as our test set. The qual-
ity of translations is evaluated by the case insensitive
NIST BLEU-4 metric (Papineni et al., 2002).1
We make use of the standard MERT (Och, 2003)
to tune the feature weights in order to maximize the
system’s BLEU score on the development set.
</bodyText>
<page confidence="0.392433">
1ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
</page>
<table confidence="0.9999045">
System Rule # MT04(%) MT05(%)
cons2str 30M 34.55 31.94
hiero-re 148M 35.29 33.22
dep2str 56M 35.82+ 33.62+
</table>
<tableCaption confidence="0.856806">
Table 1: Statistics of the extracted rules on training cor-
pus and the BLEU scores on the test sets. Where “+”
means dep2str significantly better than cons2str with p &lt;
0.01.
</tableCaption>
<subsectionHeader confidence="0.99934">
7.2 The baseline models
</subsectionHeader>
<bodyText confidence="0.999764083333333">
We take a replication of Hiero (Chiang, 2007) as
the hierarchical phrase-based model baseline. In
our experiments of this paper, we set the beam size
b = 200 and the beam threshold 0 = 0. The maxi-
mum initial phrase length is 10.
We use constituency-to-string model (Liu et al.,
2006) as the syntax-based model baseline which
make use of composed rules (Galley et al., 2006)
without handling the unaligned words. In our exper-
iments of this paper, we set the tatTable-limit=20,
tatTable-threshold=10−1, stack-limit=100, stack-
threshold=10−1,hight-limit=3, and length-limit=7.
</bodyText>
<subsectionHeader confidence="0.899271">
7.3 Results
</subsectionHeader>
<bodyText confidence="0.996077826086956">
We display the results of our experiments in Table
1. Our dependency-to-string model dep2str signif-
icantly outperforms its constituency structure-based
counterpart (cons2str) with +1.27 and +1.68 BLEU
on MT04 and MT05 respectively. Moreover, with-
out resort to phrases or parse forest, dep2str sur-
passes the hierarchical phrase-based model (hiero-
re) over +0.53 and +0.4 BLEU on MT04 and MT05
respectively on the basis of a 62% smaller rule set.
Furthermore, We compare some actual transla-
tions generated by cons2str, hiero-re and dep2str.
Figure 6 shows two translations of our test sets
MT04 and MT05, which are selected because each
holds a long distance dependency commonly used in
Chinese.
In the first example, the Chinese input holds
a complex long distance dependencies “E M
4S fit... Ij...r * T”. This dependency cor-
responds to sentence pattern “noun+prepostional
phrase+prepositional phrase+verb”, where the for-
mer prepositional phrase specifies the position and
the latter specifies the time. Both cons2str and
hiero-re are confused by this sentence and mistak-
</bodyText>
<page confidence="0.998999">
223
</page>
<figureCaption confidence="0.985369333333333">
Figure 6: Actual translations produced by the baselines and our system. For our system, we also display the long
distance dependencies correspondence in Chinese and English. Here we omit the edges irrelevant to the long distance
dependencies.
</figureCaption>
<figure confidence="0.889293118181818">
MT05----Segment 163
Reference: After a brief talk with
Powell at the US State
Department, Barnier said:
Cons2str: Barnier after brief
talks in US State Department
and Powell said:
Hiero-re: After a short meeting
with Barnier on the US State
Department, Powell said:
Dep2str: After brief talks with
Powell, the US State
Department Barnier said,
nsubj
prep prep
After
ibrief
ᐤቬ㙦
ltalks
൘
iwith
㖾ഭ
ഭ࣑䲒
ll Powell,,
о
the
劽ቄ
US
⸝Ჲ Պ䈸
State Department
ਾ
㺘⽪
i
Barnier
˖˖
isaid
,,
MT04----Segment 1096
Reference: China appreciates the
efforts of the Counter-Terrorism
Committee to promote the
implementation of the resolution
1373(2001) in all states and to
help enhance the anti-terrorist
capabilities of developing
countries.
Cons2str: China appreciates
Anti - Terrorist Committee for
promoting implementation of
the resolution No. 1373(2001)
and help developing countries
strength counter-terrorism
capability building for the
efforts,
Hiero-re: China appreciates
Anti - Terrorism Committee to
promote countries implement
resolution No . 1373 ( 2001 )
and help developing countries
strengthen anti-terrorism
capacity building support for
efforts
Dep2str: China appreciates
efforts of Anti - Terrorism
Committee to promote all
countries in the implementation
of resolution 1373 ( 2001 ) , to
help strengthen the anti-
terrorism capability building of
developing countries
nsubj dobj
৽
Ѫ
ㅜ
ڊ
2001 ˅
৽
f
ᡰ
ѝഭ 䎎䍿
ငઈՊ
׳䘋
਴ഭ
ᢗ㹼
㜭࣋
ᔪ䇮
Ⲵ
ࣚ࣋ ˈˈ
˄
1373
ਧ ߣ䇞 ǃǃ ᑞࣙ1 ਁኅѝഭᇦ
iChina
i
appreciates
efforts
of
iAnti -- i
Terrorism
i
Committee
to
promote
llall
i
countries
iin
the
ili
implementation
of......
</figure>
<bodyText confidence="0.999745736842105">
enly treat “-VJK(Powell)” as the subjective, thus
result in translations with different meaning from
the source sentence. Conversely, although “It” is
falsely translated into a comma, dep2str captures
this complex dependency and translates it into “Af-
ter ... ,(should be at) Barnier said”, which accords
with the reordering of the reference.
In the second example, the Chinese input holds
a long distance dependency “+ P9 V 1 ... 9
j7” which corresponds to a simple pattern “noun
phrase+verb+noun phrase”. However, due to the
modifiers of “9 j7” which contains two sub-
sentences including 24 words, the sentence looks
rather complicated. Cons2str and hiero-re fail to
capture this long distance dependency and provide
monotonic translations which do not reflect the
meaning of the source sentence. In contrast, dep2str
successfully captures this long distance dependency
and translates it into “China appreciates efforts of
</bodyText>
<page confidence="0.987048">
224
</page>
<bodyText confidence="0.999977777777778">
...”, which is almost the same with the reference
“China appreciates the efforts of ...”.
All these results prove the effectiveness of our
dependency-to-string model in both translation and
long distance reordering. We believe that the ad-
vantage of dep2str comes from the characteristics of
dependency structures tending to bring semantically
related elements together (e.g., verbs become adja-
cent to all their arguments) and are better suited to
lexicalized models (Quirk et al., 2005). And the in-
capability of cons2str and hiero-re in handling long
distance reordering of these sentences does not lie in
the representation of translation rules but the com-
promises in rule extraction or decoding so as to bal-
ance the speed or grammar size and performance.
The hierarchical phrase-based model prohibits any
nonterminal X from spanning a substring longer
than 10 on the source side to make the decoding al-
gorithm asymptotically linear-time (Chiang, 2005).
While constituency structure-based models typically
constrain the number of internal nodes (Galley et
al., 2006) and/or the height (Liu et al., 2006) of
translation rules so as to balance the grammar size
and performance. Both strategies limit the ability of
the models in processing long distance reordering of
sentences with long and complex modification rela-
tions.
</bodyText>
<sectionHeader confidence="0.999686" genericHeader="method">
8 Related Works
</sectionHeader>
<bodyText confidence="0.999957133333333">
As a first step towards semantics, dependency struc-
tures are attractive to machine translation. And
many efforts have been made to incorporating this
desirable knowledge into machine translation.
(Lin, 2004; Quirk et al., 2005; Ding and Palmer,
2005; Xiong et al., 2007) make use of source depen-
dency structures. (Lin, 2004) employs linear paths
as phrases and view translation as minimal path cov-
ering. (Quirk et al., 2005) extends paths to treelets,
arbitrary connected subgraphs of dependency struc-
tures, and propose a model based on treelet pairs.
Both models require projection of the source depen-
dency structure to the target side via word alignment,
and thus can not handle non-isomorphism between
languages. To alleviate this problem, (Xiong et al.,
2007) presents a dependency treelet string corre-
spondence model which directly map a dependency
structure to a target string. (Ding and Palmer, 2005)
presents a translation model based on Synchronous
Dependency Insertion Grammar(SDIG), which han-
dles some of the non-isomorphism but requires both
source and target dependency structures. Most im-
portant, all these works do not specify the ordering
information directly in translation rules, and resort
to either heuristics (Lin, 2004; Xiong et al., 2007) or
separate ordering models(Quirk et al., 2005; Ding
and Palmer, 2005) to control the word order of
translations. By comparison, our model requires
only source dependency structure, and handles non-
isomorphism and ordering problems simultaneously
by directly specifying the ordering information in
the head-dependents rules that represent the source
side as head-dependents relations and the target side
as strings.
(Shen et al., 2008) exploits target dependency
structures as dependency language models to ensure
the grammaticality of the target string. (Shen et al.,
2008) extends the hierarchical phrase-based model
and present a string-to-dependency model, which
employs string-to-dependency rules whose source
side are string and the target as well-formed depen-
dency structures. In contrast, our model exploits
source dependency structures, as a tree-based sys-
tem, it run much faster (linear time vs. cubic time,
see (Huang et al., 2006)).
</bodyText>
<sectionHeader confidence="0.979094" genericHeader="conclusions">
9 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999957590909091">
In this paper, we present a novel dependency-to-
string model, which employs head-dependents rules
that represent the source side as head-dependents
relations and the target side as string. The head-
dependents rules specify the ordering information
directly and require only substitution operation.
Thus, our model does not need heuristics or order-
ing model of the previous works to control the word
order of translations. Large scale experiments show
that our model exhibits good performance in long
distance reordering and outperforms the state-of-
the-art constituency-to-string model and hierarchi-
cal phrase-based model without resort to phrases and
parse forest. For the first time, a source dependency-
based model shows improvement over the state-of-
the-art translation models.
In our future works, we will exploit the semantic
information encoded in the dependency structures
which is expected to further improve the transla-
tions, and replace 1-best dependency structures with
dependency forests so as to alleviate the influence
caused by parse errors.
</bodyText>
<sectionHeader confidence="0.998439" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996956333333333">
This work was supported by National Natural Sci-
ence Foundation of China, Contract 60736014,
60873167, 90920004. We are grateful to the anony-
mous reviewers for their thorough reviewing and
valuable suggestions. We appreciate Yajuan Lv,
Wenbin Jiang, Hao Xiong, Yang Liu, Xinyan Xiao,
Tian Xia and Yun Huang for the insightful advices in
both experiments and writing. Special thanks goes
to Qian Chen for supporting my pursuit all through.
</bodyText>
<sectionHeader confidence="0.997241" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.8656875">
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
</reference>
<page confidence="0.981805">
225
</page>
<reference confidence="0.999717747126437">
ACL 2005, pages 263–270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of ACL 2005.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In In Proceedings of EMNLP 2002,
pages 304–311.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of ACL 2006, pages 961–968, Sydney, Australia,
July. Association for Computational Linguistics.
Peter Hellwig. 2006. Parsing with dependency gram-
mars. In Dependenz und Valenz /Dependency and Va-
lency, volume 2, pages 1081–1109. Berlin, New York.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language models.
In Proceedings of ACL 2007, pages 144–151, Prague,
Czech Republic, June.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A syntax-directed translator with extended domain of
locality. In Proceedings of the Workshop on Computa-
tionally Hard Problems and Joint Inference in Speech
and Language Processing, pages 1–8, New York City,
New York, June. Association for Computational Lin-
guistics.
Richard Hudson. 1990. English Word Grammar. Black-
ell.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In In Advances in Neural Information Pro-
cessing Systems 15 (NIPS, pages 3–10. MIT Press.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of the 2003 Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics, Edmonton, Canada, July.
Dekang Lin. 2004. A path-based transfer model for
machine translation. In Proceedings of Coling 2004,
pages 625–630, Geneva, Switzerland, Aug 23–Aug
27.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of ACL 2006, pages 609–616,
Sydney, Australia, July.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295–302, Philadelphia, Pennsylva-
nia, USA, July.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Franz Josef Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings ofACL-
2003, pages 160–167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
ACL 2002, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL 2005, pages 271–
279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of ACL 2008: HLT, pages 577–585,
Columbus, Ohio, June. Association for Computational
Linguistics.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings ofICSLP, volume 30,
pages 901–904.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A depen-
dency treelet string correspondence model for statisti-
cal machine translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
40–47, Prague, Czech Republic, June.
Arnold M. Zwicky. 1985. Heads. Journal of Linguistics,
21:1–29.
</reference>
<page confidence="0.998846">
226
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.742049">
<title confidence="0.999772">A Novel Dependency-to-String Model for Statistical Machine Translation</title>
<author confidence="0.916143">Jun Xie</author>
<author confidence="0.916143">Haitao Mi</author>
<author confidence="0.916143">Qun</author>
<affiliation confidence="0.9239885">Key Laboratory of Intelligent Information Institute of Computing</affiliation>
<address confidence="0.9296205">Chinese Academy of P.O. Box 2704, Beijing 100190,</address>
<abstract confidence="0.999546">Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the stateof-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>263--270</pages>
<contexts>
<context position="2245" citStr="Chiang, 2005" startWordPosition="320" endWordPosition="321">nd has the best inter-lingual phrasal cohesion properties (Fox, 2002). Those attractive characteristics make it possible to improve translation quality by using dependency structures. Some researchers pay more attention to use dependency structure on the target side. (Shen et al., 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristi</context>
<context position="29896" citStr="Chiang, 2005" startWordPosition="4837" endWordPosition="4838">ly related elements together (e.g., verbs become adjacent to all their arguments) and are better suited to lexicalized models (Quirk et al., 2005). And the incapability of cons2str and hiero-re in handling long distance reordering of these sentences does not lie in the representation of translation rules but the compromises in rule extraction or decoding so as to balance the speed or grammar size and performance. The hierarchical phrase-based model prohibits any nonterminal X from spanning a substring longer than 10 on the source side to make the decoding algorithm asymptotically linear-time (Chiang, 2005). While constituency structure-based models typically constrain the number of internal nodes (Galley et al., 2006) and/or the height (Liu et al., 2006) of translation rules so as to balance the grammar size and performance. Both strategies limit the ability of the models in processing long distance reordering of sentences with long and complex modification relations. 8 Related Works As a first step towards semantics, dependency structures are attractive to machine translation. And many efforts have been made to incorporating this desirable knowledge into machine translation. (Lin, 2004; Quirk </context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL 2005, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<contexts>
<context position="22128" citStr="Chiang, 2007" startWordPosition="3601" endWordPosition="3602">of the head-dependents relation rooted at n, and checks the rule set for matched translation rules. If there is no matched rule, we construct a pseudo translation rule according to the word order of the head-dependents relation. For example, suppose that we can not find any translation rule about to “(2010年) (FIFA) 世 界杯”, we will construct a pseudo translation rule “(x1:2010年) (x2:FIFA) x3:世界杯 -+ x1 x2 x3”. A larger translation is generated by substituting the variables in the target side of a translation rule with the translations of the corresponding dependents. We make use of cube pruning (Chiang, 2007; Huang and Chiang, 2007) to find the k-best items with integrated language model for each node. To balance performance and speed, we prune the search space in several ways. First, beam thresh222 old 0 , items with a score worse than 0 times of the best score in the same cell will be discarded; second, beam size b, items with a score worse than the bth best item in the same cell will be discarded. The item consist of the necessary information used in decoding. Each cell contains all the items standing for the subtree rooted at it. For our experiments, we set 0 = 10−3 and b = 300. Additionally,</context>
<context position="24576" citStr="Chiang, 2007" startWordPosition="4009" endWordPosition="4010">he case insensitive NIST BLEU-4 metric (Papineni et al., 2002).1 We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maximize the system’s BLEU score on the development set. 1ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl System Rule # MT04(%) MT05(%) cons2str 30M 34.55 31.94 hiero-re 148M 35.29 33.22 dep2str 56M 35.82+ 33.62+ Table 1: Statistics of the extracted rules on training corpus and the BLEU scores on the test sets. Where “+” means dep2str significantly better than cons2str with p &lt; 0.01. 7.2 The baseline models We take a replication of Hiero (Chiang, 2007) as the hierarchical phrase-based model baseline. In our experiments of this paper, we set the beam size b = 200 and the beam threshold 0 = 0. The maximum initial phrase length is 10. We use constituency-to-string model (Liu et al., 2006) as the syntax-based model baseline which make use of composed rules (Galley et al., 2006) without handling the unaligned words. In our experiments of this paper, we set the tatTable-limit=20, tatTable-threshold=10−1, stack-limit=100, stackthreshold=10−1,hight-limit=3, and length-limit=7. 7.3 Results We display the results of our experiments in Table 1. Our de</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="2596" citStr="Ding and Palmer, 2005" startWordPosition="375" endWordPosition="378">the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristics (Lin, 2004; Xiong et al., 2007) or separate ordering models (Quirk et al., 2005; Ding and Palmer, 2005) to control the word order of translations. In this paper, we handle this problem by directly specifying the ordering information in headdependents rules that represent the source side as head-dependents relations and the target side as string. </context>
<context position="30531" citStr="Ding and Palmer, 2005" startWordPosition="4932" endWordPosition="4935">tuency structure-based models typically constrain the number of internal nodes (Galley et al., 2006) and/or the height (Liu et al., 2006) of translation rules so as to balance the grammar size and performance. Both strategies limit the ability of the models in processing long distance reordering of sentences with long and complex modification relations. 8 Related Works As a first step towards semantics, dependency structures are attractive to machine translation. And many efforts have been made to incorporating this desirable knowledge into machine translation. (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) make use of source dependency structures. (Lin, 2004) employs linear paths as phrases and view translation as minimal path covering. (Quirk et al., 2005) extends paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on treelet pairs. Both models require projection of the source dependency structure to the target side via word alignment, and thus can not handle non-isomorphism between languages. To alleviate this problem, (Xiong et al., 2007) presents a dependency treelet string correspondence model which directly map a depend</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proceedings of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation. In</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>304--311</pages>
<contexts>
<context position="1701" citStr="Fox, 2002" startWordPosition="240" endWordPosition="241">g distance reordering, and outperforms the stateof-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models. 1 Introduction Dependency structure represents the grammatical relations that hold between the words in a sentence. It encodes semantic relations directly, and has the best inter-lingual phrasal cohesion properties (Fox, 2002). Those attractive characteristics make it possible to improve translation quality by using dependency structures. Some researchers pay more attention to use dependency structure on the target side. (Shen et al., 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cub</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J. Fox. 2002. Phrasal cohesion and statistical machine translation. In In Proceedings of EMNLP 2002, pages 304–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL 2006,</booktitle>
<pages>961--968</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="24904" citStr="Galley et al., 2006" startWordPosition="4065" endWordPosition="4068">ro-re 148M 35.29 33.22 dep2str 56M 35.82+ 33.62+ Table 1: Statistics of the extracted rules on training corpus and the BLEU scores on the test sets. Where “+” means dep2str significantly better than cons2str with p &lt; 0.01. 7.2 The baseline models We take a replication of Hiero (Chiang, 2007) as the hierarchical phrase-based model baseline. In our experiments of this paper, we set the beam size b = 200 and the beam threshold 0 = 0. The maximum initial phrase length is 10. We use constituency-to-string model (Liu et al., 2006) as the syntax-based model baseline which make use of composed rules (Galley et al., 2006) without handling the unaligned words. In our experiments of this paper, we set the tatTable-limit=20, tatTable-threshold=10−1, stack-limit=100, stackthreshold=10−1,hight-limit=3, and length-limit=7. 7.3 Results We display the results of our experiments in Table 1. Our dependency-to-string model dep2str significantly outperforms its constituency structure-based counterpart (cons2str) with +1.27 and +1.68 BLEU on MT04 and MT05 respectively. Moreover, without resort to phrases or parse forest, dep2str surpasses the hierarchical phrase-based model (hierore) over +0.53 and +0.4 BLEU on MT04 and MT</context>
<context position="30010" citStr="Galley et al., 2006" startWordPosition="4850" endWordPosition="4853">lexicalized models (Quirk et al., 2005). And the incapability of cons2str and hiero-re in handling long distance reordering of these sentences does not lie in the representation of translation rules but the compromises in rule extraction or decoding so as to balance the speed or grammar size and performance. The hierarchical phrase-based model prohibits any nonterminal X from spanning a substring longer than 10 on the source side to make the decoding algorithm asymptotically linear-time (Chiang, 2005). While constituency structure-based models typically constrain the number of internal nodes (Galley et al., 2006) and/or the height (Liu et al., 2006) of translation rules so as to balance the grammar size and performance. Both strategies limit the ability of the models in processing long distance reordering of sentences with long and complex modification relations. 8 Related Works As a first step towards semantics, dependency structures are attractive to machine translation. And many efforts have been made to incorporating this desirable knowledge into machine translation. (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) make use of source dependency structures. (Lin, 2004) emp</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of ACL 2006, pages 961–968, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Hellwig</author>
</authors>
<title>Parsing with dependency grammars.</title>
<date>2006</date>
<booktitle>In Dependenz und Valenz /Dependency and Valency,</booktitle>
<volume>2</volume>
<pages>1081--1109</pages>
<location>Berlin, New York.</location>
<contexts>
<context position="4942" citStr="Hellwig, 2006" startWordPosition="737" endWordPosition="738">s up with and surpasses the stateof-the-art translation models. 2 Dependency Structure and Head-Dependents Relation 2.1 Dependency Sturcture A dependency structure for a sentence is a directed acyclic graph with words as nodes and modification relations as edges. Each edge direct from a head to a dependent. Figure 1 (a) shows an example dependency structure of a Chinese sentence. 2010年 FIFA 世界杯 在 南非 成功 举行 2010 FIFA [World Cup] in/at [South Africa] successfully hold Each node is annotated with the part-of-speech (POS) of the related word. For convenience, we use the lexicon dependency grammar (Hellwig, 2006) which adopts a bracket representation to express a projective dependency structure. The dependency structure of Figure 1 (a) can be expressed as: ((2010年) (FIFA) 世界杯) (在(南非)) (成功) 举行 where the lexicon in brackets represents the dependents, while the lexicon out the brackets is the head. To construct the dependency structure of a sentence, the most important thing is to establish dependency relations and distinguish the head from the dependent. Here are some criteria (Zwicky, 1985; Ѯ㹼/VV 2010ᒤ/NT I ই䶎/NR FIFA/NR Ѯ㹼/VV Figure 1: Examples of dependency structure (a), headdependents relation (b),</context>
</contexts>
<marker>Hellwig, 2006</marker>
<rawString>Peter Hellwig. 2006. Parsing with dependency grammars. In Dependenz und Valenz /Dependency and Valency, volume 2, pages 1081–1109. Berlin, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL 2007,</booktitle>
<pages>144--151</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="22153" citStr="Huang and Chiang, 2007" startWordPosition="3603" endWordPosition="3606">pendents relation rooted at n, and checks the rule set for matched translation rules. If there is no matched rule, we construct a pseudo translation rule according to the word order of the head-dependents relation. For example, suppose that we can not find any translation rule about to “(2010年) (FIFA) 世 界杯”, we will construct a pseudo translation rule “(x1:2010年) (x2:FIFA) x3:世界杯 -+ x1 x2 x3”. A larger translation is generated by substituting the variables in the target side of a translation rule with the translations of the corresponding dependents. We make use of cube pruning (Chiang, 2007; Huang and Chiang, 2007) to find the k-best items with integrated language model for each node. To balance performance and speed, we prune the search space in several ways. First, beam thresh222 old 0 , items with a score worse than 0 times of the best score in the same cell will be discarded; second, beam size b, items with a score worse than the bth best item in the same cell will be discarded. The item consist of the necessary information used in decoding. Each cell contains all the items standing for the subtree rooted at it. For our experiments, we set 0 = 10−3 and b = 300. Additionally, we also prune rules that</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of ACL 2007, pages 144–151, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>A syntax-directed translator with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, New York,</location>
<contexts>
<context position="2329" citStr="Huang et al., 2006" startWordPosition="332" endWordPosition="335">ttractive characteristics make it possible to improve translation quality by using dependency structures. Some researchers pay more attention to use dependency structure on the target side. (Shen et al., 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristics (Lin, 2004; Xiong et al., 2007) or separate ordering models (Quirk et al., 2005; </context>
<context position="32485" citStr="Huang et al., 2006" startWordPosition="5225" endWordPosition="5228"> that represent the source side as head-dependents relations and the target side as strings. (Shen et al., 2008) exploits target dependency structures as dependency language models to ensure the grammaticality of the target string. (Shen et al., 2008) extends the hierarchical phrase-based model and present a string-to-dependency model, which employs string-to-dependency rules whose source side are string and the target as well-formed dependency structures. In contrast, our model exploits source dependency structures, as a tree-based system, it run much faster (linear time vs. cubic time, see (Huang et al., 2006)). 9 Conclusions and future work In this paper, we present a novel dependency-tostring model, which employs head-dependents rules that represent the source side as head-dependents relations and the target side as string. The headdependents rules specify the ordering information directly and require only substitution operation. Thus, our model does not need heuristics or ordering model of the previous works to control the word order of translations. Large scale experiments show that our model exhibits good performance in long distance reordering and outperforms the state-ofthe-art constituency-</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. A syntax-directed translator with extended domain of locality. In Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing, pages 1–8, New York City, New York, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Hudson</author>
</authors>
<date>1990</date>
<journal>English Word Grammar. Blackell.</journal>
<contexts>
<context position="5884" citStr="Hudson, 1990" startWordPosition="889" endWordPosition="890"> a sentence, the most important thing is to establish dependency relations and distinguish the head from the dependent. Here are some criteria (Zwicky, 1985; Ѯ㹼/VV 2010ᒤ/NT I ই䶎/NR FIFA/NR Ѯ㹼/VV Figure 1: Examples of dependency structure (a), headdependents relation (b), head-dependents rule (r1 of Figure 2) and head rule (d). Where “x1:世 界 杯” and “x2:在” indicate substitution sites which can be replaced by a subtree rooted at “世界杯” and “在” respectively. “x3:AD”indicates a substitution site that can be replaced by a subtree whose root has part-of-speech “AD”. The underline denotes a leaf node. Hudson, 1990) for identifying a syntactic relation between a head and a dependent between a headdependent pair: 1. head determines the syntactic category of C, and can often replace C; 2. head determines the semantic category of C; dependent gives semantic specification. 2.2 Head-Dependents Relation A head-dependents relation is composed of a head and all its dependents as shown in Figure 1(b). Since all the head-dependent pairs satisfy criteria 1 and 2, we can deduce that a head-dependents relation L holds the property that the head determines the syntactic and semantic categories of L, and can often repl</context>
</contexts>
<marker>Hudson, 1990</marker>
<rawString>Richard Hudson. 1990. English Word Grammar. Blackell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In In Advances in Neural Information Processing Systems 15 (NIPS,</booktitle>
<pages>3--10</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="23268" citStr="Klein and Manning, 2003" startWordPosition="3792" endWordPosition="3795">ubtree rooted at it. For our experiments, we set 0 = 10−3 and b = 300. Additionally, we also prune rules that have the same source side (b = 100). 7 Experiments We evaluated the performance of our dependencyto-string model by comparison with replications of the hierarchical phrase-based model and the tree-tostring models on Chinese-English translation. 7.1 Data preparation Our training corpus consists of 1.5M sentence pairs from LDC data, including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. We parse the source sentences with Stanford Parser (Klein and Manning, 2003) into projective dependency structures, whose nodes are annotated by POS tags and edges by typed dependencies. In our implementation of this paper, we make use of the POS tags only. We obtain the word alignments by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and applying “grow-diag-and” refinement(Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We use NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In In Advances in Neural Information Processing Systems 15 (NIPS, pages 3–10. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="23610" citStr="Koehn et al., 2003" startWordPosition="3847" endWordPosition="3851">lation. 7.1 Data preparation Our training corpus consists of 1.5M sentence pairs from LDC data, including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. We parse the source sentences with Stanford Parser (Klein and Manning, 2003) into projective dependency structures, whose nodes are annotated by POS tags and edges by typed dependencies. In our implementation of this paper, we make use of the POS tags only. We obtain the word alignments by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and applying “grow-diag-and” refinement(Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We use NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test set 2004 (MT04) and 2005 (MT05) as our test set. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric (Papineni et al., 2002).1 We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maximize the system’s BLEU score on the development set. 1ftp://jaguar.ncsl.nist.gov/mt/resources</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Edmonton, Canada, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A path-based transfer model for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>625--630</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="2553" citStr="Lin, 2004" startWordPosition="369" endWordPosition="370">endency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristics (Lin, 2004; Xiong et al., 2007) or separate ordering models (Quirk et al., 2005; Ding and Palmer, 2005) to control the word order of translations. In this paper, we handle this problem by directly specifying the ordering information in headdependents rules that represent the source side as head-dependent</context>
<context position="30488" citStr="Lin, 2004" startWordPosition="4926" endWordPosition="4927">me (Chiang, 2005). While constituency structure-based models typically constrain the number of internal nodes (Galley et al., 2006) and/or the height (Liu et al., 2006) of translation rules so as to balance the grammar size and performance. Both strategies limit the ability of the models in processing long distance reordering of sentences with long and complex modification relations. 8 Related Works As a first step towards semantics, dependency structures are attractive to machine translation. And many efforts have been made to incorporating this desirable knowledge into machine translation. (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) make use of source dependency structures. (Lin, 2004) employs linear paths as phrases and view translation as minimal path covering. (Quirk et al., 2005) extends paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on treelet pairs. Both models require projection of the source dependency structure to the target side via word alignment, and thus can not handle non-isomorphism between languages. To alleviate this problem, (Xiong et al., 2007) presents a dependency treelet string corre</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Dekang Lin. 2004. A path-based transfer model for machine translation. In Proceedings of Coling 2004, pages 625–630, Geneva, Switzerland, Aug 23–Aug 27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL 2006,</booktitle>
<pages>609--616</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="24814" citStr="Liu et al., 2006" startWordPosition="4050" endWordPosition="4053">/mt/resources/mteval-v11b.pl System Rule # MT04(%) MT05(%) cons2str 30M 34.55 31.94 hiero-re 148M 35.29 33.22 dep2str 56M 35.82+ 33.62+ Table 1: Statistics of the extracted rules on training corpus and the BLEU scores on the test sets. Where “+” means dep2str significantly better than cons2str with p &lt; 0.01. 7.2 The baseline models We take a replication of Hiero (Chiang, 2007) as the hierarchical phrase-based model baseline. In our experiments of this paper, we set the beam size b = 200 and the beam threshold 0 = 0. The maximum initial phrase length is 10. We use constituency-to-string model (Liu et al., 2006) as the syntax-based model baseline which make use of composed rules (Galley et al., 2006) without handling the unaligned words. In our experiments of this paper, we set the tatTable-limit=20, tatTable-threshold=10−1, stack-limit=100, stackthreshold=10−1,hight-limit=3, and length-limit=7. 7.3 Results We display the results of our experiments in Table 1. Our dependency-to-string model dep2str significantly outperforms its constituency structure-based counterpart (cons2str) with +1.27 and +1.68 BLEU on MT04 and MT05 respectively. Moreover, without resort to phrases or parse forest, dep2str surpa</context>
<context position="30047" citStr="Liu et al., 2006" startWordPosition="4857" endWordPosition="4860"> And the incapability of cons2str and hiero-re in handling long distance reordering of these sentences does not lie in the representation of translation rules but the compromises in rule extraction or decoding so as to balance the speed or grammar size and performance. The hierarchical phrase-based model prohibits any nonterminal X from spanning a substring longer than 10 on the source side to make the decoding algorithm asymptotically linear-time (Chiang, 2005). While constituency structure-based models typically constrain the number of internal nodes (Galley et al., 2006) and/or the height (Liu et al., 2006) of translation rules so as to balance the grammar size and performance. Both strategies limit the ability of the models in processing long distance reordering of sentences with long and complex modification relations. 8 Related Works As a first step towards semantics, dependency structures are attractive to machine translation. And many efforts have been made to incorporating this desirable knowledge into machine translation. (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) make use of source dependency structures. (Lin, 2004) employs linear paths as phrases and view</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of ACL 2006, pages 609–616, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="20608" citStr="Och and Ney, 2002" startWordPosition="3346" endWordPosition="3349">calized and unlexicalized head-dependents rules from f 10 append the induced rules to R 11 end 12 end spans simultaneously. In this process, we might obtain m(m &gt; 1) head-dependents rules from a headdependent fragment in handling unaligned words. Each of these rules is assigned with a fractional count 1/m. 4.4 Algorithm for Rule Acquisition The rule acquisition is a three-step process, which is summarized in Algorithm 1. We take the extracted rule set as observed data and make use of relative frequency estimator to obtain the translation probabilities P(t|s) and P(s|t). 5 The model Following (Och and Ney, 2002), we adopt a general log-linear model. Let d be a derivation that convert a source dependency structure T into a target string e. The probability of d is defined as: P (d) a ∏ Oi(d)λz (1) i where Oi are features defined on derivations and Xi are feature weights. In our experiments of this paper, we used seven features as follows: - translation probabilities P(t|s) and P(s|t); - lexical translation probabilities Plex(t|s) and Plex(s|t); - rule penalty exp(−1); - language model Plm(e); - word penalty exp(|e|). 6 Decoding Our decoder is based on bottom up chart parsing. It finds the best derivati</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 295–302, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="23517" citStr="Och and Ney, 2003" startWordPosition="3834" endWordPosition="3837">of the hierarchical phrase-based model and the tree-tostring models on Chinese-English translation. 7.1 Data preparation Our training corpus consists of 1.5M sentence pairs from LDC data, including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. We parse the source sentences with Stanford Parser (Klein and Manning, 2003) into projective dependency structures, whose nodes are annotated by POS tags and edges by typed dependencies. In our implementation of this paper, we make use of the POS tags only. We obtain the word alignments by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and applying “grow-diag-and” refinement(Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We use NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test set 2004 (MT04) and 2005 (MT05) as our test set. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric (Papineni et al., 2002).1 We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maxi</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<contexts>
<context position="12982" citStr="Och and Ney, 2004" startWordPosition="2125" endWordPosition="2128">t words aligned to n. For example, hsp(2010年)={1, 51, which corresponds to the target words “2010” and “was”. Definition 2. A head span hsp(n) is consistent if it satisfies the following property: bn′̸�nhsp(n′) n hsp(n) = 0. If the head spans of all the nodes of T′ is not consistent, dsp(n) = 0. For example, since hsp(在) is not consistent, dsp(在)=dsp(南非)={9,101, which corresponds to the target words “South” and “Africa”. The tree annotation can be accomplished by a single postorder transversal of T. The extraction of head rules from each node can be readily achieved with the same criteria as (Och and Ney, 2004). In 219 the following, we focus on head-dependents rules acquisition. 4.2 Head-Dependents Fragments Identification We then identify the head-dependents fragments that are suitable for rule induction from the annotated dependency structure. To facilitate the identification process, we first define two sets of dependency structure related to head spans and dependency spans. Definition 5. A acceptable head set ahs(T) of a dependency structure T is a set of nodes, each of which has a consistent head span. For example, the elements of the acceptable head set of the dependency structure in Figure 3</context>
<context position="18731" citStr="Och and Ney, 2004" startWordPosition="3023" endWordPosition="3026">ate substitution sites which can be replaced by a subtree rooted at “世界杯” and “在” respectively. “x3:AD”indicates a substitution site that can be replaced by a subtree whose root has partof-speech “AD”. The underline denotes a leaf node. The box indicates the starting lexicalized head-dependents rule. Figure 5 illustrates the rule generalization process under these restrictions. 4.3.3 Unaligned Words We handle the unaligned words of the target side by extending the head spans of the lexicalized head and leaf nodes on both left and right directions. This procedure is similar with the method of (Och and Ney, 2004) except that we might extend several (x1:цᶟ)(x2:)(x3:AD) Ѯ㹼 4 x1 held x3 x2 : x1:цᶟ : x2: : x3: AD :x4:VV x1 x4 x3 x2 (x1:цᶟ)(x2:)(x3:AD) x4:VV 4 x1 x4 x3 x2 generalize leaf generalize leaf generalize internal : x1:NR :x2:P ᡀ x1 lheld ll successfully 2x2 (x1:NR)(x2:P)(ᡀ) Ѯ㹼 4 x1 held successfully x2 :x4:VV head :x1:цᶟ 2:x2: ᡀ x1 x4 ll successfully x2 (x1:цᶟ)(x2:)(ᡀ) x4:VV 4 x1 x4 successfully x2 generalize internal : x4:VV : x1:NR :x2:P ᡀ x1 x4 ll successfully x2 (x1:NR)(x2:P)(ᡀ) x4:VV 4 x1 x4 successfully x2 Ѯ㹼 :x1:цᶟ :x2: ᡀ x1 lheld ll successfully x2 (x1:цᶟ)(x2:)(ᡀ) Ѯ㹼</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL2003,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="24072" citStr="Och, 2003" startWordPosition="3931" endWordPosition="3932">e word alignments by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and applying “grow-diag-and” refinement(Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We use NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test set 2004 (MT04) and 2005 (MT05) as our test set. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric (Papineni et al., 2002).1 We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maximize the system’s BLEU score on the development set. 1ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl System Rule # MT04(%) MT05(%) cons2str 30M 34.55 31.94 hiero-re 148M 35.29 33.22 dep2str 56M 35.82+ 33.62+ Table 1: Statistics of the extracted rules on training corpus and the BLEU scores on the test sets. Where “+” means dep2str significantly better than cons2str with p &lt; 0.01. 7.2 The baseline models We take a replication of Hiero (Chiang, 2007) as the hierarchical phrase-based model baseline. In our experiments of this paper, we set the b</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings ofACL2003, pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL 2002,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="24025" citStr="Papineni et al., 2002" startWordPosition="3920" endWordPosition="3923"> this paper, we make use of the POS tags only. We obtain the word alignments by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and applying “grow-diag-and” refinement(Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We use NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test set 2004 (MT04) and 2005 (MT05) as our test set. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric (Papineni et al., 2002).1 We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maximize the system’s BLEU score on the development set. 1ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl System Rule # MT04(%) MT05(%) cons2str 30M 34.55 31.94 hiero-re 148M 35.29 33.22 dep2str 56M 35.82+ 33.62+ Table 1: Statistics of the extracted rules on training corpus and the BLEU scores on the test sets. Where “+” means dep2str significantly better than cons2str with p &lt; 0.01. 7.2 The baseline models We take a replication of Hiero (Chiang, 2007) as the hierarchical phrase-based model baseline.</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL 2002, pages 311–318, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal smt.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>271--279</pages>
<contexts>
<context position="2573" citStr="Quirk et al., 2005" startWordPosition="371" endWordPosition="374">el, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristics (Lin, 2004; Xiong et al., 2007) or separate ordering models (Quirk et al., 2005; Ding and Palmer, 2005) to control the word order of translations. In this paper, we handle this problem by directly specifying the ordering information in headdependents rules that represent the source side as head-dependents relations and the </context>
<context position="29429" citStr="Quirk et al., 2005" startWordPosition="4760" endWordPosition="4763">e. In contrast, dep2str successfully captures this long distance dependency and translates it into “China appreciates efforts of 224 ...”, which is almost the same with the reference “China appreciates the efforts of ...”. All these results prove the effectiveness of our dependency-to-string model in both translation and long distance reordering. We believe that the advantage of dep2str comes from the characteristics of dependency structures tending to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and are better suited to lexicalized models (Quirk et al., 2005). And the incapability of cons2str and hiero-re in handling long distance reordering of these sentences does not lie in the representation of translation rules but the compromises in rule extraction or decoding so as to balance the speed or grammar size and performance. The hierarchical phrase-based model prohibits any nonterminal X from spanning a substring longer than 10 on the source side to make the decoding algorithm asymptotically linear-time (Chiang, 2005). While constituency structure-based models typically constrain the number of internal nodes (Galley et al., 2006) and/or the height </context>
<context position="30706" citStr="Quirk et al., 2005" startWordPosition="4962" endWordPosition="4965">he grammar size and performance. Both strategies limit the ability of the models in processing long distance reordering of sentences with long and complex modification relations. 8 Related Works As a first step towards semantics, dependency structures are attractive to machine translation. And many efforts have been made to incorporating this desirable knowledge into machine translation. (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) make use of source dependency structures. (Lin, 2004) employs linear paths as phrases and view translation as minimal path covering. (Quirk et al., 2005) extends paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on treelet pairs. Both models require projection of the source dependency structure to the target side via word alignment, and thus can not handle non-isomorphism between languages. To alleviate this problem, (Xiong et al., 2007) presents a dependency treelet string correspondence model which directly map a dependency structure to a target string. (Ding and Palmer, 2005) presents a translation model based on Synchronous Dependency Insertion Grammar(SDIG), which handles some of the non-</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt. In Proceedings of ACL 2005, pages 271– 279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL 2008: HLT,</booktitle>
<pages>577--585</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1919" citStr="Shen et al., 2008" startWordPosition="272" endWordPosition="275">ithout resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models. 1 Introduction Dependency structure represents the grammatical relations that hold between the words in a sentence. It encodes semantic relations directly, and has the best inter-lingual phrasal cohesion properties (Fox, 2002). Those attractive characteristics make it possible to improve translation quality by using dependency structures. Some researchers pay more attention to use dependency structure on the target side. (Shen et al., 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency</context>
<context position="31978" citStr="Shen et al., 2008" startWordPosition="5151" endWordPosition="5154">ency structures. Most important, all these works do not specify the ordering information directly in translation rules, and resort to either heuristics (Lin, 2004; Xiong et al., 2007) or separate ordering models(Quirk et al., 2005; Ding and Palmer, 2005) to control the word order of translations. By comparison, our model requires only source dependency structure, and handles nonisomorphism and ordering problems simultaneously by directly specifying the ordering information in the head-dependents rules that represent the source side as head-dependents relations and the target side as strings. (Shen et al., 2008) exploits target dependency structures as dependency language models to ensure the grammaticality of the target string. (Shen et al., 2008) extends the hierarchical phrase-based model and present a string-to-dependency model, which employs string-to-dependency rules whose source side are string and the target as well-formed dependency structures. In contrast, our model exploits source dependency structures, as a tree-based system, it run much faster (linear time vs. cubic time, see (Huang et al., 2006)). 9 Conclusions and future work In this paper, we present a novel dependency-tostring model,</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of ACL 2008: HLT, pages 577–585, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings ofICSLP,</booktitle>
<volume>30</volume>
<pages>901--904</pages>
<contexts>
<context position="23666" citStr="Stolcke, 2002" startWordPosition="3858" endWordPosition="3860">1.5M sentence pairs from LDC data, including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. We parse the source sentences with Stanford Parser (Klein and Manning, 2003) into projective dependency structures, whose nodes are annotated by POS tags and edges by typed dependencies. In our implementation of this paper, we make use of the POS tags only. We obtain the word alignments by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and applying “grow-diag-and” refinement(Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We use NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test set 2004 (MT04) and 2005 (MT05) as our test set. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric (Papineni et al., 2002).1 We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maximize the system’s BLEU score on the development set. 1ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl System Rule # MT04(%) MT05(%) cons2str 3</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings ofICSLP, volume 30, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>A dependency treelet string correspondence model for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>40--47</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2617" citStr="Xiong et al., 2007" startWordPosition="379" endWordPosition="382"> hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. This model significantly outperforms the state-of-the-art hierarchical phrasebased model (Chiang, 2005). However, those stringto-tree systems run slowly in cubic time (Huang et al., 2006). Using dependency structure on the source side is also a promising way, as tree-based systems run much faster (linear time vs. cubic time, see (Huang et al., 2006)). Conventional dependency structure based models (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) typically employ both substitution and insertion operation to complete translations, which make it difficult to specify ordering information directly in the translation rules. As a result, they have to resort to either heuristics (Lin, 2004; Xiong et al., 2007) or separate ordering models (Quirk et al., 2005; Ding and Palmer, 2005) to control the word order of translations. In this paper, we handle this problem by directly specifying the ordering information in headdependents rules that represent the source side as head-dependents relations and the target side as string. The head-dependents r</context>
<context position="30552" citStr="Xiong et al., 2007" startWordPosition="4936" endWordPosition="4939">models typically constrain the number of internal nodes (Galley et al., 2006) and/or the height (Liu et al., 2006) of translation rules so as to balance the grammar size and performance. Both strategies limit the ability of the models in processing long distance reordering of sentences with long and complex modification relations. 8 Related Works As a first step towards semantics, dependency structures are attractive to machine translation. And many efforts have been made to incorporating this desirable knowledge into machine translation. (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Xiong et al., 2007) make use of source dependency structures. (Lin, 2004) employs linear paths as phrases and view translation as minimal path covering. (Quirk et al., 2005) extends paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on treelet pairs. Both models require projection of the source dependency structure to the target side via word alignment, and thus can not handle non-isomorphism between languages. To alleviate this problem, (Xiong et al., 2007) presents a dependency treelet string correspondence model which directly map a dependency structure to a t</context>
</contexts>
<marker>Xiong, Liu, Lin, 2007</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A dependency treelet string correspondence model for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 40–47, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnold M Zwicky</author>
</authors>
<date>1985</date>
<journal>Heads. Journal of Linguistics,</journal>
<pages>21--1</pages>
<contexts>
<context position="5427" citStr="Zwicky, 1985" startWordPosition="815" endWordPosition="816">tated with the part-of-speech (POS) of the related word. For convenience, we use the lexicon dependency grammar (Hellwig, 2006) which adopts a bracket representation to express a projective dependency structure. The dependency structure of Figure 1 (a) can be expressed as: ((2010年) (FIFA) 世界杯) (在(南非)) (成功) 举行 where the lexicon in brackets represents the dependents, while the lexicon out the brackets is the head. To construct the dependency structure of a sentence, the most important thing is to establish dependency relations and distinguish the head from the dependent. Here are some criteria (Zwicky, 1985; Ѯ㹼/VV 2010ᒤ/NT I ই䶎/NR FIFA/NR Ѯ㹼/VV Figure 1: Examples of dependency structure (a), headdependents relation (b), head-dependents rule (r1 of Figure 2) and head rule (d). Where “x1:世 界 杯” and “x2:在” indicate substitution sites which can be replaced by a subtree rooted at “世界杯” and “在” respectively. “x3:AD”indicates a substitution site that can be replaced by a subtree whose root has part-of-speech “AD”. The underline denotes a leaf node. Hudson, 1990) for identifying a syntactic relation between a head and a dependent between a headdependent pair: 1. head determines the syntactic category of</context>
</contexts>
<marker>Zwicky, 1985</marker>
<rawString>Arnold M. Zwicky. 1985. Heads. Journal of Linguistics, 21:1–29.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>