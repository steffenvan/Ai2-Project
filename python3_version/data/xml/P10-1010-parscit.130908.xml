<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015357">
<title confidence="0.996363">
The Human Language Project:
Building a Universal Corpus of the World’s Languages
</title>
<author confidence="0.999305">
Steven Abney Steven Bird
</author>
<affiliation confidence="0.995992">
University of Michigan University of Melbourne and
abney@umich.edu University of Pennsylvania
</affiliation>
<email confidence="0.996591">
sbird@unimelb.edu.au
</email>
<sectionHeader confidence="0.993842" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999993473684211">
We present a grand challenge to build a
corpus that will include all of the world’s
languages, in a consistent structure that
permits large-scale cross-linguistic pro-
cessing, enabling the study of universal
linguistics. The focal data types, bilin-
gual texts and lexicons, relate each lan-
guage to one of a set of reference lan-
guages. We propose that the ability to train
systems to translate into and out of a given
language be the yardstick for determin-
ing when we have successfully captured a
language. We call on the computational
linguistics community to begin work on
this Universal Corpus, pursuing the many
strands of activity described here, as their
contribution to the global effort to docu-
ment the world’s linguistic heritage before
more languages fall silent.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987879661017">
The grand aim of linguistics is the construction of
a universal theory of human language. To a com-
putational linguist, it seems obvious that the first
step is to collect significant amounts of primary
data for a large variety of languages. Ideally, we
would like a complete digitization of every human
language: a Universal Corpus.
If we are ever to construct such a corpus, it must
be now. With the current rate of language loss, we
have only a small window of opportunity before
the data is gone forever. Linguistics may be unique
among the sciences in the crisis it faces. The next
generation will forgive us for the most egregious
shortcomings in theory construction and technol-
ogy development, but they will not forgive us if we
fail to preserve vanishing primary language data in
a form that enables future research.
The scope of the task is enormous. At present,
we have non-negligible quantities of machine-
readable data for only about 20–30 of the world’s
6,900 languages (Maxwell and Hughes, 2006).
Linguistics as a field is awake to the crisis. There
has been a tremendous upsurge of interest in doc-
umentary linguistics, the field concerned with the
the “creation, annotation, preservation, and dis-
semination of transparent records of a language”
(Woodbury, 2010). However, documentary lin-
guistics alone is not equal to the task. For example,
no million-word machine-readable corpus exists
for any endangered language, even though such a
quantity would be necessary for wide-ranging in-
vestigation of the language once no speakers are
available. The chances of constructing large-scale
resources will be greatly improved if computa-
tional linguists contribute their expertise.
This collaboration between linguists and com-
putational linguists will extend beyond the con-
struction of the Universal Corpus to its exploita-
tion for both theoretical and technological ends.
We envisage a new paradigm of universal linguis-
tics, in which grammars of individual languages
are built from the ground up, combining expert
manual effort with the power tools of probabilis-
tic language models and grammatical inference.
A universal grammar captures redundancies which
exist across languages, constituting a “universal
linguistic prior,” and enabling us to identify the
distinctive properties of specific languages and
families. The linguistic prior and regularities due
to common descent enable a new economy of scale
for technology development: cross-linguistic tri-
angulation can improve performance while reduc-
ing per-language data requirements.
Our aim in the present paper is to move beyond
generalities to a concrete plan of attack, and to
challenge the field to a communal effort to cre-
ate a Universal Corpus of the world’s languages,
in consistent machine-readable format, permitting
large-scale cross-linguistic processing.
</bodyText>
<page confidence="0.989011">
88
</page>
<note confidence="0.976195">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 88–97,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.568194" genericHeader="method">
2 Human Language Project
</sectionHeader>
<subsectionHeader confidence="0.952249">
2.1 Aims and scope
</subsectionHeader>
<bodyText confidence="0.999882659574468">
Although language endangerment provides ur-
gency, the corpus is not intended primarily as
a Noah’s Ark for languages. The aims go be-
yond the current crisis: we wish to support cross-
linguistic research and technology development at
the largest scale. There are existing collections
that contain multiple languages, but it is rare to
have consistent formats and annotation across lan-
guages, and few such datasets contain more than a
dozen or so languages.
If we think of a multi-lingual corpus as con-
sisting of an array of items, with columns repre-
senting languages and rows representing resource
types, the usual focus is on “vertical” processing.
Our particular concern, by contrast, is “horizontal”
processing that cuts indiscriminately across lan-
guages. Hence we require an unusual degree of
consistency across languages.
The kind of processing we wish to enable is
much like the large-scale systematic research that
motivated the Human Genome Project.
One of the greatest impacts of having
the sequence may well be in enabling
an entirely new approach to biological
research. In the past, researchers stud-
ied one or a few genes at a time. With
whole-genome sequences ... they can
approach questions systematically and
on a grand scale. They can study ...
how tens of thousands of genes and pro-
teins work together in interconnected
networks to orchestrate the chemistry of
life. (Human Genome Project, 2007)
We wish to make it possible to investigate human
language equally systematically and on an equally
grand scale: a Human Linguome Project, as it
were, though we have chosen the “Human Lan-
guage Project” as a more inviting title for the un-
dertaking. The product is a Universal Corpus,1 in
two senses of universal: in the sense of including
(ultimately) all the world’s languages, and in the
sense of enabling software and processing meth-
ods that are language-universal.
However, we do not aim for a collection that
is universal in the sense of encompassing all lan-
guage documentation efforts. Our goal is the con-
struction of a specific resource, albeit a very large
</bodyText>
<footnote confidence="0.94379">
1http://universalcorpus.org/
</footnote>
<bodyText confidence="0.999953470588236">
resource. We contrast the proposed effort with
general efforts to develop open resources, stan-
dards, and best practices. We do not aim to be all-
inclusive. The project does require large-scale col-
laboration, and a task definition that is simple and
compelling enough to achieve buy-in from a large
number of data providers. But we do not need and
do not attempt to create consensus across the en-
tire community. (Although one can hope that what
proves successful for a project of this scale will
provide a good foundation for future standards.)
Moreover, we do not aim to collect data
merely in the vague hope that it will prove use-
ful. Although we strive for maximum general-
ity, we also propose a specific driving “use case,”
namely, machine translation (MT), (Hutchins and
Somers, 1992; Koehn, 2010). The corpus pro-
vides a testing ground for the development of MT
system-construction methods that are dramatically
“leaner” in their resource requirements, and which
take advantage of cross-linguistic bootstrapping.
The large engineering question is how one can
turn the size of the task—constructing MT systems
for all the world’s languages simultaneously—to
one’s advantage, and thereby consume dramati-
cally less data per language.
The choice of MT as the use case is also driven
by scientific considerations. To explain, we re-
quire a bit of preamble.
We aim for a digitization of each human lan-
guage. What exactly does it mean to digitize an
entire language? It is natural to think in terms
of replicating the body of resources available for
well-documented languages, and the pre-eminent
resource for any language is a treebank. Producing
a treebank involves a staggering amount of man-
ual effort. It is also notoriously difficult to obtain
agreement about how parse trees should be defined
in one language, much less in many languages si-
multaneously. The idea of producing treebanks for
6,900 languages is quixotic, to put it mildly. But
is a treebank actually necessary?
Let us suppose that the purpose of a parse
tree is to mediate interpretation. A treebank, ar-
guably, represents a theoretical hypothesis about
how interpretations could be constructed; the pri-
mary data is actually the interpretations them-
selves. This suggests that we annotate sentences
with representations of meanings instead of syn-
tactic structures. Now that seems to take us out of
the frying pan into the fire. If obtaining consen-
</bodyText>
<page confidence="0.999587">
89
</page>
<bodyText confidence="0.999969719512195">
sus on parse trees is difficult, obtaining consensus
on meaning representations is impossible. How-
ever, if the language under consideration is any-
thing other than English, then a translation into
English (or some other reference language) is for
most purposes a perfectly adequate meaning rep-
resentation. That is, we view machine translation
as an approximation to language understanding.
Here is another way to put it. One measure of
adequacy of a language digitization is the abil-
ity of a human—already fluent in a reference
language—to acquire fluency in the digitized lan-
guage using only archived material. Now it would
be even better if we could use a language digiti-
zation to construct an artificial speaker of the lan-
guage. Importantly, we do not need to solve the AI
problem: the speaker need not decide what to say,
only how to translate from meanings to sentences
of the language, and from sentences back to mean-
ings. Taking sentences in a reference language as
the meaning representation, we arrive back at ma-
chine translation as the measure of success. In
short, we have successfully captured a language if
we can translate into and out of the language.
The key resource that should be built for each
language, then, is a collection of primary texts
with translations into a reference language. “Pri-
mary text” includes both written documents and
transcriptions of recordings. Large volumes of pri-
mary texts will be useful even without translation
for such tasks as language modeling and unsuper-
vised learning of morphology. Thus, we antici-
pate that the corpus will have the usual “pyrami-
dal” structure, starting from a base layer of unan-
notated text, some portion of which is translated
into a reference language at the document level to
make the next layer. Note that, for maximally au-
thentic primary texts, we assume the direction of
translation will normally be from primary text to
reference language, not the other way around.
Another layer of the corpus consists of sentence
and word alignments, required for training and
evaluating machine translation systems, and for
extracting bilingual lexicons. Curating such anno-
tations is a more specialized task than translation,
and so we expect it will only be done for a subset
of the translated texts.
In the last and smallest layer, morphology is an-
notated. This supports the development of mor-
phological analyzers, to preprocess primary texts
to identify morpheme boundaries and recognize
allomorphs, reducing the amount of data required
for training an MT system. This most-refined
target annotation corresponds to the interlinear
glossed texts that are the de facto standard of anno-
tation in the documentary linguistics community.
We postulate that interlinear glossed text is suf-
ficiently fine-grained to serve our purposes. It
invites efforts to enrich it by automatic means:
for example, there has been work on parsing the
English translations and using the word-by-word
glosses to transfer the parse tree to the object lan-
guage, effectively creating a treebank automati-
cally (Xia and Lewis, 2007). At the same time, we
believe that interlinear glossed text is sufficiently
simple and well-understood to allow rapid con-
struction of resources, and to make cross-linguistic
consistency a realistic goal.
Each of these layers—primary text, translations,
alignments, and morphological glosses—seems to
be an unavoidable piece of the overall solution.
The fact that these layers will exist in diminishing
quantity is also unavoidable. However, there is an
important consequence: the primary texts will be
permanently subject to new translation initiatives,
which themselves will be subject to new align-
ment and glossing initiatives, in which each step
is an instance of semisupervised learning (Abney,
2007). As time passes, our ability to enhance the
quantity and quality of the annotations will only
increase, thanks to effective combinations of auto-
matic, professional, and crowd-sourced effort.
</bodyText>
<subsectionHeader confidence="0.997179">
2.2 Principles
</subsectionHeader>
<bodyText confidence="0.999329647058824">
The basic principles upon which the envisioned
corpus is based are the following:
Universality. Covering as many languages as
possible is the first priority. Progress will be
gauged against concrete goals for numbers of lan-
guages, data per language, and coverage of lan-
guage families (Whalen and Simons, 2009).
Machine readability and consistency. “Cover-
ing” languages means enabling machine process-
ing seamlessly across languages. This will sup-
port new types of linguistic inquiry and the devel-
opment and testing of inference methods (for mor-
phology, parsers, machine translation) across large
numbers of typologically diverse languages.
Community effort. We cannot expect a single
organization to assemble a resource on this scale.
It will be necessary to get community buy-in, and
</bodyText>
<page confidence="0.988816">
90
</page>
<bodyText confidence="0.99994952631579">
many motivated volunteers. The repository will
not be the sole possession of any one institution.
Availability. The content of the corpus will be
available under one or more permissive licenses,
such as the Creative Commons Attribution Li-
cense (CC-BY), placing as few limits as possible
on community members’ ability to obtain and en-
hance the corpus, and redistribute derivative data.
Utility. The corpus aims to be maximally use-
ful, and minimally parochial. Annotation will be
as lightweight as possible; richer annotations will
will emerge bottom-up as they prove their utility
at the large scale.
Centrality of primary data. Primary texts and
recordings are paramount. Secondary resources
such as grammars and lexicons are important, but
no substitute for primary data. It is desirable that
secondary resources be integrated with—if not de-
rived from—primary data in the corpus.
</bodyText>
<subsectionHeader confidence="0.996407">
2.3 What to include
</subsectionHeader>
<bodyText confidence="0.99995375">
What should be included in the corpus? To some
extent, data collection will be opportunistic, but
it is appropriate to have a well-defined target in
mind. We consider the following essential.
Metadata. One means of resource identification
is to survey existing documentation for the lan-
guage, including bibliographic references and lo-
cations of web resources. Provenance and proper
citation of sources should be included for all data.
For written text. (1) Primary documents in
original printed form, e.g. scanned page images or
PDF. (2) Transcription. Not only optical charac-
ter recognition output, but also the output of tools
that extract text from PDF, will generally require
manual editing.
For spoken text. (1) Audio recordings. Both
elicited and spontaneous speech should be in-
cluded. It is highly desirous to have some con-
nected speech for every language. (2) Slow speech
“audio transcriptions.” Carefully respeaking a
spoken text can be much more efficient than writ-
ten transcription, and may one day yield to speech
recognition methods. (3) Written transcriptions.
We do not impose any requirements on the form
of transcription, though orthographic transcription
is generally much faster to produce than phonetic
transcription, and may even be more useful as
words are represented by normalized forms.
</bodyText>
<listItem confidence="0.789128">
For both written and spoken text. (1) Trans-
lations of primary documents into a refer-
ence language (possibly including commentary).
(2) Sentence-level segmentation and transla-
tion. (3) Word-level segmentation and glossing.
(4) Morpheme-level segmentation and glossing.
</listItem>
<bodyText confidence="0.999620090909091">
All documents will be included in primary
form, but the percentage of documents with man-
ual annotation, or manually corrected annotation,
decreases at increasingly fine-grained levels of an-
notation. Where manual fine-grained annotation is
unavailable, automatic methods for creating it (at a
lower quality) are desirable. Defining such meth-
ods for a large range of resource-poor languages is
an interesting computational challenge.
Secondary resources. Although it is possible to
base descriptive analyses exclusively on a text cor-
pus (Himmelmann, 2006, p. 22), the following
secondary resources should be secured if they are
available: (1) A lexicon with glosses in a reference
language. Ideally, everything should be attested in
the texts, but as a practical matter, there will be
words for which we have only a lexical entry and
no instances of use. (2) Paradigms and phonol-
ogy, for the construction of a morphological ana-
lyzer. Ideally, they should be inducible from the
texts, but published grammatical information may
go beyond what is attested in the text.
</bodyText>
<subsectionHeader confidence="0.997546">
2.4 Inadequacy of existing efforts
</subsectionHeader>
<bodyText confidence="0.999946857142857">
Our key desideratum is support for automatic pro-
cessing across a large range of languages. No data
collection effort currently exists or is proposed, to
our knowledge, that addresses this desideratum.
Traditional language archives such as the Audio
Archive of Linguistic Fieldwork (UC Berkeley),
Documentation of Endangered Languages (Max
Planck Institute, Nijmegen), the Endangered Lan-
guages Archive (SOAS, University of London),
and the Pacific And Regional Archive for Digi-
tal Sources in Endangered Cultures (Australia) of-
fer broad coverage of languages, but the majority
of their offerings are restricted in availability and
do not support machine processing. Conversely,
large-scale data collection efforts by the Linguis-
tic Data Consortium and the European Language
Resources Association cover less than one percent
of the world’s languages, with no evident plans for
major expansion of coverage. Other efforts con-
cern the definition and aggregation of language
resource metadata, including OLAC, IMDI, and
</bodyText>
<page confidence="0.994721">
91
</page>
<bodyText confidence="0.99773148">
CLARIN (Simons and Bird, 2003; Broeder and
Wittenburg, 2006; V´aradi et al., 2008), but this is
not the same as collecting and disseminating data.
Initiatives to develop standard formats for lin-
guistic annotations are orthogonal to our goals.
The success of the project will depend on con-
tributed data from many sources, in many differ-
ent formats. Converting all data formats to an
official standard, such as the RDF-based models
being developed by ISO Technical Committee 37
Sub-committee 4 Working Group 2, is simply im-
practical. These formats have onerous syntactic
and semantic requirements that demand substan-
tial further processing together with expert judg-
ment, and threaten to crush the large-scale collab-
orative data collection effort we envisage, before
it even gets off the ground. Instead, we opt for a
very lightweight format, sketched in the next sec-
tion, to minimize the effort of conversion and en-
able an immediate start. This does not limit the
options of community members who desire richer
formats, since they are free to invest the effort in
enriching the existing data. Such enrichment ef-
forts may gain broad support if they deliver a tan-
gible benefit for cross-language processing.
</bodyText>
<sectionHeader confidence="0.994771" genericHeader="method">
3 A Simple Storage Model
</sectionHeader>
<bodyText confidence="0.999929">
Here we sketch a simple approach to storage of
texts (including transcribed speech), bitexts, inter-
linear glossed text, and lexicons. We have been
deliberately schematic since the goal is just to give
grounds for confidence that there exists a general,
scalable solution.
For readability, our illustrations will include
space-separated sequences of tokens. However,
behind the scenes these could be represented as
a sequence of pairs of start and end offsets into a
primary text or speech signal, or as a sequence of
integers that reference an array of strings. Thus,
when we write (1a), bear in mind it may be imple-
mented as (1b) or (1c).
</bodyText>
<listItem confidence="0.812686666666667">
(1) a. This is a point of order.
b. (0,4), (5,7), (8,9), (10,15), (16,18), ...
c. 9347, 3053, 0038, 3342, 3468, .. .
</listItem>
<bodyText confidence="0.999674666666667">
In what follows, we focus on the minimal re-
quirements for storing and disseminating aligned
text, not the requirements for efficient in-memory
data structures. Moreover, we are agnostic about
whether the normalized, tokenized format is stored
entire or computed on demand.
We take an aligned text to be composed of a
series of aligned sentences, each consisting of a
small set of attributes and values, e.g.:
</bodyText>
<table confidence="0.988005">
ID: europarl/swedish/ep-00-01-17/18
LANGS: swd eng
SENT: det g¨aller en ordningsfr˚aga
TRANS: this is a point of order
ALIGN: 1-1 2-2 3-3 4-4 4-5 4-6
PROVENANCE: pharaoh-v1.2, ...
REV: 8947 2010-05-02 10:35:06 leobfld12
RIGHTS: Copyright (C) 2010 Uni...; CC-BY
</table>
<bodyText confidence="0.956133146341464">
The value of ID identifies the document and sen-
tence, and any collection to which the document
belongs. Individual components of the identi-
fier can be referenced or retrieved. The LANGS
attribute identifies the source and reference lan-
guage using ISO 639 codes.2 The SENT attribute
contains space-delimited tokens comprising a sen-
tence. Optional attributes TRANS and ALIGN
hold the translation and alignment, if these are
available; they are omitted in monolingual text.
A provenance attribute records any automatic or
manual processes which apply to the record, and
a revision attribute contains the version number,
timestamp, and username associated with the most
recent modification of the record, and a rights at-
tribute contains copyright and license information.
When morphological annotation is available, it
is represented by two additional attributes, LEX
and AFF. Here is a monolingual example:
ID: example/001
LANGS: eng
SENT: the dogs are barking
LEX: the dog be bark
AFF: - PL PL ING
Note that combining all attributes of these
two examples—that is, combining word-by-word
translation with morphological analysis—yields
interlinear glossed text.
A bilingual lexicon is an indispensable re-
source, whether provided as such, induced from
a collection of aligned text, or created by merg-
ing contributed and induced lexicons. A bilin-
gual lexicon can be viewed as an inventory of
cross-language correspondences between words
or groups of words. These correspondences are
just aligned text fragments, albeit much smaller
than a sentence. Thus, we take a bilingual lexicon
to be a kind of text in which each record contains
a single lexeme and its translation, represented us-
ing the LEX and TRANS attributes we have already
introduced, e.g.:
</bodyText>
<footnote confidence="0.93127">
2http://www.sil.org/iso639-3/
</footnote>
<page confidence="0.954316">
92
</page>
<table confidence="0.577524">
ID: swedishlex/v3.2/0419
LANGS: swd eng
LEX: ordningsfr˚aga
TRANS: point of order
</table>
<bodyText confidence="0.998289363636364">
In sum, the Universal Corpus is represented as
a massive store of records, each representing a
single sentence or lexical entry, using a limited
set of attributes. The store is indexed for effi-
cient access, and supports access to slices identi-
fied by language, content, provenance, rights, and
so forth. Many component collections would be
“unioned” into this single, large Corpus, with only
the record identifiers capturing the distinction be-
tween the various data sources.
Special cases of aligned text and wordlists,
spanning more than 1,000 languages, are Bible
translations and Swadesh wordlists (Resnik et al.,
1999; Swadesh, 1955). Here there are obvious
use-cases for accessing a particular verse or word
across all languages. However, it is not neces-
sary to model n-way language alignments. In-
stead, such sources are implicitly aligned by virtue
of their structure. Extracting all translations of
a verse, or all cognates of a Swadesh wordlist
item, is an index operation that returns monolin-
gual records, e.g.:
</bodyText>
<table confidence="0.708561333333333">
ID: swadesh/47 ID: swadesh/47
LANGS: fra LANGS: eng
LEX: chien LEX: dog
</table>
<sectionHeader confidence="0.973109" genericHeader="method">
4 Building the Corpus
</sectionHeader>
<bodyText confidence="0.9997978">
Data collection on this scale is a daunting
prospect, yet it is important to avoid the paraly-
sis of over-planning. We can start immediately by
leveraging existing infrastructure, and the volun-
tary effort of interested members of the language
resources community. One possibility is to found
a “Language Commons,” an open access reposi-
tory of language resources hosted in the Internet
Archive, with a lightweight method for commu-
nity members to contribute data sets.
A fully processed and indexed version of se-
lected data can be made accessible via a web ser-
vices interface to a major cloud storage facility,
such as Amazon Web Services. A common query
interface could be supported via APIs in multi-
ple NLP toolkits such as NLTK and GATE (Bird
et al., 2009; Cunningham et al., 2002), and also
in generic frameworks such as UIMA and SOAP,
leaving developers to work within their preferred
environment.
</bodyText>
<subsectionHeader confidence="0.987728">
4.1 Motivation for data providers
</subsectionHeader>
<bodyText confidence="0.999986925">
We hope that potential contributors of data will
be motivated to participate primarily by agree-
ment with the goals of the project. Even some-
one who has specialized in a particular language
or language family maintains an interest, we ex-
pect, in the universal question—the exploration of
Language writ large.
Data providers will find benefit in the availabil-
ity of volunteers for crowd-sourcing, and tools for
(semi-)automated quality control, refinement, and
presentation of data. For example, a data holder
should be able to contribute recordings and get
help in transcribing them, through a combination
of volunteer labor and automatic processing.
Documentary linguists and computational lin-
guists have much to gain from collaboration. In re-
turn for the data that documentary linguistics can
provide, computational linguistics has the poten-
tial to revolutionize the tools and practice of lan-
guage documentation.
We also seek collaboration with communities of
language speakers. The corpus provides an econ-
omy of scale for the development of literacy mate-
rials and tools for interactive language instruction,
in support of language preservation and revitaliza-
tion. For small languages, literacy in the mother
tongue is often defended on the grounds that it pro-
vides the best route to literacy in the national lan-
guage (Wagner, 1993, ch. 8). An essential ingredi-
ent of any local literacy program is to have a sub-
stantial quantity of available texts that represent
familiar topics including cultural heritage, folk-
lore, personal narratives, and current events. Tran-
sition to literacy in a language of wider commu-
nication is aided when transitional materials are
available (Waters, 1998, pp. 61ff). Mutual bene-
fits will also flow from the development of tools
for low-cost publication and broadcast in the lan-
guage, with copies of the published or broadcast
material licensed to and archived in the corpus.
</bodyText>
<subsectionHeader confidence="0.949248">
4.2 Roles
</subsectionHeader>
<bodyText confidence="0.999905625">
The enterprise requires collaboration of many in-
dividuals and groups, in a variety of roles.
Editors. A critical group are people with suffi-
cient engagement to serve as editors for particular
language families, who have access to data or are
able to negotiate redistribution rights, and oversee
the workflow of transcription, translation, and an-
notation.
</bodyText>
<page confidence="0.997461">
93
</page>
<bodyText confidence="0.999820272727273">
CL Research. All manual annotation steps need
to be automated. Each step presents a challeng-
ing semi-supervised learning and cross-linguistic
bootstrapping problem. In addition, the overall
measure of success—induction of machine trans-
lation systems from limited resources—pushes the
state of the art (Kumar et al., 2007). Numerous
other CL problems arise: active learning to im-
prove the quality of alignments and bilingual lex-
icons; automatic language identification for low-
density languages; and morphology learning.
Tool builders. We need tools for annotation, for-
mat conversion, spidering and language identifica-
tion, search, archiving, and presentation. Innova-
tive crowd-sourcing solutions are of particular in-
terest, e.g. web-based functionality for transcrib-
ing audio and video of oral literature, or setting up
a translation service based on aligned texts for a
low-density language, and collecting the improved
translations suggested by users.
Volunteer annotators. An important reason for
keeping the data model as lightweight as possible
is to enable contributions from volunteers with lit-
tle or no linguistic training. Two models are the
volunteers who scan documents and correct OCR
output in Project Gutenberg, or the undergraduate
volunteers who have constructed Greek and Latin
treebanks within Project Perseus (Crane, 2010).
Bilingual lexicons that have been extracted from
aligned text collections might be corrected using
crowd-sourcing, leading to improved translation
models and improved alignments. We also see the
Universal Corpus as an excellent opportunity for
undergraduates to participate in research, and for
native speakers to participate in the preservation of
their language.
Documentary linguists. The collection proto-
col known as Basic Oral Language Documentation
(BOLD) enables documentary linguists to collect
2–3 orders of magnitude more oral discourse than
before (Bird, 2010). Linguists can equip local
speakers to collect written texts, then to carefully
“respeak” and orally translate the texts into a refer-
ence language. With suitable tools, incorporating
active learning, local speakers could further curate
bilingual texts and lexicons. An early need is pi-
lot studies to determine costings for different cat-
egories of language.
Data agencies. The LDC and ELRA have a cen-
tral role to play, given their track record in obtain-
ing, curating, and publishing data with licenses
that facilitate language technology development.
We need to identify key resources where negoti-
ation with the original data provider, and where
payment of all preparation costs plus compensa-
tion for lost revenue, leads to new material for the
Corpus. This is a new publication model and a
new business model, but it can co-exist with the
existing models.
Language archives. Language archives have a
special role to play as holders of unique materi-
als. They could contribute existing data in its na-
tive format, for other participants to process. They
could give bilingual texts a distinct status within
their collections, to facilitate discovery.
Funding agencies. To be successful, the Human
Language Project would require substantial funds,
possibly drawing on a constellation of public and
private agencies in many countries. However, in
the spirit of starting small, and starting now, agen-
cies could require that sponsored projects which
collect texts and build lexicons contribute them to
the Language Commons. After all, the most effec-
tive time to do translation, alignment, and lexicon
work is often at the point when primary data is
first collected, and this extra work promises direct
benefits to the individual project.
</bodyText>
<subsectionHeader confidence="0.999292">
4.3 Early tasks
</subsectionHeader>
<bodyText confidence="0.999815842105263">
Seed corpus. The central challenge, we believe,
is getting critical mass. Data attracts data, and if
one can establish a sufficient seed, the effort will
snowball. We can make some concrete proposals
as to how to collect a seed. Language resources
on the web are one source—the Cr´ubad´an project
has identified resources for 400 languages, for ex-
ample (Scannell, 2008); the New Testament of the
Bible exists in about 1200 languages and contains
of the order of 100k words. We hope that exist-
ing efforts that are already well-disposed toward
electronic distribution will participate. We partic-
ularly mention the Language and Culture Archive
of the Summer Institute of Linguistics, and the
Rosetta Project. The latter is already distributed
through the Internet Archive and contains material
for 2500 languages.
Resource discovery. Existing language re-
sources need to be documented, a large un-
</bodyText>
<page confidence="0.998527">
94
</page>
<bodyText confidence="0.999955270833334">
dertaking that depends on widely distributed
knowledge. Existing published corpora from the
LDC, ELRA and dozens of other sources—a total
of 85,000 items—are already documented in the
combined catalog of the Open Language Archives
Community,3 so there is no need to recreate this
information. Other resources can be logged by
community members using a public access wiki,
with a metadata template to ensure key fields are
elicited such as resource owner, license, ISO 639
language code(s), and data type. This information
can itself be curated and stored in the form of an
OLAC archive, to permit search over the union of
the existing and newly documented items. Work
along these lines has already been initiated by
LDC and ELRA (Cieri et al., 2010).
Resource classification. Editors with knowl-
edge of particular language families will catego-
rize documented resources relative to the needs of
the project, using controlled vocabularies. This
involves examining a resource, determining the
granularity and provenance of the segmentation
and alignment, checking its ISO 639 classifi-
cations, assigning it to a logarithmic size cate-
gory, documenting its format and layout, collect-
ing sample files, and assigning a priority score.
Acquisition. Where necessary, permission will
be sought to lodge the resource in the repository.
Funding may be required to buy the rights to the
resource from its owner, as compensation for lost
revenue from future data sales. Funding may be
required to translate the source into a reference
language. The repository’s ingestion process is
followed, and the resource metadata is updated.
Text collection. Languages for which the avail-
able resources are inadequate are identified, and
the needs are prioritized, based on linguistic and
geographical diversity. Sponsorship is sought
for collecting bilingual texts in high priority lan-
guages. Workflows are developed for languages
based on a variety of factors, such as availability
of educated people with native-level proficiency
in their mother tongue and good knowledge of
a reference language, internet access in the lan-
guage area, availability of expatriate speakers in a
first-world context, and so forth. A classification
scheme is required to help predict which work-
flows will be most successful in a given situation.
</bodyText>
<footnote confidence="0.803226">
3http://www.language-archives.org/
</footnote>
<bodyText confidence="0.9998601875">
Audio protocol. The challenge posed by lan-
guages with no written literature should not be
underestimated. A promising collection method
is Basic Oral Language Documentation, which
calls for inexpensive voice recorders and net-
books, project-specific software for transcription
and sentence-aligned translation, network band-
width for upload to the repository, and suitable
training and support throughout the process.
Corpus readers. Software developers will in-
spect the file formats and identify high priority for-
mats based on information about resource priori-
ties and sizes. They will code a corpus reader, an
open source reference implementation for convert-
ing between corpus formats and the storage model
presented in section 3.
</bodyText>
<subsectionHeader confidence="0.970248">
4.4 Further challenges
</subsectionHeader>
<bodyText confidence="0.9999851875">
There are many additional difficulties that could
be listed, though we expect they can be addressed
over time, once a sufficient seed corpus is estab-
lished. Two particular issues deserve further com-
ment, however.
Licenses. Intellectual property issues surround-
ing linguistic corpora present a complex and
evolving landscape (DiPersio, 2010). For users, it
would be ideal for all materials to be available un-
der a single license that permits derivative works,
commercial use, and redistribution, such as the
Creative Commons Attribution License (CC-BY).
There would be no confusion about permissible
uses of subsets and aggregates of the collected cor-
pora, and it would be easy to view the Universal
Corpus as a single corpus. But to attract as many
data contributors as possible, we cannot make such
a license a condition of contribution.
Instead, we propose to distinguish between:
(1) a digital Archive of contributed corpora that
are stored in their original format and made avail-
able under a range of licenses, offering preserva-
tion and dissemination services to the language
resources community at large (i.e. the Language
Commons); and (2) the Universal Corpus, which
is embodied as programmatic access to an evolv-
ing subset of materials from the archive under
one of a small set of permissive licenses, licenses
whose unions and intersections are understood
(e.g. CC-BY and its non-commercial counterpart
CC-BY-NC). Apart from being a useful service in
its own right, the Archive would provide a staging
</bodyText>
<page confidence="0.996131">
95
</page>
<bodyText confidence="0.999983948717949">
ground for the Universal Corpus. Archived cor-
pora having restrictive licenses could be evaluated
for their potential as contributions to the Corpus,
making it possible to prioritize the work of nego-
tiating more liberal licenses.
There are reasons to distinguish Archive and
Corpus even beyond the license issues. The Cor-
pus, but not the Archive, is limited to the formats
that support automatic cross-linguistic processing.
Conversely, since the primary interface to the Cor-
pus is programmatic, it may include materials that
are hosted in many different archives; it only needs
to know how to access and deliver them to the user.
Incidentally, we consider it an implementation is-
sue whether the Corpus is provided as a web ser-
vice, a download service with user-side software,
user-side software with data delivered on physical
media, or a cloud application with user programs
executed server-side.
Expenses of conversion and editing. We do not
trivialize the work involved in converting docu-
ments to the formats of section 3, and in manu-
ally correcting the results of noisy automatic pro-
cesses such as optical character recognition. In-
deed, the amount of work involved is one moti-
vation for the lengths to which we have gone to
keep the data format simple. For example, we have
deliberately avoided specifying any particular to-
kenization scheme. Variation will arise as a con-
sequence, but we believe that it will be no worse
than the variability in input that current machine
translation training methods routinely deal with,
and will not greatly injure the utility of the Corpus.
The utter simplicity of the formats also widens the
pool of potential volunteers for doing the manual
work that is required. By avoiding linguistically
delicate annotation, we can take advantage of mo-
tivated but untrained volunteers such as students
and members of speaker communities.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999984188679245">
Nearly twenty years ago, the linguistics commu-
nity received a wake-up call, when Hale et al.
(1992) predicted that 90% of the world’s linguis-
tic diversity would be lost or moribund by the year
2100, and warned that linguistics might “go down
in history as the only science that presided oblivi-
ously over the disappearance of 90 per cent of the
very field to which it is dedicated.” Today, lan-
guage documentation is a high priority in main-
stream linguistics. However, the field of computa-
tional linguistics is yet to participate substantially.
The first half century of research in compu-
tational linguistics—from circa 1960 up to the
present—has touched on less than 1% of the
world’s languages. For a field which is justly
proud of its empirical methods, it is time to apply
those methods to the remaining 99% of languages.
We will never have the luxury of richly annotated
data for these languages, so we are forced to ask
ourselves: can we do more with less?
We believe the answer is “yes,” and so we chal-
lenge the computational linguistics community to
adopt a scalable computational approach to the
problem. We need leaner methods for building
machine translation systems; new algorithms for
cross-linguistic bootstrapping via multiple paths;
more effective techniques for leveraging human
effort in labeling data; scalable ways to get bilin-
gual text for unwritten languages; and large scale
social engineering to make it all happen quickly.
To believe we can build this Universal Corpus is
certainly audacious, but not to even try is arguably
irresponsible. The initial step parallels earlier ef-
forts to create large machine-readable text collec-
tions which began in the 1960s and reverberated
through each subsequent decade. Collecting bilin-
gual texts is an orthodox activity, and many alter-
native conceptions of a Human Language Project
would likely include this as an early task.
The undertaking ranks with the largest data-
collection efforts in science today. It is not achiev-
able without considerable computational sophis-
tication and the full engagement of the field of
computational linguistics. Yet we require no fun-
damentally new technologies. We can build on
our strengths in corpus-based methods, linguis-
tic models, human- and machine-supplied annota-
tions, and learning algorithms. By rising to this,
the greatest language challenge of our time, we
enable multi-lingual technology development at a
new scale, and simultaneously lay the foundations
for a new science of empirical universal linguis-
tics.
</bodyText>
<sectionHeader confidence="0.998843" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999944166666667">
We are grateful to Ed Bice, Doug Oard, Gary
Simons, participants of the Language Commons
working group meeting in Boston, students in
the “Digitizing Languages” seminar (University of
Michigan), and anonymous reviewers, for feed-
back on an earlier version of this paper.
</bodyText>
<page confidence="0.987255">
96
</page>
<sectionHeader confidence="0.995888" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999839972222222">
Steven Abney. 2007. Semisupervised Learning for
Computational Linguistics. Chapman &amp; Hall/CRC.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly Media. http://nltk.org/book.
Steven Bird. 2010. A scalable method for preserving
oral literature from small languages. In Proceedings
of the 12th International Conference on Asia-Pacific
Digital Libraries, pages 5–14.
Daan Broeder and Peter Wittenburg. 2006. The IMDI
metadata framework, its current application and fu-
ture direction. International Journal of Metadata,
Semantics and Ontologies, 1:119–132.
Christopher Cieri, Khalid Choukri, Nicoletta Calzo-
lari, D. Terence Langendoen, Johannes Leveling,
Martha Palmer, Nancy Ide, and James Pustejovsky.
2010. A road map for interoperable language re-
source metadata. In Proceedings of the 7th Interna-
tional Conference on Language Resources and Eval-
uation (LREC).
Gregory R. Crane. 2010. Perseus Digital Library:
Research in 2008/09. http://www.perseus.
tufts.edu/hopper/research/current.
Accessed Feb. 2010.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE: an
architecture for development of robust HLT appli-
cations. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics,
pages 168–175. Association for Computational
Linguistics.
Denise DiPersio. 2010. Implications of a permis-
sions culture on the development and distribution
of language resources. In FLaReNet Forum 2010.
Fostering Language Resources Network. http:
//www.flarenet.eu/.
Hale, M. Krauss, L. Watahomigie, A. Yamamoto, and
C. Craig. 1992. Endangered languages. Language,
68(1):1–42.
Nikolaus P. Himmelmann. 2006. Language documen-
tation: What is it and what is it good for? In
Jost Gippert, Nikolaus Himmelmann, and Ulrike
Mosel, editors, Essentials of Language Documenta-
tion, pages 1–30. Mouton de Gruyter.
Human Genome Project. 2007. The science
behind the Human Genome Project. http:
//www.ornl.gov/sci/techresources/
Human_Genome/project/info.shtml.
Accessed Dec. 2007.
W. John Hutchins and Harold L. Somers. 1992. An In-
troduction to Machine Translation. Academic Press.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Shankar Kumar, Franz J. Och, and Wolfgang
Macherey. 2007. Improving word alignment with
bridge languages. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 42–50,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
Mike Maxwell and Baden Hughes. 2006. Frontiers
in linguistic annotation for lower-density languages.
In Proceedings of the Workshop on Frontiers in Lin-
guistically Annotated Corpora 2006, pages 29–37,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Philip Resnik, Mari Broman Olsen, and Mona Diab.
1999. The Bible as a parallel corpus: Annotating
the ‘book of 2000 tongues’. Computers and the Hu-
manities, 33:129–153.
Kevin Scannell. 2008. The Cr´ubad´an Project: Corpus
building for under-resourced languages. In Cahiers
du Cental 5: Proceedings of the 3rd Web as Corpus
Workshop.
Gary Simons and Steven Bird. 2003. The Open Lan-
guage Archives Community: An infrastructure for
distributed archiving of language resources. Liter-
ary and Linguistic Computing, 18:117–128.
Morris Swadesh. 1955. Towards greater accuracy
in lexicostatistic dating. International Journal of
American Linguistics, 21:121–137.
Tam´as V´aradi, Steven Krauwer, Peter Wittenburg,
Martin Wynne, and Kimmo Koskenniemi. 2008.
CLARIN: common language resources and technol-
ogy infrastructure. In Proceedings of the Sixth Inter-
national Language Resources and Evaluation Con-
ference. European Language Resources Association.
Daniel A. Wagner. 1993. Literacy, Culture, and Devel-
opment: Becoming Literate in Morocco. Cambridge
University Press.
Glenys Waters. 1998. Local Literacies: Theory and
Practice. Summer Institute of Linguistics, Dallas.
Douglas H. Whalen and Gary Simons. 2009. En-
dangered language families. In Proceedings of the
1st International Conference on Language Docu-
mentation and Conservation. University of Hawaii.
http://hdl.handle.net/10125/5017.
Anthony C. Woodbury. 2010. Language documenta-
tion. In Peter K. Austin and Julia Sallabank, edi-
tors, The Cambridge Handbook of Endangered Lan-
guages. Cambridge University Press.
Fei Xia and William D. Lewis. 2007. Multilingual
structural projection across interlinearized text. In
Proceedings of the Meeting of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL). Association for Computational
Linguistics.
</reference>
<page confidence="0.999691">
97
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.631004">
<title confidence="0.9988075">The Human Language Project: Building a Universal Corpus of the World’s Languages</title>
<author confidence="0.999947">Steven Abney Steven Bird</author>
<affiliation confidence="0.816274">University of Michigan University of Melbourne and of Pennsylvania</affiliation>
<email confidence="0.994342">sbird@unimelb.edu.au</email>
<abstract confidence="0.99930795">We present a grand challenge to build a corpus that will include all of the world’s languages, in a consistent structure that permits large-scale cross-linguistic processing, enabling the study of universal linguistics. The focal data types, bilingual texts and lexicons, relate each language to one of a set of reference languages. We propose that the ability to train systems to translate into and out of a given language be the yardstick for determining when we have successfully captured a language. We call on the computational linguistics community to begin work on this Universal Corpus, pursuing the many strands of activity described here, as their contribution to the global effort to document the world’s linguistic heritage before more languages fall silent.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Semisupervised Learning for Computational Linguistics.</title>
<date>2007</date>
<publisher>Chapman &amp; Hall/CRC.</publisher>
<contexts>
<context position="12358" citStr="Abney, 2007" startWordPosition="1961" endWordPosition="1962">understood to allow rapid construction of resources, and to make cross-linguistic consistency a realistic goal. Each of these layers—primary text, translations, alignments, and morphological glosses—seems to be an unavoidable piece of the overall solution. The fact that these layers will exist in diminishing quantity is also unavoidable. However, there is an important consequence: the primary texts will be permanently subject to new translation initiatives, which themselves will be subject to new alignment and glossing initiatives, in which each step is an instance of semisupervised learning (Abney, 2007). As time passes, our ability to enhance the quantity and quality of the annotations will only increase, thanks to effective combinations of automatic, professional, and crowd-sourced effort. 2.2 Principles The basic principles upon which the envisioned corpus is based are the following: Universality. Covering as many languages as possible is the first priority. Progress will be gauged against concrete goals for numbers of languages, data per language, and coverage of language families (Whalen and Simons, 2009). Machine readability and consistency. “Covering” languages means enabling machine p</context>
</contexts>
<marker>Abney, 2007</marker>
<rawString>Steven Abney. 2007. Semisupervised Learning for Computational Linguistics. Chapman &amp; Hall/CRC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media. http://nltk.org/book.</booktitle>
<contexts>
<context position="24335" citStr="Bird et al., 2009" startWordPosition="3830" endWordPosition="3833">y by leveraging existing infrastructure, and the voluntary effort of interested members of the language resources community. One possibility is to found a “Language Commons,” an open access repository of language resources hosted in the Internet Archive, with a lightweight method for community members to contribute data sets. A fully processed and indexed version of selected data can be made accessible via a web services interface to a major cloud storage facility, such as Amazon Web Services. A common query interface could be supported via APIs in multiple NLP toolkits such as NLTK and GATE (Bird et al., 2009; Cunningham et al., 2002), and also in generic frameworks such as UIMA and SOAP, leaving developers to work within their preferred environment. 4.1 Motivation for data providers We hope that potential contributors of data will be motivated to participate primarily by agreement with the goals of the project. Even someone who has specialized in a particular language or language family maintains an interest, we expect, in the universal question—the exploration of Language writ large. Data providers will find benefit in the availability of volunteers for crowd-sourcing, and tools for (semi-)autom</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O’Reilly Media. http://nltk.org/book.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
</authors>
<title>A scalable method for preserving oral literature from small languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the 12th International Conference on Asia-Pacific Digital Libraries,</booktitle>
<pages>5--14</pages>
<contexts>
<context position="28700" citStr="Bird, 2010" startWordPosition="4490" endWordPosition="4491">s within Project Perseus (Crane, 2010). Bilingual lexicons that have been extracted from aligned text collections might be corrected using crowd-sourcing, leading to improved translation models and improved alignments. We also see the Universal Corpus as an excellent opportunity for undergraduates to participate in research, and for native speakers to participate in the preservation of their language. Documentary linguists. The collection protocol known as Basic Oral Language Documentation (BOLD) enables documentary linguists to collect 2–3 orders of magnitude more oral discourse than before (Bird, 2010). Linguists can equip local speakers to collect written texts, then to carefully “respeak” and orally translate the texts into a reference language. With suitable tools, incorporating active learning, local speakers could further curate bilingual texts and lexicons. An early need is pilot studies to determine costings for different categories of language. Data agencies. The LDC and ELRA have a central role to play, given their track record in obtaining, curating, and publishing data with licenses that facilitate language technology development. We need to identify key resources where negotiati</context>
</contexts>
<marker>Bird, 2010</marker>
<rawString>Steven Bird. 2010. A scalable method for preserving oral literature from small languages. In Proceedings of the 12th International Conference on Asia-Pacific Digital Libraries, pages 5–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daan Broeder</author>
<author>Peter Wittenburg</author>
</authors>
<title>The IMDI metadata framework, its current application and future direction.</title>
<date>2006</date>
<journal>International Journal of Metadata, Semantics and Ontologies,</journal>
<pages>1--119</pages>
<contexts>
<context position="18007" citStr="Broeder and Wittenburg, 2006" startWordPosition="2817" endWordPosition="2820">And Regional Archive for Digital Sources in Endangered Cultures (Australia) offer broad coverage of languages, but the majority of their offerings are restricted in availability and do not support machine processing. Conversely, large-scale data collection efforts by the Linguistic Data Consortium and the European Language Resources Association cover less than one percent of the world’s languages, with no evident plans for major expansion of coverage. Other efforts concern the definition and aggregation of language resource metadata, including OLAC, IMDI, and 91 CLARIN (Simons and Bird, 2003; Broeder and Wittenburg, 2006; V´aradi et al., 2008), but this is not the same as collecting and disseminating data. Initiatives to develop standard formats for linguistic annotations are orthogonal to our goals. The success of the project will depend on contributed data from many sources, in many different formats. Converting all data formats to an official standard, such as the RDF-based models being developed by ISO Technical Committee 37 Sub-committee 4 Working Group 2, is simply impractical. These formats have onerous syntactic and semantic requirements that demand substantial further processing together with expert </context>
</contexts>
<marker>Broeder, Wittenburg, 2006</marker>
<rawString>Daan Broeder and Peter Wittenburg. 2006. The IMDI metadata framework, its current application and future direction. International Journal of Metadata, Semantics and Ontologies, 1:119–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Cieri</author>
<author>Khalid Choukri</author>
<author>Nicoletta Calzolari</author>
<author>D Terence Langendoen</author>
<author>Johannes Leveling</author>
<author>Martha Palmer</author>
<author>Nancy Ide</author>
<author>James Pustejovsky</author>
</authors>
<title>A road map for interoperable language resource metadata.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="32091" citStr="Cieri et al., 2010" startWordPosition="5030" endWordPosition="5033">a total of 85,000 items—are already documented in the combined catalog of the Open Language Archives Community,3 so there is no need to recreate this information. Other resources can be logged by community members using a public access wiki, with a metadata template to ensure key fields are elicited such as resource owner, license, ISO 639 language code(s), and data type. This information can itself be curated and stored in the form of an OLAC archive, to permit search over the union of the existing and newly documented items. Work along these lines has already been initiated by LDC and ELRA (Cieri et al., 2010). Resource classification. Editors with knowledge of particular language families will categorize documented resources relative to the needs of the project, using controlled vocabularies. This involves examining a resource, determining the granularity and provenance of the segmentation and alignment, checking its ISO 639 classifications, assigning it to a logarithmic size category, documenting its format and layout, collecting sample files, and assigning a priority score. Acquisition. Where necessary, permission will be sought to lodge the resource in the repository. Funding may be required to</context>
</contexts>
<marker>Cieri, Choukri, Calzolari, Langendoen, Leveling, Palmer, Ide, Pustejovsky, 2010</marker>
<rawString>Christopher Cieri, Khalid Choukri, Nicoletta Calzolari, D. Terence Langendoen, Johannes Leveling, Martha Palmer, Nancy Ide, and James Pustejovsky. 2010. A road map for interoperable language resource metadata. In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory R Crane</author>
</authors>
<date>2010</date>
<booktitle>Perseus Digital Library: Research in 2008/09. http://www.perseus. tufts.edu/hopper/research/current. Accessed</booktitle>
<contexts>
<context position="28127" citStr="Crane, 2010" startWordPosition="4410" endWordPosition="4411">web-based functionality for transcribing audio and video of oral literature, or setting up a translation service based on aligned texts for a low-density language, and collecting the improved translations suggested by users. Volunteer annotators. An important reason for keeping the data model as lightweight as possible is to enable contributions from volunteers with little or no linguistic training. Two models are the volunteers who scan documents and correct OCR output in Project Gutenberg, or the undergraduate volunteers who have constructed Greek and Latin treebanks within Project Perseus (Crane, 2010). Bilingual lexicons that have been extracted from aligned text collections might be corrected using crowd-sourcing, leading to improved translation models and improved alignments. We also see the Universal Corpus as an excellent opportunity for undergraduates to participate in research, and for native speakers to participate in the preservation of their language. Documentary linguists. The collection protocol known as Basic Oral Language Documentation (BOLD) enables documentary linguists to collect 2–3 orders of magnitude more oral discourse than before (Bird, 2010). Linguists can equip local</context>
</contexts>
<marker>Crane, 2010</marker>
<rawString>Gregory R. Crane. 2010. Perseus Digital Library: Research in 2008/09. http://www.perseus. tufts.edu/hopper/research/current. Accessed Feb. 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamish Cunningham</author>
<author>Diana Maynard</author>
<author>Kalina Bontcheva</author>
<author>Valentin Tablan</author>
</authors>
<title>GATE: an architecture for development of robust HLT applications.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>168--175</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24361" citStr="Cunningham et al., 2002" startWordPosition="3834" endWordPosition="3837">sting infrastructure, and the voluntary effort of interested members of the language resources community. One possibility is to found a “Language Commons,” an open access repository of language resources hosted in the Internet Archive, with a lightweight method for community members to contribute data sets. A fully processed and indexed version of selected data can be made accessible via a web services interface to a major cloud storage facility, such as Amazon Web Services. A common query interface could be supported via APIs in multiple NLP toolkits such as NLTK and GATE (Bird et al., 2009; Cunningham et al., 2002), and also in generic frameworks such as UIMA and SOAP, leaving developers to work within their preferred environment. 4.1 Motivation for data providers We hope that potential contributors of data will be motivated to participate primarily by agreement with the goals of the project. Even someone who has specialized in a particular language or language family maintains an interest, we expect, in the universal question—the exploration of Language writ large. Data providers will find benefit in the availability of volunteers for crowd-sourcing, and tools for (semi-)automated quality control, refi</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. 2002. GATE: an architecture for development of robust HLT applications. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 168–175. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denise DiPersio</author>
</authors>
<title>Implications of a permissions culture on the development and distribution of language resources.</title>
<date>2010</date>
<booktitle>In FLaReNet Forum</booktitle>
<note>http: //www.flarenet.eu/.</note>
<contexts>
<context position="34772" citStr="DiPersio, 2010" startWordPosition="5426" endWordPosition="5427">ts and identify high priority formats based on information about resource priorities and sizes. They will code a corpus reader, an open source reference implementation for converting between corpus formats and the storage model presented in section 3. 4.4 Further challenges There are many additional difficulties that could be listed, though we expect they can be addressed over time, once a sufficient seed corpus is established. Two particular issues deserve further comment, however. Licenses. Intellectual property issues surrounding linguistic corpora present a complex and evolving landscape (DiPersio, 2010). For users, it would be ideal for all materials to be available under a single license that permits derivative works, commercial use, and redistribution, such as the Creative Commons Attribution License (CC-BY). There would be no confusion about permissible uses of subsets and aggregates of the collected corpora, and it would be easy to view the Universal Corpus as a single corpus. But to attract as many data contributors as possible, we cannot make such a license a condition of contribution. Instead, we propose to distinguish between: (1) a digital Archive of contributed corpora that are sto</context>
</contexts>
<marker>DiPersio, 2010</marker>
<rawString>Denise DiPersio. 2010. Implications of a permissions culture on the development and distribution of language resources. In FLaReNet Forum 2010. Fostering Language Resources Network. http: //www.flarenet.eu/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Krauss Hale</author>
<author>L Watahomigie</author>
<author>A Yamamoto</author>
<author>C Craig</author>
</authors>
<title>Endangered languages.</title>
<date>1992</date>
<journal>Language,</journal>
<volume>68</volume>
<issue>1</issue>
<contexts>
<context position="37925" citStr="Hale et al. (1992)" startWordPosition="5935" endWordPosition="5938">arise as a consequence, but we believe that it will be no worse than the variability in input that current machine translation training methods routinely deal with, and will not greatly injure the utility of the Corpus. The utter simplicity of the formats also widens the pool of potential volunteers for doing the manual work that is required. By avoiding linguistically delicate annotation, we can take advantage of motivated but untrained volunteers such as students and members of speaker communities. 5 Conclusion Nearly twenty years ago, the linguistics community received a wake-up call, when Hale et al. (1992) predicted that 90% of the world’s linguistic diversity would be lost or moribund by the year 2100, and warned that linguistics might “go down in history as the only science that presided obliviously over the disappearance of 90 per cent of the very field to which it is dedicated.” Today, language documentation is a high priority in mainstream linguistics. However, the field of computational linguistics is yet to participate substantially. The first half century of research in computational linguistics—from circa 1960 up to the present—has touched on less than 1% of the world’s languages. For </context>
</contexts>
<marker>Hale, Watahomigie, Yamamoto, Craig, 1992</marker>
<rawString>Hale, M. Krauss, L. Watahomigie, A. Yamamoto, and C. Craig. 1992. Endangered languages. Language, 68(1):1–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikolaus P Himmelmann</author>
</authors>
<title>Language documentation: What is it and what is it good for? In</title>
<date>2006</date>
<booktitle>Essentials of Language Documentation,</booktitle>
<pages>1--30</pages>
<editor>Jost Gippert, Nikolaus Himmelmann, and Ulrike Mosel, editors,</editor>
<note>Mouton de Gruyter.</note>
<contexts>
<context position="16385" citStr="Himmelmann, 2006" startWordPosition="2570" endWordPosition="2571">ion and glossing. (4) Morpheme-level segmentation and glossing. All documents will be included in primary form, but the percentage of documents with manual annotation, or manually corrected annotation, decreases at increasingly fine-grained levels of annotation. Where manual fine-grained annotation is unavailable, automatic methods for creating it (at a lower quality) are desirable. Defining such methods for a large range of resource-poor languages is an interesting computational challenge. Secondary resources. Although it is possible to base descriptive analyses exclusively on a text corpus (Himmelmann, 2006, p. 22), the following secondary resources should be secured if they are available: (1) A lexicon with glosses in a reference language. Ideally, everything should be attested in the texts, but as a practical matter, there will be words for which we have only a lexical entry and no instances of use. (2) Paradigms and phonology, for the construction of a morphological analyzer. Ideally, they should be inducible from the texts, but published grammatical information may go beyond what is attested in the text. 2.4 Inadequacy of existing efforts Our key desideratum is support for automatic processi</context>
</contexts>
<marker>Himmelmann, 2006</marker>
<rawString>Nikolaus P. Himmelmann. 2006. Language documentation: What is it and what is it good for? In Jost Gippert, Nikolaus Himmelmann, and Ulrike Mosel, editors, Essentials of Language Documentation, pages 1–30. Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Human Genome Project</author>
</authors>
<title>The science behind the Human Genome Project. http: //www.ornl.gov/sci/techresources/ Human_Genome/project/info.shtml. Accessed</title>
<date>2007</date>
<contexts>
<context position="5480" citStr="Project, 2007" startWordPosition="852" endWordPosition="853">onsistency across languages. The kind of processing we wish to enable is much like the large-scale systematic research that motivated the Human Genome Project. One of the greatest impacts of having the sequence may well be in enabling an entirely new approach to biological research. In the past, researchers studied one or a few genes at a time. With whole-genome sequences ... they can approach questions systematically and on a grand scale. They can study ... how tens of thousands of genes and proteins work together in interconnected networks to orchestrate the chemistry of life. (Human Genome Project, 2007) We wish to make it possible to investigate human language equally systematically and on an equally grand scale: a Human Linguome Project, as it were, though we have chosen the “Human Language Project” as a more inviting title for the undertaking. The product is a Universal Corpus,1 in two senses of universal: in the sense of including (ultimately) all the world’s languages, and in the sense of enabling software and processing methods that are language-universal. However, we do not aim for a collection that is universal in the sense of encompassing all language documentation efforts. Our goal </context>
</contexts>
<marker>Project, 2007</marker>
<rawString>Human Genome Project. 2007. The science behind the Human Genome Project. http: //www.ornl.gov/sci/techresources/ Human_Genome/project/info.shtml. Accessed Dec. 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W John Hutchins</author>
<author>Harold L Somers</author>
</authors>
<title>An Introduction to Machine Translation.</title>
<date>1992</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="6958" citStr="Hutchins and Somers, 1992" startWordPosition="1098" endWordPosition="1101">roject does require large-scale collaboration, and a task definition that is simple and compelling enough to achieve buy-in from a large number of data providers. But we do not need and do not attempt to create consensus across the entire community. (Although one can hope that what proves successful for a project of this scale will provide a good foundation for future standards.) Moreover, we do not aim to collect data merely in the vague hope that it will prove useful. Although we strive for maximum generality, we also propose a specific driving “use case,” namely, machine translation (MT), (Hutchins and Somers, 1992; Koehn, 2010). The corpus provides a testing ground for the development of MT system-construction methods that are dramatically “leaner” in their resource requirements, and which take advantage of cross-linguistic bootstrapping. The large engineering question is how one can turn the size of the task—constructing MT systems for all the world’s languages simultaneously—to one’s advantage, and thereby consume dramatically less data per language. The choice of MT as the use case is also driven by scientific considerations. To explain, we require a bit of preamble. We aim for a digitization of eac</context>
</contexts>
<marker>Hutchins, Somers, 1992</marker>
<rawString>W. John Hutchins and Harold L. Somers. 1992. An Introduction to Machine Translation. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6972" citStr="Koehn, 2010" startWordPosition="1102" endWordPosition="1103">cale collaboration, and a task definition that is simple and compelling enough to achieve buy-in from a large number of data providers. But we do not need and do not attempt to create consensus across the entire community. (Although one can hope that what proves successful for a project of this scale will provide a good foundation for future standards.) Moreover, we do not aim to collect data merely in the vague hope that it will prove useful. Although we strive for maximum generality, we also propose a specific driving “use case,” namely, machine translation (MT), (Hutchins and Somers, 1992; Koehn, 2010). The corpus provides a testing ground for the development of MT system-construction methods that are dramatically “leaner” in their resource requirements, and which take advantage of cross-linguistic bootstrapping. The large engineering question is how one can turn the size of the task—constructing MT systems for all the world’s languages simultaneously—to one’s advantage, and thereby consume dramatically less data per language. The choice of MT as the use case is also driven by scientific considerations. To explain, we require a bit of preamble. We aim for a digitization of each human langua</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. Statistical Machine Translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Franz J Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Improving word alignment with bridge languages.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>42--50</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="27108" citStr="Kumar et al., 2007" startWordPosition="4260" endWordPosition="4263">d groups, in a variety of roles. Editors. A critical group are people with sufficient engagement to serve as editors for particular language families, who have access to data or are able to negotiate redistribution rights, and oversee the workflow of transcription, translation, and annotation. 93 CL Research. All manual annotation steps need to be automated. Each step presents a challenging semi-supervised learning and cross-linguistic bootstrapping problem. In addition, the overall measure of success—induction of machine translation systems from limited resources—pushes the state of the art (Kumar et al., 2007). Numerous other CL problems arise: active learning to improve the quality of alignments and bilingual lexicons; automatic language identification for lowdensity languages; and morphology learning. Tool builders. We need tools for annotation, format conversion, spidering and language identification, search, archiving, and presentation. Innovative crowd-sourcing solutions are of particular interest, e.g. web-based functionality for transcribing audio and video of oral literature, or setting up a translation service based on aligned texts for a low-density language, and collecting the improved t</context>
</contexts>
<marker>Kumar, Och, Macherey, 2007</marker>
<rawString>Shankar Kumar, Franz J. Och, and Wolfgang Macherey. 2007. Improving word alignment with bridge languages. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 42–50, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Maxwell</author>
<author>Baden Hughes</author>
</authors>
<title>Frontiers in linguistic annotation for lower-density languages.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora</booktitle>
<pages>29--37</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="2022" citStr="Maxwell and Hughes, 2006" startWordPosition="324" endWordPosition="327">t be now. With the current rate of language loss, we have only a small window of opportunity before the data is gone forever. Linguistics may be unique among the sciences in the crisis it faces. The next generation will forgive us for the most egregious shortcomings in theory construction and technology development, but they will not forgive us if we fail to preserve vanishing primary language data in a form that enables future research. The scope of the task is enormous. At present, we have non-negligible quantities of machinereadable data for only about 20–30 of the world’s 6,900 languages (Maxwell and Hughes, 2006). Linguistics as a field is awake to the crisis. There has been a tremendous upsurge of interest in documentary linguistics, the field concerned with the the “creation, annotation, preservation, and dissemination of transparent records of a language” (Woodbury, 2010). However, documentary linguistics alone is not equal to the task. For example, no million-word machine-readable corpus exists for any endangered language, even though such a quantity would be necessary for wide-ranging investigation of the language once no speakers are available. The chances of constructing large-scale resources w</context>
</contexts>
<marker>Maxwell, Hughes, 2006</marker>
<rawString>Mike Maxwell and Baden Hughes. 2006. Frontiers in linguistic annotation for lower-density languages. In Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 29–37, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Mari Broman Olsen</author>
<author>Mona Diab</author>
</authors>
<title>The Bible as a parallel corpus: Annotating the ‘book of</title>
<date>1999</date>
<booktitle>tongues’. Computers and the Humanities,</booktitle>
<pages>33--129</pages>
<contexts>
<context position="23086" citStr="Resnik et al., 1999" startWordPosition="3622" endWordPosition="3625"> sum, the Universal Corpus is represented as a massive store of records, each representing a single sentence or lexical entry, using a limited set of attributes. The store is indexed for efficient access, and supports access to slices identified by language, content, provenance, rights, and so forth. Many component collections would be “unioned” into this single, large Corpus, with only the record identifiers capturing the distinction between the various data sources. Special cases of aligned text and wordlists, spanning more than 1,000 languages, are Bible translations and Swadesh wordlists (Resnik et al., 1999; Swadesh, 1955). Here there are obvious use-cases for accessing a particular verse or word across all languages. However, it is not necessary to model n-way language alignments. Instead, such sources are implicitly aligned by virtue of their structure. Extracting all translations of a verse, or all cognates of a Swadesh wordlist item, is an index operation that returns monolingual records, e.g.: ID: swadesh/47 ID: swadesh/47 LANGS: fra LANGS: eng LEX: chien LEX: dog 4 Building the Corpus Data collection on this scale is a daunting prospect, yet it is important to avoid the paralysis of over-p</context>
</contexts>
<marker>Resnik, Olsen, Diab, 1999</marker>
<rawString>Philip Resnik, Mari Broman Olsen, and Mona Diab. 1999. The Bible as a parallel corpus: Annotating the ‘book of 2000 tongues’. Computers and the Humanities, 33:129–153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Scannell</author>
</authors>
<title>The Cr´ubad´an Project: Corpus building for under-resourced languages.</title>
<date>2008</date>
<booktitle>In Cahiers du Cental 5: Proceedings of the 3rd Web as Corpus Workshop.</booktitle>
<contexts>
<context position="30816" citStr="Scannell, 2008" startWordPosition="4827" endWordPosition="4828">nguage Commons. After all, the most effective time to do translation, alignment, and lexicon work is often at the point when primary data is first collected, and this extra work promises direct benefits to the individual project. 4.3 Early tasks Seed corpus. The central challenge, we believe, is getting critical mass. Data attracts data, and if one can establish a sufficient seed, the effort will snowball. We can make some concrete proposals as to how to collect a seed. Language resources on the web are one source—the Cr´ubad´an project has identified resources for 400 languages, for example (Scannell, 2008); the New Testament of the Bible exists in about 1200 languages and contains of the order of 100k words. We hope that existing efforts that are already well-disposed toward electronic distribution will participate. We particularly mention the Language and Culture Archive of the Summer Institute of Linguistics, and the Rosetta Project. The latter is already distributed through the Internet Archive and contains material for 2500 languages. Resource discovery. Existing language resources need to be documented, a large un94 dertaking that depends on widely distributed knowledge. Existing published</context>
</contexts>
<marker>Scannell, 2008</marker>
<rawString>Kevin Scannell. 2008. The Cr´ubad´an Project: Corpus building for under-resourced languages. In Cahiers du Cental 5: Proceedings of the 3rd Web as Corpus Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary Simons</author>
<author>Steven Bird</author>
</authors>
<title>The Open Language Archives Community: An infrastructure for distributed archiving of language resources.</title>
<date>2003</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>18--117</pages>
<contexts>
<context position="17977" citStr="Simons and Bird, 2003" startWordPosition="2813" endWordPosition="2816">ndon), and the Pacific And Regional Archive for Digital Sources in Endangered Cultures (Australia) offer broad coverage of languages, but the majority of their offerings are restricted in availability and do not support machine processing. Conversely, large-scale data collection efforts by the Linguistic Data Consortium and the European Language Resources Association cover less than one percent of the world’s languages, with no evident plans for major expansion of coverage. Other efforts concern the definition and aggregation of language resource metadata, including OLAC, IMDI, and 91 CLARIN (Simons and Bird, 2003; Broeder and Wittenburg, 2006; V´aradi et al., 2008), but this is not the same as collecting and disseminating data. Initiatives to develop standard formats for linguistic annotations are orthogonal to our goals. The success of the project will depend on contributed data from many sources, in many different formats. Converting all data formats to an official standard, such as the RDF-based models being developed by ISO Technical Committee 37 Sub-committee 4 Working Group 2, is simply impractical. These formats have onerous syntactic and semantic requirements that demand substantial further pr</context>
</contexts>
<marker>Simons, Bird, 2003</marker>
<rawString>Gary Simons and Steven Bird. 2003. The Open Language Archives Community: An infrastructure for distributed archiving of language resources. Literary and Linguistic Computing, 18:117–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morris Swadesh</author>
</authors>
<title>Towards greater accuracy in lexicostatistic dating.</title>
<date>1955</date>
<journal>International Journal of American Linguistics,</journal>
<pages>21--121</pages>
<contexts>
<context position="23102" citStr="Swadesh, 1955" startWordPosition="3626" endWordPosition="3627">orpus is represented as a massive store of records, each representing a single sentence or lexical entry, using a limited set of attributes. The store is indexed for efficient access, and supports access to slices identified by language, content, provenance, rights, and so forth. Many component collections would be “unioned” into this single, large Corpus, with only the record identifiers capturing the distinction between the various data sources. Special cases of aligned text and wordlists, spanning more than 1,000 languages, are Bible translations and Swadesh wordlists (Resnik et al., 1999; Swadesh, 1955). Here there are obvious use-cases for accessing a particular verse or word across all languages. However, it is not necessary to model n-way language alignments. Instead, such sources are implicitly aligned by virtue of their structure. Extracting all translations of a verse, or all cognates of a Swadesh wordlist item, is an index operation that returns monolingual records, e.g.: ID: swadesh/47 ID: swadesh/47 LANGS: fra LANGS: eng LEX: chien LEX: dog 4 Building the Corpus Data collection on this scale is a daunting prospect, yet it is important to avoid the paralysis of over-planning. We can </context>
</contexts>
<marker>Swadesh, 1955</marker>
<rawString>Morris Swadesh. 1955. Towards greater accuracy in lexicostatistic dating. International Journal of American Linguistics, 21:121–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tam´as V´aradi</author>
<author>Steven Krauwer</author>
<author>Peter Wittenburg</author>
<author>Martin Wynne</author>
<author>Kimmo Koskenniemi</author>
</authors>
<title>CLARIN: common language resources and technology infrastructure.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation Conference. European Language Resources Association.</booktitle>
<marker>V´aradi, Krauwer, Wittenburg, Wynne, Koskenniemi, 2008</marker>
<rawString>Tam´as V´aradi, Steven Krauwer, Peter Wittenburg, Martin Wynne, and Kimmo Koskenniemi. 2008. CLARIN: common language resources and technology infrastructure. In Proceedings of the Sixth International Language Resources and Evaluation Conference. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel A Wagner</author>
</authors>
<title>Literacy, Culture, and Development: Becoming Literate in Morocco.</title>
<date>1993</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="25846" citStr="Wagner, 1993" startWordPosition="4066" endWordPosition="4067">om collaboration. In return for the data that documentary linguistics can provide, computational linguistics has the potential to revolutionize the tools and practice of language documentation. We also seek collaboration with communities of language speakers. The corpus provides an economy of scale for the development of literacy materials and tools for interactive language instruction, in support of language preservation and revitalization. For small languages, literacy in the mother tongue is often defended on the grounds that it provides the best route to literacy in the national language (Wagner, 1993, ch. 8). An essential ingredient of any local literacy program is to have a substantial quantity of available texts that represent familiar topics including cultural heritage, folklore, personal narratives, and current events. Transition to literacy in a language of wider communication is aided when transitional materials are available (Waters, 1998, pp. 61ff). Mutual benefits will also flow from the development of tools for low-cost publication and broadcast in the language, with copies of the published or broadcast material licensed to and archived in the corpus. 4.2 Roles The enterprise re</context>
</contexts>
<marker>Wagner, 1993</marker>
<rawString>Daniel A. Wagner. 1993. Literacy, Culture, and Development: Becoming Literate in Morocco. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenys Waters</author>
</authors>
<title>Local Literacies: Theory and Practice. Summer Institute of Linguistics,</title>
<date>1998</date>
<location>Dallas.</location>
<contexts>
<context position="26198" citStr="Waters, 1998" startWordPosition="4121" endWordPosition="4122">for interactive language instruction, in support of language preservation and revitalization. For small languages, literacy in the mother tongue is often defended on the grounds that it provides the best route to literacy in the national language (Wagner, 1993, ch. 8). An essential ingredient of any local literacy program is to have a substantial quantity of available texts that represent familiar topics including cultural heritage, folklore, personal narratives, and current events. Transition to literacy in a language of wider communication is aided when transitional materials are available (Waters, 1998, pp. 61ff). Mutual benefits will also flow from the development of tools for low-cost publication and broadcast in the language, with copies of the published or broadcast material licensed to and archived in the corpus. 4.2 Roles The enterprise requires collaboration of many individuals and groups, in a variety of roles. Editors. A critical group are people with sufficient engagement to serve as editors for particular language families, who have access to data or are able to negotiate redistribution rights, and oversee the workflow of transcription, translation, and annotation. 93 CL Research</context>
</contexts>
<marker>Waters, 1998</marker>
<rawString>Glenys Waters. 1998. Local Literacies: Theory and Practice. Summer Institute of Linguistics, Dallas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas H Whalen</author>
<author>Gary Simons</author>
</authors>
<title>Endangered language families.</title>
<date>2009</date>
<booktitle>In Proceedings of the 1st International Conference on Language Documentation and</booktitle>
<institution>Conservation. University of Hawaii.</institution>
<note>http://hdl.handle.net/10125/5017.</note>
<contexts>
<context position="12874" citStr="Whalen and Simons, 2009" startWordPosition="2038" endWordPosition="2041">ignment and glossing initiatives, in which each step is an instance of semisupervised learning (Abney, 2007). As time passes, our ability to enhance the quantity and quality of the annotations will only increase, thanks to effective combinations of automatic, professional, and crowd-sourced effort. 2.2 Principles The basic principles upon which the envisioned corpus is based are the following: Universality. Covering as many languages as possible is the first priority. Progress will be gauged against concrete goals for numbers of languages, data per language, and coverage of language families (Whalen and Simons, 2009). Machine readability and consistency. “Covering” languages means enabling machine processing seamlessly across languages. This will support new types of linguistic inquiry and the development and testing of inference methods (for morphology, parsers, machine translation) across large numbers of typologically diverse languages. Community effort. We cannot expect a single organization to assemble a resource on this scale. It will be necessary to get community buy-in, and 90 many motivated volunteers. The repository will not be the sole possession of any one institution. Availability. The conten</context>
</contexts>
<marker>Whalen, Simons, 2009</marker>
<rawString>Douglas H. Whalen and Gary Simons. 2009. Endangered language families. In Proceedings of the 1st International Conference on Language Documentation and Conservation. University of Hawaii. http://hdl.handle.net/10125/5017.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony C Woodbury</author>
</authors>
<title>Language documentation.</title>
<date>2010</date>
<editor>In Peter K. Austin and Julia Sallabank, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2289" citStr="Woodbury, 2010" startWordPosition="367" endWordPosition="368">construction and technology development, but they will not forgive us if we fail to preserve vanishing primary language data in a form that enables future research. The scope of the task is enormous. At present, we have non-negligible quantities of machinereadable data for only about 20–30 of the world’s 6,900 languages (Maxwell and Hughes, 2006). Linguistics as a field is awake to the crisis. There has been a tremendous upsurge of interest in documentary linguistics, the field concerned with the the “creation, annotation, preservation, and dissemination of transparent records of a language” (Woodbury, 2010). However, documentary linguistics alone is not equal to the task. For example, no million-word machine-readable corpus exists for any endangered language, even though such a quantity would be necessary for wide-ranging investigation of the language once no speakers are available. The chances of constructing large-scale resources will be greatly improved if computational linguists contribute their expertise. This collaboration between linguists and computational linguists will extend beyond the construction of the Universal Corpus to its exploitation for both theoretical and technological ends</context>
</contexts>
<marker>Woodbury, 2010</marker>
<rawString>Anthony C. Woodbury. 2010. Language documentation. In Peter K. Austin and Julia Sallabank, editors, The Cambridge Handbook of Endangered Languages. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>William D Lewis</author>
</authors>
<title>Multilingual structural projection across interlinearized text.</title>
<date>2007</date>
<booktitle>In Proceedings of the Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11653" citStr="Xia and Lewis, 2007" startWordPosition="1856" endWordPosition="1859">ognize allomorphs, reducing the amount of data required for training an MT system. This most-refined target annotation corresponds to the interlinear glossed texts that are the de facto standard of annotation in the documentary linguistics community. We postulate that interlinear glossed text is sufficiently fine-grained to serve our purposes. It invites efforts to enrich it by automatic means: for example, there has been work on parsing the English translations and using the word-by-word glosses to transfer the parse tree to the object language, effectively creating a treebank automatically (Xia and Lewis, 2007). At the same time, we believe that interlinear glossed text is sufficiently simple and well-understood to allow rapid construction of resources, and to make cross-linguistic consistency a realistic goal. Each of these layers—primary text, translations, alignments, and morphological glosses—seems to be an unavoidable piece of the overall solution. The fact that these layers will exist in diminishing quantity is also unavoidable. However, there is an important consequence: the primary texts will be permanently subject to new translation initiatives, which themselves will be subject to new align</context>
</contexts>
<marker>Xia, Lewis, 2007</marker>
<rawString>Fei Xia and William D. Lewis. 2007. Multilingual structural projection across interlinearized text. In Proceedings of the Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL). Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>