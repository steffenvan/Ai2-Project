<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003270">
<title confidence="0.9989155">
Integrating Multi-level Linguistic Knowledge with a Unified Framework for
Mandarin Speech Recognition
</title>
<author confidence="0.97989">
Xinhao Wang, Jiazhong Nie, Dingsheng Luo, Xihong Wu*
</author>
<affiliation confidence="0.95388575">
Speech and Hearing Research Center,
Key Laboratory of Machine Perception (Ministry of Education),
School of Electronics Engineering and Computer Science,
Peking University, Beijing, 100871, China
</affiliation>
<email confidence="0.996615">
{wangxh,niejz,wxh,dsluo}@cis.pku.edu.cn
</email>
<sectionHeader confidence="0.995597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999772185185185">
To improve the Mandarin large vocabulary
continuous speech recognition (LVCSR), a
unified framework based approach is intro-
duced to exploit multi-level linguistic knowl-
edge. In this framework, each knowledge
source is represented by a Weighted Finite
State Transducer (WFST), and then they are
combined to obtain a so-called analyzer for in-
tegrating multi-level knowledge sources. Due
to the uniform transducer representation, any
knowledge source can be easily integrated into
the analyzer, as long as it can be encoded
into WFSTs. Moreover, as the knowledge in
each level is modeled independently and the
combination is processed in the model level,
the information inherently in each knowledge
source has a chance to be thoroughly ex-
ploited. By simulations, the effectiveness
of the analyzer is investigated, and then a
LVCSR system embedding the presented ana-
lyzer is evaluated. Experimental results reveal
that this unified framework is an effective ap-
proach which significantly improves the per-
formance of speech recognition with a 9.9%
relative reduction of character error rate on
the HUB-4 test set, a widely used Mandarin
speech recognition task.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996862">
Language modeling is essential for large vocabu-
lary continuous speech recognition (LVCSR), which
aims to determine the prior probability of a supposed
word string W, p(W). Although the word-based n-
gram language model remains the mainstream for
</bodyText>
<note confidence="0.828541">
Corresponding author: Xihong Wu
</note>
<bodyText confidence="0.999712852941177">
most speech recognition systems, the utilization of
linguistic knowledge is too limited in this model.
Consequently, many researchers have focused on
introducing more linguistic knowledge in language
modeling, such as lexical knowledge , syntax and
semantics of language (Wang and Vergyri, 2006;
Wang et al., 2004; Charniak, 2001; Roark, 2001;
Chelba, 2000; Heeman, 1998; Chelba et al., 1997).
Recently, structured language models have been
introduced to make use of syntactic hierarchi-
cal characteristics (Roark, 2001; Charniak, 2001;
Chelba, 2000). Nevertheless, the computational
complexity of decoding will be heavily increased, as
they are parser-based models. In contrast, the class-
based language model groups the words that have
similar functions of syntax or semantics into mean-
ingful classes. As a result, it handles the questions of
data sparsity and generalization of unseen event. In
practice, the part-of-speech (POS) information, cap-
turing the syntactic role of words, has been widely
used in clustering words (Wang and Vergyri, 2006;
Maltese et al., 2001; Samuelsson and Reichl, 1999).
In Heeman’s POS language model (Heeman, 1998),
the joint probability of word sequence and associ-
ated POS sequence was estimated directly, which
has been demonstrated to be superior to the condi-
tional probability previously used in the class-based
models (Johnson, 2001). Moreover, a SuperARV
language model was presented (Wang and Harper,
2002), in which lexical features and syntactic con-
straints were tightly integrated into a linguistic struc-
ture of SuperARV serving as a class in the model.
Thus, these knowledge was integrated in the rep-
resentation level, and then the joint probabilities
</bodyText>
<page confidence="0.957847">
820
</page>
<note confidence="0.9617985">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 820–828,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.99996259375">
of words and corresponding SuperARVs were esti-
mated. However, in the class-based language mod-
els, words are taken as the model units, while other
units smaller or larger than words are unfeasible for
modeling simultaneously, such as the Chinese char-
acters for Chinese names.
Usually, speech recognition systems can only rec-
ognize the words within a predefined dictionary.
With the increase of unknown words, i.e., out-of-
vocabulary (OOV) words, the performance will de-
grade dramatically. This is because not only those
unknown words cannot be recognized correctly, but
the words surrounding them will be affected. Thus,
many efforts have been made to deal with the is-
sue of OOV words (Martins et al., 2006; Galescu,
2003; Bazzi and Glass, 2001), and various model
units smaller than words have been examined to rec-
ognize OOVs from speech, such as phonemes (Bazzi
and Glass, 2000a), variable-length phoneme se-
quence (Bazzi and Glass, 2001), syllable (Bazzi and
Glass, 2000b) and sub-word (Galescu, 2003). Since
the proper name is a typical category of OOV words
and usually takes a very large proportion among all
kinds of OOV words, it has been specially addressed
in (Hu et al., 2006; Tanigaki et al., 2000).
All those attempts mentioned above succeed in
utilizing linguistic knowledge in language modeling
in some degree respectively. In this study, a uni-
fied framework based approach, which aims to ex-
ploit information from multi-level linguistic knowl-
edge, is presented. Here, the Weighted Finite State
Transducer (WFST) turns to be an ideal choice for
our purpose. WFSTs were formerly introduced to
simplify the integration of models in speech recog-
nition, including acoustic models, phonetic mod-
els and word n-gram (Mohri, 1997; Mohri et al.,
2002). In recent years, the WFST has been suc-
cessfully applied in several state-of-the-art speech
recognition systems, such as systems developed by
the AMI project (Hain et al., 2006), IBM (Saon et
al., 2003) and AT&amp;T (Mohri et al., 1996), and in
various fields of natural language processing, such
as smoothed n-gram model, partial parsing (Abney,
1996), named entities recognition (Friburger and
Maurel, 2004), semantic interpretation (Raymond et
al., 2006) and machine translation (Tsukada and Na-
gata, 2004). In (Takaaki Hori and Minami, 2003),
the WFST has been further used for language model
adaptation, where language models of different vo-
cabularies that represented different styles were in-
tegrated through the framework of speech transla-
tion. In WFST-based systems, all of the models are
represented uniformly by WFSTs, and the general
composition algorithm (Mohri et al., 2000) com-
bines these representations flexibly and efficiently.
Thereby, rather than integrating the models step by
step in decoding stage, a complete search network is
constructed in advance. The combined WFST will
be more efficient by optimizing with determiniza-
tion, minimization and pushing algorithms of WF-
STs (Mohri, 1997). Besides, the researches on opti-
mizing the search space and improving WFST-based
speech recognition has been carried out, especially
on how to perform on-the-fly WFSTs composition
more efficiently (Hori et al., 2007; Diamantino Ca-
seiro, 2002).
In this study, we extend the linguistic knowledge
used in speech recognition. As WFSTs provide a
common and natural representation for lexical con-
straints, n-gram language model, Hidden Markov
Model models and context-dependency, multi-level
knowledge sources can be encoded into WFSTs un-
der the uniform transducer representation. Then this
group of WFSTs is flexibly combined together to
obtain an analyzer representing knowledge of per-
son and location names as well as POS information.
Afterwards, the presented analyzer is incorporated
into LVCSR to evaluate the linguistic correctness of
recognition candidates by an n-best rescoring.
Unlike other methods, this approach holds two
distinct features. Firstly, as all multi-level knowl-
edge sources are modeled independently, the model
units such as character, words, phrase, etc., can be
chosen freely. Meanwhile, the integration of these
information sources is conducted in the model level
rather than the representation level. This setup will
help to model each knowledge source sufficiently
and may promote the accuracy of speech recogni-
tion. Secondly, under this unified framework, it is
easy to combine additional knowledge source into
the framework with the only requirement that the
new knowledge source can be represented by WF-
STs. Moreover, since all knowledge sources are fi-
nally represented by a single WFST, additional ef-
forts are not required for decoding the new knowl-
edge source.
</bodyText>
<page confidence="0.996444">
821
</page>
<bodyText confidence="0.999939">
The remainder of this paper is structured as fol-
lows. In section 2, we introduce our analyzer in de-
tail, and incorporate it into a Mandarin speech recog-
nition system. In section 3, the simulations are per-
formed to evaluate the analyzer and test its effective-
ness when being applied to LVCSR. The conclusion
appears in section 4.
</bodyText>
<sectionHeader confidence="0.808043" genericHeader="method">
2 Incorporation of Multi-level linguistic
knowledge in LVCSR
</sectionHeader>
<bodyText confidence="0.999951571428571">
In this section, we start by giving a brief descrip-
tion on WFSTs. Then some special characteristics
of Chinese are investigated, and the model units are
fixed. Afterwards, each knowledge source is rep-
resented with WFSTs, and then they are combined
into a final WFST, so-called analyzer. At last, this
analyzer is incorporated into Mandarin LVCSR.
</bodyText>
<subsectionHeader confidence="0.976136">
2.1 Weighted Finite State Transducers
</subsectionHeader>
<bodyText confidence="0.999925318181818">
The Weighted Finite State Transducer (WFST) is the
generalization of the finite state automata, in which,
besides of an input label, an output label and a
weight are also placed on each transition. With these
labels, a WFST is capable of realizing a weighted re-
lation between strings. In our system, log probabili-
ties are adopted as transition weights and the relation
between two strings is associated with a weight indi-
cating the probability of the mapping between them.
Given a group of WFSTs, each of which models a
stage of a mapping cascade, the composition opera-
tion provides an efficient approach to combine them
into a single one (Mohri et al., 2002; Mohri et al.,
1996). In particular, for two WFSTs R and S, the
composition T = RoS represents the composition
of relations realized by R and S. The combination
is performed strictly on R’s output and S’s input. It
means for each path in T, mapping string r to string
s, there must exist a path mapping r to some string
t in R and a path mapping t to s in S. Decoding on
the combined WFST enables to find the joint opti-
mal results for multi-level weighted relations.
</bodyText>
<subsectionHeader confidence="0.999517">
2.2 Model Unit Selection
</subsectionHeader>
<bodyText confidence="0.99998752">
This study primarily takes the person and location
names as well as the POS information into account.
To deal with Chinese OOV words, different from
the western language in which the phoneme, sylla-
ble or sub-word are used as the model units (Bazzi
and Glass, 2000a; Bazzi and Glass, 2000b; Galescu,
2003), Chinese characters are taken as the basic
units. In general, a person name of Han nation-
ality consists of a surname and a given name usu-
ally with one or two characters. Surnames com-
monly come from a fixed set that has been histori-
cally used. According to a recent investigation on
surnames involving 296 million people, 4100 sur-
names are found, and 129 most used surnames ac-
count for 87% (conducted by the Institute of Genet-
ics and Developmental Biology, Chinese Academy
of Sciences). In contrast, the characters used in
given names can be selected freely, and in many situ-
ations, some commonly used words may also appear
in names, such as ”#44” (victory) and ”��” (the
Changjiang River). Therefore, both Chinese charac-
ters and words are considered as model units in this
study, and a word re-segmentation process on recog-
nition hypotheses is necessary, where an n-gram lan-
guage model based on word classes is adopted.
</bodyText>
<subsectionHeader confidence="0.992373">
2.3 Representation and Integration of
Multi-level Knowledge
</subsectionHeader>
<bodyText confidence="0.971399285714286">
In this work, we ignore the word boundaries of n-
best hypotheses and perform a word re-segmentation
for names recognition. Given an input Chinese
character, it is encoded by a finite state acceptor
FSAZnput. For example, the input ”AA3 TR1”
(while synthesizing molecule) is represented as in
Figure 1(a). Then a dictionary is represented by a
</bodyText>
<figureCaption confidence="0.9953065">
Figure 1: (a) is an example of the FSA representing a
given input; (b) is the FST representing a toy dictionary.
</figureCaption>
<figure confidence="0.992277714285714">
10
0 ড় 1 ៤ 2 ߚ 3 ᄤ 4 ᯊ 5
8
ᄤ:E
E:ߚᄤ
E:ᄤᯊ
7
ߚ:E
ᄤ:E
ড়:E
(a)
0
E:ᯊ ៤:E
1 3
E:ড়
ড়:E
E:ড়៤
E:៤ߚ
៤:E
4
6
</figure>
<page confidence="0.988316">
822
</page>
<bodyText confidence="0.9986986">
transducer with empty weights, denoted as FSTdict.
Figure 1(b) illustrates a toy dictionary listed in Ta-
ble 1, in which a successful path encodes a mapping
from a Chinese character sequence to some word
in the dictionary. In practice, all Chinese charac-
</bodyText>
<table confidence="0.842029">
Chinese Words English Words
合成 synthesize
成分 element
分子 molecule
子时 the period of the day from
11 p.m.to l a.m.
合 together
时 present
</table>
<tableCaption confidence="0.994606">
Table 1: The Toy dictionary
</tableCaption>
<bodyText confidence="0.985776333333333">
ters should appear in the dictionary for further in-
corporating models of names. Then the combination
of FSAinput and FSTdict, FSTseg = FSAinput o
FSTdict, will result in a WFST embracing all the
possible candidate segmentations. Afterwards an n-
gram language model based on word classes is used
to weight the candidate segmentations. As in Fig-
ure 2, a toy bigram with three words is depicted by
WFSTn−gram, and the word classes are defined in
</bodyText>
<tableCaption confidence="0.779518">
Table 2. Here, both in the training and test stages,
</tableCaption>
<figureCaption confidence="0.977747">
Figure 2: The WFST representing a toy bigram language
model, in which un(w1) denotes the unigram of w1;
bi(w1, w2) and back(w1) respectively denotes the bi-
gram of w2 and the backoff weight given the word history
w1.
</figureCaption>
<bodyText confidence="0.5807">
the strings of numbers or letters in sentences are ex-
</bodyText>
<table confidence="0.9980345">
Classes Description
wi Each word wi listed in the dictionary
CNAME Person names of Han nationality
TNAME Translated person names
LOC Location names
NUM Number expressions
LETTER Letter strings
NON Other non Chinese character strings
BEGIN Beginning of sentence
END End of sentence
</table>
<tableCaption confidence="0.99956">
Table 2: The Definition of word classes
</tableCaption>
<bodyText confidence="0.999980617647059">
tracted according to the rules, and then substituted
with the class tags, ”NUM” and ”LETTER” respec-
tively. At the same time, the words, such as ”三月”
and ”A型”, are replaced with ”NUM月” and ”LET-
TER型” in the dictionary. In addition, name classes,
including ”CNAME”, ”TNAME” and ”LOC”, will
be set according to names recognition.
Hidden Markov Models (HMMs) are adopted
both for names recognition and POS tagging. Here,
each HMM is represented with two WFSTs. Tak-
ing the POS tagging as an example, the toy POS
WFSTs with 3 different tags are illustrated in Fig-
ure 3. The emission probability of a word by a POS,
(P(word/pos)), is represented as in Figure 3(a),
and the bigram transition probabilities between POS
tags are represented as in Figure 3(b), similar to the
word n-gram. In terms of names recognition, the
HMM states correspond to 30 role tags of names,
some for model units of Chinese characters, such as
surname, the first or second character of a given per-
son name with two characters, the first or last charac-
ter of a location name and so on, but others for model
units of words, such as the word before or after a
name, the words in a name and so on. When rec-
ognizing the person names, since there is a big dif-
ference between the translated names and the names
of Han nationality, two types of person names are
modeled separately, and substituted with two differ-
ent class tags in the segmentation language model,
as ”TNAME” and ”CNAME”. Some rules, which
can be encoded into WFSTs, are responsible for the
transformation from a role sequence to correspond-
ing name class (for example, a role sequence might
consist of the surname, the first character of the
</bodyText>
<equation confidence="0.981">
1
w1/bi(w2,w1)
İ/back(w1)
w1/un(w1)
w2/bi(w1,w2)
w1/bi(w3,w1)
0 w2/un(w2)
2
İ/back(w2)
4
w2/un(w2)
w3/bi(w2,w3)
w3/bi(w1,w3)
w3/un(w3)
w3/un(w3)
İ/back(w3)
3
w2/bi(w3,w2)
w1/un(w1)
</equation>
<page confidence="0.962081">
823
</page>
<figure confidence="0.525831">
word: pos/p(word/pos)
</figure>
<figureCaption confidence="0.9975635">
Figure 3: The toy POS WFSTs. (a) is the WFST rep-
resenting the relationship between the word and the pos;
(b) is the WFSA representing the bigram transition prob-
abilities between POS tags
</figureCaption>
<bodyText confidence="0.999983166666667">
given name, and the second character of the given
name, which will be transformed to ”CNAME” in
FST3eg). Hence, taking names recognition into ac-
count, a WFST, including all possible segmentations
as well as recognized candidates of names, can be
obtained as below, denoted as WFSTworda:
</bodyText>
<equation confidence="0.95316825">
FSAZnp,,t o FSTdZet o WFSTne o WFSAn−gram
(1)
POS information is integrated as follows.
(α * WFSTworda) o WFSTPpS (2)
</equation>
<bodyText confidence="0.998583333333333">
Consequently, the desired analyzer, a combined
WFST that represents multi-level linguistic knowl-
edge sources, has been obtained.
</bodyText>
<subsectionHeader confidence="0.970324">
2.4 Incorporation in LVCSR
</subsectionHeader>
<bodyText confidence="0.9998285">
The presented analyzer models linguistic knowledge
at different levels, which will be useful to find an
optimal words sequence among a large number of
speech recognition hypotheses. Thus in this re-
search, the analyzer is incorporated after the first
pass recognition, and the n-best hypotheses are
reranked according to the total path scores adjusted
with the analyzer scores as follows.
</bodyText>
<equation confidence="0.93206275">
log (PAM (O|W))
+Q * log (PLM (W))
+ry * log (PAnalyzer (W))
(3)
</equation>
<bodyText confidence="0.999822166666667">
where PAM (O|W) and PLM (W) are the acoustic
and language scores produced in first pass decoding,
and PAnalyzer (W) reflects the linguistic correctness
of one hypothesis scored by the analyzer. Through
the reranking paradigm, a new best sentence hypoth-
esis is obtained.
</bodyText>
<sectionHeader confidence="0.998086" genericHeader="method">
3 Simulation
</sectionHeader>
<bodyText confidence="0.999957285714286">
Under the unified framework, multi-level linguistic
knowledge is represented by the analyzer as men-
tioned above. To guarantee the effectiveness of
the introduced framework in integrating knowledge
sources, the analyzer is evaluated in this section.
Then the experiments using an LVCSR system in
which the analyzer is embedded are performed.
</bodyText>
<subsectionHeader confidence="0.998566">
3.1 Analyzer Evaluation
</subsectionHeader>
<bodyText confidence="0.999756428571429">
Considering the function of the analyzer, cascaded
subtasks of word segmentation, names recognition
and POS tagging can be processed jointly, while
they are traditionally handled in a pipeline manner.
Hence, a comparison between the analyzer and the
pipeline system can be used to evaluate the effec-
tiveness of the introduced framework for knowledge
integration. As illustrated in Figure 4, two systems
based on the presented analyzer and the pipeline
manner are constructed respectively.
The evaluation data came from the People’s Daily
of China in 1998 from January to June (annotated by
the Institute of Computational Linguistics of Peking
University1), among which the January to May data
was taken as the training set, and the June data was
taken as the test set (consisted of 21,143 sentences
and about 1.2 million words). The first two thou-
sand sentences from the June data were extracted
as the development set, used to fix the composition
weight α in equation 2. A dictionary including about
113,000 words was extracted from the training data,
</bodyText>
<footnote confidence="0.953325">
1http://icl.pku.edu.cn/icl res/
</footnote>
<figure confidence="0.986592451612903">
pos1/bi(pos3,pos1)
pos3/un(pos3)
pos2/bi(pos3,pos2)
0
(a)
1
pos1/un(pos1)
pos1/bi(pos2,pos1)
0 pos2/un(pos2)
pos2/bi(pos1,pos2)
pos3/bi(pos1,pos3)
2
pos3/bi(pos2,pos3)
3
(b)
�
W� = arg max � �
W
�
� �
824
The best segmentation
Compose
Decode
Decode
FSAinput ° FSTdict °W FSTne °W FSTn_gram
WFSTpos
Compose
Decode
output output
Pipeline System Presented Analyzer
</figure>
<figureCaption confidence="0.999996">
Figure 4: The pipeline system vs The analyzer
</figureCaption>
<bodyText confidence="0.999964615384615">
in which a person or location name was accounted
as a word in vocabulary, only when the number of
its appearances was no less than three.
In Figure 5, the analyzer is compared with the
pipeline system, where the analyzer outperforms the
pipeline manner on all the subtasks in terms of Fl-
score metric. Furthermore to detect the differences,
the statistical significance test using approximate
randomization approach (Yeh, 2000) is done on the
word segmentation results. Since there are more
than 21,000 sentences in the test set, which is not
appropriate for approximate randomization test, ten
sets (500 sentences for each) are randomly selected
from the test corpus. For each set, we run 1048576
shuffles twice and calculate the significance level p-
value according to the shuffled results. It has been
shown that all p-value are less than 0.001 on the ten
sets. Accordingly the improvement is statistically
significant. Actually, this significant improvement
is reasonable, since the joint processing avoids error
propagation and provides the opportunity of shar-
ing information between different level knowledge
sources. The superiority of this analyzer also shows
that the integration of multi-level linguistic knowl-
edge under the unified framework is effective, which
may lead to improved LVCSR.
</bodyText>
<note confidence="0.539993">
96.8
POS Tagging Person Name Location Name
</note>
<figureCaption confidence="0.9843122">
Figure 5: The Performance comparison between the
pipeline system and the analyzer. The system perfor-
mances are measured with the F1-score in the tasks
of word segmentation, POS tagging, the person names
recognition and the location names recognition.
</figureCaption>
<subsectionHeader confidence="0.9893465">
3.2 Experimental Setup for Mandarin Speech
Recognition
</subsectionHeader>
<bodyText confidence="0.99988016">
In the baseline speech recognition system, the
acoustic models consisted of context-dependent
Initial-Final models, in which the left-to-right model
topology was used to represent each unit. Accord-
ing to the phonetic structures, the number of states
in each model was set to 2 or 3 for initials, and 4
or 5 for tonal finals. Each state was trained to have
32 Gaussian mixtures. The used 39-dimension fea-
ture vector comprised 12 MFCC coefficients, en-
ergy, and their first-order and second-order deltas.
Since in this work we focused on modeling knowl-
edge of language in Mandarin LVCSR, only clean
male acoustic models were trained with a speech
database that contained about 360 hours speech of
over 750 male speakers. This training data was
picked up from three continuous Mandarin speech
corpora: the 863-I, 863-II and Intel corpora. The
brief information about these three speech corpora
was listed in Table 3. As in this work, the eval-
uation data was the 1997 HUB-4 Mandarin broad-
cast news evaluation data (HUB-4 test set), to bet-
ter fit this task, the acoustic models were adapted
by the approach of maximum a posterior (MAP)
adaptation. The adaption data was drawn from the
HUB4 training set, excluding the HUB-4 develop-
</bodyText>
<figure confidence="0.931906529411765">
Pipeline Analyzer
Integrated Analyzer
91.1
90.9
Word
Segmentation
Recognition Recognition
91.8
89.9
88.5
83.3
95.9
96
92
88
84
80
</figure>
<page confidence="0.994322">
825
</page>
<table confidence="0.997883666666667">
Corpus Speakers Amount of Speech
(hours)
863-I (male) 83 56.67
863-II(male) 120 78.08
Intel (male) 556 227.30
total 759 362.05
</table>
<tableCaption confidence="0.999984">
Table 3: The information of the speech training data
</tableCaption>
<bodyText confidence="0.999078458333333">
ing set, where only the cleaned male speech data
(data under condition f0 defined as (Doddington,
1996)) was used. The partition for the clean data
was done with the acoustic segmentation software
CMUseg 0.52 (Siegler et al., 1997), and finally 8.6
hours adaptation data was obtained.
The language model was a word-based trigram
built on 60,000 words entries and trained with a cor-
pus about 1.5 billion characters. The training set
consisted of broadcast news data from the Xinhua
News Agency released by LDC (Xinhua part of Chi-
nese Gigaword), seven years data of People’s Daily
of China from 1995 to 2002 released by People’s
Daily Online3, and some other data from news web-
sites, such as yahoo, sina and so on.
In addition, the analyzer incorporated in speech
recognition was trained with a larger corpus from
People’s Daily of China, including the data in 1998
from January to June and the data in 2000 from
January to November (annotated by the Institute
of Computational Linguistics of Peking University).
The December data in 2000 was taken as the devel-
opment set used to fix the composition weight α in
equation 2.
</bodyText>
<subsectionHeader confidence="0.999208">
3.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999865555555556">
In our experiments, the clean male speech data from
the Hub-4 test set was used, and 238 sentences were
finally extracted for testing. The weight of the ana-
lyzer was empirically derived from the development
set, including 649 clean male sentences from the de-
vSet of HUB-4 Evaluation. The recognition results
are shown in Table 4. The baseline system has a
character error rate (CER) of 14.85%. When the an-
alyzer is incorporated, a 9.9% relative reduction is
</bodyText>
<footnote confidence="0.999742666666667">
2Acoustic segmentation software downloaded from
http://www.nist.gov/speech/tools/CMUseg 05targz.htm.
3http://www.people.com.cn
</footnote>
<table confidence="0.99247625">
System Err. Sub. Del. Ins.
Baseline 14.85 13.02 0.76 1.07
Analyzer 13.38 11.78 1.00 0.60
incorporation
</table>
<tableCaption confidence="0.999831">
Table 4: The Speech recognition results
</tableCaption>
<bodyText confidence="0.999917">
achieved. Furthermore, we ran the statistical signif-
icance test to detect the performance improvement,
in which the approximate randomization approach
(Yeh, 2000) was modified to output the significance
level, p-value, for the CER metric. The p-levels pro-
duced through two rounds of 1048576 shuffles are
0.0058 and 0.0057 respectively, both less than 0.01.
Thus the performance improvement imposed by the
utilization of the analyzer is statistically significant.
</bodyText>
<sectionHeader confidence="0.999297" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999986888888889">
Addressing the challenges of Mandarin large vocab-
ulary continuous speech recognition task, within the
unified framework of WFSTs, this study presents
an analyzer integrating multi-level linguistic knowl-
edge. Unlike other methods, model units, such as
characters and words, can be chosen freely in this
approach since multi-level knowledge sources are
modeled independently. As a consequence, the fi-
nal analyzer can be derived from the combination
of better optimized models based on proper model
units. Along with two level knowledge sources, i.e.,
the person and location names as well as the part-of-
speech information, the analyzer is built and evalu-
ated by a comparative simulation. Further evaluation
is also conducted on an LVCSR system in which the
analyzer is embedded. Experimental results consis-
tently reveal that the approach is effective, and suc-
cessfully improves the performance of speech recog-
nition by a 9.9% relative reduction of character error
rate on the HUB-4 test set. Also, the unified frame-
work based approach provides a property of integrat-
ing additional linguistic knowledge flexibly, such as
organization name and syntactic structure. Further-
more, the presented approach has a benefit of ef-
ficiency that additional efforts are not required for
decoding as new knowledge comes, since all knowl-
edge sources are finally encoded into a single WFST.
</bodyText>
<page confidence="0.997792">
826
</page>
<sectionHeader confidence="0.998329" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996738333333334">
The work was supported in part by the National
Natural Science Foundation of China (60435010;
60535030; 60605016), the National High Tech-
nology Research and Development Program of
China (2006AA01Z196; 2006AA010103), the Na-
tional Key Basic Research Program of China
(2004CB318005), and the New-Century Training
Program Foundation for the Talents by the Ministry
of Education of China.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999840494505495">
Steven Abney. 1996. Partial parsing via finite-state cas-
cades. Natural Language Engineering, 2(4):337–344.
Issam Bazzi and James R. Glass. 2000a. Modeling out-
of-vocabulary words for robust speech recognition. In
Proc. of 6th International Conference on Spoken Lan-
guage Processing, pages 401–404, Beijing, China, Oc-
tober.
Issam Bazzi and James Glass. 2000b. Heterogeneous
lexical units for automatic speech recognition: prelim-
inary investigations. In Proc. of ICASSP, pages 1257–
1260, Istanbul, Turkey, June.
Issam Bazzi and James Glass. 2001. Learning units
for domain-independent out-of-vocabulary word mod-
elling. In Proc. of EUROSPEECH, pages 61–64, Aal-
borg, Denmark, September.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proc. of ACL, pages 116–123,
Toulouse, France, July.
Ciprian Chelba, David Engle, Frederick Jelinek, Vic-
tor Jimenez, Sanjeev Khudanpur, Lidia Mangu, Harry
Printz, Eric Ristad, Ronald Rosenfeld, Andreas Stol-
cke, and Dekai Wu. 1997. Structure and performance
of a dependency language model. In Proc. of EU-
ROSPEECH, pages 2775–2778, Rhodes, Greece.
Ciprian Chelba. 2000. Exploiting Syntactic Structure for
Natural Language Modeling. Ph.D. thesis, Johns Hop-
kins University.
Isabel Trancoso Diamantino Caseiro. 2002. Using dy-
namic WFST composition for recognizing broadcast
news. In Proc. of ICSLP, pages 1301–1304, Denver,
Colorado, USA, September.
George Doddington. 1996. The 1996 hub-
4 annotation specification for evaluation of
speech recognition on broadcast news. In
ftp://jaguar.ncsl.nist.gov/csr96/h4/h4annot.ps.
N. Friburger and D. Maurel. 2004. Finite-state trans-
ducer cascades to extract named entities in texts. The-
oretical Computer Science, 313(1):93–104.
Lucian Galescu. 2003. Recognition of out-of-vocabulary
words with sub-lexical language models. In Proc.
of EUROSPEECH, pages 249–252, Geneva, Switzer-
land, September.
Thomas Hain, Lukas Burget, John Dines, Giulia Garau,
Martin Karafiat, Mike Lincoln, Jithendra Vepa, and
Vincent Wan. 2006. The AMI meeting transcription
system: Progress and performance. In Proc. of Rich
Transcription 2006 Spring Meeting Recognition Eval-
uation.
Peter A. Heeman. 1998. Pos tagging versus classes in
language modeling. In Proc. of the 6th Workshop on
very large corpora, pages 179–187, Montreal, Canada.
Takaaki Hori, Chiori Hori, Yasuhiro Minami, and At-
sushi Nakamura. 2007. Efficient WFST-based one-
pass decoding with on-the-fly hypothesis rescoring in
extremely large vocabulary continuous speech recog-
nition. IEEE Transactions on audio, speech, and lan-
guage processing, 15(4):1352–1365.
Xinhui Hu, Hirofumi Yamamoto, Genichiro Kikui, and
Yoshinori Sagisaka. 2006. Language modeling of
chinese personal names based on character units for
continuous chinese speech recognition. In Proc. of
INTERSPEECH, pages 249–252, Pittsburgh, USA,
September.
Mark Johnson. 2001. Joint and conditional estimation of
tagging and parsing models. In Proc. of ACL, pages
322 – 329, Toulouse, France.
G. Maltese, P. Bravetti, H. Crépy, B. J. Grainger, M. Her-
zog, and F. Palou. 2001. Combining word- and
class-based language models: A comparative study in
several languages using automatic and manual word-
clustering techniques. In Proc. of EUROSPEECH,
pages 21–24, Aalborg, Denmark, September.
Ciro Martins, Antonio Texeira, and Joao Neto. 2006.
Dynamic vocabulary adaptation for a daily and real-
time broadcast news transcription system. In Proc. of
Spoken Language Technology Workshop, pages 146–
149, December.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
1996. Weighted automata in text and speech process-
ing. In ECAI-96 Workshop.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. The design principles of a weighted finite-
state transducer library. Theoretical Computer Sci-
ence, 231(1):17–32.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech and Language,
16(1):69–88.
Mehrya Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Linguis-
tics, 23(2):269–311.
</reference>
<page confidence="0.978047">
827
</page>
<reference confidence="0.998748156862745">
Christan Raymond, Fre de ric Be chet, Renato D. Mori,
and Ge raldine Damnati. 2006. On the use of finite
state transducers for semantic interpretation. Speech
Communication, 48(3-4):288–304.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
Christer Samuelsson and Wolfgang Reichl. 1999. A
class-based language model for large-vocabulary
speechrecognition extracted from part-of-speech
statistics. In Proc. of ICASSP, pages 537–540,
Phoenix, Arizona, USA, March.
George Saon, Geoffrey Zweig, Brain KingsBury, Lidia
Mangu, and Upendra Canudhari. 2003. An architec-
ture for rapid decoding of large vocabulary conversa-
tional speech. In Proc. of Eurospeech, pages 1977–
1980, Geneva, Switzerland, September.
Matthew A. Siegler, Uday Jain, Bhiksha Raj, and
Richard M. Stern. 1997. Automatic segmentation,
classification and clustering of broadcast news audio.
In Proc. of DARPA Speech Recognition Workshop,
pages 97–99, Chantilly, Virginia, February.
Daniel Willett Takaaki Hori and Yasuhiro Minami.
2003. Language model adaptation using WFST-based
speaking-style translation. In Proc. of ICASSP, pages
I.228–I.231, Hong Kong, April.
Koichi Tanigaki, Hirofumi Yamamoto, and Yoshinori
Sagisaka. 2000. A hierarchical language model incor-
porating class-dependent word models for oov words
recognition. In Proc. of 6th International Conference
on Spoken Language Processing, pages 123–126, Bei-
jing, China, October.
Hajime Tsukada and Masaaki Nagata. 2004. Efficient
decoding for statistical machine translation with a fully
expanded WFST model. In Proc. of EMNLP, pages
427–433, Barcelona, Spain, July.
Wen Wang and Mary P. Harper. 2002. The superarv lan-
guage model: investigating the effectiveness of tightly
integrating multiple knowledge sources. In Proc. of
EMNLP, pages 238–247, Philadelphia, USA, July.
Wen Wang and Dimitra Vergyri. 2006. The use of word
n-grams and parts of speech for hierarchical cluster
language modeling. In Proc. of ICASSP, pages 1057–
1060, Toulouse, France, May.
Wen Wang, Andreas Stolcke, and Mary P. Harper. 2004.
The use of a linguistically motivated language model
in conversational speech recognition. In Proc. of
ICASSP, pages 261–264, Montreal, Canada, May.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proc. of
COLING, pages 947–953, Saarbrücken, August.
</reference>
<page confidence="0.997615">
828
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.279860">
<title confidence="0.9977265">Integrating Multi-level Linguistic Knowledge with a Unified Framework for Mandarin Speech Recognition</title>
<author confidence="0.854289">Jiazhong Nie Wang</author>
<author confidence="0.854289">Dingsheng Luo</author>
<author confidence="0.854289">Xihong</author>
<affiliation confidence="0.698506333333333">Speech and Hearing Research Key Laboratory of Machine Perception (Ministry of School of Electronics Engineering and Computer</affiliation>
<address confidence="0.582148">Peking University, Beijing, 100871,</address>
<abstract confidence="0.999447">To improve the Mandarin large vocabulary continuous speech recognition (LVCSR), a unified framework based approach is introduced to exploit multi-level linguistic knowledge. In this framework, each knowledge source is represented by a Weighted Finite State Transducer (WFST), and then they are combined to obtain a so-called analyzer for integrating multi-level knowledge sources. Due to the uniform transducer representation, any knowledge source can be easily integrated into the analyzer, as long as it can be encoded into WFSTs. Moreover, as the knowledge in each level is modeled independently and the combination is processed in the model level, the information inherently in each knowledge source has a chance to be thoroughly exploited. By simulations, the effectiveness of the analyzer is investigated, and then a LVCSR system embedding the presented analyzer is evaluated. Experimental results reveal that this unified framework is an effective approach which significantly improves the performance of speech recognition with a 9.9% relative reduction of character error rate on the HUB-4 test set, a widely used Mandarin speech recognition task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Partial parsing via finite-state cascades.</title>
<date>1996</date>
<journal>Natural Language Engineering,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="5824" citStr="Abney, 1996" startWordPosition="880" endWordPosition="881">ed Finite State Transducer (WFST) turns to be an ideal choice for our purpose. WFSTs were formerly introduced to simplify the integration of models in speech recognition, including acoustic models, phonetic models and word n-gram (Mohri, 1997; Mohri et al., 2002). In recent years, the WFST has been successfully applied in several state-of-the-art speech recognition systems, such as systems developed by the AMI project (Hain et al., 2006), IBM (Saon et al., 2003) and AT&amp;T (Mohri et al., 1996), and in various fields of natural language processing, such as smoothed n-gram model, partial parsing (Abney, 1996), named entities recognition (Friburger and Maurel, 2004), semantic interpretation (Raymond et al., 2006) and machine translation (Tsukada and Nagata, 2004). In (Takaaki Hori and Minami, 2003), the WFST has been further used for language model adaptation, where language models of different vocabularies that represented different styles were integrated through the framework of speech translation. In WFST-based systems, all of the models are represented uniformly by WFSTs, and the general composition algorithm (Mohri et al., 2000) combines these representations flexibly and efficiently. Thereby,</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Steven Abney. 1996. Partial parsing via finite-state cascades. Natural Language Engineering, 2(4):337–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Issam Bazzi</author>
<author>James R Glass</author>
</authors>
<title>Modeling outof-vocabulary words for robust speech recognition.</title>
<date>2000</date>
<booktitle>In Proc. of 6th International Conference on Spoken Language Processing,</booktitle>
<pages>401--404</pages>
<location>Beijing, China,</location>
<contexts>
<context position="4599" citStr="Bazzi and Glass, 2000" startWordPosition="683" endWordPosition="686">for Chinese names. Usually, speech recognition systems can only recognize the words within a predefined dictionary. With the increase of unknown words, i.e., out-ofvocabulary (OOV) words, the performance will degrade dramatically. This is because not only those unknown words cannot be recognized correctly, but the words surrounding them will be affected. Thus, many efforts have been made to deal with the issue of OOV words (Martins et al., 2006; Galescu, 2003; Bazzi and Glass, 2001), and various model units smaller than words have been examined to recognize OOVs from speech, such as phonemes (Bazzi and Glass, 2000a), variable-length phoneme sequence (Bazzi and Glass, 2001), syllable (Bazzi and Glass, 2000b) and sub-word (Galescu, 2003). Since the proper name is a typical category of OOV words and usually takes a very large proportion among all kinds of OOV words, it has been specially addressed in (Hu et al., 2006; Tanigaki et al., 2000). All those attempts mentioned above succeed in utilizing linguistic knowledge in language modeling in some degree respectively. In this study, a unified framework based approach, which aims to exploit information from multi-level linguistic knowledge, is presented. Her</context>
<context position="10565" citStr="Bazzi and Glass, 2000" startWordPosition="1641" endWordPosition="1644">y R and S. The combination is performed strictly on R’s output and S’s input. It means for each path in T, mapping string r to string s, there must exist a path mapping r to some string t in R and a path mapping t to s in S. Decoding on the combined WFST enables to find the joint optimal results for multi-level weighted relations. 2.2 Model Unit Selection This study primarily takes the person and location names as well as the POS information into account. To deal with Chinese OOV words, different from the western language in which the phoneme, syllable or sub-word are used as the model units (Bazzi and Glass, 2000a; Bazzi and Glass, 2000b; Galescu, 2003), Chinese characters are taken as the basic units. In general, a person name of Han nationality consists of a surname and a given name usually with one or two characters. Surnames commonly come from a fixed set that has been historically used. According to a recent investigation on surnames involving 296 million people, 4100 surnames are found, and 129 most used surnames account for 87% (conducted by the Institute of Genetics and Developmental Biology, Chinese Academy of Sciences). In contrast, the characters used in given names can be selected freely, </context>
</contexts>
<marker>Bazzi, Glass, 2000</marker>
<rawString>Issam Bazzi and James R. Glass. 2000a. Modeling outof-vocabulary words for robust speech recognition. In Proc. of 6th International Conference on Spoken Language Processing, pages 401–404, Beijing, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Issam Bazzi</author>
<author>James Glass</author>
</authors>
<title>Heterogeneous lexical units for automatic speech recognition: preliminary investigations.</title>
<date>2000</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<pages>1257--1260</pages>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="4599" citStr="Bazzi and Glass, 2000" startWordPosition="683" endWordPosition="686">for Chinese names. Usually, speech recognition systems can only recognize the words within a predefined dictionary. With the increase of unknown words, i.e., out-ofvocabulary (OOV) words, the performance will degrade dramatically. This is because not only those unknown words cannot be recognized correctly, but the words surrounding them will be affected. Thus, many efforts have been made to deal with the issue of OOV words (Martins et al., 2006; Galescu, 2003; Bazzi and Glass, 2001), and various model units smaller than words have been examined to recognize OOVs from speech, such as phonemes (Bazzi and Glass, 2000a), variable-length phoneme sequence (Bazzi and Glass, 2001), syllable (Bazzi and Glass, 2000b) and sub-word (Galescu, 2003). Since the proper name is a typical category of OOV words and usually takes a very large proportion among all kinds of OOV words, it has been specially addressed in (Hu et al., 2006; Tanigaki et al., 2000). All those attempts mentioned above succeed in utilizing linguistic knowledge in language modeling in some degree respectively. In this study, a unified framework based approach, which aims to exploit information from multi-level linguistic knowledge, is presented. Her</context>
<context position="10565" citStr="Bazzi and Glass, 2000" startWordPosition="1641" endWordPosition="1644">y R and S. The combination is performed strictly on R’s output and S’s input. It means for each path in T, mapping string r to string s, there must exist a path mapping r to some string t in R and a path mapping t to s in S. Decoding on the combined WFST enables to find the joint optimal results for multi-level weighted relations. 2.2 Model Unit Selection This study primarily takes the person and location names as well as the POS information into account. To deal with Chinese OOV words, different from the western language in which the phoneme, syllable or sub-word are used as the model units (Bazzi and Glass, 2000a; Bazzi and Glass, 2000b; Galescu, 2003), Chinese characters are taken as the basic units. In general, a person name of Han nationality consists of a surname and a given name usually with one or two characters. Surnames commonly come from a fixed set that has been historically used. According to a recent investigation on surnames involving 296 million people, 4100 surnames are found, and 129 most used surnames account for 87% (conducted by the Institute of Genetics and Developmental Biology, Chinese Academy of Sciences). In contrast, the characters used in given names can be selected freely, </context>
</contexts>
<marker>Bazzi, Glass, 2000</marker>
<rawString>Issam Bazzi and James Glass. 2000b. Heterogeneous lexical units for automatic speech recognition: preliminary investigations. In Proc. of ICASSP, pages 1257– 1260, Istanbul, Turkey, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Issam Bazzi</author>
<author>James Glass</author>
</authors>
<title>Learning units for domain-independent out-of-vocabulary word modelling.</title>
<date>2001</date>
<booktitle>In Proc. of EUROSPEECH,</booktitle>
<pages>61--64</pages>
<location>Aalborg, Denmark,</location>
<contexts>
<context position="4465" citStr="Bazzi and Glass, 2001" startWordPosition="660" endWordPosition="663">model units, while other units smaller or larger than words are unfeasible for modeling simultaneously, such as the Chinese characters for Chinese names. Usually, speech recognition systems can only recognize the words within a predefined dictionary. With the increase of unknown words, i.e., out-ofvocabulary (OOV) words, the performance will degrade dramatically. This is because not only those unknown words cannot be recognized correctly, but the words surrounding them will be affected. Thus, many efforts have been made to deal with the issue of OOV words (Martins et al., 2006; Galescu, 2003; Bazzi and Glass, 2001), and various model units smaller than words have been examined to recognize OOVs from speech, such as phonemes (Bazzi and Glass, 2000a), variable-length phoneme sequence (Bazzi and Glass, 2001), syllable (Bazzi and Glass, 2000b) and sub-word (Galescu, 2003). Since the proper name is a typical category of OOV words and usually takes a very large proportion among all kinds of OOV words, it has been specially addressed in (Hu et al., 2006; Tanigaki et al., 2000). All those attempts mentioned above succeed in utilizing linguistic knowledge in language modeling in some degree respectively. In this</context>
</contexts>
<marker>Bazzi, Glass, 2001</marker>
<rawString>Issam Bazzi and James Glass. 2001. Learning units for domain-independent out-of-vocabulary word modelling. In Proc. of EUROSPEECH, pages 61–64, Aalborg, Denmark, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>116--123</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="2177" citStr="Charniak, 2001" startWordPosition="313" endWordPosition="314">oduction Language modeling is essential for large vocabulary continuous speech recognition (LVCSR), which aims to determine the prior probability of a supposed word string W, p(W). Although the word-based ngram language model remains the mainstream for Corresponding author: Xihong Wu most speech recognition systems, the utilization of linguistic knowledge is too limited in this model. Consequently, many researchers have focused on introducing more linguistic knowledge in language modeling, such as lexical knowledge , syntax and semantics of language (Wang and Vergyri, 2006; Wang et al., 2004; Charniak, 2001; Roark, 2001; Chelba, 2000; Heeman, 1998; Chelba et al., 1997). Recently, structured language models have been introduced to make use of syntactic hierarchical characteristics (Roark, 2001; Charniak, 2001; Chelba, 2000). Nevertheless, the computational complexity of decoding will be heavily increased, as they are parser-based models. In contrast, the classbased language model groups the words that have similar functions of syntax or semantics into meaningful classes. As a result, it handles the questions of data sparsity and generalization of unseen event. In practice, the part-of-speech (POS</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proc. of ACL, pages 116–123, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>David Engle</author>
<author>Frederick Jelinek</author>
<author>Victor Jimenez</author>
<author>Sanjeev Khudanpur</author>
<author>Lidia Mangu</author>
<author>Harry Printz</author>
</authors>
<title>Eric Ristad, Ronald Rosenfeld, Andreas Stolcke, and Dekai Wu.</title>
<date>1997</date>
<booktitle>In Proc. of EUROSPEECH,</booktitle>
<pages>2775--2778</pages>
<location>Rhodes, Greece.</location>
<contexts>
<context position="2240" citStr="Chelba et al., 1997" startWordPosition="321" endWordPosition="324">ary continuous speech recognition (LVCSR), which aims to determine the prior probability of a supposed word string W, p(W). Although the word-based ngram language model remains the mainstream for Corresponding author: Xihong Wu most speech recognition systems, the utilization of linguistic knowledge is too limited in this model. Consequently, many researchers have focused on introducing more linguistic knowledge in language modeling, such as lexical knowledge , syntax and semantics of language (Wang and Vergyri, 2006; Wang et al., 2004; Charniak, 2001; Roark, 2001; Chelba, 2000; Heeman, 1998; Chelba et al., 1997). Recently, structured language models have been introduced to make use of syntactic hierarchical characteristics (Roark, 2001; Charniak, 2001; Chelba, 2000). Nevertheless, the computational complexity of decoding will be heavily increased, as they are parser-based models. In contrast, the classbased language model groups the words that have similar functions of syntax or semantics into meaningful classes. As a result, it handles the questions of data sparsity and generalization of unseen event. In practice, the part-of-speech (POS) information, capturing the syntactic role of words, has been </context>
</contexts>
<marker>Chelba, Engle, Jelinek, Jimenez, Khudanpur, Mangu, Printz, 1997</marker>
<rawString>Ciprian Chelba, David Engle, Frederick Jelinek, Victor Jimenez, Sanjeev Khudanpur, Lidia Mangu, Harry Printz, Eric Ristad, Ronald Rosenfeld, Andreas Stolcke, and Dekai Wu. 1997. Structure and performance of a dependency language model. In Proc. of EUROSPEECH, pages 2775–2778, Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
</authors>
<title>Exploiting Syntactic Structure for Natural Language Modeling.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="2204" citStr="Chelba, 2000" startWordPosition="317" endWordPosition="318"> essential for large vocabulary continuous speech recognition (LVCSR), which aims to determine the prior probability of a supposed word string W, p(W). Although the word-based ngram language model remains the mainstream for Corresponding author: Xihong Wu most speech recognition systems, the utilization of linguistic knowledge is too limited in this model. Consequently, many researchers have focused on introducing more linguistic knowledge in language modeling, such as lexical knowledge , syntax and semantics of language (Wang and Vergyri, 2006; Wang et al., 2004; Charniak, 2001; Roark, 2001; Chelba, 2000; Heeman, 1998; Chelba et al., 1997). Recently, structured language models have been introduced to make use of syntactic hierarchical characteristics (Roark, 2001; Charniak, 2001; Chelba, 2000). Nevertheless, the computational complexity of decoding will be heavily increased, as they are parser-based models. In contrast, the classbased language model groups the words that have similar functions of syntax or semantics into meaningful classes. As a result, it handles the questions of data sparsity and generalization of unseen event. In practice, the part-of-speech (POS) information, capturing th</context>
</contexts>
<marker>Chelba, 2000</marker>
<rawString>Ciprian Chelba. 2000. Exploiting Syntactic Structure for Natural Language Modeling. Ph.D. thesis, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabel Trancoso Diamantino Caseiro</author>
</authors>
<title>Using dynamic WFST composition for recognizing broadcast news.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP,</booktitle>
<pages>1301--1304</pages>
<location>Denver, Colorado, USA,</location>
<contexts>
<context position="6923" citStr="Caseiro, 2002" startWordPosition="1042" endWordPosition="1044">eneral composition algorithm (Mohri et al., 2000) combines these representations flexibly and efficiently. Thereby, rather than integrating the models step by step in decoding stage, a complete search network is constructed in advance. The combined WFST will be more efficient by optimizing with determinization, minimization and pushing algorithms of WFSTs (Mohri, 1997). Besides, the researches on optimizing the search space and improving WFST-based speech recognition has been carried out, especially on how to perform on-the-fly WFSTs composition more efficiently (Hori et al., 2007; Diamantino Caseiro, 2002). In this study, we extend the linguistic knowledge used in speech recognition. As WFSTs provide a common and natural representation for lexical constraints, n-gram language model, Hidden Markov Model models and context-dependency, multi-level knowledge sources can be encoded into WFSTs under the uniform transducer representation. Then this group of WFSTs is flexibly combined together to obtain an analyzer representing knowledge of person and location names as well as POS information. Afterwards, the presented analyzer is incorporated into LVCSR to evaluate the linguistic correctness of recogn</context>
</contexts>
<marker>Caseiro, 2002</marker>
<rawString>Isabel Trancoso Diamantino Caseiro. 2002. Using dynamic WFST composition for recognizing broadcast news. In Proc. of ICSLP, pages 1301–1304, Denver, Colorado, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>hub4 annotation specification for evaluation of speech recognition on broadcast news.</title>
<date>1996</date>
<booktitle>In ftp://jaguar.ncsl.nist.gov/csr96/h4/h4annot.ps.</booktitle>
<publisher>The</publisher>
<contexts>
<context position="22151" citStr="Doddington, 1996" startWordPosition="3542" endWordPosition="3543">, to better fit this task, the acoustic models were adapted by the approach of maximum a posterior (MAP) adaptation. The adaption data was drawn from the HUB4 training set, excluding the HUB-4 developPipeline Analyzer Integrated Analyzer 91.1 90.9 Word Segmentation Recognition Recognition 91.8 89.9 88.5 83.3 95.9 96 92 88 84 80 825 Corpus Speakers Amount of Speech (hours) 863-I (male) 83 56.67 863-II(male) 120 78.08 Intel (male) 556 227.30 total 759 362.05 Table 3: The information of the speech training data ing set, where only the cleaned male speech data (data under condition f0 defined as (Doddington, 1996)) was used. The partition for the clean data was done with the acoustic segmentation software CMUseg 0.52 (Siegler et al., 1997), and finally 8.6 hours adaptation data was obtained. The language model was a word-based trigram built on 60,000 words entries and trained with a corpus about 1.5 billion characters. The training set consisted of broadcast news data from the Xinhua News Agency released by LDC (Xinhua part of Chinese Gigaword), seven years data of People’s Daily of China from 1995 to 2002 released by People’s Daily Online3, and some other data from news websites, such as yahoo, sina a</context>
</contexts>
<marker>Doddington, 1996</marker>
<rawString>George Doddington. 1996. The 1996 hub4 annotation specification for evaluation of speech recognition on broadcast news. In ftp://jaguar.ncsl.nist.gov/csr96/h4/h4annot.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Friburger</author>
<author>D Maurel</author>
</authors>
<title>Finite-state transducer cascades to extract named entities in texts.</title>
<date>2004</date>
<journal>Theoretical Computer Science,</journal>
<volume>313</volume>
<issue>1</issue>
<contexts>
<context position="5881" citStr="Friburger and Maurel, 2004" startWordPosition="885" endWordPosition="888"> be an ideal choice for our purpose. WFSTs were formerly introduced to simplify the integration of models in speech recognition, including acoustic models, phonetic models and word n-gram (Mohri, 1997; Mohri et al., 2002). In recent years, the WFST has been successfully applied in several state-of-the-art speech recognition systems, such as systems developed by the AMI project (Hain et al., 2006), IBM (Saon et al., 2003) and AT&amp;T (Mohri et al., 1996), and in various fields of natural language processing, such as smoothed n-gram model, partial parsing (Abney, 1996), named entities recognition (Friburger and Maurel, 2004), semantic interpretation (Raymond et al., 2006) and machine translation (Tsukada and Nagata, 2004). In (Takaaki Hori and Minami, 2003), the WFST has been further used for language model adaptation, where language models of different vocabularies that represented different styles were integrated through the framework of speech translation. In WFST-based systems, all of the models are represented uniformly by WFSTs, and the general composition algorithm (Mohri et al., 2000) combines these representations flexibly and efficiently. Thereby, rather than integrating the models step by step in decod</context>
</contexts>
<marker>Friburger, Maurel, 2004</marker>
<rawString>N. Friburger and D. Maurel. 2004. Finite-state transducer cascades to extract named entities in texts. Theoretical Computer Science, 313(1):93–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucian Galescu</author>
</authors>
<title>Recognition of out-of-vocabulary words with sub-lexical language models.</title>
<date>2003</date>
<booktitle>In Proc. of EUROSPEECH,</booktitle>
<pages>249--252</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="4441" citStr="Galescu, 2003" startWordPosition="658" endWordPosition="659">e taken as the model units, while other units smaller or larger than words are unfeasible for modeling simultaneously, such as the Chinese characters for Chinese names. Usually, speech recognition systems can only recognize the words within a predefined dictionary. With the increase of unknown words, i.e., out-ofvocabulary (OOV) words, the performance will degrade dramatically. This is because not only those unknown words cannot be recognized correctly, but the words surrounding them will be affected. Thus, many efforts have been made to deal with the issue of OOV words (Martins et al., 2006; Galescu, 2003; Bazzi and Glass, 2001), and various model units smaller than words have been examined to recognize OOVs from speech, such as phonemes (Bazzi and Glass, 2000a), variable-length phoneme sequence (Bazzi and Glass, 2001), syllable (Bazzi and Glass, 2000b) and sub-word (Galescu, 2003). Since the proper name is a typical category of OOV words and usually takes a very large proportion among all kinds of OOV words, it has been specially addressed in (Hu et al., 2006; Tanigaki et al., 2000). All those attempts mentioned above succeed in utilizing linguistic knowledge in language modeling in some degr</context>
<context position="10606" citStr="Galescu, 2003" startWordPosition="1649" endWordPosition="1650"> on R’s output and S’s input. It means for each path in T, mapping string r to string s, there must exist a path mapping r to some string t in R and a path mapping t to s in S. Decoding on the combined WFST enables to find the joint optimal results for multi-level weighted relations. 2.2 Model Unit Selection This study primarily takes the person and location names as well as the POS information into account. To deal with Chinese OOV words, different from the western language in which the phoneme, syllable or sub-word are used as the model units (Bazzi and Glass, 2000a; Bazzi and Glass, 2000b; Galescu, 2003), Chinese characters are taken as the basic units. In general, a person name of Han nationality consists of a surname and a given name usually with one or two characters. Surnames commonly come from a fixed set that has been historically used. According to a recent investigation on surnames involving 296 million people, 4100 surnames are found, and 129 most used surnames account for 87% (conducted by the Institute of Genetics and Developmental Biology, Chinese Academy of Sciences). In contrast, the characters used in given names can be selected freely, and in many situations, some commonly use</context>
</contexts>
<marker>Galescu, 2003</marker>
<rawString>Lucian Galescu. 2003. Recognition of out-of-vocabulary words with sub-lexical language models. In Proc. of EUROSPEECH, pages 249–252, Geneva, Switzerland, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hain</author>
<author>Lukas Burget</author>
<author>John Dines</author>
<author>Giulia Garau</author>
<author>Martin Karafiat</author>
<author>Mike Lincoln</author>
<author>Jithendra Vepa</author>
<author>Vincent Wan</author>
</authors>
<title>The AMI meeting transcription system: Progress and performance.</title>
<date>2006</date>
<booktitle>In Proc. of Rich Transcription</booktitle>
<contexts>
<context position="5653" citStr="Hain et al., 2006" startWordPosition="850" endWordPosition="853">e degree respectively. In this study, a unified framework based approach, which aims to exploit information from multi-level linguistic knowledge, is presented. Here, the Weighted Finite State Transducer (WFST) turns to be an ideal choice for our purpose. WFSTs were formerly introduced to simplify the integration of models in speech recognition, including acoustic models, phonetic models and word n-gram (Mohri, 1997; Mohri et al., 2002). In recent years, the WFST has been successfully applied in several state-of-the-art speech recognition systems, such as systems developed by the AMI project (Hain et al., 2006), IBM (Saon et al., 2003) and AT&amp;T (Mohri et al., 1996), and in various fields of natural language processing, such as smoothed n-gram model, partial parsing (Abney, 1996), named entities recognition (Friburger and Maurel, 2004), semantic interpretation (Raymond et al., 2006) and machine translation (Tsukada and Nagata, 2004). In (Takaaki Hori and Minami, 2003), the WFST has been further used for language model adaptation, where language models of different vocabularies that represented different styles were integrated through the framework of speech translation. In WFST-based systems, all of </context>
</contexts>
<marker>Hain, Burget, Dines, Garau, Karafiat, Lincoln, Vepa, Wan, 2006</marker>
<rawString>Thomas Hain, Lukas Burget, John Dines, Giulia Garau, Martin Karafiat, Mike Lincoln, Jithendra Vepa, and Vincent Wan. 2006. The AMI meeting transcription system: Progress and performance. In Proc. of Rich Transcription 2006 Spring Meeting Recognition Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
</authors>
<title>Pos tagging versus classes in language modeling.</title>
<date>1998</date>
<booktitle>In Proc. of the 6th Workshop on very large corpora,</booktitle>
<pages>179--187</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="2218" citStr="Heeman, 1998" startWordPosition="319" endWordPosition="320"> large vocabulary continuous speech recognition (LVCSR), which aims to determine the prior probability of a supposed word string W, p(W). Although the word-based ngram language model remains the mainstream for Corresponding author: Xihong Wu most speech recognition systems, the utilization of linguistic knowledge is too limited in this model. Consequently, many researchers have focused on introducing more linguistic knowledge in language modeling, such as lexical knowledge , syntax and semantics of language (Wang and Vergyri, 2006; Wang et al., 2004; Charniak, 2001; Roark, 2001; Chelba, 2000; Heeman, 1998; Chelba et al., 1997). Recently, structured language models have been introduced to make use of syntactic hierarchical characteristics (Roark, 2001; Charniak, 2001; Chelba, 2000). Nevertheless, the computational complexity of decoding will be heavily increased, as they are parser-based models. In contrast, the classbased language model groups the words that have similar functions of syntax or semantics into meaningful classes. As a result, it handles the questions of data sparsity and generalization of unseen event. In practice, the part-of-speech (POS) information, capturing the syntactic ro</context>
</contexts>
<marker>Heeman, 1998</marker>
<rawString>Peter A. Heeman. 1998. Pos tagging versus classes in language modeling. In Proc. of the 6th Workshop on very large corpora, pages 179–187, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Hori</author>
<author>Chiori Hori</author>
<author>Yasuhiro Minami</author>
<author>Atsushi Nakamura</author>
</authors>
<title>Efficient WFST-based onepass decoding with on-the-fly hypothesis rescoring in extremely large vocabulary continuous speech recognition.</title>
<date>2007</date>
<journal>IEEE Transactions on</journal>
<pages>15--4</pages>
<contexts>
<context position="6896" citStr="Hori et al., 2007" startWordPosition="1037" endWordPosition="1040"> uniformly by WFSTs, and the general composition algorithm (Mohri et al., 2000) combines these representations flexibly and efficiently. Thereby, rather than integrating the models step by step in decoding stage, a complete search network is constructed in advance. The combined WFST will be more efficient by optimizing with determinization, minimization and pushing algorithms of WFSTs (Mohri, 1997). Besides, the researches on optimizing the search space and improving WFST-based speech recognition has been carried out, especially on how to perform on-the-fly WFSTs composition more efficiently (Hori et al., 2007; Diamantino Caseiro, 2002). In this study, we extend the linguistic knowledge used in speech recognition. As WFSTs provide a common and natural representation for lexical constraints, n-gram language model, Hidden Markov Model models and context-dependency, multi-level knowledge sources can be encoded into WFSTs under the uniform transducer representation. Then this group of WFSTs is flexibly combined together to obtain an analyzer representing knowledge of person and location names as well as POS information. Afterwards, the presented analyzer is incorporated into LVCSR to evaluate the lingu</context>
</contexts>
<marker>Hori, Hori, Minami, Nakamura, 2007</marker>
<rawString>Takaaki Hori, Chiori Hori, Yasuhiro Minami, and Atsushi Nakamura. 2007. Efficient WFST-based onepass decoding with on-the-fly hypothesis rescoring in extremely large vocabulary continuous speech recognition. IEEE Transactions on audio, speech, and language processing, 15(4):1352–1365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinhui Hu</author>
<author>Hirofumi Yamamoto</author>
<author>Genichiro Kikui</author>
<author>Yoshinori Sagisaka</author>
</authors>
<title>Language modeling of chinese personal names based on character units for continuous chinese speech recognition.</title>
<date>2006</date>
<booktitle>In Proc. of INTERSPEECH,</booktitle>
<pages>249--252</pages>
<location>Pittsburgh, USA,</location>
<contexts>
<context position="4905" citStr="Hu et al., 2006" startWordPosition="734" endWordPosition="737">the words surrounding them will be affected. Thus, many efforts have been made to deal with the issue of OOV words (Martins et al., 2006; Galescu, 2003; Bazzi and Glass, 2001), and various model units smaller than words have been examined to recognize OOVs from speech, such as phonemes (Bazzi and Glass, 2000a), variable-length phoneme sequence (Bazzi and Glass, 2001), syllable (Bazzi and Glass, 2000b) and sub-word (Galescu, 2003). Since the proper name is a typical category of OOV words and usually takes a very large proportion among all kinds of OOV words, it has been specially addressed in (Hu et al., 2006; Tanigaki et al., 2000). All those attempts mentioned above succeed in utilizing linguistic knowledge in language modeling in some degree respectively. In this study, a unified framework based approach, which aims to exploit information from multi-level linguistic knowledge, is presented. Here, the Weighted Finite State Transducer (WFST) turns to be an ideal choice for our purpose. WFSTs were formerly introduced to simplify the integration of models in speech recognition, including acoustic models, phonetic models and word n-gram (Mohri, 1997; Mohri et al., 2002). In recent years, the WFST ha</context>
</contexts>
<marker>Hu, Yamamoto, Kikui, Sagisaka, 2006</marker>
<rawString>Xinhui Hu, Hirofumi Yamamoto, Genichiro Kikui, and Yoshinori Sagisaka. 2006. Language modeling of chinese personal names based on character units for continuous chinese speech recognition. In Proc. of INTERSPEECH, pages 249–252, Pittsburgh, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Joint and conditional estimation of tagging and parsing models.</title>
<date>2001</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>322--329</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="3218" citStr="Johnson, 2001" startWordPosition="469" endWordPosition="470"> syntax or semantics into meaningful classes. As a result, it handles the questions of data sparsity and generalization of unseen event. In practice, the part-of-speech (POS) information, capturing the syntactic role of words, has been widely used in clustering words (Wang and Vergyri, 2006; Maltese et al., 2001; Samuelsson and Reichl, 1999). In Heeman’s POS language model (Heeman, 1998), the joint probability of word sequence and associated POS sequence was estimated directly, which has been demonstrated to be superior to the conditional probability previously used in the class-based models (Johnson, 2001). Moreover, a SuperARV language model was presented (Wang and Harper, 2002), in which lexical features and syntactic constraints were tightly integrated into a linguistic structure of SuperARV serving as a class in the model. Thus, these knowledge was integrated in the representation level, and then the joint probabilities 820 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 820–828, Honolulu, October 2008.c�2008 Association for Computational Linguistics of words and corresponding SuperARVs were estimated. However, in the class-based language models</context>
</contexts>
<marker>Johnson, 2001</marker>
<rawString>Mark Johnson. 2001. Joint and conditional estimation of tagging and parsing models. In Proc. of ACL, pages 322 – 329, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Maltese</author>
<author>P Bravetti</author>
<author>H Crépy</author>
<author>B J Grainger</author>
<author>M Herzog</author>
<author>F Palou</author>
</authors>
<title>Combining word- and class-based language models: A comparative study in several languages using automatic and manual wordclustering techniques.</title>
<date>2001</date>
<booktitle>In Proc. of EUROSPEECH,</booktitle>
<pages>21--24</pages>
<location>Aalborg, Denmark,</location>
<contexts>
<context position="2917" citStr="Maltese et al., 2001" startWordPosition="422" endWordPosition="425">ed to make use of syntactic hierarchical characteristics (Roark, 2001; Charniak, 2001; Chelba, 2000). Nevertheless, the computational complexity of decoding will be heavily increased, as they are parser-based models. In contrast, the classbased language model groups the words that have similar functions of syntax or semantics into meaningful classes. As a result, it handles the questions of data sparsity and generalization of unseen event. In practice, the part-of-speech (POS) information, capturing the syntactic role of words, has been widely used in clustering words (Wang and Vergyri, 2006; Maltese et al., 2001; Samuelsson and Reichl, 1999). In Heeman’s POS language model (Heeman, 1998), the joint probability of word sequence and associated POS sequence was estimated directly, which has been demonstrated to be superior to the conditional probability previously used in the class-based models (Johnson, 2001). Moreover, a SuperARV language model was presented (Wang and Harper, 2002), in which lexical features and syntactic constraints were tightly integrated into a linguistic structure of SuperARV serving as a class in the model. Thus, these knowledge was integrated in the representation level, and the</context>
</contexts>
<marker>Maltese, Bravetti, Crépy, Grainger, Herzog, Palou, 2001</marker>
<rawString>G. Maltese, P. Bravetti, H. Crépy, B. J. Grainger, M. Herzog, and F. Palou. 2001. Combining word- and class-based language models: A comparative study in several languages using automatic and manual wordclustering techniques. In Proc. of EUROSPEECH, pages 21–24, Aalborg, Denmark, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciro Martins</author>
<author>Antonio Texeira</author>
<author>Joao Neto</author>
</authors>
<title>Dynamic vocabulary adaptation for a daily and realtime broadcast news transcription system.</title>
<date>2006</date>
<booktitle>In Proc. of Spoken Language Technology Workshop,</booktitle>
<pages>146--149</pages>
<contexts>
<context position="4426" citStr="Martins et al., 2006" startWordPosition="654" endWordPosition="657">guage models, words are taken as the model units, while other units smaller or larger than words are unfeasible for modeling simultaneously, such as the Chinese characters for Chinese names. Usually, speech recognition systems can only recognize the words within a predefined dictionary. With the increase of unknown words, i.e., out-ofvocabulary (OOV) words, the performance will degrade dramatically. This is because not only those unknown words cannot be recognized correctly, but the words surrounding them will be affected. Thus, many efforts have been made to deal with the issue of OOV words (Martins et al., 2006; Galescu, 2003; Bazzi and Glass, 2001), and various model units smaller than words have been examined to recognize OOVs from speech, such as phonemes (Bazzi and Glass, 2000a), variable-length phoneme sequence (Bazzi and Glass, 2001), syllable (Bazzi and Glass, 2000b) and sub-word (Galescu, 2003). Since the proper name is a typical category of OOV words and usually takes a very large proportion among all kinds of OOV words, it has been specially addressed in (Hu et al., 2006; Tanigaki et al., 2000). All those attempts mentioned above succeed in utilizing linguistic knowledge in language modeli</context>
</contexts>
<marker>Martins, Texeira, Neto, 2006</marker>
<rawString>Ciro Martins, Antonio Texeira, and Joao Neto. 2006. Dynamic vocabulary adaptation for a daily and realtime broadcast news transcription system. In Proc. of Spoken Language Technology Workshop, pages 146– 149, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted automata in text and speech processing.</title>
<date>1996</date>
<booktitle>In ECAI-96 Workshop.</booktitle>
<contexts>
<context position="5708" citStr="Mohri et al., 1996" startWordPosition="861" endWordPosition="864">ork based approach, which aims to exploit information from multi-level linguistic knowledge, is presented. Here, the Weighted Finite State Transducer (WFST) turns to be an ideal choice for our purpose. WFSTs were formerly introduced to simplify the integration of models in speech recognition, including acoustic models, phonetic models and word n-gram (Mohri, 1997; Mohri et al., 2002). In recent years, the WFST has been successfully applied in several state-of-the-art speech recognition systems, such as systems developed by the AMI project (Hain et al., 2006), IBM (Saon et al., 2003) and AT&amp;T (Mohri et al., 1996), and in various fields of natural language processing, such as smoothed n-gram model, partial parsing (Abney, 1996), named entities recognition (Friburger and Maurel, 2004), semantic interpretation (Raymond et al., 2006) and machine translation (Tsukada and Nagata, 2004). In (Takaaki Hori and Minami, 2003), the WFST has been further used for language model adaptation, where language models of different vocabularies that represented different styles were integrated through the framework of speech translation. In WFST-based systems, all of the models are represented uniformly by WFSTs, and the </context>
<context position="9830" citStr="Mohri et al., 1996" startWordPosition="1505" endWordPosition="1508">ation of the finite state automata, in which, besides of an input label, an output label and a weight are also placed on each transition. With these labels, a WFST is capable of realizing a weighted relation between strings. In our system, log probabilities are adopted as transition weights and the relation between two strings is associated with a weight indicating the probability of the mapping between them. Given a group of WFSTs, each of which models a stage of a mapping cascade, the composition operation provides an efficient approach to combine them into a single one (Mohri et al., 2002; Mohri et al., 1996). In particular, for two WFSTs R and S, the composition T = RoS represents the composition of relations realized by R and S. The combination is performed strictly on R’s output and S’s input. It means for each path in T, mapping string r to string s, there must exist a path mapping r to some string t in R and a path mapping t to s in S. Decoding on the combined WFST enables to find the joint optimal results for multi-level weighted relations. 2.2 Model Unit Selection This study primarily takes the person and location names as well as the POS information into account. To deal with Chinese OOV w</context>
</contexts>
<marker>Mohri, Pereira, Riley, 1996</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 1996. Weighted automata in text and speech processing. In ECAI-96 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>The design principles of a weighted finitestate transducer library.</title>
<date>2000</date>
<journal>Theoretical Computer Science,</journal>
<volume>231</volume>
<issue>1</issue>
<contexts>
<context position="6358" citStr="Mohri et al., 2000" startWordPosition="957" endWordPosition="960">ral language processing, such as smoothed n-gram model, partial parsing (Abney, 1996), named entities recognition (Friburger and Maurel, 2004), semantic interpretation (Raymond et al., 2006) and machine translation (Tsukada and Nagata, 2004). In (Takaaki Hori and Minami, 2003), the WFST has been further used for language model adaptation, where language models of different vocabularies that represented different styles were integrated through the framework of speech translation. In WFST-based systems, all of the models are represented uniformly by WFSTs, and the general composition algorithm (Mohri et al., 2000) combines these representations flexibly and efficiently. Thereby, rather than integrating the models step by step in decoding stage, a complete search network is constructed in advance. The combined WFST will be more efficient by optimizing with determinization, minimization and pushing algorithms of WFSTs (Mohri, 1997). Besides, the researches on optimizing the search space and improving WFST-based speech recognition has been carried out, especially on how to perform on-the-fly WFSTs composition more efficiently (Hori et al., 2007; Diamantino Caseiro, 2002). In this study, we extend the ling</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2000</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 2000. The design principles of a weighted finitestate transducer library. Theoretical Computer Science, 231(1):17–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted finite-state transducers in speech recognition.</title>
<date>2002</date>
<journal>Computer Speech and Language,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="5475" citStr="Mohri et al., 2002" startWordPosition="822" endWordPosition="825"> has been specially addressed in (Hu et al., 2006; Tanigaki et al., 2000). All those attempts mentioned above succeed in utilizing linguistic knowledge in language modeling in some degree respectively. In this study, a unified framework based approach, which aims to exploit information from multi-level linguistic knowledge, is presented. Here, the Weighted Finite State Transducer (WFST) turns to be an ideal choice for our purpose. WFSTs were formerly introduced to simplify the integration of models in speech recognition, including acoustic models, phonetic models and word n-gram (Mohri, 1997; Mohri et al., 2002). In recent years, the WFST has been successfully applied in several state-of-the-art speech recognition systems, such as systems developed by the AMI project (Hain et al., 2006), IBM (Saon et al., 2003) and AT&amp;T (Mohri et al., 1996), and in various fields of natural language processing, such as smoothed n-gram model, partial parsing (Abney, 1996), named entities recognition (Friburger and Maurel, 2004), semantic interpretation (Raymond et al., 2006) and machine translation (Tsukada and Nagata, 2004). In (Takaaki Hori and Minami, 2003), the WFST has been further used for language model adaptat</context>
<context position="9809" citStr="Mohri et al., 2002" startWordPosition="1501" endWordPosition="1504">ST) is the generalization of the finite state automata, in which, besides of an input label, an output label and a weight are also placed on each transition. With these labels, a WFST is capable of realizing a weighted relation between strings. In our system, log probabilities are adopted as transition weights and the relation between two strings is associated with a weight indicating the probability of the mapping between them. Given a group of WFSTs, each of which models a stage of a mapping cascade, the composition operation provides an efficient approach to combine them into a single one (Mohri et al., 2002; Mohri et al., 1996). In particular, for two WFSTs R and S, the composition T = RoS represents the composition of relations realized by R and S. The combination is performed strictly on R’s output and S’s input. It means for each path in T, mapping string r to string s, there must exist a path mapping r to some string t in R and a path mapping t to s in S. Decoding on the combined WFST enables to find the joint optimal results for multi-level weighted relations. 2.2 Model Unit Selection This study primarily takes the person and location names as well as the POS information into account. To de</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2002</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. Computer Speech and Language, 16(1):69–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehrya Mohri</author>
</authors>
<title>Finite-state transducers in language and speech processing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="5454" citStr="Mohri, 1997" startWordPosition="820" endWordPosition="821">OOV words, it has been specially addressed in (Hu et al., 2006; Tanigaki et al., 2000). All those attempts mentioned above succeed in utilizing linguistic knowledge in language modeling in some degree respectively. In this study, a unified framework based approach, which aims to exploit information from multi-level linguistic knowledge, is presented. Here, the Weighted Finite State Transducer (WFST) turns to be an ideal choice for our purpose. WFSTs were formerly introduced to simplify the integration of models in speech recognition, including acoustic models, phonetic models and word n-gram (Mohri, 1997; Mohri et al., 2002). In recent years, the WFST has been successfully applied in several state-of-the-art speech recognition systems, such as systems developed by the AMI project (Hain et al., 2006), IBM (Saon et al., 2003) and AT&amp;T (Mohri et al., 1996), and in various fields of natural language processing, such as smoothed n-gram model, partial parsing (Abney, 1996), named entities recognition (Friburger and Maurel, 2004), semantic interpretation (Raymond et al., 2006) and machine translation (Tsukada and Nagata, 2004). In (Takaaki Hori and Minami, 2003), the WFST has been further used for l</context>
<context position="6680" citStr="Mohri, 1997" startWordPosition="1007" endWordPosition="1008">adaptation, where language models of different vocabularies that represented different styles were integrated through the framework of speech translation. In WFST-based systems, all of the models are represented uniformly by WFSTs, and the general composition algorithm (Mohri et al., 2000) combines these representations flexibly and efficiently. Thereby, rather than integrating the models step by step in decoding stage, a complete search network is constructed in advance. The combined WFST will be more efficient by optimizing with determinization, minimization and pushing algorithms of WFSTs (Mohri, 1997). Besides, the researches on optimizing the search space and improving WFST-based speech recognition has been carried out, especially on how to perform on-the-fly WFSTs composition more efficiently (Hori et al., 2007; Diamantino Caseiro, 2002). In this study, we extend the linguistic knowledge used in speech recognition. As WFSTs provide a common and natural representation for lexical constraints, n-gram language model, Hidden Markov Model models and context-dependency, multi-level knowledge sources can be encoded into WFSTs under the uniform transducer representation. Then this group of WFSTs</context>
</contexts>
<marker>Mohri, 1997</marker>
<rawString>Mehrya Mohri. 1997. Finite-state transducers in language and speech processing. Computational Linguistics, 23(2):269–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christan Raymond</author>
<author>Fre de ric Be chet</author>
<author>Renato D Mori</author>
<author>Ge raldine Damnati</author>
</authors>
<title>On the use of finite state transducers for semantic interpretation.</title>
<date>2006</date>
<journal>Speech Communication,</journal>
<pages>48--3</pages>
<contexts>
<context position="5929" citStr="Raymond et al., 2006" startWordPosition="891" endWordPosition="894">ly introduced to simplify the integration of models in speech recognition, including acoustic models, phonetic models and word n-gram (Mohri, 1997; Mohri et al., 2002). In recent years, the WFST has been successfully applied in several state-of-the-art speech recognition systems, such as systems developed by the AMI project (Hain et al., 2006), IBM (Saon et al., 2003) and AT&amp;T (Mohri et al., 1996), and in various fields of natural language processing, such as smoothed n-gram model, partial parsing (Abney, 1996), named entities recognition (Friburger and Maurel, 2004), semantic interpretation (Raymond et al., 2006) and machine translation (Tsukada and Nagata, 2004). In (Takaaki Hori and Minami, 2003), the WFST has been further used for language model adaptation, where language models of different vocabularies that represented different styles were integrated through the framework of speech translation. In WFST-based systems, all of the models are represented uniformly by WFSTs, and the general composition algorithm (Mohri et al., 2000) combines these representations flexibly and efficiently. Thereby, rather than integrating the models step by step in decoding stage, a complete search network is construc</context>
</contexts>
<marker>Raymond, chet, Mori, Damnati, 2006</marker>
<rawString>Christan Raymond, Fre de ric Be chet, Renato D. Mori, and Ge raldine Damnati. 2006. On the use of finite state transducers for semantic interpretation. Speech Communication, 48(3-4):288–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="2190" citStr="Roark, 2001" startWordPosition="315" endWordPosition="316">e modeling is essential for large vocabulary continuous speech recognition (LVCSR), which aims to determine the prior probability of a supposed word string W, p(W). Although the word-based ngram language model remains the mainstream for Corresponding author: Xihong Wu most speech recognition systems, the utilization of linguistic knowledge is too limited in this model. Consequently, many researchers have focused on introducing more linguistic knowledge in language modeling, such as lexical knowledge , syntax and semantics of language (Wang and Vergyri, 2006; Wang et al., 2004; Charniak, 2001; Roark, 2001; Chelba, 2000; Heeman, 1998; Chelba et al., 1997). Recently, structured language models have been introduced to make use of syntactic hierarchical characteristics (Roark, 2001; Charniak, 2001; Chelba, 2000). Nevertheless, the computational complexity of decoding will be heavily increased, as they are parser-based models. In contrast, the classbased language model groups the words that have similar functions of syntax or semantics into meaningful classes. As a result, it handles the questions of data sparsity and generalization of unseen event. In practice, the part-of-speech (POS) information</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christer Samuelsson</author>
<author>Wolfgang Reichl</author>
</authors>
<title>A class-based language model for large-vocabulary speechrecognition extracted from part-of-speech statistics.</title>
<date>1999</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<pages>537--540</pages>
<location>Phoenix, Arizona, USA,</location>
<contexts>
<context position="2947" citStr="Samuelsson and Reichl, 1999" startWordPosition="426" endWordPosition="429">actic hierarchical characteristics (Roark, 2001; Charniak, 2001; Chelba, 2000). Nevertheless, the computational complexity of decoding will be heavily increased, as they are parser-based models. In contrast, the classbased language model groups the words that have similar functions of syntax or semantics into meaningful classes. As a result, it handles the questions of data sparsity and generalization of unseen event. In practice, the part-of-speech (POS) information, capturing the syntactic role of words, has been widely used in clustering words (Wang and Vergyri, 2006; Maltese et al., 2001; Samuelsson and Reichl, 1999). In Heeman’s POS language model (Heeman, 1998), the joint probability of word sequence and associated POS sequence was estimated directly, which has been demonstrated to be superior to the conditional probability previously used in the class-based models (Johnson, 2001). Moreover, a SuperARV language model was presented (Wang and Harper, 2002), in which lexical features and syntactic constraints were tightly integrated into a linguistic structure of SuperARV serving as a class in the model. Thus, these knowledge was integrated in the representation level, and then the joint probabilities 820 </context>
</contexts>
<marker>Samuelsson, Reichl, 1999</marker>
<rawString>Christer Samuelsson and Wolfgang Reichl. 1999. A class-based language model for large-vocabulary speechrecognition extracted from part-of-speech statistics. In Proc. of ICASSP, pages 537–540, Phoenix, Arizona, USA, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Saon</author>
<author>Geoffrey Zweig</author>
<author>Brain KingsBury</author>
<author>Lidia Mangu</author>
<author>Upendra Canudhari</author>
</authors>
<title>An architecture for rapid decoding of large vocabulary conversational speech.</title>
<date>2003</date>
<booktitle>In Proc. of Eurospeech,</booktitle>
<pages>pages</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="5678" citStr="Saon et al., 2003" startWordPosition="855" endWordPosition="858"> this study, a unified framework based approach, which aims to exploit information from multi-level linguistic knowledge, is presented. Here, the Weighted Finite State Transducer (WFST) turns to be an ideal choice for our purpose. WFSTs were formerly introduced to simplify the integration of models in speech recognition, including acoustic models, phonetic models and word n-gram (Mohri, 1997; Mohri et al., 2002). In recent years, the WFST has been successfully applied in several state-of-the-art speech recognition systems, such as systems developed by the AMI project (Hain et al., 2006), IBM (Saon et al., 2003) and AT&amp;T (Mohri et al., 1996), and in various fields of natural language processing, such as smoothed n-gram model, partial parsing (Abney, 1996), named entities recognition (Friburger and Maurel, 2004), semantic interpretation (Raymond et al., 2006) and machine translation (Tsukada and Nagata, 2004). In (Takaaki Hori and Minami, 2003), the WFST has been further used for language model adaptation, where language models of different vocabularies that represented different styles were integrated through the framework of speech translation. In WFST-based systems, all of the models are represente</context>
</contexts>
<marker>Saon, Zweig, KingsBury, Mangu, Canudhari, 2003</marker>
<rawString>George Saon, Geoffrey Zweig, Brain KingsBury, Lidia Mangu, and Upendra Canudhari. 2003. An architecture for rapid decoding of large vocabulary conversational speech. In Proc. of Eurospeech, pages 1977– 1980, Geneva, Switzerland, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew A Siegler</author>
<author>Uday Jain</author>
<author>Bhiksha Raj</author>
<author>Richard M Stern</author>
</authors>
<title>Automatic segmentation, classification and clustering of broadcast news audio.</title>
<date>1997</date>
<booktitle>In Proc. of DARPA Speech Recognition Workshop,</booktitle>
<pages>97--99</pages>
<location>Chantilly, Virginia,</location>
<contexts>
<context position="22279" citStr="Siegler et al., 1997" startWordPosition="3561" endWordPosition="3564">ption data was drawn from the HUB4 training set, excluding the HUB-4 developPipeline Analyzer Integrated Analyzer 91.1 90.9 Word Segmentation Recognition Recognition 91.8 89.9 88.5 83.3 95.9 96 92 88 84 80 825 Corpus Speakers Amount of Speech (hours) 863-I (male) 83 56.67 863-II(male) 120 78.08 Intel (male) 556 227.30 total 759 362.05 Table 3: The information of the speech training data ing set, where only the cleaned male speech data (data under condition f0 defined as (Doddington, 1996)) was used. The partition for the clean data was done with the acoustic segmentation software CMUseg 0.52 (Siegler et al., 1997), and finally 8.6 hours adaptation data was obtained. The language model was a word-based trigram built on 60,000 words entries and trained with a corpus about 1.5 billion characters. The training set consisted of broadcast news data from the Xinhua News Agency released by LDC (Xinhua part of Chinese Gigaword), seven years data of People’s Daily of China from 1995 to 2002 released by People’s Daily Online3, and some other data from news websites, such as yahoo, sina and so on. In addition, the analyzer incorporated in speech recognition was trained with a larger corpus from People’s Daily of C</context>
</contexts>
<marker>Siegler, Jain, Raj, Stern, 1997</marker>
<rawString>Matthew A. Siegler, Uday Jain, Bhiksha Raj, and Richard M. Stern. 1997. Automatic segmentation, classification and clustering of broadcast news audio. In Proc. of DARPA Speech Recognition Workshop, pages 97–99, Chantilly, Virginia, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Willett Takaaki Hori</author>
<author>Yasuhiro Minami</author>
</authors>
<title>Language model adaptation using WFST-based speaking-style translation.</title>
<date>2003</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<pages>228--231</pages>
<location>Hong Kong,</location>
<contexts>
<context position="6016" citStr="Hori and Minami, 2003" startWordPosition="905" endWordPosition="908">coustic models, phonetic models and word n-gram (Mohri, 1997; Mohri et al., 2002). In recent years, the WFST has been successfully applied in several state-of-the-art speech recognition systems, such as systems developed by the AMI project (Hain et al., 2006), IBM (Saon et al., 2003) and AT&amp;T (Mohri et al., 1996), and in various fields of natural language processing, such as smoothed n-gram model, partial parsing (Abney, 1996), named entities recognition (Friburger and Maurel, 2004), semantic interpretation (Raymond et al., 2006) and machine translation (Tsukada and Nagata, 2004). In (Takaaki Hori and Minami, 2003), the WFST has been further used for language model adaptation, where language models of different vocabularies that represented different styles were integrated through the framework of speech translation. In WFST-based systems, all of the models are represented uniformly by WFSTs, and the general composition algorithm (Mohri et al., 2000) combines these representations flexibly and efficiently. Thereby, rather than integrating the models step by step in decoding stage, a complete search network is constructed in advance. The combined WFST will be more efficient by optimizing with determiniza</context>
</contexts>
<marker>Hori, Minami, 2003</marker>
<rawString>Daniel Willett Takaaki Hori and Yasuhiro Minami. 2003. Language model adaptation using WFST-based speaking-style translation. In Proc. of ICASSP, pages I.228–I.231, Hong Kong, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koichi Tanigaki</author>
<author>Hirofumi Yamamoto</author>
<author>Yoshinori Sagisaka</author>
</authors>
<title>A hierarchical language model incorporating class-dependent word models for oov words recognition.</title>
<date>2000</date>
<booktitle>In Proc. of 6th International Conference on Spoken Language Processing,</booktitle>
<pages>123--126</pages>
<location>Beijing, China,</location>
<contexts>
<context position="4929" citStr="Tanigaki et al., 2000" startWordPosition="738" endWordPosition="741">ding them will be affected. Thus, many efforts have been made to deal with the issue of OOV words (Martins et al., 2006; Galescu, 2003; Bazzi and Glass, 2001), and various model units smaller than words have been examined to recognize OOVs from speech, such as phonemes (Bazzi and Glass, 2000a), variable-length phoneme sequence (Bazzi and Glass, 2001), syllable (Bazzi and Glass, 2000b) and sub-word (Galescu, 2003). Since the proper name is a typical category of OOV words and usually takes a very large proportion among all kinds of OOV words, it has been specially addressed in (Hu et al., 2006; Tanigaki et al., 2000). All those attempts mentioned above succeed in utilizing linguistic knowledge in language modeling in some degree respectively. In this study, a unified framework based approach, which aims to exploit information from multi-level linguistic knowledge, is presented. Here, the Weighted Finite State Transducer (WFST) turns to be an ideal choice for our purpose. WFSTs were formerly introduced to simplify the integration of models in speech recognition, including acoustic models, phonetic models and word n-gram (Mohri, 1997; Mohri et al., 2002). In recent years, the WFST has been successfully appl</context>
</contexts>
<marker>Tanigaki, Yamamoto, Sagisaka, 2000</marker>
<rawString>Koichi Tanigaki, Hirofumi Yamamoto, and Yoshinori Sagisaka. 2000. A hierarchical language model incorporating class-dependent word models for oov words recognition. In Proc. of 6th International Conference on Spoken Language Processing, pages 123–126, Beijing, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hajime Tsukada</author>
<author>Masaaki Nagata</author>
</authors>
<title>Efficient decoding for statistical machine translation with a fully expanded WFST model.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>427--433</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="5980" citStr="Tsukada and Nagata, 2004" startWordPosition="898" endWordPosition="902">dels in speech recognition, including acoustic models, phonetic models and word n-gram (Mohri, 1997; Mohri et al., 2002). In recent years, the WFST has been successfully applied in several state-of-the-art speech recognition systems, such as systems developed by the AMI project (Hain et al., 2006), IBM (Saon et al., 2003) and AT&amp;T (Mohri et al., 1996), and in various fields of natural language processing, such as smoothed n-gram model, partial parsing (Abney, 1996), named entities recognition (Friburger and Maurel, 2004), semantic interpretation (Raymond et al., 2006) and machine translation (Tsukada and Nagata, 2004). In (Takaaki Hori and Minami, 2003), the WFST has been further used for language model adaptation, where language models of different vocabularies that represented different styles were integrated through the framework of speech translation. In WFST-based systems, all of the models are represented uniformly by WFSTs, and the general composition algorithm (Mohri et al., 2000) combines these representations flexibly and efficiently. Thereby, rather than integrating the models step by step in decoding stage, a complete search network is constructed in advance. The combined WFST will be more effi</context>
</contexts>
<marker>Tsukada, Nagata, 2004</marker>
<rawString>Hajime Tsukada and Masaaki Nagata. 2004. Efficient decoding for statistical machine translation with a fully expanded WFST model. In Proc. of EMNLP, pages 427–433, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Mary P Harper</author>
</authors>
<title>The superarv language model: investigating the effectiveness of tightly integrating multiple knowledge sources.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>238--247</pages>
<location>Philadelphia, USA,</location>
<contexts>
<context position="3293" citStr="Wang and Harper, 2002" startWordPosition="478" endWordPosition="481">es the questions of data sparsity and generalization of unseen event. In practice, the part-of-speech (POS) information, capturing the syntactic role of words, has been widely used in clustering words (Wang and Vergyri, 2006; Maltese et al., 2001; Samuelsson and Reichl, 1999). In Heeman’s POS language model (Heeman, 1998), the joint probability of word sequence and associated POS sequence was estimated directly, which has been demonstrated to be superior to the conditional probability previously used in the class-based models (Johnson, 2001). Moreover, a SuperARV language model was presented (Wang and Harper, 2002), in which lexical features and syntactic constraints were tightly integrated into a linguistic structure of SuperARV serving as a class in the model. Thus, these knowledge was integrated in the representation level, and then the joint probabilities 820 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 820–828, Honolulu, October 2008.c�2008 Association for Computational Linguistics of words and corresponding SuperARVs were estimated. However, in the class-based language models, words are taken as the model units, while other units smaller or larger t</context>
</contexts>
<marker>Wang, Harper, 2002</marker>
<rawString>Wen Wang and Mary P. Harper. 2002. The superarv language model: investigating the effectiveness of tightly integrating multiple knowledge sources. In Proc. of EMNLP, pages 238–247, Philadelphia, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Dimitra Vergyri</author>
</authors>
<title>The use of word n-grams and parts of speech for hierarchical cluster language modeling.</title>
<date>2006</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<pages>1057--1060</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="2142" citStr="Wang and Vergyri, 2006" startWordPosition="305" endWordPosition="308">ed Mandarin speech recognition task. 1 Introduction Language modeling is essential for large vocabulary continuous speech recognition (LVCSR), which aims to determine the prior probability of a supposed word string W, p(W). Although the word-based ngram language model remains the mainstream for Corresponding author: Xihong Wu most speech recognition systems, the utilization of linguistic knowledge is too limited in this model. Consequently, many researchers have focused on introducing more linguistic knowledge in language modeling, such as lexical knowledge , syntax and semantics of language (Wang and Vergyri, 2006; Wang et al., 2004; Charniak, 2001; Roark, 2001; Chelba, 2000; Heeman, 1998; Chelba et al., 1997). Recently, structured language models have been introduced to make use of syntactic hierarchical characteristics (Roark, 2001; Charniak, 2001; Chelba, 2000). Nevertheless, the computational complexity of decoding will be heavily increased, as they are parser-based models. In contrast, the classbased language model groups the words that have similar functions of syntax or semantics into meaningful classes. As a result, it handles the questions of data sparsity and generalization of unseen event. I</context>
</contexts>
<marker>Wang, Vergyri, 2006</marker>
<rawString>Wen Wang and Dimitra Vergyri. 2006. The use of word n-grams and parts of speech for hierarchical cluster language modeling. In Proc. of ICASSP, pages 1057– 1060, Toulouse, France, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Andreas Stolcke</author>
<author>Mary P Harper</author>
</authors>
<title>The use of a linguistically motivated language model in conversational speech recognition.</title>
<date>2004</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<pages>261--264</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="2161" citStr="Wang et al., 2004" startWordPosition="309" endWordPosition="312">nition task. 1 Introduction Language modeling is essential for large vocabulary continuous speech recognition (LVCSR), which aims to determine the prior probability of a supposed word string W, p(W). Although the word-based ngram language model remains the mainstream for Corresponding author: Xihong Wu most speech recognition systems, the utilization of linguistic knowledge is too limited in this model. Consequently, many researchers have focused on introducing more linguistic knowledge in language modeling, such as lexical knowledge , syntax and semantics of language (Wang and Vergyri, 2006; Wang et al., 2004; Charniak, 2001; Roark, 2001; Chelba, 2000; Heeman, 1998; Chelba et al., 1997). Recently, structured language models have been introduced to make use of syntactic hierarchical characteristics (Roark, 2001; Charniak, 2001; Chelba, 2000). Nevertheless, the computational complexity of decoding will be heavily increased, as they are parser-based models. In contrast, the classbased language model groups the words that have similar functions of syntax or semantics into meaningful classes. As a result, it handles the questions of data sparsity and generalization of unseen event. In practice, the par</context>
</contexts>
<marker>Wang, Stolcke, Harper, 2004</marker>
<rawString>Wen Wang, Andreas Stolcke, and Mary P. Harper. 2004. The use of a linguistically motivated language model in conversational speech recognition. In Proc. of ICASSP, pages 261–264, Montreal, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>947--953</pages>
<location>Saarbrücken,</location>
<contexts>
<context position="19287" citStr="Yeh, 2000" startWordPosition="3086" endWordPosition="3087">on Compose Decode Decode FSAinput ° FSTdict °W FSTne °W FSTn_gram WFSTpos Compose Decode output output Pipeline System Presented Analyzer Figure 4: The pipeline system vs The analyzer in which a person or location name was accounted as a word in vocabulary, only when the number of its appearances was no less than three. In Figure 5, the analyzer is compared with the pipeline system, where the analyzer outperforms the pipeline manner on all the subtasks in terms of Flscore metric. Furthermore to detect the differences, the statistical significance test using approximate randomization approach (Yeh, 2000) is done on the word segmentation results. Since there are more than 21,000 sentences in the test set, which is not appropriate for approximate randomization test, ten sets (500 sentences for each) are randomly selected from the test corpus. For each set, we run 1048576 shuffles twice and calculate the significance level pvalue according to the shuffled results. It has been shown that all p-value are less than 0.001 on the ten sets. Accordingly the improvement is statistically significant. Actually, this significant improvement is reasonable, since the joint processing avoids error propagation</context>
<context position="24085" citStr="Yeh, 2000" startWordPosition="3849" endWordPosition="3850">on. The recognition results are shown in Table 4. The baseline system has a character error rate (CER) of 14.85%. When the analyzer is incorporated, a 9.9% relative reduction is 2Acoustic segmentation software downloaded from http://www.nist.gov/speech/tools/CMUseg 05targz.htm. 3http://www.people.com.cn System Err. Sub. Del. Ins. Baseline 14.85 13.02 0.76 1.07 Analyzer 13.38 11.78 1.00 0.60 incorporation Table 4: The Speech recognition results achieved. Furthermore, we ran the statistical significance test to detect the performance improvement, in which the approximate randomization approach (Yeh, 2000) was modified to output the significance level, p-value, for the CER metric. The p-levels produced through two rounds of 1048576 shuffles are 0.0058 and 0.0057 respectively, both less than 0.01. Thus the performance improvement imposed by the utilization of the analyzer is statistically significant. 4 Conclusion Addressing the challenges of Mandarin large vocabulary continuous speech recognition task, within the unified framework of WFSTs, this study presents an analyzer integrating multi-level linguistic knowledge. Unlike other methods, model units, such as characters and words, can be chosen</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proc. of COLING, pages 947–953, Saarbrücken, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>