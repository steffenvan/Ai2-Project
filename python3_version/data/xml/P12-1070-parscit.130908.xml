<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.504086">
MIX Is Not a Tree-Adjoining Language
</title>
<author confidence="0.784675">
Makoto Kanazawa
</author>
<affiliation confidence="0.82104">
National Institute of Informatics
</affiliation>
<address confidence="0.8837625">
2–1–2 Hitotsubashi, Chiyoda-ku
Tokyo, 101–8430, Japan
</address>
<email confidence="0.998246">
kanazawa@nii.ac.jp
</email>
<note confidence="0.756225333333333">
Sylvain Salvati
INRIA Bordeaux Sud-Ouest, LaBRI
351, Cours de la Lib´eration
</note>
<address confidence="0.609074">
F-33405 Talence Cedex, France
</address>
<email confidence="0.99659">
sylvain.salvati@labri.fr
</email>
<sectionHeader confidence="0.996633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996644">
The language MIX consists of all strings over
the three-letter alphabet {a, b, c} that contain
an equal number of occurrences of each letter.
We prove Joshi’s (1985) conjecture that MIX
is not a tree-adjoining language.
</bodyText>
<sectionHeader confidence="0.991492" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.843402">
The language
</bodyText>
<equation confidence="0.936318">
MIX = { w ∈ {a, b, c}∗  ||w|a = |w|b = |w|c }
</equation>
<bodyText confidence="0.999985285714286">
has attracted considerable attention in computational
linguistics.1 This language was used by Bach (1981)
in an exercise to show that the permutation closure
of a context-free language is not necessarily context-
free.2 MIX may be considered a prototypical exam-
ple offree word order language, but, as remarked by
Bach (1981), it seems that no human language “has
such complete freedom for order”, because “typi-
cally, certain constituents act as ‘boundary domains’
for scrambling”. Joshi (1985) refers to MIX as rep-
resenting “an extreme case of the degree of free
word order permitted in a language”, which is “lin-
guistically not relevant”. Gazdar (1988) adopts a
similar position regarding the relation between MIX
</bodyText>
<footnote confidence="0.701088555555556">
1If w is a string and d is a symbol, we write |w|d to mean the
number of occurrences of d in w. We will use the notation |w |to
denote the length of w, i.e., the total number of occurrences of
symbols in w.
2According to Gazdar (1988), “MIX was originally de-
scribed by Emmon Bach and was so-dubbed by students in
the 1983 Hampshire College Summer Studies in Mathematics”.
According to Bach (1988), the name MIX was “the happy in-
vention of Bill Marsh”.
</footnote>
<bodyText confidence="0.999207727272727">
and natural languages, noting that “it seems rather
unlikely that any natural language will turn out to
have a MIX-like characteristic”.
It therefore seems natural to assume that lan-
guages such as MIX should be excluded from any
class of formal languages that purports to be a tight
formal characterization of the possible natural lan-
guages. It was in this spirit that Joshi et al. (1991)
suggested that MIX should not be in the class of so-
called mildly context-sensitive languages:
“[mildly context-sensitive grammars] cap-
ture only certain kinds of dependencies,
e.g., nested dependencies and certain lim-
ited kinds of cross-serial dependencies
(for example, in the subordinate clause
constructions in Dutch or some variations
of them, but perhaps not in the so-called
MIX (or Bach) language) ....”
Mild context-sensitivity is an informally defined no-
tion first introduced by Joshi (1985); it consists of
the three conditions of limited cross-serial depen-
dencies, constant growth, and polynomial parsing.
The first condition is only vaguely formulated, but
the other two conditions are clearly satisfied by tree-
adjoining grammars. The suggestion of Joshi et al.
(1991) was that MIX should be regarded as a vio-
lation of the condition of limited cross-serial depen-
dencies.
Joshi (1985) conjectured rather strongly that MIX
is not a tree-adjoining language: “TAGs cannot gen-
erate this language, although for TAGs the proof is
not in hand yet”. An even stronger conjecture was
made by Marsh (1985), namely, that MIX is not an
</bodyText>
<page confidence="0.964727">
666
</page>
<note confidence="0.985944">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 666–674,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999284837837838">
indexed language.3 (It is known that the indexed
languages properly include the tree-adjoining lan-
guages.) Joshi et al. (1991), however, expressed a
more pessimistic view about the conjecture:
“It is not known whether TAG ... can
generate MIX. This has turned out to be
a very difficult problem. In fact, it is
not even known whether an IG [(indexed
grammar)] can generate MIX.”
This open question has become all the more press-
ing after a recent result by Salvati (2011). This re-
sult says that MIX is in the class of multiple context-
free languages (Seki et al., 1991), or equivalently,
languages of linear context-free rewriting systems
(Vijay-Shanker et al., 1987; Weir, 1988), which has
been customarily regarded as a formal counterpart
of the informal notion of a mildly context-sensitive
language.4 It means that either we have to aban-
don the identification of multiple context-free lan-
guages with mildly context-sensitive languages, or
we should revise our conception of limited cross-
serial dependencies and stop regarding MIX-like
languages as violations of this condition. Surely, the
resolution of Joshi’s (1985) conjecture should cru-
cially affect the choice between these two alterna-
tives.
In this paper, we prove that MIX is not a tree-
adjoining language. Our proof is cast in terms of the
formalism of head grammar (Pollard, 1984; Roach,
1987), which is known to be equivalent to TAG
(Vijay-Shanker and Weir, 1994). The key to our
proof is the notion of an n-decomposition of a string
over {a, b, c}, which is similar to the notion of a
derivation in head grammars, but independent of any
particular grammar. The parameter n indicates how
unbalanced the occurrence counts of the three let-
ters can be at any point in a decomposition. We first
</bodyText>
<footnote confidence="0.996968090909091">
3The relation of MIX with indexed languages is also of in-
terest in combinatorial group theory. Gilman (2005) remarks
that “it does not ... seem to be known whether or not the
word problem of Z × Z is indexed”, alluding to the language
O2 = { w ∈ {a, ¯a, b, ¯b}∗  ||w|a = |w|¯a, |w|b = |w|¯b }. Since O2 and
MIX are rationally equivalent, O2 is indexed if and only if MIX
is indexed (Salvati, 2011).
4Joshi et al. (1991) presented linear context-free rewriting
systems as mildly context-sensitive grammars. Groenink (1997)
wrote “The class of mildly context-sensitive languages seems to
be most adequately approached by LCFRS.”
</footnote>
<bodyText confidence="0.999963454545454">
show that if MIX is generated by some head gram-
mar, then there is an n such that every string in MIX
has an n-decomposition. We then prove that if every
string in MIX has an n-decomposition, then every
string in MIX must have a 2-decomposition. Finally,
we exhibit a particular string in MIX that has no 2-
decomposition. The length of this string is 87, and
the fact that it has no 2-decomposition was first ver-
ified by a computer program accompanying this pa-
per. We include here a rigorous, mathematical proof
of this fact not relying on the computer verification.
</bodyText>
<sectionHeader confidence="0.895677" genericHeader="method">
2 Head Grammars
</sectionHeader>
<bodyText confidence="0.999936166666667">
A head grammar is a quadruple G = (N, E, P, S),
where N is a finite set of nonterminals, E is a fi-
nite set of terminal symbols (alphabet), S is a distin-
guished element of N, and P is a finite set of rules.
Each nonterminal is interpreted as a binary predicate
on strings in E∗. There are four types of rules:
</bodyText>
<equation confidence="0.99969825">
A(x1x2y1, y2) ← B(x1, x2),C(y1, y2)
A(x1, x2y1y2) ← B(x1, x2), C(y1, y2)
A(x1y1, y2x2) ← B(x1, x2),C(y1, y2)
A(w1, w2) ←
</equation>
<bodyText confidence="0.983075166666667">
Here, A, B, C ∈ N, x1, x2, y1, y2 are variables, and
w1, w2 ∈ E ∪ {ε}.5 Rules of the first three types are
binary rules and rules of the last type are terminat-
ing rules. This definition of a head grammar actu-
ally corresponds to a normal form for head gram-
mars that appears in section 3.3 of Vijay-Shanker
and Weir’s (1994) paper.6
The rules of head grammars are interpreted as im-
plications from right to left, where variables can be
instantiated to any terminal strings. Each binary
5We use ε to denote the empty string.
6This normal form is also mentioned in chapter 5, section 4
of Kracht’s (2003) book. The notation we use to express rules
of head grammars is borrowed from elementary formal sys-
tems (Smullyan, 1961; Arikawa et al., 1992), also known as
literal movement grammars (Groenink, 1997; Kracht, 2003),
which are logic programs over strings. In Vijay-Shanker and
Weir’s (1994) notation, the four rules are expressed as follows:
</bodyText>
<equation confidence="0.7564">
A → C2,2(B,C)
A → C1,2(B,C)
A → W(B, C)
A → C1,1(w1 ↑ w2)
</equation>
<page confidence="0.975979">
667
</page>
<bodyText confidence="0.996879454545455">
rule involves an operation that combines two pairs
of strings to form a new pair. The operation in-
volved in the third rule is known as wrapping; the
operations involved in the first two rules we call left
concatenation and right concatenation, respectively.
If G = (N, Σ, P, S) is a head grammar, A ∈ N, and
w1, w2 ∈ Σ∗, then we say that a fact A(w1, w2) is
derivable and write FG A(w1, w2), if A(w1, w2) can
be inferred using the rules in P. More formally, we
have I-G A(w1, w2) if one of the following conditions
holds:
</bodyText>
<listItem confidence="0.985661">
• A(w1, w2) ← is a terminating rule in P.
• I-G B(u1, u2), I-G C(v1, v2), and there is a bi-
nary rule A(α1, α2) ← B(x1, x2), C(y1, y2) in
P such that (w1, w2) is the result of substitut-
ing u1, u2, v1, v2 for x1, x2, y1, y2, respectively,
in (α1, α2).
</listItem>
<bodyText confidence="0.988027">
The language of G is
</bodyText>
<equation confidence="0.958834">
L(G) = { w1w2  |FG S(w1, w2) }.
</equation>
<bodyText confidence="0.539090333333333">
Example 1. Let G = (N, Σ, P, S), where N =
{S, A, A&apos;, C, D, E, F}, Σ = {a, ¯a, #}, and P consists of
the following rules:
</bodyText>
<equation confidence="0.9985095">
S(x1y1, y2x2) ← D(x1, x2),C(y1, y2)
C(s, #) ←
D(s, s) ←
D(x1y1, y2x2) ← F(x1, x2), D(y1, y2)
F(x1y1, y2x2) ← A(x1, x2), E(y1, y2)
A(a, a) ←
E(x1y1, y2x2) ← D(x1, x2), A&apos;(y1, y2)
A&apos;(¯a, ¯a) ←
</equation>
<bodyText confidence="0.999874555555555">
We have L(G) = { w#wR  |w ∈ D{a,¯a} }, where D{a,¯a}
is the Dyck language over {a, ¯a} and wR is the re-
versal of w. All binary rules of this grammar are
wrapping rules.
If FG A(w1, w2), a derivation tree for A(w1, w2) is
a finite binary tree whose nodes are labeled by facts
that are derived during the derivation of A(w1, w2).
A derivation tree for A(w1, w2) represents a “proof”
of I-G A(w1, w2), and is formally defined as follows:
</bodyText>
<listItem confidence="0.928551">
• If A(w1, w2) ← is a terminating rule, then a tree
with a single node labeled by A(w1, w2) is a
derivation tree for A(w1, w2).
</listItem>
<equation confidence="0.970623875">
S(aa¯a¯aa¯a, #¯aa¯a¯aaa)
D(aa¯a¯aa¯a, ¯aa¯a¯aaa)
F(aa¯a¯a, ¯a¯aaa)
A(a, a) E(a¯a¯a, ¯a¯aa)
A(a, a) E(¯a, ¯a)
D(s, s) A&apos;(¯a, ¯a)
A(a, a) E(¯a, ¯a)
D(s, s) A&apos;(¯a, ¯a)
</equation>
<figureCaption confidence="0.992037">
Figure 1: An example of a derivation tree of a head gram-
mar.
</figureCaption>
<listItem confidence="0.662625833333333">
• If FG A(w1, w2) is derived from FG B(u1, u2)
and I-G C(v1, v2) by some binary rule, then a
binary tree whose root is labeled by A(w1, w2)
and whose immediate left (right) subtree is a
derivation tree for B(u1, u2) (for C(v1, v2), re-
spectively) is a derivation tree for A(w1, w2).
</listItem>
<bodyText confidence="0.996154909090909">
If w ∈ L(G), a derivation tree for w is a derivation
tree for some S(w1, w2) such that w1w2 = w.
Example 1(continued). Figure 1 shows a derivation
tree for aa¯a¯aa¯a#¯aa¯a¯aaa.
The following lemma should be intuitively clear
from the definition of a derivation tree:
Lemma 1. Let G = (N, Σ, P, S) be a head grammar
and A be a nonterminal in N. Suppose that w ∈
L(G) has a derivation tree in which a fact A(v1, v2)
appears as a label of a node. Then there are strings
z0, z1, z2 with the following properties:
</bodyText>
<listItem confidence="0.536502">
(i) w = z0v1z1v2z2, and
(ii) FG A(u1, u2) implies z0u1z1u2z2 ∈ L(G).
</listItem>
<bodyText confidence="0.9997216">
Proof. We can prove by straightforward induction
on the height of derivation trees that whenever
A(v1, v2) appears on a node in a derivation tree for
B(w1, w2), then there exist z0, z1, z2, z3 that satisfy
one of the following conditions:
</bodyText>
<listItem confidence="0.93109925">
(a) w1 = z0v1z1v2z2, w2 = z3, and I-G A(u1, u2)
implies I-G B(z0u1z1u2z2,z3).
(b) w1 = z0, w2 = z1v1z2v2z3, and FG A(u1, u2)
implies I-G B(z0,z1u1z2u2z3).
</listItem>
<equation confidence="0.916653181818182">
C(s, #)
D(a¯a, ¯aa)
D(s, s)
F(a¯a, ¯aa)
A&apos;(¯a, ¯a)
D(a¯a, ¯aa)
D(s, s)
F(a¯a, ¯aa)
668
(c) w1 = z0v1z1, w2 = z2v2z3, and FG A(u1, u2) • each internal node whose left and right children
implies I-G B(z0u1z1, z2u2z3). are labeled by (u1, u2) and (v1, v2), respectively,
</equation>
<bodyText confidence="0.980335428571429">
is labeled by one of (u1u2v1,v2), (u1,u2v1v2),
We omit the details. ❑ (u1v1,v2u2).
We call a nonterminal A of a head grammar G use-
less if A does not appear in any derivation trees for
strings in L(G). Clearly, useless nonterminals can be
eliminated from any head grammar without affecting
the language of the grammar.
</bodyText>
<sectionHeader confidence="0.766376" genericHeader="method">
3 Decompositions of Strings in MIX
</sectionHeader>
<bodyText confidence="0.89196">
Henceforth, E = {a, b, c}. Let Z denote the set of in-
tegers. Define functions ψ1, ψ2: E∗ → Z, ψ: E∗ →
</bodyText>
<equation confidence="0.899490142857143">
Z × Z by
ψ1(w) = |w|a − |w|c,
ψ2(w) = |w|b − |w|c,
ψ(w) = (ψ1(w),ψ2(w)).
Clearly, we have ψ(a) = (1, 0), ψ(b) = (0, 1), ψ(c) =
(−1, −1), and
w ∈ MIX iff ψ(w) = (0, 0).
</equation>
<bodyText confidence="0.9714632">
Note that for all strings w1, w2 ∈ E∗, ψ(w1w2) =
ψ(w1)+ψ(w2). In other words, ψ is a homomorphism
from the free monoid E∗ to Z × Z with addition as
the monoid operation and (0, 0) as identity.
Lemma 2. Suppose that G = (N, E, P, S) is a head
grammar without useless nonterminals such that
L(G) ⊆ MIX. There exists a function TG : N → Z ×
Z such that FG A(u1, u2) implies ψ(u1u2) = TG(A).
Proof. Since G has no useless nonterminals, for
each nonterminal A of G, there is a derivation tree
for some string in L(G) in which A appears in a node
label. By Lemma 1, there are strings z0, z1, z2 such
that FG A(u1, u2) implies z0u1z1u2z2 ∈ L(G). Since
L(G) ⊆ MIX, we have ψ(z0u1z1u2z2) = (0, 0), and
hence
</bodyText>
<equation confidence="0.980732">
ψ(u1u2) = −ψ(z0z1z2). ❑
</equation>
<bodyText confidence="0.9988025">
A decomposition of w ∈ E∗ is a finite binary tree
satisfying the following conditions:
</bodyText>
<listItem confidence="0.99872825">
• the root is labeled by some (w1, w2) such that
w = w1w2,
• each leaf node is labeled by some (s1, s2) such
that s1s2 ∈ {b, c}∗ ∪ {a, c}∗ ∪ {a, b}∗.
</listItem>
<bodyText confidence="0.91968925">
Thus, the label of an internal node in a decomposi-
tion is obtained from the labels of its children by left
concatenation, right concatenation, or wrapping. It
is easy to see that if G is a head grammar over the al-
phabet E, any derivation for w ∈ L(G) induces a de-
composition of w. (Just strip off nonterminals.) Note
that unlike with derivation trees, we have placed no
bound on the length of a string that may appear on
a leaf node of a decomposition. This will be conve-
nient in some of the proofs below.
When p and q are integers, we write [p, q] for the
set { r ∈ Z  |p ≤ r ≤ q }. We call a decomposition of
w an n-decomposition if each of its nodes is labeled
by some (v1, v2) such that ψ(v1v2) ∈ [−n, n]×[−n, n].
Lemma 3. If MIX = L(G) for some head grammar
G = (E, N, P, S), then there exists an n such that each
w ∈ MIX has an n-decomposition.
Proof. We may suppose without loss of generality
that G has no useless nonterminal. Since MIX =
L(G), there is a function TG satisfying the condition
of Lemma 2. Since the set N of nonterminals of G
is finite, there is an n such that TG(A) ∈ [−n, n] ×
[−n, n] for all A ∈ N. Then it is clear that a derivation
tree for w ∈ L(G) induces an n-decomposition of
</bodyText>
<equation confidence="0.602683">
w. ❑
</equation>
<bodyText confidence="0.944040111111111">
If w = d1 ... dm ∈ Em, then for 0 ≤ i ≤ j ≤ m,
we write w[i, j] to refer to the substring di+1 ... dj
of w. (As a special case, we have w[i, i] = ε.) The
following is a key lemma in our proof:
Lemma 4. If each w ∈ MIX has an n-
decomposition, then each w ∈ MIX has a 2-
decomposition.
Proof. Assume that each w ∈ MIX has an n-
decomposition. Define a homomorphism γn : E∗ →
</bodyText>
<equation confidence="0.9537925">
E∗ by
γn(a) = an,
γn(b) = bn,
γn(c) = cn.
</equation>
<page confidence="0.890333">
669
</page>
<bodyText confidence="0.986809666666667">
Clearly, γn is an injection, and we have ψ(γn(v)) =
n · ψ(v) for all v ∈ Σ∗.
Let w ∈ MIX with |w |= m. Then w� = γn(w) ∈
MIX and |w&apos; |= mn. By assumption, w has an n-
decomposition D. We assign a 4-tuple (i, j, k, l) of
natural numbers to each node of D in such a way
that (w&apos;[i, j], w [k, l]) equals the label of the node.
This is done recursively in an obvious way, start-
ing from the root. If the root is labeled by (w1, w2),
then it is assigned (0, |w1|, |w1|, |w1w2|). If a node is
assigned a tuple (i, j, k, l) and has two children la-
beled by (u1, u2) and (v1, v2), respectively, then the
4-tuples assigned to the children are determined ac-
cording to how (u1, u2) and (v1, v2) are combined at
the parent node:
</bodyText>
<equation confidence="0.977166">
i j k l
u1 u2 v1 v2
i + |u1 |i + |u1u2|
i j k l
u1 u2 v1 v2
k + |u2 |k + |u2v1|
i j k l
i + |u1 |k + |v2|
Now define a function f : [0, mn] → { kn  |0 ≤
k ≤ m} by
{ i if n divides i,
n · [i/nJ if n does not divide i and
w&apos;[i − 1, i] ∈ {a, b},
n · ri/nJ if n does not divide i and
w&apos;[i − 1, i] = c.
</equation>
<bodyText confidence="0.985917666666667">
Clearly, f is weakly increasing in the sense that i ≤ j
implies f(i) ≤ f(j). Let D&apos; be the result of replacing
the label of each node in D by
(w�[f(i), f(j)], w�[f(k), f (l)]),
where (i, j, k, l) is the 4-tuple of natural numbers as-
signed to that node by the above procedure. It is easy
to see that D&apos; is another decomposition of w . Note
that since each of f(i), f(j), f(k), f(l) is an integral
multiple of n, we always have
</bodyText>
<equation confidence="0.942618166666667">
(w�[f(i), f (j)], w�[f(k), f(l)]) = (γn(u), γn(v))
for some substrings u, v of w. This implies that for
h = 1, 2,
ψh(w�[f(i), f(j)]w�[f(k), f(l)])
is an integral multiple of n.
Claim. D&apos; is a 2n-decomposition.
</equation>
<bodyText confidence="0.998003666666667">
We have to show that every node label (v1, v2) in D&apos;
satisfies ψ(v1v2) ∈ [−2n, 2n] × [−2n, 2n]. For h =
1, 2, define ϕh : [0, mn] × [0, mn] → Z as follows:
</bodyText>
<equation confidence="0.993065">
{ ψh(w&apos;[i, j]) if i ≤ j,
−ψh(w&apos;[j,i]) otherwise.
Then it is easy to see that for all i, j, i&apos;, j� ∈ [0, mn],
ϕh(i�, j�) = ϕh(i�, i) + ϕh(i, j) + ϕh( j, j ).
</equation>
<bodyText confidence="0.89782">
Inspecting the definition of the function f, we can
check that
</bodyText>
<equation confidence="0.9682986875">
ϕh(f(i), i) ∈ [0, n − 1]
always holds. Suppose that (i, j, k, l) is assigned
to a node in D. By assumption, we have
ψh(w&apos;[i, j]w [k, l]) ∈ [−n, n], and
ψh(w�[f(i), f(j)]w�[f(k), f(l)])
= ψh(w�[f(i), f(j)]) + ψh(w�[f(k), f(l)])
= ϕh(f(i), f(j)) + ϕh(f(k), f(l))
= ϕh(f(i), i) + ϕh(i, j) + ϕh(j, f(j))
+ ϕh(f(k), k) + ϕh(k, l) + ϕh(l, f(l))
= ϕh(f(i), i) + ψh(w [i, j]) + ϕh(j, f(j))
+ ϕh(f(k), k) + ψh(w [k, l]) + ϕh(l, f(l))
= ψh(w [i, j]w [k, l]) + ϕh(f(i), i) + ϕh(f(k), k)
+ ϕh(j, f(j)) + ϕh(l, f(l))
∈ { p + q1 + q2 + r1 + r2  |p ∈ [−n, n],
q1, q2 ∈ [0, n − 1], r1, r2 ∈ [−n + 1, 0] }
= [−3n + 2,3n − 2].
</equation>
<bodyText confidence="0.94976275">
Since ψh(w�[f(i), f(j)]w&apos;[f(k), f(l)]) must be an in-
tegral multiple of n, it follows that
ψh(w�[f(i), f(j)]w�[f(k), f(l)]) ∈ {−2n, −n, 0, n, 2n}.
This establishes the claim.
</bodyText>
<equation confidence="0.982782666666667">
u2
v2
u1
v1
f(i) =
ϕh(i, j) =
</equation>
<page confidence="0.962132">
670
</page>
<bodyText confidence="0.9913965">
We have shown that each node of D&apos; is labeled by
a pair of strings of the form (yn(u), yn(v)) such that
</bodyText>
<equation confidence="0.9921542">
VI(yn(u)yn(v)) ∈
{−2n, −n, 0, n, 2n} × {−2n, −n, 0, n, 2n}.
Now it is easy to see that inverting the homomor-
phism yn at each node of D&apos;
(yn(u), yn(v)) H (u, v)
</equation>
<bodyText confidence="0.556787">
gives a 2-decomposition of w. ❑
</bodyText>
<sectionHeader confidence="0.9434795" genericHeader="method">
4 A String in MIX That Has No
2-Decomposition
</sectionHeader>
<bodyText confidence="0.99956925">
By Lemmas 3 and 4, in order to prove that there is no
head grammar for MIX, it suffices to exhibit a string
in MIX that has no 2-decomposition. The following
is such a string:
</bodyText>
<equation confidence="0.824067">
z = a5b14a19c29b15a5.
</equation>
<bodyText confidence="0.97201804">
In this section, we prove that the string z has no 2-
decomposition.7
It helps to visualize strings in MIX as closed
curves in a plane. If w is a string in MIX, by plotting
the coordinates of VI(v) for each prefix v of w, we can
represent w by a closed curve C together with a map
t: [0, |w|] → C. The representation of the string z is
given in Figure 2.
Let us call a string w ∈ {a, b, c}∗ such that VI(w) ∈
[−2,2] × [−2,2] long if w contains all three letters,
and short otherwise. (If VI(w) � [−2,2] × [−2,2],
then w is neither short nor long.) It is easy to see
that a short string w always satisfies
|w|a ≤ 4, |w|b ≤ 4, |w|c ≤ 2.
The maximal length of a short string is 6. (For ex-
ample, a4c2 and b4c2 are short strings of length 6.)
We also call a pair of strings (v1, v2) long (or short)
if v1v2 is long (or short, respectively).
According to the definition of an n-
decomposition, a leaf node in a 2-decomposition
7This fact was first verified by the computer program ac-
companying this paper. The program, written in C, imple-
ments a generic, memoized top-down recognizer for the lan-
guage { w ∈ MIX  |w has a 2-decomposition }, and does not rely
on any special properties of the string z.
</bodyText>
<figureCaption confidence="0.988492">
Figure 2: Graphical representation of the string z =
</figureCaption>
<bodyText confidence="0.98567821875">
a5b14a19c29b15a5. Note that every point (i, j) on the di-
agonal segment has i &gt; 7 or j &lt; −2.
must be labeled by a short pair of strings. We call
a 2-decomposition normal if the label of every
internal node is long. Clearly, any 2-decomposition
can be turned into a normal 2-decomposition by
deleting all nodes that are descendants of nodes
with short labels.
One important property of the string z is the fol-
lowing:
Lemma 5. If z = x1vx2 and VI(v) ∈ [−2,2]×[−2,2],
then either v or x1x2 is short.
Proof. This is easy to see from the graphical rep-
resentation in Figure 2. If a substring v of z has
VI(v) ∈ [−2,2] × [−2,2], then the subcurve corre-
sponding to v must have initial and final coordi-
nates whose difference lies in [−2,2] × [−2,2]. If
v contains all three letters, then it must contain as
a substring at least one of ba19c, ac29b, and cb15a.
The only way to satisfy both these conditions is to
have the subcurve corresponding to v start and end
very close to the origin, so that x1x2 is short. (Note
that the distance between the coordinate (5, 0) corre-
sponding to position 5 of z and the diagonal segment
corresponding to the substring c29 is large enough
that it is impossible for v to start at position 5 and
end in the middle of c29 without violating the condi-
tion VI(v) ∈ [−2,2] × [−2,2].) ❑
Lemma 5 leads to the following observation. Let
us call a decomposition of a string concatenation-
free if each of its non-leaf labels is the wrapping of
the labels of the children.
</bodyText>
<figure confidence="0.993366">
19 a19 38
b14
82
b15
67
a5
87
a5
0 5
c29
</figure>
<page confidence="0.988685">
671
</page>
<bodyText confidence="0.970612423076923">
Lemma 6. If z has a 2-decomposition, then z has a
normal, concatenation-free 2-decomposition.
Proof. Let D be a 2-decomposition of z. Without
loss of generality, we may assume that D is nor-
mal. Suppose that D contains a node p whose la-
bel is the left or right concatenation of the labels
of its children, (u1, u2) and (v1, v2). We only con-
sider the case of left concatenation since the case
of right concatenation is entirely analogous; so we
suppose that the node p is labeled by (u1u2v1,v2).
It follows that z = x1u1u2x2 for some x1, x2, and
by Lemma 5, either u1u2 or x1x2 is short. If u1u2
is short, then the left child of p is a leaf because
D is normal. We can replace its label by (u1u2, s);
the label (u1u2v1, v2) of p will now be the wrapping
(as well as left concatenation) of the two child la-
bels, (u1u2, s) and (v1, v2). If x1x2 is short, then we
can combine by wrapping a single node labeled by
(x1, x2) with the subtree of D rooted at the left child
of p, to obtain a new 2-decomposition of z. In ei-
ther case, the result is a normal 2-decomposition of
z with fewer instances of concatenation. Repeat-
ing this procedure, we eventually obtain a normal,
concatenation-free 2-decomposition of z. ❑
Another useful property of the string z is the fol-
lowing:
</bodyText>
<construct confidence="0.3872005">
Lemma 7. Suppose that the following conditions
hold:
</construct>
<equation confidence="0.8828575">
(i) z = x1u1v1yv2u2x2,
(ii) x1yx2 is a short string, and
</equation>
<bodyText confidence="0.967158892857143">
(iii) both VI(u1u2) and VI(v1v2) are in [−2,2] ×
[−2,2].
Then either (u1, u2) or (v1, v2) is short.
Proof. Suppose (u1, u2) and (v1, v2) are both long.
Since (u1, u2) and (v1, v2) must both contain c, either
u1 ends in c and v1 starts in c, or else v2 ends in c
and u2 starts in c.
Case 1. u1 ends in c and v1 starts in c. Since
(v1, v2) must contain at least one occurrence of a,
the string v1yv2 must contain cb15a as a substring.
a5b14 a19 c29 b15 a5
v1yv2
Since x1yx2 is short, we have |y|b ≤ 4. It follows that
|v1v2|b ≥ 11. But v1yv2 is a substring of c28b15a5,
so |v1v2|a ≤ 5. This clearly contradicts VI(v1v2) ∈
[−2,2] × [−2,2].
Case 2. v2 ends in c and u2 starts in c. In this
case, cb15a5 is a suffix of u2x2. Since x1yx2 is short,
|x2|a ≤ 4. This means that cb15a is a substring of u2
and hence |u2|b = 15.
On the other hand, since (v1, v2) must contain at least
one occurrence of b, the string v1yv2 must contain
ba19c as a substring. This implies that |u1u2|a ≤ 10.
But since |u2|b = 15, we have |u1u2|b ≥ 15. This
clearly contradicts VI(u1u2) ∈ [−2,2] × [−2,2]. ❑
We now assume that z has a normal,
concatenation-free 2-decomposition D and de-
rive a contradiction. We do this by following
a certain path in D. Starting from the root, we
descend in D, always choosing a non-leaf child, as
long as there is one. We show that this path will
never terminate.
The i-th node on the path will be denoted by
pi, counting the root as the 0-th node. The la-
bel of pi will be denoted by (wi,1, wi,2). With each
i, we associate three strings xi,1, yi, xi,2 such that
xi,1wi,1yiwi,2xi,2 = z, analogously to Lemma 1. Since
VI(wi,1wi,2) ∈ [−2,2] × [−2,2] and VI(z) = (0, 0), we
will always have VI(xi,1yixi,2) ∈ [−2,2] × [−2,2].
Initially, (w0,1, w0,2) is the label of the root p0 and
x0,1 = y0 = x0,2 = s. If pi is not a leaf node, let
(ui,1, ui,2) and (vi,1, vi,2) be the labels of the left and
right children of pi, respectively. If the left child
is not a leaf node, we let pi+1 be the left child,
in which case we have (wi+1,1, wi+1,2) = (ui,1, ui,2),
xi+1,1 = xi,1, xi+1,2 = xi,2, and yi+1 = vi,1yvi,2. Oth-
erwise, pi+1 will be the right child of pi, and we
have (wi+1,1,wi+1,2) = (vi,1, vi,2), xi+1,1 = xi,1ui,1,
xi+1,2 = ui,2xi,2, and yi+1 = yi.
The path p0,p1,p2,... is naturally divided into
two parts. The initial part of the path consists of
nodes where xi,1yixi,2 is short. Note that x0,1y0x0,2 =
s is short. As long as xi,1yixi,2 is short, (wi,1, wi,2)
must be long and pi has two children labeled
by (ui,1, ui,2) and (vi,1, vi,2). By Lemma 7, either
(ui,1, ui,2) or (vi,1, vi,2) must be short. Since the length
</bodyText>
<figure confidence="0.99482025">
b15
a19
a5
c29
a5b14
u1
u2 x2
v1yv2
</figure>
<page confidence="0.991771">
672
</page>
<bodyText confidence="0.968496321428572">
of z is 87 and the length of a short string is at most 6,
exactly one of (ui,1, ui,2) and (vi,1, vi,2) must be long.
We must eventually enter the second part of
the path, where xi,1yixi,2 is no longer short. Let
μm be the Þrst node belonging to this part of the
path. Note that at μm, we have ψ(xm,1ymxm,2) =
ψ(xm−1,1ym−1xm−1,2) + ψ(v) for some short string v.
(Namely, v = um−1,1um−1,2 or v = vm−1,1vm−1,2.)
Lemma 8. If u and v are short strings and ψ(uv) E
[−2,2] x [−2,2], then |uv|d &lt;— 4 for each d E {a, b, c}.
Proof. Since u and v are short, we have |u|a &lt;—
4, |u|b &lt;— 4, |u|c &lt;— 2 and |v|a &lt;— 4, |v|b &lt;— 4, |v|c &lt;— 2. It
immediately follows that |uv|c &lt;— 4. We distinguish
two cases.
Case 1. |uv|c &lt;— 2. Since ψ(uv) E [−2,2] x [−2,2],
we must have |uv|a &lt;— 4 and |uv|b &lt;— 4.
Case 2. |uv|c &gt;— 3. Since |u|c &lt;— 2 and |v|c &lt;— 2,
we must have |u|c &gt;— 1 and |v|c &gt;— 1. Also, ψ(uv) E
[−2,2] x [−2,2] implies that |uv|a &gt;— 1 and |uv|b &gt;— 1.
Since u and v are short, it follows that one of the
following two conditions must hold:
(i) |u|a &gt;— 1, |u|b = 0 and |v|a = 0, |v|b &gt;— 1.
(ii) |u|a = 0, |u|b &gt;— 1 and |v|a &gt;— 1, |v|b = 0.
In the former case, |uv|a = |u|a &lt;— 4 and |uv|b = |v|b &lt;—
4. In the latter case, |uv|a = |v|a &lt;— 4 and |uv|b =
|u|b &lt;— 4. ❑
By Lemma 8, the number of occurrences of each
letter in xm,1ymxm,2 is in [1, 4]. This can only be if
</bodyText>
<equation confidence="0.993757">
xm,1xm,2 = aj,
ym = ckbl,
</equation>
<sectionHeader confidence="0.969535" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.963055897435897">
right children, labeled by (ui,1, ui,2) and (vi,1, vi,2), re-
spectively, such that wi,1 = ui,1vi,1 and wi,2 = vi,2ui,2.
We consider two cases.
Case 1. ui,1 contains c. Then ba19c is a substring
of ui,1. Since ui,2 is a substring of b14a5, it cannot
contain any occurrences of c. Since ψ1(ui,1ui,2) E
[−2,2], it follows that ui,1 must contain at least 17
occurrences of c; hence ba19c17 is a substring of ui,1.
Since (ui,1, ui,2) is long, (wi+1,1, wi+1,2) = (ui,1, ui,2).
Therefore, the condition (f) holds with i + 1 in place
of i.
Case 2. ui,1 does not contain c. Then (ui,1, ui,2) is
short and (wi+1,1, wi+1,2) = (vi,1, vi,2). Note that vi,1
must contain at least 17 occurrences of c, but vi,2 is
a substring of b14a5 and hence cannot contain more
than 14 occurrences of b. Since ψ2(vi,1vi,2) E [−2,2],
it follows that vi,1 must contain at least one occur-
rence of b. Therefore, ba19c17 must be a substring
of vi,1 = wi+1,1, which shows that (f) holds with i+1
in place of i.
We have proved that (f) holds for all i &gt;— m. It fol-
lows that for all i, μi has two children and hence μi+1
is deÞned. This means that the path μ0,μ1,μ2, . . .
is inÞnite, contradicting the assumption that D is a
2-decomposition of z.
We have proved the following:
Lemma 9. There is a string in MIX that has no 2-
decomposition.
Theorem 10. There is no head grammar G such that
L(G) = MIX.
Proof. Immediate from Lemmas 3, 4, and 9. ❑
for some j, k, l E [1, 4]. This means that the string z
must have been split into two strings (w0,1, w0,2) at
the root of D somewhere in the vicinity of position
67 (see Figure 2).
It immediately follows that for all i &gt;— m, wi,1 is
a substring of a5b14a19c28 and wi,2 is a substring of
b14a5. We show by induction that for all i &gt;— m, the
following condition holds:
</bodyText>
<equation confidence="0.695884">
(f) ba19c17 is a substring of wi,1.
</equation>
<bodyText confidence="0.7665735">
The condition (f) clearly holds for i = m. Now as-
sume (f). Then (wi,1, wi,2) is long, and μi has left and
</bodyText>
<reference confidence="0.998576307692308">
Setsuo Arikawa, Takeshi Shinohara, and Akihiro Ya-
mamoto. 1992. Learning elementary formal systems.
Theoretical Computer Science, 95(1):97–113.
Emmon Bach. 1981. Discontinuous constituents in gen-
eralized categorial grammars. In Victoria Burke and
James Pustejovsky, editors, Proceedings of the 11th
Annual Meeting of the North East Linguistic Society,
pages 1–12.
Emmon Bach. 1988. Categorial grammars as theories
of language. In Richard T. Oehrle, Emmon Bach, and
Deirdre Wheeler, editors, Categorial Grammars and
Natural Language Structures, pages 17–34. D. Reidel,
Dordrecht.
</reference>
<page confidence="0.990876">
673
</page>
<reference confidence="0.999623885245902">
Gerald Gazdar. 1988. Applicability of indexed gram-
mars to natural languages. In U. Reyle and C. Rohrer,
editors, Natural Language Parsing and Linguistic The-
ories, pages 69–94. D. Reidel Publishing Company,
Dordrecht.
Robert Gilman. 2005. Formal languages and their ap-
plication to combinatorial group theory. In Alexan-
dre V. Borovik, editor, Groups, Languages, Algo-
rithms, number 378 in Contemporary Mathematics,
pages 1–36. American Mathematical Society, Provi-
dence, RI.
Annius V. Groenink. 1997. Mild context-sensitivity and
tuple-based generalizations of context-free grammar.
Linguistics and Philosophy, 20:607–636.
Aravind K. Joshi, Vijay K. Shanker, and David J. Weir.
1991. The converence of mildly context-sensitive
grammar formalisms. In Peter Sells, Stuart M.
Shieber, and Thomas Wasow, editors, Foundational Is-
sues in Natural Language Processing, pages 31–81.
The MIT Press, Cambridge, MA.
Aravind K. Joshi. 1985. Tree-adjoining grammars: How
much context sensitivity is required to provide reason-
able structural descriptions? In David Dowty, Lauri
Karttunen, and Arnold M. Zwicky, editors, Natural
Language Parsing, pages 206–250. Cambridge Uni-
versity Press, Cambridge.
Markus Kracht. 2003. The Mathematics of Language,
volume 63 of Studies in Generative Grammar. Mou-
ton de Gruyter, Berlin.
William Marsh. 1985. Some conjectures on indexed
languages. Paper presented to the Association for
Symbolic Logic Meeting, Stanford University, July
15–19. Abstract appears in Journal of Symbolic
Logic 51(3):849 (1986).
Carl J. Pollard. 1984. Generalized Phrase Structure
Grammars, Head Grammars, and Natural Language.
Ph.D. thesis, Department of Linguistics, Stanford Uni-
versity.
Kelly Roach. 1987. Formal properties of head gram-
mars. In Alexis Manaster-Ramer, editor, Mathematics
of Language, pages 293–347. John Benjamins, Ams-
terdam.
Sylvain Salvati. 2011. MIX is a 2-MCFL and the word
problem in Z2 is captured by the IO and the OI hierar-
chies. Technical report, INRIA.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context free gram-
mars. Theoretical Computer Science, 88(2):191–229.
Raymond M. Smullyan. 1961. Theory of Formal Sys-
tems. Princeton University Press, Princeton, NJ.
K. Vijay-Shanker and D. J. Weir. 1994. The equivalence
of four extensions of context-free grammars. Mathe-
matical Systems Theory, 27:511–546.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions produced
by various grammatical formalisms. In 25th Annual
Meeting of the Association for Computational Linguis-
tics, pages 104–111.
David J. Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadephia, PA.
</reference>
<page confidence="0.99884">
674
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.721506">
<title confidence="0.998968">MIX Is Not a Tree-Adjoining Language</title>
<author confidence="0.978821">Makoto Kanazawa</author>
<affiliation confidence="0.999937">National Institute of Informatics</affiliation>
<address confidence="0.9918745">2–1–2 Hitotsubashi, Chiyoda-ku Tokyo, 101–8430, Japan</address>
<email confidence="0.968648">kanazawa@nii.ac.jp</email>
<author confidence="0.840508">Sylvain</author>
<affiliation confidence="0.998087">INRIA Bordeaux Sud-Ouest,</affiliation>
<address confidence="0.9663035">351, Cours de la F-33405 Talence Cedex,</address>
<email confidence="0.993425">sylvain.salvati@labri.fr</email>
<abstract confidence="0.996257333333333">The language MIX consists of all strings over three-letter alphabet contain an equal number of occurrences of each letter. We prove Joshi’s (1985) conjecture that MIX is not a tree-adjoining language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Setsuo Arikawa</author>
<author>Takeshi Shinohara</author>
<author>Akihiro Yamamoto</author>
</authors>
<title>Learning elementary formal systems.</title>
<date>1992</date>
<journal>Theoretical Computer Science,</journal>
<volume>95</volume>
<issue>1</issue>
<contexts>
<context position="7611" citStr="Arikawa et al., 1992" startWordPosition="1282" endWordPosition="1285">and rules of the last type are terminating rules. This definition of a head grammar actually corresponds to a normal form for head grammars that appears in section 3.3 of Vijay-Shanker and Weir’s (1994) paper.6 The rules of head grammars are interpreted as implications from right to left, where variables can be instantiated to any terminal strings. Each binary 5We use ε to denote the empty string. 6This normal form is also mentioned in chapter 5, section 4 of Kracht’s (2003) book. The notation we use to express rules of head grammars is borrowed from elementary formal systems (Smullyan, 1961; Arikawa et al., 1992), also known as literal movement grammars (Groenink, 1997; Kracht, 2003), which are logic programs over strings. In Vijay-Shanker and Weir’s (1994) notation, the four rules are expressed as follows: A → C2,2(B,C) A → C1,2(B,C) A → W(B, C) A → C1,1(w1 ↑ w2) 667 rule involves an operation that combines two pairs of strings to form a new pair. The operation involved in the third rule is known as wrapping; the operations involved in the first two rules we call left concatenation and right concatenation, respectively. If G = (N, Σ, P, S) is a head grammar, A ∈ N, and w1, w2 ∈ Σ∗, then we say that a</context>
</contexts>
<marker>Arikawa, Shinohara, Yamamoto, 1992</marker>
<rawString>Setsuo Arikawa, Takeshi Shinohara, and Akihiro Yamamoto. 1992. Learning elementary formal systems. Theoretical Computer Science, 95(1):97–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmon Bach</author>
</authors>
<title>Discontinuous constituents in generalized categorial grammars.</title>
<date>1981</date>
<booktitle>Proceedings of the 11th Annual Meeting of the North East Linguistic Society,</booktitle>
<pages>1--12</pages>
<editor>In Victoria Burke and James Pustejovsky, editors,</editor>
<contexts>
<context position="699" citStr="Bach (1981)" startWordPosition="102" endWordPosition="103">–1–2 Hitotsubashi, Chiyoda-ku Tokyo, 101–8430, Japan kanazawa@nii.ac.jp Sylvain Salvati INRIA Bordeaux Sud-Ouest, LaBRI 351, Cours de la Lib´eration F-33405 Talence Cedex, France sylvain.salvati@labri.fr Abstract The language MIX consists of all strings over the three-letter alphabet {a, b, c} that contain an equal number of occurrences of each letter. We prove Joshi’s (1985) conjecture that MIX is not a tree-adjoining language. 1 Introduction The language MIX = { w ∈ {a, b, c}∗ ||w|a = |w|b = |w|c } has attracted considerable attention in computational linguistics.1 This language was used by Bach (1981) in an exercise to show that the permutation closure of a context-free language is not necessarily contextfree.2 MIX may be considered a prototypical example offree word order language, but, as remarked by Bach (1981), it seems that no human language “has such complete freedom for order”, because “typically, certain constituents act as ‘boundary domains’ for scrambling”. Joshi (1985) refers to MIX as representing “an extreme case of the degree of free word order permitted in a language”, which is “linguistically not relevant”. Gazdar (1988) adopts a similar position regarding the relation betw</context>
</contexts>
<marker>Bach, 1981</marker>
<rawString>Emmon Bach. 1981. Discontinuous constituents in generalized categorial grammars. In Victoria Burke and James Pustejovsky, editors, Proceedings of the 11th Annual Meeting of the North East Linguistic Society, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmon Bach</author>
</authors>
<title>Categorial grammars as theories of language.</title>
<date>1988</date>
<booktitle>Categorial Grammars and Natural Language Structures,</booktitle>
<pages>17--34</pages>
<editor>In Richard T. Oehrle, Emmon Bach, and Deirdre Wheeler, editors,</editor>
<location>Dordrecht.</location>
<contexts>
<context position="1703" citStr="Bach (1988)" startWordPosition="275" endWordPosition="276"> to MIX as representing “an extreme case of the degree of free word order permitted in a language”, which is “linguistically not relevant”. Gazdar (1988) adopts a similar position regarding the relation between MIX 1If w is a string and d is a symbol, we write |w|d to mean the number of occurrences of d in w. We will use the notation |w |to denote the length of w, i.e., the total number of occurrences of symbols in w. 2According to Gazdar (1988), “MIX was originally described by Emmon Bach and was so-dubbed by students in the 1983 Hampshire College Summer Studies in Mathematics”. According to Bach (1988), the name MIX was “the happy invention of Bill Marsh”. and natural languages, noting that “it seems rather unlikely that any natural language will turn out to have a MIX-like characteristic”. It therefore seems natural to assume that languages such as MIX should be excluded from any class of formal languages that purports to be a tight formal characterization of the possible natural languages. It was in this spirit that Joshi et al. (1991) suggested that MIX should not be in the class of socalled mildly context-sensitive languages: “[mildly context-sensitive grammars] capture only certain kin</context>
</contexts>
<marker>Bach, 1988</marker>
<rawString>Emmon Bach. 1988. Categorial grammars as theories of language. In Richard T. Oehrle, Emmon Bach, and Deirdre Wheeler, editors, Categorial Grammars and Natural Language Structures, pages 17–34. D. Reidel, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
</authors>
<title>Applicability of indexed grammars to natural languages.</title>
<date>1988</date>
<booktitle>Natural Language Parsing and Linguistic Theories,</booktitle>
<pages>69--94</pages>
<editor>In U. Reyle and C. Rohrer, editors,</editor>
<publisher>Publishing Company,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="1245" citStr="Gazdar (1988)" startWordPosition="191" endWordPosition="192">n computational linguistics.1 This language was used by Bach (1981) in an exercise to show that the permutation closure of a context-free language is not necessarily contextfree.2 MIX may be considered a prototypical example offree word order language, but, as remarked by Bach (1981), it seems that no human language “has such complete freedom for order”, because “typically, certain constituents act as ‘boundary domains’ for scrambling”. Joshi (1985) refers to MIX as representing “an extreme case of the degree of free word order permitted in a language”, which is “linguistically not relevant”. Gazdar (1988) adopts a similar position regarding the relation between MIX 1If w is a string and d is a symbol, we write |w|d to mean the number of occurrences of d in w. We will use the notation |w |to denote the length of w, i.e., the total number of occurrences of symbols in w. 2According to Gazdar (1988), “MIX was originally described by Emmon Bach and was so-dubbed by students in the 1983 Hampshire College Summer Studies in Mathematics”. According to Bach (1988), the name MIX was “the happy invention of Bill Marsh”. and natural languages, noting that “it seems rather unlikely that any natural language</context>
</contexts>
<marker>Gazdar, 1988</marker>
<rawString>Gerald Gazdar. 1988. Applicability of indexed grammars to natural languages. In U. Reyle and C. Rohrer, editors, Natural Language Parsing and Linguistic Theories, pages 69–94. D. Reidel Publishing Company, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Gilman</author>
</authors>
<title>Formal languages and their application to combinatorial group theory.</title>
<date>2005</date>
<booktitle>Groups, Languages, Algorithms, number 378 in Contemporary Mathematics,</booktitle>
<pages>1--36</pages>
<editor>In Alexandre V. Borovik, editor,</editor>
<publisher>American Mathematical Society,</publisher>
<location>Providence, RI.</location>
<contexts>
<context position="5339" citStr="Gilman (2005)" startWordPosition="865" endWordPosition="866">joining language. Our proof is cast in terms of the formalism of head grammar (Pollard, 1984; Roach, 1987), which is known to be equivalent to TAG (Vijay-Shanker and Weir, 1994). The key to our proof is the notion of an n-decomposition of a string over {a, b, c}, which is similar to the notion of a derivation in head grammars, but independent of any particular grammar. The parameter n indicates how unbalanced the occurrence counts of the three letters can be at any point in a decomposition. We first 3The relation of MIX with indexed languages is also of interest in combinatorial group theory. Gilman (2005) remarks that “it does not ... seem to be known whether or not the word problem of Z × Z is indexed”, alluding to the language O2 = { w ∈ {a, ¯a, b, ¯b}∗ ||w|a = |w|¯a, |w|b = |w|¯b }. Since O2 and MIX are rationally equivalent, O2 is indexed if and only if MIX is indexed (Salvati, 2011). 4Joshi et al. (1991) presented linear context-free rewriting systems as mildly context-sensitive grammars. Groenink (1997) wrote “The class of mildly context-sensitive languages seems to be most adequately approached by LCFRS.” show that if MIX is generated by some head grammar, then there is an n such that e</context>
</contexts>
<marker>Gilman, 2005</marker>
<rawString>Robert Gilman. 2005. Formal languages and their application to combinatorial group theory. In Alexandre V. Borovik, editor, Groups, Languages, Algorithms, number 378 in Contemporary Mathematics, pages 1–36. American Mathematical Society, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annius V Groenink</author>
</authors>
<title>Mild context-sensitivity and tuple-based generalizations of context-free grammar. Linguistics and Philosophy,</title>
<date>1997</date>
<pages>20--607</pages>
<contexts>
<context position="5751" citStr="Groenink (1997)" startWordPosition="941" endWordPosition="942">ed the occurrence counts of the three letters can be at any point in a decomposition. We first 3The relation of MIX with indexed languages is also of interest in combinatorial group theory. Gilman (2005) remarks that “it does not ... seem to be known whether or not the word problem of Z × Z is indexed”, alluding to the language O2 = { w ∈ {a, ¯a, b, ¯b}∗ ||w|a = |w|¯a, |w|b = |w|¯b }. Since O2 and MIX are rationally equivalent, O2 is indexed if and only if MIX is indexed (Salvati, 2011). 4Joshi et al. (1991) presented linear context-free rewriting systems as mildly context-sensitive grammars. Groenink (1997) wrote “The class of mildly context-sensitive languages seems to be most adequately approached by LCFRS.” show that if MIX is generated by some head grammar, then there is an n such that every string in MIX has an n-decomposition. We then prove that if every string in MIX has an n-decomposition, then every string in MIX must have a 2-decomposition. Finally, we exhibit a particular string in MIX that has no 2- decomposition. The length of this string is 87, and the fact that it has no 2-decomposition was first verified by a computer program accompanying this paper. We include here a rigorous, m</context>
<context position="7668" citStr="Groenink, 1997" startWordPosition="1292" endWordPosition="1293">n of a head grammar actually corresponds to a normal form for head grammars that appears in section 3.3 of Vijay-Shanker and Weir’s (1994) paper.6 The rules of head grammars are interpreted as implications from right to left, where variables can be instantiated to any terminal strings. Each binary 5We use ε to denote the empty string. 6This normal form is also mentioned in chapter 5, section 4 of Kracht’s (2003) book. The notation we use to express rules of head grammars is borrowed from elementary formal systems (Smullyan, 1961; Arikawa et al., 1992), also known as literal movement grammars (Groenink, 1997; Kracht, 2003), which are logic programs over strings. In Vijay-Shanker and Weir’s (1994) notation, the four rules are expressed as follows: A → C2,2(B,C) A → C1,2(B,C) A → W(B, C) A → C1,1(w1 ↑ w2) 667 rule involves an operation that combines two pairs of strings to form a new pair. The operation involved in the third rule is known as wrapping; the operations involved in the first two rules we call left concatenation and right concatenation, respectively. If G = (N, Σ, P, S) is a head grammar, A ∈ N, and w1, w2 ∈ Σ∗, then we say that a fact A(w1, w2) is derivable and write FG A(w1, w2), if A</context>
</contexts>
<marker>Groenink, 1997</marker>
<rawString>Annius V. Groenink. 1997. Mild context-sensitivity and tuple-based generalizations of context-free grammar. Linguistics and Philosophy, 20:607–636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Vijay K Shanker</author>
<author>David J Weir</author>
</authors>
<title>The converence of mildly context-sensitive grammar formalisms.</title>
<date>1991</date>
<booktitle>Foundational Issues in Natural Language Processing,</booktitle>
<pages>31--81</pages>
<editor>In Peter Sells, Stuart M. Shieber, and Thomas Wasow, editors,</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2147" citStr="Joshi et al. (1991)" startWordPosition="350" endWordPosition="353">Gazdar (1988), “MIX was originally described by Emmon Bach and was so-dubbed by students in the 1983 Hampshire College Summer Studies in Mathematics”. According to Bach (1988), the name MIX was “the happy invention of Bill Marsh”. and natural languages, noting that “it seems rather unlikely that any natural language will turn out to have a MIX-like characteristic”. It therefore seems natural to assume that languages such as MIX should be excluded from any class of formal languages that purports to be a tight formal characterization of the possible natural languages. It was in this spirit that Joshi et al. (1991) suggested that MIX should not be in the class of socalled mildly context-sensitive languages: “[mildly context-sensitive grammars] capture only certain kinds of dependencies, e.g., nested dependencies and certain limited kinds of cross-serial dependencies (for example, in the subordinate clause constructions in Dutch or some variations of them, but perhaps not in the so-called MIX (or Bach) language) ....” Mild context-sensitivity is an informally defined notion first introduced by Joshi (1985); it consists of the three conditions of limited cross-serial dependencies, constant growth, and pol</context>
<context position="3605" citStr="Joshi et al. (1991)" startWordPosition="573" endWordPosition="576">ition of limited cross-serial dependencies. Joshi (1985) conjectured rather strongly that MIX is not a tree-adjoining language: “TAGs cannot generate this language, although for TAGs the proof is not in hand yet”. An even stronger conjecture was made by Marsh (1985), namely, that MIX is not an 666 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 666–674, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics indexed language.3 (It is known that the indexed languages properly include the tree-adjoining languages.) Joshi et al. (1991), however, expressed a more pessimistic view about the conjecture: “It is not known whether TAG ... can generate MIX. This has turned out to be a very difficult problem. In fact, it is not even known whether an IG [(indexed grammar)] can generate MIX.” This open question has become all the more pressing after a recent result by Salvati (2011). This result says that MIX is in the class of multiple contextfree languages (Seki et al., 1991), or equivalently, languages of linear context-free rewriting systems (Vijay-Shanker et al., 1987; Weir, 1988), which has been customarily regarded as a formal</context>
<context position="5649" citStr="Joshi et al. (1991)" startWordPosition="928" endWordPosition="931">vation in head grammars, but independent of any particular grammar. The parameter n indicates how unbalanced the occurrence counts of the three letters can be at any point in a decomposition. We first 3The relation of MIX with indexed languages is also of interest in combinatorial group theory. Gilman (2005) remarks that “it does not ... seem to be known whether or not the word problem of Z × Z is indexed”, alluding to the language O2 = { w ∈ {a, ¯a, b, ¯b}∗ ||w|a = |w|¯a, |w|b = |w|¯b }. Since O2 and MIX are rationally equivalent, O2 is indexed if and only if MIX is indexed (Salvati, 2011). 4Joshi et al. (1991) presented linear context-free rewriting systems as mildly context-sensitive grammars. Groenink (1997) wrote “The class of mildly context-sensitive languages seems to be most adequately approached by LCFRS.” show that if MIX is generated by some head grammar, then there is an n such that every string in MIX has an n-decomposition. We then prove that if every string in MIX has an n-decomposition, then every string in MIX must have a 2-decomposition. Finally, we exhibit a particular string in MIX that has no 2- decomposition. The length of this string is 87, and the fact that it has no 2-decompo</context>
</contexts>
<marker>Joshi, Shanker, Weir, 1991</marker>
<rawString>Aravind K. Joshi, Vijay K. Shanker, and David J. Weir. 1991. The converence of mildly context-sensitive grammar formalisms. In Peter Sells, Stuart M. Shieber, and Thomas Wasow, editors, Foundational Issues in Natural Language Processing, pages 31–81. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Tree-adjoining grammars: How much context sensitivity is required to provide reasonable structural descriptions? In</title>
<date>1985</date>
<booktitle>Natural Language Parsing,</booktitle>
<pages>206--250</pages>
<editor>David Dowty, Lauri Karttunen, and Arnold M. Zwicky, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1085" citStr="Joshi (1985)" startWordPosition="163" endWordPosition="164">ture that MIX is not a tree-adjoining language. 1 Introduction The language MIX = { w ∈ {a, b, c}∗ ||w|a = |w|b = |w|c } has attracted considerable attention in computational linguistics.1 This language was used by Bach (1981) in an exercise to show that the permutation closure of a context-free language is not necessarily contextfree.2 MIX may be considered a prototypical example offree word order language, but, as remarked by Bach (1981), it seems that no human language “has such complete freedom for order”, because “typically, certain constituents act as ‘boundary domains’ for scrambling”. Joshi (1985) refers to MIX as representing “an extreme case of the degree of free word order permitted in a language”, which is “linguistically not relevant”. Gazdar (1988) adopts a similar position regarding the relation between MIX 1If w is a string and d is a symbol, we write |w|d to mean the number of occurrences of d in w. We will use the notation |w |to denote the length of w, i.e., the total number of occurrences of symbols in w. 2According to Gazdar (1988), “MIX was originally described by Emmon Bach and was so-dubbed by students in the 1983 Hampshire College Summer Studies in Mathematics”. Accord</context>
<context position="2647" citStr="Joshi (1985)" startWordPosition="426" endWordPosition="427"> tight formal characterization of the possible natural languages. It was in this spirit that Joshi et al. (1991) suggested that MIX should not be in the class of socalled mildly context-sensitive languages: “[mildly context-sensitive grammars] capture only certain kinds of dependencies, e.g., nested dependencies and certain limited kinds of cross-serial dependencies (for example, in the subordinate clause constructions in Dutch or some variations of them, but perhaps not in the so-called MIX (or Bach) language) ....” Mild context-sensitivity is an informally defined notion first introduced by Joshi (1985); it consists of the three conditions of limited cross-serial dependencies, constant growth, and polynomial parsing. The first condition is only vaguely formulated, but the other two conditions are clearly satisfied by treeadjoining grammars. The suggestion of Joshi et al. (1991) was that MIX should be regarded as a violation of the condition of limited cross-serial dependencies. Joshi (1985) conjectured rather strongly that MIX is not a tree-adjoining language: “TAGs cannot generate this language, although for TAGs the proof is not in hand yet”. An even stronger conjecture was made by Marsh (</context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>Aravind K. Joshi. 1985. Tree-adjoining grammars: How much context sensitivity is required to provide reasonable structural descriptions? In David Dowty, Lauri Karttunen, and Arnold M. Zwicky, editors, Natural Language Parsing, pages 206–250. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Kracht</author>
</authors>
<title>The Mathematics of Language,</title>
<date>2003</date>
<booktitle>Studies in Generative Grammar. Mouton de Gruyter,</booktitle>
<volume>63</volume>
<location>Berlin.</location>
<contexts>
<context position="7683" citStr="Kracht, 2003" startWordPosition="1294" endWordPosition="1295">mar actually corresponds to a normal form for head grammars that appears in section 3.3 of Vijay-Shanker and Weir’s (1994) paper.6 The rules of head grammars are interpreted as implications from right to left, where variables can be instantiated to any terminal strings. Each binary 5We use ε to denote the empty string. 6This normal form is also mentioned in chapter 5, section 4 of Kracht’s (2003) book. The notation we use to express rules of head grammars is borrowed from elementary formal systems (Smullyan, 1961; Arikawa et al., 1992), also known as literal movement grammars (Groenink, 1997; Kracht, 2003), which are logic programs over strings. In Vijay-Shanker and Weir’s (1994) notation, the four rules are expressed as follows: A → C2,2(B,C) A → C1,2(B,C) A → W(B, C) A → C1,1(w1 ↑ w2) 667 rule involves an operation that combines two pairs of strings to form a new pair. The operation involved in the third rule is known as wrapping; the operations involved in the first two rules we call left concatenation and right concatenation, respectively. If G = (N, Σ, P, S) is a head grammar, A ∈ N, and w1, w2 ∈ Σ∗, then we say that a fact A(w1, w2) is derivable and write FG A(w1, w2), if A(w1, w2) can be</context>
</contexts>
<marker>Kracht, 2003</marker>
<rawString>Markus Kracht. 2003. The Mathematics of Language, volume 63 of Studies in Generative Grammar. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Marsh</author>
</authors>
<title>Some conjectures on indexed languages. Paper presented to the Association for Symbolic Logic Meeting,</title>
<date>1985</date>
<booktitle>Abstract appears in Journal of Symbolic Logic</booktitle>
<volume>51</volume>
<issue>3</issue>
<institution>Stanford University,</institution>
<contexts>
<context position="3252" citStr="Marsh (1985)" startWordPosition="523" endWordPosition="524"> (1985); it consists of the three conditions of limited cross-serial dependencies, constant growth, and polynomial parsing. The first condition is only vaguely formulated, but the other two conditions are clearly satisfied by treeadjoining grammars. The suggestion of Joshi et al. (1991) was that MIX should be regarded as a violation of the condition of limited cross-serial dependencies. Joshi (1985) conjectured rather strongly that MIX is not a tree-adjoining language: “TAGs cannot generate this language, although for TAGs the proof is not in hand yet”. An even stronger conjecture was made by Marsh (1985), namely, that MIX is not an 666 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 666–674, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics indexed language.3 (It is known that the indexed languages properly include the tree-adjoining languages.) Joshi et al. (1991), however, expressed a more pessimistic view about the conjecture: “It is not known whether TAG ... can generate MIX. This has turned out to be a very difficult problem. In fact, it is not even known whether an IG [(indexed grammar)] can generate </context>
</contexts>
<marker>Marsh, 1985</marker>
<rawString>William Marsh. 1985. Some conjectures on indexed languages. Paper presented to the Association for Symbolic Logic Meeting, Stanford University, July 15–19. Abstract appears in Journal of Symbolic Logic 51(3):849 (1986).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl J Pollard</author>
</authors>
<title>Generalized Phrase Structure Grammars, Head Grammars, and Natural Language.</title>
<date>1984</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Linguistics, Stanford University.</institution>
<contexts>
<context position="4818" citStr="Pollard, 1984" startWordPosition="773" endWordPosition="774"> counterpart of the informal notion of a mildly context-sensitive language.4 It means that either we have to abandon the identification of multiple context-free languages with mildly context-sensitive languages, or we should revise our conception of limited crossserial dependencies and stop regarding MIX-like languages as violations of this condition. Surely, the resolution of Joshi’s (1985) conjecture should crucially affect the choice between these two alternatives. In this paper, we prove that MIX is not a treeadjoining language. Our proof is cast in terms of the formalism of head grammar (Pollard, 1984; Roach, 1987), which is known to be equivalent to TAG (Vijay-Shanker and Weir, 1994). The key to our proof is the notion of an n-decomposition of a string over {a, b, c}, which is similar to the notion of a derivation in head grammars, but independent of any particular grammar. The parameter n indicates how unbalanced the occurrence counts of the three letters can be at any point in a decomposition. We first 3The relation of MIX with indexed languages is also of interest in combinatorial group theory. Gilman (2005) remarks that “it does not ... seem to be known whether or not the word problem</context>
</contexts>
<marker>Pollard, 1984</marker>
<rawString>Carl J. Pollard. 1984. Generalized Phrase Structure Grammars, Head Grammars, and Natural Language. Ph.D. thesis, Department of Linguistics, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kelly Roach</author>
</authors>
<title>Formal properties of head grammars.</title>
<date>1987</date>
<booktitle>Mathematics of Language,</booktitle>
<pages>293--347</pages>
<editor>In Alexis Manaster-Ramer, editor,</editor>
<publisher>John Benjamins,</publisher>
<location>Amsterdam.</location>
<contexts>
<context position="4832" citStr="Roach, 1987" startWordPosition="775" endWordPosition="776"> the informal notion of a mildly context-sensitive language.4 It means that either we have to abandon the identification of multiple context-free languages with mildly context-sensitive languages, or we should revise our conception of limited crossserial dependencies and stop regarding MIX-like languages as violations of this condition. Surely, the resolution of Joshi’s (1985) conjecture should crucially affect the choice between these two alternatives. In this paper, we prove that MIX is not a treeadjoining language. Our proof is cast in terms of the formalism of head grammar (Pollard, 1984; Roach, 1987), which is known to be equivalent to TAG (Vijay-Shanker and Weir, 1994). The key to our proof is the notion of an n-decomposition of a string over {a, b, c}, which is similar to the notion of a derivation in head grammars, but independent of any particular grammar. The parameter n indicates how unbalanced the occurrence counts of the three letters can be at any point in a decomposition. We first 3The relation of MIX with indexed languages is also of interest in combinatorial group theory. Gilman (2005) remarks that “it does not ... seem to be known whether or not the word problem of Z × Z is i</context>
</contexts>
<marker>Roach, 1987</marker>
<rawString>Kelly Roach. 1987. Formal properties of head grammars. In Alexis Manaster-Ramer, editor, Mathematics of Language, pages 293–347. John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Salvati</author>
</authors>
<title>MIX is a 2-MCFL and the word problem in Z2 is captured by the IO and the OI hierarchies.</title>
<date>2011</date>
<tech>Technical report, INRIA.</tech>
<contexts>
<context position="3949" citStr="Salvati (2011)" startWordPosition="636" endWordPosition="637">ciation for Computational Linguistics, pages 666–674, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics indexed language.3 (It is known that the indexed languages properly include the tree-adjoining languages.) Joshi et al. (1991), however, expressed a more pessimistic view about the conjecture: “It is not known whether TAG ... can generate MIX. This has turned out to be a very difficult problem. In fact, it is not even known whether an IG [(indexed grammar)] can generate MIX.” This open question has become all the more pressing after a recent result by Salvati (2011). This result says that MIX is in the class of multiple contextfree languages (Seki et al., 1991), or equivalently, languages of linear context-free rewriting systems (Vijay-Shanker et al., 1987; Weir, 1988), which has been customarily regarded as a formal counterpart of the informal notion of a mildly context-sensitive language.4 It means that either we have to abandon the identification of multiple context-free languages with mildly context-sensitive languages, or we should revise our conception of limited crossserial dependencies and stop regarding MIX-like languages as violations of this c</context>
<context position="5627" citStr="Salvati, 2011" startWordPosition="926" endWordPosition="927"> notion of a derivation in head grammars, but independent of any particular grammar. The parameter n indicates how unbalanced the occurrence counts of the three letters can be at any point in a decomposition. We first 3The relation of MIX with indexed languages is also of interest in combinatorial group theory. Gilman (2005) remarks that “it does not ... seem to be known whether or not the word problem of Z × Z is indexed”, alluding to the language O2 = { w ∈ {a, ¯a, b, ¯b}∗ ||w|a = |w|¯a, |w|b = |w|¯b }. Since O2 and MIX are rationally equivalent, O2 is indexed if and only if MIX is indexed (Salvati, 2011). 4Joshi et al. (1991) presented linear context-free rewriting systems as mildly context-sensitive grammars. Groenink (1997) wrote “The class of mildly context-sensitive languages seems to be most adequately approached by LCFRS.” show that if MIX is generated by some head grammar, then there is an n such that every string in MIX has an n-decomposition. We then prove that if every string in MIX has an n-decomposition, then every string in MIX must have a 2-decomposition. Finally, we exhibit a particular string in MIX that has no 2- decomposition. The length of this string is 87, and the fact th</context>
</contexts>
<marker>Salvati, 2011</marker>
<rawString>Sylvain Salvati. 2011. MIX is a 2-MCFL and the word problem in Z2 is captured by the IO and the OI hierarchies. Technical report, INRIA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Seki</author>
<author>Takashi Matsumura</author>
<author>Mamoru Fujii</author>
<author>Tadao Kasami</author>
</authors>
<title>On multiple context free grammars.</title>
<date>1991</date>
<journal>Theoretical Computer Science,</journal>
<volume>88</volume>
<issue>2</issue>
<contexts>
<context position="4046" citStr="Seki et al., 1991" startWordPosition="653" endWordPosition="656">. c�2012 Association for Computational Linguistics indexed language.3 (It is known that the indexed languages properly include the tree-adjoining languages.) Joshi et al. (1991), however, expressed a more pessimistic view about the conjecture: “It is not known whether TAG ... can generate MIX. This has turned out to be a very difficult problem. In fact, it is not even known whether an IG [(indexed grammar)] can generate MIX.” This open question has become all the more pressing after a recent result by Salvati (2011). This result says that MIX is in the class of multiple contextfree languages (Seki et al., 1991), or equivalently, languages of linear context-free rewriting systems (Vijay-Shanker et al., 1987; Weir, 1988), which has been customarily regarded as a formal counterpart of the informal notion of a mildly context-sensitive language.4 It means that either we have to abandon the identification of multiple context-free languages with mildly context-sensitive languages, or we should revise our conception of limited crossserial dependencies and stop regarding MIX-like languages as violations of this condition. Surely, the resolution of Joshi’s (1985) conjecture should crucially affect the choice </context>
</contexts>
<marker>Seki, Matsumura, Fujii, Kasami, 1991</marker>
<rawString>Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and Tadao Kasami. 1991. On multiple context free grammars. Theoretical Computer Science, 88(2):191–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond M Smullyan</author>
</authors>
<title>Theory of Formal Systems.</title>
<date>1961</date>
<publisher>Princeton University Press,</publisher>
<location>Princeton, NJ.</location>
<contexts>
<context position="7588" citStr="Smullyan, 1961" startWordPosition="1280" endWordPosition="1281">re binary rules and rules of the last type are terminating rules. This definition of a head grammar actually corresponds to a normal form for head grammars that appears in section 3.3 of Vijay-Shanker and Weir’s (1994) paper.6 The rules of head grammars are interpreted as implications from right to left, where variables can be instantiated to any terminal strings. Each binary 5We use ε to denote the empty string. 6This normal form is also mentioned in chapter 5, section 4 of Kracht’s (2003) book. The notation we use to express rules of head grammars is borrowed from elementary formal systems (Smullyan, 1961; Arikawa et al., 1992), also known as literal movement grammars (Groenink, 1997; Kracht, 2003), which are logic programs over strings. In Vijay-Shanker and Weir’s (1994) notation, the four rules are expressed as follows: A → C2,2(B,C) A → C1,2(B,C) A → W(B, C) A → C1,1(w1 ↑ w2) 667 rule involves an operation that combines two pairs of strings to form a new pair. The operation involved in the third rule is known as wrapping; the operations involved in the first two rules we call left concatenation and right concatenation, respectively. If G = (N, Σ, P, S) is a head grammar, A ∈ N, and w1, w2 ∈</context>
</contexts>
<marker>Smullyan, 1961</marker>
<rawString>Raymond M. Smullyan. 1961. Theory of Formal Systems. Princeton University Press, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
</authors>
<title>The equivalence of four extensions of context-free grammars.</title>
<date>1994</date>
<booktitle>Mathematical Systems Theory,</booktitle>
<pages>27--511</pages>
<contexts>
<context position="4903" citStr="Vijay-Shanker and Weir, 1994" startWordPosition="785" endWordPosition="788">guage.4 It means that either we have to abandon the identification of multiple context-free languages with mildly context-sensitive languages, or we should revise our conception of limited crossserial dependencies and stop regarding MIX-like languages as violations of this condition. Surely, the resolution of Joshi’s (1985) conjecture should crucially affect the choice between these two alternatives. In this paper, we prove that MIX is not a treeadjoining language. Our proof is cast in terms of the formalism of head grammar (Pollard, 1984; Roach, 1987), which is known to be equivalent to TAG (Vijay-Shanker and Weir, 1994). The key to our proof is the notion of an n-decomposition of a string over {a, b, c}, which is similar to the notion of a derivation in head grammars, but independent of any particular grammar. The parameter n indicates how unbalanced the occurrence counts of the three letters can be at any point in a decomposition. We first 3The relation of MIX with indexed languages is also of interest in combinatorial group theory. Gilman (2005) remarks that “it does not ... seem to be known whether or not the word problem of Z × Z is indexed”, alluding to the language O2 = { w ∈ {a, ¯a, b, ¯b}∗ ||w|a = |w</context>
</contexts>
<marker>Vijay-Shanker, Weir, 1994</marker>
<rawString>K. Vijay-Shanker and D. J. Weir. 1994. The equivalence of four extensions of context-free grammars. Mathematical Systems Theory, 27:511–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
<author>Aravind K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In 25th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="4143" citStr="Vijay-Shanker et al., 1987" startWordPosition="665" endWordPosition="668">he indexed languages properly include the tree-adjoining languages.) Joshi et al. (1991), however, expressed a more pessimistic view about the conjecture: “It is not known whether TAG ... can generate MIX. This has turned out to be a very difficult problem. In fact, it is not even known whether an IG [(indexed grammar)] can generate MIX.” This open question has become all the more pressing after a recent result by Salvati (2011). This result says that MIX is in the class of multiple contextfree languages (Seki et al., 1991), or equivalently, languages of linear context-free rewriting systems (Vijay-Shanker et al., 1987; Weir, 1988), which has been customarily regarded as a formal counterpart of the informal notion of a mildly context-sensitive language.4 It means that either we have to abandon the identification of multiple context-free languages with mildly context-sensitive languages, or we should revise our conception of limited crossserial dependencies and stop regarding MIX-like languages as violations of this condition. Surely, the resolution of Joshi’s (1985) conjecture should crucially affect the choice between these two alternatives. In this paper, we prove that MIX is not a treeadjoining language.</context>
</contexts>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In 25th Annual Meeting of the Association for Computational Linguistics, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Weir</author>
</authors>
<title>Characterizing Mildly ContextSensitive Grammar Formalisms.</title>
<date>1988</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadephia, PA.</location>
<contexts>
<context position="4156" citStr="Weir, 1988" startWordPosition="669" endWordPosition="670">y include the tree-adjoining languages.) Joshi et al. (1991), however, expressed a more pessimistic view about the conjecture: “It is not known whether TAG ... can generate MIX. This has turned out to be a very difficult problem. In fact, it is not even known whether an IG [(indexed grammar)] can generate MIX.” This open question has become all the more pressing after a recent result by Salvati (2011). This result says that MIX is in the class of multiple contextfree languages (Seki et al., 1991), or equivalently, languages of linear context-free rewriting systems (Vijay-Shanker et al., 1987; Weir, 1988), which has been customarily regarded as a formal counterpart of the informal notion of a mildly context-sensitive language.4 It means that either we have to abandon the identification of multiple context-free languages with mildly context-sensitive languages, or we should revise our conception of limited crossserial dependencies and stop regarding MIX-like languages as violations of this condition. Surely, the resolution of Joshi’s (1985) conjecture should crucially affect the choice between these two alternatives. In this paper, we prove that MIX is not a treeadjoining language. Our proof is</context>
</contexts>
<marker>Weir, 1988</marker>
<rawString>David J. Weir. 1988. Characterizing Mildly ContextSensitive Grammar Formalisms. Ph.D. thesis, University of Pennsylvania, Philadephia, PA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>