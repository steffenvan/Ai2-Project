<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.972126">
An Entity-Level Approach to Information Extraction
</title>
<author confidence="0.605739">
Aria Haghighi
</author>
<address confidence="0.501787">
UC Berkeley, CS Division
</address>
<email confidence="0.898417">
aria42@cs.berkeley.edu
</email>
<author confidence="0.684751">
Dan Klein
</author>
<address confidence="0.598122">
UC Berkeley, CS Division
</address>
<email confidence="0.991545">
klein@cs.berkeley.edu
</email>
<sectionHeader confidence="0.997305" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999954">
We present a generative model of
template-filling in which coreference
resolution and role assignment are jointly
determined. Underlying template roles
first generate abstract entities, which in
turn generate concrete textual mentions.
On the standard corporate acquisitions
dataset, joint resolution in our entity-level
model reduces error over a mention-level
discriminative approach by up to 20%.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.980016482758621">
Template-filling information extraction (IE) sys-
tems must merge information across multiple sen-
tences to identify all role fillers of interest. For
instance, in the MUC4 terrorism event extrac-
tion task, the entity filling the individual perpetra-
tor role often occurs multiple times, variously as
proper, nominal, or pronominal mentions. How-
ever, most template-filling systems (Freitag and
McCallum, 2000; Patwardhan and Riloff, 2007)
assign roles to individual textual mentions using
only local context as evidence, leaving aggrega-
tion for post-processing. While prior work has
acknowledged that coreference resolution and dis-
course analysis are integral to accurate role identi-
fication, to our knowledge no model has been pro-
posed which jointly models these phenomena.
In this work, we describe an entity-centered ap-
proach to template-filling IE problems. Our model
jointly merges surface mentions into underlying
entities (coreference resolution) and assigns roles
to those discovered entities. In the generative pro-
cess proposed here, document entities are gener-
ated for each template role, along with a set of
non-template entities. These entities then generate
mentions in a process sensitive to both lexical and
structural properties of the mention. Our model
outperforms a discriminative mention-level base-
line. Moreover, since our model is generative, it
Template
</bodyText>
<sectionHeader confidence="0.457267" genericHeader="method">
SELLER BUSINESS ACQUIRED PURCHASER
CSR Limited Oil and Gas Delhi Fund Esso Inc.
</sectionHeader>
<subsectionHeader confidence="0.441965">
Document
</subsectionHeader>
<bodyText confidence="0.550395333333333">
[S CSR] has said that [S it] has sold [S its] [B oil
interests] held in [A Delhi Fund]. [P Esso Inc.] did not
disclose how much [P they] paid for [A Dehli].
</bodyText>
<figureCaption confidence="0.9900984">
Figure 1: Example of the corporate acquisitions role-filling
task. In (a), an example template specifying the entities play-
ing each domain role. In (b), an example document with
coreferent mentions sharing the same role label. Note that
pronoun mentions provide direct clues to entity roles.
</figureCaption>
<bodyText confidence="0.9978735">
can naturally incorporate unannotated data, which
further increases accuracy.
</bodyText>
<sectionHeader confidence="0.984877" genericHeader="method">
2 Problem Setting
</sectionHeader>
<bodyText confidence="0.999664526315789">
Figure 1(a) shows an example template-filling
task from the corporate acquisitions domain (Fre-
itag, 1998).1 We have a template of K roles
(PURCHASER, AMOUNT, etc.) and we must iden-
tify which entity (if any) fills each role (CSR Lim-
ited, etc.). Often such problems are modeled at the
mention level, directly labeling individual men-
tions as in Figure 1(b). Indeed, in this data set,
the mention-level perspective is evident in the gold
annotations, which ignore pronominal references.
However, roles in this domain appear in several lo-
cations throughout the document, with pronominal
mentions often carrying the critical information
for template filling. Therefore, Section 3 presents
a model in which entities are explicitly modeled,
naturally merging information across all mention
types and explicitly representing latent structure
very much like the entity-level template structure
from Figure 1(a).
</bodyText>
<footnote confidence="0.9977">
1In Freitag (1998), some of these fields are split in two to
distinguish a full versus abbreviated name, but we ignore this
distinction. Also we ignore the status field as it doesn’t apply
to entities and its meaning is not consistent.
</footnote>
<page confidence="0.90413">
291
</page>
<note confidence="0.74412">
Proceedings of the ACL 2010 Conference Short Papers, pages 291–295,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<table confidence="0.914406315789474">
Purchaser Role
r θr fr
[bought: 0.02, [2: 0.18,
GOV-NSUBJ obtained:0.015, 3:0.12,
acquired: 0.01,...] 1: 0.09,...]
[Inc.: 0.02, [1: 0.19,
HEAD-NAM Corp.:0.015, 2:0.14,
Ltd.: 0.01,...] 0: 0.08,...]
[company: 0.02, [1: 0.02,
MOD-APPOS firm:0.015, 0:0.015,
group: 0.01,...] 2: 0.01,...]
Purchaser Entity
r Lr
HEAD-NAM Google, GOOG
HEAD-NOM company
MOD-NN search, giant
MOD-PREP California
Role Entity Parameters Other Entity Parameters
R1 R2 .... .... RK
</table>
<figure confidence="0.996271772727273">
Document
Role
Priors
Role
Entites
Other
Entities
φ
E1 E2
.... ....
EK
Entity Indicators
1
Z1 Z2
Z3
Zn
Purchaser Mention
M1 M2 M3 ........... Mn
Mentions
r w
HEAD-NAM Google
GOV-NSUBJ bought
</figure>
<figureCaption confidence="0.9977815">
Figure 2: Graphical model depiction of our generative model described in Section 3. Sample values are illustrated for key
parameters and latent variables.
</figureCaption>
<sectionHeader confidence="0.995469" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.993900968253968">
We describe our generative model for a document,
which has many similarities to the coreference-
only model of Haghighi and Klein (2010), but
which integrally models template role-fillers. We
briefly describe the key abstractions of our model.
Mentions: A mention is an observed textual
reference to a latent real-world entity. Mentions
are associated with nodes in a parse tree and are
typically realized as NPs. There are three ba-
sic forms of mentions: proper (NAM), nominal
(NOM), and pronominal (PRO). Each mention M
is represented as collection of key-value pairs.
The keys are called properties and the values are
words. The set of properties utilized here, de-
noted R, are the same as in Haghighi and Klein
(2010) and consist of the mention head, its depen-
dencies, and its governor. See Figure 2 for a con-
crete example. Mention types are trivially deter-
mined from mention head POS tag. All mention
properties and their values are observed.
Entities: An entity is a specific individual or
object in the world. Entities are always latent in
text. Where a mention has a single word for each
property, an entity has a list of signature words.
Formally, entities are mappings from properties
r E R to lists Lr of “canonical” words which that
entity uses for that property.
Roles: The elements we have described so far
are standard in many coreference systems. Our
model performs role-filling by assuming that each
entity is drawn from an underlying role. These
roles include the K template roles as well as ‘junk’
roles to represent entities which do not fill a tem-
plate role (see Section 5.2). Each role R is rep-
resented as a mapping between properties r and
pairs of multinomials (Br, fr). Br is a unigram dis-
tribution of words for property r that are seman-
tically licensed for the role (e.g., being the sub-
ject of “acquired” for the ACQUIRED role). fr is a
“fertility” distribution over the integers that char-
acterizes entity list lengths. Together, these distri-
butions control the lists Lr for entities which in-
stantiate the role.
We first present a broad sketch of our model’s
components and then detail each in a subsequent
section. We temporarily assume that all men-
tions belong to a template role-filling entity; we
lift this restriction in Section 5.2. First, a se-
mantic component generates a sequence of enti-
ties E = (Ei, ... , EK), where each Ei is gen-
erated from a corresponding role Ri. We use
R = (Ri, ... , RK) to denote the vector of tem-
plate role parameters. Note that this work assumes
that there is a one-to-one mapping between entities
and roles; in particular, at most one entity can fill
each role. This assumption is appropriate for the
domain considered here.
Once entities have been generated, a dis-
course component generates which entities will be
evoked in each of the n mention positions. We
represent these choices using entity indicators de-
noted by Z = (Zi, ... , Zn). This component uti-
lizes a learned global prior 0 over roles. The Zi in-
</bodyText>
<page confidence="0.98136">
292
</page>
<bodyText confidence="0.9999818">
dicators take values in 1, ... , K indicating the en-
tity number (and thereby the role) underlying the
ith mention position. Finally, a mention genera-
tion component renders each mention conditioned
on the underlying entity and role. Formally:
</bodyText>
<equation confidence="0.986464">
P(E, Z, M|R, φ) =
!P(Ei|Ri) [Semantic, Sec. 3.1]
⎞P(Zj|Z&lt;j, φ) ⎠[Discourse, Sec. 3.2]
⎞
P(Mj|EZ,, RZ,) ⎠[Mention, Sec. 3.3]
</equation>
<subsectionHeader confidence="0.995338">
3.1 Semantic Component
</subsectionHeader>
<bodyText confidence="0.999895428571429">
Each role R generates an entity E as follows: for
each mention property r, a word list, Lr, is drawn
by first generating a list length from the corre-
sponding fr distribution in R.2 This list is then
populated by an independent draw from R’s uni-
gram distribution θr. Formally, for each r E R, an
entity word list is drawn according to,3
</bodyText>
<equation confidence="0.9410235">
P(Lr|R) = P(len(Lr)|fr) Y P(w|θr)
w∈L,.
</equation>
<subsectionHeader confidence="0.997022">
3.2 Discourse Component
</subsectionHeader>
<bodyText confidence="0.9997795">
The discourse component draws the entity indica-
tor Zj for the jth mention according to,
</bodyText>
<equation confidence="0.9998195">
P (Zj|Z&lt;j, φ) = ( P(Zj|φ), if non-pronominal
P j, 1A = Zj,]P(j0|j), o.w.
</equation>
<bodyText confidence="0.9998745">
When the jth mention is non-pronominal, we draw
Zj from φ, a global prior over the K roles. When
Mj is a pronoun, we first draw an antecedent men-
tion position j0, such that j0 &lt; j, and then we set
Zj = Zj,. The antecedent position is selected ac-
cording to the distribution,
</bodyText>
<equation confidence="0.865745">
P(j0|j) a exp{−γTREEDIST(j0, j)}
</equation>
<bodyText confidence="0.9904345">
where TREEDIST(j0,j) represents the tree distance
between the parse nodes for Mj and Mj,.4 Mass is
</bodyText>
<footnote confidence="0.989369666666667">
2There is one exception: the sizes of the proper and nom-
inal head property lists are jointly generated, but their word
lists are still independently populated.
3While, in principle, this process can yield word lists with
duplicate words, we constrain the model during inference to
not allow that to occur.
4Sentence parse trees are merged into a right-branching
document parse tree. This allows us to extend tree distance to
inter-sentence nodes.
</footnote>
<bodyText confidence="0.984670333333333">
restricted to antecedent mention positions j0 which
occur earlier in the same sentence or in the previ-
ous sentence.5
</bodyText>
<subsectionHeader confidence="0.999257">
3.3 Mention Generation
</subsectionHeader>
<bodyText confidence="0.997187285714286">
Once the entity indicator has been drawn, we gen-
erate words associated with mention conditioned
on the underlying entity E and role R. For each
mention property r associated with the mention,
a word w is drawn utilizing E’s word list Lr as
well as the multinomials (fr, θr) from role R. The
word w is drawn according to,
</bodyText>
<equation confidence="0.9472535">
P(w|E, R)=(1 − αr)1 w E Lr] + αrP(w |θr)
len (Lr)
</equation>
<bodyText confidence="0.995692333333333">
For each property r, there is a hyper-parameter αr
which interpolates between selecting a word uni-
formly from the entity list Lr and drawing from
the underlying role distribution θr. Intuitively, a
small αr indicates that an entity prefers to re-use a
small number of words for property r. This is typi-
cally the case for proper and nominal heads as well
as modifiers. At the other extreme, setting αr to 1
indicates the property isn’t particular to the entity
itself, but rather always drawn from the underly-
ing role distribution. We set αr to 1 for pronoun
heads as well as for the governor properties.
</bodyText>
<sectionHeader confidence="0.97442" genericHeader="method">
4 Learning and Inference
</sectionHeader>
<bodyText confidence="0.999995428571429">
Since we will make use of unannotated data (see
Section 5), we utilize a variational EM algorithm
to learn parameters R and φ. The E-Step re-
quires the posterior P(E, Z|R, M, φ), which is
intractable to compute exactly. We approximate
it using a surrogate variational distribution of the
following factored form:
</bodyText>
<equation confidence="0.9829788">
! ⎛ ⎞
Yn
qi(Ei) ⎝ rj(Zj) ⎠
j=1
Each
</equation>
<bodyText confidence="0.8393953">
is a distribution over the entity in-
dicator for mention Mj, which approximates the
true posterior of Zj. Similarly,
approxi-
mates the posterior over entity
which is asso-
ciated with role
As is standard, we iteratively
update each component distribution to minimize
KL-divergence, fixing all other distri
</bodyText>
<figure confidence="0.626322315789474">
rj(Zj)
qi(Ei)
Ei
Ri.
butions:
qi +— argmin KL(Q(E, Z)|P(E, Z|M, R, φ)
qz
a exp{EQ/qz lnP(E, Z|M, R, φ))}
5The sole parameter 7 is fixed at 0.1.
YK
i=1
⎛
Yn
⎝j=1
⎛
Yn
⎝j=1
Q(E, Z) = YK
i=1
</figure>
<page confidence="0.807688">
293
</page>
<figure confidence="0.975880285714286">
Ment Acc. Ent. Acc.
INDEP
JOINT
JOINT+PRO
60.0 43.7
64.6 54.2
68.2 57.8
</figure>
<tableCaption confidence="0.9535105">
Table 1: Results on corporate acquisition tasks with given
role mention boundaries. We report mention role accuracy
and entity role accuracy (correctly labeling all entity men-
tions).
</tableCaption>
<bodyText confidence="0.978398">
For example, the update for a non-pronominal
entity indicator component rj(·) is given by:6
</bodyText>
<equation confidence="0.909278">
ln rj(z) ∝ EQ/rj ln P(E, Z, M|R, φ)
∝ Eqz ln (P(z|φ)P(Mj|Ez, Rz))
= ln P(z|φ) + Eqz ln P(Mj|Ez, Rz)
</equation>
<bodyText confidence="0.9996195">
A similar update is performed on pronominal en-
tity indicator distributions, which we omit here for
space. The update for variational entity distribu-
tion is given by:
</bodyText>
<equation confidence="0.996571666666667">
lnqi(ei) ∝ EQ/qz lnP(E, Z, M|R, φ)
�
�
∝ E{rj� ln �P(ei|Ri) P(Mj|ei, Ri)
j
j:Z
=i
= ln P(ei|Ri) + rj(i) ln P(Mj|ei, Ri)
j
</equation>
<bodyText confidence="0.98356725">
to several sam-
pled entities. We obtain entity samples by sam-
pling mention entity indicators according to rj.
For a given sample, we assume that Ei consists
of the non-pronominal head words and modifiers
of mentions such that Zj has sampled value i.
During the E-Step, we perform 5 iterations of
updating each variational factor, which results in
an approximate posterior distribution. Using ex-
pectations from this approximate posterior, our M-
Step is relatively straightforward. The role param-
eters Ri are computed from the
and
distributions, and the global role prior
from the
non-pronominal components of
</bodyText>
<equation confidence="0.8624354">
qi(ei)
qi(ei)
rj(z)
φ
rj(z).
</equation>
<sectionHeader confidence="0.998945" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.891831">
We present results on the corporate acquisitions
task, which consists of 600 annotated documents
split into a 300/300 train/test split. We use 50
training documents as a development set. In all
tecedent to a pronoun.
effe et
al., 2006).
</bodyText>
<subsectionHeader confidence="0.999178">
5.1 Gold Role Boundaries
</subsectionHeader>
<bodyText confidence="0.999722375">
294 documents, proper and (usually) nominal men-
tions are annotated with roles, while pronouns are
not. We preprocess each document identically to
Haghighi and Klein (2010): we sentence-segment
using the OpenNLP toolkit, parse sentences with
the Berkeley Parser (Petrov et al., 2006), and ex-
tract mention properties from parse trees and the
Stanford Dependency Extractor (de Marn
</bodyText>
<equation confidence="0.496807">
fies each
</equation>
<bodyText confidence="0.992214619047619">
role. It uses features as similar
as possible to the generative model (and more), in-
cluding the head word, typed dependencies of the
head, various tree features, governing word, and
several conjunctions of these features as well as
coarser versions of lexicalized features. This sys-
tem yields 60.0 mention labeling accuracy (see Ta-
ble 1). The primary difficulty in classification is
the disambiguation amongst the acquired, seller,
and purchaser roles, which have similar internal
structure, and differ primarily in their semantic
contexts. Our entity-centered model, JOINT in Ta-
ble 1, has no latent variables at training time in this
setting, since each role maps to a unique entity.
This model yields 64.6, outperforming
During development, we noted that often the
most direct evidence of the role of an entity was
associated with pronoun usage (see the first
in Figure 1). Training our model with pronominal
mentions, whose roles are latent vari
mention’s
</bodyText>
<equation confidence="0.589709">
INDEP.7
“it”
ables at train-
</equation>
<bodyText confidence="0.961174">
ing time, improves accuracy to 68.2.8
</bodyText>
<subsectionHeader confidence="0.781021">
Task
</subsectionHeader>
<bodyText confidence="0.9793218">
mentions from a parse tree using a heuri
stic ap-
It is intractable to enumerate all possible entities
ei (each consisting of several sets of words). We
instead limit the support of
</bodyText>
<footnote confidence="0.7653275">
6For simplicity of exposition, we omit terms where Mj is
an an
</footnote>
<bodyText confidence="0.999935111111111">
We first consider the simplified task where role
mention boundaries are given. We map each la-
beled token span in training and test data to a parse
tree node that shares the same head. In this set-
ting, the role-filling task is a collective classifica-
tion problem, since we know each mention is fill-
ing some role.
As our baseline, INDEP, we built a maxi-
mum entropy model which independently classi-
</bodyText>
<subsectionHeader confidence="0.9786">
5.2 Full
</subsectionHeader>
<bodyText confidence="0.999713333333333">
We now consider the more difficult setting where
role mention boundaries are not provided at test
time. In this setting, we automatically extract
</bodyText>
<footnote confidence="0.446114111111111">
7We use the mode of the variational posteriors
to
make predictions (see Section 4).
this approach incorrectly assumes that all pro-
nouns have antecedents amongst our given mentions, this did
not appear to degrade performan
rj(Zj)
8While
ce.
</footnote>
<table confidence="0.999707">
P ROLEID F1 P OVERALL F1
R R
INDEP 79.0 65.5 71.6 48.6 40.3 44.0
JOINT+PRO 80.3 69.2 74.3 53.4 46.4 49.7
BEST 80.1 70.1 74.8 57.3 49.2 52.9
</table>
<tableCaption confidence="0.87441225">
Table 2: Results on corporate acquisitions data where men-
tion boundaries are not provided. Systems must determine
which mentions are template role-fillers as well as label them.
ROLE ID only evaluates the binary decision of whether a
mention is a template role-filler or not. OVERALL includes
correctly labeling mentions. Our BEST system, see Sec-
tion 5, adds extra unannotated data to our JOINT+PRO sys-
tem.
</tableCaption>
<bodyText confidence="0.999722285714285">
proach. Our mention extraction procedure yields
95% recall over annotated role mentions and 45%
precision.9 Using extracted mentions as input, our
task is to label some subset of the mentions with
template roles. Since systems can label mentions
as non-role bearing, only recall is critical to men-
tion extraction. To adapt INDEP to this setting, we
first use a binary classifier trained to distinguish
role-bearing mentions. The baseline then classi-
fies mentions which pass this first phase as before.
We add ‘junk’ roles to our model to flexibly model
entities that do not correspond to annotated tem-
plate roles. During training, extracted mentions
which are not matched in the labeled data have
posteriors which are constrained to be amongst the
‘junk’ roles.
We first evaluate role identification (ROLE ID in
Table 2), the task of identifying mentions which
play some role in the template. The binary clas-
sifier for INDEP yields 71.6 FI. Our JOINT+PRO
system yields 74.3. On the task of identifying and
correctly labeling role mentions, our model out-
performs INDEP as well (OVERALL in Table 2). As
our model is generative, it is straightforward to uti-
lize totally unannotated data. We added 700 fully
unannotated documents from the mergers and ac-
quisitions portion of the Reuters 21857 corpus.
Training JOINT+PRO on this data as well as our
original training data yields the best performance
(BEST in Table 2).10
To our knowledge, the best previously pub-
lished results on this dataset are from Siefkes
(2008), who report 45.9 weighted FI. Our BEST
system evaluated in their slightly stricter way
yields 51.1.
</bodyText>
<footnote confidence="0.991366666666667">
9Following Patwardhan and Riloff (2009), we match ex-
tracted mentions to labeled spans if the head of the mention
matches the labeled span.
10We scaled expected counts from the unlabeled data so
that they did not overwhelm those from our (partially) labeled
data.
</footnote>
<sectionHeader confidence="0.998666" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9993559">
We have presented a joint generative model of
coreference resolution and role-filling information
extraction. This model makes role decisions at
the entity, rather than at the mention level. This
approach naturally aggregates information across
multiple mentions, incorporates unannotated data,
and yields strong performance.
Acknowledgements: This project is funded in
part by the Office of Naval Research under MURI
Grant No. N000140911081.
</bodyText>
<sectionHeader confidence="0.999361" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999746947368421">
M. C. de Marneffe, B. Maccartney, and C. D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In LREC.
Dayne Freitag and Andrew McCallum. 2000. Infor-
mation extraction with hmm structures learned by
stochastic optimization. In Association for the Ad-
vancement of Artificial Intelligence (AAAI).
Dayne Freitag. 1998. Machine learning for informa-
tion extraction in informal domains.
A. Haghighi and D. Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In North
American Association of Computational Linguistics
(NAACL).
P. Liang and D. Klein. 2007. Structured Bayesian non-
parametric models with variational inference (tuto-
rial). In Association for Computational Linguistics
(ACL).
S. Patwardhan and E. Riloff. 2007. Effective infor-
mation extraction with semantic affinity patterns and
relevant regions. In Joint Conference on Empirical
Methods in Natural Language Processing.
S. Patwardhan and E Riloff. 2009. A unified model of
phrasal and sentential evidence for information ex-
traction. In Empirical Methods in Natural Language
Processing (EMNLP).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433–440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Christian Siefkes. 2008. An Incrementally Train-
able Statistical Approach to Information Extraction:
Based on Token Classification and Rich Context
Model. VDM Verlag, Saarbr¨ucken, Germany, Ger-
many.
</reference>
<page confidence="0.998564">
295
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.845489">
<title confidence="0.999938">An Entity-Level Approach to Information Extraction</title>
<author confidence="0.99968">Aria Haghighi</author>
<affiliation confidence="0.999333">UC Berkeley, CS Division</affiliation>
<email confidence="0.993537">aria42@cs.berkeley.edu</email>
<author confidence="0.999943">Dan Klein</author>
<affiliation confidence="0.998235">UC Berkeley, CS Division</affiliation>
<email confidence="0.998994">klein@cs.berkeley.edu</email>
<abstract confidence="0.986597454545454">We present a generative model of template-filling in which coreference resolution and role assignment are jointly determined. Underlying template roles first generate abstract entities, which in turn generate concrete textual mentions. On the standard corporate acquisitions dataset, joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M C de Marneffe</author>
<author>B Maccartney</author>
<author>C D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<marker>de Marneffe, Maccartney, Manning, 2006</marker>
<rawString>M. C. de Marneffe, B. Maccartney, and C. D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
<author>Andrew McCallum</author>
</authors>
<title>Information extraction with hmm structures learned by stochastic optimization.</title>
<date>2000</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="997" citStr="Freitag and McCallum, 2000" startWordPosition="131" endWordPosition="134">in turn generate concrete textual mentions. On the standard corporate acquisitions dataset, joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20%. 1 Introduction Template-filling information extraction (IE) systems must merge information across multiple sentences to identify all role fillers of interest. For instance, in the MUC4 terrorism event extraction task, the entity filling the individual perpetrator role often occurs multiple times, variously as proper, nominal, or pronominal mentions. However, most template-filling systems (Freitag and McCallum, 2000; Patwardhan and Riloff, 2007) assign roles to individual textual mentions using only local context as evidence, leaving aggregation for post-processing. While prior work has acknowledged that coreference resolution and discourse analysis are integral to accurate role identification, to our knowledge no model has been proposed which jointly models these phenomena. In this work, we describe an entity-centered approach to template-filling IE problems. Our model jointly merges surface mentions into underlying entities (coreference resolution) and assigns roles to those discovered entities. In the</context>
</contexts>
<marker>Freitag, McCallum, 2000</marker>
<rawString>Dayne Freitag and Andrew McCallum. 2000. Information extraction with hmm structures learned by stochastic optimization. In Association for the Advancement of Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Machine learning for information extraction in informal domains.</title>
<date>1998</date>
<contexts>
<context position="2705" citStr="Freitag, 1998" startWordPosition="393" endWordPosition="395">d [S its] [B oil interests] held in [A Delhi Fund]. [P Esso Inc.] did not disclose how much [P they] paid for [A Dehli]. Figure 1: Example of the corporate acquisitions role-filling task. In (a), an example template specifying the entities playing each domain role. In (b), an example document with coreferent mentions sharing the same role label. Note that pronoun mentions provide direct clues to entity roles. can naturally incorporate unannotated data, which further increases accuracy. 2 Problem Setting Figure 1(a) shows an example template-filling task from the corporate acquisitions domain (Freitag, 1998).1 We have a template of K roles (PURCHASER, AMOUNT, etc.) and we must identify which entity (if any) fills each role (CSR Limited, etc.). Often such problems are modeled at the mention level, directly labeling individual mentions as in Figure 1(b). Indeed, in this data set, the mention-level perspective is evident in the gold annotations, which ignore pronominal references. However, roles in this domain appear in several locations throughout the document, with pronominal mentions often carrying the critical information for template filling. Therefore, Section 3 presents a model in which entit</context>
</contexts>
<marker>Freitag, 1998</marker>
<rawString>Dayne Freitag. 1998. Machine learning for information extraction in informal domains.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In North American Association of Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="4832" citStr="Haghighi and Klein (2010)" startWordPosition="718" endWordPosition="721"> Google, GOOG HEAD-NOM company MOD-NN search, giant MOD-PREP California Role Entity Parameters Other Entity Parameters R1 R2 .... .... RK Document Role Priors Role Entites Other Entities φ E1 E2 .... .... EK Entity Indicators 1 Z1 Z2 Z3 Zn Purchaser Mention M1 M2 M3 ........... Mn Mentions r w HEAD-NAM Google GOV-NSUBJ bought Figure 2: Graphical model depiction of our generative model described in Section 3. Sample values are illustrated for key parameters and latent variables. 3 Model We describe our generative model for a document, which has many similarities to the coreferenceonly model of Haghighi and Klein (2010), but which integrally models template role-fillers. We briefly describe the key abstractions of our model. Mentions: A mention is an observed textual reference to a latent real-world entity. Mentions are associated with nodes in a parse tree and are typically realized as NPs. There are three basic forms of mentions: proper (NAM), nominal (NOM), and pronominal (PRO). Each mention M is represented as collection of key-value pairs. The keys are called properties and the values are words. The set of properties utilized here, denoted R, are the same as in Haghighi and Klein (2010) and consist of t</context>
<context position="13272" citStr="Haghighi and Klein (2010)" startWordPosition="2177" endWordPosition="2180">ely straightforward. The role parameters Ri are computed from the and distributions, and the global role prior from the non-pronominal components of qi(ei) qi(ei) rj(z) φ rj(z). 5 Experiments We present results on the corporate acquisitions task, which consists of 600 annotated documents split into a 300/300 train/test split. We use 50 training documents as a development set. In all tecedent to a pronoun. effe et al., 2006). 5.1 Gold Role Boundaries 294 documents, proper and (usually) nominal mentions are annotated with roles, while pronouns are not. We preprocess each document identically to Haghighi and Klein (2010): we sentence-segment using the OpenNLP toolkit, parse sentences with the Berkeley Parser (Petrov et al., 2006), and extract mention properties from parse trees and the Stanford Dependency Extractor (de Marn fies each role. It uses features as similar as possible to the generative model (and more), including the head word, typed dependencies of the head, various tree features, governing word, and several conjunctions of these features as well as coarser versions of lexicalized features. This system yields 60.0 mention labeling accuracy (see Table 1). The primary difficulty in classification is</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>A. Haghighi and D. Klein. 2010. Coreference resolution in a modular, entity-centered model. In North American Association of Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>D Klein</author>
</authors>
<title>Structured Bayesian nonparametric models with variational inference (tutorial).</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<marker>Liang, Klein, 2007</marker>
<rawString>P. Liang and D. Klein. 2007. Structured Bayesian nonparametric models with variational inference (tutorial). In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>E Riloff</author>
</authors>
<title>Effective information extraction with semantic affinity patterns and relevant regions.</title>
<date>2007</date>
<booktitle>In Joint Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1027" citStr="Patwardhan and Riloff, 2007" startWordPosition="135" endWordPosition="138">xtual mentions. On the standard corporate acquisitions dataset, joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20%. 1 Introduction Template-filling information extraction (IE) systems must merge information across multiple sentences to identify all role fillers of interest. For instance, in the MUC4 terrorism event extraction task, the entity filling the individual perpetrator role often occurs multiple times, variously as proper, nominal, or pronominal mentions. However, most template-filling systems (Freitag and McCallum, 2000; Patwardhan and Riloff, 2007) assign roles to individual textual mentions using only local context as evidence, leaving aggregation for post-processing. While prior work has acknowledged that coreference resolution and discourse analysis are integral to accurate role identification, to our knowledge no model has been proposed which jointly models these phenomena. In this work, we describe an entity-centered approach to template-filling IE problems. Our model jointly merges surface mentions into underlying entities (coreference resolution) and assigns roles to those discovered entities. In the generative process proposed h</context>
</contexts>
<marker>Patwardhan, Riloff, 2007</marker>
<rawString>S. Patwardhan and E. Riloff. 2007. Effective information extraction with semantic affinity patterns and relevant regions. In Joint Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>E Riloff</author>
</authors>
<title>A unified model of phrasal and sentential evidence for information extraction.</title>
<date>2009</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="17750" citStr="Patwardhan and Riloff (2009)" startWordPosition="2914" endWordPosition="2917">g role mentions, our model outperforms INDEP as well (OVERALL in Table 2). As our model is generative, it is straightforward to utilize totally unannotated data. We added 700 fully unannotated documents from the mergers and acquisitions portion of the Reuters 21857 corpus. Training JOINT+PRO on this data as well as our original training data yields the best performance (BEST in Table 2).10 To our knowledge, the best previously published results on this dataset are from Siefkes (2008), who report 45.9 weighted FI. Our BEST system evaluated in their slightly stricter way yields 51.1. 9Following Patwardhan and Riloff (2009), we match extracted mentions to labeled spans if the head of the mention matches the labeled span. 10We scaled expected counts from the unlabeled data so that they did not overwhelm those from our (partially) labeled data. 6 Conclusion We have presented a joint generative model of coreference resolution and role-filling information extraction. This model makes role decisions at the entity, rather than at the mention level. This approach naturally aggregates information across multiple mentions, incorporates unannotated data, and yields strong performance. Acknowledgements: This project is fun</context>
</contexts>
<marker>Patwardhan, Riloff, 2009</marker>
<rawString>S. Patwardhan and E Riloff. 2009. A unified model of phrasal and sentential evidence for information extraction. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="13383" citStr="Petrov et al., 2006" startWordPosition="2193" endWordPosition="2196">the non-pronominal components of qi(ei) qi(ei) rj(z) φ rj(z). 5 Experiments We present results on the corporate acquisitions task, which consists of 600 annotated documents split into a 300/300 train/test split. We use 50 training documents as a development set. In all tecedent to a pronoun. effe et al., 2006). 5.1 Gold Role Boundaries 294 documents, proper and (usually) nominal mentions are annotated with roles, while pronouns are not. We preprocess each document identically to Haghighi and Klein (2010): we sentence-segment using the OpenNLP toolkit, parse sentences with the Berkeley Parser (Petrov et al., 2006), and extract mention properties from parse trees and the Stanford Dependency Extractor (de Marn fies each role. It uses features as similar as possible to the generative model (and more), including the head word, typed dependencies of the head, various tree features, governing word, and several conjunctions of these features as well as coarser versions of lexicalized features. This system yields 60.0 mention labeling accuracy (see Table 1). The primary difficulty in classification is the disambiguation amongst the acquired, seller, and purchaser roles, which have similar internal structure, a</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Siefkes</author>
</authors>
<title>An Incrementally Trainable Statistical Approach to Information Extraction: Based on Token Classification and Rich Context Model.</title>
<date>2008</date>
<publisher>VDM Verlag,</publisher>
<location>Saarbr¨ucken, Germany, Germany.</location>
<contexts>
<context position="17610" citStr="Siefkes (2008)" startWordPosition="2895" endWordPosition="2896">binary classifier for INDEP yields 71.6 FI. Our JOINT+PRO system yields 74.3. On the task of identifying and correctly labeling role mentions, our model outperforms INDEP as well (OVERALL in Table 2). As our model is generative, it is straightforward to utilize totally unannotated data. We added 700 fully unannotated documents from the mergers and acquisitions portion of the Reuters 21857 corpus. Training JOINT+PRO on this data as well as our original training data yields the best performance (BEST in Table 2).10 To our knowledge, the best previously published results on this dataset are from Siefkes (2008), who report 45.9 weighted FI. Our BEST system evaluated in their slightly stricter way yields 51.1. 9Following Patwardhan and Riloff (2009), we match extracted mentions to labeled spans if the head of the mention matches the labeled span. 10We scaled expected counts from the unlabeled data so that they did not overwhelm those from our (partially) labeled data. 6 Conclusion We have presented a joint generative model of coreference resolution and role-filling information extraction. This model makes role decisions at the entity, rather than at the mention level. This approach naturally aggregat</context>
</contexts>
<marker>Siefkes, 2008</marker>
<rawString>Christian Siefkes. 2008. An Incrementally Trainable Statistical Approach to Information Extraction: Based on Token Classification and Rich Context Model. VDM Verlag, Saarbr¨ucken, Germany, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>