<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.581824">
<title confidence="0.998767">
Speech Communication in the Wild
</title>
<author confidence="0.973184">
Martin Cooke
</author>
<affiliation confidence="0.905244333333333">
Language and Speech Laboratory
University of the Basque Country
Ikerbasque (Basque Science Foundation)
</affiliation>
<email confidence="0.988689">
m.cooke@ikerbasque.org
</email>
<sectionHeader confidence="0.977279" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999891153846154">
Much of what we know about speech perception comes from laboratory studies with clean, canonical
speech, ideal listeners and artificial tasks. But how do interlocutors manage to communicate effec-
tively in the seemingly less-than-ideal conditions of everyday listening, which frequently involve try-
ing to make sense of speech while listening in a non-native language, or in the presence of competing
sound sources, or while multitasking? In this talk I’ll examine the effect of real-world conditions on
speech perception and quantify the contributions made by factors such as binaural hearing, visual in-
formation and prior knowledge to speech communication in noise. I’ll present a computational model
which trades stimulus-related cues with information from learnt speech models, and examine how
well it handles both energetic and informational masking in a two-sentence separation task. Speech
communication also involves listening-while-talking. In the final part of the talk I’ll describe some
ways in which speakers might be making communication easier for their interlocutors, and demon-
strate the application of these principles to improving the intelligibility of natural and synthetic speech
in adverse conditions.
</bodyText>
<page confidence="0.886459">
1
</page>
<note confidence="0.8095255">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, page 1,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.360495">
<title confidence="0.998227">Speech Communication in the Wild</title>
<author confidence="0.99888">Martin Cooke</author>
<affiliation confidence="0.955293666666667">Language and Speech University of the Basque Ikerbasque (Basque Science</affiliation>
<email confidence="0.976107">m.cooke@ikerbasque.org</email>
<abstract confidence="0.999688785714286">Much of what we know about speech perception comes from laboratory studies with clean, canonical speech, ideal listeners and artificial tasks. But how do interlocutors manage to communicate effectively in the seemingly less-than-ideal conditions of everyday listening, which frequently involve trying to make sense of speech while listening in a non-native language, or in the presence of competing sound sources, or while multitasking? In this talk I’ll examine the effect of real-world conditions on speech perception and quantify the contributions made by factors such as binaural hearing, visual information and prior knowledge to speech communication in noise. I’ll present a computational model which trades stimulus-related cues with information from learnt speech models, and examine how well it handles both energetic and informational masking in a two-sentence separation task. Speech communication also involves listening-while-talking. In the final part of the talk I’ll describe some ways in which speakers might be making communication easier for their interlocutors, and demonstrate the application of these principles to improving the intelligibility of natural and synthetic speech in adverse conditions.</abstract>
<note confidence="0.627780666666667">1 of the 13th Conference of the European Chapter of the Association for Computational page 1, France, April 23 - 27 2012. Association for Computational Linguistics</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>