<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.220648">
<title confidence="0.961707">
Summarizing Neonatal Time Series Data
</title>
<author confidence="0.970998">
Somayajulu G. Sripada, Ehud Reiter, Jim Hunter and Jin Yu
</author>
<affiliation confidence="0.99907">
Department of Computing Science
University of Aberdeen, Aberdeen, U.K.
</affiliation>
<email confidence="0.998261">
fssripada,ereiter,jhunter,jyul@csd.abdn.ac.uk
</email>
<sectionHeader confidence="0.995629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994942">
We describe our investigations in gener-
ating textual summaries of physiological
time series data to aid medical personnel
in monitoring babies in neonatal intensive
care units. Our studies suggest that sum-
marization is a communicative task that
requires data analysis techniques for de-
termining the content of the summary.
We describe a prototype system that
summarizes physiological time series.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999045">
Time series data is ubiquitous — any measure-
ment humans make over a period of time pro-
duces a time series. We are building a system to
summarize physiological times series data such
as heart rate, and blood pressure measured in
neonatal intensive care units.
</bodyText>
<sectionHeader confidence="0.986643" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999874578947369">
The SumTimE project aims to develop generic
techniques to produce textual summaries of time
series data (Sripada et al, 2001). We initially
worked in two domains, meteorology and gas
turbines. In meteorology we generate textual
weather forecasts from weather data such as wind
speed, wind direction, and wave heights. In gas
turbines we generate textual summaries of unex-
pected patterns in sensor data such as exhaust
temperature, liquid fuel flow, and turbine speed.
In each of these domains we are working with
industrial collaborators and have built prototype
systems.
Using the experience from both these domains
we have now started working on physiological
time series data in collaboration with NEONATE
(Ewing et al, 2002) project. The main objective
of NEONATE has been to produce a decision sup-
port system for the medical personnel working in
the neonatal intensive care unit (NICU).
In the NEONATE project, a research nurse has
been employed to collect data from the neonatal
intensive care unit at Simpson Maternity Hospi-
tal, Edinburgh using a software tool, BabyWatch
(Ewing et al, 2002). Physiological parameters
such as heart rate, mean blood pressure and tem-
perature are recorded at one-second frequency
using various probes attached to the baby.
In order to monitor the health of babies, medi-
cal personnel (doctors and nurses) working in the
neonatal unit are required to inspect such data
continually. Currently they use visual displays of
the data. Our system will generate textual sum-
maries of these data as an aid to the medical per-
sonnel. We believe that interpreting textual
summaries is lot quicker and does not require
much mathematical (statistical) knowledge when
compared to interpreting graphical displays.
</bodyText>
<sectionHeader confidence="0.992971" genericHeader="method">
3 Knowledge Acquisition
</sectionHeader>
<bodyText confidence="0.998879833333333">
We have carried out a variety of knowledge ac-
quisition (KA) activities using multiple tech-
niques developed in the expert system
community (Scott, Clayton, and Gibson, 1991) to
understand how humans perform data summari-
zation.
</bodyText>
<page confidence="0.977745">
167
</page>
<figure confidence="0.9987978">
60.0
BM(RD)
oth,„
20.0
27 Feb 96
</figure>
<figureCaption confidence="0.999988">
Figure 1. Plot of mean blood pressure
</figureCaption>
<bodyText confidence="0.972234173913044">
Figure 1 shows a time series plot of mean
blood pressure sampled every second for three
hours. Figure 2 shows its summary extracted
from a small corpus of human written summaries
we analyzed. The summary text in Figure 2 has
the doctor&apos;s interpretation of the data (for in-
stance, .... this is an inadequate blood pressure
....&apos; and `.. and I suspect that dopamine has been
started ...&apos;) interwoven with pure data descrip-
tion (for instance, &apos;...BP is fairly stable at round
about 30kpa &apos;).
On the BP trace the BP is fairly stable at round
about 30kpa until 04:20 with the exception of
the blood sampling artifact at just about 04:08.
This is an inadequate blood pressure and has
fallen further at 04:20 and I suspect that dopa-
mine has been started at this point because from
about 04:23 there is a steady increase in the BP
until 04:50 when the BP is now 40. This is
much more adequate. There are in some oscil-
lations presumably as the infusion rate of do-
pamine has been turned down until the BP
settles down to round about 34.
</bodyText>
<figureCaption confidence="0.96734">
Figure 2. Human written summary for the data shown
in Figure 1.
</figureCaption>
<bodyText confidence="0.993374">
Based on our KA studies we have made a
number of observations about neonatal data
summarization. A few of them are:
</bodyText>
<listItem confidence="0.9830854">
• Raw data contains a number of artifacts
due to external events such as baby han-
dling and blood sampling. These artifacts
need to be separated from the input data
before summarizing. The example data
</listItem>
<bodyText confidence="0.853982">
shown in Figure 1, contains one blood
sampling artifact at 4:08.
</bodyText>
<listItem confidence="0.98995075">
• Summaries should report rises and falls in
the data.
• Summaries should report actual numerical
values of the parameter being summarized.
</listItem>
<bodyText confidence="0.997483923076923">
Artifact separation was not required in the
other two domains; it was unique to neonatal
data. One of the experts, with whom we did KA
explained that physiological data without arti-
facts reflect the true physiology of the underlying
baby. He explained further that artifact data could
be interesting in its own right if summarized
separately because such summaries show how the
underlying baby is reacting to the external ac-
tions.
Interestingly, we have made some general ob-
servations about data summarization across all
the three domains.
</bodyText>
<listItem confidence="0.997609333333334">
• Summarization needs some domain
knowledge reflecting how data will be
used. In the domain of neonatal care it is in
the form of knowledge about artifacts. In
the domain of meteorology it is in the form
of knowledge of what is important. For ex-
ample, changes in wind speeds and direc-
tions are important in marine forecasts but
not in public forecasts unless gales are
predicted. Finally in the domain of gas tur-
bine it is in the form of important patterns.
For instance, damped oscillations and steps
are significant for monitoring turbines.
• This knowledge, however, can be inte-
grated into standard data analysis algo-
</listItem>
<page confidence="0.994608">
168
</page>
<bodyText confidence="0.9998626">
rithms. In the domain of meteorology user
thresholds have been used for determining
stopping criterion for segmentation. In the
domain of gas turbines domain knowledge
has been used for classifying patterns.
</bodyText>
<sectionHeader confidence="0.972408" genericHeader="method">
4 System Architecture
</sectionHeader>
<bodyText confidence="0.989855666666667">
Our system follows the pipeline architecture for
text generation (Reiter and Dale, 2000) as shown
in Figure 3.
</bodyText>
<figureCaption confidence="0.998514">
Figure 3. Architecture of our summarization system
</figureCaption>
<bodyText confidence="0.9999805">
The first module, artifact separation is respon-
sible for detecting and removing artifacts due to
external activities such as blood sampling and
baby handling. Artifact detection in a signal de-
serves a separate study in its own right. However,
in SumTimE we are initially using a median filter
and an impossible value filter developed in our
collaborator project NEONATE.
Document planning is responsible for selecting
the &apos;important&apos; data points from the input data
and to organize them into a paragraph. We de-
scribe this module in greater detail in 4.1. The
third module, micro planning is responsible for
lexical selection and aggregation. Finally the
fourth module, realization is responsible for gen-
erating the grammatical output. We have used the
small corpus we collected from NEONATE, to
build the micro planner and realizer.
</bodyText>
<subsectionHeader confidence="0.999853">
4.1 Content Selection and Segmentation
</subsectionHeader>
<bodyText confidence="0.999964426229509">
The most important question in summarization is
&apos;what data points from the input should be in-
cluded in the summary9&apos; Any model of summari-
zation needs to find ways to reduce the size of the
input data set (or improve its accessibility) with-
out significantly altering its content (or informa-
tiveness). This process is sensitive to the domain
constraints such as limits on parameter values. It
is clear from our own studies on data summari-
zation and also from the earlier studies by others
(Shahar, 1997; Boyd, 1998; Kulkich, 1983) that
data summarization needs data analysis to deter-
mine the trends and patterns present in the data
set. RESUME (Shahar, 1997) uses knowledge
based temporal abstraction for producing ab-
stractions of clinical data. TREND (Boyd, 1998)
uses wavelets to analyze archives of weather data
to produce weather summaries. ANA (Kulkich,
1983) uses a combination of arithmetic computa-
tions and pattern matching techniques to analyse
raw data from the Dow Jones News service data-
base. SUMTIME-MOUSAM (Sripada et al, 2002)
used segmentation of input weather data to de-
termine intervals with similar trends.
Upon manual inspection of corpus texts we felt
that segmentation should work with neonatal
data. Segmentation is the process of fitting linear
segments to an input data series keeping the
maximum error introduced in segments to be
lower than the user defined value. There are
many algorithms for segmentation developed in
the KDD community. These algorithms differ
from each other in the control information they
use and the way they process data (such as top-
down and bottom-up).
We have selected one of them known as the
bottom-up algorithm. This algorithm has been
explained in great detail in (Keogh et al, 2001)
and will not be described here. According to Ke-
ogh&apos;s description, the number of segments pro-
duced (which determines the detail to which the
data is summarized) depends upon a user-
specified limit. In our case, this limit cannot be
the same for all segments. Segments joining
smaller values might have different error limit
compared to those that join larger values. These
user-defined limits (thresholds) control the seg-
mentation process in a way suited for summari-
zation. In general, data analysis algorithms such
as segmentation need to be adapted to suit the
summarization requirements (Sripada et al,
2002). For the initial prototype we have assumed
a variety of control values and produced output
summaries for each. We intend to obtain feed-
back on this from the doctors.
Given an input time series, data analysis such
as segmentation produces what we call a &apos;sum-
mary series&apos;. In our case, summary series con-
tains intervals with similar trend. In some cases,
content for the summary could be derived from
all the intervals in the summary series. However,
</bodyText>
<figure confidence="0.974231875">
Artefact
Separation
Doe.- Micro-
planning planning
Inp
Data
Realization Iputput
Text
</figure>
<page confidence="0.99622">
169
</page>
<bodyText confidence="0.999858666666667">
as we have observed in the domain of meteorol-
ogy, we have to include information related to
only &apos;significant&apos; intervals in the summary. In the
neonatal domain we need to obtain domain spe-
cific knowledge for identifying significant seg-
ments (intervals).
</bodyText>
<figureCaption confidence="0.743862428571429">
Initially BP is stable around 30 kpa until
4:23:14. In the next 28 minutes it gradually
rises to 41 kpa. It gradually falls to 34 kpa by
5:59:59.
Figure 4. Output of our system with limit = 10
BP is stable around 30 kpa until 5:59:59.
Figure 5. Output of our system with limit = 30
</figureCaption>
<bodyText confidence="0.9996714">
Figures 4 and 5 show example output of our
system running with different limit values. In this
paper we are interested in producing purely de-
scriptive textual summaries of neonatal data.
Human written summary shown in Figure 2 in-
cludes interpretative parts interwoven with the
descriptive parts. Producing interpretative sum-
maries of data requires lot of expert domain
knowledge. In the current work we do not want
to get into building specialist domain knowledge.
</bodyText>
<sectionHeader confidence="0.997415" genericHeader="method">
5 Planned Experiments
</sectionHeader>
<bodyText confidence="0.999974714285714">
We plan to conduct small pilot tests with our
software, to get general feedback on how useful
the summaries are. These would be performed
off-ward, and would involve a small number of
doctors looking at generated summaries and sug-
gesting improvements (revisions), and perhaps
making general comments as well.
</bodyText>
<subsectionHeader confidence="0.96406">
5.1 Experimental Evaluation
</subsectionHeader>
<bodyText confidence="0.999918416666667">
When our system is fully developed, we would
like to do a proper experimental evaluation. For
example, we could set up some kind of diagnosis
task, where doctors examine data from a particu-
lar baby and diagnose what is wrong with the
baby (or say whether the baby has or does not
have a particular problem?). Then we could ask a
group of doctors to do this task with (a) just
graphic visualizations and (b) graphic visualiza-
tions and text summaries, and see if there was
any significant difference in accuracy, time to
make diagnosis, or confidence in decision.
</bodyText>
<sectionHeader confidence="0.998618" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999990625">
We have described our work on summarizing
physiological data from a neonatal intensive care.
Content selection used segmentation (an existing
data analysis technique) controlled by domain
knowledge in a similar way to other prototypes.
This suggests that perhaps this is a generic ap-
proach that could be applied to summarizing
many types of time series data.
</bodyText>
<sectionHeader confidence="0.999188" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999931371428571">
Sarah Boyd. 1998. TREND: a system for generating intelli-
gent descriptions of time series data. In Proceedings of
the IEEE International Conference on Intelligent Proc-
essing Systems (ICIPS-1998).
Ewing Gary, Ferguson Lindsey, Freer Yvonne, Hunter Jim
and McIntosh Neil 2002. Observational Data Acquired
on a Neonatal Intensive Care Unit, Technical Report
AUCS/TR0205, Dept. of Comp. Science, Univ. of Aber-
deen.
Eamonn Keogh, Selina Chu, David Hart and Michael Paz-
zani. 2001. An Online Algorithm for Segmenting Time
Series. In: Proceedings of IEEE International Confer-
ence on Data Mining„ pp 289-296.
Karen Kukich. 1983. Design and implementation of a
knowledge-based report generator. In: Proceedings of the
21st Annual Meeting of the Association for Computa-
tional Linguistics (ACL-1983), pp 145-150.
Ehud Reiter and Robert Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press.
A. Carlisle Scott, Jan E. Clayton, and Elizabeth L.
Gibson. 1991. Practical Guide to Knowledge Ac-
quisition. Addison-Wesley.
Yuval Shahar. 1997. Framework for knowledge based
temporal abstraction. Artificial Intelligence, 90:79-
133.
Somayajulu, G. Sripada, Ehud Reiter, Jim Hunter and Jin
Yu. 2001 Modelling the task of Summarising Time Se-
ries Data using KA Techniques. In: Macintosh, A.,
Moulton, M. and Preece, A. (ed) Proceedings of E52001,
pp 183 —196.
Somayajulu, G. Sripada, Ehud Reiter, Jim Hunter and Jin
Yu. 2002 Segmenting Time Series for Weather
Forecasting. In: Macintosh, A., Ellis, R. and Coe-
nen, F. (ed) Proceedings of ES2002, pp. 193-206.
</reference>
<page confidence="0.997426">
170
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.993206">
<title confidence="0.999496">Summarizing Neonatal Time Series Data</title>
<author confidence="0.999656">Somayajulu G Sripada</author>
<author confidence="0.999656">Ehud Reiter</author>
<author confidence="0.999656">Jim Hunter</author>
<author confidence="0.999656">Jin Yu</author>
<affiliation confidence="0.99821">Department of Computing Science University of Aberdeen, Aberdeen, U.K.</affiliation>
<email confidence="0.999005">fssripada,ereiter,jhunter,jyul@csd.abdn.ac.uk</email>
<abstract confidence="0.999871909090909">our investigations in generating textual summaries of physiological time series data to aid medical personnel in monitoring babies in neonatal intensive care units. Our studies suggest that summarization is a communicative task that requires data analysis techniques for determining the content of the summary. We describe a prototype system that summarizes physiological time series.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sarah Boyd</author>
</authors>
<title>TREND: a system for generating intelligent descriptions of time series data.</title>
<date>1998</date>
<booktitle>In Proceedings of the IEEE International Conference on Intelligent Processing Systems (ICIPS-1998).</booktitle>
<contexts>
<context position="7518" citStr="Boyd, 1998" startWordPosition="1227" endWordPosition="1228">ted from NEONATE, to build the micro planner and realizer. 4.1 Content Selection and Segmentation The most important question in summarization is &apos;what data points from the input should be included in the summary9&apos; Any model of summarization needs to find ways to reduce the size of the input data set (or improve its accessibility) without significantly altering its content (or informativeness). This process is sensitive to the domain constraints such as limits on parameter values. It is clear from our own studies on data summarization and also from the earlier studies by others (Shahar, 1997; Boyd, 1998; Kulkich, 1983) that data summarization needs data analysis to determine the trends and patterns present in the data set. RESUME (Shahar, 1997) uses knowledge based temporal abstraction for producing abstractions of clinical data. TREND (Boyd, 1998) uses wavelets to analyze archives of weather data to produce weather summaries. ANA (Kulkich, 1983) uses a combination of arithmetic computations and pattern matching techniques to analyse raw data from the Dow Jones News service database. SUMTIME-MOUSAM (Sripada et al, 2002) used segmentation of input weather data to determine intervals with simi</context>
</contexts>
<marker>Boyd, 1998</marker>
<rawString>Sarah Boyd. 1998. TREND: a system for generating intelligent descriptions of time series data. In Proceedings of the IEEE International Conference on Intelligent Processing Systems (ICIPS-1998).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ewing Gary</author>
<author>Ferguson Lindsey</author>
<author>Freer Yvonne</author>
<author>Hunter Jim</author>
<author>McIntosh Neil</author>
</authors>
<title>Observational Data Acquired on a Neonatal Intensive Care Unit,</title>
<date>2002</date>
<tech>Technical Report AUCS/TR0205,</tech>
<institution>Dept. of Comp. Science, Univ. of Aberdeen.</institution>
<marker>Gary, Lindsey, Yvonne, Jim, Neil, 2002</marker>
<rawString>Ewing Gary, Ferguson Lindsey, Freer Yvonne, Hunter Jim and McIntosh Neil 2002. Observational Data Acquired on a Neonatal Intensive Care Unit, Technical Report AUCS/TR0205, Dept. of Comp. Science, Univ. of Aberdeen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eamonn Keogh</author>
<author>Selina Chu</author>
<author>David Hart</author>
<author>Michael Pazzani</author>
</authors>
<title>An Online Algorithm for Segmenting Time Series. In:</title>
<date>2001</date>
<booktitle>Proceedings of IEEE International Conference on Data Mining„</booktitle>
<pages>289--296</pages>
<contexts>
<context position="8745" citStr="Keogh et al, 2001" startWordPosition="1421" endWordPosition="1424">s. Upon manual inspection of corpus texts we felt that segmentation should work with neonatal data. Segmentation is the process of fitting linear segments to an input data series keeping the maximum error introduced in segments to be lower than the user defined value. There are many algorithms for segmentation developed in the KDD community. These algorithms differ from each other in the control information they use and the way they process data (such as topdown and bottom-up). We have selected one of them known as the bottom-up algorithm. This algorithm has been explained in great detail in (Keogh et al, 2001) and will not be described here. According to Keogh&apos;s description, the number of segments produced (which determines the detail to which the data is summarized) depends upon a userspecified limit. In our case, this limit cannot be the same for all segments. Segments joining smaller values might have different error limit compared to those that join larger values. These user-defined limits (thresholds) control the segmentation process in a way suited for summarization. In general, data analysis algorithms such as segmentation need to be adapted to suit the summarization requirements (Sripada et</context>
</contexts>
<marker>Keogh, Chu, Hart, Pazzani, 2001</marker>
<rawString>Eamonn Keogh, Selina Chu, David Hart and Michael Pazzani. 2001. An Online Algorithm for Segmenting Time Series. In: Proceedings of IEEE International Conference on Data Mining„ pp 289-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Design and implementation of a knowledge-based report generator. In:</title>
<date>1983</date>
<booktitle>Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics (ACL-1983),</booktitle>
<pages>145--150</pages>
<marker>Kukich, 1983</marker>
<rawString>Karen Kukich. 1983. Design and implementation of a knowledge-based report generator. In: Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics (ACL-1983), pp 145-150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6060" citStr="Reiter and Dale, 2000" startWordPosition="988" endWordPosition="991">sts but not in public forecasts unless gales are predicted. Finally in the domain of gas turbine it is in the form of important patterns. For instance, damped oscillations and steps are significant for monitoring turbines. • This knowledge, however, can be integrated into standard data analysis algo168 rithms. In the domain of meteorology user thresholds have been used for determining stopping criterion for segmentation. In the domain of gas turbines domain knowledge has been used for classifying patterns. 4 System Architecture Our system follows the pipeline architecture for text generation (Reiter and Dale, 2000) as shown in Figure 3. Figure 3. Architecture of our summarization system The first module, artifact separation is responsible for detecting and removing artifacts due to external activities such as blood sampling and baby handling. Artifact detection in a signal deserves a separate study in its own right. However, in SumTimE we are initially using a median filter and an impossible value filter developed in our collaborator project NEONATE. Document planning is responsible for selecting the &apos;important&apos; data points from the input data and to organize them into a paragraph. We describe this modu</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlisle Scott</author>
<author>Jan E Clayton</author>
<author>Elizabeth L Gibson</author>
</authors>
<title>Practical Guide to Knowledge Acquisition.</title>
<date>1991</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="2835" citStr="Scott, Clayton, and Gibson, 1991" startWordPosition="433" endWordPosition="437">bies, medical personnel (doctors and nurses) working in the neonatal unit are required to inspect such data continually. Currently they use visual displays of the data. Our system will generate textual summaries of these data as an aid to the medical personnel. We believe that interpreting textual summaries is lot quicker and does not require much mathematical (statistical) knowledge when compared to interpreting graphical displays. 3 Knowledge Acquisition We have carried out a variety of knowledge acquisition (KA) activities using multiple techniques developed in the expert system community (Scott, Clayton, and Gibson, 1991) to understand how humans perform data summarization. 167 60.0 BM(RD) oth,„ 20.0 27 Feb 96 Figure 1. Plot of mean blood pressure Figure 1 shows a time series plot of mean blood pressure sampled every second for three hours. Figure 2 shows its summary extracted from a small corpus of human written summaries we analyzed. The summary text in Figure 2 has the doctor&apos;s interpretation of the data (for instance, .... this is an inadequate blood pressure ....&apos; and `.. and I suspect that dopamine has been started ...&apos;) interwoven with pure data description (for instance, &apos;...BP is fairly stable at rou</context>
</contexts>
<marker>Scott, Clayton, Gibson, 1991</marker>
<rawString>A. Carlisle Scott, Jan E. Clayton, and Elizabeth L. Gibson. 1991. Practical Guide to Knowledge Acquisition. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Shahar</author>
</authors>
<title>Framework for knowledge based temporal abstraction.</title>
<date>1997</date>
<journal>Artificial Intelligence,</journal>
<pages>90--79</pages>
<contexts>
<context position="7506" citStr="Shahar, 1997" startWordPosition="1225" endWordPosition="1226">rpus we collected from NEONATE, to build the micro planner and realizer. 4.1 Content Selection and Segmentation The most important question in summarization is &apos;what data points from the input should be included in the summary9&apos; Any model of summarization needs to find ways to reduce the size of the input data set (or improve its accessibility) without significantly altering its content (or informativeness). This process is sensitive to the domain constraints such as limits on parameter values. It is clear from our own studies on data summarization and also from the earlier studies by others (Shahar, 1997; Boyd, 1998; Kulkich, 1983) that data summarization needs data analysis to determine the trends and patterns present in the data set. RESUME (Shahar, 1997) uses knowledge based temporal abstraction for producing abstractions of clinical data. TREND (Boyd, 1998) uses wavelets to analyze archives of weather data to produce weather summaries. ANA (Kulkich, 1983) uses a combination of arithmetic computations and pattern matching techniques to analyse raw data from the Dow Jones News service database. SUMTIME-MOUSAM (Sripada et al, 2002) used segmentation of input weather data to determine interva</context>
</contexts>
<marker>Shahar, 1997</marker>
<rawString>Yuval Shahar. 1997. Framework for knowledge based temporal abstraction. Artificial Intelligence, 90:79-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sripada Somayajulu</author>
<author>Ehud Reiter</author>
<author>Jim Hunter</author>
<author>Jin Yu</author>
</authors>
<title>Modelling the task of Summarising Time Series Data using KA Techniques. In:</title>
<date>2001</date>
<booktitle>Proceedings of E52001,</booktitle>
<pages>183--196</pages>
<publisher>Macintosh, A.,</publisher>
<marker>Somayajulu, Reiter, Hunter, Yu, 2001</marker>
<rawString>Somayajulu, G. Sripada, Ehud Reiter, Jim Hunter and Jin Yu. 2001 Modelling the task of Summarising Time Series Data using KA Techniques. In: Macintosh, A., Moulton, M. and Preece, A. (ed) Proceedings of E52001, pp 183 —196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sripada Somayajulu</author>
<author>Ehud Reiter</author>
<author>Jim Hunter</author>
<author>Jin Yu</author>
</authors>
<title>Segmenting Time Series for Weather Forecasting.</title>
<date>2002</date>
<booktitle>Proceedings of ES2002,</booktitle>
<pages>193--206</pages>
<location>In: Macintosh,</location>
<marker>Somayajulu, Reiter, Hunter, Yu, 2002</marker>
<rawString>Somayajulu, G. Sripada, Ehud Reiter, Jim Hunter and Jin Yu. 2002 Segmenting Time Series for Weather Forecasting. In: Macintosh, A., Ellis, R. and Coenen, F. (ed) Proceedings of ES2002, pp. 193-206.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>