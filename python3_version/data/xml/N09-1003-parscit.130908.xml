<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000275">
<title confidence="0.9963625">
A Study on Similarity and Relatedness
Using Distributional and WordNet-based Approaches
</title>
<author confidence="0.526962">
Eneko Agirret Enrique Alfonseca$ Keith Hall$ Jana Kravalova$5 Marius Pas¸ca$ Aitor Soroat
† IXA NLP Group, University of the Basque Country
</author>
<affiliation confidence="0.513597">
‡ Google Inc.
§ Institute of Formal and Applied Linguistics, Charles University in Prague
</affiliation>
<email confidence="0.9797925">
{e.agirre,a.soroa}@ehu.es {ealfonseca,kbhall,mars}@google.com
kravalova@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.993627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997661846153846">
This paper presents and compares WordNet-
based and distributional similarity approaches.
The strengths and weaknesses of each ap-
proach regarding similarity and relatedness
tasks are discussed, and a combination is pre-
sented. Each of our methods independently
provide the best results in their class on the
RG and WordSim353 datasets, and a super-
vised combination of them yields the best pub-
lished results on all datasets. Finally, we pio-
neer cross-lingual similarity, showing that our
methods are easily adapted for a cross-lingual
task with minor losses.
</bodyText>
<sectionHeader confidence="0.998982" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999559857142857">
Measuring semantic similarity and relatedness be-
tween terms is an important problem in lexical se-
mantics. It has applications in many natural lan-
guage processing tasks, such as Textual Entailment,
Word Sense Disambiguation or Information Extrac-
tion, and other related areas like Information Re-
trieval. The techniques used to solve this problem
can be roughly classified into two main categories:
those relying on pre-existing knowledge resources
(thesauri, semantic networks, taxonomies or ency-
clopedias) (Alvarez and Lim, 2007; Yang and Pow-
ers, 2005; Hughes and Ramage, 2007) and those in-
ducing distributional properties of words from cor-
pora (Sahami and Heilman, 2006; Chen et al., 2006;
Bollegala et al., 2007).
In this paper, we explore both families. For the
first one we apply graph based algorithms to Word-
Net, and for the second we induce distributional
similarities collected from a 1.6 Terabyte Web cor-
pus. Previous work suggests that distributional sim-
ilarities suffer from certain limitations, which make
</bodyText>
<page confidence="0.988025">
19
</page>
<bodyText confidence="0.999960178571428">
them less useful than knowledge resources for se-
mantic similarity. For example, Lin (1998b) finds
similar phrases like captive-westerner which made
sense only in the context of the corpus used, and
Budanitsky and Hirst (2006) highlight other prob-
lems that stem from the imbalance and sparseness of
the corpora. Comparatively, the experiments in this
paper demonstrate that distributional similarities can
perform as well as the knowledge-based approaches,
and a combination of the two can exceed the per-
formance of results previously reported on the same
datasets. An application to cross-lingual (CL) sim-
ilarity identification is also described, with applica-
tions such as CL Information Retrieval or CL spon-
sored search. A discussion on the differences be-
tween learning similarity and relatedness scores is
provided.
The paper is structured as follows. We first
present the WordNet-based method, followed by the
distributional methods. Section 4 is devoted to the
evaluation and results on the monolingual and cross-
lingual tasks. Section 5 presents some analysis, in-
cluding learning curves for distributional methods,
the use of distributional similarity to improve Word-
Net similarity, the contrast between similarity and
relatedness, and the combination of methods. Sec-
tion 6 presents related work, and finally, Section 7
draws the conclusions and mentions future work.
</bodyText>
<sectionHeader confidence="0.997591" genericHeader="introduction">
2 WordNet-based method
</sectionHeader>
<bodyText confidence="0.998582375">
WordNet (Fellbaum, 1998) is a lexical database of
English, which groups nouns, verbs, adjectives and
adverbs into sets of synonyms (synsets), each ex-
pressing a distinct concept. Synsets are interlinked
with conceptual-semantic and lexical relations, in-
cluding hypernymy, meronymy, causality, etc.
Given a pair of words and a graph-based repre-
sentation of WordNet, our method has basically two
</bodyText>
<note confidence="0.806152">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 19–27,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.99987775862069">
steps: We first compute the personalized PageR-
ank over WordNet separately for each of the words,
producing a probability distribution over WordNet
synsets. We then compare how similar these two dis-
crete probability distributions are by encoding them
as vectors and computing the cosine between the
vectors.
We represent WordNet as a graph G = (V, E) as
follows: graph nodes represent WordNet concepts
(synsets) and dictionary words; relations among
synsets are represented by undirected edges; and
dictionary words are linked to the synsets associated
to them by directed edges.
For each word in the pair we first compute a per-
sonalized PageRank vector of graph G (Haveliwala,
2002). Basically, personalized PageRank is com-
puted by modifying the random jump distribution
vector in the traditional PageRank equation. In our
case, we concentrate all probability mass in the tar-
get word.
Regarding PageRank implementation details, we
chose a damping value of 0.85 and finish the calcula-
tion after 30 iterations. These are default values, and
we did not optimize them. Our similarity method is
similar, but simpler, to that used by (Hughes and Ra-
mage, 2007), which report very good results on sim-
ilarity datasets. More details of our algorithm can be
found in (Agirre and Soroa, 2009). The algorithm
and needed resouces are publicly available1.
</bodyText>
<subsectionHeader confidence="0.999554">
2.1 WordNet relations and versions
</subsectionHeader>
<bodyText confidence="0.999893571428571">
The WordNet versions that we use in this work are
the Multilingual Central Repository or MCR (At-
serias et al., 2004) (which includes English Word-
Net version 1.6 and wordnets for several other lan-
guages like Spanish, Italian, Catalan and Basque),
and WordNet version 3.02. We used all the rela-
tions in MCR (except cooccurrence relations and se-
lectional preference relations) and in WordNet 3.0.
Given the recent availability of the disambiguated
gloss relations for WordNet 3.03, we also used a
version which incorporates these relations. We will
refer to the three versions as MCR16, WN30 and
WN30g, respectively. Our choice was mainly moti-
vated by the fact that MCR contains tightly aligned
</bodyText>
<footnote confidence="0.999874">
1http://http://ixa2.si.ehu.es/ukb/
2Available from http://http://wordnet.princeton.edu/
3http://wordnet.princeton.edu/glosstag
</footnote>
<bodyText confidence="0.675184">
wordnets of several languages (see below).
</bodyText>
<subsectionHeader confidence="0.999524">
2.2 Cross-linguality
</subsectionHeader>
<bodyText confidence="0.999989103448276">
MCR follows the EuroWordNet design (Vossen,
1998), which specifies an InterLingual Index (ILI)
that links the concepts across wordnets of differ-
ent languages. The wordnets for other languages in
MCR use the English WordNet synset numbers as
ILIs. This design allows a decoupling of the rela-
tions between concepts (which can be taken to be
language independent) and the links from each con-
tent word to its corresponding concepts (which is
language dependent).
As our WordNet-based method uses the graph of
the concepts and relations, we can easily compute
the similarity between words from different lan-
guages. For example, consider a English-Spanish
pair like car – coche. Given that the Spanish Word-
Net is included in MCR we can use MCR as the
common knowledge-base for the relations. We can
then compute the personalized PageRank for each
of car and coche on the same underlying graph, and
then compare the similarity between both probabil-
ity distributions.
As an alternative, we also tried to use pub-
licly available mappings for wordnets (Daude et al.,
2000)4 in order to create a 3.0 version of the Span-
ish WordNet. The mapping was used to link Spanish
variants to 3.0 synsets. We used the English Word-
Net 3.0, including glosses, to construct the graph.
The two Spanish WordNet versions are referred to
as MCR16 and WN30g.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="method">
3 Context-based methods
</sectionHeader>
<bodyText confidence="0.999852">
In this section, we describe the distributional meth-
ods used for calculating similarities between words,
and profiting from the use of a large Web-based cor-
pus.
This work is motivated by previous studies that
make use of search engines in order to collect co-
occurrence statistics between words. Turney (2001)
uses the number of hits returned by a Web search
engine to calculate the Pointwise Mutual Informa-
tion (PMI) between terms, as an indicator of syn-
onymy. Bollegala et al. (2007) calculate a number
of popular relatedness metrics based on page counts,
</bodyText>
<footnote confidence="0.977831">
4http://www.lsi.upc.es/∼nlp/tools/download-map.php.
</footnote>
<page confidence="0.997008">
20
</page>
<bodyText confidence="0.99995952631579">
like PMI, the Jaccard coefficient, the Simpson co-
efficient and the Dice coefficient, which are com-
bined with lexico-syntactic patterns as model fea-
tures. The model parameters are trained using Sup-
port Vector Machines (SVM) in order to later rank
pairs of words. A different approach is the one taken
by Sahami and Heilman (2006), who collect snip-
pets from the results of a search engine and repre-
sent each snippet as a vector, weighted with the tf·idf
score. The semantic similarity between two queries
is calculated as the inner product between the cen-
troids of the respective sets of vectors.
To calculate the similarity of two words w1 and
w2, Ruiz-Casado et al. (2005) collect snippets con-
taining w1 from a Web search engine, extract a con-
text around it, replace it with w2 and check for the
existence of that modified context in the Web.
Using a search engine to calculate similarities be-
tween words has the drawback that the data used will
always be truncated. So, for example, the numbers
of hits returned by search engines nowadays are al-
ways approximate and rounded up. The systems that
rely on collecting snippets are also limited by the
maximum number of documents returned per query,
typically around a thousand. We hypothesize that
by crawling a large corpus from the Web and doing
standard corpus analysis to collect precise statistics
for the terms we should improve over other unsu-
pervised systems that are based on search engine
results, and should yield results that are competi-
tive even when compared to knowledge-based ap-
proaches.
In order to calculate the semantic similarity be-
tween the words in a set, we have used a vector space
model, with the following three variations:
In the bag-of-words approach, for each word w
in the dataset we collect every term t that appears in
a window centered in w, and add them to the vector
together with its frequency.
In the context window approach, for each word
w in the dataset we collect every window W cen-
tered in w (removing the central word), and add it
to the vector together with its frequency (the total
number of times we saw window W around w in the
whole corpus). In this case, all punctuation symbols
are replaced with a special token, to unify patterns
like, the &lt;term&gt; said to and ’ the &lt;term&gt; said to.
Throughout the paper, when we mention a context
window of size N it means N words at each side of
the phrase of interest.
In the syntactic dependency approach, we parse
the entire corpus using an implementation of an In-
ductive Dependency parser as described in Nivre
(2006). For each word w we collect a template of
the syntactic context. We consider sequences of gov-
erning words (e.g. the parent, grand-parent, etc.) as
well as collections of descendants (e.g., immediate
children, grandchildren, etc.). This information is
then encoded as a contextual template. For example,
the context template cooks &lt;term&gt; delicious could
be contexts for nouns such as food, meals, pasta, etc.
This captures both syntactic preferences as well as
selectional preferences. Contrary to Pado and Lap-
ata (2007), we do not use the labels of the syntactic
dependencies.
Once the vectors have been obtained, the fre-
quency for each dimension in every vector is
weighted using the other vectors as contrast set, with
the k2 test, and finally the cosine similarity between
vectors is used to calculate the similarity between
each pair of terms.
Except for the syntactic dependency approach,
where closed-class words are needed by the parser,
in the other cases we have removed stopwords (pro-
nouns, prepositions, determiners and modal and
auxiliary verbs).
</bodyText>
<subsectionHeader confidence="0.999737">
3.1 Corpus used
</subsectionHeader>
<bodyText confidence="0.999969583333333">
We have used a corpus of four billion documents,
crawled from the Web in August 2008. An HTML
parser is used to extract text, the language of each
document is identified, and non-English documents
are discarded. The final corpus remaining at the end
of this process contains roughly 1.6 Terawords. All
calculations are done in parallel sharding by dimen-
sion, and it is possible to calculate all pairwise sim-
ilarities of the words in the test sets very quickly
on this corpus using the MapReduce infrastructure.
A complete run takes around 15 minutes on 2,000
cores.
</bodyText>
<subsectionHeader confidence="0.999829">
3.2 Cross-linguality
</subsectionHeader>
<bodyText confidence="0.999934">
In order to calculate similarities in a cross-lingual
setting, where some of the words are in a language l
other than English, the following algorithm is used:
</bodyText>
<page confidence="0.996672">
21
</page>
<table confidence="0.9997775">
Method Window size RG dataset WordSim353 dataset
MCR16 0.83 [0.73, 0.89] 0.53 (0.56) [0.45, 0.60]
WN30 0.79 [0.67, 0.86] 0.56 (0.58) [0.48, 0.63]
WN30g 0.83 [0.73, 0.89] 0.66 (0.69) [0.59, 0.71]
CW 1 0.83 [0.73, 0.89] 0.63 [0.57, 0.69]
2 0.83 [0.74, 0.90] 0.60 [0.53, 0.66]
3 0.85 [0.76, 0.91] 0.59 [0.52, 0.65]
4 0.89 [0.82, 0.93] 0.60 [0.53, 0.66]
5 0.80 [0.70, 0.88] 0.58 [0.51, 0.65]
6 0.75 [0.62, 0.84] 0.58 [0.50, 0.64]
7 0.72 [0.58, 0.82] 0.57 [0.49, 0.63]
BoW 1 0.81 [0.70, 0.88] 0.64 [0.57, 0.70]
2 0.80 [0.69, 0.87] 0.64 [0.58, 0.70]
3 0.79 [0.67, 0.86] 0.64 [0.58, 0.70]
4 0.78 [0.66, 0.86] 0.65 [0.58, 0.70]
5 0.77 [0.64, 0.85] 0.64 [0.58, 0.70]
6 0.76 [0.63, 0.85] 0.65 [0.58, 0.70]
7 0.75 [0.62, 0.84] 0.64 [0.58, 0.70]
Syn G1,D0 0.81 [0.70, 0.88] 0.62 [0.55, 0.68]
G2,D0 0.82 [0.72, 0.89] 0.55 [0.48, 0.62]
G3,D0 0.81 [0.71, 0.88] 0.62 [0.56, 0.68]
G1,D1 0.82 [0.72, 0.89] 0.62 [0.55, 0.68]
G2,D1 0.82 [0.73, 0.89] 0.62 [0.55, 0.68]
G3,D1 0.82 [0.72, 0.88] 0.62 [0.55, 0.68]
CW+ 4; G1,D0 0.88 [0.81, 0.93] 0.66 [0.59, 0.71]
Syn 4; G2,D0 0.87 [0.80, 0.92] 0.64 [0.57, 0.70]
4; G3,D0 0.86 [0.77, 0.91] 0.63 [0.56, 0.69]
4; G1,D1 0.83 [0.73, 0.89] 0.48 [0.40, 0.56]
4; G2,D1 0.83 [0.73, 0.89] 0.49 [0.40, 0.56]
4; G3,D1 0.82 [0.72, 0.89] 0.48 [0.40, 0.56]
</table>
<tableCaption confidence="0.999435">
Table 1: Spearman correlation results for the various WordNet-based
</tableCaption>
<bodyText confidence="0.841625714285714">
models and distributional models. CW=Context Windows, BoW=bag
of words, Syn=syntactic vectors. For Syn, the window size is actually
the tree-depth for the governors and descendants. For examples, G1
indicates that the contexts include the parents and D2 indicates that both
the children and grandchildren make up the contexts. The final grouping
includes both contextual windows (at width 4) and syntactic contexts in
the template vectors. Max scores are bolded.
</bodyText>
<listItem confidence="0.999292875">
1. Replace each non-English word in the dataset
with its 5-best translations into English using
state-of-the-art machine translation technology.
2. The vector corresponding to each Spanish word
is calculated by collecting features from all the
contexts of any of its translations.
3. Once the vectors are generated, the similarities
are calculated in the same way as before.
</listItem>
<sectionHeader confidence="0.985603" genericHeader="method">
4 Experimental results
</sectionHeader>
<subsectionHeader confidence="0.980806">
4.1 Gold-standard datasets
</subsectionHeader>
<bodyText confidence="0.9999517">
We have used two standard datasets. The first
one, RG, consists of 65 pairs of words collected by
Rubenstein and Goodenough (1965), who had them
judged by 51 human subjects in a scale from 0.0 to
4.0 according to their similarity, but ignoring any
other possible semantic relationships that might ap-
pear between the terms. The second dataset, Word-
Sim3535 (Finkelstein et al., 2002) contains 353 word
pairs, each associated with an average of 13 to 16 hu-
man judgements. In this case, both similarity and re-
</bodyText>
<footnote confidence="0.998199">
5Available at http://www.cs.technion.ac.il/
∼gabr/resources/data/wordsim353/wordsim353.html
</footnote>
<subsectionHeader confidence="0.852233">
Context RG terms and frequencies
</subsectionHeader>
<bodyText confidence="0.966284">
ll never forget the&apos; on his face when grin,2,smile,10
he had a giant&apos; on his face and grin,3,smile,2
room with a huge&apos; on her face and grin,2,smile,6
the state of every&apos; will be updated every automobile,2,car,3
repair or replace the&apos; if it is stolen automobile,2,car,2
located on the north&apos; of the Bay of shore,14,coast,2
areas on the eastern&apos; of the Adriatic Sea shore,3,coast,2
Thesaurus of Current English&apos; The Oxford Pocket Thesaurus slave,3,boy,5,shore,3,string,2
</bodyText>
<figure confidence="0.473332166666667">
wizard,4,glass,4,crane,5,smile,5
implement,5,oracle,2,lad,2
food,3,car,2,madhouse,3,jewel,3
asylum,4,tool,8,journey,6,etc.
be understood that the&apos; 10 may be designed crane,3,tool,3
a fight between a &apos; and a snake and bird,3,crane,5
</figure>
<tableCaption confidence="0.989397">
Table 2: Sample of context windows for the terms in the RG dataset.
</tableCaption>
<bodyText confidence="0.99482875">
latedness are annotated without any distinction. Sev-
eral studies indicate that the human scores consis-
tently have very high correlations with each other
(Miller and Charles, 1991; Resnik, 1995), thus val-
idating the use of these datasets for evaluating se-
mantic similarity.
For the cross-lingual evaluation, the two datasets
were modified by translating the second word in
each pair into Spanish. Two humans translated
simultaneously both datasets, with an inter-tagger
agreement of 72% for RG and 84% for Word-
Sim353.
</bodyText>
<sectionHeader confidence="0.670599" genericHeader="method">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999892157894737">
Table 1 shows the Spearman correlation obtained on
the RG and WordSim353 datasets, including the in-
terval at 0.95 of confidence6.
Overall the distributional context-window ap-
proach performs best in the RG, reaching 0.89 corre-
lation, and both WN30g and the combination of con-
text windows and syntactic context perform best on
WordSim353. Note that the confidence intervals are
quite large in both RG and WordSim353, and few of
the pairwise differences are statistically significant.
Regarding WordNet-based approaches, the use of
the glosses and WordNet 3.0 (WN30g) yields the
best results in both datasets. While MCR16 is close
to WN30g for the RG dataset, it lags well behind
on WordSim353. This discrepancy is further ana-
lyzed is Section 5.3. Note that the performance of
WordNet in the WordSim353 dataset suffers from
unknown words. In fact, there are nine pairs which
returned null similarity for this reason. The num-
</bodyText>
<footnote confidence="0.974388">
6To calculate the Spearman correlations values are trans-
formed into ranks, and we calculate the Pearson correlation on
them. The confidence intervals refer to the Pearson correlations
of the rank vectors.
</footnote>
<page confidence="0.997804">
22
</page>
<figureCaption confidence="0.869846">
Figure 1: Effect of the size of the training corpus, for the best distributional similarity model in each dataset. Left: WordSim353 with bag-of-words,
Right: RG with context windows.
</figureCaption>
<table confidence="0.999894666666667">
Dataset Method overall Δ interval
RG MCR16 0.78 -0.05 [0.66, 0.86]
WN30g 0.74 -0.09 [0.61, 0.84]
Bag of words 0.68 -0.23 [0.53, 0.79]
Context windows 0.83 -0.05 [0.73, 0.89]
WS353 MCR16 0.42 (0.53) -0.11 (-0.03) [0.34, 0.51]
WN30g 0.58 (0.67) -0.07 (-0.02) [0.51, 0.64]
Bag of words 0.53 -0.12 [0.45, 0.61]
Context windows 0.52 -0.11 [0.44, 0.59]
</table>
<tableCaption confidence="0.832946666666667">
Table 3: Results obtained by the different methods on the Span-
ish/English cross-lingual datasets. The Δ column shows the perfor-
mance difference with respect to the results on the original dataset.
</tableCaption>
<bodyText confidence="0.999840409090909">
ber in parenthesis in Table 1 for WordSim353 shows
the results for the 344 remaining pairs. Section 5.2
shows a proposal to overcome this limitation.
The bag-of-words approach tends to group to-
gether terms that can have a similar distribution of
contextual terms. Therefore, terms that are topically
related can appear in the same textual passages and
will get high values using this model. We see this
as an explanation why this model performed better
than the context window approach for WordSim353,
where annotators were instructed to provide high
ratings to related terms. On the contrary, the con-
text window approach tends to group together words
that are exchangeable in exactly the same context,
preserving order. Table 2 illustrates a few exam-
ples of context collected. Therefore, true synonyms
and hyponyms/hyperonyms will receive high simi-
larities, whereas terms related topically or based on
any other semantic relation (e.g. movie and star) will
have lower scores. This explains why this method
performed better for the RG dataset. Section 5.3
confirms these observations.
</bodyText>
<subsectionHeader confidence="0.989202">
4.3 Cross-lingual similarity
</subsectionHeader>
<bodyText confidence="0.999936631578947">
Table 3 shows the results for the English-Spanish
cross-lingual datasets. For RG, MCR16 and the
context windows methods drop only 5 percentage
points, showing that cross-lingual similarity is feasi-
ble, and that both cross-lingual strategies are robust.
The results for WordSim353 show that WN30g is
the best for this dataset, with the rest of the meth-
ods falling over 10 percentage points relative to the
monolingual experiment. A closer look at the Word-
Net results showed that most of the drop in perfor-
mance was caused by out-of-vocabulary words, due
to the smaller vocabulary of the Spanish WordNet.
Though not totally comparable, if we compute the
correlation over pairs covered in WordNet alone, the
correlation would drop only 2 percentage points. In
the case of the distributional approaches, the fall in
performance was caused by the translations, as only
61% of the words were translated into the original
word in the English datasets.
</bodyText>
<sectionHeader confidence="0.9703675" genericHeader="method">
5 Detailed analysis and system
combination
</sectionHeader>
<bodyText confidence="0.9999706">
In this section we present some analysis, including
learning curves for distributional methods, the use
of distributional similarity to improve WordNet sim-
ilarity, the contrast between similarity and related-
ness, and the combination of methods.
</bodyText>
<subsectionHeader confidence="0.996146">
5.1 Learning curves for distributional methods
</subsectionHeader>
<bodyText confidence="0.999612375">
Figure 1 shows that the correlation improves with
the size of the corpus, as expected. For the re-
sults using the WordSim353 corpus, we show the
results of the bag-of-words approach with context
size 10. Results improve from 0.5 Spearman correla-
tion up to 0.65 when increasing the corpus size three
orders of magnitude, although the effect decays at
the end, which indicates that we might not get fur-
</bodyText>
<page confidence="0.995479">
23
</page>
<table confidence="0.982842">
Method Without similar words With similar words
WN30 0.56 (0.58) [0.48, 0.63] 0.58 [0.51, 0.65]
WN30g 0.66 (0.69) [0.59, 0.71] 0.68 [0.62, 0.73]
</table>
<tableCaption confidence="0.981546">
Table 4: Results obtained replacing unknown words with their most
</tableCaption>
<table confidence="0.929999">
similar three words (WordSim353 dataset).
Method overall Similarity Relatedness
MCR16 0.53 [0.45, 0.60] 0.65 [0.56, 0.72] 0.33 [0.21, 0.43]
WN30 0.56 [0.48, 0.63] 0.73 [0.65, 0.79] 0.38 [0.27, 0.48]
WN30g 0.66 [0.59, 0.71] 0.72 [0.64, 0.78] 0.56 [0.46, 0.64]
BoW 0.65 [0.59, 0.71] 0.70 [0.63, 0.77] 0.62 [0.53, 0.69]
CW 0.60 [0.53, 0.66] 0.77 [0.71, 0.82] 0.46 [0.36, 0.55]
</table>
<tableCaption confidence="0.996424">
Table 5: Results obtained on the WordSim353 dataset and on the two
similarity and relatedness subsets.
</tableCaption>
<bodyText confidence="0.995565125">
ther gains going beyond the current size of the cor-
pus. With respect to results for the RG dataset, we
used a context-window approach with context radius
4. Here, results improve even more with data size,
probably due to the sparse data problem collecting
8-word context windows if the corpus is not large
enough. Correlation improves linearly right to the
end, where results stabilize around 0.89.
</bodyText>
<subsectionHeader confidence="0.900528">
5.2 Combining both approaches: dealing with
unknown words in WordNet
</subsectionHeader>
<bodyText confidence="0.997290923076923">
Although the vocabulary of WordNet is very ex-
tensive, applications are bound to need the similar-
ity between words which are not included in Word-
Net. This is exemplified in the WordSim353 dataset,
where 9 pairs contain words which are unknown to
WordNet. In order to overcome this shortcoming,
we could use similar words instead, as provided by
the distributional thesaurus. We used the distribu-
tional thesaurus defined in Section 3, using context
windows of width 4, to provide three similar words
for each of the unknown words in WordNet. Results
improve for both WN30 and WN30g, as shown in
Table 4, attaining our best results for WordSim353.
</bodyText>
<subsectionHeader confidence="0.995499">
5.3 Similarity vs. relatedness
</subsectionHeader>
<bodyText confidence="0.999966696428571">
We mentioned above that the annotation guidelines
of WordSim353 did not distinguish between simi-
lar and related pairs. As the results in Section 4
show, different techniques are more appropriate to
calculate either similarity or relatedness. In order to
study this effect, ideally, we would have two ver-
sions of the dataset, where annotators were given
precise instructions to distinguish similarity in one
case, and relatedness in the other. Given the lack
of such datasets, we devised a simpler approach in
order to reuse the existing human judgements. We
manually split the dataset in two parts, as follows.
First, two humans classified all pairs as be-
ing synonyms of each other, antonyms, iden-
tical, hyperonym-hyponym, hyponym-hyperonym,
holonym-meronym, meronym-holonym, and none-
of-the-above. The inter-tagger agreement rate was
0.80, with a Kappa score of 0.77. This anno-
tation was used to group the pairs in three cate-
gories: similar pairs (those classified as synonyms,
antonyms, identical, or hyponym-hyperonym), re-
lated pairs (those classified as meronym-holonym,
and pairs classified as none-of-the-above, with a hu-
man average similarity greater than 5), and unrelated
pairs (those classified as none-of-the-above that had
average similarity less than or equal to 5). We then
created two new gold-standard datasets: similarity
(the union of similar and unrelated pairs), and relat-
edness (the union of related and unrelated)7.
Table 5 shows the results on the relatedness and
similarity subsets of WordSim353 for the different
methods. Regarding WordNet methods, both WN30
and WN30g perform similarly on the similarity sub-
set, but WN30g obtains the best results by far on
the relatedness data. These results are congruent
with our expectations: two words are similar if their
synsets are in close places in the WordNet hierarchy,
and two words are related if there is a connection
between them. Most of the relations in WordNet
are of hierarchical nature, and although other rela-
tions exist, they are far less numerous, thus explain-
ing the good results for both WN30 and WN30g on
similarity, but the bad results of WN30 on related-
ness. The disambiguated glosses help find connec-
tions among related concepts, and allow our method
to better model relatedness with respect to WN30.
The low results for MCR16 also deserve some
comments. Given the fact that MCR16 performed
very well on the RG dataset, it comes as a surprise
that it performs so poorly for the similarity subset
of WordSim353. In an additional evaluation, we at-
tested that MCR16 does indeed perform as well as
MCR30g on the similar pairs subset. We believe
that this deviation could be due to the method used to
construct the similarity dataset, which includes some
pairs of loosely related pairs labeled as unrelated.
</bodyText>
<footnote confidence="0.997865">
7Available at http://alfonseca.org/eng/research/wordsim353.html
</footnote>
<page confidence="0.995091">
24
</page>
<table confidence="0.6940398">
Methods combined in the SVM RG dataset WordSim353 dataset WordSim353 similarity WordSim353 relatedness
WN30g, bag of words 0.88 [0.82, 0.93] 0.78 [0.73, 0.81] 0.81 [0.76, 0.86] 0.72 [0.65, 0.77]
WN30g, context windows 0.90 [0.84, 0.94] 0.73 [0.68, 0.79] 0.83 [0.78, 0.87] 0.64 [0.56, 0.71]
WN30g, syntax 0.89 [0.83, 0.93] 0.75 [0.70, 0.79] 0.83 [0.78, 0.87] 0.67 [0.60, 0.74]
WN30g, bag of words, context windows, syntax 0.96 [0.93, 0.97] 0.78 [0.73, 0.82] 0.83 [0.78, 0.87] 0.71 [0.65, 0.77]
</table>
<tableCaption confidence="0.999044">
Table 6: Results using a supervised combination of several systems. Max values are bolded for each dataset.
</tableCaption>
<bodyText confidence="0.9987896">
Concerning the techniques based on distributional
similarities, the method based on context windows
provides the best results for similarity, and the bag-
of-words representation outperforms most of the
other techniques for relatedness.
</bodyText>
<subsectionHeader confidence="0.996813">
5.4 Supervised combination
</subsectionHeader>
<bodyText confidence="0.999131342857143">
In order to gain an insight on which would be the up-
per bound that we could obtain when combining our
methods, we took the output of three systems (bag
of words with window size 10, context window with
size 4, and the WN30g run). Each of these outputs is
a ranking of word pairs, and we implemented an or-
acle that chooses, for each pair, the rank that is most
similar to the rank of the pair in the gold-standard.
The outputs of the oracle have a Spearman correla-
tion of 0.97 for RG and 0.92 for WordSim353, which
gives as an indication of the correlations that could
be achieved by choosing for each pair the rank out-
put by the best classifier for that pair.
The previous results motivated the use of a su-
pervised approach to combine the output of the
different systems. We created a training cor-
pus containing pairs of pairs of words from the
datasets, having as features the similarity and rank
of each pair involved as given by the differ-
ent unsupervised systems. A classifier is trained
to decide whether the first pair is more simi-
lar than the second one. For example, a train-
ing instance using two unsupervised classifiers is
0.001364, 31, 0.327515, 64, 0.084805, 57, 0.109061, 59, negative
meaning that the similarities given by the first clas-
sifier to the two pairs were 0.001364 and 0.327515
respectively, which ranked them in positions 31 and
64. The second classifier gave them similarities of
0.084805 and 0.109061 respectively, which ranked
them in positions 57 and 59. The class negative in-
dicates that in the gold-standard the first pair has a
lower score than the second pair.
We have trained a SVM to classify pairs of pairs,
and use its output to rank the entries in both datasets.
It uses a polynomial kernel with degree 4. We did
</bodyText>
<table confidence="0.999788263157895">
Method Source Spearman (MC) Pearson (MC)
(Sahami et al., 2006) Web snippets 0.62 [0.32, 0.81] 0.58 [0.26, 0.78]
(Chen et al., 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85]
(Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89]
(Leacock et al., 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91]
(Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90]
(Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92]
(Bollegala et al., 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92]
(Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93]
(Jarmasz, 2003) Roget’s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94]
(Patwardhan et al., 2006) WordNet n/a 0.91
(Alvarez and Lim, 2007) WordNet n/a 0.91
(Yang and Powers, 2005) WordNet 0.87 [0.73, 0.91] 0.92 [0.84, 0.96]
(Hughes et al., 2007) WordNet 0.90 n/a
Personalized PageRank WordNet 0.89 [0.77, 0.94] n/a
Bag of words Web corpus 0.85 [0.70, 0.93] 0.84 [0.69, 0.93]
Context window Web corpus 0.88 [0.76, 0.95] 0.89 [0.77, 0.95]
Syntactic contexts Web corpus 0.76 [0.54, 0.88] 0.74 [0.51, 0.87]
SVM Web, WN 0.92 [0.84, 0.96] 0.93 [0.85, 0.97]
</table>
<tableCaption confidence="0.999986">
Table 7: Comparison with previous approaches for MC.
</tableCaption>
<bodyText confidence="0.998747375">
not have a held-out set, so we used the standard set-
tings of Weka, without trying to modify parameters,
e.g. C. Each word pair is scored with the number
of pairs that were considered to have less similar-
ity using the SVM. The results using 10-fold cross-
validation are shown in Table 6. A combination of
all methods produces the best results reported so far
for both datasets, statistically significant for RG.
</bodyText>
<sectionHeader confidence="0.999889" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.999284388888889">
Contrary to the WordSim353 dataset, common prac-
tice with the RG dataset has been to perform the
evaluation with Pearson correlation. In our believe
Pearson is less informative, as the Pearson correla-
tion suffers much when the scores of two systems are
not linearly correlated, something which happens
often given due to the different nature of the tech-
niques applied. Some authors, e.g. Alvarez and Lim
(2007), use a non-linear function to map the system
outputs into new values distributed more similarly
to the values in the gold-standard. In their case, the
mapping function was exp (−&apos;4 ), which was chosen
empirically. Finding such a function is dependent
on the dataset used, and involves an extra step in the
similarity calculations. Alternatively, the Spearman
correlation provides an evaluation metric that is in-
dependent of such data-dependent transformations.
Most similarity researchers have published their
</bodyText>
<page confidence="0.999308">
25
</page>
<tableCaption confidence="0.999808">
Table 8: Our best results for the MC dataset.
</tableCaption>
<table confidence="0.987877666666667">
Method Source Spearman
(Strube and Ponzetto, 2006) Wikipedia 0.19–0.48
(Jarmasz, 2003) WordNet 0.33–0.35
(Jarmasz, 2003) Roget’s 0.55
(Hughes and Ramage, 2007) WordNet 0.55
(Finkelstein et al., 2002) Web corpus, WN 0.56
(Gabrilovich and Markovitch, 2007) ODP 0.65
(Gabrilovich and Markovitch, 2007) Wikipedia 0.75
SVM Web corpus, WN 0.78
</table>
<tableCaption confidence="0.999981">
Table 9: Comparison with previous work for WordSim353.
</tableCaption>
<bodyText confidence="0.999967418604651">
complete results on a smaller subset of the RG
dataset containing 30 word pairs (Miller and
Charles, 1991), usually referred to as MC, making it
possible to compare different systems using differ-
ent correlation. Table 7 shows the results of related
work on MC that was available to us, including our
own. For the authors that did not provide the de-
tailed data we include only the Pearson correlation
with no confidence intervals.
Among the unsupervised methods introduced in
this paper, the context window produced the best re-
ported Spearman correlation, although the 0.95 con-
fidence intervals are too large to allow us to accept
the hypothesis that it is better than all others meth-
ods. The supervised combination produces the best
results reported so far. For the benefit of future re-
search, our results for the MC subset are displayed
in Table 8.
Comparison on the WordSim353 dataset is eas-
ier, as all researchers have used Spearman. The
figures in Table 9) show that our WordNet-based
method outperforms all previously published Word-
Net methods. We want to note that our WordNet-
based method outperforms that of Hughes and Ram-
age (2007), which uses a similar method. Although
there are some differences in the method, we think
that the main performance gain comes from the use
of the disambiguated glosses, which they did not
use. Our distributional methods also outperform all
other corpus-based methods. The most similar ap-
proach to our distributional technique is Finkelstein
et al. (2002), who combined distributional similar-
ities from Web documents with a similarity from
WordNet. Their results are probably worse due to
the smaller data size (they used 270,000 documents)
and the differences in the calculation of the simi-
larities. The only method which outperforms our
non-supervised methods is that of (Gabrilovich and
Markovitch, 2007) when based on Wikipedia, prob-
ably because of the dense, manually distilled knowl-
edge contained in Wikipedia. All in all, our super-
vised combination gets the best published results on
this dataset.
</bodyText>
<sectionHeader confidence="0.994018" genericHeader="conclusions">
7 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999949733333333">
This paper has presented two state-of-the-art dis-
tributional and WordNet-based similarity measures,
with a study of several parameters, including per-
formance on similarity and relatedness data. We
show that the use of disambiguated glosses allows
for the best published results for WordNet-based
systems on the WordSim353 dataset, mainly due to
the better modeling of relatedness (as opposed to
similarity). Distributional similarities have proven
to be competitive when compared to knowledge-
based methods, with context windows being better
for similarity and bag of words for relatedness. Dis-
tributional similarity was effectively used to cover
out-of-vocabulary items in the WordNet-based mea-
sure providing our best unsupervised results. The
complementarity of our methods was exploited by
a supervised learner, producing the best results so
far for RG and WordSim353. Our results include
confidence values, which, surprisingly, were not in-
cluded in most previous work, and show that many
results over RG and WordSim353 are indistinguish-
able. The algorithm for WordNet-base similarity
and the necessary resources are publicly available8.
This work pioneers cross-lingual extension and
evaluation of both distributional and WordNet-based
measures. We have shown that closely aligned
wordnets provide a natural and effective way to
compute cross-lingual similarity with minor losses.
A simple translation strategy also yields good results
for distributional methods.
</bodyText>
<footnote confidence="0.960133">
8http://ixa2.si.ehu.es/ukb/
</footnote>
<note confidence="0.419788">
Word pair M&amp;C SVM
Word pair M&amp;C SVM
</note>
<bodyText confidence="0.860156892857143">
automobile, car 3.92 62
journey, voyage 3.84 54
gem, jewel 3.84 61
boy, lad 3.76 57
coast, shore 3.7 53
asylum, madhouse 3.61 45
magician, wizard 3.5 49
midday, noon 3.42 61
furnace, stove 3.11 50
food, fruit 3.08 47
bird, cock 3.05 46
bird, crane 2.97 38
implement, tool 2.95 55
brother, monk 2.82 42
crane, implement 1.68 26
brother, lad 1.66 39
car, journey 1.16 37
monk, oracle 1.1 32
food, rooster 0.89 3
coast, hill 0.87 34
forest, graveyard 0.84 27
monk, slave 0.55 17
lad, wizard 0.42 13
coast, forest 0.42 18
cord, smile 0.13 5
glass, magician 0.11 10
rooster, voyage 0.08 1
noon, string 0.08 5
</bodyText>
<page confidence="0.940397">
26
</page>
<sectionHeader confidence="0.995927" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999673052083333">
E. Agirre and A. Soroa. 2009. Personalizing pager-
ank for word sense disambiguation. In Proc. of EACL
2009, Athens, Greece.
M.A. Alvarez and S.J. Lim. 2007. A Graph Modeling
of Semantic Similarity between Words. Proc. of the
Conference on Semantic Computing, pages 355–362.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004. The meaning multi-
lingual central repository. In Proc. of Global WordNet
Conference, Brno, Czech Republic.
D. Bollegala, Matsuo Y., and M. Ishizuka. 2007. Mea-
suring semantic similarity between words using web
search engines. In Proceedings of WWW’2007.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based Measures of Lexical Semantic Relatedness.
Computational Linguistics, 32(1):13–47.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking. In
Proceedings of COCLING/ACL 2006.
J. Daude, L. Padro, and G. Rigau. 2000. Mapping Word-
Nets using structural information. In Proceedings of
ACL’2000, Hong Kong.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press,
Cambridge, Mass.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing Search in Context: The Concept Revisited. ACM
Transactions on Information Systems, 20(1):116–131.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness using Wikipedia-based Explicit
Semantic Analysis. Proc of IJCAI, pages 6–12.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW ’02: Proceedings of the 11th international con-
ference on World Wide Web, pages 517–526.
T. Hughes and D. Ramage. 2007. Lexical semantic re-
latedness with random graph walks. In Proceedings of
EMNLP-CoNLL-2007, pages 581–589.
M. Jarmasz. 2003. Roget’s Thesuarus as a lexical re-
source for Natural Language Processing.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of International Conference on Research
in Computational Linguistics, volume 33. Taiwan.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense iden-
tification. WordNet: An Electronic Lexical Database,
49(2):265–283.
D. Lin. 1998a. An information-theoretic definition of
similarity. In Proc. of ICML, pages 296–304, Wiscon-
sin, USA.
D. Lin. 1998b. Automatic Retrieval and Clustering of
Similar Words. In Proceedings of ACL-98.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1–28.
J. Nivre. 2006. Inductive Dependency Parsing, vol-
ume 34 of Text, Speech and Language Technology.
Springer.
S. Pado and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161–199.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Re-
latedness of Concepts. In Proceedings of the EACL
Workshop on Making Sense of Sense: Bringing Com-
putational Linguistics and Pycholinguistics Together,
pages 1–8, Trento, Italy.
P. Resnik. 1995. Using Information Content to Evaluate
Semantic Similarity in a Taxonomy. Proc. of IJCAI,
14:448–453.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627–633.
M Ruiz-Casado, E. Alfonseca, and P. Castells. 2005.
Using context-window overlapping in Synonym Dis-
covery and Ontology Extension. In Proceedings of
RANLP-2005, Borovets, Bulgaria,.
M. Sahami and T.D. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. Proc. of WWW, pages 377–386.
M. Strube and S.P. Ponzetto. 2006. WikiRelate! Com-
puting Semantic Relatedness Using Wikipedia. In
Proceedings of the AAAI-2006, pages 1419–1424.
P.D. Turney. 2001. Mining the Web for Synonyms: PMI-
IR versus LSA on TOEFL. Lecture Notes in Computer
Science, 2167:491–502.
P. Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. Kluwer
Academic Publishers.
Z. Wu and M. Palmer. 1994. Verb semantics and lex-
ical selection. In Proc. of ACL, pages 133–138, Las
Cruces, New Mexico.
D. Yang and D.M.W. Powers. 2005. Measuring semantic
similarity in the taxonomy of WordNet. Proceedings
of the Australasian conference on Computer Science.
</reference>
<page confidence="0.998795">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.285331">
<title confidence="0.999045">A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches</title>
<author confidence="0.776426">Marius</author>
<degree confidence="0.563143">NLP Group, University of the Basque of Formal and Applied Linguistics, Charles University in</degree>
<email confidence="0.718334">kravalova@ufal.mff.cuni.cz</email>
<abstract confidence="0.998643357142857">This paper presents and compares WordNetbased and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>A Soroa</author>
</authors>
<title>Personalizing pagerank for word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proc. of EACL 2009,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="5264" citStr="Agirre and Soroa, 2009" startWordPosition="793" endWordPosition="796">raph G (Haveliwala, 2002). Basically, personalized PageRank is computed by modifying the random jump distribution vector in the traditional PageRank equation. In our case, we concentrate all probability mass in the target word. Regarding PageRank implementation details, we chose a damping value of 0.85 and finish the calculation after 30 iterations. These are default values, and we did not optimize them. Our similarity method is similar, but simpler, to that used by (Hughes and Ramage, 2007), which report very good results on similarity datasets. More details of our algorithm can be found in (Agirre and Soroa, 2009). The algorithm and needed resouces are publicly available1. 2.1 WordNet relations and versions The WordNet versions that we use in this work are the Multilingual Central Repository or MCR (Atserias et al., 2004) (which includes English WordNet version 1.6 and wordnets for several other languages like Spanish, Italian, Catalan and Basque), and WordNet version 3.02. We used all the relations in MCR (except cooccurrence relations and selectional preference relations) and in WordNet 3.0. Given the recent availability of the disambiguated gloss relations for WordNet 3.03, we also used a version wh</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>E. Agirre and A. Soroa. 2009. Personalizing pagerank for word sense disambiguation. In Proc. of EACL 2009, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Alvarez</author>
<author>S J Lim</author>
</authors>
<title>A Graph Modeling of Semantic Similarity between Words.</title>
<date>2007</date>
<booktitle>Proc. of the Conference on Semantic Computing,</booktitle>
<pages>355--362</pages>
<contexts>
<context position="1513" citStr="Alvarez and Lim, 2007" startWordPosition="211" endWordPosition="214">hat our methods are easily adapted for a cross-lingual task with minor losses. 1 Introduction Measuring semantic similarity and relatedness between terms is an important problem in lexical semantics. It has applications in many natural language processing tasks, such as Textual Entailment, Word Sense Disambiguation or Information Extraction, and other related areas like Information Retrieval. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar p</context>
<context position="29392" citStr="Alvarez and Lim, 2007" startWordPosition="4688" endWordPosition="4691">62 [0.32, 0.81] 0.58 [0.26, 0.78] (Chen et al., 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85] (Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89] (Leacock et al., 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91] (Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90] (Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92] (Bollegala et al., 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92] (Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93] (Jarmasz, 2003) Roget’s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94] (Patwardhan et al., 2006) WordNet n/a 0.91 (Alvarez and Lim, 2007) WordNet n/a 0.91 (Yang and Powers, 2005) WordNet 0.87 [0.73, 0.91] 0.92 [0.84, 0.96] (Hughes et al., 2007) WordNet 0.90 n/a Personalized PageRank WordNet 0.89 [0.77, 0.94] n/a Bag of words Web corpus 0.85 [0.70, 0.93] 0.84 [0.69, 0.93] Context window Web corpus 0.88 [0.76, 0.95] 0.89 [0.77, 0.95] Syntactic contexts Web corpus 0.76 [0.54, 0.88] 0.74 [0.51, 0.87] SVM Web, WN 0.92 [0.84, 0.96] 0.93 [0.85, 0.97] Table 7: Comparison with previous approaches for MC. not have a held-out set, so we used the standard settings of Weka, without trying to modify parameters, e.g. C. Each word pair is scor</context>
<context position="30692" citStr="Alvarez and Lim (2007)" startWordPosition="4906" endWordPosition="4909"> the SVM. The results using 10-fold crossvalidation are shown in Table 6. A combination of all methods produces the best results reported so far for both datasets, statistically significant for RG. 6 Related work Contrary to the WordSim353 dataset, common practice with the RG dataset has been to perform the evaluation with Pearson correlation. In our believe Pearson is less informative, as the Pearson correlation suffers much when the scores of two systems are not linearly correlated, something which happens often given due to the different nature of the techniques applied. Some authors, e.g. Alvarez and Lim (2007), use a non-linear function to map the system outputs into new values distributed more similarly to the values in the gold-standard. In their case, the mapping function was exp (−&apos;4 ), which was chosen empirically. Finding such a function is dependent on the dataset used, and involves an extra step in the similarity calculations. Alternatively, the Spearman correlation provides an evaluation metric that is independent of such data-dependent transformations. Most similarity researchers have published their 25 Table 8: Our best results for the MC dataset. Method Source Spearman (Strube and Ponze</context>
</contexts>
<marker>Alvarez, Lim, 2007</marker>
<rawString>M.A. Alvarez and S.J. Lim. 2007. A Graph Modeling of Semantic Similarity between Words. Proc. of the Conference on Semantic Computing, pages 355–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Atserias</author>
<author>L Villarejo</author>
<author>G Rigau</author>
<author>E Agirre</author>
<author>J Carroll</author>
<author>B Magnini</author>
<author>P Vossen</author>
</authors>
<title>The meaning multilingual central repository.</title>
<date>2004</date>
<booktitle>In Proc. of Global WordNet Conference,</booktitle>
<location>Brno, Czech Republic.</location>
<contexts>
<context position="5476" citStr="Atserias et al., 2004" startWordPosition="826" endWordPosition="830">e target word. Regarding PageRank implementation details, we chose a damping value of 0.85 and finish the calculation after 30 iterations. These are default values, and we did not optimize them. Our similarity method is similar, but simpler, to that used by (Hughes and Ramage, 2007), which report very good results on similarity datasets. More details of our algorithm can be found in (Agirre and Soroa, 2009). The algorithm and needed resouces are publicly available1. 2.1 WordNet relations and versions The WordNet versions that we use in this work are the Multilingual Central Repository or MCR (Atserias et al., 2004) (which includes English WordNet version 1.6 and wordnets for several other languages like Spanish, Italian, Catalan and Basque), and WordNet version 3.02. We used all the relations in MCR (except cooccurrence relations and selectional preference relations) and in WordNet 3.0. Given the recent availability of the disambiguated gloss relations for WordNet 3.03, we also used a version which incorporates these relations. We will refer to the three versions as MCR16, WN30 and WN30g, respectively. Our choice was mainly motivated by the fact that MCR contains tightly aligned 1http://http://ixa2.si.e</context>
</contexts>
<marker>Atserias, Villarejo, Rigau, Agirre, Carroll, Magnini, Vossen, 2004</marker>
<rawString>J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll, B. Magnini, and P. Vossen. 2004. The meaning multilingual central repository. In Proc. of Global WordNet Conference, Brno, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bollegala</author>
<author>Y Matsuo</author>
<author>M Ishizuka</author>
</authors>
<title>Measuring semantic similarity between words using web search engines.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW’2007.</booktitle>
<contexts>
<context position="1699" citStr="Bollegala et al., 2007" startWordPosition="243" endWordPosition="246">ical semantics. It has applications in many natural language processing tasks, such as Textual Entailment, Word Sense Disambiguation or Information Extraction, and other related areas like Information Retrieval. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problems that stem from the imbalance and sparsenes</context>
<context position="8078" citStr="Bollegala et al. (2007)" startWordPosition="1238" endWordPosition="1241">0, including glosses, to construct the graph. The two Spanish WordNet versions are referred to as MCR16 and WN30g. 3 Context-based methods In this section, we describe the distributional methods used for calculating similarities between words, and profiting from the use of a large Web-based corpus. This work is motivated by previous studies that make use of search engines in order to collect cooccurrence statistics between words. Turney (2001) uses the number of hits returned by a Web search engine to calculate the Pointwise Mutual Information (PMI) between terms, as an indicator of synonymy. Bollegala et al. (2007) calculate a number of popular relatedness metrics based on page counts, 4http://www.lsi.upc.es/∼nlp/tools/download-map.php. 20 like PMI, the Jaccard coefficient, the Simpson coefficient and the Dice coefficient, which are combined with lexico-syntactic patterns as model features. The model parameters are trained using Support Vector Machines (SVM) in order to later rank pairs of words. A different approach is the one taken by Sahami and Heilman (2006), who collect snippets from the results of a search engine and represent each snippet as a vector, weighted with the tf·idf score. The semantic </context>
<context position="29146" citStr="Bollegala et al., 2007" startWordPosition="4649" endWordPosition="4652">econd pair. We have trained a SVM to classify pairs of pairs, and use its output to rank the entries in both datasets. It uses a polynomial kernel with degree 4. We did Method Source Spearman (MC) Pearson (MC) (Sahami et al., 2006) Web snippets 0.62 [0.32, 0.81] 0.58 [0.26, 0.78] (Chen et al., 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85] (Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89] (Leacock et al., 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91] (Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90] (Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92] (Bollegala et al., 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92] (Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93] (Jarmasz, 2003) Roget’s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94] (Patwardhan et al., 2006) WordNet n/a 0.91 (Alvarez and Lim, 2007) WordNet n/a 0.91 (Yang and Powers, 2005) WordNet 0.87 [0.73, 0.91] 0.92 [0.84, 0.96] (Hughes et al., 2007) WordNet 0.90 n/a Personalized PageRank WordNet 0.89 [0.77, 0.94] n/a Bag of words Web corpus 0.85 [0.70, 0.93] 0.84 [0.69, 0.93] Context window Web corpus 0.88 [0.76, 0.95] 0.89 [0.77, 0.95] Syntactic contexts Web corpus 0.76 [0.54, 0.88] 0.74 [0</context>
</contexts>
<marker>Bollegala, Matsuo, Ishizuka, 2007</marker>
<rawString>D. Bollegala, Matsuo Y., and M. Ishizuka. 2007. Measuring semantic similarity between words using web search engines. In Proceedings of WWW’2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNetbased Measures of Lexical Semantic Relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="2231" citStr="Budanitsky and Hirst (2006)" startWordPosition="327" endWordPosition="330">ies of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problems that stem from the imbalance and sparseness of the corpora. Comparatively, the experiments in this paper demonstrate that distributional similarities can perform as well as the knowledge-based approaches, and a combination of the two can exceed the performance of results previously reported on the same datasets. An application to cross-lingual (CL) similarity identification is also described, with applications such as CL Information Retrieval or CL sponsored search. A discussion on the differences between learning similarity and relatedness scores is provided. The pap</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>A. Budanitsky and G. Hirst. 2006. Evaluating WordNetbased Measures of Lexical Semantic Relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Chen</author>
<author>M Lin</author>
<author>Y Wei</author>
</authors>
<title>Novel association measures using web search with double checking.</title>
<date>2006</date>
<booktitle>In Proceedings of COCLING/ACL</booktitle>
<contexts>
<context position="1674" citStr="Chen et al., 2006" startWordPosition="239" endWordPosition="242">tant problem in lexical semantics. It has applications in many natural language processing tasks, such as Textual Entailment, Word Sense Disambiguation or Information Extraction, and other related areas like Information Retrieval. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problems that stem from th</context>
<context position="28823" citStr="Chen et al., 2006" startWordPosition="4597" endWordPosition="4600"> two pairs were 0.001364 and 0.327515 respectively, which ranked them in positions 31 and 64. The second classifier gave them similarities of 0.084805 and 0.109061 respectively, which ranked them in positions 57 and 59. The class negative indicates that in the gold-standard the first pair has a lower score than the second pair. We have trained a SVM to classify pairs of pairs, and use its output to rank the entries in both datasets. It uses a polynomial kernel with degree 4. We did Method Source Spearman (MC) Pearson (MC) (Sahami et al., 2006) Web snippets 0.62 [0.32, 0.81] 0.58 [0.26, 0.78] (Chen et al., 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85] (Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89] (Leacock et al., 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91] (Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90] (Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92] (Bollegala et al., 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92] (Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93] (Jarmasz, 2003) Roget’s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94] (Patwardhan et al., 2006) WordNet n/a 0.91 (Alvarez and Lim, 2007) WordNet n/a 0.91 (Yang and Pow</context>
</contexts>
<marker>Chen, Lin, Wei, 2006</marker>
<rawString>H. Chen, M. Lin, and Y. Wei. 2006. Novel association measures using web search with double checking. In Proceedings of COCLING/ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Daude</author>
<author>L Padro</author>
<author>G Rigau</author>
</authors>
<title>Mapping WordNets using structural information.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL’2000,</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="7304" citStr="Daude et al., 2000" startWordPosition="1106" endWordPosition="1109">uage dependent). As our WordNet-based method uses the graph of the concepts and relations, we can easily compute the similarity between words from different languages. For example, consider a English-Spanish pair like car – coche. Given that the Spanish WordNet is included in MCR we can use MCR as the common knowledge-base for the relations. We can then compute the personalized PageRank for each of car and coche on the same underlying graph, and then compare the similarity between both probability distributions. As an alternative, we also tried to use publicly available mappings for wordnets (Daude et al., 2000)4 in order to create a 3.0 version of the Spanish WordNet. The mapping was used to link Spanish variants to 3.0 synsets. We used the English WordNet 3.0, including glosses, to construct the graph. The two Spanish WordNet versions are referred to as MCR16 and WN30g. 3 Context-based methods In this section, we describe the distributional methods used for calculating similarities between words, and profiting from the use of a large Web-based corpus. This work is motivated by previous studies that make use of search engines in order to collect cooccurrence statistics between words. Turney (2001) u</context>
</contexts>
<marker>Daude, Padro, Rigau, 2000</marker>
<rawString>J. Daude, L. Padro, and G. Rigau. 2000. Mapping WordNets using structural information. In Proceedings of ACL’2000, Hong Kong.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database and Some of its Applications.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database and Some of its Applications. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Y Matias</author>
<author>E Rivlin</author>
<author>Z Solan</author>
<author>G Wolfman</author>
<author>E Ruppin</author>
</authors>
<title>Placing Search in Context: The Concept Revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="15189" citStr="Finkelstein et al., 2002" startWordPosition="2423" endWordPosition="2426">Spanish word is calculated by collecting features from all the contexts of any of its translations. 3. Once the vectors are generated, the similarities are calculated in the same way as before. 4 Experimental results 4.1 Gold-standard datasets We have used two standard datasets. The first one, RG, consists of 65 pairs of words collected by Rubenstein and Goodenough (1965), who had them judged by 51 human subjects in a scale from 0.0 to 4.0 according to their similarity, but ignoring any other possible semantic relationships that might appear between the terms. The second dataset, WordSim3535 (Finkelstein et al., 2002) contains 353 word pairs, each associated with an average of 13 to 16 human judgements. In this case, both similarity and re5Available at http://www.cs.technion.ac.il/ ∼gabr/resources/data/wordsim353/wordsim353.html Context RG terms and frequencies ll never forget the&apos; on his face when grin,2,smile,10 he had a giant&apos; on his face and grin,3,smile,2 room with a huge&apos; on her face and grin,2,smile,6 the state of every&apos; will be updated every automobile,2,car,3 repair or replace the&apos; if it is stolen automobile,2,car,2 located on the north&apos; of the Bay of shore,14,coast,2 areas on the eastern&apos; of the </context>
<context position="31451" citStr="Finkelstein et al., 2002" startWordPosition="5019" endWordPosition="5022">n their case, the mapping function was exp (−&apos;4 ), which was chosen empirically. Finding such a function is dependent on the dataset used, and involves an extra step in the similarity calculations. Alternatively, the Spearman correlation provides an evaluation metric that is independent of such data-dependent transformations. Most similarity researchers have published their 25 Table 8: Our best results for the MC dataset. Method Source Spearman (Strube and Ponzetto, 2006) Wikipedia 0.19–0.48 (Jarmasz, 2003) WordNet 0.33–0.35 (Jarmasz, 2003) Roget’s 0.55 (Hughes and Ramage, 2007) WordNet 0.55 (Finkelstein et al., 2002) Web corpus, WN 0.56 (Gabrilovich and Markovitch, 2007) ODP 0.65 (Gabrilovich and Markovitch, 2007) Wikipedia 0.75 SVM Web corpus, WN 0.78 Table 9: Comparison with previous work for WordSim353. complete results on a smaller subset of the RG dataset containing 30 word pairs (Miller and Charles, 1991), usually referred to as MC, making it possible to compare different systems using different correlation. Table 7 shows the results of related work on MC that was available to us, including our own. For the authors that did not provide the detailed data we include only the Pearson correlation with n</context>
<context position="33140" citStr="Finkelstein et al. (2002)" startWordPosition="5296" endWordPosition="5299">rdSim353 dataset is easier, as all researchers have used Spearman. The figures in Table 9) show that our WordNet-based method outperforms all previously published WordNet methods. We want to note that our WordNetbased method outperforms that of Hughes and Ramage (2007), which uses a similar method. Although there are some differences in the method, we think that the main performance gain comes from the use of the disambiguated glosses, which they did not use. Our distributional methods also outperform all other corpus-based methods. The most similar approach to our distributional technique is Finkelstein et al. (2002), who combined distributional similarities from Web documents with a similarity from WordNet. Their results are probably worse due to the smaller data size (they used 270,000 documents) and the differences in the calculation of the similarities. The only method which outperforms our non-supervised methods is that of (Gabrilovich and Markovitch, 2007) when based on Wikipedia, probably because of the dense, manually distilled knowledge contained in Wikipedia. All in all, our supervised combination gets the best published results on this dataset. 7 Conclusions and future work This paper has prese</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing Search in Context: The Concept Revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>Proc of IJCAI,</booktitle>
<pages>6--12</pages>
<contexts>
<context position="31506" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="5027" endWordPosition="5030"> ), which was chosen empirically. Finding such a function is dependent on the dataset used, and involves an extra step in the similarity calculations. Alternatively, the Spearman correlation provides an evaluation metric that is independent of such data-dependent transformations. Most similarity researchers have published their 25 Table 8: Our best results for the MC dataset. Method Source Spearman (Strube and Ponzetto, 2006) Wikipedia 0.19–0.48 (Jarmasz, 2003) WordNet 0.33–0.35 (Jarmasz, 2003) Roget’s 0.55 (Hughes and Ramage, 2007) WordNet 0.55 (Finkelstein et al., 2002) Web corpus, WN 0.56 (Gabrilovich and Markovitch, 2007) ODP 0.65 (Gabrilovich and Markovitch, 2007) Wikipedia 0.75 SVM Web corpus, WN 0.78 Table 9: Comparison with previous work for WordSim353. complete results on a smaller subset of the RG dataset containing 30 word pairs (Miller and Charles, 1991), usually referred to as MC, making it possible to compare different systems using different correlation. Table 7 shows the results of related work on MC that was available to us, including our own. For the authors that did not provide the detailed data we include only the Pearson correlation with no confidence intervals. Among the unsupervised methods </context>
<context position="33492" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="5349" endWordPosition="5352">ethod, we think that the main performance gain comes from the use of the disambiguated glosses, which they did not use. Our distributional methods also outperform all other corpus-based methods. The most similar approach to our distributional technique is Finkelstein et al. (2002), who combined distributional similarities from Web documents with a similarity from WordNet. Their results are probably worse due to the smaller data size (they used 270,000 documents) and the differences in the calculation of the similarities. The only method which outperforms our non-supervised methods is that of (Gabrilovich and Markovitch, 2007) when based on Wikipedia, probably because of the dense, manually distilled knowledge contained in Wikipedia. All in all, our supervised combination gets the best published results on this dataset. 7 Conclusions and future work This paper has presented two state-of-the-art distributional and WordNet-based similarity measures, with a study of several parameters, including performance on similarity and relatedness data. We show that the use of disambiguated glosses allows for the best published results for WordNet-based systems on the WordSim353 dataset, mainly due to the better modeling of rela</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>E. Gabrilovich and S. Markovitch. 2007. Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis. Proc of IJCAI, pages 6–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H Haveliwala</author>
</authors>
<title>Topic-sensitive pagerank.</title>
<date>2002</date>
<booktitle>In WWW ’02: Proceedings of the 11th international conference on World Wide Web,</booktitle>
<pages>517--526</pages>
<contexts>
<context position="4666" citStr="Haveliwala, 2002" startWordPosition="697" endWordPosition="698">parately for each of the words, producing a probability distribution over WordNet synsets. We then compare how similar these two discrete probability distributions are by encoding them as vectors and computing the cosine between the vectors. We represent WordNet as a graph G = (V, E) as follows: graph nodes represent WordNet concepts (synsets) and dictionary words; relations among synsets are represented by undirected edges; and dictionary words are linked to the synsets associated to them by directed edges. For each word in the pair we first compute a personalized PageRank vector of graph G (Haveliwala, 2002). Basically, personalized PageRank is computed by modifying the random jump distribution vector in the traditional PageRank equation. In our case, we concentrate all probability mass in the target word. Regarding PageRank implementation details, we chose a damping value of 0.85 and finish the calculation after 30 iterations. These are default values, and we did not optimize them. Our similarity method is similar, but simpler, to that used by (Hughes and Ramage, 2007), which report very good results on similarity datasets. More details of our algorithm can be found in (Agirre and Soroa, 2009). </context>
</contexts>
<marker>Haveliwala, 2002</marker>
<rawString>T. H. Haveliwala. 2002. Topic-sensitive pagerank. In WWW ’02: Proceedings of the 11th international conference on World Wide Web, pages 517–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hughes</author>
<author>D Ramage</author>
</authors>
<title>Lexical semantic relatedness with random graph walks.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL-2007,</booktitle>
<pages>581--589</pages>
<contexts>
<context position="1562" citStr="Hughes and Ramage, 2007" startWordPosition="220" endWordPosition="223">-lingual task with minor losses. 1 Introduction Measuring semantic similarity and relatedness between terms is an important problem in lexical semantics. It has applications in many natural language processing tasks, such as Textual Entailment, Word Sense Disambiguation or Information Extraction, and other related areas like Information Retrieval. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar phrases like captive-westerner which made sense on</context>
<context position="5137" citStr="Hughes and Ramage, 2007" startWordPosition="770" endWordPosition="774">the synsets associated to them by directed edges. For each word in the pair we first compute a personalized PageRank vector of graph G (Haveliwala, 2002). Basically, personalized PageRank is computed by modifying the random jump distribution vector in the traditional PageRank equation. In our case, we concentrate all probability mass in the target word. Regarding PageRank implementation details, we chose a damping value of 0.85 and finish the calculation after 30 iterations. These are default values, and we did not optimize them. Our similarity method is similar, but simpler, to that used by (Hughes and Ramage, 2007), which report very good results on similarity datasets. More details of our algorithm can be found in (Agirre and Soroa, 2009). The algorithm and needed resouces are publicly available1. 2.1 WordNet relations and versions The WordNet versions that we use in this work are the Multilingual Central Repository or MCR (Atserias et al., 2004) (which includes English WordNet version 1.6 and wordnets for several other languages like Spanish, Italian, Catalan and Basque), and WordNet version 3.02. We used all the relations in MCR (except cooccurrence relations and selectional preference relations) and</context>
<context position="31411" citStr="Hughes and Ramage, 2007" startWordPosition="5013" endWordPosition="5016">y to the values in the gold-standard. In their case, the mapping function was exp (−&apos;4 ), which was chosen empirically. Finding such a function is dependent on the dataset used, and involves an extra step in the similarity calculations. Alternatively, the Spearman correlation provides an evaluation metric that is independent of such data-dependent transformations. Most similarity researchers have published their 25 Table 8: Our best results for the MC dataset. Method Source Spearman (Strube and Ponzetto, 2006) Wikipedia 0.19–0.48 (Jarmasz, 2003) WordNet 0.33–0.35 (Jarmasz, 2003) Roget’s 0.55 (Hughes and Ramage, 2007) WordNet 0.55 (Finkelstein et al., 2002) Web corpus, WN 0.56 (Gabrilovich and Markovitch, 2007) ODP 0.65 (Gabrilovich and Markovitch, 2007) Wikipedia 0.75 SVM Web corpus, WN 0.78 Table 9: Comparison with previous work for WordSim353. complete results on a smaller subset of the RG dataset containing 30 word pairs (Miller and Charles, 1991), usually referred to as MC, making it possible to compare different systems using different correlation. Table 7 shows the results of related work on MC that was available to us, including our own. For the authors that did not provide the detailed data we inc</context>
<context position="32784" citStr="Hughes and Ramage (2007)" startWordPosition="5239" endWordPosition="5243">the best reported Spearman correlation, although the 0.95 confidence intervals are too large to allow us to accept the hypothesis that it is better than all others methods. The supervised combination produces the best results reported so far. For the benefit of future research, our results for the MC subset are displayed in Table 8. Comparison on the WordSim353 dataset is easier, as all researchers have used Spearman. The figures in Table 9) show that our WordNet-based method outperforms all previously published WordNet methods. We want to note that our WordNetbased method outperforms that of Hughes and Ramage (2007), which uses a similar method. Although there are some differences in the method, we think that the main performance gain comes from the use of the disambiguated glosses, which they did not use. Our distributional methods also outperform all other corpus-based methods. The most similar approach to our distributional technique is Finkelstein et al. (2002), who combined distributional similarities from Web documents with a similarity from WordNet. Their results are probably worse due to the smaller data size (they used 270,000 documents) and the differences in the calculation of the similarities</context>
</contexts>
<marker>Hughes, Ramage, 2007</marker>
<rawString>T. Hughes and D. Ramage. 2007. Lexical semantic relatedness with random graph walks. In Proceedings of EMNLP-CoNLL-2007, pages 581–589.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jarmasz</author>
</authors>
<title>Roget’s Thesuarus as a lexical resource for Natural Language Processing.</title>
<date>2003</date>
<contexts>
<context position="29281" citStr="Jarmasz, 2003" startWordPosition="4672" endWordPosition="4673">el with degree 4. We did Method Source Spearman (MC) Pearson (MC) (Sahami et al., 2006) Web snippets 0.62 [0.32, 0.81] 0.58 [0.26, 0.78] (Chen et al., 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85] (Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89] (Leacock et al., 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91] (Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90] (Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92] (Bollegala et al., 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92] (Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93] (Jarmasz, 2003) Roget’s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94] (Patwardhan et al., 2006) WordNet n/a 0.91 (Alvarez and Lim, 2007) WordNet n/a 0.91 (Yang and Powers, 2005) WordNet 0.87 [0.73, 0.91] 0.92 [0.84, 0.96] (Hughes et al., 2007) WordNet 0.90 n/a Personalized PageRank WordNet 0.89 [0.77, 0.94] n/a Bag of words Web corpus 0.85 [0.70, 0.93] 0.84 [0.69, 0.93] Context window Web corpus 0.88 [0.76, 0.95] 0.89 [0.77, 0.95] Syntactic contexts Web corpus 0.76 [0.54, 0.88] 0.74 [0.51, 0.87] SVM Web, WN 0.92 [0.84, 0.96] 0.93 [0.85, 0.97] Table 7: Comparison with previous approaches for MC. not have a held-out set</context>
<context position="31338" citStr="Jarmasz, 2003" startWordPosition="5005" endWordPosition="5006">ap the system outputs into new values distributed more similarly to the values in the gold-standard. In their case, the mapping function was exp (−&apos;4 ), which was chosen empirically. Finding such a function is dependent on the dataset used, and involves an extra step in the similarity calculations. Alternatively, the Spearman correlation provides an evaluation metric that is independent of such data-dependent transformations. Most similarity researchers have published their 25 Table 8: Our best results for the MC dataset. Method Source Spearman (Strube and Ponzetto, 2006) Wikipedia 0.19–0.48 (Jarmasz, 2003) WordNet 0.33–0.35 (Jarmasz, 2003) Roget’s 0.55 (Hughes and Ramage, 2007) WordNet 0.55 (Finkelstein et al., 2002) Web corpus, WN 0.56 (Gabrilovich and Markovitch, 2007) ODP 0.65 (Gabrilovich and Markovitch, 2007) Wikipedia 0.75 SVM Web corpus, WN 0.78 Table 9: Comparison with previous work for WordSim353. complete results on a smaller subset of the RG dataset containing 30 word pairs (Miller and Charles, 1991), usually referred to as MC, making it possible to compare different systems using different correlation. Table 7 shows the results of related work on MC that was available to us, includi</context>
</contexts>
<marker>Jarmasz, 2003</marker>
<rawString>M. Jarmasz. 2003. Roget’s Thesuarus as a lexical resource for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of International Conference on Research in Computational Linguistics,</booktitle>
<volume>33</volume>
<contexts>
<context position="29221" citStr="Jiang and Conrath, 1997" startWordPosition="4661" endWordPosition="4664">output to rank the entries in both datasets. It uses a polynomial kernel with degree 4. We did Method Source Spearman (MC) Pearson (MC) (Sahami et al., 2006) Web snippets 0.62 [0.32, 0.81] 0.58 [0.26, 0.78] (Chen et al., 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85] (Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89] (Leacock et al., 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91] (Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90] (Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92] (Bollegala et al., 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92] (Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93] (Jarmasz, 2003) Roget’s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94] (Patwardhan et al., 2006) WordNet n/a 0.91 (Alvarez and Lim, 2007) WordNet n/a 0.91 (Yang and Powers, 2005) WordNet 0.87 [0.73, 0.91] 0.92 [0.84, 0.96] (Hughes et al., 2007) WordNet 0.90 n/a Personalized PageRank WordNet 0.89 [0.77, 0.94] n/a Bag of words Web corpus 0.85 [0.70, 0.93] 0.84 [0.69, 0.93] Context window Web corpus 0.88 [0.76, 0.95] 0.89 [0.77, 0.95] Syntactic contexts Web corpus 0.76 [0.54, 0.88] 0.74 [0.51, 0.87] SVM Web, WN 0.92 [0.84, 0.96] 0.93 [0.85, 0.97] Table 7: Compari</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J.J. Jiang and D.W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of International Conference on Research in Computational Linguistics, volume 33. Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification. WordNet: An Electronic Lexical Database,</title>
<date>1998</date>
<volume>49</volume>
<issue>2</issue>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>C. Leacock and M. Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. WordNet: An Electronic Lexical Database, 49(2):265–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>296--304</pages>
<location>Wisconsin, USA.</location>
<contexts>
<context position="2095" citStr="Lin (1998" startWordPosition="308" endWordPosition="309">ias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problems that stem from the imbalance and sparseness of the corpora. Comparatively, the experiments in this paper demonstrate that distributional similarities can perform as well as the knowledge-based approaches, and a combination of the two can exceed the performance of results previously reported on the same datasets. An application to cross-lingual (CL) similarity identification is also described, with applications such as CL Information R</context>
<context position="29075" citStr="Lin, 1998" startWordPosition="4640" endWordPosition="4641">gold-standard the first pair has a lower score than the second pair. We have trained a SVM to classify pairs of pairs, and use its output to rank the entries in both datasets. It uses a polynomial kernel with degree 4. We did Method Source Spearman (MC) Pearson (MC) (Sahami et al., 2006) Web snippets 0.62 [0.32, 0.81] 0.58 [0.26, 0.78] (Chen et al., 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85] (Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89] (Leacock et al., 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91] (Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90] (Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92] (Bollegala et al., 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92] (Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93] (Jarmasz, 2003) Roget’s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94] (Patwardhan et al., 2006) WordNet n/a 0.91 (Alvarez and Lim, 2007) WordNet n/a 0.91 (Yang and Powers, 2005) WordNet 0.87 [0.73, 0.91] 0.92 [0.84, 0.96] (Hughes et al., 2007) WordNet 0.90 n/a Personalized PageRank WordNet 0.89 [0.77, 0.94] n/a Bag of words Web corpus 0.85 [0.70, 0.93] 0.84 [0.69, 0.93] Context window Web corpus 0.88 [0.76, 0.95] 0.</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998a. An information-theoretic definition of similarity. In Proc. of ICML, pages 296–304, Wisconsin, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL-98.</booktitle>
<contexts>
<context position="2095" citStr="Lin (1998" startWordPosition="308" endWordPosition="309">ias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problems that stem from the imbalance and sparseness of the corpora. Comparatively, the experiments in this paper demonstrate that distributional similarities can perform as well as the knowledge-based approaches, and a combination of the two can exceed the performance of results previously reported on the same datasets. An application to cross-lingual (CL) similarity identification is also described, with applications such as CL Information R</context>
<context position="29075" citStr="Lin, 1998" startWordPosition="4640" endWordPosition="4641">gold-standard the first pair has a lower score than the second pair. We have trained a SVM to classify pairs of pairs, and use its output to rank the entries in both datasets. It uses a polynomial kernel with degree 4. We did Method Source Spearman (MC) Pearson (MC) (Sahami et al., 2006) Web snippets 0.62 [0.32, 0.81] 0.58 [0.26, 0.78] (Chen et al., 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85] (Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89] (Leacock et al., 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91] (Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90] (Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92] (Bollegala et al., 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92] (Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93] (Jarmasz, 2003) Roget’s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94] (Patwardhan et al., 2006) WordNet n/a 0.91 (Alvarez and Lim, 2007) WordNet n/a 0.91 (Yang and Powers, 2005) WordNet 0.87 [0.73, 0.91] 0.92 [0.84, 0.96] (Hughes et al., 2007) WordNet 0.90 n/a Personalized PageRank WordNet 0.89 [0.77, 0.94] n/a Bag of words Web corpus 0.85 [0.70, 0.93] 0.84 [0.69, 0.93] Context window Web corpus 0.88 [0.76, 0.95] 0.</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998b. Automatic Retrieval and Clustering of Similar Words. In Proceedings of ACL-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>W G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="16385" citStr="Miller and Charles, 1991" startWordPosition="2590" endWordPosition="2593">reas on the eastern&apos; of the Adriatic Sea shore,3,coast,2 Thesaurus of Current English&apos; The Oxford Pocket Thesaurus slave,3,boy,5,shore,3,string,2 wizard,4,glass,4,crane,5,smile,5 implement,5,oracle,2,lad,2 food,3,car,2,madhouse,3,jewel,3 asylum,4,tool,8,journey,6,etc. be understood that the&apos; 10 may be designed crane,3,tool,3 a fight between a &apos; and a snake and bird,3,crane,5 Table 2: Sample of context windows for the terms in the RG dataset. latedness are annotated without any distinction. Several studies indicate that the human scores consistently have very high correlations with each other (Miller and Charles, 1991; Resnik, 1995), thus validating the use of these datasets for evaluating semantic similarity. For the cross-lingual evaluation, the two datasets were modified by translating the second word in each pair into Spanish. Two humans translated simultaneously both datasets, with an inter-tagger agreement of 72% for RG and 84% for WordSim353. 4.2 Results Table 1 shows the Spearman correlation obtained on the RG and WordSim353 datasets, including the interval at 0.95 of confidence6. Overall the distributional context-window approach performs best in the RG, reaching 0.89 correlation, and both WN30g a</context>
<context position="31751" citStr="Miller and Charles, 1991" startWordPosition="5066" endWordPosition="5069">-dependent transformations. Most similarity researchers have published their 25 Table 8: Our best results for the MC dataset. Method Source Spearman (Strube and Ponzetto, 2006) Wikipedia 0.19–0.48 (Jarmasz, 2003) WordNet 0.33–0.35 (Jarmasz, 2003) Roget’s 0.55 (Hughes and Ramage, 2007) WordNet 0.55 (Finkelstein et al., 2002) Web corpus, WN 0.56 (Gabrilovich and Markovitch, 2007) ODP 0.65 (Gabrilovich and Markovitch, 2007) Wikipedia 0.75 SVM Web corpus, WN 0.78 Table 9: Comparison with previous work for WordSim353. complete results on a smaller subset of the RG dataset containing 30 word pairs (Miller and Charles, 1991), usually referred to as MC, making it possible to compare different systems using different correlation. Table 7 shows the results of related work on MC that was available to us, including our own. For the authors that did not provide the detailed data we include only the Pearson correlation with no confidence intervals. Among the unsupervised methods introduced in this paper, the context window produced the best reported Spearman correlation, although the 0.95 confidence intervals are too large to allow us to accept the hypothesis that it is better than all others methods. The supervised com</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>G.A. Miller and W.G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Inductive Dependency Parsing,</title>
<date>2006</date>
<journal>Text, Speech and Language</journal>
<volume>34</volume>
<publisher>Technology. Springer.</publisher>
<contexts>
<context position="10758" citStr="Nivre (2006)" startWordPosition="1700" endWordPosition="1701"> we collect every window W centered in w (removing the central word), and add it to the vector together with its frequency (the total number of times we saw window W around w in the whole corpus). In this case, all punctuation symbols are replaced with a special token, to unify patterns like, the &lt;term&gt; said to and ’ the &lt;term&gt; said to. Throughout the paper, when we mention a context window of size N it means N words at each side of the phrase of interest. In the syntactic dependency approach, we parse the entire corpus using an implementation of an Inductive Dependency parser as described in Nivre (2006). For each word w we collect a template of the syntactic context. We consider sequences of governing words (e.g. the parent, grand-parent, etc.) as well as collections of descendants (e.g., immediate children, grandchildren, etc.). This information is then encoded as a contextual template. For example, the context template cooks &lt;term&gt; delicious could be contexts for nouns such as food, meals, pasta, etc. This captures both syntactic preferences as well as selectional preferences. Contrary to Pado and Lapata (2007), we do not use the labels of the syntactic dependencies. Once the vectors have </context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>J. Nivre. 2006. Inductive Dependency Parsing, volume 34 of Text, Speech and Language Technology. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pado</author>
<author>M Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="11278" citStr="Pado and Lapata (2007)" startWordPosition="1777" endWordPosition="1781">e entire corpus using an implementation of an Inductive Dependency parser as described in Nivre (2006). For each word w we collect a template of the syntactic context. We consider sequences of governing words (e.g. the parent, grand-parent, etc.) as well as collections of descendants (e.g., immediate children, grandchildren, etc.). This information is then encoded as a contextual template. For example, the context template cooks &lt;term&gt; delicious could be contexts for nouns such as food, meals, pasta, etc. This captures both syntactic preferences as well as selectional preferences. Contrary to Pado and Lapata (2007), we do not use the labels of the syntactic dependencies. Once the vectors have been obtained, the frequency for each dimension in every vector is weighted using the other vectors as contrast set, with the k2 test, and finally the cosine similarity between vectors is used to calculate the similarity between each pair of terms. Except for the syntactic dependency approach, where closed-class words are needed by the parser, in the other cases we have removed stopwords (pronouns, prepositions, determiners and modal and auxiliary verbs). 3.1 Corpus used We have used a corpus of four billion docume</context>
</contexts>
<marker>Pado, Lapata, 2007</marker>
<rawString>S. Pado and M. Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>T Pedersen</author>
</authors>
<title>Using WordNetbased Context Vectors to Estimate the Semantic Relatedness of Concepts.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL Workshop on Making Sense of Sense: Bringing Computational Linguistics and Pycholinguistics Together,</booktitle>
<pages>1--8</pages>
<location>Trento, Italy.</location>
<marker>Patwardhan, Pedersen, 2006</marker>
<rawString>S. Patwardhan and T. Pedersen. 2006. Using WordNetbased Context Vectors to Estimate the Semantic Relatedness of Concepts. In Proceedings of the EACL Workshop on Making Sense of Sense: Bringing Computational Linguistics and Pycholinguistics Together, pages 1–8, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using Information Content to Evaluate Semantic Similarity in a Taxonomy.</title>
<date>1995</date>
<booktitle>Proc. of IJCAI,</booktitle>
<pages>14--448</pages>
<contexts>
<context position="16400" citStr="Resnik, 1995" startWordPosition="2594" endWordPosition="2595">e Adriatic Sea shore,3,coast,2 Thesaurus of Current English&apos; The Oxford Pocket Thesaurus slave,3,boy,5,shore,3,string,2 wizard,4,glass,4,crane,5,smile,5 implement,5,oracle,2,lad,2 food,3,car,2,madhouse,3,jewel,3 asylum,4,tool,8,journey,6,etc. be understood that the&apos; 10 may be designed crane,3,tool,3 a fight between a &apos; and a snake and bird,3,crane,5 Table 2: Sample of context windows for the terms in the RG dataset. latedness are annotated without any distinction. Several studies indicate that the human scores consistently have very high correlations with each other (Miller and Charles, 1991; Resnik, 1995), thus validating the use of these datasets for evaluating semantic similarity. For the cross-lingual evaluation, the two datasets were modified by translating the second word in each pair into Spanish. Two humans translated simultaneously both datasets, with an inter-tagger agreement of 72% for RG and 84% for WordSim353. 4.2 Results Table 1 shows the Spearman correlation obtained on the RG and WordSim353 datasets, including the interval at 0.95 of confidence6. Overall the distributional context-window approach performs best in the RG, reaching 0.89 correlation, and both WN30g and the combinat</context>
<context position="29020" citStr="Resnik, 1995" startWordPosition="4631" endWordPosition="4632">itions 57 and 59. The class negative indicates that in the gold-standard the first pair has a lower score than the second pair. We have trained a SVM to classify pairs of pairs, and use its output to rank the entries in both datasets. It uses a polynomial kernel with degree 4. We did Method Source Spearman (MC) Pearson (MC) (Sahami et al., 2006) Web snippets 0.62 [0.32, 0.81] 0.58 [0.26, 0.78] (Chen et al., 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85] (Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89] (Leacock et al., 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91] (Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90] (Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92] (Bollegala et al., 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92] (Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93] (Jarmasz, 2003) Roget’s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94] (Patwardhan et al., 2006) WordNet n/a 0.91 (Alvarez and Lim, 2007) WordNet n/a 0.91 (Yang and Powers, 2005) WordNet 0.87 [0.73, 0.91] 0.92 [0.84, 0.96] (Hughes et al., 2007) WordNet 0.90 n/a Personalized PageRank WordNet 0.89 [0.77, 0.94] n/a Bag of words Web corpus 0.85 [0.70, 0.93] 0.84 [0.6</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using Information Content to Evaluate Semantic Similarity in a Taxonomy. Proc. of IJCAI, 14:448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rubenstein</author>
<author>J B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="14938" citStr="Rubenstein and Goodenough (1965)" startWordPosition="2381" endWordPosition="2384">th 4) and syntactic contexts in the template vectors. Max scores are bolded. 1. Replace each non-English word in the dataset with its 5-best translations into English using state-of-the-art machine translation technology. 2. The vector corresponding to each Spanish word is calculated by collecting features from all the contexts of any of its translations. 3. Once the vectors are generated, the similarities are calculated in the same way as before. 4 Experimental results 4.1 Gold-standard datasets We have used two standard datasets. The first one, RG, consists of 65 pairs of words collected by Rubenstein and Goodenough (1965), who had them judged by 51 human subjects in a scale from 0.0 to 4.0 according to their similarity, but ignoring any other possible semantic relationships that might appear between the terms. The second dataset, WordSim3535 (Finkelstein et al., 2002) contains 353 word pairs, each associated with an average of 13 to 16 human judgements. In this case, both similarity and re5Available at http://www.cs.technion.ac.il/ ∼gabr/resources/data/wordsim353/wordsim353.html Context RG terms and frequencies ll never forget the&apos; on his face when grin,2,smile,10 he had a giant&apos; on his face and grin,3,smile,2</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>H. Rubenstein and J.B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ruiz-Casado</author>
<author>E Alfonseca</author>
<author>P Castells</author>
</authors>
<title>Using context-window overlapping in Synonym Discovery and Ontology Extension.</title>
<date>2005</date>
<booktitle>In Proceedings of RANLP-2005, Borovets,</booktitle>
<contexts>
<context position="8878" citStr="Ruiz-Casado et al. (2005)" startWordPosition="1366" endWordPosition="1369">coefficient and the Dice coefficient, which are combined with lexico-syntactic patterns as model features. The model parameters are trained using Support Vector Machines (SVM) in order to later rank pairs of words. A different approach is the one taken by Sahami and Heilman (2006), who collect snippets from the results of a search engine and represent each snippet as a vector, weighted with the tf·idf score. The semantic similarity between two queries is calculated as the inner product between the centroids of the respective sets of vectors. To calculate the similarity of two words w1 and w2, Ruiz-Casado et al. (2005) collect snippets containing w1 from a Web search engine, extract a context around it, replace it with w2 and check for the existence of that modified context in the Web. Using a search engine to calculate similarities between words has the drawback that the data used will always be truncated. So, for example, the numbers of hits returned by search engines nowadays are always approximate and rounded up. The systems that rely on collecting snippets are also limited by the maximum number of documents returned per query, typically around a thousand. We hypothesize that by crawling a large corpus </context>
</contexts>
<marker>Ruiz-Casado, Alfonseca, Castells, 2005</marker>
<rawString>M Ruiz-Casado, E. Alfonseca, and P. Castells. 2005. Using context-window overlapping in Synonym Discovery and Ontology Extension. In Proceedings of RANLP-2005, Borovets, Bulgaria,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahami</author>
<author>T D Heilman</author>
</authors>
<title>A web-based kernel function for measuring the similarity of short text snippets.</title>
<date>2006</date>
<booktitle>Proc. of WWW,</booktitle>
<pages>377--386</pages>
<contexts>
<context position="1655" citStr="Sahami and Heilman, 2006" startWordPosition="235" endWordPosition="238"> between terms is an important problem in lexical semantics. It has applications in many natural language processing tasks, such as Textual Entailment, Word Sense Disambiguation or Information Extraction, and other related areas like Information Retrieval. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problem</context>
<context position="8534" citStr="Sahami and Heilman (2006)" startWordPosition="1306" endWordPosition="1309">he number of hits returned by a Web search engine to calculate the Pointwise Mutual Information (PMI) between terms, as an indicator of synonymy. Bollegala et al. (2007) calculate a number of popular relatedness metrics based on page counts, 4http://www.lsi.upc.es/∼nlp/tools/download-map.php. 20 like PMI, the Jaccard coefficient, the Simpson coefficient and the Dice coefficient, which are combined with lexico-syntactic patterns as model features. The model parameters are trained using Support Vector Machines (SVM) in order to later rank pairs of words. A different approach is the one taken by Sahami and Heilman (2006), who collect snippets from the results of a search engine and represent each snippet as a vector, weighted with the tf·idf score. The semantic similarity between two queries is calculated as the inner product between the centroids of the respective sets of vectors. To calculate the similarity of two words w1 and w2, Ruiz-Casado et al. (2005) collect snippets containing w1 from a Web search engine, extract a context around it, replace it with w2 and check for the existence of that modified context in the Web. Using a search engine to calculate similarities between words has the drawback that t</context>
</contexts>
<marker>Sahami, Heilman, 2006</marker>
<rawString>M. Sahami and T.D. Heilman. 2006. A web-based kernel function for measuring the similarity of short text snippets. Proc. of WWW, pages 377–386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Strube</author>
<author>S P Ponzetto</author>
</authors>
<title>WikiRelate! Computing Semantic Relatedness Using Wikipedia.</title>
<date>2006</date>
<booktitle>In Proceedings of the AAAI-2006,</booktitle>
<pages>1419--1424</pages>
<contexts>
<context position="31302" citStr="Strube and Ponzetto, 2006" startWordPosition="4999" endWordPosition="5002">z and Lim (2007), use a non-linear function to map the system outputs into new values distributed more similarly to the values in the gold-standard. In their case, the mapping function was exp (−&apos;4 ), which was chosen empirically. Finding such a function is dependent on the dataset used, and involves an extra step in the similarity calculations. Alternatively, the Spearman correlation provides an evaluation metric that is independent of such data-dependent transformations. Most similarity researchers have published their 25 Table 8: Our best results for the MC dataset. Method Source Spearman (Strube and Ponzetto, 2006) Wikipedia 0.19–0.48 (Jarmasz, 2003) WordNet 0.33–0.35 (Jarmasz, 2003) Roget’s 0.55 (Hughes and Ramage, 2007) WordNet 0.55 (Finkelstein et al., 2002) Web corpus, WN 0.56 (Gabrilovich and Markovitch, 2007) ODP 0.65 (Gabrilovich and Markovitch, 2007) Wikipedia 0.75 SVM Web corpus, WN 0.78 Table 9: Comparison with previous work for WordSim353. complete results on a smaller subset of the RG dataset containing 30 word pairs (Miller and Charles, 1991), usually referred to as MC, making it possible to compare different systems using different correlation. Table 7 shows the results of related work on </context>
</contexts>
<marker>Strube, Ponzetto, 2006</marker>
<rawString>M. Strube and S.P. Ponzetto. 2006. WikiRelate! Computing Semantic Relatedness Using Wikipedia. In Proceedings of the AAAI-2006, pages 1419–1424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Mining the Web for Synonyms:</title>
<date>2001</date>
<booktitle>PMIIR versus LSA on TOEFL. Lecture Notes in Computer Science,</booktitle>
<pages>2167--491</pages>
<contexts>
<context position="7902" citStr="Turney (2001)" startWordPosition="1209" endWordPosition="1210">e et al., 2000)4 in order to create a 3.0 version of the Spanish WordNet. The mapping was used to link Spanish variants to 3.0 synsets. We used the English WordNet 3.0, including glosses, to construct the graph. The two Spanish WordNet versions are referred to as MCR16 and WN30g. 3 Context-based methods In this section, we describe the distributional methods used for calculating similarities between words, and profiting from the use of a large Web-based corpus. This work is motivated by previous studies that make use of search engines in order to collect cooccurrence statistics between words. Turney (2001) uses the number of hits returned by a Web search engine to calculate the Pointwise Mutual Information (PMI) between terms, as an indicator of synonymy. Bollegala et al. (2007) calculate a number of popular relatedness metrics based on page counts, 4http://www.lsi.upc.es/∼nlp/tools/download-map.php. 20 like PMI, the Jaccard coefficient, the Simpson coefficient and the Dice coefficient, which are combined with lexico-syntactic patterns as model features. The model parameters are trained using Support Vector Machines (SVM) in order to later rank pairs of words. A different approach is the one ta</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>P.D. Turney. 2001. Mining the Web for Synonyms: PMIIR versus LSA on TOEFL. Lecture Notes in Computer Science, 2167:491–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
<author>editor</author>
</authors>
<date>1998</date>
<booktitle>EuroWordNet: A Multilingual Database with Lexical Semantic Networks.</booktitle>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Vossen, editor, 1998</marker>
<rawString>P. Vossen, editor. 1998. EuroWordNet: A Multilingual Database with Lexical Semantic Networks. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Wu</author>
<author>M Palmer</author>
</authors>
<title>Verb semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>133--138</pages>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="28894" citStr="Wu and Palmer, 1994" startWordPosition="4609" endWordPosition="4612"> in positions 31 and 64. The second classifier gave them similarities of 0.084805 and 0.109061 respectively, which ranked them in positions 57 and 59. The class negative indicates that in the gold-standard the first pair has a lower score than the second pair. We have trained a SVM to classify pairs of pairs, and use its output to rank the entries in both datasets. It uses a polynomial kernel with degree 4. We did Method Source Spearman (MC) Pearson (MC) (Sahami et al., 2006) Web snippets 0.62 [0.32, 0.81] 0.58 [0.26, 0.78] (Chen et al., 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85] (Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89] (Leacock et al., 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91] (Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90] (Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92] (Bollegala et al., 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92] (Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93] (Jarmasz, 2003) Roget’s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94] (Patwardhan et al., 2006) WordNet n/a 0.91 (Alvarez and Lim, 2007) WordNet n/a 0.91 (Yang and Powers, 2005) WordNet 0.87 [0.73, 0.91] 0.92 [0.84, 0.96] (Hughes et al., </context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Z. Wu and M. Palmer. 1994. Verb semantics and lexical selection. In Proc. of ACL, pages 133–138, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yang</author>
<author>D M W Powers</author>
</authors>
<title>Measuring semantic similarity in the taxonomy of WordNet.</title>
<date>2005</date>
<booktitle>Proceedings of the Australasian conference on Computer Science.</booktitle>
<contexts>
<context position="1536" citStr="Yang and Powers, 2005" startWordPosition="215" endWordPosition="219">ily adapted for a cross-lingual task with minor losses. 1 Introduction Measuring semantic similarity and relatedness between terms is an important problem in lexical semantics. It has applications in many natural language processing tasks, such as Textual Entailment, Word Sense Disambiguation or Information Extraction, and other related areas like Information Retrieval. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar phrases like captive-wes</context>
<context position="29433" citStr="Yang and Powers, 2005" startWordPosition="4695" endWordPosition="4698">t al., 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85] (Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89] (Leacock et al., 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91] (Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90] (Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92] (Bollegala et al., 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92] (Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93] (Jarmasz, 2003) Roget’s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94] (Patwardhan et al., 2006) WordNet n/a 0.91 (Alvarez and Lim, 2007) WordNet n/a 0.91 (Yang and Powers, 2005) WordNet 0.87 [0.73, 0.91] 0.92 [0.84, 0.96] (Hughes et al., 2007) WordNet 0.90 n/a Personalized PageRank WordNet 0.89 [0.77, 0.94] n/a Bag of words Web corpus 0.85 [0.70, 0.93] 0.84 [0.69, 0.93] Context window Web corpus 0.88 [0.76, 0.95] 0.89 [0.77, 0.95] Syntactic contexts Web corpus 0.76 [0.54, 0.88] 0.74 [0.51, 0.87] SVM Web, WN 0.92 [0.84, 0.96] 0.93 [0.85, 0.97] Table 7: Comparison with previous approaches for MC. not have a held-out set, so we used the standard settings of Weka, without trying to modify parameters, e.g. C. Each word pair is scored with the number of pairs that were con</context>
</contexts>
<marker>Yang, Powers, 2005</marker>
<rawString>D. Yang and D.M.W. Powers. 2005. Measuring semantic similarity in the taxonomy of WordNet. Proceedings of the Australasian conference on Computer Science.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>