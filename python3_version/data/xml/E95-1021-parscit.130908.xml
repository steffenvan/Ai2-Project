<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.99639">
Tagging French â€”
comparing a statistical and a constraint-based method
</title>
<author confidence="0.98671">
Jean-Pierre Chanod and Pasi Tapanainen
</author>
<affiliation confidence="0.961692">
Rank Xerox Research Centre, Grenoble Laboratory
</affiliation>
<address confidence="0.839633">
6, chemin de Maupertuis, 38240 Meylan, France
</address>
<email confidence="0.846267">
Jean.Pierre.Chanod,Pasi.TapanainenOxerox.fr
</email>
<sectionHeader confidence="0.992961" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999345941176471">
In this paper we compare two compet-
ing approaches to part-of-speech tagging,
statistical and constraint-based disam-
biguation, using French as our test lan-
guage. We imposed a time limit on our
experiment: the amount of time spent
on the design of our constraint system
was about the same as the time we used
to train and test the easy-to-implement
statistical model. We describe the two
systems and compare the results. The
accuracy of the statistical method is rea-
sonably good, comparable to taggers for
English. But the constraint-based tagger
seems to be superior even with the lim-
ited time we allowed ourselves for rule
development.
</bodyText>
<sectionHeader confidence="0.991217" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.983023914285714">
In this paper&apos; we compare two competing ap-
proaches to part-of-speech tagging, statistical and
constraint-based disambiguation, using French as
our test language. The process of tagging consists
of three stages: tokenisation, morphological anal-
ysis and disambiguation. The two taggers include
the same tokeniser and morphological analyser.
The tokeniser uses a finite-state transducer that
reads the input and outputs a token whenever it
has read far enough to be sure that a token is
detected. The morphological analyser contains
a transducer lexicon. It produces all the legiti-
mate tags for words that appear in the lexicon.
If a word is not in the lexicon, a guesser is con-
sulted. The guesser employs another finite-state
transducer. It reads a token and prints out a set
of tags depending on prefixes, inflectional infor-
mation and productive endings that it finds.
We make even more use of transducers in the
constraint-based tagger. The tagger reads one
sentence at a time, a string of words and alterna-
tive tags, feeds them to the grammatical transduc-
&apos;There is a longer version (17 pages) of this paper
in (Chanod and Tapanainen, 1994)
ers that remove all but one alternative tag from all
the words on the basis of contextual information.
If all the transducers described above (to-
keniser, morphological analyser and disambigua-
tor) could be composed together, we would get
one single transducer that transforms a raw input
text to a fully disambiguated output.
The statistical method contains the same to-
keniser and morphological analyser. The disam-
biguation method is a conventional one: a hidden
Markov model.
</bodyText>
<sectionHeader confidence="0.932708" genericHeader="introduction">
2 Morphological analysis and
guessing
</sectionHeader>
<bodyText confidence="0.9999695">
The morphological analyser is based on a lexical
transducer (Karttunen et al., 1992). The trans-
ducer maps each inflected surface form of a word
to its canonical lexical form followed by the ap-
propriate morphological tags.
Words not found in the lexicon are analysed by
a separate finite-state transducer, the guesser. We
developed a simple, extremely compact and effi-
cient guesser for French. It is based on the gen-
eral assumption that neologisms and uncommon
words tend to follow regular inflectional patterns.
The guesser is thus based on productive endings
(like men t for adverbs, ible for adjectives, er for
verbs). A given ending may of course point to
various categories, e.g. er identifies nouns as well
as verbs due to possible borrowings from English.
</bodyText>
<sectionHeader confidence="0.994664" genericHeader="method">
3 The statistical model
</sectionHeader>
<bodyText confidence="0.990743">
We use the Xerox part-of-speech tagger (Cutting
et al., 1992), a statistical tagger made at the Xerox
Palo Alto Research Center.
</bodyText>
<subsectionHeader confidence="0.993596">
3.1 Training
</subsectionHeader>
<bodyText confidence="0.999987285714286">
The Xerox tagger is claimed (Cutting et al,, 1992)
to be adaptable and easily trained; only a lexicon
and suitable amount of untagged text is required.
A new language-specific tagger can therefore be
built with a minimal amount of work. We started
our project by doing so. We took our lexicon
with the new tagset, a corpus of French text, and
</bodyText>
<page confidence="0.998265">
149
</page>
<bodyText confidence="0.99695080952381">
trained the tagger. We ran the tagger on another
text and counted the errors. The result was not
good; 13 % of the words were tagged incorrectly.
The tagger does not require a tagged corpus for
training, but two types of biases can be set to tell
the tagger what is correct and what is not: symbol
biases and transition biases. The symbol biases
describe what is likely in a given ambiguity class.
They represent kinds of lexical probabilities. The
transition biases describe the likelihood of various
tag pairs occurring in succession. The biases serve
as initial values before training.
We spent approximately one man-month writ-
ing biases and tuning the tagger. Our training cor-
pus was rather small, because the training had to
be repeated frequently. When it seemed that the
results could not be further improved, we tested
the tagger on a new corpus. The eventual result
was that 96.8 % of the words in the corpus were
tagged correctly. This result is about the same as
for statistical taggers of English.
</bodyText>
<subsectionHeader confidence="0.99992">
3.2 Modifying the biases
</subsectionHeader>
<bodyText confidence="0.9998764">
A 4 % error rate is not generally considered a neg-
ative result for a statistical tagger, but some of
the errors are serious. For example, a sequence of
determiner...noun...noun/verb...preposition is fre-
quently disambiguated in the wrong way, e.g. Le
train part a cinq heures (The train leaves at 5
o&apos;clock). The word part is ambiguous between a
noun and a verb (singular, third person), and it
is disambiguated incorrectly. The tagger seems to
prefer the noun reading between a singular noun
and a preposition.
One way to resolve this is to write new biases.
We added two new ones. The first one says that
a singular noun is not likely to be followed by a
noun (this is not always true but we could call
this a tendency). The second states that a sin-
gular noun is likely to be followed by a singular,
third-person verb. The result was that the prob-
lematic sentence was disambiguated correctly, but
the changes had a bad side effect. The overall er-
ror rate of the tagger increased by over 50 %. This
illustrates how difficult it is to write good biases.
Getting a correct result for a particular sentence
does not necessarily increase the overall success
rate.
</bodyText>
<sectionHeader confidence="0.980909" genericHeader="method">
4 The constraint-based model
</sectionHeader>
<subsectionHeader confidence="0.978134">
4.1 A two-level model for tagging
</subsectionHeader>
<bodyText confidence="0.933570133333333">
In the constraint-based tagger, the rules are rep-
resented as finite-state transducers. The trans-
ducers are composed with the sentence in a se-
quence. Each transducer may remove, or in prin-
ciple it may also change, one or more readings of
the words. After all the transducers have been
applied, each word in the sentence has only one
analysis.
Our constraint-based tagger is based on tech-
niques that were originally developed for mor-
phological analysis. The disambiguation rules are
similar to phonological rewrite rules (Kaplan and
Kay, 1994), and the parsing algorithm is similar
to the algorithm for combining the morphological
rules with the lexicon (Karttunen, 1994).
The tagger has a close relative in (Koskenniemi,
1990; Koskenniemi et al., 1992; Voutilainen and
Tapanainen, 1993) where the rules are represented
as finite-state machines that are conceptually in-
tersected with each other. In this tagger the dis-
ambiguation rules are applied in the same man-
ner as the morphological rules in (Koskenniemi,
1983). Another relative is represented in (Roche
and Schabes, 1994) which uses a single finite-
state transducer to transform one tag into an-
other. A constraint-based system is also presented
in (Karlsson, 1990; Karlsson et al., 1995). Related
work using finite-state machines has been done
using local grammars (Roche, 1992; Silberztein,
1993; Laporte, 1994).
</bodyText>
<subsectionHeader confidence="0.940246">
4.2 Writing the rules
4.2.1 Studying ambiguities
</subsectionHeader>
<bodyText confidence="0.999827470588235">
One quick experiment that motivated the build-
ing of the constraint-based model was the follow-
ing: we took a million words of newspaper text
and ranked ambiguous words by frequency. We
found that a very limited set of word forms covers
a large part of the total ambiguity. The 16 most
frequent ambiguous word forms2 account for 50 %
of all ambiguity. Two thirds of the ambiguity are
due to the 97 most frequent ambiguous words3.
Another interesting observation is that the
most frequent ambiguous words are usually
words which are in general corpus-independent,
i.e. words that belong to closed classes (determin-
ers, prepositions, pronouns, conjunctions), auxil-
iaries, common adverbials or common verbs, like
faire (to do, to make). The first corpus-specific
word is in the 41st position.
</bodyText>
<subsubsectionHeader confidence="0.885335">
4.2.2 Principled rules
</subsubsectionHeader>
<bodyText confidence="0.999911777777778">
For the most frequent ambiguous word forms,
one may safely define principled contextual re-
strictions to resolve ambiguities. This is in par-
ticular the case for clitic/determiner ambiguities
attached to words like le or la. Our rule says that
clitic pronouns are attached to a verb and deter-
miners to a noun with possibly an unrestricted
number of premodifiers. This is a good starting
point although some ambiguity remains as in la
</bodyText>
<footnote confidence="0.990478666666667">
2Namely de, la, le, les, des, en, du, un, a, dans,
une, pas, est, plus, Le, son
3A similar experiment shows that in the Brown cor-
pus 63 word forms cover 50 % of all the ambiguity, and
two thirds of the ambiguity is covered by 220 word
forms.
</footnote>
<page confidence="0.998072">
150
</page>
<bodyText confidence="0.999985">
place, which can be read as a determiner-noun or
clitic-verb sequence.
Some of the very frequent words have categories
that are rare, for instance the auxiliary forms a
and est can also be nouns and the pronoun cela
is also a very rare verb form. In such a case, we
restrict the use of the rarest categories to con-
texts where the most frequent reading is not at
all possible, otherwise the most frequent reading
is preferred. For instance, the word avions may be
a noun or an auxiliary verb. We prefer the noun
reading and accept the verb reading only when
the first-person pronoun nous appears in the left
context, e.g. as in nous ne les avions pas (we did
not have them).
This means that the tagger errs only when a
rare reading should be chosen in a context where
the most common reading is still acceptable. This
may never actually occur, depending on how accu-
rate the contextual restrictions are. It can even be
the case that discarding the rare readings would
not induce a detectable loss in accuracy, e.g. in
the conflict between cela as a pronoun and as a
verb. The latter is a rarely used tense of a rather
literary verb.
The principled rules do not require any tagged
corpus, and should be thus corpus-independent.
The rules are based on a short list of extremely
common words (fewer than 100 words).
</bodyText>
<subsubsectionHeader confidence="0.512101">
4.2.3 Heuristics
</subsubsectionHeader>
<bodyText confidence="0.999870147058823">
The rules described above are certainly not suf-
ficient to provide full disambiguation, even if one
considers only the most ambiguous word forms.
We need more rules for cases that the principled
rules do not disambiguate.
Some ambiguity is extremely difficult to resolve
using the information available. A very problem-
atic case is the word des, which can either be a de-
terminer, Jean mange des pommes (Jean eats ap-
ples) or an amalgamated preposition-determiner,
as in Jean aime le bruit des vagues (Jean likes the
sound of waves).
Proper treatment of such an ambiguity would
require verb subcategorisation and a description of
complex coordinations of noun and prepositional
phrases. This goes beyond the scope of both the
statistical and the constraint-based taggers. For
such cases we introduce ad-hoc heuristics. Some
are quite reasonable, e.g. the determiner reading
of des is preferred at the begining of a sentence.
Some are more or less arguable, e.g. the preposi-
tional reading is preferred after a noun.
One may identify various contexts in which ei-
ther the noun or the adjective can be preferred.
Such contextual restrictions (Chanod, 1993) are
not always true, but may be considered reason-
able for resolving the ambiguity. For instance, in
the case of two successive noun/adjective ambigu-
ities like le franc fort (the strong franc or the frank
fort), we favour the nounâ€”adjective sequence ex-
cept when the first word is a common prenominal
adjective such as bon, petit, grand, premier, as
in le petit fort (the small fort) or even le bon petit
(the good little one).
</bodyText>
<subsubsectionHeader confidence="0.966946">
4.2.4 Non-contextual rules
</subsubsectionHeader>
<bodyText confidence="0.9999923">
Our heuristics do not resolve all the ambigu-
ity. To obtain the fully unambiguous result we
make use of non-contextual heuristics. The non-
contextual rules may be thought of as lexical prob-
abilities. We guess what the most probable tag
is in the remaining ambiguities. For instance,
preposition is preferred to adjective, pronoun is
preferred to past participle, etc. The rules are ob-
viously not very reliable, but they are needed only
when the previous rules fail to fully disambiguate.
</bodyText>
<subsubsectionHeader confidence="0.870345">
4.2.5 Current rules
</subsubsectionHeader>
<bodyText confidence="0.992177">
The current system contains 75 rules, consisting
of:
</bodyText>
<listItem confidence="0.996423166666667">
â€¢ 39 reliable contextual rules dealing mostly
with frequent ambiguous words.
â€¢ 25 rules describing heuristics with various de-
grees of linguistic generality.
â€¢ 11 non-contextual rules for the remaining am-
biguities.
</listItem>
<bodyText confidence="0.99992875">
The rules were constructed in less than one
month, on the basis of 50 newspaper sentences.
All the rules are currently represented by 11 trans-
ducers.
</bodyText>
<sectionHeader confidence="0.982418" genericHeader="method">
5 The results
</sectionHeader>
<subsectionHeader confidence="0.998918">
5.1 Test A
</subsectionHeader>
<bodyText confidence="0.99998908">
For evaluation, we used a corpus totally unrelated
to the development corpus. It contains 255 sen-
tences (5752 words) randomly selected from a cor-
pus of economic reports. About 54 % of the words
are ambiguous. The text is first tagged manually
without using the disambiguators, and the output
of the tagger is then compared to the hand-tagged
result.
If we apply all the rules, we get a fully disam-
biguated result with an error rate of only 1.3 %.
This error rate is much lower than the one we get
using the hidden Markov model (3.2 %). See Fig-
ure 1.
We can also restrict the tagger to using only the
most reliable rules. Only 10 words lose the cor-
rect tag when almost 2000 out of 3085 ambiguous
words are disambiguated. Among the remaining
1136 ambiguous words about 25 % of the ambigu-
ity is due to determiner/preposition ambiguities
(words like du and des), 30 % are adjective/noun
ambiguities and 18 % are noun/verb ambiguities.
If we use both the principled and heuristic rules,
the error rate is 0.52 % while 423 words remain
ambiguous. The non-contextual rules that elim-
inate the remaining 423 ambiguities produce an
</bodyText>
<page confidence="0.982375">
151
</page>
<table confidence="0.999242142857143">
error rate remaining tag / word
(correctness) ambiguity
Lexicon + Guesser 0.03 % (99.97 %) 54% 1.64
Hidden Markov model 3.2 % (96.8 %) % 1.00
Principled rules L0.17 % (99.83 %) 20 % 1.24
Principled and heuristic rules 0.52 % (99.48 %) 7 % 1.09
All the rules 1.3 % (98.7 %) 0 % 1.00
</table>
<figureCaption confidence="0.999073">
Figure 1: The result in the test sample
</figureCaption>
<bodyText confidence="0.891806">
additional 43 errors. Overall, 98.7 % of the words
receive the correct tag.
</bodyText>
<subsectionHeader confidence="0.999481">
5.2 Test B
</subsectionHeader>
<bodyText confidence="0.999996111111111">
We also tested the taggers with more difficult text.
The 12 000 word sample of newspaper text has
typos and proper names4 that match an existing
word in the lexicon. Problems of the latter type
are relatively rare but this sample was exceptional.
Altogether the lexicon mismatches produced 0.5 %
errors to the input of the taggers. The results are
shown in Figure 2. This text also seems to be
generally more difficult to parse than the first one.
</bodyText>
<subsectionHeader confidence="0.990632">
5.3 Combination of the taggers
</subsectionHeader>
<bodyText confidence="0.9987306">
We also tried combining the taggers, using first
the rules and then the statistics (a similar ap-
proach was also used in (Tapanainen and Vouti-
lainen, 1994)). We evaluated the results obtained
by the following sequence of operations:
</bodyText>
<listItem confidence="0.701116666666667">
1) Running the constraint-based tagger without
the final, non-contextual rules.
2) Using the statistical disambiguator indepen-
dently. We select the tag proposed by the
statistical disambiguator if it is not removed
during step 1.
3) Solving the remaining ambiguities by run-
ning the final non-contextual rules of the
constraint-based tagger. This last step en-
sures that one gets a fully disambiguated
text. Actually only about 0.5 % of words were
not fully disambiguated after step 2.
</listItem>
<bodyText confidence="0.999935642857143">
We used the test sample B. After the first step,
1400 words out of 12 000 remain ambiguous. The
process of combining the three steps described
above eventually leads to more errors than run-
ning the constraint-based tagger alone. The sta-
tistical tagger introduces 220 errors on the 1400
words that remain ambiguous after step 1. In
comparison, the final set of non-contextual rules
introduces around 150 errors on the same set of
1400 words. We did not expect this result. One
possible explanation for the superior performance
of the final non-contextual rules is that they are
meant to apply after the previous rules failed to
disambiguate the word. This is in itself useful
</bodyText>
<footnote confidence="0.50358">
4like Bats, Botta, Demis, Ferrasse, Hersant,
</footnote>
<bodyText confidence="0.999878333333333">
information. The final heuristics favour tags that
have survived all conditions that restrict their use.
For instance, the contextual rules define various
contexts where the preposition tag for des is pre-
ferred. Therefore, the final heuristics favours the
determiner reading for des.
</bodyText>
<sectionHeader confidence="0.944175" genericHeader="method">
6 Analysis of errors
</sectionHeader>
<subsectionHeader confidence="0.944409">
6.1 Errors of principled and heuristic
rules
</subsectionHeader>
<bodyText confidence="0.972364675675676">
Let us now consider what kind of errors the con-
straint-based tagger produced. We do not deal
with errors produced by the last set of rules, the
non-contextual rules, because it is already known
that they are not very accurate. To make the
tagger better, they should be replaced by writing
more accurate heuristic rules.
We divide the errors into three categories: (1)
errors due to multi-word expressions, (2) errors
that should/could be resolved and (3) errors that
are hard to resolve by using the information that
is available.
The first group (15 errors), the multi-word ex-
pressions, are difficult for the syntax-based rules
because in many cases the expression does not fol-
low any conventional syntactic structure, or the
structure may be very rare. In multi-word expres-
sions some words also have categories that may
not appear anywhere else. The best way to han-
dle them is to lexicalise these expressions. When
a possible expression is recognised we can either
collapse it into one unit or leave it otherwise in-
tact except that the most &amp;quot;likely&amp;quot; interpretation
is marked.
The biggest group (41 errors) contains errors
that could have been resolved correctly but were
not. The reason for this is obvious: only a rela-
tively small amount of time was allowed for writ-
ing the rules. In addition, the rules were con-
structed on the basis of a rather small set of ex-
ample sentences. Therefore, it would be very sur-
prising if such errors did not appear in the test
sample taken from a different source. The errors
are the following:
â€¢ The biggest subgroup has 19 errors that re-
quire modifications to existing rules. Our
rules were meant to handle such cases but fail
</bodyText>
<page confidence="0.993854">
152
</page>
<table confidence="0.992163571428571">
error rate remaining tag / word
(correctness) ambiguity
Lexicon + Guesser 0.5 % (99.5 %) 48% 1.59
Hidden Markov model I 5.0 % (95.0 %) 0% 1.00
Principled rules 0.8 % (99.2 %) 23 % 1.29
Principled and heuristic rules 1.3 % (98.7 %) 12 % 1.14
All the rules 2.5 % (97.5 %) 0 % 1.00
</table>
<figureCaption confidence="0.995388">
Figure 2: The result in a difficult test sample with many lexicon mismatches
</figureCaption>
<bodyText confidence="0.5046605">
to do so correctly in some sentences. Often
only a minor correction is needed.
</bodyText>
<listItem confidence="0.956016235294118">
â€¢ Some syntactic constructions, or word se-
quences, were omitted. This caused 7 er-
rors which could easily be avoided by writ-
ing more rules. For instance, a construction
like &amp;quot;preposition + clitic + finite verb&amp;quot; was
not forbidden. The phrase a l&apos;est was anal-
ysed in this way while the correct analysis is
&amp;quot;preposition determiner + noun&amp;quot;.
â€¢ Sometimes a little bit of extra lexical infor-
mation is required. Six errors would require
more information or the kind of refinement in
the tag inventory that would not have been
appropriate for the statistical tagger.
â€¢ Nine errors could be avoided by refining ex-
isting heuristics, especially by taking into ac-
count exceptions for specific words like point,
pendant and devant.
</listItem>
<bodyText confidence="0.971282818181818">
The remaining errors (28 errors) constitute the
price we pay for using the heuristics. Removing
the rules which fail would cause a lot of ambiguity
to remain. The errors are the following:
â€¢ Fifteen errors are due to the heuristics for de
and des. There is little room for improvement
at this level of description (see Chapter 4.2.3).
However, the current, simple heuristics fully
disambiguate 850 instances of de and des out
of 914 i.e. 92 % of all the occurrences were
parsed with less than a 2 % error rate.
</bodyText>
<listItem confidence="0.9841563">
â€¢ Six errors involve nounâ€”adjective ambiguities
that are difficult to solve, for instance, in a
subject or object predicate position.
â€¢ Seven errors seem to be beyond reach for
various reasons: long coordination, rare con-
structions, etc. An example is les boites (the
boxes) where les is wrongly tagged in the test
sample because the noun form is misspelled
as boites, which is identified only as a verb by
the lexicon.
</listItem>
<subsectionHeader confidence="0.976154">
6.2 Difference between the taggers
</subsectionHeader>
<bodyText confidence="0.99964225">
We also investigated how the errors compare be-
tween the two taggers. Here we used the fully
disambiguated outputs of the taggers. The errors
belong mainly to three classes:
</bodyText>
<listItem confidence="0.997769076923077">
â€¢ Some errors appear predominantly with the
statistical tagger and almost never with the
constraint-based tagger. This is particularly
the case with the ambiguity between past par-
ticiples and adjectives.
â€¢ Some errors are common to both taggers, the
constraint-based tagger generally being more
accurate (often with a ratio of 1 to 2). These
errors cover ambiguities that are known to be
difficult to handle in general, such as the al-
ready mentioned determiner/preposition am-
biguity.
â€¢ Finally, there are errors that are specific to
</listItem>
<bodyText confidence="0.96736809375">
the constraint-based tagger. They are of-
ten related to errors that could be corrected
with some extra work. They are relatively
infrequent, thus the global accuracy of the
constraint-based tagger remains higher.
The first two classes of errors are generally dif-
ficult to correct. The easiest way to improve the
constraint-based tagger is to concentrate on the
final class. As we mentioned earlier, it is not
very easy to change the behaviour of the statistical
tagger in one place without some side-effects else-
where. This means that the errors of the first class
are probably easiest to resolve by means other
than statistics.
The first class is quite annoying for the statisti-
cal parser because it contains errors that are intu-
itively very clear and resolvable, but which are far
beyond the limits of the current statistical tagger.
We can take an easy sentence to demonstrate this:
Je ne le pense pas. I do not think so.
Tu ne le penses pas. You do not think so.
II ne le pense pas. He does not think so.
The verb pense is ambiguous5 in the first person or
in the third person. It is usually easy to determine
the person just by checking the personal pronoun
nearby. For a human or a constraint-based tagger
this is an easy task, for a statistical tagger it is not.
There are two words between the pronoun and the
verb that do not carry any information about the
person. The personal pronoun may thus be too
far from the verb because bi-gram models can see
backward no farther than le, and tri-gram models
</bodyText>
<footnote confidence="0.979194">
5That is not case with all the French verbs, e.g. Je
crois and II croit.
</footnote>
<page confidence="0.998481">
153
</page>
<bodyText confidence="0.9929112">
no farther than ne le.
Also, as mentioned earlier, resolving the adjec-
tive vs. past participle ambiguity is much harder,
if the tagger does not know whether there is an
auxiliary verb in the sentence or not.
</bodyText>
<sectionHeader confidence="0.957355" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999989508474576">
We have presented two taggers for french: a sta-
tistical one and a constraint-based one.
There are two ways to train the statistical
tagger: from a tagged corpus or using a self-
organising method that does not need a tagged
corpus. We had a strict time limit of one month
for doing the tagger and no tagged corpus was
available. This is a short time for the manual tag-
ging of a corpus and for the training of the tag-
ger. It would be risky to spend, say, three weeks
for writing a corpus, and only one week for train-
ing. The size of corpus would have to be limited,
because it should be also checked.
We selected the Xerox tagger that learns from
an untagged corpus&apos;. The task was not as straigth-
forward as we thought. Without human assistance
in the training the result was not impressive, and
we had to spend much time tuning the tagger
and guiding the learning process. In a month we
achieved 95-97 % accuracy.
The training process of a statistical tagger re-
quires some time because the linguistic informa-
tion has to be incorporated into the tagger one
way or another, it cannot be obtained for free
starting from null. Because the linguistic infor-
mation is needed, we decided to encode the infor-
mation in a more straightforward way, as explicit
linguistic disambiguation rules. It has been ar-
gued that statistical taggers are superior to rule-
based/hand-coded ones because of better accu-
racy and better adaptability (easy to train). In
our experiment, both claims turned out to be
wrong.
For the constraint-based tagger we set one
month time limit for writing the constraints by
hand. We used only linguistic intuition and a very
limited set of sentences to write the 75 constraints.
We formulated constraints of different accuracy.
Some of the constraints are almost 100 % accu-
rate, some of them just describe tendencies.
Finally, when we thought that the rules were
good enough, we took two text samples from dif-
ferent sources and tested both the taggers. The
constraint-based tagger made several naive errors
because we had forgotten, miscoded or ignored
some linguistic phenomena, but still, it made only
half of the errors that the statistical one made.
A big difference between the taggers is that the
tuning of the statistical tagger is very subtle i.e. it
is hard to predict the effect of tuning the param-
eters of the system, whereas the constraint-based
tagger is very straightforward to correct.
Our general conclusion is that the hand-coded
constraints perform better than the statistical tag-
ger and that we can still refine them. The most
important of our findings is that writing con-
straints that contain more linguistic information
than the current statistical model does not take
much time.
</bodyText>
<sectionHeader confidence="0.855805" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.92373025">
Jean-Pierre Chanod. Problemes de robustesse en
analyse syntaxique. In Actes de la conference
Informatique et langue naturelle. IRIN, Univer-
site de Nantes, 1993.
Jean-Pierre Chanod and Pasi Tapanainen. Statis-
tical and Constraint-based Taggers for French.
Technical report MLTT-016, Rank Xerox Re-
search Centre, Grenoble, 1994.
</bodyText>
<reference confidence="0.982113461538461">
Doug Cutting, Julian Kupiec, Jan Pedersen and
Penelope Sibun. A Practical Part-of-Speech
Tagger. In Third Conference on Applied Natu-
ral Language Processing. pages 133-140. Trento,
1992.
Ron Kaplan and Martin Kay. Regular Models
of Phonological Rule Systems. Computational
Linguistics Vol. 20, Number 3, pages 331-378.
Fred Karlsson. Constraint Grammar as a Frame-
work for Parsing Running Text. In proceedings
of Coding-90. Papers presented to the 13th In-
ternational Conference on Computational Lin-
guistics. Vol. 3, pages 168-173. Helsinki, 1990.
Fred Karlsson, Atro Voutilainen, Juha Heikkila
and Arto Anttila (eds.). Constraint Grammar:
a Language-Independent System for Parsing
Unrestricted Text. Mouton de Gruyter, Berlin,
1995.
Lauri Karttunen. Constructing Lexical Trans-
ducers. In proceedings of Co1ing-94. The fif-
teenth International Conference on Computa-
tional Linguistics. Vol I, pages 406-411. Kyoto,
1994.
Lauri Karttunen, Ron Kaplan and Annie Zae-
nen. Two-level morphology with composition.
In proceedings of Coling-92. The fourteenth In-
ternational Conference on Computational Lin-
guistics. Vol I, pages 141-148. Nantes, 1992.
Kimmo Koskenniemi. Two-level morphology.
A general computational model for word-form
recognition and production. University of
Helsinki, 1983.
Kimmo Koskenniemi. Finite-state parsing and
disambiguation. In proceedings of Coling-90.
Papers presented to the 13th International Con-
ference on Computational Linguistics. Vol. 2,
pages 229-232. Helsinki, 1990.
Kimmo Koskenniemi, Pa,si Tapanainen and Atro
Voutilainen. Compiling and using finite-state
</reference>
<page confidence="0.999091">
154
</page>
<reference confidence="0.93826416">
syntactic rules. In proceedings of Coling-92.
The fourteenth International Conference on
Computational Linguistics. Vol. I, pages 156-
162. Nantes, 1992.
Eric Laporte. Experiences in Lexical Disambigua-
tion Using Local Grammars. In Third Interna-
tional Conference on Computational Lexicogra-
phy. pages 163-172. Budapest, 1994.
Emmanuel Roche. Text Disambiguation by finite-
state automata, an algorithm and experiments
on corpora. In proceedings of Coling-92. The
fourteenth International Conference on Com-
putational Linguistics. Vol III, pages 993-997.
Nantes, 1992.
Emmanuel Roche and Yves Schabes. Deter-
ministic part-of-speech tagging with finite-state
transducers. Technical report TR-94-07, Mit-
subishi Electric Research Laboratories, Cam-
bridge, USA.
Max Silberztein. Dictionnaires electroniques et
analyse automatique de teites. Le systeme IN-
TEX. Masson, Paris, 1993.
Pasi Tapanainen and Atro Voutilainen. Tagging
accurately â€” Don&apos;t guess if you know. In Fourth.
Conference on Applied Natural Language Pro-
</reference>
<bodyText confidence="0.891367789473684">
cessing. pages 47-52. Stuttgart, 1994.
Atro Voutilainen and Path Tapanainen. Ambi-
guity resolution in a reductionistic parser. In
Sixth Conference of the European Chapter of the
ACL. pages 394-403. Utrecht, 1993.
A The restricted tag set
In this appendix the tag set is represented. Be-
sides the following tags, there may also be some
word-specific tags like PREP-DE, which is the
preposition reading for words de, des and du,
i.e. word de is initially ambiguous between PREP-
DE and PC. This information is mainly for
the statistical tagger to deal with, for instance,
different prepositions in a different way. The
constraint-based tagger does not need this because
it has direct access to word forms anyway. Af-
ter disambiguation, the word-specific tags may be
cleaned. The tag PREP-DE is changed back into
PREP, to reduce the redundant information.
</bodyText>
<listItem confidence="0.979168313432836">
â€¢ DET-SG: Singular determiner e.g. le, la,
mon, ma. This covers masculine as well as
feminine forms. Sample sentence: Le chien
dort dans la cuisine. (The dog is sleeping in
the kitchen).
â€¢ DET-PL Plural determiner e.g. les, mes.
This covers masculine as well as feminine
forms. Sample sentence: Les enfants jouent
avec mes livres. (The children are playing
with my books.)
â€¢ ADJ-INV Adjective invariant in number
e.g. heureux. Sample sentence: be chien est
heureux quand les enfants sont heureux. (The
dog is happy when the children are happy.)
â€¢ ADJ-SG Singular adjective e.g. gentil, gen-
title. This covers masculine as well as fem-
inine forms. Sample sentence: Le chien est
yenta (The dog is nice.)
â€¢ ADJ-PL Plural adjective e.g. gentils, gen-
tilles. This covers masculine as well as femi-
nine forms. Sample sentence: Ces chiens sont
gentils. (These dogs are nice.)
â€¢ NOUN-INV Noun invariant in number e.g.
souris, Francais. This covers masculine as
well as feminine forms. Sample sentence: Les
souris danseni. (The mice are dancing.)
â€¢ NOUN-SG Singular noun e.g. chien, fleur.
This covers masculine as well as feminine
forms. Sample sentence: C &apos;est une jolie fieur.
(It is a nice flower.)
â€¢ NOUN-PL Plural noun e.g. chiens, fieurs.
This covers masculine as well as feminine
forms. Sample sentence: Nous aimons les
fleurs. (We like flowers.)
â€¢ VAUX-INF Auxiliary verb, infinitive etre,
avoir. Sample sentence: Le chien vient d&apos;Ãªtre
puni. (The dog has just been punished.)
â€¢ VAUX-PRP Auxiliary verb, present par-
ticiple etant, ayant.
â€¢ VAUX-PAP Auxiliary verb, past participle
e.g. ete, eu. Sample sentence: Le theoreme
a ite demontre. (The theorem has been
proved.)
â€¢ VAUX-P1P2 Auxiliary verb, covers any 1st
or 2nd person form, regardless of number,
tense or mood, e.g. 1st person singular
present indicative, 2nd person plural impera-
tive: ai, soyons, es. Sample sentence: Tu es
fort. (You are strong.)
â€¢ VAUX-P3SG Auxiliary verb, covers any
3rd person singular form e.g. avait, sera,
es. Sample sentence: Elle est forte. (She is
strong.)
â€¢ VAUX-P3PL Auxiliary verb, covers any
3rd person plural form e.g. ont, seront,
avaient. Sample sentence: Riles avaient
dormi. (They had slept.)
â€¢ VERB-INF Infinitive verb e.g. danser,
finir, dormir. Sample sentence: Le chien
aime dormir. (The dog enjoys sleeping.)
â€¢ VERB-PRP Present participle
e.g. dansant, finissant, aboyant. Sample sen-
tence: Le chien arrive en aboyant. (The dog
is coming and it is barking.)
â€¢ VERB-P1P2 Any 1st or 2nd person verb
form, regardless of number, tense or mood
e.g. 1st person singular present indicative,
</listItem>
<page confidence="0.995818">
155
</page>
<bodyText confidence="0.578138">
2nd pers plural imperative: chante, finissons.
Sample sentence: Je chante. (I sing.)
</bodyText>
<listItem confidence="0.986458219512195">
â€¢ VERB-P3SG Any 3rd person singular verb
form e.g. chantera, finit, aboie. Sample sen-
tence: Le chien aboie. (The dog is barking.)
â€¢ VERB-P3PL Any 3rd person plural verb
form e.g. chanteront, finissent, aboient. Sam-
ple sentence: Les chiens aboient. (The dogs
are barking.)
â€¢ PAP-INV Past participle invariant in num-
ber e.g. surpris. Sample sentence: Le chien
m&apos;a surpris. (The dog surprised me.)
â€¢ PAP-SG Singular past participle e.g. fini,
finie. This covers masculine as well as femi-
nine forms. Sample sentence: La journee esi
finie. (The day is over.)
â€¢ PAP-PL Plural past participle e.g. finis,
finies. This covers masculine as well as fem-
inine forms. Sample sentence: Les travaux
sont finis. (The work is finished.)
â€¢ PC Non-nominative clitic pronoun such as
me, le. Sample sentence: Il me l&apos;a donne.
(He gave it to me.)
â€¢ PRON 3rd person pronoun, relative pro-
nouns excluded. e.g. il, elles, chacun. Sample
sentence: Il a parte a chacun. (He spoke to
every person.)
â€¢ PRON-P1P2 1st or 2nd person pronoun
e.g. je, in, nous. Sample sentence: Est-ce
que tu viendras avec moi? (Will you come
with me?)
â€¢ VOICILA Reserved for words voici and
voila. Sample sentence: Voici mon chien.
(Here is my dog.)
â€¢ ADV Adverbs e.g. finalement. Sample sen-
tence: Le jour est finalement arrive. (The
day has finally come.)
â€¢ NEG Negation particle. Reserved for the
word ne. Sample sentence: Le chien ne dont
pas. (The dog is not sleeping.)
â€¢ PREP Preposition e.g. dans. Sample sen-
tence: Le chien dont dans la cuisine. (The
dog sleeps in the kitchen.)
</listItem>
<bodyText confidence="0.998691">
For statistical taggers this group may be di-
vided into subgroups for different preposition
groups, like PREP-DE, PREP-A, etc.
</bodyText>
<listItem confidence="0.800709">
â€¢ CONN Connector. This class includes coor-
</listItem>
<bodyText confidence="0.851553636363636">
dinating conjuctions such as et, subordinate
conjunctions such as ions que, relative or in-
terrogative pronouns such as lequel. Words
like comme or que which have very special
behaviour are not coded as CONN. Sample
sentence: Le chien et le chat dorment quand
ii pleut. (The dog and the cat sleep when it
rains.)
For statistical taggers this group may be di-
vided into subgroups for different connectors,
like CONN-ET, CONN-Q, etc.
</bodyText>
<listItem confidence="0.9982766">
â€¢ COMME Reserved for all instances of the
word comme. Sample sentence: Il joue
comme un enfant. (He plays like a child.)
â€¢ CONJQUE Reserved for all instances of the
word que.
â€¢ NUM Numeral e.g. 12,7, 120/98, 34+0.7.
â€¢ HEURE String representing time e.g.
12h24, 12:45:00.
â€¢ MISC Miscellaneous words, such as: inter-
jection oh, salutation bonjour, onomatopoeia
miaou, wordparts i.e. words that only exist
as part of a multi-word expression, such as
priori, as part of a priori.
â€¢ CM Comma.
â€¢ PUNCT Punctuation other than comma.
</listItem>
<page confidence="0.997928">
156
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000169">
<title confidence="0.982141">Tagging French â€” comparing a statistical and a constraint-based method</title>
<author confidence="0.957966">Chanod Tapanainen</author>
<affiliation confidence="0.987813">Rank Xerox Research Centre, Grenoble Laboratory</affiliation>
<address confidence="0.98743">6, chemin de Maupertuis, 38240 Meylan, France</address>
<email confidence="0.957208">Jean.Pierre.Chanod,Pasi.TapanainenOxerox.fr</email>
<abstract confidence="0.994683954081634">In this paper we compare two competing approaches to part-of-speech tagging, statistical and constraint-based disambiguation, using French as our test language. We imposed a time limit on our experiment: the amount of time spent on the design of our constraint system was about the same as the time we used to train and test the easy-to-implement statistical model. We describe the two systems and compare the results. The accuracy of the statistical method is reasonably good, comparable to taggers for English. But the constraint-based tagger seems to be superior even with the limited time we allowed ourselves for rule development. 1 Overview In this paper&apos; we compare two competing approaches to part-of-speech tagging, statistical and constraint-based disambiguation, using French as our test language. The process of tagging consists of three stages: tokenisation, morphological analysis and disambiguation. The two taggers include the same tokeniser and morphological analyser. The tokeniser uses a finite-state transducer that reads the input and outputs a token whenever it has read far enough to be sure that a token is detected. The morphological analyser contains a transducer lexicon. It produces all the legitimate tags for words that appear in the lexicon. If a word is not in the lexicon, a guesser is consulted. The guesser employs another finite-state transducer. It reads a token and prints out a set of tags depending on prefixes, inflectional information and productive endings that it finds. We make even more use of transducers in the constraint-based tagger. The tagger reads one sentence at a time, a string of words and alternatags, feeds them to the grammatical transduc- &apos;There is a longer version (17 pages) of this paper in (Chanod and Tapanainen, 1994) ers that remove all but one alternative tag from all the words on the basis of contextual information. If all the transducers described above (tokeniser, morphological analyser and disambiguator) could be composed together, we would get one single transducer that transforms a raw input text to a fully disambiguated output. The statistical method contains the same tokeniser and morphological analyser. The disambiguation method is a conventional one: a hidden Markov model. 2 Morphological analysis and guessing The morphological analyser is based on a lexical transducer (Karttunen et al., 1992). The transducer maps each inflected surface form of a word to its canonical lexical form followed by the appropriate morphological tags. Words not found in the lexicon are analysed by a separate finite-state transducer, the guesser. We developed a simple, extremely compact and efficient guesser for French. It is based on the general assumption that neologisms and uncommon words tend to follow regular inflectional patterns. The guesser is thus based on productive endings t adverbs, adjectives, verbs). A given ending may of course point to various categories, e.g. er identifies nouns as well as verbs due to possible borrowings from English. 3 The statistical model We use the Xerox part-of-speech tagger (Cutting al., a statistical tagger made at the Xerox Palo Alto Research Center. 3.1 Training Xerox tagger is claimed (Cutting 1992) to be adaptable and easily trained; only a lexicon and suitable amount of untagged text is required. A new language-specific tagger can therefore be built with a minimal amount of work. We started our project by doing so. We took our lexicon with the new tagset, a corpus of French text, and 149 trained the tagger. We ran the tagger on another text and counted the errors. The result was not good; 13 % of the words were tagged incorrectly. The tagger does not require a tagged corpus for training, but two types of biases can be set to tell the tagger what is correct and what is not: symbol biases and transition biases. The symbol biases describe what is likely in a given ambiguity class. They represent kinds of lexical probabilities. The transition biases describe the likelihood of various tag pairs occurring in succession. The biases serve as initial values before training. We spent approximately one man-month writing biases and tuning the tagger. Our training corpus was rather small, because the training had to be repeated frequently. When it seemed that the results could not be further improved, we tested the tagger on a new corpus. The eventual result was that 96.8 % of the words in the corpus were tagged correctly. This result is about the same as for statistical taggers of English. 3.2 Modifying the biases 4 % rate is not generally considered a negative result for a statistical tagger, but some of the errors are serious. For example, a sequence of fredisambiguated in the wrong way, e.g. part heures (The train leaves at 5 word ambiguous between a noun and a verb (singular, third person), and it is disambiguated incorrectly. The tagger seems to prefer the noun reading between a singular noun and a preposition. One way to resolve this is to write new biases. We added two new ones. The first one says that a singular noun is not likely to be followed by a noun (this is not always true but we could call this a tendency). The second states that a singular noun is likely to be followed by a singular, third-person verb. The result was that the problematic sentence was disambiguated correctly, but the changes had a bad side effect. The overall error rate of the tagger increased by over 50 %. This illustrates how difficult it is to write good biases. Getting a correct result for a particular sentence does not necessarily increase the overall success rate. constraint-based model 4.1 A two-level model for tagging In the constraint-based tagger, the rules are represented as finite-state transducers. The transducers are composed with the sentence in a sequence. Each transducer may remove, or in principle it may also change, one or more readings of the words. After all the transducers have been applied, each word in the sentence has only one analysis. Our constraint-based tagger is based on techniques that were originally developed for morphological analysis. The disambiguation rules are similar to phonological rewrite rules (Kaplan and Kay, 1994), and the parsing algorithm is similar to the algorithm for combining the morphological rules with the lexicon (Karttunen, 1994). The tagger has a close relative in (Koskenniemi, 1990; Koskenniemi et al., 1992; Voutilainen and Tapanainen, 1993) where the rules are represented as finite-state machines that are conceptually intersected with each other. In this tagger the disambiguation rules are applied in the same manner as the morphological rules in (Koskenniemi, 1983). Another relative is represented in (Roche and Schabes, 1994) which uses a single finitestate transducer to transform one tag into another. A constraint-based system is also presented in (Karlsson, 1990; Karlsson et al., 1995). Related work using finite-state machines has been done using local grammars (Roche, 1992; Silberztein, 1993; Laporte, 1994). 4.2 Writing the rules 4.2.1 Studying ambiguities One quick experiment that motivated the building of the constraint-based model was the following: we took a million words of newspaper text and ranked ambiguous words by frequency. We found that a very limited set of word forms covers a large part of the total ambiguity. The 16 most ambiguous word account for 50 % of all ambiguity. Two thirds of the ambiguity are to the 97 most frequent ambiguous Another interesting observation is that the most frequent ambiguous words are usually words which are in general corpus-independent, i.e. words that belong to closed classes (determiners, prepositions, pronouns, conjunctions), auxiliaries, common adverbials or common verbs, like do, to make). The first corpus-specific word is in the 41st position. 4.2.2 Principled rules For the most frequent ambiguous word forms, one may safely define principled contextual restrictions to resolve ambiguities. This is in particular the case for clitic/determiner ambiguities to words like rule says that clitic pronouns are attached to a verb and determiners to a noun with possibly an unrestricted number of premodifiers. This is a good starting although some ambiguity remains as in le, les, en, un, a, est, plus, Le, son similar experiment shows that in the Brown corpus 63 word forms cover 50 % of all the ambiguity, and two thirds of the ambiguity is covered by 220 word forms. 150 can be read as a determiner-noun or clitic-verb sequence. Some of the very frequent words have categories that are rare, for instance the auxiliary forms a also be nouns and the pronoun is also a very rare verb form. In such a case, we restrict the use of the rarest categories to contexts where the most frequent reading is not at all possible, otherwise the most frequent reading preferred. For instance, the word be a noun or an auxiliary verb. We prefer the noun reading and accept the verb reading only when first-person pronoun in the left e.g. as in ne les avions pas did not have them). This means that the tagger errs only when a rare reading should be chosen in a context where the most common reading is still acceptable. This may never actually occur, depending on how accurate the contextual restrictions are. It can even be the case that discarding the rare readings would not induce a detectable loss in accuracy, e.g. in conflict between a pronoun and as a verb. The latter is a rarely used tense of a rather literary verb. The principled rules do not require any tagged corpus, and should be thus corpus-independent. The rules are based on a short list of extremely common words (fewer than 100 words). 4.2.3 Heuristics The rules described above are certainly not sufficient to provide full disambiguation, even if one considers only the most ambiguous word forms. We need more rules for cases that the principled rules do not disambiguate. Some ambiguity is extremely difficult to resolve using the information available. A very problemcase is the word can either be a demange des pommes eats apples) or an amalgamated preposition-determiner, in aime le bruit des vagues likes the sound of waves). Proper treatment of such an ambiguity would require verb subcategorisation and a description of complex coordinations of noun and prepositional phrases. This goes beyond the scope of both the statistical and the constraint-based taggers. For such cases we introduce ad-hoc heuristics. Some are quite reasonable, e.g. the determiner reading preferred at the begining of a sentence. Some are more or less arguable, e.g. the prepositional reading is preferred after a noun. One may identify various contexts in which either the noun or the adjective can be preferred. Such contextual restrictions (Chanod, 1993) are not always true, but may be considered reasonable for resolving the ambiguity. For instance, in the case of two successive noun/adjective ambigulike franc fort (the strong franc frank favour the nounâ€”adjective sequence except when the first word is a common prenominal such as petit, grand, premier, as petit fort (the small fort) even bon petit (the good little one). 4.2.4 Non-contextual rules Our heuristics do not resolve all the ambiguity. To obtain the fully unambiguous result we make use of non-contextual heuristics. The noncontextual rules may be thought of as lexical probabilities. We guess what the most probable tag is in the remaining ambiguities. For instance, preposition is preferred to adjective, pronoun is preferred to past participle, etc. The rules are obviously not very reliable, but they are needed only when the previous rules fail to fully disambiguate. 4.2.5 Current rules The current system contains 75 rules, consisting of: â€¢ 39 reliable contextual rules dealing mostly with frequent ambiguous words. â€¢ 25 rules describing heuristics with various degrees of linguistic generality. â€¢ 11 non-contextual rules for the remaining ambiguities. The rules were constructed in less than one month, on the basis of 50 newspaper sentences. All the rules are currently represented by 11 transducers. results 5.1 Test A For evaluation, we used a corpus totally unrelated to the development corpus. It contains 255 sentences (5752 words) randomly selected from a corpus of economic reports. About 54 % of the words are ambiguous. The text is first tagged manually without using the disambiguators, and the output of the tagger is then compared to the hand-tagged result. If we apply all the rules, we get a fully disambiguated result with an error rate of only 1.3 %. This error rate is much lower than the one we get using the hidden Markov model (3.2 %). See Figure 1. We can also restrict the tagger to using only the most reliable rules. Only 10 words lose the correct tag when almost 2000 out of 3085 ambiguous words are disambiguated. Among the remaining 1136 ambiguous words about 25 % of the ambiguity is due to determiner/preposition ambiguities like du and % are adjective/noun ambiguities and 18 % are noun/verb ambiguities. If we use both the principled and heuristic rules, the error rate is 0.52 % while 423 words remain ambiguous. The non-contextual rules that eliminate the remaining 423 ambiguities produce an 151 error rate remaining ambiguity tag / word (correctness) Lexicon + Guesser 0.03 % (99.97 %) 54% 1.64 Hidden Markov model 3.2 % (96.8 %) % 1.00 Principled rules % (99.83 %) 20 % 1.24 Principled and heuristic rules 0.52 % (99.48 %) 7 % 1.09 All the rules 1.3 % (98.7 %) 0 % 1.00 Figure 1: The result in the test sample additional 43 errors. Overall, 98.7 % of the words receive the correct tag. Test We also tested the taggers with more difficult text. The 12 000 word sample of newspaper text has and proper that match an existing word in the lexicon. Problems of the latter type are relatively rare but this sample was exceptional. Altogether the lexicon mismatches produced 0.5 % errors to the input of the taggers. The results are shown in Figure 2. This text also seems to be generally more difficult to parse than the first one. 5.3 Combination of the taggers We also tried combining the taggers, using first the rules and then the statistics (a similar approach was also used in (Tapanainen and Voutilainen, 1994)). We evaluated the results obtained by the following sequence of operations: 1) Running the constraint-based tagger without the final, non-contextual rules. 2) Using the statistical disambiguator independently. We select the tag proposed by the statistical disambiguator if it is not removed during step 1. 3) Solving the remaining ambiguities by running the final non-contextual rules of the constraint-based tagger. This last step ensures that one gets a fully disambiguated text. Actually only about 0.5 % of words were not fully disambiguated after step 2. used the test sample the first step, 1400 words out of 12 000 remain ambiguous. The process of combining the three steps described above eventually leads to more errors than running the constraint-based tagger alone. The statistical tagger introduces 220 errors on the 1400 words that remain ambiguous after step 1. In comparison, the final set of non-contextual rules introduces around 150 errors on the same set of 1400 words. We did not expect this result. One possible explanation for the superior performance of the final non-contextual rules is that they are meant to apply after the previous rules failed to disambiguate the word. This is in itself useful Botta, Demis, Ferrasse, Hersant, information. The final heuristics favour tags that have survived all conditions that restrict their use. For instance, the contextual rules define various where the preposition tag for preferred. Therefore, the final heuristics favours the reading for 6 Analysis of errors 6.1 Errors of principled and heuristic rules Let us now consider what kind of errors the constraint-based tagger produced. We do not deal with errors produced by the last set of rules, the non-contextual rules, because it is already known that they are not very accurate. To make the tagger better, they should be replaced by writing more accurate heuristic rules. We divide the errors into three categories: (1) errors due to multi-word expressions, (2) errors that should/could be resolved and (3) errors that are hard to resolve by using the information that is available. The first group (15 errors), the multi-word expressions, are difficult for the syntax-based rules because in many cases the expression does not follow any conventional syntactic structure, or the structure may be very rare. In multi-word expressions some words also have categories that may not appear anywhere else. The best way to handle them is to lexicalise these expressions. When a possible expression is recognised we can either collapse it into one unit or leave it otherwise intact except that the most &amp;quot;likely&amp;quot; interpretation is marked. The biggest group (41 errors) contains errors that could have been resolved correctly but were not. The reason for this is obvious: only a relatively small amount of time was allowed for writing the rules. In addition, the rules were constructed on the basis of a rather small set of example sentences. Therefore, it would be very surprising if such errors did not appear in the test sample taken from a different source. The errors are the following: â€¢ The biggest subgroup has 19 errors that require modifications to existing rules. Our rules were meant to handle such cases but fail 152 error rate remaining ambiguity tag / word (correctness) Lexicon + Guesser 0.5 % (99.5 %) 48% 1.59 Hidden Markov model I 5.0 % (95.0 %) 1.00 Principled rules 0.8 % (99.2 %) 23 % 1.29 Principled and heuristic rules 1.3 % (98.7 %) 12 % 1.14 All the rules 2.5 % (97.5 %) 0 % 1.00 Figure 2: The result in a difficult test sample with many lexicon mismatches to do so correctly in some sentences. Often only a minor correction is needed. â€¢ Some syntactic constructions, or word sequences, were omitted. This caused 7 errors which could easily be avoided by writing more rules. For instance, a construction like &amp;quot;preposition + clitic + finite verb&amp;quot; was forbidden. The phrase analysed in this way while the correct analysis is &amp;quot;preposition determiner + noun&amp;quot;. â€¢ Sometimes a little bit of extra lexical information is required. Six errors would require more information or the kind of refinement in the tag inventory that would not have been appropriate for the statistical tagger. â€¢ Nine errors could be avoided by refining existing heuristics, especially by taking into acexceptions for specific words like The remaining errors (28 errors) constitute the price we pay for using the heuristics. Removing the rules which fail would cause a lot of ambiguity to remain. The errors are the following: Fifteen errors are due to the heuristics for is little room for improvement at this level of description (see Chapter 4.2.3). However, the current, simple heuristics fully 850 instances of of 914 i.e. 92 % of all the occurrences were parsed with less than a 2 % error rate. â€¢ Six errors involve nounâ€”adjective ambiguities that are difficult to solve, for instance, in a subject or object predicate position. â€¢ Seven errors seem to be beyond reach for various reasons: long coordination, rare conetc. An example is boites where is tagged in the test sample because the noun form is misspelled is identified only as a verb by the lexicon. 6.2 Difference between the taggers We also investigated how the errors compare bethe two taggers. Here we used the disambiguated outputs of the taggers. The errors belong mainly to three classes: â€¢ Some errors appear predominantly with the statistical tagger and almost never with the constraint-based tagger. This is particularly the case with the ambiguity between past participles and adjectives. â€¢ Some errors are common to both taggers, the constraint-based tagger generally being more accurate (often with a ratio of 1 to 2). These errors cover ambiguities that are known to be difficult to handle in general, such as the already mentioned determiner/preposition ambiguity. â€¢ Finally, there are errors that are specific to the constraint-based tagger. They are often related to errors that could be corrected with some extra work. They are relatively infrequent, thus the global accuracy of the constraint-based tagger remains higher. The first two classes of errors are generally difficult to correct. The easiest way to improve the constraint-based tagger is to concentrate on the final class. As we mentioned earlier, it is not very easy to change the behaviour of the statistical tagger in one place without some side-effects elsewhere. This means that the errors of the first class are probably easiest to resolve by means other than statistics. The first class is quite annoying for the statistical parser because it contains errors that are intuitively very clear and resolvable, but which are far beyond the limits of the current statistical tagger. We can take an easy sentence to demonstrate this: ne le pense pas. not think so. ne le penses pas. do not think so. ne le pense pas. does not think so. verb in the first person or in the third person. It is usually easy to determine the person just by checking the personal pronoun nearby. For a human or a constraint-based tagger this is an easy task, for a statistical tagger it is not. There are two words between the pronoun and the verb that do not carry any information about the person. The personal pronoun may thus be too verb because bi-gram models can see no farther than tri-gram models is not case with all the French verbs, e.g. croit. 153 farther than le. Also, as mentioned earlier, resolving the adjective vs. past participle ambiguity is much harder, if the tagger does not know whether there is an auxiliary verb in the sentence or not. 7 Conclusion We have presented two taggers for french: a statistical one and a constraint-based one. There are two ways to train the statistical tagger: from a tagged corpus or using a selforganising method that does not need a tagged corpus. We had a strict time limit of one month for doing the tagger and no tagged corpus was available. This is a short time for the manual tagging of a corpus and for the training of the tagger. It would be risky to spend, say, three weeks for writing a corpus, and only one week for training. The size of corpus would have to be limited, because it should be also checked. We selected the Xerox tagger that learns from an untagged corpus&apos;. The task was not as straigthforward as we thought. Without human assistance in the training the result was not impressive, and we had to spend much time tuning the tagger and guiding the learning process. In a month we achieved 95-97 % accuracy. The training process of a statistical tagger requires some time because the linguistic information has to be incorporated into the tagger one way or another, it cannot be obtained for free starting from null. Because the linguistic information is needed, we decided to encode the information in a more straightforward way, as explicit linguistic disambiguation rules. It has been argued that statistical taggers are superior to rulebased/hand-coded ones because of better accuracy and better adaptability (easy to train). In our experiment, both claims turned out to be wrong. For the constraint-based tagger we set one month time limit for writing the constraints by hand. We used only linguistic intuition and a very limited set of sentences to write the 75 constraints. We formulated constraints of different accuracy. Some of the constraints are almost 100 % accurate, some of them just describe tendencies. Finally, when we thought that the rules were good enough, we took two text samples from different sources and tested both the taggers. The constraint-based tagger made several naive errors because we had forgotten, miscoded or ignored some linguistic phenomena, but still, it made only half of the errors that the statistical one made. A big difference between the taggers is that the tuning of the statistical tagger is very subtle i.e. it is hard to predict the effect of tuning the parameters of the system, whereas the constraint-based tagger is very straightforward to correct. Our general conclusion is that the hand-coded constraints perform better than the statistical tagger and that we can still refine them. The most important of our findings is that writing constraints that contain more linguistic information than the current statistical model does not take much time. References Jean-Pierre Chanod. Problemes de robustesse en syntaxique. In de la conference et langue naturelle. Universite de Nantes, 1993. Jean-Pierre Chanod and Pasi Tapanainen. Statistical and Constraint-based Taggers for French.</abstract>
<note confidence="0.84002094117647">Technical report MLTT-016, Rank Xerox Research Centre, Grenoble, 1994. Doug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun. A Practical Part-of-Speech In Conference on Applied Natu- Language Processing. 133-140. Trento, 1992. Ron Kaplan and Martin Kay. Regular Models Phonological Rule Systems. 20, Number 3, pages 331-378. Fred Karlsson. Constraint Grammar as a Framefor Parsing Running Text. In of Coding-90. Papers presented to the 13th International Conference on Computational Lin- 3, pages 168-173. Helsinki, 1990. Fred Karlsson, Atro Voutilainen, Juha Heikkila Arto Anttila (eds.). Grammar:</note>
<title confidence="0.564782">a Language-Independent System for Parsing</title>
<note confidence="0.946212210526316">Text. de Gruyter, Berlin, 1995. Lauri Karttunen. Constructing Lexical Trans- In of Co1ing-94. The fifteenth International Conference on Computa- Linguistics. I, pages 406-411. Kyoto, 1994. Lauri Karttunen, Ron Kaplan and Annie Zaenen. Two-level morphology with composition. of Coling-92. The fourteenth International Conference on Computational Lin- I, pages 141-148. Nantes, 1992. Koskenniemi. morphology. A general computational model for word-form and production. of Helsinki, 1983. Kimmo Koskenniemi. Finite-state parsing and In of Coling-90. Papers presented to the 13th International Conon Computational Linguistics. 2, pages 229-232. Helsinki, 1990. Kimmo Koskenniemi, Pa,si Tapanainen and Atro Voutilainen. Compiling and using finite-state 154 rules. In of Coling-92. The fourteenth International Conference on Linguistics. I, pages 156- 162. Nantes, 1992. Eric Laporte. Experiences in Lexical Disambigua- Using Local Grammars. In International Conference on Computational Lexicogra- 163-172. Budapest, 1994. Emmanuel Roche. Text Disambiguation by finitestate automata, an algorithm and experiments corpora. In of Coling-92. The fourteenth International Conference on Com- Linguistics. III, pages 993-997. Nantes, 1992.</note>
<author confidence="0.663049">Deter-</author>
<abstract confidence="0.934447375">ministic part-of-speech tagging with finite-state transducers. Technical report TR-94-07, Mitsubishi Electric Research Laboratories, Cambridge, USA. Silberztein. electroniques et analyse automatique de teites. Le systeme IN- Paris, 1993. Pasi Tapanainen and Atro Voutilainen. Tagging â€” Don&apos;t guess if you know. In Conference on Applied Natural Language Pro- 47-52. Stuttgart, 1994. Atro Voutilainen and Path Tapanainen. Ambiguity resolution in a reductionistic parser. In Sixth Conference of the European Chapter of the 394-403. Utrecht, 1993. A The restricted tag set In this appendix the tag set is represented. Besides the following tags, there may also be some tags like is the reading for words des word initially ambiguous between PREPinformation is mainly for the statistical tagger to deal with, for instance, different prepositions in a different way. The constraint-based tagger does not need this because it has direct access to word forms anyway. After disambiguation, the word-specific tags may be The tag changed back into reduce the redundant information. â€¢ determiner e.g. la, This covers masculine as well as forms. Sample sentence: chien dans la cuisine. dog is sleeping in the kitchen). â€¢ determiner e.g. mes. This covers masculine as well as feminine Sample sentence: Lesenfants jouent mes livres. children are playing with my books.) â€¢ invariant in number sentence: chien est heureuxquand les enfants sont heureux.(The dog is happy when the children are happy.) â€¢ adjective e.g. gencovers masculine as well as femforms. Sample sentence: chien est yenta(The dog is nice.) â€¢ adjective e.g. gencovers masculine as well as femiforms. Sample sentence: chiens sont gentils.(These dogs are nice.) â€¢ invariant in number e.g. Francais. covers masculine as as feminine forms. Sample sentence: sourisdanseni. mice are dancing.) â€¢ noun e.g. fleur. This covers masculine as well as feminine Sample sentence: &apos;est une jolie fieur. (It is a nice flower.) â€¢ noun e.g. fieurs. This covers masculine as well as feminine Sample sentence: aimons les fleurs.(We like flowers.) â€¢ verb, infinitive sentence: chien vient d&apos;Ãªtre dog has just been punished.) â€¢ verb, present parayant. â€¢ verb, past participle eu. sentence: theoreme ite demontre. theorem has been proved.) â€¢ verb, covers any 1st or 2nd person form, regardless of number, tense or mood, e.g. 1st person singular present indicative, 2nd person plural imperasoyons, es. sentence: es are strong.) â€¢ verb, covers any person singular form e.g. sera, sentence: est forte. is strong.) â€¢ verb, covers any person plural form e.g. seront, sentence: had slept.) â€¢ verb e.g. dormir. sentence: chien dormir.(The dog enjoys sleeping.) â€¢ participle finissant, aboyant. senchien arrive en aboyant. dog is coming and it is barking.) â€¢ 1st or 2nd person verb form, regardless of number, tense or mood e.g. 1st person singular present indicative, 155 pers plural imperative: finissons. sentence: chante.(I â€¢ 3rd person singular verb e.g. finit, aboie. sen- Le aboie.(The dog is barking.) â€¢ 3rd person plural verb e.g. finissent, aboient. Samsentence: chiens aboient.(The dogs are barking.) â€¢ participle invariant in nume.g. sentence: chien surpris. dog surprised me.) â€¢ past participle e.g. fini, finie. This covers masculine as well as femiforms. Sample sentence: journee esi finie.(The day is over.) â€¢ past participle e.g. covers masculine as well as femforms. Sample sentence: travaux finis.(The work is finished.) PC clitic pronoun such as Sample sentence: me l&apos;a donne. (He gave it to me.) PRON person pronoun, relative proexcluded. e.g. elles, chacun. a parte a chacun.(He spoke to every person.) â€¢ or 2nd person pronoun je, nous. sentence: tu viendras avec moi?(Will you come with me?) VOICILA for words sentence: Voicimon chien. (Here is my dog.) ADV e.g. senjour est finalementarrive. day has finally come.) NEG particle. Reserved for the ne. Sample sentence: chien ne dont dog is not sleeping.) PREP e.g. senchien dont dansla cuisine. dog sleeps in the kitchen.) For statistical taggers this group may be divided into subgroups for different preposition like PREP-A, CONN This class includes coorconjuctions such as such as que, or inpronouns such as have very special are not coded as chien et le chat dorment quand dog and the cat sleep when it rains.) For statistical taggers this group may be divided into subgroups for different connectors, CONN-Q, COMME for all instances of the sentence: joue commeun enfant. plays like a child.) CONJQUE for all instances of the NUM e.g. 12,7, 120/98, 34+0.7. HEURE representing time e.g. 12h24, 12:45:00. â€¢ MISC Miscellaneous words, such as: interi.e. words that only exist as part of a multi-word expression, such as part of priori. CM PUNCT other than comma.</abstract>
<intro confidence="0.524339">156</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Doug Cutting</author>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Penelope Sibun</author>
</authors>
<title>A Practical Part-of-Speech Tagger.</title>
<date>1992</date>
<booktitle>In Third Conference on Applied Natural Language Processing.</booktitle>
<pages>133--140</pages>
<location>Trento,</location>
<contexts>
<context position="3406" citStr="Cutting et al., 1992" startWordPosition="537" endWordPosition="540">s not found in the lexicon are analysed by a separate finite-state transducer, the guesser. We developed a simple, extremely compact and efficient guesser for French. It is based on the general assumption that neologisms and uncommon words tend to follow regular inflectional patterns. The guesser is thus based on productive endings (like men t for adverbs, ible for adjectives, er for verbs). A given ending may of course point to various categories, e.g. er identifies nouns as well as verbs due to possible borrowings from English. 3 The statistical model We use the Xerox part-of-speech tagger (Cutting et al., 1992), a statistical tagger made at the Xerox Palo Alto Research Center. 3.1 Training The Xerox tagger is claimed (Cutting et al,, 1992) to be adaptable and easily trained; only a lexicon and suitable amount of untagged text is required. A new language-specific tagger can therefore be built with a minimal amount of work. We started our project by doing so. We took our lexicon with the new tagset, a corpus of French text, and 149 trained the tagger. We ran the tagger on another text and counted the errors. The result was not good; 13 % of the words were tagged incorrectly. The tagger does not requir</context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1992</marker>
<rawString>Doug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun. A Practical Part-of-Speech Tagger. In Third Conference on Applied Natural Language Processing. pages 133-140. Trento, 1992.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ron Kaplan</author>
<author>Martin Kay</author>
</authors>
<title>Regular Models of Phonological Rule Systems.</title>
<journal>Computational Linguistics</journal>
<volume>20</volume>
<pages>331--378</pages>
<marker>Kaplan, Kay, </marker>
<rawString>Ron Kaplan and Martin Kay. Regular Models of Phonological Rule Systems. Computational Linguistics Vol. 20, Number 3, pages 331-378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Karlsson</author>
</authors>
<title>Constraint Grammar as a Framework for Parsing Running Text.</title>
<date>1990</date>
<booktitle>In proceedings of Coding-90. Papers presented to the 13th International Conference on Computational Linguistics.</booktitle>
<volume>3</volume>
<pages>168--173</pages>
<location>Helsinki,</location>
<contexts>
<context position="7309" citStr="Karlsson, 1990" startWordPosition="1200" endWordPosition="1201">mbining the morphological rules with the lexicon (Karttunen, 1994). The tagger has a close relative in (Koskenniemi, 1990; Koskenniemi et al., 1992; Voutilainen and Tapanainen, 1993) where the rules are represented as finite-state machines that are conceptually intersected with each other. In this tagger the disambiguation rules are applied in the same manner as the morphological rules in (Koskenniemi, 1983). Another relative is represented in (Roche and Schabes, 1994) which uses a single finitestate transducer to transform one tag into another. A constraint-based system is also presented in (Karlsson, 1990; Karlsson et al., 1995). Related work using finite-state machines has been done using local grammars (Roche, 1992; Silberztein, 1993; Laporte, 1994). 4.2 Writing the rules 4.2.1 Studying ambiguities One quick experiment that motivated the building of the constraint-based model was the following: we took a million words of newspaper text and ranked ambiguous words by frequency. We found that a very limited set of word forms covers a large part of the total ambiguity. The 16 most frequent ambiguous word forms2 account for 50 % of all ambiguity. Two thirds of the ambiguity are due to the 97 most</context>
</contexts>
<marker>Karlsson, 1990</marker>
<rawString>Fred Karlsson. Constraint Grammar as a Framework for Parsing Running Text. In proceedings of Coding-90. Papers presented to the 13th International Conference on Computational Linguistics. Vol. 3, pages 168-173. Helsinki, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Karlsson</author>
</authors>
<title>Atro Voutilainen, Juha Heikkila and Arto Anttila (eds.). Constraint Grammar: a Language-Independent System for Parsing Unrestricted Text. Mouton de Gruyter,</title>
<date>1995</date>
<location>Berlin,</location>
<marker>Karlsson, 1995</marker>
<rawString>Fred Karlsson, Atro Voutilainen, Juha Heikkila and Arto Anttila (eds.). Constraint Grammar: a Language-Independent System for Parsing Unrestricted Text. Mouton de Gruyter, Berlin, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Constructing Lexical Transducers.</title>
<date>1994</date>
<booktitle>In proceedings of Co1ing-94. The fifteenth International Conference on Computational Linguistics. Vol I,</booktitle>
<pages>406--411</pages>
<publisher>Kyoto,</publisher>
<contexts>
<context position="6761" citStr="Karttunen, 1994" startWordPosition="1114" endWordPosition="1115">nted as finite-state transducers. The transducers are composed with the sentence in a sequence. Each transducer may remove, or in principle it may also change, one or more readings of the words. After all the transducers have been applied, each word in the sentence has only one analysis. Our constraint-based tagger is based on techniques that were originally developed for morphological analysis. The disambiguation rules are similar to phonological rewrite rules (Kaplan and Kay, 1994), and the parsing algorithm is similar to the algorithm for combining the morphological rules with the lexicon (Karttunen, 1994). The tagger has a close relative in (Koskenniemi, 1990; Koskenniemi et al., 1992; Voutilainen and Tapanainen, 1993) where the rules are represented as finite-state machines that are conceptually intersected with each other. In this tagger the disambiguation rules are applied in the same manner as the morphological rules in (Koskenniemi, 1983). Another relative is represented in (Roche and Schabes, 1994) which uses a single finitestate transducer to transform one tag into another. A constraint-based system is also presented in (Karlsson, 1990; Karlsson et al., 1995). Related work using finite-</context>
</contexts>
<marker>Karttunen, 1994</marker>
<rawString>Lauri Karttunen. Constructing Lexical Transducers. In proceedings of Co1ing-94. The fifteenth International Conference on Computational Linguistics. Vol I, pages 406-411. Kyoto, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Ron Kaplan</author>
<author>Annie Zaenen</author>
</authors>
<title>Two-level morphology with composition.</title>
<date>1992</date>
<booktitle>In proceedings of Coling-92. The fourteenth International Conference on Computational Linguistics. Vol I,</booktitle>
<pages>141--148</pages>
<location>Nantes,</location>
<contexts>
<context position="2643" citStr="Karttunen et al., 1992" startWordPosition="411" endWordPosition="414">od and Tapanainen, 1994) ers that remove all but one alternative tag from all the words on the basis of contextual information. If all the transducers described above (tokeniser, morphological analyser and disambiguator) could be composed together, we would get one single transducer that transforms a raw input text to a fully disambiguated output. The statistical method contains the same tokeniser and morphological analyser. The disambiguation method is a conventional one: a hidden Markov model. 2 Morphological analysis and guessing The morphological analyser is based on a lexical transducer (Karttunen et al., 1992). The transducer maps each inflected surface form of a word to its canonical lexical form followed by the appropriate morphological tags. Words not found in the lexicon are analysed by a separate finite-state transducer, the guesser. We developed a simple, extremely compact and efficient guesser for French. It is based on the general assumption that neologisms and uncommon words tend to follow regular inflectional patterns. The guesser is thus based on productive endings (like men t for adverbs, ible for adjectives, er for verbs). A given ending may of course point to various categories, e.g. </context>
</contexts>
<marker>Karttunen, Kaplan, Zaenen, 1992</marker>
<rawString>Lauri Karttunen, Ron Kaplan and Annie Zaenen. Two-level morphology with composition. In proceedings of Coling-92. The fourteenth International Conference on Computational Linguistics. Vol I, pages 141-148. Nantes, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Two-level morphology. A general computational model for word-form recognition and production.</title>
<date>1983</date>
<institution>University of Helsinki,</institution>
<contexts>
<context position="7106" citStr="Koskenniemi, 1983" startWordPosition="1168" endWordPosition="1169">t were originally developed for morphological analysis. The disambiguation rules are similar to phonological rewrite rules (Kaplan and Kay, 1994), and the parsing algorithm is similar to the algorithm for combining the morphological rules with the lexicon (Karttunen, 1994). The tagger has a close relative in (Koskenniemi, 1990; Koskenniemi et al., 1992; Voutilainen and Tapanainen, 1993) where the rules are represented as finite-state machines that are conceptually intersected with each other. In this tagger the disambiguation rules are applied in the same manner as the morphological rules in (Koskenniemi, 1983). Another relative is represented in (Roche and Schabes, 1994) which uses a single finitestate transducer to transform one tag into another. A constraint-based system is also presented in (Karlsson, 1990; Karlsson et al., 1995). Related work using finite-state machines has been done using local grammars (Roche, 1992; Silberztein, 1993; Laporte, 1994). 4.2 Writing the rules 4.2.1 Studying ambiguities One quick experiment that motivated the building of the constraint-based model was the following: we took a million words of newspaper text and ranked ambiguous words by frequency. We found that a </context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>Kimmo Koskenniemi. Two-level morphology. A general computational model for word-form recognition and production. University of Helsinki, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Finite-state parsing and disambiguation.</title>
<date>1990</date>
<booktitle>In proceedings of Coling-90. Papers presented to the 13th International Conference on Computational Linguistics.</booktitle>
<volume>2</volume>
<pages>229--232</pages>
<location>Helsinki,</location>
<contexts>
<context position="6816" citStr="Koskenniemi, 1990" startWordPosition="1123" endWordPosition="1124">composed with the sentence in a sequence. Each transducer may remove, or in principle it may also change, one or more readings of the words. After all the transducers have been applied, each word in the sentence has only one analysis. Our constraint-based tagger is based on techniques that were originally developed for morphological analysis. The disambiguation rules are similar to phonological rewrite rules (Kaplan and Kay, 1994), and the parsing algorithm is similar to the algorithm for combining the morphological rules with the lexicon (Karttunen, 1994). The tagger has a close relative in (Koskenniemi, 1990; Koskenniemi et al., 1992; Voutilainen and Tapanainen, 1993) where the rules are represented as finite-state machines that are conceptually intersected with each other. In this tagger the disambiguation rules are applied in the same manner as the morphological rules in (Koskenniemi, 1983). Another relative is represented in (Roche and Schabes, 1994) which uses a single finitestate transducer to transform one tag into another. A constraint-based system is also presented in (Karlsson, 1990; Karlsson et al., 1995). Related work using finite-state machines has been done using local grammars (Roch</context>
</contexts>
<marker>Koskenniemi, 1990</marker>
<rawString>Kimmo Koskenniemi. Finite-state parsing and disambiguation. In proceedings of Coling-90. Papers presented to the 13th International Conference on Computational Linguistics. Vol. 2, pages 229-232. Helsinki, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Pa,si Tapanainen and Atro Voutilainen. Compiling and using finite-state syntactic rules.</title>
<date>1992</date>
<booktitle>In proceedings of Coling-92. The fourteenth International Conference on Computational Linguistics. Vol. I,</booktitle>
<pages>156--162</pages>
<location>Nantes,</location>
<marker>Koskenniemi, 1992</marker>
<rawString>Kimmo Koskenniemi, Pa,si Tapanainen and Atro Voutilainen. Compiling and using finite-state syntactic rules. In proceedings of Coling-92. The fourteenth International Conference on Computational Linguistics. Vol. I, pages 156-162. Nantes, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Laporte</author>
</authors>
<title>Experiences in Lexical Disambiguation Using Local Grammars. In</title>
<date>1994</date>
<booktitle>Third International Conference on Computational Lexicography.</booktitle>
<pages>163--172</pages>
<location>Budapest,</location>
<contexts>
<context position="7458" citStr="Laporte, 1994" startWordPosition="1221" endWordPosition="1222">Voutilainen and Tapanainen, 1993) where the rules are represented as finite-state machines that are conceptually intersected with each other. In this tagger the disambiguation rules are applied in the same manner as the morphological rules in (Koskenniemi, 1983). Another relative is represented in (Roche and Schabes, 1994) which uses a single finitestate transducer to transform one tag into another. A constraint-based system is also presented in (Karlsson, 1990; Karlsson et al., 1995). Related work using finite-state machines has been done using local grammars (Roche, 1992; Silberztein, 1993; Laporte, 1994). 4.2 Writing the rules 4.2.1 Studying ambiguities One quick experiment that motivated the building of the constraint-based model was the following: we took a million words of newspaper text and ranked ambiguous words by frequency. We found that a very limited set of word forms covers a large part of the total ambiguity. The 16 most frequent ambiguous word forms2 account for 50 % of all ambiguity. Two thirds of the ambiguity are due to the 97 most frequent ambiguous words3. Another interesting observation is that the most frequent ambiguous words are usually words which are in general corpus-i</context>
</contexts>
<marker>Laporte, 1994</marker>
<rawString>Eric Laporte. Experiences in Lexical Disambiguation Using Local Grammars. In Third International Conference on Computational Lexicography. pages 163-172. Budapest, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Roche</author>
</authors>
<title>Text Disambiguation by finitestate automata, an algorithm and experiments on corpora.</title>
<date>1992</date>
<booktitle>In proceedings of Coling-92. The fourteenth International Conference on Computational Linguistics. Vol III,</booktitle>
<pages>993--997</pages>
<location>Nantes,</location>
<contexts>
<context position="7423" citStr="Roche, 1992" startWordPosition="1217" endWordPosition="1218">1990; Koskenniemi et al., 1992; Voutilainen and Tapanainen, 1993) where the rules are represented as finite-state machines that are conceptually intersected with each other. In this tagger the disambiguation rules are applied in the same manner as the morphological rules in (Koskenniemi, 1983). Another relative is represented in (Roche and Schabes, 1994) which uses a single finitestate transducer to transform one tag into another. A constraint-based system is also presented in (Karlsson, 1990; Karlsson et al., 1995). Related work using finite-state machines has been done using local grammars (Roche, 1992; Silberztein, 1993; Laporte, 1994). 4.2 Writing the rules 4.2.1 Studying ambiguities One quick experiment that motivated the building of the constraint-based model was the following: we took a million words of newspaper text and ranked ambiguous words by frequency. We found that a very limited set of word forms covers a large part of the total ambiguity. The 16 most frequent ambiguous word forms2 account for 50 % of all ambiguity. Two thirds of the ambiguity are due to the 97 most frequent ambiguous words3. Another interesting observation is that the most frequent ambiguous words are usually </context>
</contexts>
<marker>Roche, 1992</marker>
<rawString>Emmanuel Roche. Text Disambiguation by finitestate automata, an algorithm and experiments on corpora. In proceedings of Coling-92. The fourteenth International Conference on Computational Linguistics. Vol III, pages 993-997. Nantes, 1992.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Emmanuel Roche</author>
<author>Yves Schabes</author>
</authors>
<title>Deterministic part-of-speech tagging with finite-state transducers.</title>
<tech>Technical report TR-94-07,</tech>
<institution>Mitsubishi Electric Research Laboratories,</institution>
<location>Cambridge, USA.</location>
<marker>Roche, Schabes, </marker>
<rawString>Emmanuel Roche and Yves Schabes. Deterministic part-of-speech tagging with finite-state transducers. Technical report TR-94-07, Mitsubishi Electric Research Laboratories, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Silberztein</author>
</authors>
<title>Dictionnaires electroniques et analyse automatique de teites. Le systeme INTEX.</title>
<date>1993</date>
<location>Masson, Paris,</location>
<contexts>
<context position="7442" citStr="Silberztein, 1993" startWordPosition="1219" endWordPosition="1220">iemi et al., 1992; Voutilainen and Tapanainen, 1993) where the rules are represented as finite-state machines that are conceptually intersected with each other. In this tagger the disambiguation rules are applied in the same manner as the morphological rules in (Koskenniemi, 1983). Another relative is represented in (Roche and Schabes, 1994) which uses a single finitestate transducer to transform one tag into another. A constraint-based system is also presented in (Karlsson, 1990; Karlsson et al., 1995). Related work using finite-state machines has been done using local grammars (Roche, 1992; Silberztein, 1993; Laporte, 1994). 4.2 Writing the rules 4.2.1 Studying ambiguities One quick experiment that motivated the building of the constraint-based model was the following: we took a million words of newspaper text and ranked ambiguous words by frequency. We found that a very limited set of word forms covers a large part of the total ambiguity. The 16 most frequent ambiguous word forms2 account for 50 % of all ambiguity. Two thirds of the ambiguity are due to the 97 most frequent ambiguous words3. Another interesting observation is that the most frequent ambiguous words are usually words which are in </context>
</contexts>
<marker>Silberztein, 1993</marker>
<rawString>Max Silberztein. Dictionnaires electroniques et analyse automatique de teites. Le systeme INTEX. Masson, Paris, 1993.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Pasi Tapanainen</author>
<author>Atro Voutilainen</author>
</authors>
<title>Tagging accurately â€” Don&apos;t guess if you know.</title>
<booktitle>In Fourth. Conference on Applied Natural Language Pro-</booktitle>
<marker>Tapanainen, Voutilainen, </marker>
<rawString>Pasi Tapanainen and Atro Voutilainen. Tagging accurately â€” Don&apos;t guess if you know. In Fourth. Conference on Applied Natural Language Pro-</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>