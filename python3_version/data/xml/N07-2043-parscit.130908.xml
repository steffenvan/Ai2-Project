<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003499">
<title confidence="0.998399">
A High Accuracy Method for Semi-supervised Information Extraction
</title>
<author confidence="0.997582">
Stephen Tratz Antonio Sanfilippo
</author>
<affiliation confidence="0.904552">
Pacific Northwest National Laboratory Pacific Northwest National Laboratory
</affiliation>
<address confidence="0.84004">
Richland, WA 99352 Richland, WA 99352
</address>
<email confidence="0.985728">
stephen.tratz@pnl.gov antonio.sanfilippo@pnl.gov
</email>
<sectionHeader confidence="0.998475" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99968205882353">
Customization to specific domains of dis-
course and/or user requirements is one of
the greatest challenges for today&apos;s Infor-
mation Extraction (IE) systems. While
demonstrably effective, both rule-based
and supervised machine learning ap-
proaches to IE customization pose too
high a burden on the user. Semi-
supervised learning approaches may in
principle offer a more resource effective
solution but are still insufficiently
accurate to grant realistic application. We
demonstrate that this limitation can be
overcome by integrating fully-supervised
learning techniques within a semi-
supervised IE approach, without
increasing resource requirements.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999568414634146">
Customization to specific discourse domains
and/or user requirements is one of the greatest
challenges for today&apos;s Information Extraction (IE)
systems. While demonstrably effective, both rule-
based and supervised machine learning approaches
to IE customization require a substantial
development effort. For example, Aone and
Ramos-Santacruz (2000) present a rule-based IE
system which handles 100 types of relations and
events. Building such a system requires the manual
construction of numerous extraction patterns
supported by customized ontologies. Soderland
(1999) uses supervised learning to induce a set of
rules from hand-tagged training examples. While
Sonderland suggests that the human effort can be
reduced by interleaving learning and manual
annotation activities, the creation of training data
remains an onerous task.
To reduce the knowledge engineering burden on
the user in constructing and porting an IE system,
unsupervised learning has been utilized, e.g. Riloff
(1996), Yangarber et al. (2000), and Sekine (2006).
Banko et al. (2007) present a self-supervised
system that aims to avoid the manual IE
customization problem by extracting all possible
relations of interest from text. Stevenson and
Greenwood (2005) propose a weakly supervised
approach to sentence filtering that uses semantic
similarity and bootstrapping to acquire IE patterns.
Stevenson&apos;s and Greenwood&apos;s approach provides
some of the best available results in weakly
supervised IE to date, with 0.58 F-measure. While
very good, an F-measure of 0.58 does not provide
sufficient reliability to grant use in a production
system.
In this paper, we show that it is possible to
provide a significant improvement over
Stevenson&apos;s and Greenwood&apos;s results, without
increasing resource requirements, by integrating
fully-supervised learning techniques within a
weakly supervised IE approach.
</bodyText>
<subsectionHeader confidence="0.999699">
1.1 Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.996047333333333">
Our method is modeled on the approach developed
by Stevenson and Greenwood (2005) but uses a
different technique for ranking candidate patterns.
Stevenson&apos;s and Greenwood&apos;s algorithm takes as
data inputs a small set of initial seed patterns and a
corpus of documents, and uses any of several
semantic similarity measures (Resnik, 1995; Jiang
and Conrath, 1997; Patwardhan et al., 2003) to
iteratively identify patterns in the document corpus
</bodyText>
<page confidence="0.985288">
169
</page>
<note confidence="0.4659045">
Proceedings of NAACL HLT 2007, Companion Volume, pages 169–172,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999676444444444">
that bear a strong resemblance to the seed patterns.
After each iteration, the top-ranking candidate
patterns are added to the seed patterns and
removed from the corpus. Our approach differs
from that of Stevenson and Greenwood in that we
use a supervised classifier to rank candidate
patterns. This grants our system greater robustness
and flexibility because the weight of classification
features can be automatically determined within a
supervised classification approach.
In building supervised classifiers to rank
candidate patterns at each iteration, we use both
positive and negative training examples. Instead of
creating manually annotated training examples, we
follow an active learning approach where training
examples are automatically chosen by ranking
candidate patterns in terms of cosine similarity
with the seed patterns. More specifically, we
select patterns that have the lowest similarity with
seed patterns to be the negative training examples.
We hypothesized that these negative examples
would contain many of the uninformative features
occurring throughout the corpus and that using
these examples would enable the classifier to
determine that these features would not be useful.
The pattern learning approach we propose
includes the following steps.
</bodyText>
<listItem confidence="0.931016173913043">
1. An unannotated corpus is required as input.
For each sentence, a set of features is
extracted. This information becomes Scand, the
set of all candidate patterns.
2. The user defines a set of seed patterns, Sseed.
These patterns contain features expected to be
found in a relevant sentence.
3. The cosine measure is used to determine the
distance between the patterns in Sseed and Scand.
The patterns in Scand are then ordered by their
lowest distance to a member of Sseed.
4. The α highest ranked patterns in Scand are
added to Spos, the set of positive training
examples.
5. Sseed and Sacc are added to Spos. Sneg, the set of
negative training examples is constructed from
R+iteration*Y of the lowest ranked patterns in
Scand. Then, a maximum entropy classifier is
built using Spos and Sneg as training data.
6. The classifier is used to score each pattern in
Scand. Scand is then sorted by these scores.
7. The top 6 patterns in Scand are added to Sacc and
removed from Scand.
</listItem>
<bodyText confidence="0.908227928571428">
8. If a suitable stopping point has been reached,
the process ends. Otherwise, Spos and Sneg are
emptied and the process continues at step 6.
We set α to 5, R to 20, Y to 15, 6 to 5, and used the
following linguistic processing tools: (1) the
OpenNLP library (opennlp.sourceforge.net) for
sentence splitting and named-entity recognition,
and (2) Connexor for syntactic parsing
(Tapanainen and Jarvinen, 1997). For the
classifier, we used the OpenNLP MaxEnt
implementation (maxent.sourceforge.net) of the
maximum entropy classification algorithm (Berger
et al. 1996). We used the MUC-6 data set as the
testing ground for our proposed approach.
</bodyText>
<subsectionHeader confidence="0.994461">
1.2 Description of Features Used
</subsectionHeader>
<bodyText confidence="0.997003368421053">
Stevenson and Greenwood (2005) use subject-
verb-object triples for their features. We use a
richer feature set. Our system can easily
accommodate more features because we let the
maximum entropy classifier determine the weight
for the features. Stevenson&apos;s and Greenwood&apos;s
approach determines weights using semantic
similarity and would require significant changes to
take into account various other features, especially
those for which a WordNet (Fellbaum, 1998)
similarity score is not available.
We use single tokens, token combinations, and
semantic information to inform our IE pattern
extraction system. Lexical items marked by the
named-entity recognition system as PERSON or
ORGANIZATION are replaced with `person&apos; and
`organization&apos;, respectively. Number tokens are
replaced with `numeric&apos;. Single Token Features
include:
</bodyText>
<listItem confidence="0.999914545454546">
• All words in the sentence and all hypernyms of
the first sense of the word with attached part-
of-speech
• All words in the sentence with attached
dependency
• The verb base of each nominalization and the
verb&apos;s first sense hypernyms are included.
Token Combinations include:
• All bigrams from the sentence
• All subject-object pairs
• All parent-child pairs from the parse tree
</listItem>
<page confidence="0.86881">
170
</page>
<bodyText confidence="0.932485285714286">
• A specially marked copy of the parent-child
pairs where the main verb is the parent.
We also added semantic features indicating if a
PERSON or ORGANIZATION was detected
within the sentence boundaries. Table1 provides an
example where a simple sentence is mapped into
the set of features we have just described.
Understanding Conference (MUC-6) as ground
truth data set to evaluate our approach. The MUC-
6 corpus (www.ldc.upenn.edu) is composed of 100
Wall Street Journal documents written during 1993
and 1994. Our task was to detect sentences which
included management succession patterns, such as
those shown in Table 2.
</bodyText>
<table confidence="0.946899424242424">
Alan G. Spoon, 42, will succeed
Mr. Graham as president of the
company.
Single Token Features
With attached dependencies:
attr:person, subj:person, mod:numeric, v-ch:will,
main:succeed, obj:person, copred:as, pcomp:president,
mod:of, det:the, pcomp:company
With part-of-speech tags:
n:person, v:succeed, v:will, dt:the, n:company,
n:institution, n:social_group, n:group, n:organization,
n:person, n:president, n:executive, n:corporat-
e_executive, n:administrator, n:head, n:leader, n:orga-
nism, n:living_thing, n:object, n:entity, num:numeric,
abbr:person, prp:as, prp:of, v:control, v:declare,
v:decree, v:express, v:ordain, v:preside, v:state
Token Combinations
Bigrams:
person+comma, comma+numeric, numeric+comma,
comma+will, will+succeed, succeed+person, person+as,
as+president, president+of, of+the, the+company
Subject Object Pairs:
sop:person+person
Parent-Child Pairs:
pc:person+person, pc:person+numeric, pc:will+person,
pc:succeed+will, pc:succeed+person, pc:succeed+as,
pc:as+president, pc:president+of, pc:of+company,
pc:company+the
Main Verb Parent-Child Pairs:
mvpc:succeed+person, mvpc:succeed+will, mv-
pc:succeed+as
Semantic Features
hasOrganization, hasPerson
</table>
<listItem confidence="0.992186727272727">
1: subj:organization, main:appoint, obj:person, hasPers-
on, hasOrganization
2: subj:organization, main:elect, obj:person, hasOrgani-
zation, hasPerson
3: subj:organization, main:promote, obj:person, hasOrg-
anization, hasPerson
4: subj:organization, main:name, obj:person, hasOrgani-
zation, hasPerson
5: subj:person, main:resign, hasPerson
6: subj:person, main:depart, hasPerson
7: subj:person, main:quit, hasPerson
</listItem>
<tableCaption confidence="0.98202">
Table 2: Feature representation of seed patterns.
</tableCaption>
<bodyText confidence="0.998953090909091">
The version of the MUC-6 corpus produced by
Soderland (1999) provided us with a specification
of succession patterns at the sentence level, but as
shown in Table 3 did not include the source text.
We reconstructed the original text by
automatically aligning the succession patterns in
the sentence structures in Soderland&apos;s version of
the MUC-6 corpus with the sentences in the
original MUC-6 corpus. This alignment produced a
set of 1581 sentences, of which 134 contained
succession patterns.
</bodyText>
<table confidence="0.99093575">
@S[
{SUBJ @CN[ FOX ]CN }
{VB NAMED @NAM }
{OBJ @PN[ LUCILLE S. SALHANY ]PN , @PS[
CHAIRMAN ]PS OF @CN[ FOX INC. ]CN &apos;S
TELEVISION PRODUCTION ARM , }
{REL_V TO SUCCEED @SUCCEED HIM . }
]@S 9301060123-5
@@TAGS Succession {PersonIn @PN[ LUCILLE S.
SALHANY ]PN}+ {Post @PS[ CHAIRMAN ]PS}+
{Org @CN[ FOX INC. ]CN}_ @@COVERED_BY
@@ENDTAGS
</table>
<tableCaption confidence="0.996078">
Table 1: Feature representation of a simple sentence.
</tableCaption>
<bodyText confidence="0.999623">
The seeds we used are adapted from the seed
patterns employed by Stevenson and Greenwood.
As shown in Table 2, only a subset of the features
described above is used in the seed patterns.
</bodyText>
<sectionHeader confidence="0.808754" genericHeader="introduction">
2 Evaluation
</sectionHeader>
<bodyText confidence="0.971626">
We used the document collection which was
initially developed for the Sixth Message
</bodyText>
<tableCaption confidence="0.98358">
Table 3: Data sample from Soderland test set.
</tableCaption>
<bodyText confidence="0.999530833333333">
As shown in Figure 1, our best score of 0.688 F-
measure was obtained on the 36th iteration; at the
end of this iteration, our algorithm selected 180
sentences including 108 of the sentences that
contained succession patterns. This is a significant
improvement over the 0.58 F-measure score
</bodyText>
<page confidence="0.996092">
171
</page>
<bodyText confidence="0.996749666666667">
reported by Stevenson and Greenwood (2005) for
the same task. The use of a supervised
classification approach to the ranking of candidate
patterns with a richer feature set were the two
determinant factors in achieving such
improvement.
</bodyText>
<figure confidence="0.998467454545454">
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 10 20 30 40 50 60 70 80
Iteration
</figure>
<figureCaption confidence="0.999827">
Figure 1: Evaluation results with MUC-6 data.
</figureCaption>
<sectionHeader confidence="0.999356" genericHeader="conclusions">
3 Conclusions
</sectionHeader>
<bodyText confidence="0.999977">
Our results show a substantial improvement over
previous efforts in weakly supervised IE methods,
suggesting that weakly supervised methods can be
made to rival rule-based or fully supervised
approaches both in resource effectiveness and
accuracy. We plan to verify the strength of our
approach evaluating against other ground truth data
sets. We also plan to detail how the various
features in our classification model contribute to
ranking of candidate patterns. An additional area
of envisioned improvement regards the use of a
random sub selection of negative candidate
patterns as training samples to counteract the
presence of sentence fragments among low-
ranking candidate patterns. Finally, we intend to
evaluate the benefit of having a human in the loop
in the first few iterations to filter out patterns
chosen by the system.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988443315789474">
C. Aone and M. Ramos-Santacruz. 2000. REES: A
Large-Scale Relation and Event Extraction System,
pages 76-83, In Proceedings of the 6th Applied
Natural Language Processing Conference (ANLP
2000), Seattle.
M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open Information Extraction
from the Web. In Proceedings of the 20th
International Joint Conference on Artificial
Intelligence (IJCAI 2007). Hyderabad, India.
A. Berger, S. Della Pietra and V. Della Pietra (1996) A
Maximum Entropy Approach to Natural Language
Processing. Computational Linguistics, volume 22,
number 1, pages 39-71.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database and some of its Applications. MIT
Press, Cambridge, MA.
J. Jiang and D. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of the 10th International Conference on
Research in Computational Linguistics, Taiwan.
S. Patwardhan, S. Banerjee, and T. Pederson. 2003.
Using measures of semantic relatedness for word
sense disambiguation. In Proceedings of the 4th
International Conferences on Intelligent Text
Processing and Computational Linguistics, pages
241-257, Mexico City.
P. Resnik. 1995. Using Information Content to evaluate
Semantic Similarity in a Taxonomy. In Proceedings
of the 14th International Joint Conference on
Artificial Intelligence (IJCAI-95), pages 448-452,
Montreal, Canada.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-level Bootstrapping.
In Proceedings of the 16th National Conference on
Artificial Intelligence. Orlando, Florida.
S. Sekine. 2006. On-Demand Information Extraction. In
Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions. Sydney, Australia.
S. Soderland. 1999. Learning Information Extraction
Rules for Semi-structured and free text. Machine
Learning, 31(1-3):233-272.
M. Stevenson and M. A. Greenwood. 2005. A Semantic
Approach to IE Pattern Induction. In Proceedings of
the 43rd Annual Meeting of the ACL (ACL 05), Ann
Arbor, Michigan.
P. Tapanainen and Timo Järvinen. 1997. A non-
projective dependency parser. In Proceedings of the
5th Conference on Applied Natural Language
Processing, pages 64–71, Washington D.C.
Association for Computational Linguistics.
R. Yangarber, R. Grishman, P. Tapanainen, and S.
Huttunen. 2000. Automatic acquisition of domain
knowledge for information extraction. In
Proceedings of the 18th International Conference of
Computational Linguistics (COLING 2002), Taipei.
Fineasure
</reference>
<page confidence="0.989088">
172
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931446">
<title confidence="0.999954">A High Accuracy Method for Semi-supervised Information Extraction</title>
<author confidence="0.999586">Stephen Tratz Antonio Sanfilippo</author>
<affiliation confidence="0.999803">Pacific Northwest National Laboratory Pacific Northwest National Laboratory</affiliation>
<address confidence="0.999918">Richland, WA 99352 Richland, WA 99352</address>
<email confidence="0.939232">stephen.tratz@pnl.govantonio.sanfilippo@pnl.gov</email>
<abstract confidence="0.999524944444445">Customization to specific domains of discourse and/or user requirements is one of the greatest challenges for today&apos;s Information Extraction (IE) systems. While demonstrably effective, both rule-based and supervised machine learning approaches to IE customization pose too high a burden on the user. Semisupervised learning approaches may in principle offer a more resource effective solution but are still insufficiently accurate to grant realistic application. We demonstrate that this limitation can be overcome by integrating fully-supervised learning techniques within a semisupervised IE approach, without increasing resource requirements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>M Ramos-Santacruz</author>
</authors>
<title>REES: A Large-Scale Relation and Event Extraction System,</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied Natural Language Processing Conference (ANLP 2000),</booktitle>
<pages>76--83</pages>
<location>Seattle.</location>
<contexts>
<context position="1278" citStr="Aone and Ramos-Santacruz (2000)" startWordPosition="162" endWordPosition="165">fective solution but are still insufficiently accurate to grant realistic application. We demonstrate that this limitation can be overcome by integrating fully-supervised learning techniques within a semisupervised IE approach, without increasing resource requirements. 1 Introduction Customization to specific discourse domains and/or user requirements is one of the greatest challenges for today&apos;s Information Extraction (IE) systems. While demonstrably effective, both rulebased and supervised machine learning approaches to IE customization require a substantial development effort. For example, Aone and Ramos-Santacruz (2000) present a rule-based IE system which handles 100 types of relations and events. Building such a system requires the manual construction of numerous extraction patterns supported by customized ontologies. Soderland (1999) uses supervised learning to induce a set of rules from hand-tagged training examples. While Sonderland suggests that the human effort can be reduced by interleaving learning and manual annotation activities, the creation of training data remains an onerous task. To reduce the knowledge engineering burden on the user in constructing and porting an IE system, unsupervised learn</context>
</contexts>
<marker>Aone, Ramos-Santacruz, 2000</marker>
<rawString>C. Aone and M. Ramos-Santacruz. 2000. REES: A Large-Scale Relation and Event Extraction System, pages 76-83, In Proceedings of the 6th Applied Natural Language Processing Conference (ANLP 2000), Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M J Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open Information Extraction from the Web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<location>Hyderabad, India.</location>
<contexts>
<context position="1984" citStr="Banko et al. (2007)" startWordPosition="267" endWordPosition="270">ng such a system requires the manual construction of numerous extraction patterns supported by customized ontologies. Soderland (1999) uses supervised learning to induce a set of rules from hand-tagged training examples. While Sonderland suggests that the human effort can be reduced by interleaving learning and manual annotation activities, the creation of training data remains an onerous task. To reduce the knowledge engineering burden on the user in constructing and porting an IE system, unsupervised learning has been utilized, e.g. Riloff (1996), Yangarber et al. (2000), and Sekine (2006). Banko et al. (2007) present a self-supervised system that aims to avoid the manual IE customization problem by extracting all possible relations of interest from text. Stevenson and Greenwood (2005) propose a weakly supervised approach to sentence filtering that uses semantic similarity and bootstrapping to acquire IE patterns. Stevenson&apos;s and Greenwood&apos;s approach provides some of the best available results in weakly supervised IE to date, with 0.58 F-measure. While very good, an F-measure of 0.58 does not provide sufficient reliability to grant use in a production system. In this paper, we show that it is possi</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open Information Extraction from the Web. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI 2007). Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<pages>39--71</pages>
<contexts>
<context position="6232" citStr="Berger et al. 1996" startWordPosition="919" endWordPosition="922">ns in Scand are added to Sacc and removed from Scand. 8. If a suitable stopping point has been reached, the process ends. Otherwise, Spos and Sneg are emptied and the process continues at step 6. We set α to 5, R to 20, Y to 15, 6 to 5, and used the following linguistic processing tools: (1) the OpenNLP library (opennlp.sourceforge.net) for sentence splitting and named-entity recognition, and (2) Connexor for syntactic parsing (Tapanainen and Jarvinen, 1997). For the classifier, we used the OpenNLP MaxEnt implementation (maxent.sourceforge.net) of the maximum entropy classification algorithm (Berger et al. 1996). We used the MUC-6 data set as the testing ground for our proposed approach. 1.2 Description of Features Used Stevenson and Greenwood (2005) use subjectverb-object triples for their features. We use a richer feature set. Our system can easily accommodate more features because we let the maximum entropy classifier determine the weight for the features. Stevenson&apos;s and Greenwood&apos;s approach determines weights using semantic similarity and would require significant changes to take into account various other features, especially those for which a WordNet (Fellbaum, 1998) similarity score is not av</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. Della Pietra and V. Della Pietra (1996) A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, volume 22, number 1, pages 39-71.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database and some of its Applications.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database and some of its Applications. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>D Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the 10th International Conference on Research in Computational Linguistics,</booktitle>
<contexts>
<context position="3183" citStr="Jiang and Conrath, 1997" startWordPosition="443" endWordPosition="446">we show that it is possible to provide a significant improvement over Stevenson&apos;s and Greenwood&apos;s results, without increasing resource requirements, by integrating fully-supervised learning techniques within a weakly supervised IE approach. 1.1 Learning Algorithm Our method is modeled on the approach developed by Stevenson and Greenwood (2005) but uses a different technique for ranking candidate patterns. Stevenson&apos;s and Greenwood&apos;s algorithm takes as data inputs a small set of initial seed patterns and a corpus of documents, and uses any of several semantic similarity measures (Resnik, 1995; Jiang and Conrath, 1997; Patwardhan et al., 2003) to iteratively identify patterns in the document corpus 169 Proceedings of NAACL HLT 2007, Companion Volume, pages 169–172, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics that bear a strong resemblance to the seed patterns. After each iteration, the top-ranking candidate patterns are added to the seed patterns and removed from the corpus. Our approach differs from that of Stevenson and Greenwood in that we use a supervised classifier to rank candidate patterns. This grants our system greater robustness and flexibility because the weight o</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J. Jiang and D. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the 10th International Conference on Research in Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>S Banerjee</author>
<author>T Pederson</author>
</authors>
<title>Using measures of semantic relatedness for word sense disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th International Conferences on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>241--257</pages>
<location>Mexico City.</location>
<contexts>
<context position="3209" citStr="Patwardhan et al., 2003" startWordPosition="447" endWordPosition="450">le to provide a significant improvement over Stevenson&apos;s and Greenwood&apos;s results, without increasing resource requirements, by integrating fully-supervised learning techniques within a weakly supervised IE approach. 1.1 Learning Algorithm Our method is modeled on the approach developed by Stevenson and Greenwood (2005) but uses a different technique for ranking candidate patterns. Stevenson&apos;s and Greenwood&apos;s algorithm takes as data inputs a small set of initial seed patterns and a corpus of documents, and uses any of several semantic similarity measures (Resnik, 1995; Jiang and Conrath, 1997; Patwardhan et al., 2003) to iteratively identify patterns in the document corpus 169 Proceedings of NAACL HLT 2007, Companion Volume, pages 169–172, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics that bear a strong resemblance to the seed patterns. After each iteration, the top-ranking candidate patterns are added to the seed patterns and removed from the corpus. Our approach differs from that of Stevenson and Greenwood in that we use a supervised classifier to rank candidate patterns. This grants our system greater robustness and flexibility because the weight of classification features </context>
</contexts>
<marker>Patwardhan, Banerjee, Pederson, 2003</marker>
<rawString>S. Patwardhan, S. Banerjee, and T. Pederson. 2003. Using measures of semantic relatedness for word sense disambiguation. In Proceedings of the 4th International Conferences on Intelligent Text Processing and Computational Linguistics, pages 241-257, Mexico City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using Information Content to evaluate Semantic Similarity in a Taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95),</booktitle>
<pages>448--452</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="3158" citStr="Resnik, 1995" startWordPosition="441" endWordPosition="442">n this paper, we show that it is possible to provide a significant improvement over Stevenson&apos;s and Greenwood&apos;s results, without increasing resource requirements, by integrating fully-supervised learning techniques within a weakly supervised IE approach. 1.1 Learning Algorithm Our method is modeled on the approach developed by Stevenson and Greenwood (2005) but uses a different technique for ranking candidate patterns. Stevenson&apos;s and Greenwood&apos;s algorithm takes as data inputs a small set of initial seed patterns and a corpus of documents, and uses any of several semantic similarity measures (Resnik, 1995; Jiang and Conrath, 1997; Patwardhan et al., 2003) to iteratively identify patterns in the document corpus 169 Proceedings of NAACL HLT 2007, Companion Volume, pages 169–172, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics that bear a strong resemblance to the seed patterns. After each iteration, the top-ranking candidate patterns are added to the seed patterns and removed from the corpus. Our approach differs from that of Stevenson and Greenwood in that we use a supervised classifier to rank candidate patterns. This grants our system greater robustness and flexibi</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using Information Content to evaluate Semantic Similarity in a Taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95), pages 448-452, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-level Bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th National Conference on Artificial Intelligence.</booktitle>
<location>Orlando, Florida.</location>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning Dictionaries for Information Extraction by Multi-level Bootstrapping. In Proceedings of the 16th National Conference on Artificial Intelligence. Orlando, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
</authors>
<title>On-Demand Information Extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions.</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="1963" citStr="Sekine (2006)" startWordPosition="265" endWordPosition="266"> events. Building such a system requires the manual construction of numerous extraction patterns supported by customized ontologies. Soderland (1999) uses supervised learning to induce a set of rules from hand-tagged training examples. While Sonderland suggests that the human effort can be reduced by interleaving learning and manual annotation activities, the creation of training data remains an onerous task. To reduce the knowledge engineering burden on the user in constructing and porting an IE system, unsupervised learning has been utilized, e.g. Riloff (1996), Yangarber et al. (2000), and Sekine (2006). Banko et al. (2007) present a self-supervised system that aims to avoid the manual IE customization problem by extracting all possible relations of interest from text. Stevenson and Greenwood (2005) propose a weakly supervised approach to sentence filtering that uses semantic similarity and bootstrapping to acquire IE patterns. Stevenson&apos;s and Greenwood&apos;s approach provides some of the best available results in weakly supervised IE to date, with 0.58 F-measure. While very good, an F-measure of 0.58 does not provide sufficient reliability to grant use in a production system. In this paper, we </context>
</contexts>
<marker>Sekine, 2006</marker>
<rawString>S. Sekine. 2006. On-Demand Information Extraction. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Soderland</author>
</authors>
<title>Learning Information Extraction Rules for Semi-structured and free text.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>31--1</pages>
<contexts>
<context position="1499" citStr="Soderland (1999)" startWordPosition="195" endWordPosition="196">increasing resource requirements. 1 Introduction Customization to specific discourse domains and/or user requirements is one of the greatest challenges for today&apos;s Information Extraction (IE) systems. While demonstrably effective, both rulebased and supervised machine learning approaches to IE customization require a substantial development effort. For example, Aone and Ramos-Santacruz (2000) present a rule-based IE system which handles 100 types of relations and events. Building such a system requires the manual construction of numerous extraction patterns supported by customized ontologies. Soderland (1999) uses supervised learning to induce a set of rules from hand-tagged training examples. While Sonderland suggests that the human effort can be reduced by interleaving learning and manual annotation activities, the creation of training data remains an onerous task. To reduce the knowledge engineering burden on the user in constructing and porting an IE system, unsupervised learning has been utilized, e.g. Riloff (1996), Yangarber et al. (2000), and Sekine (2006). Banko et al. (2007) present a self-supervised system that aims to avoid the manual IE customization problem by extracting all possible</context>
<context position="9876" citStr="Soderland (1999)" startWordPosition="1387" endWordPosition="1388">succeed+person, mvpc:succeed+will, mvpc:succeed+as Semantic Features hasOrganization, hasPerson 1: subj:organization, main:appoint, obj:person, hasPerson, hasOrganization 2: subj:organization, main:elect, obj:person, hasOrganization, hasPerson 3: subj:organization, main:promote, obj:person, hasOrganization, hasPerson 4: subj:organization, main:name, obj:person, hasOrganization, hasPerson 5: subj:person, main:resign, hasPerson 6: subj:person, main:depart, hasPerson 7: subj:person, main:quit, hasPerson Table 2: Feature representation of seed patterns. The version of the MUC-6 corpus produced by Soderland (1999) provided us with a specification of succession patterns at the sentence level, but as shown in Table 3 did not include the source text. We reconstructed the original text by automatically aligning the succession patterns in the sentence structures in Soderland&apos;s version of the MUC-6 corpus with the sentences in the original MUC-6 corpus. This alignment produced a set of 1581 sentences, of which 134 contained succession patterns. @S[ {SUBJ @CN[ FOX ]CN } {VB NAMED @NAM } {OBJ @PN[ LUCILLE S. SALHANY ]PN , @PS[ CHAIRMAN ]PS OF @CN[ FOX INC. ]CN &apos;S TELEVISION PRODUCTION ARM , } {REL_V TO SUCCEED</context>
</contexts>
<marker>Soderland, 1999</marker>
<rawString>S. Soderland. 1999. Learning Information Extraction Rules for Semi-structured and free text. Machine Learning, 31(1-3):233-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>M A Greenwood</author>
</authors>
<title>A Semantic Approach to IE Pattern Induction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL (ACL 05),</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="2163" citStr="Stevenson and Greenwood (2005)" startWordPosition="293" endWordPosition="296">e a set of rules from hand-tagged training examples. While Sonderland suggests that the human effort can be reduced by interleaving learning and manual annotation activities, the creation of training data remains an onerous task. To reduce the knowledge engineering burden on the user in constructing and porting an IE system, unsupervised learning has been utilized, e.g. Riloff (1996), Yangarber et al. (2000), and Sekine (2006). Banko et al. (2007) present a self-supervised system that aims to avoid the manual IE customization problem by extracting all possible relations of interest from text. Stevenson and Greenwood (2005) propose a weakly supervised approach to sentence filtering that uses semantic similarity and bootstrapping to acquire IE patterns. Stevenson&apos;s and Greenwood&apos;s approach provides some of the best available results in weakly supervised IE to date, with 0.58 F-measure. While very good, an F-measure of 0.58 does not provide sufficient reliability to grant use in a production system. In this paper, we show that it is possible to provide a significant improvement over Stevenson&apos;s and Greenwood&apos;s results, without increasing resource requirements, by integrating fully-supervised learning techniques wi</context>
<context position="6373" citStr="Stevenson and Greenwood (2005)" startWordPosition="942" endWordPosition="945">e, Spos and Sneg are emptied and the process continues at step 6. We set α to 5, R to 20, Y to 15, 6 to 5, and used the following linguistic processing tools: (1) the OpenNLP library (opennlp.sourceforge.net) for sentence splitting and named-entity recognition, and (2) Connexor for syntactic parsing (Tapanainen and Jarvinen, 1997). For the classifier, we used the OpenNLP MaxEnt implementation (maxent.sourceforge.net) of the maximum entropy classification algorithm (Berger et al. 1996). We used the MUC-6 data set as the testing ground for our proposed approach. 1.2 Description of Features Used Stevenson and Greenwood (2005) use subjectverb-object triples for their features. We use a richer feature set. Our system can easily accommodate more features because we let the maximum entropy classifier determine the weight for the features. Stevenson&apos;s and Greenwood&apos;s approach determines weights using semantic similarity and would require significant changes to take into account various other features, especially those for which a WordNet (Fellbaum, 1998) similarity score is not available. We use single tokens, token combinations, and semantic information to inform our IE pattern extraction system. Lexical items marked </context>
<context position="11362" citStr="Stevenson and Greenwood (2005)" startWordPosition="1632" endWordPosition="1635">the seed patterns employed by Stevenson and Greenwood. As shown in Table 2, only a subset of the features described above is used in the seed patterns. 2 Evaluation We used the document collection which was initially developed for the Sixth Message Table 3: Data sample from Soderland test set. As shown in Figure 1, our best score of 0.688 Fmeasure was obtained on the 36th iteration; at the end of this iteration, our algorithm selected 180 sentences including 108 of the sentences that contained succession patterns. This is a significant improvement over the 0.58 F-measure score 171 reported by Stevenson and Greenwood (2005) for the same task. The use of a supervised classification approach to the ranking of candidate patterns with a richer feature set were the two determinant factors in achieving such improvement. 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 10 20 30 40 50 60 70 80 Iteration Figure 1: Evaluation results with MUC-6 data. 3 Conclusions Our results show a substantial improvement over previous efforts in weakly supervised IE methods, suggesting that weakly supervised methods can be made to rival rule-based or fully supervised approaches both in resource effectiveness and accuracy. We plan to verify the stren</context>
</contexts>
<marker>Stevenson, Greenwood, 2005</marker>
<rawString>M. Stevenson and M. A. Greenwood. 2005. A Semantic Approach to IE Pattern Induction. In Proceedings of the 43rd Annual Meeting of the ACL (ACL 05), Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Tapanainen</author>
<author>Timo Järvinen</author>
</authors>
<title>A nonprojective dependency parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<pages>64--71</pages>
<institution>D.C. Association for Computational Linguistics.</institution>
<location>Washington</location>
<marker>Tapanainen, Järvinen, 1997</marker>
<rawString>P. Tapanainen and Timo Järvinen. 1997. A nonprojective dependency parser. In Proceedings of the 5th Conference on Applied Natural Language Processing, pages 64–71, Washington D.C. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yangarber</author>
<author>R Grishman</author>
<author>P Tapanainen</author>
<author>S Huttunen</author>
</authors>
<title>Automatic acquisition of domain knowledge for information extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference of Computational Linguistics (COLING 2002),</booktitle>
<location>Taipei.</location>
<contexts>
<context position="1944" citStr="Yangarber et al. (2000)" startWordPosition="260" endWordPosition="263">es 100 types of relations and events. Building such a system requires the manual construction of numerous extraction patterns supported by customized ontologies. Soderland (1999) uses supervised learning to induce a set of rules from hand-tagged training examples. While Sonderland suggests that the human effort can be reduced by interleaving learning and manual annotation activities, the creation of training data remains an onerous task. To reduce the knowledge engineering burden on the user in constructing and porting an IE system, unsupervised learning has been utilized, e.g. Riloff (1996), Yangarber et al. (2000), and Sekine (2006). Banko et al. (2007) present a self-supervised system that aims to avoid the manual IE customization problem by extracting all possible relations of interest from text. Stevenson and Greenwood (2005) propose a weakly supervised approach to sentence filtering that uses semantic similarity and bootstrapping to acquire IE patterns. Stevenson&apos;s and Greenwood&apos;s approach provides some of the best available results in weakly supervised IE to date, with 0.58 F-measure. While very good, an F-measure of 0.58 does not provide sufficient reliability to grant use in a production system.</context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, Huttunen, 2000</marker>
<rawString>R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen. 2000. Automatic acquisition of domain knowledge for information extraction. In Proceedings of the 18th International Conference of Computational Linguistics (COLING 2002), Taipei.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>