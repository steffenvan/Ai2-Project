<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014268">
<title confidence="0.994359">
UMDuluth-BlueTeam : SVCSTS - A Multilingual and Chunk Level
Semantic Similarity System
</title>
<author confidence="0.821696">
Sakethram Karumuri
Viswanadh Kumar Reddy Vuggumudi
Sai Charan Raj Chitirala
</author>
<affiliation confidence="0.999568">
Department of Computer Science
University of Minnesota Duluth
</affiliation>
<email confidence="0.995731">
fkarum006, vuggu001, chiti0011@d.umn.edu
</email>
<sectionHeader confidence="0.995587" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997693181818182">
This paper describes SVCSTS, a system that
was submitted in SemEval-2015 Task 2: Se-
mantic Textual Similarity(STS)(Agirre et al.,
2015). The task has 3 subtasks viz., English
STS, Spanish STS and Interpretable STS.
SVCSTS uses Monolingual word aligner (Sul-
tan et al., May 2014), supervised machine
learning, Google and Bing translator API’s.
Various runs of the system outperformed all
other participating systems in Interpretable
STS for non-chunked sentence input.
</bodyText>
<sectionHeader confidence="0.998947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998185176470588">
Semantic Textual Similarity gives a quantifier to
evaluate semantic equivalence between two sen-
tences. Earlier SemEval tasks (Agirre et al., 2012),
(Agirre et al., 2013), (Agirre et al., 2014) focused on
finding the semantic equivalence between sentences
in English and Spanish. A new pilot task was intro-
duced this year to find which parts (chunks) of the
sentences are equivalent in meaning.
SVCSTS is an extension to (Sultan et al., 2014)
and it handles both Spanish STS and Interpretable
STS. SVCSTS uses Monolingual word aligner (Sul-
tan et al., May 2014), supervised machine learning
techniques, Google and Bing translator API’s.
Section 2 describes a brief overview of SVCSTS’s
approach for various subtasks. Section 3 outlines
the performance of SVCSTS in various subtasks of
SemEval 2015 Task-2.
</bodyText>
<sectionHeader confidence="0.991418" genericHeader="introduction">
2 System Description
</sectionHeader>
<bodyText confidence="0.9997925">
Following 3 sub sections describe SVCSTS’s ap-
proach for the 3 subtasks.
</bodyText>
<subsectionHeader confidence="0.959844">
2.1 English STS
</subsectionHeader>
<bodyText confidence="0.9999478">
This task was about finding the semantic similarity
between English sentences. (Sultan et al., 2014) sys-
tem was used to find the semantic equivalence be-
tween two sentences and a score on a scale of 0-5
was given.
</bodyText>
<subsectionHeader confidence="0.999212">
2.2 Spanish STS
</subsectionHeader>
<bodyText confidence="0.999938909090909">
Spanish STS is built upon English STS to calculate
similarity scores for a given pair of Spanish sen-
tences on a scale of 0 to 4. Spanish sentences were
translated to English, fed to English STS system
and the scores are scaled accordingly. Translations
were done using Bing Translator API (Bing Transla-
tor API) and Google Translate API. Two translators
were used to improve the accuracy of the transla-
tions.
Google Translate API was obtained from
(Kashyap et al., 2014). We used this system to get
multiple translations of each chunk in a sentence.
Multiple sentences are generated by combining
the top two translations of each chunk. We then
randomly pick a maximum of ten sentences for
each Spanish sentence. Translation pairs are formed
by choosing corresponding numbered sentences
from sentence 1 and sentence 2 translations. We
limited the number of translations to 10 to reduce
the overall computation time.
Translation pairs were then passed to English STS
system. Final score was obtained as the average
</bodyText>
<page confidence="0.995758">
107
</page>
<bodyText confidence="0.651409">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 107–110,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
taken from all translation pairs for a given Spanish
sentence pair and the score is scaled accordingly.
</bodyText>
<subsectionHeader confidence="0.994117">
2.3 Interpretable STS
</subsectionHeader>
<bodyText confidence="0.9988015">
Existing STS systems report similarity for a pair of
sentences.
This is a pilot task where the challenge is to find
the semantic relationships between the chunks of
sentence 1 and sentence 2. Chunks from the input
sentence pair are to be aligned, labeled with the type
(described here) of alignment and are to be scored
on a scale of 0-5 based on their semantic similarity.
The type of alignments defined in the task de-
scription are:
</bodyText>
<listItem confidence="0.997150428571429">
1. EQUI : both chunks are semantically similar.
2. OPPO : both chunks are semantically opposite.
3. SPE1 : both chunks are semantically similar
but chunk1 has more information.
4. SPE2 : both chunks are semantically similar
but chunk2 has more information.
5. SIMI : similar chunks but no EQUI, OPPO,
SPE1 or SPE2.
6. REL : related chunks but no SIMI, EQUI,
OPPO, SPE1, SPE2.
7. ALIC : when 1:1 alignment of chunks is not
possible extra chunks are given ALIC
8. NOALI: a chunk has no corresponding seman-
tically similar chunk
</listItem>
<bodyText confidence="0.8202725">
There are two variations in the input for this sub-
task:
</bodyText>
<listItem confidence="0.998802">
1. Raw input - Plain sentences are provided and
the system has to identify the chunks
2. Chunked input - Chunked sentences are pro-
vided by the task organizers
</listItem>
<subsectionHeader confidence="0.814073">
2.3.1 Identifying Chunks
</subsectionHeader>
<bodyText confidence="0.981849875">
OpenNLP chunker was used to chunk the in-
put sentences and some post processing was done.
For the post processing we observed a few rules
from gold standard chunks. Those rules include
combining chunks of specific chunk tags given by
OpenNLP chunker. A large number of rules were
discovered but the following were the rules, which
maximized accuracy.
</bodyText>
<listItem confidence="0.999932">
• PP+NP+PP+NP
• PP + NP
• VP + PRT
• NP+O+NP
• VP + ADVP
• VP+PP+NP+O
• NP+O
</listItem>
<bodyText confidence="0.995793333333333">
Applying these rules we have increased accuracy
from 86.58% to 90.16% against the gold standard
chunks.
</bodyText>
<subsectionHeader confidence="0.838164">
2.3.2 Aligning Chunks
</subsectionHeader>
<bodyText confidence="0.999596666666667">
Monolingual word aligner (Sultan et al., May
2014) was used to find word alignments in the
two input sentences. For chunked input, sentences
are generated from the chunks prior to running the
word aligner. For words aligned their corresponding
chunks are aligned.
</bodyText>
<subsectionHeader confidence="0.953118">
2.3.3 Labeling Aligned Chunks
</subsectionHeader>
<bodyText confidence="0.99922625">
Supervised machine learning was performed us-
ing Scikit-Learn (scikit-learn). We used the follow-
ing features for each chunk alignment to assign a
type for the alignment.
</bodyText>
<listItem confidence="0.9980049">
1. Length of sentence 1 chunk
2. Length of sentence 2 chunk
3. Number of nouns in sentence 1 chunk
4. Number of nouns in sentence 2 chunk
5. Number of verbs in sentence 1 chunk
6. Number of verbs in sentence 2 chunk
7. Number of adjectives in sentence 1 chunk
8. Number of adjectives in sentence 2 chunk
9. Number of prepositions in sentence 1 chunk
10. Number of prepositions in sentence 2 chunk
</listItem>
<page confidence="0.97086">
108
</page>
<table confidence="0.999930294117647">
Type of Alignment Score
EQUI 5
SPE1 3.75
SPE2 3.55
ALIC NIL
NOALI 0
SIMI 2.94
REL 2.82
OPPO 4
Inputs Baseline SVCSTS
answers-forums 0.4453 0.6561
answers-students 0.6647 0.7816
belief 0.6517 0.7363
headlines 0.5312 0.8085
images 0.6039 0.8236
Mean 0.5871 0.7775
Rank 59 14
</table>
<tableCaption confidence="0.9999075">
Table 3: Scores for English STS
Table 1: Avg. alignment type scores
</tableCaption>
<table confidence="0.99939975">
Runs Features Used
Run - 1 3,4,5,6,7,8,9,10,11,12
Run - 2 3,4,5,6,7,8,9,10,11,12,13
Run - 3 1,2,3,4,5,6,7,8,9,10,11,12,13
</table>
<tableCaption confidence="0.992821">
Table 2: Features used in various runs
</tableCaption>
<bodyText confidence="0.72047775">
11. The path similarity between words of sentence
1 and sentence 2 chunks
12. Unigram overlap between sentence 1 and sen-
tence 2 chunks
13. Bigram overlap between sentence 1 and sen-
tence 2 chunks
We experimented the classification of labels us-
ing 3 classifiers LinearSVC, SVC with RBF (Radial
Basis Function) Kernel and SVC with Polynomial
Kernel. But the classifier SVC with RBF (with pa-
rameters C = 1.0, gamma=0.7) proved to give better
results.
</bodyText>
<subsectionHeader confidence="0.893027">
2.3.4 Scoring Aligned Chunks
</subsectionHeader>
<bodyText confidence="0.9999625">
Average score for each alignment type was cal-
culated from the gold standard data. The average
scores that were used to score chunk alignment are
described in Table 1.
</bodyText>
<subsectionHeader confidence="0.899699">
2.3.5 Multiple Runs
</subsectionHeader>
<bodyText confidence="0.999937">
We tried various combination of features (de-
scribed in Section 2.3.3) for training the classifier.
The details of three runs that resulted in better accu-
racy on training data are described in Table 2.
</bodyText>
<sectionHeader confidence="0.99987" genericHeader="background">
3 Results
</sectionHeader>
<bodyText confidence="0.9382375">
The results of all the subtracks were very encour-
aging. For English STS, the results are outlined in
</bodyText>
<table confidence="0.9923226">
Inputs SVCSTS
Wikipedia 0.59364
Newswire 0.65471
Mean 0.63430
Rank 4
</table>
<tableCaption confidence="0.936981">
Table 4: Scores for Spanish STS
Table 3. SVCSTS was ranked 14th among 73 runs.
</tableCaption>
<bodyText confidence="0.9870825">
The results of Spanish STS are shown in Table 4.
We were ranked 4th among 16 runs. Table 5 and Ta-
ble 6 summarize the results of Interpretable STS for
chunked and non-chunked input respectively. Runs
2 and 3 seemed to outperform many other participat-
ing systems for non-chunked sentence input.
</bodyText>
<sectionHeader confidence="0.987301" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.975773">
We thank Dr. Ted Pedersen for introducing us to
SemEval shared tasks.
</bodyText>
<table confidence="0.998056636363636">
Inputs Baseline SVCSTS
For Headlines - Run 2
F1 Ali 0.6701 0.7820
F1 Type 0.4571 0.5154
F1 Score 0.6066 0.7024
F1 Type+Score 0.4571 0.5098
For Images - Run 3
F1 Ali 0.7060 0.8336
F1 Type 0.3696 0.5759
F1 Score 0.6092 0.7511
F1 Type+Score 0.3693 0.5634
</table>
<tableCaption confidence="0.962192">
Table 5: Scores for Interpretable STS (Chunked Input)
</tableCaption>
<page confidence="0.91701">
109
</page>
<table confidence="0.999858636363636">
Inputs Baseline SVCSTS
For Headlines - Run 1
F1 Ali 0.8448 0.8861
F1 Type 0.5556 0.5962
F1 Score 0.7551 0.7960
F1 Type+Score 0.5556 0.5887
For Images - Run 2
F1 Ali 0.8388 0.8853
F1 Type 0.4328 0.6095
F1 Score 0.7210 0.7968
F1 Type+Score 0.4326 0.5964
</table>
<tableCaption confidence="0.935256">
Table 6: Scores for Interpretable STS (Raw Input)
</tableCaption>
<sectionHeader confidence="0.995741" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99817646969697">
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer,
Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Iigo
Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea,
German Rigau, Larraitz Uria, and Janyce Wiebe.
SemEval-2015 Task 2: Semantic Textual Similarity,
English, Spanish and Pilot on Interpretability. In
Proceedings of the 9th International Workshop on
Semantic Evaluation (SemEval 2015), Denver, CO,
June 2015. Association for Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. Semeval-2012 task 6: A pilot on
semantic textual similarity. In *SEM 2012: The
First Joint Conference on Lexical and Computational
Semantics – Volume 1: Proceedings of the main
conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 385–393,
Montr´eal, Canada, 7-8 June 2012. Association for
Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. *sem 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Tex-
tual Similarity, pages 32–43, Atlanta, Georgia, USA,
June 2013. Association for Computational Linguistics.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer,
Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
Rada Mihalcea, German Rigau, and Janyce Wiebe.
Semeval-2014 task 10: Multilingual semantic textual
similarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval 2014),
pages 81–91, Dublin, Ireland, August 2014. Associ-
ation for Computational Linguistics and Dublin City
University.
Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer
Sleeman, Taneeya Satyapanich, Sunil Gandhi,
and Tim Finin. Meerkat mafia: Multilingual and
cross-level semantic textual similarity systems. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval 2014), pages 416–
423, Dublin, Ireland, August 2014. Association for
Computational Linguistics and Dublin City University.
Md. Arafat Sultan, Steven Bethard, and Tamara Sumner.
Dls@cu: Sentence similarity from word alignment.
In Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval 2014), pages 241–246,
Dublin, Ireland, 2014. Association for Computational
Linguistics and Dublin City University. Winner of the
shared task.
Md. Arafat Sultan and Steven Bethard and Tamara
Sumner Back to Basics for Monolingual Alignment:
Exploiting Word Similarity and Contextual Evidence
Transactions of the Association for Computational
Linguistics, Vol. 2, (May), pages 219–230.
Pedregosa, F. and Varoquaux, G. and Gramfort, A.
and Michel, V. and Thirion, B. and Grisel, O. and
Blondel, M. and Prettenhofer, P. and Weiss, R. and
Dubourg, V. and Vanderplas, J. and Passos, A. and
Cournapeau, D. and Brucher, M. and Perrot, M. and
Duchesnay, E. Scikit-learn: Machine Learning in
Python Journal of Machine Learning Research, Vol
12, pages 2825–2830 2011
https://github.com/openlabs/Microsoft-Translator-
Python-API
</reference>
<page confidence="0.997731">
110
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.544820">
<title confidence="0.912159666666667">UMDuluth-BlueTeam : SVCSTS - A Multilingual and Chunk Semantic Similarity System Sakethram</title>
<author confidence="0.9420215">Viswanadh Kumar Reddy Sai Charan Raj</author>
<affiliation confidence="0.99996">Department of Computer University of Minnesota</affiliation>
<email confidence="0.994271">vuggu001,chiti0011@d.umn.edu</email>
<abstract confidence="0.989012">This paper describes SVCSTS, a system that was submitted in SemEval-2015 Task 2: Semantic Textual Similarity(STS)(Agirre et al., 2015). The task has 3 subtasks viz., English STS, Spanish STS and Interpretable STS. SVCSTS uses Monolingual word aligner (Sultan et al., May 2014), supervised machine learning, Google and Bing translator API’s. Various runs of the system outperformed all other participating systems in Interpretable STS for non-chunked sentence input.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
</authors>
<title>Aitor Gonzalez-Agirre, Weiwei Guo, Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe.</title>
<date>2015</date>
<booktitle>SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<location>Denver, CO,</location>
<marker>Agirre, Banea, Cardie, Cer, Diab, 2015</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, CO, June 2015. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>385--393</pages>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="902" citStr="Agirre et al., 2012" startWordPosition="122" endWordPosition="125">per describes SVCSTS, a system that was submitted in SemEval-2015 Task 2: Semantic Textual Similarity(STS)(Agirre et al., 2015). The task has 3 subtasks viz., English STS, Spanish STS and Interpretable STS. SVCSTS uses Monolingual word aligner (Sultan et al., May 2014), supervised machine learning, Google and Bing translator API’s. Various runs of the system outperformed all other participating systems in Interpretable STS for non-chunked sentence input. 1 Introduction Semantic Textual Similarity gives a quantifier to evaluate semantic equivalence between two sentences. Earlier SemEval tasks (Agirre et al., 2012), (Agirre et al., 2013), (Agirre et al., 2014) focused on finding the semantic equivalence between sentences in English and Spanish. A new pilot task was introduced this year to find which parts (chunks) of the sentences are equivalent in meaning. SVCSTS is an extension to (Sultan et al., 2014) and it handles both Spanish STS and Interpretable STS. SVCSTS uses Monolingual word aligner (Sultan et al., May 2014), supervised machine learning techniques, Google and Bing translator API’s. Section 2 describes a brief overview of SVCSTS’s approach for various subtasks. Section 3 outlines the performa</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. Semeval-2012 task 6: A pilot on semantic textual similarity. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385–393, Montr´eal, Canada, 7-8 June 2012. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>sem 2013 shared task: Semantic textual similarity.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity,</booktitle>
<pages>32--43</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="925" citStr="Agirre et al., 2013" startWordPosition="126" endWordPosition="129"> system that was submitted in SemEval-2015 Task 2: Semantic Textual Similarity(STS)(Agirre et al., 2015). The task has 3 subtasks viz., English STS, Spanish STS and Interpretable STS. SVCSTS uses Monolingual word aligner (Sultan et al., May 2014), supervised machine learning, Google and Bing translator API’s. Various runs of the system outperformed all other participating systems in Interpretable STS for non-chunked sentence input. 1 Introduction Semantic Textual Similarity gives a quantifier to evaluate semantic equivalence between two sentences. Earlier SemEval tasks (Agirre et al., 2012), (Agirre et al., 2013), (Agirre et al., 2014) focused on finding the semantic equivalence between sentences in English and Spanish. A new pilot task was introduced this year to find which parts (chunks) of the sentences are equivalent in meaning. SVCSTS is an extension to (Sultan et al., 2014) and it handles both Spanish STS and Interpretable STS. SVCSTS uses Monolingual word aligner (Sultan et al., May 2014), supervised machine learning techniques, Google and Bing translator API’s. Section 2 describes a brief overview of SVCSTS’s approach for various subtasks. Section 3 outlines the performance of SVCSTS in variou</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. *sem 2013 shared task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 32–43, Atlanta, Georgia, USA, June 2013. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
<author>Weiwei Guo</author>
<author>Rada Mihalcea</author>
<author>German Rigau</author>
<author>Janyce Wiebe</author>
</authors>
<title>Semeval-2014 task 10: Multilingual semantic textual similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>81--91</pages>
<institution>Association for Computational Linguistics and Dublin City University.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="948" citStr="Agirre et al., 2014" startWordPosition="130" endWordPosition="133">ted in SemEval-2015 Task 2: Semantic Textual Similarity(STS)(Agirre et al., 2015). The task has 3 subtasks viz., English STS, Spanish STS and Interpretable STS. SVCSTS uses Monolingual word aligner (Sultan et al., May 2014), supervised machine learning, Google and Bing translator API’s. Various runs of the system outperformed all other participating systems in Interpretable STS for non-chunked sentence input. 1 Introduction Semantic Textual Similarity gives a quantifier to evaluate semantic equivalence between two sentences. Earlier SemEval tasks (Agirre et al., 2012), (Agirre et al., 2013), (Agirre et al., 2014) focused on finding the semantic equivalence between sentences in English and Spanish. A new pilot task was introduced this year to find which parts (chunks) of the sentences are equivalent in meaning. SVCSTS is an extension to (Sultan et al., 2014) and it handles both Spanish STS and Interpretable STS. SVCSTS uses Monolingual word aligner (Sultan et al., May 2014), supervised machine learning techniques, Google and Bing translator API’s. Section 2 describes a brief overview of SVCSTS’s approach for various subtasks. Section 3 outlines the performance of SVCSTS in various subtasks of SemEval 2</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, Wiebe, 2014</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. Semeval-2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81–91, Dublin, Ireland, August 2014. Association for Computational Linguistics and Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhay Kashyap</author>
<author>Lushan Han</author>
<author>Roberto Yus</author>
<author>Jennifer Sleeman</author>
<author>Taneeya Satyapanich</author>
<author>Sunil Gandhi</author>
<author>Tim Finin</author>
</authors>
<title>Meerkat mafia: Multilingual and cross-level semantic textual similarity systems.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>416--423</pages>
<institution>Association for Computational Linguistics and Dublin City University.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="2367" citStr="Kashyap et al., 2014" startWordPosition="366" endWordPosition="369">. (Sultan et al., 2014) system was used to find the semantic equivalence between two sentences and a score on a scale of 0-5 was given. 2.2 Spanish STS Spanish STS is built upon English STS to calculate similarity scores for a given pair of Spanish sentences on a scale of 0 to 4. Spanish sentences were translated to English, fed to English STS system and the scores are scaled accordingly. Translations were done using Bing Translator API (Bing Translator API) and Google Translate API. Two translators were used to improve the accuracy of the translations. Google Translate API was obtained from (Kashyap et al., 2014). We used this system to get multiple translations of each chunk in a sentence. Multiple sentences are generated by combining the top two translations of each chunk. We then randomly pick a maximum of ten sentences for each Spanish sentence. Translation pairs are formed by choosing corresponding numbered sentences from sentence 1 and sentence 2 translations. We limited the number of translations to 10 to reduce the overall computation time. Translation pairs were then passed to English STS system. Final score was obtained as the average 107 Proceedings of the 9th International Workshop on Sema</context>
</contexts>
<marker>Kashyap, Han, Yus, Sleeman, Satyapanich, Gandhi, Finin, 2014</marker>
<rawString>Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer Sleeman, Taneeya Satyapanich, Sunil Gandhi, and Tim Finin. Meerkat mafia: Multilingual and cross-level semantic textual similarity systems. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 416– 423, Dublin, Ireland, August 2014. Association for Computational Linguistics and Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arafat Sultan</author>
<author>Steven Bethard</author>
<author>Tamara Sumner</author>
</authors>
<title>Dls@cu: Sentence similarity from word alignment.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>241--246</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="1197" citStr="Sultan et al., 2014" startWordPosition="172" endWordPosition="175">ing, Google and Bing translator API’s. Various runs of the system outperformed all other participating systems in Interpretable STS for non-chunked sentence input. 1 Introduction Semantic Textual Similarity gives a quantifier to evaluate semantic equivalence between two sentences. Earlier SemEval tasks (Agirre et al., 2012), (Agirre et al., 2013), (Agirre et al., 2014) focused on finding the semantic equivalence between sentences in English and Spanish. A new pilot task was introduced this year to find which parts (chunks) of the sentences are equivalent in meaning. SVCSTS is an extension to (Sultan et al., 2014) and it handles both Spanish STS and Interpretable STS. SVCSTS uses Monolingual word aligner (Sultan et al., May 2014), supervised machine learning techniques, Google and Bing translator API’s. Section 2 describes a brief overview of SVCSTS’s approach for various subtasks. Section 3 outlines the performance of SVCSTS in various subtasks of SemEval 2015 Task-2. 2 System Description Following 3 sub sections describe SVCSTS’s approach for the 3 subtasks. 2.1 English STS This task was about finding the semantic similarity between English sentences. (Sultan et al., 2014) system was used to find the</context>
</contexts>
<marker>Sultan, Bethard, Sumner, 2014</marker>
<rawString>Md. Arafat Sultan, Steven Bethard, and Tamara Sumner. Dls@cu: Sentence similarity from word alignment. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241–246, Dublin, Ireland, 2014. Association for Computational Linguistics and Dublin City University. Winner of the shared task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arafat Sultan</author>
<author>Steven Bethard</author>
</authors>
<title>Tamara Sumner Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence</title>
<date></date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>2</volume>
<pages>219--230</pages>
<marker>Sultan, Bethard, </marker>
<rawString>Md. Arafat Sultan and Steven Bethard and Tamara Sumner Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence Transactions of the Association for Computational Linguistics, Vol. 2, (May), pages 219–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine Learning in</title>
<date>2011</date>
<journal>Python Journal of Machine Learning Research, Vol</journal>
<volume>12</volume>
<pages>2825--2830</pages>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E. Scikit-learn: Machine Learning in Python Journal of Machine Learning Research, Vol 12, pages 2825–2830 2011</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>