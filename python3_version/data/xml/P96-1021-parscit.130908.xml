<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998901">
A Polynomial-Time Algorithm for Statistical Machine Translation
</title>
<author confidence="0.964313">
Dekai Wu
</author>
<affiliation confidence="0.86927175">
HKUST
Department of Computer Science
University of Science and Technology
Clear Water Bay, Hong Kong
</affiliation>
<email confidence="0.958354">
dekai@cs.ust .hk
</email>
<sectionHeader confidence="0.995174" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960076923077">
We introduce a polynomial-time algorithm
for statistical machine translation. This
algorithm can be used in place of the
expensive, slow best-first search strate-
gies in current statistical translation ar-
chitectures. The approach employs the
stochastic bracketing transduction gram-
mar (SBTG) model we recently introduced
to replace earlier word alignment channel
models, while retaining a bigram language
model. The new algorithm in our experi-
ence yields major speed improvement with
no significant loss of accuracy.
</bodyText>
<sectionHeader confidence="0.989152" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999948333333333">
The statistical translation model introduced by IBM
(Brown et al., 1990) views translation as a noisy
channel process. Assume, as we do throughout this
paper, that the input language is Chinese and the
task is to translate into English. The underlying
generative model, shown in Figure 1, contains a
stochastic English sentence generator whose output
is &amp;quot;corrupted&amp;quot; by the translation channel to produce
Chinese sentences. In the IBM system, the language
model employs simple n-grams, while the transla-
tion model employs several sets of parameters as
discussed below. Estimation of the parameters has
been described elsewhere (Brown et al., 1993).
Translation is performed in the reverse direction
from generation, as usual for recognition under gen-
erative models. For each Chinese sentence e that is
to be translated, the system must attempt to find
the English sentence e* such that:
</bodyText>
<listItem confidence="0.989441">
(1) e* = argmax Pr (el e)
(2) = argmax Pr (cle) Pr(e)
</listItem>
<bodyText confidence="0.978152755102041">
In the IBM model, the search for the optimal e* is
performed using a best-first heuristic &amp;quot;stack search&amp;quot;
similar to A* methods.
One of the primary obstacles to making the statis-
tical translation approach practical is slow speed of
translation, as performed in A* fashion. This price is
paid for the robustness that is obtained by using very
flexible language and translation models. The lan-
guage model allows sentences of arbitrary order and
the translation model allows arbitrary word-order
permutation. The models employ no structural con-
straints, relying instead on probability parameters
to assign low probabilities to implausible sentences.
This exhaustive space, together with massive num-
ber of parameters, permits greater modeling accu-
racy.
But while accuracy is enhanced, translation ef-
ficiency suffers due to the lack of structure in the
hypothesis space. The translation channel is char-
acterized by two sets of parameters: translation and
alignment probabilities.&apos; The translation probabil-
ities describe lexical substitution, while alignment
probabilities describe word-order permutation. The
key problem is that the formulation of alignment
probabilities a(il j, V, T) permits the Chinese word in
position j of a length-T sentence to map to any po-
sition i of a length-V English sentence. So VT align-
ments are possible, yielding an exponential space
with correspondingly slow search times.
Note there are no explicit linguistic grammars in
the IBM channel model. Useful methods do exist
for incorporating constraints fed in from other pre-
processing modules, and some of these modules do
employ linguistic grammars. For instance, we previ-
ously reported a method for improving search times
in channel translation models that exploits bracket-
ing information (Wu and Ng, 1995). If any brackets
for the Chinese sentence can be supplied as addi-
tional input information, produced for example by
a preprocessing stage, a modified version of the A*-
based algorithm can follow the brackets to guide the
search heuristically. This strategy appears to pro-
duces moderate improvements in search speed and
slightly better translations.
Such linguistic-preprocessing techniques could
&apos;Various models have been constructed by the IBM
team (Brown et al., 1993). This description corresponds
to one of the simplest ones, &amp;quot;Model 2&amp;quot;; search costs for
the more complex models are correspondingly higher.
</bodyText>
<page confidence="0.991066">
152
</page>
<figure confidence="0.987165909090909">
stochastic
English
generator
noisy
channel
English
strings
Chinese
strings
direction of generative model
direction of translation
</figure>
<figureCaption confidence="0.999965">
Figure 1: Channel translation model.
</figureCaption>
<bodyText confidence="0.999093">
also be used with the new model described below,
but the issue is independent of our focus here. In
this paper we address the underlying assumptions
of core channel model itself which does not directly
use linguistic structure.
A slightly different model is employed for a
word alignment application by Dagan et al. (Da-
gan, Church, and Gale, 1993). Instead of alignment
probabilities, offset probabilities o(k) are employed,
where k is essentially the positional distance between
the English words aligned to two adjacent Chinese
words:
</bodyText>
<equation confidence="0.407174">
(3) k = i — (A(jp,) (j jp„,,)N)
</equation>
<bodyText confidence="0.999914666666667">
wherepr e v is the position of the immediately pre-
ceding Chinese word and N is a constant that nor-
malizes for average sentence lengths in different lan-
guages. The motivation is that words that are close
to each other in the Chinese sentence should tend
to be close in the English sentence as well. The
size of the parameter set is greatly reduced from
the x x (TI x I VI parameters of the alignment
probabilities, down to a small set of jkl parameters.
However, the search space remains the same.
The A*-style stack-decoding approach is in some
ways a carryover from the speech recognition archi-
tectures that inspired the channel translation model.
It has proven highly effective for speech recognition
in both accuracy and speed, where the search space
contains no order variation since the acoustic and
text streams can be assumed to be linearly aligned.
But in contrast, for translation models the stack
search alone does not adequately compensate for
the combinatorially more complex space that results
from permitting arbitrary order variations. Indeed,
the stack-decoding approach remains impractically
slow for translation, and has not achieved the same
kind of speed as for speech recognition.
The model we describe in this paper, like Dagan
et al.&apos;s model, encourages related words to stay to-
gether, and reduces the number of parameters used
to describe word-order variation. But more impor-
tantly, it makes structural assumptions that elimi-
nate large portions of the space of alignments, based
on linguistic motivatations. This greatly reduces the
search space and makes possible a polynomial-time
optimization algorithm.
</bodyText>
<sectionHeader confidence="0.980991" genericHeader="introduction">
2 ITG and BTG Overview
</sectionHeader>
<bodyText confidence="0.999866111111111">
The new translation model is based on the recently
introduced bilingual language modeling approach.
Specifically, the model employs a bracketing trans-
duction grammar or BTG (Wu, 1995a), which is
a special case of inversion transduction grammars
or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu,
1995d). These formalisms were originally developed
for the purpose of parallel corpus annotation, with
applications for bracketing, alignment, and segmen-
tation. This paper finds they are also useful for the
translation system itself. In this section we summa-
rize the main properties of BTGs and ITGs.
An ITG consists of context-free productions where
terminal symbols come in couples, for example x I y,
where x is a Chinese word and y is an English trans-
lation of x.2 Any parse tree thus generates two
strings, one on the Chinese stream and one on the
English stream. Thus, the tree:
</bodyText>
<listItem confidence="0.90781325">
(1) [ft/I [[7/took [—/a */e W/book]NP Nip
[/for ithrou]PP IvP Is
produces, for example, the mutual translations:
(2) a. [PI UST [—*MNP ]vp [MTApp ivP Is
</listItem>
<equation confidence="0.621671666666667">
[WO Una le [yi ben shu]Np [gei ni]pp 1vP
Is
b. [I [[took [a book]Np INTP [for you]pp Ivp
</equation>
<bodyText confidence="0.9992996">
An additional mechanism accommodates a con-
servative degree of word-order variation between the
two languages. With each production of the gram-
mar is associated either a straight orientation or an
inverted orientation, respectively denoted as follows:
</bodyText>
<sectionHeader confidence="0.825721" genericHeader="method">
VP [VP PP]
VP (VP PP)
</sectionHeader>
<bodyText confidence="0.81354625">
In the case of a production with straight orien-
tation, the right-hand-side symbols are visited left-
to-right for both the Chinese and English streams.
But for a production with inverted orientation, the
2 Readers of the papers cited above should note that
we have switched the roles of English and Chinese here,
which helps simplify the presentation of the new trans-
lation algorithm.
</bodyText>
<page confidence="0.993332">
153
</page>
<figure confidence="0.894416611111111">
BTG all matchings ratio
2 1 1 1.000
3 1 1 1.000
4 2 2 1.000
5 6 6 1.000
6 22 24 0.917
7 90 120 0.750
8 394 720 0.547
9 1806 5040 0.358
10 8558 40320 0.212
11 41586 362880 0.115
12 206098 3628800 0.057
13 1037718 39916800 0.026
14 5293446 479001600 0.011
15 27297738 6227020800 0.004
16 142078746 87178291200 0.002
745387038 1307674368000 0.001
3937603038 20922789888000 0.000
</figure>
<figureCaption confidence="0.973311666666667">
Figure 2: Number of legal word alignments between
sentences of length f, with and without the BTG
restriction.
</figureCaption>
<bodyText confidence="0.9561085">
right-hand-side symbols are visited left-to-right for
Chinese and right-to-left for English. Thus, the tree:
</bodyText>
<listItem confidence="0.9798118">
(3) [Ng ([M/for fx/you]pp [*7/took [—/a */f
lbook]NP ]vP )vP Is
produces translations with different word order:
(4) a. [ft [[1]pp [*7 [—*]NP ]v ]v Is
b. [I [[took [a book]NP lvp [for you]PP 1vP Is
</listItem>
<bodyText confidence="0.9964825">
In the special case of BTGs which are employed
in the model presented below, there is only one un-
differentiated nonterminal category (aside from the
start symbol). Designating this category A, this
means all non-lexical productions are of one of these
two forms:
</bodyText>
<figure confidence="0.6515395">
A [A A • • • A]
A (A A • • • A)
</figure>
<bodyText confidence="0.997935285714286">
The degree of word-order flexibility is the criti-
cal point. BTGs make a favorable trade-off between
efficiency and expressiveness: constraints are strong
enough to allow algorithms to operate efficiently, but
without so much loss of expressiveness as to hinder
useful translation. We summarize here; details are
given elsewhere (Wu, 1995b).
With regard to efficiency, Figure 2 demonstrates
the kind of reduction that BTGs obtain in the space
of possible alignments. The number of possible
alignments, compared against the unrestricted case
where any English word may align to any Chinese
position, drops off dramatically for strings longer
than four words. (This table makes the simplifica-
tion of counting only 1-1 matchings and is merely
representative.)
With regard to expressiveness, we believe that al-
most all variation in the order of arguments in a
syntactic frame can be accommodated.3 Syntac-
tic frames generally contain four or fewer subcon-
stituents. Figure 2 shows that for the case of four
subconstituents, BTGs permit 22 out of the 24 pos-
sible alignments. The only prohibited arrangements
are &amp;quot;inside-out&amp;quot; transformations (Wu, 1995b), which
we have been unable to find any examples of in our
corpus. Moreover, extremely distorted alignments
can be handled by BTGs (Wu, 1995c), without re-
sorting to the unrestricted-alignment model.
The translation expressiveness of BTGs is by no
means perfect. They are nonetheless proving very
useful in applications and are substantially more fea-
sible than previous models. In our previous corpus
analysis applications, any expressiveness limitations
were easily tolerable since degradation was graceful.
In the present translation application, any expres-
siveness limitation simply means that certain trans-
lations are not considered.
For the remainder of the paper, we take advantage
of a convenient normal-form theorem (Wu, 1995a)
that allows us to assume without loss of generality
that the BTG only contains the binary-branching
form for the non-lexical productions.4
</bodyText>
<sectionHeader confidence="0.81073" genericHeader="method">
3 BTG-Based Search for the
Original Models
</sectionHeader>
<bodyText confidence="0.9998377">
A first approach to improving the translation search
is to limit the allowed word alignment patterns to
those permitted by a BTG. In this case, Equation (2)
is kept as the objective function and the translation
channel can be parameterized similarly to Dagan ei
al. (Dagan, Church, and Gale, 1993). The effect of
the BTG restriction is just to constrain the shapes of
the word-order distortions. A BTG rather than ITG
is used since, as we discussed earlier, pure channel
translation models operate without explicit gram-
mars, providing no constituent categories around
which a more sophisticated ITG could be structured.
But the structural constraints of the BTG can im-
prove search efficiency, even without differentiated
constituent categories. Just as in the baseline sys-
tem, we rely on the language and translation models
to take up the slack in place of an explicit grammar.
In this approach, an 0(T7) algorithm similar to the
one described later can be constructed to replace A*
search.
</bodyText>
<footnote confidence="0.9951722">
3 Note that these points are not directed at free word-
order languages. But in such languages, explicit mor-
phological inflections make role identification and trans-
lation easier.
4But see the conclusion for a caveat.
</footnote>
<page confidence="0.99934">
154
</page>
<bodyText confidence="0.9999465">
However we do not feel it is worth preserving off-
set (or alignment or distortion) parameters simply
for the sake of preserving the original translation
channel model. These parameterizations were only
intended to crudely model word-order variation. In-
stead, the BTG itself can be used directly to proba-
bilistically rank alternative alignments, as described
next.
</bodyText>
<sectionHeader confidence="0.9527075" genericHeader="method">
4 Replacing the Channel Model
with a SBTG
</sectionHeader>
<bodyText confidence="0.999852666666667">
The second possibility is to use a stochastic brack-
eting transduction grammar (SBTG) in the channel
model, replacing the translation model altogether.
In a SBTG, a probability is associated with each pro-
duction. Thus for the normal-form BTG, we have:
The translation lexicon is encoded in productions of
</bodyText>
<figure confidence="0.601124333333333">
A al) [A A] for all x,y lexical translations
A a() (AA) for all x Chinese vocabulary
A b(x,y) sjy for all y English vocabulary
A —+ x/e
A b(x,e) E/Y
b(6,Y)
</figure>
<bodyText confidence="0.9947992">
the third kind. The latter two kinds of productions
allow words of either Chinese or English to go un-
matched.
The SBTG assigns a probability Pr(c, e, q) to all
generable trees q and sentence-pairs. In principle
it can be used as the translation channel model by
normalizing with Pr(e) and integrating out Pr(q) to
give Pr(cle) in Equation (2). In practice, a strong
language model makes this unnecessary, so we can
instead optimize the simpler Viterbi approximation
</bodyText>
<equation confidence="0.528704">
(4) e* = argmax Pr(c, e, q) Pr(e)
</equation>
<bodyText confidence="0.999922542372882">
To complete the picture we add a bigram model
get—let = g(e.i 1e3_
Pr(e). 1) for the English language model
Offset, alignment, or distortion parameters are
entirely eliminated. A large part of the im-
plicit function of such parameters—to prevent align-
ments where too many frame arguments become
separated—is rendered unnecessary by the BTG&apos;s
structural constraints, which prohibit many such
configurations altogether. Another part of the pa-
rameters&apos; purpose is subsumed by the SBTG&apos;s prob-
abilities all and a0, which can be set to prefer
straight or inverted orientation depending on the
language pair. As in the original models, the lan-
guage model heavily influences the remaining order-
ing decisions.
Matters are complicated by the presence of the bi-
gram model in the objective function (which word-
alignment models, as opposed to translation models,
do not need to deal with). As in our word-alignment
model, the translation algorithm optimizes Equa-
tion (4) via dynamic programming, similar to chart
parsing (Earley, 1970) but with a probabilistic ob-
jective function as for HMMs (Viterbi, 1967). But
unlike the word-alignment model, to accommodate
the bigram model we introduce indexes in the recur-
rence not only on subtrees over the source Chinese
string, but also on the delimiting words of the target
English substrings.
Another feature of the algorithm is that segmen-
tation of the Chinese input sentence is performed
in parallel with the translation search. Conven-
tional architectures for Chinese NLP generally at-
tempt to identify word boundaries as a preprocess-
ing stage.5 Whenever the segmentation preprocessor
prematurely commits to an inappropriate segmenta-
tion, difficulties are created for later stages. This
problem is particularly acute for translation, since
the decision as to whether to regard a sequence as a
single unit depends on whether its components can
be translated compositionally. This in turn often
depends on what the target language is. In other
words, the Chinese cannot be appropriately seg-
mented except with respect to the target language of
translation—a task-driven definition of correct seg-
mentation.
The algorithm is given below. A few remarks
about the notation used: cs,.t denotes the subse-
quence of Chinese tokens cs+1, cs+2, , ct. We use
E(s..t) to denote the set of English words that are
translations the Chinese word created by taking all
tokens in c, t together. E(s, t) denotes the set of
English words that are translations of any of the
Chinese words anywhere within e, ..t. Note also that
we assume the explicit sentence-start and sentence-
end tokens co = &lt;s&gt; and cT+1 = &lt;Is&gt;, which makes
the algorithm description more parsimonious. Fi-
nally, the argmax operator is generalized to vector
notation to accomodate multiple indices.
</bodyText>
<equation confidence="0.8463743">
1. Initialization
61)tYY(i) = bi(c3 .t/Y), 0&lt;s&lt;t&lt;T
Y E E(s..t)
2. Recursion For all s, t, y, z such that
—1&lt;s&lt;t&lt;T+1
yEE(8,t)
t. zEE(s,t)
= ma,,x1.6 6 6 1
--L- [sityz 1 -Ptyz 7 - 3Otyz,
{H if bliityz &gt; etyz and 61,3tyz &gt; 6s°tyz
</equation>
<bodyText confidence="0.797323">
= 0 if etyz &gt; Olslyz and 4yz &gt; 451)tyz
</bodyText>
<footnote confidence="0.56871">
0 otherwise
&apos;Written Chinese contains no spaces to delimit words;
any spaces in the earlier examples are artifacts of the
parse tree brackets.
</footnote>
<figure confidence="0.505515">
Ostyz
Ostyz
</figure>
<page confidence="0.807406">
155
</page>
<table confidence="0.751618666666667">
Category Original A* Bracket A* BTG-Channel
Correct 67.5 69.8 68.2
Incorrect 32.5 30.2 31.8
</table>
<figureCaption confidence="0.9966">
Figure 3: Translation accuracy (percentage correct).
</figureCaption>
<bodyText confidence="0.681857">
where
</bodyText>
<figure confidence="0.972678777777778">
• max a[1 5,,s1,y stzz gY z
s&lt;s&lt;t
EE(s,$)
zEE(s,t)
• argmax a() bsszz(j) 6s1yy(k) gY z
s&lt;s.o
YeE(s,t)
zEE(3,$)
3. Reconstruction Initialize by setting the root
</figure>
<bodyText confidence="0.9921105">
of the parse tree to qo =_- (-1,T— 1, &lt;s&gt;, &lt;/ s&gt;). The
remaining descendants in the optimal parse tree are
then given recursively for any q = (s,t,y, z) by:
a probabilistic optimization problem. But perhaps
most importantly, our goal is to constrain as tightly
as possible the space of possible transduction rela-
tionships between two languages with fixed word-
order, making no other language-specific assump-
tions; we are thus driven to seek a kind of language-
universal property. In contrast, the ID/LP work
was directed at parsing a single language with free
word-order. As a consequence, it would be neces-
sary to enumerate a specific set of linear-precedence
(LP) relations for the language, and moreover the
immediate-dominance (ID) productions would typi-
cally be more complex than binary-branching. This
significantly increases time complexity, compared to
our BTG model. Although it is not mentioned in
their paper, the time complexity for ID/LP pars-
ing rises exponentially with the length of produc-
tion right-hand-sides, due to the number of permuta-
tions. ITGs avoid this with their restriction to inver-
sions, rather than permutations, and BTGs further
minimize the grammar size. We have also confirmed
empirically that our models would not be feasible
under general permutations.
</bodyText>
<figure confidence="0.887098333333333">
• argmax a11 bssyy 6,stz2 gyz
s&lt;s&lt;t
YEE(3,$)
ZEE(St)
• max 42° 63szz 55tyY gYz
3&lt;s&lt;t
YEE(s,t)
zEE(3,$)
6.2y.
</figure>
<equation confidence="0.9903295">
LEFT(g) =
RIGHT(q) =
</equation>
<bodyText confidence="0.9998710625">
Assume the number of translations per word is
bounded by some constant. Then the maximum size
of E(s,t) is proportional to t — s. The asymptotic
time complexity for the translation algorithm is thus
bounded by 0(T7). Note that in practice, actual
performance is improved by the sparseness of the
translation matrix.
An interesting connection has been suggested to
direct parsing for ID/LP grammars (Shieber, 1984),
in which word-order variations would be accommo-
dated by the parser, and related ideas for genera-
tion of free word-order languages in the TAG frame-
work (Joshi, 1987). Our work differs from the ID/LP
work in several important respects. First, we are not
merely parsing, but translating with a bigram lan-
guage model. Also, of course, we are dealing with
</bodyText>
<sectionHeader confidence="0.999525" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.99991168">
The algorithm above was tested in the SILC transla-
tion system. The translation lexicon was largely con-
structed by training on the HKUST English-Chinese
Parallel Bilingual Corpus, which consists of govern-
mental transcripts. The corpus was sentence-aligned
statistically (Wu, 1994); Chinese words and colloca-
tions were extracted (Fung and Wu, 1994; Wu and
Fung, 1994); then translation pairs were learned via
an EM procedure (Wu and Xia, 1995). The re-
sulting English vocabulary is approximately 6,500
words and the Chinese vocabulary is approximately
5,500 words, with a many-to-many translation map-
ping averaging 2.25 Chinese translations per English
word. Due to the unsupervised training, the transla-
tion lexicon contains noise and is only at about 86%
percent weighted precision.
With regard to accuracy, we merely wish to
demonstrate that for statistical MT, accuracy is not
significantly compromised by substituting our effi-
cient optimization algorithm. It is not our purpose
here to argue that accuracy can be increased with
our model. No morphological processing has been
used to correct the output, and until now we have
only been testing with a bigram model trained on
extremely limited samples. A coarse evaluation of
</bodyText>
<equation confidence="0.785500125">
NIL
(s) Oqi)
(s, o),c4), z)
NIL
NIL
(41, t, w[q], z)
Y, OV)
NIL
</equation>
<construct confidence="0.961380375">
if t--3&lt;1
if Oq =
if Oq =
otherwise
if t-3&lt;i.
if Oq =
if Oq = 0
otherwise
</construct>
<page confidence="0.931649">
156
</page>
<figure confidence="0.997124866666667">
Input:
Output:
Corpus:
Input:
Output:
Corpus:
Input:
Output:
Corpus:
Input:
Output:
Corpus:
Input:
Output:
Corpus:
</figure>
<bodyText confidence="0.785665785714286">
(Xiang gang de an ding fan rong shi WO men sheng hu6 fang shi de zhi
Hong Kong&apos;s stabilize boom is us life styles&apos;s pillar.
Our prosperity and stability underpin our way of life.
44NMINVitta
(Ben gang de jing ji qian jing yu zhang gu6, te bie shi guang dong sheng de jing
ji qian jing xi xi xiang guan.)
Hong Kong&apos;s economic foreground with China, particular Guangdong province&apos;s
economic foreground vitally interrelated.
Our economic future is inextricably bound up with China, and with Guangdong
Province in particular.
firdtittifirg.g.
(WO win quin zhi chi ta de yi jian.)
I absolutely uphold his views.
I fully support his views.
</bodyText>
<equation confidence="0.153571">
).±-,ypifiblI301111 El Mt l&apos;41/06ZIMgt
</equation>
<bodyText confidence="0.91085">
(Zhe xie an pai ke jia qiing wo men ri hOu wei chi jin r6ng wen ding de neng 11.)
These arrangements can enforce us future kept financial stabilization&apos;s competency.
These arrangements will enhance our ability to maintain monetary stability in
the years to come.
tWt, ftRAMT.A1t: ZOM, ftrig*IliTtAtIttM3R/OIAMPfiEfissi R.
gu&apos;o, wa xian zai ke yl ken ding de shuO, wO men jiang hul ti gong wei di dao
ge xiang zhii yao mu biao suO xil de jing fei.)
However, I now can certainty&apos;s say, will provide for us attain various dominant
goal necessary&apos;s current expenditure.
The consultation process is continuing but I can confirm now that the necessary
funds will be made available to meet the key targets.
</bodyText>
<figureCaption confidence="0.998393">
Figure 4: Example translation outputs.
</figureCaption>
<bodyText confidence="0.99982864">
translation accuracy was performed on a random
sample drawn from Chinese sentences of fewer than
20 words from the parallel corpus, the results of
which are shown in Figure 3. We have judged only
whether the correct meaning (as determined by the
corresponding English sentence in the parallel cor-
pus) is conveyed by the translation, paying particu-
lar attention to word order, but otherwise ignoring
morphological and function word choices. For com-
parison, the accuracies from the A*-based systems
are also shown. There is no significant difference
in the accuracy. Some examples of the output are
shown in Figure 4.
On the other hand, the new algorithm has indeed
proven to be much faster. At present we are unable
to use direct measurement to compare the speed of
the systems meaningfully, because of vast implemen-
tational differences between the systems. However,
the order-of-magnitude improvements are immedi-
ately apparent. In the earlier system, translation of
single sentences required on the order of hours (Sun
Sparc 10 workstations). In contrast the new algo-
rithm generally takes less than one minute—usually
substantially less—with no special optimization of
the code.
</bodyText>
<sectionHeader confidence="0.998841" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998740263157895">
We have introduced a new algorithm for the run-
time optimization step in statistical machine trans-
lation systems, whose polynomial-time complexity
addresses one of the primary obstacles to practicality
facing statistical MT. The underlying model for the
algorithm is a combination of the stochastic BTG
and bigram models. The improvement in speed does
not appear to impair accuracy significantly.
We have implemented a version that accepts ITGs
rather than BTGs, and plan to experiment with
more heavily structured models. However, it is im-
portant to note that the search complexity rises ex-
ponentially rather than polynomially with the size of
the grammar, just as for context-free parsing (Bar-
ton, Berwick, and Ristad, 1987). This is not relevant
to the BTG-based model we have described since its
grammar size is fixed; in fact the BTG&apos;s minimal
grammar size has been an important advantage over
more linguistically-motivated ITG-based models.
</bodyText>
<page confidence="0.992928">
157
</page>
<bodyText confidence="0.99997375">
We have also implemented a generalized version
that accepts arbitrary grammars not restricted to
normal form, with two motivations. The pragmatic
benefit is that structured grammars become easier
to write, and more concise. The expressiveness ben-
efit is that a wider family of probability distribu-
tions can be written. As stated earlier, the normal
form theorem guarantees that the same set of shapes
will be explored by our search algorithm, regardless
of whether a binary-branching BTG or an arbitrary
BTG is used. But it may sometimes be useful to
place probabilities on n-ary productions that vary
with n in a way that cannot be expressed by com-
posing binary productions; for example one might
wish to encourage longer straight productions. The
generalized version permits such strategies.
Currently we are evaluating robustness extensions
of the algorithm that permit words suggested by the
language model to be inserted in the output sen-
tence, which the original A* algorithms permitted.
</bodyText>
<sectionHeader confidence="0.991645" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999130166666667">
Thanks to an anonymous referee for valuable com-
ments, and to the SILC group members: Xuanyin
Xia, Eva Wai-Man Fong, Cindy Ng, Hong-sing
Wong, and Daniel Ka-Leung Chan. Many thanks
also to Kathleen McKeown and her group for dis-
cussion, support, and assistance.
</bodyText>
<sectionHeader confidence="0.998195" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999789215189874">
Barton, G. Edward, Robert C. Berwick, and
Eric Sven Ristad. 1987. Computational Complex-
ity and Natural Language. MIT Press, Cambridge,
MA.
Brown, Peter F., John Cocke, Stephen A. DellaPi-
etra, Vincent J. DellaPietra, Frederick Jelinek,
John D. Lafferty, Robert L. Mercer, and Paul S.
Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):29-
85.
Brown, Peter F., Stephen A. DellaPietra, Vincent J.
DellaPietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation:
Parameter estimation. Computational Linguis-
tics, 19(2):263-311.
Dagan, Ido, Kenneth W. Church, and William A.
Gale. 1993. Robust bilingual word alignment
for machine aided translation. In Proceedings of
the Workshop on Very Large Corpora, pages 1-8,
Columbus, OH, June.
Earley, Jay. 1970. An efficient context-free pars-
ing algorithm. Communications of the Associa-
tion for Computing Machinery, 13(2):94-102.
Fung, Pascale and Dekai Wu. 1994. Statistical aug-
mentation of a Chinese machine-readable dictio-
nary. In Proceedings of the Second Annual Work-
shop on Very Large Corpora, pages 69-85, Kyoto,
August.
Joshi, Aravind K. 1987. Word-order variation in
natural language generation. In Proceedings of
AAAI-87, Sixth National Conference on Artificial
Intelligence, pages 550-555.
Shieber, Stuart M. 1984. Direct parsing of ID/LP
grammars. Linguistics and Philosophy, 7:135-
154.
Viterbi, Andrew J. 1967. Error bounds for convolu-
tional codes and an asymptotically optimal decod-
ing algorithm. IEEE Transactions on Information
Theory, 13:260-269.
Wu, Dekai. 1994. Aligning a parallel English-
Chinese corpus statistically with lexical criteria.
In Proceedings of the 32nd Annual Conference
of the Association for Computational Linguistics,
pages 80-87, Las Cruces, New Mexico, June.
Wu, Dekai. 1995a. An algorithm for simultaneously
bracketing parallel texts by aligning words. In
Proceedings of the 33rd Annual Conference of the
Association for Computational Linguistics, pages
244-251, Cambridge, Massachusetts, June.
Wu, Dekai. 1995b. Grammarless extraction of
phrasal translation examples from parallel texts.
In TMI-95, Proceedings of the Sixth International
Conference on Theoretical and Methodological Is-
sues in Machine Translation, volume 2, pages
354-372, Leuven, Belgium, July.
Wu, Dekai. 1995c. Stochastic inversion trans-
duction grammars, with application to segmen-
tation, bracketing, and alignment of parallel cor-
pora. In Proceedings of IJCAI-95, Fourteenth .In-
ternational Joint Conference on Artificial Intelli-
gence, pages 1328-1334, Montreal, August.
Wu, Dekai. 1995d. Trainable coarse bilingual gram-
mars for parallel text bracketing. In Proceed-
ings of the Third Annual Workshop on Very Large
Corpora, pages 69-81, Cambridge, Massachusetts,
June.
Wu, Dekai and Pascale Fung. 1994. Improving
Chinese tokenization with linguistic filters on sta-
tistical lexical acquisition. In Proceedings of the
Fourth Conference on Applied Natural Language
Processing, pages 180-181, Stuttgart, October.
Wu, Dekai and Cindy Ng. 1995. Using brackets
to improve search for statistical machine transla-
tion. In PACLIC-10, Pacific Asia Conference on
Language, Information and Computation, pages
195-204, Hong Kong, December.
Wu, Dekai and Xuanyin Xia. 1995. Large-scale au-
tomatic extraction of an English-Chinese lexicon.
Machine Translation, 9(3-4):285-313.
</reference>
<page confidence="0.997071">
158
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999934">A Polynomial-Time Algorithm for Statistical Machine Translation</title>
<author confidence="0.991517">Dekai Wu</author>
<affiliation confidence="0.872981333333333">HKUST Department of Computer Science University of Science and Technology</affiliation>
<address confidence="0.999295">Clear Water Bay, Hong Kong</address>
<email confidence="0.929041">dekai@cs.ust.hk</email>
<abstract confidence="0.995795092391305">We introduce a polynomial-time algorithm for statistical machine translation. This algorithm can be used in place of the expensive, slow best-first search strategies in current statistical translation architectures. The approach employs the stochastic bracketing transduction grammar (SBTG) model we recently introduced to replace earlier word alignment channel models, while retaining a bigram language model. The new algorithm in our experience yields major speed improvement with no significant loss of accuracy. 1 Motivation statistical translation model introduced IBM (Brown et al., 1990) views translation as a noisy channel process. Assume, as we do throughout this paper, that the input language is Chinese and the task is to translate into English. The underlying generative model, shown in Figure 1, contains a stochastic English sentence generator whose output is &amp;quot;corrupted&amp;quot; by the translation channel to produce Chinese sentences. In the IBM system, the language model employs simple n-grams, while the translation model employs several sets of parameters as discussed below. Estimation of the parameters has been described elsewhere (Brown et al., 1993). Translation is performed in the reverse direction from generation, as usual for recognition under generative models. For each Chinese sentence e that is to be translated, the system must attempt to find the English sentence e* such that: (1) e* = argmax Pr (el e) = Pr Pr(e) the model, the search for optimal e* is using a heuristic &amp;quot;stack search&amp;quot; A* methods. One of the primary obstacles to making the statistical translation approach practical is slow speed of translation, as performed in A* fashion. This price is paid for the robustness that is obtained by using very flexible language and translation models. The language model allows sentences of arbitrary order and the translation model allows arbitrary word-order permutation. The models employ no structural constraints, relying instead on probability parameters to assign low probabilities to implausible sentences. This exhaustive space, together with massive number of parameters, permits greater modeling accuracy. But while accuracy is enhanced, translation efficiency suffers due to the lack of structure in the hypothesis space. The translation channel is characterized by two sets of parameters: translation and alignment probabilities.&apos; The translation probabilities describe lexical substitution, while alignment probabilities describe word-order permutation. The key problem is that the formulation of alignment a(il the Chinese word in a length-T sentence to map to any poi of a length-V English sentence. So alignments are possible, yielding an exponential space with correspondingly slow search times. Note there are no explicit linguistic grammars in model. Useful methods do exist for incorporating constraints fed in from other preprocessing modules, and some of these modules do employ linguistic grammars. For instance, we previously reported a method for improving search times in channel translation models that exploits bracketing information (Wu and Ng, 1995). If any brackets for the Chinese sentence can be supplied as additional input information, produced for example by preprocessing a modified version of the A*based algorithm can follow the brackets to guide the search heuristically. This strategy appears to produces moderate improvements in search speed and slightly better translations. Such linguistic-preprocessing techniques could models have been constructed by IBM team (Brown et al., 1993). This description corresponds to one of the simplest ones, &amp;quot;Model 2&amp;quot;; search costs for the more complex models are correspondingly higher. 152 stochastic English generator noisy channel English strings Chinese strings direction of generative model direction of translation Figure 1: Channel translation model. also be used with the new model described below, but the issue is independent of our focus here. In this paper we address the underlying assumptions of core channel model itself which does not directly use linguistic structure. A slightly different model is employed for a alignment application by Dagan al. (Dagan, Church, and Gale, 1993). Instead of alignment offset probabilities employed, essentially the positional distance between the English words aligned to two adjacent Chinese words: = i e v the position of the immediately pre- Chinese word and a constant that normalizes for average sentence lengths in different languages. The motivation is that words that are close to each other in the Chinese sentence should tend to be close in the English sentence as well. The size of the parameter set is greatly reduced from the x x (TI x I VI parameters of the alignment probabilities, down to a small set of jkl parameters. However, the search space remains the same. The A*-style stack-decoding approach is in some ways a carryover from the speech recognition architectures that inspired the channel translation model. It has proven highly effective for speech recognition in both accuracy and speed, where the search space contains no order variation since the acoustic and text streams can be assumed to be linearly aligned. But in contrast, for translation models the stack search alone does not adequately compensate for the combinatorially more complex space that results from permitting arbitrary order variations. Indeed, the stack-decoding approach remains impractically slow for translation, and has not achieved the same kind of speed as for speech recognition. The model we describe in this paper, like Dagan al.&apos;s encourages related words to stay together, and reduces the number of parameters used to describe word-order variation. But more importantly, it makes structural assumptions that eliminate large portions of the space of alignments, based on linguistic motivatations. This greatly reduces the search space and makes possible a polynomial-time optimization algorithm. 2 ITG and BTG Overview The new translation model is based on the recently language modeling the model employs a transgrammar BTG (Wu, 1995a), which is special case of transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d). These formalisms were originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. This paper finds they are also useful for the translation system itself. In this section we summarize the main properties of BTGs and ITGs. An ITG consists of context-free productions where symbols come in example I y, a Chinese word and an English transof Any parse tree thus generates two strings, one on the Chinese stream and one on the English stream. Thus, the tree: [ft/I [[7/took [—/a */e W/book]NP [/for ithrou]PP IvP Is produces, for example, the mutual translations: a. [PI ]vp [MTApp ivP Una le [yi ben ni]pp 1vP Is [I [[took [a book]Np you]pp Ivp An additional mechanism accommodates a conservative degree of word-order variation between the two languages. With each production of the gramis associated either a or an respectively denoted as follows: VP [VP PP] VP (VP PP) In the case of a production with straight orientation, the right-hand-side symbols are visited leftto-right for both the Chinese and English streams. But for a production with inverted orientation, the 2Readers of the papers cited above should note that have the roles of English and Chinese here, which helps simplify the presentation of the new translation algorithm.</abstract>
<note confidence="0.35908825">153 BTG all matchings ratio 2 1 1 1.000 1.000 1.000 1.000 0.917 0.750 0.547 0.358 0.212 0.115 0.057 0.026 0.011 0.004 0.002 0.001 0.000 3 1 1</note>
<phone confidence="0.497494">4 2 6 22 90 394 1806 8558 41586 206098 1037718 5293446 27297738 142078746 745387038 3937603038 2 6 24 120 720 5040 40320 362880 3628800 39916800 479001600 6227020800 87178291200 1307674368000 20922789888000</phone>
<note confidence="0.758055230769231">5 6 7 8 9 10 11 12 13 14 15 16 Figure 2: Number of legal word alignments between</note>
<abstract confidence="0.982789099667774">of length and without the BTG restriction. right-hand-side symbols are visited left-to-right for Chinese and right-to-left for English. Thus, the tree: [Ng ([M/for fx/you]pp [*7/took [—/a lbook]NP ]vP )vP Is produces translations with different word order: a. [ft [[1]pp[*7 ]v ]v Is b. [I [[took [a book]NP lvp [for you]PP 1vP Is In the special case of BTGs which are employed in the model presented below, there is only one undifferentiated nonterminal category (aside from the start symbol). Designating this category A, this means all non-lexical productions are of one of these two forms: [A A • • • A (A A • • • A) The degree of word-order flexibility is the critical point. BTGs make a favorable trade-off between efficiency and expressiveness: constraints are strong enough to allow algorithms to operate efficiently, but without so much loss of expressiveness as to hinder useful translation. We summarize here; details are given elsewhere (Wu, 1995b). With regard to efficiency, Figure 2 demonstrates the kind of reduction that BTGs obtain in the space of possible alignments. The number of possible alignments, compared against the unrestricted case where any English word may align to any Chinese position, drops off dramatically for strings longer than four words. (This table makes the simplification of counting only 1-1 matchings and is merely representative.) With regard to expressiveness, we believe that almost all variation in the order of arguments in a frame can be Syntactic frames generally contain four or fewer subconstituents. Figure 2 shows that for the case of four subconstituents, BTGs permit 22 out of the 24 possible alignments. The only prohibited arrangements are &amp;quot;inside-out&amp;quot; transformations (Wu, 1995b), which we have been unable to find any examples of in our corpus. Moreover, extremely distorted alignments can be handled by BTGs (Wu, 1995c), without resorting to the unrestricted-alignment model. The translation expressiveness of BTGs is by no means perfect. They are nonetheless proving very useful in applications and are substantially more feasible than previous models. In our previous corpus analysis applications, any expressiveness limitations were easily tolerable since degradation was graceful. In the present translation application, any expressiveness limitation simply means that certain translations are not considered. For the remainder of the paper, we take advantage of a convenient normal-form theorem (Wu, 1995a) that allows us to assume without loss of generality that the BTG only contains the binary-branching for the non-lexical 3 BTG-Based Search for the Original Models A first approach to improving the translation search is to limit the allowed word alignment patterns to those permitted by a BTG. In this case, Equation (2) is kept as the objective function and the translation can be parameterized similarly to Dagan Church, and Gale, 1993). The effect of BTG restriction is just to constrain the the word-order distortions. A BTG rather than ITG is used since, as we discussed earlier, pure channel translation models operate without explicit grammars, providing no constituent categories around which a more sophisticated ITG could be structured. But the structural constraints of the BTG can improve search efficiency, even without differentiated constituent categories. Just as in the baseline system, we rely on the language and translation models to take up the slack in place of an explicit grammar. this approach, an similar to the one described later can be constructed to replace A* search. 3Note that these points are not directed at free wordorder languages. But in such languages, explicit morphological inflections make role identification and translation easier. conclusion for a caveat. 154 However we do not feel it is worth preserving offset (or alignment or distortion) parameters simply for the sake of preserving the original translation channel model. These parameterizations were only intended to crudely model word-order variation. Instead, the BTG itself can be used directly to probabilistically rank alternative alignments, as described next. 4 Replacing the Channel Model with a SBTG The second possibility is to use a stochastic bracketing transduction grammar (SBTG) in the channel model, replacing the translation model altogether. In a SBTG, a probability is associated with each production. Thus for the normal-form BTG, we have: The translation lexicon is encoded in productions of A A A A A al) [A A] sjy x/e E/Y all translations all vocabulary for all y English vocabulary b(x,y) —+ b(x,e) the third kind. The latter two kinds of productions allow words of either Chinese or English to go unmatched. The SBTG assigns a probability Pr(c, e, q) to all generable trees q and sentence-pairs. In principle it can be used as the translation channel model by normalizing with Pr(e) and integrating out Pr(q) to give Pr(cle) in Equation (2). In practice, a strong language model makes this unnecessary, so we can instead optimize the simpler Viterbi approximation = Pr(c, e, q) Pr(e) To complete the picture we add a bigram model = for the English language model Offset, alignment, or distortion parameters are eliminated. A large part of the plicit function of such parameters—to prevent alignments where too many frame arguments become separated—is rendered unnecessary by the BTG&apos;s structural constraints, which prohibit many such configurations altogether. Another part of the parameters&apos; purpose is subsumed by the SBTG&apos;s probabilities all and a0, which can be set to prefer straight or inverted orientation depending on the language pair. As in the original models, the language model heavily influences the remaining ordering decisions. Matters are complicated by the presence of the bigram model in the objective function (which wordalignment models, as opposed to translation models, do not need to deal with). As in our word-alignment model, the translation algorithm optimizes Equation (4) via dynamic programming, similar to chart parsing (Earley, 1970) but with a probabilistic objective function as for HMMs (Viterbi, 1967). But unlike the word-alignment model, to accommodate the bigram model we introduce indexes in the recurrence not only on subtrees over the source Chinese string, but also on the delimiting words of the target English substrings. Another feature of the algorithm is that segmentation of the Chinese input sentence is performed in parallel with the translation search. Conventional architectures for Chinese NLP generally attempt to identify word boundaries as a preprocess- Whenever the segmentation preprocessor prematurely commits to an inappropriate segmentation, difficulties are created for later stages. This problem is particularly acute for translation, since the decision as to whether to regard a sequence as a single unit depends on whether its components can be translated compositionally. This in turn often depends on what the target language is. In other words, the Chinese cannot be appropriately segmented except with respect to the target language of of correct segmentation. The algorithm is given below. A few remarks the notation used: denotes the subseof Chinese tokens cs+2, , ct. We use denote the set of English words that are translations the Chinese word created by taking all in c, ttogether. t) the set of English words that are translations of any of the words anywhere within e, Note also that we assume the explicit sentence-start and sentencetokens = &lt;s&gt; and = &lt;Is&gt;, which makes the algorithm description more parsimonious. Finally, the argmax operator is generalized to vector notation to accomodate multiple indices. 1. Initialization = bi(c3 .t/Y), 0&lt;s&lt;t&lt;T Y E E(s..t) Recursion For all t, y, z that —1&lt;s&lt;t&lt;T+1 yEE(8,t) 6 6 1 - &gt; &gt; 0 ifetyz &gt; Olslyz &gt; 0 otherwise contains no spaces to delimit words; any spaces in the earlier examples are artifacts of the parse tree brackets. Ostyz Ostyz 155 Category Original A* Bracket A* BTG-Channel Correct 67.5 69.8 68.2 Incorrect 32.5 30.2 31.8 Figure 3: Translation accuracy (percentage correct). where max stzz gY z s&lt;s&lt;t EE(s,$) zEE(s,t) argmax 6s1yy(k) z s&lt;s.o YeE(s,t) zEE(3,$) 3. Reconstruction Initialize by setting the root the parse tree to =_- (-1,T— 1, &lt;s&gt;, &lt;/ s&gt;). remaining descendants in the optimal parse tree are given recursively for any = (s,t,y, z) a probabilistic optimization problem. But perhaps most importantly, our goal is to constrain as tightly as possible the space of possible transduction relationships between two languages with fixed wordorder, making no other language-specific assumptions; we are thus driven to seek a kind of languageuniversal property. In contrast, the ID/LP work was directed at parsing a single language with free word-order. As a consequence, it would be necessary to enumerate a specific set of linear-precedence (LP) relations for the language, and moreover the immediate-dominance (ID) productions would typically be more complex than binary-branching. This significantly increases time complexity, compared to our BTG model. Although it is not mentioned in their paper, the time complexity for ID/LP parsing rises exponentially with the length of production right-hand-sides, due to the number of permutations. ITGs avoid this with their restriction to inversions, rather than permutations, and BTGs further minimize the grammar size. We have also confirmed empirically that our models would not be feasible under general permutations. argmax gyz s&lt;s&lt;t YEE(3,$) ZEE(St) max 42° 63szz 55tyY 3&lt;s&lt;t YEE(s,t) zEE(3,$) 6.2y. LEFT(g) = RIGHT(q) = Assume the number of translations per word is bounded by some constant. Then the maximum size proportional to — s. asymptotic time complexity for the translation algorithm is thus by that in practice, actual performance is improved by the sparseness of the translation matrix. An interesting connection has been suggested to direct parsing for ID/LP grammars (Shieber, 1984), in which word-order variations would be accommodated by the parser, and related ideas for generation of free word-order languages in the TAG framework (Joshi, 1987). Our work differs from the ID/LP work in several important respects. First, we are not merely parsing, but translating with a bigram language model. Also, of course, we are dealing with 5 Results The algorithm above was tested in the SILC translation system. The translation lexicon was largely constructed by training on the HKUST English-Chinese Parallel Bilingual Corpus, which consists of governmental transcripts. The corpus was sentence-aligned statistically (Wu, 1994); Chinese words and collocations were extracted (Fung and Wu, 1994; Wu and Fung, 1994); then translation pairs were learned via an EM procedure (Wu and Xia, 1995). The resulting English vocabulary is approximately 6,500 words and the Chinese vocabulary is approximately 5,500 words, with a many-to-many translation mapping averaging 2.25 Chinese translations per English word. Due to the unsupervised training, the translation lexicon contains noise and is only at about 86% percent weighted precision. With regard to accuracy, we merely wish to demonstrate that for statistical MT, accuracy is not significantly compromised by substituting our efficient optimization algorithm. It is not our purpose here to argue that accuracy can be increased with our model. No morphological processing has been used to correct the output, and until now we have only been testing with a bigram model trained on extremely limited samples. A coarse evaluation of NIL z) NIL NIL z) NIL if t--3&lt;1 = = otherwise if t-3&lt;i. = = 0 otherwise</abstract>
<note confidence="0.931007235294118">156 Input: Output: Corpus: Input: Output: Corpus: Input: Output: Corpus: Input: Output: Corpus: Input: Output: Corpus: (Xiang gang de an ding fan rong shi WO men sheng hu6 fang shi de zhi</note>
<abstract confidence="0.974779090909091">Hong Kong&apos;s stabilize boom is us life styles&apos;s pillar. Our prosperity and stability underpin our way of life. 44NMINVitta (Ben gang de jing ji qian jing yu zhang gu6, te bie shi guang dong sheng de jing ji qian jing xi xi xiang guan.) Hong Kong&apos;s economic foreground with China, particular Guangdong province&apos;s economic foreground vitally interrelated. Our economic future is inextricably bound up with China, and with Guangdong Province in particular. firdtittifirg.g. (WO win quin zhi chi ta de yi jian.) I absolutely uphold his views. I fully support his views. Mt (Zhe xie an pai ke jia qiing wo men ri hOu wei chi jin r6ng wen ding de neng 11.) These arrangements can enforce us future kept financial stabilization&apos;s competency. These arrangements will enhance our ability to maintain monetary stability in the years to come. tWt, ftRAMT.A1t: ZOM, ftrig*IliTtAtIttM3R/OIAMPfiEfissi R. wa zai ke yl ken ding de shuO, wO men jiang hul ti gong wei di dao ge xiang zhii yao mu biao suO xil de jing fei.) However, I now can certainty&apos;s say, will provide for us attain various dominant goal necessary&apos;s current expenditure. The consultation process is continuing but I can confirm now that the necessary funds will be made available to meet the key targets. Figure 4: Example translation outputs. translation accuracy was performed on a random sample drawn from Chinese sentences of fewer than 20 words from the parallel corpus, the results of which are shown in Figure 3. We have judged only whether the correct meaning (as determined by the corresponding English sentence in the parallel corpus) is conveyed by the translation, paying particular attention to word order, but otherwise ignoring morphological and function word choices. For comparison, the accuracies from the A*-based systems are also shown. There is no significant difference in the accuracy. Some examples of the output are shown in Figure 4. On the other hand, the new algorithm has indeed proven to be much faster. At present we are unable to use direct measurement to compare the speed of the systems meaningfully, because of vast implementational differences between the systems. However, the order-of-magnitude improvements are immediately apparent. In the earlier system, translation of single sentences required on the order of hours (Sun Sparc 10 workstations). In contrast the new algorithm generally takes less than one minute—usually substantially less—with no special optimization of the code. 6 Conclusion We have introduced a new algorithm for the runtime optimization step in statistical machine translation systems, whose polynomial-time complexity addresses one of the primary obstacles to practicality facing statistical MT. The underlying model for the algorithm is a combination of the stochastic BTG and bigram models. The improvement in speed does not appear to impair accuracy significantly. We have implemented a version that accepts ITGs rather than BTGs, and plan to experiment with more heavily structured models. However, it is important to note that the search complexity rises exponentially rather than polynomially with the size of the grammar, just as for context-free parsing (Barton, Berwick, and Ristad, 1987). This is not relevant to the BTG-based model we have described since its grammar size is fixed; in fact the BTG&apos;s minimal grammar size has been an important advantage over more linguistically-motivated ITG-based models. 157 We have also implemented a generalized version that accepts arbitrary grammars not restricted to normal form, with two motivations. The pragmatic benefit is that structured grammars become easier to write, and more concise. The expressiveness benefit is that a wider family of probability distributions can be written. As stated earlier, the normal form theorem guarantees that the same set of shapes will be explored by our search algorithm, regardless of whether a binary-branching BTG or an arbitrary BTG is used. But it may sometimes be useful to place probabilities on n-ary productions that vary with n in a way that cannot be expressed by composing binary productions; for example one might wish to encourage longer straight productions. The generalized version permits such strategies. Currently we are evaluating robustness extensions of the algorithm that permit words suggested by the language model to be inserted in the output sentence, which the original A* algorithms permitted. Acknowledgements Thanks to an anonymous referee for valuable comments, and to the SILC group members: Xuanyin Xia, Eva Wai-Man Fong, Cindy Ng, Hong-sing Wong, and Daniel Ka-Leung Chan. Many thanks also to Kathleen McKeown and her group for discussion, support, and assistance.</abstract>
<note confidence="0.76914735443038">References Barton, G. Edward, Robert C. Berwick, and Sven Ristad. 1987. Complexand Natural Language. Press, Cambridge, MA. Brown, Peter F., John Cocke, Stephen A. DellaPietra, Vincent J. DellaPietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine Linguistics, 16(2):29- 85. Brown, Peter F., Stephen A. DellaPietra, Vincent J. DellaPietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: estimation. Linguis- Dagan, Ido, Kenneth W. Church, and William A. Gale. 1993. Robust bilingual word alignment machine aided translation. In of Workshop on Very Large Corpora, 1-8, Columbus, OH, June. Earley, Jay. 1970. An efficient context-free parsalgorithm. of the Associafor Computing Machinery, Fung, Pascale and Dekai Wu. 1994. Statistical augof a Chinese machine-readable dictio- In of the Second Annual Workon Very Large Corpora, 69-85, Kyoto, August. Joshi, Aravind K. 1987. Word-order variation in language generation. In of AAAI-87, Sixth National Conference on Artificial 550-555. Shieber, Stuart M. 1984. Direct parsing of ID/LP and Philosophy, 7:135- 154. Viterbi, Andrew J. 1967. Error bounds for convolutional codes and an asymptotically optimal decodalgorithm. Transactions on Information Wu, Dekai. 1994. Aligning a parallel English- Chinese corpus statistically with lexical criteria. of the 32nd Annual Conference of the Association for Computational Linguistics, pages 80-87, Las Cruces, New Mexico, June. Wu, Dekai. 1995a. An algorithm for simultaneously bracketing parallel texts by aligning words. In Proceedings of the 33rd Annual Conference of the for Computational Linguistics, 244-251, Cambridge, Massachusetts, June. Wu, Dekai. 1995b. Grammarless extraction of phrasal translation examples from parallel texts. Proceedings of the Sixth International Conference on Theoretical and Methodological Isin Machine Translation, 2, pages 354-372, Leuven, Belgium, July. Wu, Dekai. 1995c. Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel cor- In of IJCAI-95, Fourteenth ternational Joint Conference on Artificial Intelli- 1328-1334, Montreal, August. Wu, Dekai. 1995d. Trainable coarse bilingual gramfor parallel text bracketing. In Proceedings of the Third Annual Workshop on Very Large 69-81, Cambridge, Massachusetts, June. Wu, Dekai and Pascale Fung. 1994. Improving Chinese tokenization with linguistic filters on stalexical acquisition. In of the Fourth Conference on Applied Natural Language 180-181, Stuttgart, October. Wu, Dekai and Cindy Ng. 1995. Using brackets to improve search for statistical machine transla- In Pacific Asia Conference on Information and Computation, 195-204, Hong Kong, December. Wu, Dekai and Xuanyin Xia. 1995. Large-scale automatic extraction of an English-Chinese lexicon. Translation, 158</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Edward Barton</author>
<author>Robert C Berwick</author>
<author>Eric Sven Ristad</author>
</authors>
<date>1987</date>
<booktitle>Computational Complexity and Natural Language.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="24473" citStr="Barton, Berwick, and Ristad, 1987" startWordPosition="3930" endWordPosition="3935"> translation systems, whose polynomial-time complexity addresses one of the primary obstacles to practicality facing statistical MT. The underlying model for the algorithm is a combination of the stochastic BTG and bigram models. The improvement in speed does not appear to impair accuracy significantly. We have implemented a version that accepts ITGs rather than BTGs, and plan to experiment with more heavily structured models. However, it is important to note that the search complexity rises exponentially rather than polynomially with the size of the grammar, just as for context-free parsing (Barton, Berwick, and Ristad, 1987). This is not relevant to the BTG-based model we have described since its grammar size is fixed; in fact the BTG&apos;s minimal grammar size has been an important advantage over more linguistically-motivated ITG-based models. 157 We have also implemented a generalized version that accepts arbitrary grammars not restricted to normal form, with two motivations. The pragmatic benefit is that structured grammars become easier to write, and more concise. The expressiveness benefit is that a wider family of probability distributions can be written. As stated earlier, the normal form theorem guarantees t</context>
</contexts>
<marker>Barton, Berwick, Ristad, 1987</marker>
<rawString>Barton, G. Edward, Robert C. Berwick, and Eric Sven Ristad. 1987. Computational Complexity and Natural Language. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A DellaPietra</author>
<author>Vincent J DellaPietra</author>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--2</pages>
<contexts>
<context position="801" citStr="Brown et al., 1990" startWordPosition="110" endWordPosition="113">.ust .hk Abstract We introduce a polynomial-time algorithm for statistical machine translation. This algorithm can be used in place of the expensive, slow best-first search strategies in current statistical translation architectures. The approach employs the stochastic bracketing transduction grammar (SBTG) model we recently introduced to replace earlier word alignment channel models, while retaining a bigram language model. The new algorithm in our experience yields major speed improvement with no significant loss of accuracy. 1 Motivation The statistical translation model introduced by IBM (Brown et al., 1990) views translation as a noisy channel process. Assume, as we do throughout this paper, that the input language is Chinese and the task is to translate into English. The underlying generative model, shown in Figure 1, contains a stochastic English sentence generator whose output is &amp;quot;corrupted&amp;quot; by the translation channel to produce Chinese sentences. In the IBM system, the language model employs simple n-grams, while the translation model employs several sets of parameters as discussed below. Estimation of the parameters has been described elsewhere (Brown et al., 1993). Translation is performed</context>
</contexts>
<marker>Brown, Cocke, DellaPietra, DellaPietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Brown, Peter F., John Cocke, Stephen A. DellaPietra, Vincent J. DellaPietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):29-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A DellaPietra</author>
<author>Vincent J DellaPietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="1375" citStr="Brown et al., 1993" startWordPosition="199" endWordPosition="202"> model introduced by IBM (Brown et al., 1990) views translation as a noisy channel process. Assume, as we do throughout this paper, that the input language is Chinese and the task is to translate into English. The underlying generative model, shown in Figure 1, contains a stochastic English sentence generator whose output is &amp;quot;corrupted&amp;quot; by the translation channel to produce Chinese sentences. In the IBM system, the language model employs simple n-grams, while the translation model employs several sets of parameters as discussed below. Estimation of the parameters has been described elsewhere (Brown et al., 1993). Translation is performed in the reverse direction from generation, as usual for recognition under generative models. For each Chinese sentence e that is to be translated, the system must attempt to find the English sentence e* such that: (1) e* = argmax Pr (el e) (2) = argmax Pr (cle) Pr(e) In the IBM model, the search for the optimal e* is performed using a best-first heuristic &amp;quot;stack search&amp;quot; similar to A* methods. One of the primary obstacles to making the statistical translation approach practical is slow speed of translation, as performed in A* fashion. This price is paid for the robustn</context>
<context position="3922" citStr="Brown et al., 1993" startWordPosition="595" endWordPosition="598">nce, we previously reported a method for improving search times in channel translation models that exploits bracketing information (Wu and Ng, 1995). If any brackets for the Chinese sentence can be supplied as additional input information, produced for example by a preprocessing stage, a modified version of the A*- based algorithm can follow the brackets to guide the search heuristically. This strategy appears to produces moderate improvements in search speed and slightly better translations. Such linguistic-preprocessing techniques could &apos;Various models have been constructed by the IBM team (Brown et al., 1993). This description corresponds to one of the simplest ones, &amp;quot;Model 2&amp;quot;; search costs for the more complex models are correspondingly higher. 152 stochastic English generator noisy channel English strings Chinese strings direction of generative model direction of translation Figure 1: Channel translation model. also be used with the new model described below, but the issue is independent of our focus here. In this paper we address the underlying assumptions of core channel model itself which does not directly use linguistic structure. A slightly different model is employed for a word alignment a</context>
</contexts>
<marker>Brown, DellaPietra, DellaPietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen A. DellaPietra, Vincent J. DellaPietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<title>Robust bilingual word alignment for machine aided translation.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora,</booktitle>
<pages>1--8</pages>
<location>Columbus, OH,</location>
<contexts>
<context position="4579" citStr="Dagan, Church, and Gale, 1993" startWordPosition="695" endWordPosition="700">ds to one of the simplest ones, &amp;quot;Model 2&amp;quot;; search costs for the more complex models are correspondingly higher. 152 stochastic English generator noisy channel English strings Chinese strings direction of generative model direction of translation Figure 1: Channel translation model. also be used with the new model described below, but the issue is independent of our focus here. In this paper we address the underlying assumptions of core channel model itself which does not directly use linguistic structure. A slightly different model is employed for a word alignment application by Dagan et al. (Dagan, Church, and Gale, 1993). Instead of alignment probabilities, offset probabilities o(k) are employed, where k is essentially the positional distance between the English words aligned to two adjacent Chinese words: (3) k = i — (A(jp,) (j jp„,,)N) wherepr e v is the position of the immediately preceding Chinese word and N is a constant that normalizes for average sentence lengths in different languages. The motivation is that words that are close to each other in the Chinese sentence should tend to be close in the English sentence as well. The size of the parameter set is greatly reduced from the x x (TI x I VI parame</context>
<context position="11680" citStr="Dagan, Church, and Gale, 1993" startWordPosition="1849" endWordPosition="1853"> means that certain translations are not considered. For the remainder of the paper, we take advantage of a convenient normal-form theorem (Wu, 1995a) that allows us to assume without loss of generality that the BTG only contains the binary-branching form for the non-lexical productions.4 3 BTG-Based Search for the Original Models A first approach to improving the translation search is to limit the allowed word alignment patterns to those permitted by a BTG. In this case, Equation (2) is kept as the objective function and the translation channel can be parameterized similarly to Dagan ei al. (Dagan, Church, and Gale, 1993). The effect of the BTG restriction is just to constrain the shapes of the word-order distortions. A BTG rather than ITG is used since, as we discussed earlier, pure channel translation models operate without explicit grammars, providing no constituent categories around which a more sophisticated ITG could be structured. But the structural constraints of the BTG can improve search efficiency, even without differentiated constituent categories. Just as in the baseline system, we rely on the language and translation models to take up the slack in place of an explicit grammar. In this approach, </context>
</contexts>
<marker>Dagan, Church, Gale, 1993</marker>
<rawString>Dagan, Ido, Kenneth W. Church, and William A. Gale. 1993. Robust bilingual word alignment for machine aided translation. In Proceedings of the Workshop on Very Large Corpora, pages 1-8, Columbus, OH, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<pages>13--2</pages>
<contexts>
<context position="14981" citStr="Earley, 1970" startWordPosition="2390" endWordPosition="2391">ogether. Another part of the parameters&apos; purpose is subsumed by the SBTG&apos;s probabilities all and a0, which can be set to prefer straight or inverted orientation depending on the language pair. As in the original models, the language model heavily influences the remaining ordering decisions. Matters are complicated by the presence of the bigram model in the objective function (which wordalignment models, as opposed to translation models, do not need to deal with). As in our word-alignment model, the translation algorithm optimizes Equation (4) via dynamic programming, similar to chart parsing (Earley, 1970) but with a probabilistic objective function as for HMMs (Viterbi, 1967). But unlike the word-alignment model, to accommodate the bigram model we introduce indexes in the recurrence not only on subtrees over the source Chinese string, but also on the delimiting words of the target English substrings. Another feature of the algorithm is that segmentation of the Chinese input sentence is performed in parallel with the translation search. Conventional architectures for Chinese NLP generally attempt to identify word boundaries as a preprocessing stage.5 Whenever the segmentation preprocessor prema</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, Jay. 1970. An efficient context-free parsing algorithm. Communications of the Association for Computing Machinery, 13(2):94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Dekai Wu</author>
</authors>
<title>Statistical augmentation of a Chinese machine-readable dictionary.</title>
<date>1994</date>
<booktitle>In Proceedings of the Second Annual Workshop on Very Large Corpora,</booktitle>
<pages>69--85</pages>
<location>Kyoto,</location>
<contexts>
<context position="20006" citStr="Fung and Wu, 1994" startWordPosition="3205" endWordPosition="3208"> generation of free word-order languages in the TAG framework (Joshi, 1987). Our work differs from the ID/LP work in several important respects. First, we are not merely parsing, but translating with a bigram language model. Also, of course, we are dealing with 5 Results The algorithm above was tested in the SILC translation system. The translation lexicon was largely constructed by training on the HKUST English-Chinese Parallel Bilingual Corpus, which consists of governmental transcripts. The corpus was sentence-aligned statistically (Wu, 1994); Chinese words and collocations were extracted (Fung and Wu, 1994; Wu and Fung, 1994); then translation pairs were learned via an EM procedure (Wu and Xia, 1995). The resulting English vocabulary is approximately 6,500 words and the Chinese vocabulary is approximately 5,500 words, with a many-to-many translation mapping averaging 2.25 Chinese translations per English word. Due to the unsupervised training, the translation lexicon contains noise and is only at about 86% percent weighted precision. With regard to accuracy, we merely wish to demonstrate that for statistical MT, accuracy is not significantly compromised by substituting our efficient optimizatio</context>
</contexts>
<marker>Fung, Wu, 1994</marker>
<rawString>Fung, Pascale and Dekai Wu. 1994. Statistical augmentation of a Chinese machine-readable dictionary. In Proceedings of the Second Annual Workshop on Very Large Corpora, pages 69-85, Kyoto, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Word-order variation in natural language generation.</title>
<date>1987</date>
<booktitle>In Proceedings of AAAI-87, Sixth National Conference on Artificial Intelligence,</booktitle>
<pages>550--555</pages>
<contexts>
<context position="19464" citStr="Joshi, 1987" startWordPosition="3122" endWordPosition="3123">(3,$) 6.2y. LEFT(g) = RIGHT(q) = Assume the number of translations per word is bounded by some constant. Then the maximum size of E(s,t) is proportional to t — s. The asymptotic time complexity for the translation algorithm is thus bounded by 0(T7). Note that in practice, actual performance is improved by the sparseness of the translation matrix. An interesting connection has been suggested to direct parsing for ID/LP grammars (Shieber, 1984), in which word-order variations would be accommodated by the parser, and related ideas for generation of free word-order languages in the TAG framework (Joshi, 1987). Our work differs from the ID/LP work in several important respects. First, we are not merely parsing, but translating with a bigram language model. Also, of course, we are dealing with 5 Results The algorithm above was tested in the SILC translation system. The translation lexicon was largely constructed by training on the HKUST English-Chinese Parallel Bilingual Corpus, which consists of governmental transcripts. The corpus was sentence-aligned statistically (Wu, 1994); Chinese words and collocations were extracted (Fung and Wu, 1994; Wu and Fung, 1994); then translation pairs were learned </context>
</contexts>
<marker>Joshi, 1987</marker>
<rawString>Joshi, Aravind K. 1987. Word-order variation in natural language generation. In Proceedings of AAAI-87, Sixth National Conference on Artificial Intelligence, pages 550-555.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Direct parsing of ID/LP grammars.</title>
<date>1984</date>
<journal>Linguistics and Philosophy,</journal>
<pages>7--135</pages>
<contexts>
<context position="19298" citStr="Shieber, 1984" startWordPosition="3094" endWordPosition="3095">ally that our models would not be feasible under general permutations. • argmax a11 bssyy 6,stz2 gyz s&lt;s&lt;t YEE(3,$) ZEE(St) • max 42° 63szz 55tyY gYz 3&lt;s&lt;t YEE(s,t) zEE(3,$) 6.2y. LEFT(g) = RIGHT(q) = Assume the number of translations per word is bounded by some constant. Then the maximum size of E(s,t) is proportional to t — s. The asymptotic time complexity for the translation algorithm is thus bounded by 0(T7). Note that in practice, actual performance is improved by the sparseness of the translation matrix. An interesting connection has been suggested to direct parsing for ID/LP grammars (Shieber, 1984), in which word-order variations would be accommodated by the parser, and related ideas for generation of free word-order languages in the TAG framework (Joshi, 1987). Our work differs from the ID/LP work in several important respects. First, we are not merely parsing, but translating with a bigram language model. Also, of course, we are dealing with 5 Results The algorithm above was tested in the SILC translation system. The translation lexicon was largely constructed by training on the HKUST English-Chinese Parallel Bilingual Corpus, which consists of governmental transcripts. The corpus was</context>
</contexts>
<marker>Shieber, 1984</marker>
<rawString>Shieber, Stuart M. 1984. Direct parsing of ID/LP grammars. Linguistics and Philosophy, 7:135-154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimal decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>13--260</pages>
<contexts>
<context position="15053" citStr="Viterbi, 1967" startWordPosition="2402" endWordPosition="2403">G&apos;s probabilities all and a0, which can be set to prefer straight or inverted orientation depending on the language pair. As in the original models, the language model heavily influences the remaining ordering decisions. Matters are complicated by the presence of the bigram model in the objective function (which wordalignment models, as opposed to translation models, do not need to deal with). As in our word-alignment model, the translation algorithm optimizes Equation (4) via dynamic programming, similar to chart parsing (Earley, 1970) but with a probabilistic objective function as for HMMs (Viterbi, 1967). But unlike the word-alignment model, to accommodate the bigram model we introduce indexes in the recurrence not only on subtrees over the source Chinese string, but also on the delimiting words of the target English substrings. Another feature of the algorithm is that segmentation of the Chinese input sentence is performed in parallel with the translation search. Conventional architectures for Chinese NLP generally attempt to identify word boundaries as a preprocessing stage.5 Whenever the segmentation preprocessor prematurely commits to an inappropriate segmentation, difficulties are create</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Viterbi, Andrew J. 1967. Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. IEEE Transactions on Information Theory, 13:260-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Aligning a parallel EnglishChinese corpus statistically with lexical criteria.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>80--87</pages>
<location>Las Cruces, New Mexico,</location>
<contexts>
<context position="19940" citStr="Wu, 1994" startWordPosition="3196" endWordPosition="3197">would be accommodated by the parser, and related ideas for generation of free word-order languages in the TAG framework (Joshi, 1987). Our work differs from the ID/LP work in several important respects. First, we are not merely parsing, but translating with a bigram language model. Also, of course, we are dealing with 5 Results The algorithm above was tested in the SILC translation system. The translation lexicon was largely constructed by training on the HKUST English-Chinese Parallel Bilingual Corpus, which consists of governmental transcripts. The corpus was sentence-aligned statistically (Wu, 1994); Chinese words and collocations were extracted (Fung and Wu, 1994; Wu and Fung, 1994); then translation pairs were learned via an EM procedure (Wu and Xia, 1995). The resulting English vocabulary is approximately 6,500 words and the Chinese vocabulary is approximately 5,500 words, with a many-to-many translation mapping averaging 2.25 Chinese translations per English word. Due to the unsupervised training, the translation lexicon contains noise and is only at about 86% percent weighted precision. With regard to accuracy, we merely wish to demonstrate that for statistical MT, accuracy is not s</context>
</contexts>
<marker>Wu, 1994</marker>
<rawString>Wu, Dekai. 1994. Aligning a parallel EnglishChinese corpus statistically with lexical criteria. In Proceedings of the 32nd Annual Conference of the Association for Computational Linguistics, pages 80-87, Las Cruces, New Mexico, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>An algorithm for simultaneously bracketing parallel texts by aligning words.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>244--251</pages>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="6638" citStr="Wu, 1995" startWordPosition="1027" endWordPosition="1028">his paper, like Dagan et al.&apos;s model, encourages related words to stay together, and reduces the number of parameters used to describe word-order variation. But more importantly, it makes structural assumptions that eliminate large portions of the space of alignments, based on linguistic motivatations. This greatly reduces the search space and makes possible a polynomial-time optimization algorithm. 2 ITG and BTG Overview The new translation model is based on the recently introduced bilingual language modeling approach. Specifically, the model employs a bracketing transduction grammar or BTG (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d). These formalisms were originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. This paper finds they are also useful for the translation system itself. In this section we summarize the main properties of BTGs and ITGs. An ITG consists of context-free productions where terminal symbols come in couples, for example x I y, where x is a Chinese word and y is an English translation of x.2 Any parse tree thus ge</context>
<context position="9660" citStr="Wu, 1995" startWordPosition="1540" endWordPosition="1541">TGs which are employed in the model presented below, there is only one undifferentiated nonterminal category (aside from the start symbol). Designating this category A, this means all non-lexical productions are of one of these two forms: A [A A • • • A] A (A A • • • A) The degree of word-order flexibility is the critical point. BTGs make a favorable trade-off between efficiency and expressiveness: constraints are strong enough to allow algorithms to operate efficiently, but without so much loss of expressiveness as to hinder useful translation. We summarize here; details are given elsewhere (Wu, 1995b). With regard to efficiency, Figure 2 demonstrates the kind of reduction that BTGs obtain in the space of possible alignments. The number of possible alignments, compared against the unrestricted case where any English word may align to any Chinese position, drops off dramatically for strings longer than four words. (This table makes the simplification of counting only 1-1 matchings and is merely representative.) With regard to expressiveness, we believe that almost all variation in the order of arguments in a syntactic frame can be accommodated.3 Syntactic frames generally contain four or f</context>
<context position="11199" citStr="Wu, 1995" startWordPosition="1774" endWordPosition="1775">y BTGs (Wu, 1995c), without resorting to the unrestricted-alignment model. The translation expressiveness of BTGs is by no means perfect. They are nonetheless proving very useful in applications and are substantially more feasible than previous models. In our previous corpus analysis applications, any expressiveness limitations were easily tolerable since degradation was graceful. In the present translation application, any expressiveness limitation simply means that certain translations are not considered. For the remainder of the paper, we take advantage of a convenient normal-form theorem (Wu, 1995a) that allows us to assume without loss of generality that the BTG only contains the binary-branching form for the non-lexical productions.4 3 BTG-Based Search for the Original Models A first approach to improving the translation search is to limit the allowed word alignment patterns to those permitted by a BTG. In this case, Equation (2) is kept as the objective function and the translation channel can be parameterized similarly to Dagan ei al. (Dagan, Church, and Gale, 1993). The effect of the BTG restriction is just to constrain the shapes of the word-order distortions. A BTG rather than I</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Wu, Dekai. 1995a. An algorithm for simultaneously bracketing parallel texts by aligning words. In Proceedings of the 33rd Annual Conference of the Association for Computational Linguistics, pages 244-251, Cambridge, Massachusetts, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Grammarless extraction of phrasal translation examples from parallel texts.</title>
<date>1995</date>
<booktitle>In TMI-95, Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<volume>2</volume>
<pages>354--372</pages>
<location>Leuven, Belgium,</location>
<contexts>
<context position="6638" citStr="Wu, 1995" startWordPosition="1027" endWordPosition="1028">his paper, like Dagan et al.&apos;s model, encourages related words to stay together, and reduces the number of parameters used to describe word-order variation. But more importantly, it makes structural assumptions that eliminate large portions of the space of alignments, based on linguistic motivatations. This greatly reduces the search space and makes possible a polynomial-time optimization algorithm. 2 ITG and BTG Overview The new translation model is based on the recently introduced bilingual language modeling approach. Specifically, the model employs a bracketing transduction grammar or BTG (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d). These formalisms were originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. This paper finds they are also useful for the translation system itself. In this section we summarize the main properties of BTGs and ITGs. An ITG consists of context-free productions where terminal symbols come in couples, for example x I y, where x is a Chinese word and y is an English translation of x.2 Any parse tree thus ge</context>
<context position="9660" citStr="Wu, 1995" startWordPosition="1540" endWordPosition="1541">TGs which are employed in the model presented below, there is only one undifferentiated nonterminal category (aside from the start symbol). Designating this category A, this means all non-lexical productions are of one of these two forms: A [A A • • • A] A (A A • • • A) The degree of word-order flexibility is the critical point. BTGs make a favorable trade-off between efficiency and expressiveness: constraints are strong enough to allow algorithms to operate efficiently, but without so much loss of expressiveness as to hinder useful translation. We summarize here; details are given elsewhere (Wu, 1995b). With regard to efficiency, Figure 2 demonstrates the kind of reduction that BTGs obtain in the space of possible alignments. The number of possible alignments, compared against the unrestricted case where any English word may align to any Chinese position, drops off dramatically for strings longer than four words. (This table makes the simplification of counting only 1-1 matchings and is merely representative.) With regard to expressiveness, we believe that almost all variation in the order of arguments in a syntactic frame can be accommodated.3 Syntactic frames generally contain four or f</context>
<context position="11199" citStr="Wu, 1995" startWordPosition="1774" endWordPosition="1775">y BTGs (Wu, 1995c), without resorting to the unrestricted-alignment model. The translation expressiveness of BTGs is by no means perfect. They are nonetheless proving very useful in applications and are substantially more feasible than previous models. In our previous corpus analysis applications, any expressiveness limitations were easily tolerable since degradation was graceful. In the present translation application, any expressiveness limitation simply means that certain translations are not considered. For the remainder of the paper, we take advantage of a convenient normal-form theorem (Wu, 1995a) that allows us to assume without loss of generality that the BTG only contains the binary-branching form for the non-lexical productions.4 3 BTG-Based Search for the Original Models A first approach to improving the translation search is to limit the allowed word alignment patterns to those permitted by a BTG. In this case, Equation (2) is kept as the objective function and the translation channel can be parameterized similarly to Dagan ei al. (Dagan, Church, and Gale, 1993). The effect of the BTG restriction is just to constrain the shapes of the word-order distortions. A BTG rather than I</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Wu, Dekai. 1995b. Grammarless extraction of phrasal translation examples from parallel texts. In TMI-95, Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation, volume 2, pages 354-372, Leuven, Belgium, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI-95, Fourteenth .International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1328--1334</pages>
<location>Montreal,</location>
<contexts>
<context position="6638" citStr="Wu, 1995" startWordPosition="1027" endWordPosition="1028">his paper, like Dagan et al.&apos;s model, encourages related words to stay together, and reduces the number of parameters used to describe word-order variation. But more importantly, it makes structural assumptions that eliminate large portions of the space of alignments, based on linguistic motivatations. This greatly reduces the search space and makes possible a polynomial-time optimization algorithm. 2 ITG and BTG Overview The new translation model is based on the recently introduced bilingual language modeling approach. Specifically, the model employs a bracketing transduction grammar or BTG (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d). These formalisms were originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. This paper finds they are also useful for the translation system itself. In this section we summarize the main properties of BTGs and ITGs. An ITG consists of context-free productions where terminal symbols come in couples, for example x I y, where x is a Chinese word and y is an English translation of x.2 Any parse tree thus ge</context>
<context position="9660" citStr="Wu, 1995" startWordPosition="1540" endWordPosition="1541">TGs which are employed in the model presented below, there is only one undifferentiated nonterminal category (aside from the start symbol). Designating this category A, this means all non-lexical productions are of one of these two forms: A [A A • • • A] A (A A • • • A) The degree of word-order flexibility is the critical point. BTGs make a favorable trade-off between efficiency and expressiveness: constraints are strong enough to allow algorithms to operate efficiently, but without so much loss of expressiveness as to hinder useful translation. We summarize here; details are given elsewhere (Wu, 1995b). With regard to efficiency, Figure 2 demonstrates the kind of reduction that BTGs obtain in the space of possible alignments. The number of possible alignments, compared against the unrestricted case where any English word may align to any Chinese position, drops off dramatically for strings longer than four words. (This table makes the simplification of counting only 1-1 matchings and is merely representative.) With regard to expressiveness, we believe that almost all variation in the order of arguments in a syntactic frame can be accommodated.3 Syntactic frames generally contain four or f</context>
<context position="11199" citStr="Wu, 1995" startWordPosition="1774" endWordPosition="1775">y BTGs (Wu, 1995c), without resorting to the unrestricted-alignment model. The translation expressiveness of BTGs is by no means perfect. They are nonetheless proving very useful in applications and are substantially more feasible than previous models. In our previous corpus analysis applications, any expressiveness limitations were easily tolerable since degradation was graceful. In the present translation application, any expressiveness limitation simply means that certain translations are not considered. For the remainder of the paper, we take advantage of a convenient normal-form theorem (Wu, 1995a) that allows us to assume without loss of generality that the BTG only contains the binary-branching form for the non-lexical productions.4 3 BTG-Based Search for the Original Models A first approach to improving the translation search is to limit the allowed word alignment patterns to those permitted by a BTG. In this case, Equation (2) is kept as the objective function and the translation channel can be parameterized similarly to Dagan ei al. (Dagan, Church, and Gale, 1993). The effect of the BTG restriction is just to constrain the shapes of the word-order distortions. A BTG rather than I</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Wu, Dekai. 1995c. Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora. In Proceedings of IJCAI-95, Fourteenth .International Joint Conference on Artificial Intelligence, pages 1328-1334, Montreal, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Trainable coarse bilingual grammars for parallel text bracketing.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Annual Workshop on Very Large Corpora,</booktitle>
<pages>69--81</pages>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="6638" citStr="Wu, 1995" startWordPosition="1027" endWordPosition="1028">his paper, like Dagan et al.&apos;s model, encourages related words to stay together, and reduces the number of parameters used to describe word-order variation. But more importantly, it makes structural assumptions that eliminate large portions of the space of alignments, based on linguistic motivatations. This greatly reduces the search space and makes possible a polynomial-time optimization algorithm. 2 ITG and BTG Overview The new translation model is based on the recently introduced bilingual language modeling approach. Specifically, the model employs a bracketing transduction grammar or BTG (Wu, 1995a), which is a special case of inversion transduction grammars or ITGs (Wu, 1995c; Wu, 1995c; Wu, 1995b; Wu, 1995d). These formalisms were originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. This paper finds they are also useful for the translation system itself. In this section we summarize the main properties of BTGs and ITGs. An ITG consists of context-free productions where terminal symbols come in couples, for example x I y, where x is a Chinese word and y is an English translation of x.2 Any parse tree thus ge</context>
<context position="9660" citStr="Wu, 1995" startWordPosition="1540" endWordPosition="1541">TGs which are employed in the model presented below, there is only one undifferentiated nonterminal category (aside from the start symbol). Designating this category A, this means all non-lexical productions are of one of these two forms: A [A A • • • A] A (A A • • • A) The degree of word-order flexibility is the critical point. BTGs make a favorable trade-off between efficiency and expressiveness: constraints are strong enough to allow algorithms to operate efficiently, but without so much loss of expressiveness as to hinder useful translation. We summarize here; details are given elsewhere (Wu, 1995b). With regard to efficiency, Figure 2 demonstrates the kind of reduction that BTGs obtain in the space of possible alignments. The number of possible alignments, compared against the unrestricted case where any English word may align to any Chinese position, drops off dramatically for strings longer than four words. (This table makes the simplification of counting only 1-1 matchings and is merely representative.) With regard to expressiveness, we believe that almost all variation in the order of arguments in a syntactic frame can be accommodated.3 Syntactic frames generally contain four or f</context>
<context position="11199" citStr="Wu, 1995" startWordPosition="1774" endWordPosition="1775">y BTGs (Wu, 1995c), without resorting to the unrestricted-alignment model. The translation expressiveness of BTGs is by no means perfect. They are nonetheless proving very useful in applications and are substantially more feasible than previous models. In our previous corpus analysis applications, any expressiveness limitations were easily tolerable since degradation was graceful. In the present translation application, any expressiveness limitation simply means that certain translations are not considered. For the remainder of the paper, we take advantage of a convenient normal-form theorem (Wu, 1995a) that allows us to assume without loss of generality that the BTG only contains the binary-branching form for the non-lexical productions.4 3 BTG-Based Search for the Original Models A first approach to improving the translation search is to limit the allowed word alignment patterns to those permitted by a BTG. In this case, Equation (2) is kept as the objective function and the translation channel can be parameterized similarly to Dagan ei al. (Dagan, Church, and Gale, 1993). The effect of the BTG restriction is just to constrain the shapes of the word-order distortions. A BTG rather than I</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Wu, Dekai. 1995d. Trainable coarse bilingual grammars for parallel text bracketing. In Proceedings of the Third Annual Workshop on Very Large Corpora, pages 69-81, Cambridge, Massachusetts, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Pascale Fung</author>
</authors>
<title>Improving Chinese tokenization with linguistic filters on statistical lexical acquisition.</title>
<date>1994</date>
<booktitle>In Proceedings of the Fourth Conference on Applied Natural Language Processing,</booktitle>
<pages>180--181</pages>
<location>Stuttgart,</location>
<contexts>
<context position="20026" citStr="Wu and Fung, 1994" startWordPosition="3209" endWordPosition="3212"> word-order languages in the TAG framework (Joshi, 1987). Our work differs from the ID/LP work in several important respects. First, we are not merely parsing, but translating with a bigram language model. Also, of course, we are dealing with 5 Results The algorithm above was tested in the SILC translation system. The translation lexicon was largely constructed by training on the HKUST English-Chinese Parallel Bilingual Corpus, which consists of governmental transcripts. The corpus was sentence-aligned statistically (Wu, 1994); Chinese words and collocations were extracted (Fung and Wu, 1994; Wu and Fung, 1994); then translation pairs were learned via an EM procedure (Wu and Xia, 1995). The resulting English vocabulary is approximately 6,500 words and the Chinese vocabulary is approximately 5,500 words, with a many-to-many translation mapping averaging 2.25 Chinese translations per English word. Due to the unsupervised training, the translation lexicon contains noise and is only at about 86% percent weighted precision. With regard to accuracy, we merely wish to demonstrate that for statistical MT, accuracy is not significantly compromised by substituting our efficient optimization algorithm. It is n</context>
</contexts>
<marker>Wu, Fung, 1994</marker>
<rawString>Wu, Dekai and Pascale Fung. 1994. Improving Chinese tokenization with linguistic filters on statistical lexical acquisition. In Proceedings of the Fourth Conference on Applied Natural Language Processing, pages 180-181, Stuttgart, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Cindy Ng</author>
</authors>
<title>Using brackets to improve search for statistical machine translation.</title>
<date>1995</date>
<booktitle>In PACLIC-10, Pacific Asia Conference on Language, Information and Computation,</booktitle>
<pages>195--204</pages>
<location>Hong Kong,</location>
<contexts>
<context position="3451" citStr="Wu and Ng, 1995" startWordPosition="524" endWordPosition="527">a(il j, V, T) permits the Chinese word in position j of a length-T sentence to map to any position i of a length-V English sentence. So VT alignments are possible, yielding an exponential space with correspondingly slow search times. Note there are no explicit linguistic grammars in the IBM channel model. Useful methods do exist for incorporating constraints fed in from other preprocessing modules, and some of these modules do employ linguistic grammars. For instance, we previously reported a method for improving search times in channel translation models that exploits bracketing information (Wu and Ng, 1995). If any brackets for the Chinese sentence can be supplied as additional input information, produced for example by a preprocessing stage, a modified version of the A*- based algorithm can follow the brackets to guide the search heuristically. This strategy appears to produces moderate improvements in search speed and slightly better translations. Such linguistic-preprocessing techniques could &apos;Various models have been constructed by the IBM team (Brown et al., 1993). This description corresponds to one of the simplest ones, &amp;quot;Model 2&amp;quot;; search costs for the more complex models are corresponding</context>
</contexts>
<marker>Wu, Ng, 1995</marker>
<rawString>Wu, Dekai and Cindy Ng. 1995. Using brackets to improve search for statistical machine translation. In PACLIC-10, Pacific Asia Conference on Language, Information and Computation, pages 195-204, Hong Kong, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Xuanyin Xia</author>
</authors>
<title>Large-scale automatic extraction of an English-Chinese lexicon.</title>
<date>1995</date>
<journal>Machine Translation,</journal>
<pages>9--3</pages>
<contexts>
<context position="20102" citStr="Wu and Xia, 1995" startWordPosition="3222" endWordPosition="3225">om the ID/LP work in several important respects. First, we are not merely parsing, but translating with a bigram language model. Also, of course, we are dealing with 5 Results The algorithm above was tested in the SILC translation system. The translation lexicon was largely constructed by training on the HKUST English-Chinese Parallel Bilingual Corpus, which consists of governmental transcripts. The corpus was sentence-aligned statistically (Wu, 1994); Chinese words and collocations were extracted (Fung and Wu, 1994; Wu and Fung, 1994); then translation pairs were learned via an EM procedure (Wu and Xia, 1995). The resulting English vocabulary is approximately 6,500 words and the Chinese vocabulary is approximately 5,500 words, with a many-to-many translation mapping averaging 2.25 Chinese translations per English word. Due to the unsupervised training, the translation lexicon contains noise and is only at about 86% percent weighted precision. With regard to accuracy, we merely wish to demonstrate that for statistical MT, accuracy is not significantly compromised by substituting our efficient optimization algorithm. It is not our purpose here to argue that accuracy can be increased with our model. </context>
</contexts>
<marker>Wu, Xia, 1995</marker>
<rawString>Wu, Dekai and Xuanyin Xia. 1995. Large-scale automatic extraction of an English-Chinese lexicon. Machine Translation, 9(3-4):285-313.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>