<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9994995">
An Unsupervised Approach to Prepositional Phrase Attachment
using Contextually Similar Words
</title>
<author confidence="0.996711">
Patrick Pantel and Dekang Lin
</author>
<affiliation confidence="0.9936255">
Department of Computing Science
University of Alberta1
</affiliation>
<address confidence="0.928501">
Edmonton, Alberta T6G 2H1 Canada
</address>
<email confidence="0.984878">
{ppantel, lindek}@cs.ualberta.ca
</email>
<sectionHeader confidence="0.995382" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997579375">
Prepositional phrase attachment is a
common source of ambiguity in natural
language processing. We present an
unsupervised corpus-based approach to
prepositional phrase attachment that
achieves similar performance to supervised
methods. Unlike previous unsupervised
approaches in which training data is
obtained by heuristic extraction of
unambiguous examples from a corpus, we
use an iterative process to extract training
data from an automatically parsed corpus.
Attachment decisions are made using a
linear combination of features and low
frequency events are approximated using
contextually similar words.
</bodyText>
<sectionHeader confidence="0.961515" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.9995298">
Prepositional phrase attachment is a common
source of ambiguity in natural language
processing. The goal is to determine the
attachment site of a prepositional phrase in a
sentence. Consider the following examples:
</bodyText>
<listItem confidence="0.9984435">
1. Mary ate the salad with a fork.
2. Mary ate the salad with croutons.
</listItem>
<bodyText confidence="0.983502625">
In both cases, the task is to decide whether the
prepositional phrase headed by the preposition
with attaches to the noun phrase (NP) headed by
salad or the verb phrase (VP) headed by ate. In
the first sentence, with attaches to the VP since
Mary is using a fork to eat her salad. In sentence
2, with attaches to the NP since it is the salad
that contains croutons.
</bodyText>
<footnote confidence="0.956604">
1This research was conducted at the University of Manitoba.
</footnote>
<bodyText confidence="0.999902970588235">
Formally, prepositional phrase attachment is
simplified to the following classification task.
Given a 4-tuple of the form (V, N1, P, N2), where
V is the head verb, N1 is the head noun of the
object of V, P is a preposition, and N2 is the head
noun of the prepositional complement, the goal
is to classify as either adverbial attachment
(attaching to V) or adjectival attachment
(attaching to N1). For example, the 4-tuple (eat,
salad, with, fork) has target classification V.
In this paper, we present an unsupervised
corpus-based approach to prepositional phrase
attachment that outperforms previous
unsupervised techniques and approaches the
performance of supervised methods. Unlike
previous unsupervised approaches in which
training data is obtained by heuristic extraction
of unambiguous examples from a corpus, we use
an iterative process to extract training data from
an automatically parsed corpus. The attachment
decision for a 4-tuple (V, N1, P, N2) is made as
follows. First, we replace V and N2 by their
contextually similar words and compute the
average adverbial attachment score. Similarly,
the average adjectival attachment score is
computed by replacing N1 and N2 by their
contextually similar words. Attachment scores
are obtained using a linear combination of
features of the 4-tuple. Finally, we combine the
average attachment scores with the attachment
score of N2 attaching to the original V and the
attachment score of N2 attaching to the original
N1. The proposed classification represents the
attachment site that scored highest.
</bodyText>
<sectionHeader confidence="0.99221" genericHeader="related work">
1 Previous Work
</sectionHeader>
<bodyText confidence="0.999732381818182">
Altmann and Steedman (1988) showed that
current discourse context is often required for
disambiguating attachments. Recent work shows
that it is generally sufficient to utilize lexical
information (Brill and Resnik, 1994; Collins and
Brooks, 1995; Hindle and Rooth, 1993;
Ratnaparkhi et al., 1994).
One of the earliest corpus-based approaches to
prepositional phrase attachment used lexical
preference by computing co-occurrence
frequencies (lexical associations) of verbs and
nouns with prepositions (Hindle and Rooth,
1993). Training data was obtained by extracting
all phrases of the form (V, N1, P, N2) from a
large parsed corpus.
Supervised methods later improved
attachment accuracy. Ratnaparkhi et al. (1994)
used a maximum entropy model considering
only lexical information from within the verb
phrase (ignoring N2). They experimented with
both word features and word class features, their
combination yielding 81.6% attachment
accuracy.
Later, Collins and Brooks (1995) achieved
84.5% accuracy by employing a backed-off
model to smooth for unseen events. They
discovered that P is the most informative lexical
item for attachment disambiguation and keeping
low frequency events increases performance.
A non-statistical supervised approach by Brill
and Resnik (1994) yielded 81.8% accuracy using
a transformation-based approach (Brill, 1995)
and incorporating word-class information. They
report that the top 20 transformations learned
involved specific prepositions supporting
Collins and Brooksâ€™ claim that the preposition is
the most important lexical item for resolving the
attachment ambiguity.
The state of the art is a supervised algorithm
that employs a semantically tagged corpus
(Stetina and Nagao, 1997). Each word in a
labelled corpus is sense-tagged using an
unsupervised word-sense disambiguation
algorithm with WordNet (Miller, 1990). Testing
examples are classified using a decision tree
induced from the training examples. They report
88.1% attachment accuracy approaching the
human accuracy of 88.2% (Ratnaparkhi et al.,
1994).
The current unsupervised state of the art
achieves 81.9% attachment accuracy
(Ratnaparkhi, 1998). Using an extraction
heuristic, unambiguous prepositional phrase
attachments of the form (V, P, N2) and (N1, P,
N2) are extracted from a large corpus. Co-
</bodyText>
<figureCaption confidence="0.80113295">
eat:
object: almond 1, apple 25, bean 5, beam 1, binge 1,
bread 13, cake 17, cheese 8, dish 14,
disorder 20, egg 31, grape 12, grub 2, hay 3,
junk 1, meat 70, poultry 3, rabbit 4, soup 5,
sandwich 18, pasta 7, vegetable 35, ...
subject: adult 3, animal 8, beetle 1, cat 3, child 41,
decrease 1, dog 24, family 29, guest 7, kid
22, patient 7, refugee 2, rider 1, Russian 1,
shark 2, something 19, We 239, wolf 5, ...
salad:
adj-modifier: assorted 1, crisp 4, fresh 13, good 3, grilled
5, leftover 3, mixed 4, olive 3, prepared 3,
side 4, small 6, special 5, vegetable 3, ...
object-of: add 3, consume 1, dress 1, grow 1, harvest 2,
have 20, like 5, love 1, mix 1, pick 1, place
3, prepare 4, return 3, rinse 1, season 1, serve
8, sprinkle 1, taste 1, test 1, Toss 8, try 3, ...
Figure 1. Excepts of entries in the collocation database for
eat and salad.
</figureCaption>
<tableCaption confidence="0.916503">
Table 1. The top 20 most similar words of eat and salad as
given by (Lin, 1998b).
</tableCaption>
<sectionHeader confidence="0.291979" genericHeader="method">
WORD SIMILAR WORDS (WITH SIMILARITY SCORE)
</sectionHeader>
<bodyText confidence="0.995915357142857">
EAT cook 0.127, drink 0.108, consume 0.101, feed 0.094,
taste 0.093, like 0.092, serve 0.089, bake 0.087, sleep
0.086, pick 0.085, fry 0.084, freeze 0.081, enjoy
0.079, smoke 0.078, harvest 0.076, love 0.076, chop
0.074, sprinkle 0.072, Toss 0.072, chew 0.072
SALAD soup 0.172, sandwich 0.169, sauce 0.152, pasta
0.149, dish 0.135, vegetable 0.135, cheese 0.132,
dessert 0.13, entree 0.121, bread 0.116, meat 0.116,
chicken 0.115, pizza 0.114, rice 0.112, seafood 0.11,
dressing 0.109, cake 0.107, steak 0.105, noodle
0.105, bean 0.102
occurrence frequencies are then used to
disambiguate examples with ambiguous
attachments.
</bodyText>
<sectionHeader confidence="0.996092" genericHeader="method">
2 Resources
</sectionHeader>
<bodyText confidence="0.99988525">
The input to our algorithm includes a collocation
database and a corpus-based thesaurus, both
available on the Internet2. Below, we briefly
describe these resources.
</bodyText>
<subsectionHeader confidence="0.998385">
2.1 Collocation database
</subsectionHeader>
<bodyText confidence="0.999810666666667">
Given a word w in a dependency relationship
(such as subject or object), the collocation
database is used to retrieve the words that
occurred in that relationship with w, in a large
corpus, along with their frequencies (Lin,
1998a). Figure 1 shows excerpts of the entries in
</bodyText>
<footnote confidence="0.969633">
2Available at www.cs.ualberta.ca/~lindek/demos.htm.
</footnote>
<bodyText confidence="0.999565">
the collocation database for the words eat and
salad. The database contains a total of 11
million unique dependency relationships.
</bodyText>
<subsectionHeader confidence="0.998073">
2.2 Corpus-based thesaurus
</subsectionHeader>
<bodyText confidence="0.999987875">
Using the collocation database, Lin (1998b) used
an unsupervised method to construct a corpus-
based thesaurus consisting of 11839 nouns, 3639
verbs and 5658 adjectives/adverbs. Given a
word w, the thesaurus returns a set of similar
words of w along with their similarity to w. For
example, the 20 most similar words of eat and
salad are shown in Table 1.
</bodyText>
<sectionHeader confidence="0.983544" genericHeader="method">
3 Training Data Extraction
</sectionHeader>
<bodyText confidence="0.999994461538461">
We parsed a 125-million word newspaper
corpus with Minipar3, a descendent of Principar
(Lin, 1994). Minipar outputs dependency trees
(Lin, 1999) from the input sentences. For
example, the following sentence is decomposed
into a dependency tree:
Occasionally, the parser generates incorrect
dependency trees. For example, in the above
sentence, the prepositional phrase headed by
with should attach to saw (as opposed to dog).
Two separate sets of training data were then
extracted from this corpus. Below, we briefly
describe how we obtained these data sets.
</bodyText>
<subsectionHeader confidence="0.99984">
3.1 Ambiguous Data Set
</subsectionHeader>
<bodyText confidence="0.9998858">
For each input sentence, Minipar outputs a
single dependency tree. For a sentence
containing one or more prepositions, we use a
program to detect any alternative prepositional
attachment sites. For example, in the above
sentence, the program would detect that with
could attach to saw. Using an iterative
algorithm, we initially create a table of co-
occurrence frequencies for 3-tuples of the form
(V, P, N2) and (N1, P, N2). For each k possible
attachment site of a preposition P, we increment
the frequency of the corresponding 3-tuple by
1/k. For example, Table 2 shows the initial co-
occurrence frequency table for the
corresponding 3-tuples of the above sentence.
</bodyText>
<footnote confidence="0.885343">
3Available at www.cs.ualberta.ca/~lindek/minipar.htm.
</footnote>
<tableCaption confidence="0.9023065">
Table 2. Initial co-occurrence frequency table entries for A
man in the park saw a dog with a telescope.
</tableCaption>
<table confidence="0.877767">
V OR N1 P N2 FREQUENCY
man in park 1.0
saw with telescope 0.5
dog with telescope 0.5
</table>
<tableCaption confidence="0.8588045">
Table 3. Co-occurrence frequency table entries for A man
in the park saw a dog with a telescope after one iteration.
</tableCaption>
<bodyText confidence="0.972854916666667">
V OR N1 P N2 FREQUENCY
man in park 1.0
saw with telescope 0.913
dog with telescope 0.087
In the following iterations of the algorithm, we
update the frequency table as follows. For each k
possible attachment site of a preposition P, we
refine its attachment score using the formulas
described in Section 4: VScore(Vk, Pk, N2k) and
NScore(N1k, Pk, N2k). For any tuple (Wk, Pk, N2k),
where Wk is either Vk or N2k, we update its
frequency as:
</bodyText>
<equation confidence="0.979122909090909">
Score W P N
( k k
, , )
2k
fr ( ) = k
Wk Pk N
, , 2 k âˆ‘ = ( )
Score W P N
i i
, , 2
i 1 i
</equation>
<bodyText confidence="0.999436888888889">
where Score(Wk, Pk, N2k) = VScore(Wk, Pk, N2k)
if Wk = Vk; otherwise Score(Wk, Pk, N2k) =
NScore(Wk, Pk, N2k).
Suppose that after the initial frequency table is
set NScore(man, in, park) = 1.23, VScore(saw,
with, telescope) = 3.65, and NScore(dog, with,
telescope) = 0.35. Then, the updated co-
occurrence frequencies for (man, in, park) and
(saw, with, telescope) are:
</bodyText>
<equation confidence="0.81471075">
= 1.23
fr(man, in, park) 1.23 = 1.0
3.65
fr(saw, with, telescope) = 3.65+0. 35 = 0.913
</equation>
<bodyText confidence="0.999877333333333">
Table 3 shows the updated frequency table
after the first iteration of the algorithm. The
resulting database contained 8,900,000 triples.
</bodyText>
<subsectionHeader confidence="0.999604">
3.2 Unambiguous Data Set
</subsectionHeader>
<bodyText confidence="0.9995945">
As in (Ratnaparkhi, 1998), we constructed a
training data set consisting of only unambiguous
</bodyText>
<figure confidence="0.558320666666667">
subj
pcomp
obj
pcomp
det mod det det mod det
A man in the park saw a dog with a telescope.
</figure>
<bodyText confidence="0.999799">
attachments of the form (V, P, N2) and (N1, P,
N2). We only extract a 3-tuple from a sentence
when our program finds no alternative
attachment site for its preposition. Each
extracted 3-tuple is assigned a frequency count
of 1. For example, in the previous sentence,
(man, in, park) is extracted since it contains only
one attachment site; (dog, with, telescope) is not
extracted since with has an alternative
attachment site. The resulting database
contained 4,400,000 triples.
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="method">
4 Classification Model
</sectionHeader>
<bodyText confidence="0.999522392857143">
Roth (1998) presented a unified framework for
natural language disambiguation tasks.
Essentially, several language learning algorithms
(e.g. naÃ¯ve Bayes estimation, back-off
estimation, transformation-based learning) were
successfully cast as learning linear separators in
their feature space. Roth modelled prepositional
phrase attachment as linear combinations of
features. The features consisted of all 15
possible sub-sequences of the 4-tuple (V, N1, P,
N2) shown in Table 4. The asterix (*) in features
represent wildcards.
Roth used supervised learning to adjust the
weights of the features. In our experiments, we
only considered features that contained P since
the preposition is the most important lexical item
(Collins and Brooks, 1995). Furthermore, we
omitted features that included both V and N1
since their co-occurrence is independent of the
attachment decision. The resulting subset of
features considered in our system is shown in
bold in Table 4 (equivalent to assigning a weight
of 0 or 1 to each feature).
Let |head, rel, mod |represent the frequency,
obtained from the training data, of the head
occurring in the given relationship rel with the
modifier. We then assign a score to each feature
as follows:
</bodyText>
<table confidence="0.999589666666667">
(*, *, P, *) = log(|*, P, * |/ |*, *, *|)
(V, *, P, N2) = log(|V, P, N2 |/ |*, *, *|)
(*, N1, P, N2) = log(|N1, P, N2 |/ |*, *, *|)
(V, *, P, *) = log(|V, P, * |/ |V, *, *|)
(*, N1, P, *) = log(|N1, P, * |/ |N1, *, *|)
(*, *, P, N2) = log(|*, P, N2 |/ |*, *, N2|)
</table>
<tableCaption confidence="0.9176725">
1, 2, and 3 are the prior probabilities of P, V P
N2, and N1 P N2 respectively. 4, 5, and 6
Table 4. The 15 features for prepositional phrase
attachment.
</tableCaption>
<table confidence="0.89968925">
FEATURES
(V, *, *, *) (V, *, P, *) (*, N1, *, N2)
(V, N1, *, *) (V, *, *, N2) (*, N1, P, N2)
(V, N1, P, *) (V, *, P, N2) (*, *, P, *)
(V, N1, *, N2) (*, N1, *, *) (*, *, *, N2)
(V, N1, P, N2) (*, N1, P, *) (*, *, P, N2)
represent conditional probabilities P(V, P  |V),
P(N1, P  |N1), and P(P N2  |N2) respectively.
</table>
<bodyText confidence="0.97193375">
We estimate the adverbial and adjectival
attachment scores, VScore(V, P, N2) and
NScore(N1, P, N2), as a linear combination of
these features:
</bodyText>
<equation confidence="0.9977235">
VScore(V, P, N2) = (*, *, P, *) + (V, *, P, N2) +
(V, *, P, *) + (*, *, P, N2)
NScore(N1, P, N2) = (*, *, P, *) + (*, N1, P, N2) +
(*, N1, P, *) + (*, *, P, N2)
</equation>
<bodyText confidence="0.9998554">
For example, the attachment scores for (eat,
salad, with, fork) are VScore(eat, with, fork) =
-3.47 and NScore(salad, with, fork) = -4.77. The
model correctly assigns a higher score to the
adverbial attachment.
</bodyText>
<sectionHeader confidence="0.989552" genericHeader="method">
5 Contextually Similar Words
</sectionHeader>
<bodyText confidence="0.999975333333333">
The contextually similar words of a word w are
words similar to the intended meaning of w in its
context. Below, we describe an algorithm for
constructing contextually similar words and we
present a method for approximating the
attachment scores using these words.
</bodyText>
<subsectionHeader confidence="0.973769">
5.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.999962210526316">
For our purposes, a context of w is simply a
dependency relationship involving w. For
example, a dependency relationship for saw in
the example sentence of Section 3 is
saw:obj:dog. Figure 2 gives the data flow
diagram for our algorithm for constructing the
contextually similar words of w. We retrieve
from the collocation database the words that
occurred in the same dependency relationship as
w. We refer to this set of words as the cohort of
w for the dependency relationship. Consider the
words eat and salad in the context eat salad.
The cohort of eat consists of verbs that appeared
with object salad in Figure 1 (e.g. add, consume,
cover, ...) and the cohort of salad consists of
nouns that appeared as object of eat in Figure 1
(e.g. almond, apple, bean, ...).
Intersecting the set of similar words and the
cohort then forms the set of contextually similar
words of w. For example, Table 5 shows the
contextually similar words of eat and salad in
the context eat salad and the contextually
similar words of fork in the contexts eat with
fork and salad with fork. The words in the first
row are retrieved by intersecting the similar
words of eat in Table 1 with the cohort of eat
while the second row represents the intersection
of the similar words of salad in Table 1 and the
cohort of salad. The third and fourth rows are
determined in a similar manner. In the
nonsensical context salad with fork (in row 4),
no contextually similar words are found.
While previous word sense disambiguation
algorithms rely on a lexicon to provide sense
inventories of words, the contextually similar
words provide a way of distinguishing between
different senses of words without committing to
any particular sense inventory.
</bodyText>
<subsectionHeader confidence="0.999329">
5.2 Attachment Approximation
</subsectionHeader>
<bodyText confidence="0.999116647058824">
Often, sparse data reduces our confidence in the
attachment scores of Section 4. Using
contextually similar words, we can approximate
these scores. Given the tuple (V, N1, P, N2),
adverbial attachments are approximated as
follows. We first construct a list CSV containing
the contextually similar words of V in context
V:obj:N1 and a list CSN2V containing the
contextually similar words of N2 in context
V:P:N2 (i.e. assuming adverbial attachment). For
each verb v in CSV, we compute VScore(v, P, N2)
and set SV as the average of the largest k of these
scores. Similarly, for each noun n in CSN2V, we
compute VScore(V, P, n) and set SN2V as the
average of the largest k of these scores. Then,
the approximated adverbial attachment score,
Vscore&apos;, is:
</bodyText>
<equation confidence="0.914382">
VScore&apos;(V, P, N2) = max(SV, SN2V)
</equation>
<bodyText confidence="0.999911166666667">
We approximate the adjectival attachment
score in a similar way. First, we construct a list
CSN1 containing the contextually similar words
of N1 in context V:obj:N1 and a list CSN2N1
containing the contextually similar words of N2
in context N1:P:N2 (i.e. assuming adjectival
</bodyText>
<tableCaption confidence="0.997951">
Table 5. Contextually similar words of eat and salad.
</tableCaption>
<table confidence="0.968633571428571">
WORD CONTEXT CONTEXTUALLY SIMILAR WORDS
EAT eat salad consume, taste, like, serve, pick,
harvest, love, sprinkle, Toss, ...
SALAD eat salad soup, sandwich, pasta, dish, cheese,
vegetable, bread, meat, cake, bean, ...
FORK eat with fork spoon, knife, finger
FORK salad with fork ---
</table>
<figureCaption confidence="0.998513333333333">
Figure 2. Data flow diagram for identifying the
contextually similar words of a word in a dependency
relationship.
</figureCaption>
<bodyText confidence="0.9998645">
attachment). Now, we compute SN1 as the
average of the largest k of NScore(n, P, N2) for
each noun n in CSN1 and SN2N1 as the average of
the largest k of NScore(N1, P, n) for each noun n
in CSN2N1. Then, the approximated adjectival
attachment score, NScore&apos;, is:
</bodyText>
<equation confidence="0.950316">
NScore&apos;(N1, P, N2) = max(SN1, SN2N1)
</equation>
<bodyText confidence="0.999811222222222">
For example, suppose we wish to approximate
the attachment score for the 4-tuple (eat, salad,
with, fork). First, we retrieve the contextually
similar words of eat and salad in context eat
salad, and the contextually similar words of fork
in contexts eat with fork and salad with fork as
shown in Table 5. Let k = 2. Table 6 shows the
calculation of SV and SN2V while the calculation
of SN1 and SN2N1 is shown in Table 7. Only the
</bodyText>
<figure confidence="0.983036461538461">
word in dependency
relationship
Corpus-Based
Thesaurus
Collocation
DB
Get Similar
Words
Retrieve
Similar Words Cohorts
Intersect
Contextually
Similar Words
</figure>
<bodyText confidence="0.8850835">
top k = 2 scores are shown in these tables. We
have:
</bodyText>
<equation confidence="0.850042">
VScore&apos; (eat, with, fork) = max(SV, SN2V)
= -2.92
NScore&apos; (salad, with, fork) = max(SN1, SN2N1)
= -4.87
</equation>
<bodyText confidence="0.986793666666667">
Hence, the approximation correctly prefers the
adverbial attachment to the adjectival
attachment.
</bodyText>
<sectionHeader confidence="0.958903" genericHeader="method">
6 Attachment Algorithm
</sectionHeader>
<bodyText confidence="0.992052416666667">
Figure 3 describes the prepositional phrase
attachment algorithm. As in previous
approaches, examples with P = of are always
classified as adjectival attachments.
Suppose we wish to approximate the
attachment score for the 4-tuple (eat, salad,
with, fork). From the previous section, Step 1
returns averageV = -2.92 and averageN1 = -4.87.
From Section 4, Step 2 gives aV = -3.47 and
aN1 = -4.77. In our training data, fV = 2.97 and
fN1 = 0, thus Step 3 gives f = 0.914. In Step 4, we
compute:
</bodyText>
<equation confidence="0.700877">
S(V) = -3.42 and
S(N1) = -4.78
</equation>
<bodyText confidence="0.998357428571429">
Since S(V) &gt; S(N1), the algorithm correctly
classifies this example as an adverbial
attachment.
Given the 4-tuple (eat, salad, with, croutons),
the algorithm returns S(V) = -4.31 and S(N1) =
-3.88. Hence, the algorithm correctly attaches
the prepositional phrase to the noun salad.
</bodyText>
<sectionHeader confidence="0.980396" genericHeader="method">
7 Experimental Results
</sectionHeader>
<bodyText confidence="0.999811333333333">
In this section, we describe our test data and the
baseline for our experiments. Finally, we present
our results.
</bodyText>
<subsectionHeader confidence="0.999908">
7.1 Test Data
</subsectionHeader>
<bodyText confidence="0.9996875">
The test data consists of 3097 examples derived
from the manually annotated attachments in the
Penn Treebank Wall Street Journal data
(Ratnaparkhi et al., 1994)4. Each line in the test
data consists of a 4-tuple and a target
classification: V N1 P N2 target.
</bodyText>
<tableCaption confidence="0.3585515625">
4Available at ftp.cis.upenn.edu/pub/adwait/PPattachData.
Input: A 4-tuple (V, N1, P, N2)
Step 1: Using the contextually similar words algorithm
and the formulas from Section 5.2 compute:
averageV = VScore&apos;(V, P, N2)
averageN1 = NScore&apos;(N1, P, N2)
Step 2: Compute the adverbial attachment score, av,
and the adjectival attachment score, an1:
aV = VScore(V, P, N2)
aN1 = NScore(N1, P, N2)
Step 3: Retrieve from the training data set the
frequency of the 3-tuples (V, P, N2) and
(N1, P, N2) 4 fV and fN1, respectively.
Let f = (fV + fN1 + 0.2) / (fV + fN1 +0.5)
Step 4: Combine the scores of Steps 1-3 to obtain the
final attachment scores:
</tableCaption>
<equation confidence="0.98668725">
S(V) = fav + (1 âˆ’ f)averagev
S(N1) = fan1 + (1 âˆ’ f)averagen1
Output:The attachment decision: N1 if S(N1) &gt; S(V) or
P = of; V otherwise.
</equation>
<figureCaption confidence="0.997426">
Figure 3. The prepositional phrase attachment algorithm.
</figureCaption>
<tableCaption confidence="0.941649">
Table 6. Calculation of SV and SN2V for (eat, salad, with,
fork).
</tableCaption>
<table confidence="0.999166285714286">
4-TUPLE VSCORE
(mix, salad, with, fork) -2.60
(sprinkle, salad, with, fork) -3.24
SV -2.92
(eat, salad, with, spoon) -3.06
(eat, salad, with, finger) -3.50
SN2V -3.28
</table>
<tableCaption confidence="0.980032">
Table 7. Calculation of SN1 and SN2N1 for (eat, salad, with,
fork).
</tableCaption>
<table confidence="0.931754666666667">
4-TUPLE NSCORE
(eat, pasta, with, fork) -4.71
(eat, cake, with, fork) -5.02
</table>
<subsectionHeader confidence="0.582011">
SN1 -4.87
</subsectionHeader>
<bodyText confidence="0.7717695">
--- n/a
--- n/a
</bodyText>
<subsectionHeader confidence="0.82849">
SN2N1 n/a
</subsectionHeader>
<bodyText confidence="0.9997654">
The data set contains several erroneous tuples
and attachments. For instance, 133 examples
contain the word the as N1 or N2. There are also
improbable attachments such as (sing, birthday,
to, you) with the target attachment birthday.
</bodyText>
<tableCaption confidence="0.881496">
Table 8. Precision and recall for attachment sites V and N1.
</tableCaption>
<subsectionHeader confidence="0.766673">
7.2 Baseline
</subsectionHeader>
<bodyText confidence="0.999915727272727">
Choosing the most common attachment site, N1,
yields an accuracy of 58.96%. However, we
achieve 70.39% accuracy by classifying each
occurrence of P = of as N1, and V otherwise.
Human accuracy, given the full context of a
sentence, is 93.2% and drops to 88.2% when
given only tuples of the form (V, N1, P, N2)
(Ratnaparkhi et al., 1994). Assuming that human
accuracy is the upper bound for automatic
methods, we expect our accuracy to be bounded
above by 88.2% and below by 70.39%.
</bodyText>
<sectionHeader confidence="0.697981" genericHeader="evaluation">
7.3 Results
</sectionHeader>
<bodyText confidence="0.999990857142857">
We used the 3097-example testing corpus
described in Section 7.1. Table 8 presents the
precision and recall of our algorithm and Table 9
presents a performance comparison between our
system and previous supervised and
unsupervised approaches using the same test
data. We describe the different classifiers below:
</bodyText>
<figureCaption confidence="0.688643875">
clbase: the baseline described in Section 7.2
clR1: uses a maximum entropy model
(Ratnaparkhi et al., 1994)
clBR5: uses transformation-based learning (Brill
and Resnik, 1994)
clCB: uses a backed-off model (Collins and
Brooks, 1995)
clSN: induces a decision tree with a sense-tagged
</figureCaption>
<bodyText confidence="0.940597294117647">
corpus, using a semantic dictionary
(Stetina and Nagao, 1997)
clHR6: uses lexical preference (Hindle and Rooth,
1993)
clR2: uses a heuristic extraction of unambiguous
attachments (Ratnaparkhi, 1998)
clPL: uses the algorithm described in this paper
Our classifier outperforms all previous
unsupervised techniques and approaches the
performance of supervised algorithm.
We reconstructed the two earlier unsupervised
classifiers clHR and clR2. Table 10 presents the
accuracy of our reconstructed classifiers. The
originally reported accuracy for clR2 is within the
95% confidence interval of our reconstruction.
Our reconstruction of clHR achieved slightly
higher accuracy than the original report.
</bodyText>
<footnote confidence="0.997890666666667">
5The accuracy is reported in (Collins and Brooks, 1995).
6The accuracy was obtained on a smaller test set but, from
the same source as our test data.
</footnote>
<table confidence="0.767182666666667">
CLASS ACTUAL CORRECT INCORRECT PRECISION RECALL
V 1203 994 209 82.63% 78.21%
N1 1894 1617 277 84.31% 88.55%
</table>
<tableCaption confidence="0.964326">
Table 9. Performance comparison with other approaches.
</tableCaption>
<table confidence="0.990369">
METHOD LEARNING ACCURACY
CLBASE --- 70.39%
CLR1 supervised 81.6%
CLBR supervised 81.9%
CLCB supervised 84.5%
CLSN supervised 88.1%
CLHR unsupervised 75.8%
CLR2 unsupervised 81.91%
CLPL unsupervised 84.31%
</table>
<tableCaption confidence="0.976128">
Table 10. Accuracy of our reconstruction of (Hindle &amp;
Rooth, 1993) and (Ratnaparkhi, 1998).
</tableCaption>
<table confidence="0.9506384">
METHOD ORIGINAL RECONSTRUCTED
REPORTED SYSTEM ACCURACY
ACCURACY (95% CONF)
CLHR 75.8% 78.40% Â± 1.45%
CLR2 81.91% 82.40% Â± 1.34%
</table>
<tableCaption confidence="0.75483775">
Our classifier used a mixture of the two
training data sets described in Section 3. In
Table 11, we compare the performance of our
system on the following training data sets:
UNAMB: the data set of unambiguous examples
described in Section 3.2
EM0: the data set of Section 3.1 after
frequency table initialization
</tableCaption>
<reference confidence="0.2461284">
EM1: EM0 + one iteration of algorithm 3.1
EM2: EM0 + two iterations of algorithm 3.1
EM3: EM0 + three iterations of algorithm 3.1
1/8-EM1: one eighth of the data in EM1
MIX: The concatenation of UNAMB and EM1
</reference>
<bodyText confidence="0.55106875">
Table 11 illustrates a slight but consistent
increase in performance when using contextually
similar words. However, since the confidence
intervals overlap, we cannot claim with certainty
</bodyText>
<tableCaption confidence="0.993889">
Table 11. Performance comparison of different data sets.
</tableCaption>
<table confidence="0.993414454545454">
DATABASE ACCURACY ACCURACY
WITHOUT WITH
SIMWORDS SIMWORDS
(95% CONF) (95% CONF)
UNAMBIGUOUS 83.15% Â± 1.32% 83.60% Â± 1.30%
EM0 82.24% Â± 1.35% 82.69% Â± 1.33%
EM1 83.76% Â± 1.30% 83.92% Â± 1.29%
EM2 83.66% Â± 1.30% 83.70% Â± 1.31%
EM3 83.20% Â± 1.32% 83.20% Â± 1.32%
1/8-EM1 82.98% Â± 1.32% 83.15% Â± 1.32%
MIX 84.11% Â± 1.29% 84.31% Â± 1.28%
</table>
<tableCaption confidence="0.999795">
Table 12. Performance with removal of the as N1 or N2.
</tableCaption>
<figure confidence="0.872137166666667">
DATA SET ACCURACY ACCURACY
WITHOUT WITH
SIMWORDS SIMWORDS
(95% CONF) (95% CONF)
WITH THE 84.11% Â± 1.29% 84.31% Â± 1.32%
WITHOUT THE 84.44% Â± 1.31% 84.65% Â± 1.30%
</figure>
<bodyText confidence="0.991844416666667">
that the contextually similar words improve
performance.
In Section 7.1, we mentioned some testing
examples contained N1 = the or N2 = the. For
supervised algorithms, the is represented in the
training set as any other noun. Consequently,
these algorithms collect training data for the and
performance is not affected. However,
unsupervised methods break down on such
examples. In Table 12, we illustrate the
performance increase of our system when
removing these erroneous examples.
</bodyText>
<sectionHeader confidence="0.928304" genericHeader="conclusions">
Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999990636363636">
The algorithms presented in this paper advance
the state of the art for unsupervised approaches
to prepositional phrase attachment and draws
near the performance of supervised methods.
Currently, we are exploring different functions
for combining contextually similar word
approximations with the attachment scores. A
promising approach considers the mutual
information between the prepositional
relationship of candidate attachments and N2. As
the mutual information decreases, our
confidence in the attachment score decreases
and the contextually similar word approximation
is weighted higher. Also, improving the
construction algorithm for contextually similar
words would possibly improve the accuracy of
the system. One approach first clusters the
similar words. Then, dependency relationships
are used to select the most representative
clusters as the contextually similar words. The
assumption is that more representative similar
words produce better approximations.
</bodyText>
<sectionHeader confidence="0.996364" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996228">
The authors wish to thank the reviewers for their
helpful comments. This research was partly
supported by Natural Sciences and Engineering
Research Council of Canada grant OGP121338
and scholarship PGSB207797.
</bodyText>
<sectionHeader confidence="0.999063" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999604125">
Altmann, G. and Steedman, M. 1988. Interaction with Context
During Human Sentence Processing. Cognition, 30:191-238.
Brill, E. 1995. Transformation-based Error-driven Learning and
Natural Language Processing: A case study in part of speech
tagging. Computational Linguistics, December.
Brill, E. and Resnik. P. 1994. A Rule-Based Approach to
Prepositional Phrase Attachment Disambiguation. In
Proceedings of COLING-94. Kyoto, Japan.
Collins, M. and Brooks, J. 1995. Prepositional Phrase Attachment
through a Backed-off Model. In Proceedings of the Third
Workshop on Very Large Corpora, pp. 27-38. Cambridge,
Massachusetts.
Hindle, D. and Rooth, M. 1993. Structural Ambiguity and Lexical
Relations. Computational Linguistics, 19(1):103-120.
Lin, D. 1999. Automatic Identification of Non-Compositional
Phrases. In Proceedings of ACL-99, pp. 317-324. College Park,
Maryland.
Lin, D. 1998a. Extracting Collocations from Text Corpora.
Workshop on Computational Terminology. Montreal, Canada.
Lin, D. 1998b. Automatic Retrieval and Clustering of Similar
Words. In Proceedings of COLING-ACL98. Montreal, Canada.
Lin, D. (1994). Principar - an Efficient, Broad-Coverage,
Principle-Based Parser. In Proceedings of COLING-94. Kyoto,
Japan.
Miller, G. 1990. Wordnet: an On-Line Lexical Database.
International Journal of Lexicography, 1990.
Ratnaparkhi, A. 1998. Unsupervised Statistical Models for
Prepositional Phrase Attachment. In Proceedings of COLING-
ACL98. Montreal, Canada.
Ratnaparkhi, A., Reynar, J., and Roukos, S. 1994. A Maximum
Entropy Model for Prepositional Phrase Attachment. In
Proceedings of the ABPA Human Language Technology
Workshop, pp. 250-255. Plainsboro, N.J.
Roth, D. 1998. Learning to Resolve Natural Language
Ambiguities: A Unified Approach. In Proceedings of AAAI-98,
pp. 806-813. Madison, Wisconsin.
Stetina, J. and Nagao, M. 1997. Corpus Based PP Attachment
Ambiguity Resolution with a Semantic Dictionary. In
Proceedings of the Fifth Workshop on Very Large Corpora, pp.
66-80. Beijing and Hong Kong.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.828917">
<title confidence="0.998059">An Unsupervised Approach to Prepositional Phrase Attachment using Contextually Similar Words</title>
<author confidence="0.999332">Pantel Lin</author>
<affiliation confidence="0.997846">Department of Computing Science</affiliation>
<address confidence="0.9161275">of Edmonton, Alberta T6G 2H1 Canada</address>
<email confidence="0.997414">ppantel@cs.ualberta.ca</email>
<email confidence="0.997414">lindek@cs.ualberta.ca</email>
<abstract confidence="0.999828588235294">Prepositional phrase attachment is a common source of ambiguity in natural language processing. We present an unsupervised corpus-based approach to prepositional phrase attachment that achieves similar performance to supervised methods. Unlike previous unsupervised approaches in which training data is obtained by heuristic extraction of unambiguous examples from a corpus, we use an iterative process to extract training data from an automatically parsed corpus. Attachment decisions are made using a linear combination of features and low frequency events are approximated using contextually similar words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>EM1: EM0 + one iteration of algorithm 3.1 EM2: EM0 + two iterations of algorithm 3.1 EM3:</title>
<booktitle>EM0 + three iterations of algorithm 3.1 1/8-EM1: one eighth of the data in EM1 MIX: The concatenation of UNAMB and EM1</booktitle>
<marker></marker>
<rawString>EM1: EM0 + one iteration of algorithm 3.1 EM2: EM0 + two iterations of algorithm 3.1 EM3: EM0 + three iterations of algorithm 3.1 1/8-EM1: one eighth of the data in EM1 MIX: The concatenation of UNAMB and EM1</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Altmann</author>
<author>M Steedman</author>
</authors>
<title>Interaction with Context During Human Sentence Processing.</title>
<date>1988</date>
<journal>Cognition,</journal>
<pages>30--191</pages>
<contexts>
<context position="3187" citStr="Altmann and Steedman (1988)" startWordPosition="484" endWordPosition="487">follows. First, we replace V and N2 by their contextually similar words and compute the average adverbial attachment score. Similarly, the average adjectival attachment score is computed by replacing N1 and N2 by their contextually similar words. Attachment scores are obtained using a linear combination of features of the 4-tuple. Finally, we combine the average attachment scores with the attachment score of N2 attaching to the original V and the attachment score of N2 attaching to the original N1. The proposed classification represents the attachment site that scored highest. 1 Previous Work Altmann and Steedman (1988) showed that current discourse context is often required for disambiguating attachments. Recent work shows that it is generally sufficient to utilize lexical information (Brill and Resnik, 1994; Collins and Brooks, 1995; Hindle and Rooth, 1993; Ratnaparkhi et al., 1994). One of the earliest corpus-based approaches to prepositional phrase attachment used lexical preference by computing co-occurrence frequencies (lexical associations) of verbs and nouns with prepositions (Hindle and Rooth, 1993). Training data was obtained by extracting all phrases of the form (V, N1, P, N2) from a large parsed </context>
</contexts>
<marker>Altmann, Steedman, 1988</marker>
<rawString>Altmann, G. and Steedman, M. 1988. Interaction with Context During Human Sentence Processing. Cognition, 30:191-238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based Error-driven Learning and Natural Language Processing: A case study in part of speech tagging.</title>
<date>1995</date>
<booktitle>Computational Linguistics,</booktitle>
<contexts>
<context position="4509" citStr="Brill, 1995" startWordPosition="672" endWordPosition="673">y model considering only lexical information from within the verb phrase (ignoring N2). They experimented with both word features and word class features, their combination yielding 81.6% attachment accuracy. Later, Collins and Brooks (1995) achieved 84.5% accuracy by employing a backed-off model to smooth for unseen events. They discovered that P is the most informative lexical item for attachment disambiguation and keeping low frequency events increases performance. A non-statistical supervised approach by Brill and Resnik (1994) yielded 81.8% accuracy using a transformation-based approach (Brill, 1995) and incorporating word-class information. They report that the top 20 transformations learned involved specific prepositions supporting Collins and Brooksâ€™ claim that the preposition is the most important lexical item for resolving the attachment ambiguity. The state of the art is a supervised algorithm that employs a semantically tagged corpus (Stetina and Nagao, 1997). Each word in a labelled corpus is sense-tagged using an unsupervised word-sense disambiguation algorithm with WordNet (Miller, 1990). Testing examples are classified using a decision tree induced from the training examples. T</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Brill, E. 1995. Transformation-based Error-driven Learning and Natural Language Processing: A case study in part of speech tagging. Computational Linguistics, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P</author>
</authors>
<title>A Rule-Based Approach to Prepositional Phrase Attachment Disambiguation.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING-94. Kyoto,</booktitle>
<marker>P, 1994</marker>
<rawString>Brill, E. and Resnik. P. 1994. A Rule-Based Approach to Prepositional Phrase Attachment Disambiguation. In Proceedings of COLING-94. Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>J Brooks</author>
</authors>
<title>Prepositional Phrase Attachment through a Backed-off Model.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>27--38</pages>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="3406" citStr="Collins and Brooks, 1995" startWordPosition="515" endWordPosition="518">textually similar words. Attachment scores are obtained using a linear combination of features of the 4-tuple. Finally, we combine the average attachment scores with the attachment score of N2 attaching to the original V and the attachment score of N2 attaching to the original N1. The proposed classification represents the attachment site that scored highest. 1 Previous Work Altmann and Steedman (1988) showed that current discourse context is often required for disambiguating attachments. Recent work shows that it is generally sufficient to utilize lexical information (Brill and Resnik, 1994; Collins and Brooks, 1995; Hindle and Rooth, 1993; Ratnaparkhi et al., 1994). One of the earliest corpus-based approaches to prepositional phrase attachment used lexical preference by computing co-occurrence frequencies (lexical associations) of verbs and nouns with prepositions (Hindle and Rooth, 1993). Training data was obtained by extracting all phrases of the form (V, N1, P, N2) from a large parsed corpus. Supervised methods later improved attachment accuracy. Ratnaparkhi et al. (1994) used a maximum entropy model considering only lexical information from within the verb phrase (ignoring N2). They experimented wit</context>
<context position="12327" citStr="Collins and Brooks, 1995" startWordPosition="1959" endWordPosition="1962">ng algorithms (e.g. naÃ¯ve Bayes estimation, back-off estimation, transformation-based learning) were successfully cast as learning linear separators in their feature space. Roth modelled prepositional phrase attachment as linear combinations of features. The features consisted of all 15 possible sub-sequences of the 4-tuple (V, N1, P, N2) shown in Table 4. The asterix (*) in features represent wildcards. Roth used supervised learning to adjust the weights of the features. In our experiments, we only considered features that contained P since the preposition is the most important lexical item (Collins and Brooks, 1995). Furthermore, we omitted features that included both V and N1 since their co-occurrence is independent of the attachment decision. The resulting subset of features considered in our system is shown in bold in Table 4 (equivalent to assigning a weight of 0 or 1 to each feature). Let |head, rel, mod |represent the frequency, obtained from the training data, of the head occurring in the given relationship rel with the modifier. We then assign a score to each feature as follows: (*, *, P, *) = log(|*, P, * |/ |*, *, *|) (V, *, P, N2) = log(|V, P, N2 |/ |*, *, *|) (*, N1, P, N2) = log(|N1, P, N2 |</context>
<context position="22593" citStr="Collins and Brooks, 1995" startWordPosition="3753" endWordPosition="3756">xpect our accuracy to be bounded above by 88.2% and below by 70.39%. 7.3 Results We used the 3097-example testing corpus described in Section 7.1. Table 8 presents the precision and recall of our algorithm and Table 9 presents a performance comparison between our system and previous supervised and unsupervised approaches using the same test data. We describe the different classifiers below: clbase: the baseline described in Section 7.2 clR1: uses a maximum entropy model (Ratnaparkhi et al., 1994) clBR5: uses transformation-based learning (Brill and Resnik, 1994) clCB: uses a backed-off model (Collins and Brooks, 1995) clSN: induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997) clHR6: uses lexical preference (Hindle and Rooth, 1993) clR2: uses a heuristic extraction of unambiguous attachments (Ratnaparkhi, 1998) clPL: uses the algorithm described in this paper Our classifier outperforms all previous unsupervised techniques and approaches the performance of supervised algorithm. We reconstructed the two earlier unsupervised classifiers clHR and clR2. Table 10 presents the accuracy of our reconstructed classifiers. The originally reported accuracy for clR2 i</context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>Collins, M. and Brooks, J. 1995. Prepositional Phrase Attachment through a Backed-off Model. In Proceedings of the Third Workshop on Very Large Corpora, pp. 27-38. Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
<author>M Rooth</author>
</authors>
<date>1993</date>
<booktitle>Structural Ambiguity and Lexical Relations. Computational Linguistics,</booktitle>
<pages>19--1</pages>
<contexts>
<context position="3430" citStr="Hindle and Rooth, 1993" startWordPosition="519" endWordPosition="522">ttachment scores are obtained using a linear combination of features of the 4-tuple. Finally, we combine the average attachment scores with the attachment score of N2 attaching to the original V and the attachment score of N2 attaching to the original N1. The proposed classification represents the attachment site that scored highest. 1 Previous Work Altmann and Steedman (1988) showed that current discourse context is often required for disambiguating attachments. Recent work shows that it is generally sufficient to utilize lexical information (Brill and Resnik, 1994; Collins and Brooks, 1995; Hindle and Rooth, 1993; Ratnaparkhi et al., 1994). One of the earliest corpus-based approaches to prepositional phrase attachment used lexical preference by computing co-occurrence frequencies (lexical associations) of verbs and nouns with prepositions (Hindle and Rooth, 1993). Training data was obtained by extracting all phrases of the form (V, N1, P, N2) from a large parsed corpus. Supervised methods later improved attachment accuracy. Ratnaparkhi et al. (1994) used a maximum entropy model considering only lexical information from within the verb phrase (ignoring N2). They experimented with both word features and</context>
<context position="22761" citStr="Hindle and Rooth, 1993" startWordPosition="3778" endWordPosition="3781">ecision and recall of our algorithm and Table 9 presents a performance comparison between our system and previous supervised and unsupervised approaches using the same test data. We describe the different classifiers below: clbase: the baseline described in Section 7.2 clR1: uses a maximum entropy model (Ratnaparkhi et al., 1994) clBR5: uses transformation-based learning (Brill and Resnik, 1994) clCB: uses a backed-off model (Collins and Brooks, 1995) clSN: induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997) clHR6: uses lexical preference (Hindle and Rooth, 1993) clR2: uses a heuristic extraction of unambiguous attachments (Ratnaparkhi, 1998) clPL: uses the algorithm described in this paper Our classifier outperforms all previous unsupervised techniques and approaches the performance of supervised algorithm. We reconstructed the two earlier unsupervised classifiers clHR and clR2. Table 10 presents the accuracy of our reconstructed classifiers. The originally reported accuracy for clR2 is within the 95% confidence interval of our reconstruction. Our reconstruction of clHR achieved slightly higher accuracy than the original report. 5The accuracy is repo</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Hindle, D. and Rooth, M. 1993. Structural Ambiguity and Lexical Relations. Computational Linguistics, 19(1):103-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic Identification of Non-Compositional Phrases.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL-99,</booktitle>
<pages>317--324</pages>
<location>College Park, Maryland.</location>
<contexts>
<context position="8281" citStr="Lin, 1999" startWordPosition="1284" endWordPosition="1285">ns a total of 11 million unique dependency relationships. 2.2 Corpus-based thesaurus Using the collocation database, Lin (1998b) used an unsupervised method to construct a corpusbased thesaurus consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs. Given a word w, the thesaurus returns a set of similar words of w along with their similarity to w. For example, the 20 most similar words of eat and salad are shown in Table 1. 3 Training Data Extraction We parsed a 125-million word newspaper corpus with Minipar3, a descendent of Principar (Lin, 1994). Minipar outputs dependency trees (Lin, 1999) from the input sentences. For example, the following sentence is decomposed into a dependency tree: Occasionally, the parser generates incorrect dependency trees. For example, in the above sentence, the prepositional phrase headed by with should attach to saw (as opposed to dog). Two separate sets of training data were then extracted from this corpus. Below, we briefly describe how we obtained these data sets. 3.1 Ambiguous Data Set For each input sentence, Minipar outputs a single dependency tree. For a sentence containing one or more prepositions, we use a program to detect any alternative </context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>Lin, D. 1999. Automatic Identification of Non-Compositional Phrases. In Proceedings of ACL-99, pp. 317-324. College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Extracting Collocations from Text Corpora.</title>
<date>1998</date>
<booktitle>Workshop on Computational Terminology.</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="6395" citStr="Lin, 1998" startWordPosition="997" endWordPosition="998"> kid 22, patient 7, refugee 2, rider 1, Russian 1, shark 2, something 19, We 239, wolf 5, ... salad: adj-modifier: assorted 1, crisp 4, fresh 13, good 3, grilled 5, leftover 3, mixed 4, olive 3, prepared 3, side 4, small 6, special 5, vegetable 3, ... object-of: add 3, consume 1, dress 1, grow 1, harvest 2, have 20, like 5, love 1, mix 1, pick 1, place 3, prepare 4, return 3, rinse 1, season 1, serve 8, sprinkle 1, taste 1, test 1, Toss 8, try 3, ... Figure 1. Excepts of entries in the collocation database for eat and salad. Table 1. The top 20 most similar words of eat and salad as given by (Lin, 1998b). WORD SIMILAR WORDS (WITH SIMILARITY SCORE) EAT cook 0.127, drink 0.108, consume 0.101, feed 0.094, taste 0.093, like 0.092, serve 0.089, bake 0.087, sleep 0.086, pick 0.085, fry 0.084, freeze 0.081, enjoy 0.079, smoke 0.078, harvest 0.076, love 0.076, chop 0.074, sprinkle 0.072, Toss 0.072, chew 0.072 SALAD soup 0.172, sandwich 0.169, sauce 0.152, pasta 0.149, dish 0.135, vegetable 0.135, cheese 0.132, dessert 0.13, entree 0.121, bread 0.116, meat 0.116, chicken 0.115, pizza 0.114, rice 0.112, seafood 0.11, dressing 0.109, cake 0.107, steak 0.105, noodle 0.105, bean 0.102 occurrence freque</context>
<context position="7797" citStr="Lin (1998" startWordPosition="1204" endWordPosition="1205">he Internet2. Below, we briefly describe these resources. 2.1 Collocation database Given a word w in a dependency relationship (such as subject or object), the collocation database is used to retrieve the words that occurred in that relationship with w, in a large corpus, along with their frequencies (Lin, 1998a). Figure 1 shows excerpts of the entries in 2Available at www.cs.ualberta.ca/~lindek/demos.htm. the collocation database for the words eat and salad. The database contains a total of 11 million unique dependency relationships. 2.2 Corpus-based thesaurus Using the collocation database, Lin (1998b) used an unsupervised method to construct a corpusbased thesaurus consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs. Given a word w, the thesaurus returns a set of similar words of w along with their similarity to w. For example, the 20 most similar words of eat and salad are shown in Table 1. 3 Training Data Extraction We parsed a 125-million word newspaper corpus with Minipar3, a descendent of Principar (Lin, 1994). Minipar outputs dependency trees (Lin, 1999) from the input sentences. For example, the following sentence is decomposed into a dependency tree: Occasionally, t</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. 1998a. Extracting Collocations from Text Corpora. Workshop on Computational Terminology. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL98.</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="6395" citStr="Lin, 1998" startWordPosition="997" endWordPosition="998"> kid 22, patient 7, refugee 2, rider 1, Russian 1, shark 2, something 19, We 239, wolf 5, ... salad: adj-modifier: assorted 1, crisp 4, fresh 13, good 3, grilled 5, leftover 3, mixed 4, olive 3, prepared 3, side 4, small 6, special 5, vegetable 3, ... object-of: add 3, consume 1, dress 1, grow 1, harvest 2, have 20, like 5, love 1, mix 1, pick 1, place 3, prepare 4, return 3, rinse 1, season 1, serve 8, sprinkle 1, taste 1, test 1, Toss 8, try 3, ... Figure 1. Excepts of entries in the collocation database for eat and salad. Table 1. The top 20 most similar words of eat and salad as given by (Lin, 1998b). WORD SIMILAR WORDS (WITH SIMILARITY SCORE) EAT cook 0.127, drink 0.108, consume 0.101, feed 0.094, taste 0.093, like 0.092, serve 0.089, bake 0.087, sleep 0.086, pick 0.085, fry 0.084, freeze 0.081, enjoy 0.079, smoke 0.078, harvest 0.076, love 0.076, chop 0.074, sprinkle 0.072, Toss 0.072, chew 0.072 SALAD soup 0.172, sandwich 0.169, sauce 0.152, pasta 0.149, dish 0.135, vegetable 0.135, cheese 0.132, dessert 0.13, entree 0.121, bread 0.116, meat 0.116, chicken 0.115, pizza 0.114, rice 0.112, seafood 0.11, dressing 0.109, cake 0.107, steak 0.105, noodle 0.105, bean 0.102 occurrence freque</context>
<context position="7797" citStr="Lin (1998" startWordPosition="1204" endWordPosition="1205">he Internet2. Below, we briefly describe these resources. 2.1 Collocation database Given a word w in a dependency relationship (such as subject or object), the collocation database is used to retrieve the words that occurred in that relationship with w, in a large corpus, along with their frequencies (Lin, 1998a). Figure 1 shows excerpts of the entries in 2Available at www.cs.ualberta.ca/~lindek/demos.htm. the collocation database for the words eat and salad. The database contains a total of 11 million unique dependency relationships. 2.2 Corpus-based thesaurus Using the collocation database, Lin (1998b) used an unsupervised method to construct a corpusbased thesaurus consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs. Given a word w, the thesaurus returns a set of similar words of w along with their similarity to w. For example, the 20 most similar words of eat and salad are shown in Table 1. 3 Training Data Extraction We parsed a 125-million word newspaper corpus with Minipar3, a descendent of Principar (Lin, 1994). Minipar outputs dependency trees (Lin, 1999) from the input sentences. For example, the following sentence is decomposed into a dependency tree: Occasionally, t</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. 1998b. Automatic Retrieval and Clustering of Similar Words. In Proceedings of COLING-ACL98. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Principar - an Efficient, Broad-Coverage, Principle-Based Parser.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING-94. Kyoto,</booktitle>
<contexts>
<context position="8235" citStr="Lin, 1994" startWordPosition="1278" endWordPosition="1279">r the words eat and salad. The database contains a total of 11 million unique dependency relationships. 2.2 Corpus-based thesaurus Using the collocation database, Lin (1998b) used an unsupervised method to construct a corpusbased thesaurus consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs. Given a word w, the thesaurus returns a set of similar words of w along with their similarity to w. For example, the 20 most similar words of eat and salad are shown in Table 1. 3 Training Data Extraction We parsed a 125-million word newspaper corpus with Minipar3, a descendent of Principar (Lin, 1994). Minipar outputs dependency trees (Lin, 1999) from the input sentences. For example, the following sentence is decomposed into a dependency tree: Occasionally, the parser generates incorrect dependency trees. For example, in the above sentence, the prepositional phrase headed by with should attach to saw (as opposed to dog). Two separate sets of training data were then extracted from this corpus. Below, we briefly describe how we obtained these data sets. 3.1 Ambiguous Data Set For each input sentence, Minipar outputs a single dependency tree. For a sentence containing one or more preposition</context>
</contexts>
<marker>Lin, 1994</marker>
<rawString>Lin, D. (1994). Principar - an Efficient, Broad-Coverage, Principle-Based Parser. In Proceedings of COLING-94. Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: an On-Line Lexical Database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<contexts>
<context position="5016" citStr="Miller, 1990" startWordPosition="743" endWordPosition="744">oach by Brill and Resnik (1994) yielded 81.8% accuracy using a transformation-based approach (Brill, 1995) and incorporating word-class information. They report that the top 20 transformations learned involved specific prepositions supporting Collins and Brooksâ€™ claim that the preposition is the most important lexical item for resolving the attachment ambiguity. The state of the art is a supervised algorithm that employs a semantically tagged corpus (Stetina and Nagao, 1997). Each word in a labelled corpus is sense-tagged using an unsupervised word-sense disambiguation algorithm with WordNet (Miller, 1990). Testing examples are classified using a decision tree induced from the training examples. They report 88.1% attachment accuracy approaching the human accuracy of 88.2% (Ratnaparkhi et al., 1994). The current unsupervised state of the art achieves 81.9% attachment accuracy (Ratnaparkhi, 1998). Using an extraction heuristic, unambiguous prepositional phrase attachments of the form (V, P, N2) and (N1, P, N2) are extracted from a large corpus. Coeat: object: almond 1, apple 25, bean 5, beam 1, binge 1, bread 13, cake 17, cheese 8, dish 14, disorder 20, egg 31, grape 12, grub 2, hay 3, junk 1, me</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>Miller, G. 1990. Wordnet: an On-Line Lexical Database. International Journal of Lexicography, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Unsupervised Statistical Models for Prepositional Phrase Attachment.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL98.</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="5310" citStr="Ratnaparkhi, 1998" startWordPosition="784" endWordPosition="785"> is the most important lexical item for resolving the attachment ambiguity. The state of the art is a supervised algorithm that employs a semantically tagged corpus (Stetina and Nagao, 1997). Each word in a labelled corpus is sense-tagged using an unsupervised word-sense disambiguation algorithm with WordNet (Miller, 1990). Testing examples are classified using a decision tree induced from the training examples. They report 88.1% attachment accuracy approaching the human accuracy of 88.2% (Ratnaparkhi et al., 1994). The current unsupervised state of the art achieves 81.9% attachment accuracy (Ratnaparkhi, 1998). Using an extraction heuristic, unambiguous prepositional phrase attachments of the form (V, P, N2) and (N1, P, N2) are extracted from a large corpus. Coeat: object: almond 1, apple 25, bean 5, beam 1, binge 1, bread 13, cake 17, cheese 8, dish 14, disorder 20, egg 31, grape 12, grub 2, hay 3, junk 1, meat 70, poultry 3, rabbit 4, soup 5, sandwich 18, pasta 7, vegetable 35, ... subject: adult 3, animal 8, beetle 1, cat 3, child 41, decrease 1, dog 24, family 29, guest 7, kid 22, patient 7, refugee 2, rider 1, Russian 1, shark 2, something 19, We 239, wolf 5, ... salad: adj-modifier: assorted </context>
<context position="10920" citStr="Ratnaparkhi, 1998" startWordPosition="1745" endWordPosition="1746">= VScore(Wk, Pk, N2k) if Wk = Vk; otherwise Score(Wk, Pk, N2k) = NScore(Wk, Pk, N2k). Suppose that after the initial frequency table is set NScore(man, in, park) = 1.23, VScore(saw, with, telescope) = 3.65, and NScore(dog, with, telescope) = 0.35. Then, the updated cooccurrence frequencies for (man, in, park) and (saw, with, telescope) are: = 1.23 fr(man, in, park) 1.23 = 1.0 3.65 fr(saw, with, telescope) = 3.65+0. 35 = 0.913 Table 3 shows the updated frequency table after the first iteration of the algorithm. The resulting database contained 8,900,000 triples. 3.2 Unambiguous Data Set As in (Ratnaparkhi, 1998), we constructed a training data set consisting of only unambiguous subj pcomp obj pcomp det mod det det mod det A man in the park saw a dog with a telescope. attachments of the form (V, P, N2) and (N1, P, N2). We only extract a 3-tuple from a sentence when our program finds no alternative attachment site for its preposition. Each extracted 3-tuple is assigned a frequency count of 1. For example, in the previous sentence, (man, in, park) is extracted since it contains only one attachment site; (dog, with, telescope) is not extracted since with has an alternative attachment site. The resulting </context>
<context position="22842" citStr="Ratnaparkhi, 1998" startWordPosition="3790" endWordPosition="3791">een our system and previous supervised and unsupervised approaches using the same test data. We describe the different classifiers below: clbase: the baseline described in Section 7.2 clR1: uses a maximum entropy model (Ratnaparkhi et al., 1994) clBR5: uses transformation-based learning (Brill and Resnik, 1994) clCB: uses a backed-off model (Collins and Brooks, 1995) clSN: induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997) clHR6: uses lexical preference (Hindle and Rooth, 1993) clR2: uses a heuristic extraction of unambiguous attachments (Ratnaparkhi, 1998) clPL: uses the algorithm described in this paper Our classifier outperforms all previous unsupervised techniques and approaches the performance of supervised algorithm. We reconstructed the two earlier unsupervised classifiers clHR and clR2. Table 10 presents the accuracy of our reconstructed classifiers. The originally reported accuracy for clR2 is within the 95% confidence interval of our reconstruction. Our reconstruction of clHR achieved slightly higher accuracy than the original report. 5The accuracy is reported in (Collins and Brooks, 1995). 6The accuracy was obtained on a smaller test </context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Ratnaparkhi, A. 1998. Unsupervised Statistical Models for Prepositional Phrase Attachment. In Proceedings of COLINGACL98. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
<author>J Reynar</author>
<author>S Roukos</author>
</authors>
<title>A Maximum Entropy Model for Prepositional Phrase Attachment.</title>
<date>1994</date>
<booktitle>In Proceedings of the ABPA Human Language Technology Workshop,</booktitle>
<pages>250--255</pages>
<location>Plainsboro, N.J.</location>
<contexts>
<context position="3457" citStr="Ratnaparkhi et al., 1994" startWordPosition="523" endWordPosition="526">ained using a linear combination of features of the 4-tuple. Finally, we combine the average attachment scores with the attachment score of N2 attaching to the original V and the attachment score of N2 attaching to the original N1. The proposed classification represents the attachment site that scored highest. 1 Previous Work Altmann and Steedman (1988) showed that current discourse context is often required for disambiguating attachments. Recent work shows that it is generally sufficient to utilize lexical information (Brill and Resnik, 1994; Collins and Brooks, 1995; Hindle and Rooth, 1993; Ratnaparkhi et al., 1994). One of the earliest corpus-based approaches to prepositional phrase attachment used lexical preference by computing co-occurrence frequencies (lexical associations) of verbs and nouns with prepositions (Hindle and Rooth, 1993). Training data was obtained by extracting all phrases of the form (V, N1, P, N2) from a large parsed corpus. Supervised methods later improved attachment accuracy. Ratnaparkhi et al. (1994) used a maximum entropy model considering only lexical information from within the verb phrase (ignoring N2). They experimented with both word features and word class features, their</context>
<context position="5212" citStr="Ratnaparkhi et al., 1994" startWordPosition="769" endWordPosition="772">rmations learned involved specific prepositions supporting Collins and Brooksâ€™ claim that the preposition is the most important lexical item for resolving the attachment ambiguity. The state of the art is a supervised algorithm that employs a semantically tagged corpus (Stetina and Nagao, 1997). Each word in a labelled corpus is sense-tagged using an unsupervised word-sense disambiguation algorithm with WordNet (Miller, 1990). Testing examples are classified using a decision tree induced from the training examples. They report 88.1% attachment accuracy approaching the human accuracy of 88.2% (Ratnaparkhi et al., 1994). The current unsupervised state of the art achieves 81.9% attachment accuracy (Ratnaparkhi, 1998). Using an extraction heuristic, unambiguous prepositional phrase attachments of the form (V, P, N2) and (N1, P, N2) are extracted from a large corpus. Coeat: object: almond 1, apple 25, bean 5, beam 1, binge 1, bread 13, cake 17, cheese 8, dish 14, disorder 20, egg 31, grape 12, grub 2, hay 3, junk 1, meat 70, poultry 3, rabbit 4, soup 5, sandwich 18, pasta 7, vegetable 35, ... subject: adult 3, animal 8, beetle 1, cat 3, child 41, decrease 1, dog 24, family 29, guest 7, kid 22, patient 7, refuge</context>
<context position="19905" citStr="Ratnaparkhi et al., 1994" startWordPosition="3300" endWordPosition="3303"> S(V) = -3.42 and S(N1) = -4.78 Since S(V) &gt; S(N1), the algorithm correctly classifies this example as an adverbial attachment. Given the 4-tuple (eat, salad, with, croutons), the algorithm returns S(V) = -4.31 and S(N1) = -3.88. Hence, the algorithm correctly attaches the prepositional phrase to the noun salad. 7 Experimental Results In this section, we describe our test data and the baseline for our experiments. Finally, we present our results. 7.1 Test Data The test data consists of 3097 examples derived from the manually annotated attachments in the Penn Treebank Wall Street Journal data (Ratnaparkhi et al., 1994)4. Each line in the test data consists of a 4-tuple and a target classification: V N1 P N2 target. 4Available at ftp.cis.upenn.edu/pub/adwait/PPattachData. Input: A 4-tuple (V, N1, P, N2) Step 1: Using the contextually similar words algorithm and the formulas from Section 5.2 compute: averageV = VScore&apos;(V, P, N2) averageN1 = NScore&apos;(N1, P, N2) Step 2: Compute the adverbial attachment score, av, and the adjectival attachment score, an1: aV = VScore(V, P, N2) aN1 = NScore(N1, P, N2) Step 3: Retrieve from the training data set the frequency of the 3-tuples (V, P, N2) and (N1, P, N2) 4 fV and fN1,</context>
<context position="21891" citStr="Ratnaparkhi et al., 1994" startWordPosition="3645" endWordPosition="3648">ins several erroneous tuples and attachments. For instance, 133 examples contain the word the as N1 or N2. There are also improbable attachments such as (sing, birthday, to, you) with the target attachment birthday. Table 8. Precision and recall for attachment sites V and N1. 7.2 Baseline Choosing the most common attachment site, N1, yields an accuracy of 58.96%. However, we achieve 70.39% accuracy by classifying each occurrence of P = of as N1, and V otherwise. Human accuracy, given the full context of a sentence, is 93.2% and drops to 88.2% when given only tuples of the form (V, N1, P, N2) (Ratnaparkhi et al., 1994). Assuming that human accuracy is the upper bound for automatic methods, we expect our accuracy to be bounded above by 88.2% and below by 70.39%. 7.3 Results We used the 3097-example testing corpus described in Section 7.1. Table 8 presents the precision and recall of our algorithm and Table 9 presents a performance comparison between our system and previous supervised and unsupervised approaches using the same test data. We describe the different classifiers below: clbase: the baseline described in Section 7.2 clR1: uses a maximum entropy model (Ratnaparkhi et al., 1994) clBR5: uses transform</context>
</contexts>
<marker>Ratnaparkhi, Reynar, Roukos, 1994</marker>
<rawString>Ratnaparkhi, A., Reynar, J., and Roukos, S. 1994. A Maximum Entropy Model for Prepositional Phrase Attachment. In Proceedings of the ABPA Human Language Technology Workshop, pp. 250-255. Plainsboro, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning to Resolve Natural Language Ambiguities: A Unified Approach.</title>
<date>1998</date>
<booktitle>In Proceedings of AAAI-98,</booktitle>
<pages>806--813</pages>
<location>Madison, Wisconsin.</location>
<contexts>
<context position="11592" citStr="Roth (1998)" startWordPosition="1858" endWordPosition="1859">iguous subj pcomp obj pcomp det mod det det mod det A man in the park saw a dog with a telescope. attachments of the form (V, P, N2) and (N1, P, N2). We only extract a 3-tuple from a sentence when our program finds no alternative attachment site for its preposition. Each extracted 3-tuple is assigned a frequency count of 1. For example, in the previous sentence, (man, in, park) is extracted since it contains only one attachment site; (dog, with, telescope) is not extracted since with has an alternative attachment site. The resulting database contained 4,400,000 triples. 4 Classification Model Roth (1998) presented a unified framework for natural language disambiguation tasks. Essentially, several language learning algorithms (e.g. naÃ¯ve Bayes estimation, back-off estimation, transformation-based learning) were successfully cast as learning linear separators in their feature space. Roth modelled prepositional phrase attachment as linear combinations of features. The features consisted of all 15 possible sub-sequences of the 4-tuple (V, N1, P, N2) shown in Table 4. The asterix (*) in features represent wildcards. Roth used supervised learning to adjust the weights of the features. In our experi</context>
</contexts>
<marker>Roth, 1998</marker>
<rawString>Roth, D. 1998. Learning to Resolve Natural Language Ambiguities: A Unified Approach. In Proceedings of AAAI-98, pp. 806-813. Madison, Wisconsin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Stetina</author>
<author>M Nagao</author>
</authors>
<title>Corpus Based PP Attachment Ambiguity Resolution with a Semantic Dictionary.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Workshop on Very Large Corpora,</booktitle>
<pages>66--80</pages>
<institution>Beijing and Hong Kong.</institution>
<contexts>
<context position="4882" citStr="Stetina and Nagao, 1997" startWordPosition="723" endWordPosition="726"> informative lexical item for attachment disambiguation and keeping low frequency events increases performance. A non-statistical supervised approach by Brill and Resnik (1994) yielded 81.8% accuracy using a transformation-based approach (Brill, 1995) and incorporating word-class information. They report that the top 20 transformations learned involved specific prepositions supporting Collins and Brooksâ€™ claim that the preposition is the most important lexical item for resolving the attachment ambiguity. The state of the art is a supervised algorithm that employs a semantically tagged corpus (Stetina and Nagao, 1997). Each word in a labelled corpus is sense-tagged using an unsupervised word-sense disambiguation algorithm with WordNet (Miller, 1990). Testing examples are classified using a decision tree induced from the training examples. They report 88.1% attachment accuracy approaching the human accuracy of 88.2% (Ratnaparkhi et al., 1994). The current unsupervised state of the art achieves 81.9% attachment accuracy (Ratnaparkhi, 1998). Using an extraction heuristic, unambiguous prepositional phrase attachments of the form (V, P, N2) and (N1, P, N2) are extracted from a large corpus. Coeat: object: almon</context>
<context position="22705" citStr="Stetina and Nagao, 1997" startWordPosition="3770" endWordPosition="3773"> corpus described in Section 7.1. Table 8 presents the precision and recall of our algorithm and Table 9 presents a performance comparison between our system and previous supervised and unsupervised approaches using the same test data. We describe the different classifiers below: clbase: the baseline described in Section 7.2 clR1: uses a maximum entropy model (Ratnaparkhi et al., 1994) clBR5: uses transformation-based learning (Brill and Resnik, 1994) clCB: uses a backed-off model (Collins and Brooks, 1995) clSN: induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997) clHR6: uses lexical preference (Hindle and Rooth, 1993) clR2: uses a heuristic extraction of unambiguous attachments (Ratnaparkhi, 1998) clPL: uses the algorithm described in this paper Our classifier outperforms all previous unsupervised techniques and approaches the performance of supervised algorithm. We reconstructed the two earlier unsupervised classifiers clHR and clR2. Table 10 presents the accuracy of our reconstructed classifiers. The originally reported accuracy for clR2 is within the 95% confidence interval of our reconstruction. Our reconstruction of clHR achieved slightly higher </context>
</contexts>
<marker>Stetina, Nagao, 1997</marker>
<rawString>Stetina, J. and Nagao, M. 1997. Corpus Based PP Attachment Ambiguity Resolution with a Semantic Dictionary. In Proceedings of the Fifth Workshop on Very Large Corpora, pp. 66-80. Beijing and Hong Kong.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>