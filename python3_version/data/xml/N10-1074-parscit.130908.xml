<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000041">
<title confidence="0.995125">
Learning Words and Their Meanings from Unsegmented Child-directed
Speech
</title>
<author confidence="0.981576">
Bevan K. Jones &amp; Mark Johnson Michael C. Frank
</author>
<affiliation confidence="0.9805235">
Dept of Cognitive and Linguistic Sciences Dept of Brain and Cognitive Science
Brown University Massachusetts Institute of Technology
</affiliation>
<address confidence="0.656727">
Providence, RI 02912, USA Cambridge, MA 02139, USA
</address>
<email confidence="0.969886">
{Bevan Jones,Mark Johnson}@Brown.edu mcfrank@mit.edu
</email>
<sectionHeader confidence="0.995897" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999790588235294">
Most work on language acquisition treats
word segmentation—the identification of lin-
guistic segments from continuous speech—
and word learning—the mapping of those seg-
ments to meanings—as separate problems.
These two abilities develop in parallel, how-
ever, raising the question of whether they
might interact. To explore the question, we
present a new Bayesian segmentation model
that incorporates aspects of word learning and
compare it to a model that ignores word mean-
ings. The model that learns word meanings
proposes more adult-like segmentations for
the meaning-bearing words. This result sug-
gests that the non-linguistic context may sup-
ply important information for learning word
segmentations as well as word meanings.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999928367346939">
Acquiring a language entails mastering many learn-
ing tasks simultaneously, including identifying
where words begin and end in continuous speech
and learning meanings for those words. It is com-
mon to treat these tasks as separate, sequential pro-
cesses, where segmentation is a prerequisite to word
learning but otherwise there are few if any depen-
dencies. The earliest evidence of segmentation,
however, is for words bordering a child’s own name
(Bortfeld et al., 2005). In addition, infants begin
learning their first words before they achieve adult-
level competence in segmentation. These two pieces
of evidence raise the question of whether the tasks of
meaning learning and segmentation might mutually
inform one another.
To explore this question we present a joint model
that simultaneously identifies word boundaries and
attempts to associate meanings with words. In do-
ing so we make two contributions. First, by model-
ing the two levels of structure in parallel we simu-
late a more realistic situation. Second, a joint model
allows us to explore possible synergies and interac-
tions. We find evidence that our joint model per-
forms better on a segmentation task than an alterna-
tive model that does not learn word meanings.
The picture in Figure 1 depicts a language learn-
ing situation from our corpus (originally from Fer-
nald and Morikawa, 1993; recoded in Frank et al.,
2009) where a mother talks while playing with var-
ious toys. Setting down the dog and picking up the
hand puppet of a pig, she asks, “Is that the pig?”
Starting out, a young learner not only does not know
that the word “pig” refers to the puppet but does not
even know that “pig” is a word at all. Our model
simulates the learning task, taking as input the un-
segmented phonemic representation of the speech
along with the set of objects in the non-linguistic
context as shown in Figure 1 (a), and infers both a
segmentation and a word-object mapping as in Fig-
ure 1 (b).
One can formulate the word learning task as
that of finding a reasonably small set of reusable
word-meaning pairs consistent with the underlying
communicative intent. Infant directed speech often
refers to objects in the immediate environment, and
early word learning seems to involve associating fre-
quently co-occurring word-object pairs (Akhtar and
Montague, 1999; Markman, 1990). Several compu-
tational models are based on this idea that a word
</bodyText>
<page confidence="0.965561">
501
</page>
<note confidence="0.8485675">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 501–509,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999721">
Figure 1: (a) The input to our system for the utterance
</figureCaption>
<bodyText confidence="0.940322527272727">
“Is that the pig?” consists of an unsegmented sequence
of phonemes and the set of objects representing the non-
linguistic context. These objects were manually iden-
tified by inspecting the associated video, a frame from
which is shown above. (b) The gold-standard segmenta-
tion and word-object assignments of the same utterance,
against which the output of our system is evaluated (all
words except “pIg” are mapped to a special “null” object,
as explained in the text).
that frequently occurs in the presence of an object
and not so frequently in its absence is likely to re-
fer to that object (Frank et al., 2009a; Siskind, 1996;
Yu and Ballard, 2007). Importantly, all these models
assume words are pre-segmented in the input.
While the word segmentation task relates less
clearly to the communicative content, it can be for-
mulated according to a similar objective, that of at-
tempting to explain the sound sequences in the input
in terms of some reasonably small set of reusable
units, or words. Computational models have suc-
cessfully addressed the problem in much this way
(Johnson and Goldwater, 2009; Goldwater et al.,
2009; Brent, 1999), and the general approach is con-
sistent with experimental observations that humans
are sensitive to statistics of sound sequences (Saffran
et al., 1996; Frank et al., 2007).
The two tasks can be integrated in a relatively
seamless way, since, as we have just formulated
them, they have a common objective, that of finding
a minimal, consistent set of reusable units. However,
the two deal with different types of information with
different dependencies. The basic idea is that learn-
ing a vocabulary that both meets the constraints of
the word-learning task and is consistent with the ob-
jective of the segmentation task can yield a better
segmentation. That is, we hope to find a synergy in
the joint inference of meaning and segmentation.
Note that to the best of our knowledge there is
very little computational work that combines word
form and word meaning learning (Frank et al. 2006
takes a first step but their model is applicable only
to small artificial languages). Frank et al. (2009a)
and Regier (2003) review pure word learning mod-
els and, in addition to the papers we have already
cited, Brent (1999) presents a fairly comprehensive
review of previous pure segmentation models. How-
ever, none of the models reviewed make any attempt
to jointly address the two problems. Similarly, in the
behavioral literature on development, we are aware
of only one segmentation study (Graf-Estes et al.,
2007) that involves non-linguistic context, though
this study treats the two tasks sequentially rather
than jointly.
We now describe our model and inference proce-
dure and follow with evaluation and discussion.
</bodyText>
<sectionHeader confidence="0.942016" genericHeader="method">
2 Model Definition
</sectionHeader>
<bodyText confidence="0.996617238095238">
Cross-situational meaning learning in our joint word
learning and segmenting model is inspired by the
model of Frank et al. (2009a). Our model can
be viewed as a variant of the Latent Dirichlet Al-
location (LDA) topic model of Blei et al. (2003),
where topics are drawn from the objects in the non-
linguistic context. The model associates each utter-
ance with a single referent object, the topic, and ev-
ery word in the utterance is either generated from a
distribution over words associated with that object
or else from a distribution associated with a special
“null” object shared by all utterances. Note that in
this paper we use “topic” to denote the referent ob-
ject of an utterance, otherwise we depart from topic
modeling convention and use the term “object” in-
stead.
Segmentation is based on the unigram model pro-
posed by Brent (1999) and reformulated by Goldwa-
ter et al. (2009) in terms of a Dirichlet process. Since
both LDA and the unigram segmenter are based on
unigram distributions it is relatively straightforward
</bodyText>
<page confidence="0.997748">
502
</page>
<figureCaption confidence="0.9951316">
Figure 2: Topical Unigram Model: Oj is the set of objects
in the non-linguistic context of the jth utterance, zj is the
utterance topic, wji is the ith word of the utterance, xji is
the category of the word (referring or non-referring), and
the other variables are distribution parameters.
</figureCaption>
<bodyText confidence="0.9834916">
to integrate the two to simultaneously infer word
boundaries and word-object associations.
Figure 2 illustrates a slightly simplified form of
the model, and the the relevant distributions are as
follows:
</bodyText>
<equation confidence="0.997821875">
Gz  |z, α0, α1
e
π ∼ Beta(1,1)
x|π ∼ Bernoulli(π)
�
Gz if x = 1
w|G,z,x ∼
G0 if x = 0
</equation>
<bodyText confidence="0.995114230769231">
Note that Uniform(O) denotes a discrete uniform
distribution over the elements of the set O. P0 is
described later.
Briefly, each utterance has a single topic zj, drawn
from the objects in the non-linguistic context Oj,
and then for each word wji we first flip a coin xji
to determine if it refers to the topic or not. Then, de-
pending on xji the word is either drawn from a dis-
tribution specific to the topic (xji = 1) or from a dis-
tribution associated with the “null” object (xji = 0).
In slightly greater detail but still glossing over the
details of how the multinomial parameters are gen-
erated, the generative story proceeds as follows:
</bodyText>
<listItem confidence="0.9973178">
1. For each utterance, indexed by j
2. (a) Pick a single topic zj uniformly from the set
of objects in the environment Oj
(b) For each word wji of the utterance
(c) i. Determine if it refers to zj or not by set-
</listItem>
<bodyText confidence="0.94317545">
ting xji to 1(referring) with probability π,
and to 0 (non-referring) otherwise.
ii. if xji is 1, draw wji from the topic specific
distribution over words Gz,.
iii. otherwise, draw wji from Go, the distribu-
tion over words associated with the “null”
object.
This generative story is a simplification since it
does not describe how we model utterance bound-
aries. It is important for segmentation purposes
to explicitly model utterance boundaries since, un-
like utterance-internal word boundaries, we as-
sume utterance boundaries are observed. Thus,
the story is complicated by the fact that there is
a chance each time we generate a word that we
also generate an utterance boundary. The choice of
whether to terminate the utterance or not is captured
by a Bernoulli(γ) random variable $ji indicating
whether the ith word was the last word of the jth
utterance.
</bodyText>
<equation confidence="0.52121">
γ ∼ Beta(1,1)
$|γ ∼ Bernoulli(γ)
</equation>
<bodyText confidence="0.999688428571429">
The Gz multinomial parameters are generated
from a Dirichlet process with base distribution over
words, P0, which describes how new word types
are generated from their constituent phonemes.
Phonemes are generated sequentially, i.i.d. uni-
formly from m phonemic types. In addition, there
is a probability p# of generating a word boundary.
</bodyText>
<equation confidence="0.968468">
1
P0(w) = (1 − p#)|w|−1p#
m|w|
</equation>
<bodyText confidence="0.999958">
The concentration parameters α0 and α1 also play
a critical role in the generation of words and word
types. Any given word has a certain probability
of either being produced from the set of previously
seen word types, or from an entirely new one. The
</bodyText>
<equation confidence="0.7878805">
z|O ∼ Uniform(O)
�
DP(α1, P0) if z =6 0
DP(α0, P0) otherwis
P0
∼
</equation>
<page confidence="0.987521">
503
</page>
<bodyText confidence="0.99979028">
greater the concentration parameter, the more likely
the model is to appeal to the base distribution P0 to
introduce a new word type.
Like Frank et al. (2009a), we distinguish between
two coarse grammatical categories, referring and
non-referring. Referring words are generated by the
topic, while non-referring words are drawn from G0,
a distribution associated with the “null” object. The
distinction ensures sparse word-object maps that
obey the principle of mutual exclusion. Otherwise
all words in the utterance would be associated with
the topic object, resulting in a very large set of words
for each object that is very likely to overlap with the
words for other objects. As a further bias toward
a small lexicon, we employ different concentration
parameters (α0 and α1) for the non-referring and re-
ferring words, using a much smaller value for the
referring words. Intuitively, there should be a rela-
tively small prior probability of introducing a new
word-object pair, corresponding to a small α1 value.
On the other hand, most other words don’t refer to
the topic object (or any other object for that matter),
corresponding to a much larger α0 value.
Note that this topical unigram model is a straight-
forward generalization of the unigram segmentation
model (Goldwater et al., 2009) to the case of multi-
ple topics. In fact, if all words were assumed to refer
to the same object (or to no object at all) the models
would be identical.
Unlike LDA, each “document” has only one topic,
which is necessitated by the fact that in our model
documents correspond single utterances. The ut-
terances in our corpus of child directed speech are
often only four or five words long, whereas the
general LDA model assumes documents are much
larger. Thus, there may not be enough words to in-
fer a useful utterance specific distribution over top-
ics. Consequently, rather than inferring a separate
topic distribution for each utterance, we simply as-
sume a uniform distribution over objects in the non-
linguistic context. In effect, we rely entirely on the
non-linguistic context and word-object associations
to infer topics. Though necessitated by data sparsity
issues, we also note that it is very rare in our cor-
pus for utterances to refer to more than one object in
the non-linguistic context, so the choice of a single
topic may also be a more accurate model. In fact,
even with multi-sentence documents, LDA may per-
form better if only one topic is assumed per sentence
(Gruber et al., 2007).
</bodyText>
<sectionHeader confidence="0.999222" genericHeader="method">
3 Inference
</sectionHeader>
<bodyText confidence="0.999997571428571">
We use a collapsed Gibbs sampling procedure, in-
tegrating over all possible Gz, 7r, and -y values and
then iteratively sample values for each variable con-
ditioned on the current state of all other variables.
We visit each utterance once per iteration, sample a
topic, and then visit each possible word boundary lo-
cation to sample the boundary and word categories
simultaneously according to their joint probability.
A single topic is sampled for each utterance, con-
ditioned on the words and their current determina-
tions as referring or non-referring. Since zj is drawn
from a uniform distribution, this probability is sim-
ply proportionate to the conditional probability of
the words given zj and the xji variables.
</bodyText>
<equation confidence="0.999225666666667">
Γ((Ew3 nW. whz + α1P0((w))
P(zj |wj, xj, h−j) a Γ(Ewj n(h)j + α1P0(w))
Γ(n(h)
w,zj + α1P0(w))
Γ(n(h�)
w,zj + α1P0(w))
</equation>
<bodyText confidence="0.978778625">
Here, P(zj|wj, xj, h−j) is the probability of topic
zj given the current hypothesis h for all variables ex-
cluding those for the current utterance. Also, n(h�j)
w,zj
is the count of occurrences of word type w that refer
to topic zj among the current variable assignments,
and Wj is the set of word types appearing in utter-
ance j. The vectors of word and category variables
in utterance j are represented as wj and xj, respec-
tively. Note that only referring words have any bear-
ing on the appropriate selection of zj and so all fac-
tors involving only non-referring words are absorbed
by the constant of proportionality.
The word categories can be sampled conditioned
on the current word boundary states according to the
following conditional probability, where n(h�ji)
</bodyText>
<figure confidence="0.549174">
xji is
the number of words categorized according to label
Wj
ri�
w
</figure>
<page confidence="0.981647">
504
</page>
<bodyText confidence="0.959625">
xji over the entire corpus excluding word wji.
</bodyText>
<equation confidence="0.999287166666667">
P(xji|wji, zj, h−ji) a P(wji|zj, xji, h−ji)
·P(xji|h−ji)
(h−ji)
nxji + 1
· (h−ji)
n• + 2
</equation>
<bodyText confidence="0.999832545454545">
In practice, however, we actually sample the word
category variables jointly with the boundary states,
using a scheme similar to that outlined in Gold-
water et al. (2009). We visit each possible word
boundary location (any point between two consec-
utive phonemes) and compute probabilities for the
hypotheses for which the phonemic environment
makes up either one word or two. As illustrated be-
low there are two sets of cases: those where we treat
the segment as a single word, and those where we
treat it as two words.
</bodyText>
<equation confidence="0.997283">
x1 x2 x3
...#w1#... vs. . . .#w2#w3#...
T T
</equation>
<bodyText confidence="0.99994675">
The probabilities of the hypotheses can be derived
by application of equation 1. Since the x variables
can each describe two possible events, there are a to-
tal of six different cases to consider for each bound-
ary assignment: two cases without and four with a
word boundary.
The probability of each of the two cases without
a word boundary can be computed as follows:
</bodyText>
<equation confidence="0.996523">
(h−)
nw1,x1z + αx1P0(w1)
P(w1, x1|z, h−) =
(h−)
n•,x1z + αx1
</equation>
<bodyText confidence="0.999984714285714">
Here h− signifies the current hypothesis for all
variables excluding those for the current segment
and n(h−) $1is the count for h− of either utterance fi-
nal words if w1 is utterance final or non-utterance
final words if w1 is also not utterance final.
In the four cases with a word boundary, we have
two words and two categories to sample.
</bodyText>
<equation confidence="0.995467157894737">
(h−)
nw2,x2z + αx2P0(w2)
P(w2, x2, w3, x3|z, h−) = (h−)
n•,x2z + αx2
(h−)
nx2 + 1 ·
(h−)
n• + 2
(h−)
nw3,x3z + δx2(x3)δw2(w3) + αx3P0(w3)
·
(h−)
n•,x3z + δx2(x3) + αx3
(h−)
n$3 + δ$2($3) + 1
·
(h−)
n• + 3
Here δx(y) is 1 if x = y and 0 otherwise.
</equation>
<sectionHeader confidence="0.999876" genericHeader="evaluation">
4 Results &amp; Model Comparisons
</sectionHeader>
<subsectionHeader confidence="0.904281">
4.1 Corpus
</subsectionHeader>
<bodyText confidence="0.9164661875">
Our training corpus (Fernald and Morikawa, 1993;
Frank et al., 2009b) consists of about 22,000 words
and 5,600 utterances. Video recordings consisting
of mother-child play over pairs of toys were ortho-
graphically transcribed, and each utterance was an-
notated with the set of objects present in the non-
linguistic context. The object referred to by the ut-
terance, if any, was noted, as described in Frank et al.
(2009b). We used the VoxForge dictionary to map
orthographic words to phoneme sequences in a pro-
cess similar to that described in Brent (1999).
Figure 1 (a) presents an example of the coding
of phonemic transcription and non-linguistic context
for a single utterance. The input to the system con-
sists solely of the phonemic transcription and the ob-
jects in the non-linguistic context.
</bodyText>
<subsectionHeader confidence="0.946817">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999889818181818">
We ran the sampler ten times for 100,000 iterations
with parameter settings of α1 = 0.01, α0 = 20, and
p# = 0.5, keeping only the final sample for evalu-
ation. We defined the word-object pairs for a sam-
ple as the words in the referring category that were
paired at least once with a particular topic. These
pairs were then compared against a gold standard
set of word-object pairs, while segmentation perfor-
mance was evaluated by comparing the final bound-
ary assignments against the gold standard segmenta-
tion.
</bodyText>
<equation confidence="0.99114588">
(h−ji)
nwji,xjizj + αxjiP0(wji)
=
(h−ji)
n•,xjizj + αxji
(h−)
nx1 + 1 ·
(h−)
n• + 2
n(h ) + 1
$1
(h−)
n• + 2
·
(1)
·
n(h ) + 1
$2=0
(h−)
n• + 2
(h−)
nx3 + δx2(x3) + 1
·
(h−)
n• + 3
</equation>
<page confidence="0.992396">
505
</page>
<subsectionHeader confidence="0.576236">
4.2.1 Word Learning
</subsectionHeader>
<bodyText confidence="0.999959282051282">
To explore the contribution of word boundaries
to the joint word learning and segmenting task, we
compare our full joint model against a variant that
only infers topics, using the gold standard segmen-
tation as input. In this way we also reproduce the
usual assumption of a sequential relationship be-
tween segmentation and word learning and test the
necessity of the simplifying assumption. The re-
sults are shown in Table 2. We compare them with
three different metric types: topic accuracy; preci-
sion, recall, and F-score of the word-object pairs;
and Kullback-Liebler (KL) divergence.
First, treating utterances with no referring words
as though they have no topic, we compute the ac-
curacy of the inferred topics. Note that we don’t
report accuracy for the the variant with no non-
linguistic context, since in this case the objects are
interchangeable, and we have a problem identifying
which cluster corresponds to which topic. Table 2
shows that the joint segmentation and word learning
model gets the topic right for 81% of the utterances.
The variant that assumes pre-segmented input does
comparably well with an accuracy of 79%. Surpris-
ingly, it seems that knowing the gold segmentation
doesn’t add very much, at least for the topic infer-
ence task.
To evaluate how well we discovered the word-
object map, we manually compiled a list of all the
nouns in the corpus that named one of the 30 ob-
jects. We used this set of nouns, cross-referenced
with their topic objects, as a gold standard set of
word-object pairs. By counting the co-occurrences,
we also compute a gold standard probability distri-
bution for the words given the topic, P(w|z, x = 1).
Precision, recall and F-score are computed as per
Frank et al. (2009a). In particular, precision is the
fraction of gold pairs among the sampled set and re-
call is the fraction of sampled pairs among the gold
standard pairs.
</bodyText>
<equation confidence="0.740867">
|Sampled ∩ Gold ||Sampled ∩ Gold|
p = |Sampled |, r =|Gold|
</equation>
<bodyText confidence="0.9897915">
KL divergence is a way of measuring the differ-
ence between distributions. Small numbers gener-
ally indicate a close match and is zero only when
the two are equal. Using the empirical distribution
</bodyText>
<table confidence="0.997497454545455">
Object Words
BOX thebox box
BRUSH brush
BUNNY rabbit Rosie
BUS bus
CAR car thecar
CHEESE cheese
DOG thedoggy doggy
DOLL doll thedoll yeah benice
DOUGH dough
ERNIE Ernie
</table>
<tableCaption confidence="0.90893">
Table 1: Subset of an inferred word-object mapping. For
clarity, the proposed words have been converted to stan-
dard English orthography.
</tableCaption>
<table confidence="0.999335333333333">
p r f KL acc
Joint 0.21 0.45 0.28 2.78 0.81
Gold Seg 0.21 0.60 0.31 1.82 0.79
</table>
<tableCaption confidence="0.607732">
Table 2: Word Learning Performance. Comparing
precision, recall, and F-score of word-object pairs,
DKL(P (w, z)||Q(w, z)), and accuracy of utterance top-
ics for the full joint model and a variant that only infers
meanings given a gold standard segmentation.
</tableCaption>
<bodyText confidence="0.9996282">
over gold topics P(z), we use the standard formula
for KL divergence to compare the gold standard dis-
tribution P against the inferred distribution Q. I.e.,
we compute DKL(P(w, z)||Q(w, z)).
The model learns fairly meaningful word-object
associations; results are shown in Table 2. As in the
case of topic accuracy, the joint and word learning
only variants perform similarly, this time with some-
what better performance for the easier task with an
F-score and KL divergence of 0.31 and 1.82 vs. 0.28
and 2.78 for the joint task.
Table 1 illustrates the sort of word-object pairs
the model discovers. As can be seen, many of the
errors are due to the segmentation, usually under-
segmentation errors where it segments two words as
one. This is a general problem with the unigram seg-
menter on which our model is based (Goldwater et
al., 2009). Yet, even though these segmentation er-
rors are also counted as word learning errors, they
are often still meaningful in the sense that the true
referring word is a subsequence.
So, word segmentation has an impact on word
learning. Yet, the joint model still tends to uncover
reasonable meanings. The next question is whether
these meanings have an impact on the segmentation.
</bodyText>
<page confidence="0.9941">
506
</page>
<table confidence="0.9997274">
NoCon Random Joint
Referring Nouns 0.36 0.35 0.50
Neighbors 0.33 0.33 0.37
Utt Final Nouns 0.36 0.36 0.52
Entire Corpus 0.53 0.53 0.54
</table>
<tableCaption confidence="0.999815">
Table 3: Segmentation performance. F-score for three
</tableCaption>
<bodyText confidence="0.602533">
subsets and the full corpus for three variants: the model
without non-linguistic context, the model with random
topics, and the full joint model.
</bodyText>
<subsectionHeader confidence="0.484505">
4.2.2 Word Segmentation
</subsectionHeader>
<bodyText confidence="0.999961228070176">
To measure the impact of word learning on seg-
mentation, we again compare the model on the full
joint task against two other variants: one where top-
ics are randomly selected, and one that ignores the
non-linguistic context. For the random topics vari-
ant, we choose each topic during initialization ac-
cording to the empirical distribution over gold topics
and treat these topic assignments as observed vari-
ables for subsequent iterations. The variant that ig-
nores non-linguistic context draws topics uniformly
from the entire set of objects ever discussed in the
corpus, another test of the contribution of the non-
linguistic context to segmentation. We report token
F-score, computed as per Goldwater et al. (2009),
where any segment proposed by the model is a true
positive only if it matches the gold segmentation and
is a false positive otherwise. Any segment in the
gold data not found by the model is a false negative.
Table 3 shows the segmentation performance for
various subsets as well as for the entire corpus. Be-
cause we are primarily interested in synergies be-
tween word learning and segmentation, we focus on
the words most directly impacted by the meanings:
gold standard referring nouns and their neighboring
words.
The model behaves the same with randomized
topics as without context; it learns none of the gold
standard pairs (no matter how we identify clusters
with topics for the contextless case). On all subsets,
the full joint model outperforms the other two vari-
ants. In particular, the greatest gain is for the refer-
ring nouns, with a 21% reduction in error. Also, sim-
ilar to the findings of Bortfeld et al. (2005) regarding
6 month olds’ abilities to segment words adjoining a
familiar name, we also find that neighboring words
benefit from sharing a word boundary with a learned
word.
The model performs exceptionally well on utter-
ance final referring nouns, with a 24% reduction
in error. This may explain certain psycholinguistic
observations. Frank et al. (2006) performed an ar-
tificial language experiment with humans subjects
demonstrating that adults were able to learn words
at the same time as they learned to segment the lan-
guage. However, subjects did much better on a word
learning task when the meaning bearing words were
consistently placed at the end of utterances. There
are several possible reasons why this might have
been the case. For instance, it is common in English
for the object noun to occur at the end of the sen-
tence, and since the subjects were all English speak-
ers, they may have found it easier to learn an artifi-
cial language with a similar pattern. However, our
model predicts another simple possibility: the seg-
mentation task is easier at the end because one of
the two word boundaries is already known (the ut-
terance boundary itself).
</bodyText>
<subsectionHeader confidence="0.957053">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.99995736">
The word learning model generally prefers a very
sparse word-to-object map. This is enforced by us-
ing a concentration parameter α1 that is quite small
relative to the α0 parameter, and it biases the model
so that the distributions over referring words are
very different from that over non-referring words. A
small concentration parameter biases the estimator
to prefer a small set of word types. In contrast, the
relatively large concentration parameter for the non-
referring words tends to result in most of the words
receiving highest probability as non-referring words.
The model thus categorizes words accordingly. It is
in part due to this tendency towards sparse word-
object maps that the model enforces mutual exclu-
sivity, a phenomenon well documented among natu-
ral word learners (Markman, 1990).
Aside from contributing to mutual exclusivity
and specialization among the topical word distri-
butions, the small concentration parameter also has
important implications for the segmentation task.
A very small value for α1 discourages the learner
from acquiring more word types for each mean-
ing than absolutely necessary, thereby forcing the
segmenter to use fewer types to explain the se-
quence of phonemes. A model without any notion
</bodyText>
<page confidence="0.987637">
507
</page>
<bodyText confidence="0.999970209302326">
of meaning cannot maintain separate distributions
for different topics, and must in some sense treat all
words as non-referring. A segmenting model with-
out meanings cannot share the word learner’s reluc-
tance to propose new meaning-bearing word types
and might propose three separate types for “your
book”, “a book”, and “the book”. However, with
a small enough prior on new referring word types,
the word learner that discovers a common refer-
ent for all three sequences and, preferring fewer re-
ferring word types, is more likely to discover the
common subsequence “book”. With a single word-
object pair (“book”, BOOK), the word learner could
explain reference for all three sequences instead of
using the three separate pairs (“yourbook”, BOOK),
(“abook”, BOOK), and (“thebook”, BOOK).
While relying on non-linguistic context helps seg-
ment the meaning-bearing words, the overall im-
provement is small in our current corpus. One rea-
son for this small improvement was that only 9%
of the tokens in the corpus were referring words.
In corpora containing a larger variety of objects –
and in cases where sub- and super-ordinate labels
like “eyes” and “ears” are coded – this percentage is
likely to be much higher, leading to a greater boost
in overall segmentation performance.
We should acknowledge that the decisions en-
tailed in enriching the annotations are neither triv-
ial nor without theoretic implication, however. It is
not immediately obvious how to represent the non-
linguistic correlates of verbs, for instance. Devel-
opmentally, verbs are typically acquired much later
than nouns, and it has been argued that this may be
due to the difficulty of producing a cognitive rep-
resentation of the associated meaning (Gentner and
Boroditsky, 2001). Even among concrete nouns, not
all are equal. Children tend to have a bias toward
whole objects when mapping novel words to their
non-linguistic counterparts (Markman, 1990). De-
cisions about more sophisticated encoding of non-
linguistic information may thus require more knowl-
edge about children’s representations of the world
around them
</bodyText>
<sectionHeader confidence="0.995643" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999979416666667">
We find (1) that it is possible to jointly infer both
meanings and a segmentation in a fully unsupervised
way and (2) that doing so improves the segmenta-
tion performance of our model. In particular, we
found that although the word learning side suffered
from segmentation errors, and performed worse than
a model that learned from a gold standard segmen-
tation, the loss was only slight. On the other hand,
segmentation performance for the meaning bearing
words improved a great deal. The first result sug-
gests that is not necessary to assume fully segmented
input in order to learn word meanings, and that the
segmentation and word learning tasks can be effec-
tively modeled in parallel, allowing us to explore po-
tential developmental interactions. The second re-
sult suggests that synergies do actually exist and ar-
gue not only that we can model the two as parallel
processes, but that doing so could prove fruitful.
Our model is relatively simple both in terms of
word learning and in terms of word segmentation.
For instance, social cues and shared attention, or dis-
course effects, might all play a role (Frank et al.,
2009b). Shared features or other relationships can
also potentially impact how quickly one might gen-
eralize a label to multiple instances (Tenenbaum and
Xu, 2000). There are many ways to elaborate on the
word learning task, with additional potential syner-
gistic implications.
We might also elaborate the linguistic structures
we incorporate into the word learning model. For
instance, Johnson (2008) explores synergies in syl-
lable and morphological structures in word segmen-
tation. Aspects of linguistic structure, such as mor-
phology, may contribute to word meaning learning
beyond its contribution to word segmentation per-
formance.
</bodyText>
<sectionHeader confidence="0.998236" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9956416">
This research was funded by NSF awards 0544127
and 0631667 to Mark Johnson and by NSF DDRIG
0746251 to Michael C. Frank. We would also like
to thank Anne Fernald for providing the corpus and
Maeve Cullinane for help in coding it.
</bodyText>
<sectionHeader confidence="0.99867" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.965554333333333">
Nameera Akhtar and Lisa Montague. 1999. Early lexi-
cal acquisition: The role of cross-situational learning.
First Language, 19(57 Pt 3):347–358.
</reference>
<page confidence="0.975166">
508
</page>
<reference confidence="0.999787740740741">
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent dirichlet allocation. Journal ofMachine Learning
Research, 3:993–1022.
Heather Bortfeld, James L. Morgan, Roberta Michnick
Golinkoff, and Karen Rathbun. 2005. Mommy
and me: Familiar names help launch babies into
speechstream segmentation. Psychological Science,
16(4):298–304.
Michael R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discovery.
Machine Learning, 34:71–105.
Anne Fernald and Hiromi Morikawa. 1993. Common
themes and cultural variations in japanese and ameri-
can mothers’ speech to infants. In Child Development,
number 3, pages 637–656, June.
Michael C. Frank, Vikash Mansinghka, Edward Gibson,
and Joshua B. Tenenbaum. 2006. Word segmentation
as word learning: Integrating stress and meaning with
distributional cues. In Proceedings of the 31st Annual
Boston University Conference on Language Develop-
ment.
Michael C. Frank, Sharon Goldwater, Vikash Mans-
inghka, Tom Griffiths, and Joshua Tenenbaum. 2007.
Modeling human performance in statistical word seg-
mentation. Proceedings of the 29th Annual Meeting of
the Cognitive Science Society, pages 281–286.
Michael C. Frank, Noah D. Goodman, and Joshua B.
Tenenbaum. 2009a. Using speakers’ referential inten-
tions to model early cross-situational word learning.
Psychological Science, 5:578–585.
Michael C. Frank, Noah D. Goodman, Joshua B. Tenen-
baum, and Anne Fernald. 2009b. Continuity of dis-
course provides information for word learning.
Dedre Gentner and Lera Boroditsky. 2001. Individua-
tion, relativity, and early word learning. Language,
culture, &amp; cognition, 3:215–56.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21–54.
Katharine Graf-Estes, Julia L. Evans, Martha W. Alibali,
and Jenny R. Saffran. 2007. Can infants map meaning
to newly segmented words? statistical segmentation
and word learning. Psychological Science, 18(3):254–
260.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic markov models. In Artificial Intelligence
and Statistics (AISTATS), March.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 317–325, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Mark Johnson. 2008. Using adaptor grammars to identi-
fying synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics, Columbus, Ohio. Association for Computational
Linguistics.
Ellen M. Markman. 1990. Constraints children place on
word learning. Cognitive Science, 14:57–77.
Terry Regier. 2003. Emergent constraints on word-
learning: A computational review. Trends in Cognitive
Sciences, 7:263–268.
Jenny R. Saffran, Elissa L. Newport, and Richard N.
Aslin. 1996. Word segmentation: The role of dis-
tributional cues. Journal of memory and Language,
35:606–621.
Jeffrey M. Siskind. 1996. A computational study
of cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61(1-2):39–91.
Joshua B. Tenenbaum and Fei Xu. 2000. Word learn-
ing as bayesian inference. In Proceedings of the 22nd
Annual Conference of the Cognitive Science Society,
pages 517–522.
Chen Yu and Dana H. Ballard. 2007. A unified model of
early word learning: Integrating statistical and social
cues. Neurocomputing, 70(13-15):2149–2165.
</reference>
<page confidence="0.998497">
509
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.929428">
<title confidence="0.9989545">Learning Words and Their Meanings from Unsegmented Child-directed Speech</title>
<author confidence="0.999995">Bevan K Jones</author>
<author confidence="0.999995">Mark Johnson Michael C Frank</author>
<affiliation confidence="0.9999555">Dept of Cognitive and Linguistic Sciences Dept of Brain and Cognitive Science Brown University Massachusetts Institute of Technology</affiliation>
<address confidence="0.999838">Providence, RI 02912, USA Cambridge, MA 02139, USA</address>
<email confidence="0.957006">Jones,Markmcfrank@mit.edu</email>
<abstract confidence="0.998499277777778">Most work on language acquisition treats word segmentation—the identification of linguistic segments from continuous speech— and word learning—the mapping of those segments to meanings—as separate problems. These two abilities develop in parallel, however, raising the question of whether they might interact. To explore the question, we present a new Bayesian segmentation model that incorporates aspects of word learning and compare it to a model that ignores word meanings. The model that learns word meanings proposes more adult-like segmentations for the meaning-bearing words. This result suggests that the non-linguistic context may supply important information for learning word segmentations as well as word meanings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nameera Akhtar</author>
<author>Lisa Montague</author>
</authors>
<title>Early lexical acquisition: The role of cross-situational learning. First Language,</title>
<date>1999</date>
<tech>Pt 3):347–358.</tech>
<contexts>
<context position="3414" citStr="Akhtar and Montague, 1999" startWordPosition="539" endWordPosition="542">mulates the learning task, taking as input the unsegmented phonemic representation of the speech along with the set of objects in the non-linguistic context as shown in Figure 1 (a), and infers both a segmentation and a word-object mapping as in Figure 1 (b). One can formulate the word learning task as that of finding a reasonably small set of reusable word-meaning pairs consistent with the underlying communicative intent. Infant directed speech often refers to objects in the immediate environment, and early word learning seems to involve associating frequently co-occurring word-object pairs (Akhtar and Montague, 1999; Markman, 1990). Several computational models are based on this idea that a word 501 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 501–509, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Figure 1: (a) The input to our system for the utterance “Is that the pig?” consists of an unsegmented sequence of phonemes and the set of objects representing the nonlinguistic context. These objects were manually identified by inspecting the associated video, a frame from which is shown above. (b) The gold-standar</context>
</contexts>
<marker>Akhtar, Montague, 1999</marker>
<rawString>Nameera Akhtar and Lisa Montague. 1999. Early lexical acquisition: The role of cross-situational learning. First Language, 19(57 Pt 3):347–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="6767" citStr="Blei et al. (2003)" startWordPosition="1089" endWordPosition="1092">ddress the two problems. Similarly, in the behavioral literature on development, we are aware of only one segmentation study (Graf-Estes et al., 2007) that involves non-linguistic context, though this study treats the two tasks sequentially rather than jointly. We now describe our model and inference procedure and follow with evaluation and discussion. 2 Model Definition Cross-situational meaning learning in our joint word learning and segmenting model is inspired by the model of Frank et al. (2009a). Our model can be viewed as a variant of the Latent Dirichlet Allocation (LDA) topic model of Blei et al. (2003), where topics are drawn from the objects in the nonlinguistic context. The model associates each utterance with a single referent object, the topic, and every word in the utterance is either generated from a distribution over words associated with that object or else from a distribution associated with a special “null” object shared by all utterances. Note that in this paper we use “topic” to denote the referent object of an utterance, otherwise we depart from topic modeling convention and use the term “object” instead. Segmentation is based on the unigram model proposed by Brent (1999) and r</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Ng, and Michael Jordan. 2003. Latent dirichlet allocation. Journal ofMachine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heather Bortfeld</author>
<author>James L Morgan</author>
<author>Roberta Michnick Golinkoff</author>
<author>Karen Rathbun</author>
</authors>
<title>Mommy and me: Familiar names help launch babies into speechstream segmentation.</title>
<date>2005</date>
<journal>Psychological Science,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="1576" citStr="Bortfeld et al., 2005" startWordPosition="229" endWordPosition="232">result suggests that the non-linguistic context may supply important information for learning word segmentations as well as word meanings. 1 Introduction Acquiring a language entails mastering many learning tasks simultaneously, including identifying where words begin and end in continuous speech and learning meanings for those words. It is common to treat these tasks as separate, sequential processes, where segmentation is a prerequisite to word learning but otherwise there are few if any dependencies. The earliest evidence of segmentation, however, is for words bordering a child’s own name (Bortfeld et al., 2005). In addition, infants begin learning their first words before they achieve adultlevel competence in segmentation. These two pieces of evidence raise the question of whether the tasks of meaning learning and segmentation might mutually inform one another. To explore this question we present a joint model that simultaneously identifies word boundaries and attempts to associate meanings with words. In doing so we make two contributions. First, by modeling the two levels of structure in parallel we simulate a more realistic situation. Second, a joint model allows us to explore possible synergies </context>
<context position="23993" citStr="Bortfeld et al. (2005)" startWordPosition="4062" endWordPosition="4065">pus. Because we are primarily interested in synergies between word learning and segmentation, we focus on the words most directly impacted by the meanings: gold standard referring nouns and their neighboring words. The model behaves the same with randomized topics as without context; it learns none of the gold standard pairs (no matter how we identify clusters with topics for the contextless case). On all subsets, the full joint model outperforms the other two variants. In particular, the greatest gain is for the referring nouns, with a 21% reduction in error. Also, similar to the findings of Bortfeld et al. (2005) regarding 6 month olds’ abilities to segment words adjoining a familiar name, we also find that neighboring words benefit from sharing a word boundary with a learned word. The model performs exceptionally well on utterance final referring nouns, with a 24% reduction in error. This may explain certain psycholinguistic observations. Frank et al. (2006) performed an artificial language experiment with humans subjects demonstrating that adults were able to learn words at the same time as they learned to segment the language. However, subjects did much better on a word learning task when the meani</context>
</contexts>
<marker>Bortfeld, Morgan, Golinkoff, Rathbun, 2005</marker>
<rawString>Heather Bortfeld, James L. Morgan, Roberta Michnick Golinkoff, and Karen Rathbun. 2005. Mommy and me: Familiar names help launch babies into speechstream segmentation. Psychological Science, 16(4):298–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--71</pages>
<contexts>
<context position="4893" citStr="Brent, 1999" startWordPosition="783" endWordPosition="784"> so frequently in its absence is likely to refer to that object (Frank et al., 2009a; Siskind, 1996; Yu and Ballard, 2007). Importantly, all these models assume words are pre-segmented in the input. While the word segmentation task relates less clearly to the communicative content, it can be formulated according to a similar objective, that of attempting to explain the sound sequences in the input in terms of some reasonably small set of reusable units, or words. Computational models have successfully addressed the problem in much this way (Johnson and Goldwater, 2009; Goldwater et al., 2009; Brent, 1999), and the general approach is consistent with experimental observations that humans are sensitive to statistics of sound sequences (Saffran et al., 1996; Frank et al., 2007). The two tasks can be integrated in a relatively seamless way, since, as we have just formulated them, they have a common objective, that of finding a minimal, consistent set of reusable units. However, the two deal with different types of information with different dependencies. The basic idea is that learning a vocabulary that both meets the constraints of the word-learning task and is consistent with the objective of th</context>
<context position="7361" citStr="Brent (1999)" startWordPosition="1193" endWordPosition="1194">Blei et al. (2003), where topics are drawn from the objects in the nonlinguistic context. The model associates each utterance with a single referent object, the topic, and every word in the utterance is either generated from a distribution over words associated with that object or else from a distribution associated with a special “null” object shared by all utterances. Note that in this paper we use “topic” to denote the referent object of an utterance, otherwise we depart from topic modeling convention and use the term “object” instead. Segmentation is based on the unigram model proposed by Brent (1999) and reformulated by Goldwater et al. (2009) in terms of a Dirichlet process. Since both LDA and the unigram segmenter are based on unigram distributions it is relatively straightforward 502 Figure 2: Topical Unigram Model: Oj is the set of objects in the non-linguistic context of the jth utterance, zj is the utterance topic, wji is the ith word of the utterance, xji is the category of the word (referring or non-referring), and the other variables are distribution parameters. to integrate the two to simultaneously infer word boundaries and word-object associations. Figure 2 illustrates a sligh</context>
<context position="17035" citStr="Brent (1999)" startWordPosition="2872" endWordPosition="2873">y and 0 otherwise. 4 Results &amp; Model Comparisons 4.1 Corpus Our training corpus (Fernald and Morikawa, 1993; Frank et al., 2009b) consists of about 22,000 words and 5,600 utterances. Video recordings consisting of mother-child play over pairs of toys were orthographically transcribed, and each utterance was annotated with the set of objects present in the nonlinguistic context. The object referred to by the utterance, if any, was noted, as described in Frank et al. (2009b). We used the VoxForge dictionary to map orthographic words to phoneme sequences in a process similar to that described in Brent (1999). Figure 1 (a) presents an example of the coding of phonemic transcription and non-linguistic context for a single utterance. The input to the system consists solely of the phonemic transcription and the objects in the non-linguistic context. 4.2 Evaluation We ran the sampler ten times for 100,000 iterations with parameter settings of α1 = 0.01, α0 = 20, and p# = 0.5, keeping only the final sample for evaluation. We defined the word-object pairs for a sample as the words in the referring category that were paired at least once with a particular topic. These pairs were then compared against a g</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>Michael R. Brent. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34:71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Fernald</author>
<author>Hiromi Morikawa</author>
</authors>
<title>Common themes and cultural variations in japanese and american mothers’ speech to infants.</title>
<date>1993</date>
<booktitle>In Child Development, number 3,</booktitle>
<pages>637--656</pages>
<contexts>
<context position="2453" citStr="Fernald and Morikawa, 1993" startWordPosition="372" endWordPosition="376">her. To explore this question we present a joint model that simultaneously identifies word boundaries and attempts to associate meanings with words. In doing so we make two contributions. First, by modeling the two levels of structure in parallel we simulate a more realistic situation. Second, a joint model allows us to explore possible synergies and interactions. We find evidence that our joint model performs better on a segmentation task than an alternative model that does not learn word meanings. The picture in Figure 1 depicts a language learning situation from our corpus (originally from Fernald and Morikawa, 1993; recoded in Frank et al., 2009) where a mother talks while playing with various toys. Setting down the dog and picking up the hand puppet of a pig, she asks, “Is that the pig?” Starting out, a young learner not only does not know that the word “pig” refers to the puppet but does not even know that “pig” is a word at all. Our model simulates the learning task, taking as input the unsegmented phonemic representation of the speech along with the set of objects in the non-linguistic context as shown in Figure 1 (a), and infers both a segmentation and a word-object mapping as in Figure 1 (b). One </context>
<context position="16530" citStr="Fernald and Morikawa, 1993" startWordPosition="2785" endWordPosition="2788">variables excluding those for the current segment and n(h−) $1is the count for h− of either utterance final words if w1 is utterance final or non-utterance final words if w1 is also not utterance final. In the four cases with a word boundary, we have two words and two categories to sample. (h−) nw2,x2z + αx2P0(w2) P(w2, x2, w3, x3|z, h−) = (h−) n•,x2z + αx2 (h−) nx2 + 1 · (h−) n• + 2 (h−) nw3,x3z + δx2(x3)δw2(w3) + αx3P0(w3) · (h−) n•,x3z + δx2(x3) + αx3 (h−) n$3 + δ$2($3) + 1 · (h−) n• + 3 Here δx(y) is 1 if x = y and 0 otherwise. 4 Results &amp; Model Comparisons 4.1 Corpus Our training corpus (Fernald and Morikawa, 1993; Frank et al., 2009b) consists of about 22,000 words and 5,600 utterances. Video recordings consisting of mother-child play over pairs of toys were orthographically transcribed, and each utterance was annotated with the set of objects present in the nonlinguistic context. The object referred to by the utterance, if any, was noted, as described in Frank et al. (2009b). We used the VoxForge dictionary to map orthographic words to phoneme sequences in a process similar to that described in Brent (1999). Figure 1 (a) presents an example of the coding of phonemic transcription and non-linguistic c</context>
</contexts>
<marker>Fernald, Morikawa, 1993</marker>
<rawString>Anne Fernald and Hiromi Morikawa. 1993. Common themes and cultural variations in japanese and american mothers’ speech to infants. In Child Development, number 3, pages 637–656, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C Frank</author>
<author>Vikash Mansinghka</author>
<author>Edward Gibson</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Word segmentation as word learning: Integrating stress and meaning with distributional cues.</title>
<date>2006</date>
<booktitle>In Proceedings of the 31st Annual Boston University Conference on Language Development.</booktitle>
<contexts>
<context position="5780" citStr="Frank et al. 2006" startWordPosition="929" endWordPosition="932"> them, they have a common objective, that of finding a minimal, consistent set of reusable units. However, the two deal with different types of information with different dependencies. The basic idea is that learning a vocabulary that both meets the constraints of the word-learning task and is consistent with the objective of the segmentation task can yield a better segmentation. That is, we hope to find a synergy in the joint inference of meaning and segmentation. Note that to the best of our knowledge there is very little computational work that combines word form and word meaning learning (Frank et al. 2006 takes a first step but their model is applicable only to small artificial languages). Frank et al. (2009a) and Regier (2003) review pure word learning models and, in addition to the papers we have already cited, Brent (1999) presents a fairly comprehensive review of previous pure segmentation models. However, none of the models reviewed make any attempt to jointly address the two problems. Similarly, in the behavioral literature on development, we are aware of only one segmentation study (Graf-Estes et al., 2007) that involves non-linguistic context, though this study treats the two tasks seq</context>
<context position="24346" citStr="Frank et al. (2006)" startWordPosition="4117" endWordPosition="4120">usters with topics for the contextless case). On all subsets, the full joint model outperforms the other two variants. In particular, the greatest gain is for the referring nouns, with a 21% reduction in error. Also, similar to the findings of Bortfeld et al. (2005) regarding 6 month olds’ abilities to segment words adjoining a familiar name, we also find that neighboring words benefit from sharing a word boundary with a learned word. The model performs exceptionally well on utterance final referring nouns, with a 24% reduction in error. This may explain certain psycholinguistic observations. Frank et al. (2006) performed an artificial language experiment with humans subjects demonstrating that adults were able to learn words at the same time as they learned to segment the language. However, subjects did much better on a word learning task when the meaning bearing words were consistently placed at the end of utterances. There are several possible reasons why this might have been the case. For instance, it is common in English for the object noun to occur at the end of the sentence, and since the subjects were all English speakers, they may have found it easier to learn an artificial language with a s</context>
</contexts>
<marker>Frank, Mansinghka, Gibson, Tenenbaum, 2006</marker>
<rawString>Michael C. Frank, Vikash Mansinghka, Edward Gibson, and Joshua B. Tenenbaum. 2006. Word segmentation as word learning: Integrating stress and meaning with distributional cues. In Proceedings of the 31st Annual Boston University Conference on Language Development.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C Frank</author>
<author>Sharon Goldwater</author>
<author>Vikash Mansinghka</author>
<author>Tom Griffiths</author>
<author>Joshua Tenenbaum</author>
</authors>
<title>Modeling human performance in statistical word segmentation.</title>
<date>2007</date>
<booktitle>Proceedings of the 29th Annual Meeting of the Cognitive Science Society,</booktitle>
<pages>281--286</pages>
<contexts>
<context position="5066" citStr="Frank et al., 2007" startWordPosition="808" endWordPosition="811"> are pre-segmented in the input. While the word segmentation task relates less clearly to the communicative content, it can be formulated according to a similar objective, that of attempting to explain the sound sequences in the input in terms of some reasonably small set of reusable units, or words. Computational models have successfully addressed the problem in much this way (Johnson and Goldwater, 2009; Goldwater et al., 2009; Brent, 1999), and the general approach is consistent with experimental observations that humans are sensitive to statistics of sound sequences (Saffran et al., 1996; Frank et al., 2007). The two tasks can be integrated in a relatively seamless way, since, as we have just formulated them, they have a common objective, that of finding a minimal, consistent set of reusable units. However, the two deal with different types of information with different dependencies. The basic idea is that learning a vocabulary that both meets the constraints of the word-learning task and is consistent with the objective of the segmentation task can yield a better segmentation. That is, we hope to find a synergy in the joint inference of meaning and segmentation. Note that to the best of our know</context>
</contexts>
<marker>Frank, Goldwater, Mansinghka, Griffiths, Tenenbaum, 2007</marker>
<rawString>Michael C. Frank, Sharon Goldwater, Vikash Mansinghka, Tom Griffiths, and Joshua Tenenbaum. 2007. Modeling human performance in statistical word segmentation. Proceedings of the 29th Annual Meeting of the Cognitive Science Society, pages 281–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C Frank</author>
<author>Noah D Goodman</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Using speakers’ referential intentions to model early cross-situational word learning.</title>
<date>2009</date>
<journal>Psychological Science,</journal>
<pages>5--578</pages>
<contexts>
<context position="2485" citStr="Frank et al., 2009" startWordPosition="379" endWordPosition="382">t a joint model that simultaneously identifies word boundaries and attempts to associate meanings with words. In doing so we make two contributions. First, by modeling the two levels of structure in parallel we simulate a more realistic situation. Second, a joint model allows us to explore possible synergies and interactions. We find evidence that our joint model performs better on a segmentation task than an alternative model that does not learn word meanings. The picture in Figure 1 depicts a language learning situation from our corpus (originally from Fernald and Morikawa, 1993; recoded in Frank et al., 2009) where a mother talks while playing with various toys. Setting down the dog and picking up the hand puppet of a pig, she asks, “Is that the pig?” Starting out, a young learner not only does not know that the word “pig” refers to the puppet but does not even know that “pig” is a word at all. Our model simulates the learning task, taking as input the unsegmented phonemic representation of the speech along with the set of objects in the non-linguistic context as shown in Figure 1 (a), and infers both a segmentation and a word-object mapping as in Figure 1 (b). One can formulate the word learning </context>
<context position="4364" citStr="Frank et al., 2009" startWordPosition="696" endWordPosition="699">terance “Is that the pig?” consists of an unsegmented sequence of phonemes and the set of objects representing the nonlinguistic context. These objects were manually identified by inspecting the associated video, a frame from which is shown above. (b) The gold-standard segmentation and word-object assignments of the same utterance, against which the output of our system is evaluated (all words except “pIg” are mapped to a special “null” object, as explained in the text). that frequently occurs in the presence of an object and not so frequently in its absence is likely to refer to that object (Frank et al., 2009a; Siskind, 1996; Yu and Ballard, 2007). Importantly, all these models assume words are pre-segmented in the input. While the word segmentation task relates less clearly to the communicative content, it can be formulated according to a similar objective, that of attempting to explain the sound sequences in the input in terms of some reasonably small set of reusable units, or words. Computational models have successfully addressed the problem in much this way (Johnson and Goldwater, 2009; Goldwater et al., 2009; Brent, 1999), and the general approach is consistent with experimental observations</context>
<context position="5885" citStr="Frank et al. (2009" startWordPosition="947" endWordPosition="950">r, the two deal with different types of information with different dependencies. The basic idea is that learning a vocabulary that both meets the constraints of the word-learning task and is consistent with the objective of the segmentation task can yield a better segmentation. That is, we hope to find a synergy in the joint inference of meaning and segmentation. Note that to the best of our knowledge there is very little computational work that combines word form and word meaning learning (Frank et al. 2006 takes a first step but their model is applicable only to small artificial languages). Frank et al. (2009a) and Regier (2003) review pure word learning models and, in addition to the papers we have already cited, Brent (1999) presents a fairly comprehensive review of previous pure segmentation models. However, none of the models reviewed make any attempt to jointly address the two problems. Similarly, in the behavioral literature on development, we are aware of only one segmentation study (Graf-Estes et al., 2007) that involves non-linguistic context, though this study treats the two tasks sequentially rather than jointly. We now describe our model and inference procedure and follow with evaluati</context>
<context position="10717" citStr="Frank et al. (2009" startWordPosition="1779" endWordPosition="1782">i.i.d. uniformly from m phonemic types. In addition, there is a probability p# of generating a word boundary. 1 P0(w) = (1 − p#)|w|−1p# m|w| The concentration parameters α0 and α1 also play a critical role in the generation of words and word types. Any given word has a certain probability of either being produced from the set of previously seen word types, or from an entirely new one. The z|O ∼ Uniform(O) � DP(α1, P0) if z =6 0 DP(α0, P0) otherwis P0 ∼ 503 greater the concentration parameter, the more likely the model is to appeal to the base distribution P0 to introduce a new word type. Like Frank et al. (2009a), we distinguish between two coarse grammatical categories, referring and non-referring. Referring words are generated by the topic, while non-referring words are drawn from G0, a distribution associated with the “null” object. The distinction ensures sparse word-object maps that obey the principle of mutual exclusion. Otherwise all words in the utterance would be associated with the topic object, resulting in a very large set of words for each object that is very likely to overlap with the words for other objects. As a further bias toward a small lexicon, we employ different concentration p</context>
<context position="16550" citStr="Frank et al., 2009" startWordPosition="2789" endWordPosition="2792">r the current segment and n(h−) $1is the count for h− of either utterance final words if w1 is utterance final or non-utterance final words if w1 is also not utterance final. In the four cases with a word boundary, we have two words and two categories to sample. (h−) nw2,x2z + αx2P0(w2) P(w2, x2, w3, x3|z, h−) = (h−) n•,x2z + αx2 (h−) nx2 + 1 · (h−) n• + 2 (h−) nw3,x3z + δx2(x3)δw2(w3) + αx3P0(w3) · (h−) n•,x3z + δx2(x3) + αx3 (h−) n$3 + δ$2($3) + 1 · (h−) n• + 3 Here δx(y) is 1 if x = y and 0 otherwise. 4 Results &amp; Model Comparisons 4.1 Corpus Our training corpus (Fernald and Morikawa, 1993; Frank et al., 2009b) consists of about 22,000 words and 5,600 utterances. Video recordings consisting of mother-child play over pairs of toys were orthographically transcribed, and each utterance was annotated with the set of objects present in the nonlinguistic context. The object referred to by the utterance, if any, was noted, as described in Frank et al. (2009b). We used the VoxForge dictionary to map orthographic words to phoneme sequences in a process similar to that described in Brent (1999). Figure 1 (a) presents an example of the coding of phonemic transcription and non-linguistic context for a single </context>
<context position="19728" citStr="Frank et al. (2009" startWordPosition="3344" endWordPosition="3347">y well with an accuracy of 79%. Surprisingly, it seems that knowing the gold segmentation doesn’t add very much, at least for the topic inference task. To evaluate how well we discovered the wordobject map, we manually compiled a list of all the nouns in the corpus that named one of the 30 objects. We used this set of nouns, cross-referenced with their topic objects, as a gold standard set of word-object pairs. By counting the co-occurrences, we also compute a gold standard probability distribution for the words given the topic, P(w|z, x = 1). Precision, recall and F-score are computed as per Frank et al. (2009a). In particular, precision is the fraction of gold pairs among the sampled set and recall is the fraction of sampled pairs among the gold standard pairs. |Sampled ∩ Gold ||Sampled ∩ Gold| p = |Sampled |, r =|Gold| KL divergence is a way of measuring the difference between distributions. Small numbers generally indicate a close match and is zero only when the two are equal. Using the empirical distribution Object Words BOX thebox box BRUSH brush BUNNY rabbit Rosie BUS bus CAR car thecar CHEESE cheese DOG thedoggy doggy DOLL doll thedoll yeah benice DOUGH dough ERNIE Ernie Table 1: Subset of a</context>
<context position="29642" citStr="Frank et al., 2009" startWordPosition="4983" endWordPosition="4986">sts that is not necessary to assume fully segmented input in order to learn word meanings, and that the segmentation and word learning tasks can be effectively modeled in parallel, allowing us to explore potential developmental interactions. The second result suggests that synergies do actually exist and argue not only that we can model the two as parallel processes, but that doing so could prove fruitful. Our model is relatively simple both in terms of word learning and in terms of word segmentation. For instance, social cues and shared attention, or discourse effects, might all play a role (Frank et al., 2009b). Shared features or other relationships can also potentially impact how quickly one might generalize a label to multiple instances (Tenenbaum and Xu, 2000). There are many ways to elaborate on the word learning task, with additional potential synergistic implications. We might also elaborate the linguistic structures we incorporate into the word learning model. For instance, Johnson (2008) explores synergies in syllable and morphological structures in word segmentation. Aspects of linguistic structure, such as morphology, may contribute to word meaning learning beyond its contribution to wo</context>
</contexts>
<marker>Frank, Goodman, Tenenbaum, 2009</marker>
<rawString>Michael C. Frank, Noah D. Goodman, and Joshua B. Tenenbaum. 2009a. Using speakers’ referential intentions to model early cross-situational word learning. Psychological Science, 5:578–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C Frank</author>
<author>Noah D Goodman</author>
<author>Joshua B Tenenbaum</author>
<author>Anne Fernald</author>
</authors>
<title>Continuity of discourse provides information for word learning.</title>
<date>2009</date>
<contexts>
<context position="2485" citStr="Frank et al., 2009" startWordPosition="379" endWordPosition="382">t a joint model that simultaneously identifies word boundaries and attempts to associate meanings with words. In doing so we make two contributions. First, by modeling the two levels of structure in parallel we simulate a more realistic situation. Second, a joint model allows us to explore possible synergies and interactions. We find evidence that our joint model performs better on a segmentation task than an alternative model that does not learn word meanings. The picture in Figure 1 depicts a language learning situation from our corpus (originally from Fernald and Morikawa, 1993; recoded in Frank et al., 2009) where a mother talks while playing with various toys. Setting down the dog and picking up the hand puppet of a pig, she asks, “Is that the pig?” Starting out, a young learner not only does not know that the word “pig” refers to the puppet but does not even know that “pig” is a word at all. Our model simulates the learning task, taking as input the unsegmented phonemic representation of the speech along with the set of objects in the non-linguistic context as shown in Figure 1 (a), and infers both a segmentation and a word-object mapping as in Figure 1 (b). One can formulate the word learning </context>
<context position="4364" citStr="Frank et al., 2009" startWordPosition="696" endWordPosition="699">terance “Is that the pig?” consists of an unsegmented sequence of phonemes and the set of objects representing the nonlinguistic context. These objects were manually identified by inspecting the associated video, a frame from which is shown above. (b) The gold-standard segmentation and word-object assignments of the same utterance, against which the output of our system is evaluated (all words except “pIg” are mapped to a special “null” object, as explained in the text). that frequently occurs in the presence of an object and not so frequently in its absence is likely to refer to that object (Frank et al., 2009a; Siskind, 1996; Yu and Ballard, 2007). Importantly, all these models assume words are pre-segmented in the input. While the word segmentation task relates less clearly to the communicative content, it can be formulated according to a similar objective, that of attempting to explain the sound sequences in the input in terms of some reasonably small set of reusable units, or words. Computational models have successfully addressed the problem in much this way (Johnson and Goldwater, 2009; Goldwater et al., 2009; Brent, 1999), and the general approach is consistent with experimental observations</context>
<context position="5885" citStr="Frank et al. (2009" startWordPosition="947" endWordPosition="950">r, the two deal with different types of information with different dependencies. The basic idea is that learning a vocabulary that both meets the constraints of the word-learning task and is consistent with the objective of the segmentation task can yield a better segmentation. That is, we hope to find a synergy in the joint inference of meaning and segmentation. Note that to the best of our knowledge there is very little computational work that combines word form and word meaning learning (Frank et al. 2006 takes a first step but their model is applicable only to small artificial languages). Frank et al. (2009a) and Regier (2003) review pure word learning models and, in addition to the papers we have already cited, Brent (1999) presents a fairly comprehensive review of previous pure segmentation models. However, none of the models reviewed make any attempt to jointly address the two problems. Similarly, in the behavioral literature on development, we are aware of only one segmentation study (Graf-Estes et al., 2007) that involves non-linguistic context, though this study treats the two tasks sequentially rather than jointly. We now describe our model and inference procedure and follow with evaluati</context>
<context position="10717" citStr="Frank et al. (2009" startWordPosition="1779" endWordPosition="1782">i.i.d. uniformly from m phonemic types. In addition, there is a probability p# of generating a word boundary. 1 P0(w) = (1 − p#)|w|−1p# m|w| The concentration parameters α0 and α1 also play a critical role in the generation of words and word types. Any given word has a certain probability of either being produced from the set of previously seen word types, or from an entirely new one. The z|O ∼ Uniform(O) � DP(α1, P0) if z =6 0 DP(α0, P0) otherwis P0 ∼ 503 greater the concentration parameter, the more likely the model is to appeal to the base distribution P0 to introduce a new word type. Like Frank et al. (2009a), we distinguish between two coarse grammatical categories, referring and non-referring. Referring words are generated by the topic, while non-referring words are drawn from G0, a distribution associated with the “null” object. The distinction ensures sparse word-object maps that obey the principle of mutual exclusion. Otherwise all words in the utterance would be associated with the topic object, resulting in a very large set of words for each object that is very likely to overlap with the words for other objects. As a further bias toward a small lexicon, we employ different concentration p</context>
<context position="16550" citStr="Frank et al., 2009" startWordPosition="2789" endWordPosition="2792">r the current segment and n(h−) $1is the count for h− of either utterance final words if w1 is utterance final or non-utterance final words if w1 is also not utterance final. In the four cases with a word boundary, we have two words and two categories to sample. (h−) nw2,x2z + αx2P0(w2) P(w2, x2, w3, x3|z, h−) = (h−) n•,x2z + αx2 (h−) nx2 + 1 · (h−) n• + 2 (h−) nw3,x3z + δx2(x3)δw2(w3) + αx3P0(w3) · (h−) n•,x3z + δx2(x3) + αx3 (h−) n$3 + δ$2($3) + 1 · (h−) n• + 3 Here δx(y) is 1 if x = y and 0 otherwise. 4 Results &amp; Model Comparisons 4.1 Corpus Our training corpus (Fernald and Morikawa, 1993; Frank et al., 2009b) consists of about 22,000 words and 5,600 utterances. Video recordings consisting of mother-child play over pairs of toys were orthographically transcribed, and each utterance was annotated with the set of objects present in the nonlinguistic context. The object referred to by the utterance, if any, was noted, as described in Frank et al. (2009b). We used the VoxForge dictionary to map orthographic words to phoneme sequences in a process similar to that described in Brent (1999). Figure 1 (a) presents an example of the coding of phonemic transcription and non-linguistic context for a single </context>
<context position="19728" citStr="Frank et al. (2009" startWordPosition="3344" endWordPosition="3347">y well with an accuracy of 79%. Surprisingly, it seems that knowing the gold segmentation doesn’t add very much, at least for the topic inference task. To evaluate how well we discovered the wordobject map, we manually compiled a list of all the nouns in the corpus that named one of the 30 objects. We used this set of nouns, cross-referenced with their topic objects, as a gold standard set of word-object pairs. By counting the co-occurrences, we also compute a gold standard probability distribution for the words given the topic, P(w|z, x = 1). Precision, recall and F-score are computed as per Frank et al. (2009a). In particular, precision is the fraction of gold pairs among the sampled set and recall is the fraction of sampled pairs among the gold standard pairs. |Sampled ∩ Gold ||Sampled ∩ Gold| p = |Sampled |, r =|Gold| KL divergence is a way of measuring the difference between distributions. Small numbers generally indicate a close match and is zero only when the two are equal. Using the empirical distribution Object Words BOX thebox box BRUSH brush BUNNY rabbit Rosie BUS bus CAR car thecar CHEESE cheese DOG thedoggy doggy DOLL doll thedoll yeah benice DOUGH dough ERNIE Ernie Table 1: Subset of a</context>
<context position="29642" citStr="Frank et al., 2009" startWordPosition="4983" endWordPosition="4986">sts that is not necessary to assume fully segmented input in order to learn word meanings, and that the segmentation and word learning tasks can be effectively modeled in parallel, allowing us to explore potential developmental interactions. The second result suggests that synergies do actually exist and argue not only that we can model the two as parallel processes, but that doing so could prove fruitful. Our model is relatively simple both in terms of word learning and in terms of word segmentation. For instance, social cues and shared attention, or discourse effects, might all play a role (Frank et al., 2009b). Shared features or other relationships can also potentially impact how quickly one might generalize a label to multiple instances (Tenenbaum and Xu, 2000). There are many ways to elaborate on the word learning task, with additional potential synergistic implications. We might also elaborate the linguistic structures we incorporate into the word learning model. For instance, Johnson (2008) explores synergies in syllable and morphological structures in word segmentation. Aspects of linguistic structure, such as morphology, may contribute to word meaning learning beyond its contribution to wo</context>
</contexts>
<marker>Frank, Goodman, Tenenbaum, Fernald, 2009</marker>
<rawString>Michael C. Frank, Noah D. Goodman, Joshua B. Tenenbaum, and Anne Fernald. 2009b. Continuity of discourse provides information for word learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dedre Gentner</author>
<author>Lera Boroditsky</author>
</authors>
<title>Individuation, relativity, and early word learning.</title>
<date>2001</date>
<booktitle>Language, culture, &amp; cognition,</booktitle>
<pages>3--215</pages>
<contexts>
<context position="28149" citStr="Gentner and Boroditsky, 2001" startWordPosition="4735" endWordPosition="4738">rdinate labels like “eyes” and “ears” are coded – this percentage is likely to be much higher, leading to a greater boost in overall segmentation performance. We should acknowledge that the decisions entailed in enriching the annotations are neither trivial nor without theoretic implication, however. It is not immediately obvious how to represent the nonlinguistic correlates of verbs, for instance. Developmentally, verbs are typically acquired much later than nouns, and it has been argued that this may be due to the difficulty of producing a cognitive representation of the associated meaning (Gentner and Boroditsky, 2001). Even among concrete nouns, not all are equal. Children tend to have a bias toward whole objects when mapping novel words to their non-linguistic counterparts (Markman, 1990). Decisions about more sophisticated encoding of nonlinguistic information may thus require more knowledge about children’s representations of the world around them 5 Conclusion and Future Work We find (1) that it is possible to jointly infer both meanings and a segmentation in a fully unsupervised way and (2) that doing so improves the segmentation performance of our model. In particular, we found that although the word </context>
</contexts>
<marker>Gentner, Boroditsky, 2001</marker>
<rawString>Dedre Gentner and Lera Boroditsky. 2001. Individuation, relativity, and early word learning. Language, culture, &amp; cognition, 3:215–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<contexts>
<context position="4879" citStr="Goldwater et al., 2009" startWordPosition="779" endWordPosition="782">nce of an object and not so frequently in its absence is likely to refer to that object (Frank et al., 2009a; Siskind, 1996; Yu and Ballard, 2007). Importantly, all these models assume words are pre-segmented in the input. While the word segmentation task relates less clearly to the communicative content, it can be formulated according to a similar objective, that of attempting to explain the sound sequences in the input in terms of some reasonably small set of reusable units, or words. Computational models have successfully addressed the problem in much this way (Johnson and Goldwater, 2009; Goldwater et al., 2009; Brent, 1999), and the general approach is consistent with experimental observations that humans are sensitive to statistics of sound sequences (Saffran et al., 1996; Frank et al., 2007). The two tasks can be integrated in a relatively seamless way, since, as we have just formulated them, they have a common objective, that of finding a minimal, consistent set of reusable units. However, the two deal with different types of information with different dependencies. The basic idea is that learning a vocabulary that both meets the constraints of the word-learning task and is consistent with the o</context>
<context position="7405" citStr="Goldwater et al. (2009)" startWordPosition="1198" endWordPosition="1202">are drawn from the objects in the nonlinguistic context. The model associates each utterance with a single referent object, the topic, and every word in the utterance is either generated from a distribution over words associated with that object or else from a distribution associated with a special “null” object shared by all utterances. Note that in this paper we use “topic” to denote the referent object of an utterance, otherwise we depart from topic modeling convention and use the term “object” instead. Segmentation is based on the unigram model proposed by Brent (1999) and reformulated by Goldwater et al. (2009) in terms of a Dirichlet process. Since both LDA and the unigram segmenter are based on unigram distributions it is relatively straightforward 502 Figure 2: Topical Unigram Model: Oj is the set of objects in the non-linguistic context of the jth utterance, zj is the utterance topic, wji is the ith word of the utterance, xji is the category of the word (referring or non-referring), and the other variables are distribution parameters. to integrate the two to simultaneously infer word boundaries and word-object associations. Figure 2 illustrates a slightly simplified form of the model, and the th</context>
<context position="11853" citStr="Goldwater et al., 2009" startWordPosition="1960" endWordPosition="1963"> objects. As a further bias toward a small lexicon, we employ different concentration parameters (α0 and α1) for the non-referring and referring words, using a much smaller value for the referring words. Intuitively, there should be a relatively small prior probability of introducing a new word-object pair, corresponding to a small α1 value. On the other hand, most other words don’t refer to the topic object (or any other object for that matter), corresponding to a much larger α0 value. Note that this topical unigram model is a straightforward generalization of the unigram segmentation model (Goldwater et al., 2009) to the case of multiple topics. In fact, if all words were assumed to refer to the same object (or to no object at all) the models would be identical. Unlike LDA, each “document” has only one topic, which is necessitated by the fact that in our model documents correspond single utterances. The utterances in our corpus of child directed speech are often only four or five words long, whereas the general LDA model assumes documents are much larger. Thus, there may not be enough words to infer a useful utterance specific distribution over topics. Consequently, rather than inferring a separate top</context>
<context position="15034" citStr="Goldwater et al. (2009)" startWordPosition="2503" endWordPosition="2507">o all factors involving only non-referring words are absorbed by the constant of proportionality. The word categories can be sampled conditioned on the current word boundary states according to the following conditional probability, where n(h�ji) xji is the number of words categorized according to label Wj ri� w 504 xji over the entire corpus excluding word wji. P(xji|wji, zj, h−ji) a P(wji|zj, xji, h−ji) ·P(xji|h−ji) (h−ji) nxji + 1 · (h−ji) n• + 2 In practice, however, we actually sample the word category variables jointly with the boundary states, using a scheme similar to that outlined in Goldwater et al. (2009). We visit each possible word boundary location (any point between two consecutive phonemes) and compute probabilities for the hypotheses for which the phonemic environment makes up either one word or two. As illustrated below there are two sets of cases: those where we treat the segment as a single word, and those where we treat it as two words. x1 x2 x3 ...#w1#... vs. . . .#w2#w3#... T T The probabilities of the hypotheses can be derived by application of equation 1. Since the x variables can each describe two possible events, there are a total of six different cases to consider for each bou</context>
<context position="21617" citStr="Goldwater et al., 2009" startWordPosition="3668" endWordPosition="3671">meaningful word-object associations; results are shown in Table 2. As in the case of topic accuracy, the joint and word learning only variants perform similarly, this time with somewhat better performance for the easier task with an F-score and KL divergence of 0.31 and 1.82 vs. 0.28 and 2.78 for the joint task. Table 1 illustrates the sort of word-object pairs the model discovers. As can be seen, many of the errors are due to the segmentation, usually undersegmentation errors where it segments two words as one. This is a general problem with the unigram segmenter on which our model is based (Goldwater et al., 2009). Yet, even though these segmentation errors are also counted as word learning errors, they are often still meaningful in the sense that the true referring word is a subsequence. So, word segmentation has an impact on word learning. Yet, the joint model still tends to uncover reasonable meanings. The next question is whether these meanings have an impact on the segmentation. 506 NoCon Random Joint Referring Nouns 0.36 0.35 0.50 Neighbors 0.33 0.33 0.37 Utt Final Nouns 0.36 0.36 0.52 Entire Corpus 0.53 0.53 0.54 Table 3: Segmentation performance. F-score for three subsets and the full corpus fo</context>
<context position="23069" citStr="Goldwater et al. (2009)" startWordPosition="3902" endWordPosition="3905"> full joint task against two other variants: one where topics are randomly selected, and one that ignores the non-linguistic context. For the random topics variant, we choose each topic during initialization according to the empirical distribution over gold topics and treat these topic assignments as observed variables for subsequent iterations. The variant that ignores non-linguistic context draws topics uniformly from the entire set of objects ever discussed in the corpus, another test of the contribution of the nonlinguistic context to segmentation. We report token F-score, computed as per Goldwater et al. (2009), where any segment proposed by the model is a true positive only if it matches the gold segmentation and is a false positive otherwise. Any segment in the gold data not found by the model is a false negative. Table 3 shows the segmentation performance for various subsets as well as for the entire corpus. Because we are primarily interested in synergies between word learning and segmentation, we focus on the words most directly impacted by the meanings: gold standard referring nouns and their neighboring words. The model behaves the same with randomized topics as without context; it learns non</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2009. A bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katharine Graf-Estes</author>
<author>Julia L Evans</author>
<author>Martha W Alibali</author>
<author>Jenny R Saffran</author>
</authors>
<title>Can infants map meaning to newly segmented words? statistical segmentation and word learning.</title>
<date>2007</date>
<journal>Psychological Science,</journal>
<volume>18</volume>
<issue>3</issue>
<pages>260</pages>
<contexts>
<context position="6299" citStr="Graf-Estes et al., 2007" startWordPosition="1013" endWordPosition="1016">re is very little computational work that combines word form and word meaning learning (Frank et al. 2006 takes a first step but their model is applicable only to small artificial languages). Frank et al. (2009a) and Regier (2003) review pure word learning models and, in addition to the papers we have already cited, Brent (1999) presents a fairly comprehensive review of previous pure segmentation models. However, none of the models reviewed make any attempt to jointly address the two problems. Similarly, in the behavioral literature on development, we are aware of only one segmentation study (Graf-Estes et al., 2007) that involves non-linguistic context, though this study treats the two tasks sequentially rather than jointly. We now describe our model and inference procedure and follow with evaluation and discussion. 2 Model Definition Cross-situational meaning learning in our joint word learning and segmenting model is inspired by the model of Frank et al. (2009a). Our model can be viewed as a variant of the Latent Dirichlet Allocation (LDA) topic model of Blei et al. (2003), where topics are drawn from the objects in the nonlinguistic context. The model associates each utterance with a single referent o</context>
</contexts>
<marker>Graf-Estes, Evans, Alibali, Saffran, 2007</marker>
<rawString>Katharine Graf-Estes, Julia L. Evans, Martha W. Alibali, and Jenny R. Saffran. 2007. Can infants map meaning to newly segmented words? statistical segmentation and word learning. Psychological Science, 18(3):254– 260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Gruber</author>
<author>Michal Rosen-Zvi</author>
<author>Yair Weiss</author>
</authors>
<title>Hidden topic markov models.</title>
<date>2007</date>
<booktitle>In Artificial Intelligence and Statistics (AISTATS),</booktitle>
<contexts>
<context position="13046" citStr="Gruber et al., 2007" startWordPosition="2167" endWordPosition="2170">an inferring a separate topic distribution for each utterance, we simply assume a uniform distribution over objects in the nonlinguistic context. In effect, we rely entirely on the non-linguistic context and word-object associations to infer topics. Though necessitated by data sparsity issues, we also note that it is very rare in our corpus for utterances to refer to more than one object in the non-linguistic context, so the choice of a single topic may also be a more accurate model. In fact, even with multi-sentence documents, LDA may perform better if only one topic is assumed per sentence (Gruber et al., 2007). 3 Inference We use a collapsed Gibbs sampling procedure, integrating over all possible Gz, 7r, and -y values and then iteratively sample values for each variable conditioned on the current state of all other variables. We visit each utterance once per iteration, sample a topic, and then visit each possible word boundary location to sample the boundary and word categories simultaneously according to their joint probability. A single topic is sampled for each utterance, conditioned on the words and their current determinations as referring or non-referring. Since zj is drawn from a uniform dis</context>
</contexts>
<marker>Gruber, Rosen-Zvi, Weiss, 2007</marker>
<rawString>Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007. Hidden topic markov models. In Artificial Intelligence and Statistics (AISTATS), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North Ameri-</booktitle>
<contexts>
<context position="4855" citStr="Johnson and Goldwater, 2009" startWordPosition="775" endWordPosition="778">requently occurs in the presence of an object and not so frequently in its absence is likely to refer to that object (Frank et al., 2009a; Siskind, 1996; Yu and Ballard, 2007). Importantly, all these models assume words are pre-segmented in the input. While the word segmentation task relates less clearly to the communicative content, it can be formulated according to a similar objective, that of attempting to explain the sound sequences in the input in terms of some reasonably small set of reusable units, or words. Computational models have successfully addressed the problem in much this way (Johnson and Goldwater, 2009; Goldwater et al., 2009; Brent, 1999), and the general approach is consistent with experimental observations that humans are sensitive to statistics of sound sequences (Saffran et al., 1996; Frank et al., 2007). The two tasks can be integrated in a relatively seamless way, since, as we have just formulated them, they have a common objective, that of finding a minimal, consistent set of reusable units. However, the two deal with different types of information with different dependencies. The basic idea is that learning a vocabulary that both meets the constraints of the word-learning task and </context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North Ameri-</rawString>
</citation>
<citation valid="true">
<title>can Chapter of the Association for Computational Linguistics,</title>
<date></date>
<pages>317--325</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<marker></marker>
<rawString>can Chapter of the Association for Computational Linguistics, pages 317–325, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Using adaptor grammars to identifying synergies in the unsupervised acquisition of linguistic structure.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<marker>Johnson, 2008</marker>
<rawString>Mark Johnson. 2008. Using adaptor grammars to identifying synergies in the unsupervised acquisition of linguistic structure. In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Markman</author>
</authors>
<title>Constraints children place on word learning.</title>
<date>1990</date>
<journal>Cognitive Science,</journal>
<pages>14--57</pages>
<contexts>
<context position="3430" citStr="Markman, 1990" startWordPosition="543" endWordPosition="544">taking as input the unsegmented phonemic representation of the speech along with the set of objects in the non-linguistic context as shown in Figure 1 (a), and infers both a segmentation and a word-object mapping as in Figure 1 (b). One can formulate the word learning task as that of finding a reasonably small set of reusable word-meaning pairs consistent with the underlying communicative intent. Infant directed speech often refers to objects in the immediate environment, and early word learning seems to involve associating frequently co-occurring word-object pairs (Akhtar and Montague, 1999; Markman, 1990). Several computational models are based on this idea that a word 501 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 501–509, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Figure 1: (a) The input to our system for the utterance “Is that the pig?” consists of an unsegmented sequence of phonemes and the set of objects representing the nonlinguistic context. These objects were manually identified by inspecting the associated video, a frame from which is shown above. (b) The gold-standard segmentation a</context>
<context position="25966" citStr="Markman, 1990" startWordPosition="4389" endWordPosition="4390"> biases the model so that the distributions over referring words are very different from that over non-referring words. A small concentration parameter biases the estimator to prefer a small set of word types. In contrast, the relatively large concentration parameter for the nonreferring words tends to result in most of the words receiving highest probability as non-referring words. The model thus categorizes words accordingly. It is in part due to this tendency towards sparse wordobject maps that the model enforces mutual exclusivity, a phenomenon well documented among natural word learners (Markman, 1990). Aside from contributing to mutual exclusivity and specialization among the topical word distributions, the small concentration parameter also has important implications for the segmentation task. A very small value for α1 discourages the learner from acquiring more word types for each meaning than absolutely necessary, thereby forcing the segmenter to use fewer types to explain the sequence of phonemes. A model without any notion 507 of meaning cannot maintain separate distributions for different topics, and must in some sense treat all words as non-referring. A segmenting model without mean</context>
<context position="28324" citStr="Markman, 1990" startWordPosition="4764" endWordPosition="4765">decisions entailed in enriching the annotations are neither trivial nor without theoretic implication, however. It is not immediately obvious how to represent the nonlinguistic correlates of verbs, for instance. Developmentally, verbs are typically acquired much later than nouns, and it has been argued that this may be due to the difficulty of producing a cognitive representation of the associated meaning (Gentner and Boroditsky, 2001). Even among concrete nouns, not all are equal. Children tend to have a bias toward whole objects when mapping novel words to their non-linguistic counterparts (Markman, 1990). Decisions about more sophisticated encoding of nonlinguistic information may thus require more knowledge about children’s representations of the world around them 5 Conclusion and Future Work We find (1) that it is possible to jointly infer both meanings and a segmentation in a fully unsupervised way and (2) that doing so improves the segmentation performance of our model. In particular, we found that although the word learning side suffered from segmentation errors, and performed worse than a model that learned from a gold standard segmentation, the loss was only slight. On the other hand, </context>
</contexts>
<marker>Markman, 1990</marker>
<rawString>Ellen M. Markman. 1990. Constraints children place on word learning. Cognitive Science, 14:57–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Regier</author>
</authors>
<title>Emergent constraints on wordlearning: A computational review.</title>
<date>2003</date>
<booktitle>Trends in Cognitive Sciences,</booktitle>
<pages>7--263</pages>
<contexts>
<context position="5905" citStr="Regier (2003)" startWordPosition="952" endWordPosition="953">erent types of information with different dependencies. The basic idea is that learning a vocabulary that both meets the constraints of the word-learning task and is consistent with the objective of the segmentation task can yield a better segmentation. That is, we hope to find a synergy in the joint inference of meaning and segmentation. Note that to the best of our knowledge there is very little computational work that combines word form and word meaning learning (Frank et al. 2006 takes a first step but their model is applicable only to small artificial languages). Frank et al. (2009a) and Regier (2003) review pure word learning models and, in addition to the papers we have already cited, Brent (1999) presents a fairly comprehensive review of previous pure segmentation models. However, none of the models reviewed make any attempt to jointly address the two problems. Similarly, in the behavioral literature on development, we are aware of only one segmentation study (Graf-Estes et al., 2007) that involves non-linguistic context, though this study treats the two tasks sequentially rather than jointly. We now describe our model and inference procedure and follow with evaluation and discussion. 2</context>
</contexts>
<marker>Regier, 2003</marker>
<rawString>Terry Regier. 2003. Emergent constraints on wordlearning: A computational review. Trends in Cognitive Sciences, 7:263–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Saffran</author>
<author>Elissa L Newport</author>
<author>Richard N Aslin</author>
</authors>
<title>Word segmentation: The role of distributional cues.</title>
<date>1996</date>
<journal>Journal of memory and Language,</journal>
<pages>35--606</pages>
<contexts>
<context position="5045" citStr="Saffran et al., 1996" startWordPosition="804" endWordPosition="807">se models assume words are pre-segmented in the input. While the word segmentation task relates less clearly to the communicative content, it can be formulated according to a similar objective, that of attempting to explain the sound sequences in the input in terms of some reasonably small set of reusable units, or words. Computational models have successfully addressed the problem in much this way (Johnson and Goldwater, 2009; Goldwater et al., 2009; Brent, 1999), and the general approach is consistent with experimental observations that humans are sensitive to statistics of sound sequences (Saffran et al., 1996; Frank et al., 2007). The two tasks can be integrated in a relatively seamless way, since, as we have just formulated them, they have a common objective, that of finding a minimal, consistent set of reusable units. However, the two deal with different types of information with different dependencies. The basic idea is that learning a vocabulary that both meets the constraints of the word-learning task and is consistent with the objective of the segmentation task can yield a better segmentation. That is, we hope to find a synergy in the joint inference of meaning and segmentation. Note that to</context>
</contexts>
<marker>Saffran, Newport, Aslin, 1996</marker>
<rawString>Jenny R. Saffran, Elissa L. Newport, and Richard N. Aslin. 1996. Word segmentation: The role of distributional cues. Journal of memory and Language, 35:606–621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey M Siskind</author>
</authors>
<title>A computational study of cross-situational techniques for learning word-tomeaning mappings.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--1</pages>
<contexts>
<context position="4380" citStr="Siskind, 1996" startWordPosition="700" endWordPosition="701">pig?” consists of an unsegmented sequence of phonemes and the set of objects representing the nonlinguistic context. These objects were manually identified by inspecting the associated video, a frame from which is shown above. (b) The gold-standard segmentation and word-object assignments of the same utterance, against which the output of our system is evaluated (all words except “pIg” are mapped to a special “null” object, as explained in the text). that frequently occurs in the presence of an object and not so frequently in its absence is likely to refer to that object (Frank et al., 2009a; Siskind, 1996; Yu and Ballard, 2007). Importantly, all these models assume words are pre-segmented in the input. While the word segmentation task relates less clearly to the communicative content, it can be formulated according to a similar objective, that of attempting to explain the sound sequences in the input in terms of some reasonably small set of reusable units, or words. Computational models have successfully addressed the problem in much this way (Johnson and Goldwater, 2009; Goldwater et al., 2009; Brent, 1999), and the general approach is consistent with experimental observations that humans are</context>
</contexts>
<marker>Siskind, 1996</marker>
<rawString>Jeffrey M. Siskind. 1996. A computational study of cross-situational techniques for learning word-tomeaning mappings. Cognition, 61(1-2):39–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua B Tenenbaum</author>
<author>Fei Xu</author>
</authors>
<title>Word learning as bayesian inference.</title>
<date>2000</date>
<booktitle>In Proceedings of the 22nd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>517--522</pages>
<contexts>
<context position="29800" citStr="Tenenbaum and Xu, 2000" startWordPosition="5007" endWordPosition="5010">tively modeled in parallel, allowing us to explore potential developmental interactions. The second result suggests that synergies do actually exist and argue not only that we can model the two as parallel processes, but that doing so could prove fruitful. Our model is relatively simple both in terms of word learning and in terms of word segmentation. For instance, social cues and shared attention, or discourse effects, might all play a role (Frank et al., 2009b). Shared features or other relationships can also potentially impact how quickly one might generalize a label to multiple instances (Tenenbaum and Xu, 2000). There are many ways to elaborate on the word learning task, with additional potential synergistic implications. We might also elaborate the linguistic structures we incorporate into the word learning model. For instance, Johnson (2008) explores synergies in syllable and morphological structures in word segmentation. Aspects of linguistic structure, such as morphology, may contribute to word meaning learning beyond its contribution to word segmentation performance. Acknowledgments This research was funded by NSF awards 0544127 and 0631667 to Mark Johnson and by NSF DDRIG 0746251 to Michael C.</context>
</contexts>
<marker>Tenenbaum, Xu, 2000</marker>
<rawString>Joshua B. Tenenbaum and Fei Xu. 2000. Word learning as bayesian inference. In Proceedings of the 22nd Annual Conference of the Cognitive Science Society, pages 517–522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Yu</author>
<author>Dana H Ballard</author>
</authors>
<title>A unified model of early word learning: Integrating statistical and social cues.</title>
<date>2007</date>
<journal>Neurocomputing,</journal>
<pages>70--13</pages>
<contexts>
<context position="4403" citStr="Yu and Ballard, 2007" startWordPosition="702" endWordPosition="705">of an unsegmented sequence of phonemes and the set of objects representing the nonlinguistic context. These objects were manually identified by inspecting the associated video, a frame from which is shown above. (b) The gold-standard segmentation and word-object assignments of the same utterance, against which the output of our system is evaluated (all words except “pIg” are mapped to a special “null” object, as explained in the text). that frequently occurs in the presence of an object and not so frequently in its absence is likely to refer to that object (Frank et al., 2009a; Siskind, 1996; Yu and Ballard, 2007). Importantly, all these models assume words are pre-segmented in the input. While the word segmentation task relates less clearly to the communicative content, it can be formulated according to a similar objective, that of attempting to explain the sound sequences in the input in terms of some reasonably small set of reusable units, or words. Computational models have successfully addressed the problem in much this way (Johnson and Goldwater, 2009; Goldwater et al., 2009; Brent, 1999), and the general approach is consistent with experimental observations that humans are sensitive to statistic</context>
</contexts>
<marker>Yu, Ballard, 2007</marker>
<rawString>Chen Yu and Dana H. Ballard. 2007. A unified model of early word learning: Integrating statistical and social cues. Neurocomputing, 70(13-15):2149–2165.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>