<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.050271">
<title confidence="0.960877">
Deep Learning for NLP (without Magic)
</title>
<author confidence="0.730751">
Richard Socher Yoshua Bengio* Christopher D. Manning
</author>
<email confidence="0.617011">
richard@socher.org bengioy@iro.umontreal.ca, manning@stanford.edu
</email>
<note confidence="0.8296565">
Computer Science Department, Stanford University
* DIRO, Universit´e de Montr´eal, Montr´eal, QC, Canada
</note>
<sectionHeader confidence="0.980744" genericHeader="abstract">
1 Abtract
</sectionHeader>
<bodyText confidence="0.999962805555556">
Machine learning is everywhere in today’s NLP, but
by and large machine learning amounts to numerical
optimization of weights for human designed repre-
sentations and features. The goal of deep learning
is to explore how computers can take advantage of
data to develop features and representations appro-
priate for complex interpretation tasks. This tuto-
rial aims to cover the basic motivation, ideas, mod-
els and learning algorithms in deep learning for nat-
ural language processing. Recently, these methods
have been shown to perform very well on various
NLP tasks such as language modeling, POS tag-
ging, named entity recognition, sentiment analysis
and paraphrase detection, among others. The most
attractive quality of these techniques is that they can
perform well without any external hand-designed re-
sources or time-intensive feature engineering. De-
spite these advantages, many researchers in NLP are
not familiar with these methods. Our focus is on
insight and understanding, using graphical illustra-
tions and simple, intuitive derivations. The goal of
the tutorial is to make the inner workings of these
techniques transparent, intuitive and their results in-
terpretable, rather than black boxes labeled ”magic
here”.
The first part of the tutorial presents the basics of
neural networks, neural word vectors, several simple
models based on local windows and the math and
algorithms of training via backpropagation. In this
section applications include language modeling and
POS tagging.
In the second section we present recursive neural
networks which can learn structured tree outputs as
well as vector representations for phrases and sen-
tences. We cover both equations as well as applica-
tions. We show how training can be achieved by a
</bodyText>
<page confidence="0.863835">
5
</page>
<bodyText confidence="0.999603888888889">
modified version of the backpropagation algorithm
introduced before. These modifications allow the al-
gorithm to work on tree structures. Applications in-
clude sentiment analysis and paraphrase detection.
We also draw connections to recent work in seman-
tic compositionality in vector spaces. The princi-
ple goal, again, is to make these methods appear in-
tuitive and interpretable rather than mathematically
confusing. By this point in the tutorial, the audience
members should have a clear understanding of how
to build a deep learning system for word-, sentence-
and document-level tasks.
The last part of the tutorial gives a general
overview of the different applications of deep learn-
ing in NLP, including bag of words models. We will
provide a discussion of NLP-oriented issues in mod-
eling, interpretation, representational power, and op-
timization.
</bodyText>
<subsectionHeader confidence="0.347686">
Tutorial Abstracts of ACL 2012, page 5,
</subsectionHeader>
<affiliation confidence="0.396052">
Jeju, Republic of Korea, 8 July 2012. c�2012 Association for Computational Linguistics
</affiliation>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.207173">
<title confidence="0.990769">Deep Learning for NLP (without Magic)</title>
<author confidence="0.998269">Socher Yoshua Christopher D Manning</author>
<affiliation confidence="0.999989">Computer Science Department, Stanford University</affiliation>
<address confidence="0.567945">Universit´e de Montr´eal, Montr´eal, QC, Canada</address>
<abstract confidence="0.986525142857143">1 Abtract Machine learning is everywhere in today’s NLP, but by and large machine learning amounts to numerical optimization of weights for human designed representations and features. The goal of deep learning is to explore how computers can take advantage of data to develop features and representations appropriate for complex interpretation tasks. This tutorial aims to cover the basic motivation, ideas, models and learning algorithms in deep learning for natural language processing. Recently, these methods have been shown to perform very well on various NLP tasks such as language modeling, POS tagging, named entity recognition, sentiment analysis and paraphrase detection, among others. The most attractive quality of these techniques is that they can perform well without any external hand-designed resources or time-intensive feature engineering. Despite these advantages, many researchers in NLP are not familiar with these methods. Our focus is on insight and understanding, using graphical illustrations and simple, intuitive derivations. The goal of the tutorial is to make the inner workings of these techniques transparent, intuitive and their results interpretable, rather than black boxes labeled ”magic here”. The first part of the tutorial presents the basics of neural networks, neural word vectors, several simple models based on local windows and the math and algorithms of training via backpropagation. In this section applications include language modeling and POS tagging. In the second section we present recursive neural networks which can learn structured tree outputs as well as vector representations for phrases and sentences. We cover both equations as well as applications. We show how training can be achieved by a 5 modified version of the backpropagation algorithm introduced before. These modifications allow the algorithm to work on tree structures. Applications include sentiment analysis and paraphrase detection. We also draw connections to recent work in semantic compositionality in vector spaces. The principle goal, again, is to make these methods appear intuitive and interpretable rather than mathematically confusing. By this point in the tutorial, the audience members should have a clear understanding of how to build a deep learning system for word-, sentenceand document-level tasks. The last part of the tutorial gives a general overview of the different applications of deep learning in NLP, including bag of words models. We will provide a discussion of NLP-oriented issues in modeling, interpretation, representational power, and optimization.</abstract>
<note confidence="0.868017">Abstracts of ACL page 5, Republic of Korea, 8 July 2012. Association for Computational Linguistics</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>