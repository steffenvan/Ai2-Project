<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000034">
<title confidence="0.651468">
Language Model Based Arabic Word Segmentation
Young-Suk Lee Kishore Papineni Salim Roukos
</title>
<author confidence="0.594769333333333">
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
Ossama Emam Hany Hassan
</author>
<affiliation confidence="0.795094">
IBM Cairo Technology Development Center
</affiliation>
<address confidence="0.419926">
P.O.Box 166, El-Ahram, Giza, Egypt
</address>
<sectionHeader confidence="0.998999" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925966666667">
Morphologically rich languages like
Arabic present significant challenges to many
natural language processing applications
because a word often conveys complex
meanings decomposable into several
morphemes (i.e. prefix, stem, suffix). By
segmenting words into morphemes, we can
improve the performance of natural language
systems including machine translation (Brown
et al. 1993) and information retrieval (Franz,
M. and McCarley, S. 2002). In this paper, we
present a general word segmentation algorithm
for handling inflectional morphology capable
of segmenting a word into a prefix*-stem-
suffix* sequence, using a small manually
segmented corpus and a table of
prefixes/suffixes of the language. We do not
address Arabic infix morphology where many
stems correspond to the same root with various
infix variations; we treat all the stems of a
common root as separate atomic units. The use
of a stem as a morpheme (unit of meaning) is
better suited than the use of a root for the
applications we are considering in information
retrieval and machine translation (e.g. different
stems of the same root translate into different
English words.) Examples of Arabic words and
their segmentation into prefix*-stem-suffix* are
given in Table 1, where &apos;#&apos; indicates a
morpheme being a prefix, and &apos;+&apos; a suffix.1 As
</bodyText>
<footnote confidence="0.98919525">
1 Arabic is presented in both native and Buckwalter
transliterated Arabic whenever possible. All native
Arabic is to be read from right-to-left, and transliterated
Arabic is to be read from left-to-right. The convention of
</footnote>
<sectionHeader confidence="0.754664" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999938732142857">
We approximate Arabic’s rich
morphology by a model that a word
consists of a sequence of morphemes in
the pattern prefix*-stem-suffix* (*
denotes zero or more occurrences of a
morpheme). Our method is seeded by a
small manually segmented Arabic corpus
and uses it to bootstrap an unsupervised
algorithm to build the Arabic word
segmenter from a large unsegmented
Arabic corpus. The algorithm uses a
trigram language model to determine the
most probable morpheme sequence for a
given input. The language model is
initially estimated from a small manually
segmented corpus of about 110,000
words. To improve the segmentation
accuracy, we use an unsupervised
algorithm for automatically acquiring
new stems from a 155 million word
unsegmented corpus, and re-estimate the
model parameters with the expanded
vocabulary and training corpus. The
resulting Arabic word segmentation
system achieves around 97% exact match
accuracy on a test corpus containing
28,449 word tokens. We believe this is a
state-of-the-art performance and the
algorithm can be used for many highly
inflected languages provided that one can
create a small manually segmented
corpus of the language of interest.
shown in Table 1, a word may include multiple
prefixes, as in L_� (l: for, Al: the), or multiple
suffixes, as in 4_5 (t: feminine singular, h: his).
A word may also consist only of a stem, as in
,11 (AlY, to/towards).
The algorithm implementation involves (i)
language model training on a morpheme-
segmented corpus, (ii) segmentation of input
text into a sequence of morphemes using the
language model parameters, and (iii)
unsupervised acquisition of new stems from a
large unsegmented corpus. The only linguistic
resources required include a small manually
segmented corpus ranging from 20,000 words
to 100,000 words, a table of prefixes and
suffixes of the language and a large
unsegmented corpus.
In Section 2, we discuss related work. In
Section 3, we describe the segmentation
algorithm. In Section 4, we discuss the
unsupervised algorithm for new stem
acquisition. In Section 5, we present
experimental results. In Section 6, we
summarize the paper.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999778164383562">
Our work adopts major components of the
algorithm from (Luo &amp; Roukos 1996):
language model (LM) parameter estimation
from a segmented corpus and input
segmentation on the basis of LM probabilities.
However, our work diverges from their work
in two crucial respects: (i) new technique of
computing all possible segmentations of a
word into prefix*-stem-suffix* for decoding,
and (ii) unsupervised algorithm for new stem
acquisition based on a stem candidate&apos;s
similarity to stems occurring in the training
corpus.
(Darwish 2002) presents a supervised
technique which identifies the root of an
Arabic word by stripping away the prefix and
the suffix of the word on the basis of manually
acquired dictionary of word-root pairs and the
likelihood that a prefix and a suffix would
occur with the template from which the root is
derived. He reports 92.7% segmentation
marking a prefix with &apos;#&amp;quot; and a suffix with &apos;+&apos; will be
adopted throughout the paper.
accuracy on a 9,606 word evaluation corpus.
His technique pre-supposes at most one prefix
and one suffix per stem regardless of the actual
number and meanings of prefixes/suffixes
associated with the stem. (Beesley 1996)
presents a finite-state morphological analyzer
for Arabic, which displays the root, pattern,
and prefixes/suffixes. The analyses are based
on manually acquired lexicons and rules.
Although his analyzer is comprehensive in the
types of knowledge it presents, it has been
criticized for their extensive development time
and lack of robustness, cf. (Darwish 2002).
(Yarowsky and Wicentowsky 2000)
presents a minimally supervised morphological
analysis with a performance of over 99.2%
accuracy for the 3,888 past-tense test cases in
English. The core algorithm lies in the
estimation of a probabilistic alignment
between inflected forms and root forms. The
probability estimation is based on the lemma
alignment by frequency ratio similarity among
different inflectional forms derived from the
same lemma, given a table of inflectional
parts-of-speech, a list of the canonical suffixes
for each part of speech, and a list of the
candidate noun, verb and adjective roots of the
language. Their algorithm does not handle
multiple affixes per word.
(Goldsmith 2000) presents an unsupervised
technique based on the expectation-
maximization algorithm and minimum
description length to segment exactly one
suffix per word, resulting in an F-score of 81.8
for suffix identification in English according to
(Schone and Jurafsky 2001). (Schone and
Jurafsky 2001) proposes an unsupervised
algorithm capable of automatically inducing
the morphology of inflectional languages using
only text corpora. Their algorithm combines
cues from orthography, semantics, and
contextual information to induce
morphological relationships in German, Dutch,
and English, among others. They report F-
scores between 85 and 93 for suffix analyses
and between 78 and 85 for circumfix analyses
in these languages. Although their algorithm
captures prefix-suffix combinations or
circumfixes, it does not handle the multiple
affixes per word we observe in Arabic.
</bodyText>
<page confidence="0.996467">
2
</page>
<table confidence="0.999697333333333">
Words Prefixes Stems Suffixes
Arabic Translit. Arabic Translit. Arabic Translit. Arabic Translit.
تﺎѧﻳﻻﻮѧﻟ ا AlwlAyAt #لا Al# يﻻو wlAy تا + +At
ﻪѧﺗﺎѧﻴѧﺣ HyAth ﺎѧﻴѧﺣ HyA ﻩ+ ت + +t +h
لﻮѧﺼﺤﻠѧﻟ llHSwl #لا #ل l# Al# لﻮѧﺼﺣ HSwl
ﻰѧﻟا AlY ﻰѧﻟا AlY
</table>
<tableCaption confidence="0.999617">
Table 1 Segmentation of Arabic Words into Prefix*-Stem-Suffix*
</tableCaption>
<sectionHeader confidence="0.905765" genericHeader="method">
3 Morpheme Segmentation
</sectionHeader>
<subsectionHeader confidence="0.923802">
3.1 Trigram Language Model
</subsectionHeader>
<bodyText confidence="0.999381">
Given an Arabic sentence, we use a trigram
language model on morphemes to segment it
into a sequence of morphemes {m1, m2, ...,mn}.
The input to the morpheme segmenter is a
sequence of Arabic tokens – we use a
tokenizer that looks only at white space and
other punctuation, e.g. quotation marks,
parentheses, period, comma, etc. A sample of
a manually segmented corpus is given below2.
Here multiple occurrences of prefixes and
suffixes per word are marked with an
underline.
</bodyText>
<equation confidence="0.99312275">
# لا ﰲ ﻞﺣ يﺬﻟا ﻦﻳﺎﻓﺮﻳا نﺎآ #و
# لا ﺎﺴﳕ #لا ة + ﺰﺋﺎﺟ ﰲ لوا #لا
#ل نﺪﻨﻟ ﱄا
. راﻮﻏﺎﺟ ﻖﻳﺮﻓ رﺎﺷا ﺎﻣ ﺐﺴﺣ ة + يروﺮﺿ
راﻮﻏﺎﺟ ﰲ برﺎﲡ
نﺎﻜﻣ ﻲﺗرﻮﺑ ﻮﻧﺎﻴﺳﻮﻟ ﻲﻠﻳزاﺮﺑ #لا
ﺪﺣا #لا اﺪﻏ قﺎﺒﺳ #لا ﰲ ﻦﻳﺎﻓﺮﻳا
ﰲ ﻩ+ تا + ﻮﻄﺧ ﱄوا نﻮآ # ي #س يﺬﻟا
</equation>
<construct confidence="0.673073">
jAgwAr. w# s# y# Hl sA}q Al# tjArb fy
jAgwAr Al# brAzyly lwsyAnw bwrty mkAn
AyrfAyn fy Al# sbAq gdA Al# AHd Al*y s#
y# kwn Awly xTw +At +h fy EAlm sbAq +At
AlfwrmwlA
</construct>
<bodyText confidence="0.999619952380952">
Many instances of prefixes and suffixes in
Arabic are meaning bearing and correspond to
a word in English such as pronouns and
prepositions. Therefore, we choose a
segmentation into multiple prefixes and
suffixes. Segmentation into one prefix and one
suffix per word, cf. (Darwish 2002), is not very
useful for applications like statistical machine
translation, (Brown et al. 1993), for which an
accurate word-to-word alignment between the
source and the target languages is critical for
high quality translations.
The trigram language model probabilities
of morpheme sequences, p(mi|mi-1, mi-2), are
estimated from the morpheme-segmented
corpus. At token boundaries, the morphemes
from previous tokens constitute the histories of
the current morpheme in the trigram language
model. The trigram model is smoothed using
deleted interpolation with the bigram and
unigram models, (Jelinek 1997), as in (1):
</bodyText>
<equation confidence="0.99916575">
#ب ﺮﻌﺷ يراﲑﻓ ة + رﺎﻴﺳ ﻲﻠﻋ ﻲﺿﺎﻣ # لا
#لا ﱄا ﻩ + ت + ﺮﻄﺿا ﻩ + ﻦﻄﺑ ﰲ مﻻا
دﻮﻋ #ي #س ﻮه # و برﺎﲡ
#لا ﻦﻣ بﺎﺤﺴﻧا
ﺰآﺮﻣ
مﺎﻋ
اﺮﺟا
#لا تا
#لا ء
+ صﻮﺤﻓ
#لا ﻖﺋﺎﺳ ﻞﺣ #ي #س #و
ﻻﻮﻣرﻮﻔﻟا تا + قﺎﺒﺳ ﱂﺎﻋ
</equation>
<bodyText confidence="0.478953363636364">
w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al#
Awl fy jA}z +p Al# nmsA Al# EAm Al#
mADy Ely syAr +p fyrAry $Er b# AlAm fy
bTn +h ADTr +t +h Aly Al# AnsHAb mn Al#
tjArb w# hw s# y# Ewd Aly lndn l# AjrA&apos; Al#
fHwS +At Al# Drwry +p Hsb mA A$Ar fryq
2 A manually segmented Arabic corpus containing about
140K word tokens has been provided by LDC
(http://www.ldc.upenn.edu). We divided this corpus into
training and the development test sets as described in
Section 5.
</bodyText>
<equation confidence="0.999402">
(1) p(m3  |m1 ,m2) = λ3 p(m3 |m1 ,m2) + λ2
p(m3 |m2) + λ3 p(m3), where λ1+λ2 +λ3 = 1.
</equation>
<bodyText confidence="0.999966833333333">
A small morpheme-segmented corpus
results in a relatively high out of vocabulary
rate for the stems. We describe below an
unsupervised acquisition of new stems from a
large unsegmented Arabic corpus. However,
we first describe the segmentation algorithm.
</bodyText>
<subsectionHeader confidence="0.534342">
3.2 Decoder for Morpheme Segmentation
</subsectionHeader>
<page confidence="0.990612">
3
</page>
<bodyText confidence="0.992971785714286">
We take the unit of decoding to be a sentence
that has been tokenized using white space and
punctuation. The task of a decoder is to find
the morpheme sequence which maximizes the
trigram probability of the input sentence, as in
(2):
(2) SEGMENTATIONbest = Argmax IIi=1, N
p(mi|mi-1mi-2), N = number of morphemes in
the input.
Search algorithm for (2) is informally
described for each word token as follows:
Step 1: Compute all possible segmentations of
the token (to be elaborated in 3.2.1).
Step 2: Compute the trigram language model
score of each segmentation. For some
segmentations of a token, the stem may be an
out of vocabulary item. In that case, we use an
“UNKNOWN” class in the trigram language
model with the model probability given by
p(UNKNOWN|mi-1, mi-2) * UNK_Fraction, where
UNK_Fraction is 1e-9 determined on empirical
grounds. This allows us to segment new words
with a high accuracy even with a relatively
high number of unknown stems in the
language model vocabulary, cf. experimental
results in Tables 5 &amp; 6.
Step 3: Keep the top N highest scored
segmentations.
</bodyText>
<subsectionHeader confidence="0.773522">
3.2.1 Possible Segmentations of a Word
</subsectionHeader>
<bodyText confidence="0.976035875">
Possible segmentations of a word token are
restricted to those derivable from a table of
prefixes and suffixes of the language for
decoder speed-up and improved accuracy.
Table 2 shows examples of atomic (e.g. لا,
تا) and multi-component (e.g. لﺎѧﺑو, ﺎѧﻬﺗا)
prefixes and suffixes, along with their
component morphemes in native Arabic.3
</bodyText>
<footnote confidence="0.731344888888889">
3 We have acquired the prefix/suffix table from a 110K
word manually segmented LDC corpus (51 prefixes &amp; 72
suffixes) and from IBM-Egypt (additional 14 prefixes &amp;
122 suffixes). The performance improvement by the
additional prefix/suffix list ranges from 0.07% to 0.54%
according to the manually segmented training corpus
size. The smaller the manually segmented corpus size is,
the bigger the performance improvement by adding
additional prefix/suffix list is.
</footnote>
<table confidence="0.99930875">
Prefixes Suffixes
لا #لا تا تا+
لﺎѧﺑ #لا #ب ﺎѧﻬﺗا ﺎه+ تا+
لﺎѧﺑو #لا #ب #و ﻢﻬѧﻧو ﻢه+ نو+
</table>
<tableCaption confidence="0.993911">
Table 2 Prefix/Suffix Table
</tableCaption>
<bodyText confidence="0.9989312">
Each token is assumed to have the structure
prefix*-stem-suffix*, and is compared against
the prefix/suffix table for segmentation. Given
a word token, (i) identify all of the matching
prefixes and suffixes from the table, (ii) further
segment each matching prefix/suffix at each
character position, and (iii) enumerate all
prefix*-stem-suffix* sequences derivable from
(i) and (ii).
Table 3 shows all of its possible
segmentations of the token ﺎهرﺮآاو
(wAkrrhA; &apos;and I repeat it&apos;),4 where 0 indicates
the null prefix/suffix and the Seg Score is the
language model probabilities of each
segmentation S1 ... S12. For this token, there
are two matching prefixes #و(w#) and
#او(wA#) from the prefix table, and two
matching suffixes ا+(+A) and ﺎه+(+hA)
from the suffix table. S1, S2, &amp; S3 are the
segmentations given the null prefix 0 and
suffixes 0, +A, +hA. S4, S5, &amp; S6 are the
segmentations given the prefix w# and suffixes
0, +A, +hA. S7, S8, &amp; S9 are the
segmentations given the prefix wA# and
suffixes 0, +A, +hA. S10, S11, &amp; S12 are the
segmentations given the prefix sequence w#
A# derived from the prefix wA# and suffixes
0, +A, +hA. As illustrated by S12, derivation
of sub-segmentations of the matching
prefixes/suffixes enables the system to identify
possible segmentations which would have been
missed otherwise. In this case, segmentation
including the derived prefix sequence
ﺎه+ رﺮآ #ا #و(n# A# krr +hA) happens to
be the correct one.
</bodyText>
<subsectionHeader confidence="0.683438">
3.2.2. Prefix-Suffix Filter
</subsectionHeader>
<bodyText confidence="0.9415065">
While the number of possible segmentations is
maximized by sub-segmenting matching
</bodyText>
<footnote confidence="0.598112">
4 A sentence in which the token occurs is as follows: ﺎﻬﺘﻠﻗ
ﺔﻴﻄﻔﻨﻟا تﺎﻘﺘﺸﻤﻟا ﻲﻓ ﺎﻤﻧاو مﺎﺨﻟا ﻂﻔﻨﻟا ﻲﻓ ﺖﺴﻴﻟ ﺔﻠﻜﺸﻤﻟﺎﻓ ﺎهرﺮآاو
(qlthA wAkrrhA fAlm$klp lyst fy AlfnT AlxAm wAnmA fy
Alm$tqAt AlnfTyp.)
</footnote>
<page confidence="0.989811">
4
</page>
<bodyText confidence="0.999420818181818">
prefixes and suffixes, some of illegitimate sub-
segmentations are filtered out on the basis of
the knowledge specific to the manually
segmented corpus. For instance, sub-
segmentation of the suffix hA into +h +A is
ruled out because there is no suffix sequence
+h +A in the training corpus. Likewise, sub-
segmentation of the prefix Al into A# l# is
filtered out. Filtering out improbable
prefix/suffix sequences improves the
segmentation accuracy, as shown in Table 5.
</bodyText>
<table confidence="0.989715153846154">
Prefix Stem Suffix Seg Scores
S1 0 wAkrrhA 0 2.6071e-05
S2 0 wAkrrh +A 1.36561e-06
S3 0 wAkrr +hA 9.45933e-07
S4 w# AkrrhA 0 2.72648e-06
S5 w# Akrrh +A 5.64843e-07
S6 w# Akrr +hA 4.52229e-05
S7 wA# krrhA 0 7.58256e-10
S8 wA# krrh +A 5.09988e-11
S9 wA# krr +hA 1.91774e-08
S10 w# A# krrhA 0 7.69038e-07
S11 w# A# krrh +A 1.82663e-07
S12 w# A# krr +hA 0.000944511
</table>
<tableCaption confidence="0.9570315">
Table 3 Possible Segmentations of
LxjjtSIy (wAkrrhA)
</tableCaption>
<sectionHeader confidence="0.9650715" genericHeader="method">
4 Unsupervised Acquisition of New
Stems
</sectionHeader>
<bodyText confidence="0.999820066666667">
Once the seed segmenter is developed on the
basis of a manually segmented corpus, the
performance may be improved by iteratively
expanding the stem vocabulary and retraining
the language model on a large automatically
segmented Arabic corpus.
Given a small manually segmented corpus
and a large unsegmented corpus, segmenter
development proceeds as follows.
Initialization: Develop the seed segmenter
Segmenter0 trained on the manually segmented
corpus Corpus0, using the language model
vocabulary, Vocab0, acquired from Corpus0.
Iteration: For i = 1 to N, N = the number of
partitions of the unsegmented corpus
</bodyText>
<listItem confidence="0.750768">
i. Use Segmenteri-1 to segment Corpusi.
ii. Acquire new stems from the newly
segmented Corpusi. Add the new stems to
</listItem>
<bodyText confidence="0.948396619047619">
Vocabi-1, creating an expanded vocabulary
Vocabi.
iii. Develop Segmenteri trained on Corpus0
through Corpusi with Vocabi.
Optimal Performance Identification:
Identify the Corpusi and Vocabi, which result
in the best performance, i.e. system training
with Corpusi+1 and Vocabi+1 does not improve
the performance any more.
Unsupervised acquisition of new stems
from an automatically segmented new corpus
is a three-step process: (i) select new stem
candidates on the basis of a frequency
threshold, (ii) filter out new stem candidates
containing a sub-string with a high likelihood
of being a prefix, suffix, or prefix-suffix. The
likelihood of a sub-string being a prefix, suffix,
and prefix-suffix of a token is computed as in
(5) to (7), (iii) further filter out new stem
candidates on the basis of contextual
information, as in (8).
</bodyText>
<listItem confidence="0.980456428571429">
(5) Pscore = number of tokens with prefix P /
number of tokens starting with sub-string P
(6) Sscore = number of tokens with suffix S /
number of tokens ending with sub-string S
(7) PSscore = number of tokens with prefix P
and suffix S / number of tokens starting with
sub-string P and ending with sub-string S
</listItem>
<bodyText confidence="0.99940675">
Stem candidates containing a sub-string with a
high prefix, suffix, or prefix-suffix likelihood
are filtered out. Example sub-strings with the
prefix, suffix, prefix-suffix likelihood 0.85 or
higher in a 110K word manually segmented
corpus are given in Table 4. If a token starts
with the sub-string ـﻨﺱ (sn), and end with ﺎﻬـ
(hA), the sub-string&apos;s likelihood of being the
prefix-suffix of the token is 1. If a token starts
with the sub-string ﻞѧﻟ (ll), the sub-string&apos;s
likelihood of being the prefix of the token is
0.945, etc.
</bodyText>
<table confidence="0.9959092">
Arabic Transliteration Score
ﺎﻬـ + stem # ـﻨﺱ sn# stem+hA 1.0
ة+ stem # ـѧﻟا Al# stem+p 0.984
stem # ﻞѧﻟ ll# stem 0.945
تا+ stem stem+At 0.889
</table>
<tableCaption confidence="0.999105">
Table 4 Prefix/Suffix Likelihood Score
</tableCaption>
<page confidence="0.984935">
5
</page>
<listItem confidence="0.9838935">
(8) Contextual Filter: (i) Filter out stems co-
occurring with prefixes/suffixes not present in
the training corpus. (ii) Filter out stems whose
prefix/suffix distributions are highly
disproportionate to those seen in the training
corpus.
</listItem>
<bodyText confidence="0.992939785714286">
According to (8), if a stem is followed by
a potential suffix +m, not present in the
training corpus, then it is filtered out as an
illegitimate stem. In addition, if a stem is
preceded by a prefix and/or followed by a
suffix with a significantly higher proportion
than that observed in the training corpus, it is
filtered out. For instance, the probability for
the suffix +A to follow a stem is less than 50%
in the training corpus regardless of the stem
properties, and therefore, if a candidate stem is
followed by +A with the probability of over
70%, e.g. mAnyl +A, then it is filtered out as
an illegitimate stem.
</bodyText>
<sectionHeader confidence="0.983363" genericHeader="method">
5 Performance Evaluations
</sectionHeader>
<bodyText confidence="0.999990142857143">
We present experimental results illustrating the
impact of three factors on segmentation error
rate: (i) the base algorithm, i.e. language model
training and decoding, (ii) language model
vocabulary and training corpus size, and (iii)
manually segmented training corpus size.
Segmentation error rate is defined in (9).
</bodyText>
<listItem confidence="0.9411575">
(9) (number of incorrectly segmented tokens /
total number of tokens) x 100
</listItem>
<bodyText confidence="0.999828153846154">
Evaluations have been performed on a
development test corpus containing 28,449
word tokens. The test set is extracted from
20001115_AFP_ARB.0060.xml.txt through
20001115_AFP_ARB.0236.xml.txt of the
LDC Arabic Treebank: Part 1 v 2.0 Corpus.
Impact of the core algorithm and the
unsupervised stem acquisition has been
measured on segmenters developed from 4
different sizes of manually segmented seed
corpora: 10K, 20K, 40K, and 110K words.
The experimental results are shown in
Table 5. The baseline performances are
obtained by assigning each token the most
frequently occurring segmentation in the
manually segmented training corpus. The
column headed by &apos;3-gram LM&apos; indicates the
impact of the segmenter using only trigram
language model probabilities for decoding.
Regardless of the manually segmented training
corpus size, use of trigram language model
probabilities reduces the word error rate of the
corresponding baseline by approximately 50%.
The column headed by &apos;3-gram LM + PS
Filter&apos; indicates the impact of the core
algorithm plus Prefix-Suffix Filter discussed in
Section 3.2.2. Prefix-Suffix Filter reduces the
word error rate ranging from 7.4% for the
smallest (10K word) manually segmented
corpus to 21.8% for the largest (110K word)
manually segmented corpus - around 1%
absolute reduction for all segmenters. The
column headed by &apos;3-gram LM + PS Filter +
New Stems&apos; shows the impact of unsupervised
stem acquisition from a 155 million word
Arabic corpus. Word error rate reduction due
to the unsupervised stem acquisition is 38% for
the segmenter developed from the 10K word
manually segmented corpus and 32% for the
segmenter developed from 110K word
manually segmented corpus.
Language model vocabulary size (LM VOC
Size) and the unknown stem ratio (OOV ratio)
of various segmenters is given in Table 6. For
unsupervised stem acquisition, we have set the
frequency threshold at 10 for every 10-15
million word corpus, i.e. any new morphemes
occurring more than 10 times in a 10-15
million word corpus are considered to be new
stem candidates. Prefix, suffix, prefix-suffix
likelihood score to further filter out illegitimate
stem candidates was set at 0.5 for the
segmenters developed from 10K, 20K, and
40K manually segmented corpora, whereas it
was set at 0.85 for the segmenters developed
from a 110K manually segmented corpus.
Both the frequency threshold and the optimal
prefix, suffix, prefix-suffix likelihood scores
were determined on empirical grounds.
Contextual Filter stated in (8) has been applied
only to the segmenter developed from 110K
manually segmented training corpus.5
Comparison of Tables 5 and 6 indicates a high
correlation between the segmentation error rate
and the unknown stem ratio.
</bodyText>
<footnote confidence="0.870934">
5 Without the Contextual Filter, the error rate of the
same segmenter is 3.1%.
</footnote>
<page confidence="0.998072">
6
</page>
<table confidence="0.999775">
Manually Segmented Baseline 3-gram LM 3-gram LM + 3-gram LM + PS
Training Corpus Size PS Filter Filter + New Stems
10K Words 26.0% 14.7% 13.6% 8.5%
20K Words 19.7% 9.1% 8.0% 5.9%
40K Words 14.3% 7.6% 6.5% 5.1%
110K Words 11.0% 5.5% 4.3% 2.9%
</table>
<tableCaption confidence="0.987185">
Table 5 Impact of Core Algorithm and LM Vocabulary Size on Segmentation Error Rate
</tableCaption>
<table confidence="0.999728714285714">
Manually Segmented 3-gram LM 3-gram LM + PS Filter + New Stems
Training Corpus Size
LM VOC Size OOV Ratio LM VOC Size OOV Ratio
10K Words 2,496 20.4% 22,964 7.8%
20K Words 4,111 11.4% 25,237 5.3%
40K Words 5,531 9.0% 21,156 4.7%
110K Words 8,196 5.8% 25,306 1.9%
</table>
<tableCaption confidence="0.989184">
Table 6 Language Model Vocabulary Size and Out of Vocabulary Ratio
</tableCaption>
<table confidence="0.999623">
Manually Segmented 3-gram LM + PS Filter + New Stems
Training Corpus Size
Unknown Stem Alywm Other Errors Total # of Errors
10 K Words 1,844 (76.9%) 98 (4.1%) 455 (19.0%) 2,397
20 K Words 1,174 (71.1%) 82 (5.0%) 395 (23.9%) 1,651
40 K Words 1,005 (69.9%) 81 (5.6%) 351 (24.4%) 1,437
110 K Words 333 (39.6%) 82 (9.8%) 426 (50.7%) 841
</table>
<tableCaption confidence="0.997189">
Table 7 Segmentation Error Analyses
</tableCaption>
<bodyText confidence="0.988248255319149">
Table 7 gives the error analyses of four
segmenters according to three factors: (i)
errors due to unknown stems, (ii) errors
involving مﻮﻴѧﻟا (Alywm), and (iii) errors due to
other factors. Interestingly, the segmenter
developed from a 110K manually segmented
corpus has the lowest percentage of “unknown
stem” errors at 39.6% indicating that our
unsupervised acquisition of new stems is
working well, as well as suggesting to use a
larger unsegmented corpus for unsupervised
stem acquisition.
مﻮﻴѧﻟا (Alywm) should be segmented
differently depending on its part-of-speech to
capture the semantic ambiguities. If it is an
adverb or a proper noun, it is segmented as
مﻮﻴѧﻟا &apos;today/Al-Youm&apos;, whereas if it is a noun,
it is segmented as مﻮѧﻳ #لا &apos;the day.&apos; Proper
segmentation of مﻮﻴѧﻟا primarily requires its
part-of-speech information, and cannot be
easily handled by morpheme trigram models
alone.
Other errors include over-segmentation of
foreign words such as ﻦѧﻴѧﺗﻮѧﺑ (bwtyn) as ب#
ﻦѧﻴѧﺗو and ﺮѧﺘѧﻴѧﻟ (lytr) &apos;litre&apos; as ﺮѧﺗ #ي #ل.
These errors are attributed to the segmentation
ambiguities of these tokens: ﻦѧﻴѧﺗﻮѧﺑ is
ambiguous between &apos;ﻦѧﻴѧﺗﻮѧﺑ (Putin)&apos; and &apos;ب#
ﻦѧﻴѧﺗو (by aorta)&apos;. ﺮѧﺘѧﻴѧﻟ is ambiguous
between &apos;ﺮѧﺘѧﻴѧﻟ (litre)&apos; and &apos; ﺮѧﺗ #ي #ل (for him
to harm)&apos;. These errors may also be corrected
by incorporating part-of-speech information
for disambiguation.
To address the segmentation ambiguity
problem, as illustrated by &apos; ﻦѧﻴѧﺗﻮѧﺑ (Putin)&apos; vs.
&apos; ﻦѧﻴѧﺗو #ب (by aorta)&apos;, we have developed a
joint model for segmentation and part-of-
speech tagging for which the best
segmentation of an input sentence is obtained
according to the formula (10), where ti is the
part-of-speech of morpheme mi, and N is the
number of morphemes in the input sentence.
(10) SEGMENTATIONbest = Argmax Πi=1,N
p(mi|mi-1 mi-2) p(ti|ti-1 ti-2) p(mi|ti)
By using the joint model, the segmentation
word error rate of the best performing
segmenter has been reduced by about 10%
</bodyText>
<page confidence="0.997203">
7
</page>
<bodyText confidence="0.972738">
from 2.9% (cf. the last column of Table 5) to
2.6%.
</bodyText>
<sectionHeader confidence="0.993137" genericHeader="evaluation">
5 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.99996768">
We have presented a robust word segmentation
algorithm which segments a word into a
prefix*-stem-suffix* sequence, along with
experimental results. Our Arabic word
segmentation system implementing the
algorithm achieves around 97% segmentation
accuracy on a development test corpus
containing 28,449 word tokens. Since the
algorithm can identify any number of prefixes
and suffixes of a given token, it is generally
applicable to various language families
including agglutinative languages (Korean,
Turkish, Finnish), highly inflected languages
(Russian, Czech) as well as semitic languages
(Arabic, Hebrew).
Our future work includes (i) application
of the current technique to other highly
inflected languages, (ii) application of the
unsupervised stem acquisition technique on
about 1 billion word unsegmented Arabic
corpus, and (iii) adoption of a novel
morphological analysis technique to handle
irregular morphology, as realized in Arabic
broken plurals YL+S (ktAb) &apos;book&apos; vs. ��„�&lt;
(ktb) &apos;books&apos;.
</bodyText>
<sectionHeader confidence="0.978154" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999980909090909">
This work was partially supported by the
Defense Advanced Research Projects Agency
and monitored by SPAWAR under contract No.
N66001-99-2-8916. The views and findings
contained in this material are those of the
authors and do not necessarily reflect the
position of policy of the Government and no
official endorsement should be inferred. We
would like to thank Martin Franz for discussions
on language model building, and his help with
the use of ViaVoice language model toolkit.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999873153846154">
Beesley, K. 1996. Arabic Finite-State
Morphological Analysis and Generation.
Proceedings of COLING-96, pages 89− 94.
Brown, P., Della Pietra, S., Della Pietra, V.,
and Mercer, R. 1993. The mathematics of
statistical machine translation: Parameter
Estimation. Computational Linguistics,
19(2): 263−311.
Darwish, K. 2002. Building a Shallow Arabic
Morphological Analyzer in One Day.
Proceedings of the Workshop on
Computational Approaches to Semitic
Languages, pages 47−54.
Franz, M. and McCarley, S. 2002. Arabic
Information Retrieval at IBM. Proceedings
of TREC 2002, pages 402− 405.
Goldsmith, J. 2000. Unsupervised learning
of the morphology of a natural language.
Computational Linguistics, 27(1).
Jelinek, F. 1997. Statistical Methods for
Speech Recognition. The MIT Press.
Luo, X. and Roukos, S. 1996. An Iterative
Algorithm to Build Chinese Language
Models. Proceedings of ACL-96, pages
139−143.
Schone, P. and Jurafsky, D. 2001.
Knowledge-Free Induction of Inflectional
Morphologies. Proceedings of North
American Chapter of Association for
Computational Linguistics.
Yarowsky, D. and Wicentowski, R. 2000.
Minimally supervised morphological
analysis by multimodal alignment.
Proceedings of ACL-2000, pages 207− 216.
Yarowsky, D, Ngai G. and Wicentowski, R.
2001. Inducting Multilingual Text Analysis
Tools via Robust Projection across Aligned
Corpora. Proceedings of HLT 2001, pages
161−168.
</reference>
<page confidence="0.998493">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.807083">
<title confidence="0.999961">Language Model Based Arabic Word Segmentation</title>
<author confidence="0.998046">Young-Suk Lee Kishore Papineni Salim Roukos</author>
<affiliation confidence="0.99991">IBM T. J. Watson Research Center</affiliation>
<address confidence="0.950071">Yorktown Heights, NY 10598</address>
<author confidence="0.824194">Ossama Emam Hany Hassan</author>
<affiliation confidence="0.997442">IBM Cairo Technology Development Center</affiliation>
<address confidence="0.997697">P.O.Box 166, El-Ahram, Giza, Egypt</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Beesley</author>
</authors>
<title>Arabic Finite-State Morphological Analysis and Generation.</title>
<date>1996</date>
<booktitle>Proceedings of COLING-96,</booktitle>
<pages>89--94</pages>
<contexts>
<context position="5118" citStr="Beesley 1996" startWordPosition="798" endWordPosition="799">ique which identifies the root of an Arabic word by stripping away the prefix and the suffix of the word on the basis of manually acquired dictionary of word-root pairs and the likelihood that a prefix and a suffix would occur with the template from which the root is derived. He reports 92.7% segmentation marking a prefix with &apos;#&amp;quot; and a suffix with &apos;+&apos; will be adopted throughout the paper. accuracy on a 9,606 word evaluation corpus. His technique pre-supposes at most one prefix and one suffix per stem regardless of the actual number and meanings of prefixes/suffixes associated with the stem. (Beesley 1996) presents a finite-state morphological analyzer for Arabic, which displays the root, pattern, and prefixes/suffixes. The analyses are based on manually acquired lexicons and rules. Although his analyzer is comprehensive in the types of knowledge it presents, it has been criticized for their extensive development time and lack of robustness, cf. (Darwish 2002). (Yarowsky and Wicentowsky 2000) presents a minimally supervised morphological analysis with a performance of over 99.2% accuracy for the 3,888 past-tense test cases in English. The core algorithm lies in the estimation of a probabilistic</context>
</contexts>
<marker>Beesley, 1996</marker>
<rawString>Beesley, K. 1996. Arabic Finite-State Morphological Analysis and Generation. Proceedings of COLING-96, pages 89− 94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>Della Pietra</author>
<author>Della Pietra S</author>
<author>V</author>
<author>R Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="642" citStr="Brown et al. 1993" startWordPosition="84" endWordPosition="87">bic Word Segmentation Young-Suk Lee Kishore Papineni Salim Roukos IBM T. J. Watson Research Center Yorktown Heights, NY 10598 Ossama Emam Hany Hassan IBM Cairo Technology Development Center P.O.Box 166, El-Ahram, Giza, Egypt 1 Introduction Morphologically rich languages like Arabic present significant challenges to many natural language processing applications because a word often conveys complex meanings decomposable into several morphemes (i.e. prefix, stem, suffix). By segmenting words into morphemes, we can improve the performance of natural language systems including machine translation (Brown et al. 1993) and information retrieval (Franz, M. and McCarley, S. 2002). In this paper, we present a general word segmentation algorithm for handling inflectional morphology capable of segmenting a word into a prefix*-stemsuffix* sequence, using a small manually segmented corpus and a table of prefixes/suffixes of the language. We do not address Arabic infix morphology where many stems correspond to the same root with various infix variations; we treat all the stems of a common root as separate atomic units. The use of a stem as a morpheme (unit of meaning) is better suited than the use of a root for the</context>
<context position="8643" citStr="Brown et al. 1993" startWordPosition="1366" endWordPosition="1369">ﺑ #لا ﺪﺣا #لا اﺪﻏ قﺎﺒﺳ #لا ﰲ ﻦﻳﺎﻓﺮﻳا ﰲ ﻩ+ تا + ﻮﻄﺧ ﱄوا نﻮآ # ي #س يﺬﻟا jAgwAr. w# s# y# Hl sA}q Al# tjArb fy jAgwAr Al# brAzyly lwsyAnw bwrty mkAn AyrfAyn fy Al# sbAq gdA Al# AHd Al*y s# y# kwn Awly xTw +At +h fy EAlm sbAq +At AlfwrmwlA Many instances of prefixes and suffixes in Arabic are meaning bearing and correspond to a word in English such as pronouns and prepositions. Therefore, we choose a segmentation into multiple prefixes and suffixes. Segmentation into one prefix and one suffix per word, cf. (Darwish 2002), is not very useful for applications like statistical machine translation, (Brown et al. 1993), for which an accurate word-to-word alignment between the source and the target languages is critical for high quality translations. The trigram language model probabilities of morpheme sequences, p(mi|mi-1, mi-2), are estimated from the morpheme-segmented corpus. At token boundaries, the morphemes from previous tokens constitute the histories of the current morpheme in the trigram language model. The trigram model is smoothed using deleted interpolation with the bigram and unigram models, (Jelinek 1997), as in (1): #ب ﺮﻌﺷ يراﲑﻓ ة + رﺎﻴﺳ ﻲﻠﻋ ﻲﺿﺎﻣ # لا #لا ﱄا ﻩ + ت + ﺮﻄﺿا ﻩ + ﻦﻄﺑ ﰲ مﻻا دﻮﻋ #ي </context>
</contexts>
<marker>Brown, Pietra, S, V, Mercer, 1993</marker>
<rawString>Brown, P., Della Pietra, S., Della Pietra, V., and Mercer, R. 1993. The mathematics of statistical machine translation: Parameter Estimation. Computational Linguistics, 19(2): 263−311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Darwish</author>
</authors>
<title>Building a Shallow Arabic Morphological Analyzer in One Day.</title>
<date>2002</date>
<booktitle>Proceedings of the Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>47--54</pages>
<contexts>
<context position="4477" citStr="Darwish 2002" startWordPosition="691" endWordPosition="692">we present experimental results. In Section 6, we summarize the paper. 2 Related Work Our work adopts major components of the algorithm from (Luo &amp; Roukos 1996): language model (LM) parameter estimation from a segmented corpus and input segmentation on the basis of LM probabilities. However, our work diverges from their work in two crucial respects: (i) new technique of computing all possible segmentations of a word into prefix*-stem-suffix* for decoding, and (ii) unsupervised algorithm for new stem acquisition based on a stem candidate&apos;s similarity to stems occurring in the training corpus. (Darwish 2002) presents a supervised technique which identifies the root of an Arabic word by stripping away the prefix and the suffix of the word on the basis of manually acquired dictionary of word-root pairs and the likelihood that a prefix and a suffix would occur with the template from which the root is derived. He reports 92.7% segmentation marking a prefix with &apos;#&amp;quot; and a suffix with &apos;+&apos; will be adopted throughout the paper. accuracy on a 9,606 word evaluation corpus. His technique pre-supposes at most one prefix and one suffix per stem regardless of the actual number and meanings of prefixes/suffixes</context>
<context position="8548" citStr="Darwish 2002" startWordPosition="1354" endWordPosition="1355">#لا #ل نﺪﻨﻟ ﱄا . راﻮﻏﺎﺟ ﻖﻳﺮﻓ رﺎﺷا ﺎﻣ ﺐﺴﺣ ة + يروﺮﺿ راﻮﻏﺎﺟ ﰲ برﺎﲡ نﺎﻜﻣ ﻲﺗرﻮﺑ ﻮﻧﺎﻴﺳﻮﻟ ﻲﻠﻳزاﺮﺑ #لا ﺪﺣا #لا اﺪﻏ قﺎﺒﺳ #لا ﰲ ﻦﻳﺎﻓﺮﻳا ﰲ ﻩ+ تا + ﻮﻄﺧ ﱄوا نﻮآ # ي #س يﺬﻟا jAgwAr. w# s# y# Hl sA}q Al# tjArb fy jAgwAr Al# brAzyly lwsyAnw bwrty mkAn AyrfAyn fy Al# sbAq gdA Al# AHd Al*y s# y# kwn Awly xTw +At +h fy EAlm sbAq +At AlfwrmwlA Many instances of prefixes and suffixes in Arabic are meaning bearing and correspond to a word in English such as pronouns and prepositions. Therefore, we choose a segmentation into multiple prefixes and suffixes. Segmentation into one prefix and one suffix per word, cf. (Darwish 2002), is not very useful for applications like statistical machine translation, (Brown et al. 1993), for which an accurate word-to-word alignment between the source and the target languages is critical for high quality translations. The trigram language model probabilities of morpheme sequences, p(mi|mi-1, mi-2), are estimated from the morpheme-segmented corpus. At token boundaries, the morphemes from previous tokens constitute the histories of the current morpheme in the trigram language model. The trigram model is smoothed using deleted interpolation with the bigram and unigram models, (Jelinek </context>
</contexts>
<marker>Darwish, 2002</marker>
<rawString>Darwish, K. 2002. Building a Shallow Arabic Morphological Analyzer in One Day. Proceedings of the Workshop on Computational Approaches to Semitic Languages, pages 47−54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Franz</author>
<author>S McCarley</author>
</authors>
<title>Arabic Information Retrieval at IBM.</title>
<date>2002</date>
<booktitle>Proceedings of TREC 2002,</booktitle>
<pages>402--405</pages>
<marker>Franz, McCarley, 2002</marker>
<rawString>Franz, M. and McCarley, S. 2002. Arabic Information Retrieval at IBM. Proceedings of TREC 2002, pages 402− 405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="6178" citStr="Goldsmith 2000" startWordPosition="956" endWordPosition="957">ysis with a performance of over 99.2% accuracy for the 3,888 past-tense test cases in English. The core algorithm lies in the estimation of a probabilistic alignment between inflected forms and root forms. The probability estimation is based on the lemma alignment by frequency ratio similarity among different inflectional forms derived from the same lemma, given a table of inflectional parts-of-speech, a list of the canonical suffixes for each part of speech, and a list of the candidate noun, verb and adjective roots of the language. Their algorithm does not handle multiple affixes per word. (Goldsmith 2000) presents an unsupervised technique based on the expectationmaximization algorithm and minimum description length to segment exactly one suffix per word, resulting in an F-score of 81.8 for suffix identification in English according to (Schone and Jurafsky 2001). (Schone and Jurafsky 2001) proposes an unsupervised algorithm capable of automatically inducing the morphology of inflectional languages using only text corpora. Their algorithm combines cues from orthography, semantics, and contextual information to induce morphological relationships in German, Dutch, and English, among others. They </context>
</contexts>
<marker>Goldsmith, 2000</marker>
<rawString>Goldsmith, J. 2000. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1997</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="9153" citStr="Jelinek 1997" startWordPosition="1439" endWordPosition="1440">sh 2002), is not very useful for applications like statistical machine translation, (Brown et al. 1993), for which an accurate word-to-word alignment between the source and the target languages is critical for high quality translations. The trigram language model probabilities of morpheme sequences, p(mi|mi-1, mi-2), are estimated from the morpheme-segmented corpus. At token boundaries, the morphemes from previous tokens constitute the histories of the current morpheme in the trigram language model. The trigram model is smoothed using deleted interpolation with the bigram and unigram models, (Jelinek 1997), as in (1): #ب ﺮﻌﺷ يراﲑﻓ ة + رﺎﻴﺳ ﻲﻠﻋ ﻲﺿﺎﻣ # لا #لا ﱄا ﻩ + ت + ﺮﻄﺿا ﻩ + ﻦﻄﺑ ﰲ مﻻا دﻮﻋ #ي #س ﻮه # و برﺎﲡ #لا ﻦﻣ بﺎﺤﺴﻧا ﺰآﺮﻣ مﺎﻋ اﺮﺟا #لا تا #لا ء + صﻮﺤﻓ #لا ﻖﺋﺎﺳ ﻞﺣ #ي #س #و ﻻﻮﻣرﻮﻔﻟا تا + قﺎﺒﺳ ﱂﺎﻋ w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al# Awl fy jA}z +p Al# nmsA Al# EAm Al# mADy Ely syAr +p fyrAry $Er b# AlAm fy bTn +h ADTr +t +h Aly Al# AnsHAb mn Al# tjArb w# hw s# y# Ewd Aly lndn l# AjrA&apos; Al# fHwS +At Al# Drwry +p Hsb mA A$Ar fryq 2 A manually segmented Arabic corpus containing about 140K word tokens has been provided by LDC (http://www.ldc.upenn.edu). We divided this corpus into training and t</context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>Jelinek, F. 1997. Statistical Methods for Speech Recognition. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>S Roukos</author>
</authors>
<title>An Iterative Algorithm to Build Chinese Language Models.</title>
<date>1996</date>
<booktitle>Proceedings of ACL-96,</booktitle>
<pages>139--143</pages>
<contexts>
<context position="4024" citStr="Luo &amp; Roukos 1996" startWordPosition="622" endWordPosition="625">pervised acquisition of new stems from a large unsegmented corpus. The only linguistic resources required include a small manually segmented corpus ranging from 20,000 words to 100,000 words, a table of prefixes and suffixes of the language and a large unsegmented corpus. In Section 2, we discuss related work. In Section 3, we describe the segmentation algorithm. In Section 4, we discuss the unsupervised algorithm for new stem acquisition. In Section 5, we present experimental results. In Section 6, we summarize the paper. 2 Related Work Our work adopts major components of the algorithm from (Luo &amp; Roukos 1996): language model (LM) parameter estimation from a segmented corpus and input segmentation on the basis of LM probabilities. However, our work diverges from their work in two crucial respects: (i) new technique of computing all possible segmentations of a word into prefix*-stem-suffix* for decoding, and (ii) unsupervised algorithm for new stem acquisition based on a stem candidate&apos;s similarity to stems occurring in the training corpus. (Darwish 2002) presents a supervised technique which identifies the root of an Arabic word by stripping away the prefix and the suffix of the word on the basis o</context>
</contexts>
<marker>Luo, Roukos, 1996</marker>
<rawString>Luo, X. and Roukos, S. 1996. An Iterative Algorithm to Build Chinese Language Models. Proceedings of ACL-96, pages 139−143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Schone</author>
<author>D Jurafsky</author>
</authors>
<title>Knowledge-Free Induction of Inflectional Morphologies.</title>
<date>2001</date>
<booktitle>Proceedings of North</booktitle>
<contexts>
<context position="6440" citStr="Schone and Jurafsky 2001" startWordPosition="992" endWordPosition="995">ma alignment by frequency ratio similarity among different inflectional forms derived from the same lemma, given a table of inflectional parts-of-speech, a list of the canonical suffixes for each part of speech, and a list of the candidate noun, verb and adjective roots of the language. Their algorithm does not handle multiple affixes per word. (Goldsmith 2000) presents an unsupervised technique based on the expectationmaximization algorithm and minimum description length to segment exactly one suffix per word, resulting in an F-score of 81.8 for suffix identification in English according to (Schone and Jurafsky 2001). (Schone and Jurafsky 2001) proposes an unsupervised algorithm capable of automatically inducing the morphology of inflectional languages using only text corpora. Their algorithm combines cues from orthography, semantics, and contextual information to induce morphological relationships in German, Dutch, and English, among others. They report Fscores between 85 and 93 for suffix analyses and between 78 and 85 for circumfix analyses in these languages. Although their algorithm captures prefix-suffix combinations or circumfixes, it does not handle the multiple affixes per word we observe in Arab</context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>Schone, P. and Jurafsky, D. 2001. Knowledge-Free Induction of Inflectional Morphologies. Proceedings of North</rawString>
</citation>
<citation valid="false">
<institution>American Chapter of Association for Computational Linguistics.</institution>
<marker></marker>
<rawString>American Chapter of Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>R Wicentowski</author>
</authors>
<title>Minimally supervised morphological analysis by multimodal alignment.</title>
<date>2000</date>
<booktitle>Proceedings of ACL-2000,</booktitle>
<pages>207--216</pages>
<marker>Yarowsky, Wicentowski, 2000</marker>
<rawString>Yarowsky, D. and Wicentowski, R. 2000. Minimally supervised morphological analysis by multimodal alignment. Proceedings of ACL-2000, pages 207− 216. Yarowsky, D, Ngai G. and Wicentowski, R. 2001. Inducting Multilingual Text Analysis Tools via Robust Projection across Aligned Corpora. Proceedings of HLT 2001, pages 161−168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>