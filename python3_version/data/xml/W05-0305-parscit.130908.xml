<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.969862">
Attribution and the (Non-)Alignment of Syntactic and Discourse Arguments
of Connectives
</title>
<author confidence="0.987104">
Nikhil Dinesh and Alan Lee and Eleni Miltsakaki and Rashmi Prasad and Aravind Joshi
</author>
<affiliation confidence="0.997392">
University of Pennsylvania
</affiliation>
<address confidence="0.637426">
Philadelphia, PA 19104 USA
</address>
<email confidence="0.994948">
{nikhild,aleewk,elenimi,rjprasad,joshi}@linc.cis.upenn.edu
</email>
<author confidence="0.996611">
Bonnie Webber
</author>
<affiliation confidence="0.999274">
University of Edinburgh
</affiliation>
<address confidence="0.877914">
Edinburgh, EH8 9LW Scotland
</address>
<email confidence="0.997731">
bonnie@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997371" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999874166666667">
The annotations of the Penn Discourse
Treebank (PDTB) include (1) discourse
connectives and their arguments, and (2)
attribution of each argument of each con-
nective and of the relation it denotes. Be-
cause the PDTB covers the same text as
the Penn TreeBank WSJ corpus, syntac-
tic and discourse annotation can be com-
pared. This has revealed significant dif-
ferences between syntactic structure and
discourse structure, in terms of the argu-
ments of connectives, due in large part to
attribution. We describe these differences,
an algorithm for detecting them, and fi-
nally some experimental results. These re-
sults have implications for automating dis-
course annotation based on syntactic an-
notation.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999002538461539">
The overall goal of the Penn Discourse Treebank
(PDTB) is to annotate the million word WSJ cor-
pus in the Penn TreeBank (Marcus et al., 1993) with
a layer of discourse annotations. A preliminary re-
port on this project was presented at the 2004 work-
shop on Frontiers in Corpus Annotation (Miltsakaki
et al., 2004a), where we described our annotation
of discourse connectives (both explicit and implicit)
along with their (clausal) arguments.
Further work done since then includes the an-
notation of attribution: that is, who has expressed
each argument to a discourse connective (the writer
or some other speaker or author) and who has ex-
</bodyText>
<page confidence="0.97855">
29
</page>
<bodyText confidence="0.999988607142857">
pressed the discourse relation itself. These ascrip-
tions need not be the same. Of particular interest is
the fact that attribution may or may not play a role
in the relation established by a connective. This may
lead to a lack of congruence between arguments at
the syntactic and the discourse levels. The issue of
congruence is of interest both from the perspective
of annotation (where it means that, even within a
single sentence, one cannot merely transfer the an-
notation of syntactic arguments of a subordinate or
coordinate conjunction to its discourse arguments),
and from the perspective of inferences that these an-
notations will support in future applications of the
PDTB.
The paper is organized as follows. We give a brief
overview of the annotation of connectives and their
arguments in the PDTB in Section 2. In Section 3,
we describe the annotation of the attribution of the
arguments of a connective and the relation it con-
veys. In Sections 4 and 5, we describe mismatches
that arise between the discourse arguments of a con-
nective and the syntactic annotation as provided by
the Penn TreeBank (PTB), in the cases where all the
arguments of the connective are in the same sen-
tence. In Section 6, we will discuss some implica-
tions of these issues for the theory and practice of
discourse annotation and their relevance even at the
level of sentence-bound annotation.
</bodyText>
<sectionHeader confidence="0.914118" genericHeader="method">
2 Overview of the PDTB
</sectionHeader>
<bodyText confidence="0.99989">
The PDTB builds on the DLTAG approach to dis-
course structure (Webber and Joshi, 1998; Webber
et al., 1999; Webber et al., 2003) in which con-
nectives are discourse-level predicates which project
predicate-argument structure on a par with verbs at
</bodyText>
<note confidence="0.98202">
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999470307692308">
the sentence level. Initial work on the PDTB has
been described in Miltsakaki et al. (2004a), Milt-
sakaki et al. (2004b), Prasad et al. (2004).
The key contribution of the PDTB design frame-
work is its bottom-up approach to discourse struc-
ture: Instead of appealing to an abstract (and arbi-
trary) set of discourse relations whose identification
may confound multiple sources of discourse mean-
ing, we start with the annotation of discourse con-
nectives and their arguments, thus exposing a clearly
defined level of discourse representation.
The PDTB annotates as explicit discourse connec-
tives all subordinating conjunctions, coordinating
conjunctions and discourse adverbials. These pred-
icates establish relations between two abstract ob-
jects such as events, states and propositions (Asher,
1993).1
We use Conn to denote the connective, and Arg1
and Arg2 to denote the textual spans from which the
abstract object arguments are computed.2 In (1), the
subordinating conjunction since establishes a tem-
poral relation between the event of the earthquake
hitting and a state where no music is played by a
certain woman. In all the examples in this paper, as
in (1), Arg1 is italicized, Arg2 is in boldface, and
Conn is underlined.
</bodyText>
<listItem confidence="0.5288625">
(1) She hasn’t played any music since the earthquake
hit.
</listItem>
<bodyText confidence="0.99935875">
What counts as a legal argument? Since we take
discourse relations to hold between abstract objects,
we require that an argument contains at least one
clause-level predication (usually a verb – tensed or
untensed), though it may span as much as a sequence
of clauses or sentences. The two exceptions are
nominal phrases that express an event or a state, and
discourse deictics that denote an abstract object.
</bodyText>
<footnote confidence="0.9136146">
1For example, discourse adverbials like as a result are dis-
tinguished from clausal adverbials like strangely which require
only a single abstract object (Forbes, 2003).
2Each connective has exactly two arguments. The argument
that appears in the clause syntactically associated with the con-
</footnote>
<bodyText confidence="0.895380058823529">
nective, we call Arg2. The other argument is called Arg1. Both
Arg1 and Arg2 can be in the same sentence, as is the case for
subordinating conjunctions (e.g., because). The linear order of
the arguments will be Arg2 Arg1 if the subordinate clause ap-
pears sentence initially; Arg1 Arg2 if the subordinate clause ap-
pears sentence finally; and undefined if it appears sentence me-
dially. For an adverbial connective like however, Arg1 is in the
prior discourse. Hence, the linear order of its arguments will be
Arg1 Arg2.
Because our annotation is on the same corpus as
the PTB, annotators may select as arguments textual
spans that omit content that can be recovered from
syntax. In (2), for example, the relative clause is
selected as Arg1 of even though, and its subject can
be recovered from its syntactic analysis in the PTB.
In (3), the subject of the infinitival clause in Arg1 is
similarly available.
</bodyText>
<listItem confidence="0.991304857142857">
(2) Workers described “clouds of blue dust” that hung
over parts of the factory even though exhaust fans
ventilated the air.
(3) The average maturity for funds open only to institu-
tions, considered by some to be a stronger indicator
because those managers watch the market closely,
reached a high point for the year – 33 days.
</listItem>
<bodyText confidence="0.99979575">
The PDTB also annotates implicit connectives be-
tween adjacent sentences where no explicit connec-
tive occurs. For example, in (4), the two sentences
are contrasted in a way similar to having an explicit
connective like but occurring between them. Anno-
tators are asked to provide, when possible, an ex-
plicit connective that best describes the relation, and
in this case in contrast was chosen.
</bodyText>
<listItem confidence="0.970676666666667">
(4) The $6 billion that some 40 companies are looking to
raise in the year ending March 21 compares with only
$2.7 billion raise on the capital market in the previous
year. IMPLICIT - in contrast In fiscal 1984, before
Mr. Gandhi came into power, only $810 million
was raised.
</listItem>
<bodyText confidence="0.99879425">
When complete, the PDTB will contain approxi-
mately 35K annotations: 15K annotations of the 100
explicit connectives identified in the corpus and 20K
annotations of implicit connectives.3
</bodyText>
<sectionHeader confidence="0.605234" genericHeader="method">
3 Annotation of attribution
</sectionHeader>
<bodyText confidence="0.999942181818182">
Wiebe and her colleagues have pointed out the
importance of ascribing beliefs and assertions ex-
pressed in text to the agent(s) holding or making
them (Riloff and Wiebe, 2003; Wiebe et al., 2004;
Wiebe et al., 2005). They have also gone a consid-
erable way towards specifying how such subjective
material should be annotated (Wiebe, 2002). Since
we take discourse connectives to convey semantic
predicate-argument relations between abstract ob-
jects, one can distinguish a variety of cases depend-
ing on the attribution of the discourse relation or its
</bodyText>
<footnote confidence="0.9957085">
3The annotation guidelines for the PDTB are available at
http://www.cis.upenn.edu/—pdtb.
</footnote>
<page confidence="0.999138">
30
</page>
<bodyText confidence="0.988832954545455">
arguments; that is, whether the relation or arguments
are ascribed to the author of the text or someone
other than the author.
Case 1: The relation and both arguments are at-
tributed to the same source. In (5), the concessive
relation between Arg1 and Arg2, anchored on the
connective even though is attributed to the speaker
Dick Mayer, because he is quoted as having said
it. Even where a connective and its arguments are
not included in a single quotation, the attribution can
still be marked explicitly as shown in (6), where only
Arg2 is quoted directly but both Arg1 and Arg2 can
be attibuted to Mr. Prideaux. Attribution to some
speaker can also be marked in reported speech as
shown in the annotation of so that in (7).
(5) “Now, Philip Morris Kraft General Foods’ parent
company is committed to the coffee business and to
increased advertising for Maxwell House,” says Dick
Mayer, president of the General Foods USA division.
“Even though brand loyalty is rather strong for cof-
fee, we need advertising to maintain and strengthen
it.”
</bodyText>
<listItem confidence="0.928876625">
(6) B.A.T isn’t predicting a postponement because the
units “are quality businesses and we are en-
couraged by the breadth of inquiries,” said Mr.
Prideaux.
(7) Like other large Valley companies, Intel also noted
that it has factories in several parts of the nation,
so that a breakdown at one location shouldn’t leave
customers in a total pinch.
</listItem>
<bodyText confidence="0.99423014">
Wherever there is a clear indication that a relation
is attributed to someone other than the author of the
text, we annotate the relation with the feature value
SA for “speaker attribution” which is the case for
(5), (6), and (7). The arguments in these examples
are given the feature value IN to indicate that they
“inherit” the attribution of the relation. If the rela-
tion and its arguments are attributed to the writer,
they are given the feature values WA and IN respec-
tively.
Relations are attributed to the writer of the text by
default. Such cases include many instances of re-
lations whose attribution is ambiguous between the
writer or some other speaker. In (8), for example,
we cannot tell if the relation anchored on although
is attributed to the spokeswoman or the author of the
text. As a default, we always take it to be attributed
to the writer.
Case 2: One or both arguments have a different at-
tribution value from the relation. While the default
value for the attribution of an argument is the attribu-
tion of its relation, it can differ as in (8). Here, as in-
dicated above, the relation is attributed to the writer
(annotated WA) by default, but Arg2 is attributed to
Delmed (annotated SA, for some speaker other than
the writer, and other than the one establishing the
relation).
(8) The current distribution arrangement ends in March
1990 , although Delmed said it will continue to pro-
vide some supplies of the peritoneal dialysis prod-
ucts to National Medical, the spokeswoman said.
Annotating the corpus with attribution is neces-
sary because in many cases the text containing the
source of attribution is located in a different sen-
tence. Such is the case for (5) where the relation
conveyed by even though, and its arguments are at-
tributed to Dick Mayer.
We are also adding attribution values to the anno-
tation of the implicit connectives. Implicit connec-
tives express relations that are inferred by the reader.
In such cases, the author intends for the reader to
infer a discourse relation. As with explicit connec-
tives, we have found it useful to distinguish implicit
relations intended by the writer of the article from
those intended by some other author or speaker. To
give an example, the implicit relation in (9) is at-
tributed to the writer. However, in (10) both Arg1
and Arg2 have been expressed by the speaker whose
speech is being quoted. In this case, the implicit re-
lation is attributed to the speaker.
</bodyText>
<listItem confidence="0.969940666666667">
(9) Investors in stock funds didn’t panic the week-
end after mid-October’s 190-point market plunge.
IMPLICIT-instead Most of those who left stock
funds simply switched into money market funds.
(10) “People say they swim, and that may mean they’ve
been to the beach this year,” Fitness and Sports. “It’s
hard to know if people are responding truthfully.
IMPLICIT-because People are too embarrassed to
say they haven’t done anything.”
</listItem>
<bodyText confidence="0.999324375">
The annotation of attribution is currently under-
way. The final version of the PDTB will include an-
notations of attribution for all the annotated connec-
tives and their arguments.
Note that in the Rhetorical Structure Theory
(RST) annotation scheme (Carlson et al., 2003), at-
tribution is treated as a discourse relation. We, on
the other hand, do not treat attribution as a discourse
</bodyText>
<page confidence="0.999443">
31
</page>
<bodyText confidence="0.999863125">
relation. In PDTB, discourse relations (associated
with an explicit or implicit connective) hold between
two abstracts objects, such as events, states, etc. At-
tribution relates a proposition to an entity, not to an-
other proposition, event, etc. This is an important
difference between the two frameworks. One conse-
quence of this difference is briefly discussed in Foot-
note 4 in the next section.
</bodyText>
<sectionHeader confidence="0.8039445" genericHeader="method">
4 Arguments of Subordinating
Conjunctions in the PTB
</sectionHeader>
<bodyText confidence="0.97185853125">
A natural question that arises with the annotation
of arguments of subordinating conjunctions (SUB-
CONJS) in the PDTB is to what extent they can be
detected directly from the syntactic annotation in the
PTB. In the simplest case, Arg2 of a SUBCONJ is its
complement in the syntactic representation. This is
indeed the case for (11), where since is analyzed as
a preposition in the PTB taking an S complement
which is Arg2 in the PDTB, as shown in Figure 1.
(11) Since the budget measures cash flow, a new $1 di-
rect loan is treated as a $1 expenditure.
Furthermore, in (11), since together with its com-
plement (Arg2) is analyzed as an SBAR which mod-
ifies the clause a new $1 direct loan is treated as a
$1 expenditure, and this clause is Arg1 in the PDTB.
Can the arguments always be detected in this
way? In this section, we present statistics showing
that this is not the case and an analysis that shows
that this lack of congruence between the PDTB and
the PTB is not just a matter of annotator disagree-
ment.
Consider example (12), where the PTB requires
annotators to include the verb of attribution said
and its subject Delmed in the complement of al-
though. But although as a discourse connective de-
nies the expectation that the supply of dialysis prod-
ucts will be discontinued when the distribution ar-
rangement ends. It does not convey the expectation
that Delmed will not say such things. On the other
hand, in (13), the contrast established by while is be-
tween the opinions of two entities i.e., advocates and
their opponents.4
</bodyText>
<footnote confidence="0.92697625">
4This distinction is hard to capture in an RST-based pars-
ing framework (Marcu, 2000). According to the RST-based an-
notation scheme (Carlson et al., 2003) ‘although Delmed said’
and ‘while opponents argued’ are elementary discourse units
</footnote>
<listItem confidence="0.950177222222222">
(12) The current distribution arrangement ends in March
1990, although Delmed said it will continue to pro-
vide some supplies of the peritoneal dialysis prod-
ucts to National Medical, the spokeswoman said.
(13) Advocates said the 90-cent-an-hour rise, to $4.25 an
hour by April 1991, is too smallfor the working poor,
while opponents argued that the increase will still
hurt small business and cost many thousands of
jobs.
</listItem>
<bodyText confidence="0.999771820512821">
In Section 5, we will identify additional cases. What
we will then argue is that it will be insufficient to
train an algorithm for identifying discourse argu-
ments simply on the basis of syntactically analysed
text.
We now present preliminary measurements of
these and other mismatches between the two corpora
for SUBCONJS. To do this we describe a procedural
algorithm which builds on the idea presented at the
start of this section. The statistics are preliminary in
that only the annotations of a single annotator were
considered, and we have not attempted to exclude
cases in which annotators disagree.
We consider only those SUBCONJS for which both
arguments are located in the same sentence as the
connective (which is the case for approximately 99%
of the annotated instances). The syntactic configura-
tion of such relations pattern in a way shown in Fig-
ure 1. Note that it is not necessary for any of Conn,
Arg1, or Arg2 to have a single node in the parse tree
that dominates it exactly. In Figure 1 we do obtain a
single node for Conn, and Arg2 but for Arg1, it is
the set of nodes {NP, V P} that dominate it exactly.
Connectives like so that, and even if are not domi-
nated by a single node, and cases where the annota-
tor has decided that a (parenthetical) clausal element
is not minimally necessary to the interpretation of
Arg2 will necessitate choosing multiple nodes that
dominate Arg2 exactly.
Given the node(s) in the parse tree that dominate
Conn ({IN} in Figure 1), the algorithm we present
tries to find node(s) in the parse tree that dominate
Arg1 and Arg2 exactly using the operation of tree
subtraction (Sections 4.1, and 4.2). We then discuss
its execution on (11) in Section 4.3.
annotated in the same way: as satellites of the relation Attribu-
tion. RST does not recognize that satellite segments, such as
the ones given above, sometimes participate in a higher RST
relation along with their nuclei and sometimes not.
</bodyText>
<page confidence="0.998719">
32
</page>
<figureCaption confidence="0.999949">
Figure 1: The syntactic configuration for (11), and the execution of the tree subtraction algorithm on this configuration.
</figureCaption>
<bodyText confidence="0.772013">
Given NConn = {IN}, our goal is to find NArg1 =
{NP, V P}, and NArg2 = {S2}. Steps:
</bodyText>
<figure confidence="0.96495648">
• hConn = IN
• xConn+Arg2 = SBAR - parent(hConn)
• xConn+Arg1+Arg2
= S12
- lowest Ancestorparent(xConn+A,,,1) with la-
bel S or SBAR. Note that x E Ancestorx
• NArg2
= xConn+Arg2 — NConn
= SBAR — {IN}
= {S2}
• NArg1
= xConn+Arg1+Arg2 — {xConn+Arg2}
= S12 — {SBAR}
= {NP, V P}
is treated as a
$1 expenditure
A new $1 direct
loan
S12
SBAR NP
VP
IN S2
the budget mea-
sures cash flow
since
</figure>
<subsectionHeader confidence="0.91721">
4.1 Tree subtraction
</subsectionHeader>
<bodyText confidence="0.865801222222222">
We will now define the operation of tree subtraction
the graphical intuition for which is given in Figure
2. Let T be the set of nodes in the tree.
Definition 4.1. The ancestors of any node t 2 T,
denoted by Ancestort C_ T is a set of nodes such
that t 2 Ancestort and parent(u, t) ) ([u 2
Ancestort] ^ [Ancestoru C Ancestort])
Definition 4.2. Consider a node x 2 T, and a set
of nodes Y C T — fxg, we define the set Z&apos; =
fnjn 2 T— fxg ^ x 2 Ancestorn ^ (8y 2 Y, y 26
Ancestorn ^ n 26 Ancestory)g. Given such an x
and Y, the operation of tree subtraction gives a set
of nodes Z such that, Z = fz1jz1 2 Z&apos; ^ (8z2 2
Z&apos;, z2 26 (Ancestorz1 — fz1g))g
We denote this by x — Y = Z.
The nodes z 2 Z are the highest descendants of
x, which do not dominate any node y 2 Y and are
not dominated by any node in Y .
</bodyText>
<subsectionHeader confidence="0.995466">
4.2 Algorithm to detect the arguments
</subsectionHeader>
<bodyText confidence="0.944244">
For any t 2 T, let Lt denote the set of leaves(or
terminals) dominated by t and for A C_ T we denote
the set of leaves dominated by A as LA = U La.
daEA
</bodyText>
<figureCaption confidence="0.995896">
Figure 2: Tree subtraction x — Y = Z
</figureCaption>
<bodyText confidence="0.9973065">
For any set of leaves L we define N&apos;L to be a set
of nodes of maximum cardinality such that LN, =
</bodyText>
<equation confidence="0.813206">
U Ln = L
dnEN,L
</equation>
<bodyText confidence="0.950715833333333">
The set NL = fn1jn1 2 N&apos;L ^ (8n2 2 N&apos;L, n2 26
(Ancestorn1 — fn1g))g. We can think of Conn,
Arg1 and Arg2 each as a set of leaves and we use
NConn, NArg1 and NArg2 to denote the set of high-
est nodes which dominate them respectively.
Given NConn, our task is then to find NArg1 and
</bodyText>
<equation confidence="0.999598">
X — {y1, y2} = {z1, z2}
y1 z2
y2 z1
X
</equation>
<page confidence="0.993257">
33
</page>
<bodyText confidence="0.585672">
NArg2. The algorithm does the following:
</bodyText>
<listItem confidence="0.9950032">
1. Let hConn(the head) be the last node in NConn in an in-
order traversal of the tree.
2. xConn+Arg2 = parent(hConn)
3. Repeat while parent(xConn+Arg2) has label S or SBAR,
and has only two children:
</listItem>
<equation confidence="0.985538">
xConn+Arg2 = parent(xConn+Arg2)
</equation>
<bodyText confidence="0.99972675">
This ensures the inclusion of complementizers and subor-
dinating conjuctions associated with the clause in Arg1.
The convention adopted by the PDTB was to include such
elements in the clause with which they were associated.
</bodyText>
<equation confidence="0.437053">
4. xConn+Arg1+Arg2 is the lowest node with label S or
SBAR such that:
xConn+Arg1+Arg2 E Ancestorparent(xConn+ATg2)
</equation>
<listItem confidence="0.7108435">
5. Repeat while parent(xConn+Arg1+Arg2) has label S or
SBAR, and has only two children:
</listItem>
<equation confidence="0.97426725">
xConn+Arg1+Arg2 = parent(xConn+Arg1+Arg2)
6. NArg2 = xConn+Arg2 — NConn (tree subtraction)
7. NArg1 = xConn+Arg1+Arg2—{xConn+Arg2} (tree sub-
traction)
</equation>
<subsectionHeader confidence="0.998523">
4.3 Executing the algorithm on (11)
</subsectionHeader>
<bodyText confidence="0.98249435">
The idea behind the algorithm is as follows. Since
we may not be able to find a single node that domi-
nates Conn, Arg1, and/or Arg2 exactly, we attempt
to find a node that dominates Conn and Arg2 to-
gether denoted by xConn+Arg2 (SBAR in Figure 1),
and a node that dominates Conn, Arg1 and Arg2
together denoted by xConn+Arg1+Arg2 (S12 in Fig-
ure 1). Note that this is an approximation, and there
may be no single node that dominates Conn, and
Arg2 exactly.
Given xConn+Arg2 the idea is to remove all the
material corresponding to Conn (NConn) under that
node and call the rest of the material Arg2. This is
what the operation of tree subtraction gives us, i.e.,
xConn+Arg2 — NConn which is {S2} in Figure 1.
Similarly, given xConn+Arg1+Arg2 we would like
to remove the material corresponding to Conn
and Arg2 and {xConn+Arg2} is that material.
xConn+Arg1+Arg2 — {xConn+Arg2} gives us the
nodes {NP, V P} which is the desired Arg1.
</bodyText>
<sectionHeader confidence="0.933475" genericHeader="method">
5 Evaluation of the tree subtraction
algorithm
</sectionHeader>
<bodyText confidence="0.999474666666667">
Describing the mismatches between the syntactic
and discourse levels of annotation requires a detailed
analysis of the cases where the tree subtraction al-
gorithm does not detect the same arguments as an-
notated by the PDTB. Hence this first set of exper-
iments was carried out only on Sections 00-01 of
the WSJ corpus (about 3500 sentences), which is ac-
cepted by the community to be development data.
First, the tree subtraction algorithm was run on
the PTB annotations in these two sections. The ar-
guments detected by the algorithm were classified
as: (a) Exact, if the argument detected by the al-
gorithm exactly matches the annotation; (b) Extra
Material, if the argument detected contains some
additional material in comparison with the annota-
tion; and (c) Omitted Material, if some annotated
material was not included in the argument detected.
The results are summarized in Table 1.
</bodyText>
<table confidence="0.9984458">
Argument Exact Extra Material Omitted Material
Arg1 82.5% 12.6% 4.9%
(353) (54) (21)
Arg2 93.7% 2.6% 3.7%
(401) (11) (16)
</table>
<tableCaption confidence="0.989898">
Table 1: Tree subtraction on the PTB annotations for SUB-
CONJS. Section 00-01(428 instances)
</tableCaption>
<subsectionHeader confidence="0.964538">
5.1 Analysis of the results in Table 1
5.1.1 Extra Material
</subsectionHeader>
<bodyText confidence="0.9992841">
There were 54 (11) cases where Arg1 (Arg2) in
the PTB (obtained via tree subtraction) contained
more material than the corresponding annotation in
the PDTB. We describe only the cases for Arg1,
since they were a superset of the cases for Arg2.
Second VP-coordinate - In these cases, Arg1 of
the SUBCONJ was associated with the second of two
coordinated VPs. Example (14) is the relation an-
notated by the PDTB, while (15) is the relation pro-
duced by tree subtraction.
</bodyText>
<listItem confidence="0.974537666666667">
(14) She became an abortionist accidentally, and continued
because it enabled her to buy jam, cocoa and other
war-rationed goodies.
(15) She became an abortionist accidentally, and contin-
ued because it enabled her to buy jam, cocoa and
other war-rationed goodies.
</listItem>
<footnote confidence="0.41171">
Such mismatches can be either due to the fact
that the algorithm looks only for nodes of type S
or SBAR, or due to disagreement between the PTB
and PDTB. Further investigation is needed to under-
</footnote>
<page confidence="0.998037">
34
</page>
<bodyText confidence="0.997405081081081">
stand this issue more precisely.5 The percentage of
such mismatches (with respect to the total number
of cases of extra material) is recorded in the first col-
umn of Table 2, along with the number of instances
in parentheses.
Lower Verb - These are cases of a true mismatch
between the PDTB and the PTB, where the PDTB
has associated Arg1 with a lower clause than the
PTB. 9 of the 13 “lower verb” cases for Arg1 were
due to verbs of attribution, as in (12). (The percent-
age of “lower verb” mismatches is given in the sec-
ond column of Table 2, along with the number of
instances in parentheses.)
Clausal Adjuncts - Finally, we considered cases
where clause(s) judged not to be minimally neces-
sary to the interpretation of Arg1 were included.
(16) shows the relation annotated by the PDTB,
where the subordinate clause headed by partly be-
cause is not part of Arg1, but the tree subtraction
algorithm includes it as shown in (17).
(16) When Ms. Evans took her job, several important
divisions that had reported to herpredecessor weren’t
included partly because she didn’t wish to be a full
administrator.
(17) When Ms. Evans took her job, several important
divisions that had reported to herpredecessor weren’t
included partly because she didn’t wish to be a full
administrator.
To get an idea of the number of cases where a
single irrelevant clause was included, we determined
the number of instances for which pruning out one
node from Arg1 resulted in an exact match. This is
given in the third column of Table 2. The second
row of Table 2 illustrates the same information for
Arg2. Most of these are instances where irrelevant
clauses were included in the argument detected from
the PTB.
</bodyText>
<table confidence="0.997169166666667">
Argument Second VP Lower One Node Other
Coordinate Verb Pruned
Arg1 16.7% 24.1% 31.5% 27.7%
(9) (13) (17) (15)
Arg2 0% 9.1% 72.7% 18.2%
(0) (1) (8) (2)
</table>
<tableCaption confidence="0.8835965">
Table 2: Cases which result in extra material being included
in the arguments.
</tableCaption>
<footnote confidence="0.929562">
5It is also possible for the PDTB to associate an argument
with only the first of two coordinated VPs, but the number of
such cases were insignificant.
</footnote>
<subsubsectionHeader confidence="0.794485">
5.1.2 Omitted Material
</subsubsectionHeader>
<bodyText confidence="0.963149130434783">
The main source of these errors in Arg1 are the
higher verb cases. Here the PDTB has associated
Arg1 with a higher clause than the PTB. Examples
(18) and (19) show the annotated and algorithmi-
cally produced relations respectively. This is the in-
verse of the aforementioned lower verb cases, and
the majority of these cases are due to the verb of at-
tribution being a part of the relation.
(18) Longer maturities are thought to indicate declining
interest rates because they permit portfolio man-
agers to retain relatively higher rates for a longer
period.
(19) Longer maturities are thought to indicate declining in-
terest rates because they permit portfolio managers
to retain relatively higher rates for a longer period.
To get an approximate idea of these errors, we
checked if selecting a higher S or SBAR made the
Arg1 exact or include extra material. These are the
columns Two up exact and Two up extra included
in Table 3. At this time, we lack a precise under-
standing of the remaining mismatches in Arg1, and
the ones resulting in material being omitted from
Arg2.
</bodyText>
<table confidence="0.953022666666667">
Argument Two up exact Two up extra Other
included
Arg1 47.6% (10) 14.3% (3) 28.1% (8)
</table>
<tableCaption confidence="0.9411375">
Table 3: Cases which result in material being omitted from
Arg1 as a result of excluding a higher verb
</tableCaption>
<subsectionHeader confidence="0.996512">
5.2 Additional experiments
</subsectionHeader>
<bodyText confidence="0.9974575">
We also evaluated the performance of the tree sub-
traction procedure on the PTB annotations on Sec-
tions 02-24 of the WSJ corpus, and the results are
summarized in Table 4.
</bodyText>
<table confidence="0.99591">
Argument Exact Extra Material Omitted Material
Arg1 76.1% 17.6% 6.3%
Arg2 92.5% 3.6% 3.9%
</table>
<tableCaption confidence="0.988388">
Table 4: Tree subtraction on PTB annotations for the SUB-
CONJS(approx. 5K instances). Sections 02-24
</tableCaption>
<bodyText confidence="0.999891666666667">
Finally we evaluated the algorithm on the output
of a statistical parser. The parser implementation in
(Bikel, 2002) was used in this experiment and it was
run in a mode which emulated the Collins (1997)
parser. The parser was trained on Sections 02-21
and Sections 22-24 were used as test data, where
</bodyText>
<page confidence="0.997738">
35
</page>
<tableCaption confidence="0.480536333333333">
the parser was run and the tree subtraction algorithm
was run on its output. The results are summarized in
Table 5.
</tableCaption>
<table confidence="0.931138166666667">
ter Building a Discourse-Tagged Corpus in the framework
of Rhetorical Structure Theory, pages 85–112. Kluwer Aca-
demic Publishers.
Argument Exact Extra Material Omitted Material
Arg1 65.5% 25.2% 9.3%
Arg2 84.7% 0% 15.3%
</table>
<tableCaption confidence="0.9359275">
Table 5: Tree subtraction on the output of a statistical parser
(approx. 600 instances). Sections 22-24.
</tableCaption>
<sectionHeader confidence="0.999436" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999759642857143">
While it is clear that discourse annotation goes be-
yond syntactic annotation, one might have thought
that at least for the annotation of arguments of subor-
dinating conjunctions, these two levels of annotation
would converge. However, we have shown that this
is not always the case. We have also described an
algorithm for discovering such divergences, which
can serve as a useful baseline for future efforts to de-
tect the arguments with greater accuracy. The statis-
tics presented suggest that the annotation of the dis-
course arguments of the subordinating conjunctions
needs to proceed separately from syntactic annota-
tion – certainly when annotating other English cor-
pora and very possibly for other languages as well.
A major source of the mismatches between syn-
tax and discourse is the effect of attribution, either
that of the arguments or of the relation denoted by
the connective. We believe that the annotation of at-
tribution in the PDTB will prove to be a useful aid
to applications that need to detect the relations con-
veyed by discourse connectives with a high degree
of reliability, as well as in constraining the infer-
ences that may be drawn with respect to the writer’s
commitment to the relation or the arguments. The
results in this paper also raise the more general ques-
tion of whether there may be other mismatches be-
tween the syntactic and discourse annotations at the
sentence level.
</bodyText>
<sectionHeader confidence="0.999646" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999158444444444">
Nicholas Asher. 1993. Reference to Abstract Objects in Dis-
course. Kluwer Academic Press.
Daniel Bikel. 2002. Design of a Multi-lingual, Parallel-
processing Statistical Parsing Engine. In HLT.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski,
2003. Current Directions in Discourse and Dialogue, chap-
Michael Collins. 1997. Three Generative, Lexicalized Models
for Statistical Parsing. In 35th Annual Meeting of the ACL.
Katherine Forbes. 2003. Discourse Semantics of S-Modifying
Adverbials. Ph.D. thesis, Department of Linguistics, Uni-
versity of Pennsylvania.
Daniel Marcu. 2000. The Rhetorical Parsing of Unrestricted
Texts: A Surface-Based Approach. Computational Linguis-
tics, 26(3):395–448.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large scale anno-
tated corpus of english: the Penn Treebank. Computational
Linguistics, 19.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and Bonnie
Webber. 2004a. Annotating Discourse Connectives and
their Arguments. In the HLT/NAACL workshop on Frontiers
in Corpus Annotation, Boston, MA.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and Bonnie
Webber. 2004b. The Penn Discourse Treebank. In the Lan-
guage Resources and Evaluation Conference, Lisbon, Portu-
gal.
Rashmi Prasad, Eleni Miltsakaki, Aravind Joshi, and Bonnie
Webber. 2004. Annotation and Data Mining of the Penn
Discourse TreeBank. In ACL Workshop on Discourse Anno-
tation, Barcelona, Spain.
Ellen Riloff and Janyce Wiebe. 2003. Learning Extraction Pat-
terns for Subjective Expressions. In Proceedings of the SIG-
DAT Conference on Empirical Methods in Natural Language
Processing (EMNLP ’03), pages 105–112, Sapporo, Japan.
Bonnie Webber and Aravind Joshi. 1998. Anchoring a
Lexicalized Tree-Adjoining Grammar for Discourse. In
ACL/COLING Workshop on Discourse Relations and Dis-
course Markers, Montreal, Canada, August.
Bonnie Webber, Alistair Knott, Matthew Stone, and Aravind
Joshi. 1999. Discourse Relations: A Structural and Presup-
positional Account using Lexicalized TAG. In ACL, College
Park, MD, June.
Bonnie Webber, Aravind Joshi, Matthew Stone, and Alistair
Knott. 2003. Anaphora and Discourse Structure. Computa-
tional Linguistics, 29(4):545–87.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell,
and Melanie Martin. 2004. Learning subjective language.
Computational Linguistics, 30(3):277–308.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. An-
notating expressions of opinions and emotions in language.
Language Resources and Evaluation, 1(2).
Janyce Wiebe. 2002. Instructions for annotating opinions in
newspaper articles. Technical Report TR-02-101, Depart-
ment of Computer Science, University of Pittsburgh.
</reference>
<page confidence="0.998958">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.545678">
<title confidence="0.997858">Attribution and the (Non-)Alignment of Syntactic and Discourse Arguments of Connectives</title>
<author confidence="0.997319">Dinesh Lee Miltsakaki Prasad</author>
<affiliation confidence="0.999707">University of</affiliation>
<address confidence="0.992193">Philadelphia, PA 19104</address>
<author confidence="0.741582">Bonnie</author>
<affiliation confidence="0.997347">University of</affiliation>
<address confidence="0.942734">Edinburgh, EH8 9LW</address>
<email confidence="0.997567">bonnie@inf.ed.ac.uk</email>
<abstract confidence="0.988617263157895">The annotations of the Penn Discourse Treebank (PDTB) include (1) discourse connectives and their arguments, and (2) each argument of each connective and of the relation it denotes. Because the PDTB covers the same text as the Penn TreeBank WSJ corpus, syntactic and discourse annotation can be compared. This has revealed significant differences between syntactic structure and discourse structure, in terms of the arguments of connectives, due in large part to attribution. We describe these differences, an algorithm for detecting them, and finally some experimental results. These results have implications for automating discourse annotation based on syntactic annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
</authors>
<title>Reference to Abstract Objects in Discourse.</title>
<date>1993</date>
<publisher>Kluwer Academic Press.</publisher>
<contexts>
<context position="4334" citStr="Asher, 1993" startWordPosition="685" endWordPosition="686">n framework is its bottom-up approach to discourse structure: Instead of appealing to an abstract (and arbitrary) set of discourse relations whose identification may confound multiple sources of discourse meaning, we start with the annotation of discourse connectives and their arguments, thus exposing a clearly defined level of discourse representation. The PDTB annotates as explicit discourse connectives all subordinating conjunctions, coordinating conjunctions and discourse adverbials. These predicates establish relations between two abstract objects such as events, states and propositions (Asher, 1993).1 We use Conn to denote the connective, and Arg1 and Arg2 to denote the textual spans from which the abstract object arguments are computed.2 In (1), the subordinating conjunction since establishes a temporal relation between the event of the earthquake hitting and a state where no music is played by a certain woman. In all the examples in this paper, as in (1), Arg1 is italicized, Arg2 is in boldface, and Conn is underlined. (1) She hasn’t played any music since the earthquake hit. What counts as a legal argument? Since we take discourse relations to hold between abstract objects, we require</context>
</contexts>
<marker>Asher, 1993</marker>
<rawString>Nicholas Asher. 1993. Reference to Abstract Objects in Discourse. Kluwer Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Bikel</author>
</authors>
<title>Design of a Multi-lingual, Parallelprocessing Statistical Parsing Engine.</title>
<date>2002</date>
<booktitle>In HLT.</booktitle>
<contexts>
<context position="27383" citStr="Bikel, 2002" startWordPosition="4686" endWordPosition="4687"> (8) Table 3: Cases which result in material being omitted from Arg1 as a result of excluding a higher verb 5.2 Additional experiments We also evaluated the performance of the tree subtraction procedure on the PTB annotations on Sections 02-24 of the WSJ corpus, and the results are summarized in Table 4. Argument Exact Extra Material Omitted Material Arg1 76.1% 17.6% 6.3% Arg2 92.5% 3.6% 3.9% Table 4: Tree subtraction on PTB annotations for the SUBCONJS(approx. 5K instances). Sections 02-24 Finally we evaluated the algorithm on the output of a statistical parser. The parser implementation in (Bikel, 2002) was used in this experiment and it was run in a mode which emulated the Collins (1997) parser. The parser was trained on Sections 02-21 and Sections 22-24 were used as test data, where 35 the parser was run and the tree subtraction algorithm was run on its output. The results are summarized in Table 5. ter Building a Discourse-Tagged Corpus in the framework of Rhetorical Structure Theory, pages 85–112. Kluwer Academic Publishers. Argument Exact Extra Material Omitted Material Arg1 65.5% 25.2% 9.3% Arg2 84.7% 0% 15.3% Table 5: Tree subtraction on the output of a statistical parser (approx. 600</context>
</contexts>
<marker>Bikel, 2002</marker>
<rawString>Daniel Bikel. 2002. Design of a Multi-lingual, Parallelprocessing Statistical Parsing Engine. In HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary Ellen Okurowski</author>
</authors>
<title>Three Generative, Lexicalized Models for Statistical Parsing.</title>
<date>2003</date>
<booktitle>Current Directions in Discourse and Dialogue, chapMichael Collins.</booktitle>
<contexts>
<context position="12803" citStr="Carlson et al., 2003" startWordPosition="2107" endWordPosition="2110">int market plunge. IMPLICIT-instead Most of those who left stock funds simply switched into money market funds. (10) “People say they swim, and that may mean they’ve been to the beach this year,” Fitness and Sports. “It’s hard to know if people are responding truthfully. IMPLICIT-because People are too embarrassed to say they haven’t done anything.” The annotation of attribution is currently underway. The final version of the PDTB will include annotations of attribution for all the annotated connectives and their arguments. Note that in the Rhetorical Structure Theory (RST) annotation scheme (Carlson et al., 2003), attribution is treated as a discourse relation. We, on the other hand, do not treat attribution as a discourse 31 relation. In PDTB, discourse relations (associated with an explicit or implicit connective) hold between two abstracts objects, such as events, states, etc. Attribution relates a proposition to an entity, not to another proposition, event, etc. This is an important difference between the two frameworks. One consequence of this difference is briefly discussed in Footnote 4 in the next section. 4 Arguments of Subordinating Conjunctions in the PTB A natural question that arises with</context>
<context position="15055" citStr="Carlson et al., 2003" startWordPosition="2498" endWordPosition="2501"> to include the verb of attribution said and its subject Delmed in the complement of although. But although as a discourse connective denies the expectation that the supply of dialysis products will be discontinued when the distribution arrangement ends. It does not convey the expectation that Delmed will not say such things. On the other hand, in (13), the contrast established by while is between the opinions of two entities i.e., advocates and their opponents.4 4This distinction is hard to capture in an RST-based parsing framework (Marcu, 2000). According to the RST-based annotation scheme (Carlson et al., 2003) ‘although Delmed said’ and ‘while opponents argued’ are elementary discourse units (12) The current distribution arrangement ends in March 1990, although Delmed said it will continue to provide some supplies of the peritoneal dialysis products to National Medical, the spokeswoman said. (13) Advocates said the 90-cent-an-hour rise, to $4.25 an hour by April 1991, is too smallfor the working poor, while opponents argued that the increase will still hurt small business and cost many thousands of jobs. In Section 5, we will identify additional cases. What we will then argue is that it will be ins</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2003</marker>
<rawString>Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski, 2003. Current Directions in Discourse and Dialogue, chapMichael Collins. 1997. Three Generative, Lexicalized Models for Statistical Parsing. In 35th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katherine Forbes</author>
</authors>
<title>Discourse Semantics of S-Modifying Adverbials.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Linguistics, University of Pennsylvania.</institution>
<contexts>
<context position="5399" citStr="Forbes, 2003" startWordPosition="863" endWordPosition="864"> any music since the earthquake hit. What counts as a legal argument? Since we take discourse relations to hold between abstract objects, we require that an argument contains at least one clause-level predication (usually a verb – tensed or untensed), though it may span as much as a sequence of clauses or sentences. The two exceptions are nominal phrases that express an event or a state, and discourse deictics that denote an abstract object. 1For example, discourse adverbials like as a result are distinguished from clausal adverbials like strangely which require only a single abstract object (Forbes, 2003). 2Each connective has exactly two arguments. The argument that appears in the clause syntactically associated with the connective, we call Arg2. The other argument is called Arg1. Both Arg1 and Arg2 can be in the same sentence, as is the case for subordinating conjunctions (e.g., because). The linear order of the arguments will be Arg2 Arg1 if the subordinate clause appears sentence initially; Arg1 Arg2 if the subordinate clause appears sentence finally; and undefined if it appears sentence medially. For an adverbial connective like however, Arg1 is in the prior discourse. Hence, the linear o</context>
</contexts>
<marker>Forbes, 2003</marker>
<rawString>Katherine Forbes. 2003. Discourse Semantics of S-Modifying Adverbials. Ph.D. thesis, Department of Linguistics, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Rhetorical Parsing of Unrestricted Texts: A Surface-Based Approach.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="14986" citStr="Marcu, 2000" startWordPosition="2489" endWordPosition="2490">nt. Consider example (12), where the PTB requires annotators to include the verb of attribution said and its subject Delmed in the complement of although. But although as a discourse connective denies the expectation that the supply of dialysis products will be discontinued when the distribution arrangement ends. It does not convey the expectation that Delmed will not say such things. On the other hand, in (13), the contrast established by while is between the opinions of two entities i.e., advocates and their opponents.4 4This distinction is hard to capture in an RST-based parsing framework (Marcu, 2000). According to the RST-based annotation scheme (Carlson et al., 2003) ‘although Delmed said’ and ‘while opponents argued’ are elementary discourse units (12) The current distribution arrangement ends in March 1990, although Delmed said it will continue to provide some supplies of the peritoneal dialysis products to National Medical, the spokeswoman said. (13) Advocates said the 90-cent-an-hour rise, to $4.25 an hour by April 1991, is too smallfor the working poor, while opponents argued that the increase will still hurt small business and cost many thousands of jobs. In Section 5, we will iden</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The Rhetorical Parsing of Unrestricted Texts: A Surface-Based Approach. Computational Linguistics, 26(3):395–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large scale annotated corpus of english: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="1228" citStr="Marcus et al., 1993" startWordPosition="178" endWordPosition="181">e text as the Penn TreeBank WSJ corpus, syntactic and discourse annotation can be compared. This has revealed significant differences between syntactic structure and discourse structure, in terms of the arguments of connectives, due in large part to attribution. We describe these differences, an algorithm for detecting them, and finally some experimental results. These results have implications for automating discourse annotation based on syntactic annotation. 1 Introduction The overall goal of the Penn Discourse Treebank (PDTB) is to annotate the million word WSJ corpus in the Penn TreeBank (Marcus et al., 1993) with a layer of discourse annotations. A preliminary report on this project was presented at the 2004 workshop on Frontiers in Corpus Annotation (Miltsakaki et al., 2004a), where we described our annotation of discourse connectives (both explicit and implicit) along with their (clausal) arguments. Further work done since then includes the annotation of attribution: that is, who has expressed each argument to a discourse connective (the writer or some other speaker or author) and who has ex29 pressed the discourse relation itself. These ascriptions need not be the same. Of particular interest </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large scale annotated corpus of english: the Penn Treebank. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleni Miltsakaki</author>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>Annotating Discourse Connectives and their Arguments.</title>
<date>2004</date>
<booktitle>In the HLT/NAACL workshop on Frontiers in Corpus Annotation,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="1398" citStr="Miltsakaki et al., 2004" startWordPosition="207" endWordPosition="210">iscourse structure, in terms of the arguments of connectives, due in large part to attribution. We describe these differences, an algorithm for detecting them, and finally some experimental results. These results have implications for automating discourse annotation based on syntactic annotation. 1 Introduction The overall goal of the Penn Discourse Treebank (PDTB) is to annotate the million word WSJ corpus in the Penn TreeBank (Marcus et al., 1993) with a layer of discourse annotations. A preliminary report on this project was presented at the 2004 workshop on Frontiers in Corpus Annotation (Miltsakaki et al., 2004a), where we described our annotation of discourse connectives (both explicit and implicit) along with their (clausal) arguments. Further work done since then includes the annotation of attribution: that is, who has expressed each argument to a discourse connective (the writer or some other speaker or author) and who has ex29 pressed the discourse relation itself. These ascriptions need not be the same. Of particular interest is the fact that attribution may or may not play a role in the relation established by a connective. This may lead to a lack of congruence between arguments at the syntac</context>
<context position="3631" citStr="Miltsakaki et al. (2004" startWordPosition="579" endWordPosition="582">ctice of discourse annotation and their relevance even at the level of sentence-bound annotation. 2 Overview of the PDTB The PDTB builds on the DLTAG approach to discourse structure (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) in which connectives are discourse-level predicates which project predicate-argument structure on a par with verbs at Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics the sentence level. Initial work on the PDTB has been described in Miltsakaki et al. (2004a), Miltsakaki et al. (2004b), Prasad et al. (2004). The key contribution of the PDTB design framework is its bottom-up approach to discourse structure: Instead of appealing to an abstract (and arbitrary) set of discourse relations whose identification may confound multiple sources of discourse meaning, we start with the annotation of discourse connectives and their arguments, thus exposing a clearly defined level of discourse representation. The PDTB annotates as explicit discourse connectives all subordinating conjunctions, coordinating conjunctions and discourse adverbials. These predicates</context>
</contexts>
<marker>Miltsakaki, Prasad, Joshi, Webber, 2004</marker>
<rawString>Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and Bonnie Webber. 2004a. Annotating Discourse Connectives and their Arguments. In the HLT/NAACL workshop on Frontiers in Corpus Annotation, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleni Miltsakaki</author>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse Treebank.</title>
<date>2004</date>
<booktitle>In the Language Resources and Evaluation Conference,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="1398" citStr="Miltsakaki et al., 2004" startWordPosition="207" endWordPosition="210">iscourse structure, in terms of the arguments of connectives, due in large part to attribution. We describe these differences, an algorithm for detecting them, and finally some experimental results. These results have implications for automating discourse annotation based on syntactic annotation. 1 Introduction The overall goal of the Penn Discourse Treebank (PDTB) is to annotate the million word WSJ corpus in the Penn TreeBank (Marcus et al., 1993) with a layer of discourse annotations. A preliminary report on this project was presented at the 2004 workshop on Frontiers in Corpus Annotation (Miltsakaki et al., 2004a), where we described our annotation of discourse connectives (both explicit and implicit) along with their (clausal) arguments. Further work done since then includes the annotation of attribution: that is, who has expressed each argument to a discourse connective (the writer or some other speaker or author) and who has ex29 pressed the discourse relation itself. These ascriptions need not be the same. Of particular interest is the fact that attribution may or may not play a role in the relation established by a connective. This may lead to a lack of congruence between arguments at the syntac</context>
<context position="3631" citStr="Miltsakaki et al. (2004" startWordPosition="579" endWordPosition="582">ctice of discourse annotation and their relevance even at the level of sentence-bound annotation. 2 Overview of the PDTB The PDTB builds on the DLTAG approach to discourse structure (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) in which connectives are discourse-level predicates which project predicate-argument structure on a par with verbs at Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics the sentence level. Initial work on the PDTB has been described in Miltsakaki et al. (2004a), Miltsakaki et al. (2004b), Prasad et al. (2004). The key contribution of the PDTB design framework is its bottom-up approach to discourse structure: Instead of appealing to an abstract (and arbitrary) set of discourse relations whose identification may confound multiple sources of discourse meaning, we start with the annotation of discourse connectives and their arguments, thus exposing a clearly defined level of discourse representation. The PDTB annotates as explicit discourse connectives all subordinating conjunctions, coordinating conjunctions and discourse adverbials. These predicates</context>
</contexts>
<marker>Miltsakaki, Prasad, Joshi, Webber, 2004</marker>
<rawString>Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and Bonnie Webber. 2004b. The Penn Discourse Treebank. In the Language Resources and Evaluation Conference, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Eleni Miltsakaki</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>Annotation and Data Mining of the Penn Discourse TreeBank.</title>
<date>2004</date>
<booktitle>In ACL Workshop on Discourse Annotation,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="3682" citStr="Prasad et al. (2004)" startWordPosition="588" endWordPosition="591"> at the level of sentence-bound annotation. 2 Overview of the PDTB The PDTB builds on the DLTAG approach to discourse structure (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) in which connectives are discourse-level predicates which project predicate-argument structure on a par with verbs at Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics the sentence level. Initial work on the PDTB has been described in Miltsakaki et al. (2004a), Miltsakaki et al. (2004b), Prasad et al. (2004). The key contribution of the PDTB design framework is its bottom-up approach to discourse structure: Instead of appealing to an abstract (and arbitrary) set of discourse relations whose identification may confound multiple sources of discourse meaning, we start with the annotation of discourse connectives and their arguments, thus exposing a clearly defined level of discourse representation. The PDTB annotates as explicit discourse connectives all subordinating conjunctions, coordinating conjunctions and discourse adverbials. These predicates establish relations between two abstract objects s</context>
</contexts>
<marker>Prasad, Miltsakaki, Joshi, Webber, 2004</marker>
<rawString>Rashmi Prasad, Eleni Miltsakaki, Aravind Joshi, and Bonnie Webber. 2004. Annotation and Data Mining of the Penn Discourse TreeBank. In ACL Workshop on Discourse Annotation, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning Extraction Patterns for Subjective Expressions.</title>
<date>2003</date>
<booktitle>In Proceedings of the SIGDAT Conference on Empirical Methods in Natural Language Processing (EMNLP ’03),</booktitle>
<pages>105--112</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="7811" citStr="Riloff and Wiebe, 2003" startWordPosition="1265" endWordPosition="1268">re looking to raise in the year ending March 21 compares with only $2.7 billion raise on the capital market in the previous year. IMPLICIT - in contrast In fiscal 1984, before Mr. Gandhi came into power, only $810 million was raised. When complete, the PDTB will contain approximately 35K annotations: 15K annotations of the 100 explicit connectives identified in the corpus and 20K annotations of implicit connectives.3 3 Annotation of attribution Wiebe and her colleagues have pointed out the importance of ascribing beliefs and assertions expressed in text to the agent(s) holding or making them (Riloff and Wiebe, 2003; Wiebe et al., 2004; Wiebe et al., 2005). They have also gone a considerable way towards specifying how such subjective material should be annotated (Wiebe, 2002). Since we take discourse connectives to convey semantic predicate-argument relations between abstract objects, one can distinguish a variety of cases depending on the attribution of the discourse relation or its 3The annotation guidelines for the PDTB are available at http://www.cis.upenn.edu/—pdtb. 30 arguments; that is, whether the relation or arguments are ascribed to the author of the text or someone other than the author. Case </context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Ellen Riloff and Janyce Wiebe. 2003. Learning Extraction Patterns for Subjective Expressions. In Proceedings of the SIGDAT Conference on Empirical Methods in Natural Language Processing (EMNLP ’03), pages 105–112, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
<author>Aravind Joshi</author>
</authors>
<title>Anchoring a Lexicalized Tree-Adjoining Grammar for Discourse.</title>
<date>1998</date>
<booktitle>In ACL/COLING Workshop on Discourse Relations and Discourse Markers,</booktitle>
<location>Montreal, Canada,</location>
<contexts>
<context position="3213" citStr="Webber and Joshi, 1998" startWordPosition="513" endWordPosition="516">the attribution of the arguments of a connective and the relation it conveys. In Sections 4 and 5, we describe mismatches that arise between the discourse arguments of a connective and the syntactic annotation as provided by the Penn TreeBank (PTB), in the cases where all the arguments of the connective are in the same sentence. In Section 6, we will discuss some implications of these issues for the theory and practice of discourse annotation and their relevance even at the level of sentence-bound annotation. 2 Overview of the PDTB The PDTB builds on the DLTAG approach to discourse structure (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) in which connectives are discourse-level predicates which project predicate-argument structure on a par with verbs at Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics the sentence level. Initial work on the PDTB has been described in Miltsakaki et al. (2004a), Miltsakaki et al. (2004b), Prasad et al. (2004). The key contribution of the PDTB design framework is its bottom-up approach to discourse structure: Instead of appealing to an ab</context>
</contexts>
<marker>Webber, Joshi, 1998</marker>
<rawString>Bonnie Webber and Aravind Joshi. 1998. Anchoring a Lexicalized Tree-Adjoining Grammar for Discourse. In ACL/COLING Workshop on Discourse Relations and Discourse Markers, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
<author>Alistair Knott</author>
<author>Matthew Stone</author>
<author>Aravind Joshi</author>
</authors>
<title>Discourse Relations: A Structural and Presuppositional Account using Lexicalized TAG.</title>
<date>1999</date>
<booktitle>In ACL,</booktitle>
<location>College Park, MD,</location>
<contexts>
<context position="3234" citStr="Webber et al., 1999" startWordPosition="517" endWordPosition="520">rguments of a connective and the relation it conveys. In Sections 4 and 5, we describe mismatches that arise between the discourse arguments of a connective and the syntactic annotation as provided by the Penn TreeBank (PTB), in the cases where all the arguments of the connective are in the same sentence. In Section 6, we will discuss some implications of these issues for the theory and practice of discourse annotation and their relevance even at the level of sentence-bound annotation. 2 Overview of the PDTB The PDTB builds on the DLTAG approach to discourse structure (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) in which connectives are discourse-level predicates which project predicate-argument structure on a par with verbs at Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics the sentence level. Initial work on the PDTB has been described in Miltsakaki et al. (2004a), Miltsakaki et al. (2004b), Prasad et al. (2004). The key contribution of the PDTB design framework is its bottom-up approach to discourse structure: Instead of appealing to an abstract (and arbitrary</context>
</contexts>
<marker>Webber, Knott, Stone, Joshi, 1999</marker>
<rawString>Bonnie Webber, Alistair Knott, Matthew Stone, and Aravind Joshi. 1999. Discourse Relations: A Structural and Presuppositional Account using Lexicalized TAG. In ACL, College Park, MD, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
<author>Aravind Joshi</author>
<author>Matthew Stone</author>
<author>Alistair Knott</author>
</authors>
<date>2003</date>
<booktitle>Anaphora and Discourse Structure. Computational Linguistics,</booktitle>
<pages>29--4</pages>
<contexts>
<context position="3256" citStr="Webber et al., 2003" startWordPosition="521" endWordPosition="524">ive and the relation it conveys. In Sections 4 and 5, we describe mismatches that arise between the discourse arguments of a connective and the syntactic annotation as provided by the Penn TreeBank (PTB), in the cases where all the arguments of the connective are in the same sentence. In Section 6, we will discuss some implications of these issues for the theory and practice of discourse annotation and their relevance even at the level of sentence-bound annotation. 2 Overview of the PDTB The PDTB builds on the DLTAG approach to discourse structure (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) in which connectives are discourse-level predicates which project predicate-argument structure on a par with verbs at Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics the sentence level. Initial work on the PDTB has been described in Miltsakaki et al. (2004a), Miltsakaki et al. (2004b), Prasad et al. (2004). The key contribution of the PDTB design framework is its bottom-up approach to discourse structure: Instead of appealing to an abstract (and arbitrary) set of discourse rel</context>
</contexts>
<marker>Webber, Joshi, Stone, Knott, 2003</marker>
<rawString>Bonnie Webber, Aravind Joshi, Matthew Stone, and Alistair Knott. 2003. Anaphora and Discourse Structure. Computational Linguistics, 29(4):545–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Rebecca Bruce</author>
<author>Matthew Bell</author>
<author>Melanie Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="7831" citStr="Wiebe et al., 2004" startWordPosition="1269" endWordPosition="1272">he year ending March 21 compares with only $2.7 billion raise on the capital market in the previous year. IMPLICIT - in contrast In fiscal 1984, before Mr. Gandhi came into power, only $810 million was raised. When complete, the PDTB will contain approximately 35K annotations: 15K annotations of the 100 explicit connectives identified in the corpus and 20K annotations of implicit connectives.3 3 Annotation of attribution Wiebe and her colleagues have pointed out the importance of ascribing beliefs and assertions expressed in text to the agent(s) holding or making them (Riloff and Wiebe, 2003; Wiebe et al., 2004; Wiebe et al., 2005). They have also gone a considerable way towards specifying how such subjective material should be annotated (Wiebe, 2002). Since we take discourse connectives to convey semantic predicate-argument relations between abstract objects, one can distinguish a variety of cases depending on the attribution of the discourse relation or its 3The annotation guidelines for the PDTB are available at http://www.cis.upenn.edu/—pdtb. 30 arguments; that is, whether the relation or arguments are ascribed to the author of the text or someone other than the author. Case 1: The relation and </context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin. 2004. Learning subjective language. Computational Linguistics, 30(3):277–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="7852" citStr="Wiebe et al., 2005" startWordPosition="1273" endWordPosition="1276"> 21 compares with only $2.7 billion raise on the capital market in the previous year. IMPLICIT - in contrast In fiscal 1984, before Mr. Gandhi came into power, only $810 million was raised. When complete, the PDTB will contain approximately 35K annotations: 15K annotations of the 100 explicit connectives identified in the corpus and 20K annotations of implicit connectives.3 3 Annotation of attribution Wiebe and her colleagues have pointed out the importance of ascribing beliefs and assertions expressed in text to the agent(s) holding or making them (Riloff and Wiebe, 2003; Wiebe et al., 2004; Wiebe et al., 2005). They have also gone a considerable way towards specifying how such subjective material should be annotated (Wiebe, 2002). Since we take discourse connectives to convey semantic predicate-argument relations between abstract objects, one can distinguish a variety of cases depending on the attribution of the discourse relation or its 3The annotation guidelines for the PDTB are available at http://www.cis.upenn.edu/—pdtb. 30 arguments; that is, whether the relation or arguments are ascribed to the author of the text or someone other than the author. Case 1: The relation and both arguments are at</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 1(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
</authors>
<title>Instructions for annotating opinions in newspaper articles.</title>
<date>2002</date>
<tech>Technical Report TR-02-101,</tech>
<institution>Department of Computer Science, University of Pittsburgh.</institution>
<contexts>
<context position="7974" citStr="Wiebe, 2002" startWordPosition="1294" endWordPosition="1295">re Mr. Gandhi came into power, only $810 million was raised. When complete, the PDTB will contain approximately 35K annotations: 15K annotations of the 100 explicit connectives identified in the corpus and 20K annotations of implicit connectives.3 3 Annotation of attribution Wiebe and her colleagues have pointed out the importance of ascribing beliefs and assertions expressed in text to the agent(s) holding or making them (Riloff and Wiebe, 2003; Wiebe et al., 2004; Wiebe et al., 2005). They have also gone a considerable way towards specifying how such subjective material should be annotated (Wiebe, 2002). Since we take discourse connectives to convey semantic predicate-argument relations between abstract objects, one can distinguish a variety of cases depending on the attribution of the discourse relation or its 3The annotation guidelines for the PDTB are available at http://www.cis.upenn.edu/—pdtb. 30 arguments; that is, whether the relation or arguments are ascribed to the author of the text or someone other than the author. Case 1: The relation and both arguments are attributed to the same source. In (5), the concessive relation between Arg1 and Arg2, anchored on the connective even though</context>
</contexts>
<marker>Wiebe, 2002</marker>
<rawString>Janyce Wiebe. 2002. Instructions for annotating opinions in newspaper articles. Technical Report TR-02-101, Department of Computer Science, University of Pittsburgh.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>