<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012417">
<title confidence="0.999569">
A Latent Topic Extracting Method based on Events in a Document
and its Application
</title>
<author confidence="0.994863">
Risa Kitajima Ichiro Kobayashi
</author>
<affiliation confidence="0.999501">
Ochanomizu University Ochanomizu University
</affiliation>
<email confidence="0.997915">
kitajima.risa@is.ocha.ac.jp koba@is.ocha.ac.jp
</email>
<sectionHeader confidence="0.99859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9977939">
Recently, several latent topic analysis methods
such as LSI, pLSI, and LDA have been widely
used for text analysis. However, those meth-
ods basically assign topics to words, but do not
account for the events in a document. With
this background, in this paper, we propose a
latent topic extracting method which assigns
topics to events. We also show that our pro-
posed method is useful to generate a document
summary based on a latent topic.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999657">
Recently, several latent topic analysis methods such
as Latent Semantic Indexing (LSI) (Deerwester
et al., 1990), Probabilistic LSI (pLSI) (Hofmann,
1999), and Latent Dirichlet Allocation (LDA) (Blei
et al., 2003) have been widely used for text analy-
sis. However, those methods basically assign top-
ics to words, but do not account for the events in a
document. Here, we define a unit of informing the
content of document at the level of sentence as an
“Event” 1, and propose a model that treats a docu-
ment as a set of Events. We use LDA as a latent
topic analysis method, and assign topics to Events
in a document. To examine our proposed method’s
performance on extracting latent topics from a doc-
ument, we compare the accuracy of our method to
that of the conventional methods through a common
document retrieval task. Furthermore, as an appli-
cation of our method, we apply it to a query-biased
document summarization (Tombros and Sanderson,
</bodyText>
<footnote confidence="0.368052">
&apos;For the definition of an Event, see Section 3.
</footnote>
<page confidence="0.984821">
30
</page>
<bodyText confidence="0.976538666666667">
1998; Okumura and Mochizuki, 2000; Berger and
Mittal, 2000) to verify that the method is useful for
various applications.
</bodyText>
<sectionHeader confidence="0.999364" genericHeader="introduction">
2 Related Studies
</sectionHeader>
<bodyText confidence="0.999967310344828">
Suzuki et al. (2010) proposed a flexible latent top-
ics inference in which topics are assigned to phrases
in a document. Matsumoto et al. (2005) showed
that the accuracy of document classification will be
improved by introducing a feature dealing with the
dependency relationships among words.
In case of assigning topics to words, it is likely
that two documents, which have the same word fre-
quency in themselves, tend to be estimated as they
have the same topic probablistic distribution without
considering the dependency relation among words.
However, there are many cases where the relation-
ship among words is regarded as more important
rather than the frequency of words as the feature
identifying the topics of a document. For example,
in case of classifying opinions to objects in a doc-
ument, we have to identify what sort of opinion is
assigned to the target objects, therefore, we have to
focus on the relationship among words in a sentence,
not only on the frequent words appeared in a docu-
ment. For this reason, we propose a method to as-
sign topics to Events instead of words.
As for studies on document summarization, there
are various methods, such as the method based on
word frequency (Luhn, 1958; Nenkova and Van-
derwende, 2005), and the method based on a graph
(Radev, 2004; Wan and Yang, 2006). Moreover,
several methods using a latent topic model have
been proposed (Bing et al., 2005; Arora and Ravin-
</bodyText>
<subsectionHeader confidence="0.374606">
Proceedings of the ACL-HLT 2011 Student Session, pages 30–35,
</subsectionHeader>
<bodyText confidence="0.9742928">
Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics
dran, 2008; Bhandari et al., 2008; Henning, 2009;
Haghighi and Vanderwende, 2009). In those stud-
ies, the methods estimate a topic distribution on each
sentence in the same way as the latent semantic anal-
ysis methods normally do that on each document,
and generate a summary based on the distribution.
We also show that our proposed method is useful for
the document summarization based on extracting la-
tent topics from sentences.
</bodyText>
<sectionHeader confidence="0.985183" genericHeader="method">
3 Topic Extraction based on Events
</sectionHeader>
<bodyText confidence="0.99976325">
In this study, since we deal with a document as a
set of Events, we extract Events from each docu-
ment; define some of the extracted Events as the in-
dex terms for the whole objective documents; and
then make an Event-by-document matrix consisting
of the frequency of Events to the documents. A la-
tent topic distribution is estimated based on this ma-
trix.
</bodyText>
<subsectionHeader confidence="0.999928">
3.1 Definition of an Event
</subsectionHeader>
<bodyText confidence="0.9998573125">
In this study, we define a pair of words in depen-
dent relation which meets the following conditions:
(Subject, Predicate) or (Predicate1, Predicate2) , as
an Event. A noun and unknown words correspond
to Subject, while a verb, adjective and adjective
verb correspond to Predicate. To extract these pairs,
we analyze the dependency structure of sentences
in a document by a Japanese dependency structure
analyzer, CaboCha 2. The reason why we define
(Predicete1, Predicate2) as an Event is because we
recognized the necessity of such type of an Event by
investigating the extracted pairs of words and com-
paring them with the content of the target document
in preliminary experiments, and could not extract
any Event in case of extracting an Event from the
sentences without subject.
</bodyText>
<subsectionHeader confidence="0.999746">
3.2 Making an Event-by-Document Matrix
</subsectionHeader>
<bodyText confidence="0.999615">
In making a word-by-document matrix, high-
frequent words appeared in any documents, and ex-
tremely infrequent words are usually not included in
the matrix. In our method, high-frequent Events like
the former case were not observed in preliminary ex-
periments. We think the reason for this is because an
Event, a pair of words, can be more meaningful than
</bodyText>
<footnote confidence="0.949673">
2http://chasen.org/ taku/software/cabocha/
</footnote>
<bodyText confidence="0.999954285714286">
a single word, therefore, an Event is particularly a
good feature to express the meaning of a document.
Meanwhile, the average number of Events per sen-
tence is 4.90, while the average number of words per
sentence is 8.93. A lot of infrequent Events were ob-
served in the experiments because of the nature of an
Event, i.e., a pair of words. This means that the same
process of making a word-by-document matrix can-
not be applied to making an Event-by-document ma-
trix because the nature of an Event as a feature ex-
pressing a document is different from that of a word.
In concrete, if the events, which once appear in doc-
uments, would be removed from the candidates to
be a part of a document vector, there might be a case
where the constructed document vector does not re-
flect the content of the original documents. Consid-
ering this, in order to make the constructed docu-
ment vector reflect the content of the original doc-
uments, we do not remove the Event only itself ex-
tracted from a sentence, even though it appears only
once in a document.
</bodyText>
<subsectionHeader confidence="0.999687">
3.3 Estimating a Topic Distribution
</subsectionHeader>
<bodyText confidence="0.999986166666667">
After making an Event-by-document matrix, a la-
tent topic distribution of each Event is estimated by
means of Latent Dirichlet Allocation. Latent Dirich-
let Allocation is a generative probabilistic model that
allows multiple topics to occur in a document, and
gets the topic distribution based on the idea that
each topic emerges in a document based on a certain
probability. Each topic is expressed as a multinomial
distribution of words.
In this study, since a topic is assigned to an Event,
each topic is expressed as a multinomial distribution
of Events. As a method to estimate a topic distri-
bution, while a variational Bayes method (Blei et
al., 2003) and its application (Teh et al., 2006) have
been proposed, in this study we use Gibbs sampling
method (Grififths and Steyvers, 2004). Furthermore,
we define a sum of topic distributions of the events
in a query as the topic distribution of the query.
</bodyText>
<sectionHeader confidence="0.998436" genericHeader="method">
4 Performance Evaluation Experiment
</sectionHeader>
<bodyText confidence="0.999739">
Through a common document retrieval task, we
compare our method with the conventional method
and evaluate both of them. In concrete, we regard
the documents which have a similar topic distribu-
</bodyText>
<page confidence="0.999328">
31
</page>
<bodyText confidence="0.999970714285714">
tion to a query’s topic distribution as the result of
retrieval, and then examine whether or not the esti-
mated topic distribution can represent the latent se-
mantics of each document based on the accuracy of
retrieval results. Henceforth, we call the conven-
tional word-based LDA “wordLDA” and our pro-
posed event-based LDA “eventLDA”.
</bodyText>
<subsectionHeader confidence="0.915334">
4.1 Measures for Topic Distribution
</subsectionHeader>
<bodyText confidence="0.999979181818182">
As measures for identifying the similarity of
topic distribution, we adopt Kullback-Leibler Di-
vergence (Kullback and Leibler, 1951), Symmetric
Kullback-Leibler Divergence (Kullback and Leibler,
1951), Jensen-Shannon Divergence (Lin, 2002), and
cosine similarity. As for wordLDA, Henning (2009)
has reported that Jensen-Shannon Divergence shows
the best performance among the above measures in
terms of estimating the similarity between two sen-
tences. We also compare the performance of the
above measures when using eventLDA.
</bodyText>
<subsectionHeader confidence="0.988341">
4.2 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999763708333333">
As for the documents used in the experiment, we use
a set of data including users’ reviews and their eval-
uations for hotels and their facilities, provided by
Rakuten Travel3. Each review has five-grade eval-
uations of a hotel’s facilities such as room, location,
and so on. Since the data hold the relationships be-
tween objects and their evaluations, therefore, it is
said that they are appropriate for the performance
evaluation of our method because the relationship is
usually expressed in a pair of words, i.e., an Event.
The query we used in the experiment was “a room is
good”. The total number of documents is 2000, con-
sisting of 1000 documents randomly selected from
the users’ reviews whose evaluation for “a room” is
1 (bad) and 1000 documents randomly selected from
the reviews whose evaluation is 5 (good). The latter
1000 documents are regarded as the objective doc-
uments in retrieval. Because of this experiment de-
sign, it is clear that the random choice for retrieving
“good” vs. “bad” is 50%. As for the evaluation mea-
sure, we adopt 11-point interpolated average preci-
sion.
In this experiment, a comparison between the
both methods, i.e., wordLDA and eventLDA, is con-
</bodyText>
<footnote confidence="0.797555">
3http://travel.rakuten.co.jp/
</footnote>
<bodyText confidence="0.999730928571428">
ducted from the viewpoints of the proper number
of topics and the most useful measure to estimate
similarity. At first, we use Jensen-Shannon Diver-
gence as the measure to estimate the similarity of
topic distribution, changing the number of topics k
in the following, k = 5, k = 10, k = 20, k = 50,
k = 100, and k = 200. Next, the number of topics
is fixed based on the result of the first process, and
then it is decided which measure is the most useful
by applying each measure to estimate the similarity
of topic distributions. Here, the iteration count of
Gibbs Sampling is 200. The number of trials is 20,
and all trials are averaged. The same experiment is
conducted for wordLDA to compare both results.
</bodyText>
<subsectionHeader confidence="0.940486">
4.3 Result
</subsectionHeader>
<bodyText confidence="0.9991815">
Table 1 shows the retrieval result examined by 11-
point interpolated average precision, changing the
number of topics k. High accuracy is shown at k = 5
in eventLDA, and k = 50 in wordLDA, respectively.
Overall, we see that eventLDA keeps higher accu-
racy than wordLDA.
</bodyText>
<table confidence="0.994274428571429">
number of topics wordLDA eventLDA
5 0.5152 0.6256
10 0.5473 0.5744
20 0.5649 0.5874
50 0.5767 0.5740
100 0.5474 0.5783
200 0.5392 0.5870
</table>
<tableCaption confidence="0.999989">
Table 1: Result based on the number of topics.
</tableCaption>
<bodyText confidence="0.993480714285714">
Table 2 shows the retrieval result examined by
11-point interpolated average precision under vari-
ous measures. The number of topics k is k = 50
in wordLDA and k = 5 in eventLDA respectively,
based on the above result. Under any measures,
we see that eventLDA keeps higher accuracy than
wordLDA.
</bodyText>
<table confidence="0.9984252">
similarity measure wordLDA eventLDA
Kullback-Leibler 0.5009 0.5056
Symmetric Kullback-Leibler 0.5695 0.6762
Jensen-Shannon 0.5753 0.6754
cosine 0.5684 0.6859
</table>
<tableCaption confidence="0.998458">
Table 2: Performance under various measures.
</tableCaption>
<subsectionHeader confidence="0.906424">
4.4 Discussions
</subsectionHeader>
<bodyText confidence="0.9952725">
The result of the experiment shows that eventLDA
provides a better performance than wordLDA, there-
</bodyText>
<page confidence="0.996459">
32
</page>
<bodyText confidence="0.999979444444445">
fore, we see our method can properly treat the latent
topics of a document. In addition, as for a prop-
erty of eventLDA, we see that it can provide detail
classification with a small number of topics. As the
reason for this, we think that a topic distribution on
a feature is narrowed down to some extent by using
an Event as the feature instead of a word, and then
as a result, the possibility of generating error topics
decreased.
On the other hand, a proper measure for our
method is identified as cosine similarity, although
cosine similarity is not a measure to estimate prob-
abilistic distribution. It is unexpected that the mea-
sures proper to estimate probabilistic distribution got
the result of lower performance than cosine similar-
ity. From this, there are some space where we need
to examine the characteristics of topic distribution as
a probabilistic distribution.
</bodyText>
<sectionHeader confidence="0.97918" genericHeader="method">
5 Application to Summarization
</sectionHeader>
<bodyText confidence="0.999994714285714">
Here, we show multi-document summarization as
an application of our proposed method. We make
a query-biased summary, and show the effectiveness
of our method by comparing the accuracy of a gener-
ated summary by our method with that of summaries
by the representative summarization methods often
used as benchmark methods to compare.
</bodyText>
<subsectionHeader confidence="0.993754">
5.1 Extracting Sentences by MMR-MD
</subsectionHeader>
<bodyText confidence="0.9998808">
In extracting important sentences, considering only
similarity to a given query, we may generate a redun-
dant summary. To avoid this problem, a measure,
MMR-MD (Maximal Marginal Relevance Multi-
Document), was proposed (Goldstein et al., 2000).
This measure is the one which prevents extracting
similar sentences by providing penalty score that
corresponds to similarity between a newly extracted
sentence and the previously extracted sentences. It
is defined by Eq. 1 (Okumura and Nanba, 2005).
</bodyText>
<equation confidence="0.9998825">
MMR-MD = argmaxCiER\S[ASim1(Ci,Q)
−(1−A)maxCjESSim2(Ci,Cj)] (1)
</equation>
<bodyText confidence="0.893376333333333">
We aim to choose sentences whose content is sim-
ilar to query’s content based on a latent topic, while
reducing the redundancy of choosing similar sen-
tences to the previously chosen sentences. There-
fore, we adopt the similarity of topic distributions
Ci sentence in the document sets
</bodyText>
<figure confidence="0.56489775">
Q query
R a set of sentences retrieved by Q from the document sets
S a set of sentences in R already extracted
A weighting parameter
</figure>
<bodyText confidence="0.99743475">
for Sim1 which estimates similarity between a sen-
tence and a query, and adopt cosine similarity based
on Events as a feature unit for Sime which estimates
the similarity with the sentences previously chosen.
As the measures to estimate topic distribution simi-
larity, we use the four measures explained in Section
4.1. Here, as for the weighting parameter A, we set
A = 0.5.
</bodyText>
<subsectionHeader confidence="0.990878">
5.2 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.9999197">
In the experiment, we use a data set provided at NT-
CIR4 (NII Test Collection for IR Systems 4) TSC3
(Text Summarization Challenge 3) 4 .
The data consists of 30 topic sets of documents
in which each set has about 10 Japanese newspaper
articles, and the total number of the sentences in the
data is 3587. In order to make evaluation for the re-
sult provided by our method easier, we compile a set
of questions, provided by the data sets for evaluating
the result of summarization, as a query, and then use
it as a query for query-biased summarization. As an
evaluation method, we adopt precision and coverage
used at TSC3 (Hirao et al., 2004), and the number
of extracted sentences is the same as used in TSC3.
Precision is an evaluation measure which indicates
the ratio of the number of correct sentences to that
of the sentences generated by the system. Coverage
is an evaluation measure which indicates the degree
of how the system output is close to the summary
generated by a human, taking account of the redun-
dancy.
Moreover, to examine the characteristics of the
proposed method, we compare both methods in
terms of the number of topics and the proper mea-
sure to estimate similarity. The number of trials is
20 at each condition. 5 sets of documents selected
at random from 30 sets of documents are used in the
trials, and all the trials are totally averaged. As a
target for comparison with the proposed method, we
also conduct an experiment using wordLDA.
</bodyText>
<footnote confidence="0.971055">
4http://research.nii.ac.jp/ntcir/index-en.html
</footnote>
<page confidence="0.998142">
33
</page>
<subsectionHeader confidence="0.953944">
5.3 Result
</subsectionHeader>
<bodyText confidence="0.998976857142857">
As a result, there is no difference among the four
measures — the same result is obtained by the
four measures. Table 3 shows comparison between
eventLDA and wordLDA in terms of precision and
coverage. The number of topics providing the high-
est accuracy is k = 5 for wordLDA, and k = 10 for
eventLDA, respectively.
</bodyText>
<table confidence="0.99428">
number of topics wordLDA eventLDA
Precision Coverage Precision Coverage
5 0.314 0.249 0.404 0.323
10 0.264 0.211 0.418 0.340
20 0.261 0.183 0.413 0.325
50 0.253 0.171 0.392 0.319
</table>
<tableCaption confidence="0.999976">
Table 3: Comparison of the number of topics.
</tableCaption>
<bodyText confidence="0.998024333333333">
Furthermore, Table 4 shows comparison between
the proposed method and representative summa-
rization methods which do not deal with latent
topics. As representative summarization methods
to compare our method, we took up the Lead
method (Brandow et al., 1995) which is effective
for document sumarization of newspapers, and the
important sentence extraction-based summarization
method using TF-IDF.
</bodyText>
<table confidence="0.9991574">
method Precision Coverage
Lead 0.426 0.212
TF-IDF 0.454 0.305
wordLDA (k=5) 0.314 0.249
eventLDA (k=10) 0.418 0.340
</table>
<tableCaption confidence="0.99994">
Table 4: Comparison of each method.
</tableCaption>
<subsectionHeader confidence="0.980108">
5.4 Discussions
</subsectionHeader>
<bodyText confidence="0.999907384615385">
Under any condition, eventLDA provides a higher
accuracy than wordLDA. We see that the proposed
method is useful for estimating a topic on a sentence.
As the reason for that the accuracy does not depend
on any kinds of similarity measures, we think that
an estimated topic distribution is biased to a particu-
lar topic, therefore, there was not any influence due
to the kinds of similarity measures. Moreover, the
proper number of topics of eventLDA is bigger than
that of wordLDA. We consider the reason for this
is because we used newspaper articles as the objec-
tive documents, so it can be thought that the top-
ics onto the words in the articles were specific to
some extent; in other words, the words often used
in a particular field are often used in newspaper ar-
ticles, therefore, we think that wordLDA can clas-
sify the documents with the small number of top-
ics. In comparison with the representative methods,
the proposed method takes close accuracy to their
accuracy, therefore, we see that the performance of
our method is at the same level as those representa-
tive methods which directly deal with words in doc-
uments. In particular, as for coverage, our method
shows high accuracy. We think the reason for this
is because a comprehensive summary was made by
latent topics.
</bodyText>
<sectionHeader confidence="0.998011" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99997568">
In this paper, we have defined a pair of words with
dependency relationship as “Event” and proposed a
latent topic extracting method in which the content
of a document is comprehended by assigning latent
topics onto Events. We have examined the ability
of our proposed method in Section 4, and as its ap-
plication, we have shown a document summariza-
tion using the proposed method in Section 5. We
have shown that eventLDA has higher ability than
wordLDA in terms of estimating a topic distribu-
tion on even a sentence or a document; furthermore,
even in case of assigning a topic on an Event, we see
that latent topics can be properly estimated. Since
an Event can hold a relationship between a pair of
words, it can be said that our proposed method, i.e.,
eventLDA, can comprehend the content of a docu-
ment more deeper and proper than the conventional
method, i.e., wordLDA. Therefore, eventLDA can
be effectively applied to various document data sets
rather than wordLDA can be. We have also shown
that another feature other than a word, i.e., an Event
is also useful to estimate latent topics in a document.
As future works, we will conduct experiments with
various types of data and query, and further investi-
gate the characteristic of our proposed method.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99596725">
We would like to thank Rakuten, Inc. for permission
to use the resources of Rakuten Travel, and thank
the National Institute of Informatics for providing
NTCIR data sets.
</bodyText>
<page confidence="0.998047">
34
</page>
<sectionHeader confidence="0.996334" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999736284313726">
Adam Berger and Vibhu O. Mittal. 2000. Query-relevant
summarization using FAQs. In ACL ’00 Proceedings
of the 38th Annual Meeting on Association for Com-
putational Linguistics:294–301.
Anastasios Tombros and Mark Sanderson. 1998. Ad-
vantages of query biased summaries in information re-
trieval. In Proceedings of the 21st Annual Interna-
tional ACM-SIGIR Conference on Research and De-
velopment in Information Retrieval:2–10.
Ani Nenkova and Lucy Vanderwende. 2005. The Im-
pact of Frequency on Summarization. Technical re-
port, Microsoft Research.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing Content Models for Multi-Document Summariza-
tion. In Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of the
ACL:362–370.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research,3:993–1022.
Dragomir R. Radev. 2004. Lexrank: graph-based cen-
trality as salience in text summarization. Journal of
Artificial Intelligence Research (JAIR.
Harendra Bhandari, Masashi Shimbo, Takahiko Ito, and
Yuji Matsumoto. 2008. Generic Text Summarization
Using Probabilistic Latent Semantic Indexing. In Pro-
ceedings of the 3rd International Joint Conference on
Natural Langugage Proceeding:133-140.
H. P. Luhn. 1958. The automatic creation of literature
abstracts. IBMJournal of Research and Development.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-document sum-
marization by sentence extraction. In Proceedings
of the 2000 NAALP-ANLP Workshop on Automatic
Summarization:40–48.
Jianhua Lin. 2002. Divergence Measures based on the
Shannon Entropy. IEEE Transactions on Information
Theory, 37(1):145–151.
Leonhard Henning. 2009. Topic-based Multi-Document
Summarization with Probabilistic Latent Semantic
Analysis. Recent Advances in Natural Language
Processing:144–149.
Manabu Okumura and Eiji Nanba. 2005. Sci-
ence of knowledge: Automatic Text Summarization.(in
Japanese) ohmsha.
Manabu Okumura and Hajime Mochizuki. 2000. Query-
Biased Summarization Based on Lexical Chaining.
Computational Intelligence,16(4):578–585.
Qin Bing, Liu Ting, Zhang Yu, and Li Sheng. 2005. Re-
search on Multi-Document Summarization Based on
Latent Semantic Indexing. Journal ofHarbin Institute
of Technology,12(1):91–94.
Rachit Arora and Balaraman Ravindran. 2008. Latent
dirichlet allocation based multi-document summariza-
tion. In Proceedings of the 2nd Workshop on Analytics
for Noisy Unstructured Text Data.
Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995.
Automatic condensation of electronic publications by
sentence selection. Information Processing and Man-
agement: an International Journal - Special issue:
summarizing text,31(5):675–685.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal of the
American Society of Information Science, 41(6):391–
407.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment Classification Us-
ing Word Sub-sequences and Dependency Sub-trees.
In Proceedings of the 9th Pacific-Asia Interna-
tional Conference on Knowledge Discovery and Data
Mining:301–310.
Solomon Kullback and Richard A. Leibler. 1951. On
Information and Sufficiency. Annuals ofMathematical
Statistics, 22:49–86.
Thomas L. Grififths and Mark Steyvers. 2004. Find-
ing scientific topics. In Proceedings of the Na-
tional Academy of Sciences of the United States of
America,101:5228–5235.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. In Proceedings of the 22nd Annual In-
ternational ACM-SIGIR Conference on Research and
Development in Information Retrieval:50–57.
Tsutomu Hirao, Takahiro Fukusima, Manabu Okumura,
Chikashi Nobata, and Hidetsugu Nanba. 2004. Cor-
pus and evaluation measures for multiple document
summarization with multiple sources. In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics:535–541.
Xiaojun Wan and Jianwu Yang. 2006. Improved affinity
graph based multi-document summarization. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAACL, Companion Volume: Short Papers
Yasuhiro Suzuki, Takashi Uemura, Takuya Kida, and Hi-
roki Arimura. 2010. Extension to word phrase on la-
tent dirichlet allocation. Forum on Data Engineering
and Information Management,i-6.
Yee W. Teh, David Newman, and Max Welling. 2006.
A Collapsed Variational Bayesian Inference Algorithm
for Latent Dirichlet Allocation. Advances in Neural
Information Processing Systems Conference,19:1353–
1360.
</reference>
<page confidence="0.999318">
35
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.651889">
<title confidence="0.999001">A Latent Topic Extracting Method based on Events in a and its Application</title>
<author confidence="0.99737">Risa Kitajima Ichiro Kobayashi</author>
<affiliation confidence="0.999958">Ochanomizu University Ochanomizu University</affiliation>
<email confidence="0.663783">kitajima.risa@is.ocha.ac.jpkoba@is.ocha.ac.jp</email>
<abstract confidence="0.998757636363636">Recently, several latent topic analysis methods such as LSI, pLSI, and LDA have been widely used for text analysis. However, those methods basically assign topics to words, but do not account for the events in a document. With this background, in this paper, we propose a latent topic extracting method which assigns topics to events. We also show that our proposed method is useful to generate a document summary based on a latent topic.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Vibhu O Mittal</author>
</authors>
<title>Query-relevant summarization using FAQs.</title>
<date>2000</date>
<booktitle>In ACL ’00 Proceedings of the 38th Annual Meeting on Association for Computational Linguistics:294–301.</booktitle>
<contexts>
<context position="1722" citStr="Berger and Mittal, 2000" startWordPosition="278" endWordPosition="281">vel of sentence as an “Event” 1, and propose a model that treats a document as a set of Events. We use LDA as a latent topic analysis method, and assign topics to Events in a document. To examine our proposed method’s performance on extracting latent topics from a document, we compare the accuracy of our method to that of the conventional methods through a common document retrieval task. Furthermore, as an application of our method, we apply it to a query-biased document summarization (Tombros and Sanderson, &apos;For the definition of an Event, see Section 3. 30 1998; Okumura and Mochizuki, 2000; Berger and Mittal, 2000) to verify that the method is useful for various applications. 2 Related Studies Suzuki et al. (2010) proposed a flexible latent topics inference in which topics are assigned to phrases in a document. Matsumoto et al. (2005) showed that the accuracy of document classification will be improved by introducing a feature dealing with the dependency relationships among words. In case of assigning topics to words, it is likely that two documents, which have the same word frequency in themselves, tend to be estimated as they have the same topic probablistic distribution without considering the depend</context>
</contexts>
<marker>Berger, Mittal, 2000</marker>
<rawString>Adam Berger and Vibhu O. Mittal. 2000. Query-relevant summarization using FAQs. In ACL ’00 Proceedings of the 38th Annual Meeting on Association for Computational Linguistics:294–301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anastasios Tombros</author>
<author>Mark Sanderson</author>
</authors>
<title>Advantages of query biased summaries in information retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval:2–10.</booktitle>
<marker>Tombros, Sanderson, 1998</marker>
<rawString>Anastasios Tombros and Mark Sanderson. 1998. Advantages of query biased summaries in information retrieval. In Proceedings of the 21st Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval:2–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Lucy Vanderwende</author>
</authors>
<title>The Impact of Frequency on Summarization.</title>
<date>2005</date>
<tech>Technical report, Microsoft Research.</tech>
<contexts>
<context position="3046" citStr="Nenkova and Vanderwende, 2005" startWordPosition="500" endWordPosition="504"> regarded as more important rather than the frequency of words as the feature identifying the topics of a document. For example, in case of classifying opinions to objects in a document, we have to identify what sort of opinion is assigned to the target objects, therefore, we have to focus on the relationship among words in a sentence, not only on the frequent words appeared in a document. For this reason, we propose a method to assign topics to Events instead of words. As for studies on document summarization, there are various methods, such as the method based on word frequency (Luhn, 1958; Nenkova and Vanderwende, 2005), and the method based on a graph (Radev, 2004; Wan and Yang, 2006). Moreover, several methods using a latent topic model have been proposed (Bing et al., 2005; Arora and RavinProceedings of the ACL-HLT 2011 Student Session, pages 30–35, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics dran, 2008; Bhandari et al., 2008; Henning, 2009; Haghighi and Vanderwende, 2009). In those studies, the methods estimate a topic distribution on each sentence in the same way as the latent semantic analysis methods normally do that on each document, and generate a summary base</context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>Ani Nenkova and Lucy Vanderwende. 2005. The Impact of Frequency on Summarization. Technical report, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring Content Models for Multi-Document Summarization. In Human Language Technologies: The</title>
<date>2009</date>
<booktitle>Annual Conference of the North American Chapter of the ACL:362–370.</booktitle>
<contexts>
<context position="3449" citStr="Haghighi and Vanderwende, 2009" startWordPosition="564" endWordPosition="567">reason, we propose a method to assign topics to Events instead of words. As for studies on document summarization, there are various methods, such as the method based on word frequency (Luhn, 1958; Nenkova and Vanderwende, 2005), and the method based on a graph (Radev, 2004; Wan and Yang, 2006). Moreover, several methods using a latent topic model have been proposed (Bing et al., 2005; Arora and RavinProceedings of the ACL-HLT 2011 Student Session, pages 30–35, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics dran, 2008; Bhandari et al., 2008; Henning, 2009; Haghighi and Vanderwende, 2009). In those studies, the methods estimate a topic distribution on each sentence in the same way as the latent semantic analysis methods normally do that on each document, and generate a summary based on the distribution. We also show that our proposed method is useful for the document summarization based on extracting latent topics from sentences. 3 Topic Extraction based on Events In this study, since we deal with a document as a set of Events, we extract Events from each document; define some of the extracted Events as the index terms for the whole objective documents; and then make an Event-</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring Content Models for Multi-Document Summarization. In Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL:362–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,3:993–1022.</journal>
<contexts>
<context position="881" citStr="Blei et al., 2003" startWordPosition="129" endWordPosition="132">uch as LSI, pLSI, and LDA have been widely used for text analysis. However, those methods basically assign topics to words, but do not account for the events in a document. With this background, in this paper, we propose a latent topic extracting method which assigns topics to events. We also show that our proposed method is useful to generate a document summary based on a latent topic. 1 Introduction Recently, several latent topic analysis methods such as Latent Semantic Indexing (LSI) (Deerwester et al., 1990), Probabilistic LSI (pLSI) (Hofmann, 1999), and Latent Dirichlet Allocation (LDA) (Blei et al., 2003) have been widely used for text analysis. However, those methods basically assign topics to words, but do not account for the events in a document. Here, we define a unit of informing the content of document at the level of sentence as an “Event” 1, and propose a model that treats a document as a set of Events. We use LDA as a latent topic analysis method, and assign topics to Events in a document. To examine our proposed method’s performance on extracting latent topics from a document, we compare the accuracy of our method to that of the conventional methods through a common document retrieva</context>
<context position="7162" citStr="Blei et al., 2003" startWordPosition="1195" endWordPosition="1198">rix, a latent topic distribution of each Event is estimated by means of Latent Dirichlet Allocation. Latent Dirichlet Allocation is a generative probabilistic model that allows multiple topics to occur in a document, and gets the topic distribution based on the idea that each topic emerges in a document based on a certain probability. Each topic is expressed as a multinomial distribution of words. In this study, since a topic is assigned to an Event, each topic is expressed as a multinomial distribution of Events. As a method to estimate a topic distribution, while a variational Bayes method (Blei et al., 2003) and its application (Teh et al., 2006) have been proposed, in this study we use Gibbs sampling method (Grififths and Steyvers, 2004). Furthermore, we define a sum of topic distributions of the events in a query as the topic distribution of the query. 4 Performance Evaluation Experiment Through a common document retrieval task, we compare our method with the conventional method and evaluate both of them. In concrete, we regard the documents which have a similar topic distribu31 tion to a query’s topic distribution as the result of retrieval, and then examine whether or not the estimated topic </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research,3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: graph-based centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research (JAIR.</journal>
<contexts>
<context position="3092" citStr="Radev, 2004" startWordPosition="512" endWordPosition="513"> the feature identifying the topics of a document. For example, in case of classifying opinions to objects in a document, we have to identify what sort of opinion is assigned to the target objects, therefore, we have to focus on the relationship among words in a sentence, not only on the frequent words appeared in a document. For this reason, we propose a method to assign topics to Events instead of words. As for studies on document summarization, there are various methods, such as the method based on word frequency (Luhn, 1958; Nenkova and Vanderwende, 2005), and the method based on a graph (Radev, 2004; Wan and Yang, 2006). Moreover, several methods using a latent topic model have been proposed (Bing et al., 2005; Arora and RavinProceedings of the ACL-HLT 2011 Student Session, pages 30–35, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics dran, 2008; Bhandari et al., 2008; Henning, 2009; Haghighi and Vanderwende, 2009). In those studies, the methods estimate a topic distribution on each sentence in the same way as the latent semantic analysis methods normally do that on each document, and generate a summary based on the distribution. We also show that our p</context>
</contexts>
<marker>Radev, 2004</marker>
<rawString>Dragomir R. Radev. 2004. Lexrank: graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harendra Bhandari</author>
<author>Masashi Shimbo</author>
<author>Takahiko Ito</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Generic Text Summarization Using Probabilistic Latent Semantic Indexing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Joint Conference on Natural Langugage Proceeding:133-140.</booktitle>
<contexts>
<context position="3401" citStr="Bhandari et al., 2008" startWordPosition="558" endWordPosition="561">ords appeared in a document. For this reason, we propose a method to assign topics to Events instead of words. As for studies on document summarization, there are various methods, such as the method based on word frequency (Luhn, 1958; Nenkova and Vanderwende, 2005), and the method based on a graph (Radev, 2004; Wan and Yang, 2006). Moreover, several methods using a latent topic model have been proposed (Bing et al., 2005; Arora and RavinProceedings of the ACL-HLT 2011 Student Session, pages 30–35, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics dran, 2008; Bhandari et al., 2008; Henning, 2009; Haghighi and Vanderwende, 2009). In those studies, the methods estimate a topic distribution on each sentence in the same way as the latent semantic analysis methods normally do that on each document, and generate a summary based on the distribution. We also show that our proposed method is useful for the document summarization based on extracting latent topics from sentences. 3 Topic Extraction based on Events In this study, since we deal with a document as a set of Events, we extract Events from each document; define some of the extracted Events as the index terms for the wh</context>
</contexts>
<marker>Bhandari, Shimbo, Ito, Matsumoto, 2008</marker>
<rawString>Harendra Bhandari, Masashi Shimbo, Takahiko Ito, and Yuji Matsumoto. 2008. Generic Text Summarization Using Probabilistic Latent Semantic Indexing. In Proceedings of the 3rd International Joint Conference on Natural Langugage Proceeding:133-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts. IBMJournal of Research and Development.</title>
<date>1958</date>
<contexts>
<context position="3014" citStr="Luhn, 1958" startWordPosition="498" endWordPosition="499">ong words is regarded as more important rather than the frequency of words as the feature identifying the topics of a document. For example, in case of classifying opinions to objects in a document, we have to identify what sort of opinion is assigned to the target objects, therefore, we have to focus on the relationship among words in a sentence, not only on the frequent words appeared in a document. For this reason, we propose a method to assign topics to Events instead of words. As for studies on document summarization, there are various methods, such as the method based on word frequency (Luhn, 1958; Nenkova and Vanderwende, 2005), and the method based on a graph (Radev, 2004; Wan and Yang, 2006). Moreover, several methods using a latent topic model have been proposed (Bing et al., 2005; Arora and RavinProceedings of the ACL-HLT 2011 Student Session, pages 30–35, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics dran, 2008; Bhandari et al., 2008; Henning, 2009; Haghighi and Vanderwende, 2009). In those studies, the methods estimate a topic distribution on each sentence in the same way as the latent semantic analysis methods normally do that on each docum</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H. P. Luhn. 1958. The automatic creation of literature abstracts. IBMJournal of Research and Development.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
<author>Mark Kantrowitz</author>
</authors>
<title>Multi-document summarization by sentence extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 NAALP-ANLP Workshop on Automatic Summarization:40–48.</booktitle>
<contexts>
<context position="13082" citStr="Goldstein et al., 2000" startWordPosition="2159" endWordPosition="2162">o Summarization Here, we show multi-document summarization as an application of our proposed method. We make a query-biased summary, and show the effectiveness of our method by comparing the accuracy of a generated summary by our method with that of summaries by the representative summarization methods often used as benchmark methods to compare. 5.1 Extracting Sentences by MMR-MD In extracting important sentences, considering only similarity to a given query, we may generate a redundant summary. To avoid this problem, a measure, MMR-MD (Maximal Marginal Relevance MultiDocument), was proposed (Goldstein et al., 2000). This measure is the one which prevents extracting similar sentences by providing penalty score that corresponds to similarity between a newly extracted sentence and the previously extracted sentences. It is defined by Eq. 1 (Okumura and Nanba, 2005). MMR-MD = argmaxCiER\S[ASim1(Ci,Q) −(1−A)maxCjESSim2(Ci,Cj)] (1) We aim to choose sentences whose content is similar to query’s content based on a latent topic, while reducing the redundancy of choosing similar sentences to the previously chosen sentences. Therefore, we adopt the similarity of topic distributions Ci sentence in the document sets </context>
</contexts>
<marker>Goldstein, Mittal, Carbonell, Kantrowitz, 2000</marker>
<rawString>Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz. 2000. Multi-document summarization by sentence extraction. In Proceedings of the 2000 NAALP-ANLP Workshop on Automatic Summarization:40–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Lin</author>
</authors>
<title>Divergence Measures based on the Shannon Entropy.</title>
<date>2002</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="8252" citStr="Lin, 2002" startWordPosition="1364" endWordPosition="1365">31 tion to a query’s topic distribution as the result of retrieval, and then examine whether or not the estimated topic distribution can represent the latent semantics of each document based on the accuracy of retrieval results. Henceforth, we call the conventional word-based LDA “wordLDA” and our proposed event-based LDA “eventLDA”. 4.1 Measures for Topic Distribution As measures for identifying the similarity of topic distribution, we adopt Kullback-Leibler Divergence (Kullback and Leibler, 1951), Symmetric Kullback-Leibler Divergence (Kullback and Leibler, 1951), Jensen-Shannon Divergence (Lin, 2002), and cosine similarity. As for wordLDA, Henning (2009) has reported that Jensen-Shannon Divergence shows the best performance among the above measures in terms of estimating the similarity between two sentences. We also compare the performance of the above measures when using eventLDA. 4.2 Experimental Settings As for the documents used in the experiment, we use a set of data including users’ reviews and their evaluations for hotels and their facilities, provided by Rakuten Travel3. Each review has five-grade evaluations of a hotel’s facilities such as room, location, and so on. Since the dat</context>
</contexts>
<marker>Lin, 2002</marker>
<rawString>Jianhua Lin. 2002. Divergence Measures based on the Shannon Entropy. IEEE Transactions on Information Theory, 37(1):145–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonhard Henning</author>
</authors>
<title>Topic-based Multi-Document Summarization with Probabilistic Latent Semantic Analysis.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing:144–149.</booktitle>
<contexts>
<context position="3416" citStr="Henning, 2009" startWordPosition="562" endWordPosition="563">ment. For this reason, we propose a method to assign topics to Events instead of words. As for studies on document summarization, there are various methods, such as the method based on word frequency (Luhn, 1958; Nenkova and Vanderwende, 2005), and the method based on a graph (Radev, 2004; Wan and Yang, 2006). Moreover, several methods using a latent topic model have been proposed (Bing et al., 2005; Arora and RavinProceedings of the ACL-HLT 2011 Student Session, pages 30–35, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics dran, 2008; Bhandari et al., 2008; Henning, 2009; Haghighi and Vanderwende, 2009). In those studies, the methods estimate a topic distribution on each sentence in the same way as the latent semantic analysis methods normally do that on each document, and generate a summary based on the distribution. We also show that our proposed method is useful for the document summarization based on extracting latent topics from sentences. 3 Topic Extraction based on Events In this study, since we deal with a document as a set of Events, we extract Events from each document; define some of the extracted Events as the index terms for the whole objective d</context>
<context position="8307" citStr="Henning (2009)" startWordPosition="1372" endWordPosition="1373">lt of retrieval, and then examine whether or not the estimated topic distribution can represent the latent semantics of each document based on the accuracy of retrieval results. Henceforth, we call the conventional word-based LDA “wordLDA” and our proposed event-based LDA “eventLDA”. 4.1 Measures for Topic Distribution As measures for identifying the similarity of topic distribution, we adopt Kullback-Leibler Divergence (Kullback and Leibler, 1951), Symmetric Kullback-Leibler Divergence (Kullback and Leibler, 1951), Jensen-Shannon Divergence (Lin, 2002), and cosine similarity. As for wordLDA, Henning (2009) has reported that Jensen-Shannon Divergence shows the best performance among the above measures in terms of estimating the similarity between two sentences. We also compare the performance of the above measures when using eventLDA. 4.2 Experimental Settings As for the documents used in the experiment, we use a set of data including users’ reviews and their evaluations for hotels and their facilities, provided by Rakuten Travel3. Each review has five-grade evaluations of a hotel’s facilities such as room, location, and so on. Since the data hold the relationships between objects and their eval</context>
</contexts>
<marker>Henning, 2009</marker>
<rawString>Leonhard Henning. 2009. Topic-based Multi-Document Summarization with Probabilistic Latent Semantic Analysis. Recent Advances in Natural Language Processing:144–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Okumura</author>
<author>Eiji Nanba</author>
</authors>
<title>Science of knowledge: Automatic Text Summarization.(in Japanese) ohmsha.</title>
<date>2005</date>
<contexts>
<context position="13333" citStr="Okumura and Nanba, 2005" startWordPosition="2197" endWordPosition="2200">summaries by the representative summarization methods often used as benchmark methods to compare. 5.1 Extracting Sentences by MMR-MD In extracting important sentences, considering only similarity to a given query, we may generate a redundant summary. To avoid this problem, a measure, MMR-MD (Maximal Marginal Relevance MultiDocument), was proposed (Goldstein et al., 2000). This measure is the one which prevents extracting similar sentences by providing penalty score that corresponds to similarity between a newly extracted sentence and the previously extracted sentences. It is defined by Eq. 1 (Okumura and Nanba, 2005). MMR-MD = argmaxCiER\S[ASim1(Ci,Q) −(1−A)maxCjESSim2(Ci,Cj)] (1) We aim to choose sentences whose content is similar to query’s content based on a latent topic, while reducing the redundancy of choosing similar sentences to the previously chosen sentences. Therefore, we adopt the similarity of topic distributions Ci sentence in the document sets Q query R a set of sentences retrieved by Q from the document sets S a set of sentences in R already extracted A weighting parameter for Sim1 which estimates similarity between a sentence and a query, and adopt cosine similarity based on Events as a f</context>
</contexts>
<marker>Okumura, Nanba, 2005</marker>
<rawString>Manabu Okumura and Eiji Nanba. 2005. Science of knowledge: Automatic Text Summarization.(in Japanese) ohmsha.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Okumura</author>
<author>Hajime Mochizuki</author>
</authors>
<title>QueryBiased Summarization Based on Lexical Chaining.</title>
<date>2000</date>
<journal>Computational Intelligence,16(4):578–585.</journal>
<contexts>
<context position="1696" citStr="Okumura and Mochizuki, 2000" startWordPosition="274" endWordPosition="277">content of document at the level of sentence as an “Event” 1, and propose a model that treats a document as a set of Events. We use LDA as a latent topic analysis method, and assign topics to Events in a document. To examine our proposed method’s performance on extracting latent topics from a document, we compare the accuracy of our method to that of the conventional methods through a common document retrieval task. Furthermore, as an application of our method, we apply it to a query-biased document summarization (Tombros and Sanderson, &apos;For the definition of an Event, see Section 3. 30 1998; Okumura and Mochizuki, 2000; Berger and Mittal, 2000) to verify that the method is useful for various applications. 2 Related Studies Suzuki et al. (2010) proposed a flexible latent topics inference in which topics are assigned to phrases in a document. Matsumoto et al. (2005) showed that the accuracy of document classification will be improved by introducing a feature dealing with the dependency relationships among words. In case of assigning topics to words, it is likely that two documents, which have the same word frequency in themselves, tend to be estimated as they have the same topic probablistic distribution with</context>
</contexts>
<marker>Okumura, Mochizuki, 2000</marker>
<rawString>Manabu Okumura and Hajime Mochizuki. 2000. QueryBiased Summarization Based on Lexical Chaining. Computational Intelligence,16(4):578–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Bing</author>
<author>Liu Ting</author>
<author>Zhang Yu</author>
<author>Li Sheng</author>
</authors>
<date>2005</date>
<booktitle>Research on Multi-Document Summarization Based on Latent Semantic Indexing. Journal ofHarbin Institute of Technology,12(1):91–94.</booktitle>
<contexts>
<context position="3205" citStr="Bing et al., 2005" startWordPosition="529" endWordPosition="532">in a document, we have to identify what sort of opinion is assigned to the target objects, therefore, we have to focus on the relationship among words in a sentence, not only on the frequent words appeared in a document. For this reason, we propose a method to assign topics to Events instead of words. As for studies on document summarization, there are various methods, such as the method based on word frequency (Luhn, 1958; Nenkova and Vanderwende, 2005), and the method based on a graph (Radev, 2004; Wan and Yang, 2006). Moreover, several methods using a latent topic model have been proposed (Bing et al., 2005; Arora and RavinProceedings of the ACL-HLT 2011 Student Session, pages 30–35, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics dran, 2008; Bhandari et al., 2008; Henning, 2009; Haghighi and Vanderwende, 2009). In those studies, the methods estimate a topic distribution on each sentence in the same way as the latent semantic analysis methods normally do that on each document, and generate a summary based on the distribution. We also show that our proposed method is useful for the document summarization based on extracting latent topics from sentences. 3 Topic</context>
</contexts>
<marker>Bing, Ting, Yu, Sheng, 2005</marker>
<rawString>Qin Bing, Liu Ting, Zhang Yu, and Li Sheng. 2005. Research on Multi-Document Summarization Based on Latent Semantic Indexing. Journal ofHarbin Institute of Technology,12(1):91–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachit Arora</author>
<author>Balaraman Ravindran</author>
</authors>
<title>Latent dirichlet allocation based multi-document summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2nd Workshop on Analytics for Noisy Unstructured Text Data.</booktitle>
<marker>Arora, Ravindran, 2008</marker>
<rawString>Rachit Arora and Balaraman Ravindran. 2008. Latent dirichlet allocation based multi-document summarization. In Proceedings of the 2nd Workshop on Analytics for Noisy Unstructured Text Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Brandow</author>
<author>Karl Mitze</author>
<author>Lisa F Rau</author>
</authors>
<title>Automatic condensation of electronic publications by sentence selection. Information Processing and Management: an International Journal - Special issue: summarizing text,31(5):675–685.</title>
<date>1995</date>
<contexts>
<context position="16536" citStr="Brandow et al., 1995" startWordPosition="2739" endWordPosition="2742">of precision and coverage. The number of topics providing the highest accuracy is k = 5 for wordLDA, and k = 10 for eventLDA, respectively. number of topics wordLDA eventLDA Precision Coverage Precision Coverage 5 0.314 0.249 0.404 0.323 10 0.264 0.211 0.418 0.340 20 0.261 0.183 0.413 0.325 50 0.253 0.171 0.392 0.319 Table 3: Comparison of the number of topics. Furthermore, Table 4 shows comparison between the proposed method and representative summarization methods which do not deal with latent topics. As representative summarization methods to compare our method, we took up the Lead method (Brandow et al., 1995) which is effective for document sumarization of newspapers, and the important sentence extraction-based summarization method using TF-IDF. method Precision Coverage Lead 0.426 0.212 TF-IDF 0.454 0.305 wordLDA (k=5) 0.314 0.249 eventLDA (k=10) 0.418 0.340 Table 4: Comparison of each method. 5.4 Discussions Under any condition, eventLDA provides a higher accuracy than wordLDA. We see that the proposed method is useful for estimating a topic on a sentence. As the reason for that the accuracy does not depend on any kinds of similarity measures, we think that an estimated topic distribution is bia</context>
</contexts>
<marker>Brandow, Mitze, Rau, 1995</marker>
<rawString>Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995. Automatic condensation of electronic publications by sentence selection. Information Processing and Management: an International Journal - Special issue: summarizing text,31(5):675–685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<pages>407</pages>
<contexts>
<context position="780" citStr="Deerwester et al., 1990" startWordPosition="115" endWordPosition="118">y kitajima.risa@is.ocha.ac.jp koba@is.ocha.ac.jp Abstract Recently, several latent topic analysis methods such as LSI, pLSI, and LDA have been widely used for text analysis. However, those methods basically assign topics to words, but do not account for the events in a document. With this background, in this paper, we propose a latent topic extracting method which assigns topics to events. We also show that our proposed method is useful to generate a document summary based on a latent topic. 1 Introduction Recently, several latent topic analysis methods such as Latent Semantic Indexing (LSI) (Deerwester et al., 1990), Probabilistic LSI (pLSI) (Hofmann, 1999), and Latent Dirichlet Allocation (LDA) (Blei et al., 2003) have been widely used for text analysis. However, those methods basically assign topics to words, but do not account for the events in a document. Here, we define a unit of informing the content of document at the level of sentence as an “Event” 1, and propose a model that treats a document as a set of Events. We use LDA as a latent topic analysis method, and assign topics to Events in a document. To examine our proposed method’s performance on extracting latent topics from a document, we comp</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society of Information Science, 41(6):391– 407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shotaro Matsumoto</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Sentiment Classification Using Word Sub-sequences and Dependency Sub-trees.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th Pacific-Asia International Conference on Knowledge Discovery and Data Mining:301–310.</booktitle>
<contexts>
<context position="1946" citStr="Matsumoto et al. (2005)" startWordPosition="316" endWordPosition="319">ance on extracting latent topics from a document, we compare the accuracy of our method to that of the conventional methods through a common document retrieval task. Furthermore, as an application of our method, we apply it to a query-biased document summarization (Tombros and Sanderson, &apos;For the definition of an Event, see Section 3. 30 1998; Okumura and Mochizuki, 2000; Berger and Mittal, 2000) to verify that the method is useful for various applications. 2 Related Studies Suzuki et al. (2010) proposed a flexible latent topics inference in which topics are assigned to phrases in a document. Matsumoto et al. (2005) showed that the accuracy of document classification will be improved by introducing a feature dealing with the dependency relationships among words. In case of assigning topics to words, it is likely that two documents, which have the same word frequency in themselves, tend to be estimated as they have the same topic probablistic distribution without considering the dependency relation among words. However, there are many cases where the relationship among words is regarded as more important rather than the frequency of words as the feature identifying the topics of a document. For example, i</context>
</contexts>
<marker>Matsumoto, Takamura, Okumura, 2005</marker>
<rawString>Shotaro Matsumoto, Hiroya Takamura, and Manabu Okumura. 2005. Sentiment Classification Using Word Sub-sequences and Dependency Sub-trees. In Proceedings of the 9th Pacific-Asia International Conference on Knowledge Discovery and Data Mining:301–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Kullback</author>
<author>Richard A Leibler</author>
</authors>
<date>1951</date>
<booktitle>On Information and Sufficiency. Annuals ofMathematical Statistics,</booktitle>
<pages>22--49</pages>
<contexts>
<context position="8145" citStr="Kullback and Leibler, 1951" startWordPosition="1351" endWordPosition="1354"> the conventional method and evaluate both of them. In concrete, we regard the documents which have a similar topic distribu31 tion to a query’s topic distribution as the result of retrieval, and then examine whether or not the estimated topic distribution can represent the latent semantics of each document based on the accuracy of retrieval results. Henceforth, we call the conventional word-based LDA “wordLDA” and our proposed event-based LDA “eventLDA”. 4.1 Measures for Topic Distribution As measures for identifying the similarity of topic distribution, we adopt Kullback-Leibler Divergence (Kullback and Leibler, 1951), Symmetric Kullback-Leibler Divergence (Kullback and Leibler, 1951), Jensen-Shannon Divergence (Lin, 2002), and cosine similarity. As for wordLDA, Henning (2009) has reported that Jensen-Shannon Divergence shows the best performance among the above measures in terms of estimating the similarity between two sentences. We also compare the performance of the above measures when using eventLDA. 4.2 Experimental Settings As for the documents used in the experiment, we use a set of data including users’ reviews and their evaluations for hotels and their facilities, provided by Rakuten Travel3. Each</context>
</contexts>
<marker>Kullback, Leibler, 1951</marker>
<rawString>Solomon Kullback and Richard A. Leibler. 1951. On Information and Sufficiency. Annuals ofMathematical Statistics, 22:49–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Grififths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Academy of Sciences of the United States of America,101:5228–5235.</booktitle>
<contexts>
<context position="7295" citStr="Grififths and Steyvers, 2004" startWordPosition="1217" endWordPosition="1220">cation is a generative probabilistic model that allows multiple topics to occur in a document, and gets the topic distribution based on the idea that each topic emerges in a document based on a certain probability. Each topic is expressed as a multinomial distribution of words. In this study, since a topic is assigned to an Event, each topic is expressed as a multinomial distribution of Events. As a method to estimate a topic distribution, while a variational Bayes method (Blei et al., 2003) and its application (Teh et al., 2006) have been proposed, in this study we use Gibbs sampling method (Grififths and Steyvers, 2004). Furthermore, we define a sum of topic distributions of the events in a query as the topic distribution of the query. 4 Performance Evaluation Experiment Through a common document retrieval task, we compare our method with the conventional method and evaluate both of them. In concrete, we regard the documents which have a similar topic distribu31 tion to a query’s topic distribution as the result of retrieval, and then examine whether or not the estimated topic distribution can represent the latent semantics of each document based on the accuracy of retrieval results. Henceforth, we call the </context>
</contexts>
<marker>Grififths, Steyvers, 2004</marker>
<rawString>Thomas L. Grififths and Mark Steyvers. 2004. Finding scientific topics. In Proceedings of the National Academy of Sciences of the United States of America,101:5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic Latent Semantic Indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval:50–57.</booktitle>
<contexts>
<context position="822" citStr="Hofmann, 1999" startWordPosition="122" endWordPosition="123">tract Recently, several latent topic analysis methods such as LSI, pLSI, and LDA have been widely used for text analysis. However, those methods basically assign topics to words, but do not account for the events in a document. With this background, in this paper, we propose a latent topic extracting method which assigns topics to events. We also show that our proposed method is useful to generate a document summary based on a latent topic. 1 Introduction Recently, several latent topic analysis methods such as Latent Semantic Indexing (LSI) (Deerwester et al., 1990), Probabilistic LSI (pLSI) (Hofmann, 1999), and Latent Dirichlet Allocation (LDA) (Blei et al., 2003) have been widely used for text analysis. However, those methods basically assign topics to words, but do not account for the events in a document. Here, we define a unit of informing the content of document at the level of sentence as an “Event” 1, and propose a model that treats a document as a set of Events. We use LDA as a latent topic analysis method, and assign topics to Events in a document. To examine our proposed method’s performance on extracting latent topics from a document, we compare the accuracy of our method to that of </context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic Latent Semantic Indexing. In Proceedings of the 22nd Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval:50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsutomu Hirao</author>
<author>Takahiro Fukusima</author>
<author>Manabu Okumura</author>
<author>Chikashi Nobata</author>
<author>Hidetsugu Nanba</author>
</authors>
<title>Corpus and evaluation measures for multiple document summarization with multiple sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics:535–541.</booktitle>
<contexts>
<context position="14855" citStr="Hirao et al., 2004" startWordPosition="2459" endWordPosition="2462"> use a data set provided at NTCIR4 (NII Test Collection for IR Systems 4) TSC3 (Text Summarization Challenge 3) 4 . The data consists of 30 topic sets of documents in which each set has about 10 Japanese newspaper articles, and the total number of the sentences in the data is 3587. In order to make evaluation for the result provided by our method easier, we compile a set of questions, provided by the data sets for evaluating the result of summarization, as a query, and then use it as a query for query-biased summarization. As an evaluation method, we adopt precision and coverage used at TSC3 (Hirao et al., 2004), and the number of extracted sentences is the same as used in TSC3. Precision is an evaluation measure which indicates the ratio of the number of correct sentences to that of the sentences generated by the system. Coverage is an evaluation measure which indicates the degree of how the system output is close to the summary generated by a human, taking account of the redundancy. Moreover, to examine the characteristics of the proposed method, we compare both methods in terms of the number of topics and the proper measure to estimate similarity. The number of trials is 20 at each condition. 5 se</context>
</contexts>
<marker>Hirao, Fukusima, Okumura, Nobata, Nanba, 2004</marker>
<rawString>Tsutomu Hirao, Takahiro Fukusima, Manabu Okumura, Chikashi Nobata, and Hidetsugu Nanba. 2004. Corpus and evaluation measures for multiple document summarization with multiple sources. In Proceedings of the 20th International Conference on Computational Linguistics:535–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
</authors>
<title>Improved affinity graph based multi-document summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</booktitle>
<contexts>
<context position="3113" citStr="Wan and Yang, 2006" startWordPosition="514" endWordPosition="517">identifying the topics of a document. For example, in case of classifying opinions to objects in a document, we have to identify what sort of opinion is assigned to the target objects, therefore, we have to focus on the relationship among words in a sentence, not only on the frequent words appeared in a document. For this reason, we propose a method to assign topics to Events instead of words. As for studies on document summarization, there are various methods, such as the method based on word frequency (Luhn, 1958; Nenkova and Vanderwende, 2005), and the method based on a graph (Radev, 2004; Wan and Yang, 2006). Moreover, several methods using a latent topic model have been proposed (Bing et al., 2005; Arora and RavinProceedings of the ACL-HLT 2011 Student Session, pages 30–35, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics dran, 2008; Bhandari et al., 2008; Henning, 2009; Haghighi and Vanderwende, 2009). In those studies, the methods estimate a topic distribution on each sentence in the same way as the latent semantic analysis methods normally do that on each document, and generate a summary based on the distribution. We also show that our proposed method is use</context>
</contexts>
<marker>Wan, Yang, 2006</marker>
<rawString>Xiaojun Wan and Jianwu Yang. 2006. Improved affinity graph based multi-document summarization. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasuhiro Suzuki</author>
<author>Takashi Uemura</author>
<author>Takuya Kida</author>
<author>Hiroki Arimura</author>
</authors>
<title>Extension to word phrase on latent dirichlet allocation.</title>
<date>2010</date>
<booktitle>Forum on Data Engineering and Information Management,i-6.</booktitle>
<contexts>
<context position="1823" citStr="Suzuki et al. (2010)" startWordPosition="295" endWordPosition="298">A as a latent topic analysis method, and assign topics to Events in a document. To examine our proposed method’s performance on extracting latent topics from a document, we compare the accuracy of our method to that of the conventional methods through a common document retrieval task. Furthermore, as an application of our method, we apply it to a query-biased document summarization (Tombros and Sanderson, &apos;For the definition of an Event, see Section 3. 30 1998; Okumura and Mochizuki, 2000; Berger and Mittal, 2000) to verify that the method is useful for various applications. 2 Related Studies Suzuki et al. (2010) proposed a flexible latent topics inference in which topics are assigned to phrases in a document. Matsumoto et al. (2005) showed that the accuracy of document classification will be improved by introducing a feature dealing with the dependency relationships among words. In case of assigning topics to words, it is likely that two documents, which have the same word frequency in themselves, tend to be estimated as they have the same topic probablistic distribution without considering the dependency relation among words. However, there are many cases where the relationship among words is regard</context>
</contexts>
<marker>Suzuki, Uemura, Kida, Arimura, 2010</marker>
<rawString>Yasuhiro Suzuki, Takashi Uemura, Takuya Kida, and Hiroki Arimura. 2010. Extension to word phrase on latent dirichlet allocation. Forum on Data Engineering and Information Management,i-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee W Teh</author>
<author>David Newman</author>
<author>Max Welling</author>
</authors>
<title>A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation.</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems Conference,19:1353–</booktitle>
<pages>1360</pages>
<contexts>
<context position="7201" citStr="Teh et al., 2006" startWordPosition="1202" endWordPosition="1205"> Event is estimated by means of Latent Dirichlet Allocation. Latent Dirichlet Allocation is a generative probabilistic model that allows multiple topics to occur in a document, and gets the topic distribution based on the idea that each topic emerges in a document based on a certain probability. Each topic is expressed as a multinomial distribution of words. In this study, since a topic is assigned to an Event, each topic is expressed as a multinomial distribution of Events. As a method to estimate a topic distribution, while a variational Bayes method (Blei et al., 2003) and its application (Teh et al., 2006) have been proposed, in this study we use Gibbs sampling method (Grififths and Steyvers, 2004). Furthermore, we define a sum of topic distributions of the events in a query as the topic distribution of the query. 4 Performance Evaluation Experiment Through a common document retrieval task, we compare our method with the conventional method and evaluate both of them. In concrete, we regard the documents which have a similar topic distribu31 tion to a query’s topic distribution as the result of retrieval, and then examine whether or not the estimated topic distribution can represent the latent s</context>
</contexts>
<marker>Teh, Newman, Welling, 2006</marker>
<rawString>Yee W. Teh, David Newman, and Max Welling. 2006. A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation. Advances in Neural Information Processing Systems Conference,19:1353– 1360.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>