<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006029">
<title confidence="0.965023">
Coreference Resolution with Reconcile
</title>
<note confidence="0.853024">
Veselin Stoyanov Claire Cardie Nathan Gilbert David Buttler
Center for Language Department of Ellen Riloff David Hysom
and Speech Processing Computer Science School of Computing Lawrence Livermore
Johns Hopkins Univ. Cornell University University of Utah National Laboratory
Baltimore, MD Ithaca, NY Salt Lake City, UT Livermore, CA
</note>
<email confidence="0.791968">
ves@cs.jhu.edu cardie@cs.cornell.edu ngilbert@cs.utah.edu buttler1@llnl.gov
riloff@cs.utah.edu hysom1@llnl.gov
</email>
<sectionHeader confidence="0.991755" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999769076923077">
Despite the existence of several noun phrase coref-
erence resolution data sets as well as several for-
mal evaluations on the task, it remains frustratingly
difficult to compare results across different corefer-
ence resolution systems. This is due to the high cost
of implementing a complete end-to-end coreference
resolution system, which often forces researchers
to substitute available gold-standard information in
lieu of implementing a module that would compute
that information. Unfortunately, this leads to incon-
sistent and often unrealistic evaluation scenarios.
With the aim to facilitate consistent and realis-
tic experimental evaluations in coreference resolu-
tion, we present Reconcile, an infrastructure for the
development of learning-based noun phrase (NP)
coreference resolution systems. Reconcile is de-
signed to facilitate the rapid creation of corefer-
ence resolution systems, easy implementation of
new feature sets and approaches to coreference res-
olution, and empirical evaluation of coreference re-
solvers across a variety of benchmark data sets and
standard scoring metrics. We describe Reconcile
and present experimental results showing that Rec-
oncile can be used to create a coreference resolver
that achieves performance comparable to state-of-
the-art systems on six benchmark data sets.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999344803278689">
Noun phrase coreference resolution (or simply
coreference resolution) is the problem of identi-
fying all noun phrases (NPs) that refer to the same
entity in a text. The problem of coreference res-
olution is fundamental in the field of natural lan-
guage processing (NLP) because of its usefulness
for other NLP tasks, as well as the theoretical in-
terest in understanding the computational mech-
anisms involved in government, binding and lin-
guistic reference.
Several formal evaluations have been conducted
for the coreference resolution task (e.g., MUC-6
(1995), ACE NIST (2004)), and the data sets cre-
ated for these evaluations have become standard
benchmarks in the field (e.g., MUC and ACE data
sets). However, it is still frustratingly difficult to
compare results across different coreference res-
olution systems. Reported coreference resolu-
tion scores vary wildly across data sets, evaluation
metrics, and system configurations.
We believe that one root cause of these dispar-
ities is the high cost of implementing an end-to-
end coreference resolution system. Coreference
resolution is a complex problem, and successful
systems must tackle a variety of non-trivial sub-
problems that are central to the coreference task —
e.g., mention/markable detection, anaphor identi-
fication — and that require substantial implemen-
tation efforts. As a result, many researchers ex-
ploit gold-standard annotations, when available, as
a substitute for component technologies to solve
these subproblems. For example, many published
research results use gold standard annotations to
identify NPs (substituting for mention/markable
detection), to distinguish anaphoric NPs from non-
anaphoric NPs (substituting for anaphoricity de-
termination), to identify named entities (substitut-
ing for named entity recognition), and to identify
the semantic types of NPs (substituting for seman-
tic class identification). Unfortunately, the use of
gold standard annotations for key/critical compo-
nent technologies leads to an unrealistic evalua-
tion setting, and makes it impossible to directly
compare results against coreference resolvers that
solve all of these subproblems from scratch.
Comparison of coreference resolvers is further
hindered by the use of several competing (and
non-trivial) evaluation measures, and data sets that
have substantially different task definitions and
annotation formats. Additionally, coreference res-
olution is a pervasive problem in NLP and many
NLP applications could benefit from an effective
coreference resolver that can be easily configured
and customized.
To address these issues, we have created a plat-
form for coreference resolution, called Reconcile,
that can serve as a software infrastructure to sup-
port the creation of, experimentation with, and
evaluation of coreference resolvers. Reconcile
was designed with the following seven desiderata
in mind:
</bodyText>
<listItem confidence="0.993351">
• implement the basic underlying software ar-
</listItem>
<page confidence="0.965239">
156
</page>
<note confidence="0.4882415">
Proceedings of the ACL 2010 Conference Short Papers, pages 156–161,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.698814666666667">
chitecture of contemporary state-of-the-art
learning-based coreference resolution sys-
tems;
</bodyText>
<listItem confidence="0.997251066666667">
• support experimentation on most of the stan-
dard coreference resolution data sets;
• implement most popular coreference resolu-
tion scoring metrics;
• exhibit state-of-the-art coreference resolution
performance (i.e., it can be configured to cre-
ate a resolver that achieves performance close
to the best reported results);
• can be easily extended with new methods and
features;
• is relatively fast and easy to configure and
run;
• has a set of pre-built resolvers that can be
used as black-box coreference resolution sys-
tems.
</listItem>
<bodyText confidence="0.99967325">
While several other coreference resolution sys-
tems are publicly available (e.g., Poesio and
Kabadjov (2004), Qiu et al. (2004) and Versley et
al. (2008)), none meets all seven of these desider-
ata (see Related Work). Reconcile is a modular
software platform that abstracts the basic archi-
tecture of most contemporary supervised learning-
based coreference resolution systems (e.g., Soon
et al. (2001), Ng and Cardie (2002), Bengtson and
Roth (2008)) and achieves performance compara-
ble to the state-of-the-art on several benchmark
data sets. Additionally, Reconcile can be eas-
ily reconfigured to use different algorithms, fea-
tures, preprocessing elements, evaluation settings
and metrics.
In the rest of this paper, we review related work
(Section 2), describe Reconcile’s organization and
components (Section 3) and show experimental re-
sults for Reconcile on six data sets and two evalu-
ation metrics (Section 4).
</bodyText>
<sectionHeader confidence="0.999572" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999888076923077">
Several coreference resolution systems are cur-
rently publicly available. JavaRap (Qiu et al.,
2004) is an implementation of the Lappin and
Leass’ (1994) Resolution of Anaphora Procedure
(RAP). JavaRap resolves only pronouns and, thus,
it is not directly comparable to Reconcile. GuiTaR
(Poesio and Kabadjov, 2004) and BART (Versley
et al., 2008) (which can be considered a succes-
sor of GuiTaR) are both modular systems that tar-
get the full coreference resolution task. As such,
both systems come close to meeting the majority
of the desiderata set forth in Section 1. BART,
in particular, can be considered an alternative to
Reconcile, although we believe that Reconcile’s
approach is more flexible than BART’s. In addi-
tion, the architecture and system components of
Reconcile (including a comprehensive set of fea-
tures that draw on the expertise of state-of-the-art
supervised learning approaches, such as Bengtson
and Roth (2008)) result in performance closer to
the state-of-the-art.
Coreference resolution has received much re-
search attention, resulting in an array of ap-
proaches, algorithms and features. Reconcile
is modeled after typical supervised learning ap-
proaches to coreference resolution (e.g. the archi-
tecture introduced by Soon et al. (2001)) because
of the popularity and relatively good performance
of these systems.
However, there have been other approaches
to coreference resolution, including unsupervised
and semi-supervised approaches (e.g. Haghighi
and Klein (2007)), structured approaches (e.g.
McCallum and Wellner (2004) and Finley and
Joachims (2005)), competition approaches (e.g.
Yang et al. (2003)) and a bell-tree search approach
(Luo et al. (2004)). Most of these approaches rely
on some notion of pairwise feature-based similar-
ity and can be directly implemented in Reconcile.
</bodyText>
<sectionHeader confidence="0.995895" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.999480133333333">
Reconcile was designed to be a research testbed
capable of implementing most current approaches
to coreference resolution. Reconcile is written in
Java, to be portable across platforms, and was de-
signed to be easily reconfigurable with respect to
subcomponents, feature sets, parameter settings,
etc.
Reconcile’s architecture is illustrated in Figure
1. For simplicity, Figure 1 shows Reconcile’s op-
eration during the classification phase (i.e., assum-
ing that a trained classifier is present).
The basic architecture of the system includes
five major steps. Starting with a corpus of docu-
ments together with a manually annotated corefer-
ence resolution answer key1, Reconcile performs
</bodyText>
<footnote confidence="0.970817">
1Only required during training.
</footnote>
<page confidence="0.986616">
157
</page>
<figureCaption confidence="0.999917">
Figure 1: The Reconcile classification architecture.
</figureCaption>
<bodyText confidence="0.897622">
the following steps, in order:
</bodyText>
<listItem confidence="0.971445586206896">
1. Preprocessing. All documents are passed
through a series of (external) linguistic pro-
cessors such as tokenizers, part-of-speech
taggers, syntactic parsers, etc. These com-
ponents produce annotations of the text. Ta-
ble 1 lists the preprocessors currently inter-
faced in Reconcile. Note that Reconcile in-
cludes several in-house NP detectors, that
conform to the different data sets’ defini-
tions of what constitutes a NP (e.g., MUC
vs. ACE). All of the extractors utilize a syn-
tactic parse of the text and the output of a
Named Entity (NE) extractor, but extract dif-
ferent constructs as specialized in the corre-
sponding definition. The NP extractors suc-
cessfully recognize about 95% of the NPs in
the MUC and ACE gold standards.
2. Feature generation. Using annotations pro-
duced during preprocessing, Reconcile pro-
duces feature vectors for pairs of NPs. For
example, a feature might denote whether the
two NPs agree in number, or whether they
have any words in common. Reconcile in-
cludes over 80 features, inspired by other suc-
cessful coreference resolution systems such
as Soon et al. (2001) and Ng and Cardie
(2002).
3. Classification. Reconcile learns a classifier
that operates on feature vectors representing
</listItem>
<table confidence="0.999454416666667">
Task Systems
Sentence UIUC (CC Group, 2009)
splitter OpenNLP (Baldridge, J., 2005)
Tokenizer OpenNLP (Baldridge, J., 2005)
POS OpenNLP (Baldridge, J., 2005)
Tagger + the two parsers below
Parser Stanford (Klein and Manning, 2003)
Berkeley (Petrov and Klein, 2007)
Dep. parser Stanford (Klein and Manning, 2003)
NE OpenNLP (Baldridge, J., 2005)
Recognizer Stanford (Finkel et al., 2005)
NP Detector In-house
</table>
<tableCaption confidence="0.92641">
Table 1: Preprocessing components available in
Reconcile.
</tableCaption>
<bodyText confidence="0.987878333333333">
pairs of NPs and it is trained to assign a score
indicating the likelihood that the NPs in the
pair are coreferent.
</bodyText>
<listItem confidence="0.855941">
4. Clustering. A clustering algorithm consoli-
dates the predictions output by the classifier
and forms the final set of coreference clusters
(chains).2
5. Scoring. Finally, during testing Reconcile
runs scoring algorithms that compare the
chains produced by the system to the gold-
standard chains in the answer key.
</listItem>
<bodyText confidence="0.855607">
Each of the five steps above can invoke differ-
ent components. Reconcile’s modularity makes it
</bodyText>
<footnote confidence="0.99747975">
2Some structured coreference resolution algorithms (e.g.,
McCallum and Wellner (2004) and Finley and Joachims
(2005)) combine the classification and clustering steps above.
Reconcile can easily accommodate this modification.
</footnote>
<page confidence="0.987826">
158
</page>
<table confidence="0.9997749">
Step Available modules
Classification various learners in the Weka toolkit
libSVM (Chang and Lin, 2001)
SVMlight (Joachims, 2002)
Clustering Single-link
Best-First
Most Recent First
Scoring MUC score (Vilain et al., 1995)
B3 score (Bagga and Baldwin, 1998)
CEAF score (Luo, 2005)
</table>
<tableCaption confidence="0.9804815">
Table 2: Available implementations for different
modules available in Reconcile.
</tableCaption>
<bodyText confidence="0.998129461538461">
easy for new components to be implemented and
existing ones to be removed or replaced. Recon-
cile’s standard distribution comes with a compre-
hensive set of implemented components – those
available for steps 2–5 are shown in Table 2. Rec-
oncile contains over 38,000 lines of original Java
code. Only about 15% of the code is concerned
with running existing components in the prepro-
cessing step, while the rest deals with NP extrac-
tion, implementations of features, clustering algo-
rithms and scorers. More details about Recon-
cile’s architecture and available components and
features can be found in Stoyanov et al. (2010).
</bodyText>
<sectionHeader confidence="0.998476" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.989554">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.9999889">
Reconcile incorporates the six most commonly
used coreference resolution data sets, two from the
MUC conferences (MUC-6, 1995; MUC-7, 1997)
and four from the ACE Program (NIST, 2004).
For ACE, we incorporate only the newswire por-
tion. When available, Reconcile employs the stan-
dard test/train split. Otherwise, we randomly split
the data into a training and test set following a
70/30 ratio. Performance is evaluated according
to the B3 and MUC scoring metrics.
</bodyText>
<subsectionHeader confidence="0.99548">
4.2 The Reconcile2010 Configuration
</subsectionHeader>
<bodyText confidence="0.999302777777778">
Reconcile can be easily configured with differ-
ent algorithms for markable detection, anaphoric-
ity determination, feature extraction, etc., and run
against several scoring metrics. For the purpose of
this sample evaluation, we create only one partic-
ular instantiation of Reconcile, which we will call
Reconcile2010 to differentiate it from the general
platform. Reconcile2010 is configured using the
following components:
</bodyText>
<listItem confidence="0.840754428571428">
1. Preprocessing
(a) Sentence Splitter: OpenNLP
(b) Tokenizer: OpenNLP
(c) POS Tagger: OpenNLP
(d) Parser: Berkeley
(e) Named Entity Recognizer: Stanford
2. Feature Set - A hand-selected subset of 60 out of the
more than 80 features available. The features were se-
lected to include most of the features from Soon et al.
Soon et al. (2001), Ng and Cardie (2002) and Bengtson
and Roth (2008).
3. Classifier - Averaged Perceptron
4. Clustering - Single-link - Positive decision threshold
was tuned by cross validation of the training set.
</listItem>
<subsectionHeader confidence="0.999421">
4.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999605882352941">
The first two rows of Table 3 show the perfor-
mance of Reconcile2010. For all data sets, B3
scores are higher than MUC scores. The MUC
score is highest for the MUC6 data set, while B3
scores are higher for the ACE data sets as com-
pared to the MUC data sets.
Due to the difficulties outlined in Section 1,
results for Reconcile presented here are directly
comparable only to a limited number of scores
reported in the literature. The bottom three
rows of Table 3 list these comparable scores,
which show that Reconcile2010 exhibits state-of-
the-art performance for supervised learning-based
coreference resolvers. A more detailed study of
Reconcile-based coreference resolution systems
in different evaluation scenarios can be found in
Stoyanov et al. (2009).
</bodyText>
<sectionHeader confidence="0.997159" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999909772727273">
Reconcile is a general architecture for coreference
resolution that can be used to easily create various
coreference resolvers. Reconcile provides broad
support for experimentation in coreference reso-
lution, including implementation of the basic ar-
chitecture of contemporary state-of-the-art coref-
erence systems and a variety of individual mod-
ules employed in these systems. Additionally,
Reconcile handles all of the formatting and scor-
ing peculiarities of the most widely used coref-
erence resolution data sets (those created as part
of the MUC and ACE conferences) and, thus,
allows for easy implementation and evaluation
across these data sets. We hope that Reconcile
will support experimental research in coreference
resolution and provide a state-of-the-art corefer-
ence resolver for both researchers and application
developers. We believe that in this way Recon-
cile will facilitate meaningful and consistent com-
parisons of coreference resolution systems. The
full Reconcile release is available for download at
http://www.cs.utah.edu/nlp/reconcile/.
</bodyText>
<page confidence="0.99669">
159
</page>
<table confidence="0.998446714285714">
System Score Data sets
MUC6 MUC7 ACE-2 ACE03 ACE04 ACE05
Reconcile2010 MUC 68.50 62.80 65.99 67.87 62.03 67.41
B3 70.88 65.86 78.29 79.39 76.50 73.71
Soon et al. (2001) MUC 62.6 60.4 – – – –
Ng and Cardie (2002) MUC 70.4 63.4 – – – –
Yang et al. (2003) MUC 71.3 60.2 – – – –
</table>
<tableCaption confidence="0.97299">
Table 3: Scores for Reconcile on six data sets and scores for comparable coreference systems.
</tableCaption>
<sectionHeader confidence="0.987894" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.8858364">
This research was supported in part by the Na-
tional Science Foundation under Grant # 0937060
to the Computing Research Association for the
CIFellows Project, Lawrence Livermore National
Laboratory subcontract B573245, Department of
Homeland Security Grant N0014-07-1-0152, and
Air Force Contract FA8750-09-C-0172 under the
DARPA Machine Reading Program.
The authors would like to thank the anonymous
reviewers for their useful comments.
</bodyText>
<sectionHeader confidence="0.99775" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998506693333333">
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In Linguistic Coreference Workshop
at the Language Resources and Evaluation Conference.
Baldridge, J. 2005. The OpenNLP project.
http://opennlp.sourceforge.net/.
E. Bengtson and D. Roth. 2008. Understanding the value of
features for coreference resolution. In Proceedings of the
2008 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
CC Group. 2009. Sentence Segmentation Tool.
http://l2r.cs.uiuc.edu/ cogcomp/atool.php?tkey=SS.
C. Chang and C. Lin. 2001. LIBSVM: a Li-
brary for Support Vector Machines. Available at
http://www.csie.ntu.edu.tw/cjlin/libsvm.
J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating
Non-local Information into Information Extraction Sys-
tems by Gibbs Sampling. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics and
44th Annual Meeting of the ACL.
T. Finley and T. Joachims. 2005. Supervised clustering with
support vector machines. In Proceedings of the Twenty-
second International Conference on Machine Learning
(ICML 2005).
A. Haghighi and D. Klein. 2007. Unsupervised Coreference
Resolution in a Nonparametric Bayesian Model. In Pro-
ceedings of the 45th Annual Meeting of the ACL.
T. Joachims. 2002. SVMLi,ht, http://svmlight.joachims.org.
D. Klein and C. Manning. 2003. Fast Exact Inference with
a Factored Model for Natural Language Parsing. In Ad-
vances in Neural Information Processing (NIPS 2003).
S. Lappin and H. Leass. 1994. An algorithm for pronom-
inal anaphora resolution. Computational Linguistics,
20(4):535–561.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous coreference
resolution algorithm based on the bell tree. In Proceed-
ings of the 42nd Annual Meeting of the ACL.
X. Luo. 2005. On Coreference Resolution Performance
Metrics. In Proceedings of Human Language Technology
Conference and Conference on Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP).
A. McCallum and B. Wellner. 2004. Conditional Models
of Identity Uncertainty with Application to Noun Coref-
erence. In Advances in Neural Information Processing
(NIPS 2004).
MUC-6. 1995. Coreference Task Definition. In Proceedings
of the Sixth Message Understanding Conference (MUC-
6).
MUC-7. 1997. Coreference Task Definition. In Proceed-
ings of the Seventh Message Understanding Conference
(MUC-7).
V. Ng and C. Cardie. 2002. Improving Machine Learning
Approaches to Coreference Resolution. In Proceedings of
the 40th Annual Meeting of the ACL.
NIST. 2004. The ACE Evaluation Plan. NIST.
S. Petrov and D. Klein. 2007. Improved Inference for Un-
lexicalized Parsing. In Proceedings of the Joint Meeting
of the Human Language Technology Conference and the
North American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL 2007).
M. Poesio and M. Kabadjov. 2004. A general-purpose,
off-the-shelf anaphora resolution module: implementation
and preliminary evaluation. In Proceedings of the Lan-
guage Resources and Evaluation Conference.
L. Qiu, M.-Y. Kan, and T.-S. Chua. 2004. A public reference
implementation of the rap anaphora resolution algorithm.
In Proceedings of the Language Resources and Evaluation
Conference.
W. Soon, H. Ng, and D. Lim. 2001. A Machine Learning Ap-
proach to Coreference of Noun Phrases. Computational
Linguistics, 27(4):521–541.
V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009. Co-
nundrums in noun phrase coreference resolution: Mak-
ing sense of the state-of-the-art. In Proceedings of
ACL/IJCNLP.
</reference>
<page confidence="0.968599">
160
</page>
<reference confidence="0.9982254">
V. Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler, and
D. Hysom. 2010. Reconcile: A coreference resolution
research platform. Technical report, Cornell University.
Y. Versley, S. Ponzetto, M. Poesio, V. Eidelman, A. Jern,
J. Smith, X. Yang, and A. Moschitti. 2008. BART: A
modular toolkit for coreference resolution. In Proceed-
ings of the Language Resources and Evaluation Confer-
ence.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A Model-Theoretic Coreference
Scoring Theme. In Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6).
X. Yang, G. Zhou, J. Su, and C. Tan. 2003. Coreference
resolution using competition learning approach. In Pro-
ceedings of the 41st Annual Meeting of the ACL.
</reference>
<page confidence="0.998199">
161
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.406393">
<title confidence="0.999785">Coreference Resolution with Reconcile</title>
<author confidence="0.992865">Veselin Stoyanov Claire Cardie Nathan Gilbert David Buttler</author>
<affiliation confidence="0.808862666666667">Center for Language Department of Ellen Riloff David Hysom and Speech Processing Computer Science School of Computing Lawrence Livermore Johns Hopkins Univ. Cornell University University of Utah National Laboratory</affiliation>
<address confidence="0.942896">Baltimore, MD Ithaca, NY Salt Lake City, UT Livermore, CA</address>
<email confidence="0.840626">ves@cs.jhu.educardie@cs.cornell.edungilbert@cs.utah.eduriloff@cs.utah.edubuttler1@llnl.govhysom1@llnl.gov</email>
<abstract confidence="0.999422703703704">Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference resolution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Linguistic Coreference Workshop at the Language Resources and Evaluation Conference.</booktitle>
<contexts>
<context position="11736" citStr="Bagga and Baldwin, 1998" startWordPosition="1727" endWordPosition="1730">o the goldstandard chains in the answer key. Each of the five steps above can invoke different components. Reconcile’s modularity makes it 2Some structured coreference resolution algorithms (e.g., McCallum and Wellner (2004) and Finley and Joachims (2005)) combine the classification and clustering steps above. Reconcile can easily accommodate this modification. 158 Step Available modules Classification various learners in the Weka toolkit libSVM (Chang and Lin, 2001) SVMlight (Joachims, 2002) Clustering Single-link Best-First Most Recent First Scoring MUC score (Vilain et al., 1995) B3 score (Bagga and Baldwin, 1998) CEAF score (Luo, 2005) Table 2: Available implementations for different modules available in Reconcile. easy for new components to be implemented and existing ones to be removed or replaced. Reconcile’s standard distribution comes with a comprehensive set of implemented components – those available for steps 2–5 are shown in Table 2. Reconcile contains over 38,000 lines of original Java code. Only about 15% of the code is concerned with running existing components in the preprocessing step, while the rest deals with NP extraction, implementations of features, clustering algorithms and scorers</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Algorithms for scoring coreference chains. In Linguistic Coreference Workshop at the Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Baldridge</author>
</authors>
<date>2005</date>
<journal>The OpenNLP</journal>
<note>project. http://opennlp.sourceforge.net/.</note>
<marker>Baldridge, 2005</marker>
<rawString>Baldridge, J. 2005. The OpenNLP project. http://opennlp.sourceforge.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bengtson</author>
<author>D Roth</author>
</authors>
<title>Understanding the value of features for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="5933" citStr="Bengtson and Roth (2008)" startWordPosition="852" endWordPosition="855">ith new methods and features; • is relatively fast and easy to configure and run; • has a set of pre-built resolvers that can be used as black-box coreference resolution systems. While several other coreference resolution systems are publicly available (e.g., Poesio and Kabadjov (2004), Qiu et al. (2004) and Versley et al. (2008)), none meets all seven of these desiderata (see Related Work). Reconcile is a modular software platform that abstracts the basic architecture of most contemporary supervised learningbased coreference resolution systems (e.g., Soon et al. (2001), Ng and Cardie (2002), Bengtson and Roth (2008)) and achieves performance comparable to the state-of-the-art on several benchmark data sets. Additionally, Reconcile can be easily reconfigured to use different algorithms, features, preprocessing elements, evaluation settings and metrics. In the rest of this paper, we review related work (Section 2), describe Reconcile’s organization and components (Section 3) and show experimental results for Reconcile on six data sets and two evaluation metrics (Section 4). 2 Related Work Several coreference resolution systems are currently publicly available. JavaRap (Qiu et al., 2004) is an implementatio</context>
<context position="7345" citStr="Bengtson and Roth (2008)" startWordPosition="1069" endWordPosition="1072"> 2004) and BART (Versley et al., 2008) (which can be considered a successor of GuiTaR) are both modular systems that target the full coreference resolution task. As such, both systems come close to meeting the majority of the desiderata set forth in Section 1. BART, in particular, can be considered an alternative to Reconcile, although we believe that Reconcile’s approach is more flexible than BART’s. In addition, the architecture and system components of Reconcile (including a comprehensive set of features that draw on the expertise of state-of-the-art supervised learning approaches, such as Bengtson and Roth (2008)) result in performance closer to the state-of-the-art. Coreference resolution has received much research attention, resulting in an array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum an</context>
<context position="13795" citStr="Bengtson and Roth (2008)" startWordPosition="2049" endWordPosition="2052">e purpose of this sample evaluation, we create only one particular instantiation of Reconcile, which we will call Reconcile2010 to differentiate it from the general platform. Reconcile2010 is configured using the following components: 1. Preprocessing (a) Sentence Splitter: OpenNLP (b) Tokenizer: OpenNLP (c) POS Tagger: OpenNLP (d) Parser: Berkeley (e) Named Entity Recognizer: Stanford 2. Feature Set - A hand-selected subset of 60 out of the more than 80 features available. The features were selected to include most of the features from Soon et al. Soon et al. (2001), Ng and Cardie (2002) and Bengtson and Roth (2008). 3. Classifier - Averaged Perceptron 4. Clustering - Single-link - Positive decision threshold was tuned by cross validation of the training set. 4.3 Experimental Results The first two rows of Table 3 show the performance of Reconcile2010. For all data sets, B3 scores are higher than MUC scores. The MUC score is highest for the MUC6 data set, while B3 scores are higher for the ACE data sets as compared to the MUC data sets. Due to the difficulties outlined in Section 1, results for Reconcile presented here are directly comparable only to a limited number of scores reported in the literature. </context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>E. Bengtson and D. Roth. 2008. Understanding the value of features for coreference resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>CC Group</author>
</authors>
<title>Sentence Segmentation Tool.</title>
<date>2009</date>
<note>http://l2r.cs.uiuc.edu/ cogcomp/atool.php?tkey=SS.</note>
<contexts>
<context position="10305" citStr="Group, 2009" startWordPosition="1518" endWordPosition="1519">tors successfully recognize about 95% of the NPs in the MUC and ACE gold standards. 2. Feature generation. Using annotations produced during preprocessing, Reconcile produces feature vectors for pairs of NPs. For example, a feature might denote whether the two NPs agree in number, or whether they have any words in common. Reconcile includes over 80 features, inspired by other successful coreference resolution systems such as Soon et al. (2001) and Ng and Cardie (2002). 3. Classification. Reconcile learns a classifier that operates on feature vectors representing Task Systems Sentence UIUC (CC Group, 2009) splitter OpenNLP (Baldridge, J., 2005) Tokenizer OpenNLP (Baldridge, J., 2005) POS OpenNLP (Baldridge, J., 2005) Tagger + the two parsers below Parser Stanford (Klein and Manning, 2003) Berkeley (Petrov and Klein, 2007) Dep. parser Stanford (Klein and Manning, 2003) NE OpenNLP (Baldridge, J., 2005) Recognizer Stanford (Finkel et al., 2005) NP Detector In-house Table 1: Preprocessing components available in Reconcile. pairs of NPs and it is trained to assign a score indicating the likelihood that the NPs in the pair are coreferent. 4. Clustering. A clustering algorithm consolidates the predict</context>
</contexts>
<marker>Group, 2009</marker>
<rawString>CC Group. 2009. Sentence Segmentation Tool. http://l2r.cs.uiuc.edu/ cogcomp/atool.php?tkey=SS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chang</author>
<author>C Lin</author>
</authors>
<title>LIBSVM: a Library for Support Vector Machines. Available at http://www.csie.ntu.edu.tw/cjlin/libsvm.</title>
<date>2001</date>
<contexts>
<context position="11583" citStr="Chang and Lin, 2001" startWordPosition="1705" endWordPosition="1708">oreference clusters (chains).2 5. Scoring. Finally, during testing Reconcile runs scoring algorithms that compare the chains produced by the system to the goldstandard chains in the answer key. Each of the five steps above can invoke different components. Reconcile’s modularity makes it 2Some structured coreference resolution algorithms (e.g., McCallum and Wellner (2004) and Finley and Joachims (2005)) combine the classification and clustering steps above. Reconcile can easily accommodate this modification. 158 Step Available modules Classification various learners in the Weka toolkit libSVM (Chang and Lin, 2001) SVMlight (Joachims, 2002) Clustering Single-link Best-First Most Recent First Scoring MUC score (Vilain et al., 1995) B3 score (Bagga and Baldwin, 1998) CEAF score (Luo, 2005) Table 2: Available implementations for different modules available in Reconcile. easy for new components to be implemented and existing ones to be removed or replaced. Reconcile’s standard distribution comes with a comprehensive set of implemented components – those available for steps 2–5 are shown in Table 2. Reconcile contains over 38,000 lines of original Java code. Only about 15% of the code is concerned with runni</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>C. Chang and C. Lin. 2001. LIBSVM: a Library for Support Vector Machines. Available at http://www.csie.ntu.edu.tw/cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="10647" citStr="Finkel et al., 2005" startWordPosition="1566" endWordPosition="1569">includes over 80 features, inspired by other successful coreference resolution systems such as Soon et al. (2001) and Ng and Cardie (2002). 3. Classification. Reconcile learns a classifier that operates on feature vectors representing Task Systems Sentence UIUC (CC Group, 2009) splitter OpenNLP (Baldridge, J., 2005) Tokenizer OpenNLP (Baldridge, J., 2005) POS OpenNLP (Baldridge, J., 2005) Tagger + the two parsers below Parser Stanford (Klein and Manning, 2003) Berkeley (Petrov and Klein, 2007) Dep. parser Stanford (Klein and Manning, 2003) NE OpenNLP (Baldridge, J., 2005) Recognizer Stanford (Finkel et al., 2005) NP Detector In-house Table 1: Preprocessing components available in Reconcile. pairs of NPs and it is trained to assign a score indicating the likelihood that the NPs in the pair are coreferent. 4. Clustering. A clustering algorithm consolidates the predictions output by the classifier and forms the final set of coreference clusters (chains).2 5. Scoring. Finally, during testing Reconcile runs scoring algorithms that compare the chains produced by the system to the goldstandard chains in the answer key. Each of the five steps above can invoke different components. Reconcile’s modularity makes</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Finley</author>
<author>T Joachims</author>
</authors>
<title>Supervised clustering with support vector machines.</title>
<date>2005</date>
<booktitle>In Proceedings of the Twentysecond International Conference on Machine Learning (ICML</booktitle>
<contexts>
<context position="7992" citStr="Finley and Joachims (2005)" startWordPosition="1158" endWordPosition="1161">ce closer to the state-of-the-art. Coreference resolution has received much research attention, resulting in an array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum and Wellner (2004) and Finley and Joachims (2005)), competition approaches (e.g. Yang et al. (2003)) and a bell-tree search approach (Luo et al. (2004)). Most of these approaches rely on some notion of pairwise feature-based similarity and can be directly implemented in Reconcile. 3 System Description Reconcile was designed to be a research testbed capable of implementing most current approaches to coreference resolution. Reconcile is written in Java, to be portable across platforms, and was designed to be easily reconfigurable with respect to subcomponents, feature sets, parameter settings, etc. Reconcile’s architecture is illustrated in Fi</context>
<context position="11367" citStr="Finley and Joachims (2005)" startWordPosition="1676" endWordPosition="1679">nd it is trained to assign a score indicating the likelihood that the NPs in the pair are coreferent. 4. Clustering. A clustering algorithm consolidates the predictions output by the classifier and forms the final set of coreference clusters (chains).2 5. Scoring. Finally, during testing Reconcile runs scoring algorithms that compare the chains produced by the system to the goldstandard chains in the answer key. Each of the five steps above can invoke different components. Reconcile’s modularity makes it 2Some structured coreference resolution algorithms (e.g., McCallum and Wellner (2004) and Finley and Joachims (2005)) combine the classification and clustering steps above. Reconcile can easily accommodate this modification. 158 Step Available modules Classification various learners in the Weka toolkit libSVM (Chang and Lin, 2001) SVMlight (Joachims, 2002) Clustering Single-link Best-First Most Recent First Scoring MUC score (Vilain et al., 1995) B3 score (Bagga and Baldwin, 1998) CEAF score (Luo, 2005) Table 2: Available implementations for different modules available in Reconcile. easy for new components to be implemented and existing ones to be removed or replaced. Reconcile’s standard distribution comes</context>
</contexts>
<marker>Finley, Joachims, 2005</marker>
<rawString>T. Finley and T. Joachims. 2005. Supervised clustering with support vector machines. In Proceedings of the Twentysecond International Conference on Machine Learning (ICML 2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Unsupervised Coreference Resolution in a Nonparametric Bayesian Model.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the</booktitle>
<note>SVMLi,ht, http://svmlight.joachims.org.</note>
<contexts>
<context position="7903" citStr="Haghighi and Klein (2007)" startWordPosition="1146" endWordPosition="1149">rt supervised learning approaches, such as Bengtson and Roth (2008)) result in performance closer to the state-of-the-art. Coreference resolution has received much research attention, resulting in an array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum and Wellner (2004) and Finley and Joachims (2005)), competition approaches (e.g. Yang et al. (2003)) and a bell-tree search approach (Luo et al. (2004)). Most of these approaches rely on some notion of pairwise feature-based similarity and can be directly implemented in Reconcile. 3 System Description Reconcile was designed to be a research testbed capable of implementing most current approaches to coreference resolution. Reconcile is written in Java, to be portable across platforms, and was designed to be easily reconfigurable with respect to subcompone</context>
</contexts>
<marker>Haghighi, Klein, 2007</marker>
<rawString>A. Haghighi and D. Klein. 2007. Unsupervised Coreference Resolution in a Nonparametric Bayesian Model. In Proceedings of the 45th Annual Meeting of the ACL. T. Joachims. 2002. SVMLi,ht, http://svmlight.joachims.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Fast Exact Inference with a Factored Model for Natural Language Parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing (NIPS</booktitle>
<contexts>
<context position="10491" citStr="Klein and Manning, 2003" startWordPosition="1543" endWordPosition="1546"> feature vectors for pairs of NPs. For example, a feature might denote whether the two NPs agree in number, or whether they have any words in common. Reconcile includes over 80 features, inspired by other successful coreference resolution systems such as Soon et al. (2001) and Ng and Cardie (2002). 3. Classification. Reconcile learns a classifier that operates on feature vectors representing Task Systems Sentence UIUC (CC Group, 2009) splitter OpenNLP (Baldridge, J., 2005) Tokenizer OpenNLP (Baldridge, J., 2005) POS OpenNLP (Baldridge, J., 2005) Tagger + the two parsers below Parser Stanford (Klein and Manning, 2003) Berkeley (Petrov and Klein, 2007) Dep. parser Stanford (Klein and Manning, 2003) NE OpenNLP (Baldridge, J., 2005) Recognizer Stanford (Finkel et al., 2005) NP Detector In-house Table 1: Preprocessing components available in Reconcile. pairs of NPs and it is trained to assign a score indicating the likelihood that the NPs in the pair are coreferent. 4. Clustering. A clustering algorithm consolidates the predictions output by the classifier and forms the final set of coreference clusters (chains).2 5. Scoring. Finally, during testing Reconcile runs scoring algorithms that compare the chains pro</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. Fast Exact Inference with a Factored Model for Natural Language Parsing. In Advances in Neural Information Processing (NIPS 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lappin</author>
<author>H Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<marker>Lappin, Leass, 1994</marker>
<rawString>S. Lappin and H. Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20(4):535–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>S Roukos</author>
</authors>
<title>A mention-synchronous coreference resolution algorithm based on the bell tree.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="8094" citStr="Luo et al. (2004)" startWordPosition="1174" endWordPosition="1177"> array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum and Wellner (2004) and Finley and Joachims (2005)), competition approaches (e.g. Yang et al. (2003)) and a bell-tree search approach (Luo et al. (2004)). Most of these approaches rely on some notion of pairwise feature-based similarity and can be directly implemented in Reconcile. 3 System Description Reconcile was designed to be a research testbed capable of implementing most current approaches to coreference resolution. Reconcile is written in Java, to be portable across platforms, and was designed to be easily reconfigurable with respect to subcomponents, feature sets, parameter settings, etc. Reconcile’s architecture is illustrated in Figure 1. For simplicity, Figure 1 shows Reconcile’s operation during the classification phase (i.e., as</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos. 2004. A mention-synchronous coreference resolution algorithm based on the bell tree. In Proceedings of the 42nd Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
</authors>
<title>On Coreference Resolution Performance Metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP).</booktitle>
<contexts>
<context position="11759" citStr="Luo, 2005" startWordPosition="1733" endWordPosition="1734">er key. Each of the five steps above can invoke different components. Reconcile’s modularity makes it 2Some structured coreference resolution algorithms (e.g., McCallum and Wellner (2004) and Finley and Joachims (2005)) combine the classification and clustering steps above. Reconcile can easily accommodate this modification. 158 Step Available modules Classification various learners in the Weka toolkit libSVM (Chang and Lin, 2001) SVMlight (Joachims, 2002) Clustering Single-link Best-First Most Recent First Scoring MUC score (Vilain et al., 1995) B3 score (Bagga and Baldwin, 1998) CEAF score (Luo, 2005) Table 2: Available implementations for different modules available in Reconcile. easy for new components to be implemented and existing ones to be removed or replaced. Reconcile’s standard distribution comes with a comprehensive set of implemented components – those available for steps 2–5 are shown in Table 2. Reconcile contains over 38,000 lines of original Java code. Only about 15% of the code is concerned with running existing components in the preprocessing step, while the rest deals with NP extraction, implementations of features, clustering algorithms and scorers. More details about Re</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>X. Luo. 2005. On Coreference Resolution Performance Metrics. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Conditional Models of Identity Uncertainty with Application to Noun Coreference.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing (NIPS</booktitle>
<contexts>
<context position="7961" citStr="McCallum and Wellner (2004)" startWordPosition="1153" endWordPosition="1156">Roth (2008)) result in performance closer to the state-of-the-art. Coreference resolution has received much research attention, resulting in an array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum and Wellner (2004) and Finley and Joachims (2005)), competition approaches (e.g. Yang et al. (2003)) and a bell-tree search approach (Luo et al. (2004)). Most of these approaches rely on some notion of pairwise feature-based similarity and can be directly implemented in Reconcile. 3 System Description Reconcile was designed to be a research testbed capable of implementing most current approaches to coreference resolution. Reconcile is written in Java, to be portable across platforms, and was designed to be easily reconfigurable with respect to subcomponents, feature sets, parameter settings, etc. Reconcile’s ar</context>
<context position="11336" citStr="McCallum and Wellner (2004)" startWordPosition="1671" endWordPosition="1674">ble in Reconcile. pairs of NPs and it is trained to assign a score indicating the likelihood that the NPs in the pair are coreferent. 4. Clustering. A clustering algorithm consolidates the predictions output by the classifier and forms the final set of coreference clusters (chains).2 5. Scoring. Finally, during testing Reconcile runs scoring algorithms that compare the chains produced by the system to the goldstandard chains in the answer key. Each of the five steps above can invoke different components. Reconcile’s modularity makes it 2Some structured coreference resolution algorithms (e.g., McCallum and Wellner (2004) and Finley and Joachims (2005)) combine the classification and clustering steps above. Reconcile can easily accommodate this modification. 158 Step Available modules Classification various learners in the Weka toolkit libSVM (Chang and Lin, 2001) SVMlight (Joachims, 2002) Clustering Single-link Best-First Most Recent First Scoring MUC score (Vilain et al., 1995) B3 score (Bagga and Baldwin, 1998) CEAF score (Luo, 2005) Table 2: Available implementations for different modules available in Reconcile. easy for new components to be implemented and existing ones to be removed or replaced. Reconcil</context>
</contexts>
<marker>McCallum, Wellner, 2004</marker>
<rawString>A. McCallum and B. Wellner. 2004. Conditional Models of Identity Uncertainty with Application to Noun Coreference. In Advances in Neural Information Processing (NIPS 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-6</author>
</authors>
<title>Coreference Task Definition.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC6).</booktitle>
<marker>MUC-6, 1995</marker>
<rawString>MUC-6. 1995. Coreference Task Definition. In Proceedings of the Sixth Message Understanding Conference (MUC6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-7</author>
</authors>
<title>Coreference Task Definition.</title>
<date>1997</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference (MUC-7).</booktitle>
<marker>MUC-7, 1997</marker>
<rawString>MUC-7. 1997. Coreference Task Definition. In Proceedings of the Seventh Message Understanding Conference (MUC-7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving Machine Learning Approaches to Coreference Resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="5907" citStr="Ng and Cardie (2002)" startWordPosition="848" endWordPosition="851">n be easily extended with new methods and features; • is relatively fast and easy to configure and run; • has a set of pre-built resolvers that can be used as black-box coreference resolution systems. While several other coreference resolution systems are publicly available (e.g., Poesio and Kabadjov (2004), Qiu et al. (2004) and Versley et al. (2008)), none meets all seven of these desiderata (see Related Work). Reconcile is a modular software platform that abstracts the basic architecture of most contemporary supervised learningbased coreference resolution systems (e.g., Soon et al. (2001), Ng and Cardie (2002), Bengtson and Roth (2008)) and achieves performance comparable to the state-of-the-art on several benchmark data sets. Additionally, Reconcile can be easily reconfigured to use different algorithms, features, preprocessing elements, evaluation settings and metrics. In the rest of this paper, we review related work (Section 2), describe Reconcile’s organization and components (Section 3) and show experimental results for Reconcile on six data sets and two evaluation metrics (Section 4). 2 Related Work Several coreference resolution systems are currently publicly available. JavaRap (Qiu et al.,</context>
<context position="10165" citStr="Ng and Cardie (2002)" startWordPosition="1497" endWordPosition="1500">text and the output of a Named Entity (NE) extractor, but extract different constructs as specialized in the corresponding definition. The NP extractors successfully recognize about 95% of the NPs in the MUC and ACE gold standards. 2. Feature generation. Using annotations produced during preprocessing, Reconcile produces feature vectors for pairs of NPs. For example, a feature might denote whether the two NPs agree in number, or whether they have any words in common. Reconcile includes over 80 features, inspired by other successful coreference resolution systems such as Soon et al. (2001) and Ng and Cardie (2002). 3. Classification. Reconcile learns a classifier that operates on feature vectors representing Task Systems Sentence UIUC (CC Group, 2009) splitter OpenNLP (Baldridge, J., 2005) Tokenizer OpenNLP (Baldridge, J., 2005) POS OpenNLP (Baldridge, J., 2005) Tagger + the two parsers below Parser Stanford (Klein and Manning, 2003) Berkeley (Petrov and Klein, 2007) Dep. parser Stanford (Klein and Manning, 2003) NE OpenNLP (Baldridge, J., 2005) Recognizer Stanford (Finkel et al., 2005) NP Detector In-house Table 1: Preprocessing components available in Reconcile. pairs of NPs and it is trained to assi</context>
<context position="13766" citStr="Ng and Cardie (2002)" startWordPosition="2044" endWordPosition="2047">l scoring metrics. For the purpose of this sample evaluation, we create only one particular instantiation of Reconcile, which we will call Reconcile2010 to differentiate it from the general platform. Reconcile2010 is configured using the following components: 1. Preprocessing (a) Sentence Splitter: OpenNLP (b) Tokenizer: OpenNLP (c) POS Tagger: OpenNLP (d) Parser: Berkeley (e) Named Entity Recognizer: Stanford 2. Feature Set - A hand-selected subset of 60 out of the more than 80 features available. The features were selected to include most of the features from Soon et al. Soon et al. (2001), Ng and Cardie (2002) and Bengtson and Roth (2008). 3. Classifier - Averaged Perceptron 4. Clustering - Single-link - Positive decision threshold was tuned by cross validation of the training set. 4.3 Experimental Results The first two rows of Table 3 show the performance of Reconcile2010. For all data sets, B3 scores are higher than MUC scores. The MUC score is highest for the MUC6 data set, while B3 scores are higher for the ACE data sets as compared to the MUC data sets. Due to the difficulties outlined in Section 1, results for Reconcile presented here are directly comparable only to a limited number of scores</context>
<context position="16008" citStr="Ng and Cardie (2002)" startWordPosition="2389" endWordPosition="2392"> hope that Reconcile will support experimental research in coreference resolution and provide a state-of-the-art coreference resolver for both researchers and application developers. We believe that in this way Reconcile will facilitate meaningful and consistent comparisons of coreference resolution systems. The full Reconcile release is available for download at http://www.cs.utah.edu/nlp/reconcile/. 159 System Score Data sets MUC6 MUC7 ACE-2 ACE03 ACE04 ACE05 Reconcile2010 MUC 68.50 62.80 65.99 67.87 62.03 67.41 B3 70.88 65.86 78.29 79.39 76.50 73.71 Soon et al. (2001) MUC 62.6 60.4 – – – – Ng and Cardie (2002) MUC 70.4 63.4 – – – – Yang et al. (2003) MUC 71.3 60.2 – – – – Table 3: Scores for Reconcile on six data sets and scores for comparable coreference systems. Acknowledgments This research was supported in part by the National Science Foundation under Grant # 0937060 to the Computing Research Association for the CIFellows Project, Lawrence Livermore National Laboratory subcontract B573245, Department of Homeland Security Grant N0014-07-1-0152, and Air Force Contract FA8750-09-C-0172 under the DARPA Machine Reading Program. The authors would like to thank the anonymous reviewers for their useful</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002. Improving Machine Learning Approaches to Coreference Resolution. In Proceedings of the 40th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The ACE Evaluation Plan.</title>
<date>2004</date>
<publisher>NIST.</publisher>
<contexts>
<context position="2384" citStr="NIST (2004)" startWordPosition="336" endWordPosition="337">x benchmark data sets. 1 Introduction Noun phrase coreference resolution (or simply coreference resolution) is the problem of identifying all noun phrases (NPs) that refer to the same entity in a text. The problem of coreference resolution is fundamental in the field of natural language processing (NLP) because of its usefulness for other NLP tasks, as well as the theoretical interest in understanding the computational mechanisms involved in government, binding and linguistic reference. Several formal evaluations have been conducted for the coreference resolution task (e.g., MUC-6 (1995), ACE NIST (2004)), and the data sets created for these evaluations have become standard benchmarks in the field (e.g., MUC and ACE data sets). However, it is still frustratingly difficult to compare results across different coreference resolution systems. Reported coreference resolution scores vary wildly across data sets, evaluation metrics, and system configurations. We believe that one root cause of these disparities is the high cost of implementing an end-toend coreference resolution system. Coreference resolution is a complex problem, and successful systems must tackle a variety of non-trivial subproblem</context>
<context position="12669" citStr="NIST, 2004" startWordPosition="1877" endWordPosition="1878">wn in Table 2. Reconcile contains over 38,000 lines of original Java code. Only about 15% of the code is concerned with running existing components in the preprocessing step, while the rest deals with NP extraction, implementations of features, clustering algorithms and scorers. More details about Reconcile’s architecture and available components and features can be found in Stoyanov et al. (2010). 4 Evaluation 4.1 Data Sets Reconcile incorporates the six most commonly used coreference resolution data sets, two from the MUC conferences (MUC-6, 1995; MUC-7, 1997) and four from the ACE Program (NIST, 2004). For ACE, we incorporate only the newswire portion. When available, Reconcile employs the standard test/train split. Otherwise, we randomly split the data into a training and test set following a 70/30 ratio. Performance is evaluated according to the B3 and MUC scoring metrics. 4.2 The Reconcile2010 Configuration Reconcile can be easily configured with different algorithms for markable detection, anaphoricity determination, feature extraction, etc., and run against several scoring metrics. For the purpose of this sample evaluation, we create only one particular instantiation of Reconcile, whi</context>
</contexts>
<marker>NIST, 2004</marker>
<rawString>NIST. 2004. The ACE Evaluation Plan. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Meeting of the Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL</booktitle>
<contexts>
<context position="10525" citStr="Petrov and Klein, 2007" startWordPosition="1548" endWordPosition="1551">For example, a feature might denote whether the two NPs agree in number, or whether they have any words in common. Reconcile includes over 80 features, inspired by other successful coreference resolution systems such as Soon et al. (2001) and Ng and Cardie (2002). 3. Classification. Reconcile learns a classifier that operates on feature vectors representing Task Systems Sentence UIUC (CC Group, 2009) splitter OpenNLP (Baldridge, J., 2005) Tokenizer OpenNLP (Baldridge, J., 2005) POS OpenNLP (Baldridge, J., 2005) Tagger + the two parsers below Parser Stanford (Klein and Manning, 2003) Berkeley (Petrov and Klein, 2007) Dep. parser Stanford (Klein and Manning, 2003) NE OpenNLP (Baldridge, J., 2005) Recognizer Stanford (Finkel et al., 2005) NP Detector In-house Table 1: Preprocessing components available in Reconcile. pairs of NPs and it is trained to assign a score indicating the likelihood that the NPs in the pair are coreferent. 4. Clustering. A clustering algorithm consolidates the predictions output by the classifier and forms the final set of coreference clusters (chains).2 5. Scoring. Finally, during testing Reconcile runs scoring algorithms that compare the chains produced by the system to the goldsta</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>S. Petrov and D. Klein. 2007. Improved Inference for Unlexicalized Parsing. In Proceedings of the Joint Meeting of the Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>M Kabadjov</author>
</authors>
<title>A general-purpose, off-the-shelf anaphora resolution module: implementation and preliminary evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference.</booktitle>
<contexts>
<context position="5595" citStr="Poesio and Kabadjov (2004)" startWordPosition="798" endWordPosition="801"> experimentation on most of the standard coreference resolution data sets; • implement most popular coreference resolution scoring metrics; • exhibit state-of-the-art coreference resolution performance (i.e., it can be configured to create a resolver that achieves performance close to the best reported results); • can be easily extended with new methods and features; • is relatively fast and easy to configure and run; • has a set of pre-built resolvers that can be used as black-box coreference resolution systems. While several other coreference resolution systems are publicly available (e.g., Poesio and Kabadjov (2004), Qiu et al. (2004) and Versley et al. (2008)), none meets all seven of these desiderata (see Related Work). Reconcile is a modular software platform that abstracts the basic architecture of most contemporary supervised learningbased coreference resolution systems (e.g., Soon et al. (2001), Ng and Cardie (2002), Bengtson and Roth (2008)) and achieves performance comparable to the state-of-the-art on several benchmark data sets. Additionally, Reconcile can be easily reconfigured to use different algorithms, features, preprocessing elements, evaluation settings and metrics. In the rest of this p</context>
</contexts>
<marker>Poesio, Kabadjov, 2004</marker>
<rawString>M. Poesio and M. Kabadjov. 2004. A general-purpose, off-the-shelf anaphora resolution module: implementation and preliminary evaluation. In Proceedings of the Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Qiu</author>
<author>M-Y Kan</author>
<author>T-S Chua</author>
</authors>
<title>A public reference implementation of the rap anaphora resolution algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference.</booktitle>
<contexts>
<context position="5614" citStr="Qiu et al. (2004)" startWordPosition="802" endWordPosition="805">the standard coreference resolution data sets; • implement most popular coreference resolution scoring metrics; • exhibit state-of-the-art coreference resolution performance (i.e., it can be configured to create a resolver that achieves performance close to the best reported results); • can be easily extended with new methods and features; • is relatively fast and easy to configure and run; • has a set of pre-built resolvers that can be used as black-box coreference resolution systems. While several other coreference resolution systems are publicly available (e.g., Poesio and Kabadjov (2004), Qiu et al. (2004) and Versley et al. (2008)), none meets all seven of these desiderata (see Related Work). Reconcile is a modular software platform that abstracts the basic architecture of most contemporary supervised learningbased coreference resolution systems (e.g., Soon et al. (2001), Ng and Cardie (2002), Bengtson and Roth (2008)) and achieves performance comparable to the state-of-the-art on several benchmark data sets. Additionally, Reconcile can be easily reconfigured to use different algorithms, features, preprocessing elements, evaluation settings and metrics. In the rest of this paper, we review rel</context>
</contexts>
<marker>Qiu, Kan, Chua, 2004</marker>
<rawString>L. Qiu, M.-Y. Kan, and T.-S. Chua. 2004. A public reference implementation of the rap anaphora resolution algorithm. In Proceedings of the Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Soon</author>
<author>H Ng</author>
<author>D Lim</author>
</authors>
<title>A Machine Learning Approach to Coreference of Noun Phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="5885" citStr="Soon et al. (2001)" startWordPosition="844" endWordPosition="847">orted results); • can be easily extended with new methods and features; • is relatively fast and easy to configure and run; • has a set of pre-built resolvers that can be used as black-box coreference resolution systems. While several other coreference resolution systems are publicly available (e.g., Poesio and Kabadjov (2004), Qiu et al. (2004) and Versley et al. (2008)), none meets all seven of these desiderata (see Related Work). Reconcile is a modular software platform that abstracts the basic architecture of most contemporary supervised learningbased coreference resolution systems (e.g., Soon et al. (2001), Ng and Cardie (2002), Bengtson and Roth (2008)) and achieves performance comparable to the state-of-the-art on several benchmark data sets. Additionally, Reconcile can be easily reconfigured to use different algorithms, features, preprocessing elements, evaluation settings and metrics. In the rest of this paper, we review related work (Section 2), describe Reconcile’s organization and components (Section 3) and show experimental results for Reconcile on six data sets and two evaluation metrics (Section 4). 2 Related Work Several coreference resolution systems are currently publicly available</context>
<context position="7671" citStr="Soon et al. (2001)" startWordPosition="1116" endWordPosition="1119">hough we believe that Reconcile’s approach is more flexible than BART’s. In addition, the architecture and system components of Reconcile (including a comprehensive set of features that draw on the expertise of state-of-the-art supervised learning approaches, such as Bengtson and Roth (2008)) result in performance closer to the state-of-the-art. Coreference resolution has received much research attention, resulting in an array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum and Wellner (2004) and Finley and Joachims (2005)), competition approaches (e.g. Yang et al. (2003)) and a bell-tree search approach (Luo et al. (2004)). Most of these approaches rely on some notion of pairwise feature-based similarity and can be directly implemented in Reconcile. 3 System Description Reconcile was designed to</context>
<context position="10140" citStr="Soon et al. (2001)" startWordPosition="1492" endWordPosition="1495">syntactic parse of the text and the output of a Named Entity (NE) extractor, but extract different constructs as specialized in the corresponding definition. The NP extractors successfully recognize about 95% of the NPs in the MUC and ACE gold standards. 2. Feature generation. Using annotations produced during preprocessing, Reconcile produces feature vectors for pairs of NPs. For example, a feature might denote whether the two NPs agree in number, or whether they have any words in common. Reconcile includes over 80 features, inspired by other successful coreference resolution systems such as Soon et al. (2001) and Ng and Cardie (2002). 3. Classification. Reconcile learns a classifier that operates on feature vectors representing Task Systems Sentence UIUC (CC Group, 2009) splitter OpenNLP (Baldridge, J., 2005) Tokenizer OpenNLP (Baldridge, J., 2005) POS OpenNLP (Baldridge, J., 2005) Tagger + the two parsers below Parser Stanford (Klein and Manning, 2003) Berkeley (Petrov and Klein, 2007) Dep. parser Stanford (Klein and Manning, 2003) NE OpenNLP (Baldridge, J., 2005) Recognizer Stanford (Finkel et al., 2005) NP Detector In-house Table 1: Preprocessing components available in Reconcile. pairs of NPs </context>
<context position="13744" citStr="Soon et al. (2001)" startWordPosition="2040" endWordPosition="2043">d run against several scoring metrics. For the purpose of this sample evaluation, we create only one particular instantiation of Reconcile, which we will call Reconcile2010 to differentiate it from the general platform. Reconcile2010 is configured using the following components: 1. Preprocessing (a) Sentence Splitter: OpenNLP (b) Tokenizer: OpenNLP (c) POS Tagger: OpenNLP (d) Parser: Berkeley (e) Named Entity Recognizer: Stanford 2. Feature Set - A hand-selected subset of 60 out of the more than 80 features available. The features were selected to include most of the features from Soon et al. Soon et al. (2001), Ng and Cardie (2002) and Bengtson and Roth (2008). 3. Classifier - Averaged Perceptron 4. Clustering - Single-link - Positive decision threshold was tuned by cross validation of the training set. 4.3 Experimental Results The first two rows of Table 3 show the performance of Reconcile2010. For all data sets, B3 scores are higher than MUC scores. The MUC score is highest for the MUC6 data set, while B3 scores are higher for the ACE data sets as compared to the MUC data sets. Due to the difficulties outlined in Section 1, results for Reconcile presented here are directly comparable only to a li</context>
<context position="15965" citStr="Soon et al. (2001)" startWordPosition="2378" endWordPosition="2381">and evaluation across these data sets. We hope that Reconcile will support experimental research in coreference resolution and provide a state-of-the-art coreference resolver for both researchers and application developers. We believe that in this way Reconcile will facilitate meaningful and consistent comparisons of coreference resolution systems. The full Reconcile release is available for download at http://www.cs.utah.edu/nlp/reconcile/. 159 System Score Data sets MUC6 MUC7 ACE-2 ACE03 ACE04 ACE05 Reconcile2010 MUC 68.50 62.80 65.99 67.87 62.03 67.41 B3 70.88 65.86 78.29 79.39 76.50 73.71 Soon et al. (2001) MUC 62.6 60.4 – – – – Ng and Cardie (2002) MUC 70.4 63.4 – – – – Yang et al. (2003) MUC 71.3 60.2 – – – – Table 3: Scores for Reconcile on six data sets and scores for comparable coreference systems. Acknowledgments This research was supported in part by the National Science Foundation under Grant # 0937060 to the Computing Research Association for the CIFellows Project, Lawrence Livermore National Laboratory subcontract B573245, Department of Homeland Security Grant N0014-07-1-0152, and Air Force Contract FA8750-09-C-0172 under the DARPA Machine Reading Program. The authors would like to tha</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. Soon, H. Ng, and D. Lim. 2001. A Machine Learning Approach to Coreference of Noun Phrases. Computational Linguistics, 27(4):521–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>N Gilbert</author>
<author>C Cardie</author>
<author>E Riloff</author>
</authors>
<title>Conundrums in noun phrase coreference resolution: Making sense of the state-of-the-art.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/IJCNLP.</booktitle>
<contexts>
<context position="14722" citStr="Stoyanov et al. (2009)" startWordPosition="2198" endWordPosition="2201"> score is highest for the MUC6 data set, while B3 scores are higher for the ACE data sets as compared to the MUC data sets. Due to the difficulties outlined in Section 1, results for Reconcile presented here are directly comparable only to a limited number of scores reported in the literature. The bottom three rows of Table 3 list these comparable scores, which show that Reconcile2010 exhibits state-ofthe-art performance for supervised learning-based coreference resolvers. A more detailed study of Reconcile-based coreference resolution systems in different evaluation scenarios can be found in Stoyanov et al. (2009). 5 Conclusions Reconcile is a general architecture for coreference resolution that can be used to easily create various coreference resolvers. Reconcile provides broad support for experimentation in coreference resolution, including implementation of the basic architecture of contemporary state-of-the-art coreference systems and a variety of individual modules employed in these systems. Additionally, Reconcile handles all of the formatting and scoring peculiarities of the most widely used coreference resolution data sets (those created as part of the MUC and ACE conferences) and, thus, allows</context>
</contexts>
<marker>Stoyanov, Gilbert, Cardie, Riloff, 2009</marker>
<rawString>V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009. Conundrums in noun phrase coreference resolution: Making sense of the state-of-the-art. In Proceedings of ACL/IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>C Cardie</author>
<author>N Gilbert</author>
<author>E Riloff</author>
<author>D Buttler</author>
<author>D Hysom</author>
</authors>
<title>Reconcile: A coreference resolution research platform.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>Cornell University.</institution>
<contexts>
<context position="12458" citStr="Stoyanov et al. (2010)" startWordPosition="1842" endWordPosition="1845">ile. easy for new components to be implemented and existing ones to be removed or replaced. Reconcile’s standard distribution comes with a comprehensive set of implemented components – those available for steps 2–5 are shown in Table 2. Reconcile contains over 38,000 lines of original Java code. Only about 15% of the code is concerned with running existing components in the preprocessing step, while the rest deals with NP extraction, implementations of features, clustering algorithms and scorers. More details about Reconcile’s architecture and available components and features can be found in Stoyanov et al. (2010). 4 Evaluation 4.1 Data Sets Reconcile incorporates the six most commonly used coreference resolution data sets, two from the MUC conferences (MUC-6, 1995; MUC-7, 1997) and four from the ACE Program (NIST, 2004). For ACE, we incorporate only the newswire portion. When available, Reconcile employs the standard test/train split. Otherwise, we randomly split the data into a training and test set following a 70/30 ratio. Performance is evaluated according to the B3 and MUC scoring metrics. 4.2 The Reconcile2010 Configuration Reconcile can be easily configured with different algorithms for markable</context>
</contexts>
<marker>Stoyanov, Cardie, Gilbert, Riloff, Buttler, Hysom, 2010</marker>
<rawString>V. Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler, and D. Hysom. 2010. Reconcile: A coreference resolution research platform. Technical report, Cornell University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Versley</author>
<author>S Ponzetto</author>
<author>M Poesio</author>
<author>V Eidelman</author>
<author>A Jern</author>
<author>J Smith</author>
<author>X Yang</author>
<author>A Moschitti</author>
</authors>
<title>BART: A modular toolkit for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference.</booktitle>
<contexts>
<context position="5640" citStr="Versley et al. (2008)" startWordPosition="807" endWordPosition="810">ce resolution data sets; • implement most popular coreference resolution scoring metrics; • exhibit state-of-the-art coreference resolution performance (i.e., it can be configured to create a resolver that achieves performance close to the best reported results); • can be easily extended with new methods and features; • is relatively fast and easy to configure and run; • has a set of pre-built resolvers that can be used as black-box coreference resolution systems. While several other coreference resolution systems are publicly available (e.g., Poesio and Kabadjov (2004), Qiu et al. (2004) and Versley et al. (2008)), none meets all seven of these desiderata (see Related Work). Reconcile is a modular software platform that abstracts the basic architecture of most contemporary supervised learningbased coreference resolution systems (e.g., Soon et al. (2001), Ng and Cardie (2002), Bengtson and Roth (2008)) and achieves performance comparable to the state-of-the-art on several benchmark data sets. Additionally, Reconcile can be easily reconfigured to use different algorithms, features, preprocessing elements, evaluation settings and metrics. In the rest of this paper, we review related work (Section 2), des</context>
</contexts>
<marker>Versley, Ponzetto, Poesio, Eidelman, Jern, Smith, Yang, Moschitti, 2008</marker>
<rawString>Y. Versley, S. Ponzetto, M. Poesio, V. Eidelman, A. Jern, J. Smith, X. Yang, and A. Moschitti. 2008. BART: A modular toolkit for coreference resolution. In Proceedings of the Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A Model-Theoretic Coreference Scoring Theme.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC-6).</booktitle>
<contexts>
<context position="11701" citStr="Vilain et al., 1995" startWordPosition="1721" endWordPosition="1724">chains produced by the system to the goldstandard chains in the answer key. Each of the five steps above can invoke different components. Reconcile’s modularity makes it 2Some structured coreference resolution algorithms (e.g., McCallum and Wellner (2004) and Finley and Joachims (2005)) combine the classification and clustering steps above. Reconcile can easily accommodate this modification. 158 Step Available modules Classification various learners in the Weka toolkit libSVM (Chang and Lin, 2001) SVMlight (Joachims, 2002) Clustering Single-link Best-First Most Recent First Scoring MUC score (Vilain et al., 1995) B3 score (Bagga and Baldwin, 1998) CEAF score (Luo, 2005) Table 2: Available implementations for different modules available in Reconcile. easy for new components to be implemented and existing ones to be removed or replaced. Reconcile’s standard distribution comes with a comprehensive set of implemented components – those available for steps 2–5 are shown in Table 2. Reconcile contains over 38,000 lines of original Java code. Only about 15% of the code is concerned with running existing components in the preprocessing step, while the rest deals with NP extraction, implementations of features</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A Model-Theoretic Coreference Scoring Theme. In Proceedings of the Sixth Message Understanding Conference (MUC-6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>G Zhou</author>
<author>J Su</author>
<author>C Tan</author>
</authors>
<title>Coreference resolution using competition learning approach.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="8042" citStr="Yang et al. (2003)" startWordPosition="1165" endWordPosition="1168">has received much research attention, resulting in an array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum and Wellner (2004) and Finley and Joachims (2005)), competition approaches (e.g. Yang et al. (2003)) and a bell-tree search approach (Luo et al. (2004)). Most of these approaches rely on some notion of pairwise feature-based similarity and can be directly implemented in Reconcile. 3 System Description Reconcile was designed to be a research testbed capable of implementing most current approaches to coreference resolution. Reconcile is written in Java, to be portable across platforms, and was designed to be easily reconfigurable with respect to subcomponents, feature sets, parameter settings, etc. Reconcile’s architecture is illustrated in Figure 1. For simplicity, Figure 1 shows Reconcile’s</context>
</contexts>
<marker>Yang, Zhou, Su, Tan, 2003</marker>
<rawString>X. Yang, G. Zhou, J. Su, and C. Tan. 2003. Coreference resolution using competition learning approach. In Proceedings of the 41st Annual Meeting of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>