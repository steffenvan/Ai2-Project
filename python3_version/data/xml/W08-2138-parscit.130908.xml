<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000157">
<title confidence="0.958208">
DeSRL: A Linear-Time Semantic Role Labeling System
</title>
<author confidence="0.884783">
Massimiliano Ciaramita†∗ Giuseppe Attardi‡
</author>
<email confidence="0.949889">
massi@yahoo-inc.com attardi@di.unipi.it
</email>
<author confidence="0.823698">
Felice Dell’Orletta‡ Mihai Surdeanu†,¦
</author>
<email confidence="0.776396">
dellorle@di.unipi.it mihai.surdeanu@barcelonamedia.org
</email>
<author confidence="0.4192435">
t: Yahoo! Research Barcelona, Ocata 1, 08003, Barcelona, Catalunya, Spain
t: Dipartimento di Informatica, Universit`a di Pisa, L. B. Pontecorvo 3, I-56127, Pisa, Italy
</author>
<affiliation confidence="0.3143">
o: Barcelona Media Innovation Center, Ocata 1, 08003, Barcelona, Catalunya, Spain
</affiliation>
<sectionHeader confidence="0.962722" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999873705882353">
This paper describes the DeSRL sys-
tem, a joined effort of Yahoo! Research
Barcelona and Universit`a di Pisa for the
CoNLL-2008 Shared Task (Surdeanu et
al., 2008). The system is characterized by
an efficient pipeline of linear complexity
components, each carrying out a different
sub-task. Classifier errors and ambigui-
ties are addressed with several strategies:
revision models, voting, and reranking.
The system participated in the closed chal-
lenge ranking third in the complete prob-
lem evaluation with the following scores:
82.06 labeled macro F1 for the overall task,
86.6 labeled attachment for syntactic de-
pendencies, and 77.5 labeled F1 for se-
mantic dependencies.
</bodyText>
<sectionHeader confidence="0.926259" genericHeader="keywords">
1 System description
</sectionHeader>
<bodyText confidence="0.999926833333333">
DeSRL is implemented as a sequence of compo-
nents of linear complexity relative to the sentence
length. We decompose the problem into three sub-
tasks: parsing, predicate identification and clas-
sification (PIC), and argument identification and
classification (AIC). We address each of these sub-
tasks with separate components without backward
feedback between sub-tasks. However, the use of
multiple parsers at the beginning of the process,
and re-ranking at the end, contribute beneficial
stochastic aspects to the system. Figure 1 summa-
rizes the system architecture. We detail the parsing
</bodyText>
<note confidence="0.832327">
* All authors contributed equally to this work.
</note>
<footnote confidence="0.9692345">
* °c 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.869546">
sub-task in Section 2 and the semantic sub-tasks
(PIC and AIC) in Section 3.
</bodyText>
<sectionHeader confidence="0.993837" genericHeader="introduction">
2 Parsing
</sectionHeader>
<bodyText confidence="0.975553166666667">
In the parsing sub-task we use a combination strat-
egy on top of three individual parsing models,
two developed in-house –DeSRleft−to−right and
DeSRrevision
right−to−left– and a third using an off-the-
shelf parser, Malt 1.0.01.
</bodyText>
<subsectionHeader confidence="0.990628">
2.1 DeSRleft−to−right
</subsectionHeader>
<bodyText confidence="0.9997232">
This model is a version of DeSR (Attardi, 2006),
a deterministic classifier-based Shift/Reduce
parser. The parser processes input tokens advanc-
ing on the input from left to right with Shift ac-
tions and accumulates processed tokens on a stack
with Reduce actions. The parser has been adapted
for this year’s shared task and extended with addi-
tional classifiers, e.g., Multi Layer Perceptron and
multiple SVMs.2
The parser uses the following features:
</bodyText>
<listItem confidence="0.958668125">
1. SPLIT LEMMA: from tokens −1, 0, 1, prev(0),
leftChild(0), rightChild(0)
2. PPOSS: from −2, −1, 0, 1, 2, 3, prev(0), next(−1),
leftChild(−1), leftChild(0), rightChild(−1),
rightChild(0)
3. DEPREL: from leftChild(−1), leftChild(0),
rightChild(−1)
4. HDIST: from −1, 0
</listItem>
<bodyText confidence="0.999745428571429">
In the above list negative numbers refer to tokens
on the stack, positive numbers to tokens in the in-
put queue. We use the following path operators:
leftChild(x) refers to the leftmost child of token
x, rightChild(x) to the rightmost child of token
x, prev(x) and next(x) respectively to the token
preceding or following x in the sentence.
</bodyText>
<footnote confidence="0.997513">
1http://w3.msi.vxu.se/—nivre/research/
MaltParser.html
2This parser is available for download at: http://
sourceforge.net/projects/desr/.
</footnote>
<page confidence="0.644944">
258
</page>
<note confidence="0.3756415">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 258–262
Manchester, August 2008
</note>
<figureCaption confidence="0.998924">
Figure 1: DeSRL system architecture.
</figureCaption>
<figure confidence="0.998163166666667">
PIC
AIC
DeSR left−to−right
revision
DeSR right−to−left
Malt
Voting
Input
Output
Argument
Frame
Reranking
</figure>
<bodyText confidence="0.999860176470588">
The first three types of features are directly ex-
tracted from the attributes of tokens present in the
training corpus. The fourth feature represents the
distance of the token to the head of the noun phrase
to which it belongs, or “O” if it does not belong to
a noun phrase. This distance is computed with a
simple heuristic, based on a pattern of POS tags.
Attardi and Dell’Orletta (2008) have shown that
this feature improves the accuracy of a shift/reduce
dependency parser by providing approximate in-
formation about NP chunks in the sentence. In fact
no token besides the head of a noun phrase can
have a head referring to a token outside the noun
phrase. Hence the parser can learn to avoid creat-
ing such links. The addition of this feature yields
an increase of 0.80% in Labeled Accuracy on the
development set.
</bodyText>
<subsectionHeader confidence="0.992577">
2.2 Revision Parser: DeSRrevision
</subsectionHeader>
<bodyText confidence="0.997149078947368">
right−to−left
Our second individual parsing model implements
an alternative to the method of revising parse trees
of Attardi and Ciaramita (2007) (see also (Hall &amp;
Novak, 2005)). The original approach consisted in
training a classifier to revise the errors of a base-
line parser. The approach assumed that only lo-
cal revisions to the parse tree would be needed,
since the dependency parser mostly gets individual
phrases correctly. The experiments showed that in-
deed most of the corrections can be expressed by
a small set of (about 20) complex movement rules.
Furthermore, there was evidence that one could get
higher improvements from the tree revision classi-
fier if this was trained on the output of a lower ac-
curacy parser. The reason for this is that the num-
ber of errors is higher and this provides a larger
amount of training data.
For the CoNLL 2008 shared task, we refined this
idea, but instead of using an independent classi-
fier for the revision, we use the parser itself. The
second parser is trained on the original corpus ex-
tended with dependency information predicted by
a lower accuracy parser. To obtain the base parser
we use DeSR trained on half the training corpus
using a Maximum Entropy (ME) classifier. The
ME classifier is considerably faster to train but has
a lower accuracy: this model achieved an LAS of
76.49% on the development set. Using the out-
put of the ME-based parser we extend the original
corpus with four additional columns: the lemma
of the predicted head (PHLEMMA), the PPOSS of
the predicted head (PHPPOSS), the dependency of
the predicted head (PHDEPREL), and the indica-
tion of whether a token appears before or after its
predicted head. A second parser is trained on this
corpus, scanning sentences from right to left and
using the following additional features:
</bodyText>
<listItem confidence="0.999796">
1. PHPPOSS: from −1, 0
2. PHLEMMA: from −1, 0
3. PHDEPREL: from −1, 0
4. PHHDIST: from 0
</listItem>
<bodyText confidence="0.999684571428571">
Performing parsing in reverse order helps reduce
several of the errors that a deterministic parser
makes when dependency links span a long distance
in the input sequence. Experiments on the CoNLL
2007 corpora (Dell’Orletta, 2008) have shown that
this indeed occurs, especially for distances in the
range from 6 to 23. In particular, the most signifi-
cant improvements are for dependencies with label
COORD (+ 6%) and P (+ 8%).
The revision parser achieves an LAS of 85.81%
on the development set. Note that the extra fea-
tures from the forward parser are indeed use-
ful, since a simple backward parser only achieves
82.56% LAS on the development set.
</bodyText>
<subsectionHeader confidence="0.997584">
2.3 Parser Combination
</subsectionHeader>
<bodyText confidence="0.880680384615385">
The final step consists in combining the out-
puts of the three individual models a simple
voting scheme: for each token we use major-
ity voting to select its head and dependency la-
bel. In case of ties, we chose the dependency
predicted by our overall best individual model
(DeSRrevision ) 3
right−to−left .
Note that typical approaches to parser
combination combine the outputs of inde-
pendent parsers, while in our case one base
model (DeSRrevision is trained with
right−to−left)
</bodyText>
<footnote confidence="0.9837175">
3We tried several voting strategies but none performed bet-
ter.
</footnote>
<page confidence="0.996209">
259
</page>
<bodyText confidence="0.999829">
information predicted by another individual
model(DeSRleft−to−right). To the best of our
knowledge, combining individual parsing models
that are inter-dependent is novel.
</bodyText>
<sectionHeader confidence="0.982286" genericHeader="method">
3 Semantic Role Labeling
</sectionHeader>
<bodyText confidence="0.999913333333333">
We implement the Semantic Role Labeling (SRL)
problem using three components: PIC, AIC, and
reranking of predicted argument frames.
</bodyText>
<subsectionHeader confidence="0.999385">
3.1 Predicate Identification and Classification
</subsectionHeader>
<bodyText confidence="0.9976685">
The PIC component carries out the identification
of predicates, as well as their partial disambigua-
tion, and it is implemented as a multiclass average
Perceptron classifier (Crammer &amp; Singer, 2003).
For each token i we extract the following features
((,) stands for token combination):
</bodyText>
<listItem confidence="0.998390363636364">
1. SPLIT LEMMA: from hi−1, ii,i−1, i, i+1, hi, i+1i
2. SPLIT FORM: from i − 2, i − 1, i, i + 1�i + 2
3. PPOSS: from hi−2, i−1i, hi−1, ii, i−1, i, i+1, hi, i+
1i, hi + 1, i + 2i
4. WORD SHAPE: e.g., “Xx*” for “Brazil”, from hi−2, i−
1, ii, hi − 1, ii, i − 1, i, i + 1, hi, i + 1i, hi, i + 1, i + 2i
5. Number of children of node i
6. For each children j of i: split lemmaj, ppossj,
depreli,j, hsplit lemmai, split lemmaji, hppossi,
ppossji
7. Difference of positions: j − i, for each child j of i.
</listItem>
<bodyText confidence="0.99980052631579">
The PIC component uses one single classifier map-
ping tokens to one of 8 classes corresponding to
the rolesets suffixes 1 to 6, the 6 most frequent
types, plus a class grouping all other rolesets, and
a class for non predicates; i.e., Y = 10, 1, 2,.., 71.
Each token classified as y7 is mapped by default to
the first sense y1. This approach is capable of dis-
tinguishing between different predicates based on
features 1 and 2, but it can also exploit information
that is shared between predicates due to similar
frame structures. The latter property is intuitively
useful especially for low-frequency predicates.
The classifier has an accuracy in the multiclass
problem, considering also the mistakes due to the
non-predicted classes, of 96.2%, and an F-score of
92.7% with respect to the binary predicate iden-
tification problem. To extract features from trees
(5-7) we use our parser’s output on training, devel-
opment and evaluation data.
</bodyText>
<subsectionHeader confidence="0.9834755">
3.2 Argument Identification and
Classification
</subsectionHeader>
<bodyText confidence="0.999241">
Algorithm 1 describes our AIC framework. The al-
gorithm receives as input a sentence S where pred-
icates have been identified and classified using the
</bodyText>
<equation confidence="0.5132484">
Algorithm 1: AIC
input : sentence S; inference strategy I; model w
foreach predicate p in S do
set frame Fin = {}
foreach token i in S do
if validCandidate(i) then
y = arg maxyEY score(1(p, i), w, y)
if y =6 nil then
add argument (i,y) to Fin
Font = inference(Fin, I)
</equation>
<bodyText confidence="0.975787571428572">
output: set of all frames Font
PIC component, an inference strategy Z is used
to guarantee that the generated best frames satisfy
the domain constraints, plus an AIC classification
model w. We learn w using a multiclass Percep-
tron, using as output label set Y all argument labels
that appear more than 10 times in training plus a nil
label assigned to all other tokens.
During both training and evaluation we se-
lect only the candidate tokens that pass the
validCandidate filter. This function requires that
the length of the dependency path between pred-
icate and candidate argument be less than 6, the
length of the dependency path between argument
and the first common ancestor be less than 3, and
the length of the dependency path between the
predicate and the first common ancestor be less
than 5. This heuristic covers over 98% of the ar-
guments in training.
In the worst case, Algorithm 1 has quadratic
complexity in the sentence size. But, on average,
the algorithm has linear time complexity because
the number of predicates per sentence is small (av-
eraging less than five for sentences of 25 words).
The function 4b generates the feature vector for
a given predicate-argument tuple. 4b extracts the
following features from a given tuple of a predicate
p and argument a:
</bodyText>
<listItem confidence="0.9988997">
1. token(a)4, token(modifier of a) if a is the
head of a prepositional phrase, and token(p).
2. Patterns of PPOSS tags and DEPREL labels
for: (a) the predicate children, (b) the children
of the predicate ancestor across VC and IM
dependencies, and (c) the siblings of the same
ancestor. In all paths we mark the position of
p, a and any of their ancestors.
3. The dependency path between p and a. We
add three versions of this feature: just the
</listItem>
<footnote confidence="0.968262">
4token extracts the split lemma, split form, and PPOSS
tag of a given token.
</footnote>
<page confidence="0.996405">
260
</page>
<bodyText confidence="0.993608">
path, and the path prefixed with p and a’s
PPOSS tags or split lemmas.
</bodyText>
<listItem confidence="0.999471333333333">
4. Length of the dependency path.
5. Distance in tokens between p and a.
6. Position of a relative to p: before or after.
</listItem>
<bodyText confidence="0.99993425">
We implemented two inference strategies:
greedy and reranking. The greedy strategy sorts
all arguments in a frame Fin in descending order
of their scores and iteratively adds each argument
to the output frame Fout only if it respects the do-
main constraints with the other arguments already
selected. The only domain constraint we use is that
core arguments cannot repeat.
</bodyText>
<subsectionHeader confidence="0.999691">
3.3 Reranking of Argument Frames
</subsectionHeader>
<bodyText confidence="0.9953868">
The reranking inference strategy adapts the ap-
proach of Toutanova et al. (2005) to the depen-
dency representation with notable changes in can-
didate selection, feature set, and learning model.
For candidate selection we modify Algorithm 1:
instead of storing only y for each argument in Fin
we store the top k best labels. Then, from the ar-
guments in Fin, we generate the top k frames with
the highest score, where the score of a frame is the
product of all its argument probabilities, computed
as the softmax function on the output of the Per-
ceptron. In this set of candidate frames we mark
the frame with the highest F1 score as the positive
example and all others as negative examples.
From each frame we extract these features:
</bodyText>
<listItem confidence="0.999054047619048">
1. Position of the frame in the set ordered by
frame scores. Hence, smaller positions in-
dicate candidate frames that the local model
considered better (Marquez et al., 2007).
2. The complete sequence of arguments and
predicate for this frame (Toutanova, 2005).
We add four variants of this feature: just the
sequence and sequence expanded with: (a)
predicate voice, (b) predicate split lemma,
and (c) combination of voice and split lemma.
3. The complete sequence of arguments and
predicate for this frame combined with their
PPOSS tags. Same as above, we add four
variants of this feature.
4. Overlap with the PropBank or NomBank
frame for the same predicate lemma and
sense. We add the precision, recall, and F1
score of the overlap as features (Marquez et
al., 2007).
5. For each frame argument, we add the features
from the local AIC model prefixed with the
</listItem>
<table confidence="0.99810725">
WSJ + Brown WSJ Brown
Labeled macro Fl 82.69 83.83 73.51
LAS 87.37 88.21 80.60
Labeled Fl 78.00 79.43 66.41
</table>
<tableCaption confidence="0.935681">
Table 1: DeSRL results in the closed challenge,
for the overall task, syntactic dependencies, and
semantic dependencies.
</tableCaption>
<table confidence="0.999892666666667">
Devel WSJ Brown
DeSRleft−to−right 85.61 86.54 79.74
DeSRrevision 85.81 86.19 78.91
right−to−left
MaltParser 84.10 85.50 77.06
Voting 87.37 88.21 80.60
</table>
<tableCaption confidence="0.999355">
Table 2: LAS of individual and combined parsers.
</tableCaption>
<bodyText confidence="0.9895439">
corresponding argument label in the current
frame (Toutanova, 2005).
The reranking classifier is implemented as multi-
layer perceptron with one hidden layer of 5 units,
trained to solve a regression problem with a least
square criterion function. Previously we experi-
mented, unsuccessfully, with a multiclass Percep-
tron and a ranking Perceptron. The limited number
of hidden units guarantees a small computational
overhead with respect to a linear model.
</bodyText>
<sectionHeader confidence="0.998613" genericHeader="evaluation">
4 Results and Analysis
</sectionHeader>
<bodyText confidence="0.999790277777778">
Table 1 shows the overall results of our system
in the closed challenge. Note that these scores
are higher than those of our submitted run mainly
due to improved parsing models (discussed be-
low) whose training ended after the deadline. The
score of the submitted system is the third best
for the complete task. The system throughput in
our best configuration is 28 words/second, or 30
words/second without reranking. In exploratory
experiments on feature selection for the re-ranking
model we found that several features classes do
not contribute anything and could be filtered out
speeding up significantly this last SRL step. Note
however that currently over 90% of the runtime is
occupied by the syntactic parsers’ SVM classifiers.
We estimate that we can increase throughput one
order of magnitude simply by switching to a faster,
multiclass classifier in parsing.
</bodyText>
<subsectionHeader confidence="0.999657">
4.1 Analysis of Parsing
</subsectionHeader>
<bodyText confidence="0.999901">
Table 2 lists the labeled attachment scores (LAS)
achieved by each parser and by their combination
on the development set, the WSJ and Brown test
sets. The results are improved with respect to the
official run, by using a revision parser trained on
the output of the lower accuracy ME parser, as
</bodyText>
<page confidence="0.990858">
261
</page>
<table confidence="0.986369833333333">
Labeled Fl Unlabeled Fl
Syntax PIC Inference Devel WSJ Brown Devel WSJ Brown
gold gold greedy 88.95 90.21 84.95 93.71 94.34 93.29
predicted gold greedy 85.96 86.70 78.68 90.60 90.98 88.02
predicted predicted greedy 79.88 79.27 66.41 86.07 85.33 80.14
predicted predicted reranking 80.13 79.43 66.41 86.33 85.62 80.41
</table>
<tableCaption confidence="0.980062">
Table 3: Scores of the SRL component under various configurations.
</tableCaption>
<table confidence="0.99978225">
Devel WSJ Brown
Unlabeled Fl 92.69 90.88 86.96
Labeled Fl (PIC) 87.29 84.87 71.99
Labeled Fl (Sense 1) 79.62 78.94 70.11
</table>
<tableCaption confidence="0.999617">
Table 4: Scores of the PIC component.
</tableCaption>
<bodyText confidence="0.999928285714286">
mentioned earlier. These results show that vot-
ing helps significantly (+1.56% over the best single
parser) even though inter-dependent models were
used. However, our simple voting scheme does
not guarantee that a well-formed tree is generated,
leaving room for further improvements; e.g., as
in (Sagae &amp; Lavie, 2006).
</bodyText>
<subsectionHeader confidence="0.998495">
4.2 Analysis of SRL
</subsectionHeader>
<bodyText confidence="0.999991022222222">
Table 3 shows the labeled and unlabeled F1 scores
of our SRL component as we move from gold to
predicted information for syntax and PIC. For the
shared task setting –predicted syntax and predicted
PIC– we show results for the two inference strate-
gies implemented: greedy and reranking. The first
line in the table indicates that the performance of
the SRL component when using gold syntax and
gold PIC is good: the labeled F1 is 90 points for the
in-domain corpus and approximately 85 points for
the out-of-domain corpus. Argument classification
suffers the most on out-of-domain input: there is
a difference of 5 points between the labeled scores
on WSJ and Brown, even though the correspond-
ing unlabeled scores are comparable.
The second line in the table replicates the setup
of the 2005 CoNLL shared task: predicted syntax
but gold PIC. This yields a moderate drop of 3 la-
beled F1 points on in-domain data and a larger drop
of 6 points for out-of-domain data.
We see larger drops when switching to predicted
PIC (line 3): 5-6 labeled F1 points in domain and
12 points out of domain. This drop is caused by the
PIC component, e.g., if a predicate is missed the
whole frame is lost. Table 4 lists the scores of our
PIC component, which we compare with a base-
line system that assigns sense 1 to all identified
predicates. The table indicates that, even though
our disambiguation component improves signifi-
cantly over the baseline, it performs poorly, espe-
cially on out-of-domain data. Same as SRL, the
classification sub-task suffers the most out of do-
main (there is a difference of 15 points between
unlabeled and labeled F1 scores on Brown).
Finally, the reranking inference strategy yields
only modest improvements (last line in Table 3).
We attribute these results to the fact that, unlike
Toutanova et al. (2005), we use only one tree to
generate frame candidates, hence the variation in
the candidate frames is small. Considering that the
processing overhead of reranking is already large
(it quadruples the runtime of our AIC component),
we do not consider reranking a practical extension
to a SRL system when processing speed is a dom-
inant requirement.
</bodyText>
<sectionHeader confidence="0.999114" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999384">
G. Attardi. 2006. Experiments with a Multilan-
guage Non-Projective Dependency Parser. In Proc.
of CoNNL-X 2006.
G. Attardi and M. Ciaramita. 2007. Tree Revi-
sion Learning for Dependency Parsing. In Proc. of
NAACL/HLTC 2007.
G. Attardi, F. Dell’Orletta. 2008. Chunking and De-
pendency Parsing. In Proc. of Workshop on Partial
Parsing.
K. Crammer and Y. Singer. 2003. Ultraconservative
Online Algorithms for Multiclass Problems. Journal
of Machine Learning Research 3: pp.951-991.
F. Dell’Orletta. 2008. Improving the Accuracy of De-
pendency Parsing. PhD Thesis. Dipartimento di In-
formatica, Universit`a di Pisa, forthcoming.
K. Hall and V. Novak. 2005. Corrective Modeling
for Non-Projective Dependency Parsing. In Proc. of
IWPT.
L. Marquez, L. Padro, M. Surdeanu, and L. Villarejo.
2007. UPC: Experiments with Joint Learning within
SemEval Task 9. In Proc. of SemEval 2007.
K. Sagae and A. Lavie. 2006. Parser Combination by
reparsing. In Proc. of HLT/NAACL.
M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Depen-
dencies. In Proc. of CoNLL-2008.
K. Toutanova, A. Haghighi, and C. Manning. 2005.
Joint Learning Improves Semantic Role Labeling. In
Proc. of ACL.
</reference>
<page confidence="0.997296">
262
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.99996">DeSRL: A Linear-Time Semantic Role Labeling System</title>
<author confidence="0.966623">Massimiliano</author>
<email confidence="0.5924905">massi@yahoo-inc.comattardi@di.unipi.itdellorle@di.unipi.itmihai.surdeanu@barcelonamedia.org</email>
<note confidence="0.407072666666667">Yahoo! Research Barcelona, Ocata 1, 08003, Barcelona, Catalunya, Spain Dipartimento di Informatica, Universit`a di Pisa, L. B. Pontecorvo 3, I-56127, Pisa, Italy Barcelona Media Innovation Center, Ocata 1, 08003, Barcelona, Catalunya, Spain</note>
<abstract confidence="0.993344447761194">This paper describes the DeSRL system, a joined effort of Yahoo! Research Barcelona and Universit`a di Pisa for the CoNLL-2008 Shared Task (Surdeanu et al., 2008). The system is characterized by an efficient pipeline of linear complexity components, each carrying out a different sub-task. Classifier errors and ambiguities are addressed with several strategies: revision models, voting, and reranking. The system participated in the closed challenge ranking third in the complete problem evaluation with the following scores: 82.06 labeled macro F1 for the overall task, 86.6 labeled attachment for syntactic dependencies, and 77.5 labeled F1 for semantic dependencies. 1 System description DeSRL is implemented as a sequence of components of linear complexity relative to the sentence length. We decompose the problem into three subtasks: parsing, predicate identification and classification (PIC), and argument identification and classification (AIC). We address each of these subtasks with separate components without backward feedback between sub-tasks. However, the use of multiple parsers at the beginning of the process, and re-ranking at the end, contribute beneficial stochastic aspects to the system. Figure 1 summarizes the system architecture. We detail the parsing authors contributed equally to this work. 2008. Licensed under the Commons Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. sub-task in Section 2 and the semantic sub-tasks (PIC and AIC) in Section 3. 2 Parsing In the parsing sub-task we use a combination strategy on top of three individual parsing models, developed in-house and and a third using an off-theparser, Malt This model is a version of DeSR (Attardi, 2006), deterministic classifier-based parser. The parser processes input tokens advancon the input from left to right with actions and accumulates processed tokens on a stack The parser has been adapted for this year’s shared task and extended with additional classifiers, e.g., Multi Layer Perceptron and The parser uses the following features: SPLIT from tokens 2. from rightChild(0) 3. from 4. from In the above list negative numbers refer to tokens on the stack, positive numbers to tokens in the input queue. We use the following path operators: to the leftmost child of token the rightmost child of token to the token or following the sentence. MaltParser.html parser is available for download at:</abstract>
<address confidence="0.704604">258 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, Manchester, August 2008</address>
<abstract confidence="0.951917366946779">Figure 1: DeSRL system architecture. PIC AIC revision Malt Voting Input Output Argument Frame Reranking The first three types of features are directly extracted from the attributes of tokens present in the training corpus. The fourth feature represents the distance of the token to the head of the noun phrase to which it belongs, or “O” if it does not belong to a noun phrase. This distance is computed with a simple heuristic, based on a pattern of POS tags. Attardi and Dell’Orletta (2008) have shown that this feature improves the accuracy of a shift/reduce dependency parser by providing approximate information about NP chunks in the sentence. In fact no token besides the head of a noun phrase can have a head referring to a token outside the noun phrase. Hence the parser can learn to avoid creating such links. The addition of this feature yields an increase of 0.80% in Labeled Accuracy on the development set. Revision Parser: Our second individual parsing model implements an alternative to the method of revising parse trees of Attardi and Ciaramita (2007) (see also (Hall &amp; Novak, 2005)). The original approach consisted in training a classifier to revise the errors of a baseline parser. The approach assumed that only local revisions to the parse tree would be needed, since the dependency parser mostly gets individual phrases correctly. The experiments showed that indeed most of the corrections can be expressed by a small set of (about 20) complex movement rules. Furthermore, there was evidence that one could get higher improvements from the tree revision classifier if this was trained on the output of a lower accuracy parser. The reason for this is that the number of errors is higher and this provides a larger amount of training data. For the CoNLL 2008 shared task, we refined this idea, but instead of using an independent classifier for the revision, we use the parser itself. The second parser is trained on the original corpus extended with dependency information predicted by a lower accuracy parser. To obtain the base parser we use DeSR trained on half the training corpus using a Maximum Entropy (ME) classifier. The ME classifier is considerably faster to train but has a lower accuracy: this model achieved an LAS of 76.49% on the development set. Using the output of the ME-based parser we extend the original corpus with four additional columns: the lemma the predicted head the PPOSS of predicted head the dependency of predicted head and the indication of whether a token appears before or after its predicted head. A second parser is trained on this corpus, scanning sentences from right to left and using the following additional features: 1. from 2. from 3. from 4. from Performing parsing in reverse order helps reduce several of the errors that a deterministic parser makes when dependency links span a long distance in the input sequence. Experiments on the CoNLL 2007 corpora (Dell’Orletta, 2008) have shown that this indeed occurs, especially for distances in the range from 6 to 23. In particular, the most significant improvements are for dependencies with label 6%) and 8%). The revision parser achieves an LAS of 85.81% on the development set. Note that the extra features from the forward parser are indeed useful, since a simple backward parser only achieves 82.56% LAS on the development set. 2.3 Parser Combination The final step consists in combining the outputs of the three individual models a simple voting scheme: for each token we use majority voting to select its head and dependency label. In case of ties, we chose the dependency predicted by our overall best individual model Note that typical approaches to parser combination combine the outputs of independent parsers, while in our case one base is trained with tried several voting strategies but none performed better. 259 information predicted by another individual To the best of our knowledge, combining individual parsing models that are inter-dependent is novel. 3 Semantic Role Labeling We implement the Semantic Role Labeling (SRL) problem using three components: PIC, AIC, and reranking of predicted argument frames. 3.1 Predicate Identification and Classification The PIC component carries out the identification of predicates, as well as their partial disambiguation, and it is implemented as a multiclass average Perceptron classifier (Crammer &amp; Singer, 2003). each token extract the following features for token combination): SPLIT from i, i+1, SPLIT from i i, i + 1�i + 2 3. from i, i+1, i+ + 1, i + WORD e.g., “Xx*” for “Brazil”, from i i, i + 1, i + i + 1, i + Number of children of node For each children split split Difference of positions: for each child The PIC component uses one single classifier mapping tokens to one of 8 classes corresponding to the rolesets suffixes 1 to 6, the 6 most frequent types, plus a class grouping all other rolesets, and class for non predicates; i.e., token classified as mapped by default to first sense This approach is capable of distinguishing between different predicates based on features 1 and 2, but it can also exploit information that is shared between predicates due to similar frame structures. The latter property is intuitively useful especially for low-frequency predicates. The classifier has an accuracy in the multiclass problem, considering also the mistakes due to the non-predicted classes, of 96.2%, and an F-score of 92.7% with respect to the binary predicate identification problem. To extract features from trees (5-7) we use our parser’s output on training, development and evaluation data. 3.2 Argument Identification and Classification Algorithm 1 describes our AIC framework. The alreceives as input a sentence predicates have been identified and classified using the AIC sentence inference strategy model frame arg score(1(p, i), y) argument to set of all frames component, an inference strategy used to guarantee that the generated best frames satisfy the domain constraints, plus an AIC classification We learn a multiclass Percepusing as output label set argument labels appear more than 10 times in training plus a label assigned to all other tokens. During both training and evaluation we select only the candidate tokens that pass the This function requires that the length of the dependency path between predicate and candidate argument be less than 6, the length of the dependency path between argument and the first common ancestor be less than 3, and the length of the dependency path between the predicate and the first common ancestor be less than 5. This heuristic covers over 98% of the arguments in training. In the worst case, Algorithm 1 has quadratic complexity in the sentence size. But, on average, the algorithm has linear time complexity because the number of predicates per sentence is small (averaging less than five for sentences of 25 words). function the feature vector for given predicate-argument tuple. the following features from a given tuple of a predicate argument 1. of the of a prepositional phrase, and 2. Patterns of PPOSS tags and DEPREL labels for: (a) the predicate children, (b) the children the predicate ancestor across dependencies, and (c) the siblings of the same ancestor. In all paths we mark the position of any of their ancestors. The dependency path between We add three versions of this feature: just the the split lemma, split form, and PPOSS tag of a given token. 260 and the path prefixed with PPOSS tags or split lemmas. 4. Length of the dependency path. Distance in tokens between Position of to before or after. We implemented two inference strategies: The greedy strategy sorts arguments in a frame descending order of their scores and iteratively adds each argument the output frame if it respects the domain constraints with the other arguments already selected. The only domain constraint we use is that core arguments cannot repeat. 3.3 Reranking of Argument Frames The reranking inference strategy adapts the approach of Toutanova et al. (2005) to the dependency representation with notable changes in candidate selection, feature set, and learning model. For candidate selection we modify Algorithm 1: of storing only each argument in store the top labels. Then, from the arin we generate the top with the highest score, where the score of a frame is the product of all its argument probabilities, computed the on the output of the Perceptron. In this set of candidate frames we mark frame with the highest as the positive example and all others as negative examples. From each frame we extract these features: 1. Position of the frame in the set ordered by frame scores. Hence, smaller positions indicate candidate frames that the local model considered better (Marquez et al., 2007). 2. The complete sequence of arguments and predicate for this frame (Toutanova, 2005). We add four variants of this feature: just the sequence and sequence expanded with: (a) predicate voice, (b) predicate split lemma, and (c) combination of voice and split lemma. 3. The complete sequence of arguments and predicate for this frame combined with their PPOSS tags. Same as above, we add four variants of this feature. 4. Overlap with the PropBank or NomBank frame for the same predicate lemma and We add the precision, recall, and score of the overlap as features (Marquez et al., 2007). 5. For each frame argument, we add the features from the local AIC model prefixed with the WSJ + Brown WSJ Brown macro 82.69 83.83 73.51 LAS 87.37 88.21 80.60 Labeled Fl 78.00 79.43 66.41 Table 1: DeSRL results in the closed challenge, for the overall task, syntactic dependencies, and semantic dependencies. Devel WSJ Brown 85.61 86.54 79.74 85.81 86.19 78.91 MaltParser 84.10 85.50 77.06 Voting 87.37 88.21 80.60 Table 2: LAS of individual and combined parsers. corresponding argument label in the current frame (Toutanova, 2005). The reranking classifier is implemented as multilayer perceptron with one hidden layer of 5 units, trained to solve a regression problem with a least square criterion function. Previously we experimented, unsuccessfully, with a multiclass Perceptron and a ranking Perceptron. The limited number of hidden units guarantees a small computational overhead with respect to a linear model. 4 Results and Analysis Table 1 shows the overall results of our system in the closed challenge. Note that these scores are higher than those of our submitted run mainly due to improved parsing models (discussed below) whose training ended after the deadline. The score of the submitted system is the third best for the complete task. The system throughput in our best configuration is 28 words/second, or 30 words/second without reranking. In exploratory experiments on feature selection for the re-ranking model we found that several features classes do not contribute anything and could be filtered out speeding up significantly this last SRL step. Note however that currently over 90% of the runtime is occupied by the syntactic parsers’ SVM classifiers. We estimate that we can increase throughput one order of magnitude simply by switching to a faster, multiclass classifier in parsing. 4.1 Analysis of Parsing Table 2 lists the labeled attachment scores (LAS) achieved by each parser and by their combination on the development set, the WSJ and Brown test sets. The results are improved with respect to the official run, by using a revision parser trained on the output of the lower accuracy ME parser, as 261 Syntax PIC Inference Devel WSJ Brown Devel WSJ Brown gold gold greedy 88.95 90.21 84.95 93.71 94.34 93.29 predicted gold greedy 85.96 86.70 78.68 90.60 90.98 88.02 predicted predicted greedy 79.88 79.27 66.41 86.07 85.33 80.14 predicted predicted reranking 80.13 79.43 66.41 86.33 85.62 80.41 Table 3: Scores of the SRL component under various configurations. Devel WSJ Brown 92.69 90.88 86.96 (PIC) 87.29 84.87 71.99 1) 79.62 78.94 70.11 Table 4: Scores of the PIC component. mentioned earlier. These results show that voting helps significantly (+1.56% over the best single parser) even though inter-dependent models were used. However, our simple voting scheme does not guarantee that a well-formed tree is generated, leaving room for further improvements; e.g., as in (Sagae &amp; Lavie, 2006). 4.2 Analysis of SRL 3 shows the labeled and unlabeled of our SRL component as we move from gold to predicted information for syntax and PIC. For the shared task setting –predicted syntax and predicted PIC– we show results for the two inference strategies implemented: greedy and reranking. The first line in the table indicates that the performance of the SRL component when using gold syntax and PIC is good: the labeled 90 points for the in-domain corpus and approximately 85 points for the out-of-domain corpus. Argument classification suffers the most on out-of-domain input: there is a difference of 5 points between the labeled scores on WSJ and Brown, even though the corresponding unlabeled scores are comparable. The second line in the table replicates the setup of the 2005 CoNLL shared task: predicted syntax but gold PIC. This yields a moderate drop of 3 laon in-domain data and a larger drop of 6 points for out-of-domain data. We see larger drops when switching to predicted (line 3): 5-6 labeled in domain and 12 points out of domain. This drop is caused by the PIC component, e.g., if a predicate is missed the whole frame is lost. Table 4 lists the scores of our PIC component, which we compare with a baseline system that assigns sense 1 to all identified predicates. The table indicates that, even though our disambiguation component improves significantly over the baseline, it performs poorly, especially on out-of-domain data. Same as SRL, the classification sub-task suffers the most out of domain (there is a difference of 15 points between and labeled on Brown). Finally, the reranking inference strategy yields only modest improvements (last line in Table 3). We attribute these results to the fact that, unlike Toutanova et al. (2005), we use only one tree to generate frame candidates, hence the variation in the candidate frames is small. Considering that the processing overhead of reranking is already large (it quadruples the runtime of our AIC component), we do not consider reranking a practical extension to a SRL system when processing speed is a dominant requirement.</abstract>
<note confidence="0.730129">References G. Attardi. 2006. Experiments with a Multilan-</note>
<title confidence="0.699887833333333">Non-Projective Dependency Parser. In CoNNL-X G. Attardi and M. Ciaramita. 2007. Tree Revi- Learning for Dependency Parsing. In of G. Attardi, F. Dell’Orletta. 2008. Chunking and De- Parsing. In of Workshop on Partial</title>
<note confidence="0.941360076923077">Crammer and Y. Singer. 2003. Algorithms for Multiclass Problems. of Machine Learning Research 3: pp.951-991. F. Dell’Orletta. 2008. Improving the Accuracy of De- Parsing. PhD Thesis. di In- Universit`a di forthcoming. K. Hall and V. Novak. 2005. Corrective Modeling Non-Projective Dependency Parsing. In of L. Marquez, L. Padro, M. Surdeanu, and L. Villarejo. 2007. UPC: Experiments with Joint Learning within Task 9. In of SemEval K. Sagae and A. Lavie. 2006. Parser Combination by In of</note>
<author confidence="0.663830333333333">The CoNLL- Shared Task on Joint Parsing of Syntactic</author>
<author confidence="0.663830333333333">Semantic Depen-</author>
<note confidence="0.5796126">In of K. Toutanova, A. Haghighi, and C. Manning. 2005. Joint Learning Improves Semantic Role Labeling. In of 262</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Attardi</author>
</authors>
<title>Experiments with a Multilanguage Non-Projective Dependency Parser.</title>
<date>2006</date>
<booktitle>In Proc. of CoNNL-X</booktitle>
<contexts>
<context position="2377" citStr="Attardi, 2006" startWordPosition="334" endWordPosition="335">ng * All authors contributed equally to this work. * °c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. sub-task in Section 2 and the semantic sub-tasks (PIC and AIC) in Section 3. 2 Parsing In the parsing sub-task we use a combination strategy on top of three individual parsing models, two developed in-house –DeSRleft−to−right and DeSRrevision right−to−left– and a third using an off-theshelf parser, Malt 1.0.01. 2.1 DeSRleft−to−right This model is a version of DeSR (Attardi, 2006), a deterministic classifier-based Shift/Reduce parser. The parser processes input tokens advancing on the input from left to right with Shift actions and accumulates processed tokens on a stack with Reduce actions. The parser has been adapted for this year’s shared task and extended with additional classifiers, e.g., Multi Layer Perceptron and multiple SVMs.2 The parser uses the following features: 1. SPLIT LEMMA: from tokens −1, 0, 1, prev(0), leftChild(0), rightChild(0) 2. PPOSS: from −2, −1, 0, 1, 2, 3, prev(0), next(−1), leftChild(−1), leftChild(0), rightChild(−1), rightChild(0) 3. DEPREL</context>
</contexts>
<marker>Attardi, 2006</marker>
<rawString>G. Attardi. 2006. Experiments with a Multilanguage Non-Projective Dependency Parser. In Proc. of CoNNL-X 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Attardi</author>
<author>M Ciaramita</author>
</authors>
<title>Tree Revision Learning for Dependency Parsing.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL/HLTC</booktitle>
<contexts>
<context position="4798" citStr="Attardi and Ciaramita (2007)" startWordPosition="707" endWordPosition="710"> Dell’Orletta (2008) have shown that this feature improves the accuracy of a shift/reduce dependency parser by providing approximate information about NP chunks in the sentence. In fact no token besides the head of a noun phrase can have a head referring to a token outside the noun phrase. Hence the parser can learn to avoid creating such links. The addition of this feature yields an increase of 0.80% in Labeled Accuracy on the development set. 2.2 Revision Parser: DeSRrevision right−to−left Our second individual parsing model implements an alternative to the method of revising parse trees of Attardi and Ciaramita (2007) (see also (Hall &amp; Novak, 2005)). The original approach consisted in training a classifier to revise the errors of a baseline parser. The approach assumed that only local revisions to the parse tree would be needed, since the dependency parser mostly gets individual phrases correctly. The experiments showed that indeed most of the corrections can be expressed by a small set of (about 20) complex movement rules. Furthermore, there was evidence that one could get higher improvements from the tree revision classifier if this was trained on the output of a lower accuracy parser. The reason for thi</context>
</contexts>
<marker>Attardi, Ciaramita, 2007</marker>
<rawString>G. Attardi and M. Ciaramita. 2007. Tree Revision Learning for Dependency Parsing. In Proc. of NAACL/HLTC 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Attardi</author>
<author>F Dell’Orletta</author>
</authors>
<title>Chunking and Dependency Parsing.</title>
<date>2008</date>
<booktitle>In Proc. of Workshop on Partial Parsing.</booktitle>
<marker>Attardi, Dell’Orletta, 2008</marker>
<rawString>G. Attardi, F. Dell’Orletta. 2008. Chunking and Dependency Parsing. In Proc. of Workshop on Partial Parsing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative Online Algorithms for Multiclass Problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<volume>3</volume>
<pages>951--991</pages>
<contexts>
<context position="8336" citStr="Crammer &amp; Singer, 2003" startWordPosition="1292" endWordPosition="1295"> voting strategies but none performed better. 259 information predicted by another individual model(DeSRleft−to−right). To the best of our knowledge, combining individual parsing models that are inter-dependent is novel. 3 Semantic Role Labeling We implement the Semantic Role Labeling (SRL) problem using three components: PIC, AIC, and reranking of predicted argument frames. 3.1 Predicate Identification and Classification The PIC component carries out the identification of predicates, as well as their partial disambiguation, and it is implemented as a multiclass average Perceptron classifier (Crammer &amp; Singer, 2003). For each token i we extract the following features ((,) stands for token combination): 1. SPLIT LEMMA: from hi−1, ii,i−1, i, i+1, hi, i+1i 2. SPLIT FORM: from i − 2, i − 1, i, i + 1�i + 2 3. PPOSS: from hi−2, i−1i, hi−1, ii, i−1, i, i+1, hi, i+ 1i, hi + 1, i + 2i 4. WORD SHAPE: e.g., “Xx*” for “Brazil”, from hi−2, i− 1, ii, hi − 1, ii, i − 1, i, i + 1, hi, i + 1i, hi, i + 1, i + 2i 5. Number of children of node i 6. For each children j of i: split lemmaj, ppossj, depreli,j, hsplit lemmai, split lemmaji, hppossi, ppossji 7. Difference of positions: j − i, for each child j of i. The PIC compon</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Y. Singer. 2003. Ultraconservative Online Algorithms for Multiclass Problems. Journal of Machine Learning Research 3: pp.951-991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Dell’Orletta</author>
</authors>
<title>Improving the Accuracy of Dependency Parsing. PhD Thesis. Dipartimento di Informatica, Universit`a di</title>
<date>2008</date>
<location>Pisa, forthcoming.</location>
<marker>Dell’Orletta, 2008</marker>
<rawString>F. Dell’Orletta. 2008. Improving the Accuracy of Dependency Parsing. PhD Thesis. Dipartimento di Informatica, Universit`a di Pisa, forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hall</author>
<author>V Novak</author>
</authors>
<title>Corrective Modeling for Non-Projective Dependency Parsing.</title>
<date>2005</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="4829" citStr="Hall &amp; Novak, 2005" startWordPosition="713" endWordPosition="716">s feature improves the accuracy of a shift/reduce dependency parser by providing approximate information about NP chunks in the sentence. In fact no token besides the head of a noun phrase can have a head referring to a token outside the noun phrase. Hence the parser can learn to avoid creating such links. The addition of this feature yields an increase of 0.80% in Labeled Accuracy on the development set. 2.2 Revision Parser: DeSRrevision right−to−left Our second individual parsing model implements an alternative to the method of revising parse trees of Attardi and Ciaramita (2007) (see also (Hall &amp; Novak, 2005)). The original approach consisted in training a classifier to revise the errors of a baseline parser. The approach assumed that only local revisions to the parse tree would be needed, since the dependency parser mostly gets individual phrases correctly. The experiments showed that indeed most of the corrections can be expressed by a small set of (about 20) complex movement rules. Furthermore, there was evidence that one could get higher improvements from the tree revision classifier if this was trained on the output of a lower accuracy parser. The reason for this is that the number of errors </context>
</contexts>
<marker>Hall, Novak, 2005</marker>
<rawString>K. Hall and V. Novak. 2005. Corrective Modeling for Non-Projective Dependency Parsing. In Proc. of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Marquez</author>
<author>L Padro</author>
<author>M Surdeanu</author>
<author>L Villarejo</author>
</authors>
<date>2007</date>
<booktitle>UPC: Experiments with Joint Learning within SemEval Task 9. In Proc. of SemEval</booktitle>
<contexts>
<context position="13629" citStr="Marquez et al., 2007" startWordPosition="2237" endWordPosition="2240">in Fin we store the top k best labels. Then, from the arguments in Fin, we generate the top k frames with the highest score, where the score of a frame is the product of all its argument probabilities, computed as the softmax function on the output of the Perceptron. In this set of candidate frames we mark the frame with the highest F1 score as the positive example and all others as negative examples. From each frame we extract these features: 1. Position of the frame in the set ordered by frame scores. Hence, smaller positions indicate candidate frames that the local model considered better (Marquez et al., 2007). 2. The complete sequence of arguments and predicate for this frame (Toutanova, 2005). We add four variants of this feature: just the sequence and sequence expanded with: (a) predicate voice, (b) predicate split lemma, and (c) combination of voice and split lemma. 3. The complete sequence of arguments and predicate for this frame combined with their PPOSS tags. Same as above, we add four variants of this feature. 4. Overlap with the PropBank or NomBank frame for the same predicate lemma and sense. We add the precision, recall, and F1 score of the overlap as features (Marquez et al., 2007). 5.</context>
</contexts>
<marker>Marquez, Padro, Surdeanu, Villarejo, 2007</marker>
<rawString>L. Marquez, L. Padro, M. Surdeanu, and L. Villarejo. 2007. UPC: Experiments with Joint Learning within SemEval Task 9. In Proc. of SemEval 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>A Lavie</author>
</authors>
<title>Parser Combination by reparsing.</title>
<date>2006</date>
<booktitle>In Proc. of HLT/NAACL.</booktitle>
<contexts>
<context position="17276" citStr="Sagae &amp; Lavie, 2006" startWordPosition="2822" endWordPosition="2825">85.33 80.14 predicted predicted reranking 80.13 79.43 66.41 86.33 85.62 80.41 Table 3: Scores of the SRL component under various configurations. Devel WSJ Brown Unlabeled Fl 92.69 90.88 86.96 Labeled Fl (PIC) 87.29 84.87 71.99 Labeled Fl (Sense 1) 79.62 78.94 70.11 Table 4: Scores of the PIC component. mentioned earlier. These results show that voting helps significantly (+1.56% over the best single parser) even though inter-dependent models were used. However, our simple voting scheme does not guarantee that a well-formed tree is generated, leaving room for further improvements; e.g., as in (Sagae &amp; Lavie, 2006). 4.2 Analysis of SRL Table 3 shows the labeled and unlabeled F1 scores of our SRL component as we move from gold to predicted information for syntax and PIC. For the shared task setting –predicted syntax and predicted PIC– we show results for the two inference strategies implemented: greedy and reranking. The first line in the table indicates that the performance of the SRL component when using gold syntax and gold PIC is good: the labeled F1 is 90 points for the in-domain corpus and approximately 85 points for the out-of-domain corpus. Argument classification suffers the most on out-of-domai</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>K. Sagae and A. Lavie. 2006. Parser Combination by reparsing. In Proc. of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>R Johansson</author>
<author>A Meyers</author>
<author>L M`arquez</author>
<author>J Nivre</author>
</authors>
<date>2008</date>
<booktitle>The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proc. of CoNLL-2008.</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez and J. Nivre. 2008. The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proc. of CoNLL-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>A Haghighi</author>
<author>C Manning</author>
</authors>
<title>Joint Learning Improves Semantic Role Labeling.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="12805" citStr="Toutanova et al. (2005)" startWordPosition="2092" endWordPosition="2095">or split lemmas. 4. Length of the dependency path. 5. Distance in tokens between p and a. 6. Position of a relative to p: before or after. We implemented two inference strategies: greedy and reranking. The greedy strategy sorts all arguments in a frame Fin in descending order of their scores and iteratively adds each argument to the output frame Fout only if it respects the domain constraints with the other arguments already selected. The only domain constraint we use is that core arguments cannot repeat. 3.3 Reranking of Argument Frames The reranking inference strategy adapts the approach of Toutanova et al. (2005) to the dependency representation with notable changes in candidate selection, feature set, and learning model. For candidate selection we modify Algorithm 1: instead of storing only y for each argument in Fin we store the top k best labels. Then, from the arguments in Fin, we generate the top k frames with the highest score, where the score of a frame is the product of all its argument probabilities, computed as the softmax function on the output of the Perceptron. In this set of candidate frames we mark the frame with the highest F1 score as the positive example and all others as negative ex</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>K. Toutanova, A. Haghighi, and C. Manning. 2005. Joint Learning Improves Semantic Role Labeling. In Proc. of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>