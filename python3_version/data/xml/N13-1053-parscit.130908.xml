<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012656">
<title confidence="0.994788">
Negative Deceptive Opinion Spam
</title>
<author confidence="0.995384">
Myle Ott Claire Cardie
</author>
<affiliation confidence="0.996276">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.643929">
Ithaca, NY 14853
</address>
<email confidence="0.998702">
{myleott,cardie}@cs.cornell.edu
</email>
<author confidence="0.990913">
Jeffrey T. Hancock
</author>
<affiliation confidence="0.9946115">
Department of Communication
Cornell University
</affiliation>
<address confidence="0.643288">
Ithaca, NY 14853
</address>
<email confidence="0.998523">
jeff.hancock@cornell.edu
</email>
<sectionHeader confidence="0.995629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99973716">
The rising influence of user-generated online
reviews (Cone, 2011) has led to growing in-
centive for businesses to solicit and manufac-
ture DECEPTIVE OPINION SPAM—fictitious
reviews that have been deliberately written to
sound authentic and deceive the reader. Re-
cently, Ott et al. (2011) have introduced an
opinion spam dataset containing gold standard
deceptive positive hotel reviews. However, the
complementary problem of negative deceptive
opinion spam, intended to slander competitive
offerings, remains largely unstudied. Follow-
ing an approach similar to Ott et al. (2011), in
this work we create and study the first dataset
of deceptive opinion spam with negative sen-
timent reviews. Based on this dataset, we find
that standard n-gram text categorization tech-
niques can detect negative deceptive opinion
spam with performance far surpassing that of
human judges. Finally, in conjunction with
the aforementioned positive review dataset,
we consider the possible interactions between
sentiment and deception, and present initial
results that encourage further exploration of
this relationship.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954435897436">
Consumer’s purchase decisions are increasingly in-
fluenced by user-generated online reviews of prod-
ucts and services (Cone, 2011). Accordingly,
there is a growing incentive for businesses to so-
licit and manufacture DECEPTIVE OPINION SPAM—
fictitious reviews that have been deliberately writ-
ten to sound authentic and deceive the reader (Ott et
al., 2011). For example, Ott et al. (2012) has esti-
mated that between 1% and 6% of positive hotel re-
views appear to be deceptive, suggesting that some
hotels may be posting fake positive reviews in order
to hype their own offerings.
In this work we distinguish between two kinds of
deceptive opinion spam, depending on the sentiment
expressed in the review. In particular, reviews in-
tended to promote or hype an offering, and which
therefore express a positive sentiment towards the
offering, are called positive deceptive opinion spam.
In contrast, reviews intended to disparage or slander
competitive offerings, and which therefore express a
negative sentiment towards the offering, are called
negative deceptive opinion spam. While previous
related work (Ott et al., 2011; Ott et al., 2012) has
explored characteristics of positive deceptive opin-
ion spam, the complementary problem of negative
deceptive opinion spam remains largely unstudied.
Following the framework of Ott et al. (2011), we
use Amazon’s Mechanical Turk service to produce
the first publicly available1 dataset of negative de-
ceptive opinion spam, containing 400 gold standard
deceptive negative reviews of 20 popular Chicago
hotels. To validate the credibility of our decep-
tive reviews, we show that human deception detec-
tion performance on the negative reviews is low, in
agreement with decades of traditional deception de-
tection research (Bond and DePaulo, 2006). We then
show that standard n-gram text categorization tech-
niques can be used to detect negative deceptive opin-
ion spam with approximately 86% accuracy — far
</bodyText>
<footnote confidence="0.9906435">
1Dataset available at: http://www.cs.cornell.
edu/˜myleott/op_spam.
</footnote>
<page confidence="0.956429">
497
</page>
<subsectionHeader confidence="0.287645">
Proceedings of NAACL-HLT 2013, pages 497–501,
</subsectionHeader>
<bodyText confidence="0.9771850625">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
surpassing that of the human judges.
In conjunction with Ott et al. (2011)’s positive de-
ceptive opinion spam dataset, we then explore the
interaction between sentiment and deception with
respect to three types of language features: (1)
changes in first-person singular use, often attributed
to psychological distancing (Newman et al., 2003),
(2) decreased spatial awareness and more narrative
form, consistent with theories of reality monitor-
ing (Johnson and Raye, 1981) and imaginative writ-
ing (Biber et al., 1999; Rayson et al., 2001), and (3)
increased negative emotion terms, often attributed to
leakage cues (Ekman and Friesen, 1969), but per-
haps better explained in our case as an exaggeration
of the underlying review sentiment.
</bodyText>
<sectionHeader confidence="0.995391" genericHeader="introduction">
2 Dataset
</sectionHeader>
<bodyText confidence="0.99996">
One of the biggest challenges facing studies of de-
ception is obtaining labeled data. Recently, Ott et
al. (2011) have proposed an approach for generat-
ing positive deceptive opinion spam using Amazon’s
popular Mechanical Turk crowdsourcing service. In
this section we discuss our efforts to extend Ott et
al. (2011)’s dataset to additionally include negative
deceptive opinion spam.
</bodyText>
<subsectionHeader confidence="0.994389">
2.1 Deceptive Reviews from Mechanical Turk
</subsectionHeader>
<bodyText confidence="0.956214514285714">
Deceptive negative reviews are gathered from Me-
chanical Turk using the same procedure as Ott et
al. (2011). In particular, we create and divide 400
HITs evenly across the 20 most popular hotels in
Chicago, such that we obtain 20 reviews for each
hotel. We allow workers to complete only a single
HIT each, so that each review is written by a unique
worker.2 We further require workers to be located
in the United States and to have an average past ap-
proval rating of at least 90%. We allow a maximum
of 30 minutes to complete the HIT, and reward ac-
cepted submissions with one US dollar ($1).
Each HIT instructs a worker to imagine that they
work for the marketing department of a hotel, and
that their manager has asked them to write a fake
negative review of a competitor’s hotel to be posted
online. Accompanying each HIT is the name and
2While Mechanical Turk does not provide a convenient
mechanism for ensuring the uniqueness of workers, this con-
straint can be enforced with Javascript. The script is available
at:http://uniqueturker.myleott.com.
URL of the hotel for which the fake negative re-
view is to be written, and instructions that: (1) work-
ers should not complete more than one similar HIT,
(2) submissions must be of sufficient quality, i.e.,
written for the correct hotel, legible, reasonable in
length,3 and not plagiarized,4 and, (3) the HIT is for
academic research purposes.
Submissions are manually inspected to ensure
that they are written for the correct hotel and to
ensure that they convey a generally negative senti-
ment.5 The average accepted review length was 178
words, higher than for the positive reviews gathered
by Ott et al. (2011), who report an average review
length of 116 words.
</bodyText>
<subsectionHeader confidence="0.998019">
2.2 Truthful Reviews from the Web
</subsectionHeader>
<bodyText confidence="0.9999583125">
Negative (1- or 2-star) truthful reviews are mined
from six popular online review communities: Expe-
dia, Hotels.com, Orbitz, Priceline, TripAdvisor, and
Yelp. While reviews mined from these communities
cannot be considered gold standard truthful, recent
work (Mayzlin et al., 2012; Ott et al., 2012) suggests
that deception rates among travel review portals is
reasonably small.
Following Ott et al. (2011), we sample a subset
of the available truthful reviews so that we retain an
equal number of truthful and deceptive reviews (20
each) for each hotel. However, because the truthful
reviews are on average longer than our deceptive re-
views, we sample the truthful reviews according to
a log-normal distribution fit to the lengths of our de-
ceptive reviews, similarly to Ott et al. (2011).6
</bodyText>
<sectionHeader confidence="0.980399" genericHeader="method">
3 Deception Detection Performance
</sectionHeader>
<bodyText confidence="0.99972425">
In this section we report the deception detection per-
formance of three human judges (Section 3.1) and
supervised n-gram Support Vector Machine (SVM)
classifiers (Section 3.2).
</bodyText>
<footnote confidence="0.588684888888889">
3We define “reasonable length” to be &gt; 150 characters.
4We use http://plagiarisma.net to determine
whether or not a review is plagiarized.
5We discarded and replaced approximately 2% of the sub-
missions, where it was clear that the worker had misread the
instructions and instead written a deceptive positive review.
6We use the R package GAMLSS (Rigby and Stasinopou-
los, 2005) to fit a log-normal distribution (left truncated at 150
characters) to the lengths of the deceptive reviews.
</footnote>
<page confidence="0.981757">
498
</page>
<table confidence="0.999697428571429">
TRUTHFUL DECEPTIVE
Accuracy P R F P R F
HUMAN JUDGE 1 65.0% 65.0 65.0 65.0 65.0 65.0 65.0
JUDGE 2 61.9% 63.0 57.5 60.1 60.9 66.3 63.5
JUDGE 3 57.5% 57.3 58.8 58.0 57.7 56.3 57.0
META MAJORITY 69.4% 70.1 67.5 68.8 68.7 71.3 69.9
SKEPTIC 58.1% 78.3 22.5 35.0 54.7 93.8 69.1
</table>
<tableCaption confidence="0.999133">
Table 1: Deception detection performance, incl. (P)recision, (R)ecall, and (F)1-score, for three human judges and two
meta-judges on a set of 160 negative reviews. The largest value in each column is indicated with boldface.
</tableCaption>
<subsectionHeader confidence="0.998498">
3.1 Human Performance
</subsectionHeader>
<bodyText confidence="0.999942638297872">
Recent large-scale meta-analyses have shown hu-
man deception detection performance is low, with
accuracies often not much better than chance (Bond
and DePaulo, 2006). Indeed, Ott et al. (2011) found
that two out of three human judges were unable to
perform statistically significantly better than chance
(at the p &lt; 0.05 level) at detecting positive decep-
tive opinion spam. Nevertheless, it is important to
subject our reviews to human judgments to validate
their convincingness. In particular, if human detec-
tion performance is found to be very high, then it
would cast doubt on the usefulness of the Mechan-
ical Turk approach for soliciting gold standard de-
ceptive opinion spam.
Following Ott et al. (2011), we asked three vol-
unteer undergraduate university students to read and
make assessments on a subset of the negative review
dataset described in Section 2. Specifically, we ran-
domized all 40 deceptive and truthful reviews from
each of four hotels (160 reviews total). We then
asked the volunteers to read each review and mark
whether they believed it to be truthful or deceptive.
Performance for the three human judges appears
in Table 1. We additionally show the deception de-
tection performance of two meta-judges that aggre-
gate the assessments of the individual human judges:
(1) the MAJORITY meta-judge predicts deceptive
when at least two out of three human judges predict
deceptive (and truthful otherwise), and (2) the SKEP-
TIC meta-judge predicts deceptive when at least one
out of three human judges predicts deceptive (and
truthful otherwise).
A two-tailed binomial test suggests that JUDGE 1
and JUDGE 2 both perform better than chance (p =
0.0002, 0.003, respectively), while JUDGE 3 fails to
reject the null hypothesis of performing at-chance
(p = 0.07). However, while the best human judge
is accurate 65% of the time, inter-annotator agree-
ment computed using Fleiss’ kappa is only slight
at 0.07 (Landis and Koch, 1977). Furthermore,
based on Cohen’s kappa, the highest pairwise inter-
annotator agreement is only 0.26, between JUDGE
1 and JUDGE 2. These low agreements suggest
that while the judges may perform statistically better
than chance, they are identifying different reviews
as deceptive, i.e., few reviews are consistently iden-
tified as deceptive.
</bodyText>
<subsectionHeader confidence="0.999401">
3.2 Automated Classifier Performance
</subsectionHeader>
<bodyText confidence="0.99992744">
Standard n-gram–based text categorization tech-
niques have been shown to be effective at detect-
ing deception in text (Jindal and Liu, 2008; Mihal-
cea and Strapparava, 2009; Ott et al., 2011; Feng et
al., 2012). Following Ott et al. (2011), we evaluate
the performance of linear Support Vector Machine
(SVM) classifiers trained with unigram and bigram
term-frequency features on our novel negative de-
ceptive opinion spam dataset. We employ the same
5-fold stratified cross-validation (CV) procedure as
Ott et al. (2011), whereby for each cross-validation
iteration we train our model on all reviews for 16
hotels, and test our model on all reviews for the re-
maining 4 hotels. The SVM cost parameter, C, is
tuned by nested cross-validation on the training data.
Results appear in Table 2. Each row lists the sen-
timent of the train and test reviews, where “Cross
Val.” corresponds to the cross-validation procedure
described above, and “Held Out” corresponds to
classifiers trained on reviews of one sentiment and
tested on the other. The results suggest that n-gram–
based SVM classifiers can detect negative decep-
tive opinion spam in a balanced dataset with perfor-
mance far surpassing that of untrained human judges
(see Section 3.1). Furthermore, our results show that
</bodyText>
<page confidence="0.998201">
499
</page>
<table confidence="0.999636454545454">
TRUTHFUL DECEPTIVE
Train Sentiment Test Sentiment Accuracy P R F P R F
POSITIVE POSITIVE (800 reviews, Cross Val.) 89.3% 89.6 88.8 89.2 88.9 89.8 89.3
(800 reviews)
NEGATIVE (800 reviews, Held Out) 75.1% 69.0 91.3 78.6 87.1 59.0 70.3
NEGATIVE POSITIVE (800 reviews, Held Out) 81.4% 76.3 91.0 83.0 88.9 71.8 79.4
(800 reviews)
NEGATIVE (800 reviews, Cross Val.) 86.0% 86.4 85.5 85.9 85.6 86.5 86.1
COMBINED POSITIVE (800 reviews, Cross Val.) 88.4% 87.7 89.3 88.5 89.1 87.5 88.3
(1600 reviews)
NEGATIVE (800 reviews, Cross Val.) 86.0% 85.3 87.0 86.1 86.7 85.0 85.9
</table>
<tableCaption confidence="0.999473">
Table 2: Automated classifier performance for different train and test sets, incl. (P)recision, (R)ecall and (F)1-score.
</tableCaption>
<bodyText confidence="0.999648307692308">
classifiers trained and tested on reviews of differ-
ent sentiments perform worse, despite having more
training data,7 than classifiers trained and tested on
reviews of the same sentiment. This suggests that
cues to deception differ depending on the sentiment
of the text (see Section 4).
Interestingly, we find that training on the com-
bined sentiment dataset results in performance that
is comparable to that of the “same sentiment” classi-
fiers (88.4% vs. 89.3% accuracy for positive reviews
and 86.0% vs. 86.0% accuracy for negative reviews).
This is explainable in part by the increased training
set size (1,280 vs. 640 reviews per 4 training folds).
</bodyText>
<sectionHeader confidence="0.949226" genericHeader="method">
4 Interaction of Sentiment and Deception
</sectionHeader>
<bodyText confidence="0.9835678">
An important question is how language features op-
erate in our fake negative reviews compared with the
fake positive reviews of Ott et al. (2011). For exam-
ple, fake positive reviews included less spatial lan-
guage (e.g., floor, small, location, etc.) because in-
dividuals who had not actually experienced the ho-
tel simply had less spatial detail available for their
review (Johnson and Raye, 1981). This was also the
case for our negative reviews, with less spatial lan-
guage observed for fake negative reviews relative to
truthful. Likewise, our fake negative reviews had
more verbs relative to nouns than truthful, suggest-
ing a more narrative style that is indicative of imag-
inative writing (Biber et al., 1999; Rayson et al.,
2001), a pattern also observed by Ott et al. (2011).
There were, however, several important differ-
ences in the deceptive language of fake negative rel-
ative to fake positive reviews. First, as might be
expected, negative emotion terms were more fre-
7“Cross Val.” classifiers are effectively trained on 80% of
the data and tested on the remaining 20%, whereas “Held Out”
classifiers are trained and tested on 100% of each data.
quent, according to LIWC (Pennebaker et al., 2007),
in our fake negative reviews than in the fake posi-
tive reviews. But, importantly, the fake negative re-
viewers over-produced negative emotion terms (e.g.,
terrible, disappointed) relative to the truthful re-
views in the same way that fake positive reviewers
over-produced positive emotion terms (e.g., elegant,
luxurious). Combined, these data suggest that the
more frequent negative emotion terms in the present
dataset are not the result of “leakage cues” that re-
veal the emotional distress of lying (Ekman and
Friesen, 1969). Instead, the differences suggest that
fake hotel reviewers exaggerate the sentiment they
are trying to convey relative to similarly-valenced
truthful reviews.
Second, the effect of deception on the pattern of
pronoun frequency was not the same across posi-
tive and negative reviews. In particular, while first
person singular pronouns were produced more fre-
quently in fake reviews than truthful, consistent with
the case for positive reviews, the increase was di-
minished in the negative reviews examined here. In
the positive reviews reported by Ott et al. (2011),
the rate of first person singular in fake reviews
(M=4.36%, SD=2.96%) was twice the rate observed
in truthful reviews (M=2.18%, SD=2.04%). In con-
trast, the rate of first person singular in the deceptive
negative reviews (M=4.47%, SD=2.83%) was only
57% greater than for truthful reviews (M=2.85%,
SD=2.23%). These results suggest that the empha-
sis on the self, perhaps as a strategy of convinc-
ing the reader that the author had actually been to
the hotel, is not as evident in the fake negative re-
views, perhaps because the negative tone of the re-
views caused the reviewers to psychologically dis-
tance themselves from their negative statements, a
phenomenon observed in several other deception
studies, e.g., Hancock et al. (2008).
</bodyText>
<page confidence="0.990098">
500
</page>
<sectionHeader confidence="0.998559" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999935192307692">
We have created the first publicly-available corpus
of gold standard negative deceptive opinion spam,
containing 400 reviews of 20 Chicago hotels, which
we have used to compare the deception detection ca-
pabilities of untrained human judges and standard
n-gram–based Support Vector Machine classifiers.
Our results demonstrate that while human deception
detection performance is greater for negative rather
than positive deceptive opinion spam, the best detec-
tion performance is still achieved through automated
classifiers, with approximately 86% accuracy.
We have additionally explored, albeit briefly, the
relationship between sentiment and deception by
utilizing Ott et al. (2011)’s positive deceptive opin-
ion spam dataset in conjunction with our own. In
particular, we have identified several features of lan-
guage that seem to remain consistent across senti-
ment, such as decreased awareness of spatial details
and exaggerated language. We have also identified
other features that vary with the sentiment, such as
first person singular use, although further work is re-
quired to determine if these differences may be ex-
ploited to improve deception detection performance.
Indeed, future work may wish to jointly model sen-
timent and deception in order to better determine the
effect each has on language use.
</bodyText>
<sectionHeader confidence="0.997643" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999391166666667">
This work was supported in part by NSF Grant BCS-
0904822, a DARPA Deft grant, the Jack Kent Cooke
Foundation, and by a gift from Google. We also thank
the three Cornell undergraduate volunteer judges, as well
as the NAACL reviewers for their insightful comments,
suggestions and advice on various aspects of this work.
</bodyText>
<sectionHeader confidence="0.998578" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99981725">
D. Biber, S. Johansson, G. Leech, S. Conrad, E. Finegan,
and R. Quirk. 1999. Longman grammar of spoken and
written English, volume 2. MIT Press.
C.F. Bond and B.M. DePaulo. 2006. Accuracy of de-
ception judgments. Personality and Social Psychology
Review, 10(3):214.
Cone. 2011. 2011 Online Influence Trend Tracker. On-
line: http://www.coneinc.com/negative-
reviews-online-reverse-purchase-
decisions, August.
P. Ekman and W.V. Friesen. 1969. Nonverbal leakage
and clues to deception. Psychiatry, 32(1):88.
Song Feng, Ritwik Banerjee, and Yejin Choi. 2012. Syn-
tactic stylometry for deception detection. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Short Papers-Volume
2, pages 171–175. Association for Computational Lin-
guistics.
J.T. Hancock, L.E. Curry, S. Goorha, and M. Woodworth.
2008. On lying and being lied to: A linguistic anal-
ysis of deception in computer-mediated communica-
tion. Discourse Processes, 45(1):1–23.
N. Jindal and B. Liu. 2008. Opinion spam and analysis.
In Proceedings of the international conference on Web
search and web data mining, pages 219–230. ACM.
M.K. Johnson and C.L. Raye. 1981. Reality monitoring.
Psychological Review, 88(1):67–85.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159.
Dina Mayzlin, Yaniv Dover, and Judith A Chevalier.
2012. Promotional reviews: An empirical investiga-
tion of online review manipulation. Technical report,
National Bureau of Economic Research.
R. Mihalcea and C. Strapparava. 2009. The lie detector:
Explorations in the automatic recognition of deceptive
language. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 309–312. Association
for Computational Linguistics.
M.L. Newman, J.W. Pennebaker, D.S. Berry, and J.M.
Richards. 2003. Lying words: Predicting deception
from linguistic styles. Personality and Social Psychol-
ogy Bulletin, 29(5):665.
M. Ott, Y. Choi, C. Cardie, and J.T. Hancock. 2011.
Finding deceptive opinion spam by any stretch of the
imagination. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages 309–
319. Association for Computational Linguistics.
Myle Ott, Claire Cardie, and Jeff Hancock. 2012. Es-
timating the prevalence of deception in online review
communities. In Proceedings of the 21st international
conference on World Wide Web, pages 201–210. ACM.
J.W. Pennebaker, C.K. Chung, M. Ireland, A. Gonzales,
and R.J. Booth. 2007. The development and psycho-
metric properties of LIWC2007. Austin, TX: LIWC
(www.liwc.net).
P. Rayson, A. Wilson, and G. Leech. 2001. Grammatical
word class variation within the British National Cor-
pus sampler. Language and Computers, 36(1):295–
306.
R. A. Rigby and D. M. Stasinopoulos. 2005. Generalized
additive models for location, scale and shape,(with dis-
cussion). Applied Statistics, 54:507–554.
</reference>
<page confidence="0.997728">
501
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.688346">
<title confidence="0.99971">Negative Deceptive Opinion Spam</title>
<author confidence="0.991311">Myle Ott Claire Cardie</author>
<affiliation confidence="0.962605">Department of Computer Cornell</affiliation>
<address confidence="0.879706">Ithaca, NY</address>
<email confidence="0.998891">myleott@cs.cornell.edu</email>
<email confidence="0.998891">cardie@cs.cornell.edu</email>
<author confidence="0.999511">T Jeffrey</author>
<affiliation confidence="0.97939">Department of Cornell</affiliation>
<address confidence="0.916126">Ithaca, NY</address>
<email confidence="0.999715">jeff.hancock@cornell.edu</email>
<abstract confidence="0.999223653846154">The rising influence of user-generated online reviews (Cone, 2011) has led to growing incentive for businesses to solicit and manufac- OPINION reviews that have been deliberately written to sound authentic and deceive the reader. Recently, Ott et al. (2011) have introduced an opinion spam dataset containing gold standard reviews. However, the problem of opinion spam, intended to slander competitive offerings, remains largely unstudied. Following an approach similar to Ott et al. (2011), in this work we create and study the first dataset deceptive opinion spam with sentiment reviews. Based on this dataset, we find standard text categorization techniques can detect negative deceptive opinion spam with performance far surpassing that of human judges. Finally, in conjunction with aforementioned dataset, we consider the possible interactions between sentiment and deception, and present initial results that encourage further exploration of this relationship.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Biber</author>
<author>S Johansson</author>
<author>G Leech</author>
<author>S Conrad</author>
<author>E Finegan</author>
<author>R Quirk</author>
</authors>
<title>Longman grammar of spoken and written English,</title>
<date>1999</date>
<volume>2</volume>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4036" citStr="Biber et al., 1999" startWordPosition="588" endWordPosition="591">2013, pages 497–501, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics surpassing that of the human judges. In conjunction with Ott et al. (2011)’s positive deceptive opinion spam dataset, we then explore the interaction between sentiment and deception with respect to three types of language features: (1) changes in first-person singular use, often attributed to psychological distancing (Newman et al., 2003), (2) decreased spatial awareness and more narrative form, consistent with theories of reality monitoring (Johnson and Raye, 1981) and imaginative writing (Biber et al., 1999; Rayson et al., 2001), and (3) increased negative emotion terms, often attributed to leakage cues (Ekman and Friesen, 1969), but perhaps better explained in our case as an exaggeration of the underlying review sentiment. 2 Dataset One of the biggest challenges facing studies of deception is obtaining labeled data. Recently, Ott et al. (2011) have proposed an approach for generating positive deceptive opinion spam using Amazon’s popular Mechanical Turk crowdsourcing service. In this section we discuss our efforts to extend Ott et al. (2011)’s dataset to additionally include negative deceptive </context>
<context position="14121" citStr="Biber et al., 1999" startWordPosition="2223" endWordPosition="2226">ed with the fake positive reviews of Ott et al. (2011). For example, fake positive reviews included less spatial language (e.g., floor, small, location, etc.) because individuals who had not actually experienced the hotel simply had less spatial detail available for their review (Johnson and Raye, 1981). This was also the case for our negative reviews, with less spatial language observed for fake negative reviews relative to truthful. Likewise, our fake negative reviews had more verbs relative to nouns than truthful, suggesting a more narrative style that is indicative of imaginative writing (Biber et al., 1999; Rayson et al., 2001), a pattern also observed by Ott et al. (2011). There were, however, several important differences in the deceptive language of fake negative relative to fake positive reviews. First, as might be expected, negative emotion terms were more fre7“Cross Val.” classifiers are effectively trained on 80% of the data and tested on the remaining 20%, whereas “Held Out” classifiers are trained and tested on 100% of each data. quent, according to LIWC (Pennebaker et al., 2007), in our fake negative reviews than in the fake positive reviews. But, importantly, the fake negative review</context>
</contexts>
<marker>Biber, Johansson, Leech, Conrad, Finegan, Quirk, 1999</marker>
<rawString>D. Biber, S. Johansson, G. Leech, S. Conrad, E. Finegan, and R. Quirk. 1999. Longman grammar of spoken and written English, volume 2. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C F Bond</author>
<author>B M DePaulo</author>
</authors>
<title>Accuracy of deception judgments.</title>
<date>2006</date>
<journal>Personality and Social Psychology Review,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="3162" citStr="Bond and DePaulo, 2006" startWordPosition="462" endWordPosition="465">cteristics of positive deceptive opinion spam, the complementary problem of negative deceptive opinion spam remains largely unstudied. Following the framework of Ott et al. (2011), we use Amazon’s Mechanical Turk service to produce the first publicly available1 dataset of negative deceptive opinion spam, containing 400 gold standard deceptive negative reviews of 20 popular Chicago hotels. To validate the credibility of our deceptive reviews, we show that human deception detection performance on the negative reviews is low, in agreement with decades of traditional deception detection research (Bond and DePaulo, 2006). We then show that standard n-gram text categorization techniques can be used to detect negative deceptive opinion spam with approximately 86% accuracy — far 1Dataset available at: http://www.cs.cornell. edu/˜myleott/op_spam. 497 Proceedings of NAACL-HLT 2013, pages 497–501, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics surpassing that of the human judges. In conjunction with Ott et al. (2011)’s positive deceptive opinion spam dataset, we then explore the interaction between sentiment and deception with respect to three types of language features: (1) chan</context>
<context position="8615" citStr="Bond and DePaulo, 2006" startWordPosition="1337" endWordPosition="1340">65.0 65.0 65.0 65.0 65.0 65.0 JUDGE 2 61.9% 63.0 57.5 60.1 60.9 66.3 63.5 JUDGE 3 57.5% 57.3 58.8 58.0 57.7 56.3 57.0 META MAJORITY 69.4% 70.1 67.5 68.8 68.7 71.3 69.9 SKEPTIC 58.1% 78.3 22.5 35.0 54.7 93.8 69.1 Table 1: Deception detection performance, incl. (P)recision, (R)ecall, and (F)1-score, for three human judges and two meta-judges on a set of 160 negative reviews. The largest value in each column is indicated with boldface. 3.1 Human Performance Recent large-scale meta-analyses have shown human deception detection performance is low, with accuracies often not much better than chance (Bond and DePaulo, 2006). Indeed, Ott et al. (2011) found that two out of three human judges were unable to perform statistically significantly better than chance (at the p &lt; 0.05 level) at detecting positive deceptive opinion spam. Nevertheless, it is important to subject our reviews to human judgments to validate their convincingness. In particular, if human detection performance is found to be very high, then it would cast doubt on the usefulness of the Mechanical Turk approach for soliciting gold standard deceptive opinion spam. Following Ott et al. (2011), we asked three volunteer undergraduate university studen</context>
</contexts>
<marker>Bond, DePaulo, 2006</marker>
<rawString>C.F. Bond and B.M. DePaulo. 2006. Accuracy of deception judgments. Personality and Social Psychology Review, 10(3):214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cone</author>
</authors>
<title>Online Influence Trend Tracker. Online: http://www.coneinc.com/negativereviews-online-reverse-purchasedecisions,</title>
<date>2011</date>
<contexts>
<context position="1512" citStr="Cone, 2011" startWordPosition="208" endWordPosition="209">of deceptive opinion spam with negative sentiment reviews. Based on this dataset, we find that standard n-gram text categorization techniques can detect negative deceptive opinion spam with performance far surpassing that of human judges. Finally, in conjunction with the aforementioned positive review dataset, we consider the possible interactions between sentiment and deception, and present initial results that encourage further exploration of this relationship. 1 Introduction Consumer’s purchase decisions are increasingly influenced by user-generated online reviews of products and services (Cone, 2011). Accordingly, there is a growing incentive for businesses to solicit and manufacture DECEPTIVE OPINION SPAM— fictitious reviews that have been deliberately written to sound authentic and deceive the reader (Ott et al., 2011). For example, Ott et al. (2012) has estimated that between 1% and 6% of positive hotel reviews appear to be deceptive, suggesting that some hotels may be posting fake positive reviews in order to hype their own offerings. In this work we distinguish between two kinds of deceptive opinion spam, depending on the sentiment expressed in the review. In particular, reviews inte</context>
</contexts>
<marker>Cone, 2011</marker>
<rawString>Cone. 2011. 2011 Online Influence Trend Tracker. Online: http://www.coneinc.com/negativereviews-online-reverse-purchasedecisions, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
<author>W V Friesen</author>
</authors>
<title>Nonverbal leakage and clues to deception.</title>
<date>1969</date>
<journal>Psychiatry,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="4160" citStr="Ekman and Friesen, 1969" startWordPosition="607" endWordPosition="610">t of the human judges. In conjunction with Ott et al. (2011)’s positive deceptive opinion spam dataset, we then explore the interaction between sentiment and deception with respect to three types of language features: (1) changes in first-person singular use, often attributed to psychological distancing (Newman et al., 2003), (2) decreased spatial awareness and more narrative form, consistent with theories of reality monitoring (Johnson and Raye, 1981) and imaginative writing (Biber et al., 1999; Rayson et al., 2001), and (3) increased negative emotion terms, often attributed to leakage cues (Ekman and Friesen, 1969), but perhaps better explained in our case as an exaggeration of the underlying review sentiment. 2 Dataset One of the biggest challenges facing studies of deception is obtaining labeled data. Recently, Ott et al. (2011) have proposed an approach for generating positive deceptive opinion spam using Amazon’s popular Mechanical Turk crowdsourcing service. In this section we discuss our efforts to extend Ott et al. (2011)’s dataset to additionally include negative deceptive opinion spam. 2.1 Deceptive Reviews from Mechanical Turk Deceptive negative reviews are gathered from Mechanical Turk using </context>
<context position="15140" citStr="Ekman and Friesen, 1969" startWordPosition="2386" endWordPosition="2389">s are trained and tested on 100% of each data. quent, according to LIWC (Pennebaker et al., 2007), in our fake negative reviews than in the fake positive reviews. But, importantly, the fake negative reviewers over-produced negative emotion terms (e.g., terrible, disappointed) relative to the truthful reviews in the same way that fake positive reviewers over-produced positive emotion terms (e.g., elegant, luxurious). Combined, these data suggest that the more frequent negative emotion terms in the present dataset are not the result of “leakage cues” that reveal the emotional distress of lying (Ekman and Friesen, 1969). Instead, the differences suggest that fake hotel reviewers exaggerate the sentiment they are trying to convey relative to similarly-valenced truthful reviews. Second, the effect of deception on the pattern of pronoun frequency was not the same across positive and negative reviews. In particular, while first person singular pronouns were produced more frequently in fake reviews than truthful, consistent with the case for positive reviews, the increase was diminished in the negative reviews examined here. In the positive reviews reported by Ott et al. (2011), the rate of first person singular </context>
</contexts>
<marker>Ekman, Friesen, 1969</marker>
<rawString>P. Ekman and W.V. Friesen. 1969. Nonverbal leakage and clues to deception. Psychiatry, 32(1):88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Feng</author>
<author>Ritwik Banerjee</author>
<author>Yejin Choi</author>
</authors>
<title>Syntactic stylometry for deception detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,</booktitle>
<pages>171--175</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10972" citStr="Feng et al., 2012" startWordPosition="1716" endWordPosition="1719">ight at 0.07 (Landis and Koch, 1977). Furthermore, based on Cohen’s kappa, the highest pairwise interannotator agreement is only 0.26, between JUDGE 1 and JUDGE 2. These low agreements suggest that while the judges may perform statistically better than chance, they are identifying different reviews as deceptive, i.e., few reviews are consistently identified as deceptive. 3.2 Automated Classifier Performance Standard n-gram–based text categorization techniques have been shown to be effective at detecting deception in text (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009; Ott et al., 2011; Feng et al., 2012). Following Ott et al. (2011), we evaluate the performance of linear Support Vector Machine (SVM) classifiers trained with unigram and bigram term-frequency features on our novel negative deceptive opinion spam dataset. We employ the same 5-fold stratified cross-validation (CV) procedure as Ott et al. (2011), whereby for each cross-validation iteration we train our model on all reviews for 16 hotels, and test our model on all reviews for the remaining 4 hotels. The SVM cost parameter, C, is tuned by nested cross-validation on the training data. Results appear in Table 2. Each row lists the sen</context>
</contexts>
<marker>Feng, Banerjee, Choi, 2012</marker>
<rawString>Song Feng, Ritwik Banerjee, and Yejin Choi. 2012. Syntactic stylometry for deception detection. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 171–175. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Hancock</author>
<author>L E Curry</author>
<author>S Goorha</author>
<author>M Woodworth</author>
</authors>
<title>On lying and being lied to: A linguistic analysis of deception in computer-mediated communication.</title>
<date>2008</date>
<booktitle>Discourse Processes,</booktitle>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="16436" citStr="Hancock et al. (2008)" startWordPosition="2592" endWordPosition="2595"> reviews (M=2.18%, SD=2.04%). In contrast, the rate of first person singular in the deceptive negative reviews (M=4.47%, SD=2.83%) was only 57% greater than for truthful reviews (M=2.85%, SD=2.23%). These results suggest that the emphasis on the self, perhaps as a strategy of convincing the reader that the author had actually been to the hotel, is not as evident in the fake negative reviews, perhaps because the negative tone of the reviews caused the reviewers to psychologically distance themselves from their negative statements, a phenomenon observed in several other deception studies, e.g., Hancock et al. (2008). 500 5 Conclusion We have created the first publicly-available corpus of gold standard negative deceptive opinion spam, containing 400 reviews of 20 Chicago hotels, which we have used to compare the deception detection capabilities of untrained human judges and standard n-gram–based Support Vector Machine classifiers. Our results demonstrate that while human deception detection performance is greater for negative rather than positive deceptive opinion spam, the best detection performance is still achieved through automated classifiers, with approximately 86% accuracy. We have additionally exp</context>
</contexts>
<marker>Hancock, Curry, Goorha, Woodworth, 2008</marker>
<rawString>J.T. Hancock, L.E. Curry, S. Goorha, and M. Woodworth. 2008. On lying and being lied to: A linguistic analysis of deception in computer-mediated communication. Discourse Processes, 45(1):1–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Jindal</author>
<author>B Liu</author>
</authors>
<title>Opinion spam and analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the international conference on Web search and web data mining,</booktitle>
<pages>219--230</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10902" citStr="Jindal and Liu, 2008" startWordPosition="1703" endWordPosition="1706"> time, inter-annotator agreement computed using Fleiss’ kappa is only slight at 0.07 (Landis and Koch, 1977). Furthermore, based on Cohen’s kappa, the highest pairwise interannotator agreement is only 0.26, between JUDGE 1 and JUDGE 2. These low agreements suggest that while the judges may perform statistically better than chance, they are identifying different reviews as deceptive, i.e., few reviews are consistently identified as deceptive. 3.2 Automated Classifier Performance Standard n-gram–based text categorization techniques have been shown to be effective at detecting deception in text (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009; Ott et al., 2011; Feng et al., 2012). Following Ott et al. (2011), we evaluate the performance of linear Support Vector Machine (SVM) classifiers trained with unigram and bigram term-frequency features on our novel negative deceptive opinion spam dataset. We employ the same 5-fold stratified cross-validation (CV) procedure as Ott et al. (2011), whereby for each cross-validation iteration we train our model on all reviews for 16 hotels, and test our model on all reviews for the remaining 4 hotels. The SVM cost parameter, C, is tuned by nested cross-validation o</context>
</contexts>
<marker>Jindal, Liu, 2008</marker>
<rawString>N. Jindal and B. Liu. 2008. Opinion spam and analysis. In Proceedings of the international conference on Web search and web data mining, pages 219–230. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M K Johnson</author>
<author>C L Raye</author>
</authors>
<title>Reality monitoring.</title>
<date>1981</date>
<journal>Psychological Review,</journal>
<volume>88</volume>
<issue>1</issue>
<contexts>
<context position="3992" citStr="Johnson and Raye, 1981" startWordPosition="580" endWordPosition="583">u/˜myleott/op_spam. 497 Proceedings of NAACL-HLT 2013, pages 497–501, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics surpassing that of the human judges. In conjunction with Ott et al. (2011)’s positive deceptive opinion spam dataset, we then explore the interaction between sentiment and deception with respect to three types of language features: (1) changes in first-person singular use, often attributed to psychological distancing (Newman et al., 2003), (2) decreased spatial awareness and more narrative form, consistent with theories of reality monitoring (Johnson and Raye, 1981) and imaginative writing (Biber et al., 1999; Rayson et al., 2001), and (3) increased negative emotion terms, often attributed to leakage cues (Ekman and Friesen, 1969), but perhaps better explained in our case as an exaggeration of the underlying review sentiment. 2 Dataset One of the biggest challenges facing studies of deception is obtaining labeled data. Recently, Ott et al. (2011) have proposed an approach for generating positive deceptive opinion spam using Amazon’s popular Mechanical Turk crowdsourcing service. In this section we discuss our efforts to extend Ott et al. (2011)’s dataset</context>
<context position="13807" citStr="Johnson and Raye, 1981" startWordPosition="2171" endWordPosition="2174">uracy for positive reviews and 86.0% vs. 86.0% accuracy for negative reviews). This is explainable in part by the increased training set size (1,280 vs. 640 reviews per 4 training folds). 4 Interaction of Sentiment and Deception An important question is how language features operate in our fake negative reviews compared with the fake positive reviews of Ott et al. (2011). For example, fake positive reviews included less spatial language (e.g., floor, small, location, etc.) because individuals who had not actually experienced the hotel simply had less spatial detail available for their review (Johnson and Raye, 1981). This was also the case for our negative reviews, with less spatial language observed for fake negative reviews relative to truthful. Likewise, our fake negative reviews had more verbs relative to nouns than truthful, suggesting a more narrative style that is indicative of imaginative writing (Biber et al., 1999; Rayson et al., 2001), a pattern also observed by Ott et al. (2011). There were, however, several important differences in the deceptive language of fake negative relative to fake positive reviews. First, as might be expected, negative emotion terms were more fre7“Cross Val.” classifi</context>
</contexts>
<marker>Johnson, Raye, 1981</marker>
<rawString>M.K. Johnson and C.L. Raye. 1981. Reality monitoring. Psychological Review, 88(1):67–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="10390" citStr="Landis and Koch, 1977" startWordPosition="1627" endWordPosition="1630">icts deceptive when at least two out of three human judges predict deceptive (and truthful otherwise), and (2) the SKEPTIC meta-judge predicts deceptive when at least one out of three human judges predicts deceptive (and truthful otherwise). A two-tailed binomial test suggests that JUDGE 1 and JUDGE 2 both perform better than chance (p = 0.0002, 0.003, respectively), while JUDGE 3 fails to reject the null hypothesis of performing at-chance (p = 0.07). However, while the best human judge is accurate 65% of the time, inter-annotator agreement computed using Fleiss’ kappa is only slight at 0.07 (Landis and Koch, 1977). Furthermore, based on Cohen’s kappa, the highest pairwise interannotator agreement is only 0.26, between JUDGE 1 and JUDGE 2. These low agreements suggest that while the judges may perform statistically better than chance, they are identifying different reviews as deceptive, i.e., few reviews are consistently identified as deceptive. 3.2 Automated Classifier Performance Standard n-gram–based text categorization techniques have been shown to be effective at detecting deception in text (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009; Ott et al., 2011; Feng et al., 2012). Following Ott et</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J.R. Landis and G.G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dina Mayzlin</author>
<author>Yaniv Dover</author>
<author>Judith A Chevalier</author>
</authors>
<title>Promotional reviews: An empirical investigation of online review manipulation.</title>
<date>2012</date>
<tech>Technical report,</tech>
<institution>National Bureau of Economic Research.</institution>
<contexts>
<context position="6721" citStr="Mayzlin et al., 2012" startWordPosition="1029" endWordPosition="1032">e manually inspected to ensure that they are written for the correct hotel and to ensure that they convey a generally negative sentiment.5 The average accepted review length was 178 words, higher than for the positive reviews gathered by Ott et al. (2011), who report an average review length of 116 words. 2.2 Truthful Reviews from the Web Negative (1- or 2-star) truthful reviews are mined from six popular online review communities: Expedia, Hotels.com, Orbitz, Priceline, TripAdvisor, and Yelp. While reviews mined from these communities cannot be considered gold standard truthful, recent work (Mayzlin et al., 2012; Ott et al., 2012) suggests that deception rates among travel review portals is reasonably small. Following Ott et al. (2011), we sample a subset of the available truthful reviews so that we retain an equal number of truthful and deceptive reviews (20 each) for each hotel. However, because the truthful reviews are on average longer than our deceptive reviews, we sample the truthful reviews according to a log-normal distribution fit to the lengths of our deceptive reviews, similarly to Ott et al. (2011).6 3 Deception Detection Performance In this section we report the deception detection perfo</context>
</contexts>
<marker>Mayzlin, Dover, Chevalier, 2012</marker>
<rawString>Dina Mayzlin, Yaniv Dover, and Judith A Chevalier. 2012. Promotional reviews: An empirical investigation of online review manipulation. Technical report, National Bureau of Economic Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Strapparava</author>
</authors>
<title>The lie detector: Explorations in the automatic recognition of deceptive language.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>309--312</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10934" citStr="Mihalcea and Strapparava, 2009" startWordPosition="1707" endWordPosition="1711"> agreement computed using Fleiss’ kappa is only slight at 0.07 (Landis and Koch, 1977). Furthermore, based on Cohen’s kappa, the highest pairwise interannotator agreement is only 0.26, between JUDGE 1 and JUDGE 2. These low agreements suggest that while the judges may perform statistically better than chance, they are identifying different reviews as deceptive, i.e., few reviews are consistently identified as deceptive. 3.2 Automated Classifier Performance Standard n-gram–based text categorization techniques have been shown to be effective at detecting deception in text (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009; Ott et al., 2011; Feng et al., 2012). Following Ott et al. (2011), we evaluate the performance of linear Support Vector Machine (SVM) classifiers trained with unigram and bigram term-frequency features on our novel negative deceptive opinion spam dataset. We employ the same 5-fold stratified cross-validation (CV) procedure as Ott et al. (2011), whereby for each cross-validation iteration we train our model on all reviews for 16 hotels, and test our model on all reviews for the remaining 4 hotels. The SVM cost parameter, C, is tuned by nested cross-validation on the training data. Results app</context>
</contexts>
<marker>Mihalcea, Strapparava, 2009</marker>
<rawString>R. Mihalcea and C. Strapparava. 2009. The lie detector: Explorations in the automatic recognition of deceptive language. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 309–312. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Newman</author>
<author>J W Pennebaker</author>
<author>D S Berry</author>
<author>J M Richards</author>
</authors>
<title>Lying words: Predicting deception from linguistic styles.</title>
<date>2003</date>
<journal>Personality and Social Psychology Bulletin,</journal>
<volume>29</volume>
<issue>5</issue>
<contexts>
<context position="3862" citStr="Newman et al., 2003" startWordPosition="561" endWordPosition="564">o detect negative deceptive opinion spam with approximately 86% accuracy — far 1Dataset available at: http://www.cs.cornell. edu/˜myleott/op_spam. 497 Proceedings of NAACL-HLT 2013, pages 497–501, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics surpassing that of the human judges. In conjunction with Ott et al. (2011)’s positive deceptive opinion spam dataset, we then explore the interaction between sentiment and deception with respect to three types of language features: (1) changes in first-person singular use, often attributed to psychological distancing (Newman et al., 2003), (2) decreased spatial awareness and more narrative form, consistent with theories of reality monitoring (Johnson and Raye, 1981) and imaginative writing (Biber et al., 1999; Rayson et al., 2001), and (3) increased negative emotion terms, often attributed to leakage cues (Ekman and Friesen, 1969), but perhaps better explained in our case as an exaggeration of the underlying review sentiment. 2 Dataset One of the biggest challenges facing studies of deception is obtaining labeled data. Recently, Ott et al. (2011) have proposed an approach for generating positive deceptive opinion spam using Am</context>
</contexts>
<marker>Newman, Pennebaker, Berry, Richards, 2003</marker>
<rawString>M.L. Newman, J.W. Pennebaker, D.S. Berry, and J.M. Richards. 2003. Lying words: Predicting deception from linguistic styles. Personality and Social Psychology Bulletin, 29(5):665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ott</author>
<author>Y Choi</author>
<author>C Cardie</author>
<author>J T Hancock</author>
</authors>
<title>Finding deceptive opinion spam by any stretch of the imagination.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>309--319</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="848" citStr="Ott et al. (2011)" startWordPosition="111" endWordPosition="114">14853 jeff.hancock@cornell.edu Abstract The rising influence of user-generated online reviews (Cone, 2011) has led to growing incentive for businesses to solicit and manufacture DECEPTIVE OPINION SPAM—fictitious reviews that have been deliberately written to sound authentic and deceive the reader. Recently, Ott et al. (2011) have introduced an opinion spam dataset containing gold standard deceptive positive hotel reviews. However, the complementary problem of negative deceptive opinion spam, intended to slander competitive offerings, remains largely unstudied. Following an approach similar to Ott et al. (2011), in this work we create and study the first dataset of deceptive opinion spam with negative sentiment reviews. Based on this dataset, we find that standard n-gram text categorization techniques can detect negative deceptive opinion spam with performance far surpassing that of human judges. Finally, in conjunction with the aforementioned positive review dataset, we consider the possible interactions between sentiment and deception, and present initial results that encourage further exploration of this relationship. 1 Introduction Consumer’s purchase decisions are increasingly influenced by use</context>
<context position="2501" citStr="Ott et al., 2011" startWordPosition="362" endWordPosition="365">may be posting fake positive reviews in order to hype their own offerings. In this work we distinguish between two kinds of deceptive opinion spam, depending on the sentiment expressed in the review. In particular, reviews intended to promote or hype an offering, and which therefore express a positive sentiment towards the offering, are called positive deceptive opinion spam. In contrast, reviews intended to disparage or slander competitive offerings, and which therefore express a negative sentiment towards the offering, are called negative deceptive opinion spam. While previous related work (Ott et al., 2011; Ott et al., 2012) has explored characteristics of positive deceptive opinion spam, the complementary problem of negative deceptive opinion spam remains largely unstudied. Following the framework of Ott et al. (2011), we use Amazon’s Mechanical Turk service to produce the first publicly available1 dataset of negative deceptive opinion spam, containing 400 gold standard deceptive negative reviews of 20 popular Chicago hotels. To validate the credibility of our deceptive reviews, we show that human deception detection performance on the negative reviews is low, in agreement with decades of trad</context>
<context position="4380" citStr="Ott et al. (2011)" startWordPosition="644" endWordPosition="647">es in first-person singular use, often attributed to psychological distancing (Newman et al., 2003), (2) decreased spatial awareness and more narrative form, consistent with theories of reality monitoring (Johnson and Raye, 1981) and imaginative writing (Biber et al., 1999; Rayson et al., 2001), and (3) increased negative emotion terms, often attributed to leakage cues (Ekman and Friesen, 1969), but perhaps better explained in our case as an exaggeration of the underlying review sentiment. 2 Dataset One of the biggest challenges facing studies of deception is obtaining labeled data. Recently, Ott et al. (2011) have proposed an approach for generating positive deceptive opinion spam using Amazon’s popular Mechanical Turk crowdsourcing service. In this section we discuss our efforts to extend Ott et al. (2011)’s dataset to additionally include negative deceptive opinion spam. 2.1 Deceptive Reviews from Mechanical Turk Deceptive negative reviews are gathered from Mechanical Turk using the same procedure as Ott et al. (2011). In particular, we create and divide 400 HITs evenly across the 20 most popular hotels in Chicago, such that we obtain 20 reviews for each hotel. We allow workers to complete only </context>
<context position="6356" citStr="Ott et al. (2011)" startWordPosition="974" endWordPosition="977">.com. URL of the hotel for which the fake negative review is to be written, and instructions that: (1) workers should not complete more than one similar HIT, (2) submissions must be of sufficient quality, i.e., written for the correct hotel, legible, reasonable in length,3 and not plagiarized,4 and, (3) the HIT is for academic research purposes. Submissions are manually inspected to ensure that they are written for the correct hotel and to ensure that they convey a generally negative sentiment.5 The average accepted review length was 178 words, higher than for the positive reviews gathered by Ott et al. (2011), who report an average review length of 116 words. 2.2 Truthful Reviews from the Web Negative (1- or 2-star) truthful reviews are mined from six popular online review communities: Expedia, Hotels.com, Orbitz, Priceline, TripAdvisor, and Yelp. While reviews mined from these communities cannot be considered gold standard truthful, recent work (Mayzlin et al., 2012; Ott et al., 2012) suggests that deception rates among travel review portals is reasonably small. Following Ott et al. (2011), we sample a subset of the available truthful reviews so that we retain an equal number of truthful and dece</context>
<context position="8642" citStr="Ott et al. (2011)" startWordPosition="1342" endWordPosition="1345">GE 2 61.9% 63.0 57.5 60.1 60.9 66.3 63.5 JUDGE 3 57.5% 57.3 58.8 58.0 57.7 56.3 57.0 META MAJORITY 69.4% 70.1 67.5 68.8 68.7 71.3 69.9 SKEPTIC 58.1% 78.3 22.5 35.0 54.7 93.8 69.1 Table 1: Deception detection performance, incl. (P)recision, (R)ecall, and (F)1-score, for three human judges and two meta-judges on a set of 160 negative reviews. The largest value in each column is indicated with boldface. 3.1 Human Performance Recent large-scale meta-analyses have shown human deception detection performance is low, with accuracies often not much better than chance (Bond and DePaulo, 2006). Indeed, Ott et al. (2011) found that two out of three human judges were unable to perform statistically significantly better than chance (at the p &lt; 0.05 level) at detecting positive deceptive opinion spam. Nevertheless, it is important to subject our reviews to human judgments to validate their convincingness. In particular, if human detection performance is found to be very high, then it would cast doubt on the usefulness of the Mechanical Turk approach for soliciting gold standard deceptive opinion spam. Following Ott et al. (2011), we asked three volunteer undergraduate university students to read and make assessm</context>
<context position="10952" citStr="Ott et al., 2011" startWordPosition="1712" endWordPosition="1715">’ kappa is only slight at 0.07 (Landis and Koch, 1977). Furthermore, based on Cohen’s kappa, the highest pairwise interannotator agreement is only 0.26, between JUDGE 1 and JUDGE 2. These low agreements suggest that while the judges may perform statistically better than chance, they are identifying different reviews as deceptive, i.e., few reviews are consistently identified as deceptive. 3.2 Automated Classifier Performance Standard n-gram–based text categorization techniques have been shown to be effective at detecting deception in text (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009; Ott et al., 2011; Feng et al., 2012). Following Ott et al. (2011), we evaluate the performance of linear Support Vector Machine (SVM) classifiers trained with unigram and bigram term-frequency features on our novel negative deceptive opinion spam dataset. We employ the same 5-fold stratified cross-validation (CV) procedure as Ott et al. (2011), whereby for each cross-validation iteration we train our model on all reviews for 16 hotels, and test our model on all reviews for the remaining 4 hotels. The SVM cost parameter, C, is tuned by nested cross-validation on the training data. Results appear in Table 2. Ea</context>
<context position="13557" citStr="Ott et al. (2011)" startWordPosition="2131" endWordPosition="2134">ion differ depending on the sentiment of the text (see Section 4). Interestingly, we find that training on the combined sentiment dataset results in performance that is comparable to that of the “same sentiment” classifiers (88.4% vs. 89.3% accuracy for positive reviews and 86.0% vs. 86.0% accuracy for negative reviews). This is explainable in part by the increased training set size (1,280 vs. 640 reviews per 4 training folds). 4 Interaction of Sentiment and Deception An important question is how language features operate in our fake negative reviews compared with the fake positive reviews of Ott et al. (2011). For example, fake positive reviews included less spatial language (e.g., floor, small, location, etc.) because individuals who had not actually experienced the hotel simply had less spatial detail available for their review (Johnson and Raye, 1981). This was also the case for our negative reviews, with less spatial language observed for fake negative reviews relative to truthful. Likewise, our fake negative reviews had more verbs relative to nouns than truthful, suggesting a more narrative style that is indicative of imaginative writing (Biber et al., 1999; Rayson et al., 2001), a pattern al</context>
<context position="15704" citStr="Ott et al. (2011)" startWordPosition="2473" endWordPosition="2476">emotional distress of lying (Ekman and Friesen, 1969). Instead, the differences suggest that fake hotel reviewers exaggerate the sentiment they are trying to convey relative to similarly-valenced truthful reviews. Second, the effect of deception on the pattern of pronoun frequency was not the same across positive and negative reviews. In particular, while first person singular pronouns were produced more frequently in fake reviews than truthful, consistent with the case for positive reviews, the increase was diminished in the negative reviews examined here. In the positive reviews reported by Ott et al. (2011), the rate of first person singular in fake reviews (M=4.36%, SD=2.96%) was twice the rate observed in truthful reviews (M=2.18%, SD=2.04%). In contrast, the rate of first person singular in the deceptive negative reviews (M=4.47%, SD=2.83%) was only 57% greater than for truthful reviews (M=2.85%, SD=2.23%). These results suggest that the emphasis on the self, perhaps as a strategy of convincing the reader that the author had actually been to the hotel, is not as evident in the fake negative reviews, perhaps because the negative tone of the reviews caused the reviewers to psychologically dista</context>
<context position="17138" citStr="Ott et al. (2011)" startWordPosition="2690" endWordPosition="2693">rd negative deceptive opinion spam, containing 400 reviews of 20 Chicago hotels, which we have used to compare the deception detection capabilities of untrained human judges and standard n-gram–based Support Vector Machine classifiers. Our results demonstrate that while human deception detection performance is greater for negative rather than positive deceptive opinion spam, the best detection performance is still achieved through automated classifiers, with approximately 86% accuracy. We have additionally explored, albeit briefly, the relationship between sentiment and deception by utilizing Ott et al. (2011)’s positive deceptive opinion spam dataset in conjunction with our own. In particular, we have identified several features of language that seem to remain consistent across sentiment, such as decreased awareness of spatial details and exaggerated language. We have also identified other features that vary with the sentiment, such as first person singular use, although further work is required to determine if these differences may be exploited to improve deception detection performance. Indeed, future work may wish to jointly model sentiment and deception in order to better determine the effect </context>
</contexts>
<marker>Ott, Choi, Cardie, Hancock, 2011</marker>
<rawString>M. Ott, Y. Choi, C. Cardie, and J.T. Hancock. 2011. Finding deceptive opinion spam by any stretch of the imagination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 309– 319. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myle Ott</author>
<author>Claire Cardie</author>
<author>Jeff Hancock</author>
</authors>
<title>Estimating the prevalence of deception in online review communities.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference on World Wide Web,</booktitle>
<pages>201--210</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1769" citStr="Ott et al. (2012)" startWordPosition="247" endWordPosition="250">conjunction with the aforementioned positive review dataset, we consider the possible interactions between sentiment and deception, and present initial results that encourage further exploration of this relationship. 1 Introduction Consumer’s purchase decisions are increasingly influenced by user-generated online reviews of products and services (Cone, 2011). Accordingly, there is a growing incentive for businesses to solicit and manufacture DECEPTIVE OPINION SPAM— fictitious reviews that have been deliberately written to sound authentic and deceive the reader (Ott et al., 2011). For example, Ott et al. (2012) has estimated that between 1% and 6% of positive hotel reviews appear to be deceptive, suggesting that some hotels may be posting fake positive reviews in order to hype their own offerings. In this work we distinguish between two kinds of deceptive opinion spam, depending on the sentiment expressed in the review. In particular, reviews intended to promote or hype an offering, and which therefore express a positive sentiment towards the offering, are called positive deceptive opinion spam. In contrast, reviews intended to disparage or slander competitive offerings, and which therefore express </context>
<context position="6740" citStr="Ott et al., 2012" startWordPosition="1033" endWordPosition="1036">o ensure that they are written for the correct hotel and to ensure that they convey a generally negative sentiment.5 The average accepted review length was 178 words, higher than for the positive reviews gathered by Ott et al. (2011), who report an average review length of 116 words. 2.2 Truthful Reviews from the Web Negative (1- or 2-star) truthful reviews are mined from six popular online review communities: Expedia, Hotels.com, Orbitz, Priceline, TripAdvisor, and Yelp. While reviews mined from these communities cannot be considered gold standard truthful, recent work (Mayzlin et al., 2012; Ott et al., 2012) suggests that deception rates among travel review portals is reasonably small. Following Ott et al. (2011), we sample a subset of the available truthful reviews so that we retain an equal number of truthful and deceptive reviews (20 each) for each hotel. However, because the truthful reviews are on average longer than our deceptive reviews, we sample the truthful reviews according to a log-normal distribution fit to the lengths of our deceptive reviews, similarly to Ott et al. (2011).6 3 Deception Detection Performance In this section we report the deception detection performance of three hum</context>
</contexts>
<marker>Ott, Cardie, Hancock, 2012</marker>
<rawString>Myle Ott, Claire Cardie, and Jeff Hancock. 2012. Estimating the prevalence of deception in online review communities. In Proceedings of the 21st international conference on World Wide Web, pages 201–210. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Pennebaker</author>
<author>C K Chung</author>
<author>M Ireland</author>
<author>A Gonzales</author>
<author>R J Booth</author>
</authors>
<title>The development and psychometric properties of LIWC2007.</title>
<date>2007</date>
<location>Austin, TX:</location>
<contexts>
<context position="14613" citStr="Pennebaker et al., 2007" startWordPosition="2305" endWordPosition="2308">rbs relative to nouns than truthful, suggesting a more narrative style that is indicative of imaginative writing (Biber et al., 1999; Rayson et al., 2001), a pattern also observed by Ott et al. (2011). There were, however, several important differences in the deceptive language of fake negative relative to fake positive reviews. First, as might be expected, negative emotion terms were more fre7“Cross Val.” classifiers are effectively trained on 80% of the data and tested on the remaining 20%, whereas “Held Out” classifiers are trained and tested on 100% of each data. quent, according to LIWC (Pennebaker et al., 2007), in our fake negative reviews than in the fake positive reviews. But, importantly, the fake negative reviewers over-produced negative emotion terms (e.g., terrible, disappointed) relative to the truthful reviews in the same way that fake positive reviewers over-produced positive emotion terms (e.g., elegant, luxurious). Combined, these data suggest that the more frequent negative emotion terms in the present dataset are not the result of “leakage cues” that reveal the emotional distress of lying (Ekman and Friesen, 1969). Instead, the differences suggest that fake hotel reviewers exaggerate t</context>
</contexts>
<marker>Pennebaker, Chung, Ireland, Gonzales, Booth, 2007</marker>
<rawString>J.W. Pennebaker, C.K. Chung, M. Ireland, A. Gonzales, and R.J. Booth. 2007. The development and psychometric properties of LIWC2007. Austin, TX: LIWC (www.liwc.net).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Rayson</author>
<author>A Wilson</author>
<author>G Leech</author>
</authors>
<title>Grammatical word class variation within the British National Corpus sampler.</title>
<date>2001</date>
<journal>Language and Computers,</journal>
<volume>36</volume>
<issue>1</issue>
<pages>306</pages>
<contexts>
<context position="4058" citStr="Rayson et al., 2001" startWordPosition="592" endWordPosition="595"> Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics surpassing that of the human judges. In conjunction with Ott et al. (2011)’s positive deceptive opinion spam dataset, we then explore the interaction between sentiment and deception with respect to three types of language features: (1) changes in first-person singular use, often attributed to psychological distancing (Newman et al., 2003), (2) decreased spatial awareness and more narrative form, consistent with theories of reality monitoring (Johnson and Raye, 1981) and imaginative writing (Biber et al., 1999; Rayson et al., 2001), and (3) increased negative emotion terms, often attributed to leakage cues (Ekman and Friesen, 1969), but perhaps better explained in our case as an exaggeration of the underlying review sentiment. 2 Dataset One of the biggest challenges facing studies of deception is obtaining labeled data. Recently, Ott et al. (2011) have proposed an approach for generating positive deceptive opinion spam using Amazon’s popular Mechanical Turk crowdsourcing service. In this section we discuss our efforts to extend Ott et al. (2011)’s dataset to additionally include negative deceptive opinion spam. 2.1 Dece</context>
<context position="14143" citStr="Rayson et al., 2001" startWordPosition="2227" endWordPosition="2230">itive reviews of Ott et al. (2011). For example, fake positive reviews included less spatial language (e.g., floor, small, location, etc.) because individuals who had not actually experienced the hotel simply had less spatial detail available for their review (Johnson and Raye, 1981). This was also the case for our negative reviews, with less spatial language observed for fake negative reviews relative to truthful. Likewise, our fake negative reviews had more verbs relative to nouns than truthful, suggesting a more narrative style that is indicative of imaginative writing (Biber et al., 1999; Rayson et al., 2001), a pattern also observed by Ott et al. (2011). There were, however, several important differences in the deceptive language of fake negative relative to fake positive reviews. First, as might be expected, negative emotion terms were more fre7“Cross Val.” classifiers are effectively trained on 80% of the data and tested on the remaining 20%, whereas “Held Out” classifiers are trained and tested on 100% of each data. quent, according to LIWC (Pennebaker et al., 2007), in our fake negative reviews than in the fake positive reviews. But, importantly, the fake negative reviewers over-produced nega</context>
</contexts>
<marker>Rayson, Wilson, Leech, 2001</marker>
<rawString>P. Rayson, A. Wilson, and G. Leech. 2001. Grammatical word class variation within the British National Corpus sampler. Language and Computers, 36(1):295– 306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Rigby</author>
<author>D M Stasinopoulos</author>
</authors>
<title>Generalized additive models for location, scale and shape,(with discussion).</title>
<date>2005</date>
<journal>Applied Statistics,</journal>
<pages>54--507</pages>
<contexts>
<context position="7818" citStr="Rigby and Stasinopoulos, 2005" startWordPosition="1203" endWordPosition="1207">e reviews, similarly to Ott et al. (2011).6 3 Deception Detection Performance In this section we report the deception detection performance of three human judges (Section 3.1) and supervised n-gram Support Vector Machine (SVM) classifiers (Section 3.2). 3We define “reasonable length” to be &gt; 150 characters. 4We use http://plagiarisma.net to determine whether or not a review is plagiarized. 5We discarded and replaced approximately 2% of the submissions, where it was clear that the worker had misread the instructions and instead written a deceptive positive review. 6We use the R package GAMLSS (Rigby and Stasinopoulos, 2005) to fit a log-normal distribution (left truncated at 150 characters) to the lengths of the deceptive reviews. 498 TRUTHFUL DECEPTIVE Accuracy P R F P R F HUMAN JUDGE 1 65.0% 65.0 65.0 65.0 65.0 65.0 65.0 JUDGE 2 61.9% 63.0 57.5 60.1 60.9 66.3 63.5 JUDGE 3 57.5% 57.3 58.8 58.0 57.7 56.3 57.0 META MAJORITY 69.4% 70.1 67.5 68.8 68.7 71.3 69.9 SKEPTIC 58.1% 78.3 22.5 35.0 54.7 93.8 69.1 Table 1: Deception detection performance, incl. (P)recision, (R)ecall, and (F)1-score, for three human judges and two meta-judges on a set of 160 negative reviews. The largest value in each column is indicated with</context>
</contexts>
<marker>Rigby, Stasinopoulos, 2005</marker>
<rawString>R. A. Rigby and D. M. Stasinopoulos. 2005. Generalized additive models for location, scale and shape,(with discussion). Applied Statistics, 54:507–554.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>