<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.945566">
String Re-writing Kernel
</title>
<author confidence="0.994455">
Fan Bu1, Hang Li2 and Xiaoyan Zhu3
</author>
<affiliation confidence="0.978226">
1,3State Key Laboratory of Intelligent Technology and Systems
1,3Tsinghua National Laboratory for Information Sci. and Tech.
1,3Department of Computer Sci. and Tech., Tsinghua University
2Microsoft Research Asia, No. 5 Danling Street, Beijing 100080,China
</affiliation>
<email confidence="0.912879666666667">
1bufan0000@gmail.com
2hangli@microsoft.com
3zxy-dcs@tsinghua.edu.cn
</email>
<sectionHeader confidence="0.996467" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999302789473684">
Learning for sentence re-writing is a funda-
mental task in natural language processing and
information retrieval. In this paper, we pro-
pose a new class of kernel functions, referred
to as string re-writing kernel, to address the
problem. A string re-writing kernel measures
the similarity between two pairs of strings,
each pair representing re-writing of a string.
It can capture the lexical and structural sim-
ilarity between two pairs of sentences with-
out the need of constructing syntactic trees.
We further propose an instance of string re-
writing kernel which can be computed effi-
ciently. Experimental results on benchmark
datasets show that our method can achieve bet-
ter results than state-of-the-art methods on two
sentence re-writing learning tasks: paraphrase
identification and recognizing textual entail-
ment.
</bodyText>
<sectionHeader confidence="0.998886" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999598916666667">
Learning for sentence re-writing is a fundamental
task in natural language processing and information
retrieval, which includes paraphrasing, textual en-
tailment and transformation between query and doc-
ument title in search.
The key question here is how to represent the re-
writing of sentences. In previous research on sen-
tence re-writing learning such as paraphrase identifi-
cation and recognizing textual entailment, most rep-
resentations are based on the lexicons (Zhang and
Patrick, 2005; Lintean and Rus, 2011; de Marneffe
et al., 2006) or the syntactic trees (Das and Smith,
</bodyText>
<figure confidence="0.9871066">
* *
wrote . Shakespeare wrote Hamlet.
* was written by . Hamlet was written by Shakespeare.
*
(A) (B)
</figure>
<figureCaption confidence="0.9847685">
Figure 1: Example of re-writing. (A) is a re-writing rule
and (B) is a re-writing of sentence.
</figureCaption>
<bodyText confidence="0.999317148148148">
2009; Heilman and Smith, 2010) of the sentence
pairs.
In (Lin and Pantel, 2001; Barzilay and Lee, 2003),
re-writing rules serve as underlying representations
for paraphrase generation/discovery. Motivated by
the work, we represent re-writing of sentences by
all possible re-writing rules that can be applied into
it. For example, in Fig. 1, (A) is one re-writing rule
that can be applied into the sentence re-writing (B).
Specifically, we propose a new class of kernel func-
tions (Sch¨olkopf and Smola, 2002), called string re-
writing kernel (SRK), which defines the similarity
between two re-writings (pairs) of strings as the in-
ner product between them in the feature space in-
duced by all the re-writing rules. SRK is different
from existing kernels in that it is for re-writing and
defined on two pairs of strings. SRK can capture the
lexical and structural similarity between re-writings
of sentences and does not need to parse the sentences
and create the syntactic trees of them.
One challenge for using SRK lies in the high com-
putational cost of straightforwardly computing the
kernel, because it involves two re-writings of strings
(i.e., four strings) and a large number of re-writing
rules. We are able to develop an instance of SRK,
referred to as kb-SRK, which directly computes the
number of common rewriting rules without explic-
</bodyText>
<page confidence="0.98922">
449
</page>
<note confidence="0.985761">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 449–458,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998898">
itly calculating the inner product between feature
vectors, and thus drastically reduce the time com-
plexity.
Experimental results on benchmark datasets show
that SRK achieves better results than the state-of-
the-art methods in paraphrase identification and rec-
ognizing textual entailment. Note that SRK is very
flexible to the formulations of sentences. For ex-
ample, informally written sentences such as long
queries in search can also be effectively handled.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999639489795918">
The string kernel function, first proposed by Lodhi
et al. (2002), measures the similarity between two
strings by their shared substrings. Leslie et al.
(2002) proposed the k-spectrum kernel which repre-
sents strings by their contiguous substrings of length
k. Leslie et al. (2004) further proposed a number of
string kernels including the wildcard kernel to fa-
cilitate inexact matching between the strings. The
string kernels defined on two pairs of objects (in-
cluding strings) were also developed, which decom-
pose the similarity into product of similarities be-
tween individual objects using tensor product (Basil-
ico and Hofmann, 2004; Ben-Hur and Noble, 2005)
or Cartesian product (Kashima et al., 2009).
The task of paraphrasing usually consists of para-
phrase pattern generation and paraphrase identifica-
tion. Paraphrase pattern generation is to automat-
ically extract semantically equivalent patterns (Lin
and Pantel, 2001; Bhagat and Ravichandran, 2008)
or sentences (Barzilay and Lee, 2003). Paraphrase
identification is to identify whether two given sen-
tences are a paraphrase of each other. The meth-
ods proposed so far formalized the problem as clas-
sification and used various types of features such
as bag-of-words feature, edit distance (Zhang and
Patrick, 2005), dissimilarity kernel (Lintean and
Rus, 2011) predicate-argument structure (Qiu et al.,
2006), and tree edit model (which is based on a tree
kernel) (Heilman and Smith, 2010) in the classifica-
tion task. Among the most successful methods, Wan
et al. (2006) enriched the feature set by the BLEU
metric and dependency relations. Das and Smith
(2009) used the quasi-synchronous grammar formal-
ism to incorporate features from WordNet, named
entity recognizer, POS tagger, and dependency la-
bels from aligned trees.
The task of recognizing textual entailment is to
decide whether the hypothesis sentence can be en-
tailed by the premise sentence (Giampiccolo et al.,
2007). In recognizing textual entailment, de Marn-
effe et al. (2006) classified sentences pairs on the
basis of word alignments. MacCartney and Man-
ning (2008) used an inference procedure based on
natural logic and combined it with the methods by
de Marneffe et al. (2006). Harmeling (2007) and
Heilman and Smith (2010) classified sequence pairs
based on transformation on syntactic trees. Zanzotto
et al. (2007) used a kernel method on syntactic tree
pairs (Moschitti and Zanzotto, 2007).
</bodyText>
<sectionHeader confidence="0.9310445" genericHeader="method">
3 Kernel Approach to Sentence
Re-Writing Learning
</sectionHeader>
<bodyText confidence="0.999738666666667">
We formalize sentence re-writing learning as a ker-
nel method. Following the literature of string kernel,
we use the terms “string” and “character” instead of
“sentence” and “word”.
Suppose that we are given training data consisting
of re-writings of strings and their responses
</bodyText>
<equation confidence="0.98142">
((s1,t1),y1),...,((sn,tn),yn) E(Σ* xΣ*) x Y
</equation>
<bodyText confidence="0.9999513125">
where Σ denotes the character set, Σ* = U∞i=0 Σi de-
notes the string set, which is the Kleene closure of
set Σ, Y denotes the set of responses, and n is the
number of instances. (si,ti) is a re-writing consist-
ing of the source string si and the target string ti.
yi is the response which can be a category, ordinal
number, or real number. In this paper, for simplic-
ity we assume that Y = {±1} (e.g. paraphrase/non-
paraphrase). Given a new string re-writing (s,t) E
Σ* xΣ*, our goal is to predict its response y. That is,
the training data consists of binary classes of string
re-writings, and the prediction is made for the new
re-writing based on learning from the training data.
We take the kernel approach to address the learn-
ing task. The kernel on re-writings of strings is de-
fined as
</bodyText>
<equation confidence="0.982390333333333">
K : (Σ* x Σ*) x (Σ* x Σ*) —* R
satisfying for all (si,ti), (sj,tj) E Σ* x Σ*,
K((si,ti),(sj,tj)) = (Φ(si,ti),Φ(sj,tj))
</equation>
<bodyText confidence="0.9997145">
where Φ maps each re-writing (pair) of strings into
a high dimensional Hilbert space _ V, referred to as
</bodyText>
<page confidence="0.994973">
450
</page>
<bodyText confidence="0.9987035">
feature space. By the representer theorem (Kimel-
dorf and Wahba, 1971; Sch¨olkopf and Smola, 2002),
it can be shown that the response y of a new string
re-writing (s,t) can always be represented as
</bodyText>
<equation confidence="0.603188">
αiyiK((si,ti),(s,t)))
</equation>
<bodyText confidence="0.999990444444444">
where αi ≥ 0,(i = 1,··· ,n) are parameters. That is,
it is determined by a linear combination of the sim-
ilarities between the new instance and the instances
in training set. It is also known that by employing a
learning model such as SVM (Vapnik, 2000), such a
linear combination can be automatically learned by
solving a quadratic optimization problem. The ques-
tion then becomes how to design the kernel function
for the task.
</bodyText>
<sectionHeader confidence="0.839753" genericHeader="method">
4 String Re-writing Kernel
</sectionHeader>
<bodyText confidence="0.999955666666667">
Let Σ be the set of characters and Σ∗ be the set of
strings. Let wildcard domain D ⊆ Σ∗ be the set of
strings which can be replaced by wildcards.
The string re-writing kernel measures the similar-
ity between two string re-writings through the re-
writing rules that can be applied into them. For-
mally, given re-writing rule set R and wildcard do-
main D, the string re-writing kernel (SRK) is defined
as
</bodyText>
<equation confidence="0.975058">
K((s1,t1),(s2,t2)) = hΦ(s1,t1),Φ(s2,t2)i (1)
where Φ(s,t) = (φr(s,t))r∈R and
φr(s,t) = nλi (2)
</equation>
<bodyText confidence="0.999509964285714">
where n is the number of contiguous substring pairs
of (s,t) that re-writing rule r matches, i is the num-
ber of wildcards in r, and λ ∈ (0,1] is a factor pun-
ishing each occurrence of wildcard.
A re-writing rule is defined as a triple r =
(βs,βt,τ) where βs,βt ∈ (Σ ∪ {∗})∗ denote source
and target string patterns and τ ⊆ ind∗(βs)×ind∗(βt)
denotes the alignments between the wildcards in the
two string patterns. Here ind∗(β) denotes the set of
indexes of wildcards in β.
We say that a re-writing rule (βs,βt,τ) matches a
string pair (s,t), if and only if string patterns βs and
βt can be changed into s and t respectively by sub-
stituting each wildcard in the string patterns with an
element in the strings, where the elements are de-
fined in the wildcard domain D and the wildcards
βs[i] and βt[ j] are substituted by the same elements,
when there is an alignment (i, j) ∈ τ.
For example, the re-writing rule in Fig. 1 (A)
can be formally written as r = (βs,βt,τ) where
βs = (∗,wrote,∗), βt = (∗,was,written,by,∗) and
τ = {(1,5),(3,1)}. It matches with the string pair in
Fig. 1 (B).
String re-writing kernel is a class of kernels which
depends on re-writing rule set R and wildcard do-
main D. Here we provide some examples. Obvi-
ously, the effectiveness and efficiency of SRK de-
pend on the choice of R and D.
</bodyText>
<construct confidence="0.880624642857143">
Example 1. We define the pairwise k-spectrum ker-
nel (ps-SRK) Kps
k as the re-writing rule kernel un-
der R = {(βs,βt,τ)|βs,βt ∈ Σk,τ = /0} and any
D. It can be shown that Kps
k ((s1,t1),(s2,t2)) =
Kspec
k (s1,s2)Kspec
k (t1,t2) where Kspec
k (x,y) is equiv-
alent to the k-spectrum kernel proposed by Leslie et
al. (2002).
Example 2. The pairwise k-wildcard kernel (pw-
SRK) Kkpw is defined as the re-writing rule kernel
</construct>
<equation confidence="0.901924">
underR= {(βs,βt,τ)|βs,βt ∈ (Σ∪{∗})k,τ = /0} and
D = Σ. It can be shown that Kpw
k ((s1,t1),(s2,t2)) =
Kwc
(k,k)(s1,s2)Kwc
(k,k)(t1,t2) where Kwc
</equation>
<bodyText confidence="0.84537">
(k,k)(x,y) is a spe-
cial case (m=k) of the (k,m)-wildcard kernel pro-
posed by Leslie et al. (2004).
Both kernels shown above are represented as the
product of two kernels defined separately on strings
s1,s2 and t1,t2, and that is to say that they do not
consider the alignment relations between the strings.
</bodyText>
<sectionHeader confidence="0.7517155" genericHeader="method">
5 K-gram Bijective String Re-writing
Kernel
</sectionHeader>
<bodyText confidence="0.999901333333333">
Next we propose another instance of string re-
writing kernel, called the k-gram bijective string re-
writing kernel (kb-SRK). As will be seen, kb-SRK
can be computed efficiently, although it is defined
on two pairs of strings and is not decomposed (note
that ps-SRK and pw-SRK are decomposed).
</bodyText>
<subsectionHeader confidence="0.96472">
5.1 Definition
</subsectionHeader>
<bodyText confidence="0.999978666666667">
The kb-SRK has the following properties: (1) A
wildcard can only substitute a single character, de-
noted as “?”. (2) The two string patterns in a re-
writing rule are of length k. (3) The alignment
relation in a re-writing rule is bijective, i.e., there
is a one-to-one mapping between the wildcards in
</bodyText>
<equation confidence="0.9738335">
n
∑
i=1
y = sign(
</equation>
<page confidence="0.956117">
451
</page>
<bodyText confidence="0.998887555555556">
the string patterns. Formally, the k-gram bijective
string re-writing kernel Kk is defined as a string
re-writing kernel under the re-writing rule set R =
{(βs,βt,τ)|βs,βt E (ΣU{?})k,τ is bijective} and the
wildcard domain D = Σ.
Since each re-writing rule contains two string pat-
terns of length k and each wildcard can only substi-
tute one character, a re-writing rule can only match
k-gram pairs in (s,t). We can rewrite Eq. (2) as
</bodyText>
<equation confidence="0.9943225">
α𝑠1 = abbccbb ; α𝑠2 = abcccdd;
αt1 = cbcbbcb ; αt2 = cbccdcd;
</equation>
<figureCaption confidence="0.94774">
Figure 2: Example of two k-gram pairs.
</figureCaption>
<equation confidence="0.368981">
α𝑠D = (a, a), (b, b), (𝐛, 𝐜), (c, c), (c, c), (𝐛, d), (𝐛, d)
α𝑡D = (c, c), (b, b), (c, c), (𝐛, 𝐜), (𝐛, d), (c, c), (𝐛, d)
</equation>
<figureCaption confidence="0.989264">
Figure 3: Example of the pair of double lists combined
</figureCaption>
<equation confidence="0.9984485">
φr(s,t) = ∑ ∑ ¯φr(αs,αt) (3) from the two k-gram pairs in Fig. 2. Non-identical dou-
αsEk-grams(s) αtEk-grams(t) bles are in bold.
</equation>
<bodyText confidence="0.989448333333333">
where ¯φr(αs,αt) = λi if r (with i wildcards) matches
(αs,αt), otherwise ¯φr(αs,αt) = 0.
For ease of computation, we re-write kb-SRK as
</bodyText>
<equation confidence="0.956449714285714">
Kk((s1,t1),(s2,t2))
� ∑ ∑ ¯Kk((αs1,αt1),(αs2,αt2))
αs1 E k-grams(s1) αs2 E k-grams(s2)
αt1 E k-grams(t1) αt2 E k-grams(t2)
(4)
where ¯Kk = ∑ ¯φr(αs1,αt1)¯φr(αs2,αt2) (5)
rER
</equation>
<subsectionHeader confidence="0.960206">
5.2 Algorithm for Computing Kernel
</subsectionHeader>
<bodyText confidence="0.999966583333333">
A straightforward computation of kb-SRK would
be intractable. The computation of Kk in Eq. (4)
needs computations of ¯Kk conducted O((n − k +
1)4) times, where n denotes the maximum length
of strings. Furthermore, the computation of ¯Kk in
Eq. (5) needs to perform matching of all the re-
writing rules with the two k-gram pairs (αs1, αt1),
(αs2, αt2), which has time complexity O(k!).
In this section, we will introduce an efficient algo-
rithm, which can compute ¯Kk and Kk with the time
complexities of O(k) and O(kn2), respectively. The
latter is verified empirically.
</bodyText>
<subsectionHeader confidence="0.981837">
5.2.1 Transformation of Problem
</subsectionHeader>
<bodyText confidence="0.979702636363636">
For ease of manipulation, our method transforms
the computation of kernel on k-grams into the com-
putation on a new data structure called lists of dou-
bles. We first explain how to make the transforma-
tion.
Suppose that α1,α2 E Σk are k-grams, we use
α1[i] and α2[i] to represent the i-th characters of
them. We call a pair of characters a double. Thus
Σ x Σ denotes the set of doubles and αDs ,αDt E (Σ x
Σ)k denote lists of doubles. The following operation
combines two k-grams into a list of doubles.
</bodyText>
<equation confidence="0.976833">
α1 ® α2 = ((α1[1],α2[1]),···,(α1[k],α2[k])).
</equation>
<bodyText confidence="0.981379833333333">
We denotes α1 ® α2[i] as the i-th element of the
list. Fig. 3 shows example lists of doubles combined
from k-grams.
We introduce the set of identical doubles I =
{(c,c)|c E Σ} and the set of non-identical doubles
N = {(c,c&apos;)|c,c&apos; E Σ and c 7�c&apos;}. Obviously, IUN=
ΣxΣ and ITN = /0.
We define the set of re-writing rules for double
lists RD = {rD = Ds
ll (R, βDt , τ)  |βDs , βDt E (I U {?})k, τ
is a bijective alignm
&apos;ent} where βDs and βDt are lists
of identical doubles including wildcards and with
length k. We say rule rD matches a pair of double
lists (αD s ,αD t ) iff. βD s ,βD t can be changed into αD s
and αD t by substituting each wildcard pair to a dou-
ble in Σ x Σ , and the double substituting the wild-
card pair βDs [i] and βDt [j] must be an identical dou-
ble when there is an alignment (i, j) E τ. The rule
set defined here and the rule set in Sec. 4 only differ
on the elements where re-writing occurs. Fig. 4 (B)
shows an example of re-writing rule for double lists.
The pair of double lists in Fig. 3 can match with the
re-writing rule.
</bodyText>
<subsubsectionHeader confidence="0.905193">
5.2.2 Computing ¯Kk
</subsubsectionHeader>
<bodyText confidence="0.999667">
We consider how to compute ¯Kk by extending the
computation from k-grams to double lists.
The following lemma shows that computing the
weighted sum of re-writing rules matching k-gram
pairs (αs1,αt1) and (αs2,αt2) is equivalent to com-
puting the weighted sum of re-writing rules for dou-
ble lists matching (αs1 ® αs2,αt1 ® αt2).
</bodyText>
<page confidence="0.990068">
452
</page>
<figure confidence="0.98645">
a b ? c c ? ? (a,a) (b,b) ? (c,c) (c,c) ? ?
</figure>
<figureCaption confidence="0.86754225">
Figure 4: For re-writing rule (A) matching both k-gram
pairs shown in Fig. 2, there is a corresponding re-writing
rule for double lists (B) matching the pair of double lists
shown in Fig. 3.
</figureCaption>
<figure confidence="0.56359">
#Σ×Σ(α𝑠D) = {(a, a): 1, (b, b): 1, (𝐛, 𝐜): 1, (𝐛, d): 2, (c, c): 2}
#Σ×Σ(α𝑡D) = {(a, a): 0, (b, b): 1, (𝐛, 𝐜): 1, (𝐛, d): 2, (c, c): 3}
</figure>
<figureCaption confidence="0.909101">
Figure 5: Example of #ΣxΣ(�) for the two double lists
shown in Fig. 3. Doubles not appearing in both αDs and
αDt are not shown.
</figureCaption>
<bodyText confidence="0.9606451">
Lemma 1. For any two k-gram pairs (αs1,αt1) and
(αs2,αt2), there exists a one-to-one mapping from
the set of re-writing rules matching them to the set of
re-writing rules matching the corresponding double
lists (αs1 (9 αs2,αt1 (9 αt2).
The re-writing rule in Fig. 4 (A) matches the k-
gram pairs in Fig. 2. Equivalently, the re-writing
rule for double lists in Fig. 4 (B) matches the pair
of double lists in Fig. 3. By lemma 1 and Eq. 5, we
have
</bodyText>
<equation confidence="0.9885665">
¯Kk = ∑ ¯φrD(αs1 (9 αs2,αt1 (9 αt2) (6)
rDERD
</equation>
<bodyText confidence="0.999557333333333">
where ¯φrD(αD s ,αD t ) = λ2i if the rewriting rule for
double lists rD with i wildcards matches (αDs ,αDt ),
otherwise ¯φrD(αD s ,αD t ) = 0. To get ¯Kk, we just need
to compute the weighted sum of re-writing rules for
double lists matching (αs1 (9 αs2,αt1 (9 αt2). Thus,
we can work on the “combined” pair of double lists
instead of two pairs of k-grams.
Instead of enumerating all possible re-writing
rules and checking whether they can match the given
pair of double lists, we only calculate the number of
possibilities of “generating” from the pair of double
lists to the re-writing rules matching it, which can be
carried out efficiently. We say that a re-writing rule
of double lists can be generated from a pair of double
lists (αDs , αDt ), if they match with each other. From
the definition of RD, in each generation, the identi-
cal doubles in αDs and αDt can be either or not sub-
stituted by an aligned wildcard pair in the re-writing
</bodyText>
<table confidence="0.677372375">
Algorithm 1: Computing ¯Kk
Input: k-gram pair (αs1,αt1) and (αs2,αt2)
Output: ¯Kk((αs1,αt1),(αs2,αt2))
1 Set (αD s ,αD t ) = (αs1 (9αs2,αt1 (9αt2) ;
2 Compute #ΣxΣ(αDs ) and #ΣxΣ(αDt );
3 result=1;
4 for each e E Σ x Σ satisfies
#e(αDs ) +#e(αDt ) =� 0 do
</table>
<bodyText confidence="0.610854">
ge = 0, ne = min{#e(αD s ),#e(αD t )I ;
for 0 &lt; i &lt; ne do
</bodyText>
<equation confidence="0.994869">
ge = ge + a(e)
i λ2i;
result = result * g;
9 return result;
</equation>
<bodyText confidence="0.981497045454546">
rule, and all the non-identical doubles in αDs and αDt
must be substituted by aligned wildcard pairs. From
this observation and Eq. 6, ¯Kk only depends on the
number of times each double occurs in the double
lists.
Let e be a double. We denote #e(αD) as the num-
ber of times e occurs in the list of doubles αD. Also,
for a set of doubles S C_ Σ x Σ, we denote #S(αD) as
a vector in which each element represents #e(αD) of
each double e E S. We can find a function g such
that
¯Kk = g(#ΣxΣ(αs1 (9 αs2),#ΣxΣ(αt1 (9 αt2)) (7)
Alg. 1 shows how to compute ¯Kk. #ΣxΣ(.) is com-
puted from the two pairs of k-grams in line 1-2. The
final score is made through the iterative calculation
on the two lists (lines 4-8).
The key of Alg. 1 is the calculation of ge based on
ai (line 7). Here we use a(e)
(e) i to denote the number
of possibilities for which i pairs of aligned wildcards
can be generated from e in both αDs and αDt . a(e) ican
be computed as follows.
</bodyText>
<listItem confidence="0.9721535">
(1) If e E N and #e(αDs ) =� #e(αD t ), then a(e)
i = 0
for any i.
(2) If e E N and #e(αDs ) = #e(αDt ) = j, then a(e)
j =
j! and a(e)
</listItem>
<bodyText confidence="0.559349">
i = 0 for any i =� j.
</bodyText>
<listItem confidence="0.86385">
(3) If e E I, then a(e)
</listItem>
<equation confidence="0.9597985">
i =�#e(αD s ) )�#e(αD t ) )i!.
i i
</equation>
<bodyText confidence="0.999851666666667">
We next explain the rationale behind the above
computations. In (1), since #e(αDs ) =� #e(αDt ), it is
impossible to generate a re-writing rule in which all
</bodyText>
<figure confidence="0.934833833333333">
c b c ? ? c ? (c,c) (b,b) (c,c) ? ? (c,c) ?
(A) (B)
5
6
7
8
</figure>
<page confidence="0.999361">
453
</page>
<bodyText confidence="0.999800416666667">
the occurrences of non-identical double e are substi-
tuted by pairs of aligned wildcards. In (2), j pairs of
aligned wildcards can be generated from all the oc-
currences of non-identical double e in both αDs and
αDt . The number of combinations thus is j!. In (3),
a pair of aligned wildcards can either be generated
or not from a pair of identical doubles in αDs and
αDt . We can select i occurrences of identical double
e from αDs , i occurrences from αDt , and generate all
possible aligned wildcards from them.
In the loop of lines 4-8, we only need to con-
sider a(e) ifor 0 G i G min{#e(αD s ),#e(αD t )�, because
</bodyText>
<equation confidence="0.6581305">
ai = 0 for the rest of i.
(e)
</equation>
<bodyText confidence="0.745856">
To sum up, Eq. 7 can be computed as below,
which is exactly the computation at lines 3-8.
</bodyText>
<figure confidence="0.964393888888889">
Algorithm 2: Computing Kk
Input: string pair (s1,t1) and (s2,t2), window
size k
Output: Kk((s1,t1),(s2,t2))
1 Initialize two maps ms and mt and two counters
cs and ct;
2 for each k-gram αs1 in s1 do
for each k-gram αs2 in s2 do
Update ms with key-value pair
(#N(αs1 ® αs2),#ΣxΣ(αs1 ® αs2));
cs[#ΣxΣ(αs1 ® αs2)] ++ ;
6 for each k-gram αt1 in t1 do
for each k-gram αt2 in t2 do
Update mt with key-value pair
(#N(αt1 ®αt2),#ΣxΣ(αt1 ® αt2));
3
4
5
</figure>
<equation confidence="0.886806444444444">
T�-T7 ne
g(#ΣxΣ(αDs ),#ΣxΣ(αtD)) = 11 (∑
ai λ2i) (8)
(e)
ct[#ΣxΣ(αt1 ®αt2)]++ ;
7
8
9
eEΣxΣ i=0
</equation>
<bodyText confidence="0.973">
For the k-gram pairs in Fig. 2, we first create
lists of doubles in Fig. 3 and compute #ΣxΣ(·) for
them (lines 1-2 of Alg. 1), as shown in Fig. 5. We
next compute Kk from #ΣxΣ(αDs ) and #ΣxΣ(αDt ) in
Fig. 5 (lines 3-8 of Alg. 1) and obtain Kk = (1)(1 +
</bodyText>
<equation confidence="0.7586935">
λ2)(λ2)(2λ4)(1 + 6λ2 + 6λ4) = 12λ12 + 24λ10 +
14λ8 +2λ6.
</equation>
<subsubsectionHeader confidence="0.68196">
5.2.3 Computing Kk
</subsubsectionHeader>
<bodyText confidence="0.9511591875">
Algorithm 2 shows how to compute Kk. It pre-
pares two maps ms and mt and two vectors of coun-
ters cs and ct. In ms and mt, each key #N(.) maps a
set of values #ΣxΣ(.). Counters cs and ct count the
frequency of each #ΣxΣ(.). Recall that #N(αs1 ®αs2)
denotes a vector whose element is #e(αs1 ® αs2) for
e E N. #ΣxΣ(αs1 ® αs2) denotes a vector whose ele-
ment is #e(αs1 ®αs2) where e is any possible double.
One can easily verify the output of the al-
gorithm is exactly the value of Kk. First,
¯Kk((αs1,αt1),(αs2,αt2)) = 0 if #N(αs1 ® αs2) =�
#N(αt1 ® αt2). Therefore, we only need to consider
those αs1 ® αs2 and αt1 ® αt2 which have the same
key (lines 10-13). We group the k-gram pairs by
their key in lines 2-5 and lines 6-9.
Moreover, the following relation holds
</bodyText>
<equation confidence="0.959277888888889">
¯Kk((αs1,αt1),(αs2,αt2)) = ¯Kk((α�s1,α�t1),(α�s2,α�t2))
if #ΣxΣ(αs1 ®αs2) = #ΣxΣ(αs1 ®αs2) and #ΣxΣ(αt1 ®
αt2) = #ΣxΣ(α�t1 ,w,, as2, c , tea s are
2
10 for each key E ms.keys n mt.keys do
for each vs E ms[key] do
for each vt E mt[key] do
result+= cs[vs]ct[vt]g(vs,vt) ;
14 return result;
</equation>
<bodyText confidence="0.9925325">
other k-grams. Therefore, we only need to take
#ΣxΣ(αs1 ®αs2) and #ΣxΣ(αt1 ®αt2) as the value un-
der each key and count its frequency. That is to say,
#ΣxΣ provides sufficient statistics for computing ¯Kk.
The quantity g(vs,vt) in line 13 is computed by
Alg. 1 (lines 3-8).
</bodyText>
<subsectionHeader confidence="0.99916">
5.3 Time Complexity
</subsectionHeader>
<bodyText confidence="0.999482857142857">
The time complexities of Alg. 1 and Alg. 2 are
shown below.
For Alg. 1, lines 1-2 can be executed in
O(k). The time for executing line 7 is less
than #e(αDs ) + #e(αDt ) + 1 for each e satisfying
#e(αDs ) =� 0 or #e(αDt ) =� 0 . Since ∑eEΣxΣ#e(αDs ) =
∑eEΣxΣ#e(αDt ) = k, the time for executing lines 3-8
is less than 4k, which results in the O(k) time com-
plexity of Alg. 1.
For Alg. 2, we denote n = max{|s1|,|s2|,|t1|,|t2|I.
It is easy to see that if the maps and counters in the
algorithm are implemented by hash maps, the time
complexities of lines 2-5 and lines 6-9 are O(kn2).
However, analyzing the time complexity of lines 10-
</bodyText>
<page confidence="0.7730755">
11
12
13
454
</page>
<figure confidence="0.975755">
2.5
2
1.5
Worst
Avg.
1 2 3 4 5 6 7 8
window size K
</figure>
<figureCaption confidence="0.9975465">
Figure 6: Relation between ratio C/n2avg and window size
k when running Alg. 2 on MSR Paraphrases Corpus.
</figureCaption>
<bodyText confidence="0.91940125">
13 is quite difficult.
Lemma 2 and Theorem 1 provide an upper bound
of the number of times computing g(vs,vt) in line 13,
denoted as C.
</bodyText>
<equation confidence="0.8436855">
Lemma 2. For αs1 Ek-grams(s1) and αs2,α� s2 Ek-
grams(s2), we have #EXE(αs1 ® αs2) =
#EXE(αs1 ® α�s2) if #N(αs1 ® αs2) = #N(αs1 ® αs2).
Theorem 1. C is O(n3).
</equation>
<bodyText confidence="0.999940416666667">
By Lemma 2, each ms[key] contains at most
n − k + 1 elements. Together with the fact that
Y_key ms[key] = (n − k + 1)2, Theorem 1 is proved.
It can be also proved that C is O(n2) when k = 1.
Empirical study shows that O(n3) is a loose upper
bound for C. Let navg denote the average length of
s1, t1, s2 and t2. Our experiment on all pairs of sen-
tences on MSR Paraphrase (Fig. 6) shows that C is in
the same order of n2avg in the worst case and C/n2avg
decreases with increasing k in both average case and
worst case, which indicates that C is O(n2) and the
overall time complexity of Alg. 2 is O(kn2).
</bodyText>
<sectionHeader confidence="0.998718" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.996427823529412">
We evaluated the performances of the three types
of string re-writing kernels on paraphrase identifica-
tion and recognizing textual entailment: pairwise k-
spectrum kernel (ps-SRK), pairwise k-wildcard ker-
nel (pw-SRK), and k-gram bijective string re-writing
kernel (kb-SRK). We set λ = 1 for all kernels. The
performances were measured by accuracy (e.g. per-
centage of correct classifications).
In both experiments, we used LIBSVM with de-
fault parameters (Chang et al., 2011) as the clas-
sifier. All the sentences in the training and test
sets were segmented into words by the tokenizer at
OpenNLP (Baldrige et al., ). We further conducted
stemming on the words with Iveonik English Stem-
mer (http://www.iveonik.com/).
We normalized each kernel by ˜K(x,y) =
K(x,y) and then tried them under different
</bodyText>
<equation confidence="0.631315">
-,1K(x,x)K(y,y)
</equation>
<bodyText confidence="0.999191">
window sizes k. We also tried to combine the
kernels with two lexical features “unigram precision
and recall” proposed in (Wan et al., 2006), referred
to as PR. For each kernel K, we tested the window
size settings of K1 + ... + Kkmax (kmax E 11,2,3,4})
together with the combination with PR and we
report the best accuracies of them in Tab 1 and
Tab 2.
</bodyText>
<subsectionHeader confidence="0.999151">
6.1 Paraphrase Identification
</subsectionHeader>
<bodyText confidence="0.9999093125">
The task of paraphrase identification is to examine
whether two sentences have the same meaning. We
trained and tested all the methods on the MSR Para-
phrase Corpus (Dolan and Brockett, 2005; Quirk
et al., 2004) consisting of 4,076 sentence pairs for
training and 1,725 sentence pairs for testing.
The experimental results on different SRKs are
shown in Table 1. It can be seen that kb-SRK out-
performs ps-SRK and pw-SRK. The results by the
state-of-the-art methods reported in previous work
are also included in Table 1. kb-SRK outperforms
the existing lexical approach (Zhang and Patrick,
2005) and kernel approach (Lintean and Rus, 2011).
It also works better than the other approaches listed
in the table, which use syntactic trees or dependency
relations.
</bodyText>
<figureCaption confidence="0.9769722">
Fig. 7 gives detailed results of the kernels under
different maximum k-gram lengths kmax with and
without PR. The results of ps-SRK and pw-SRK
without combining PR under different k are all be-
low 71%, therefore they are not shown for clar-
</figureCaption>
<table confidence="0.995729083333333">
Method Acc.
Zhang and Patrick (2005) 71.9
Lintean and Rus (2011) 73.6
Heilman and Smith (2010) 73.2
Qiu et al. (2006) 72.0
Wan et al. (2006) 75.6
Das and Smith (2009) 73.9
Das and Smith (2009)(PoE) 76.1
Our baseline (PR) 73.6
Our method (ps-SRK) 75.6
Our method (pw-SRK) 75.0
Our method (kb-SRK) 76.3
</table>
<tableCaption confidence="0.999996">
Table 1: Comparison with state-of-the-arts on MSRP.
</tableCaption>
<figure confidence="0.967901842105263">
1
Vn.v.2
0.5
0
455
Accuracy(%) 76.5 kb_SRK+PR 65.5 kb_SRK+PR
76 kb_SRK 64.5 kb_SRK
ps_SRK+PR ps_SRK+PR
pw_SRK+PR pw_SRK+PR
PR PR
75.5 Accuracy (%) 63.5
75 62.5
74.5 61.5
74 60.5
73.5
1 2 3 4
window size kmax
1 2 3window size4
kmax
</figure>
<figureCaption confidence="0.9935915">
Figure 7: Performances of different kernels under differ-
ent maximum window size kmax on MSRP.
</figureCaption>
<bodyText confidence="0.999885222222222">
ity. By comparing the results of kb-SRK and pw-
SRK we can see that the bijective property in kb-
SRK is really helpful for improving the performance
(note that both methods use wildcards). Further-
more, the performances of kb-SRK with and without
combining PR increase dramatically with increasing
kmax and reach the peaks (better than state-of-the-art)
when kmax is four, which shows the power of the lex-
ical and structural similarity captured by kb-SRK.
</bodyText>
<subsectionHeader confidence="0.999323">
6.2 Recognizing Textual Entailment
</subsectionHeader>
<bodyText confidence="0.999508846153846">
Recognizing textual entailment is to determine
whether a sentence (sometimes a short paragraph)
can entail the other sentence (Giampiccolo et al.,
2007). RTE-3 is a widely used benchmark dataset.
Following the common practice, we combined the
development set of RTE-3 and the whole datasets of
RTE-1 and RTE-2 as training data and took the test
set of RTE-3 as test data. The train and test sets con-
tain 3,767 and 800 sentence pairs.
The results are shown in Table 2. Again, kb-SRK
outperforms ps-SRK and pw-SRK. As indicated
in (Heilman and Smith, 2010), the top-performing
RTE systems are often built with significant engi-
</bodyText>
<table confidence="0.989599909090909">
Method Acc.
Harmeling (2007) 59.5
de Marneffe et al. (2006) 60.5
M&amp;M, (2007) (NL) 59.4
M&amp;M, (2007) (Hybrid) 64.3
Zanzotto et al. (2007) 65.75
Heilman and Smith (2010) 62.8
Our baseline (PR) 62.0
Our method (ps-SRK) 64.6
Our method (pw-SRK) 63.8
Our method (kb-SRK) 65.1
</table>
<tableCaption confidence="0.99987">
Table 2: Comparison with state-of-the-arts on RTE-3.
</tableCaption>
<figureCaption confidence="0.991809">
Figure 8: Performances of different kernels under differ-
ent maximum window size kmax on RTE-3.
</figureCaption>
<bodyText confidence="0.999738214285714">
neering efforts. Therefore, we only compare with
the six systems which involves less engineering. kb-
SRK still outperforms most of those state-of-the-art
methods even if it does not exploit any other lexical
semantic sources and syntactic analysis tools.
Fig. 8 shows the results of the kernels under dif-
ferent parameter settings. Again, the results of ps-
SRK and pw-SRK without combining PR are too
low to be shown (all below 55%). We can see that
PR is an effective method for this dataset and the
overall performances are substantially improved af-
ter combining it with the kernels. The performance
of kb-SRK reaches the peak when window size be-
comes two.
</bodyText>
<sectionHeader confidence="0.998205" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999877615384616">
In this paper, we have proposed a novel class of ker-
nel functions for sentence re-writing, called string
re-writing kernel (SRK). SRK measures the lexical
and structural similarity between two pairs of sen-
tences without using syntactic trees. The approach
is theoretically sound and is flexible to formulations
of sentences. A specific instance of SRK, referred
to as kb-SRK, has been developed which can bal-
ance the effectiveness and efficiency for sentence
re-writing. Experimental results show that kb-SRK
achieve better results than state-of-the-art methods
on paraphrase identification and recognizing textual
entailment.
</bodyText>
<sectionHeader confidence="0.998157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.8941835">
This work is supported by the National Basic Re-
search Program (973 Program) No. 2012CB316301.
</bodyText>
<sectionHeader confidence="0.991607" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.734602">
Baldrige, J. , Morton, T. and Bierner G. OpenNLP.
http://opennlp.sourceforge.net/.
</reference>
<page confidence="0.996105">
456
</page>
<reference confidence="0.998328726415095">
Barzilay, R. and Lee, L. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pp. 16–23.
Basilico, J. and Hofmann, T. 2004. Unifying collab-
orative and content-based filtering. Proceedings of
the twenty-first international conference on Machine
learning, pp. 9, 2004.
Ben-Hur, A. and Noble, W.S. 2005. Kernel methods for
predicting protein–protein interactions. Bioinformat-
ics, vol. 21, pp. i38–i46, Oxford Univ Press.
Bhagat, R. and Ravichandran, D. 2008. Large scale ac-
quisition ofparaphrases for learning surface patterns.
Proceedings of ACL-08: HLT, pp. 674–682.
Chang, C. and Lin, C. 2011. LIBSVM: A library for sup-
port vector machines. ACM Transactions on Intelli-
gent Systems and Technology vol. 2, issue 3, pp. 27:1–
27:27. Software available at http://www.csie.
ntu.edu.tw/˜cjlin/libsvm
Das, D. and Smith, N.A. 2009. Paraphrase identifi-
cation as probabilistic quasi-synchronous recognition.
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pp. 468–476.
de Marneffe, M., MacCartney, B., Grenager, T., Cer, D.,
Rafferty A. and Manning C.D. 2006. Learning to dis-
tinguish valid textual entailments. Proc. of the Second
PASCAL Challenges Workshop.
Dolan, W.B. and Brockett, C. 2005. Automatically con-
structing a corpus of sentential paraphrases. Proc. of
IWP.
Giampiccolo, D., Magnini B., Dagan I., and Dolan B.,
editors 2007. The third pascal recognizing textual en-
tailment challenge. Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pp. 1–9.
Harmeling, S. 2007. An extensible probabilistic
transformation-based approach to the third recogniz-
ing textual entailment challenge. Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pp. 137–142, 2007.
Heilman, M. and Smith, N.A. 2010. Tree edit models for
recognizing textual entailments, paraphrases, and an-
swers to questions. Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pp. 1011-1019.
Kashima, H. , Oyama, S. , Yamanishi, Y. and Tsuda, K.
2009. On pairwise kernels: An efficient alternative
and generalization analysis. Advances in Knowledge
Discovery and Data Mining, pp. 1030-1037, 2009,
Springer.
Kimeldorf, G. and Wahba, G. 1971. Some results on
Tchebycheffian spline functions. Journal of Mathemat-
ical Analysis and Applications, Vol.33, No.1, pp.82-
95, Elsevier.
Lin, D. and Pantel, P. 2001. DIRT-discovery of inference
rules from text. Proc. of ACM SIGKDD Conference
on Knowledge Discovery and Data Mining.
Lintean, M. and Rus, V. 2011. Dissimilarity Kernels
for Paraphrase Identification. Twenty-Fourth Interna-
tional FLAIRS Conference.
Leslie, C. , Eskin, E. and Noble, W.S. 2002. The spec-
trum kernel: a string kernel for SVM protein classifi-
cation. Pacific symposium on biocomputing vol. 575,
pp. 564-575, Hawaii, USA.
Leslie, C. and Kuang, R. 2004. Fast string kernels using
inexact matching for protein sequences. The Journal
of Machine Learning Research vol. 5, pp. 1435-1455.
Lodhi, H. , Saunders, C. , Shawe-Taylor, J. , Cristianini,
N. and Watkins, C. 2002. Text classification using
string kernels. The Journal of Machine Learning Re-
search vol. 2, pp. 419-444.
MacCartney, B. and Manning, C.D. 2008. Modeling se-
mantic containment and exclusion in natural language
inference. Proceedings of the 22nd International Con-
ference on Computational Linguistics, vol. 1, pp. 521-
528, 2008.
Moschitti, A. and Zanzotto, F.M. 2007. Fast and Effec-
tive Kernels for Relational Learning from Texts. Pro-
ceedings of the 24th Annual International Conference
on Machine Learning, Corvallis, OR, USA, 2007.
Qiu, L. and Kan, M.Y. and Chua, T.S. 2006. Para-
phrase recognition via dissimilarity significance clas-
sification. Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pp. 18–26.
Quirk, C. , Brockett, C. and Dolan, W. 2004. Monolin-
gual machine translation for paraphrase generation.
Proceedings of EMNLP 2004, pp. 142-149, Barcelona,
Spain.
Sch¨olkopf, B. and Smola, A.J. 2002. Learning with
kernels: Support vector machines, regularization, op-
timization, and beyond. The MIT Press, Cambridge,
MA.
Vapnik, V.N. 2000. The nature of statistical learning
theory. Springer Verlag.
Wan, S. , Dras, M. , Dale, R. and Paris, C. 2006. Using
dependency-based features to take the “Para-farce”
out ofparaphrase. Proc. of the Australasian Language
Technology Workshop, pp. 131–138.
Zanzotto, F.M. , Pennacchiotti, M. and Moschitti, A.
2007. Shallow semantics in fast textual entailment
</reference>
<page confidence="0.979971">
457
</page>
<reference confidence="0.999004">
rule learners. Proceedings of the ACL-PASCAL
workshop on textual entailment and paraphrasing, pp.
72–77.
Zhang, Y. and Patrick, J. 2005. Paraphrase identifica-
tion by text canonicalization. Proceedings of the Aus-
tralasian Language Technology Workshop, pp. 160–
166.
</reference>
<page confidence="0.997734">
458
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.211903">
<title confidence="0.9018195">String Re-writing Kernel Hang and</title>
<author confidence="0.42732">Key Laboratory of Intelligent Technology</author>
<affiliation confidence="0.785503">National Laboratory for Information Sci. and of Computer Sci. and Tech., Tsinghua</affiliation>
<address confidence="0.952576">Research Asia, No. 5 Danling Street, Beijing</address>
<abstract confidence="0.98852875">Learning for sentence re-writing is a fundamental task in natural language processing and information retrieval. In this paper, we propose a new class of kernel functions, referred to as string re-writing kernel, to address the problem. A string re-writing kernel measures similarity between pairs of each pair representing re-writing of a string. It can capture the lexical and structural similarity between two pairs of sentences without the need of constructing syntactic trees. We further propose an instance of string rewriting kernel which can be computed efficiently. Experimental results on benchmark datasets show that our method can achieve better results than state-of-the-art methods on two sentence re-writing learning tasks: paraphrase identification and recognizing textual entailment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>T Morton</author>
<author>Bierner G OpenNLP</author>
</authors>
<note>http://opennlp.sourceforge.net/.</note>
<marker>Morton, OpenNLP, </marker>
<rawString>Baldrige, J. , Morton, T. and Bierner G. OpenNLP. http://opennlp.sourceforge.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiple-sequence alignment.</title>
<date>2003</date>
<booktitle>Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="2104" citStr="Barzilay and Lee, 2003" startWordPosition="314" endWordPosition="317"> represent the rewriting of sentences. In previous research on sentence re-writing learning such as paraphrase identification and recognizing textual entailment, most representations are based on the lexicons (Zhang and Patrick, 2005; Lintean and Rus, 2011; de Marneffe et al., 2006) or the syntactic trees (Das and Smith, * * wrote . Shakespeare wrote Hamlet. * was written by . Hamlet was written by Shakespeare. * (A) (B) Figure 1: Example of re-writing. (A) is a re-writing rule and (B) is a re-writing of sentence. 2009; Heilman and Smith, 2010) of the sentence pairs. In (Lin and Pantel, 2001; Barzilay and Lee, 2003), re-writing rules serve as underlying representations for paraphrase generation/discovery. Motivated by the work, we represent re-writing of sentences by all possible re-writing rules that can be applied into it. For example, in Fig. 1, (A) is one re-writing rule that can be applied into the sentence re-writing (B). Specifically, we propose a new class of kernel functions (Sch¨olkopf and Smola, 2002), called string rewriting kernel (SRK), which defines the similarity between two re-writings (pairs) of strings as the inner product between them in the feature space induced by all the re-writing</context>
<context position="5007" citStr="Barzilay and Lee, 2003" startWordPosition="762" endWordPosition="765">exact matching between the strings. The string kernels defined on two pairs of objects (including strings) were also developed, which decompose the similarity into product of similarities between individual objects using tensor product (Basilico and Hofmann, 2004; Ben-Hur and Noble, 2005) or Cartesian product (Kashima et al., 2009). The task of paraphrasing usually consists of paraphrase pattern generation and paraphrase identification. Paraphrase pattern generation is to automatically extract semantically equivalent patterns (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008) or sentences (Barzilay and Lee, 2003). Paraphrase identification is to identify whether two given sentences are a paraphrase of each other. The methods proposed so far formalized the problem as classification and used various types of features such as bag-of-words feature, edit distance (Zhang and Patrick, 2005), dissimilarity kernel (Lintean and Rus, 2011) predicate-argument structure (Qiu et al., 2006), and tree edit model (which is based on a tree kernel) (Heilman and Smith, 2010) in the classification task. Among the most successful methods, Wan et al. (2006) enriched the feature set by the BLEU metric and dependency relation</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Barzilay, R. and Lee, L. 2003. Learning to paraphrase: An unsupervised approach using multiple-sequence alignment. Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pp. 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Basilico</author>
<author>T Hofmann</author>
</authors>
<title>Unifying collaborative and content-based filtering.</title>
<date>2004</date>
<booktitle>Proceedings of the twenty-first international conference on Machine learning,</booktitle>
<pages>9</pages>
<contexts>
<context position="4647" citStr="Basilico and Hofmann, 2004" startWordPosition="710" endWordPosition="714">ng kernel function, first proposed by Lodhi et al. (2002), measures the similarity between two strings by their shared substrings. Leslie et al. (2002) proposed the k-spectrum kernel which represents strings by their contiguous substrings of length k. Leslie et al. (2004) further proposed a number of string kernels including the wildcard kernel to facilitate inexact matching between the strings. The string kernels defined on two pairs of objects (including strings) were also developed, which decompose the similarity into product of similarities between individual objects using tensor product (Basilico and Hofmann, 2004; Ben-Hur and Noble, 2005) or Cartesian product (Kashima et al., 2009). The task of paraphrasing usually consists of paraphrase pattern generation and paraphrase identification. Paraphrase pattern generation is to automatically extract semantically equivalent patterns (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008) or sentences (Barzilay and Lee, 2003). Paraphrase identification is to identify whether two given sentences are a paraphrase of each other. The methods proposed so far formalized the problem as classification and used various types of features such as bag-of-words feature, edi</context>
</contexts>
<marker>Basilico, Hofmann, 2004</marker>
<rawString>Basilico, J. and Hofmann, T. 2004. Unifying collaborative and content-based filtering. Proceedings of the twenty-first international conference on Machine learning, pp. 9, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ben-Hur</author>
<author>W S Noble</author>
</authors>
<title>Kernel methods for predicting protein–protein interactions.</title>
<date>2005</date>
<journal>Bioinformatics,</journal>
<volume>21</volume>
<pages>38--46</pages>
<publisher>Univ Press.</publisher>
<location>Oxford</location>
<contexts>
<context position="4673" citStr="Ben-Hur and Noble, 2005" startWordPosition="715" endWordPosition="718">oposed by Lodhi et al. (2002), measures the similarity between two strings by their shared substrings. Leslie et al. (2002) proposed the k-spectrum kernel which represents strings by their contiguous substrings of length k. Leslie et al. (2004) further proposed a number of string kernels including the wildcard kernel to facilitate inexact matching between the strings. The string kernels defined on two pairs of objects (including strings) were also developed, which decompose the similarity into product of similarities between individual objects using tensor product (Basilico and Hofmann, 2004; Ben-Hur and Noble, 2005) or Cartesian product (Kashima et al., 2009). The task of paraphrasing usually consists of paraphrase pattern generation and paraphrase identification. Paraphrase pattern generation is to automatically extract semantically equivalent patterns (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008) or sentences (Barzilay and Lee, 2003). Paraphrase identification is to identify whether two given sentences are a paraphrase of each other. The methods proposed so far formalized the problem as classification and used various types of features such as bag-of-words feature, edit distance (Zhang and Patr</context>
</contexts>
<marker>Ben-Hur, Noble, 2005</marker>
<rawString>Ben-Hur, A. and Noble, W.S. 2005. Kernel methods for predicting protein–protein interactions. Bioinformatics, vol. 21, pp. i38–i46, Oxford Univ Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bhagat</author>
<author>D Ravichandran</author>
</authors>
<title>Large scale acquisition ofparaphrases for learning surface patterns.</title>
<date>2008</date>
<booktitle>Proceedings of ACL-08: HLT,</booktitle>
<pages>674--682</pages>
<contexts>
<context position="4969" citStr="Bhagat and Ravichandran, 2008" startWordPosition="756" endWordPosition="759">ncluding the wildcard kernel to facilitate inexact matching between the strings. The string kernels defined on two pairs of objects (including strings) were also developed, which decompose the similarity into product of similarities between individual objects using tensor product (Basilico and Hofmann, 2004; Ben-Hur and Noble, 2005) or Cartesian product (Kashima et al., 2009). The task of paraphrasing usually consists of paraphrase pattern generation and paraphrase identification. Paraphrase pattern generation is to automatically extract semantically equivalent patterns (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008) or sentences (Barzilay and Lee, 2003). Paraphrase identification is to identify whether two given sentences are a paraphrase of each other. The methods proposed so far formalized the problem as classification and used various types of features such as bag-of-words feature, edit distance (Zhang and Patrick, 2005), dissimilarity kernel (Lintean and Rus, 2011) predicate-argument structure (Qiu et al., 2006), and tree edit model (which is based on a tree kernel) (Heilman and Smith, 2010) in the classification task. Among the most successful methods, Wan et al. (2006) enriched the feature set by t</context>
</contexts>
<marker>Bhagat, Ravichandran, 2008</marker>
<rawString>Bhagat, R. and Ravichandran, D. 2008. Large scale acquisition ofparaphrases for learning surface patterns. Proceedings of ACL-08: HLT, pp. 674–682.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chang</author>
<author>C Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology</journal>
<volume>2</volume>
<pages>27--1</pages>
<note>Software available at http://www.csie. ntu.edu.tw/˜cjlin/libsvm</note>
<marker>Chang, Lin, 2011</marker>
<rawString>Chang, C. and Lin, C. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology vol. 2, issue 3, pp. 27:1– 27:27. Software available at http://www.csie. ntu.edu.tw/˜cjlin/libsvm</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>N A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>468--476</pages>
<contexts>
<context position="5630" citStr="Das and Smith (2009)" startWordPosition="862" endWordPosition="865">araphrase identification is to identify whether two given sentences are a paraphrase of each other. The methods proposed so far formalized the problem as classification and used various types of features such as bag-of-words feature, edit distance (Zhang and Patrick, 2005), dissimilarity kernel (Lintean and Rus, 2011) predicate-argument structure (Qiu et al., 2006), and tree edit model (which is based on a tree kernel) (Heilman and Smith, 2010) in the classification task. Among the most successful methods, Wan et al. (2006) enriched the feature set by the BLEU metric and dependency relations. Das and Smith (2009) used the quasi-synchronous grammar formalism to incorporate features from WordNet, named entity recognizer, POS tagger, and dependency labels from aligned trees. The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al., 2007). In recognizing textual entailment, de Marneffe et al. (2006) classified sentences pairs on the basis of word alignments. MacCartney and Manning (2008) used an inference procedure based on natural logic and combined it with the methods by de Marneffe et al. (2006). Harmeling (2007)</context>
<context position="26281" citStr="Das and Smith (2009)" startWordPosition="4707" endWordPosition="4710">existing lexical approach (Zhang and Patrick, 2005) and kernel approach (Lintean and Rus, 2011). It also works better than the other approaches listed in the table, which use syntactic trees or dependency relations. Fig. 7 gives detailed results of the kernels under different maximum k-gram lengths kmax with and without PR. The results of ps-SRK and pw-SRK without combining PR under different k are all below 71%, therefore they are not shown for clarMethod Acc. Zhang and Patrick (2005) 71.9 Lintean and Rus (2011) 73.6 Heilman and Smith (2010) 73.2 Qiu et al. (2006) 72.0 Wan et al. (2006) 75.6 Das and Smith (2009) 73.9 Das and Smith (2009)(PoE) 76.1 Our baseline (PR) 73.6 Our method (ps-SRK) 75.6 Our method (pw-SRK) 75.0 Our method (kb-SRK) 76.3 Table 1: Comparison with state-of-the-arts on MSRP. 1 Vn.v.2 0.5 0 455 Accuracy(%) 76.5 kb_SRK+PR 65.5 kb_SRK+PR 76 kb_SRK 64.5 kb_SRK ps_SRK+PR ps_SRK+PR pw_SRK+PR pw_SRK+PR PR PR 75.5 Accuracy (%) 63.5 75 62.5 74.5 61.5 74 60.5 73.5 1 2 3 4 window size kmax 1 2 3window size4 kmax Figure 7: Performances of different kernels under different maximum window size kmax on MSRP. ity. By comparing the results of kb-SRK and pwSRK we can see that the bijective property</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Das, D. and Smith, N.A. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pp. 468–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M de Marneffe</author>
<author>B MacCartney</author>
<author>T Grenager</author>
<author>D Cer</author>
<author>A Rafferty</author>
<author>C D Manning</author>
</authors>
<title>Learning to distinguish valid textual entailments.</title>
<date>2006</date>
<booktitle>Proc. of the Second PASCAL Challenges Workshop.</booktitle>
<marker>de Marneffe, MacCartney, Grenager, Cer, Rafferty, Manning, 2006</marker>
<rawString>de Marneffe, M., MacCartney, B., Grenager, T., Cer, D., Rafferty A. and Manning C.D. 2006. Learning to distinguish valid textual entailments. Proc. of the Second PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Dolan</author>
<author>C Brockett</author>
</authors>
<title>Automatically constructing a corpus of sentential paraphrases.</title>
<date>2005</date>
<booktitle>Proc. of IWP.</booktitle>
<contexts>
<context position="25307" citStr="Dolan and Brockett, 2005" startWordPosition="4543" endWordPosition="4546">nd then tried them under different -,1K(x,x)K(y,y) window sizes k. We also tried to combine the kernels with two lexical features “unigram precision and recall” proposed in (Wan et al., 2006), referred to as PR. For each kernel K, we tested the window size settings of K1 + ... + Kkmax (kmax E 11,2,3,4}) together with the combination with PR and we report the best accuracies of them in Tab 1 and Tab 2. 6.1 Paraphrase Identification The task of paraphrase identification is to examine whether two sentences have the same meaning. We trained and tested all the methods on the MSR Paraphrase Corpus (Dolan and Brockett, 2005; Quirk et al., 2004) consisting of 4,076 sentence pairs for training and 1,725 sentence pairs for testing. The experimental results on different SRKs are shown in Table 1. It can be seen that kb-SRK outperforms ps-SRK and pw-SRK. The results by the state-of-the-art methods reported in previous work are also included in Table 1. kb-SRK outperforms the existing lexical approach (Zhang and Patrick, 2005) and kernel approach (Lintean and Rus, 2011). It also works better than the other approaches listed in the table, which use syntactic trees or dependency relations. Fig. 7 gives detailed results </context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>Dolan, W.B. and Brockett, C. 2005. Automatically constructing a corpus of sentential paraphrases. Proc. of IWP.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Giampiccolo</author>
<author>B Magnini</author>
<author>I Dagan</author>
<author>B Dolan</author>
</authors>
<title>editors 2007. The third pascal recognizing textual entailment challenge.</title>
<booktitle>Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>1--9</pages>
<marker>Giampiccolo, Magnini, Dagan, Dolan, </marker>
<rawString>Giampiccolo, D., Magnini B., Dagan I., and Dolan B., editors 2007. The third pascal recognizing textual entailment challenge. Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pp. 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harmeling</author>
</authors>
<title>An extensible probabilistic transformation-based approach to the third recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="6230" citStr="Harmeling (2007)" startWordPosition="958" endWordPosition="959">and Smith (2009) used the quasi-synchronous grammar formalism to incorporate features from WordNet, named entity recognizer, POS tagger, and dependency labels from aligned trees. The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al., 2007). In recognizing textual entailment, de Marneffe et al. (2006) classified sentences pairs on the basis of word alignments. MacCartney and Manning (2008) used an inference procedure based on natural logic and combined it with the methods by de Marneffe et al. (2006). Harmeling (2007) and Heilman and Smith (2010) classified sequence pairs based on transformation on syntactic trees. Zanzotto et al. (2007) used a kernel method on syntactic tree pairs (Moschitti and Zanzotto, 2007). 3 Kernel Approach to Sentence Re-Writing Learning We formalize sentence re-writing learning as a kernel method. Following the literature of string kernel, we use the terms “string” and “character” instead of “sentence” and “word”. Suppose that we are given training data consisting of re-writings of strings and their responses ((s1,t1),y1),...,((sn,tn),yn) E(Σ* xΣ*) x Y where Σ denotes the characte</context>
<context position="27932" citStr="Harmeling (2007)" startWordPosition="4981" endWordPosition="4982"> determine whether a sentence (sometimes a short paragraph) can entail the other sentence (Giampiccolo et al., 2007). RTE-3 is a widely used benchmark dataset. Following the common practice, we combined the development set of RTE-3 and the whole datasets of RTE-1 and RTE-2 as training data and took the test set of RTE-3 as test data. The train and test sets contain 3,767 and 800 sentence pairs. The results are shown in Table 2. Again, kb-SRK outperforms ps-SRK and pw-SRK. As indicated in (Heilman and Smith, 2010), the top-performing RTE systems are often built with significant engiMethod Acc. Harmeling (2007) 59.5 de Marneffe et al. (2006) 60.5 M&amp;M, (2007) (NL) 59.4 M&amp;M, (2007) (Hybrid) 64.3 Zanzotto et al. (2007) 65.75 Heilman and Smith (2010) 62.8 Our baseline (PR) 62.0 Our method (ps-SRK) 64.6 Our method (pw-SRK) 63.8 Our method (kb-SRK) 65.1 Table 2: Comparison with state-of-the-arts on RTE-3. Figure 8: Performances of different kernels under different maximum window size kmax on RTE-3. neering efforts. Therefore, we only compare with the six systems which involves less engineering. kbSRK still outperforms most of those state-of-the-art methods even if it does not exploit any other lexical sem</context>
</contexts>
<marker>Harmeling, 2007</marker>
<rawString>Harmeling, S. 2007. An extensible probabilistic transformation-based approach to the third recognizing textual entailment challenge. Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pp. 137–142, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>N A Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>1011--1019</pages>
<contexts>
<context position="2031" citStr="Heilman and Smith, 2010" startWordPosition="301" endWordPosition="304">etween query and document title in search. The key question here is how to represent the rewriting of sentences. In previous research on sentence re-writing learning such as paraphrase identification and recognizing textual entailment, most representations are based on the lexicons (Zhang and Patrick, 2005; Lintean and Rus, 2011; de Marneffe et al., 2006) or the syntactic trees (Das and Smith, * * wrote . Shakespeare wrote Hamlet. * was written by . Hamlet was written by Shakespeare. * (A) (B) Figure 1: Example of re-writing. (A) is a re-writing rule and (B) is a re-writing of sentence. 2009; Heilman and Smith, 2010) of the sentence pairs. In (Lin and Pantel, 2001; Barzilay and Lee, 2003), re-writing rules serve as underlying representations for paraphrase generation/discovery. Motivated by the work, we represent re-writing of sentences by all possible re-writing rules that can be applied into it. For example, in Fig. 1, (A) is one re-writing rule that can be applied into the sentence re-writing (B). Specifically, we propose a new class of kernel functions (Sch¨olkopf and Smola, 2002), called string rewriting kernel (SRK), which defines the similarity between two re-writings (pairs) of strings as the inne</context>
<context position="5458" citStr="Heilman and Smith, 2010" startWordPosition="833" endWordPosition="836">e pattern generation is to automatically extract semantically equivalent patterns (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008) or sentences (Barzilay and Lee, 2003). Paraphrase identification is to identify whether two given sentences are a paraphrase of each other. The methods proposed so far formalized the problem as classification and used various types of features such as bag-of-words feature, edit distance (Zhang and Patrick, 2005), dissimilarity kernel (Lintean and Rus, 2011) predicate-argument structure (Qiu et al., 2006), and tree edit model (which is based on a tree kernel) (Heilman and Smith, 2010) in the classification task. Among the most successful methods, Wan et al. (2006) enriched the feature set by the BLEU metric and dependency relations. Das and Smith (2009) used the quasi-synchronous grammar formalism to incorporate features from WordNet, named entity recognizer, POS tagger, and dependency labels from aligned trees. The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al., 2007). In recognizing textual entailment, de Marneffe et al. (2006) classified sentences pairs on the basis of word </context>
<context position="26209" citStr="Heilman and Smith (2010)" startWordPosition="4692" endWordPosition="4695">orted in previous work are also included in Table 1. kb-SRK outperforms the existing lexical approach (Zhang and Patrick, 2005) and kernel approach (Lintean and Rus, 2011). It also works better than the other approaches listed in the table, which use syntactic trees or dependency relations. Fig. 7 gives detailed results of the kernels under different maximum k-gram lengths kmax with and without PR. The results of ps-SRK and pw-SRK without combining PR under different k are all below 71%, therefore they are not shown for clarMethod Acc. Zhang and Patrick (2005) 71.9 Lintean and Rus (2011) 73.6 Heilman and Smith (2010) 73.2 Qiu et al. (2006) 72.0 Wan et al. (2006) 75.6 Das and Smith (2009) 73.9 Das and Smith (2009)(PoE) 76.1 Our baseline (PR) 73.6 Our method (ps-SRK) 75.6 Our method (pw-SRK) 75.0 Our method (kb-SRK) 76.3 Table 1: Comparison with state-of-the-arts on MSRP. 1 Vn.v.2 0.5 0 455 Accuracy(%) 76.5 kb_SRK+PR 65.5 kb_SRK+PR 76 kb_SRK 64.5 kb_SRK ps_SRK+PR ps_SRK+PR pw_SRK+PR pw_SRK+PR PR PR 75.5 Accuracy (%) 63.5 75 62.5 74.5 61.5 74 60.5 73.5 1 2 3 4 window size kmax 1 2 3window size4 kmax Figure 7: Performances of different kernels under different maximum window size kmax on MSRP. ity. By comparin</context>
<context position="27834" citStr="Heilman and Smith, 2010" startWordPosition="4965" endWordPosition="4968">ral similarity captured by kb-SRK. 6.2 Recognizing Textual Entailment Recognizing textual entailment is to determine whether a sentence (sometimes a short paragraph) can entail the other sentence (Giampiccolo et al., 2007). RTE-3 is a widely used benchmark dataset. Following the common practice, we combined the development set of RTE-3 and the whole datasets of RTE-1 and RTE-2 as training data and took the test set of RTE-3 as test data. The train and test sets contain 3,767 and 800 sentence pairs. The results are shown in Table 2. Again, kb-SRK outperforms ps-SRK and pw-SRK. As indicated in (Heilman and Smith, 2010), the top-performing RTE systems are often built with significant engiMethod Acc. Harmeling (2007) 59.5 de Marneffe et al. (2006) 60.5 M&amp;M, (2007) (NL) 59.4 M&amp;M, (2007) (Hybrid) 64.3 Zanzotto et al. (2007) 65.75 Heilman and Smith (2010) 62.8 Our baseline (PR) 62.0 Our method (ps-SRK) 64.6 Our method (pw-SRK) 63.8 Our method (kb-SRK) 65.1 Table 2: Comparison with state-of-the-arts on RTE-3. Figure 8: Performances of different kernels under different maximum window size kmax on RTE-3. neering efforts. Therefore, we only compare with the six systems which involves less engineering. kbSRK still ou</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Heilman, M. and Smith, N.A. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 1011-1019.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oyama</author>
</authors>
<title>On pairwise kernels: An efficient alternative and generalization analysis.</title>
<date>2009</date>
<booktitle>Advances in Knowledge Discovery and Data Mining,</booktitle>
<pages>1030--1037</pages>
<publisher>Springer.</publisher>
<marker>Oyama, 2009</marker>
<rawString>Kashima, H. , Oyama, S. , Yamanishi, Y. and Tsuda, K. 2009. On pairwise kernels: An efficient alternative and generalization analysis. Advances in Knowledge Discovery and Data Mining, pp. 1030-1037, 2009, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kimeldorf</author>
<author>G Wahba</author>
</authors>
<title>Some results on Tchebycheffian spline functions.</title>
<date>1971</date>
<journal>Journal of Mathematical Analysis and Applications,</journal>
<volume>33</volume>
<pages>82--95</pages>
<publisher>Elsevier.</publisher>
<contexts>
<context position="7886" citStr="Kimeldorf and Wahba, 1971" startWordPosition="1241" endWordPosition="1245">) E Σ* xΣ*, our goal is to predict its response y. That is, the training data consists of binary classes of string re-writings, and the prediction is made for the new re-writing based on learning from the training data. We take the kernel approach to address the learning task. The kernel on re-writings of strings is defined as K : (Σ* x Σ*) x (Σ* x Σ*) —* R satisfying for all (si,ti), (sj,tj) E Σ* x Σ*, K((si,ti),(sj,tj)) = (Φ(si,ti),Φ(sj,tj)) where Φ maps each re-writing (pair) of strings into a high dimensional Hilbert space _ V, referred to as 450 feature space. By the representer theorem (Kimeldorf and Wahba, 1971; Sch¨olkopf and Smola, 2002), it can be shown that the response y of a new string re-writing (s,t) can always be represented as αiyiK((si,ti),(s,t))) where αi ≥ 0,(i = 1,··· ,n) are parameters. That is, it is determined by a linear combination of the similarities between the new instance and the instances in training set. It is also known that by employing a learning model such as SVM (Vapnik, 2000), such a linear combination can be automatically learned by solving a quadratic optimization problem. The question then becomes how to design the kernel function for the task. 4 String Re-writing K</context>
</contexts>
<marker>Kimeldorf, Wahba, 1971</marker>
<rawString>Kimeldorf, G. and Wahba, G. 1971. Some results on Tchebycheffian spline functions. Journal of Mathematical Analysis and Applications, Vol.33, No.1, pp.82-95, Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>DIRT-discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>Proc. of ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="2079" citStr="Lin and Pantel, 2001" startWordPosition="310" endWordPosition="313">uestion here is how to represent the rewriting of sentences. In previous research on sentence re-writing learning such as paraphrase identification and recognizing textual entailment, most representations are based on the lexicons (Zhang and Patrick, 2005; Lintean and Rus, 2011; de Marneffe et al., 2006) or the syntactic trees (Das and Smith, * * wrote . Shakespeare wrote Hamlet. * was written by . Hamlet was written by Shakespeare. * (A) (B) Figure 1: Example of re-writing. (A) is a re-writing rule and (B) is a re-writing of sentence. 2009; Heilman and Smith, 2010) of the sentence pairs. In (Lin and Pantel, 2001; Barzilay and Lee, 2003), re-writing rules serve as underlying representations for paraphrase generation/discovery. Motivated by the work, we represent re-writing of sentences by all possible re-writing rules that can be applied into it. For example, in Fig. 1, (A) is one re-writing rule that can be applied into the sentence re-writing (B). Specifically, we propose a new class of kernel functions (Sch¨olkopf and Smola, 2002), called string rewriting kernel (SRK), which defines the similarity between two re-writings (pairs) of strings as the inner product between them in the feature space indu</context>
<context position="4937" citStr="Lin and Pantel, 2001" startWordPosition="752" endWordPosition="755">er of string kernels including the wildcard kernel to facilitate inexact matching between the strings. The string kernels defined on two pairs of objects (including strings) were also developed, which decompose the similarity into product of similarities between individual objects using tensor product (Basilico and Hofmann, 2004; Ben-Hur and Noble, 2005) or Cartesian product (Kashima et al., 2009). The task of paraphrasing usually consists of paraphrase pattern generation and paraphrase identification. Paraphrase pattern generation is to automatically extract semantically equivalent patterns (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008) or sentences (Barzilay and Lee, 2003). Paraphrase identification is to identify whether two given sentences are a paraphrase of each other. The methods proposed so far formalized the problem as classification and used various types of features such as bag-of-words feature, edit distance (Zhang and Patrick, 2005), dissimilarity kernel (Lintean and Rus, 2011) predicate-argument structure (Qiu et al., 2006), and tree edit model (which is based on a tree kernel) (Heilman and Smith, 2010) in the classification task. Among the most successful methods, Wan et al. (200</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Lin, D. and Pantel, P. 2001. DIRT-discovery of inference rules from text. Proc. of ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lintean</author>
<author>V Rus</author>
</authors>
<title>Dissimilarity Kernels for Paraphrase Identification.</title>
<date>2011</date>
<booktitle>Twenty-Fourth International FLAIRS Conference.</booktitle>
<contexts>
<context position="1737" citStr="Lintean and Rus, 2011" startWordPosition="247" endWordPosition="250">tence re-writing learning tasks: paraphrase identification and recognizing textual entailment. 1 Introduction Learning for sentence re-writing is a fundamental task in natural language processing and information retrieval, which includes paraphrasing, textual entailment and transformation between query and document title in search. The key question here is how to represent the rewriting of sentences. In previous research on sentence re-writing learning such as paraphrase identification and recognizing textual entailment, most representations are based on the lexicons (Zhang and Patrick, 2005; Lintean and Rus, 2011; de Marneffe et al., 2006) or the syntactic trees (Das and Smith, * * wrote . Shakespeare wrote Hamlet. * was written by . Hamlet was written by Shakespeare. * (A) (B) Figure 1: Example of re-writing. (A) is a re-writing rule and (B) is a re-writing of sentence. 2009; Heilman and Smith, 2010) of the sentence pairs. In (Lin and Pantel, 2001; Barzilay and Lee, 2003), re-writing rules serve as underlying representations for paraphrase generation/discovery. Motivated by the work, we represent re-writing of sentences by all possible re-writing rules that can be applied into it. For example, in Fig</context>
<context position="5329" citStr="Lintean and Rus, 2011" startWordPosition="812" endWordPosition="815">al., 2009). The task of paraphrasing usually consists of paraphrase pattern generation and paraphrase identification. Paraphrase pattern generation is to automatically extract semantically equivalent patterns (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008) or sentences (Barzilay and Lee, 2003). Paraphrase identification is to identify whether two given sentences are a paraphrase of each other. The methods proposed so far formalized the problem as classification and used various types of features such as bag-of-words feature, edit distance (Zhang and Patrick, 2005), dissimilarity kernel (Lintean and Rus, 2011) predicate-argument structure (Qiu et al., 2006), and tree edit model (which is based on a tree kernel) (Heilman and Smith, 2010) in the classification task. Among the most successful methods, Wan et al. (2006) enriched the feature set by the BLEU metric and dependency relations. Das and Smith (2009) used the quasi-synchronous grammar formalism to incorporate features from WordNet, named entity recognizer, POS tagger, and dependency labels from aligned trees. The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampic</context>
<context position="25756" citStr="Lintean and Rus, 2011" startWordPosition="4615" endWordPosition="4618">phrase identification is to examine whether two sentences have the same meaning. We trained and tested all the methods on the MSR Paraphrase Corpus (Dolan and Brockett, 2005; Quirk et al., 2004) consisting of 4,076 sentence pairs for training and 1,725 sentence pairs for testing. The experimental results on different SRKs are shown in Table 1. It can be seen that kb-SRK outperforms ps-SRK and pw-SRK. The results by the state-of-the-art methods reported in previous work are also included in Table 1. kb-SRK outperforms the existing lexical approach (Zhang and Patrick, 2005) and kernel approach (Lintean and Rus, 2011). It also works better than the other approaches listed in the table, which use syntactic trees or dependency relations. Fig. 7 gives detailed results of the kernels under different maximum k-gram lengths kmax with and without PR. The results of ps-SRK and pw-SRK without combining PR under different k are all below 71%, therefore they are not shown for clarMethod Acc. Zhang and Patrick (2005) 71.9 Lintean and Rus (2011) 73.6 Heilman and Smith (2010) 73.2 Qiu et al. (2006) 72.0 Wan et al. (2006) 75.6 Das and Smith (2009) 73.9 Das and Smith (2009)(PoE) 76.1 Our baseline (PR) 73.6 Our method (ps-</context>
</contexts>
<marker>Lintean, Rus, 2011</marker>
<rawString>Lintean, M. and Rus, V. 2011. Dissimilarity Kernels for Paraphrase Identification. Twenty-Fourth International FLAIRS Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Eskin</author>
<author>W S Noble</author>
</authors>
<title>The spectrum kernel: a string kernel for SVM protein classification. Pacific symposium on biocomputing</title>
<date>2002</date>
<volume>575</volume>
<pages>564--575</pages>
<location>Hawaii, USA.</location>
<marker>Eskin, Noble, 2002</marker>
<rawString>Leslie, C. , Eskin, E. and Noble, W.S. 2002. The spectrum kernel: a string kernel for SVM protein classification. Pacific symposium on biocomputing vol. 575, pp. 564-575, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leslie</author>
<author>R Kuang</author>
</authors>
<title>Fast string kernels using inexact matching for protein sequences.</title>
<date>2004</date>
<journal>The Journal of Machine Learning Research</journal>
<volume>5</volume>
<pages>1435--1455</pages>
<marker>Leslie, Kuang, 2004</marker>
<rawString>Leslie, C. and Kuang, R. 2004. Fast string kernels using inexact matching for protein sequences. The Journal of Machine Learning Research vol. 5, pp. 1435-1455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shawe-Taylor</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>The Journal of Machine Learning Research</journal>
<volume>2</volume>
<pages>419--444</pages>
<marker>Shawe-Taylor, 2002</marker>
<rawString>Lodhi, H. , Saunders, C. , Shawe-Taylor, J. , Cristianini, N. and Watkins, C. 2002. Text classification using string kernels. The Journal of Machine Learning Research vol. 2, pp. 419-444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Modeling semantic containment and exclusion in natural language inference.</title>
<date>2008</date>
<booktitle>Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>521--528</pages>
<contexts>
<context position="6099" citStr="MacCartney and Manning (2008)" startWordPosition="933" endWordPosition="937">sification task. Among the most successful methods, Wan et al. (2006) enriched the feature set by the BLEU metric and dependency relations. Das and Smith (2009) used the quasi-synchronous grammar formalism to incorporate features from WordNet, named entity recognizer, POS tagger, and dependency labels from aligned trees. The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al., 2007). In recognizing textual entailment, de Marneffe et al. (2006) classified sentences pairs on the basis of word alignments. MacCartney and Manning (2008) used an inference procedure based on natural logic and combined it with the methods by de Marneffe et al. (2006). Harmeling (2007) and Heilman and Smith (2010) classified sequence pairs based on transformation on syntactic trees. Zanzotto et al. (2007) used a kernel method on syntactic tree pairs (Moschitti and Zanzotto, 2007). 3 Kernel Approach to Sentence Re-Writing Learning We formalize sentence re-writing learning as a kernel method. Following the literature of string kernel, we use the terms “string” and “character” instead of “sentence” and “word”. Suppose that we are given training dat</context>
</contexts>
<marker>MacCartney, Manning, 2008</marker>
<rawString>MacCartney, B. and Manning, C.D. 2008. Modeling semantic containment and exclusion in natural language inference. Proceedings of the 22nd International Conference on Computational Linguistics, vol. 1, pp. 521-528, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>F M Zanzotto</author>
</authors>
<title>Fast and Effective Kernels for Relational Learning from Texts.</title>
<date>2007</date>
<booktitle>Proceedings of the 24th Annual International Conference on Machine Learning,</booktitle>
<location>Corvallis, OR, USA,</location>
<contexts>
<context position="6428" citStr="Moschitti and Zanzotto, 2007" startWordPosition="986" endWordPosition="989">sk of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al., 2007). In recognizing textual entailment, de Marneffe et al. (2006) classified sentences pairs on the basis of word alignments. MacCartney and Manning (2008) used an inference procedure based on natural logic and combined it with the methods by de Marneffe et al. (2006). Harmeling (2007) and Heilman and Smith (2010) classified sequence pairs based on transformation on syntactic trees. Zanzotto et al. (2007) used a kernel method on syntactic tree pairs (Moschitti and Zanzotto, 2007). 3 Kernel Approach to Sentence Re-Writing Learning We formalize sentence re-writing learning as a kernel method. Following the literature of string kernel, we use the terms “string” and “character” instead of “sentence” and “word”. Suppose that we are given training data consisting of re-writings of strings and their responses ((s1,t1),y1),...,((sn,tn),yn) E(Σ* xΣ*) x Y where Σ denotes the character set, Σ* = U∞i=0 Σi denotes the string set, which is the Kleene closure of set Σ, Y denotes the set of responses, and n is the number of instances. (si,ti) is a re-writing consisting of the source </context>
</contexts>
<marker>Moschitti, Zanzotto, 2007</marker>
<rawString>Moschitti, A. and Zanzotto, F.M. 2007. Fast and Effective Kernels for Relational Learning from Texts. Proceedings of the 24th Annual International Conference on Machine Learning, Corvallis, OR, USA, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Qiu</author>
<author>M Y Kan</author>
<author>T S Chua</author>
</authors>
<title>Paraphrase recognition via dissimilarity significance classification.</title>
<date>2006</date>
<booktitle>Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>18--26</pages>
<contexts>
<context position="5377" citStr="Qiu et al., 2006" startWordPosition="818" endWordPosition="821"> of paraphrase pattern generation and paraphrase identification. Paraphrase pattern generation is to automatically extract semantically equivalent patterns (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008) or sentences (Barzilay and Lee, 2003). Paraphrase identification is to identify whether two given sentences are a paraphrase of each other. The methods proposed so far formalized the problem as classification and used various types of features such as bag-of-words feature, edit distance (Zhang and Patrick, 2005), dissimilarity kernel (Lintean and Rus, 2011) predicate-argument structure (Qiu et al., 2006), and tree edit model (which is based on a tree kernel) (Heilman and Smith, 2010) in the classification task. Among the most successful methods, Wan et al. (2006) enriched the feature set by the BLEU metric and dependency relations. Das and Smith (2009) used the quasi-synchronous grammar formalism to incorporate features from WordNet, named entity recognizer, POS tagger, and dependency labels from aligned trees. The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al., 2007). In recognizing textual entai</context>
<context position="26232" citStr="Qiu et al. (2006)" startWordPosition="4697" endWordPosition="4700">o included in Table 1. kb-SRK outperforms the existing lexical approach (Zhang and Patrick, 2005) and kernel approach (Lintean and Rus, 2011). It also works better than the other approaches listed in the table, which use syntactic trees or dependency relations. Fig. 7 gives detailed results of the kernels under different maximum k-gram lengths kmax with and without PR. The results of ps-SRK and pw-SRK without combining PR under different k are all below 71%, therefore they are not shown for clarMethod Acc. Zhang and Patrick (2005) 71.9 Lintean and Rus (2011) 73.6 Heilman and Smith (2010) 73.2 Qiu et al. (2006) 72.0 Wan et al. (2006) 75.6 Das and Smith (2009) 73.9 Das and Smith (2009)(PoE) 76.1 Our baseline (PR) 73.6 Our method (ps-SRK) 75.6 Our method (pw-SRK) 75.0 Our method (kb-SRK) 76.3 Table 1: Comparison with state-of-the-arts on MSRP. 1 Vn.v.2 0.5 0 455 Accuracy(%) 76.5 kb_SRK+PR 65.5 kb_SRK+PR 76 kb_SRK 64.5 kb_SRK ps_SRK+PR ps_SRK+PR pw_SRK+PR pw_SRK+PR PR PR 75.5 Accuracy (%) 63.5 75 62.5 74.5 61.5 74 60.5 73.5 1 2 3 4 window size kmax 1 2 3window size4 kmax Figure 7: Performances of different kernels under different maximum window size kmax on MSRP. ity. By comparing the results of kb-SRK</context>
</contexts>
<marker>Qiu, Kan, Chua, 2006</marker>
<rawString>Qiu, L. and Kan, M.Y. and Chua, T.S. 2006. Paraphrase recognition via dissimilarity significance classification. Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pp. 18–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brockett</author>
<author>W Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation.</title>
<date>2004</date>
<booktitle>Proceedings of EMNLP 2004,</booktitle>
<pages>142--149</pages>
<location>Barcelona,</location>
<marker>Brockett, Dolan, 2004</marker>
<rawString>Quirk, C. , Brockett, C. and Dolan, W. 2004. Monolingual machine translation for paraphrase generation. Proceedings of EMNLP 2004, pp. 142-149, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sch¨olkopf</author>
<author>A J Smola</author>
</authors>
<title>Learning with kernels: Support vector machines, regularization, optimization, and beyond.</title>
<date>2002</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Sch¨olkopf, Smola, 2002</marker>
<rawString>Sch¨olkopf, B. and Smola, A.J. 2002. Learning with kernels: Support vector machines, regularization, optimization, and beyond. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>The nature of statistical learning theory.</title>
<date>2000</date>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="8289" citStr="Vapnik, 2000" startWordPosition="1315" endWordPosition="1316">),(sj,tj)) = (Φ(si,ti),Φ(sj,tj)) where Φ maps each re-writing (pair) of strings into a high dimensional Hilbert space _ V, referred to as 450 feature space. By the representer theorem (Kimeldorf and Wahba, 1971; Sch¨olkopf and Smola, 2002), it can be shown that the response y of a new string re-writing (s,t) can always be represented as αiyiK((si,ti),(s,t))) where αi ≥ 0,(i = 1,··· ,n) are parameters. That is, it is determined by a linear combination of the similarities between the new instance and the instances in training set. It is also known that by employing a learning model such as SVM (Vapnik, 2000), such a linear combination can be automatically learned by solving a quadratic optimization problem. The question then becomes how to design the kernel function for the task. 4 String Re-writing Kernel Let Σ be the set of characters and Σ∗ be the set of strings. Let wildcard domain D ⊆ Σ∗ be the set of strings which can be replaced by wildcards. The string re-writing kernel measures the similarity between two string re-writings through the rewriting rules that can be applied into them. Formally, given re-writing rule set R and wildcard domain D, the string re-writing kernel (SRK) is defined a</context>
</contexts>
<marker>Vapnik, 2000</marker>
<rawString>Vapnik, V.N. 2000. The nature of statistical learning theory. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dras</author>
</authors>
<title>Using dependency-based features to take the “Para-farce” out ofparaphrase.</title>
<date>2006</date>
<booktitle>Proc. of the Australasian Language Technology Workshop,</booktitle>
<pages>131--138</pages>
<marker>Dras, 2006</marker>
<rawString>Wan, S. , Dras, M. , Dale, R. and Paris, C. 2006. Using dependency-based features to take the “Para-farce” out ofparaphrase. Proc. of the Australasian Language Technology Workshop, pp. 131–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pennacchiotti</author>
<author>A Moschitti</author>
</authors>
<title>Shallow semantics in fast textual entailment rule learners.</title>
<date>2007</date>
<booktitle>Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing,</booktitle>
<pages>72--77</pages>
<marker>Pennacchiotti, Moschitti, 2007</marker>
<rawString>Zanzotto, F.M. , Pennacchiotti, M. and Moschitti, A. 2007. Shallow semantics in fast textual entailment rule learners. Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pp. 72–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>J Patrick</author>
</authors>
<title>Paraphrase identification by text canonicalization.</title>
<date>2005</date>
<booktitle>Proceedings of the Australasian Language Technology Workshop,</booktitle>
<pages>160--166</pages>
<contexts>
<context position="1714" citStr="Zhang and Patrick, 2005" startWordPosition="243" endWordPosition="246">he-art methods on two sentence re-writing learning tasks: paraphrase identification and recognizing textual entailment. 1 Introduction Learning for sentence re-writing is a fundamental task in natural language processing and information retrieval, which includes paraphrasing, textual entailment and transformation between query and document title in search. The key question here is how to represent the rewriting of sentences. In previous research on sentence re-writing learning such as paraphrase identification and recognizing textual entailment, most representations are based on the lexicons (Zhang and Patrick, 2005; Lintean and Rus, 2011; de Marneffe et al., 2006) or the syntactic trees (Das and Smith, * * wrote . Shakespeare wrote Hamlet. * was written by . Hamlet was written by Shakespeare. * (A) (B) Figure 1: Example of re-writing. (A) is a re-writing rule and (B) is a re-writing of sentence. 2009; Heilman and Smith, 2010) of the sentence pairs. In (Lin and Pantel, 2001; Barzilay and Lee, 2003), re-writing rules serve as underlying representations for paraphrase generation/discovery. Motivated by the work, we represent re-writing of sentences by all possible re-writing rules that can be applied into </context>
<context position="5283" citStr="Zhang and Patrick, 2005" startWordPosition="806" endWordPosition="809">d Noble, 2005) or Cartesian product (Kashima et al., 2009). The task of paraphrasing usually consists of paraphrase pattern generation and paraphrase identification. Paraphrase pattern generation is to automatically extract semantically equivalent patterns (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008) or sentences (Barzilay and Lee, 2003). Paraphrase identification is to identify whether two given sentences are a paraphrase of each other. The methods proposed so far formalized the problem as classification and used various types of features such as bag-of-words feature, edit distance (Zhang and Patrick, 2005), dissimilarity kernel (Lintean and Rus, 2011) predicate-argument structure (Qiu et al., 2006), and tree edit model (which is based on a tree kernel) (Heilman and Smith, 2010) in the classification task. Among the most successful methods, Wan et al. (2006) enriched the feature set by the BLEU metric and dependency relations. Das and Smith (2009) used the quasi-synchronous grammar formalism to incorporate features from WordNet, named entity recognizer, POS tagger, and dependency labels from aligned trees. The task of recognizing textual entailment is to decide whether the hypothesis sentence ca</context>
<context position="25712" citStr="Zhang and Patrick, 2005" startWordPosition="4608" endWordPosition="4611">6.1 Paraphrase Identification The task of paraphrase identification is to examine whether two sentences have the same meaning. We trained and tested all the methods on the MSR Paraphrase Corpus (Dolan and Brockett, 2005; Quirk et al., 2004) consisting of 4,076 sentence pairs for training and 1,725 sentence pairs for testing. The experimental results on different SRKs are shown in Table 1. It can be seen that kb-SRK outperforms ps-SRK and pw-SRK. The results by the state-of-the-art methods reported in previous work are also included in Table 1. kb-SRK outperforms the existing lexical approach (Zhang and Patrick, 2005) and kernel approach (Lintean and Rus, 2011). It also works better than the other approaches listed in the table, which use syntactic trees or dependency relations. Fig. 7 gives detailed results of the kernels under different maximum k-gram lengths kmax with and without PR. The results of ps-SRK and pw-SRK without combining PR under different k are all below 71%, therefore they are not shown for clarMethod Acc. Zhang and Patrick (2005) 71.9 Lintean and Rus (2011) 73.6 Heilman and Smith (2010) 73.2 Qiu et al. (2006) 72.0 Wan et al. (2006) 75.6 Das and Smith (2009) 73.9 Das and Smith (2009)(PoE)</context>
</contexts>
<marker>Zhang, Patrick, 2005</marker>
<rawString>Zhang, Y. and Patrick, J. 2005. Paraphrase identification by text canonicalization. Proceedings of the Australasian Language Technology Workshop, pp. 160– 166.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>