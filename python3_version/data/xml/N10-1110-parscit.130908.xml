<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001781">
<title confidence="0.999125">
Investigations into the Crandem Approach to Word Recognition
</title>
<author confidence="0.998207">
Rohit Prabhavalkar, Preethi Jyothi, William Hartmann, Jeremy Morris, and Eric Fosler-Lussier
</author>
<affiliation confidence="0.9945205">
Department of Computer Science and Engineering
The Ohio State University, Columbus, OH
</affiliation>
<email confidence="0.998513">
{prabhava,jyothi,hartmanw,morrijer,fosler}@cse.ohio-state.edu
</email>
<sectionHeader confidence="0.995629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999783625">
We suggest improvements to a previously pro-
posed framework for integrating Conditional
Random Fields and Hidden Markov Models,
dubbed a Crandem system (2009). The pre-
vious authors’ work suggested that local la-
bel posteriors derived from the CRF were too
low-entropy for use in word-level automatic
speech recognition. As an alternative to the
log posterior representation used in their sys-
tem, we explore frame-level representations
derived from the CRF feature functions. We
also describe a weight normalization transfor-
mation that leads to increased entropy of the
CRF posteriors. We report significant gains
over the previous Crandem system on the Wall
Street Journal word recognition task.
</bodyText>
<sectionHeader confidence="0.998907" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990143145833334">
Conditional Random Fields (CRFs) (Lafferty et
al., 2001) have recently emerged as a promising
new paradigm in the domain of Automatic Speech
Recognition (ASR). Unlike Hidden Markov Mod-
els (HMMs), CRFs are direct discriminative models:
they predict the probability of a label sequence con-
ditioned on the input. As a result, CRFs can capture
long-range dependencies in the data and avoid the
need for restrictive independence assumptions. Vari-
ants of CRFs have been successfully used in phone
recognition tasks (Gunawardana et al., 2005; Morris
and Fosler-Lussier, 2008; Hifny and Renals, 2009).
While the improvements in the phone recognition
task are encouraging, recent efforts have been di-
rected towards extending the CRF paradigm to the
word recognition level (Zweig and Nguyen, 2009;
Morris and Fosler-Lussier, 2009). The Crandem
system (Morris and Fosler-Lussier, 2009) is one of
the promising approaches in this regard. The Cran-
dem system is directly inspired by the techniques
of the Tandem system (Hermansky et al., 2000),
where phone-label posterior estimates produced by
a Multi-Layer Perceptron (MLP) are transformed
into a suitable acoustic representation for a standard
HMM. In both systems, the frame-based log poste-
rior vector of P (phone|acoustics) overall phones is
decorrelated using the Karhunen-Loeve (KL) trans-
form; unlike MLPs, CRFs take into account the en-
tire label sequence when computing local posteriors.
However, posterior estimates from the CRF tend to
be overconfident compared to MLP posteriors (Mor-
ris and Fosler-Lussier, 2009).
In this paper, we analyze the interplay between
the various steps involved in the Crandem process.
Is the local posterior representation from the CRF
the best representation? Given that the CRF poste-
rior estimates can be overconfident, what transfor-
mations to the posteriors are appropriate?
In Section 2 we briefly describe CRFs and the
Crandem framework. We suggest techniques for im-
proving Crandem word recognition performance in
Section 3. Details of experiments and our results are
discussed in Sections 4 and 5 respectively. We con-
clude with a discussion of future work in Section 6.
2 CRFs and the Crandem System
Conditional random fields (Lafferty et al., 2001) ex-
press the probability of a label sequence Q condi-
tioned on the input data X as a log-linear sum of
</bodyText>
<page confidence="0.935517">
725
</page>
<subsubsectionHeader confidence="0.475963">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 725–728,
</subsubsectionHeader>
<subsectionHeader confidence="0.10791">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.361357">
weighted feature functions,
</bodyText>
<equation confidence="0.99909525">
exp � �j Ajsj(qt, X) + �j µjfj(qt−1, qt, X)
t
Z(X)
(1)
</equation>
<bodyText confidence="0.999963153846154">
where sj(�) and fj(�) are known as state feature
functions and transition feature functions respec-
tively, and λj and µj are the associated weights.
Z(X) is a normalization term that ensures a valid
probability distribution. Given a set of labeled ex-
amples, the CRF is trained to maximize the con-
ditional log-likelihood of the training set. The
log-likelihood is concave over the entire parameter
space, and can be maximized using standard convex
optimization techniques (Lafferty et al., 2001; Sha
and Pereira, 2003). The local posterior probability
of a particular label can be computed via a forward-
backward style algorithm. Mathematically,
</bodyText>
<equation confidence="0.959513">
αt(q|X)βt(q|X)
p(qt = q|X) = (2)
Z(X)
</equation>
<bodyText confidence="0.999966">
where αt(q|X) and βt(q|X) accumulate contribu-
tions associated with possible assignments of la-
bels before and after the current time-step t. The
Crandem system utilizes these local posterior val-
ues from the CRF analogously to the way in which
MLP-posteriors are treated in the Tandem frame-
work (Hermansky et al., 2000), by applying a log
transformation to the posteriors. These transformed
outputs are then decorrelated using a KL-transform
and then dimensionality-reduced to be used as a re-
placement for MFCCs in a HMM system. While
the MLP is usually reduced to 39 dimensions, the
standard CRF benefits from a higher dimensionality
reduction (to 19 dimensions). The decorrelated out-
puts are then used as an input representation for a
conventional HMM system.
</bodyText>
<sectionHeader confidence="0.9976815" genericHeader="method">
3 Improving Crandem Recognition
Results
</sectionHeader>
<bodyText confidence="0.987949375">
Morris and Fosler-Lussier (2009) indicate that the
local posterior outputs from the CRF model pro-
duces features that are more heavily skewed to the
dominant phone class than the MLP system, leading
to an increase in word recognition errors. In order
to correct for this, we perform a non-linear trans-
formation on the local CRF posterior representa-
tion before applying a KL-transform and subsequent
stages. Specifically, we normalize all of the weights
λj and µj in Equation 1 by a fixed positive constant
n to obtain normalized weights λ0j and µ0j. We note
that the probability of a label sequence computed us-
ing the transformed weights, p0(Q|X), is equivalent
to taking the nth-root of the CRF probability com-
puted using the unnormalized weights, with a new
normalization term Z0(X)
</bodyText>
<equation confidence="0.9941055">
p0(Q|X) = p(Q|X)1/n (3)
Z0(X)
</equation>
<bodyText confidence="0.99994736">
where, p(Q|X) is as defined in Equation 1. Also
observe that the monotonicity of the nth-root func-
tion ensures that if p(Q1|X) &gt; p(Q2|X) then
p0(Q1|X) &gt; p0(Q2|X). In other words, the rank
order of the n-best phone recognition results are not
impacted by this change. The transformation does,
however, increase the entropy between the domi-
nant class from the CRF and its competitors, since
p0(Q|X) &lt; p(Q|X). As we shall discuss in Section
5, this transformation helps improve word recogni-
tion performance in the Crandem framework.
Our second set of experiments are based on the
following observation regarding the CRF posteriors.
As can be seen from Equation 2, the CRF posteri-
ors involve a global normalization over the entire ut-
terance as opposed to the local normalization of the
MLP posteriors in the output softmax layer. This
motivates the use of representations derived from
the CRF that are ‘local’ in some sense. We there-
fore propose two alternative representations that are
modeled along the lines of the linear outputs from an
MLP. The first uses the sum of the state feature func-
tions, to obtain a vector fstate(X,t) for each time
step t and input utterance X of length |Q |dimen-
sions, where Q is the set of possible phone labels
</bodyText>
<equation confidence="0.505608666666667">
� T
f state (X, t) = λjsj (q, X)1 � bq E Q
j
</equation>
<bodyText confidence="0.945558142857143">
(4)
where q is a particular phone label. Note that the
lack of an exponential term in this representation en-
sures that the representation is less ‘spiky’ than the
CRF posteriors. Additionally, the decoupling of the
representation from the transition feature functions
could potentially allow the system to represent rel-
</bodyText>
<equation confidence="0.947856">
p(Q|X) _
</equation>
<page confidence="0.989375">
726
</page>
<bodyText confidence="0.999856909090909">
ative ambiguity between multiple phones hypothe-
sized for a given frame.
The second ‘local’ representation that we experi-
mented with incorporates the CRF transition feature
functions as follows. For each utterance X we per-
form a Viterbi decoding of the most likely state se-
quence Qbest = argmaxQ{p(Q|X)} hypothesized
for the utterance X. We then augmented the state
feature representation with the sum of the transition
features corresponding to the phone label hypothe-
sized for the previous frame (qbest
</bodyText>
<equation confidence="0.8395274">
���) to obtain a vector
ftrans(X, t) of length |Q|,
ftrans(X, t) = L ajsj(q, X) + µjfj(qbestt��, q, X)
j j
(5)
</equation>
<bodyText confidence="0.999959083333333">
As a final note, following (Morris and Fosler-
Lussier, 2009), our CRF systems are trained using
the linear outputs of MLPs as its state feature func-
tions and transition biases as the transition feature
functions. Hence, fstate is a linear transformation of
the MLP linear outputs down to |Q |dimensions.1
Both fstate and ftrans can thus be viewed as an im-
plicit mapping performed by the CRF of the in-
put feature function dimensions down to |Q |dimen-
sions. Note that the CRF implicitly uses informa-
tion concerning the underlying phone labels unlike
dimensionality reduction using KL-transform.
</bodyText>
<sectionHeader confidence="0.999024" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9999376">
To evaluate our proposed techniques, we carried
out word recognition experiments on the speaker-
independent portion of the Wall Street Journal 5K
closed vocabulary task (WSJ0). Since the corpus is
not phonetically transcribed, we first trained a stan-
dard HMM recognition system using PLP features
and produced phonetic transcriptions by force align-
ing the training data. These were used to train an
MLP phone classifier with a softmax output layer,
using a 9-frame window of PLPs with 4000 hidden
layer units to predict one of the 41 phone labels (in-
cluding silence and short pause). The linear outputs
of the MLP were used to train a baseline Tandem
system. We then trained a CRF using the MLP lin-
ear outputs as its state feature functions. We extract
</bodyText>
<footnote confidence="0.997825">
1We note that our system uses an additional state bias feature
that has a fixed value of 1. However, since this is a constant
term, it has no role to play in the derived representation.
</footnote>
<table confidence="0.999359714285714">
System Accuracy (%)
Crandem-baseline 89.4%
Tandem-baseline 91.8%
Crandem-NormMax 91.4%
Crandem-Norm5 92.1%
Crandem-state 91.7%
Crandem-trans 91.0%
</table>
<tableCaption confidence="0.999723">
Table 1: Word recognition results on the WSJ0 task
</tableCaption>
<bodyText confidence="0.999478888888889">
local posteriors as well as the two ‘local’ representa-
tions described in Section 3. These input represen-
tations were then normalized at the utterance level,
before applying a KL-transformation to decorrelate
them and reduce dimensionality to 39 dimensions.
Finally, each of these representations was used to
train a HMM system with intra-word triphones and
16 Gaussians per mixture using the Hidden Markov
Model Toolkit (Young et al., 2002).
</bodyText>
<sectionHeader confidence="0.999813" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999745884615385">
Results for each of the experiments described in
Section 4 are reported in Table 1 on the 330-
sentence standard 5K non-verbalized test set. The
Crandem-baseline represents the system of (Mor-
ris and Fosler-Lussier, 2009). Normalizing the
CRF weights of the system by either the weight
with largest absolute value (CRF-NormMax) or by
5 (tuned on the development set) leads to signif-
icant improvements (p ≤ 0.005) over the Cran-
dem baseline. Similarly, using either the state fea-
ture sum (Crandem-state) or the representation aug-
mented with the transition features (Crandem-trans)
leads to significant improvements (p ≤ 0.005) over
the Crandem baseline. Note that the performance of
these systems is comparable to the Tandem baseline.
To further analyze the results obtained using the
state feature sum representations and the Tandem
baseline, we compute the mean distance for each
phone HMM from every other phone HMM ob-
tained at the end of the GMM-HMM training phase.
The distance between two HMMs is computed as a
uniformly weighted sum of the average distances be-
tween the GMMs of a one-to-one alignment of states
corresponding to the two HMMs. GMM distances
are computed using a 0.5-weighted sum of inter-
dispersions normalized by self-dispersions (Wang et
</bodyText>
<sectionHeader confidence="0.433786" genericHeader="evaluation">
IT
</sectionHeader>
<page confidence="0.892997">
727
</page>
<figureCaption confidence="0.9642005">
Figure 1: Normalized mean distances for each of the phone models from every other phone model trained using the
Tandem MLP baseline and the state feature sum representation.
</figureCaption>
<bodyText confidence="0.994334272727273">
al., 2004). Distances between monomodal Gaus-
sian distributions were computed using the Bhat-
tacharyya distance measure. The phone HMM dis-
tances are normalized using the maximum phone
distance for each system. As can be seen in Figure
1, the mean distances obtained from the state feature
sum representation are consistently greater than the
corresponding distances in the Tandem-MLP sys-
tem, indicating larger separability of the phones in
the feature space. Similar trends were seen with the
transition feature sum representation.
</bodyText>
<sectionHeader confidence="0.998732" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.982833277777778">
In this paper, we report significant improvements
over the Crandem baseline. The weight normaliza-
tion experiments confirmed the hypothesis that in-
creasing the entropy of the CRF posteriors leads to
better word-level recognition. Our experiments with
directly extracting frame-level representations from
the CRF reinforce this conclusion. Although our ex-
periments with the systems using the state feature
sum and transition feature augmented representation
did not lead to improvements over the Tandem base-
line, the increased separability of the phone models
trained using these representations is encouraging.
In the future, we intend to examine techniques by
which these representations could be used to further
improve word recognition results.
Acknowledgement: The authors gratefully ac-
knowledge support by NSF grants IIS-0643901 and
IIS-0905420 for this work.
</bodyText>
<sectionHeader confidence="0.998648" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999456875">
A. Gunawardana, M. Mahajan, A. Acero, and J. Platt.
2005. Hidden conditional random fields for phone
classification. Interspeech.
H. Hermansky, D. Ellis, and S. Sharma. 2000. Tan-
dem connectionist feature stream extraction for con-
ventional hmm systems. ICASSP.
Y. Hifny and S. Renals. 2009. Speech recognition using
augmented conditional random fields. IEEE Trans-
actions on Audio, Speech, and Language Processing,
17(2):354–365.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. ICML.
J. Morris and E. Fosler-Lussier. 2008. Conditional ran-
dom fields for integrating local discriminative classi-
fiers. IEEE Transactions on Acoustics, Speech, and
Language Processing, 16(3):617–628.
J. Morris and E. Fosler-Lussier. 2009. Crandem: Con-
ditional random fields for word recognition. Inter-
speech.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. NAACL.
Xu Wang, Peng Xuan, and Wang Bingxi. 2004. A gmm-
based telephone channel classification for mandarin
speech recognition. ICSP.
S. Young, G. Evermann, T. Hain, D. Kershaw, G. Moore,
J. Odell, D. Ollason, D. Povey, V. Valtchev, and
P. Woodland. 2002. The HTK Book. Cambridge Uni-
versity Press.
G. Zweig and P. Nguyen. 2009. A segmental crf ap-
proach to large vocabulary continuous speech recogni-
tion. ASRU.
</reference>
<page confidence="0.996829">
728
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.820816">
<title confidence="0.9992">Investigations into the Crandem Approach to Word Recognition</title>
<author confidence="0.986958">Rohit Prabhavalkar</author>
<author confidence="0.986958">Preethi Jyothi</author>
<author confidence="0.986958">William Hartmann</author>
<author confidence="0.986958">Jeremy Morris</author>
<author confidence="0.986958">Eric</author>
<affiliation confidence="0.9642735">Department of Computer Science and The Ohio State University, Columbus,</affiliation>
<abstract confidence="0.992668647058824">We suggest improvements to a previously proposed framework for integrating Conditional Random Fields and Hidden Markov Models, dubbed a Crandem system (2009). The previous authors’ work suggested that local label posteriors derived from the CRF were too low-entropy for use in word-level automatic speech recognition. As an alternative to the log posterior representation used in their system, we explore frame-level representations derived from the CRF feature functions. We also describe a weight normalization transformation that leads to increased entropy of the CRF posteriors. We report significant gains over the previous Crandem system on the Wall Street Journal word recognition task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Gunawardana</author>
<author>M Mahajan</author>
<author>A Acero</author>
<author>J Platt</author>
</authors>
<title>Hidden conditional random fields for phone classification.</title>
<date>2005</date>
<journal>Interspeech.</journal>
<contexts>
<context position="1555" citStr="Gunawardana et al., 2005" startWordPosition="219" endWordPosition="222"> previous Crandem system on the Wall Street Journal word recognition task. 1 Introduction Conditional Random Fields (CRFs) (Lafferty et al., 2001) have recently emerged as a promising new paradigm in the domain of Automatic Speech Recognition (ASR). Unlike Hidden Markov Models (HMMs), CRFs are direct discriminative models: they predict the probability of a label sequence conditioned on the input. As a result, CRFs can capture long-range dependencies in the data and avoid the need for restrictive independence assumptions. Variants of CRFs have been successfully used in phone recognition tasks (Gunawardana et al., 2005; Morris and Fosler-Lussier, 2008; Hifny and Renals, 2009). While the improvements in the phone recognition task are encouraging, recent efforts have been directed towards extending the CRF paradigm to the word recognition level (Zweig and Nguyen, 2009; Morris and Fosler-Lussier, 2009). The Crandem system (Morris and Fosler-Lussier, 2009) is one of the promising approaches in this regard. The Crandem system is directly inspired by the techniques of the Tandem system (Hermansky et al., 2000), where phone-label posterior estimates produced by a Multi-Layer Perceptron (MLP) are transformed into a</context>
</contexts>
<marker>Gunawardana, Mahajan, Acero, Platt, 2005</marker>
<rawString>A. Gunawardana, M. Mahajan, A. Acero, and J. Platt. 2005. Hidden conditional random fields for phone classification. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hermansky</author>
<author>D Ellis</author>
<author>S Sharma</author>
</authors>
<title>Tandem connectionist feature stream extraction for conventional hmm systems.</title>
<date>2000</date>
<publisher>ICASSP.</publisher>
<contexts>
<context position="2050" citStr="Hermansky et al., 2000" startWordPosition="295" endWordPosition="298">tive independence assumptions. Variants of CRFs have been successfully used in phone recognition tasks (Gunawardana et al., 2005; Morris and Fosler-Lussier, 2008; Hifny and Renals, 2009). While the improvements in the phone recognition task are encouraging, recent efforts have been directed towards extending the CRF paradigm to the word recognition level (Zweig and Nguyen, 2009; Morris and Fosler-Lussier, 2009). The Crandem system (Morris and Fosler-Lussier, 2009) is one of the promising approaches in this regard. The Crandem system is directly inspired by the techniques of the Tandem system (Hermansky et al., 2000), where phone-label posterior estimates produced by a Multi-Layer Perceptron (MLP) are transformed into a suitable acoustic representation for a standard HMM. In both systems, the frame-based log posterior vector of P (phone|acoustics) overall phones is decorrelated using the Karhunen-Loeve (KL) transform; unlike MLPs, CRFs take into account the entire label sequence when computing local posteriors. However, posterior estimates from the CRF tend to be overconfident compared to MLP posteriors (Morris and Fosler-Lussier, 2009). In this paper, we analyze the interplay between the various steps in</context>
<context position="4636" citStr="Hermansky et al., 2000" startWordPosition="703" endWordPosition="706">the entire parameter space, and can be maximized using standard convex optimization techniques (Lafferty et al., 2001; Sha and Pereira, 2003). The local posterior probability of a particular label can be computed via a forwardbackward style algorithm. Mathematically, αt(q|X)βt(q|X) p(qt = q|X) = (2) Z(X) where αt(q|X) and βt(q|X) accumulate contributions associated with possible assignments of labels before and after the current time-step t. The Crandem system utilizes these local posterior values from the CRF analogously to the way in which MLP-posteriors are treated in the Tandem framework (Hermansky et al., 2000), by applying a log transformation to the posteriors. These transformed outputs are then decorrelated using a KL-transform and then dimensionality-reduced to be used as a replacement for MFCCs in a HMM system. While the MLP is usually reduced to 39 dimensions, the standard CRF benefits from a higher dimensionality reduction (to 19 dimensions). The decorrelated outputs are then used as an input representation for a conventional HMM system. 3 Improving Crandem Recognition Results Morris and Fosler-Lussier (2009) indicate that the local posterior outputs from the CRF model produces features that </context>
</contexts>
<marker>Hermansky, Ellis, Sharma, 2000</marker>
<rawString>H. Hermansky, D. Ellis, and S. Sharma. 2000. Tandem connectionist feature stream extraction for conventional hmm systems. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Hifny</author>
<author>S Renals</author>
</authors>
<title>Speech recognition using augmented conditional random fields.</title>
<date>2009</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>17</volume>
<issue>2</issue>
<contexts>
<context position="1613" citStr="Hifny and Renals, 2009" startWordPosition="227" endWordPosition="230">cognition task. 1 Introduction Conditional Random Fields (CRFs) (Lafferty et al., 2001) have recently emerged as a promising new paradigm in the domain of Automatic Speech Recognition (ASR). Unlike Hidden Markov Models (HMMs), CRFs are direct discriminative models: they predict the probability of a label sequence conditioned on the input. As a result, CRFs can capture long-range dependencies in the data and avoid the need for restrictive independence assumptions. Variants of CRFs have been successfully used in phone recognition tasks (Gunawardana et al., 2005; Morris and Fosler-Lussier, 2008; Hifny and Renals, 2009). While the improvements in the phone recognition task are encouraging, recent efforts have been directed towards extending the CRF paradigm to the word recognition level (Zweig and Nguyen, 2009; Morris and Fosler-Lussier, 2009). The Crandem system (Morris and Fosler-Lussier, 2009) is one of the promising approaches in this regard. The Crandem system is directly inspired by the techniques of the Tandem system (Hermansky et al., 2000), where phone-label posterior estimates produced by a Multi-Layer Perceptron (MLP) are transformed into a suitable acoustic representation for a standard HMM. In b</context>
</contexts>
<marker>Hifny, Renals, 2009</marker>
<rawString>Y. Hifny and S. Renals. 2009. Speech recognition using augmented conditional random fields. IEEE Transactions on Audio, Speech, and Language Processing, 17(2):354–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<publisher>ICML.</publisher>
<contexts>
<context position="1077" citStr="Lafferty et al., 2001" startWordPosition="144" endWordPosition="147"> system (2009). The previous authors’ work suggested that local label posteriors derived from the CRF were too low-entropy for use in word-level automatic speech recognition. As an alternative to the log posterior representation used in their system, we explore frame-level representations derived from the CRF feature functions. We also describe a weight normalization transformation that leads to increased entropy of the CRF posteriors. We report significant gains over the previous Crandem system on the Wall Street Journal word recognition task. 1 Introduction Conditional Random Fields (CRFs) (Lafferty et al., 2001) have recently emerged as a promising new paradigm in the domain of Automatic Speech Recognition (ASR). Unlike Hidden Markov Models (HMMs), CRFs are direct discriminative models: they predict the probability of a label sequence conditioned on the input. As a result, CRFs can capture long-range dependencies in the data and avoid the need for restrictive independence assumptions. Variants of CRFs have been successfully used in phone recognition tasks (Gunawardana et al., 2005; Morris and Fosler-Lussier, 2008; Hifny and Renals, 2009). While the improvements in the phone recognition task are encou</context>
<context position="3251" citStr="Lafferty et al., 2001" startWordPosition="480" endWordPosition="483"> the various steps involved in the Crandem process. Is the local posterior representation from the CRF the best representation? Given that the CRF posterior estimates can be overconfident, what transformations to the posteriors are appropriate? In Section 2 we briefly describe CRFs and the Crandem framework. We suggest techniques for improving Crandem word recognition performance in Section 3. Details of experiments and our results are discussed in Sections 4 and 5 respectively. We conclude with a discussion of future work in Section 6. 2 CRFs and the Crandem System Conditional random fields (Lafferty et al., 2001) express the probability of a label sequence Q conditioned on the input data X as a log-linear sum of 725 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 725–728, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics weighted feature functions, exp � �j Ajsj(qt, X) + �j µjfj(qt−1, qt, X) t Z(X) (1) where sj(�) and fj(�) are known as state feature functions and transition feature functions respectively, and λj and µj are the associated weights. Z(X) is a normalization term that ensures a valid probability dis</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>E Fosler-Lussier</author>
</authors>
<title>Conditional random fields for integrating local discriminative classifiers.</title>
<date>2008</date>
<journal>IEEE Transactions on Acoustics, Speech, and Language Processing,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="1588" citStr="Morris and Fosler-Lussier, 2008" startWordPosition="223" endWordPosition="226">n the Wall Street Journal word recognition task. 1 Introduction Conditional Random Fields (CRFs) (Lafferty et al., 2001) have recently emerged as a promising new paradigm in the domain of Automatic Speech Recognition (ASR). Unlike Hidden Markov Models (HMMs), CRFs are direct discriminative models: they predict the probability of a label sequence conditioned on the input. As a result, CRFs can capture long-range dependencies in the data and avoid the need for restrictive independence assumptions. Variants of CRFs have been successfully used in phone recognition tasks (Gunawardana et al., 2005; Morris and Fosler-Lussier, 2008; Hifny and Renals, 2009). While the improvements in the phone recognition task are encouraging, recent efforts have been directed towards extending the CRF paradigm to the word recognition level (Zweig and Nguyen, 2009; Morris and Fosler-Lussier, 2009). The Crandem system (Morris and Fosler-Lussier, 2009) is one of the promising approaches in this regard. The Crandem system is directly inspired by the techniques of the Tandem system (Hermansky et al., 2000), where phone-label posterior estimates produced by a Multi-Layer Perceptron (MLP) are transformed into a suitable acoustic representation</context>
</contexts>
<marker>Morris, Fosler-Lussier, 2008</marker>
<rawString>J. Morris and E. Fosler-Lussier. 2008. Conditional random fields for integrating local discriminative classifiers. IEEE Transactions on Acoustics, Speech, and Language Processing, 16(3):617–628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>E Fosler-Lussier</author>
</authors>
<title>Crandem: Conditional random fields for word recognition.</title>
<date>2009</date>
<journal>Interspeech.</journal>
<contexts>
<context position="1841" citStr="Morris and Fosler-Lussier, 2009" startWordPosition="261" endWordPosition="264">(HMMs), CRFs are direct discriminative models: they predict the probability of a label sequence conditioned on the input. As a result, CRFs can capture long-range dependencies in the data and avoid the need for restrictive independence assumptions. Variants of CRFs have been successfully used in phone recognition tasks (Gunawardana et al., 2005; Morris and Fosler-Lussier, 2008; Hifny and Renals, 2009). While the improvements in the phone recognition task are encouraging, recent efforts have been directed towards extending the CRF paradigm to the word recognition level (Zweig and Nguyen, 2009; Morris and Fosler-Lussier, 2009). The Crandem system (Morris and Fosler-Lussier, 2009) is one of the promising approaches in this regard. The Crandem system is directly inspired by the techniques of the Tandem system (Hermansky et al., 2000), where phone-label posterior estimates produced by a Multi-Layer Perceptron (MLP) are transformed into a suitable acoustic representation for a standard HMM. In both systems, the frame-based log posterior vector of P (phone|acoustics) overall phones is decorrelated using the Karhunen-Loeve (KL) transform; unlike MLPs, CRFs take into account the entire label sequence when computing local </context>
<context position="5151" citStr="Morris and Fosler-Lussier (2009)" startWordPosition="782" endWordPosition="785">m the CRF analogously to the way in which MLP-posteriors are treated in the Tandem framework (Hermansky et al., 2000), by applying a log transformation to the posteriors. These transformed outputs are then decorrelated using a KL-transform and then dimensionality-reduced to be used as a replacement for MFCCs in a HMM system. While the MLP is usually reduced to 39 dimensions, the standard CRF benefits from a higher dimensionality reduction (to 19 dimensions). The decorrelated outputs are then used as an input representation for a conventional HMM system. 3 Improving Crandem Recognition Results Morris and Fosler-Lussier (2009) indicate that the local posterior outputs from the CRF model produces features that are more heavily skewed to the dominant phone class than the MLP system, leading to an increase in word recognition errors. In order to correct for this, we perform a non-linear transformation on the local CRF posterior representation before applying a KL-transform and subsequent stages. Specifically, we normalize all of the weights λj and µj in Equation 1 by a fixed positive constant n to obtain normalized weights λ0j and µ0j. We note that the probability of a label sequence computed using the transformed wei</context>
<context position="10587" citStr="Morris and Fosler-Lussier, 2009" startWordPosition="1684" endWordPosition="1688">al’ representations described in Section 3. These input representations were then normalized at the utterance level, before applying a KL-transformation to decorrelate them and reduce dimensionality to 39 dimensions. Finally, each of these representations was used to train a HMM system with intra-word triphones and 16 Gaussians per mixture using the Hidden Markov Model Toolkit (Young et al., 2002). 5 Results Results for each of the experiments described in Section 4 are reported in Table 1 on the 330- sentence standard 5K non-verbalized test set. The Crandem-baseline represents the system of (Morris and Fosler-Lussier, 2009). Normalizing the CRF weights of the system by either the weight with largest absolute value (CRF-NormMax) or by 5 (tuned on the development set) leads to significant improvements (p ≤ 0.005) over the Crandem baseline. Similarly, using either the state feature sum (Crandem-state) or the representation augmented with the transition features (Crandem-trans) leads to significant improvements (p ≤ 0.005) over the Crandem baseline. Note that the performance of these systems is comparable to the Tandem baseline. To further analyze the results obtained using the state feature sum representations and </context>
</contexts>
<marker>Morris, Fosler-Lussier, 2009</marker>
<rawString>J. Morris and E. Fosler-Lussier. 2009. Crandem: Conditional random fields for word recognition. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<publisher>NAACL.</publisher>
<contexts>
<context position="4154" citStr="Sha and Pereira, 2003" startWordPosition="627" endWordPosition="630">putational Linguistics weighted feature functions, exp � �j Ajsj(qt, X) + �j µjfj(qt−1, qt, X) t Z(X) (1) where sj(�) and fj(�) are known as state feature functions and transition feature functions respectively, and λj and µj are the associated weights. Z(X) is a normalization term that ensures a valid probability distribution. Given a set of labeled examples, the CRF is trained to maximize the conditional log-likelihood of the training set. The log-likelihood is concave over the entire parameter space, and can be maximized using standard convex optimization techniques (Lafferty et al., 2001; Sha and Pereira, 2003). The local posterior probability of a particular label can be computed via a forwardbackward style algorithm. Mathematically, αt(q|X)βt(q|X) p(qt = q|X) = (2) Z(X) where αt(q|X) and βt(q|X) accumulate contributions associated with possible assignments of labels before and after the current time-step t. The Crandem system utilizes these local posterior values from the CRF analogously to the way in which MLP-posteriors are treated in the Tandem framework (Hermansky et al., 2000), by applying a log transformation to the posteriors. These transformed outputs are then decorrelated using a KL-trans</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Wang</author>
<author>Peng Xuan</author>
<author>Wang Bingxi</author>
</authors>
<title>A gmmbased telephone channel classification for mandarin speech recognition.</title>
<date>2004</date>
<publisher>ICSP.</publisher>
<marker>Wang, Xuan, Bingxi, 2004</marker>
<rawString>Xu Wang, Peng Xuan, and Wang Bingxi. 2004. A gmmbased telephone channel classification for mandarin speech recognition. ICSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>G Evermann</author>
<author>T Hain</author>
<author>D Kershaw</author>
<author>G Moore</author>
<author>J Odell</author>
<author>D Ollason</author>
<author>D Povey</author>
<author>V Valtchev</author>
<author>P Woodland</author>
</authors>
<title>The HTK Book.</title>
<date>2002</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="10355" citStr="Young et al., 2002" startWordPosition="1648" endWordPosition="1651"> Crandem-baseline 89.4% Tandem-baseline 91.8% Crandem-NormMax 91.4% Crandem-Norm5 92.1% Crandem-state 91.7% Crandem-trans 91.0% Table 1: Word recognition results on the WSJ0 task local posteriors as well as the two ‘local’ representations described in Section 3. These input representations were then normalized at the utterance level, before applying a KL-transformation to decorrelate them and reduce dimensionality to 39 dimensions. Finally, each of these representations was used to train a HMM system with intra-word triphones and 16 Gaussians per mixture using the Hidden Markov Model Toolkit (Young et al., 2002). 5 Results Results for each of the experiments described in Section 4 are reported in Table 1 on the 330- sentence standard 5K non-verbalized test set. The Crandem-baseline represents the system of (Morris and Fosler-Lussier, 2009). Normalizing the CRF weights of the system by either the weight with largest absolute value (CRF-NormMax) or by 5 (tuned on the development set) leads to significant improvements (p ≤ 0.005) over the Crandem baseline. Similarly, using either the state feature sum (Crandem-state) or the representation augmented with the transition features (Crandem-trans) leads to s</context>
</contexts>
<marker>Young, Evermann, Hain, Kershaw, Moore, Odell, Ollason, Povey, Valtchev, Woodland, 2002</marker>
<rawString>S. Young, G. Evermann, T. Hain, D. Kershaw, G. Moore, J. Odell, D. Ollason, D. Povey, V. Valtchev, and P. Woodland. 2002. The HTK Book. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zweig</author>
<author>P Nguyen</author>
</authors>
<title>A segmental crf approach to large vocabulary continuous speech recognition.</title>
<date>2009</date>
<publisher>ASRU.</publisher>
<contexts>
<context position="1807" citStr="Zweig and Nguyen, 2009" startWordPosition="257" endWordPosition="260">ke Hidden Markov Models (HMMs), CRFs are direct discriminative models: they predict the probability of a label sequence conditioned on the input. As a result, CRFs can capture long-range dependencies in the data and avoid the need for restrictive independence assumptions. Variants of CRFs have been successfully used in phone recognition tasks (Gunawardana et al., 2005; Morris and Fosler-Lussier, 2008; Hifny and Renals, 2009). While the improvements in the phone recognition task are encouraging, recent efforts have been directed towards extending the CRF paradigm to the word recognition level (Zweig and Nguyen, 2009; Morris and Fosler-Lussier, 2009). The Crandem system (Morris and Fosler-Lussier, 2009) is one of the promising approaches in this regard. The Crandem system is directly inspired by the techniques of the Tandem system (Hermansky et al., 2000), where phone-label posterior estimates produced by a Multi-Layer Perceptron (MLP) are transformed into a suitable acoustic representation for a standard HMM. In both systems, the frame-based log posterior vector of P (phone|acoustics) overall phones is decorrelated using the Karhunen-Loeve (KL) transform; unlike MLPs, CRFs take into account the entire la</context>
</contexts>
<marker>Zweig, Nguyen, 2009</marker>
<rawString>G. Zweig and P. Nguyen. 2009. A segmental crf approach to large vocabulary continuous speech recognition. ASRU.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>