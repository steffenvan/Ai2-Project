<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005604">
<title confidence="0.9991165">
Incremental Generation by Incremental Parsing:
Tactical Generation in Dynamic Syntax
</title>
<author confidence="0.996295">
Matthew Purver
</author>
<affiliation confidence="0.9221705">
Department of Computer Science
King&apos;s College London
</affiliation>
<address confidence="0.923268">
Strand, London WC2R 2LS, UK
</address>
<email confidence="0.998404">
matthew.purver@kcl.ac.uk
</email>
<author confidence="0.972933">
Masayuki Otsuka
</author>
<affiliation confidence="0.906167">
Department of Philosophy
King&apos;s College London
</affiliation>
<address confidence="0.912399">
Strand, London WC2R 2LS, UK
</address>
<email confidence="0.997903">
masayuki.otsuka@kcl.ac.uk
</email>
<sectionHeader confidence="0.997377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999785">
The paper shows how an incremental
tactical generator can be constructed
based on the incremental parsing frame-
work described in Dynamic Syntax
(DS)(Kempson et al., 2001), without
adding a generator-specific vocabulary
or intermediate levels of representation.
The resulting generator is defined purely
in terms of the parsing process, together
with a notion of tree subsumption. This
is shown to have various advantages in-
cluding easy self-monitoring and psy-
cholinguistic plausibility. A simple Pro-
log implementation is described, to-
gether with various possible improve-
ments in efficiency.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999601714285714">
In this paper we give a description of a proto-
type tactical generator based on the DS approach.
DS defines a formalism that allows the articula-
tion of natural-language grammars that reflect left-
to-right processing: natural-language strings are
paired with decorated semantic trees by a pro-
cess of monotonic growth over sequences of par-
tial trees associated with the processing of each
word. As such it incorporates a prototype parser
as its central subpart. The generation process de-
scribed here performs the reverse operation, pro-
ducing possible output strings from a defined se-
mantic tree. The generation method is defined
entirely in terms of parsing, providing a tight
parsing-generation correspondence allowing for
self-monitoring without a separate parsing pro-
cess, and a simple reflection of certain facts about
natural language use in dialogue. We also describe
a Prolog implementation of a DS system which in-
corporates both parser and generator, and discuss
various possible improvements in efficiency.
Firstly we give some background on the DS for-
malism in section 2. We then describe our ap-
proach to generation in section 3 and the imple-
mentation in section 4. We then explain the com-
putational and psycholinguistic advantages in sec-
tion 5, and draw conclusions and outline further
work in section 6.
</bodyText>
<sectionHeader confidence="0.995039" genericHeader="method">
2 Dynamic Syntax
</sectionHeader>
<bodyText confidence="0.999940041666667">
DS is a parsing-directed grammar formalism
which claims that parsing is the central mechanism
of language processing. DS defines parsing as a
process of establishing mappings from an initial
tree to complete trees using general computational
rules (roughly corresponding to syntactic rules)
and specific lexical actions projected by lexical
items in the input string. The resulting tree struc-
tures represent semantic interpretations for a given
input string, and are described by the language DU
based on a modal tree logic LOFT (Blackburn and
Meyer-Viol, 1994).
Trees are either complete or partial. Partial
trees have: (a) node addresses that are partially
specified by underspecified modalities; or (b) node
decorations that are underspecified by the use of
requirements and meta-variables. Such partial
specifications are always introduced with imposed
requirements, which jointly constitute a set of con-
straints on possible tree growth. Trees are com-
plete AT all such requirements are satisfied, leaving
no remaining partiality or underspecification.
For example, in &amp;quot;left-dislocation&amp;quot;, for which an
initial constituent node is introduced into the tree
</bodyText>
<page confidence="0.997212">
79
</page>
<bodyText confidence="0.99938603125">
related to the root node using an underspecified
modality, this &amp;quot;unfixed&amp;quot; node bears the require-
ment that it has to be updated to a fully specified
relation; and this requirement is met by later merg-
ing of this node with some local node which is in
a fully specified relation to the root node. Simi-
larly, if underspecified decorations are projected,
they must be updated to fully specified values in
the subsequent states (e.g. a pronoun projects a
meta-variable which has to be substituted with a
proper term).
The parsing model is defined to reflect the idea
that parsing is a process of progressive and mono-
tonic extensions of tree structures given the se-
quence of words as input. It is goal-directed, in
the sense that all requirements must be satisfied
for wellformedness (grammaticality).
Technically, a set of parse paths is a partially or-
dered set of possible trees where the initial tree is
the lowest bound, and the goal trees are the high-
est bounds. The partial ordering &lt;AcT on the set
indicates a monotonic extension relation on pairs
of trees, bearing a label of the name of the compu-
tational/lexical action performed in the transition.
The search for a parse path can be viewed as
the search for a successful composition of actions
which yields complete trees. The set of sequences
of permissible actions is obtained by interweav-
ing the linearly ordered set of lexical actions pro-
jectable by the input string into a finite number of
partially ordered Kleene*-iterated computational
actions, e.g. ACT(John sneezed) = (C* &lt;
john &lt; C* &lt; sneezed &lt; C*).
Generally, the initial tree has only one node
bearing the requirement to establish a formula of
type t, represented as ?Ty(t). The input string
is scanned from left to right, with each lexical
item projecting a lexical action to update the trees.
Computational rules are transition functions on
trees, which are conditional in the sense that if the
input tree satisfies the condition of some compu-
tational rule, it yields the output tree updating the
input tree. The parse is successful when the parser
produces at least one complete tree having the root
node decorated with a formula of type t, after us-
ing computational rules and all the lexical actions
from the input string. In this case, the input string
is called grammatical and the complete trees repre-
sent interpretations for the string. In other words,
in DS, grammaticality of string is defined in terms
of pars ability.
DS specifies the theoretical parser, but, being a
grammar formalism, it has no specification as to
how a parser performs, i.e. no algorithm is de-
fined giving the concrete parser. A parse state can
be thought of as a pair of a string and a set of pos-
sible trees (semantic interpretations for the string).
Without any parsing strategy, we may define the
parser to generate all the possible trees using the
computational rules (C-possible trees) for the in-
put parser state, then scan the input string to ap-
ply the lexical action to all of the trees to update
the parser state. This routine is repeated until the
parser finishes scanning the input string.
</bodyText>
<sectionHeader confidence="0.997879" genericHeader="method">
3 Generation
</sectionHeader>
<bodyText confidence="0.999556">
As DS identifies grammaticality with parsability,
rather than using concepts such as syntactic con-
stituents or heads, standard approaches to genera-
tion such as head-driven methods (Shieber et al.,
1990) cannot be applied. Instead, generation must
be defined in terms of parsing.
</bodyText>
<subsectionHeader confidence="0.999905">
3.1 Connecting Parsing and Generation
</subsectionHeader>
<bodyText confidence="0.999124428571429">
Despite informal observations made in psycholin-
guistics that production and comprehension sys-
tems have much in common (Garrett, 1982; Fra-
zier, 1982), parsing and generation have gener-
ally been treated as quite separate research enter-
prises, both in the psycholinguistic and computa-
tional linguistic communities.
Nevertheless, following Shieber (1988), at-
tempts have been made to define the two in
terms of a broadly common architecture and us-
ing shared reversible grammars (see e.g. (Er-
bach, 1991; Neumann, 1994; Gardent and Thater,
2001)).
Although the contribution made by the present
system is modest in only addressing the level of
tactical generation, it nevertheless contributes to
this co-articulation of parsing and generation by
defining a generation system which purports to re-
flect the process of left-to-right incremental pro-
duction, by using incremental parsing as the basic
building block.
</bodyText>
<page confidence="0.947223">
80
</page>
<figure confidence="0.985523555555555">
C*
Parse (C*) : SUCCESS
{?Ty(t)}
{?Ty(e), {?Ty(e t)}
Subsumption : SUCCESS
Generate
snores
Parse (john) : SUCCESS
{?Ty(t)}
{Ty(e), Fo(john)} {?Ty(e t),
Subsumption : SUCCESS
Generate : (john)
john
Parse (snores)
Subsumption
Generate
/nary
snores
</figure>
<equation confidence="0.921851333333333">
Parse (snores) : SUCCESS
{?Ty(t), 0}
{Ty(e), Fo(john)} {Ty(e t), Fo(snore)}
Parse (mary) : SUCCESS
{?Ty(t)}
{Ty(e), Fo(mary)} {?Ty(e t), G}
Subsumption : FAIL
Generate : 0
Subsumption : SUCCESS
Generate : (john, snores)
Parse (C*) : SUCCESS
{Ty(t), Fo(snore(john)),G}
GOAL TREE
{Ty(t), Fo(snore(john)), 0}
{Ty(e), Fo(john)} {Ty(e —&gt; t), Fo(snore)}
{Ty(e), Fo(john)} {Ty(e t), Fo(snore)}
Subsumption : SUCCESS
Generate : (john, snores)
</equation>
<figureCaption confidence="0.997827">
Figure 1: Generating &amp;quot;john snores&amp;quot;
</figureCaption>
<page confidence="0.987288">
81
</page>
<subsectionHeader confidence="0.99895">
3.2 Generation as Parsing
</subsectionHeader>
<bodyText confidence="0.99993458490566">
Generation can be achieved using very little be-
yond the standard DS notion of parsability. We
assume a fully specified goal tree as inputl : given
this, the generator incrementally produces a set of
corresponding strings by following standard pars-
ing routines and using the goal tree as a check. In
other words, for a naive generator, all that is re-
quired is a notion of tree subsumption: this allows
any candidate partial tree produced in the genera-
tion process to be checked against the goal tree to
determine whether it is a sensible candidate (by
checking for tree mergeability without inconsis-
tency in node relations and decorations).
At any point in the parsing process, a DS parser
state can be seen as a pair of a partial string (the
input so far) and a set of associated partial trees.
With generation, we view the generator state as a
set of these parser states (a set of pairs of &apos;possibly
acceptable&apos; partial strings and their associated sets
of partial trees).
At each stage of generation, each pair is ex-
tended by tentatively extending the partial string
by adding any word from the lexicon. The asso-
ciated set of possible (partial) trees is produced
using the standard parser — it may of course be
empty if the word under consideration cannot be
grammatically added to the partial string — and
only those which are subsumed by the goal tree
are kept. Any strings associated with empty tree
sets are then rejected.
Generation is complete when the process can be
continued no further (any string extension results
in an empty parser state), and the set of output
strings is taken as those for which the associated
tree set contains a member identical to the goal
tree — see figure 1.
Lexical selection is implicit: any word in the
lexicon which is not associated with a node (or for-
11n assuming a source tree, we are currently ignoring
some issues that might be required in a dialogue system, such
as concept generation, and translation from some flatmean-
ing representation into a decorated source tree (not a trivial
issue, as it is here that we will meet Shieber (1988)&apos;s logical
equivalence problem). We are therefore treating a genera-
tor as a module which supports that part of the generation
process called sugaring (Ranta, 1994) or linguistic realisation
(Reiter and Dale, 1997) — translation from an unambiguously
structured object in a meaning representation language to a
natural-language string.
mula decorating a node) in the goal tree will pro-
duce empty tree sets (as the trees produced will not
be subsumed by the goal tree), and therefore can-
not produce acceptable extended partial strings.2
</bodyText>
<sectionHeader confidence="0.998712" genericHeader="method">
4 Implementation
</sectionHeader>
<bodyText confidence="0.9998314">
The formal definition of DS species constraints
of possible forms and extensions of tree struc-
ture and node decorations in terms of axioms (in
LOFT) and algebraic definitions, and update ac-
tions (input/output relations of grammar rules) are
defined in a way that they respect the constraints.
This leaves room for implementation as to how the
parser performs. This section describes the parser
algorithm and possible strategies to improve effi-
ciency.
</bodyText>
<subsectionHeader confidence="0.915317">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.99995496">
A prototype DS system has been implemented in
Prolog3 The number of lexical entry types is cur-
rently small (i.e. the &amp;quot;grammar&amp;quot; is only small),
and limited to English, but some relatively com-
plex constructions such as left-dislocation and rel-
ative clauses can be processed.
The implementation remains close to the logi-
cal DS formalism. A tree node is represented as a
pair of a node name and a set4 of labels; a tree
is represented as a pair of a set of nodes and a
pointer (a node name). Labels can be requirements
?REQ, directed requirements ? ( [ D IR] , REQ) or
features +FEAT.
There are some representational differences:
mother-daughter relations are not expressed di-
rectly as labels but are implicit in the node naming
scheme (e.g. node 0 has daughters 00, 01); LINKs
are represented as 2-daughters; and unfixed nodes
are *-daughters.
Computational actions can now be defined in
terms of list manipulation. Prolog backtracking is
used to allow any number of actions to be applied
to any parser state (where possible).
Lexical actions are defined similarly and given
as templates for individual parts-of-speech and
</bodyText>
<footnote confidence="0.97825">
2This may not be the most efficient way of rejecting un-
suitable words — see section 4.2.3 below.
3The system can be accessed at
http://st228.dcs.kcl.ac.uk:8080/ds.
4We represent sets as Prolog lists: no use is made of list
order.
</footnote>
<page confidence="0.998203">
82
</page>
<bodyText confidence="0.9997719375">
verb subcategorisation frames. The templates can
then be interfaced to a standard computational lex-
icon (we are currently using one derived from the
OALD (Hornby, 1974)).
Parsing is now easily defined declaratively: the
initial parser state is one in which the only pos-
sible tree is a single root node with a ?Ty(t) re-
quirement, and none of the input string has been
consumed; a final parser state is one in which
a complete (all requirements discharged) tree is
available and the entire input string has been con-
sumed; and possible intermediate states are pro-
duced from other states by any number of com-
putational actions, plus lexical actions defined by
the consumption of the next word from the input
string.
</bodyText>
<subsectionHeader confidence="0.843947">
4.2 Efficiency
4.2.1 Parsing
</subsectionHeader>
<bodyText confidence="0.999949076923077">
As described above, the current parser is highly
unconstrained (it is defined declaratively and lib-
eral use is made of Prolog backtracking). As gen-
eration is performed by parsing, any efficiency in
parsing will be reflected in generation. Parsing ef-
ficiency can be improved by considering certain
computational actions5 as required whenever they
can be applied (thus reducing the number of pos-
sible partial trees and the need for backtracking).
Further improvements in parsing may be pos-
sible by using e.g. probabilistic methods to con-
strain search, but this is outside the scope of the
current work.
</bodyText>
<subsubsectionHeader confidence="0.600377">
4.2.2 Generation
</subsubsectionHeader>
<bodyText confidence="0.999993">
The naive generation process described in sec-
tion 3 is not as inefficient as it might initially ap-
pear. Statically speaking, the basic concept is to
generate all possible strings and check whether
parsing them gives a sensible tree — therefore one
might expect NW possible paths given a lexicon
of N words and typical string length W. However,
the incremental nature means that this is not the
case: unsuitable search paths are eliminated on a
stage-by-stage basis (by the parser for ungrammat-
ical paths, and by the subsumption check for paths
</bodyText>
<subsectionHeader confidence="0.46531">
5Currently thinning, elimination, and star-adjunction.
</subsectionHeader>
<bodyText confidence="0.999868833333333">
incompatible with the goal tree).6 The implemen-
tation of this naive version generates simple sen-
tences in a few seconds with a small test lexicon.
However, we briefly explore efficiency im-
provements by lexical selection and search
method.
</bodyText>
<subsectionHeader confidence="0.587436">
4.2.3 Lexical Selection
</subsectionHeader>
<bodyText confidence="0.999965181818182">
Lexical selection uses the decorations of the in-
put goal tree to produce a limited set of lexical
items which can be used by the generator, consist-
ing of a set L of words which correspond to logi-
cal formula decorations (e.g. N, V, Adj, Det) and a
set F of functional words (e.g. relativiser, comple-
mentiser). This reduces the search space consider-
ably, with the number of paths at most (L F) w
and in fact significantly less.
Members of L are chosen on the basis of goal
tree node search followed by lexical lookup: only
those words corresponding to Fo(X) semantic
formula decorations are admitted. Some members
of F can be chosen on the basis of tree features
(e.g. LINK structures for relativisers), although
any defined in the grammar to have truly vacu-
ous semantics must just be selected by default.
For pronouns, node features can also be used (to
admit only relevant gender &amp; number), and simi-
larly for verbs to control tense. This modification
has been implemented and results in significant
speed increase (although no formal corpus eval-
uation has yet been performed as our grammar is
still small, even more complex sentences includ-
ing relative clauses can now be generated in a few
seconds with a large 40,000 word lexicon to search
through).
Heuristics could be used to govern lexical
lookup, based on e.g. recency of words in the cur-
rent dialogue context (ensuring more recently used
words are used in preference), and prior probabil-
ities (ensuring more common words are used in
preference to rarer ones).
</bodyText>
<subsectionHeader confidence="0.551363">
4.2.4 Search Method
</subsectionHeader>
<bodyText confidence="0.993117666666667">
The incremental nature of the generation pro-
cess allows us to apply various search methods,
51n fact, in the best case there will be N x W paths,
although this only applies for an unambiguous lexicon and
grammar that gives a strict one-to-one correspondence be-
tween strings and trees.
</bodyText>
<page confidence="0.997525">
83
</page>
<bodyText confidence="0.9997558125">
and stop the process once any suitable string
has been found (rather than continuing until all
search paths are terminated). Using a depth-first
approach with this method will therefore cause
only one string to be produced; whereas using a
breadth-first approach will cause only the set of
all strings of the shortest possible length to be pro-
duced. Both approaches have been successfully
implemented.
This provides scope for probabilistic (or other
heuristic) methods to increase efficiency in the fu-
ture by constraining the search using techniques
well known in areas such as speech recognition
(e.g. beam search). However, a prerequisite for
this will be a theory of heuristically constrained
DS parsing.
</bodyText>
<sectionHeader confidence="0.962841" genericHeader="method">
5 Computational and Psycholinguistic
Features
</sectionHeader>
<bodyText confidence="0.999869315789474">
The view of generation as incremental parsing ap-
pears to have certain advantages from both com-
putational and psycholinguistic viewpoints. For
computational dialogue systems, it ensures that
self-monitoring information is available, and dis-
plays the property of incrementality. Psycholin-
guistically, as well as conforming to the general
principle that production and comprehension are
tightly coupled, it allows us to to explain cer-
tain phenomena characteristic of dialogue (where
standard models of generation cannot), and there-
fore promises to meet the challenge set out by
Pickering and Garrod (2003) that linguistic sys-
tems be evaluated by their success in reflecting
such phenomena. This includes alignment be-
tween dialogue participants at many levels (lex-
ical, syntactic and semantic), and the ability of
participants to collaborate on and complete each
other&apos;s sentences.
</bodyText>
<subsectionHeader confidence="0.998623">
5.1 Self-Monitoring
</subsectionHeader>
<bodyText confidence="0.999968285714285">
Self-monitoring (the step-by-step monitoring of
the generation process by an associated parse
routine) has long been taken to be essen-
tial by psycholinguists (de Smedt and Kem-
pen, 1990, for example). As discussed by
Neumann and van Noord (1994), it can also be
useful in a computational dialogue system, as it al-
lows generation to be controlled from the point of
view of the expected hearer. As well as checking
parsability, it can be used e.g. to prevent ambigu-
ous utterances (if a self-monitoring process can
spot that a string under consideration during gen-
eration is ambiguous, that string can be excluded
on the basis that it will be less clear for the hearer
than other unambiguous alternatives).
With standard generation approaches, self-
monitoring must be carried out in parallel by
a parsing module which communicates with the
generation module in some suitable manner. With
our approach, however, self-monitoring comes
built-in, as parsing is the building block for gener-
ation: all information that would be produced by
a separate parsing process is already available and
there is no need for a separate module.
The basic requirement for parsability by a
hearer is of course guaranteed, as strings are pro-
duced by incremental parsing. Further control can
be added easily by including the desired check at
the subsumption-checking stage: if desired, re-
jection of ambiguous strings could be added by
checking for partial trees that are not subsumed by
the goal tree, and rejecting any parser paths which
produce such trees (rather than just removing such
trees from the path) — as long as alternative unam-
biguous paths are available.
</bodyText>
<subsectionHeader confidence="0.974775">
5.2 Incrementality
</subsectionHeader>
<bodyText confidence="0.999990944444445">
The sense in which our generation process (and
the DS parsing process) is incremental differs
from that standardly used in both generation and
psycholinguistic literature. In the generation liter-
ature (see e.g. (Erbach, 1991; Stone and Doran,
1997)), incrementality is taken to refer to the abil-
ity to produce substrings corresponding to (incom-
plete) sub-parts of the semantic representation, but
without any requirement that this reflect left-to-
right processing. This concept is useful for dia-
logue system architecture, in that processing bot-
tlenecks in the semantic production module can be
worked around by passing any complete sub-parts
to an incremental generation module as they be-
come available. In the psycholinguistic literature,
&amp;quot;strong&amp;quot; incrementality (Sturt and Crocker, 1996;
de Smedt and Kempen, 1990) is taken to require
the projection of fixed tree relations as early and
</bodyText>
<page confidence="0.995905">
84
</page>
<bodyText confidence="0.9999064">
as close to word-by-word processing as possible7
— and this progressive update enforces revision and
backtracking.
Our use of the term is closer to the psycholin-
guistic concept in reflecting left-to-right parsing,
with all relevant processing being complete after
the addition of each word.8 It is this monotonicity
that allows unsuitable generation paths to be ruled
out on a word-by-word basis (see section 4.2.2
above), and that allows us to select lexical items
based just on a subsumption check, allowing se-
lection to be economical compared to lexicalist
approaches such as (Gardent and Thater, 2001)9
where syntactic variation is encoded in distinct
lexical entries, multiplying the number of gener-
ation search paths by the number of distinct struc-
tures licensed by the lexical item.
However, our approach can also be consid-
ered incremental in the computational sense: sub-
strings can be generated from suitable goal sub-
trees with no change to the algorithm (the root
node of the generation tree is merely given a node
name which is not fully specified, and require-
ments which correspond to the root node of the
goal sub-tree). I °
</bodyText>
<subsectionHeader confidence="0.963241">
5.3 Alignment
</subsectionHeader>
<bodyText confidence="0.997102743589744">
Our view of generation simplifies an explana-
tion of the multi-level alignment described by
Pickering and Garrod (2003). Standard genera-
tion models have little trouble explaining how
lexical choice can be co-ordinated between dia-
logue participants (word preference can be ad-
justed based on recency) — but syntactic mirror-
ing as observed by e.g. Branigan et al. (2000) is
more problematic (grammar rule preferences must
be adjusted, which is non-trivial).
7 Sturt and Crocker reflecton how different aspects of lan-
guage processing are more or less incremental, with syntac-
tic processing involving word-by-word update of the syntac-
tic tree, semantic processing, eg pronoun resolution, allowing
some delay.
8However, it differs from the concept of strong incremen-
tality, by allowing the articulation of partial trees, in particu-
lar trees in which not all relations are uniquely determined.
9We are not claiming the overall approach is more eco-
nomical.
19Though this approach remains to be fully worked out,
some first steps in this direction have been taken in consider-
ing head-final languages such as Japanese.
In DS, alignment of syntactic and semantic
structure comes by definition, given the reduction
of syntax to the progressive projection of seman-
tically transparent structure. Lexical alignment
can be explained as a preference for repeated re-
cent lexical items, bypassing the general lexicon
search. Apparent alignment of syntactic structure
over and above semantic alignment, as in repe-
tition of double object constructions as opposed
to a switch to the prepositional phrase equiva-
lent, can be explained by exactly the same mech-
anism: since syntactic and semantic preferences
are encoded in the lexical actions associated di-
rectly with words (rather than in any general gram-
mar rules), so adjustment of syntactic and seman-
tic constructions reduces directly to lexical choice.
</bodyText>
<subsectionHeader confidence="0.938855">
5.4 Collaboration / Completion
</subsectionHeader>
<bodyText confidence="0.9999916">
The phenomenon of shared utterances, in which
participants are able to switch roles and complete
one another&apos;s utterances at any point in a sentence,
is problematic for standard models of generation,
as Pickering and Garrod (2003) observe.&amp;quot;
The treatment of generation as incremental
parsing allows a simple explanation. Speaker and
hearer are building the same partial trees in par-
allel, using the same parsing process and directly
corresponding (if not perhaps identical) lexical en-
tries: the only significant difference is that the
speaker has knowledge of the goal tree that is be-
ing generated. If the hearer can guess or deduce
the goal tree,I2 (s)he is in just as good a position
to complete the utterance as the original speaker.
</bodyText>
<sectionHeader confidence="0.993015" genericHeader="conclusions">
6 Conclusions and Further Work
</sectionHeader>
<subsectionHeader confidence="0.835524">
6.1 Conclusions
</subsectionHeader>
<bodyText confidence="0.889048333333334">
The paper sets out a formal definition of a gener-
ation process for DS, in terms of the parsing pro-
cess, and describes a naive computational imple-
mentation together with some possible improve-
ments. In closing, we note that this (a) parsing-
oriented, and (b) incremental view of generation
11In particular, any shift prior to the sentence head is prob-
lematic for head-driven and LTAG approaches.
12We have nothing to say here about how this deduction
is performed: of course it will depend on world and domain
knowledge, the participants&apos; shared situation and experience,
and so on.
</bodyText>
<page confidence="0.999283">
85
</page>
<bodyText confidence="0.99984325">
promises to provide a basis for an explanation
of certain psychol ingui stic observations about dia-
logue such as co-ordination and collaboration be-
tween speakers.
</bodyText>
<subsectionHeader confidence="0.892258">
6.2 Further Work
</subsectionHeader>
<bodyText confidence="0.999911846153846">
The current system is a prototype which we in-
tend to extend in two main directions. Firstly,
efficiency can be improved by employing com-
putational tactics such as probabilistic/heuristic
parsing and generation, and possibly more goal-
directed methods. Secondly, we intend to incor-
porate the parsing/generation model into a full di-
alogue system, to test out in detail the extent to
which the apparent potential for modelling dia-
logue phenomena such as alignment, collaboration
etc. can actually be fulfilled. This will require a
detailed model of semantic tree creation and ma-
nipulation.
</bodyText>
<sectionHeader confidence="0.999143" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999603333333333">
The authors would like to thank Ruth Kempson,
Matthew Stone and two anonymous EACL re-
viewers for very helpful comments.
</bodyText>
<sectionHeader confidence="0.99904" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999574492957747">
Patrick Blackburn and Wilfried Meyer-Viol. 1994.
Linguistics, logic and finite trees. Bulletin of the
IGPL, 2:3-31.
Holly Branigan, Martin Pickering, and Alexandra Cle-
land. 2000. Syntactic co-ordination in dialogue.
Cognition, 75:13-25.
Koenrad de Smedt and Gerard Kempen. 1990. Incre-
mental sentence production, self-correction and co-
ordination. In G. Kempen, editor, Natural Language
Generation, pages 365-376. Martinus Nijhoff, Dor-
drecht.
Gregor Erbach. 1991. A bottom-up algorithm for pars-
ing and generation. CLAUS report, Universitat des
Saarlandes, Saarbriicken, February.
Lyn Frazier. 1982. Shared components of production
and perception. In M. Arbib et al., editor, Neural
Models of Language Processes, chapter 11, pages
225-236. Academic Press, New York.
Claire Gardent and Stefan Thater. 2001. Generat-
ing with a grammar based on tree descriptions: a
constraint-based approach. In Proceedings of the
39th Annual Meeting of the ACL. Association for
Computational Linguistics.
Merrill Garrett. 1982. Remarks on the relation be-
tween language production and language compre-
hension systems. In M. Arbib et al., editor, Neural
Models of Language Processes, chapter 10, pages
209-224. Academic Press, New York.
Albert S. Hornby. 1974. Oxford Advanced Learner&apos;s
Dictionary of Current English. Oxford University
Press, third edition. With the assistance of Anthony
P. Cowie and J. Windsor Lewis.
Ruth Kempson, Wilfried Meyer-Viol, and Dov Gabbay.
2001. Dynamic Syntax: The Flow of Language Un-
derstanding. Blackwell.
Giinter Neumann and Gertjan van Noord. 1994. Re-
versibility and self-monitoring in natural language
generation. In T. Strzalkowski, editor, Reversible
Grammars in Natural Language Processing. Kluwer
Academic Publishers.
Gunter Neumann. 1994. A Uniform Computational
Model for Natural Language Parsing and Gener-
ation. Ph.D. thesis, Universitat des Saarlandes,
S aarbrUcken.
Martin Pickering and Simon Garrod. 2003. Toward
a mechanistic psychology of dialogue. Behavioral
and Brain Sciences, forthcoming.
Aarne Ranta. 1994. Type-Theoretical Grammar. Ox-
ford University Press.
Ehud Reiter and Robert Dale. 1997. Building ap-
plied natural language generation systems. In K. van
Deemter and M. Stone, editors, Formal Issues in
Natural Language Generation. CORSO C05.
Stuart Shieber, Gertjan van Noord, Robert Moore,
and Fernando Pereira. 1990. A semantic head-
driven generation algorithm for unification-based
formalisms. Computational Linguistics, 16(1).
Stuart Shieber. 1988. A uniform architecture for pars-
ing and generation. In Proceedings of the 12th Inter-
national Conference on Computational Linguistics,
pages 614-619. COLING.
Matthew Stone and Christine Doran. 1997. Sentence
planning as description using tree-adjoining gram-
mar. In P. Cohen and W. Wahlster, editors, Pro-
ceedings of the 35th Annual Meeting of the ACL,
pages 198-205. Association for Computational Lin-
guistics.
Patrick Sturt and Matthew Crocker. 1996. Monotonic
syntactic processing: a cross-linguistic study of at-
tachment and reanalysis. Language and Cognitive
Processes, 11:448-494.
</reference>
<page confidence="0.998554">
86
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.207599">
<title confidence="0.996502">Incremental Generation by Incremental Parsing: Tactical Generation in Dynamic Syntax</title>
<author confidence="0.996372">Matthew</author>
<affiliation confidence="0.9888425">Department of Computer King&apos;s College</affiliation>
<address confidence="0.551842">Strand, London WC2R 2LS,</address>
<email confidence="0.996304">matthew.purver@kcl.ac.uk</email>
<author confidence="0.776227">Masayuki</author>
<affiliation confidence="0.992752">Department of King&apos;s College</affiliation>
<address confidence="0.548595">Strand, London WC2R 2LS,</address>
<email confidence="0.998229">masayuki.otsuka@kcl.ac.uk</email>
<abstract confidence="0.996084">The paper shows how an incremental tactical generator can be constructed based on the incremental parsing framework described in Dynamic Syntax (DS)(Kempson et al., 2001), without adding a generator-specific vocabulary or intermediate levels of representation. The resulting generator is defined purely in terms of the parsing process, together with a notion of tree subsumption. This is shown to have various advantages including easy self-monitoring and psycholinguistic plausibility. A simple Prolog implementation is described, together with various possible improvements in efficiency.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Patrick Blackburn</author>
<author>Wilfried Meyer-Viol</author>
</authors>
<title>Linguistics, logic and finite trees.</title>
<date>1994</date>
<journal>Bulletin of the IGPL,</journal>
<pages>2--3</pages>
<contexts>
<context position="2828" citStr="Blackburn and Meyer-Viol, 1994" startWordPosition="422" endWordPosition="425">w conclusions and outline further work in section 6. 2 Dynamic Syntax DS is a parsing-directed grammar formalism which claims that parsing is the central mechanism of language processing. DS defines parsing as a process of establishing mappings from an initial tree to complete trees using general computational rules (roughly corresponding to syntactic rules) and specific lexical actions projected by lexical items in the input string. The resulting tree structures represent semantic interpretations for a given input string, and are described by the language DU based on a modal tree logic LOFT (Blackburn and Meyer-Viol, 1994). Trees are either complete or partial. Partial trees have: (a) node addresses that are partially specified by underspecified modalities; or (b) node decorations that are underspecified by the use of requirements and meta-variables. Such partial specifications are always introduced with imposed requirements, which jointly constitute a set of constraints on possible tree growth. Trees are complete AT all such requirements are satisfied, leaving no remaining partiality or underspecification. For example, in &amp;quot;left-dislocation&amp;quot;, for which an initial constituent node is introduced into the tree 79 </context>
</contexts>
<marker>Blackburn, Meyer-Viol, 1994</marker>
<rawString>Patrick Blackburn and Wilfried Meyer-Viol. 1994. Linguistics, logic and finite trees. Bulletin of the IGPL, 2:3-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holly Branigan</author>
<author>Martin Pickering</author>
<author>Alexandra Cleland</author>
</authors>
<title>Syntactic co-ordination in dialogue.</title>
<date>2000</date>
<journal>Cognition,</journal>
<pages>75--13</pages>
<contexts>
<context position="22727" citStr="Branigan et al. (2000)" startWordPosition="3662" endWordPosition="3665">nerated from suitable goal subtrees with no change to the algorithm (the root node of the generation tree is merely given a node name which is not fully specified, and requirements which correspond to the root node of the goal sub-tree). I ° 5.3 Alignment Our view of generation simplifies an explanation of the multi-level alignment described by Pickering and Garrod (2003). Standard generation models have little trouble explaining how lexical choice can be co-ordinated between dialogue participants (word preference can be adjusted based on recency) — but syntactic mirroring as observed by e.g. Branigan et al. (2000) is more problematic (grammar rule preferences must be adjusted, which is non-trivial). 7 Sturt and Crocker reflecton how different aspects of language processing are more or less incremental, with syntactic processing involving word-by-word update of the syntactic tree, semantic processing, eg pronoun resolution, allowing some delay. 8However, it differs from the concept of strong incrementality, by allowing the articulation of partial trees, in particular trees in which not all relations are uniquely determined. 9We are not claiming the overall approach is more economical. 19Though this appr</context>
</contexts>
<marker>Branigan, Pickering, Cleland, 2000</marker>
<rawString>Holly Branigan, Martin Pickering, and Alexandra Cleland. 2000. Syntactic co-ordination in dialogue. Cognition, 75:13-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koenrad de Smedt</author>
<author>Gerard Kempen</author>
</authors>
<title>Incremental sentence production, self-correction and coordination.</title>
<date>1990</date>
<booktitle>Natural Language Generation,</booktitle>
<pages>365--376</pages>
<editor>In G. Kempen, editor,</editor>
<publisher>Martinus Nijhoff,</publisher>
<location>Dordrecht.</location>
<marker>de Smedt, Kempen, 1990</marker>
<rawString>Koenrad de Smedt and Gerard Kempen. 1990. Incremental sentence production, self-correction and coordination. In G. Kempen, editor, Natural Language Generation, pages 365-376. Martinus Nijhoff, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Erbach</author>
</authors>
<title>A bottom-up algorithm for parsing and generation.</title>
<date>1991</date>
<booktitle>CLAUS report, Universitat des Saarlandes, Saarbriicken,</booktitle>
<contexts>
<context position="7352" citStr="Erbach, 1991" startWordPosition="1165" endWordPosition="1167">) cannot be applied. Instead, generation must be defined in terms of parsing. 3.1 Connecting Parsing and Generation Despite informal observations made in psycholinguistics that production and comprehension systems have much in common (Garrett, 1982; Frazier, 1982), parsing and generation have generally been treated as quite separate research enterprises, both in the psycholinguistic and computational linguistic communities. Nevertheless, following Shieber (1988), attempts have been made to define the two in terms of a broadly common architecture and using shared reversible grammars (see e.g. (Erbach, 1991; Neumann, 1994; Gardent and Thater, 2001)). Although the contribution made by the present system is modest in only addressing the level of tactical generation, it nevertheless contributes to this co-articulation of parsing and generation by defining a generation system which purports to reflect the process of left-to-right incremental production, by using incremental parsing as the basic building block. 80 C* Parse (C*) : SUCCESS {?Ty(t)} {?Ty(e), {?Ty(e t)} Subsumption : SUCCESS Generate snores Parse (john) : SUCCESS {?Ty(t)} {Ty(e), Fo(john)} {?Ty(e t), Subsumption : SUCCESS Generate : (joh</context>
<context position="20543" citStr="Erbach, 1991" startWordPosition="3316" endWordPosition="3317"> easily by including the desired check at the subsumption-checking stage: if desired, rejection of ambiguous strings could be added by checking for partial trees that are not subsumed by the goal tree, and rejecting any parser paths which produce such trees (rather than just removing such trees from the path) — as long as alternative unambiguous paths are available. 5.2 Incrementality The sense in which our generation process (and the DS parsing process) is incremental differs from that standardly used in both generation and psycholinguistic literature. In the generation literature (see e.g. (Erbach, 1991; Stone and Doran, 1997)), incrementality is taken to refer to the ability to produce substrings corresponding to (incomplete) sub-parts of the semantic representation, but without any requirement that this reflect left-toright processing. This concept is useful for dialogue system architecture, in that processing bottlenecks in the semantic production module can be worked around by passing any complete sub-parts to an incremental generation module as they become available. In the psycholinguistic literature, &amp;quot;strong&amp;quot; incrementality (Sturt and Crocker, 1996; de Smedt and Kempen, 1990) is taken</context>
</contexts>
<marker>Erbach, 1991</marker>
<rawString>Gregor Erbach. 1991. A bottom-up algorithm for parsing and generation. CLAUS report, Universitat des Saarlandes, Saarbriicken, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyn Frazier</author>
</authors>
<title>Shared components of production and perception.</title>
<date>1982</date>
<booktitle>Neural Models of Language Processes, chapter 11,</booktitle>
<pages>225--236</pages>
<editor>In M. Arbib et al., editor,</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="7004" citStr="Frazier, 1982" startWordPosition="1111" endWordPosition="1113">al action to all of the trees to update the parser state. This routine is repeated until the parser finishes scanning the input string. 3 Generation As DS identifies grammaticality with parsability, rather than using concepts such as syntactic constituents or heads, standard approaches to generation such as head-driven methods (Shieber et al., 1990) cannot be applied. Instead, generation must be defined in terms of parsing. 3.1 Connecting Parsing and Generation Despite informal observations made in psycholinguistics that production and comprehension systems have much in common (Garrett, 1982; Frazier, 1982), parsing and generation have generally been treated as quite separate research enterprises, both in the psycholinguistic and computational linguistic communities. Nevertheless, following Shieber (1988), attempts have been made to define the two in terms of a broadly common architecture and using shared reversible grammars (see e.g. (Erbach, 1991; Neumann, 1994; Gardent and Thater, 2001)). Although the contribution made by the present system is modest in only addressing the level of tactical generation, it nevertheless contributes to this co-articulation of parsing and generation by defining a</context>
</contexts>
<marker>Frazier, 1982</marker>
<rawString>Lyn Frazier. 1982. Shared components of production and perception. In M. Arbib et al., editor, Neural Models of Language Processes, chapter 11, pages 225-236. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Gardent</author>
<author>Stefan Thater</author>
</authors>
<title>Generating with a grammar based on tree descriptions: a constraint-based approach.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the ACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7394" citStr="Gardent and Thater, 2001" startWordPosition="1170" endWordPosition="1173"> generation must be defined in terms of parsing. 3.1 Connecting Parsing and Generation Despite informal observations made in psycholinguistics that production and comprehension systems have much in common (Garrett, 1982; Frazier, 1982), parsing and generation have generally been treated as quite separate research enterprises, both in the psycholinguistic and computational linguistic communities. Nevertheless, following Shieber (1988), attempts have been made to define the two in terms of a broadly common architecture and using shared reversible grammars (see e.g. (Erbach, 1991; Neumann, 1994; Gardent and Thater, 2001)). Although the contribution made by the present system is modest in only addressing the level of tactical generation, it nevertheless contributes to this co-articulation of parsing and generation by defining a generation system which purports to reflect the process of left-to-right incremental production, by using incremental parsing as the basic building block. 80 C* Parse (C*) : SUCCESS {?Ty(t)} {?Ty(e), {?Ty(e t)} Subsumption : SUCCESS Generate snores Parse (john) : SUCCESS {?Ty(t)} {Ty(e), Fo(john)} {?Ty(e t), Subsumption : SUCCESS Generate : (john) john Parse (snores) Subsumption Generat</context>
<context position="21815" citStr="Gardent and Thater, 2001" startWordPosition="3511" endWordPosition="3514">ons as early and 84 as close to word-by-word processing as possible7 — and this progressive update enforces revision and backtracking. Our use of the term is closer to the psycholinguistic concept in reflecting left-to-right parsing, with all relevant processing being complete after the addition of each word.8 It is this monotonicity that allows unsuitable generation paths to be ruled out on a word-by-word basis (see section 4.2.2 above), and that allows us to select lexical items based just on a subsumption check, allowing selection to be economical compared to lexicalist approaches such as (Gardent and Thater, 2001)9 where syntactic variation is encoded in distinct lexical entries, multiplying the number of generation search paths by the number of distinct structures licensed by the lexical item. However, our approach can also be considered incremental in the computational sense: substrings can be generated from suitable goal subtrees with no change to the algorithm (the root node of the generation tree is merely given a node name which is not fully specified, and requirements which correspond to the root node of the goal sub-tree). I ° 5.3 Alignment Our view of generation simplifies an explanation of th</context>
</contexts>
<marker>Gardent, Thater, 2001</marker>
<rawString>Claire Gardent and Stefan Thater. 2001. Generating with a grammar based on tree descriptions: a constraint-based approach. In Proceedings of the 39th Annual Meeting of the ACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Merrill Garrett</author>
</authors>
<title>Remarks on the relation between language production and language comprehension systems.</title>
<date>1982</date>
<booktitle>Neural Models of Language Processes, chapter 10,</booktitle>
<pages>209--224</pages>
<editor>In M. Arbib et al., editor,</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="6988" citStr="Garrett, 1982" startWordPosition="1109" endWordPosition="1110">apply the lexical action to all of the trees to update the parser state. This routine is repeated until the parser finishes scanning the input string. 3 Generation As DS identifies grammaticality with parsability, rather than using concepts such as syntactic constituents or heads, standard approaches to generation such as head-driven methods (Shieber et al., 1990) cannot be applied. Instead, generation must be defined in terms of parsing. 3.1 Connecting Parsing and Generation Despite informal observations made in psycholinguistics that production and comprehension systems have much in common (Garrett, 1982; Frazier, 1982), parsing and generation have generally been treated as quite separate research enterprises, both in the psycholinguistic and computational linguistic communities. Nevertheless, following Shieber (1988), attempts have been made to define the two in terms of a broadly common architecture and using shared reversible grammars (see e.g. (Erbach, 1991; Neumann, 1994; Gardent and Thater, 2001)). Although the contribution made by the present system is modest in only addressing the level of tactical generation, it nevertheless contributes to this co-articulation of parsing and generati</context>
</contexts>
<marker>Garrett, 1982</marker>
<rawString>Merrill Garrett. 1982. Remarks on the relation between language production and language comprehension systems. In M. Arbib et al., editor, Neural Models of Language Processes, chapter 10, pages 209-224. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert S Hornby</author>
</authors>
<title>Oxford Advanced Learner&apos;s Dictionary of Current English.</title>
<date>1974</date>
<publisher>Oxford University Press,</publisher>
<note>third edition. With the assistance of</note>
<contexts>
<context position="13129" citStr="Hornby, 1974" startWordPosition="2113" endWordPosition="2114">ion. Prolog backtracking is used to allow any number of actions to be applied to any parser state (where possible). Lexical actions are defined similarly and given as templates for individual parts-of-speech and 2This may not be the most efficient way of rejecting unsuitable words — see section 4.2.3 below. 3The system can be accessed at http://st228.dcs.kcl.ac.uk:8080/ds. 4We represent sets as Prolog lists: no use is made of list order. 82 verb subcategorisation frames. The templates can then be interfaced to a standard computational lexicon (we are currently using one derived from the OALD (Hornby, 1974)). Parsing is now easily defined declaratively: the initial parser state is one in which the only possible tree is a single root node with a ?Ty(t) requirement, and none of the input string has been consumed; a final parser state is one in which a complete (all requirements discharged) tree is available and the entire input string has been consumed; and possible intermediate states are produced from other states by any number of computational actions, plus lexical actions defined by the consumption of the next word from the input string. 4.2 Efficiency 4.2.1 Parsing As described above, the cur</context>
</contexts>
<marker>Hornby, 1974</marker>
<rawString>Albert S. Hornby. 1974. Oxford Advanced Learner&apos;s Dictionary of Current English. Oxford University Press, third edition. With the assistance of Anthony P. Cowie and J. Windsor Lewis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruth Kempson</author>
<author>Wilfried Meyer-Viol</author>
<author>Dov Gabbay</author>
</authors>
<title>Dynamic Syntax: The Flow of Language Understanding.</title>
<date>2001</date>
<publisher>Blackwell.</publisher>
<marker>Kempson, Meyer-Viol, Gabbay, 2001</marker>
<rawString>Ruth Kempson, Wilfried Meyer-Viol, and Dov Gabbay. 2001. Dynamic Syntax: The Flow of Language Understanding. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giinter Neumann</author>
<author>Gertjan van Noord</author>
</authors>
<title>Reversibility and self-monitoring in natural language generation.</title>
<date>1994</date>
<booktitle>Reversible Grammars in Natural Language Processing.</booktitle>
<editor>In T. Strzalkowski, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Neumann, van Noord, 1994</marker>
<rawString>Giinter Neumann and Gertjan van Noord. 1994. Reversibility and self-monitoring in natural language generation. In T. Strzalkowski, editor, Reversible Grammars in Natural Language Processing. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunter Neumann</author>
</authors>
<title>A Uniform Computational Model for Natural Language Parsing and Generation.</title>
<date>1994</date>
<booktitle>Ph.D. thesis, Universitat des Saarlandes, S aarbrUcken.</booktitle>
<contexts>
<context position="7367" citStr="Neumann, 1994" startWordPosition="1168" endWordPosition="1169">plied. Instead, generation must be defined in terms of parsing. 3.1 Connecting Parsing and Generation Despite informal observations made in psycholinguistics that production and comprehension systems have much in common (Garrett, 1982; Frazier, 1982), parsing and generation have generally been treated as quite separate research enterprises, both in the psycholinguistic and computational linguistic communities. Nevertheless, following Shieber (1988), attempts have been made to define the two in terms of a broadly common architecture and using shared reversible grammars (see e.g. (Erbach, 1991; Neumann, 1994; Gardent and Thater, 2001)). Although the contribution made by the present system is modest in only addressing the level of tactical generation, it nevertheless contributes to this co-articulation of parsing and generation by defining a generation system which purports to reflect the process of left-to-right incremental production, by using incremental parsing as the basic building block. 80 C* Parse (C*) : SUCCESS {?Ty(t)} {?Ty(e), {?Ty(e t)} Subsumption : SUCCESS Generate snores Parse (john) : SUCCESS {?Ty(t)} {Ty(e), Fo(john)} {?Ty(e t), Subsumption : SUCCESS Generate : (john) john Parse (</context>
</contexts>
<marker>Neumann, 1994</marker>
<rawString>Gunter Neumann. 1994. A Uniform Computational Model for Natural Language Parsing and Generation. Ph.D. thesis, Universitat des Saarlandes, S aarbrUcken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Pickering</author>
<author>Simon Garrod</author>
</authors>
<title>Toward a mechanistic psychology of dialogue.</title>
<date>2003</date>
<journal>Behavioral and Brain Sciences, forthcoming.</journal>
<contexts>
<context position="18363" citStr="Pickering and Garrod (2003)" startWordPosition="2966" endWordPosition="2969">sycholinguistic Features The view of generation as incremental parsing appears to have certain advantages from both computational and psycholinguistic viewpoints. For computational dialogue systems, it ensures that self-monitoring information is available, and displays the property of incrementality. Psycholinguistically, as well as conforming to the general principle that production and comprehension are tightly coupled, it allows us to to explain certain phenomena characteristic of dialogue (where standard models of generation cannot), and therefore promises to meet the challenge set out by Pickering and Garrod (2003) that linguistic systems be evaluated by their success in reflecting such phenomena. This includes alignment between dialogue participants at many levels (lexical, syntactic and semantic), and the ability of participants to collaborate on and complete each other&apos;s sentences. 5.1 Self-Monitoring Self-monitoring (the step-by-step monitoring of the generation process by an associated parse routine) has long been taken to be essential by psycholinguists (de Smedt and Kempen, 1990, for example). As discussed by Neumann and van Noord (1994), it can also be useful in a computational dialogue system, </context>
<context position="22479" citStr="Pickering and Garrod (2003)" startWordPosition="3622" endWordPosition="3625">d in distinct lexical entries, multiplying the number of generation search paths by the number of distinct structures licensed by the lexical item. However, our approach can also be considered incremental in the computational sense: substrings can be generated from suitable goal subtrees with no change to the algorithm (the root node of the generation tree is merely given a node name which is not fully specified, and requirements which correspond to the root node of the goal sub-tree). I ° 5.3 Alignment Our view of generation simplifies an explanation of the multi-level alignment described by Pickering and Garrod (2003). Standard generation models have little trouble explaining how lexical choice can be co-ordinated between dialogue participants (word preference can be adjusted based on recency) — but syntactic mirroring as observed by e.g. Branigan et al. (2000) is more problematic (grammar rule preferences must be adjusted, which is non-trivial). 7 Sturt and Crocker reflecton how different aspects of language processing are more or less incremental, with syntactic processing involving word-by-word update of the syntactic tree, semantic processing, eg pronoun resolution, allowing some delay. 8However, it di</context>
<context position="24504" citStr="Pickering and Garrod (2003)" startWordPosition="3936" endWordPosition="3939">constructions as opposed to a switch to the prepositional phrase equivalent, can be explained by exactly the same mechanism: since syntactic and semantic preferences are encoded in the lexical actions associated directly with words (rather than in any general grammar rules), so adjustment of syntactic and semantic constructions reduces directly to lexical choice. 5.4 Collaboration / Completion The phenomenon of shared utterances, in which participants are able to switch roles and complete one another&apos;s utterances at any point in a sentence, is problematic for standard models of generation, as Pickering and Garrod (2003) observe.&amp;quot; The treatment of generation as incremental parsing allows a simple explanation. Speaker and hearer are building the same partial trees in parallel, using the same parsing process and directly corresponding (if not perhaps identical) lexical entries: the only significant difference is that the speaker has knowledge of the goal tree that is being generated. If the hearer can guess or deduce the goal tree,I2 (s)he is in just as good a position to complete the utterance as the original speaker. 6 Conclusions and Further Work 6.1 Conclusions The paper sets out a formal definition of a ge</context>
</contexts>
<marker>Pickering, Garrod, 2003</marker>
<rawString>Martin Pickering and Simon Garrod. 2003. Toward a mechanistic psychology of dialogue. Behavioral and Brain Sciences, forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aarne Ranta</author>
</authors>
<title>Type-Theoretical Grammar.</title>
<date>1994</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="10755" citStr="Ranta, 1994" startWordPosition="1726" endWordPosition="1727"> contains a member identical to the goal tree — see figure 1. Lexical selection is implicit: any word in the lexicon which is not associated with a node (or for11n assuming a source tree, we are currently ignoring some issues that might be required in a dialogue system, such as concept generation, and translation from some flatmeaning representation into a decorated source tree (not a trivial issue, as it is here that we will meet Shieber (1988)&apos;s logical equivalence problem). We are therefore treating a generator as a module which supports that part of the generation process called sugaring (Ranta, 1994) or linguistic realisation (Reiter and Dale, 1997) — translation from an unambiguously structured object in a meaning representation language to a natural-language string. mula decorating a node) in the goal tree will produce empty tree sets (as the trees produced will not be subsumed by the goal tree), and therefore cannot produce acceptable extended partial strings.2 4 Implementation The formal definition of DS species constraints of possible forms and extensions of tree structure and node decorations in terms of axioms (in LOFT) and algebraic definitions, and update actions (input/output re</context>
</contexts>
<marker>Ranta, 1994</marker>
<rawString>Aarne Ranta. 1994. Type-Theoretical Grammar. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building applied natural language generation systems.</title>
<date>1997</date>
<booktitle>Formal Issues in Natural Language Generation. CORSO C05.</booktitle>
<editor>In K. van Deemter and M. Stone, editors,</editor>
<contexts>
<context position="10805" citStr="Reiter and Dale, 1997" startWordPosition="1731" endWordPosition="1734"> tree — see figure 1. Lexical selection is implicit: any word in the lexicon which is not associated with a node (or for11n assuming a source tree, we are currently ignoring some issues that might be required in a dialogue system, such as concept generation, and translation from some flatmeaning representation into a decorated source tree (not a trivial issue, as it is here that we will meet Shieber (1988)&apos;s logical equivalence problem). We are therefore treating a generator as a module which supports that part of the generation process called sugaring (Ranta, 1994) or linguistic realisation (Reiter and Dale, 1997) — translation from an unambiguously structured object in a meaning representation language to a natural-language string. mula decorating a node) in the goal tree will produce empty tree sets (as the trees produced will not be subsumed by the goal tree), and therefore cannot produce acceptable extended partial strings.2 4 Implementation The formal definition of DS species constraints of possible forms and extensions of tree structure and node decorations in terms of axioms (in LOFT) and algebraic definitions, and update actions (input/output relations of grammar rules) are defined in a way tha</context>
</contexts>
<marker>Reiter, Dale, 1997</marker>
<rawString>Ehud Reiter and Robert Dale. 1997. Building applied natural language generation systems. In K. van Deemter and M. Stone, editors, Formal Issues in Natural Language Generation. CORSO C05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Gertjan van Noord</author>
<author>Robert Moore</author>
<author>Fernando Pereira</author>
</authors>
<title>A semantic headdriven generation algorithm for unification-based formalisms.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<marker>Shieber, van Noord, Moore, Pereira, 1990</marker>
<rawString>Stuart Shieber, Gertjan van Noord, Robert Moore, and Fernando Pereira. 1990. A semantic headdriven generation algorithm for unification-based formalisms. Computational Linguistics, 16(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
</authors>
<title>A uniform architecture for parsing and generation.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics,</booktitle>
<pages>614--619</pages>
<publisher>COLING.</publisher>
<contexts>
<context position="7206" citStr="Shieber (1988)" startWordPosition="1139" endWordPosition="1140">her than using concepts such as syntactic constituents or heads, standard approaches to generation such as head-driven methods (Shieber et al., 1990) cannot be applied. Instead, generation must be defined in terms of parsing. 3.1 Connecting Parsing and Generation Despite informal observations made in psycholinguistics that production and comprehension systems have much in common (Garrett, 1982; Frazier, 1982), parsing and generation have generally been treated as quite separate research enterprises, both in the psycholinguistic and computational linguistic communities. Nevertheless, following Shieber (1988), attempts have been made to define the two in terms of a broadly common architecture and using shared reversible grammars (see e.g. (Erbach, 1991; Neumann, 1994; Gardent and Thater, 2001)). Although the contribution made by the present system is modest in only addressing the level of tactical generation, it nevertheless contributes to this co-articulation of parsing and generation by defining a generation system which purports to reflect the process of left-to-right incremental production, by using incremental parsing as the basic building block. 80 C* Parse (C*) : SUCCESS {?Ty(t)} {?Ty(e), {</context>
<context position="10592" citStr="Shieber (1988)" startWordPosition="1701" endWordPosition="1702">an be continued no further (any string extension results in an empty parser state), and the set of output strings is taken as those for which the associated tree set contains a member identical to the goal tree — see figure 1. Lexical selection is implicit: any word in the lexicon which is not associated with a node (or for11n assuming a source tree, we are currently ignoring some issues that might be required in a dialogue system, such as concept generation, and translation from some flatmeaning representation into a decorated source tree (not a trivial issue, as it is here that we will meet Shieber (1988)&apos;s logical equivalence problem). We are therefore treating a generator as a module which supports that part of the generation process called sugaring (Ranta, 1994) or linguistic realisation (Reiter and Dale, 1997) — translation from an unambiguously structured object in a meaning representation language to a natural-language string. mula decorating a node) in the goal tree will produce empty tree sets (as the trees produced will not be subsumed by the goal tree), and therefore cannot produce acceptable extended partial strings.2 4 Implementation The formal definition of DS species constraints </context>
</contexts>
<marker>Shieber, 1988</marker>
<rawString>Stuart Shieber. 1988. A uniform architecture for parsing and generation. In Proceedings of the 12th International Conference on Computational Linguistics, pages 614-619. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Stone</author>
<author>Christine Doran</author>
</authors>
<title>Sentence planning as description using tree-adjoining grammar.</title>
<date>1997</date>
<booktitle>Proceedings of the 35th Annual Meeting of the ACL,</booktitle>
<pages>198--205</pages>
<editor>In P. Cohen and W. Wahlster, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<contexts>
<context position="20567" citStr="Stone and Doran, 1997" startWordPosition="3318" endWordPosition="3321">luding the desired check at the subsumption-checking stage: if desired, rejection of ambiguous strings could be added by checking for partial trees that are not subsumed by the goal tree, and rejecting any parser paths which produce such trees (rather than just removing such trees from the path) — as long as alternative unambiguous paths are available. 5.2 Incrementality The sense in which our generation process (and the DS parsing process) is incremental differs from that standardly used in both generation and psycholinguistic literature. In the generation literature (see e.g. (Erbach, 1991; Stone and Doran, 1997)), incrementality is taken to refer to the ability to produce substrings corresponding to (incomplete) sub-parts of the semantic representation, but without any requirement that this reflect left-toright processing. This concept is useful for dialogue system architecture, in that processing bottlenecks in the semantic production module can be worked around by passing any complete sub-parts to an incremental generation module as they become available. In the psycholinguistic literature, &amp;quot;strong&amp;quot; incrementality (Sturt and Crocker, 1996; de Smedt and Kempen, 1990) is taken to require the projecti</context>
</contexts>
<marker>Stone, Doran, 1997</marker>
<rawString>Matthew Stone and Christine Doran. 1997. Sentence planning as description using tree-adjoining grammar. In P. Cohen and W. Wahlster, editors, Proceedings of the 35th Annual Meeting of the ACL, pages 198-205. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Sturt</author>
<author>Matthew Crocker</author>
</authors>
<title>Monotonic syntactic processing: a cross-linguistic study of attachment and reanalysis.</title>
<date>1996</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>11--448</pages>
<contexts>
<context position="21106" citStr="Sturt and Crocker, 1996" startWordPosition="3397" endWordPosition="3400">rature. In the generation literature (see e.g. (Erbach, 1991; Stone and Doran, 1997)), incrementality is taken to refer to the ability to produce substrings corresponding to (incomplete) sub-parts of the semantic representation, but without any requirement that this reflect left-toright processing. This concept is useful for dialogue system architecture, in that processing bottlenecks in the semantic production module can be worked around by passing any complete sub-parts to an incremental generation module as they become available. In the psycholinguistic literature, &amp;quot;strong&amp;quot; incrementality (Sturt and Crocker, 1996; de Smedt and Kempen, 1990) is taken to require the projection of fixed tree relations as early and 84 as close to word-by-word processing as possible7 — and this progressive update enforces revision and backtracking. Our use of the term is closer to the psycholinguistic concept in reflecting left-to-right parsing, with all relevant processing being complete after the addition of each word.8 It is this monotonicity that allows unsuitable generation paths to be ruled out on a word-by-word basis (see section 4.2.2 above), and that allows us to select lexical items based just on a subsumption ch</context>
</contexts>
<marker>Sturt, Crocker, 1996</marker>
<rawString>Patrick Sturt and Matthew Crocker. 1996. Monotonic syntactic processing: a cross-linguistic study of attachment and reanalysis. Language and Cognitive Processes, 11:448-494.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>