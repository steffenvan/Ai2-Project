<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001150">
<title confidence="0.996637">
Speech Translation for Triage of Emergency Phonecalls in Minority
Languages
</title>
<author confidence="0.945725">
Udhyakumar Nallasamy, Alan W Black, Tanja Jerry Weltman
</author>
<affiliation confidence="0.794847">
Schultz, Robert Frederking Louisiana State University
Language Technologies Institute Baton Rouge,
Carnegie Mellon University Louisiana 70802 USA
</affiliation>
<address confidence="0.8568015">
5000 Forbes Avenue jweltm2@lsu.edu
Pittsburgh, PA 15213 USA
</address>
<email confidence="0.964129">
udhay@cmu.edu,
{awb,ref,tanja}@cs.cmu.edu
</email>
<figureCaption confidence="0.998144">
Figure 1. Ayudame system architecture
</figureCaption>
<sectionHeader confidence="0.980341" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999832">
We describe Ayudame, a system de-
signed to recognize and translate Spanish
emergency calls for better dispatching.
We analyze the research challenges in
adapting speech translation technology to
9-1-1 domain. We report our initial re-
search in 9-1-1 translation system design,
ASR experiments, and utterance classifi-
cation for translation.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959">
In the development of real-world-applicable lan-
guage technologies, it is good to find an applica-
tion with a significant need, and with a complex-
ity that appears to be within the capabilities of
current existing technology. Based on our ex-
perience in building speech-to-speech translation,
we believe that some important potential uses of
the technology do not require a full, complete
speech-to-speech translation system; something
much more lightweight can be sufficient to aid
the end users (Gao et al, 2006).
A particular task of this kind is dealing with
emergency call dispatch for police, ambulance,
fire and other emergency services (in the US the
emergency number is 9-1-1). A dispatcher must
answer a large variety of calls and, due to the
multilingual nature of American society, they
may receive non-English calls and be unable to
service them due to lack of knowledge of the
caller language.
</bodyText>
<note confidence="0.585344">
© 2008. Licensed under the Creative Commons At-
</note>
<footnote confidence="0.888371">
tribution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
</footnote>
<bodyText confidence="0.999031333333333">
As a part of a pilot study into the feasibility of
dealing with non-English calls by a mono-lingual
English-speaking dispatcher, we have designed a
translation system that will aid the dispatcher in
communicating without understanding the
caller’s language.
</bodyText>
<page confidence="0.996218">
48
</page>
<note confidence="0.695229">
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 48–53
Manchester, August 2008
</note>
<bodyText confidence="0.999775916666667">
The fundamental idea is to use utterance clas-
sification of the non-English input. The non-
English is first recognized by a speech recogni-
tion system; then the output is classified into a
small number of domain-specific classes called
Domain Acts (DAs) that can indicate directly to
the dispatcher the general intended meaning of
the spoken phrase. Each DA may have a few
important parameters to be translated, such as
street addresses (Levin et al, 2003; Langley
2003). The dispatcher can then select from a lim-
ited number of canned responses to this through
a simple menu system. We believe the reduction
in complexity of such a system compared to a
full speech-to-speech translation will be advanta-
geous because it should be much cheaper to con-
struct, easier to port to new languages, and, im-
portantly, sufficient to do the job of processing
emergency calls.
In the “NineOneOne” project, we have de-
signed an initial prototype system, which we call
“Ayudame” (Spanish word for “Help me”). Fig-
ure 1 gives an overview of the system architec-
ture.
</bodyText>
<sectionHeader confidence="0.946925" genericHeader="method">
2 The NineOneOne Domain
</sectionHeader>
<bodyText confidence="0.999779097560975">
Our initial interest in this domain was due to con-
tact from the Cape Coral Police Department
(CCPD) in Florida. They were interested in in-
vestigating how speech-to-speech translations
could be used in emergency 9-1-1 dispatch sys-
tems. Most current emergency dispatching cen-
ters use some proprietary human translation ser-
vice, such as Language Line (Language Line
Services). Although this service provides human
translation services for some 180 languages, it is
far from ideal. Once the dispatcher notes that the
caller cannot speak/understand English, they
must initiate the call to Language Line, including
identifying themselves to the Language Line op-
erator, before the call can actually continue. This
delay can be up to a minute, which is not ideal in
an emergency situation.
After consulting with CCPD, and collecting a
number of example calls, it was clear that full
speech-to-speech translation was not necessary
and that a limited form of translation through
utterance classification (Lavie et al, 2001) might
be sufficient to provide a rapid response to non-
English calls. The language for our study is
Spanish. Cape Coral is on the Gulf Coast of
Florida and has fewer Spanish speakers than e.g.
the Miami area, but still sufficient that a number
of calls are made to their emergency service in
Spanish, yet many of their operators are not
sufficiently fluent in Spanish to deal with the
calls.
There are a number of key pieces of
information that a dispatcher tries to collect
before passing on the information to the
appropriate emergency service. This includes
things like location, type of emergency, urgency,
if anyone is hurt, if the situation is dangerous,
etc. In fact many dispaching organizations have
existing, well-defined policies on what
information they should collect for different
types of emergencies.
</bodyText>
<sectionHeader confidence="0.992684" genericHeader="method">
3 Initial system design
</sectionHeader>
<bodyText confidence="0.987011175">
Based on the domain&apos;s characteristics, in addition
to avoiding full-blown translation, we are follow-
ing a highly asymmetrical design for the system
(Frederking et al, 2000). The dispatcher is al-
ready seated at a workstation, and we intend to
keep them “in the loop”, for both technical and
social reasons. So in the dispatcher-to-caller di-
rection, we can work with text and menus, sim-
plifying the technology and avoiding some cog-
nitive complexity for the operator. So in the dis-
patcher-to-caller direction we require
■ no English ASR,
■ no true English-to-Spanish MT, and
■ simple, domain-limited, Spanish speech
synthesis.
The caller-to-dispatcher direction is much more
interesting. In this direction we require
■ Spanish ASR that can handle emotional
spontaneous telephone speech in mixed
dialects,
■ Spanish-to-English MT, but
■ no English Speech Synthesis.
We have begun to consider the user interfaces
for Ayudame as well. For ease of integration
with pre-existing dispatcher workstations, we
have chosen to use a web-based graphical inter-
face. For initial testing of the prototype, we plan
to run in “shadow” mode, in parallel with live
dispatching using the traditional approach. Thus
Ayudame will have a listen-only connection to
the telephone line, and will run a web server to
interact with the dispatcher. Figure 2 shows an
initial design of the web-based interface. There
are sections for a transcript, the current caller
utterance, the current dispatcher response
choices, and a button to transfer the interaction to
a human translator as a fall-back option. For
each utterance, the DA classification is displayed
in addition to the actual utterance (in case the
dispatcher knows some Spanish).
</bodyText>
<page confidence="0.99914">
49
</page>
<figureCaption confidence="0.999422">
Figure 2. Example of initial GUI design
</figureCaption>
<sectionHeader confidence="0.971064" genericHeader="method">
4 Automatic Speech Recognition
</sectionHeader>
<bodyText confidence="0.999973121212121">
An important requirement for such a system is
the ability to be able to recognize the incoming
non-English speech with a word error rate suffi-
ciently low for utterance classification and pa-
rameter translation to be possible. The issues in
speech recognition for this particular domain in-
clude: telephone speech (which is through a lim-
ited bandwidth channel); background noise (the
calls are often from outside or in noisy places);
various dialects of Spanish, and potential stressed
speech. Although initially we expected a sub-
stantial issue with recognizing stressed speakers,
as one might expect in emergency situations, in
the calls we have collected so far, although it is
not a negligible issue, it is far less important that
we first expected.
The Spanish ASR system is built using the
Janus Recognition Toolkit (JRTk) (Finke et al,
1997) featuring the HMM-based IBIS decoder
(Soltau et al, 2001). Our speech corpus consists
of 75 transcribed 9-1-1 calls, with average call
duration of 6.73 minutes (min: 2.31 minutes,
max: 13.47 minutes). The average duration of
Spanish speech (between interpreter and caller)
amounts to 4.8 minutes per call. Each call has
anywhere from 46 to 182 speaker turns with an
average of 113 speaker turns per call. The turns
that have significant overlap between speakers
are omitted from the training and test set. The
acoustic models are trained on 50 Spanish 9-1-1
calls, which amount to 4 hours of speech data.
The system uses three-state, left-to-right, sub-
phonetically tied acoustic models with 400 con-
text-dependent distributions with the same num-
ber of codebooks. Each codebook has 32 gaus-
sians per state. The front-end feature extraction
uses standard 39 dimensional Mel-scale cepstral
coefficients and applies Linear Discriminant
Analysis (LDA) calculated from the training
data. The acoustic models are seeded with initial
alignments from GlobalPhone Spanish acoustic
models trained on 20 hours of speech recorded
from native Spanish speakers (Schultz et al,
1997). The vocabulary size is 65K words. The
language model consists of a trigram model
trained on the manual transcriptions of 40 calls
and interpolated with a background model
trained on GlobalPhone Spanish text data con-
sisting of 1.5 million words (Schultz et al, 1997).
The interpolation weights are determined using
the transcriptions of 10 calls (development set).
The test data consists of 15 telephone calls from
different speakers, which amounts to a total of 1
hour. Both development and test set calls con-
sisted of manually segmented and transcribed
speaker turns that do not have a significant over-
lap with other speakers. The perplexity of the test
set according to the language model is 96.7.
The accuracy of the Spanish ASR on the test
set is 76.5%. This is a good result for spontane-
ous telephone-quality speech by multiple un-
known speakers, and compares favourably to the
ASR accuracy of other spoken dialog systems.
We had initially planned to investigate novel
ASR techniques designed for stressed speech and
multiple dialects, but to our surprise these do not
</bodyText>
<page confidence="0.980115">
50
</page>
<bodyText confidence="0.999465428571429">
seem to be required for this application. Note
that critical information such as addresses will be
synthesized back to the caller for confirmation in
the full system. So, for the time-being we will
concentrate on the accuracy of the DA classifica-
tion until we can show that improving ASR accu-
racy would significantly help.
</bodyText>
<sectionHeader confidence="0.99315" genericHeader="method">
5 Utterance Classification
</sectionHeader>
<bodyText confidence="0.9998908">
As mentioned above, the translation approach we
are using is based on utterance classification. The
Spanish to English translation in the Ayudame
system is a two-step process. The ASR
hypothesis is first classified into domain-specific
Domain Acts (DA). Each DA has a
predetermined set of parameters. These
parameters are identified and translated using a
rule-based framework. For this approach to be
accomplished with reasonable effort levels, the
total number of types of parameters and their
complexity must be fairly limited in the domain,
such as addresses and injury types. This section
explains our DA tagset and classification
experiments.
</bodyText>
<subsectionHeader confidence="0.997041">
5.1 Initial classification and results
</subsectionHeader>
<bodyText confidence="0.997931">
The initial evaluation (Nallasamy et al, 2008)
included a total of 845 manually labeled turns in
our 9-1-1 corpus. We used a set of 10 tags to an-
notate the dialog turns. The distribution of the
tags are listed below
</bodyText>
<table confidence="0.999651636363636">
Tag (Representation) Frequency
Giving Name 80
Giving Address 118
Giving Phone number 29
Requesting Ambulance 8
Requesting Fire Service 11
Requesting Police 24
Reporting Injury/Urgency 61
Yes 119
No 24
Others 371
</table>
<tableCaption confidence="0.8214695">
Table 1. Distribution of first-pass tags in the
corpus.
</tableCaption>
<bodyText confidence="0.998683333333333">
We extracted bag-of-word features and trained a
Support Vector Machine (SVM) classifier (Bur-
ges, 1998) using the above dataset. A 10-fold
stratified cross-validation has produced an aver-
age accuracy of 60.12%. The accuracies of indi-
vidual tags are listed below.
</bodyText>
<table confidence="0.999134909090909">
Tag Accuracy (%)
Giving Name 57.50
Giving Address 38.98
Giving Phone number 48.28
Req. Ambulance 62.50
Req. Fire Service 54.55
Req. Police 41.67
Reporting Injury/Urgency 39.34
Yes 52.94
No 54.17
Others 75.74
</table>
<tableCaption confidence="0.999776">
Table 2. Classification accuracies of first-pass
</tableCaption>
<bodyText confidence="0.726897">
tags.
</bodyText>
<subsectionHeader confidence="0.987665">
5.2 Tag-set improvements
</subsectionHeader>
<bodyText confidence="0.943546548387097">
We improved both the DA tagset and the
classification framework in our second-pass
classification, compared to our initial
experiment. We had identified several issues in
our first-pass classification:
■ We had forced each dialog turn to have a
single tag. However, the tags and the
dialog turns don’t conform to this
assumption. For example, the dialog
“Yes, my husband has breathing prob-
lem. We are at two sixty-one Oak
Street”1 should get 3 tags: “Yes”, “Giv-
ing-Address”, “Requesting-Ambulance”.
■ Our analysis of the dataset also showed
that the initial set of tags are not
exhaustive enough to cover the whole
range of dialogs required to be translated
and conveyed to the dispatcher.
We made several iterations over the tagset to
ensure that it is both compact and achieves
requisite coverage. The final tag set consists of
67 entries. We manually annotated 59 calls with
our new tagset using a web interface. The
distribution of the top 20 tags is listed below.
The whole list of tags can be found in the
NineOneOne project webpage:
http://www.cs.cmu.edu/~911/
1 The dialog is English Translation of “sí, mi esposo le falta
el aire. es acá en el dos sesenta y uno Oak Street”. It is
extracted from the transcription of a CCPD 9-1-1
emergency call, with address modified to protect privacy
</bodyText>
<page confidence="0.996861">
51
</page>
<table confidence="0.999844904761905">
Tag (Representation) Frequency
Yes 227
Giving-Address 133
Giving-Location 113
Giving-Name 107
No 106
Other 94
OK 81
Thank-You 51
Reporting-Conflict 43
Describing-Vehicle 42
Giving-Telephone-Number 40
Hello 36
Reporting-Urgency-Or-Injury 34
Describing-Residence 28
Dont-Know 19
Dont-Understand 16
Giving-Age 15
Goodbye 15
Giving-Medical-Symptoms 14
Requesting-Police 12
</table>
<tableCaption confidence="0.7181025">
Table 3. Distribution of top 20 second-pass
tags
</tableCaption>
<bodyText confidence="0.999867">
The new tagset is hierarchical, which allows
us to evaluate the classifier at different levels of
the hierarchy, and eventually select the best
trade-off between the number of tags and
classification accuracy. For example, the first
level of tags for reporting incidents includes the
five most common incidents, viz, Reporting-
Conflict, Reporting-Robbery, Reporting-Traffic-
accident, Reporting-Urgency-or-Injury and
Reporting-Fire. The second level of tags are used
to convey more detailed information about the
above incidents (eg. Reporting-Weapons in the
case of conflict) or rare incidents (eg. Reporting-
Animal-Problem).
</bodyText>
<subsectionHeader confidence="0.99991">
5.3 Second-pass classification and Results
</subsectionHeader>
<bodyText confidence="0.9976874375">
We also improved our classification
framework to allow multiple tags for a single
turn and to easily accomodate any new tags in
the future. Our earlier DA classification used a
multi-class classifier, as each turn was restricted
to have a single tag. To accomodate multiple tags
for a single turn, we trained binary classifiers for
each tag. All the utterances of the corresponding
tag are marked positive examples and the rest are
marked as negative examples. Our new data set
has 1140 dialog turns and 1331 annotations. Note
that the number of annotations is more than the
number of labelled turns as each turn may have
multiple tags. We report classification accuracies
in the following table for each tag based on 10-
fold cross-validation:
</bodyText>
<table confidence="0.999660285714286">
Tag (Representation) Accuracy(%)
Yes 87.32
Giving-Address 42.71
Giving-Location 87.32
Giving-Name 42.71
No 37.63
Other 54.98
OK 72.5
Thank-You 41.14
Reporting-Conflict 79.33
Describing-Vehicle 96.82
Giving-Telephone-Number 39.37
Hello 38.79
Reporting-Urgency-Or-Injury 49.8
Describing-Residence 92.75
Dont-Know 41.67
Dont-Understand 36.03
Giving-Age 64.95
Goodbye 87.27
Giving-Medical-Symptoms 47.44
Requesting-Police 79.94
</table>
<tableCaption confidence="0.789316">
Table 4. Classification accuracies of
individual second-pass tags
</tableCaption>
<bodyText confidence="0.999958181818182">
The average accuracy of the 20 tags is
58.42%. Although multiple classifiers increase
the computational complexity during run-time,
they are independent of each other, so we can run
them in parallel. To ensure the consistency and
clarity of the new tag set, we had a second
annotator label 39 calls. The inter-coder
agreement (Kappa coefficient) between the two
annotators is 0.67. This is considered substantial
agreement between the annotators, and confirms
the consistency of the tag set.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9997234">
The work reported here demonstrates that we can
produce Spanish ASR for Spanish emergency
calls with reasonable accuracy (76.5%), and clas-
sify manual transcriptions of these calls with rea-
sonable accuracy (60.12% on the original tagset,
</bodyText>
<page confidence="0.994535">
52
</page>
<bodyText confidence="0.999947615384615">
58.42% on the new, improved tagset). We be-
lieve these results are good enough to justify the
next phase of research, in which we will develop,
user-test, and evaluate a full pilot system. We are
also investigating a number of additional tech-
niques to improve the DA classification accura-
cies. Further we believe that we can design the
overall dialog system to ameliorate the inevitable
remaining misclassifications, based in part on the
confusion matrix of actual errors (Nallasamy et
al, 2008). But only actual user tests of a pilot
system will allow us to know whether an even-
tual deployable system is really feasible.
</bodyText>
<sectionHeader confidence="0.994863" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.97788475">
This project is funded by NSF Grant No: IIS-
0627957 “NineOneOne: Exploratory Research
on Recognizing Non-English Speech for Emer-
gency Triage in Disaster Response”. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the
authors and do not necessarily reflect the views
of sponsors.
</bodyText>
<sectionHeader confidence="0.997673" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999391320754717">
Burges C J C, A tutorial on support vector machines
for pattern recognition, In Proc. Data Mining and
Knowledge Discovery, pp 2(2):955-974, USA,
1998
Finke M, Geutner P, Hild H, Kemp T, Ries K and
Westphal M, The Karlsruhe-Verbmobil Speech
Recognition Engine, In Proc. IEEE International
Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pp. 83-86, Germany, 1997
Frederking R, Rudnicky A, Hogan C and Lenzo K,
Interactive Speech Translation in the Diplomat
Project, Machine Translation Journal 15(1-2),
Special issue on Spoken Language Translation, pp.
61-66, USA, 2000
Gao Y, Zhou B, Sarikaya R, Afify M, Kuo H, Zhu W,
Deng Y, Prosser C, Zhang W and Besacier L, IBM
MASTOR SYSTEM: Multilingual Automatic
Speech-to-Speech Translator, In Proc. First Inter-
national Workshop on Medical Speech Translation,
pp. 53-56, USA, 2006
Langley C, Domain Action Classification and Argu-
ment Parsing for Interlingua-based Spoken Lan-
guage Translation. PhD thesis, Carnegie Mellon
University, Pittsburgh, PA, 2003
Language Line Services http://www.languageline.com
Lavie A, Balducci F, Coletti P, Langley C, Lazzari G,
Pianesi F, Taddei L and Waibel A, Architecture
and Design Considerations in NESPOLE!: a
Speech Translation System for E-Commerce Ap-
plications,. In Proc. Human Language Technolo-
gies (HLT), pp 31-34, USA, 2001
Levin L, Langley C, Lavie A, Gates D, Wallace D and
Peterson K, Domain Specific Speech Acts for Spo-
ken Language Translation, In Proc. 4th SIGdial
Workshop on Discourse and Dialogue, pp. 208-
217, Japan, 2003
Nallasamy U, Black A, Schultz T and Frederking R,
NineOneOne: Recognizing and Classifying Speech
for Handling Minority Language Emergency Calls,
In Proc. 6th International conference on Language
Resources and Evaluation (LREC), Morocco, 2008
NineOneOne project webpage
[www.cs.cmu.edu/~911]
Schultz T, Westphal M and Waibel A, The
GlobalPhone Project: Multilingual LVCSR with
JANUS-3, In Proc. Multilingual Information Re-
trieval Dialogs: 2nd SQEL Workshop, pp. 20-27,
Czech Republic, 1997
Soltau H, Metze F, F¨ugen C and Waibel A, A One
Pass-Decoder Based on Polymorphic Linguistic
Context Assignment, In Proc. IEEE workshop on
Automatic Speech Recognition and Understanding
(ASRU), Italy, 2001
</reference>
<page confidence="0.99934">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.203527">
<title confidence="0.998136">Speech Translation for Triage of Emergency Phonecalls in Minority Languages</title>
<author confidence="0.977682">Udhyakumar Nallasamy</author>
<author confidence="0.977682">Alan W Black</author>
<author confidence="0.977682">Jerry Schultz</author>
<author confidence="0.977682">Robert Louisiana State</author>
<affiliation confidence="0.817565">Language Technologies Baton</affiliation>
<address confidence="0.739962666666667">Carnegie Mellon Louisiana 70802 USA 5000 Forbes jweltm2@lsu.edu Pittsburgh, PA 15213 USA</address>
<email confidence="0.9899965">udhay@cmu.edu,{awb,ref,tanja}@cs.cmu.edu</email>
<note confidence="0.675036">Figure 1. Ayudame system architecture</note>
<abstract confidence="0.9903718">We describe Ayudame, a system designed to recognize and translate Spanish emergency calls for better dispatching. We analyze the research challenges in adapting speech translation technology to 9-1-1 domain. We report our initial research in 9-1-1 translation system design, ASR experiments, and utterance classification for translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C J C Burges</author>
</authors>
<title>A tutorial on support vector machines for pattern recognition,</title>
<date>1998</date>
<booktitle>In Proc. Data Mining and Knowledge Discovery,</booktitle>
<pages>2--2</pages>
<location>USA,</location>
<contexts>
<context position="11697" citStr="Burges, 1998" startWordPosition="1831" endWordPosition="1833">classification and results The initial evaluation (Nallasamy et al, 2008) included a total of 845 manually labeled turns in our 9-1-1 corpus. We used a set of 10 tags to annotate the dialog turns. The distribution of the tags are listed below Tag (Representation) Frequency Giving Name 80 Giving Address 118 Giving Phone number 29 Requesting Ambulance 8 Requesting Fire Service 11 Requesting Police 24 Reporting Injury/Urgency 61 Yes 119 No 24 Others 371 Table 1. Distribution of first-pass tags in the corpus. We extracted bag-of-word features and trained a Support Vector Machine (SVM) classifier (Burges, 1998) using the above dataset. A 10-fold stratified cross-validation has produced an average accuracy of 60.12%. The accuracies of individual tags are listed below. Tag Accuracy (%) Giving Name 57.50 Giving Address 38.98 Giving Phone number 48.28 Req. Ambulance 62.50 Req. Fire Service 54.55 Req. Police 41.67 Reporting Injury/Urgency 39.34 Yes 52.94 No 54.17 Others 75.74 Table 2. Classification accuracies of first-pass tags. 5.2 Tag-set improvements We improved both the DA tagset and the classification framework in our second-pass classification, compared to our initial experiment. We had identified</context>
</contexts>
<marker>Burges, 1998</marker>
<rawString>Burges C J C, A tutorial on support vector machines for pattern recognition, In Proc. Data Mining and Knowledge Discovery, pp 2(2):955-974, USA, 1998</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Finke</author>
<author>P Geutner</author>
<author>H Hild</author>
<author>T Kemp</author>
<author>K Ries</author>
<author>M Westphal</author>
</authors>
<title>The Karlsruhe-Verbmobil Speech Recognition Engine, In</title>
<date>1997</date>
<booktitle>Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<pages>83--86</pages>
<contexts>
<context position="7829" citStr="Finke et al, 1997" startWordPosition="1216" endWordPosition="1219">he issues in speech recognition for this particular domain include: telephone speech (which is through a limited bandwidth channel); background noise (the calls are often from outside or in noisy places); various dialects of Spanish, and potential stressed speech. Although initially we expected a substantial issue with recognizing stressed speakers, as one might expect in emergency situations, in the calls we have collected so far, although it is not a negligible issue, it is far less important that we first expected. The Spanish ASR system is built using the Janus Recognition Toolkit (JRTk) (Finke et al, 1997) featuring the HMM-based IBIS decoder (Soltau et al, 2001). Our speech corpus consists of 75 transcribed 9-1-1 calls, with average call duration of 6.73 minutes (min: 2.31 minutes, max: 13.47 minutes). The average duration of Spanish speech (between interpreter and caller) amounts to 4.8 minutes per call. Each call has anywhere from 46 to 182 speaker turns with an average of 113 speaker turns per call. The turns that have significant overlap between speakers are omitted from the training and test set. The acoustic models are trained on 50 Spanish 9-1-1 calls, which amount to 4 hours of speech </context>
</contexts>
<marker>Finke, Geutner, Hild, Kemp, Ries, Westphal, 1997</marker>
<rawString>Finke M, Geutner P, Hild H, Kemp T, Ries K and Westphal M, The Karlsruhe-Verbmobil Speech Recognition Engine, In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 83-86, Germany, 1997</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Frederking</author>
<author>A Rudnicky</author>
<author>C Hogan</author>
<author>K Lenzo</author>
</authors>
<title>Interactive Speech Translation in the Diplomat Project,</title>
<date>2000</date>
<journal>Machine Translation Journal 15(1-2), Special issue on Spoken Language Translation,</journal>
<pages>61--66</pages>
<location>USA,</location>
<contexts>
<context position="5374" citStr="Frederking et al, 2000" startWordPosition="824" endWordPosition="827">. There are a number of key pieces of information that a dispatcher tries to collect before passing on the information to the appropriate emergency service. This includes things like location, type of emergency, urgency, if anyone is hurt, if the situation is dangerous, etc. In fact many dispaching organizations have existing, well-defined policies on what information they should collect for different types of emergencies. 3 Initial system design Based on the domain&apos;s characteristics, in addition to avoiding full-blown translation, we are following a highly asymmetrical design for the system (Frederking et al, 2000). The dispatcher is already seated at a workstation, and we intend to keep them “in the loop”, for both technical and social reasons. So in the dispatcher-to-caller direction, we can work with text and menus, simplifying the technology and avoiding some cognitive complexity for the operator. So in the dispatcher-to-caller direction we require ■ no English ASR, ■ no true English-to-Spanish MT, and ■ simple, domain-limited, Spanish speech synthesis. The caller-to-dispatcher direction is much more interesting. In this direction we require ■ Spanish ASR that can handle emotional spontaneous teleph</context>
</contexts>
<marker>Frederking, Rudnicky, Hogan, Lenzo, 2000</marker>
<rawString>Frederking R, Rudnicky A, Hogan C and Lenzo K, Interactive Speech Translation in the Diplomat Project, Machine Translation Journal 15(1-2), Special issue on Spoken Language Translation, pp. 61-66, USA, 2000</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gao</author>
<author>B Zhou</author>
<author>R Sarikaya</author>
<author>M Afify</author>
<author>H Kuo</author>
<author>W Zhu</author>
<author>Y Deng</author>
<author>C Prosser</author>
<author>W Zhang</author>
<author>L Besacier</author>
</authors>
<title>IBM MASTOR SYSTEM: Multilingual Automatic Speech-to-Speech Translator, In</title>
<date>2006</date>
<booktitle>Proc. First International Workshop on Medical Speech Translation,</booktitle>
<pages>53--56</pages>
<location>USA,</location>
<contexts>
<context position="1290" citStr="Gao et al, 2006" startWordPosition="178" endWordPosition="181"> translation system design, ASR experiments, and utterance classification for translation. 1 Introduction In the development of real-world-applicable language technologies, it is good to find an application with a significant need, and with a complexity that appears to be within the capabilities of current existing technology. Based on our experience in building speech-to-speech translation, we believe that some important potential uses of the technology do not require a full, complete speech-to-speech translation system; something much more lightweight can be sufficient to aid the end users (Gao et al, 2006). A particular task of this kind is dealing with emergency call dispatch for police, ambulance, fire and other emergency services (in the US the emergency number is 9-1-1). A dispatcher must answer a large variety of calls and, due to the multilingual nature of American society, they may receive non-English calls and be unable to service them due to lack of knowledge of the caller language. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. As a part of a pilot study in</context>
</contexts>
<marker>Gao, Zhou, Sarikaya, Afify, Kuo, Zhu, Deng, Prosser, Zhang, Besacier, 2006</marker>
<rawString>Gao Y, Zhou B, Sarikaya R, Afify M, Kuo H, Zhu W, Deng Y, Prosser C, Zhang W and Besacier L, IBM MASTOR SYSTEM: Multilingual Automatic Speech-to-Speech Translator, In Proc. First International Workshop on Medical Speech Translation, pp. 53-56, USA, 2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Langley</author>
</authors>
<title>Domain Action Classification and Argument Parsing for Interlingua-based Spoken Language Translation.</title>
<date>2003</date>
<tech>PhD thesis,</tech>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="2747" citStr="Langley 2003" startWordPosition="401" endWordPosition="402"> Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 48–53 Manchester, August 2008 The fundamental idea is to use utterance classification of the non-English input. The nonEnglish is first recognized by a speech recognition system; then the output is classified into a small number of domain-specific classes called Domain Acts (DAs) that can indicate directly to the dispatcher the general intended meaning of the spoken phrase. Each DA may have a few important parameters to be translated, such as street addresses (Levin et al, 2003; Langley 2003). The dispatcher can then select from a limited number of canned responses to this through a simple menu system. We believe the reduction in complexity of such a system compared to a full speech-to-speech translation will be advantageous because it should be much cheaper to construct, easier to port to new languages, and, importantly, sufficient to do the job of processing emergency calls. In the “NineOneOne” project, we have designed an initial prototype system, which we call “Ayudame” (Spanish word for “Help me”). Figure 1 gives an overview of the system architecture. 2 The NineOneOne Domain</context>
</contexts>
<marker>Langley, 2003</marker>
<rawString>Langley C, Domain Action Classification and Argument Parsing for Interlingua-based Spoken Language Translation. PhD thesis, Carnegie Mellon University, Pittsburgh, PA, 2003</rawString>
</citation>
<citation valid="false">
<institution>Language Line</institution>
<note>Services http://www.languageline.com</note>
<marker></marker>
<rawString>Language Line Services http://www.languageline.com</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>F Balducci</author>
<author>P Coletti</author>
<author>C Langley</author>
<author>G Lazzari</author>
<author>F Pianesi</author>
<author>L Taddei</author>
<author>A Waibel</author>
</authors>
<title>Architecture and Design Considerations in NESPOLE!: a Speech Translation System for E-Commerce Applications,. In</title>
<date>2001</date>
<booktitle>Proc. Human Language Technologies (HLT),</booktitle>
<pages>31--34</pages>
<location>USA,</location>
<contexts>
<context position="4361" citStr="Lavie et al, 2001" startWordPosition="660" endWordPosition="663"> provides human translation services for some 180 languages, it is far from ideal. Once the dispatcher notes that the caller cannot speak/understand English, they must initiate the call to Language Line, including identifying themselves to the Language Line operator, before the call can actually continue. This delay can be up to a minute, which is not ideal in an emergency situation. After consulting with CCPD, and collecting a number of example calls, it was clear that full speech-to-speech translation was not necessary and that a limited form of translation through utterance classification (Lavie et al, 2001) might be sufficient to provide a rapid response to nonEnglish calls. The language for our study is Spanish. Cape Coral is on the Gulf Coast of Florida and has fewer Spanish speakers than e.g. the Miami area, but still sufficient that a number of calls are made to their emergency service in Spanish, yet many of their operators are not sufficiently fluent in Spanish to deal with the calls. There are a number of key pieces of information that a dispatcher tries to collect before passing on the information to the appropriate emergency service. This includes things like location, type of emergency</context>
</contexts>
<marker>Lavie, Balducci, Coletti, Langley, Lazzari, Pianesi, Taddei, Waibel, 2001</marker>
<rawString>Lavie A, Balducci F, Coletti P, Langley C, Lazzari G, Pianesi F, Taddei L and Waibel A, Architecture and Design Considerations in NESPOLE!: a Speech Translation System for E-Commerce Applications,. In Proc. Human Language Technologies (HLT), pp 31-34, USA, 2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Levin</author>
<author>C Langley</author>
<author>A Lavie</author>
<author>D Gates</author>
<author>D Wallace</author>
<author>K Peterson</author>
</authors>
<title>Domain Specific Speech Acts for Spoken Language Translation,</title>
<date>2003</date>
<booktitle>In Proc. 4th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>208--217</pages>
<contexts>
<context position="2732" citStr="Levin et al, 2003" startWordPosition="397" endWordPosition="400">ge. 48 Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 48–53 Manchester, August 2008 The fundamental idea is to use utterance classification of the non-English input. The nonEnglish is first recognized by a speech recognition system; then the output is classified into a small number of domain-specific classes called Domain Acts (DAs) that can indicate directly to the dispatcher the general intended meaning of the spoken phrase. Each DA may have a few important parameters to be translated, such as street addresses (Levin et al, 2003; Langley 2003). The dispatcher can then select from a limited number of canned responses to this through a simple menu system. We believe the reduction in complexity of such a system compared to a full speech-to-speech translation will be advantageous because it should be much cheaper to construct, easier to port to new languages, and, importantly, sufficient to do the job of processing emergency calls. In the “NineOneOne” project, we have designed an initial prototype system, which we call “Ayudame” (Spanish word for “Help me”). Figure 1 gives an overview of the system architecture. 2 The Ni</context>
</contexts>
<marker>Levin, Langley, Lavie, Gates, Wallace, Peterson, 2003</marker>
<rawString>Levin L, Langley C, Lavie A, Gates D, Wallace D and Peterson K, Domain Specific Speech Acts for Spoken Language Translation, In Proc. 4th SIGdial Workshop on Discourse and Dialogue, pp. 208-217, Japan, 2003</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Nallasamy</author>
<author>A Black</author>
<author>T Schultz</author>
<author>R Frederking</author>
</authors>
<title>NineOneOne: Recognizing and Classifying Speech for Handling Minority Language Emergency Calls, In</title>
<date>2008</date>
<booktitle>Proc. 6th International conference on Language Resources and Evaluation (LREC),</booktitle>
<note>NineOneOne project webpage [www.cs.cmu.edu/~911]</note>
<contexts>
<context position="11157" citStr="Nallasamy et al, 2008" startWordPosition="1741" endWordPosition="1744">nglish translation in the Ayudame system is a two-step process. The ASR hypothesis is first classified into domain-specific Domain Acts (DA). Each DA has a predetermined set of parameters. These parameters are identified and translated using a rule-based framework. For this approach to be accomplished with reasonable effort levels, the total number of types of parameters and their complexity must be fairly limited in the domain, such as addresses and injury types. This section explains our DA tagset and classification experiments. 5.1 Initial classification and results The initial evaluation (Nallasamy et al, 2008) included a total of 845 manually labeled turns in our 9-1-1 corpus. We used a set of 10 tags to annotate the dialog turns. The distribution of the tags are listed below Tag (Representation) Frequency Giving Name 80 Giving Address 118 Giving Phone number 29 Requesting Ambulance 8 Requesting Fire Service 11 Requesting Police 24 Reporting Injury/Urgency 61 Yes 119 No 24 Others 371 Table 1. Distribution of first-pass tags in the corpus. We extracted bag-of-word features and trained a Support Vector Machine (SVM) classifier (Burges, 1998) using the above dataset. A 10-fold stratified cross-validat</context>
</contexts>
<marker>Nallasamy, Black, Schultz, Frederking, 2008</marker>
<rawString>Nallasamy U, Black A, Schultz T and Frederking R, NineOneOne: Recognizing and Classifying Speech for Handling Minority Language Emergency Calls, In Proc. 6th International conference on Language Resources and Evaluation (LREC), Morocco, 2008 NineOneOne project webpage [www.cs.cmu.edu/~911]</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Schultz</author>
<author>M Westphal</author>
<author>A Waibel</author>
</authors>
<title>The GlobalPhone Project: Multilingual LVCSR with JANUS-3,</title>
<date>1997</date>
<booktitle>In Proc. Multilingual Information Retrieval Dialogs: 2nd SQEL Workshop,</booktitle>
<pages>20--27</pages>
<location>Czech Republic,</location>
<contexts>
<context position="8995" citStr="Schultz et al, 1997" startWordPosition="1396" endWordPosition="1399">panish 9-1-1 calls, which amount to 4 hours of speech data. The system uses three-state, left-to-right, subphonetically tied acoustic models with 400 context-dependent distributions with the same number of codebooks. Each codebook has 32 gaussians per state. The front-end feature extraction uses standard 39 dimensional Mel-scale cepstral coefficients and applies Linear Discriminant Analysis (LDA) calculated from the training data. The acoustic models are seeded with initial alignments from GlobalPhone Spanish acoustic models trained on 20 hours of speech recorded from native Spanish speakers (Schultz et al, 1997). The vocabulary size is 65K words. The language model consists of a trigram model trained on the manual transcriptions of 40 calls and interpolated with a background model trained on GlobalPhone Spanish text data consisting of 1.5 million words (Schultz et al, 1997). The interpolation weights are determined using the transcriptions of 10 calls (development set). The test data consists of 15 telephone calls from different speakers, which amounts to a total of 1 hour. Both development and test set calls consisted of manually segmented and transcribed speaker turns that do not have a significant</context>
</contexts>
<marker>Schultz, Westphal, Waibel, 1997</marker>
<rawString>Schultz T, Westphal M and Waibel A, The GlobalPhone Project: Multilingual LVCSR with JANUS-3, In Proc. Multilingual Information Retrieval Dialogs: 2nd SQEL Workshop, pp. 20-27, Czech Republic, 1997</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Soltau</author>
<author>F Metze</author>
<author>C F¨ugen</author>
<author>A Waibel</author>
</authors>
<title>A One Pass-Decoder Based on Polymorphic Linguistic Context Assignment,</title>
<date>2001</date>
<booktitle>In Proc. IEEE workshop on Automatic Speech Recognition and Understanding (ASRU),</booktitle>
<marker>Soltau, Metze, F¨ugen, Waibel, 2001</marker>
<rawString>Soltau H, Metze F, F¨ugen C and Waibel A, A One Pass-Decoder Based on Polymorphic Linguistic Context Assignment, In Proc. IEEE workshop on Automatic Speech Recognition and Understanding (ASRU), Italy, 2001</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>