<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000628">
<title confidence="0.9964135">
Applying Explanation-based Learning to Control and Speeding-up
Natural Language Generation
</title>
<author confidence="0.819711">
Giinter Neumann
</author>
<affiliation confidence="0.481661">
DFKI GmbH
</affiliation>
<address confidence="0.7968145">
Stuhlsatzenhausweg 3
66123 Saarbriicken, Germany
</address>
<email confidence="0.86222">
neumannOdfki.uni-sb.de
</email>
<sectionHeader confidence="0.992111" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999768916666667">
This paper presents a method for the au-
tomatic extraction of subgrammars to con-
trol and speeding-up natural language gen-
eration NLG. The method is based on
explanation-based learning EBL. The main
advantage for the proposed new method
for NLG is that the complexity of the
grammatical decision making process dur-
ing NLG can be vastly reduced, because
the EBL method supports the adaption of
a NLG system to a particular use of a lan-
guage.
</bodyText>
<sectionHeader confidence="0.997905" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931196078432">
In recent years, a Machine Learning tech-
nique known as Explanation-based Learning EBL
(Mitchell, Keller, and Kedar-Cabelli, 1986; van
Harmelen and Bundy, 1988; Minton et al., 1989) has
successfully been applied to control and speeding-up
natural language parsing (Rayner, 1988; Samuelsson
and Rayner, 1991; Neumann, 1994a; Samuelsson,
1994; Srinivas and Joshi, 1995; Rayner and Carter,
1996). The core idea of EBL is to transform the
derivations (or explanations) computed by a prob-
lem solver (e.g., a parser) to some generalized and
compact forms, which can be used very efficiently
for solving similar problems in the future. EBL has
primarily been used for parsing to automatically spe-
cialize a given source grammar to a specific domain.
In that case, EBL is used as a method for adapting a
general grammar and/or parser to the sub-language
defined by a suitable training corpus (Rayner and
Carter, 1996).
A specialized grammar can be seen as describ-
ing a domain-specific set of prototypical construc-
tions. Therefore, the EBL approach is also very
interesting for natural language generation (NLG).
Informally, NLG is the production of a natural
language text from computer-internal representa-
tion of information, where NLG can be seen as
a complex—potentially cascaded—decision making
process. Commonly, a NLG system is decomposed
into two major components, viz, the strategic com-
ponent which decides &apos;what to say&apos; and the tacti-
cal component which decides &apos;how to say&apos; the result
of the strategic component. The input of the tacti-
cal component is basically a semantic representation
computed by the strategic component. Using a lexi-
con and a grammar, its main task is the computation
of potentially all possible strings associated with a
semantic input. Now, in the same sense as EBL is
used in parsing as a means to control the range of
possible strings as well as their degree of ambigu-
ity, it can also be used for the tactical component
to control the range of possible semantic input and
their degree of paraphrases.
In this paper, we present a novel method for the
automatic extraction of subgrammars for the control
and speeding-up of natural language generation. Its
main advantage for NLG is that the complexity of
the (linguistically oriented) decision making process
during natural language generation can be vastly re-
duced, because the EBL method supports adaption
of a NLG system to a particular language use. The
core properties of this new method are:
</bodyText>
<listItem confidence="0.9988851">
• prototypical occuring grammatical construc-
tions can automatically be extracted;
• generation of these constructions is vastly sped
up using simple but efficient mechanisms;
• the new method supports partial matching, in
the sense that new semantic input need not be
completely covered by previously trained exam-
ples;
• it can easily be integrated with recently de-
veloped chart-based generators as described in,
</listItem>
<page confidence="0.998383">
214
</page>
<bodyText confidence="0.9919684">
e.g., (Neumann, 1994b; Kay, 1996; Shemtov,
1996).
The method has been completely implemented
and tested with a broad-coverage HPSG-based
grammar for English (see sec. 5 for more details).
</bodyText>
<sectionHeader confidence="0.990638" genericHeader="introduction">
2 Foundations
</sectionHeader>
<bodyText confidence="0.997765265625001">
The main focus of this paper is tactical generation,
i.e., the mapping of structures (usually represent-
ing semantic information eventually decorated with
some functional features) to strings using a lexicon
and a grammar. Thus stated, we view tactical gen-
eration as the inverse process of parsing. Informally,
EBL can be considered as an intelligent storage unit
of example-based generalized parts of the grammat-
ical search space determined via training by the tac-
tical generator.&apos; Processing of similar new input is
then reduced to simple lookup and matching oper-
ations, which circumvent re-computation of this al-
ready known search space.
We concentrate on constraint-based grammar for-
malism following a sign-based approach consider-
ing linguistic objects (i.e., words and phrases) as
utterance-meaning associations (Pollard and Sag,
1994). Thus viewed, a grammar is a formal state-
ment of the relation between utterances in a natu-
ral language and representations of their meanings
in some logical or other artificial language, where
such representations are usually called logical forms
(Shieber, 1993). The result of the tactical generator
is a feature structure (or a set of such structures in
the case of multiple paraphrases) containing among
others the input logical form, the computed string,
and a representation of the derivation.
In our current implementation we are using TDL,
a typed feature-based language and inference system
for constraint-based grammars (Krieger and Schafer,
1994). TDL allows the user to define hierarchically-
ordered types consisting of type and feature con-
straints. As shown later, a systematic use of type
information leads to a very compact representation
of the extracted data and supports an elegant but
efficient generalization step.
We are adapting a &amp;quot;flat&amp;quot; representation of log-
ical forms as described in (Kay, 1996; Copestake
et al., 1996). This is a minimally structured, but
descriptively adequate means to represent seman-
tic information, which allows for various types of
under-/overspecification, facilitates generation and
the specification of semantic transfer equivalences
&apos;In case a reversible grammar is used the parser can
even be used for processing the training corpus.
used for machine translation (Copestake et al., 1996;
Shemtov, 1996).2
Informally, a flat representation is obtained by
the use of extra variables which explicitly repre-
sent the relationship between the entities of a logical
form and scope information. In our current system
we are using the framework called minimal recur-
sion semantics (MRS) described in (Copestake et
al., 1996). Using their typed feature structure nota-
tion figure 1 displays a possible MRS of the string
&amp;quot;Sandy gives a chair to Kim&amp;quot; (abbreviated where
convenient).
The value of the feature LISZT is actually treated
like a set, i.e., the relative order of the elements is
immaterial. The feature HANDEL is used to repre-
sent scope information, and INDEX plays much the
same role as a lambda variable in conventional rep-
resentations (for more details see (Copestake et al.,
1996)).
</bodyText>
<figure confidence="0.986922142857143">
3 Overview of the method
semantic
sop
AM
retrieve &amp;
Instantiate
grammatical
processor
sub lurce
g mamar
extract &amp;
generalise
index
resulta
</figure>
<figureCaption confidence="0.999929">
Figure 3: A blueprint of the architecture.
</figureCaption>
<bodyText confidence="0.999938647058824">
The above figure displays the overall architecture
of the EBL learning method. The right-hand part
of the diagram shows the linguistic competence base
(LCB) and the left the EBL-based subgrammar pro-
cessing component (SGP).
LCB corresponds to the tactical component of a
general natural language generation system NLG. In
this paper we assume that the strategic component
of the NLG has already computed the MRS repre-
sentation of the information of an underlying com-
puter program. SGP consists of a training module
TM, an application module AM, and the subgram-
&apos;But note, our approach does not depend on a flat
representation of logical forms. However, in the case
of conventional representation form, the mechanisms for
indexing the trained structures would require more com-
plex abstract data types (see sec. 4 for more details).
</bodyText>
<page confidence="0.957274">
215
</page>
<equation confidence="0.969400333333333">
HANDEL hl
INDEX e2
HANDEL h9
hll BV x7
e2.1 , RESTR hl 0 ,
Some SCOPE hl 1
LISZT
SandyRel
K INST x5,
[HANDEL h4 HANDEL hl
EVENT e2
ACT x5
1
&apos; Temp Over [EVENT
HANDEL
PREPARG x6
GiveRel LUND x7
HANDEL hl 0 HANDEL h12
, Kimm iNsT x6
ChairRel[INsT x7 1, ARG v13 [HANDEL h14])
To PREP x6
</equation>
<figureCaption confidence="0.999891">
Figure 1: The MRS of the string &amp;quot;Sandy gives a chair to Kim&amp;quot;
</figureCaption>
<figure confidence="0.984438833333333">
LISZT ( SandyRel
[HANDEL h4], GiveRei [HANDEL hi], TempOver
[HANDEL h1], some [HAND h9] ,
[HANDEL hi 0], To [HANDEL hip], KimRei[HANDEL h/41)
EL
[ ChairRel
</figure>
<figureCaption confidence="0.999972">
Figure 2: The generalized MRS of the string &amp;quot;Sandy gives a chair to Kim&amp;quot;
</figureCaption>
<bodyText confidence="0.999939936507937">
mar, automatically determined by TM and applied
by AM.
Briefly, the flow of control is as follows: During
the training phase of the system, a new logical form
mrs is given as input to the LCB. After grammatical
processing, the resulting feature structure f s(mrs)
(i.e., a feature structure that contains among others
the input MRS, the computed string and a repre-
sentation of the derivation tree) is passed to TM.
TM extracts and generalizes the derivation tree of
f s(mrs), which we call the template templ(mrs)
of f s(mrs). templ(mrs) is then stored in a deci-
sion tree, where indices are computed from the MRS
found under the root of templ(mrs). During the ap-
plication phase, a new semantic input mrs&apos; is used
for the retrieval of the decision tree. If a candidate
template can be found and successfully instantiated,
the resulting feature structure f s(mrs&apos;) constitutes
the generation result of mrs&apos;
Thus described, the approach seems to facilitate
only exact retrieval and matching of a new seman-
tic input. However, before we describe how partial
matching is realized, we will demonstrate in more de-
tail the exact matching strategy using the example
MRS shown in figure 1.
Training phase The training module TM starts
right after the resulting feature structure f s for the
input MRS mrs has been computed. In the first
phase, TM extracts and generalizes the derivation
tree of f s, called the template of f s. Each node of
the template contains the rule name used in the cor-
responding derivation step and a generalization of
the local MRS. A generalized MRS is the abstrac-
tion of the LISZT value of a MRS where each element
only contains the (lexical semantic) type and HAN-
DEL information (the HANDEL information is used
for directing lexical choice (see below)).
In our example mrs, figure 2 displays the gener-
alized MRS mrs9 . For convenience, we will use the
more compact notation:
{(SandyRel h4), (GiveRel h1),
(TempOver h1), (Some h9),
(ChairRel h10), (To h12), (KimRel h14)}
Using this notation, figure 4 (see next page) dis-
plays the template templ(mr s) obtained from f s.
Note that it memorizes not only the rule application
structure of a successful process but also the way the
grammar mutually relates the compositional parts of
the input MRS.
In the next step of the training module TM, the
generalized MRS mrs g information of the root node
of templ(mrs) is used for building up an index in
a decision tree. Remember that the relative order
of the elements of a MRS is immaterial. For that
reason, the elements of mrs9 are alphabetically or-
dered, so that we can treat it as a sequence when
used as a new index in the decision tree.
The alphabetic ordering has two advantages.
Firstly, we can store different templates under a
common prefix, which allows for efficient storage and
retrieval. Secondly, it allows for a simple efficient
treatment of MRS as sets during the retrieval phase
of the application phase.
</bodyText>
<page confidence="0.995016">
216
</page>
<table confidence="0.932632818181818">
SubjhD
I (SandyRel h4). (GiveRel h1), (TempOver hi).
(Some h9). (ChairRel h10). (To h12). (KimRel h14))
ProperLe HCompNc
((SandyRel h4)) ((GiveRel hi). (TempOver hl)
(Some h9). (ChairRel hill). (To h12). (KimRel h14))
MvTo+DitransLe DetN
1(G i veRe( h 1 ). ((Some h9).
(TempOver hl)) (ChairRel h10))
DetSgLe IntrNLe
((Some h9)) I (ChairRel h10))
</table>
<figureCaption confidence="0.945336">
Figure 4: The template templ(mrs). Rule names
are in bold.
</figureCaption>
<bodyText confidence="0.980317">
Application phase The application module AM
basically performs the following steps:
</bodyText>
<listItem confidence="0.9424243125">
1. Retrieval: For a new MRS mrs&apos; we first con-
struct the alphabetically sorted generalized MRS
mrsy. mrs9 is then used as a path description
for traversing the decision tree. For reasons we
will explain soon, traversal is directed by, type
subsumption. Traversal is successful if mrs9 has
been completely processed and if the end node
in the decision tree contains a template. Note
that because of the alphabetic ordering, the rel-
ative order of the elements of new input mrs&apos; is
immaterial.
2. Expansion: A successfully retrieved template
tempi is expanded by deterministically applying
the rules denoted by the non-terminal elements
from the top downwards in the order specified
by tempi. In some sense, expansion just re-plays
</listItem>
<bodyText confidence="0.8531777">
the derivation obtained in the past. This will
result in a grammatically fully expanded fea-
ture structure, where only lexical specific infor-
mation is still missing. But note that through
structure sharing the terminal elements will al-
ready be constrained by syntactic information.&apos;
31t is possible to perform the expansion step off-line
as early as the training phase, in which case the applica-
tion phase can be sped up, however at the price of more
memory being taken up.
</bodyText>
<listItem confidence="0.937803947368421">
3. Lexical lookup: From each terminal element of
the unexpanded template tempi the type and
HANDEL information is used to select the cor-
responding element from the input MRS mrs&apos;
(note that in general the MRS elements of the
mrs&apos; are much more constrained than their cor-
responding elements in the generalized MRS
mrs 9). The chosen input MRS element is then
used for performing lexical lookup, where lexi-
cal elements are indexed by their relation name.
In general this will lead to a set of lexical can-
didates.
4. Lexical instantiation: In the last step of the ap-
plication phase, the set of selected lexical el-
ements is unified with the constraints of the
terminal elements in the order specified by the
terminal yield. We also call this step terminal-
matching. In our current system terminal-
matching is performed from left to right. Since
</listItem>
<bodyText confidence="0.961194411764706">
the ordering of the terminal yield is given by the
template, it is also possible to follow other se-
lection strategies, e.g., a semantic head-driven
strategy, which could lead to more efficient
terminal-matching, because the head element is
supposed to provide selectional restriction in-
formation for its dependents.
A template together with its corresponding index
describes all sentences of the language that share
the same derivation and whose MRS are consistent
with that of the index. Furthermore, the index and
the MRS of a template together define a normaliza-
tion for the permutation of the elements of a new
input MRS. The proposed EBL method guarantees
soundness because retaining and applying the orig-
inal derivation in a template enforces the full con-
straints of the original grammar.
Achieving more generality So far, the applica-
tion phase will only be able to re-use templates for
a semantic input which has the same semantic type
information. However, it is possible to achieve more
generality, if we apply a further abstraction step on
a generalized MRS. This is simply achieved by se-
lecting a supertype of a MRS element instead of the
given specialized type.
The type abstraction step is based on the stan-
dard assumption that the word-specific lexical se-
mantic types can be grouped into classes represent-
ing morpho-syntactic paradigms. These classes de-
fine the upper bounds for the abstraction process. In
our current system, these upper bounds are directly
used as the supertypes to be considered during the
type abstraction step. More precisely, for each el-
ement x of a generalized MRS mrsg it is checked
</bodyText>
<table confidence="0.745014714285714">
DetN
((To h12). (KimRel h14))
HCompNc
( (GiveRel h1). (TempOver hi).
(Some h9). (ChairRel
PrepNoModLe ProperLe
((To h12)) 1(KimRel h14))
</table>
<page confidence="0.997584">
217
</page>
<bodyText confidence="0.991517714285714">
whether its type Tx is subsumed by an upper bound
T, (we assume disjoint sets). Only if this is the case,
Ts replaces Tz in mrs9 .4 Applying this type abstrac-
tion strategy on the MRS of figure 1, we obtain:
{ (Named h4), (ActUndPrep h1),
(TempOver h1), (Some h9),
(RegNom h10), (To h12), (Named h14)}
</bodyText>
<figure confidence="0.775689111111111">
SubjhD
((Named h4). (ActUndPrep hi). (TempOver hi),
(Some h9). (RegNom h10), (To h12). (Named h14)(
ProperLe HCompNc
((Named h4)) ((ActUndPrep hi). (TempOver hi)
(Some h9), (RegNom hit)), (To h12). (Named h 14) )
My&apos;ro+DitransLe
((ActUndPrep hi),
(TempOver h I ))
</figure>
<figureCaption confidence="0.993076">
Figure 5: The more generalized derivation tree dtg
of dt.
</figureCaption>
<bodyText confidence="0.999460923076923">
where e.g., NAMED is the common supertype of
SANDYREL and KimREL, and ACTUNDPREP is the
supertype of GIVEREL. Figure 5 shows the tem-
plate tern p19 obtained from f s using the more gen-
eral MRS information. Note, that the MRS of the
root node is used for building up an index in the
decision tree.
Now, if retrieval of the decision tree is directed
by type subsumption, the same template can be re-
trieved and potentially instantiated for a wider range
of new MRS input, namely for those which are type
compatible wrt. subsumption relation. Thus, the
template templ9 can now be used to generate, e.g.,
the string &amp;quot;Kim gives a table to Peter&amp;quot;, as well as
the string &amp;quot;Noam donates a book to Peter&amp;quot;.
However, it will not be able to generate a sentence
like &amp;quot;A man gives a book to Kim&amp;quot;, since the retrieval
40f course, if a very fine-grained lexical semantic type
hierarchy is defined then a more careful selection would
be possible to obtained different degrees of type abstrac-
tion and to achieve a more domain-sensitive determina-
tion of the subgrammars. However, more complex type
abstraction strategies are then needed which would be
able to find appropriate supertypes automatically.
phase will already fail. In the next section, we will
show how to overcome even this kind of restriction.
</bodyText>
<sectionHeader confidence="0.99419" genericHeader="method">
4 Partial Matching
</sectionHeader>
<bodyText confidence="0.97898075">
The core idea behind partial matching is that in case
an exact match of an input MRS fails we want at
least as many subparts as possible to be instantiated.
Since the instantiated template of a MRS subpart
corresponds to a phrasal sign, we also call it a phrasal
template. For example, assuming that the training
phase has only to be performed for the example in
figure 1, then for the MRS of &amp;quot;A man gives a book to
Kim&amp;quot;, a partial match would generate the strings &amp;quot;a
man&amp;quot; and &amp;quot;gives a book to Kim&amp;quot; .5 The instantiated
phrasal templates are then combined by the tactical
component to produce larger units (if possible, see
below).
Extended training phase The training module
is adapted as follows: Starting from a template
tempi obtained for the training example in the man-
ner described above, we extract recursively all pos-
sible subtrees tempi, also called phrasal templates.
Next, each phrasal template is inserted in the deci-
sion tree in the way described above.
It is possible to direct the subtree extraction pro-
cess with the application of filters, which are ap-
plied to the whole remaining subtree in each recur-
sive step. By using these filters it is possible to re-
strict the range of structural properties of candidate
phrasal templates (e.g., extract only saturated NPs,
or subtrees having at least two daughters, or sub-
trees which have no immediate recursive structures).
These filters serve the same means as the &amp;quot;chunking
criteria&amp;quot; described in (Rayner and Carter, 1996).
During the training phase it is recognized for each
phrasal template tempi, whether the decision tree
already contains a path pointing to a previously ex-
tracted and already stored, phrasal template tempts,
such that tempi, = templs. In that case, tempi, is
not inserted and the recursion stops at that branch.
Extended application phase For the applica-
tion module, only the retrieval operation of the de-
cision tree need be adapted.
Remember that the input of the retrieval opera-
tion is the sorted generalized MRS mrs9 of the input
MRS mrs. Therefore, mrs9 can be handled like a
sequence. The task of the retrieval operation in the
case of a partial match is now to potentially find all
subsequences of mrs9 which lead to a template.
51f we would allow for an exhaustive partial match
(see below) then the strings &amp;quot;a book&amp;quot; and &amp;quot;Kim&amp;quot; would
additionally be generated.
</bodyText>
<table confidence="0.961225272727273">
((To h12), (Name h14))
HCompNe
((ActUndPrep h 1 ). (TempOver hi).
(Some h9). (RegNom h10)(
PrepNoModLe ProperLe
((To hl2)) ((Name h14))
DetN
((Some h9),
(RegNom h10)(
DetSgLe IntrNLe
((Some h9)) ((RegNom h10))
</table>
<page confidence="0.99757">
218
</page>
<bodyText confidence="0.999708965517241">
In case of exact matching strategy, the decision
tree must be visited only once for a new input. In
the case of partial matching, however, the decision
tree describes only possible prefixes for a new input.
Hence, we have to recursively repeat retrieval of the
decision tree as long as the remaining suffix is not
empty. In other words, the decision tree is now a
finite representation of an infinite structure, because
implicitly, each endpoint of an index bears a pointer
to the root of the decision tree.
Assuming that the following template/index pairs
have been inserted into the decision tree: (ab, ti),
(abcd,t2), (bcd,t3). Then retrieval using the path
abed will return all three templates, retrieval using
aabbcd will return template t1 and t3, and abc will
only return t1.6
Interleaving with normal processing Our
EBL method can easily be integrated with normal
processing, because each instantiated template can
be used directly as an already found sub-solution.
In case of an agenda-driven chart generator of the
kind described in (Neumann, 1994a; Kay, 1996), an
instantiated template can be directly added as a
passive edge to the generator&apos;s agenda. If passive
edges with a wider span are given higher priority
than those with a smaller span, the tactical gener-
ator would try to combine the largest derivations
before smaller ones, i.e., it would prefer those struc-
tures determined by EBL.
</bodyText>
<sectionHeader confidence="0.995281" genericHeader="method">
5 Implementation
</sectionHeader>
<bodyText confidence="0.995341040816327">
The EBL method just described has been fully im-
plemented and tested with a broad coverage HPSG-
based English grammar including more than 2000
fully specified lexical entries.7 The TDL grammar
formalism is very powerful, supporting distributed
disjunction, full negation, as well as full boolean type
logic.
In our current system, an efficient chart-based
bidirectional parser is used for performing the train-
ing phase. During training, the user can interac-
tively select which of the parser&apos;s readings should
be considered by the EBL module. In this way the
user can control which sort of structural ambigui-
ties should be avoided because they are known to
cause misunderstandings. For interleaving the EBL
application phase with normal processing a first pro-
61t is possible to parameterize our system to per-
form an exhaustive or a non-exhaustive strategy. In the
non-exhaustive mode, the longest matching prefixes are
preferred.
&apos;This grammar has been developed at CSLI, Stan-
ford, and kindly be provided to the author.
totype of a chart generator has been implemented
using the same grammar as used for parsing.
First tests has been carried out using a small test
set of 179 sentences. Currently, a parser is used for
processing the test set during training. Generation
of the extracted templates is performed solely by
the EBL application phase (i.e., we did not consid-
ered integration of EBL and chart generation). The
application phase is very efficient. The average pro-
cessing time for indexing and instantiation of a sen-
tence level template (determined through parsing) of
an input MRS is approximately one second.&apos; Com-
pared to parsing the corresponding string the factor
of speed up is between 10 to 20. A closer look to
the four basic EBL-generation steps: indexing, in-
stantiation, lexical lookup, and terminal matching
showed that the latter is the most expensive one (up
to 70% of computing time). The main reasons are
that 1.) lexical lookup often returns several lexical
readings for an MRS element (which introduces lex-
ical non-determinism) and 2.) the lexical elements
introduce most of the disjunctive constraints which
makes unification very complex. Currently, termi-
nal matching is performed left to right. However,
we hope to increase the efficiency of this step by us-
ing head-oriented strategies, since this might help to
re-solve disjunctive constraints as early as possible.
</bodyText>
<sectionHeader confidence="0.999636" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999988238095238">
The only other approach I am aware of which
also considers EBL for NLG is (Samuelsson, 1995a;
Samuelsson, 1995b). However, he focuses on the
compilation of a logic grammar using LR-compiling
techniques, where EBL-related methods are used to
optimize the compiled LR tables, in order to avoid
spurious non-determinisms during normal genera-
tion. He considers neither the extraction of a spe-
cialized grammar for supporting controlled language
generation, nor strong integration with the normal
generator.
However, these properties are very important for
achieving high applicability. Automatic grammar
extraction is worthwhile because it can be used to
support the definition of a controlled domain-specific
language use on the basis of training with a gen-
eral source grammar. Furthermore, in case exact
matching is requested only the application module
is needed for processing the subgrammar. In case
of normal processing, our EBL method serves as a
speed-up mechanism for those structures which have
</bodyText>
<footnote confidence="0.874743333333333">
8EBL-based generation of all possible templates of an
input MRS is less than 2 seconds. The tests have been
performed using a Sun UltraSparc.
</footnote>
<page confidence="0.998256">
219
</page>
<bodyText confidence="0.996703555555555">
&amp;quot;actually been used or uttered&amp;quot;. However, complete-
ness is preserved.
We view generation systems which are based on
&amp;quot;canned text&amp;quot; and linguistically-based systems sim-
ply as two endpoints of a contiguous scale of possible
system architectures (see also (Dale et al., 1994)).
Thus viewed, our approach is directed towards the
automatic creation of application-specific generation
systems.
</bodyText>
<sectionHeader confidence="0.960768" genericHeader="conclusions">
7 Conclusion and Future Directions
</sectionHeader>
<bodyText confidence="0.999570882352941">
We have presented a method of automatic extrac-
tion of subgrammars for controlling and speeding up
natural language generation (NLG). The method is
based on explanation-based learning (EBL), which
has already been successfully applied for parsing.
We showed how the method can be used to train
a system to a specific use of grammatical and lexical
usage.
We already have implemented a similar EBL
method for parsing, which supports on-line learn-
ing as well as statistical-based management of ex-
tracted data. In the future we plan to combine EBL-
based generation and parsing to one uniform EBL
approach usable for high-level performance strate-
gies which are based on a strict interleaving of pars-
ing and generation (cf. (Neumann and van Noord,
1994; Neumann, 1994a)).
</bodyText>
<sectionHeader confidence="0.992296" genericHeader="acknowledgments">
8 Acknowledgement
</sectionHeader>
<bodyText confidence="0.9998316">
The research underlying this paper was supported
by a research grant from the German Bundesmin-
isterium fiir Bildung, Wissenschaft, Forschung
und Technologie (BMB+F) to the DFKI project
PARADIME FKZ ITW 9704.
I would like to thank the HPSG people from CSLI,
Stanford for their kind support and for providing the
HPSG-based English grammar. In particular I want
to thank Dan Flickinger and Ivan Sag. Many thanks
also to Walter Kasper for fruitful discussions.
</bodyText>
<sectionHeader confidence="0.998132" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9994985">
Copestake, A., D. Flickinger, R. Malouf, S. Riehe-
mann, and I. Sag. 1996. Translation using
minimal recursion semantics. In Proceedings,
6th International Conference on Theoretical and
Methodological Issues in Machine Translation.
Dale, R., W. Finkler, R. Kittredge, N. Lenke,
G. Neumann, C. Peters, and M. Stede. 1994. Re-
port from working group 2: Lexicalization and
architecture. In W. Hoeppner, H. Horacek, and
J. Moore, editors, Principles of Natural Language
Generation, Dagstuhl-Seminar-Report; 93. Schlo13
Dagstuhl, Saarland, Germany, Europe, pages 30-
39.
Kay, M. 1996. Chart generation. In 34th An-
nual Meeting of the Association for Computa-
tional Linguistics, Santa Cruz, Ca.
Krieger, Hans-Ulrich and Ulrich Schafer. 1994.
TD.C—a type description language for constraint-
based grammars. In Proceedings of the 15th Inter-
national Conference on Computational Linguis-
tics, COLING-94, pages 893-899.
Minton, S., J. G. Carbonell, C. A. Knoblock,
D. R.Kuoklca, 0. Etzioni, and Y.Gi. 1989.
Explanation-based learning: A problem solving
perspective. Artificial Intelligence, 40:63-115.
Mitchell, T., R. Keller, and S. Kedar-Cabelli.
1986. Explanation-based generalization: a uni-
fying view. Machine Learning, 1:47-80.
Neumann, G. 1994a. Application of explanation-
based learning for efficient processing of constraint
based grammars. In Proceedings of the Tenth
IEEE Conference on Artificial Intelligence for Ap-
plications, pages 208-215, San Antonio, Texas,
March.
Neumann, G. 1994b. A Uniform Computational
Model for Natural Language Parsing and Gener-
ation. Ph.D. thesis, Universitat des Saarlandes,
Germany, Europe, November.
Neumann, G. and G. van Noord. 1994. Re-
versibility and self-monitoring in natural language
generation. In Tomek Strzalkowski, editor, Re-
versible Grammar in Natural Language Process-
ing. Kluwer, pages 59-96.
Pollard, C. and I. M. Sag. 1994. Head-Driven
Phrase Structure Grammar. Center for the Study
of Language and Information Stanford.
Rayner, M. 1988. Applying explanation-based gen-
eralization to natural language processing. In Pro-
ceedings of the International Conference on Fifth
Generation Computer Systems, Tokyo.
Rayner, M. and D. Carter. 1996. Fast parsing us-
ing pruning and grammar specialization. In 34th
Annual Meeting of the Association for Computa-
tional Linguistics, Morristown, New Jersey.
Samuelsson, C. 1994. Fast Natural-Language Pars-
ing Using Explanation-Based Learning. Ph.D.
thesis, Swedish Institute of Computer Science,
Kista, Sweden, Europe.
Samuelsson, C. 1995a. An efficient algorithm for
surface generation. In Proceedings of the 14th In-
ternational Joint Conference on Artificial Intelli-
gence, pages 1414-1419, Montreal, Canada.
</reference>
<page confidence="0.957454">
220
</page>
<reference confidence="0.99926376">
Samuelsson, C. 1995b. Example-based optimiza-
tion of surface-generation tables. In Proceedings
of Recent Advances in Natural Language Process-
ing, Velingrad, Bulgaria, Europe.
Samuelsson, C. and M. Rayner. 1991. Quantita-
tive evaluation of explanation-based learning as
an optimization tool for a large-scale natural lan-
guage system. In IJCAI-91, pages 609-615, Syd-
ney, Australia.
Shemtov, H. 1996. Generation of Paraphrases from
Ambiguous Logical Forms. In Proceedings of the
16th International Conference on Computational
Linguistics (COLING), pages 919-924, Kopen-
hagen, Denmark, Europe.
Shieber, S. M. 1993. The problem of logical-form
equivalence. Computational Linguistics, 19:179-
190.
Srinivas, B. and A. Joshi. 1995. Some novel ap-
plications of explanation-based learning to pars-
ing lexicalized tree-adjoining grammars. In 38th
Annual Meeting of the Association for Computa-
tional Linguistics, Cambridge, MA.
van Harmelen, F. and A. Bundy. 1988. Explanation-
based generalization=partial evaluation. Artifi-
cial Intelligence, 36:401-412.
</reference>
<page confidence="0.998333">
221
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.779478">
<title confidence="0.999568">Applying Explanation-based Learning to Control and Speeding-up Natural Language Generation</title>
<author confidence="0.996177">Giinter Neumann</author>
<affiliation confidence="0.998622">DFKI GmbH</affiliation>
<address confidence="0.989449">Stuhlsatzenhausweg 3 66123 Saarbriicken, Germany</address>
<abstract confidence="0.984485615384615">This paper presents a method for the automatic extraction of subgrammars to control and speeding-up natural language generation NLG. The method is based on explanation-based learning EBL. The main advantage for the proposed new method for NLG is that the complexity of the grammatical decision making process during NLG can be vastly reduced, because the EBL method supports the adaption of a NLG system to a particular use of a language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>R Malouf</author>
<author>S Riehemann</author>
<author>I Sag</author>
</authors>
<title>Translation using minimal recursion semantics.</title>
<date>1996</date>
<booktitle>In Proceedings, 6th International Conference on Theoretical and Methodological Issues in Machine Translation.</booktitle>
<contexts>
<context position="5622" citStr="Copestake et al., 1996" startWordPosition="875" endWordPosition="878">input logical form, the computed string, and a representation of the derivation. In our current implementation we are using TDL, a typed feature-based language and inference system for constraint-based grammars (Krieger and Schafer, 1994). TDL allows the user to define hierarchicallyordered types consisting of type and feature constraints. As shown later, a systematic use of type information leads to a very compact representation of the extracted data and supports an elegant but efficient generalization step. We are adapting a &amp;quot;flat&amp;quot; representation of logical forms as described in (Kay, 1996; Copestake et al., 1996). This is a minimally structured, but descriptively adequate means to represent semantic information, which allows for various types of under-/overspecification, facilitates generation and the specification of semantic transfer equivalences &apos;In case a reversible grammar is used the parser can even be used for processing the training corpus. used for machine translation (Copestake et al., 1996; Shemtov, 1996).2 Informally, a flat representation is obtained by the use of extra variables which explicitly represent the relationship between the entities of a logical form and scope information. In o</context>
</contexts>
<marker>Copestake, Flickinger, Malouf, Riehemann, Sag, 1996</marker>
<rawString>Copestake, A., D. Flickinger, R. Malouf, S. Riehemann, and I. Sag. 1996. Translation using minimal recursion semantics. In Proceedings, 6th International Conference on Theoretical and Methodological Issues in Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>W Finkler</author>
<author>R Kittredge</author>
<author>N Lenke</author>
<author>G Neumann</author>
<author>C Peters</author>
<author>M Stede</author>
</authors>
<title>Report from working group 2: Lexicalization and architecture.</title>
<date>1994</date>
<booktitle>Principles of Natural Language Generation, Dagstuhl-Seminar-Report; 93. Schlo13 Dagstuhl,</booktitle>
<pages>30--39</pages>
<editor>In W. Hoeppner, H. Horacek, and J. Moore, editors,</editor>
<location>Saarland, Germany, Europe,</location>
<contexts>
<context position="25366" citStr="Dale et al., 1994" startWordPosition="4149" endWordPosition="4152">ct matching is requested only the application module is needed for processing the subgrammar. In case of normal processing, our EBL method serves as a speed-up mechanism for those structures which have 8EBL-based generation of all possible templates of an input MRS is less than 2 seconds. The tests have been performed using a Sun UltraSparc. 219 &amp;quot;actually been used or uttered&amp;quot;. However, completeness is preserved. We view generation systems which are based on &amp;quot;canned text&amp;quot; and linguistically-based systems simply as two endpoints of a contiguous scale of possible system architectures (see also (Dale et al., 1994)). Thus viewed, our approach is directed towards the automatic creation of application-specific generation systems. 7 Conclusion and Future Directions We have presented a method of automatic extraction of subgrammars for controlling and speeding up natural language generation (NLG). The method is based on explanation-based learning (EBL), which has already been successfully applied for parsing. We showed how the method can be used to train a system to a specific use of grammatical and lexical usage. We already have implemented a similar EBL method for parsing, which supports on-line learning a</context>
</contexts>
<marker>Dale, Finkler, Kittredge, Lenke, Neumann, Peters, Stede, 1994</marker>
<rawString>Dale, R., W. Finkler, R. Kittredge, N. Lenke, G. Neumann, C. Peters, and M. Stede. 1994. Report from working group 2: Lexicalization and architecture. In W. Hoeppner, H. Horacek, and J. Moore, editors, Principles of Natural Language Generation, Dagstuhl-Seminar-Report; 93. Schlo13 Dagstuhl, Saarland, Germany, Europe, pages 30-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Chart generation.</title>
<date>1996</date>
<booktitle>In 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Santa Cruz, Ca.</location>
<contexts>
<context position="3571" citStr="Kay, 1996" startWordPosition="564" endWordPosition="565">ation can be vastly reduced, because the EBL method supports adaption of a NLG system to a particular language use. The core properties of this new method are: • prototypical occuring grammatical constructions can automatically be extracted; • generation of these constructions is vastly sped up using simple but efficient mechanisms; • the new method supports partial matching, in the sense that new semantic input need not be completely covered by previously trained examples; • it can easily be integrated with recently developed chart-based generators as described in, 214 e.g., (Neumann, 1994b; Kay, 1996; Shemtov, 1996). The method has been completely implemented and tested with a broad-coverage HPSG-based grammar for English (see sec. 5 for more details). 2 Foundations The main focus of this paper is tactical generation, i.e., the mapping of structures (usually representing semantic information eventually decorated with some functional features) to strings using a lexicon and a grammar. Thus stated, we view tactical generation as the inverse process of parsing. Informally, EBL can be considered as an intelligent storage unit of example-based generalized parts of the grammatical search space </context>
<context position="5597" citStr="Kay, 1996" startWordPosition="873" endWordPosition="874">others the input logical form, the computed string, and a representation of the derivation. In our current implementation we are using TDL, a typed feature-based language and inference system for constraint-based grammars (Krieger and Schafer, 1994). TDL allows the user to define hierarchicallyordered types consisting of type and feature constraints. As shown later, a systematic use of type information leads to a very compact representation of the extracted data and supports an elegant but efficient generalization step. We are adapting a &amp;quot;flat&amp;quot; representation of logical forms as described in (Kay, 1996; Copestake et al., 1996). This is a minimally structured, but descriptively adequate means to represent semantic information, which allows for various types of under-/overspecification, facilitates generation and the specification of semantic transfer equivalences &apos;In case a reversible grammar is used the parser can even be used for processing the training corpus. used for machine translation (Copestake et al., 1996; Shemtov, 1996).2 Informally, a flat representation is obtained by the use of extra variables which explicitly represent the relationship between the entities of a logical form an</context>
<context position="21193" citStr="Kay, 1996" startWordPosition="3490" endWordPosition="3491">ars a pointer to the root of the decision tree. Assuming that the following template/index pairs have been inserted into the decision tree: (ab, ti), (abcd,t2), (bcd,t3). Then retrieval using the path abed will return all three templates, retrieval using aabbcd will return template t1 and t3, and abc will only return t1.6 Interleaving with normal processing Our EBL method can easily be integrated with normal processing, because each instantiated template can be used directly as an already found sub-solution. In case of an agenda-driven chart generator of the kind described in (Neumann, 1994a; Kay, 1996), an instantiated template can be directly added as a passive edge to the generator&apos;s agenda. If passive edges with a wider span are given higher priority than those with a smaller span, the tactical generator would try to combine the largest derivations before smaller ones, i.e., it would prefer those structures determined by EBL. 5 Implementation The EBL method just described has been fully implemented and tested with a broad coverage HPSGbased English grammar including more than 2000 fully specified lexical entries.7 The TDL grammar formalism is very powerful, supporting distributed disjunc</context>
</contexts>
<marker>Kay, 1996</marker>
<rawString>Kay, M. 1996. Chart generation. In 34th Annual Meeting of the Association for Computational Linguistics, Santa Cruz, Ca.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans-Ulrich Krieger</author>
<author>Ulrich Schafer</author>
</authors>
<title>TD.C—a type description language for constraintbased grammars.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics, COLING-94,</booktitle>
<pages>893--899</pages>
<contexts>
<context position="5237" citStr="Krieger and Schafer, 1994" startWordPosition="813" endWordPosition="816">ormal statement of the relation between utterances in a natural language and representations of their meanings in some logical or other artificial language, where such representations are usually called logical forms (Shieber, 1993). The result of the tactical generator is a feature structure (or a set of such structures in the case of multiple paraphrases) containing among others the input logical form, the computed string, and a representation of the derivation. In our current implementation we are using TDL, a typed feature-based language and inference system for constraint-based grammars (Krieger and Schafer, 1994). TDL allows the user to define hierarchicallyordered types consisting of type and feature constraints. As shown later, a systematic use of type information leads to a very compact representation of the extracted data and supports an elegant but efficient generalization step. We are adapting a &amp;quot;flat&amp;quot; representation of logical forms as described in (Kay, 1996; Copestake et al., 1996). This is a minimally structured, but descriptively adequate means to represent semantic information, which allows for various types of under-/overspecification, facilitates generation and the specification of seman</context>
</contexts>
<marker>Krieger, Schafer, 1994</marker>
<rawString>Krieger, Hans-Ulrich and Ulrich Schafer. 1994. TD.C—a type description language for constraintbased grammars. In Proceedings of the 15th International Conference on Computational Linguistics, COLING-94, pages 893-899.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Minton</author>
<author>J G Carbonell</author>
<author>C A Knoblock</author>
<author>D R Kuoklca</author>
</authors>
<title>Explanation-based learning: A problem solving perspective.</title>
<date>1989</date>
<journal>Artificial Intelligence,</journal>
<pages>40--63</pages>
<contexts>
<context position="832" citStr="Minton et al., 1989" startWordPosition="122" endWordPosition="125">sents a method for the automatic extraction of subgrammars to control and speeding-up natural language generation NLG. The method is based on explanation-based learning EBL. The main advantage for the proposed new method for NLG is that the complexity of the grammatical decision making process during NLG can be vastly reduced, because the EBL method supports the adaption of a NLG system to a particular use of a language. 1 Introduction In recent years, a Machine Learning technique known as Explanation-based Learning EBL (Mitchell, Keller, and Kedar-Cabelli, 1986; van Harmelen and Bundy, 1988; Minton et al., 1989) has successfully been applied to control and speeding-up natural language parsing (Rayner, 1988; Samuelsson and Rayner, 1991; Neumann, 1994a; Samuelsson, 1994; Srinivas and Joshi, 1995; Rayner and Carter, 1996). The core idea of EBL is to transform the derivations (or explanations) computed by a problem solver (e.g., a parser) to some generalized and compact forms, which can be used very efficiently for solving similar problems in the future. EBL has primarily been used for parsing to automatically specialize a given source grammar to a specific domain. In that case, EBL is used as a method f</context>
</contexts>
<marker>Minton, Carbonell, Knoblock, Kuoklca, 1989</marker>
<rawString>Minton, S., J. G. Carbonell, C. A. Knoblock, D. R.Kuoklca, 0. Etzioni, and Y.Gi. 1989. Explanation-based learning: A problem solving perspective. Artificial Intelligence, 40:63-115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitchell</author>
<author>R Keller</author>
<author>S Kedar-Cabelli</author>
</authors>
<title>Explanation-based generalization: a unifying view.</title>
<date>1986</date>
<booktitle>Machine Learning,</booktitle>
<pages>1--47</pages>
<contexts>
<context position="780" citStr="Mitchell, Keller, and Kedar-Cabelli, 1986" startWordPosition="112" endWordPosition="116">6123 Saarbriicken, Germany neumannOdfki.uni-sb.de Abstract This paper presents a method for the automatic extraction of subgrammars to control and speeding-up natural language generation NLG. The method is based on explanation-based learning EBL. The main advantage for the proposed new method for NLG is that the complexity of the grammatical decision making process during NLG can be vastly reduced, because the EBL method supports the adaption of a NLG system to a particular use of a language. 1 Introduction In recent years, a Machine Learning technique known as Explanation-based Learning EBL (Mitchell, Keller, and Kedar-Cabelli, 1986; van Harmelen and Bundy, 1988; Minton et al., 1989) has successfully been applied to control and speeding-up natural language parsing (Rayner, 1988; Samuelsson and Rayner, 1991; Neumann, 1994a; Samuelsson, 1994; Srinivas and Joshi, 1995; Rayner and Carter, 1996). The core idea of EBL is to transform the derivations (or explanations) computed by a problem solver (e.g., a parser) to some generalized and compact forms, which can be used very efficiently for solving similar problems in the future. EBL has primarily been used for parsing to automatically specialize a given source grammar to a spec</context>
</contexts>
<marker>Mitchell, Keller, Kedar-Cabelli, 1986</marker>
<rawString>Mitchell, T., R. Keller, and S. Kedar-Cabelli. 1986. Explanation-based generalization: a unifying view. Machine Learning, 1:47-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Neumann</author>
</authors>
<title>Application of explanationbased learning for efficient processing of constraint based grammars.</title>
<date>1994</date>
<booktitle>In Proceedings of the Tenth IEEE Conference on Artificial Intelligence for Applications,</booktitle>
<pages>208--215</pages>
<location>San Antonio, Texas,</location>
<contexts>
<context position="972" citStr="Neumann, 1994" startWordPosition="143" endWordPosition="144">nation-based learning EBL. The main advantage for the proposed new method for NLG is that the complexity of the grammatical decision making process during NLG can be vastly reduced, because the EBL method supports the adaption of a NLG system to a particular use of a language. 1 Introduction In recent years, a Machine Learning technique known as Explanation-based Learning EBL (Mitchell, Keller, and Kedar-Cabelli, 1986; van Harmelen and Bundy, 1988; Minton et al., 1989) has successfully been applied to control and speeding-up natural language parsing (Rayner, 1988; Samuelsson and Rayner, 1991; Neumann, 1994a; Samuelsson, 1994; Srinivas and Joshi, 1995; Rayner and Carter, 1996). The core idea of EBL is to transform the derivations (or explanations) computed by a problem solver (e.g., a parser) to some generalized and compact forms, which can be used very efficiently for solving similar problems in the future. EBL has primarily been used for parsing to automatically specialize a given source grammar to a specific domain. In that case, EBL is used as a method for adapting a general grammar and/or parser to the sub-language defined by a suitable training corpus (Rayner and Carter, 1996). A specializ</context>
<context position="3559" citStr="Neumann, 1994" startWordPosition="562" endWordPosition="563">l language generation can be vastly reduced, because the EBL method supports adaption of a NLG system to a particular language use. The core properties of this new method are: • prototypical occuring grammatical constructions can automatically be extracted; • generation of these constructions is vastly sped up using simple but efficient mechanisms; • the new method supports partial matching, in the sense that new semantic input need not be completely covered by previously trained examples; • it can easily be integrated with recently developed chart-based generators as described in, 214 e.g., (Neumann, 1994b; Kay, 1996; Shemtov, 1996). The method has been completely implemented and tested with a broad-coverage HPSG-based grammar for English (see sec. 5 for more details). 2 Foundations The main focus of this paper is tactical generation, i.e., the mapping of structures (usually representing semantic information eventually decorated with some functional features) to strings using a lexicon and a grammar. Thus stated, we view tactical generation as the inverse process of parsing. Informally, EBL can be considered as an intelligent storage unit of example-based generalized parts of the grammatical s</context>
<context position="21180" citStr="Neumann, 1994" startWordPosition="3488" endWordPosition="3489">t of an index bears a pointer to the root of the decision tree. Assuming that the following template/index pairs have been inserted into the decision tree: (ab, ti), (abcd,t2), (bcd,t3). Then retrieval using the path abed will return all three templates, retrieval using aabbcd will return template t1 and t3, and abc will only return t1.6 Interleaving with normal processing Our EBL method can easily be integrated with normal processing, because each instantiated template can be used directly as an already found sub-solution. In case of an agenda-driven chart generator of the kind described in (Neumann, 1994a; Kay, 1996), an instantiated template can be directly added as a passive edge to the generator&apos;s agenda. If passive edges with a wider span are given higher priority than those with a smaller span, the tactical generator would try to combine the largest derivations before smaller ones, i.e., it would prefer those structures determined by EBL. 5 Implementation The EBL method just described has been fully implemented and tested with a broad coverage HPSGbased English grammar including more than 2000 fully specified lexical entries.7 The TDL grammar formalism is very powerful, supporting distri</context>
</contexts>
<marker>Neumann, 1994</marker>
<rawString>Neumann, G. 1994a. Application of explanationbased learning for efficient processing of constraint based grammars. In Proceedings of the Tenth IEEE Conference on Artificial Intelligence for Applications, pages 208-215, San Antonio, Texas, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Neumann</author>
</authors>
<title>A Uniform Computational Model for Natural Language Parsing and Generation.</title>
<date>1994</date>
<booktitle>Ph.D. thesis, Universitat des Saarlandes,</booktitle>
<location>Germany, Europe,</location>
<contexts>
<context position="972" citStr="Neumann, 1994" startWordPosition="143" endWordPosition="144">nation-based learning EBL. The main advantage for the proposed new method for NLG is that the complexity of the grammatical decision making process during NLG can be vastly reduced, because the EBL method supports the adaption of a NLG system to a particular use of a language. 1 Introduction In recent years, a Machine Learning technique known as Explanation-based Learning EBL (Mitchell, Keller, and Kedar-Cabelli, 1986; van Harmelen and Bundy, 1988; Minton et al., 1989) has successfully been applied to control and speeding-up natural language parsing (Rayner, 1988; Samuelsson and Rayner, 1991; Neumann, 1994a; Samuelsson, 1994; Srinivas and Joshi, 1995; Rayner and Carter, 1996). The core idea of EBL is to transform the derivations (or explanations) computed by a problem solver (e.g., a parser) to some generalized and compact forms, which can be used very efficiently for solving similar problems in the future. EBL has primarily been used for parsing to automatically specialize a given source grammar to a specific domain. In that case, EBL is used as a method for adapting a general grammar and/or parser to the sub-language defined by a suitable training corpus (Rayner and Carter, 1996). A specializ</context>
<context position="3559" citStr="Neumann, 1994" startWordPosition="562" endWordPosition="563">l language generation can be vastly reduced, because the EBL method supports adaption of a NLG system to a particular language use. The core properties of this new method are: • prototypical occuring grammatical constructions can automatically be extracted; • generation of these constructions is vastly sped up using simple but efficient mechanisms; • the new method supports partial matching, in the sense that new semantic input need not be completely covered by previously trained examples; • it can easily be integrated with recently developed chart-based generators as described in, 214 e.g., (Neumann, 1994b; Kay, 1996; Shemtov, 1996). The method has been completely implemented and tested with a broad-coverage HPSG-based grammar for English (see sec. 5 for more details). 2 Foundations The main focus of this paper is tactical generation, i.e., the mapping of structures (usually representing semantic information eventually decorated with some functional features) to strings using a lexicon and a grammar. Thus stated, we view tactical generation as the inverse process of parsing. Informally, EBL can be considered as an intelligent storage unit of example-based generalized parts of the grammatical s</context>
<context position="21180" citStr="Neumann, 1994" startWordPosition="3488" endWordPosition="3489">t of an index bears a pointer to the root of the decision tree. Assuming that the following template/index pairs have been inserted into the decision tree: (ab, ti), (abcd,t2), (bcd,t3). Then retrieval using the path abed will return all three templates, retrieval using aabbcd will return template t1 and t3, and abc will only return t1.6 Interleaving with normal processing Our EBL method can easily be integrated with normal processing, because each instantiated template can be used directly as an already found sub-solution. In case of an agenda-driven chart generator of the kind described in (Neumann, 1994a; Kay, 1996), an instantiated template can be directly added as a passive edge to the generator&apos;s agenda. If passive edges with a wider span are given higher priority than those with a smaller span, the tactical generator would try to combine the largest derivations before smaller ones, i.e., it would prefer those structures determined by EBL. 5 Implementation The EBL method just described has been fully implemented and tested with a broad coverage HPSGbased English grammar including more than 2000 fully specified lexical entries.7 The TDL grammar formalism is very powerful, supporting distri</context>
</contexts>
<marker>Neumann, 1994</marker>
<rawString>Neumann, G. 1994b. A Uniform Computational Model for Natural Language Parsing and Generation. Ph.D. thesis, Universitat des Saarlandes, Germany, Europe, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Neumann</author>
<author>G van Noord</author>
</authors>
<title>Reversibility and self-monitoring in natural language generation.</title>
<date>1994</date>
<booktitle>Reversible Grammar in Natural Language Processing.</booktitle>
<pages>59--96</pages>
<editor>In Tomek Strzalkowski, editor,</editor>
<publisher>Kluwer,</publisher>
<marker>Neumann, van Noord, 1994</marker>
<rawString>Neumann, G. and G. van Noord. 1994. Reversibility and self-monitoring in natural language generation. In Tomek Strzalkowski, editor, Reversible Grammar in Natural Language Processing. Kluwer, pages 59-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I M Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar. Center for the Study of Language and Information Stanford.</title>
<date>1994</date>
<contexts>
<context position="4580" citStr="Pollard and Sag, 1994" startWordPosition="712" endWordPosition="715">mar. Thus stated, we view tactical generation as the inverse process of parsing. Informally, EBL can be considered as an intelligent storage unit of example-based generalized parts of the grammatical search space determined via training by the tactical generator.&apos; Processing of similar new input is then reduced to simple lookup and matching operations, which circumvent re-computation of this already known search space. We concentrate on constraint-based grammar formalism following a sign-based approach considering linguistic objects (i.e., words and phrases) as utterance-meaning associations (Pollard and Sag, 1994). Thus viewed, a grammar is a formal statement of the relation between utterances in a natural language and representations of their meanings in some logical or other artificial language, where such representations are usually called logical forms (Shieber, 1993). The result of the tactical generator is a feature structure (or a set of such structures in the case of multiple paraphrases) containing among others the input logical form, the computed string, and a representation of the derivation. In our current implementation we are using TDL, a typed feature-based language and inference system </context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, C. and I. M. Sag. 1994. Head-Driven Phrase Structure Grammar. Center for the Study of Language and Information Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
</authors>
<title>Applying explanation-based generalization to natural language processing.</title>
<date>1988</date>
<booktitle>In Proceedings of the International Conference on Fifth Generation Computer Systems,</booktitle>
<location>Tokyo.</location>
<contexts>
<context position="928" citStr="Rayner, 1988" startWordPosition="137" endWordPosition="138">eneration NLG. The method is based on explanation-based learning EBL. The main advantage for the proposed new method for NLG is that the complexity of the grammatical decision making process during NLG can be vastly reduced, because the EBL method supports the adaption of a NLG system to a particular use of a language. 1 Introduction In recent years, a Machine Learning technique known as Explanation-based Learning EBL (Mitchell, Keller, and Kedar-Cabelli, 1986; van Harmelen and Bundy, 1988; Minton et al., 1989) has successfully been applied to control and speeding-up natural language parsing (Rayner, 1988; Samuelsson and Rayner, 1991; Neumann, 1994a; Samuelsson, 1994; Srinivas and Joshi, 1995; Rayner and Carter, 1996). The core idea of EBL is to transform the derivations (or explanations) computed by a problem solver (e.g., a parser) to some generalized and compact forms, which can be used very efficiently for solving similar problems in the future. EBL has primarily been used for parsing to automatically specialize a given source grammar to a specific domain. In that case, EBL is used as a method for adapting a general grammar and/or parser to the sub-language defined by a suitable training c</context>
</contexts>
<marker>Rayner, 1988</marker>
<rawString>Rayner, M. 1988. Applying explanation-based generalization to natural language processing. In Proceedings of the International Conference on Fifth Generation Computer Systems, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>D Carter</author>
</authors>
<title>Fast parsing using pruning and grammar specialization.</title>
<date>1996</date>
<booktitle>In 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Morristown, New Jersey.</location>
<contexts>
<context position="1043" citStr="Rayner and Carter, 1996" startWordPosition="151" endWordPosition="154">ed new method for NLG is that the complexity of the grammatical decision making process during NLG can be vastly reduced, because the EBL method supports the adaption of a NLG system to a particular use of a language. 1 Introduction In recent years, a Machine Learning technique known as Explanation-based Learning EBL (Mitchell, Keller, and Kedar-Cabelli, 1986; van Harmelen and Bundy, 1988; Minton et al., 1989) has successfully been applied to control and speeding-up natural language parsing (Rayner, 1988; Samuelsson and Rayner, 1991; Neumann, 1994a; Samuelsson, 1994; Srinivas and Joshi, 1995; Rayner and Carter, 1996). The core idea of EBL is to transform the derivations (or explanations) computed by a problem solver (e.g., a parser) to some generalized and compact forms, which can be used very efficiently for solving similar problems in the future. EBL has primarily been used for parsing to automatically specialize a given source grammar to a specific domain. In that case, EBL is used as a method for adapting a general grammar and/or parser to the sub-language defined by a suitable training corpus (Rayner and Carter, 1996). A specialized grammar can be seen as describing a domain-specific set of prototypi</context>
<context position="19042" citStr="Rayner and Carter, 1996" startWordPosition="3133" endWordPosition="3136"> phrasal templates. Next, each phrasal template is inserted in the decision tree in the way described above. It is possible to direct the subtree extraction process with the application of filters, which are applied to the whole remaining subtree in each recursive step. By using these filters it is possible to restrict the range of structural properties of candidate phrasal templates (e.g., extract only saturated NPs, or subtrees having at least two daughters, or subtrees which have no immediate recursive structures). These filters serve the same means as the &amp;quot;chunking criteria&amp;quot; described in (Rayner and Carter, 1996). During the training phase it is recognized for each phrasal template tempi, whether the decision tree already contains a path pointing to a previously extracted and already stored, phrasal template tempts, such that tempi, = templs. In that case, tempi, is not inserted and the recursion stops at that branch. Extended application phase For the application module, only the retrieval operation of the decision tree need be adapted. Remember that the input of the retrieval operation is the sorted generalized MRS mrs9 of the input MRS mrs. Therefore, mrs9 can be handled like a sequence. The task o</context>
</contexts>
<marker>Rayner, Carter, 1996</marker>
<rawString>Rayner, M. and D. Carter. 1996. Fast parsing using pruning and grammar specialization. In 34th Annual Meeting of the Association for Computational Linguistics, Morristown, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Samuelsson</author>
</authors>
<title>Fast Natural-Language Parsing Using Explanation-Based Learning.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Swedish Institute of Computer Science,</institution>
<location>Kista, Sweden, Europe.</location>
<contexts>
<context position="991" citStr="Samuelsson, 1994" startWordPosition="145" endWordPosition="146">rning EBL. The main advantage for the proposed new method for NLG is that the complexity of the grammatical decision making process during NLG can be vastly reduced, because the EBL method supports the adaption of a NLG system to a particular use of a language. 1 Introduction In recent years, a Machine Learning technique known as Explanation-based Learning EBL (Mitchell, Keller, and Kedar-Cabelli, 1986; van Harmelen and Bundy, 1988; Minton et al., 1989) has successfully been applied to control and speeding-up natural language parsing (Rayner, 1988; Samuelsson and Rayner, 1991; Neumann, 1994a; Samuelsson, 1994; Srinivas and Joshi, 1995; Rayner and Carter, 1996). The core idea of EBL is to transform the derivations (or explanations) computed by a problem solver (e.g., a parser) to some generalized and compact forms, which can be used very efficiently for solving similar problems in the future. EBL has primarily been used for parsing to automatically specialize a given source grammar to a specific domain. In that case, EBL is used as a method for adapting a general grammar and/or parser to the sub-language defined by a suitable training corpus (Rayner and Carter, 1996). A specialized grammar can be s</context>
</contexts>
<marker>Samuelsson, 1994</marker>
<rawString>Samuelsson, C. 1994. Fast Natural-Language Parsing Using Explanation-Based Learning. Ph.D. thesis, Swedish Institute of Computer Science, Kista, Sweden, Europe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Samuelsson</author>
</authors>
<title>An efficient algorithm for surface generation.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1414--1419</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="24041" citStr="Samuelsson, 1995" startWordPosition="3950" endWordPosition="3951">70% of computing time). The main reasons are that 1.) lexical lookup often returns several lexical readings for an MRS element (which introduces lexical non-determinism) and 2.) the lexical elements introduce most of the disjunctive constraints which makes unification very complex. Currently, terminal matching is performed left to right. However, we hope to increase the efficiency of this step by using head-oriented strategies, since this might help to re-solve disjunctive constraints as early as possible. 6 Discussion The only other approach I am aware of which also considers EBL for NLG is (Samuelsson, 1995a; Samuelsson, 1995b). However, he focuses on the compilation of a logic grammar using LR-compiling techniques, where EBL-related methods are used to optimize the compiled LR tables, in order to avoid spurious non-determinisms during normal generation. He considers neither the extraction of a specialized grammar for supporting controlled language generation, nor strong integration with the normal generator. However, these properties are very important for achieving high applicability. Automatic grammar extraction is worthwhile because it can be used to support the definition of a controlled do</context>
</contexts>
<marker>Samuelsson, 1995</marker>
<rawString>Samuelsson, C. 1995a. An efficient algorithm for surface generation. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 1414-1419, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Samuelsson</author>
</authors>
<title>Example-based optimization of surface-generation tables.</title>
<date>1995</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing,</booktitle>
<location>Velingrad, Bulgaria, Europe.</location>
<contexts>
<context position="24041" citStr="Samuelsson, 1995" startWordPosition="3950" endWordPosition="3951">70% of computing time). The main reasons are that 1.) lexical lookup often returns several lexical readings for an MRS element (which introduces lexical non-determinism) and 2.) the lexical elements introduce most of the disjunctive constraints which makes unification very complex. Currently, terminal matching is performed left to right. However, we hope to increase the efficiency of this step by using head-oriented strategies, since this might help to re-solve disjunctive constraints as early as possible. 6 Discussion The only other approach I am aware of which also considers EBL for NLG is (Samuelsson, 1995a; Samuelsson, 1995b). However, he focuses on the compilation of a logic grammar using LR-compiling techniques, where EBL-related methods are used to optimize the compiled LR tables, in order to avoid spurious non-determinisms during normal generation. He considers neither the extraction of a specialized grammar for supporting controlled language generation, nor strong integration with the normal generator. However, these properties are very important for achieving high applicability. Automatic grammar extraction is worthwhile because it can be used to support the definition of a controlled do</context>
</contexts>
<marker>Samuelsson, 1995</marker>
<rawString>Samuelsson, C. 1995b. Example-based optimization of surface-generation tables. In Proceedings of Recent Advances in Natural Language Processing, Velingrad, Bulgaria, Europe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Samuelsson</author>
<author>M Rayner</author>
</authors>
<title>Quantitative evaluation of explanation-based learning as an optimization tool for a large-scale natural language system.</title>
<date>1991</date>
<booktitle>In IJCAI-91,</booktitle>
<pages>609--615</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="957" citStr="Samuelsson and Rayner, 1991" startWordPosition="139" endWordPosition="142"> The method is based on explanation-based learning EBL. The main advantage for the proposed new method for NLG is that the complexity of the grammatical decision making process during NLG can be vastly reduced, because the EBL method supports the adaption of a NLG system to a particular use of a language. 1 Introduction In recent years, a Machine Learning technique known as Explanation-based Learning EBL (Mitchell, Keller, and Kedar-Cabelli, 1986; van Harmelen and Bundy, 1988; Minton et al., 1989) has successfully been applied to control and speeding-up natural language parsing (Rayner, 1988; Samuelsson and Rayner, 1991; Neumann, 1994a; Samuelsson, 1994; Srinivas and Joshi, 1995; Rayner and Carter, 1996). The core idea of EBL is to transform the derivations (or explanations) computed by a problem solver (e.g., a parser) to some generalized and compact forms, which can be used very efficiently for solving similar problems in the future. EBL has primarily been used for parsing to automatically specialize a given source grammar to a specific domain. In that case, EBL is used as a method for adapting a general grammar and/or parser to the sub-language defined by a suitable training corpus (Rayner and Carter, 199</context>
</contexts>
<marker>Samuelsson, Rayner, 1991</marker>
<rawString>Samuelsson, C. and M. Rayner. 1991. Quantitative evaluation of explanation-based learning as an optimization tool for a large-scale natural language system. In IJCAI-91, pages 609-615, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Shemtov</author>
</authors>
<title>Generation of Paraphrases from Ambiguous Logical Forms.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>919--924</pages>
<location>Kopenhagen, Denmark, Europe.</location>
<contexts>
<context position="3587" citStr="Shemtov, 1996" startWordPosition="566" endWordPosition="567">e vastly reduced, because the EBL method supports adaption of a NLG system to a particular language use. The core properties of this new method are: • prototypical occuring grammatical constructions can automatically be extracted; • generation of these constructions is vastly sped up using simple but efficient mechanisms; • the new method supports partial matching, in the sense that new semantic input need not be completely covered by previously trained examples; • it can easily be integrated with recently developed chart-based generators as described in, 214 e.g., (Neumann, 1994b; Kay, 1996; Shemtov, 1996). The method has been completely implemented and tested with a broad-coverage HPSG-based grammar for English (see sec. 5 for more details). 2 Foundations The main focus of this paper is tactical generation, i.e., the mapping of structures (usually representing semantic information eventually decorated with some functional features) to strings using a lexicon and a grammar. Thus stated, we view tactical generation as the inverse process of parsing. Informally, EBL can be considered as an intelligent storage unit of example-based generalized parts of the grammatical search space determined via t</context>
<context position="6033" citStr="Shemtov, 1996" startWordPosition="935" endWordPosition="936">epresentation of the extracted data and supports an elegant but efficient generalization step. We are adapting a &amp;quot;flat&amp;quot; representation of logical forms as described in (Kay, 1996; Copestake et al., 1996). This is a minimally structured, but descriptively adequate means to represent semantic information, which allows for various types of under-/overspecification, facilitates generation and the specification of semantic transfer equivalences &apos;In case a reversible grammar is used the parser can even be used for processing the training corpus. used for machine translation (Copestake et al., 1996; Shemtov, 1996).2 Informally, a flat representation is obtained by the use of extra variables which explicitly represent the relationship between the entities of a logical form and scope information. In our current system we are using the framework called minimal recursion semantics (MRS) described in (Copestake et al., 1996). Using their typed feature structure notation figure 1 displays a possible MRS of the string &amp;quot;Sandy gives a chair to Kim&amp;quot; (abbreviated where convenient). The value of the feature LISZT is actually treated like a set, i.e., the relative order of the elements is immaterial. The feature HA</context>
</contexts>
<marker>Shemtov, 1996</marker>
<rawString>Shemtov, H. 1996. Generation of Paraphrases from Ambiguous Logical Forms. In Proceedings of the 16th International Conference on Computational Linguistics (COLING), pages 919-924, Kopenhagen, Denmark, Europe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>The problem of logical-form equivalence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--179</pages>
<contexts>
<context position="4843" citStr="Shieber, 1993" startWordPosition="755" endWordPosition="756">sing of similar new input is then reduced to simple lookup and matching operations, which circumvent re-computation of this already known search space. We concentrate on constraint-based grammar formalism following a sign-based approach considering linguistic objects (i.e., words and phrases) as utterance-meaning associations (Pollard and Sag, 1994). Thus viewed, a grammar is a formal statement of the relation between utterances in a natural language and representations of their meanings in some logical or other artificial language, where such representations are usually called logical forms (Shieber, 1993). The result of the tactical generator is a feature structure (or a set of such structures in the case of multiple paraphrases) containing among others the input logical form, the computed string, and a representation of the derivation. In our current implementation we are using TDL, a typed feature-based language and inference system for constraint-based grammars (Krieger and Schafer, 1994). TDL allows the user to define hierarchicallyordered types consisting of type and feature constraints. As shown later, a systematic use of type information leads to a very compact representation of the ext</context>
</contexts>
<marker>Shieber, 1993</marker>
<rawString>Shieber, S. M. 1993. The problem of logical-form equivalence. Computational Linguistics, 19:179-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Srinivas</author>
<author>A Joshi</author>
</authors>
<title>Some novel applications of explanation-based learning to parsing lexicalized tree-adjoining grammars.</title>
<date>1995</date>
<booktitle>In 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="1017" citStr="Srinivas and Joshi, 1995" startWordPosition="147" endWordPosition="150">n advantage for the proposed new method for NLG is that the complexity of the grammatical decision making process during NLG can be vastly reduced, because the EBL method supports the adaption of a NLG system to a particular use of a language. 1 Introduction In recent years, a Machine Learning technique known as Explanation-based Learning EBL (Mitchell, Keller, and Kedar-Cabelli, 1986; van Harmelen and Bundy, 1988; Minton et al., 1989) has successfully been applied to control and speeding-up natural language parsing (Rayner, 1988; Samuelsson and Rayner, 1991; Neumann, 1994a; Samuelsson, 1994; Srinivas and Joshi, 1995; Rayner and Carter, 1996). The core idea of EBL is to transform the derivations (or explanations) computed by a problem solver (e.g., a parser) to some generalized and compact forms, which can be used very efficiently for solving similar problems in the future. EBL has primarily been used for parsing to automatically specialize a given source grammar to a specific domain. In that case, EBL is used as a method for adapting a general grammar and/or parser to the sub-language defined by a suitable training corpus (Rayner and Carter, 1996). A specialized grammar can be seen as describing a domain</context>
</contexts>
<marker>Srinivas, Joshi, 1995</marker>
<rawString>Srinivas, B. and A. Joshi. 1995. Some novel applications of explanation-based learning to parsing lexicalized tree-adjoining grammars. In 38th Annual Meeting of the Association for Computational Linguistics, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F van Harmelen</author>
<author>A Bundy</author>
</authors>
<title>Explanationbased generalization=partial evaluation.</title>
<date>1988</date>
<journal>Artificial Intelligence,</journal>
<pages>36--401</pages>
<marker>van Harmelen, Bundy, 1988</marker>
<rawString>van Harmelen, F. and A. Bundy. 1988. Explanationbased generalization=partial evaluation. Artificial Intelligence, 36:401-412.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>