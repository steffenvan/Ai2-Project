<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.991787">
Polynomial Time Joint Structural Inference for Sentence Compression
</title>
<author confidence="0.985089">
Xian Qian and Yang Liu
</author>
<affiliation confidence="0.966453">
The University of Texas at Dallas
</affiliation>
<address confidence="0.734626">
800 W. Campbell Rd., Richardson, TX, USA
</address>
<email confidence="0.996765">
{qx,yangl}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.979557" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976555555556">
We propose two polynomial time infer-
ence algorithms to compress sentences un-
der bigram and dependency-factored ob-
jectives. The first algorithm is exact and
requires O(n6) running time. It extend-
s Eisner’s cubic time parsing algorithm
by using virtual dependency arcs to link
deleted words. Two signatures are added
to each span, indicating the number of
deleted words and the rightmost kept word
within the span. The second algorithm is
a fast approximation of the first one. It re-
laxes the compression ratio constraint us-
ing Lagrangian relaxation, and thereby re-
quires O(n4) running time. Experimental
results on the popular sentence compres-
sion corpus demonstrate the effectiveness
and efficiency of our proposed approach.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997829111111111">
Sentence compression aims to shorten a sentence
by removing uninformative words to reduce read-
ing time. It has been widely used in compres-
sive summarization (Liu and Liu, 2009; Li et al.,
2013; Martins and Smith, 2009; Chali and Hasan,
2012; Qian and Liu, 2013). To make the com-
pressed sentence readable, some techniques con-
sider the n-gram language models of the com-
pressed sentence (Clarke and Lapata, 2008; Mc-
Donald, 2006). Recent studies used a subtree dele-
tion model for compression (Berg-Kirkpatrick et
al., 2011; Morita et al., 2013; Qian and Liu, 2013),
which deletes a word only if its modifier in the
parse tree is deleted. Despite its empirical suc-
cess, such a model fails to generate compressions
that are not subject to the subtree constraint (see
Figure 1). In fact, we parsed the Edinburgh sen-
tence compression corpus using the MSTparserl,
</bodyText>
<footnote confidence="0.925999">
1http://sourceforge.net/projects/mstparser/
</footnote>
<note confidence="0.4404205">
ROOT Warren says the economy continues the steady improvement
ROOT Warren says the economy continues the steady improvement
</note>
<figureCaption confidence="0.832386">
Figure 1: The compressed sentence is not a sub-
tree of the original sentence. Words in gray are
removed.
</figureCaption>
<bodyText confidence="0.999706862068966">
and found that 2561 of 5379 sentences (47.6%) do
not satisfy the subtree deletion model.
Methods beyond the subtree model are also ex-
plored. Trevor et al. proposed synchronous tree
substitution grammar (Cohn and Lapata, 2009),
which allows local distortion of the tree topolo-
gy and can thus naturally capture structural mis-
matches. (Genest and Lapalme, 2012; Thadani
and McKeown, 2013) proposed the joint compres-
sion model, which simultaneously considers the n-
gram model and dependency parse tree of the com-
pressed sentence. However, the time complexity
greatly increases since the parse tree dynamical-
ly depends on the compression. They used Integer
Linear Programming (ILP) for inference which re-
quires exponential running time in the worst case.
In this paper, we propose a new exact decod-
ing algorithm for the joint model using dynam-
ic programming. Our method extends Eisner’s
cubic time parsing algorithm by adding signa-
tures to each span, which indicate the number of
deleted words and the rightmost kept word with-
in the span, resulting in O(n6) time complexity
and O(n4) space complexity. We further propose a
faster approximate algorithm based on Lagrangian
relaxation, which has TO(n4) running time and
O(n3) space complexity (T is the iteration num-
ber in the subgradient decent algorithm). Experi-
ments on the popular Edinburgh dataset show that
</bodyText>
<page confidence="0.976337">
327
</page>
<bodyText confidence="0.642377">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 327–332,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</bodyText>
<equation confidence="0.979386545454545">
dep
w 0i
w
dep
wijdep
wi2 wdep i+1
x0 (root) x x2 ... xi xi+1 ... xj ...
1 xn
bgr
w2i bgr wi i+1
bgr wi+1 j
</equation>
<figureCaption confidence="0.633141">
Figure 2: Graph illustration for the objective func-
</figureCaption>
<bodyText confidence="0.775392666666667">
tion. In this example, words x2, xi, xi+1, xj are
kept, others are deleted. The value of the ob-
jective function is wtok
</bodyText>
<equation confidence="0.97524225">
2 + wtiok + wtoki+1 + wtok
j +
dep dep depdep bgr bgr bgr
0i +w i2 +wii+1+wij +w2i +wii+1+wi+1j.
</equation>
<bodyText confidence="0.9640735">
the proposed approach is 10 times faster than a
high-performance commercial ILP solver.
</bodyText>
<sectionHeader confidence="0.990808" genericHeader="method">
2 Task Definition
</sectionHeader>
<bodyText confidence="0.999923142857143">
We define the sentence compression task as: given
a sentence composed of n words, x = x1, ... , xn,
and a length L G n, we need to remove (n − L)
words from x, so that the sum of the weights of
the dependency tree and word bigrams of the re-
maining part is maximized. Formally, we solve
the following optimization problem:
</bodyText>
<equation confidence="0.9862535">
dep
wij zizjyij (1)
∏
wij zizj
bgr
i&lt;k&lt;j
∑s.t. z is binary , zi=L
i
</equation>
<bodyText confidence="0.969955142857143">
y is a projective parse tree over the
subgraph: {xi|zi = 11
where z is a binary vector, zi indicates xi is kep-
t or not. y is a square matrix denoting the pro-
jective dependency parse tree over the remaining
words, yij indicates if xi is the head of xj (note
that each word has exactly one head). wtok
i is the
informativeness of xi, wbgr
ij is the score of bigram
xixj in an n-gram model, wdep is the score of de-
pendency arc xi —* xj in an arc-factored depen-
dency parsing model. Hence, the first part of the
objective function is the total score of the kep-
t words, the second and third parts are the scores
of the parse tree and bigrams of the compressed
∏
sentence, zizj i&lt;k&lt;j(1 − zk) = 1 indicates both
xi and xj are kept, and are adjacent after compres-
sion. A graph illustration of the objective function
is shown in Figure 2.
</bodyText>
<figureCaption confidence="0.770917">
Figure 3: Connect deleted words using virtual arc-
s.
</figureCaption>
<sectionHeader confidence="0.994974" genericHeader="method">
3 Proposed Method
</sectionHeader>
<subsectionHeader confidence="0.999875">
3.1 Eisner’s Cubic Time Parsing Algorithm
</subsectionHeader>
<bodyText confidence="0.999959842105263">
Throughout the paper, we assume that all the parse
trees are projective. Our method is a generaliza-
tion of Eisner’s dynamic programming algorithm
(Eisner, 1996), where two types of structures are
used in each iteration, incomplete spans and com-
plete spans. A span is a subtree over a number of
consecutive words, with the leftmost or the right-
most word as its root. An incomplete span denoted
as Iij is a subtree inside a single arc xi —* xj, with
root xi. A complete span is denoted as Qj, where
xi is the root of the subtree, and xj is the furthest
descendant of xi.
Eisner’s algorithm searches the optimal tree in
a bottom up order. In each step, it merges two
adjacent spans into a larger one. There are two
rules for merging spans: one merges two complete
spans into an incomplete span, the other merges an
incomplete span and a complete span into a large
complete span.
</bodyText>
<subsectionHeader confidence="0.996186">
3.2 Exact O(n6) Time Algorithm
</subsectionHeader>
<bodyText confidence="0.943381285714286">
First we consider an easy case, where the bigram
scores wij in the objective function are ignored.
bgr
The scores of unigrams wtok
i can be transfered
to the dependency arcs, so that we can remove al-
l linear terms wtok
</bodyText>
<equation confidence="0.917771666666667">
i zi from the objective function.
That is:
∑
wtok
i zi +
i,j
(dep tok
wij + wj )zizjyij
This can be easily verifed. If zj = 0, then in both
</equation>
<bodyText confidence="0.97536025">
equations, all terms having zj are zero; If zj = 1,
i.e., xj is kept, since it has exactly one head word
xk in the compressed sentence, the sum of the
terms having zj is wtok j+ wdep for both equations.
kj
Therefore, we only need to consider the scores
of arcs. For any compressed sentence, we could
augment its dependency tree by adding a virtual
</bodyText>
<figure confidence="0.942074684210526">
∑max
z,y
i
∑
wtok
i zi +
i,j
∑
+
i&lt;j
(1 − zk)
∑
i
∑=
i,j
ij zizjyij
dep
w
ROOT Warren says the economy continues the steady improvement
</figure>
<page confidence="0.714091">
328
</page>
<figureCaption confidence="0.899972">
Figure 4: Merging rules for dependency-factored
sentence compression. Incomplete spans and
complete spans are represented by trapezoids and
triangles respectively.
</figureCaption>
<bodyText confidence="0.996997294117647">
arc i − 1 —* i for each deleted word xi. If the first
word x1 is deleted, we connect it to the root of the
parse tree x0, as shown in Figure 3. In this way,
we derive a full parse tree of the original sentence.
This is a one-to-one mapping. We can reversely
get the the compressed parse tree by removing all
virtual arcs from the full parse tree. We restrict
the score of all the virtual arcs to be zero, so that
scores of the two parse trees are equivalent.
Now the problem is to search the optimal full
parse tree with n − L virtual arcs.
We modify Eisner’s algorithm by adding a sig-
nature to each span indicating the number of vir-
tual arcs within the span. Let Iij(k) and Cij(k)
denote the incomplete and complete spans with k
virtual arcs respectively. When merging two span-
s, there are 4 cases, as shown in Figure 4.
</bodyText>
<listItem confidence="0.8623055">
• Case 1 Link two complete spans by a virtual
arc : Iii+1(1) = Cii(0) + Ci+1
</listItem>
<bodyText confidence="0.835536">
i+1(0).
The two complete spans must be single word-
s, as the length of the virtual arc is 1.
</bodyText>
<listItem confidence="0.994573833333333">
• Case 2 Link two complete spans by a non-
virtual arc: Iij(k) = Cir(k′) + Cjr+1(k′′), k′ +
k′′ = k.
• Case 3 Merge an incomplete span and a com-
plete span. The incomplete span is covered
by a virtual arc: Iij(j − i) = Iii+1(1) +
</listItem>
<page confidence="0.851254">
Ci+1
</page>
<bodyText confidence="0.846117">
j (j − i − 1). The number of the virtu-
al arcs within Ci+1
j must be j − i − 1, since
the descendants of the modifier of a virtual
arc xj must be removed.
</bodyText>
<listItem confidence="0.99796725">
• Case 4 Merge an incomplete span and a com-
plete span. The incomplete span is covered
by a non-virtual arc: Cij(k) = Iir(k′) +
Cr j (k′′),k′ + k′′ = k.
</listItem>
<bodyText confidence="0.996141933333333">
The score of the new span is the sum of the two
spans. For case 2, the weight of the dependency
arc i —* j, wdep
ij is also added to the final score.
The root node is allowed to have two modifiers:
one is the modifier in the compressed sentence, the
other is the first word if it is removed.
For each combination, the algorithm enumer-
ates the number of virtual arcs in the left and right
spans, and the split position (e.g., k′, k′′, r in case
2), thus it takes O(n3) running time. The overall
time complexity is O(n5) and the space complex-
ity is O(n3).
Next, we consider the bigram scores. The fol-
lowing proposition is obvious.
</bodyText>
<construct confidence="0.6473095">
Proposition 1. For any right-headed span Iij or
Cij, i &gt; j, words xi, xj must be kept.
</construct>
<bodyText confidence="0.998121631578948">
Proof. Suppose xj is removed, there must be a vir-
tual arc j − 1 —* j which is a conflict with the fact
that xj is the leftmost word. As xj is a descendant
of xi, xi must be kept.
When merging two spans, a new bigram is cre-
ated, which connects the rightmost kept words in
the left span and the leftmost kept word in the right
span. According to the proposition above, if the
right span is right-headed, its leftmost word is kep-
t. If the right span is left-headed, there are two
cases: its leftmost word is kept, or no word in the
span is kept. In any case, we only need to consider
the leftmost word in the right span.
Let Iij(k, p) and Cij(k, p) denote the single and
complete span with k virtual arcs and the right-
most kept word xp. According to the proposition
above, we have, for any right-headed span p = i.
We slightly modify the two merging rules
above, and obtain:
</bodyText>
<listItem confidence="0.9911935">
• Case 2’ Link two complete spans by a
non-virtual arc: Iij(k,j) = Cir(k′, p) +
</listItem>
<equation confidence="0.595464142857143">
Cjr+1(k′′, j), k′ + k′′ = k. The score of the
new span is the sum of the two spans plus
wij + wbgr
dep p,r+1.
+ =
Case 1
i i+1 i i+1
</equation>
<figure confidence="0.960965">
+
... ...
=
Case 3
i+1 j
i j
i i+1
Case 4
Case 2
i r
+
r+1 j
=
i j
i r
+
r j
=
j
</figure>
<page confidence="0.99048">
329
</page>
<listItem confidence="0.823276714285714">
• Case 4’ Merge an incomplete span and a
complete span. The incomplete span is cov-
ered by a non-virtual arc. For left-headed
spans, the rule is Cij(k, q) = Iir(k′, p) +
Crj (k″, q), k′ + k″= k, and the score of
the new span is the sum of the two span-
s plus wbgr
</listItem>
<bodyText confidence="0.990133">
pr ; for right-headed spans, the rule
is Cij(k, i) = Iir(k′, i) + Cr j (k″, r), and the
score of the new span is the sum of the two
spans.
The modified algorithm requires O(n6) running
time and O(n4) space complexity.
</bodyText>
<subsectionHeader confidence="0.99551">
3.3 Approximate O(n4) Time Algorithm
</subsectionHeader>
<bodyText confidence="0.99934925">
In this section, we propose an approximate algo-
rithm where the length constraint ∑i zi = L is re-
laxed by Lagrangian Relaxation. The relaxed ver-
sion of Problem (1) is
</bodyText>
<equation confidence="0.9892281">
dep
wij zizjyij (2)
+
∑ wij zj
bg r zi ∏
i&lt;k&lt;j
i&lt;j
+A( ∑ zi − L)
i
s.t. z is binary
</equation>
<bodyText confidence="0.992137875">
y is a projective parse tree over the
subgraph: {xi|zi = 11
Fixing A, the optimal z, y can be found using a
simpler version of the algorithm above. We drop
the signature of the virtual arc number from each
span, and thus obtain an O(n4) time algorithm. S-
pace complexity is O(n3). Fixing z, y, the dual
variable is updated by
</bodyText>
<equation confidence="0.9707905">
A = A + α(L − ∑ zi)
i
</equation>
<bodyText confidence="0.9997545">
where α &gt; 0 is the learning rate. In this paper, our
choice of α is the same as (Rush et al., 2010).
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999556">
4.1 Data and Settings
</subsectionHeader>
<bodyText confidence="0.99992732">
We evaluate our method on the data set from
(Clarke and Lapata, 2008). It includes 82
newswire articles with manually produced com-
pression for each sentence. We use the same par-
titions as (Martins and Smith, 2009), i.e., 1,188
sentences for training and 441 for testing.
Our model is discriminative – the scores of
the unigrams, bigrams and dependency arcs are
the linear functions of features, that is, wtok
i =
vT f(xi), where f is the feature vector of xi, and v
is the weight vector of features. The learning task
is to estimate the feature weight vector based on
the manually compressed sentences.
We run a second order dependency parser
trained on the English Penn Treebank corpus to
generate the parse trees of the compressed sen-
tences. Then we augment these parse trees by
adding virtual arcs and get the full parse trees
of their corresponding original sentences. In this
way, the annoation is transformed into a set of
sentences with their augmented parse trees. The
learning task is similar to training a parser. We run
a CRF based POS tagger to generate POS related
features.
We adopt the compression evaluation metric as
used in (Martins and Smith, 2009) that measures
the macro F-measure for the retained unigrams
(Fugr), and the one used in (Clarke and Lapata,
2008) that calculates the F1 score of the grammat-
ical relations labeled by RASP (Briscoe and Car-
roll, 2002).
We compare our method with other 4 state-of-
the-art systems. The first is linear chain CRFs,
where the compression task is casted as a bina-
ry sequence labeling problem. It usually achieves
high unigram F1 score but low grammatical rela-
tion F1 score since it only considers the local inter-
dependence between adjacent words. The second
is the subtree deletion model (Berg-Kirkpatrick et
al., 2011) which is solved by integer linear pro-
gramming (ILP)2. The third one is the bigram
model proposed by McDonald (McDonald, 2006)
which adopts dynamic programming for efficient
inference. The last one jointly infers tree struc-
tures alongside bigrams using ILP (Thadani and
McKeown, 2013). For fair comparison, system-
s were restricted to produce compressions that
matched their average gold compression rate if
possible.
</bodyText>
<subsectionHeader confidence="0.942464">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.9997415">
Three types of features are used to learn our mod-
el: unigram features, bigram features and depen-
dency features, as shown in Table 1. We also use
the in-between features proposed by (McDonald et
</bodyText>
<footnote confidence="0.9568565">
2We use Gurobi as the ILP solver in the paper.
http://www.gurobi.com/
</footnote>
<equation confidence="0.6748444">
∑max
z,y
i
∑
wtok
i zi +
i,j
min
λ
(1 − zk)
</equation>
<page confidence="0.992259">
330
</page>
<table confidence="0.908146041666667">
Features for unigram xi
wi−2, wi−1, wi, wi+1, wi+2
ti−2, ti−1, ti, ti+1, ti+2
witi
wi−1wi, wiwi+1
ti−2ti−1, ti−1ti, titi+1, ti+1ti+2
ti−2ti−1ti, ti−1titi+1, titi+1ti+2
whether wi is a stopword
Features for selected bigram xixj
distance between the two words: j − i
wiwj, wi−1wj, wi+1wj, wiwj−1, wiwj+1
titj, ti−1tj, ti+1tj, titj−1, titj+1
Concatenation of the templates above
ftitktj|i &lt; k &lt; j}
Dependency Features for arc xh ! xm
distance between the head and modifier h − m
dependency type
direction of the dependency arc (left/right)
whwm, wh−1wm, wh+1wm, whwm−1, whwm+1
thtm, th−1tm, th+1tm, thtm−1, thtm+1
th−1thtm−1tm, thth+1tm−1tm
th−1thtmtm+1, thth+1tmtm+1
Concatenation of the templates above
fthtktm|xk lies between xh and xm}
</table>
<tableCaption confidence="0.985342">
Table 1: Feature templates. wi denotes the word
</tableCaption>
<bodyText confidence="0.810636666666667">
form of token xi and ti denotes the POS tag of xi.
al., 2005), which were shown to be very effective
for dependency parsing.
</bodyText>
<subsectionHeader confidence="0.817509">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999014444444444">
We show the comparison results in Table 2. As
expected, the joint models (ours and TM13) con-
sistently outperform the subtree deletion model, s-
ince the joint models do not suffer from the sub-
tree restriction. They also outperform McDon-
ald’s, demonstrating the effectiveness of consid-
ering the grammar structure for compression. It
is not surprising that CRFs achieve high unigram
F scores but low syntactic F scores as they do not
</bodyText>
<table confidence="0.998973714285714">
System C Rate Funi RASP Sec.
Ours(Approx) 0.68 0.802 0.598 0.056
Ours(Exact) 0.68 0.805 0.599 0.610
Subtree 0.68 0.761 0.575 0.022
TM13 0.68 0.804 0.599 0.592
McDonald06 0.71 0.776 0.561 0.010
CRFs 0.73 0.790 0.501 0.002
</table>
<tableCaption confidence="0.99218">
Table 2: Comparison results under various quality
</tableCaption>
<bodyText confidence="0.996737842105263">
metrics, including unigram F1 score (Funi), syn-
tactic F1 score (RASP), and compression speed
(seconds per sentence). C Rate is the compression
ratio of the system generated output. For fair com-
parison, systems were restricted to produce com-
pressions that matched their average gold com-
pression rate if possible.
consider the fluency of the compressed sentence.
Compared with TM13’s system, our model with
exact decoding is not significantly faster due to the
high order of the time complexity. On the oth-
er hand, our approximate approach is much more
efficient, about 10 times faster than TM13’ sys-
tem, and achieves competitive accuracy with the
exact approach. Note that it is worth pointing
out that the exact approach can output compressed
sentences of all lengths, whereas the approximate
method can only output one sentence at a specific
compression rate.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99987295">
In this paper, we proposed two polynomial time
decoding algorithms using joint inference for sen-
tence compression. The first one is an exac-
t dynamic programming algorithm, and requires
O(n6) running time. This one does not show
significant advantage in speed over ILP. The sec-
ond one is an approximation of the first algorith-
m. It adopts Lagrangian relaxation to eliminate the
compression ratio constraint, yielding lower time
complexity TO(n4). In practice it achieves nearly
the same accuracy as the exact one, but is much
faster.3
The main assumption of our method is that the
dependency parse tree is projective, which is not
true for some other languages. In that case, our
method is invalid, but (Thadani and McKeown,
2013) still works. In the future, we will study the
non-projective cases based on the recent parsing
techniques for 1-endpoint-crossing trees (Pitler et
al., 2013).
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999336285714286">
We thank three anonymous reviewers for their
valuable comments. This work is partly support-
ed by NSF award IIS-0845484 and DARPA under
Contract No. FA8750-13-2-0041. Any opinion-
s expressed in this material are those of the au-
thors and do not necessarily reflect the views of
the funding agencies.
</bodyText>
<sectionHeader confidence="0.99905" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.928026333333333">
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings ofACL-HLT, pages 481–490, June.
</reference>
<footnote confidence="0.9641515">
3Our code is available at http://code.google.com/p/sent-
compress/
</footnote>
<page confidence="0.996306">
331
</page>
<reference confidence="0.999753345454546">
T. Briscoe and J. Carroll. 2002. Robust accurate statis-
tical annotation of general text.
Yllias Chali and Sadid A. Hasan. 2012. On the effec-
tiveness of using sentence compression models for
query-focused multi-document summarization. In
Proceedings of COLING, pages 457–474.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. J. Artif. Intell. Res. (JAIR),
31:399–429.
Trevor Cohn and Mirella Lapata. 2009. Sentence
compression as tree transduction. J. Artif. Int. Res.,
34(1):637–674, April.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: an exploration. In Pro-
ceedings of COLING.
Pierre-Etienne Genest and Guy Lapalme. 2012. Fully
abstractive approach to guided summarization. In
Proceedings of the ACL, pages 354–358.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013.
Document summarization via guided sentence com-
pression. In Proceedings of EMNLP, October.
Fei Liu and Yang Liu. 2009. From extractive to ab-
stractive meeting summaries: Can it be done by
sentence compression? In Proceedings of ACL-
IJCNLP 2009, pages 261–264, August.
Andr´e F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, pages 1–9.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL.
Ryan McDonald. 2006. Discriminative Sentence
Compression with Soft Syntactic Constraints. In
Proceedings of EACL, April.
Hajime Morita, Ryohei Sasano, Hiroya Takamura, and
Manabu Okumura. 2013. Subtree extractive sum-
marization via submodular maximization. In Pro-
ceedings of ACL, pages 1023–1032, August.
Emily Pitler, Sampath Kannan, and Mitchell Marcus.
2013. Finding optimal 1-endpoint-crossing trees. In
Transactions of the Association for Computational
Linguistics, 2013 Volume 1.
Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proceedings
of EMNLP, pages 1492–1502, October.
Alexander M Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural
language processing. In Proceedings of EMNLP.
Kapil Thadani and Kathleen McKeown. 2013. Sen-
tence compression with joint structural inference. In
Proceedings of the CoNLL, August.
</reference>
<page confidence="0.99825">
332
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.929389">
<title confidence="0.998229">Polynomial Time Joint Structural Inference for Sentence Compression</title>
<author confidence="0.983706">Qian</author>
<affiliation confidence="0.976295">The University of Texas at</affiliation>
<address confidence="0.960365">800 W. Campbell Rd., Richardson, TX,</address>
<abstract confidence="0.998981736842105">We propose two polynomial time inference algorithms to compress sentences under bigram and dependency-factored objectives. The first algorithm is exact and time. It extends Eisner’s cubic time parsing algorithm by using virtual dependency arcs to link deleted words. Two signatures are added to each span, indicating the number of deleted words and the rightmost kept word within the span. The second algorithm is a fast approximation of the first one. It relaxes the compression ratio constraint using Lagrangian relaxation, and thereby retime. Experimental results on the popular sentence compression corpus demonstrate the effectiveness and efficiency of our proposed approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL-HLT,</booktitle>
<pages>481--490</pages>
<contexts>
<context position="1461" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="225" endWordPosition="228">tence compression corpus demonstrate the effectiveness and efficiency of our proposed approach. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In fact, we parsed the Edinburgh sentence compression corpus using the MSTparserl, 1http://sourceforge.net/projects/mstparser/ ROOT Warren says the economy continues the steady improvement ROOT Warren says the economy continues the steady improvement Figure 1: The compressed sentence is not a subtree of the original sentence. Words in gray are </context>
<context position="13816" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="2551" endWordPosition="2554">s and Smith, 2009) that measures the macro F-measure for the retained unigrams (Fugr), and the one used in (Clarke and Lapata, 2008) that calculates the F1 score of the grammatical relations labeled by RASP (Briscoe and Carroll, 2002). We compare our method with other 4 state-ofthe-art systems. The first is linear chain CRFs, where the compression task is casted as a binary sequence labeling problem. It usually achieves high unigram F1 score but low grammatical relation F1 score since it only considers the local interdependence between adjacent words. The second is the subtree deletion model (Berg-Kirkpatrick et al., 2011) which is solved by integer linear programming (ILP)2. The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference. The last one jointly infers tree structures alongside bigrams using ILP (Thadani and McKeown, 2013). For fair comparison, systems were restricted to produce compressions that matched their average gold compression rate if possible. 4.2 Features Three types of features are used to learn our model: unigram features, bigram features and dependency features, as shown in Table 1. We also use the in-between features p</context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceedings ofACL-HLT, pages 481–490, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<contexts>
<context position="13420" citStr="Briscoe and Carroll, 2002" startWordPosition="2485" endWordPosition="2489">nt these parse trees by adding virtual arcs and get the full parse trees of their corresponding original sentences. In this way, the annoation is transformed into a set of sentences with their augmented parse trees. The learning task is similar to training a parser. We run a CRF based POS tagger to generate POS related features. We adopt the compression evaluation metric as used in (Martins and Smith, 2009) that measures the macro F-measure for the retained unigrams (Fugr), and the one used in (Clarke and Lapata, 2008) that calculates the F1 score of the grammatical relations labeled by RASP (Briscoe and Carroll, 2002). We compare our method with other 4 state-ofthe-art systems. The first is linear chain CRFs, where the compression task is casted as a binary sequence labeling problem. It usually achieves high unigram F1 score but low grammatical relation F1 score since it only considers the local interdependence between adjacent words. The second is the subtree deletion model (Berg-Kirkpatrick et al., 2011) which is solved by integer linear programming (ILP)2. The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference. The last one jointl</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>T. Briscoe and J. Carroll. 2002. Robust accurate statistical annotation of general text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yllias Chali</author>
<author>Sadid A Hasan</author>
</authors>
<title>On the effectiveness of using sentence compression models for query-focused multi-document summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>457--474</pages>
<contexts>
<context position="1183" citStr="Chali and Hasan, 2012" startWordPosition="180" endWordPosition="183">ed words and the rightmost kept word within the span. The second algorithm is a fast approximation of the first one. It relaxes the compression ratio constraint using Lagrangian relaxation, and thereby requires O(n4) running time. Experimental results on the popular sentence compression corpus demonstrate the effectiveness and efficiency of our proposed approach. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In fact, we parsed the Edinburgh sentence compression corpus using t</context>
</contexts>
<marker>Chali, Hasan, 2012</marker>
<rawString>Yllias Chali and Sadid A. Hasan. 2012. On the effectiveness of using sentence compression models for query-focused multi-document summarization. In Proceedings of COLING, pages 457–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression: An integer linear programming approach.</title>
<date>2008</date>
<journal>J. Artif. Intell. Res. (JAIR),</journal>
<pages>31--399</pages>
<contexts>
<context position="1351" citStr="Clarke and Lapata, 2008" startWordPosition="208" endWordPosition="211"> Lagrangian relaxation, and thereby requires O(n4) running time. Experimental results on the popular sentence compression corpus demonstrate the effectiveness and efficiency of our proposed approach. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In fact, we parsed the Edinburgh sentence compression corpus using the MSTparserl, 1http://sourceforge.net/projects/mstparser/ ROOT Warren says the economy continues the steady improvement ROOT Warren says the economy continues the stea</context>
<context position="12106" citStr="Clarke and Lapata, 2008" startWordPosition="2261" endWordPosition="2264">ij (2) + ∑ wij zj bg r zi ∏ i&lt;k&lt;j i&lt;j +A( ∑ zi − L) i s.t. z is binary y is a projective parse tree over the subgraph: {xi|zi = 11 Fixing A, the optimal z, y can be found using a simpler version of the algorithm above. We drop the signature of the virtual arc number from each span, and thus obtain an O(n4) time algorithm. Space complexity is O(n3). Fixing z, y, the dual variable is updated by A = A + α(L − ∑ zi) i where α &gt; 0 is the learning rate. In this paper, our choice of α is the same as (Rush et al., 2010). 4 Experiments 4.1 Data and Settings We evaluate our method on the data set from (Clarke and Lapata, 2008). It includes 82 newswire articles with manually produced compression for each sentence. We use the same partitions as (Martins and Smith, 2009), i.e., 1,188 sentences for training and 441 for testing. Our model is discriminative – the scores of the unigrams, bigrams and dependency arcs are the linear functions of features, that is, wtok i = vT f(xi), where f is the feature vector of xi, and v is the weight vector of features. The learning task is to estimate the feature weight vector based on the manually compressed sentences. We run a second order dependency parser trained on the English Pen</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. J. Artif. Intell. Res. (JAIR), 31:399–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression as tree transduction.</title>
<date>2009</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="2295" citStr="Cohn and Lapata, 2009" startWordPosition="357" endWordPosition="360">to the subtree constraint (see Figure 1). In fact, we parsed the Edinburgh sentence compression corpus using the MSTparserl, 1http://sourceforge.net/projects/mstparser/ ROOT Warren says the economy continues the steady improvement ROOT Warren says the economy continues the steady improvement Figure 1: The compressed sentence is not a subtree of the original sentence. Words in gray are removed. and found that 2561 of 5379 sentences (47.6%) do not satisfy the subtree deletion model. Methods beyond the subtree model are also explored. Trevor et al. proposed synchronous tree substitution grammar (Cohn and Lapata, 2009), which allows local distortion of the tree topology and can thus naturally capture structural mismatches. (Genest and Lapalme, 2012; Thadani and McKeown, 2013) proposed the joint compression model, which simultaneously considers the ngram model and dependency parse tree of the compressed sentence. However, the time complexity greatly increases since the parse tree dynamically depends on the compression. They used Integer Linear Programming (ILP) for inference which requires exponential running time in the worst case. In this paper, we propose a new exact decoding algorithm for the joint model</context>
</contexts>
<marker>Cohn, Lapata, 2009</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2009. Sentence compression as tree transduction. J. Artif. Int. Res., 34(1):637–674, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: an exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="5613" citStr="Eisner, 1996" startWordPosition="950" endWordPosition="951"> first part of the objective function is the total score of the kept words, the second and third parts are the scores of the parse tree and bigrams of the compressed ∏ sentence, zizj i&lt;k&lt;j(1 − zk) = 1 indicates both xi and xj are kept, and are adjacent after compression. A graph illustration of the objective function is shown in Figure 2. Figure 3: Connect deleted words using virtual arcs. 3 Proposed Method 3.1 Eisner’s Cubic Time Parsing Algorithm Throughout the paper, we assume that all the parse trees are projective. Our method is a generalization of Eisner’s dynamic programming algorithm (Eisner, 1996), where two types of structures are used in each iteration, incomplete spans and complete spans. A span is a subtree over a number of consecutive words, with the leftmost or the rightmost word as its root. An incomplete span denoted as Iij is a subtree inside a single arc xi —* xj, with root xi. A complete span is denoted as Qj, where xi is the root of the subtree, and xj is the furthest descendant of xi. Eisner’s algorithm searches the optimal tree in a bottom up order. In each step, it merges two adjacent spans into a larger one. There are two rules for merging spans: one merges two complete</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: an exploration. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre-Etienne Genest</author>
<author>Guy Lapalme</author>
</authors>
<title>Fully abstractive approach to guided summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>354--358</pages>
<contexts>
<context position="2427" citStr="Genest and Lapalme, 2012" startWordPosition="378" endWordPosition="381">://sourceforge.net/projects/mstparser/ ROOT Warren says the economy continues the steady improvement ROOT Warren says the economy continues the steady improvement Figure 1: The compressed sentence is not a subtree of the original sentence. Words in gray are removed. and found that 2561 of 5379 sentences (47.6%) do not satisfy the subtree deletion model. Methods beyond the subtree model are also explored. Trevor et al. proposed synchronous tree substitution grammar (Cohn and Lapata, 2009), which allows local distortion of the tree topology and can thus naturally capture structural mismatches. (Genest and Lapalme, 2012; Thadani and McKeown, 2013) proposed the joint compression model, which simultaneously considers the ngram model and dependency parse tree of the compressed sentence. However, the time complexity greatly increases since the parse tree dynamically depends on the compression. They used Integer Linear Programming (ILP) for inference which requires exponential running time in the worst case. In this paper, we propose a new exact decoding algorithm for the joint model using dynamic programming. Our method extends Eisner’s cubic time parsing algorithm by adding signatures to each span, which indica</context>
</contexts>
<marker>Genest, Lapalme, 2012</marker>
<rawString>Pierre-Etienne Genest and Guy Lapalme. 2012. Fully abstractive approach to guided summarization. In Proceedings of the ACL, pages 354–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Li</author>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Yang Liu</author>
</authors>
<title>Document summarization via guided sentence compression.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<contexts>
<context position="1135" citStr="Li et al., 2013" startWordPosition="172" endWordPosition="175"> each span, indicating the number of deleted words and the rightmost kept word within the span. The second algorithm is a fast approximation of the first one. It relaxes the compression ratio constraint using Lagrangian relaxation, and thereby requires O(n4) running time. Experimental results on the popular sentence compression corpus demonstrate the effectiveness and efficiency of our proposed approach. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In fact, we parsed t</context>
</contexts>
<marker>Li, Liu, Weng, Liu, 2013</marker>
<rawString>Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013. Document summarization via guided sentence compression. In Proceedings of EMNLP, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Yang Liu</author>
</authors>
<title>From extractive to abstractive meeting summaries: Can it be done by sentence compression?</title>
<date>2009</date>
<booktitle>In Proceedings of ACLIJCNLP 2009,</booktitle>
<pages>261--264</pages>
<contexts>
<context position="1118" citStr="Liu and Liu, 2009" startWordPosition="168" endWordPosition="171">atures are added to each span, indicating the number of deleted words and the rightmost kept word within the span. The second algorithm is a fast approximation of the first one. It relaxes the compression ratio constraint using Lagrangian relaxation, and thereby requires O(n4) running time. Experimental results on the popular sentence compression corpus demonstrate the effectiveness and efficiency of our proposed approach. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In </context>
</contexts>
<marker>Liu, Liu, 2009</marker>
<rawString>Fei Liu and Yang Liu. 2009. From extractive to abstractive meeting summaries: Can it be done by sentence compression? In Proceedings of ACLIJCNLP 2009, pages 261–264, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="1160" citStr="Martins and Smith, 2009" startWordPosition="176" endWordPosition="179">ating the number of deleted words and the rightmost kept word within the span. The second algorithm is a fast approximation of the first one. It relaxes the compression ratio constraint using Lagrangian relaxation, and thereby requires O(n4) running time. Experimental results on the popular sentence compression corpus demonstrate the effectiveness and efficiency of our proposed approach. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In fact, we parsed the Edinburgh sentence com</context>
<context position="12250" citStr="Martins and Smith, 2009" startWordPosition="2285" endWordPosition="2288"> optimal z, y can be found using a simpler version of the algorithm above. We drop the signature of the virtual arc number from each span, and thus obtain an O(n4) time algorithm. Space complexity is O(n3). Fixing z, y, the dual variable is updated by A = A + α(L − ∑ zi) i where α &gt; 0 is the learning rate. In this paper, our choice of α is the same as (Rush et al., 2010). 4 Experiments 4.1 Data and Settings We evaluate our method on the data set from (Clarke and Lapata, 2008). It includes 82 newswire articles with manually produced compression for each sentence. We use the same partitions as (Martins and Smith, 2009), i.e., 1,188 sentences for training and 441 for testing. Our model is discriminative – the scores of the unigrams, bigrams and dependency arcs are the linear functions of features, that is, wtok i = vT f(xi), where f is the feature vector of xi, and v is the weight vector of features. The learning task is to estimate the feature weight vector based on the manually compressed sentences. We run a second order dependency parser trained on the English Penn Treebank corpus to generate the parse trees of the compressed sentences. Then we augment these parse trees by adding virtual arcs and get the </context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>Andr´e F. T. Martins and Noah A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative Sentence Compression with Soft Syntactic Constraints.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<contexts>
<context position="1368" citStr="McDonald, 2006" startWordPosition="212" endWordPosition="214">nd thereby requires O(n4) running time. Experimental results on the popular sentence compression corpus demonstrate the effectiveness and efficiency of our proposed approach. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In fact, we parsed the Edinburgh sentence compression corpus using the MSTparserl, 1http://sourceforge.net/projects/mstparser/ ROOT Warren says the economy continues the steady improvement ROOT Warren says the economy continues the steady improvement Fi</context>
<context position="13942" citStr="McDonald, 2006" startWordPosition="2574" endWordPosition="2575">culates the F1 score of the grammatical relations labeled by RASP (Briscoe and Carroll, 2002). We compare our method with other 4 state-ofthe-art systems. The first is linear chain CRFs, where the compression task is casted as a binary sequence labeling problem. It usually achieves high unigram F1 score but low grammatical relation F1 score since it only considers the local interdependence between adjacent words. The second is the subtree deletion model (Berg-Kirkpatrick et al., 2011) which is solved by integer linear programming (ILP)2. The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference. The last one jointly infers tree structures alongside bigrams using ILP (Thadani and McKeown, 2013). For fair comparison, systems were restricted to produce compressions that matched their average gold compression rate if possible. 4.2 Features Three types of features are used to learn our model: unigram features, bigram features and dependency features, as shown in Table 1. We also use the in-between features proposed by (McDonald et 2We use Gurobi as the ILP solver in the paper. http://www.gurobi.com/ ∑max z,y i ∑ wtok i zi + i,j min</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative Sentence Compression with Soft Syntactic Constraints. In Proceedings of EACL, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hajime Morita</author>
<author>Ryohei Sasano</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Subtree extractive summarization via submodular maximization.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1023--1032</pages>
<contexts>
<context position="1482" citStr="Morita et al., 2013" startWordPosition="229" endWordPosition="232">trate the effectiveness and efficiency of our proposed approach. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In fact, we parsed the Edinburgh sentence compression corpus using the MSTparserl, 1http://sourceforge.net/projects/mstparser/ ROOT Warren says the economy continues the steady improvement ROOT Warren says the economy continues the steady improvement Figure 1: The compressed sentence is not a subtree of the original sentence. Words in gray are removed. and found th</context>
</contexts>
<marker>Morita, Sasano, Takamura, Okumura, 2013</marker>
<rawString>Hajime Morita, Ryohei Sasano, Hiroya Takamura, and Manabu Okumura. 2013. Subtree extractive summarization via submodular maximization. In Proceedings of ACL, pages 1023–1032, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Sampath Kannan</author>
<author>Mitchell Marcus</author>
</authors>
<title>Finding optimal 1-endpoint-crossing trees.</title>
<date>2013</date>
<booktitle>In Transactions of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<marker>Pitler, Kannan, Marcus, 2013</marker>
<rawString>Emily Pitler, Sampath Kannan, and Mitchell Marcus. 2013. Finding optimal 1-endpoint-crossing trees. In Transactions of the Association for Computational Linguistics, 2013 Volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Fast joint compression and summarization via graph cuts.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1492--1502</pages>
<contexts>
<context position="1204" citStr="Qian and Liu, 2013" startWordPosition="184" endWordPosition="187">ost kept word within the span. The second algorithm is a fast approximation of the first one. It relaxes the compression ratio constraint using Lagrangian relaxation, and thereby requires O(n4) running time. Experimental results on the popular sentence compression corpus demonstrate the effectiveness and efficiency of our proposed approach. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In fact, we parsed the Edinburgh sentence compression corpus using the MSTparserl, 1http:</context>
</contexts>
<marker>Qian, Liu, 2013</marker>
<rawString>Xian Qian and Yang Liu. 2013. Fast joint compression and summarization via graph cuts. In Proceedings of EMNLP, pages 1492–1502, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="11999" citStr="Rush et al., 2010" startWordPosition="2242" endWordPosition="2245">nt ∑i zi = L is relaxed by Lagrangian Relaxation. The relaxed version of Problem (1) is dep wij zizjyij (2) + ∑ wij zj bg r zi ∏ i&lt;k&lt;j i&lt;j +A( ∑ zi − L) i s.t. z is binary y is a projective parse tree over the subgraph: {xi|zi = 11 Fixing A, the optimal z, y can be found using a simpler version of the algorithm above. We drop the signature of the virtual arc number from each span, and thus obtain an O(n4) time algorithm. Space complexity is O(n3). Fixing z, y, the dual variable is updated by A = A + α(L − ∑ zi) i where α &gt; 0 is the learning rate. In this paper, our choice of α is the same as (Rush et al., 2010). 4 Experiments 4.1 Data and Settings We evaluate our method on the data set from (Clarke and Lapata, 2008). It includes 82 newswire articles with manually produced compression for each sentence. We use the same partitions as (Martins and Smith, 2009), i.e., 1,188 sentences for training and 441 for testing. Our model is discriminative – the scores of the unigrams, bigrams and dependency arcs are the linear functions of features, that is, wtok i = vT f(xi), where f is the feature vector of xi, and v is the weight vector of features. The learning task is to estimate the feature weight vector bas</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kapil Thadani</author>
<author>Kathleen McKeown</author>
</authors>
<title>Sentence compression with joint structural inference.</title>
<date>2013</date>
<booktitle>In Proceedings of the CoNLL,</booktitle>
<contexts>
<context position="2455" citStr="Thadani and McKeown, 2013" startWordPosition="382" endWordPosition="385">s/mstparser/ ROOT Warren says the economy continues the steady improvement ROOT Warren says the economy continues the steady improvement Figure 1: The compressed sentence is not a subtree of the original sentence. Words in gray are removed. and found that 2561 of 5379 sentences (47.6%) do not satisfy the subtree deletion model. Methods beyond the subtree model are also explored. Trevor et al. proposed synchronous tree substitution grammar (Cohn and Lapata, 2009), which allows local distortion of the tree topology and can thus naturally capture structural mismatches. (Genest and Lapalme, 2012; Thadani and McKeown, 2013) proposed the joint compression model, which simultaneously considers the ngram model and dependency parse tree of the compressed sentence. However, the time complexity greatly increases since the parse tree dynamically depends on the compression. They used Integer Linear Programming (ILP) for inference which requires exponential running time in the worst case. In this paper, we propose a new exact decoding algorithm for the joint model using dynamic programming. Our method extends Eisner’s cubic time parsing algorithm by adding signatures to each span, which indicate the number of deleted wor</context>
<context position="14100" citStr="Thadani and McKeown, 2013" startWordPosition="2595" endWordPosition="2598">ystems. The first is linear chain CRFs, where the compression task is casted as a binary sequence labeling problem. It usually achieves high unigram F1 score but low grammatical relation F1 score since it only considers the local interdependence between adjacent words. The second is the subtree deletion model (Berg-Kirkpatrick et al., 2011) which is solved by integer linear programming (ILP)2. The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference. The last one jointly infers tree structures alongside bigrams using ILP (Thadani and McKeown, 2013). For fair comparison, systems were restricted to produce compressions that matched their average gold compression rate if possible. 4.2 Features Three types of features are used to learn our model: unigram features, bigram features and dependency features, as shown in Table 1. We also use the in-between features proposed by (McDonald et 2We use Gurobi as the ILP solver in the paper. http://www.gurobi.com/ ∑max z,y i ∑ wtok i zi + i,j min λ (1 − zk) 330 Features for unigram xi wi−2, wi−1, wi, wi+1, wi+2 ti−2, ti−1, ti, ti+1, ti+2 witi wi−1wi, wiwi+1 ti−2ti−1, ti−1ti, titi+1, ti+1ti+2 ti−2ti−1t</context>
</contexts>
<marker>Thadani, McKeown, 2013</marker>
<rawString>Kapil Thadani and Kathleen McKeown. 2013. Sentence compression with joint structural inference. In Proceedings of the CoNLL, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>