<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002709">
<title confidence="0.998402">
Joint Training of Dependency Parsing Filters through
Latent Support Vector Machines
</title>
<author confidence="0.999643">
Colin Cherry Shane Bergsma
</author>
<affiliation confidence="0.9923175">
Institute for Information Technology Center for Language and Speech Processing
National Research Council Canada Johns Hopkins University
</affiliation>
<email confidence="0.996122">
colin.cherry@nrc-cnrc.gc.ca sbergsma@jhu.edu
</email>
<sectionHeader confidence="0.993839" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999789142857143">
Graph-based dependency parsing can be sped
up significantly if implausible arcs are elim-
inated from the search-space before parsing
begins. State-of-the-art methods for arc fil-
tering use separate classifiers to make point-
wise decisions about the tree; they label tokens
with roles such as root, leaf, or attaches-to-
the-left, and then filter arcs accordingly. Be-
cause these classifiers overlap substantially in
their filtering consequences, we propose to
train them jointly, so that each classifier can
focus on the gaps of the others. We inte-
grate the various pointwise decisions as latent
variables in a single arc-level SVM classifier.
This novel framework allows us to combine
nine pointwise filters, and adjust their sensi-
tivity using a shared threshold based on arc
length. Our system filters 32% more arcs than
the independently-trained classifiers, without
reducing filtering speed. This leads to faster
parsing with no reduction in accuracy.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974384615384">
A dependency tree represents syntactic relationships
between words using directed arcs (Me´lˇcuk, 1987).
Each token in the sentence is a node in the tree,
and each arc connects a head to its modifier. There
are two dominant approaches to dependency pars-
ing: graph-based and transition-based, where graph-
based parsing is understood to be slower, but often
more accurate (McDonald and Nivre, 2007).
In the graph-based setting, a complete search
finds the highest-scoring tree under a model that de-
composes over one or two arcs at a time. Much of
the time for parsing is spent scoring each poten-
tial arc in the complete dependency graph (John-
son, 2007), one for each ordered word-pair in the
sentence. Potential arcs are scored using rich linear
models that are discriminatively trained to maximize
parsing accuracy (McDonald et al., 2005). The vast
majority of these arcs are bad; in an n-word sen-
tence, only n of the n2 potential arcs are correct. If
many arcs can be filtered before parsing begins, then
the entire process can be sped up substantially.
Previously, we proposed a cascade of filters to
prune potential arcs (Bergsma and Cherry, 2010).
One stage of this cascade operates one token at a
time, labeling each token t according to various roles
in the tree:
</bodyText>
<listItem confidence="0.934022333333333">
• Not-a-head (NaH): t is not the head of any arc
• Head-to-left (HtL{1/5/*}): t’s head is to its
left within 1, 5 or any number of words
• Head-to-right (HtR{1/5/*}): as head-to-left
• Root (Root): t is the root node, which elimi-
nates arcs according to projectivity
</listItem>
<bodyText confidence="0.999455466666667">
Similar to Roark and Hollingshead (2008), each role
has a corresponding binary classifier. These token-
role classifiers were shown to be more effective than
vine parsing (Eisner and Smith, 2005; Dreyer et
al., 2006), a competing filtering scheme that filters
arcs based on their length (leveraging the observa-
tion that most dependencies are short).
In this work, we propose a novel filtering frame-
work that integrates all the information used in
token-role classification and vine parsing, but of-
fers a number of advantages. In our previous work,
classifier decisions would often overlap: different
token-role classifiers would agree to filter the same
arc. Based on this observation, we propose a joint
training framework where only the most confident
</bodyText>
<page confidence="0.941969">
200
</page>
<note confidence="0.735572">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 200–205,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.846412111111111">
Bob1 ate2 the3 pizza4 with5 his6 salad7 fork8
NN VBD DT NN IN POS NN NN
NaH3 (T) HtR*6 (T)
(T)
(F)
(F)
HtR56
HtR16
HtL16
</figure>
<figureCaption confidence="0.9401986">
Figure 1: The dotted arc can be filtered by labeling any of the
boxed roles as True; i.e., predicting that the head thea is not the
head of any arc, or that the modifier hiss attaches elsewhere.
Role truth values, derived from the gold-standard tree (in grey),
are listed adjacent to the boxes, in parentheses.
</figureCaption>
<bodyText confidence="0.999991476190476">
classifier is given credit for eliminating an arc. The
identity of the responsible classifier is modeled as
a latent variable, which is filled in during training
using a latent SVM (LSVM) formulation. Our use
of an LSVM to assign credit during joint training
differs substantially from previous LSVM applica-
tions, which have induced latent linguistic structures
(Cherry and Quirk, 2008; Chang et al., 2010) or sen-
tence labels (Yessenalina et al., 2010).
In our framework, each classifier learns to fo-
cus on the cases where the other classifiers are less
confident. Furthermore, the integrated approach di-
rectly optimizes for arc-filtering accuracy (rather
than token-labeling fidelity). We trade-off filtering
precision/recall using two hyperparameters, while
the previous approach trained classifiers for eight
different tasks resulting in sixteen hyperparameters.
Ultimately, the biggest gains in filter quality are
achieved when we jointly train the token-role classi-
fiers together with a dynamic threshold that is based
on arc length and shared across all classifiers.
</bodyText>
<sectionHeader confidence="0.776544" genericHeader="method">
2 Joint Training of Token Roles
</sectionHeader>
<bodyText confidence="0.999916475">
In our previous system, filtering is conducted by
training a separate SVM classifier for each of the
eight token-roles described in Section 1. Each clas-
sifier uses a training set with one example per tree-
bank token, where each token is assigned a binary
label derived from the gold-standard tree. Figure 1
depicts five of the eight token roles, along with their
truth values. The role labelers can be tuned for high
precision with label-specific cost parameters; these
are tuned separately for each classifier. At test time,
each of the eight classifiers assigns a binary label
to each of the n tokens in the sentence. Potential
arcs are then filtered from the complete dependency
graph according to these token labels. In Figure 1,
a positive assignment to any of the indicated token-
roles is sufficient to filter the dotted arc.
In the current work, we maintain almost the same
test-time framework, but we alter training substan-
tially, so that the various token-role classifiers are
trained jointly. To do so, we propose a classifica-
tion scheme focused on arcs.1 During training, each
arc is assigned a filtering event as a latent variable.
Events generalize the token-roles from our previous
system (e.g. NaH3, HtR*6). Events are assigned bi-
nary labels during filtering; positive events are said
to be detected. In general, events can correspond
to any phenomenon, so long as the following holds:
For each arc a, we must be able to deterministically
construct the set Za of all events that would filter
a if detected.2 Figure 1 shows that Zthe,,hiss =
{NaH3, HtR*6, HtR56, HtR16, HtL161.
To detect events, we maintain the eight token-role
classifiers from the previous system, but they be-
come subclassifiers of our joint system. For no-
tational convenience, we pack them into a single
weight vector w. Thus, the event z = NaH3 is de-
tected only if w · 46(NaH3) &gt; 0, where 46(z) is z’s
feature vector. Given this notation, we can cast the
filtering decision for an arc a as a maximum. We
filter a only if:
</bodyText>
<equation confidence="0.467262">
f(Za) &gt; 0 where f(Za) = max [w · 46(z)] (1)
zEZo
</equation>
<bodyText confidence="0.874164352941176">
We have reformulated our problem, which previ-
ously involved a number of independent token clas-
sifiers, as a single arc classifier f() with an inner max
over latent events. Note the asymmetry inherent in
(1). To filter an arc, [w · 46(z) &gt; 0] must hold for at
least one z E Za; but to keep an arc, [w · 46(z) G 0]
must hold for all z E Za. Also note that tokens
have completely disappeared from our formalism:
the classifier is framed only in terms of events and
arcs; token-roles are encapsulated inside events.
To provide a large-margin training objective for
our joint classifier, we adapt the latent SVM (Felzen-
1A joint filtering formalism for CFG parsing or SCFG trans-
lation would likewise focus on hyper-edges or spans.
2This same requirement is also needed by the previous,
independently-trained filters at test time, so that arcs can be fil-
tered according to the roles assigned to tokens.
</bodyText>
<page confidence="0.992864">
201
</page>
<bodyText confidence="0.9995164">
szwalb et al., 2010; Yu and Joachims, 2009) to our
problem. Given a training set A of (a, y) pairs,
where a is an arc in context and y is the correct filter
label for a (1 to filter, 0 otherwise), LSVM training
selects w� to minimize:
</bodyText>
<equation confidence="0.9814525">
1 2||�w||2+ Cy max [0,1 + f(Za|-y) − f(Za|y)]
(a,y)EA
</equation>
<bodyText confidence="0.998332475">
(2)
where Cy is a label-specific regularization parame-
ter, and the event set Z is now conditioned on the
label y: Za|1 = Za, and Za|0 = {Nonea}. Nonea
is a rejection event, which indicates that a is not
filtered. The rejection event slightly alters our de-
cision rule; rather than thresholding at 0, we now
filter a only if f(Za) &gt; w� · 46(Nonea). One can set
4b(Nonea) +— 0 for all a to fix the threshold at 0.
Though not convex, (2) can be solved to a lo-
cal minimum with an EM-like alternating minimiza-
tion procedure (Felzenszwalb et al., 2010; Yu and
Joachims, 2009). The learner alternates between
picking the highest-scoring latent event za E Za|y
for each example (a, y), and training a multiclass
SVM to solve an approximation to (2) where Za|y is
replaced with {za}. Intuitively, the first step assigns
the event za to a, making za responsible for a’s ob-
served label. The second step optimizes the model to
ensure that each za is detected, leading to the desired
arc-filtering decisions. As the process iterates, event
assignment becomes increasingly refined, leading to
a more accurate joint filter.
The resulting joint filter has only two hyper-
parameters: the label-specific cost parameters C1
and Co. These allow us to tune our system for high
precision by increasing the cost of misclassifying an
arc that should not be filtered (C1 « Co).
Joint training also implicitly affects the relative
costs of subclassifier decisions. By minimizing an
arc-level hinge loss with latent events (which in turn
correspond to token-roles), we assign costs to token-
roles based on arc accuracy. Consequently, 1) A
token-level decision that affects multiple arcs im-
pacts multiple instances of hinge loss, and 2) No
extra credit (penalty) is given for multiple decisions
that (in)correctly filter the same arc. Therefore, an
NaH decision that filters thirty arcs is given more
weight than an HtL5 decision that filters only one
(Item 1), unless those thirty arcs are already filtered
</bodyText>
<equation confidence="0.842051333333333">
The1 big2 dog3 chased4 the5 cat6
DT ADJ NN VBD DT NN
NaH3 = 0.5
</equation>
<figureCaption confidence="0.9962495">
Figure 2: A hypothetical example of dynamic threshold-
ing, where a weak assertion that do93 should not be a head
(w · 6(NaH3) = 0.5) is sufficient to rule out two arcs. Each
arc’s threshold (w · 6(Nonea)) is shown next to its arrow.
</figureCaption>
<bodyText confidence="0.961959">
by higher-scoring subclassifiers (Item 2).
</bodyText>
<sectionHeader confidence="0.9972" genericHeader="method">
3 Accounting for Arc Length
</sectionHeader>
<bodyText confidence="0.984706107142857">
We can extend our system by expanding our event
set Z. By adding an arc-level event Vinea to each
Za, we can introduce a vine filter to prune long arcs.
Similarly, we have already introduced another arc-
level event, the rejection event Nonea. By assign-
ing features to Nonea, we learn a dynamic thresh-
old on all filters, which considers properties of the
arc before acting on any other event. We parameter-
ize both Vinea and Nonea with the same two fea-
tures, inspired by tag-specific vine parsing (Eisner
and Smith, 2005):
� Bias 1
HeadTag ModTag Dir(a) Len(a)
where HeadTag ModTag Dir(a) concatenates the
part-of-speech tags of a’s head and modifier tokens
to its direction (left or right), and Len(a) gives the
unsigned distance between a’s head and modifier.
In the context of Vinea, these two features al-
low the system to learn tag-pair-specific limits on
arc length. In the context of Nonea, these features
protect short arcs and arcs that connect frequently-
linked tag-pairs, allowing our token-role filters to be
more aggressive on arcs that do not have these char-
acteristics. The dynamic threshold also alters our
interpretation of filtering events: where before they
were either active or inactive, events are now as-
signed scores, which are compared with the thresh-
old to make final filtering decisions (Figure 2).3
</bodyText>
<footnote confidence="0.8955422">
3Because tokens and arcs are scored independently and cou-
pled only through score comparison, the impact of Vinea and
Nonea on classification speed should be no greater than doing
vine and token-role filtering in sequence. In practice, it is no
slower than running token-role filtering on its own.
</footnote>
<table confidence="0.243353">
1.0 1.1 0.6 0.3 0.2
</table>
<page confidence="0.995366">
202
</page>
<sectionHeader confidence="0.999068" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999991852941176">
We extract dependency structures from the Penn
Treebank using the head rules of Yamada and Mat-
sumoto (2003).4 We divide the Treebank into train
(sections 2–21), development (22) and test (23). We
part-of-speech tag our data using a perceptron tagger
similar to the one described by Collins (2002). The
training set is tagged with jack-knifing: the data is
split into 10 folds and each fold is tagged by a sys-
tem trained on the other 9 folds. Development and
test sets are tagged using the entire training set.
We train our joint filter using an in-house latent
SVM framework, which repeatedly calls a multi-
class exponentiated gradient SVM (Collins et al.,
2008). LSVM training was stopped after 4 itera-
tions, as determined during development.5 For the
token-role classifiers, we re-implement the Bergsma
and Cherry (2010) feature set, initializing w with
high-precision subclassifiers trained independently
for each token-role. Vine and None subclassifiers
are initialized with a zero vector. At test time, we
extract subclassifiers from the joint weight vector,
and use them as parameters in the filtering tools of
Bergsma and Cherry (2010).6
Parsing experiments are carried out using the
MST parser (McDonald et al., 2005),7 which we
have modified to filter arcs before carrying out fea-
ture extraction. It is trained using 5-best MIRA
(Crammer and Singer, 2003).
Following Bergsma and Cherry (2010), we mea-
sure intrinsic filter quality with reduction, the pro-
portion of total arcs removed, and coverage, the pro-
portion of true arcs retained. For parsing results, we
present dependency accuracy, the percentage of to-
kens that are assigned the correct head.
</bodyText>
<subsectionHeader confidence="0.999148">
4.1 Impact of Joint Training
</subsectionHeader>
<bodyText confidence="0.9991295">
Our technical contribution consists of our proposed
joint training scheme for token-role filters, along
</bodyText>
<footnote confidence="0.997793">
4As implemented at http://w3.msi.vxu.se/∼nivre/
research/Penn2Malt.html
5The LSVM is well on its way to convergence: fewer than
3% of arcs have event assignments that are still in flux.
6http://code.google.com/p/arcfilter/. Since our
contribution is mainly in better filter training, we were able to
use the arcfilter (testing) code with only small changes. We have
added our new joint filter, along with the Joint P1 model to the
arcfilter package, labeled as ultra filters.
7http://sourceforge.net/projects/mstparser/
</footnote>
<table confidence="0.9981048">
Indep. Joint
System Cov. Red. Cov. Red.
Token 99.73 60.5 99.71 59.0
+ Vine 99.62 68.6 99.69 63.3
+ None N/A 99.76 71.6
</table>
<tableCaption confidence="0.999976">
Table 1: Ablation analysis of intrinsic filter quality.
</tableCaption>
<bodyText confidence="0.999980171428571">
with two extensions: the addition of vine filters
(Vine) and a dynamic threshold (None). Using pa-
rameters determined to perform well during devel-
opment,8 we examine test-set performance as we in-
corporate each of these components. For the token-
role and vine subclassifiers, we compare against an
independently-trained ensemble of the same classi-
fiers.9 Note that None cannot be trained indepen-
dently, as its shared dynamic threshold considers arc
and token views of the data simultaneously. Results
are shown in Table 1.
Our complete system outperforms all variants in
terms of both coverage and reduction. However, one
can see that neither joint system is able to outper-
form its independently-trained counter-part without
the dynamic threshold provided by None. This is
because the desirable credit-assignment properties
of our joint training procedure are achieved through
duplication (Zadrozny et al., 2003). That is, the
LSVM knows that a specific event is important be-
cause it appears in event sets Za, for many arcs from
the same sentence. Without None, the filtering deci-
sions implied by each copy of an event are identical.
Because these replicated events are associated with
arcs that are presented to the LSVM as independent
examples, they appear to be not only important, but
also low-variance, and therefore easy. This leads to
overfitting. We had hoped that the benefits of joint
training would outweigh this drawback, but our re-
sults show that they do not. However, in addition to
its other desirable properties (protecting short arcs),
the dynamic threshold imposed by None restores in-
dependence between arcs that share a common event
(Figure 2). This alleviates overfitting and enables
strong performance.
</bodyText>
<footnote confidence="0.9449572">
8C0=1e-2, C1=1e-5
9Each subclassifier is a token-level SVM trained with token-
role labels extracted from the training treebank. Using develop-
ment data, we search over regularization parameters so that each
classifier yields more than 99.93% arc-level coverage.
</footnote>
<page confidence="0.994145">
203
</page>
<table confidence="0.998806857142857">
Filter Filter Intrinsic MST-1 MST-2
Cov. Red. Time Acc. Sent/sec* Acc. Sent/sec*
None 100.00 00.0 0s 91.28 16 92.05 10
B&amp;C R+L 99.70 54.1 7s 91.24 29 92.00 17
Joint P1 99.76 71.6 7s 91.28 38 92.06 22
B&amp;C R+L+Q 99.43 78.3 19s 91.23 35 91.98 22
Joint P2 99.56 77.9 7s 91.29 44 92.05 25
</table>
<tableCaption confidence="0.9537295">
Table 2: Parsing with jointly-trained filters outperforms independently-trained filters (R+L), as well as a more complex
cascade (R+L+Q). *Accounts for total time spent parsing and applying filters, averaged over five runs.
</tableCaption>
<subsectionHeader confidence="0.996961">
4.2 Comparison to the state of the art
</subsectionHeader>
<bodyText confidence="0.998316039215686">
We directly compare our filters to those of Bergsma
and Cherry (2010) in terms of both intrinsic fil-
ter quality and impact on the MST parser. The
B&amp;C system consists of three stages: rules (R), lin-
ear token-role filters (L) and quadratic arc filters
(Q). The Q stage uses rich arc-level features simi-
lar to those of the MST parser. We compare against
independently-trained token-role filters (R+L), as
well as the complete cascade (R+L+Q), using the
models provided online.10 Our comparison points,
Joint P1 and P2 were built by tuning our complete
joint system to roughly match the coverage values
of R+L and R+L+Q on development data.11 Results
are shown in Table 2.
Comparing Joint P1 to R+L, we can see that for
a fixed set of pointwise filters, joint training with
a dynamic threshold outperforms independent train-
ing substantially. We achieve a 32% improvement
in reduction with no impact on coverage and no in-
crease in filtering overhead (time).
Comparing Joint P2 to R+L+Q, we see that Joint
P2 achieves similar levels of reduction with far less
filtering overhead; our filters take only 7 seconds
to apply instead of 19. This increases the speed of
the (already fast) filtered MST-1 parser from 35 sen-
tences per second to 44, resulting in a total speed-
up of 2.75 with respect to the unfiltered parser. The
improvement is less impressive for MST-2, where
the overhead for filter application is a less substan-
tial fraction of parsing time; however, our training
framework also has other benefits with respect to
R+L+Q, including a single unified training algo-
10Results are not identical to those reported in our previous
paper, due to our use of a different part-of-speech tagger. Note
that parsing accuracies for the B&amp;C systems have improved.
11P1: C0=1e-2, C1=1e-5, P2: C0=1e-2, C1=2e-5
rithm, fewer hyper-parameters and a smaller test-
time memory footprint. Finally, the jointly trained
filters have no impact on parsing accuracy, where
both B&amp;C filters have a small negative effect.
The performance of Joint-P2+MST-2 is compa-
rable to the system of Huang and Sagae (2010),
who report a parsing speed of 25 sentences per
second and an accuracy of 92.1 on the same test
set, using a transition-based parser enhanced with
dynamic-programming state combination.12 Graph-
based and transition-based systems tend to make dif-
ferent types of errors (McDonald and Nivre, 2007).
Therefore, having fast, accurate parsers for both ap-
proaches presents an opportunity for large-scale, ro-
bust parser combination.
</bodyText>
<sectionHeader confidence="0.998903" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999018125">
We have presented a novel use of latent SVM
technology to train a number of filters jointly,
with a shared dynamic threshold. By training a
family of dependency filters in this manner, each
subclassifier focuses on the examples where it is
most needed, with our dynamic threshold adjust-
ing filter sensitivity based on arc length. This al-
lows us to outperform a 3-stage filter cascade in
terms of speed-up, while also reducing the im-
pact of filtering on parsing accuracy. Our filter-
ing code and trained models are available online at
http://code.google.com/p/arcfilter. In
the future, we plan to apply our joint training tech-
nique to other rich filtering regimes (Zhang et al.,
2010), and to other NLP problems that combine the
predictions of overlapping classifiers.
</bodyText>
<footnote confidence="0.9670835">
12The usual caveats for cross-machine, cross-implementation
speed comparisons apply.
</footnote>
<page confidence="0.997612">
204
</page>
<sectionHeader confidence="0.995841" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999739967213115">
Shane Bergsma and Colin Cherry. 2010. Fast and accu-
rate arc filtering for dependency parsing. In COLING.
Ming-Wei Chang, Dan Goldwasser, Dan Roth, and Vivek
Srikumar. 2010. Discriminative learning over con-
strained latent representations. In HLT-NAACL.
Colin Cherry and Chris Quirk. 2008. Discriminative,
syntactic language modeling through latent SVMs. In
AMTA.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter L. Bartlett. 2008. Exponentiated
gradient algorithms for conditional random fields and
max-margin markov networks. JMLR, 9:1775–1822.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In EMNLP.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. JMLR,
3:951–991.
Markus Dreyer, David A. Smith, and Noah A. Smith.
2006. Vine parsing and minimum risk reranking for
speed and precision. In CoNLL.
Jason Eisner and Noah A. Smith. 2005. Parsing with soft
and hard constraints on dependency length. In IWPT.
Pedro F. Felzenszwalb, Ross B. Girshick, David
McAllester, and Deva Ramanan. 2010. Object detec-
tion with discriminatively trained part based models.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 32(9).
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In ACL.
Mark Johnson. 2007. Transforming projective bilexical
dependency grammars into efficiently-parsable CFGs
with unfold-fold. In ACL.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In EMNLP-CoNLL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL.
Igor A. Me´lˇcuk. 1987. Dependency syntax: theory and
practice. State University of New York Press.
Brian Roark and Kristy Hollingshead. 2008. Classifying
chart cells for quadratic complexity context-free infer-
ence. In COLING.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
IWPT.
Ainur Yessenalina, Yisong Yue, and Claire Cardie. 2010.
Multi-level structured models for document-level sen-
timent classification. In EMNLP.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
ICML.
Bianca Zadrozny, John Langford, and Naoki Abe. 2003.
Cost-sensitive learning by cost-proportionate example
weighting. In Third IEEE International Conference on
Data Mining.
Yue Zhang, Byung-Gyu Ahn, Stephen Clark, Curt Van
Wyk, James R. Curran, and Laura Rimell. 2010.
Chart pruning for fast lexicalised-grammar parsing. In
EMNLP.
</reference>
<page confidence="0.998858">
205
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960986">
<title confidence="0.9973835">Joint Training of Dependency Parsing Filters Latent Support Vector Machines</title>
<author confidence="0.999165">Colin Cherry Shane Bergsma</author>
<affiliation confidence="0.9991865">Institute for Information Technology Center for Language and Speech Processing National Research Council Canada Johns Hopkins University</affiliation>
<email confidence="0.988012">colin.cherry@nrc-cnrc.gc.casbergsma@jhu.edu</email>
<abstract confidence="0.999074954545455">Graph-based dependency parsing can be sped up significantly if implausible arcs are eliminated from the search-space before parsing begins. State-of-the-art methods for arc filtering use separate classifiers to make pointwise decisions about the tree; they label tokens roles such as or attaches-toand then filter arcs accordingly. Because these classifiers overlap substantially in their filtering consequences, we propose to train them jointly, so that each classifier can focus on the gaps of the others. We integrate the various pointwise decisions as latent variables in a single arc-level SVM classifier. This novel framework allows us to combine nine pointwise filters, and adjust their sensitivity using a shared threshold based on arc length. Our system filters 32% more arcs than the independently-trained classifiers, without reducing filtering speed. This leads to faster parsing with no reduction in accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Colin Cherry</author>
</authors>
<title>Fast and accurate arc filtering for dependency parsing.</title>
<date>2010</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="2415" citStr="Bergsma and Cherry, 2010" startWordPosition="369" endWordPosition="372">s at a time. Much of the time for parsing is spent scoring each potential arc in the complete dependency graph (Johnson, 2007), one for each ordered word-pair in the sentence. Potential arcs are scored using rich linear models that are discriminatively trained to maximize parsing accuracy (McDonald et al., 2005). The vast majority of these arcs are bad; in an n-word sentence, only n of the n2 potential arcs are correct. If many arcs can be filtered before parsing begins, then the entire process can be sped up substantially. Previously, we proposed a cascade of filters to prune potential arcs (Bergsma and Cherry, 2010). One stage of this cascade operates one token at a time, labeling each token t according to various roles in the tree: • Not-a-head (NaH): t is not the head of any arc • Head-to-left (HtL{1/5/*}): t’s head is to its left within 1, 5 or any number of words • Head-to-right (HtR{1/5/*}): as head-to-left • Root (Root): t is the root node, which eliminates arcs according to projectivity Similar to Roark and Hollingshead (2008), each role has a corresponding binary classifier. These tokenrole classifiers were shown to be more effective than vine parsing (Eisner and Smith, 2005; Dreyer et al., 2006)</context>
<context position="13359" citStr="Bergsma and Cherry (2010)" startWordPosition="2213" endWordPosition="2216">part-of-speech tag our data using a perceptron tagger similar to the one described by Collins (2002). The training set is tagged with jack-knifing: the data is split into 10 folds and each fold is tagged by a system trained on the other 9 folds. Development and test sets are tagged using the entire training set. We train our joint filter using an in-house latent SVM framework, which repeatedly calls a multiclass exponentiated gradient SVM (Collins et al., 2008). LSVM training was stopped after 4 iterations, as determined during development.5 For the token-role classifiers, we re-implement the Bergsma and Cherry (2010) feature set, initializing w with high-precision subclassifiers trained independently for each token-role. Vine and None subclassifiers are initialized with a zero vector. At test time, we extract subclassifiers from the joint weight vector, and use them as parameters in the filtering tools of Bergsma and Cherry (2010).6 Parsing experiments are carried out using the MST parser (McDonald et al., 2005),7 which we have modified to filter arcs before carrying out feature extraction. It is trained using 5-best MIRA (Crammer and Singer, 2003). Following Bergsma and Cherry (2010), we measure intrinsi</context>
<context position="17625" citStr="Bergsma and Cherry (2010)" startWordPosition="2881" endWordPosition="2884">age. 203 Filter Filter Intrinsic MST-1 MST-2 Cov. Red. Time Acc. Sent/sec* Acc. Sent/sec* None 100.00 00.0 0s 91.28 16 92.05 10 B&amp;C R+L 99.70 54.1 7s 91.24 29 92.00 17 Joint P1 99.76 71.6 7s 91.28 38 92.06 22 B&amp;C R+L+Q 99.43 78.3 19s 91.23 35 91.98 22 Joint P2 99.56 77.9 7s 91.29 44 92.05 25 Table 2: Parsing with jointly-trained filters outperforms independently-trained filters (R+L), as well as a more complex cascade (R+L+Q). *Accounts for total time spent parsing and applying filters, averaged over five runs. 4.2 Comparison to the state of the art We directly compare our filters to those of Bergsma and Cherry (2010) in terms of both intrinsic filter quality and impact on the MST parser. The B&amp;C system consists of three stages: rules (R), linear token-role filters (L) and quadratic arc filters (Q). The Q stage uses rich arc-level features similar to those of the MST parser. We compare against independently-trained token-role filters (R+L), as well as the complete cascade (R+L+Q), using the models provided online.10 Our comparison points, Joint P1 and P2 were built by tuning our complete joint system to roughly match the coverage values of R+L and R+L+Q on development data.11 Results are shown in Table 2. </context>
</contexts>
<marker>Bergsma, Cherry, 2010</marker>
<rawString>Shane Bergsma and Colin Cherry. 2010. Fast and accurate arc filtering for dependency parsing. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Wei Chang</author>
<author>Dan Goldwasser</author>
<author>Dan Roth</author>
<author>Vivek Srikumar</author>
</authors>
<title>Discriminative learning over constrained latent representations.</title>
<date>2010</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="4596" citStr="Chang et al., 2010" startWordPosition="726" endWordPosition="729">ing that the head thea is not the head of any arc, or that the modifier hiss attaches elsewhere. Role truth values, derived from the gold-standard tree (in grey), are listed adjacent to the boxes, in parentheses. classifier is given credit for eliminating an arc. The identity of the responsible classifier is modeled as a latent variable, which is filled in during training using a latent SVM (LSVM) formulation. Our use of an LSVM to assign credit during joint training differs substantially from previous LSVM applications, which have induced latent linguistic structures (Cherry and Quirk, 2008; Chang et al., 2010) or sentence labels (Yessenalina et al., 2010). In our framework, each classifier learns to focus on the cases where the other classifiers are less confident. Furthermore, the integrated approach directly optimizes for arc-filtering accuracy (rather than token-labeling fidelity). We trade-off filtering precision/recall using two hyperparameters, while the previous approach trained classifiers for eight different tasks resulting in sixteen hyperparameters. Ultimately, the biggest gains in filter quality are achieved when we jointly train the token-role classifiers together with a dynamic thresh</context>
</contexts>
<marker>Chang, Goldwasser, Roth, Srikumar, 2010</marker>
<rawString>Ming-Wei Chang, Dan Goldwasser, Dan Roth, and Vivek Srikumar. 2010. Discriminative learning over constrained latent representations. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Chris Quirk</author>
</authors>
<title>Discriminative, syntactic language modeling through latent SVMs.</title>
<date>2008</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="4575" citStr="Cherry and Quirk, 2008" startWordPosition="722" endWordPosition="725">s as True; i.e., predicting that the head thea is not the head of any arc, or that the modifier hiss attaches elsewhere. Role truth values, derived from the gold-standard tree (in grey), are listed adjacent to the boxes, in parentheses. classifier is given credit for eliminating an arc. The identity of the responsible classifier is modeled as a latent variable, which is filled in during training using a latent SVM (LSVM) formulation. Our use of an LSVM to assign credit during joint training differs substantially from previous LSVM applications, which have induced latent linguistic structures (Cherry and Quirk, 2008; Chang et al., 2010) or sentence labels (Yessenalina et al., 2010). In our framework, each classifier learns to focus on the cases where the other classifiers are less confident. Furthermore, the integrated approach directly optimizes for arc-filtering accuracy (rather than token-labeling fidelity). We trade-off filtering precision/recall using two hyperparameters, while the previous approach trained classifiers for eight different tasks resulting in sixteen hyperparameters. Ultimately, the biggest gains in filter quality are achieved when we jointly train the token-role classifiers together </context>
</contexts>
<marker>Cherry, Quirk, 2008</marker>
<rawString>Colin Cherry and Chris Quirk. 2008. Discriminative, syntactic language modeling through latent SVMs. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Amir Globerson</author>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Peter L Bartlett</author>
</authors>
<title>Exponentiated gradient algorithms for conditional random fields and max-margin markov networks.</title>
<date>2008</date>
<journal>JMLR,</journal>
<pages>9--1775</pages>
<contexts>
<context position="13199" citStr="Collins et al., 2008" startWordPosition="2190" endWordPosition="2193">e Penn Treebank using the head rules of Yamada and Matsumoto (2003).4 We divide the Treebank into train (sections 2–21), development (22) and test (23). We part-of-speech tag our data using a perceptron tagger similar to the one described by Collins (2002). The training set is tagged with jack-knifing: the data is split into 10 folds and each fold is tagged by a system trained on the other 9 folds. Development and test sets are tagged using the entire training set. We train our joint filter using an in-house latent SVM framework, which repeatedly calls a multiclass exponentiated gradient SVM (Collins et al., 2008). LSVM training was stopped after 4 iterations, as determined during development.5 For the token-role classifiers, we re-implement the Bergsma and Cherry (2010) feature set, initializing w with high-precision subclassifiers trained independently for each token-role. Vine and None subclassifiers are initialized with a zero vector. At test time, we extract subclassifiers from the joint weight vector, and use them as parameters in the filtering tools of Bergsma and Cherry (2010).6 Parsing experiments are carried out using the MST parser (McDonald et al., 2005),7 which we have modified to filter a</context>
</contexts>
<marker>Collins, Globerson, Koo, Carreras, Bartlett, 2008</marker>
<rawString>Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, and Peter L. Bartlett. 2008. Exponentiated gradient algorithms for conditional random fields and max-margin markov networks. JMLR, 9:1775–1822.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="12834" citStr="Collins (2002)" startWordPosition="2128" endWordPosition="2129">nd arcs are scored independently and coupled only through score comparison, the impact of Vinea and Nonea on classification speed should be no greater than doing vine and token-role filtering in sequence. In practice, it is no slower than running token-role filtering on its own. 1.0 1.1 0.6 0.3 0.2 202 4 Experiments We extract dependency structures from the Penn Treebank using the head rules of Yamada and Matsumoto (2003).4 We divide the Treebank into train (sections 2–21), development (22) and test (23). We part-of-speech tag our data using a perceptron tagger similar to the one described by Collins (2002). The training set is tagged with jack-knifing: the data is split into 10 folds and each fold is tagged by a system trained on the other 9 folds. Development and test sets are tagged using the entire training set. We train our joint filter using an in-house latent SVM framework, which repeatedly calls a multiclass exponentiated gradient SVM (Collins et al., 2008). LSVM training was stopped after 4 iterations, as determined during development.5 For the token-role classifiers, we re-implement the Bergsma and Cherry (2010) feature set, initializing w with high-precision subclassifiers trained ind</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>JMLR,</journal>
<pages>3--951</pages>
<contexts>
<context position="13901" citStr="Crammer and Singer, 2003" startWordPosition="2296" endWordPosition="2299">ent.5 For the token-role classifiers, we re-implement the Bergsma and Cherry (2010) feature set, initializing w with high-precision subclassifiers trained independently for each token-role. Vine and None subclassifiers are initialized with a zero vector. At test time, we extract subclassifiers from the joint weight vector, and use them as parameters in the filtering tools of Bergsma and Cherry (2010).6 Parsing experiments are carried out using the MST parser (McDonald et al., 2005),7 which we have modified to filter arcs before carrying out feature extraction. It is trained using 5-best MIRA (Crammer and Singer, 2003). Following Bergsma and Cherry (2010), we measure intrinsic filter quality with reduction, the proportion of total arcs removed, and coverage, the proportion of true arcs retained. For parsing results, we present dependency accuracy, the percentage of tokens that are assigned the correct head. 4.1 Impact of Joint Training Our technical contribution consists of our proposed joint training scheme for token-role filters, along 4As implemented at http://w3.msi.vxu.se/∼nivre/ research/Penn2Malt.html 5The LSVM is well on its way to convergence: fewer than 3% of arcs have event assignments that are s</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. JMLR, 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>David A Smith</author>
<author>Noah A Smith</author>
</authors>
<title>Vine parsing and minimum risk reranking for speed and precision.</title>
<date>2006</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="3015" citStr="Dreyer et al., 2006" startWordPosition="473" endWordPosition="476">ma and Cherry, 2010). One stage of this cascade operates one token at a time, labeling each token t according to various roles in the tree: • Not-a-head (NaH): t is not the head of any arc • Head-to-left (HtL{1/5/*}): t’s head is to its left within 1, 5 or any number of words • Head-to-right (HtR{1/5/*}): as head-to-left • Root (Root): t is the root node, which eliminates arcs according to projectivity Similar to Roark and Hollingshead (2008), each role has a corresponding binary classifier. These tokenrole classifiers were shown to be more effective than vine parsing (Eisner and Smith, 2005; Dreyer et al., 2006), a competing filtering scheme that filters arcs based on their length (leveraging the observation that most dependencies are short). In this work, we propose a novel filtering framework that integrates all the information used in token-role classification and vine parsing, but offers a number of advantages. In our previous work, classifier decisions would often overlap: different token-role classifiers would agree to filter the same arc. Based on this observation, we propose a joint training framework where only the most confident 200 Proceedings of the 49th Annual Meeting of the Association </context>
</contexts>
<marker>Dreyer, Smith, Smith, 2006</marker>
<rawString>Markus Dreyer, David A. Smith, and Noah A. Smith. 2006. Vine parsing and minimum risk reranking for speed and precision. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Noah A Smith</author>
</authors>
<title>Parsing with soft and hard constraints on dependency length.</title>
<date>2005</date>
<booktitle>In IWPT.</booktitle>
<contexts>
<context position="2993" citStr="Eisner and Smith, 2005" startWordPosition="469" endWordPosition="472">ne potential arcs (Bergsma and Cherry, 2010). One stage of this cascade operates one token at a time, labeling each token t according to various roles in the tree: • Not-a-head (NaH): t is not the head of any arc • Head-to-left (HtL{1/5/*}): t’s head is to its left within 1, 5 or any number of words • Head-to-right (HtR{1/5/*}): as head-to-left • Root (Root): t is the root node, which eliminates arcs according to projectivity Similar to Roark and Hollingshead (2008), each role has a corresponding binary classifier. These tokenrole classifiers were shown to be more effective than vine parsing (Eisner and Smith, 2005; Dreyer et al., 2006), a competing filtering scheme that filters arcs based on their length (leveraging the observation that most dependencies are short). In this work, we propose a novel filtering framework that integrates all the information used in token-role classification and vine parsing, but offers a number of advantages. In our previous work, classifier decisions would often overlap: different token-role classifiers would agree to filter the same arc. Based on this observation, we propose a joint training framework where only the most confident 200 Proceedings of the 49th Annual Meeti</context>
<context position="11400" citStr="Eisner and Smith, 2005" startWordPosition="1893" endWordPosition="1896">nea)) is shown next to its arrow. by higher-scoring subclassifiers (Item 2). 3 Accounting for Arc Length We can extend our system by expanding our event set Z. By adding an arc-level event Vinea to each Za, we can introduce a vine filter to prune long arcs. Similarly, we have already introduced another arclevel event, the rejection event Nonea. By assigning features to Nonea, we learn a dynamic threshold on all filters, which considers properties of the arc before acting on any other event. We parameterize both Vinea and Nonea with the same two features, inspired by tag-specific vine parsing (Eisner and Smith, 2005): � Bias 1 HeadTag ModTag Dir(a) Len(a) where HeadTag ModTag Dir(a) concatenates the part-of-speech tags of a’s head and modifier tokens to its direction (left or right), and Len(a) gives the unsigned distance between a’s head and modifier. In the context of Vinea, these two features allow the system to learn tag-pair-specific limits on arc length. In the context of Nonea, these features protect short arcs and arcs that connect frequentlylinked tag-pairs, allowing our token-role filters to be more aggressive on arcs that do not have these characteristics. The dynamic threshold also alters our </context>
</contexts>
<marker>Eisner, Smith, 2005</marker>
<rawString>Jason Eisner and Noah A. Smith. 2005. Parsing with soft and hard constraints on dependency length. In IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro F Felzenszwalb</author>
<author>Ross B Girshick</author>
<author>David McAllester</author>
<author>Deva Ramanan</author>
</authors>
<title>Object detection with discriminatively trained part based models.</title>
<date>2010</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>32</volume>
<issue>9</issue>
<contexts>
<context position="9082" citStr="Felzenszwalb et al., 2010" startWordPosition="1501" endWordPosition="1504">ng selects w� to minimize: 1 2||�w||2+ Cy max [0,1 + f(Za|-y) − f(Za|y)] (a,y)EA (2) where Cy is a label-specific regularization parameter, and the event set Z is now conditioned on the label y: Za|1 = Za, and Za|0 = {Nonea}. Nonea is a rejection event, which indicates that a is not filtered. The rejection event slightly alters our decision rule; rather than thresholding at 0, we now filter a only if f(Za) &gt; w� · 46(Nonea). One can set 4b(Nonea) +— 0 for all a to fix the threshold at 0. Though not convex, (2) can be solved to a local minimum with an EM-like alternating minimization procedure (Felzenszwalb et al., 2010; Yu and Joachims, 2009). The learner alternates between picking the highest-scoring latent event za E Za|y for each example (a, y), and training a multiclass SVM to solve an approximation to (2) where Za|y is replaced with {za}. Intuitively, the first step assigns the event za to a, making za responsible for a’s observed label. The second step optimizes the model to ensure that each za is detected, leading to the desired arc-filtering decisions. As the process iterates, event assignment becomes increasingly refined, leading to a more accurate joint filter. The resulting joint filter has only </context>
</contexts>
<marker>Felzenszwalb, Girshick, McAllester, Ramanan, 2010</marker>
<rawString>Pedro F. Felzenszwalb, Ross B. Girshick, David McAllester, and Deva Ramanan. 2010. Object detection with discriminatively trained part based models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="19637" citStr="Huang and Sagae (2010)" startWordPosition="3214" endWordPosition="3217"> training framework also has other benefits with respect to R+L+Q, including a single unified training algo10Results are not identical to those reported in our previous paper, due to our use of a different part-of-speech tagger. Note that parsing accuracies for the B&amp;C systems have improved. 11P1: C0=1e-2, C1=1e-5, P2: C0=1e-2, C1=2e-5 rithm, fewer hyper-parameters and a smaller testtime memory footprint. Finally, the jointly trained filters have no impact on parsing accuracy, where both B&amp;C filters have a small negative effect. The performance of Joint-P2+MST-2 is comparable to the system of Huang and Sagae (2010), who report a parsing speed of 25 sentences per second and an accuracy of 92.1 on the same test set, using a transition-based parser enhanced with dynamic-programming state combination.12 Graphbased and transition-based systems tend to make different types of errors (McDonald and Nivre, 2007). Therefore, having fast, accurate parsers for both approaches presents an opportunity for large-scale, robust parser combination. 5 Conclusion We have presented a novel use of latent SVM technology to train a number of filters jointly, with a shared dynamic threshold. By training a family of dependency f</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Transforming projective bilexical dependency grammars into efficiently-parsable CFGs with unfold-fold.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1916" citStr="Johnson, 2007" startWordPosition="287" endWordPosition="289">tic relationships between words using directed arcs (Me´lˇcuk, 1987). Each token in the sentence is a node in the tree, and each arc connects a head to its modifier. There are two dominant approaches to dependency parsing: graph-based and transition-based, where graphbased parsing is understood to be slower, but often more accurate (McDonald and Nivre, 2007). In the graph-based setting, a complete search finds the highest-scoring tree under a model that decomposes over one or two arcs at a time. Much of the time for parsing is spent scoring each potential arc in the complete dependency graph (Johnson, 2007), one for each ordered word-pair in the sentence. Potential arcs are scored using rich linear models that are discriminatively trained to maximize parsing accuracy (McDonald et al., 2005). The vast majority of these arcs are bad; in an n-word sentence, only n of the n2 potential arcs are correct. If many arcs can be filtered before parsing begins, then the entire process can be sped up substantially. Previously, we proposed a cascade of filters to prune potential arcs (Bergsma and Cherry, 2010). One stage of this cascade operates one token at a time, labeling each token t according to various </context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Transforming projective bilexical dependency grammars into efficiently-parsable CFGs with unfold-fold. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1662" citStr="McDonald and Nivre, 2007" startWordPosition="240" endWordPosition="243">ing a shared threshold based on arc length. Our system filters 32% more arcs than the independently-trained classifiers, without reducing filtering speed. This leads to faster parsing with no reduction in accuracy. 1 Introduction A dependency tree represents syntactic relationships between words using directed arcs (Me´lˇcuk, 1987). Each token in the sentence is a node in the tree, and each arc connects a head to its modifier. There are two dominant approaches to dependency parsing: graph-based and transition-based, where graphbased parsing is understood to be slower, but often more accurate (McDonald and Nivre, 2007). In the graph-based setting, a complete search finds the highest-scoring tree under a model that decomposes over one or two arcs at a time. Much of the time for parsing is spent scoring each potential arc in the complete dependency graph (Johnson, 2007), one for each ordered word-pair in the sentence. Potential arcs are scored using rich linear models that are discriminatively trained to maximize parsing accuracy (McDonald et al., 2005). The vast majority of these arcs are bad; in an n-word sentence, only n of the n2 potential arcs are correct. If many arcs can be filtered before parsing begi</context>
<context position="19931" citStr="McDonald and Nivre, 2007" startWordPosition="3260" endWordPosition="3263">. 11P1: C0=1e-2, C1=1e-5, P2: C0=1e-2, C1=2e-5 rithm, fewer hyper-parameters and a smaller testtime memory footprint. Finally, the jointly trained filters have no impact on parsing accuracy, where both B&amp;C filters have a small negative effect. The performance of Joint-P2+MST-2 is comparable to the system of Huang and Sagae (2010), who report a parsing speed of 25 sentences per second and an accuracy of 92.1 on the same test set, using a transition-based parser enhanced with dynamic-programming state combination.12 Graphbased and transition-based systems tend to make different types of errors (McDonald and Nivre, 2007). Therefore, having fast, accurate parsers for both approaches presents an opportunity for large-scale, robust parser combination. 5 Conclusion We have presented a novel use of latent SVM technology to train a number of filters jointly, with a shared dynamic threshold. By training a family of dependency filters in this manner, each subclassifier focuses on the examples where it is most needed, with our dynamic threshold adjusting filter sensitivity based on arc length. This allows us to outperform a 3-stage filter cascade in terms of speed-up, while also reducing the impact of filtering on par</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2103" citStr="McDonald et al., 2005" startWordPosition="314" endWordPosition="317"> dominant approaches to dependency parsing: graph-based and transition-based, where graphbased parsing is understood to be slower, but often more accurate (McDonald and Nivre, 2007). In the graph-based setting, a complete search finds the highest-scoring tree under a model that decomposes over one or two arcs at a time. Much of the time for parsing is spent scoring each potential arc in the complete dependency graph (Johnson, 2007), one for each ordered word-pair in the sentence. Potential arcs are scored using rich linear models that are discriminatively trained to maximize parsing accuracy (McDonald et al., 2005). The vast majority of these arcs are bad; in an n-word sentence, only n of the n2 potential arcs are correct. If many arcs can be filtered before parsing begins, then the entire process can be sped up substantially. Previously, we proposed a cascade of filters to prune potential arcs (Bergsma and Cherry, 2010). One stage of this cascade operates one token at a time, labeling each token t according to various roles in the tree: • Not-a-head (NaH): t is not the head of any arc • Head-to-left (HtL{1/5/*}): t’s head is to its left within 1, 5 or any number of words • Head-to-right (HtR{1/5/*}): a</context>
<context position="13762" citStr="McDonald et al., 2005" startWordPosition="2273" endWordPosition="2276">ulticlass exponentiated gradient SVM (Collins et al., 2008). LSVM training was stopped after 4 iterations, as determined during development.5 For the token-role classifiers, we re-implement the Bergsma and Cherry (2010) feature set, initializing w with high-precision subclassifiers trained independently for each token-role. Vine and None subclassifiers are initialized with a zero vector. At test time, we extract subclassifiers from the joint weight vector, and use them as parameters in the filtering tools of Bergsma and Cherry (2010).6 Parsing experiments are carried out using the MST parser (McDonald et al., 2005),7 which we have modified to filter arcs before carrying out feature extraction. It is trained using 5-best MIRA (Crammer and Singer, 2003). Following Bergsma and Cherry (2010), we measure intrinsic filter quality with reduction, the proportion of total arcs removed, and coverage, the proportion of true arcs retained. For parsing results, we present dependency accuracy, the percentage of tokens that are assigned the correct head. 4.1 Impact of Joint Training Our technical contribution consists of our proposed joint training scheme for token-role filters, along 4As implemented at http://w3.msi.</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Me´lˇcuk</author>
</authors>
<title>Dependency syntax: theory and practice.</title>
<date>1987</date>
<publisher>Press.</publisher>
<institution>State University of New York</institution>
<marker>Me´lˇcuk, 1987</marker>
<rawString>Igor A. Me´lˇcuk. 1987. Dependency syntax: theory and practice. State University of New York Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Kristy Hollingshead</author>
</authors>
<title>Classifying chart cells for quadratic complexity context-free inference.</title>
<date>2008</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="2841" citStr="Roark and Hollingshead (2008)" startWordPosition="445" endWordPosition="448">. If many arcs can be filtered before parsing begins, then the entire process can be sped up substantially. Previously, we proposed a cascade of filters to prune potential arcs (Bergsma and Cherry, 2010). One stage of this cascade operates one token at a time, labeling each token t according to various roles in the tree: • Not-a-head (NaH): t is not the head of any arc • Head-to-left (HtL{1/5/*}): t’s head is to its left within 1, 5 or any number of words • Head-to-right (HtR{1/5/*}): as head-to-left • Root (Root): t is the root node, which eliminates arcs according to projectivity Similar to Roark and Hollingshead (2008), each role has a corresponding binary classifier. These tokenrole classifiers were shown to be more effective than vine parsing (Eisner and Smith, 2005; Dreyer et al., 2006), a competing filtering scheme that filters arcs based on their length (leveraging the observation that most dependencies are short). In this work, we propose a novel filtering framework that integrates all the information used in token-role classification and vine parsing, but offers a number of advantages. In our previous work, classifier decisions would often overlap: different token-role classifiers would agree to filt</context>
</contexts>
<marker>Roark, Hollingshead, 2008</marker>
<rawString>Brian Roark and Kristy Hollingshead. 2008. Classifying chart cells for quadratic complexity context-free inference. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In IWPT.</booktitle>
<contexts>
<context position="12645" citStr="Yamada and Matsumoto (2003)" startWordPosition="2095" endWordPosition="2099">filtering events: where before they were either active or inactive, events are now assigned scores, which are compared with the threshold to make final filtering decisions (Figure 2).3 3Because tokens and arcs are scored independently and coupled only through score comparison, the impact of Vinea and Nonea on classification speed should be no greater than doing vine and token-role filtering in sequence. In practice, it is no slower than running token-role filtering on its own. 1.0 1.1 0.6 0.3 0.2 202 4 Experiments We extract dependency structures from the Penn Treebank using the head rules of Yamada and Matsumoto (2003).4 We divide the Treebank into train (sections 2–21), development (22) and test (23). We part-of-speech tag our data using a perceptron tagger similar to the one described by Collins (2002). The training set is tagged with jack-knifing: the data is split into 10 folds and each fold is tagged by a system trained on the other 9 folds. Development and test sets are tagged using the entire training set. We train our joint filter using an in-house latent SVM framework, which repeatedly calls a multiclass exponentiated gradient SVM (Collins et al., 2008). LSVM training was stopped after 4 iterations</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Yisong Yue</author>
<author>Claire Cardie</author>
</authors>
<title>Multi-level structured models for document-level sentiment classification.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="4642" citStr="Yessenalina et al., 2010" startWordPosition="734" endWordPosition="737">f any arc, or that the modifier hiss attaches elsewhere. Role truth values, derived from the gold-standard tree (in grey), are listed adjacent to the boxes, in parentheses. classifier is given credit for eliminating an arc. The identity of the responsible classifier is modeled as a latent variable, which is filled in during training using a latent SVM (LSVM) formulation. Our use of an LSVM to assign credit during joint training differs substantially from previous LSVM applications, which have induced latent linguistic structures (Cherry and Quirk, 2008; Chang et al., 2010) or sentence labels (Yessenalina et al., 2010). In our framework, each classifier learns to focus on the cases where the other classifiers are less confident. Furthermore, the integrated approach directly optimizes for arc-filtering accuracy (rather than token-labeling fidelity). We trade-off filtering precision/recall using two hyperparameters, while the previous approach trained classifiers for eight different tasks resulting in sixteen hyperparameters. Ultimately, the biggest gains in filter quality are achieved when we jointly train the token-role classifiers together with a dynamic threshold that is based on arc length and shared acr</context>
</contexts>
<marker>Yessenalina, Yue, Cardie, 2010</marker>
<rawString>Ainur Yessenalina, Yisong Yue, and Claire Cardie. 2010. Multi-level structured models for document-level sentiment classification. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chun-Nam John Yu</author>
<author>Thorsten Joachims</author>
</authors>
<title>Learning structural SVMs with latent variables.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="8292" citStr="Yu and Joachims, 2009" startWordPosition="1349" endWordPosition="1352">d for all z E Za. Also note that tokens have completely disappeared from our formalism: the classifier is framed only in terms of events and arcs; token-roles are encapsulated inside events. To provide a large-margin training objective for our joint classifier, we adapt the latent SVM (Felzen1A joint filtering formalism for CFG parsing or SCFG translation would likewise focus on hyper-edges or spans. 2This same requirement is also needed by the previous, independently-trained filters at test time, so that arcs can be filtered according to the roles assigned to tokens. 201 szwalb et al., 2010; Yu and Joachims, 2009) to our problem. Given a training set A of (a, y) pairs, where a is an arc in context and y is the correct filter label for a (1 to filter, 0 otherwise), LSVM training selects w� to minimize: 1 2||�w||2+ Cy max [0,1 + f(Za|-y) − f(Za|y)] (a,y)EA (2) where Cy is a label-specific regularization parameter, and the event set Z is now conditioned on the label y: Za|1 = Za, and Za|0 = {Nonea}. Nonea is a rejection event, which indicates that a is not filtered. The rejection event slightly alters our decision rule; rather than thresholding at 0, we now filter a only if f(Za) &gt; w� · 46(Nonea). One can</context>
</contexts>
<marker>Yu, Joachims, 2009</marker>
<rawString>Chun-Nam John Yu and Thorsten Joachims. 2009. Learning structural SVMs with latent variables. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bianca Zadrozny</author>
<author>John Langford</author>
<author>Naoki Abe</author>
</authors>
<title>Cost-sensitive learning by cost-proportionate example weighting.</title>
<date>2003</date>
<booktitle>In Third IEEE International Conference on Data Mining.</booktitle>
<contexts>
<context position="15933" citStr="Zadrozny et al., 2003" startWordPosition="2603" endWordPosition="2606">inst an independently-trained ensemble of the same classifiers.9 Note that None cannot be trained independently, as its shared dynamic threshold considers arc and token views of the data simultaneously. Results are shown in Table 1. Our complete system outperforms all variants in terms of both coverage and reduction. However, one can see that neither joint system is able to outperform its independently-trained counter-part without the dynamic threshold provided by None. This is because the desirable credit-assignment properties of our joint training procedure are achieved through duplication (Zadrozny et al., 2003). That is, the LSVM knows that a specific event is important because it appears in event sets Za, for many arcs from the same sentence. Without None, the filtering decisions implied by each copy of an event are identical. Because these replicated events are associated with arcs that are presented to the LSVM as independent examples, they appear to be not only important, but also low-variance, and therefore easy. This leads to overfitting. We had hoped that the benefits of joint training would outweigh this drawback, but our results show that they do not. However, in addition to its other desir</context>
</contexts>
<marker>Zadrozny, Langford, Abe, 2003</marker>
<rawString>Bianca Zadrozny, John Langford, and Naoki Abe. 2003. Cost-sensitive learning by cost-proportionate example weighting. In Third IEEE International Conference on Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Byung-Gyu Ahn</author>
<author>Stephen Clark</author>
<author>Curt Van Wyk</author>
<author>James R Curran</author>
<author>Laura Rimell</author>
</authors>
<title>Chart pruning for fast lexicalised-grammar parsing.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<marker>Zhang, Ahn, Clark, Van Wyk, Curran, Rimell, 2010</marker>
<rawString>Yue Zhang, Byung-Gyu Ahn, Stephen Clark, Curt Van Wyk, James R. Curran, and Laura Rimell. 2010. Chart pruning for fast lexicalised-grammar parsing. In EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>