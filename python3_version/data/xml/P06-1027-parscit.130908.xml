<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000830">
<title confidence="0.997397">
Semi-Supervised Conditional Random Fields for Improved Sequence
Segmentation and Labeling
</title>
<author confidence="0.998116">
Feng Jiao
</author>
<affiliation confidence="0.996878">
University of Waterloo
</affiliation>
<sectionHeader confidence="0.957732" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999825">
We present a new semi-supervised training
procedure for conditional random fields
(CRFs) that can be used to train sequence
segmentors and labelers from a combina-
tion of labeled and unlabeled training data.
Our approach is based on extending the
minimum entropy regularization frame-
work to the structured prediction case,
yielding a training objective that combines
unlabeled conditional entropy with labeled
conditional likelihood. Although the train-
ing objective is no longer concave, it can
still be used to improve an initial model
(e.g. obtained from supervised training)
by iterative ascent. We apply our new
training algorithm to the problem of iden-
tifying gene and protein mentions in bio-
logical texts, and show that incorporating
unlabeled data improves the performance
of the supervised CRF in this case.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.980565636363636">
Semi-supervised learning is often touted as one
of the most natural forms of training for language
processing tasks, since unlabeled data is so plen-
tiful whereas labeled data is usually quite limited
or expensive to obtain. The attractiveness of semi-
supervised learning for language tasks is further
heightened by the fact that the models learned are
large and complex, and generally even thousands
of labeled examples can only sparsely cover the
parameter space. Moreover, in complex structured
prediction tasks, such as parsing or sequence mod-
eling (part-of-speech tagging, word segmentation,
named entity recognition, and so on), it is con-
siderably more difficult to obtain labeled training
data than for classification tasks (such as docu-
ment classification), since hand-labeling individ-
ual words and word boundaries is much harder
than assigning text-level class labels.
Many approaches have been proposed for semi-
supervised learning in the past, including: genera-
tive models (Castelli and Cover 1996; Cohen and
Cozman 2006; Nigam et al. 2000), self-learning
</bodyText>
<author confidence="0.540794">
Shaojun Wang Chi-Hoon Lee
Russell Greiner Dale Schuurmans
</author>
<affiliation confidence="0.641075">
University of Alberta
</affiliation>
<bodyText confidence="0.999768644444445">
(Celeux and Govaert 1992; Yarowsky 1995), co-
training (Blum and Mitchell 1998), information-
theoretic regularization (Corduneanu and Jaakkola
2006; Grandvalet and Bengio 2004), and graph-
based transductive methods (Zhou et al. 2004;
Zhou et al. 2005; Zhu et al. 2003). Unfortu-
nately, these techniques have been developed pri-
marily for single class label classification prob-
lems, or class label classification with a struc-
tured input (Zhou et al. 2004; Zhou et al. 2005;
Zhu et al. 2003). Although still highly desirable,
semi-supervised learning for structured classifica-
tion problems like sequence segmentation and la-
beling have not been as widely studied as in the
other semi-supervised settings mentioned above,
with the sole exception of generative models.
With generative models, it is natural to include
unlabeled data using an expectation-maximization
approach (Nigam et al. 2000). However, gener-
ative models generally do not achieve the same
accuracy as discriminatively trained models, and
therefore it is preferable to focus on discriminative
approaches. Unfortunately, it is far from obvious
how unlabeled training data can be naturally in-
corporated into a discriminative training criterion.
For example, unlabeled data simply cancels from
the objective if one attempts to use a traditional
conditional likelihood criterion. Nevertheless, re-
cent progress has been made on incorporating un-
labeled data in discriminative training procedures.
For example, dependencies can be introduced be-
tween the labels of nearby instances and thereby
have an effect on training (Zhu et al. 2003; Li and
McCallum 2005; Altun et al. 2005). These models
are trained to encourage nearby data points to have
the same class label, and they can obtain impres-
sive accuracy using a very small amount of labeled
data. However, since they model pairwise similar-
ities among data points, most of these approaches
require joint inference over the whole data set at
test time, which is not practical for large data sets.
In this paper, we propose a new semi-supervised
training method for conditional random fields
(CRFs) that incorporates both labeled and unla-
beled sequence data to estimate a discriminative
</bodyText>
<page confidence="0.980723">
209
</page>
<bodyText confidence="0.998955840909091">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 209–216,
Sydney, July 2006. c�2006 Association for Computational Linguistics
structured predictor. CRFs are a flexible and pow-
erful model for structured predictors based on
undirected graphical models that have been glob-
ally conditioned on a set of input covariates (Laf-
ferty et al. 2001). CRFs have proved to be partic-
ularly useful for sequence segmentation and label-
ing tasks, since, as conditional models of the la-
bels given inputs, they relax the independence as-
sumptions made by traditional generative models
like hidden Markov models. As such, CRFs pro-
vide additional flexibility for using arbitrary over-
lapping features of the input sequence to define a
structured conditional model over the output se-
quence, while maintaining two advantages: first,
efficient dynamic program can be used for infer-
ence in both classification and training, and sec-
ond, the training objective is concave in the model
parameters, which permits global optimization.
To obtain a new semi-supervised training algo-
rithm for CRFs, we extend the minimum entropy
regularization framework of Grandvalet and Ben-
gio (2004) to structured predictors. The result-
ing objective combines the likelihood of the CRF
on labeled training data with its conditional en-
tropy on unlabeled training data. Unfortunately,
the maximization objective is no longer concave,
but we can still use it to effectively improve an
initial supervised model. To develop an effective
training procedure, we first show how the deriva-
tive of the new objective can be computed from
the covariance matrix of the features on the unla-
beled data (combined with the labeled conditional
likelihood). This relationship facilitates the devel-
opment of an efficient dynamic programming for
computing the gradient, and thereby allows us to
perform efficient iterative ascent for training. We
apply our new training technique to the problem of
sequence labeling and segmentation, and demon-
strate it specifically on the problem of identify-
ing gene and protein mentions in biological texts.
Our results show the advantage of semi-supervised
learning over the standard supervised algorithm.
</bodyText>
<sectionHeader confidence="0.962239" genericHeader="method">
2 Semi-supervised CRF training
</sectionHeader>
<bodyText confidence="0.9994524">
In what follows, we use the same notation as (Laf-
ferty et al. 2001). Let be a random variable over
data sequences to be labeled, and be a random
variable over corresponding label sequences. All
components, , of are assumed to range over
a finite label alphabet . For example, might
range over sentences and over part-of-speech
taggings of those sentences; hence would be the
set of possible part-of-speech tags in this case.
Assume we have a set of labeled examples,
</bodyText>
<listItem confidence="0.690575">
, and unla-
beled examples, . We
</listItem>
<bodyText confidence="0.996470074074074">
would like to build a CRF model
over sequential input and output data ,
where ,
and
Our goal is to learn such a model from the com-
bined set of labeled and unlabeled examples,
.
The standard supervised CRF training proce-
dure is based upon maximizing the log conditional
likelihood of the labeled examples in
where is any standard regularizer on, e.g.
. Regularization can be used to
limit over-fitting on rare features and avoid degen-
eracy in the case of correlated features. Obviously,
(1) ignores the unlabeled examples in .
To make full use of the available training data,
we propose a semi-supervised learning algorithm
that exploits a form of entropy regularization on
the unlabeled data. Specifically, for a semi-
supervised CRF, we propose to maximize the fol-
lowing objective
where the first term is the penalized log condi-
tional likelihood of the labeled data under the
CRF, (1), and the second line is the negative con-
ditional entropy of the CRF on the unlabeled data.
Here, is a tradeoff parameter that controls the
influence of the unlabeled data.
</bodyText>
<page confidence="0.988382">
210
</page>
<bodyText confidence="0.999919">
This approach resembles that taken by (Grand-
valet and Bengio 2004) for single variable classi-
fication, but here applied to structured CRF train-
ing. The motivation is that minimizing conditional
entropy over unlabeled data encourages the algo-
rithm to find putative labelings for the unlabeled
data that are mutually reinforcing with the super-
vised labels; that is, greater certainty on the pu-
tative labelings coincides with greater conditional
likelihood on the supervised labels, and vice versa.
For a single classification variable this criterion
has been shown to effectively partition unlabeled
data into clusters (Grandvalet and Bengio 2004;
Roberts et al. 2000).
To motivate the approach in more detail, con-
sider the overlap between the probability distribu-
tion over a label sequenceand the empirical dis-
tribution of on the unlabeled data . The
overlap can be measured by the Kullback-Leibler
divergence
</bodyText>
<sectionHeader confidence="0.9237" genericHeader="method">
3 An efficient training procedure
</sectionHeader>
<bodyText confidence="0.9999443125">
As (2) is not concave, many of the standard global
maximization techniques do not apply. However,
one can still use unlabeled data to improve a su-
pervised CRF via iterative ascent. To derive an ef-
ficient iterative ascent procedure, we need to com-
pute gradient of (2) with respect to the parameters
. Taking derivative of the objective function (2)
with respect to yields Appendix A for the deriva-
tion)
The first three items on the right hand side are
just the standard gradient of the CRF objective,
(Lafferty et al. 2001), and the final
item is the gradient of the entropy regularizer (the
derivation of which is given in Appendix A.
Here, is the condi-
tional covariance matrix of the features,
</bodyText>
<equation confidence="0.7958905">
(3)
. It is well
</equation>
<bodyText confidence="0.97619992">
known that Kullback-Leibler divergence (Cover
and Thomas 1991) is positive and increases as the
overlap between the two distributions decreases.
In other words, maximizing Kullback-Leibler di-
vergence implies that the overlap between two dis-
tributions is minimized. The total overlap over all
possible label sequences can be defined as
,
given sample sequence. In particular, the
th element of this matrix is given by
which motivates the negative entropy term in (2).
The combined training objective (2) exploits
unlabeled data to improve the CRF model, as
we will see. However, one drawback with this
approach is that the entropy regularization term
is not concave. To see why, note that the en-
tropy regularizer can be seen as a composition,
, where ,
and ,
. For scalar, the
second derivative of a composition, , is
given by (Boyd and Vandenberghe 2004)
Although and are concave here, since is not
nondecreasing,is not necessarily concave. So in
general there are local maxima in (2).
</bodyText>
<equation confidence="0.427626">
(4)
</equation>
<bodyText confidence="0.999964428571429">
To efficiently calculate the gradient, we need
to be able to efficiently compute the expectations
with respect to in (3) and (4). However, this
can pose a challenge in general, because there are
exponentially many values for. Techniques for
computing the linear feature expectations in (3)
are already well known if is sufficiently struc-
tured (e.g. forms a Markov chain) (Lafferty et
al. 2001). However, we now have to develop effi-
cient techniques for computing the quadratic fea-
ture expectations in (4).
For the quadratic feature expectations, first note
that the diagonal terms, , are straightfor-
ward, since each feature is an indicator, we have
</bodyText>
<page confidence="0.992487">
211
</page>
<bodyText confidence="0.989361302325581">
that , and therefore the diag-
onal terms in the conditional covariance are just
linear feature expectations
as before.
For the off diagonal terms, , however,
we need to develop a new algorithm. Fortunately,
for structured label sequences, , one can devise
an efficient algorithm for calculating the quadratic
expectations based on nested dynamic program-
ming. To illustrate the idea, we assume that the
dependencies of , conditioned on , form a
Markov chain.
Define one feature for each state pair,
and one feature for each state-observation pair
, which we express with indicator functions
and
respectively.
Following (Lafferty et al. 2001), we also add spe-
cial start and stop states, start and
stop. The conditional probability of a label se-
quence can now be expressed concisely in a ma-
trix form. For each positionin the observation
sequence, define the matrix random
variable by
and , for ,
, can be recursively calculated.
First define the summary matrix
Then the quadratic feature expectations can be
computed by the following recursion, where the
two double sums in each expectation correspond
to the two cases depending on which feature oc-
curs first (occuring before).
where
Hereis the edge with labelsand
is the vertex with label.
For each index define the for-
ward vectors with base case
and recurrence
Similarly, the backward vectors are given by
With these definitions, the expectation of
the product of each pair of feature func-
tions,
, ,
</bodyText>
<page confidence="0.957427">
212
</page>
<figure confidence="0.9693326">
start
0
1
2
stop
</figure>
<bodyText confidence="0.998198666666667">
The computation of these expectations can be or-
ganized in a trellis, as illustrated in Figure 1.
Once we obtain the gradient of the objective
function (2), we use limited-memory L-BFGS, a
quasi-Newton optimization algorithm (McCallum
2002; Nocedal and Wright 2000), to find the local
maxima with the initial value being set to be the
optimal solution of the supervised CRF on labeled
data.
</bodyText>
<sectionHeader confidence="0.985738" genericHeader="method">
4 Time and space complexity
</sectionHeader>
<bodyText confidence="0.99721984">
The time and space complexity of the semi-
supervised CRF training procedure is greater
than that of standard supervised CRF training,
but nevertheless remains a small degree poly-
nomial in the size of the training data. Let
= size of the labeled set
= size of the unlabeled set
= labeled sequence length
= unlabeled sequence length
= test sequence length
= number of states
= number of training iterations.
Then the time required to classify a test sequence
is , independent of training method, since
the Viterbi decoder needs to access each path.
For training, supervised CRF training requires
time, whereas semi-supervised CRF
training requires time.
The additional cost for semi-supervised training
arises from the extra nested loop required to cal-
culated the quadratic feature expectations, which
introduces in an additional factor.
However, the space requirements of the two
training methods are the same. That is, even
though the covariance matrix has size
</bodyText>
<figureCaption confidence="0.998532">
Figure 1: Trellis for computing the expectation of a feature
</figureCaption>
<bodyText confidence="0.8714995">
product over a pair of feature functions, vs , where the
feature occurs first. This leads to one double sum.
</bodyText>
<sectionHeader confidence="0.798315" genericHeader="method">
5 Identifying gene and protein mentions
</sectionHeader>
<bodyText confidence="0.999989130434782">
We have developed our new semi-supervised
training procedure to address the problem of infor-
mation extraction from biomedical text, which has
received significant attention in the past few years.
We have specifically focused on the problem of
identifying explicit mentions of gene and protein
names (McDonald and Pereira 2005). Recently,
McDonald and Pereira (2005) have obtained inter-
esting results on this problem by using a standard
supervised CRF approach. However, our con-
tention is that stronger results could be obtained
in this domain by exploiting a large corpus of un-
annotated biomedical text to improve the quality
of the predictions, which we now show.
Given a biomedical text, the task of identify-
ing gene mentions can be interpreted as a tagging
task, where each word in the text can be labeled
with a tag that indicates whether it is the beginning
of gene mention (B), the continuation of a gene
mention (I), or outside of any gene mention (O).
To compare the performance of different taggers
learned by different mechanisms, one can measure
the precision, recall and F-measure, given by
</bodyText>
<figure confidence="0.816818555555555">
# correct predictions
precision =
# predicted gene mentions
# correct predictions
recall =
# true gene mentions
precision recall
F-measure =
precisionrecall
</figure>
<bodyText confidence="0.988524357142857">
In our evaluation, we compared the proposed
semi-supervised learning approach to the state of
the art supervised CRF of McDonald and Pereira
(2005), and also to self-training (Celeux and Gov-
aert 1992; Yarowsky 1995), using the same fea-
ture set as (McDonald and Pereira 2005). The
CRF training procedures, supervised and semi-
,
there is never any need to store the entire matrix in
memory. Rather, since we only need to compute
the product of the covariance with, the calcu-
lation can be performed iteratively without using
extra space beyond that already required by super-
vised CRF training.
</bodyText>
<page confidence="0.99806">
213
</page>
<bodyText confidence="0.999473137254902">
supervised, were run with the same regularization
function, , used in (McDonald and
Pereira 2005).
First we evaluated the performance of the semi-
supervised CRF in detail, by varying the ratio be-
tween the amount of labeled and unlabeled data,
and also varying the tradeoff parameter. We
choose a labeled training set consisting of 5448
words, and considered alternative unlabeled train-
ing sets, (5210 words), (10,208 words), and
(25,145 words), consisting of the same, 2 times
and 5 times as many sentences as respectively.
All of these sets were disjoint and selected ran-
domly from the full corpus, the smaller one in
(McDonald et al. 2005), consisting of 184,903
words in total. To determine sensitivity to the pa-
rameter we examined a range of discrete values
.
In our first experiment, we train the CRF models
using labeled set and unlabeled sets , and
respectively. Then test the performance on the
sets , and respectively The results of our
evaluation are shown in Table 1. The performance
of the supervised CRF algorithm, trained only on
the labeled set , is given on the first row in Table
1 (corresponding to ). By comparison, the
results obtained by the semi-supervised CRFs on
the held-out sets , and are given in Table 1
by increasing the value of.
The results of this experiment demonstrate quite
clearly that in most cases the semi-supervised CRF
obtains higher precision, recall and F-measure
than the fully supervised CRF, yielding a 20% im-
provement in the best case.
In our second experiment, again we train the
CRF models using labeled set and unlabeled
sets ,and respectively with increasing val-
ues of, but we test the performance on the held-
out set which is the full corpus minus the la-
beled set and unlabeled sets , and. The
results of our evaluation are shown in Table 2 and
Figure 2. The blue line in Figure 2 is the result
of the supervised CRF algorithm, trained only on
the labeled set . In particular, by using the super-
vised CRF model, the system predicted 3334 out
of 7472 gene mentions, of which 2435 were cor-
rect, resulting in a precision of 0.73, recall of 0.33
and F-measure of 0.45. The other curves are those
of the semi-supervised CRFs.
The results of this experiment demonstrate quite
clearly that the semi-supervised CRFs simultane-
</bodyText>
<figure confidence="0.9979715">
set B
set C
set D
CRF
2500
2000
1500
1000
500
0
0.1 0.5 1 5 7 10 12 14 16 18 20
gamma
</figure>
<figureCaption confidence="0.995833">
Figure 2: Performance of the supervised and semi-
supervised CRFs. The sets , and refer to the unlabeled
training set used by the semi-supervised algorithm.
</figureCaption>
<bodyText confidence="0.999787615384615">
ously increase both the number of predicted gene
mentions and the number of correct predictions,
thus the precision remains almost the same as the
supervised CRF, and the recall increases signifi-
cantly.
Both experiments as illustrated in Figure 2 and
Tables 1 and 2 show that clearly better results
are obtained by incorporating additional unlabeled
training data, even when evaluating on disjoint
testing data (Figure 2). The performance of the
semi-supervised CRF is not overly sensitive to the
tradeoff parameter, except that cannot be set
too large.
</bodyText>
<subsectionHeader confidence="0.998959">
5.1 Comparison to self-training
</subsectionHeader>
<bodyText confidence="0.999799545454545">
For completeness, we also compared our results to
the self-learning algorithm, which has commonly
been referred to as bootstrapping in natural lan-
guage processing and originally popularized by
the work of Yarowsky in word sense disambigua-
tion (Abney 2004; Yarowsky 1995). In fact, sim-
ilar ideas have been developed in pattern recogni-
tion under the name of the decision-directed algo-
rithm (Duda and Hart 1973), and also traced back
to 1970s in the EM literature (Celeux and Govaert
1992). The basic algorithm works as follows:
</bodyText>
<listItem confidence="0.978747">
1. Given and , begin with a seed set of
labeled examples, , chosen from .
2. For
(a) Train the supervised CRF on labeled ex-
</listItem>
<bodyText confidence="0.995300285714286">
amples , obtaining .
,find
via
Viterbi decoding or other inference al-
gorithm, and add the pair to
the set of labeled examples (replacing
any previous label for if present).
</bodyText>
<figure confidence="0.95194125">
number of correct prediction (TP)
3500
3000
(b) For each sequence
</figure>
<page confidence="0.997737">
214
</page>
<tableCaption confidence="0.999799">
Table 1: Performance of the semi-supervised CRFs obtained on the held-out sets , and
</tableCaption>
<table confidence="0.999918875">
Test Set B, Trained on A and B Test Set C, Trained on A and C Test Set D, Trained on A and D
Precision Recall F-Measure Precision Recall F-Measure Precision Recall F-Measure
0 0.80 0.36 0.50 0.77 0.29 0.43 0.74 0.30 0.43
0.1 0.82 0.4 0.54 0.79 0.32 0.46 0.74 0.31 0.44
0.5 0.82 0.4 0.54 0.79 0.33 0.46 0.74 0.31 0.44
1 0.82 0.4 0.54 0.77 0.34 0.47 0.73 0.33 0.45
5 0.84 0.45 0.59 0.78 0.38 0.51 0.72 0.36 0.48
10 0.78 0.46 0.58 0.66 0.38 0.48 0.66 0.38 0.47
</table>
<tableCaption confidence="0.976729">
Table 2: Performance of the semi-supervised CRFs trained by using unlabeled sets , and
</tableCaption>
<table confidence="0.997044428571429">
Test Set E, Trained on A and B Test Set E, Trained on A and C Test Set E, Trained on A and D
# predicted # correct prediction # predicted # correct prediction # predicted # correct prediction
0.1 3345 2446 3376 2470 3366 2466
0.5 3413 2489 3450 2510 3376 2469
1 3446 2503 3588 2580 3607 2590
5 4089 2878 4206 2947 4165 2888
10 4450 2799 4762 2827 4778 2845
</table>
<bodyText confidence="0.914438555555556">
(c) If for each ,,
stop; otherwise , iterate.
We implemented this self training approach and
tried it in our experiments. Unfortunately, we
were not able to obtain any improvements over the
standard supervised CRF with self-learning, using
the sets and . The
semi-supervised CRF remains the best of the ap-
proaches we have tried on this problem.
</bodyText>
<sectionHeader confidence="0.995306" genericHeader="conclusions">
6 Conclusions and further directions
</sectionHeader>
<bodyText confidence="0.999992727272727">
We have presented a new semi-supervised training
algorithm for CRFs, based on extending minimum
conditional entropy regularization to the struc-
tured prediction case. Our approach is motivated
by the information-theoretic argument (Grand-
valet and Bengio 2004; Roberts et al. 2000) that
unlabeled examples can provide the most bene-
fit when classes have small overlap. An itera-
tive ascent optimization procedure was developed
for this new criterion, which exploits a nested dy-
namic programming approach to efficiently com-
pute the covariance matrix of the features.
We applied our new approach to the problem of
identifying gene name occurrences in biological
text, exploiting the availability of auxiliary unla-
beled data to improve the performance of the state
of the art supervised CRF approach in this do-
main. Our semi-supervised CRF approach shares
all of the benefits of the standard CRF training,
including the ability to exploit arbitrary features
of the inputs, while obtaining improved accuracy
through the use of unlabeled data. The main draw-
back is that training time is increased because of
the extra nested loop needed to calculate feature
covariances. Nevertheless, the algorithm is suf-
ficiently efficient to be trained on unlabeled data
sets that yield a notable improvement in classifi-
cation accuracy over standard supervised training.
To further accelerate the training process of our
semi-supervised CRFs, we may apply stochastic
gradient optimization method with adaptive gain
adjustment as proposed by Vishwanathan et al.
(2006).
</bodyText>
<sectionHeader confidence="0.997632" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9575735">
Research supported by Genome Alberta, Genome Canada,
and the Alberta Ingenuity Centre for Machine Learning.
</bodyText>
<sectionHeader confidence="0.998532" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998926588235294">
S. Abney. (2004). Understanding the Yarowsky algorithm.
Computational Linguistics, 30(3):365-395.
Y. Altun, D. McAllester and M. Belkin. (2005). Maximum
margin semi-supervised learning for structured variables.
Advances in Neural Information Processing Systems 18.
A. Blum and T. Mitchell. (1998). Combining labeled and
unlabeled data with co-training. Proceedings of the Work-
shop on Computational Learning Theory, 92-100.
S. Boyd and L. Vandenberghe. (2004). Convex Optimization.
Cambridge University Press.
V. Castelli and T. Cover. (1996). The relative value of la-
beled and unlabeled samples in pattern recognition with
an unknown mixing parameter. IEEE Trans. on Informa-
tion Theory, 42(6):2102-2117.
G. Celeux and G. Govaert. (1992). A classification EM al-
gorithm for clustering and two stochastic versions. Com-
putational Statistics and Data Analysis, 14:315-332.
</reference>
<page confidence="0.984841">
215
</page>
<reference confidence="0.999452315789473">
I. Cohen and F. Cozman. (2006). Risks of semi-supervised
learning. Semi-Supervised Learning, O. Chapelle, B.
Scholk¨opf and A. Zien, (Editors), 55-70, MIT Press.
A. Corduneanu and T. Jaakkola. (2006). Data dependent
regularization. Semi-Supervised Learning, O. Chapelle,
B. Scholk¨opf and A. Zien, (Editors), 163-182, MIT Press.
T. Cover and J. Thomas, (1991). Elements of Information
Theory, John Wiley &amp; Sons.
R. Duda and P. Hart. (1973). Pattern Classification and
Scene Analysis, John Wiley &amp; Sons.
Y. Grandvalet and Y. Bengio. (2004). Semi-supervised learn-
ing by entropy minimization, Advances in Neural Infor-
mation Processing Systems, 17:529-536.
J. Lafferty, A. McCallum and F. Pereira. (2001). Conditional
random fields: probabilistic models for segmenting and
labeling sequence data. Proceedings of the 18th Interna-
tional Conference on Machine Learning, 282-289.
W. Li and A. McCallum. (2005). Semi-supervised sequence
modeling with syntactic topic models. Proceedings of
Twentieth National Conference on Artificial Intelligence,
813-818.
A. McCallum. (2002). MALLET: A machine learning for
language toolkit. [http://mallet.cs.umass.edu]
R. McDonald, K. Lerman and Y. Jin. (2005). Con-
ditional random field biomedical entity tagger.
[http://www.seas.upenn.edu/sryantm/software/BioTagger/]
R. McDonald and F. Pereira. (2005). Identifying gene and
protein mentions in text using conditional random fields.
BMC Bioinformatics 2005, 6(Suppl 1):S6.
K. Nigam, A. McCallum, S. Thrun and T. Mitchell. (2000).
Text classification from labeled and unlabeled documents
using EM. Machine learning. 39(2/3):135-167.
J. Nocedal and S. Wright. (2000). Numerical Optimization,
Springer.
S. Roberts, R. Everson and I. Rezek. (2000). Maximum cer-
tainty data partitioning. Pattern Recognition, 33(5):833-
839.
S. Vishwanathan, N. Schraudolph, M. Schmidt and K. Mur-
phy. (2006). Accelerated training of conditional random
fields with stochastic meta-descent. Proceedings of the
23th International Conference on Machine Learning.
D. Yarowsky. (1995). Unsupervised word sense disambigua-
tion rivaling supervised methods. Proceedings of the 33rd
Annual Meeting of the Association for Computational Lin-
guistics, 189-196.
D. Zhou, O. Bousquet, T. Navin Lal, J. Weston and B.
Sch¨olkopf. (2004). Learning with local and global con-
sistency. Advances in Neural Information Processing Sys-
tems, 16:321-328.
D. Zhou, J. Huang and B. Sch¨olkopf. (2005). Learning from
labeled and unlabeled data on a directed graph. Proceed-
ings of the 22nd International Conference on Machine
Learning, 1041-1048.
X. Zhu, Z. Ghahramani and J. Lafferty. (2003). Semi-
supervised learning using Gaussian fields and harmonic
functions. Proceedings of the 20th International Confer-
ence on Machine Learning, 912-919.
</reference>
<bodyText confidence="0.991926166666667">
A Deriving the gradient of the entropy
We wish to show that
First, note that some simple calculation yields
and
Therefore
In the vector form, this can be written as (5)
</bodyText>
<figure confidence="0.613505">
(5)
</figure>
<page confidence="0.993219">
216
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.989617">
<title confidence="0.9970415">Semi-Supervised Conditional Random Fields for Improved Sequence Segmentation and Labeling</title>
<author confidence="0.999559">Feng Jiao</author>
<affiliation confidence="0.999035">University of Waterloo</affiliation>
<abstract confidence="0.999838904761905">We present a new semi-supervised training procedure for conditional random fields (CRFs) that can be used to train sequence segmentors and labelers from a combination of labeled and unlabeled training data. Our approach is based on extending the minimum entropy regularization framework to the structured prediction case, yielding a training objective that combines unlabeled conditional entropy with labeled conditional likelihood. Although the training objective is no longer concave, it can still be used to improve an initial model (e.g. obtained from supervised training) by iterative ascent. We apply our new training algorithm to the problem of identifying gene and protein mentions in biological texts, and show that incorporating unlabeled data improves the performance of the supervised CRF in this case.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Understanding the Yarowsky algorithm.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--3</pages>
<contexts>
<context position="19683" citStr="Abney 2004" startWordPosition="3187" endWordPosition="3188">ated in Figure 2 and Tables 1 and 2 show that clearly better results are obtained by incorporating additional unlabeled training data, even when evaluating on disjoint testing data (Figure 2). The performance of the semi-supervised CRF is not overly sensitive to the tradeoff parameter, except that cannot be set too large. 5.1 Comparison to self-training For completeness, we also compared our results to the self-learning algorithm, which has commonly been referred to as bootstrapping in natural language processing and originally popularized by the work of Yarowsky in word sense disambiguation (Abney 2004; Yarowsky 1995). In fact, similar ideas have been developed in pattern recognition under the name of the decision-directed algorithm (Duda and Hart 1973), and also traced back to 1970s in the EM literature (Celeux and Govaert 1992). The basic algorithm works as follows: 1. Given and , begin with a seed set of labeled examples, , chosen from . 2. For (a) Train the supervised CRF on labeled examples , obtaining . ,find via Viterbi decoding or other inference algorithm, and add the pair to the set of labeled examples (replacing any previous label for if present). number of correct prediction (TP</context>
</contexts>
<marker>Abney, 2004</marker>
<rawString>S. Abney. (2004). Understanding the Yarowsky algorithm. Computational Linguistics, 30(3):365-395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Altun</author>
<author>D McAllester</author>
<author>M Belkin</author>
</authors>
<title>Maximum margin semi-supervised learning for structured variables.</title>
<date>2005</date>
<booktitle>Advances in Neural Information Processing Systems 18.</booktitle>
<contexts>
<context position="3734" citStr="Altun et al. 2005" startWordPosition="556" endWordPosition="559">s preferable to focus on discriminative approaches. Unfortunately, it is far from obvious how unlabeled training data can be naturally incorporated into a discriminative training criterion. For example, unlabeled data simply cancels from the objective if one attempts to use a traditional conditional likelihood criterion. Nevertheless, recent progress has been made on incorporating unlabeled data in discriminative training procedures. For example, dependencies can be introduced between the labels of nearby instances and thereby have an effect on training (Zhu et al. 2003; Li and McCallum 2005; Altun et al. 2005). These models are trained to encourage nearby data points to have the same class label, and they can obtain impressive accuracy using a very small amount of labeled data. However, since they model pairwise similarities among data points, most of these approaches require joint inference over the whole data set at test time, which is not practical for large data sets. In this paper, we propose a new semi-supervised training method for conditional random fields (CRFs) that incorporates both labeled and unlabeled sequence data to estimate a discriminative 209 Proceedings of the 21st International</context>
</contexts>
<marker>Altun, McAllester, Belkin, 2005</marker>
<rawString>Y. Altun, D. McAllester and M. Belkin. (2005). Maximum margin semi-supervised learning for structured variables. Advances in Neural Information Processing Systems 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>Proceedings of the Workshop on Computational Learning Theory,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="2183" citStr="Blum and Mitchell 1998" startWordPosition="323" endWordPosition="326">ed entity recognition, and so on), it is considerably more difficult to obtain labeled training data than for classification tasks (such as document classification), since hand-labeling individual words and word boundaries is much harder than assigning text-level class labels. Many approaches have been proposed for semisupervised learning in the past, including: generative models (Castelli and Cover 1996; Cohen and Cozman 2006; Nigam et al. 2000), self-learning Shaojun Wang Chi-Hoon Lee Russell Greiner Dale Schuurmans University of Alberta (Celeux and Govaert 1992; Yarowsky 1995), cotraining (Blum and Mitchell 1998), informationtheoretic regularization (Corduneanu and Jaakkola 2006; Grandvalet and Bengio 2004), and graphbased transductive methods (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003). Unfortunately, these techniques have been developed primarily for single class label classification problems, or class label classification with a structured input (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003). Although still highly desirable, semi-supervised learning for structured classification problems like sequence segmentation and labeling have not been as widely studied as in the other semi-su</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. (1998). Combining labeled and unlabeled data with co-training. Proceedings of the Workshop on Computational Learning Theory, 92-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Boyd</author>
<author>L Vandenberghe</author>
</authors>
<title>Convex Optimization.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="10635" citStr="Boyd and Vandenberghe 2004" startWordPosition="1678" endWordPosition="1681">tween two distributions is minimized. The total overlap over all possible label sequences can be defined as , given sample sequence. In particular, the th element of this matrix is given by which motivates the negative entropy term in (2). The combined training objective (2) exploits unlabeled data to improve the CRF model, as we will see. However, one drawback with this approach is that the entropy regularization term is not concave. To see why, note that the entropy regularizer can be seen as a composition, , where , and , . For scalar, the second derivative of a composition, , is given by (Boyd and Vandenberghe 2004) Although and are concave here, since is not nondecreasing,is not necessarily concave. So in general there are local maxima in (2). (4) To efficiently calculate the gradient, we need to be able to efficiently compute the expectations with respect to in (3) and (4). However, this can pose a challenge in general, because there are exponentially many values for. Techniques for computing the linear feature expectations in (3) are already well known if is sufficiently structured (e.g. forms a Markov chain) (Lafferty et al. 2001). However, we now have to develop efficient techniques for computing th</context>
</contexts>
<marker>Boyd, Vandenberghe, 2004</marker>
<rawString>S. Boyd and L. Vandenberghe. (2004). Convex Optimization. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Castelli</author>
<author>T Cover</author>
</authors>
<title>The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter.</title>
<date>1996</date>
<journal>IEEE Trans. on Information Theory,</journal>
<pages>42--6</pages>
<contexts>
<context position="1967" citStr="Castelli and Cover 1996" startWordPosition="291" endWordPosition="294">y even thousands of labeled examples can only sparsely cover the parameter space. Moreover, in complex structured prediction tasks, such as parsing or sequence modeling (part-of-speech tagging, word segmentation, named entity recognition, and so on), it is considerably more difficult to obtain labeled training data than for classification tasks (such as document classification), since hand-labeling individual words and word boundaries is much harder than assigning text-level class labels. Many approaches have been proposed for semisupervised learning in the past, including: generative models (Castelli and Cover 1996; Cohen and Cozman 2006; Nigam et al. 2000), self-learning Shaojun Wang Chi-Hoon Lee Russell Greiner Dale Schuurmans University of Alberta (Celeux and Govaert 1992; Yarowsky 1995), cotraining (Blum and Mitchell 1998), informationtheoretic regularization (Corduneanu and Jaakkola 2006; Grandvalet and Bengio 2004), and graphbased transductive methods (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003). Unfortunately, these techniques have been developed primarily for single class label classification problems, or class label classification with a structured input (Zhou et al. 2004; Zhou et al. </context>
</contexts>
<marker>Castelli, Cover, 1996</marker>
<rawString>V. Castelli and T. Cover. (1996). The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter. IEEE Trans. on Information Theory, 42(6):2102-2117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Celeux</author>
<author>G Govaert</author>
</authors>
<title>A classification EM algorithm for clustering and two stochastic versions.</title>
<date>1992</date>
<booktitle>Computational Statistics and Data Analysis,</booktitle>
<pages>14--315</pages>
<contexts>
<context position="2130" citStr="Celeux and Govaert 1992" startWordPosition="315" endWordPosition="318">eling (part-of-speech tagging, word segmentation, named entity recognition, and so on), it is considerably more difficult to obtain labeled training data than for classification tasks (such as document classification), since hand-labeling individual words and word boundaries is much harder than assigning text-level class labels. Many approaches have been proposed for semisupervised learning in the past, including: generative models (Castelli and Cover 1996; Cohen and Cozman 2006; Nigam et al. 2000), self-learning Shaojun Wang Chi-Hoon Lee Russell Greiner Dale Schuurmans University of Alberta (Celeux and Govaert 1992; Yarowsky 1995), cotraining (Blum and Mitchell 1998), informationtheoretic regularization (Corduneanu and Jaakkola 2006; Grandvalet and Bengio 2004), and graphbased transductive methods (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003). Unfortunately, these techniques have been developed primarily for single class label classification problems, or class label classification with a structured input (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003). Although still highly desirable, semi-supervised learning for structured classification problems like sequence segmentation and labeling ha</context>
<context position="15940" citStr="Celeux and Govaert 1992" startWordPosition="2545" endWordPosition="2549">hether it is the beginning of gene mention (B), the continuation of a gene mention (I), or outside of any gene mention (O). To compare the performance of different taggers learned by different mechanisms, one can measure the precision, recall and F-measure, given by # correct predictions precision = # predicted gene mentions # correct predictions recall = # true gene mentions precision recall F-measure = precisionrecall In our evaluation, we compared the proposed semi-supervised learning approach to the state of the art supervised CRF of McDonald and Pereira (2005), and also to self-training (Celeux and Govaert 1992; Yarowsky 1995), using the same feature set as (McDonald and Pereira 2005). The CRF training procedures, supervised and semi, there is never any need to store the entire matrix in memory. Rather, since we only need to compute the product of the covariance with, the calculation can be performed iteratively without using extra space beyond that already required by supervised CRF training. 213 supervised, were run with the same regularization function, , used in (McDonald and Pereira 2005). First we evaluated the performance of the semisupervised CRF in detail, by varying the ratio between the a</context>
<context position="19915" citStr="Celeux and Govaert 1992" startWordPosition="3225" endWordPosition="3228">mi-supervised CRF is not overly sensitive to the tradeoff parameter, except that cannot be set too large. 5.1 Comparison to self-training For completeness, we also compared our results to the self-learning algorithm, which has commonly been referred to as bootstrapping in natural language processing and originally popularized by the work of Yarowsky in word sense disambiguation (Abney 2004; Yarowsky 1995). In fact, similar ideas have been developed in pattern recognition under the name of the decision-directed algorithm (Duda and Hart 1973), and also traced back to 1970s in the EM literature (Celeux and Govaert 1992). The basic algorithm works as follows: 1. Given and , begin with a seed set of labeled examples, , chosen from . 2. For (a) Train the supervised CRF on labeled examples , obtaining . ,find via Viterbi decoding or other inference algorithm, and add the pair to the set of labeled examples (replacing any previous label for if present). number of correct prediction (TP) 3500 3000 (b) For each sequence 214 Table 1: Performance of the semi-supervised CRFs obtained on the held-out sets , and Test Set B, Trained on A and B Test Set C, Trained on A and C Test Set D, Trained on A and D Precision Recall</context>
</contexts>
<marker>Celeux, Govaert, 1992</marker>
<rawString>G. Celeux and G. Govaert. (1992). A classification EM algorithm for clustering and two stochastic versions. Computational Statistics and Data Analysis, 14:315-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Cohen</author>
<author>F Cozman</author>
</authors>
<title>Risks of semi-supervised learning. Semi-Supervised Learning,</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1990" citStr="Cohen and Cozman 2006" startWordPosition="295" endWordPosition="298">ed examples can only sparsely cover the parameter space. Moreover, in complex structured prediction tasks, such as parsing or sequence modeling (part-of-speech tagging, word segmentation, named entity recognition, and so on), it is considerably more difficult to obtain labeled training data than for classification tasks (such as document classification), since hand-labeling individual words and word boundaries is much harder than assigning text-level class labels. Many approaches have been proposed for semisupervised learning in the past, including: generative models (Castelli and Cover 1996; Cohen and Cozman 2006; Nigam et al. 2000), self-learning Shaojun Wang Chi-Hoon Lee Russell Greiner Dale Schuurmans University of Alberta (Celeux and Govaert 1992; Yarowsky 1995), cotraining (Blum and Mitchell 1998), informationtheoretic regularization (Corduneanu and Jaakkola 2006; Grandvalet and Bengio 2004), and graphbased transductive methods (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003). Unfortunately, these techniques have been developed primarily for single class label classification problems, or class label classification with a structured input (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003).</context>
</contexts>
<marker>Cohen, Cozman, 2006</marker>
<rawString>I. Cohen and F. Cozman. (2006). Risks of semi-supervised learning. Semi-Supervised Learning, O. Chapelle, B. Scholk¨opf and A. Zien, (Editors), 55-70, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Corduneanu</author>
<author>T Jaakkola</author>
</authors>
<title>Data dependent regularization. Semi-Supervised Learning,</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2250" citStr="Corduneanu and Jaakkola 2006" startWordPosition="330" endWordPosition="333">ficult to obtain labeled training data than for classification tasks (such as document classification), since hand-labeling individual words and word boundaries is much harder than assigning text-level class labels. Many approaches have been proposed for semisupervised learning in the past, including: generative models (Castelli and Cover 1996; Cohen and Cozman 2006; Nigam et al. 2000), self-learning Shaojun Wang Chi-Hoon Lee Russell Greiner Dale Schuurmans University of Alberta (Celeux and Govaert 1992; Yarowsky 1995), cotraining (Blum and Mitchell 1998), informationtheoretic regularization (Corduneanu and Jaakkola 2006; Grandvalet and Bengio 2004), and graphbased transductive methods (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003). Unfortunately, these techniques have been developed primarily for single class label classification problems, or class label classification with a structured input (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003). Although still highly desirable, semi-supervised learning for structured classification problems like sequence segmentation and labeling have not been as widely studied as in the other semi-supervised settings mentioned above, with the sole exception of gener</context>
</contexts>
<marker>Corduneanu, Jaakkola, 2006</marker>
<rawString>A. Corduneanu and T. Jaakkola. (2006). Data dependent regularization. Semi-Supervised Learning, O. Chapelle, B. Scholk¨opf and A. Zien, (Editors), 163-182, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cover</author>
<author>J Thomas</author>
</authors>
<title>Elements of Information Theory,</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="9843" citStr="Cover and Thomas 1991" startWordPosition="1545" endWordPosition="1548">ed CRF via iterative ascent. To derive an efficient iterative ascent procedure, we need to compute gradient of (2) with respect to the parameters . Taking derivative of the objective function (2) with respect to yields Appendix A for the derivation) The first three items on the right hand side are just the standard gradient of the CRF objective, (Lafferty et al. 2001), and the final item is the gradient of the entropy regularizer (the derivation of which is given in Appendix A. Here, is the conditional covariance matrix of the features, (3) . It is well known that Kullback-Leibler divergence (Cover and Thomas 1991) is positive and increases as the overlap between the two distributions decreases. In other words, maximizing Kullback-Leibler divergence implies that the overlap between two distributions is minimized. The total overlap over all possible label sequences can be defined as , given sample sequence. In particular, the th element of this matrix is given by which motivates the negative entropy term in (2). The combined training objective (2) exploits unlabeled data to improve the CRF model, as we will see. However, one drawback with this approach is that the entropy regularization term is not conca</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>T. Cover and J. Thomas, (1991). Elements of Information Theory, John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Duda</author>
<author>P Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis,</title>
<date>1973</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="19837" citStr="Duda and Hart 1973" startWordPosition="3211" endWordPosition="3214">evaluating on disjoint testing data (Figure 2). The performance of the semi-supervised CRF is not overly sensitive to the tradeoff parameter, except that cannot be set too large. 5.1 Comparison to self-training For completeness, we also compared our results to the self-learning algorithm, which has commonly been referred to as bootstrapping in natural language processing and originally popularized by the work of Yarowsky in word sense disambiguation (Abney 2004; Yarowsky 1995). In fact, similar ideas have been developed in pattern recognition under the name of the decision-directed algorithm (Duda and Hart 1973), and also traced back to 1970s in the EM literature (Celeux and Govaert 1992). The basic algorithm works as follows: 1. Given and , begin with a seed set of labeled examples, , chosen from . 2. For (a) Train the supervised CRF on labeled examples , obtaining . ,find via Viterbi decoding or other inference algorithm, and add the pair to the set of labeled examples (replacing any previous label for if present). number of correct prediction (TP) 3500 3000 (b) For each sequence 214 Table 1: Performance of the semi-supervised CRFs obtained on the held-out sets , and Test Set B, Trained on A and B </context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>R. Duda and P. Hart. (1973). Pattern Classification and Scene Analysis, John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Grandvalet</author>
<author>Y Bengio</author>
</authors>
<title>Semi-supervised learning by entropy minimization,</title>
<date>2004</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>17--529</pages>
<contexts>
<context position="2279" citStr="Grandvalet and Bengio 2004" startWordPosition="334" endWordPosition="337">ing data than for classification tasks (such as document classification), since hand-labeling individual words and word boundaries is much harder than assigning text-level class labels. Many approaches have been proposed for semisupervised learning in the past, including: generative models (Castelli and Cover 1996; Cohen and Cozman 2006; Nigam et al. 2000), self-learning Shaojun Wang Chi-Hoon Lee Russell Greiner Dale Schuurmans University of Alberta (Celeux and Govaert 1992; Yarowsky 1995), cotraining (Blum and Mitchell 1998), informationtheoretic regularization (Corduneanu and Jaakkola 2006; Grandvalet and Bengio 2004), and graphbased transductive methods (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003). Unfortunately, these techniques have been developed primarily for single class label classification problems, or class label classification with a structured input (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003). Although still highly desirable, semi-supervised learning for structured classification problems like sequence segmentation and labeling have not been as widely studied as in the other semi-supervised settings mentioned above, with the sole exception of generative models. With generative</context>
<context position="5516" citStr="Grandvalet and Bengio (2004)" startWordPosition="834" endWordPosition="838">ions made by traditional generative models like hidden Markov models. As such, CRFs provide additional flexibility for using arbitrary overlapping features of the input sequence to define a structured conditional model over the output sequence, while maintaining two advantages: first, efficient dynamic program can be used for inference in both classification and training, and second, the training objective is concave in the model parameters, which permits global optimization. To obtain a new semi-supervised training algorithm for CRFs, we extend the minimum entropy regularization framework of Grandvalet and Bengio (2004) to structured predictors. The resulting objective combines the likelihood of the CRF on labeled training data with its conditional entropy on unlabeled training data. Unfortunately, the maximization objective is no longer concave, but we can still use it to effectively improve an initial supervised model. To develop an effective training procedure, we first show how the derivative of the new objective can be computed from the covariance matrix of the features on the unlabeled data (combined with the labeled conditional likelihood). This relationship facilitates the development of an efficient</context>
<context position="8192" citStr="Grandvalet and Bengio 2004" startWordPosition="1275" endWordPosition="1279">he unlabeled examples in . To make full use of the available training data, we propose a semi-supervised learning algorithm that exploits a form of entropy regularization on the unlabeled data. Specifically, for a semisupervised CRF, we propose to maximize the following objective where the first term is the penalized log conditional likelihood of the labeled data under the CRF, (1), and the second line is the negative conditional entropy of the CRF on the unlabeled data. Here, is a tradeoff parameter that controls the influence of the unlabeled data. 210 This approach resembles that taken by (Grandvalet and Bengio 2004) for single variable classification, but here applied to structured CRF training. The motivation is that minimizing conditional entropy over unlabeled data encourages the algorithm to find putative labelings for the unlabeled data that are mutually reinforcing with the supervised labels; that is, greater certainty on the putative labelings coincides with greater conditional likelihood on the supervised labels, and vice versa. For a single classification variable this criterion has been shown to effectively partition unlabeled data into clusters (Grandvalet and Bengio 2004; Roberts et al. 2000)</context>
<context position="21947" citStr="Grandvalet and Bengio 2004" startWordPosition="3592" endWordPosition="3596">ch ,, stop; otherwise , iterate. We implemented this self training approach and tried it in our experiments. Unfortunately, we were not able to obtain any improvements over the standard supervised CRF with self-learning, using the sets and . The semi-supervised CRF remains the best of the approaches we have tried on this problem. 6 Conclusions and further directions We have presented a new semi-supervised training algorithm for CRFs, based on extending minimum conditional entropy regularization to the structured prediction case. Our approach is motivated by the information-theoretic argument (Grandvalet and Bengio 2004; Roberts et al. 2000) that unlabeled examples can provide the most benefit when classes have small overlap. An iterative ascent optimization procedure was developed for this new criterion, which exploits a nested dynamic programming approach to efficiently compute the covariance matrix of the features. We applied our new approach to the problem of identifying gene name occurrences in biological text, exploiting the availability of auxiliary unlabeled data to improve the performance of the state of the art supervised CRF approach in this domain. Our semi-supervised CRF approach shares all of t</context>
</contexts>
<marker>Grandvalet, Bengio, 2004</marker>
<rawString>Y. Grandvalet and Y. Bengio. (2004). Semi-supervised learning by entropy minimization, Advances in Neural Information Processing Systems, 17:529-536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>Proceedings of the 18th International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="4705" citStr="Lafferty et al. 2001" startWordPosition="708" endWordPosition="712">or large data sets. In this paper, we propose a new semi-supervised training method for conditional random fields (CRFs) that incorporates both labeled and unlabeled sequence data to estimate a discriminative 209 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 209–216, Sydney, July 2006. c�2006 Association for Computational Linguistics structured predictor. CRFs are a flexible and powerful model for structured predictors based on undirected graphical models that have been globally conditioned on a set of input covariates (Lafferty et al. 2001). CRFs have proved to be particularly useful for sequence segmentation and labeling tasks, since, as conditional models of the labels given inputs, they relax the independence assumptions made by traditional generative models like hidden Markov models. As such, CRFs provide additional flexibility for using arbitrary overlapping features of the input sequence to define a structured conditional model over the output sequence, while maintaining two advantages: first, efficient dynamic program can be used for inference in both classification and training, and second, the training objective is conc</context>
<context position="6635" citStr="Lafferty et al. 2001" startWordPosition="1008" endWordPosition="1012">ith the labeled conditional likelihood). This relationship facilitates the development of an efficient dynamic programming for computing the gradient, and thereby allows us to perform efficient iterative ascent for training. We apply our new training technique to the problem of sequence labeling and segmentation, and demonstrate it specifically on the problem of identifying gene and protein mentions in biological texts. Our results show the advantage of semi-supervised learning over the standard supervised algorithm. 2 Semi-supervised CRF training In what follows, we use the same notation as (Lafferty et al. 2001). Let be a random variable over data sequences to be labeled, and be a random variable over corresponding label sequences. All components, , of are assumed to range over a finite label alphabet . For example, might range over sentences and over part-of-speech taggings of those sentences; hence would be the set of possible part-of-speech tags in this case. Assume we have a set of labeled examples, , and unlabeled examples, . We would like to build a CRF model over sequential input and output data , where , and Our goal is to learn such a model from the combined set of labeled and unlabeled exam</context>
<context position="9591" citStr="Lafferty et al. 2001" startWordPosition="1502" endWordPosition="1505">. The overlap can be measured by the Kullback-Leibler divergence 3 An efficient training procedure As (2) is not concave, many of the standard global maximization techniques do not apply. However, one can still use unlabeled data to improve a supervised CRF via iterative ascent. To derive an efficient iterative ascent procedure, we need to compute gradient of (2) with respect to the parameters . Taking derivative of the objective function (2) with respect to yields Appendix A for the derivation) The first three items on the right hand side are just the standard gradient of the CRF objective, (Lafferty et al. 2001), and the final item is the gradient of the entropy regularizer (the derivation of which is given in Appendix A. Here, is the conditional covariance matrix of the features, (3) . It is well known that Kullback-Leibler divergence (Cover and Thomas 1991) is positive and increases as the overlap between the two distributions decreases. In other words, maximizing Kullback-Leibler divergence implies that the overlap between two distributions is minimized. The total overlap over all possible label sequences can be defined as , given sample sequence. In particular, the th element of this matrix is gi</context>
<context position="11164" citStr="Lafferty et al. 2001" startWordPosition="1764" endWordPosition="1767">r scalar, the second derivative of a composition, , is given by (Boyd and Vandenberghe 2004) Although and are concave here, since is not nondecreasing,is not necessarily concave. So in general there are local maxima in (2). (4) To efficiently calculate the gradient, we need to be able to efficiently compute the expectations with respect to in (3) and (4). However, this can pose a challenge in general, because there are exponentially many values for. Techniques for computing the linear feature expectations in (3) are already well known if is sufficiently structured (e.g. forms a Markov chain) (Lafferty et al. 2001). However, we now have to develop efficient techniques for computing the quadratic feature expectations in (4). For the quadratic feature expectations, first note that the diagonal terms, , are straightforward, since each feature is an indicator, we have 211 that , and therefore the diagonal terms in the conditional covariance are just linear feature expectations as before. For the off diagonal terms, , however, we need to develop a new algorithm. Fortunately, for structured label sequences, , one can devise an efficient algorithm for calculating the quadratic expectations based on nested dyna</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum and F. Pereira. (2001). Conditional random fields: probabilistic models for segmenting and labeling sequence data. Proceedings of the 18th International Conference on Machine Learning, 282-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Li</author>
<author>A McCallum</author>
</authors>
<title>Semi-supervised sequence modeling with syntactic topic models.</title>
<date>2005</date>
<booktitle>Proceedings of Twentieth National Conference on Artificial Intelligence,</booktitle>
<pages>813--818</pages>
<contexts>
<context position="3714" citStr="Li and McCallum 2005" startWordPosition="552" endWordPosition="555">ls, and therefore it is preferable to focus on discriminative approaches. Unfortunately, it is far from obvious how unlabeled training data can be naturally incorporated into a discriminative training criterion. For example, unlabeled data simply cancels from the objective if one attempts to use a traditional conditional likelihood criterion. Nevertheless, recent progress has been made on incorporating unlabeled data in discriminative training procedures. For example, dependencies can be introduced between the labels of nearby instances and thereby have an effect on training (Zhu et al. 2003; Li and McCallum 2005; Altun et al. 2005). These models are trained to encourage nearby data points to have the same class label, and they can obtain impressive accuracy using a very small amount of labeled data. However, since they model pairwise similarities among data points, most of these approaches require joint inference over the whole data set at test time, which is not practical for large data sets. In this paper, we propose a new semi-supervised training method for conditional random fields (CRFs) that incorporates both labeled and unlabeled sequence data to estimate a discriminative 209 Proceedings of th</context>
</contexts>
<marker>Li, McCallum, 2005</marker>
<rawString>W. Li and A. McCallum. (2005). Semi-supervised sequence modeling with syntactic topic models. Proceedings of Twentieth National Conference on Artificial Intelligence, 813-818.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
</authors>
<title>MALLET: A machine learning for language toolkit. [http://mallet.cs.umass.edu]</title>
<date>2002</date>
<contexts>
<context position="13130" citStr="McCallum 2002" startWordPosition="2091" endWordPosition="2092">ond to the two cases depending on which feature occurs first (occuring before). where Hereis the edge with labelsand is the vertex with label. For each index define the forward vectors with base case and recurrence Similarly, the backward vectors are given by With these definitions, the expectation of the product of each pair of feature functions, , , 212 start 0 1 2 stop The computation of these expectations can be organized in a trellis, as illustrated in Figure 1. Once we obtain the gradient of the objective function (2), we use limited-memory L-BFGS, a quasi-Newton optimization algorithm (McCallum 2002; Nocedal and Wright 2000), to find the local maxima with the initial value being set to be the optimal solution of the supervised CRF on labeled data. 4 Time and space complexity The time and space complexity of the semisupervised CRF training procedure is greater than that of standard supervised CRF training, but nevertheless remains a small degree polynomial in the size of the training data. Let = size of the labeled set = size of the unlabeled set = labeled sequence length = unlabeled sequence length = test sequence length = number of states = number of training iterations. Then the time r</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>A. McCallum. (2002). MALLET: A machine learning for language toolkit. [http://mallet.cs.umass.edu]</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Lerman</author>
<author>Y Jin</author>
</authors>
<date>2005</date>
<note>Conditional random field biomedical entity tagger. [http://www.seas.upenn.edu/sryantm/software/BioTagger/]</note>
<contexts>
<context position="16975" citStr="McDonald et al. 2005" startWordPosition="2719" endWordPosition="2722">n with the same regularization function, , used in (McDonald and Pereira 2005). First we evaluated the performance of the semisupervised CRF in detail, by varying the ratio between the amount of labeled and unlabeled data, and also varying the tradeoff parameter. We choose a labeled training set consisting of 5448 words, and considered alternative unlabeled training sets, (5210 words), (10,208 words), and (25,145 words), consisting of the same, 2 times and 5 times as many sentences as respectively. All of these sets were disjoint and selected randomly from the full corpus, the smaller one in (McDonald et al. 2005), consisting of 184,903 words in total. To determine sensitivity to the parameter we examined a range of discrete values . In our first experiment, we train the CRF models using labeled set and unlabeled sets , and respectively. Then test the performance on the sets , and respectively The results of our evaluation are shown in Table 1. The performance of the supervised CRF algorithm, trained only on the labeled set , is given on the first row in Table 1 (corresponding to ). By comparison, the results obtained by the semi-supervised CRFs on the held-out sets , and are given in Table 1 by increa</context>
</contexts>
<marker>McDonald, Lerman, Jin, 2005</marker>
<rawString>R. McDonald, K. Lerman and Y. Jin. (2005). Conditional random field biomedical entity tagger. [http://www.seas.upenn.edu/sryantm/software/BioTagger/]</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Identifying gene and protein mentions in text using conditional random fields.</title>
<date>2005</date>
<journal>BMC Bioinformatics</journal>
<pages>6--1</pages>
<contexts>
<context position="14807" citStr="McDonald and Pereira 2005" startWordPosition="2360" endWordPosition="2363">he two training methods are the same. That is, even though the covariance matrix has size Figure 1: Trellis for computing the expectation of a feature product over a pair of feature functions, vs , where the feature occurs first. This leads to one double sum. 5 Identifying gene and protein mentions We have developed our new semi-supervised training procedure to address the problem of information extraction from biomedical text, which has received significant attention in the past few years. We have specifically focused on the problem of identifying explicit mentions of gene and protein names (McDonald and Pereira 2005). Recently, McDonald and Pereira (2005) have obtained interesting results on this problem by using a standard supervised CRF approach. However, our contention is that stronger results could be obtained in this domain by exploiting a large corpus of unannotated biomedical text to improve the quality of the predictions, which we now show. Given a biomedical text, the task of identifying gene mentions can be interpreted as a tagging task, where each word in the text can be labeled with a tag that indicates whether it is the beginning of gene mention (B), the continuation of a gene mention (I), or</context>
<context position="16432" citStr="McDonald and Pereira 2005" startWordPosition="2628" endWordPosition="2631">ing approach to the state of the art supervised CRF of McDonald and Pereira (2005), and also to self-training (Celeux and Govaert 1992; Yarowsky 1995), using the same feature set as (McDonald and Pereira 2005). The CRF training procedures, supervised and semi, there is never any need to store the entire matrix in memory. Rather, since we only need to compute the product of the covariance with, the calculation can be performed iteratively without using extra space beyond that already required by supervised CRF training. 213 supervised, were run with the same regularization function, , used in (McDonald and Pereira 2005). First we evaluated the performance of the semisupervised CRF in detail, by varying the ratio between the amount of labeled and unlabeled data, and also varying the tradeoff parameter. We choose a labeled training set consisting of 5448 words, and considered alternative unlabeled training sets, (5210 words), (10,208 words), and (25,145 words), consisting of the same, 2 times and 5 times as many sentences as respectively. All of these sets were disjoint and selected randomly from the full corpus, the smaller one in (McDonald et al. 2005), consisting of 184,903 words in total. To determine sens</context>
</contexts>
<marker>McDonald, Pereira, 2005</marker>
<rawString>R. McDonald and F. Pereira. (2005). Identifying gene and protein mentions in text using conditional random fields. BMC Bioinformatics 2005, 6(Suppl 1):S6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A McCallum</author>
<author>S Thrun</author>
<author>T Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine learning.</booktitle>
<pages>39--2</pages>
<contexts>
<context position="2010" citStr="Nigam et al. 2000" startWordPosition="299" endWordPosition="302">arsely cover the parameter space. Moreover, in complex structured prediction tasks, such as parsing or sequence modeling (part-of-speech tagging, word segmentation, named entity recognition, and so on), it is considerably more difficult to obtain labeled training data than for classification tasks (such as document classification), since hand-labeling individual words and word boundaries is much harder than assigning text-level class labels. Many approaches have been proposed for semisupervised learning in the past, including: generative models (Castelli and Cover 1996; Cohen and Cozman 2006; Nigam et al. 2000), self-learning Shaojun Wang Chi-Hoon Lee Russell Greiner Dale Schuurmans University of Alberta (Celeux and Govaert 1992; Yarowsky 1995), cotraining (Blum and Mitchell 1998), informationtheoretic regularization (Corduneanu and Jaakkola 2006; Grandvalet and Bengio 2004), and graphbased transductive methods (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003). Unfortunately, these techniques have been developed primarily for single class label classification problems, or class label classification with a structured input (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003). Although still high</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>K. Nigam, A. McCallum, S. Thrun and T. Mitchell. (2000). Text classification from labeled and unlabeled documents using EM. Machine learning. 39(2/3):135-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nocedal</author>
<author>S Wright</author>
</authors>
<date>2000</date>
<publisher>Numerical Optimization, Springer.</publisher>
<contexts>
<context position="13156" citStr="Nocedal and Wright 2000" startWordPosition="2093" endWordPosition="2096">cases depending on which feature occurs first (occuring before). where Hereis the edge with labelsand is the vertex with label. For each index define the forward vectors with base case and recurrence Similarly, the backward vectors are given by With these definitions, the expectation of the product of each pair of feature functions, , , 212 start 0 1 2 stop The computation of these expectations can be organized in a trellis, as illustrated in Figure 1. Once we obtain the gradient of the objective function (2), we use limited-memory L-BFGS, a quasi-Newton optimization algorithm (McCallum 2002; Nocedal and Wright 2000), to find the local maxima with the initial value being set to be the optimal solution of the supervised CRF on labeled data. 4 Time and space complexity The time and space complexity of the semisupervised CRF training procedure is greater than that of standard supervised CRF training, but nevertheless remains a small degree polynomial in the size of the training data. Let = size of the labeled set = size of the unlabeled set = labeled sequence length = unlabeled sequence length = test sequence length = number of states = number of training iterations. Then the time required to classify a test</context>
</contexts>
<marker>Nocedal, Wright, 2000</marker>
<rawString>J. Nocedal and S. Wright. (2000). Numerical Optimization, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Roberts</author>
<author>R Everson</author>
<author>I Rezek</author>
</authors>
<title>Maximum certainty data partitioning. Pattern Recognition,</title>
<date>2000</date>
<pages>33--5</pages>
<contexts>
<context position="8792" citStr="Roberts et al. 2000" startWordPosition="1366" endWordPosition="1369">let and Bengio 2004) for single variable classification, but here applied to structured CRF training. The motivation is that minimizing conditional entropy over unlabeled data encourages the algorithm to find putative labelings for the unlabeled data that are mutually reinforcing with the supervised labels; that is, greater certainty on the putative labelings coincides with greater conditional likelihood on the supervised labels, and vice versa. For a single classification variable this criterion has been shown to effectively partition unlabeled data into clusters (Grandvalet and Bengio 2004; Roberts et al. 2000). To motivate the approach in more detail, consider the overlap between the probability distribution over a label sequenceand the empirical distribution of on the unlabeled data . The overlap can be measured by the Kullback-Leibler divergence 3 An efficient training procedure As (2) is not concave, many of the standard global maximization techniques do not apply. However, one can still use unlabeled data to improve a supervised CRF via iterative ascent. To derive an efficient iterative ascent procedure, we need to compute gradient of (2) with respect to the parameters . Taking derivative of th</context>
<context position="21969" citStr="Roberts et al. 2000" startWordPosition="3597" endWordPosition="3600">ate. We implemented this self training approach and tried it in our experiments. Unfortunately, we were not able to obtain any improvements over the standard supervised CRF with self-learning, using the sets and . The semi-supervised CRF remains the best of the approaches we have tried on this problem. 6 Conclusions and further directions We have presented a new semi-supervised training algorithm for CRFs, based on extending minimum conditional entropy regularization to the structured prediction case. Our approach is motivated by the information-theoretic argument (Grandvalet and Bengio 2004; Roberts et al. 2000) that unlabeled examples can provide the most benefit when classes have small overlap. An iterative ascent optimization procedure was developed for this new criterion, which exploits a nested dynamic programming approach to efficiently compute the covariance matrix of the features. We applied our new approach to the problem of identifying gene name occurrences in biological text, exploiting the availability of auxiliary unlabeled data to improve the performance of the state of the art supervised CRF approach in this domain. Our semi-supervised CRF approach shares all of the benefits of the sta</context>
</contexts>
<marker>Roberts, Everson, Rezek, 2000</marker>
<rawString>S. Roberts, R. Everson and I. Rezek. (2000). Maximum certainty data partitioning. Pattern Recognition, 33(5):833-839.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vishwanathan</author>
<author>N Schraudolph</author>
<author>M Schmidt</author>
<author>K Murphy</author>
</authors>
<title>Accelerated training of conditional random fields with stochastic meta-descent.</title>
<date>2006</date>
<booktitle>Proceedings of the 23th International Conference on Machine Learning.</booktitle>
<marker>Vishwanathan, Schraudolph, Schmidt, Murphy, 2006</marker>
<rawString>S. Vishwanathan, N. Schraudolph, M. Schmidt and K. Murphy. (2006). Accelerated training of conditional random fields with stochastic meta-descent. Proceedings of the 23th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="2146" citStr="Yarowsky 1995" startWordPosition="319" endWordPosition="320">ging, word segmentation, named entity recognition, and so on), it is considerably more difficult to obtain labeled training data than for classification tasks (such as document classification), since hand-labeling individual words and word boundaries is much harder than assigning text-level class labels. Many approaches have been proposed for semisupervised learning in the past, including: generative models (Castelli and Cover 1996; Cohen and Cozman 2006; Nigam et al. 2000), self-learning Shaojun Wang Chi-Hoon Lee Russell Greiner Dale Schuurmans University of Alberta (Celeux and Govaert 1992; Yarowsky 1995), cotraining (Blum and Mitchell 1998), informationtheoretic regularization (Corduneanu and Jaakkola 2006; Grandvalet and Bengio 2004), and graphbased transductive methods (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003). Unfortunately, these techniques have been developed primarily for single class label classification problems, or class label classification with a structured input (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003). Although still highly desirable, semi-supervised learning for structured classification problems like sequence segmentation and labeling have not been as w</context>
<context position="15956" citStr="Yarowsky 1995" startWordPosition="2550" endWordPosition="2551">g of gene mention (B), the continuation of a gene mention (I), or outside of any gene mention (O). To compare the performance of different taggers learned by different mechanisms, one can measure the precision, recall and F-measure, given by # correct predictions precision = # predicted gene mentions # correct predictions recall = # true gene mentions precision recall F-measure = precisionrecall In our evaluation, we compared the proposed semi-supervised learning approach to the state of the art supervised CRF of McDonald and Pereira (2005), and also to self-training (Celeux and Govaert 1992; Yarowsky 1995), using the same feature set as (McDonald and Pereira 2005). The CRF training procedures, supervised and semi, there is never any need to store the entire matrix in memory. Rather, since we only need to compute the product of the covariance with, the calculation can be performed iteratively without using extra space beyond that already required by supervised CRF training. 213 supervised, were run with the same regularization function, , used in (McDonald and Pereira 2005). First we evaluated the performance of the semisupervised CRF in detail, by varying the ratio between the amount of labeled</context>
<context position="19699" citStr="Yarowsky 1995" startWordPosition="3189" endWordPosition="3190">re 2 and Tables 1 and 2 show that clearly better results are obtained by incorporating additional unlabeled training data, even when evaluating on disjoint testing data (Figure 2). The performance of the semi-supervised CRF is not overly sensitive to the tradeoff parameter, except that cannot be set too large. 5.1 Comparison to self-training For completeness, we also compared our results to the self-learning algorithm, which has commonly been referred to as bootstrapping in natural language processing and originally popularized by the work of Yarowsky in word sense disambiguation (Abney 2004; Yarowsky 1995). In fact, similar ideas have been developed in pattern recognition under the name of the decision-directed algorithm (Duda and Hart 1973), and also traced back to 1970s in the EM literature (Celeux and Govaert 1992). The basic algorithm works as follows: 1. Given and , begin with a seed set of labeled examples, , chosen from . 2. For (a) Train the supervised CRF on labeled examples , obtaining . ,find via Viterbi decoding or other inference algorithm, and add the pair to the set of labeled examples (replacing any previous label for if present). number of correct prediction (TP) 3500 3000 (b) </context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. (1995). Unsupervised word sense disambiguation rivaling supervised methods. Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, 189-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhou</author>
<author>O Bousquet</author>
<author>T Navin Lal</author>
<author>J Weston</author>
<author>B Sch¨olkopf</author>
</authors>
<title>Learning with local and global consistency.</title>
<date>2004</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>16--321</pages>
<marker>Zhou, Bousquet, Lal, Weston, Sch¨olkopf, 2004</marker>
<rawString>D. Zhou, O. Bousquet, T. Navin Lal, J. Weston and B. Sch¨olkopf. (2004). Learning with local and global consistency. Advances in Neural Information Processing Systems, 16:321-328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhou</author>
<author>J Huang</author>
<author>B Sch¨olkopf</author>
</authors>
<title>Learning from labeled and unlabeled data on a directed graph.</title>
<date>2005</date>
<booktitle>Proceedings of the 22nd International Conference on Machine Learning,</booktitle>
<pages>1041--1048</pages>
<marker>Zhou, Huang, Sch¨olkopf, 2005</marker>
<rawString>D. Zhou, J. Huang and B. Sch¨olkopf. (2005). Learning from labeled and unlabeled data on a directed graph. Proceedings of the 22nd International Conference on Machine Learning, 1041-1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
<author>J Lafferty</author>
</authors>
<title>Semisupervised learning using Gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>Proceedings of the 20th International Conference on Machine Learning,</booktitle>
<pages>912--919</pages>
<contexts>
<context position="2370" citStr="Zhu et al. 2003" startWordPosition="351" endWordPosition="354">al words and word boundaries is much harder than assigning text-level class labels. Many approaches have been proposed for semisupervised learning in the past, including: generative models (Castelli and Cover 1996; Cohen and Cozman 2006; Nigam et al. 2000), self-learning Shaojun Wang Chi-Hoon Lee Russell Greiner Dale Schuurmans University of Alberta (Celeux and Govaert 1992; Yarowsky 1995), cotraining (Blum and Mitchell 1998), informationtheoretic regularization (Corduneanu and Jaakkola 2006; Grandvalet and Bengio 2004), and graphbased transductive methods (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003). Unfortunately, these techniques have been developed primarily for single class label classification problems, or class label classification with a structured input (Zhou et al. 2004; Zhou et al. 2005; Zhu et al. 2003). Although still highly desirable, semi-supervised learning for structured classification problems like sequence segmentation and labeling have not been as widely studied as in the other semi-supervised settings mentioned above, with the sole exception of generative models. With generative models, it is natural to include unlabeled data using an expectation-maximization approach</context>
<context position="3692" citStr="Zhu et al. 2003" startWordPosition="548" endWordPosition="551">vely trained models, and therefore it is preferable to focus on discriminative approaches. Unfortunately, it is far from obvious how unlabeled training data can be naturally incorporated into a discriminative training criterion. For example, unlabeled data simply cancels from the objective if one attempts to use a traditional conditional likelihood criterion. Nevertheless, recent progress has been made on incorporating unlabeled data in discriminative training procedures. For example, dependencies can be introduced between the labels of nearby instances and thereby have an effect on training (Zhu et al. 2003; Li and McCallum 2005; Altun et al. 2005). These models are trained to encourage nearby data points to have the same class label, and they can obtain impressive accuracy using a very small amount of labeled data. However, since they model pairwise similarities among data points, most of these approaches require joint inference over the whole data set at test time, which is not practical for large data sets. In this paper, we propose a new semi-supervised training method for conditional random fields (CRFs) that incorporates both labeled and unlabeled sequence data to estimate a discriminative</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>X. Zhu, Z. Ghahramani and J. Lafferty. (2003). Semisupervised learning using Gaussian fields and harmonic functions. Proceedings of the 20th International Conference on Machine Learning, 912-919.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>