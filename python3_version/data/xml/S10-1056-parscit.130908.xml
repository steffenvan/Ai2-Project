<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032317">
<title confidence="0.9969785">
UTDMet: Combining WordNet and Corpus Data for
Argument Coercion Detection
</title>
<author confidence="0.995458">
Kirk Roberts and Sanda Harabagiu
</author>
<affiliation confidence="0.995726">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.626823">
Richardson, Texas, USA
</address>
<email confidence="0.999668">
{kirk,sanda}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913647058824">
This paper describes our system for the
classification of argument coercion for
SemEval-2010 Task 7. We present two ap-
proaches to classifying an argument’s se-
mantic class, which is then compared to
the predicate’s expected semantic class to
detect coercions. The first approach is
based on learning the members of an arbi-
trary semantic class using WordNet’s hy-
pernymy structure. The second approach
leverages automatically extracted seman-
tic parse information from a large corpus
to identify similar arguments by the pred-
icates that select them. We show the re-
sults these approaches obtain on the task
as well as how they can improve a tradi-
tional feature-based approach.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.978863375">
Argument coercion (a type of metonymy) occurs
when the expected semantic class (relative to the
a predicate) is substituted for an object of a dif-
ferent semantic class. Metonymy is a pervasive
phenomenon in language and the interpretation of
metonymic expressions can impact tasks from se-
mantic parsing (Scheffczyk et al., 2006) to ques-
tion answering (Harabagiu et al., 2005). A seminal
example in metonymy from (Lakoff and Johnson,
1980) is:
(1) The ham sandwich is waiting for his check.
The ARG1 for the predicate wait is typically an
animate, but the “ham sandwich” is clearly not an
animate. Rather, the argument is coerced to ful-
fill the predicate’s typing requirement. This coer-
cion is allowed because an object that would nor-
mally fulfill the typing requirement (the customer)
can be uniquely identified by an attribute (the ham
sandwich he ordered).
SemEval-2010 Task 7 (“Argument Selection
and Coercion”) (Pustejovsky and Rumshisky,
2009) was designed to evaluate systems that de-
tect such coercions and provide a “compositional
history” of argument selection relative to the pred-
icate. In order to accomplish this, an argument is
annotated with both the semantic class to which it
belongs (the “source” type) as well as the class ex-
pected by the predicate (the “target” type). How-
ever, in the data provided, the target type was un-
ambiguous given the lemmatized predicate, so the
remainder of this paper discusses source type clas-
sification. The detection of coercion is then sim-
ply performed by checking if the classified source
type and target type are different.
In our system, we explore two approaches with
separate underlying assumptions about how arbi-
trary semantic classes can be learned. In our first
approach, we assume a semantic class can be de-
fined a priori from a set of seed terms and that
WordNet is capable of defining the membership
of that semantic class. We apply the PageRank al-
gorithm in order to weight WordNet synsets given
a set of seed concepts. In our second approach,
we assume that arguments in the same semantic
class will be selected by similar verbs. We apply a
statistical test to determine the most representative
predicates for an argument. This approach benefits
from a large corpus from which we automatically
extracted 200 million predicate-argument pairs.
The remainder of this paper is organized as fol-
lows. Section 2 discusses our WordNet-based ap-
proach. Section 3 describes our corpus approach.
Section 4 discusses our experiments and results.
Section 5 provides a conclusion and direction for
future work. Due to space limitations, previous
work is discussed when relevant.
</bodyText>
<sectionHeader confidence="0.935049" genericHeader="method">
2 PageRanking WordNet Hypernyms
</sectionHeader>
<bodyText confidence="0.9229775">
Our first approach assumes that semantic class
members can be defined and acquired a priori.
</bodyText>
<page confidence="0.957591">
252
</page>
<bodyText confidence="0.9740708125">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 252–255,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
Given a set of seed concepts, we mine WordNet
for other concepts that may be in the same seman-
tic class. Clearly, this approach has both practical
limitations (WordNet does not contain every pos-
sible concept) and linguistic limitations (concepts
may belong to different semantic classes based on
their context). However, given the often vague na-
ture of semantic classes (is a building an ARTI-
FACT or a LOCATION?), access to a weighted list
of semantic class members can prove useful for ar-
guments not seen in the train set.
Using (Esuli and Sebastiani, 2007) as inspira-
tion, we have implemented our own naive ver-
sion of WordNet PageRank. They use sense-
disambiguated glosses provided by eXtended
WordNet (Harabagiu et al., 1999) to link synsets
by starting with positive (or negative) sentiment
concepts in order to find other concepts with pos-
itive (or negative) sentiment values. For our
task, however, hypernymy relations are more ap-
propriate for determining a given synset’s mem-
bership in a semantic class. Hypernymy de-
fines an IS-A relationship between the parent
class (the hypernym) and one of its child classes
(the hyponym). Furthermore, while PageRank as-
sumes directed edges (e.g., hyperlinks in a web
page), we use undirected edges. In this way, if
HYPERNYMOF(A, B), then A’s membership in a
semantic class strengthens B’s and vice versa.
Briefly, the formula for PageRank is:
</bodyText>
<equation confidence="0.999627">
a(k) = αa(k−1)W + (1 − α)e (1)
</equation>
<bodyText confidence="0.986385666666667">
where a(k) is the weight vector containing weights
for every synset in WordNet at time k; Wz,j is the
inverse of the total number of hypernyms and hy-
ponyms for synset i if synset j is a hypernym or
hyponym of synset i; e is the initial score vector;
and α is a tuning parameter. In our implementa-
tion, a(0) is initialized to all zeros; α is fixed at
0.5; and ez = 1 if synset i is in the seed set S,
and zero otherwise. The process is then run until
convergence, defined by |a(k)
z − a(k−1) z  |&lt; 0.0001
for all i.
The result of this PageRank is a weighted list
containing every synset reachable by a hyper-
nym/hyponym relation from a seed concept. We
ran the PageRank algorithm six times, once for
each semantic class, using the arguments in the
train set as seeds. For arguments that are polyse-
mous, we make a first WordNet sense assumption.
Representative examples of the concepts gener-
ated from this approach are shown in Table 1.
</bodyText>
<table confidence="0.997645444444445">
ARTIFACT DOCUMENT
funny wagon .377 white paper .342
liquor .353 progress report .342
iced tea .338 screenplay .324
tartan .325 papyrus .313
alpaca .325 pie chart .308
EVENT LOCATION
rock concert .382 heliport .381
rodeo .369 mukataa .380
radium therapy .357 subway station .342
seminar .347 dairy farm .326
pub crawl .346 gateway .320
PROPOSITION SOUND
dibs .363 whoosh .353
white paper .322 squish .353
tall tale .319 yodel .339
commendation .310 theme song .320
field theory .309 oldie .312
</table>
<tableCaption confidence="0.959174">
Table 1: Some of the concepts (and scores) learned
from applying PageRank to WordNet hypernyms.
</tableCaption>
<sectionHeader confidence="0.9022905" genericHeader="method">
3 Leveraging a Large Corpus of
Semantic Parse Annotations
</sectionHeader>
<bodyText confidence="0.9999583125">
Our second approach assumes that semantic class
members are arguments of similar predicates. As
(Pustejovsky and Rumshisky, 2009) elaborate,
predicates select an argument from a specific se-
mantic class, therefore terms that belong in the
same semantic class should be selected by simi-
lar predicates. However, this assumption is often
violated: type coercion allows predicates to have
arguments outside their intended semantic class.
Our solution to this problem, partially inspired by
(Lapata and Lascarides, 2003), is to collect statis-
tics from an enormous amount of data in order to
statistically filter out these coercions.
The English Gigaword Forth Edition corpus1
contains over 8.5 million documents of newswire
text collected over a 15 year period. We processed
these documents with the SENNA2 (Collobert and
Weston, 2009) suite of natural language tools,
which includes a part-of-speech tagger, phrase
chunker, named entity recognizer, and PropBank
semantic role labeler. We chose SENNA due to its
speed, yet it still performs comparably with many
state-of-the-art systems. Of the 8.5 million doc-
uments in English Gigaword, 8 million were suc-
cessfully processed. For each predicate-argument
pair in these documents, we gathered counts by
argument type and argument head. The head was
determined with simple heuristics from the chunk
parse and parts-of-speech for each argument (ar-
guments consisting of more than three phrase
chunks were discarded). When available, named
entity types (e.g., PERSON, ORGANIZATION, LO-
</bodyText>
<footnote confidence="0.995631">
1LDC2009T13
2http://ml.nec-labs.com/senna/
</footnote>
<page confidence="0.980365">
253
</page>
<table confidence="0.999662727272727">
coffee book meeting station report voice
drink write hold own release hear
sip read attend build publish raise
brew publish schedule open confirm give
serve title chair attack issue add
spill sell convene close comment have
smell buy arrange operate submit silence
sell balance call fill deny sound
pour illustrate host shut file lend
buy research plan storm prepare crack
rise review make set voice find
</table>
<tableCaption confidence="0.9439475">
Table 2: Top ten predicates for the most common
word in the train set for the six semantic classes.
</tableCaption>
<bodyText confidence="0.994925818181818">
CATION) were substituted for heads. This resulted
in over 511 million predicate-argument pairs for
argument types ARG0, ARG1, and ARG2. For this
task, however, we chose only to use ARG1 argu-
ments (direct objects), which resulted in 210 mil-
lion pairs, 7.65 million of which were unique. The
ARG1 argument was chosen because most of the
arguments in the data are direct objects 3.
The “best” predicates for a given argument are
defined by a ranking based on Fisher’s exact test
(Fisher, 1922):
</bodyText>
<equation confidence="0.986848666666667">
_ (a + b)! (c + d)! (a + c)! (b + d)!
2
p n!a!b!c!d!
</equation>
<bodyText confidence="0.999992">
where a is the number of times the given argument
was used with the given predicate, b is the number
of times the argument was used with a different
predicate, c is the number of times the predicate
was used with a different argument, d is the num-
ber of times neither the given argument or predi-
cate was used, and n = a+b+c+d. The top ranked
(lowest p) predicates for the most common argu-
ments in the training data are shown in Table 2.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999917">
We have conducted several experiments to test
the performance of the approaches outlined in
Sections 2 and 3 along with additional features
commonly found in information extraction liter-
ature. All experiments were conducted using the
SVMmulticlass support vector machine library4.
</bodyText>
<subsectionHeader confidence="0.992233">
4.1 WordNet PageRank
</subsectionHeader>
<bodyText confidence="0.99984">
We experimented with the output of our WordNet
PageRank implementation along three separate di-
mensions: (1) which sense to use (since we did
not incorporate a word sense disambiguation sys-
tem), (2) whether to use the highest scoring se-
</bodyText>
<footnote confidence="0.9995396">
3The notable exception to this, however, is arrive, where
the data uses the destination argument. In the PropBank
scheme (Palmer et al., 2005), this would correspond to the
ARG4, which usually signifies an end state.
4http://svmlight.joachims.org/svm multiclass.html
</footnote>
<bodyText confidence="0.999294">
mantic class or every class an argument belonged
to, and (3) how to use the weight output by the al-
gorithm. The results of these experiments yielded
a single feature for each class that returns true if
the argument is in that class, regardless of weight.
This resulted in a micro-precision score of 75.6%.
</bodyText>
<subsectionHeader confidence="0.993258">
4.2 Gigaword Predicates
</subsectionHeader>
<bodyText confidence="0.99997625">
We experimented with both (i) the number of pred-
icates to use for an argument and (ii) the score
threshold to use. Ultimately, the Fisher score did
not prove nearly as useful as a classifier as it did
as a ranker. Since the distribution of predicates
for each argument varied significantly, choosing a
high number of predicates would yield good re-
sults for some arguments but not others. However,
because of size of the training data, we were able
to choose the top 5 predicates for each argument
as features and still achieve a reasonable micro-
precision score of 89.6%.
</bodyText>
<subsectionHeader confidence="0.998007">
4.3 Other Features
</subsectionHeader>
<bodyText confidence="0.999984884615385">
Many other features common in information ex-
traction are well-suited for this task. Given that
SVMs can support millions of features, we chose
to add many features simpler than those previously
described in order to improve the final perfor-
mance of the classifier. These include the lemma
of the argument (both the last word’s lemma and
every word’s lemma), the lemma of the predicate,
the number of words in the argument, the casing of
the argument, the part-of-speech of the argument’s
last word, the WordNet synset and all (recursive)
hypernyms of the argument. Additionally, since
the EVENT class is both the most common and
the most often confused, we introduced two fea-
tures based on annotated resources. The first fea-
ture indicates the most common part-of-speech for
the un-lemmatized argument in the Treebank cor-
pus. This helped classify examples such as think-
ing which was confused with a PROPOSITION for
the predicate deny. Second, we introduced a fea-
ture that indicated if the un-lemmatized argument
was considered an event in the TimeBank cor-
pus (Pustejovsky et al., 2003) at least five times.
This helped to distinguish events such as meet-
ing, which was confused with a LOCATION for the
predicate arrive.
</bodyText>
<subsectionHeader confidence="0.997613">
4.4 Ablation Test
</subsectionHeader>
<bodyText confidence="0.999908">
We conducted an ablation test using combina-
tions of five feature sets: (1) our WordNet PageR-
</bodyText>
<page confidence="0.995654">
254
</page>
<table confidence="0.9985722">
+WNSH +WNPR +GWPA +EVNT
WORD 89.2 94.2 95.0 95.6 96.1
EVNT
31.1 89.7 89.9 90.8
GWPA
89.6 90.8 91.0
WNPR
75.6 89.4
WNSH
89.0
</table>
<tableCaption confidence="0.994514">
Table 3: Ablation test of feature sets showing
micro-precision scores.
</tableCaption>
<table confidence="0.99997625">
Precision Recall
Selection vs. Coercion Macro 95.4 95.7
Micro 96.3 96.3
Macro 96.5 95.7
Source Type
Micro 96.1 96.1
Macro 100.0 100.0
Target Type
Micro 100.0 100.0
Macro 85.5 95.2
Joint Type
Micro 96.1 96.1
</table>
<tableCaption confidence="0.997139">
Table 4: Results for UTDMET on SemEval-2010
Task 7.
</tableCaption>
<bodyText confidence="0.998664785714286">
ank feature (WNPR), (2) our Gigaword Predicates
feature (GWPA), (3) word, lemma, and part-of-
speech features (WORD), (4) WordNet synset and
hypernym features (WNSH), and (5) Treebank and
TimeBank features (EVNT). Of these 25 − 1 =
31 tests, 15 are shown in Table 3. The Giga-
word Predicates (GWPA) was the best overall fea-
ture, but each feature set ended up helping the fi-
nal score. WordNet PageRank (WNPR) even im-
proved the score when combined WordNet hyper-
nym features (WNSH) despite the fact that they
are heavily related. Ultimately, WordNet PageR-
ank had a greater precision, while the other Word-
Net features had greater recall.
</bodyText>
<subsectionHeader confidence="0.988316">
4.5 Task 7 Results
</subsectionHeader>
<bodyText confidence="0.999958571428571">
Table 4 shows the official results for UTDMET on
the Task 7 data. The target type was unambigu-
ous given the lemmatized predicate. For classify-
ing selection vs. coercion, we simply checked to
see if the classified source type was the same as
the target type. If this was the case, we returned
selection, otherwise a coercion existed.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999510818181818">
We have presented two approaches for determin-
ing the semantic class of a predicate’s argument.
The two approaches capture different information
and combine well to classify the “source” type in
SemEval-2010 Task 7. We showed how this can be
incorporated into a system to detect coercions as
well as the argument’s compositional history rel-
ative to its predicate. In future work we plan to
extend this system to more complex tasks such as
when the predicate may be polysemous or unseen
predicates may be encountered.
</bodyText>
<sectionHeader confidence="0.995209" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999413">
The authors would like to thank Bryan Rink for
several insights during the course of this work.
</bodyText>
<sectionHeader confidence="0.999416" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999851177777778">
Ronan Collobert and Jason Weston. 2009. Deep
Learning in Natural Language Processing. Tutorial
at NIPS.
Andrea Esuli and Fabrizio Sebastiani. 2007. PageR-
anking WordNet Synsets: An Application to Opin-
ion Mining. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics, pages 424–431.
Ronald A. Fisher. 1922. On the interpretation of χ2
from contingency tables, and the calculation of p.
85(1):87–94.
Sanda Harabagiu, George Miller, and Dan Moldovan.
1999. WordNet 2 - A Morphologically and Se-
mantically Enhanced Resource. In Proceedings of
the SIGLEX Workshop on Standardizing Lexical Re-
sources, pages 1–7.
Sanda Harabagiu, Andrew Hickl, John Lehmann, and
Dan Moldovan. 2005. Experiments with Inter-
active Question-Answering. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 205–214.
George Lakoff and Mark Johnson. 1980. Metaphors
We Live By. University of Chicago Press.
Maria Lapata and Alex Lascarides. 2003. A Proba-
bilistic Account of Logical Metonymy. Computa-
tional Linguistics, 21(2):261–315.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71–106.
James Pustejovsky and Anna Rumshisky. 2009.
SemEval-2010 Task 7: Argument Selection and Co-
ercion. In Proceedings of the NAACL HLT Work-
shop on Semantic Evaluations: Recent Achieve-
ments and Future Directions, pages 88–93.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, and Marcia Lazo. 2003. The TIMEBANK
Corpus. In Proceedings of Corpus Linguistics,
pages 647–656.
Jan Scheffczyk, Adam Pease, and Michael Ellsworth.
2006. Linking FrameNet to the Suggested Upper
Merged Ontology. In Proceedings ofFormal Ontol-
ogy in Information Systems, pages 289–300.
</reference>
<page confidence="0.998407">
255
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.918603">
<title confidence="0.9995185">UTDMet: Combining WordNet and Corpus Data for Argument Coercion Detection</title>
<author confidence="0.998246">Kirk Roberts</author>
<author confidence="0.998246">Sanda Harabagiu</author>
<affiliation confidence="0.99941">Human Language Technology Research Institute University of Texas at Dallas</affiliation>
<address confidence="0.99901">Richardson, Texas, USA</address>
<abstract confidence="0.995664611111111">This paper describes our system for the classification of argument coercion for SemEval-2010 Task 7. We present two approaches to classifying an argument’s semantic class, which is then compared to the predicate’s expected semantic class to detect coercions. The first approach is based on learning the members of an arbitrary semantic class using WordNet’s hypernymy structure. The second approach leverages automatically extracted semantic parse information from a large corpus to identify similar arguments by the predicates that select them. We show the results these approaches obtain on the task as well as how they can improve a traditional feature-based approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<date>2009</date>
<booktitle>Deep Learning in Natural Language Processing. Tutorial at NIPS.</booktitle>
<contexts>
<context position="7691" citStr="Collobert and Weston, 2009" startWordPosition="1248" endWordPosition="1251"> therefore terms that belong in the same semantic class should be selected by similar predicates. However, this assumption is often violated: type coercion allows predicates to have arguments outside their intended semantic class. Our solution to this problem, partially inspired by (Lapata and Lascarides, 2003), is to collect statistics from an enormous amount of data in order to statistically filter out these coercions. The English Gigaword Forth Edition corpus1 contains over 8.5 million documents of newswire text collected over a 15 year period. We processed these documents with the SENNA2 (Collobert and Weston, 2009) suite of natural language tools, which includes a part-of-speech tagger, phrase chunker, named entity recognizer, and PropBank semantic role labeler. We chose SENNA due to its speed, yet it still performs comparably with many state-of-the-art systems. Of the 8.5 million documents in English Gigaword, 8 million were successfully processed. For each predicate-argument pair in these documents, we gathered counts by argument type and argument head. The head was determined with simple heuristics from the chunk parse and parts-of-speech for each argument (arguments consisting of more than three phr</context>
</contexts>
<marker>Collobert, Weston, 2009</marker>
<rawString>Ronan Collobert and Jason Weston. 2009. Deep Learning in Natural Language Processing. Tutorial at NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>PageRanking WordNet Synsets: An Application to Opinion Mining.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>424--431</pages>
<contexts>
<context position="4422" citStr="Esuli and Sebastiani, 2007" startWordPosition="702" endWordPosition="705">n, 15-16 July 2010. c�2010 Association for Computational Linguistics Given a set of seed concepts, we mine WordNet for other concepts that may be in the same semantic class. Clearly, this approach has both practical limitations (WordNet does not contain every possible concept) and linguistic limitations (concepts may belong to different semantic classes based on their context). However, given the often vague nature of semantic classes (is a building an ARTIFACT or a LOCATION?), access to a weighted list of semantic class members can prove useful for arguments not seen in the train set. Using (Esuli and Sebastiani, 2007) as inspiration, we have implemented our own naive version of WordNet PageRank. They use sensedisambiguated glosses provided by eXtended WordNet (Harabagiu et al., 1999) to link synsets by starting with positive (or negative) sentiment concepts in order to find other concepts with positive (or negative) sentiment values. For our task, however, hypernymy relations are more appropriate for determining a given synset’s membership in a semantic class. Hypernymy defines an IS-A relationship between the parent class (the hypernym) and one of its child classes (the hyponym). Furthermore, while PageRa</context>
</contexts>
<marker>Esuli, Sebastiani, 2007</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2007. PageRanking WordNet Synsets: An Application to Opinion Mining. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 424–431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald A Fisher</author>
</authors>
<title>On the interpretation of χ2 from contingency tables, and the calculation of p.</title>
<date>1922</date>
<pages>85--1</pages>
<contexts>
<context position="9427" citStr="Fisher, 1922" startWordPosition="1524" endWordPosition="1525">ew make set voice find Table 2: Top ten predicates for the most common word in the train set for the six semantic classes. CATION) were substituted for heads. This resulted in over 511 million predicate-argument pairs for argument types ARG0, ARG1, and ARG2. For this task, however, we chose only to use ARG1 arguments (direct objects), which resulted in 210 million pairs, 7.65 million of which were unique. The ARG1 argument was chosen because most of the arguments in the data are direct objects 3. The “best” predicates for a given argument are defined by a ranking based on Fisher’s exact test (Fisher, 1922): _ (a + b)! (c + d)! (a + c)! (b + d)! 2 p n!a!b!c!d! where a is the number of times the given argument was used with the given predicate, b is the number of times the argument was used with a different predicate, c is the number of times the predicate was used with a different argument, d is the number of times neither the given argument or predicate was used, and n = a+b+c+d. The top ranked (lowest p) predicates for the most common arguments in the training data are shown in Table 2. 4 Experiments We have conducted several experiments to test the performance of the approaches outlined in Se</context>
</contexts>
<marker>Fisher, 1922</marker>
<rawString>Ronald A. Fisher. 1922. On the interpretation of χ2 from contingency tables, and the calculation of p. 85(1):87–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>George Miller</author>
<author>Dan Moldovan</author>
</authors>
<title>WordNet 2 - A Morphologically and Semantically Enhanced Resource.</title>
<date>1999</date>
<booktitle>In Proceedings of the SIGLEX Workshop on Standardizing Lexical Resources,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="4591" citStr="Harabagiu et al., 1999" startWordPosition="729" endWordPosition="732"> Clearly, this approach has both practical limitations (WordNet does not contain every possible concept) and linguistic limitations (concepts may belong to different semantic classes based on their context). However, given the often vague nature of semantic classes (is a building an ARTIFACT or a LOCATION?), access to a weighted list of semantic class members can prove useful for arguments not seen in the train set. Using (Esuli and Sebastiani, 2007) as inspiration, we have implemented our own naive version of WordNet PageRank. They use sensedisambiguated glosses provided by eXtended WordNet (Harabagiu et al., 1999) to link synsets by starting with positive (or negative) sentiment concepts in order to find other concepts with positive (or negative) sentiment values. For our task, however, hypernymy relations are more appropriate for determining a given synset’s membership in a semantic class. Hypernymy defines an IS-A relationship between the parent class (the hypernym) and one of its child classes (the hyponym). Furthermore, while PageRank assumes directed edges (e.g., hyperlinks in a web page), we use undirected edges. In this way, if HYPERNYMOF(A, B), then A’s membership in a semantic class strengthen</context>
</contexts>
<marker>Harabagiu, Miller, Moldovan, 1999</marker>
<rawString>Sanda Harabagiu, George Miller, and Dan Moldovan. 1999. WordNet 2 - A Morphologically and Semantically Enhanced Resource. In Proceedings of the SIGLEX Workshop on Standardizing Lexical Resources, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Andrew Hickl</author>
<author>John Lehmann</author>
<author>Dan Moldovan</author>
</authors>
<title>Experiments with Interactive Question-Answering.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>205--214</pages>
<contexts>
<context position="1306" citStr="Harabagiu et al., 2005" startWordPosition="196" endWordPosition="199">ic parse information from a large corpus to identify similar arguments by the predicates that select them. We show the results these approaches obtain on the task as well as how they can improve a traditional feature-based approach. 1 Introduction Argument coercion (a type of metonymy) occurs when the expected semantic class (relative to the a predicate) is substituted for an object of a different semantic class. Metonymy is a pervasive phenomenon in language and the interpretation of metonymic expressions can impact tasks from semantic parsing (Scheffczyk et al., 2006) to question answering (Harabagiu et al., 2005). A seminal example in metonymy from (Lakoff and Johnson, 1980) is: (1) The ham sandwich is waiting for his check. The ARG1 for the predicate wait is typically an animate, but the “ham sandwich” is clearly not an animate. Rather, the argument is coerced to fulfill the predicate’s typing requirement. This coercion is allowed because an object that would normally fulfill the typing requirement (the customer) can be uniquely identified by an attribute (the ham sandwich he ordered). SemEval-2010 Task 7 (“Argument Selection and Coercion”) (Pustejovsky and Rumshisky, 2009) was designed to evaluate s</context>
</contexts>
<marker>Harabagiu, Hickl, Lehmann, Moldovan, 2005</marker>
<rawString>Sanda Harabagiu, Andrew Hickl, John Lehmann, and Dan Moldovan. 2005. Experiments with Interactive Question-Answering. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 205–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
<author>Mark Johnson</author>
</authors>
<title>Metaphors We Live By.</title>
<date>1980</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="1369" citStr="Lakoff and Johnson, 1980" startWordPosition="206" endWordPosition="209">arguments by the predicates that select them. We show the results these approaches obtain on the task as well as how they can improve a traditional feature-based approach. 1 Introduction Argument coercion (a type of metonymy) occurs when the expected semantic class (relative to the a predicate) is substituted for an object of a different semantic class. Metonymy is a pervasive phenomenon in language and the interpretation of metonymic expressions can impact tasks from semantic parsing (Scheffczyk et al., 2006) to question answering (Harabagiu et al., 2005). A seminal example in metonymy from (Lakoff and Johnson, 1980) is: (1) The ham sandwich is waiting for his check. The ARG1 for the predicate wait is typically an animate, but the “ham sandwich” is clearly not an animate. Rather, the argument is coerced to fulfill the predicate’s typing requirement. This coercion is allowed because an object that would normally fulfill the typing requirement (the customer) can be uniquely identified by an attribute (the ham sandwich he ordered). SemEval-2010 Task 7 (“Argument Selection and Coercion”) (Pustejovsky and Rumshisky, 2009) was designed to evaluate systems that detect such coercions and provide a “compositional </context>
</contexts>
<marker>Lakoff, Johnson, 1980</marker>
<rawString>George Lakoff and Mark Johnson. 1980. Metaphors We Live By. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
<author>Alex Lascarides</author>
</authors>
<title>A Probabilistic Account of Logical Metonymy.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="7376" citStr="Lapata and Lascarides, 2003" startWordPosition="1198" endWordPosition="1201">s) learned from applying PageRank to WordNet hypernyms. 3 Leveraging a Large Corpus of Semantic Parse Annotations Our second approach assumes that semantic class members are arguments of similar predicates. As (Pustejovsky and Rumshisky, 2009) elaborate, predicates select an argument from a specific semantic class, therefore terms that belong in the same semantic class should be selected by similar predicates. However, this assumption is often violated: type coercion allows predicates to have arguments outside their intended semantic class. Our solution to this problem, partially inspired by (Lapata and Lascarides, 2003), is to collect statistics from an enormous amount of data in order to statistically filter out these coercions. The English Gigaword Forth Edition corpus1 contains over 8.5 million documents of newswire text collected over a 15 year period. We processed these documents with the SENNA2 (Collobert and Weston, 2009) suite of natural language tools, which includes a part-of-speech tagger, phrase chunker, named entity recognizer, and PropBank semantic role labeler. We chose SENNA due to its speed, yet it still performs comparably with many state-of-the-art systems. Of the 8.5 million documents in </context>
</contexts>
<marker>Lapata, Lascarides, 2003</marker>
<rawString>Maria Lapata and Alex Lascarides. 2003. A Probabilistic Account of Logical Metonymy. Computational Linguistics, 21(2):261–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Paul Kingsbury</author>
<author>Daniel Gildea</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="10612" citStr="Palmer et al., 2005" startWordPosition="1730" endWordPosition="1733">ce of the approaches outlined in Sections 2 and 3 along with additional features commonly found in information extraction literature. All experiments were conducted using the SVMmulticlass support vector machine library4. 4.1 WordNet PageRank We experimented with the output of our WordNet PageRank implementation along three separate dimensions: (1) which sense to use (since we did not incorporate a word sense disambiguation system), (2) whether to use the highest scoring se3The notable exception to this, however, is arrive, where the data uses the destination argument. In the PropBank scheme (Palmer et al., 2005), this would correspond to the ARG4, which usually signifies an end state. 4http://svmlight.joachims.org/svm multiclass.html mantic class or every class an argument belonged to, and (3) how to use the weight output by the algorithm. The results of these experiments yielded a single feature for each class that returns true if the argument is in that class, regardless of weight. This resulted in a micro-precision score of 75.6%. 4.2 Gigaword Predicates We experimented with both (i) the number of predicates to use for an argument and (ii) the score threshold to use. Ultimately, the Fisher score d</context>
</contexts>
<marker>Palmer, Kingsbury, Gildea, 2005</marker>
<rawString>Martha Palmer, Paul Kingsbury, and Daniel Gildea. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Anna Rumshisky</author>
</authors>
<title>SemEval-2010 Task 7: Argument Selection and Coercion.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions,</booktitle>
<pages>88--93</pages>
<contexts>
<context position="1879" citStr="Pustejovsky and Rumshisky, 2009" startWordPosition="288" endWordPosition="291">t al., 2006) to question answering (Harabagiu et al., 2005). A seminal example in metonymy from (Lakoff and Johnson, 1980) is: (1) The ham sandwich is waiting for his check. The ARG1 for the predicate wait is typically an animate, but the “ham sandwich” is clearly not an animate. Rather, the argument is coerced to fulfill the predicate’s typing requirement. This coercion is allowed because an object that would normally fulfill the typing requirement (the customer) can be uniquely identified by an attribute (the ham sandwich he ordered). SemEval-2010 Task 7 (“Argument Selection and Coercion”) (Pustejovsky and Rumshisky, 2009) was designed to evaluate systems that detect such coercions and provide a “compositional history” of argument selection relative to the predicate. In order to accomplish this, an argument is annotated with both the semantic class to which it belongs (the “source” type) as well as the class expected by the predicate (the “target” type). However, in the data provided, the target type was unambiguous given the lemmatized predicate, so the remainder of this paper discusses source type classification. The detection of coercion is then simply performed by checking if the classified source type and </context>
<context position="6991" citStr="Pustejovsky and Rumshisky, 2009" startWordPosition="1141" endWordPosition="1144">ca .325 pie chart .308 EVENT LOCATION rock concert .382 heliport .381 rodeo .369 mukataa .380 radium therapy .357 subway station .342 seminar .347 dairy farm .326 pub crawl .346 gateway .320 PROPOSITION SOUND dibs .363 whoosh .353 white paper .322 squish .353 tall tale .319 yodel .339 commendation .310 theme song .320 field theory .309 oldie .312 Table 1: Some of the concepts (and scores) learned from applying PageRank to WordNet hypernyms. 3 Leveraging a Large Corpus of Semantic Parse Annotations Our second approach assumes that semantic class members are arguments of similar predicates. As (Pustejovsky and Rumshisky, 2009) elaborate, predicates select an argument from a specific semantic class, therefore terms that belong in the same semantic class should be selected by similar predicates. However, this assumption is often violated: type coercion allows predicates to have arguments outside their intended semantic class. Our solution to this problem, partially inspired by (Lapata and Lascarides, 2003), is to collect statistics from an enormous amount of data in order to statistically filter out these coercions. The English Gigaword Forth Edition corpus1 contains over 8.5 million documents of newswire text collec</context>
</contexts>
<marker>Pustejovsky, Rumshisky, 2009</marker>
<rawString>James Pustejovsky and Anna Rumshisky. 2009. SemEval-2010 Task 7: Argument Selection and Coercion. In Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 88–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Sauri</author>
<author>Andrew See</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Dragomir Radev</author>
<author>Beth Sundheim</author>
<author>David Day</author>
<author>Lisa Ferro</author>
<author>Marcia Lazo</author>
</authors>
<title>The TIMEBANK Corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of Corpus Linguistics,</booktitle>
<pages>647--656</pages>
<contexts>
<context position="12741" citStr="Pustejovsky et al., 2003" startWordPosition="2082" endWordPosition="2085">-of-speech of the argument’s last word, the WordNet synset and all (recursive) hypernyms of the argument. Additionally, since the EVENT class is both the most common and the most often confused, we introduced two features based on annotated resources. The first feature indicates the most common part-of-speech for the un-lemmatized argument in the Treebank corpus. This helped classify examples such as thinking which was confused with a PROPOSITION for the predicate deny. Second, we introduced a feature that indicated if the un-lemmatized argument was considered an event in the TimeBank corpus (Pustejovsky et al., 2003) at least five times. This helped to distinguish events such as meeting, which was confused with a LOCATION for the predicate arrive. 4.4 Ablation Test We conducted an ablation test using combinations of five feature sets: (1) our WordNet PageR254 +WNSH +WNPR +GWPA +EVNT WORD 89.2 94.2 95.0 95.6 96.1 EVNT 31.1 89.7 89.9 90.8 GWPA 89.6 90.8 91.0 WNPR 75.6 89.4 WNSH 89.0 Table 3: Ablation test of feature sets showing micro-precision scores. Precision Recall Selection vs. Coercion Macro 95.4 95.7 Micro 96.3 96.3 Macro 96.5 95.7 Source Type Micro 96.1 96.1 Macro 100.0 100.0 Target Type Micro 100.0</context>
</contexts>
<marker>Pustejovsky, Hanks, Sauri, See, Gaizauskas, Setzer, Radev, Sundheim, Day, Ferro, Lazo, 2003</marker>
<rawString>James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, and Marcia Lazo. 2003. The TIMEBANK Corpus. In Proceedings of Corpus Linguistics, pages 647–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Scheffczyk</author>
<author>Adam Pease</author>
<author>Michael Ellsworth</author>
</authors>
<title>Linking FrameNet to the Suggested Upper Merged Ontology.</title>
<date>2006</date>
<booktitle>In Proceedings ofFormal Ontology in Information Systems,</booktitle>
<pages>289--300</pages>
<contexts>
<context position="1259" citStr="Scheffczyk et al., 2006" startWordPosition="188" endWordPosition="191">pproach leverages automatically extracted semantic parse information from a large corpus to identify similar arguments by the predicates that select them. We show the results these approaches obtain on the task as well as how they can improve a traditional feature-based approach. 1 Introduction Argument coercion (a type of metonymy) occurs when the expected semantic class (relative to the a predicate) is substituted for an object of a different semantic class. Metonymy is a pervasive phenomenon in language and the interpretation of metonymic expressions can impact tasks from semantic parsing (Scheffczyk et al., 2006) to question answering (Harabagiu et al., 2005). A seminal example in metonymy from (Lakoff and Johnson, 1980) is: (1) The ham sandwich is waiting for his check. The ARG1 for the predicate wait is typically an animate, but the “ham sandwich” is clearly not an animate. Rather, the argument is coerced to fulfill the predicate’s typing requirement. This coercion is allowed because an object that would normally fulfill the typing requirement (the customer) can be uniquely identified by an attribute (the ham sandwich he ordered). SemEval-2010 Task 7 (“Argument Selection and Coercion”) (Pustejovsky </context>
</contexts>
<marker>Scheffczyk, Pease, Ellsworth, 2006</marker>
<rawString>Jan Scheffczyk, Adam Pease, and Michael Ellsworth. 2006. Linking FrameNet to the Suggested Upper Merged Ontology. In Proceedings ofFormal Ontology in Information Systems, pages 289–300.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>