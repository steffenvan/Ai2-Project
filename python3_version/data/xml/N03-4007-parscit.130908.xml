<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011857">
<note confidence="0.9221995">
Demonstration of the CROSSMARC System of HLT-NAACL 2003
Proceedings
</note>
<title confidence="0.831733">
Vangelis Karkaletsis , Constantine D. Spyropoulos , Dimitris Souflis✁ , Claire Grover✂ ,
Demonstrations , pp.
Edmonton, May-June 2
</title>
<author confidence="0.539593">
Ben Hachey , Maria Teresa Pazienza , Michele Vindigni , Emmanuel Cartier , Jos´e Coch
</author>
<affiliation confidence="0.452624">
Institute for Informatics and Telecommunications, NCSR “Demokritos”
</affiliation>
<email confidence="0.338521">
vangelis, costass @iit.demokritos.gr
</email>
<author confidence="0.364009">
Velti S.A.
</author>
<affiliation confidence="0.574097">
Dsouflis@velti.net
Division of Informatics, University of Edinburgh
</affiliation>
<email confidence="0.569908">
grover, bhachey @ed.ac.uk
</email>
<bodyText confidence="0.6421415">
D.I.S.P., Universita di Roma Tor Vergata
pazienza, vindigni @info.uniroma2.it
Lingway
emmanuel.cartier, Jose.Coch @lingway.com
</bodyText>
<note confidence="0.5029105">
13-14
003
</note>
<sectionHeader confidence="0.998598" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939923076923">
The EC-funded R&amp;D project, CROSSMARC, is develop-
ing technology for extracting information from domain-
specific web pages, employing language technology
methods as well as machine learning methods in order
to facilitate technology porting to new domains. CROSS-
MARC also employs localisation methodologies and user
modelling techniques in order to provide the results of
extraction in accordance with the user’s personal pref-
erences and constraints. The system’s implementation
is based on a multi-agent architecture, which ensures a
clear separation of responsibilities and provides the sys-
tem with clear interfaces and robust and intelligent infor-
mation processing capabilities.
</bodyText>
<sectionHeader confidence="0.93063" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.999619">
The CROSSMARC architecture consists of the following
main processing stages:
Collection of domain-specific web pages, involving
two sub-stages:
- domain-specific web crawling (focused crawling)
for the identification of web sites that are of rele-
vance to the particular domain (e.g. retailers of elec-
tronic products).
- domain-specific spidering of the retrieved web sites
in order to identify web pages of interest (e.g. laptop
product descriptions).
Information extraction from the domain-specific web
pages, which involves two main sub-stages:
- named entity recognition to identify named enti-
ties such as product manufacturer name or company
name in descriptions inside the web page written in
any of the project’s four languages (English, Greek,
French, Italian) (Grover et al. 2002). Cross-lingual
name matching techniques are also employed in or-
der to link expressions referring to the same named
entities across languages.
- fact extraction to identify those named entities that
fill the slots of the template specifying the infor-
mation to be extracted from each web page. To
achieve this the project combines wrapper-induction
approaches for fact extraction with language-based
information extraction in order to develop site inde-
pendent wrappers for the domain examined.
Data Storage, to store the extracted information (from
the web page descriptions in any of the project’s four
languages) into a common database.
Data Presentation, to present the extracted information
to the end-user through a multilingual user interface, in
accordance with the user’s language and preferences.
As a cross-lingual multi-domain system, the goal of
CROSSMARC is to cover a wide area of possible knowl-
edge domains and a wide range of conceivable facts in
each domain. To achieve this we construct an ontology
of each domain which reflects a certain degree of domain
expert knowledge (Pazienza et al. 2003). Cross-linguality
is achieved with the lexica, which provide language spe-
cific synonyms for all the ontology entries. During infor-
mation extraction, web pages are matched against the do-
main ontology and an abstract representation of this real
world information (facts) is generated.
As shown in Figure 1, the CROSSMARC multi-agent
architecture includes agents for web page collection
(crawling agent, spidering agent), information extraction,
data storage and data presentation. These agents commu-
nicate through the blackboard. The Crawling Agent de-
fines a schedule for invoking the focused crawler which is
</bodyText>
<figureCaption confidence="0.999814">
Figure 1: Architecture of the CROSSMARC system
</figureCaption>
<bodyText confidence="0.999643827586207">
written to the blackboard and can be refined by the human
administrator. The Spidering Agent is an autonomous
software component, which retrieves sites to spider from
the blackboard and locates interesting web pages within
them by traversing their links. Again, status information
is written to the blackboard.
The multi-lingual IE system is a distributed one where
the individual monolingual components are autonomous
processors, which need not all be installed on the same
machine. (These components have been developed us-
ing a wide range of base technologies: see, for example,
Petasis et al. (2002), Mikheev et al. (1998), Pazienza and
Vindigni (2000)). The IE systems are not offered as web
services, therefore a proxy mechanism is required, util-
ising established remote access mechanisms (e.g. HTTP)
to act as a front-end for every IE system in the project. In
effect, this proxy mechanism turns every IE system into a
web service. For this purpose, we have developed an In-
formation Extraction Remote Invocation module (IERI)
which takes XHTML pages as input and routes them to
the corresponding monolingual IE system according to
the language they are written in. The Information Extrac-
tion Agent retrieves pages stored on the blackboard by the
Spidering Agent, invokes the Information Extraction sys-
tem (through IERI) for each language and writes the ex-
tracted facts (or error messages) on the blackboard. This
information will then be used by the Data Storage Agent
in order to read the extracted facts and to store them in
the product database.
</bodyText>
<sectionHeader confidence="0.993232" genericHeader="method">
3 The CROSSMARC Demonstration
</sectionHeader>
<bodyText confidence="0.999941666666667">
The first part of the CROSSMARC demonstration is the
user-interface accessed via a web-page. The user is pre-
sented with the prototype user-interface which supports
menu-driven querying of the product databases for the
two domains. The user enters his/her preferences and is
presented with information about matching products in-
cluding links to the pages which contain the offers.
The main part of the demonstration shows the full
information extraction system including web crawl-
ing, site spidering and Information Extraction. The
demonstration show the results of the individual mod-
ules including real-time spidering of web-sites to
find pages which contain product offers and real-
time information extraction from the pages in the
four project languages, English, French, Italian and
Greek. Screen shots of various parts of the system are
available at http://www.iit.demokritos.gr/
skel/crossmarc/demo-images.htm
</bodyText>
<sectionHeader confidence="0.997564" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99989275">
This research is funded by the European Commis-
sion (IST2000-25366). Further information about the
CROSSMARC project can be found at http://www.
iit.demokritos.gr/skel/crossmarc/.
</bodyText>
<sectionHeader confidence="0.99897" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998665653846154">
C. Grover, S. McDonald, V. Karkaletsis, D. Farmakiotou,
G. Samaritakis, G. Petasis, M.T. Pazienza, M. Vin-
digni, F. Vichot and F. Wolinski. 2002. Multilingual
XML-Based Named Entity Recognition In Proceed-
ings of the International Conference on Language Re-
sources and Evaluation (LREC 2002).
A. Mikheev, C. Grover, and M. Moens. 1998. Descrip-
tion of the LTG system used for MUC-7. In Seventh
Message Understanding Conference (MUC–7): Pro-
ceedings of a Conference held in Fairfax, Virginia,
29April–1 May, 1998. http://www.muc.saic.
com/proceedings/muc_7_toc.html.
M. T. Pazienza, A. Stellato, M. Vindigni, A. Valarakos,
and V. Karkaletsis. 2003. Ontology integration in a
multilingual e-retail system. In Proceedings of the Hu-
man Computer Interaction International (HCII’2003),
Special Session on ”Ontologies and Multilinguality in
User Interfaces.
M. T. Pazienza and M. Vindigni. 2000. Identification and
classification of Italian complex proper names. In Pro-
ceedings ofACIDCA2000 International Conference.
G. Petasis, V. Karkaletsis, G. Paliouras, I. Androutsopou-
los, and C. D. Spyropoulos. 2002. Ellogon: A new
text engineering platform. In Proceedings of the Third
International Conference on Language Resources and
Evaluation (LREC 2002).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.002624">
<note confidence="0.949515">Demonstration of the CROSSMARC System of HLT-NAACL 2003 Proceedings Karkaletsis , Constantine D. Spyropoulos , Dimitris Claire Demonstrations , pp. Edmonton, May-June 2</note>
<author confidence="0.879535">Michele Vindigni</author>
<affiliation confidence="0.836809">Institute for Informatics and Telecommunications, NCSR</affiliation>
<title confidence="0.360325">vangelis, costass @iit.demokritos.gr</title>
<author confidence="0.852969">S A Velti</author>
<affiliation confidence="0.734853">Dsouflis@velti.net Division of Informatics, University of</affiliation>
<email confidence="0.533591">grover,bhachey@ed.ac.uk</email>
<author confidence="0.56427">Universita di_Roma Tor D I S P</author>
<email confidence="0.507754">pazienza,vindigni@info.uniroma2.it</email>
<author confidence="0.26995">Lingway</author>
<email confidence="0.707798">emmanuel.cartier,Jose.Coch@lingway.com</email>
<address confidence="0.3042885">13-14 003</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Grover</author>
<author>S McDonald</author>
<author>V Karkaletsis</author>
<author>D Farmakiotou</author>
<author>G Samaritakis</author>
<author>G Petasis</author>
<author>M T Pazienza</author>
<author>M Vindigni</author>
<author>F Vichot</author>
<author>F Wolinski</author>
</authors>
<title>Multilingual XML-Based Named Entity Recognition</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="2135" citStr="Grover et al. 2002" startWordPosition="293" endWordPosition="296">wling (focused crawling) for the identification of web sites that are of relevance to the particular domain (e.g. retailers of electronic products). - domain-specific spidering of the retrieved web sites in order to identify web pages of interest (e.g. laptop product descriptions). Information extraction from the domain-specific web pages, which involves two main sub-stages: - named entity recognition to identify named entities such as product manufacturer name or company name in descriptions inside the web page written in any of the project’s four languages (English, Greek, French, Italian) (Grover et al. 2002). Cross-lingual name matching techniques are also employed in order to link expressions referring to the same named entities across languages. - fact extraction to identify those named entities that fill the slots of the template specifying the information to be extracted from each web page. To achieve this the project combines wrapper-induction approaches for fact extraction with language-based information extraction in order to develop site independent wrappers for the domain examined. Data Storage, to store the extracted information (from the web page descriptions in any of the project’s fo</context>
</contexts>
<marker>Grover, McDonald, Karkaletsis, Farmakiotou, Samaritakis, Petasis, Pazienza, Vindigni, Vichot, Wolinski, 2002</marker>
<rawString>C. Grover, S. McDonald, V. Karkaletsis, D. Farmakiotou, G. Samaritakis, G. Petasis, M.T. Pazienza, M. Vindigni, F. Vichot and F. Wolinski. 2002. Multilingual XML-Based Named Entity Recognition In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
<author>C Grover</author>
<author>M Moens</author>
</authors>
<title>Description of the LTG system used for MUC-7.</title>
<date>1998</date>
<booktitle>In Seventh Message Understanding Conference (MUC–7): Proceedings of a Conference held in</booktitle>
<location>Fairfax, Virginia, 29April–1</location>
<note>http://www.muc.saic. com/proceedings/muc_7_toc.html.</note>
<contexts>
<context position="4528" citStr="Mikheev et al. (1998)" startWordPosition="659" endWordPosition="662">ten to the blackboard and can be refined by the human administrator. The Spidering Agent is an autonomous software component, which retrieves sites to spider from the blackboard and locates interesting web pages within them by traversing their links. Again, status information is written to the blackboard. The multi-lingual IE system is a distributed one where the individual monolingual components are autonomous processors, which need not all be installed on the same machine. (These components have been developed using a wide range of base technologies: see, for example, Petasis et al. (2002), Mikheev et al. (1998), Pazienza and Vindigni (2000)). The IE systems are not offered as web services, therefore a proxy mechanism is required, utilising established remote access mechanisms (e.g. HTTP) to act as a front-end for every IE system in the project. In effect, this proxy mechanism turns every IE system into a web service. For this purpose, we have developed an Information Extraction Remote Invocation module (IERI) which takes XHTML pages as input and routes them to the corresponding monolingual IE system according to the language they are written in. The Information Extraction Agent retrieves pages store</context>
</contexts>
<marker>Mikheev, Grover, Moens, 1998</marker>
<rawString>A. Mikheev, C. Grover, and M. Moens. 1998. Description of the LTG system used for MUC-7. In Seventh Message Understanding Conference (MUC–7): Proceedings of a Conference held in Fairfax, Virginia, 29April–1 May, 1998. http://www.muc.saic. com/proceedings/muc_7_toc.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Pazienza</author>
<author>A Stellato</author>
<author>M Vindigni</author>
<author>A Valarakos</author>
<author>V Karkaletsis</author>
</authors>
<title>Ontology integration in a multilingual e-retail system.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Computer Interaction International (HCII’2003), Special Session on ”Ontologies and Multilinguality in User Interfaces.</booktitle>
<contexts>
<context position="3247" citStr="Pazienza et al. 2003" startWordPosition="466" endWordPosition="469">. Data Storage, to store the extracted information (from the web page descriptions in any of the project’s four languages) into a common database. Data Presentation, to present the extracted information to the end-user through a multilingual user interface, in accordance with the user’s language and preferences. As a cross-lingual multi-domain system, the goal of CROSSMARC is to cover a wide area of possible knowledge domains and a wide range of conceivable facts in each domain. To achieve this we construct an ontology of each domain which reflects a certain degree of domain expert knowledge (Pazienza et al. 2003). Cross-linguality is achieved with the lexica, which provide language specific synonyms for all the ontology entries. During information extraction, web pages are matched against the domain ontology and an abstract representation of this real world information (facts) is generated. As shown in Figure 1, the CROSSMARC multi-agent architecture includes agents for web page collection (crawling agent, spidering agent), information extraction, data storage and data presentation. These agents communicate through the blackboard. The Crawling Agent defines a schedule for invoking the focused crawler </context>
</contexts>
<marker>Pazienza, Stellato, Vindigni, Valarakos, Karkaletsis, 2003</marker>
<rawString>M. T. Pazienza, A. Stellato, M. Vindigni, A. Valarakos, and V. Karkaletsis. 2003. Ontology integration in a multilingual e-retail system. In Proceedings of the Human Computer Interaction International (HCII’2003), Special Session on ”Ontologies and Multilinguality in User Interfaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Pazienza</author>
<author>M Vindigni</author>
</authors>
<title>Identification and classification of Italian complex proper names.</title>
<date>2000</date>
<booktitle>In Proceedings ofACIDCA2000 International Conference.</booktitle>
<contexts>
<context position="4558" citStr="Pazienza and Vindigni (2000)" startWordPosition="663" endWordPosition="666">nd can be refined by the human administrator. The Spidering Agent is an autonomous software component, which retrieves sites to spider from the blackboard and locates interesting web pages within them by traversing their links. Again, status information is written to the blackboard. The multi-lingual IE system is a distributed one where the individual monolingual components are autonomous processors, which need not all be installed on the same machine. (These components have been developed using a wide range of base technologies: see, for example, Petasis et al. (2002), Mikheev et al. (1998), Pazienza and Vindigni (2000)). The IE systems are not offered as web services, therefore a proxy mechanism is required, utilising established remote access mechanisms (e.g. HTTP) to act as a front-end for every IE system in the project. In effect, this proxy mechanism turns every IE system into a web service. For this purpose, we have developed an Information Extraction Remote Invocation module (IERI) which takes XHTML pages as input and routes them to the corresponding monolingual IE system according to the language they are written in. The Information Extraction Agent retrieves pages stored on the blackboard by the Spi</context>
</contexts>
<marker>Pazienza, Vindigni, 2000</marker>
<rawString>M. T. Pazienza and M. Vindigni. 2000. Identification and classification of Italian complex proper names. In Proceedings ofACIDCA2000 International Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Petasis</author>
<author>V Karkaletsis</author>
<author>G Paliouras</author>
<author>I Androutsopoulos</author>
<author>C D Spyropoulos</author>
</authors>
<title>Ellogon: A new text engineering platform.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="4505" citStr="Petasis et al. (2002)" startWordPosition="655" endWordPosition="658">e CROSSMARC system written to the blackboard and can be refined by the human administrator. The Spidering Agent is an autonomous software component, which retrieves sites to spider from the blackboard and locates interesting web pages within them by traversing their links. Again, status information is written to the blackboard. The multi-lingual IE system is a distributed one where the individual monolingual components are autonomous processors, which need not all be installed on the same machine. (These components have been developed using a wide range of base technologies: see, for example, Petasis et al. (2002), Mikheev et al. (1998), Pazienza and Vindigni (2000)). The IE systems are not offered as web services, therefore a proxy mechanism is required, utilising established remote access mechanisms (e.g. HTTP) to act as a front-end for every IE system in the project. In effect, this proxy mechanism turns every IE system into a web service. For this purpose, we have developed an Information Extraction Remote Invocation module (IERI) which takes XHTML pages as input and routes them to the corresponding monolingual IE system according to the language they are written in. The Information Extraction Agen</context>
</contexts>
<marker>Petasis, Karkaletsis, Paliouras, Androutsopoulos, Spyropoulos, 2002</marker>
<rawString>G. Petasis, V. Karkaletsis, G. Paliouras, I. Androutsopoulos, and C. D. Spyropoulos. 2002. Ellogon: A new text engineering platform. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC 2002).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>