<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.56433">
Comparison between Tagged Corpora for the Named Entity
Task
</title>
<author confidence="0.728694">
Chikashi NOBATA Nigel COLLIER and Jun&apos;ichi TSUJII
</author>
<affiliation confidence="0.816024">
Kansai Advanced Research Center Department of Information Science
Communications Research Laboratory Graduate School of Science
</affiliation>
<address confidence="0.358118">
588-2 Iwaoka, Iwaoka-cho, Nishi-ku University of Tokyo, Hongo 7-3-1
Kobe, Hyogo, 651-2492 JAPAN Bunkyo-ku, Tokyo, 113-0033 JAPAN
</address>
<email confidence="0.54141">
nova@cr1 .go. jp tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.985787" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999981909090909">
We present two measures for compar-
ing corpora based on information the-
ory statistics such as gain ratio as well
as simple term-class frequency counts.
We tested the predictions made by these
measures about corpus difficulty in two
domains — news and molecular biol-
ogy — using the result of two well-used
paradigms for NE, decision trees and
HMMs and found that gain ratio was the
more reliable predictor.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999892390243903">
With the advent of the information society and
increasing availability of large amounts of infor-
mation in electronic form, new technologies such
as information extraction are emerging to meet
user&apos;s information access needs. Recent evalu-
ation conferences such as TREC (Voorhees and
Harman, 2000) showed the feasibility of this task
and highlighted the need to combine information
retrieval (IR) and extraction (TE) to go beyond
simply offering the user a long ranked list of in-
teresting documents to providing facts for user&apos;s
questions.
The problem of domain dependence remains a
serious one and in fact there has been very little
work so far to compare the difficulty of IE tasks for
different domains and their corpora. Such knowl-
edge is useful for developing IE systems that are
portable between domains. This paper begins to
address this issue, in particular the lowest level of
TE task, defined in the TIPSTER sponsored MUC-
6 conference (MUC, 1995) as named entity (NE).
This is emerging as a key technology in several
other IE-related tasks such as question answer-
ing. We seek here to show theoretically motivated
measures for comparing the difficulty of corpora
for the NE task in two domains, newswire and
molecular-biology. We then test the predictions
made by these measures against actual system
performance.
Recently TE systems based on supervised learn-
ing paradigms such as hidden Markov models
(Bikel et al., 1997), maximum entropy (Borth-
wick et al., 1998) and decision trees (Seidne et
al., 1998) have emerged that should be easier to
adapt to new domains than the dictionary-based
systems of the past. Much of this work has taken
advantage of smoothing techniques to overcome
problems associated with data sparseness (Chen
and Goodman, 1996).
The two corpora we use in our NE experiments
represent the following domains:
</bodyText>
<listItem confidence="0.980855833333333">
• Newswire: acquisition of names of people, or-
ganizations and monetary units etc., from the
MUC-6 data set.
• Molecular-biology: acquisition of proteins,
DNAs, RNAs etc. from a subset of the MED-
LINE database (MEDLINE, 1999).
</listItem>
<bodyText confidence="0.999407">
Information extraction in the molecular-biology
domain (Selcimizu et al., 1998) (Craven and Kum-
lien, 1999) (Rindflesch et al., 2000) has recently
become a topic of interest to the NLP community.
This is a result of the need to formalise the huge
number of research results that appear in free-text
form in online collections of journal abstracts and
papers such as MEDLINE for databases such as
Swissprot (Bairoch and Apweiler, 1997) and also
to search such collections for facts in an intelligent
way.
The purpose of our study is not to show a high
level of absolute system performance. In fact since
we use only the MUC-6 executive succession data
set of 60 articles and a new MEDLINE data set
of 100 articles we cannot hope to achieve perfor-
mance limits. What we aim to do is to compare
model performance against the predictions of cor-
pus difficulty made by two different methods. In
the rest of this paper we firstly introduce the NE
models used for evaluation, the two corpora we
</bodyText>
<page confidence="0.988741">
20
</page>
<bodyText confidence="0.99883025">
examined and then the difficulty comparison met-
rics. Predictive scores from the metrics are ex-
amined against the actual performance of the NE
models.
</bodyText>
<sectionHeader confidence="0.984288" genericHeader="introduction">
2 Models
</sectionHeader>
<bodyText confidence="0.999632076923077">
Recent studies into the use of supervised learning-
based models for the NE task in the molecular-
biology domain have shown that models based on
hidden Markov models (HMMs) (Collier et al.,
2000) and decision trees (Nobata et al., 1999) are
not only adaptable to this highly technical do-
main, but are also much more generalizable to new
classes of words than systems based on traditional
hand-built heuristic rules such as (Fukuda et al.,
1998). We now describe two models used in our
experiments based on the decision trees package
C4.5 (Quinlan, 1993) and HMMs (Rabiner and
Juang, 1986).
</bodyText>
<subsectionHeader confidence="0.939724">
2.1 Decision tree named entity
recogniser:NE-DT
</subsectionHeader>
<bodyText confidence="0.99455035483871">
A decision tree is a type of classifier which
has &amp;quot;leaf nodes&amp;quot; indicating classes and &amp;quot;decision
nodes&amp;quot; that specify some test to be carried out,
with one branch or subtree for each possible out-
come of the test. A decision tree can be used
to classify an object by starting at the root of
the tree and moving through it until a leaf is en-
countered. When we can define suitable features
for the decision tree, the system can achieve good
performance with only a small amount of training
data.
The system we used is based on one that was
originally created for Japanese documents (Sekine
et al., 1998). It has two phases, one for creating
the decision tree from training data and the other
for generating the class-tagged text based on the
decision tree. When generating decision trees, tri-
gams of words were used. For this system, words
are considered to be quadruple features. The fol-
lowing features are used to generate conditions in
the decision tree:
Part-of-speech information: There are 45
part-of-speech categories, whose definitions
are based on Pennsylvania Treebank&apos;s cat-
egories. We use a tagger based on Adwait
Ratnaparkhrs method (Ratnaparkhi, 1996).
Character type information: Orthographic
information is considered such as upper case,
lower case, capitalization, numerical expres-
sions, symbols. These character features
are the same as those used by NEHMM
</bodyText>
<tableCaption confidence="0.8002925">
described in the next section and shown in
Table 1.
</tableCaption>
<bodyText confidence="0.987898">
Word lists specific to the domain: Word
lists are made from the training corpus.
Only the 200 highest frequency words are
used.
</bodyText>
<subsectionHeader confidence="0.820272">
2.2 Hidden Markov model named entity
recog-niser: NEHMM
</subsectionHeader>
<bodyText confidence="0.999891857142857">
HMMs are a widely used class of learning algo-
rithms and can be considered to be stochastic fi-
nite state machines. In the following model, sum-
marized here from the full description given in
(Collier et al., 2000), we consider words to be or-
dered pairs consisting of a surface word, W, and
a word feature, F, given as &lt; W,F &gt; . The word
features themselves are discussed below. As is
common practice, we need to calculate the prob-
abilities for a word sequence for the first word&apos;s
name class and every other word differently since
we have no initial name-class to make a transition
from. Accordingly we use the following equation
to calculate the initial name class probability,
</bodyText>
<equation confidence="0.8916695">
Pr(NCil &lt; Wfirat, Ff ire; &gt;) =
0.0 f (NC firstj &lt; W first, Ffirst &gt;)
cri f (NCfirst I &lt; Ff iret &gt;)
02 f (NCfirst) (1)
</equation>
<bodyText confidence="0.5630645">
and for all other words and their name classes
as follows:
</bodyText>
<equation confidence="0.996140571428571">
Pr(NCtl &lt; Wt,Ft &gt;,&lt; &gt;,NCt-1) =
Aof(NCtl &lt;W, &gt;,&lt; NCt-1.)+
ANCti &lt; &gt;,&lt; &gt;,NCt-1) +
A2 f (NCtl &lt; Wt, Ft &gt;,&lt; -,Ft-i &gt;,NCt-i) +
X31(NCti &lt;..,Ft &gt;,&lt; Ft-i &gt;,NCt-i) +
f (NCtINCt-i) +
A5 f (NCt) (2)
</equation>
<bodyText confidence="0.998780333333333">
where (I) is calculated with maximum-
likelihood estimates from counts on training data.
In our current system we set the constants Ai
and ai by hand and let E7j = 1.0, E Ai = 1.0,
&gt; cri &gt; 02, Ao &gt; &gt; A. The cur-
rent name-class NCt is conditioned on the cur-
rent word and feature, the previous name-class,
NCt_i, and previous word and feature.
Equations 1 and 2 implement a linear-
interpolating HMM that incorporates a number of
sub-models designed to reduce the effects of data
sparseness.
</bodyText>
<page confidence="0.999718">
71
</page>
<tableCaption confidence="0.999607">
Table 1: Word features with examples
</tableCaption>
<table confidence="0.997205428571429">
Word Feature Example Feature Ex.
TwoDigitNumber 25 OpenSquare [
FourDigitNumber 2000 CloseSquare [
DigitNumber 15012 Colon .
SingleCap M SemiColon ;
GreekLetter alpha Percent %
CapsAndDigits 12 OpenParen (
TwoCaps RaIGDS CloseParen )
LettersAndDigits p52 Comma ,
InitCap Interleukin FullStop .
LowCaps kappaB Determiner the
Lowercase kinases Conjunction and
Hyphon - Other *14
Backslash /
</table>
<bodyText confidence="0.999447173913043">
Once the state transition probabilities have
been calculated according to Equations 1 and 2,
the Viterbi algorithm (Viterbi, 1967) is used to
search the state space of possible name class as-
signments in linear time to find the highest prob-
ability path, i.e. to maximise Pr (W, NC). The fi-
nal stage of our algorithm that is used after name-
class tagging is complete is to use a clean-up mod-
ule called Unity. This creates a frequency list
of words and name-classes and then re-tags the
text using the most frequently used name class
assigned by the HMM. We have generally found
that this improves F-score performance by be-
tween 2 and 4%, both for re-tagging spuriously
tagged words and for finding untagged words in
unknown contexts that had been correctly tagged
elsewhere in the text.
Table 1 shows the character features that we
used in both NEHMM and NE-DT. Our intuition
is that such features will help the model to find
similarities between known words that were found
in the training set and unknown words and so
overcome the unknown word problem.
</bodyText>
<sectionHeader confidence="0.996035" genericHeader="method">
3 Corpora
</sectionHeader>
<bodyText confidence="0.9999215">
We used two corpora in our experiments repre-
senting two popular domains in 1E, molecular-
biology (from MEDLINE) and newswire texts
(from MUC-6). These are now described.
</bodyText>
<subsectionHeader confidence="0.979887">
3.1 MIJC-6
</subsectionHeader>
<bodyText confidence="0.999914444444444">
The corpus for MUC-6 (MUC, 1995) contains 60
articles, from the test corpus for the dry and for-
mal runs. An example can be seen in Figure 1. We
can see several interesting features of the domain
such as the focus of NEs on people and organizer
tion profiles. Moreover we see that there are many
pre-name clue words such as &amp;quot;Ms.&amp;quot; or &amp;quot;Rep.&amp;quot; indi-
cating that a Republican politician&apos;s name should
follow.
</bodyText>
<subsectionHeader confidence="0.995975">
3.2 Biology
</subsectionHeader>
<bodyText confidence="0.99973605882353">
In our tests in the domain of molecular-biology
we are using abstracts available from PubMed&apos;s
MEDLINE. The MEDLINE database is an online
collection of abstracts for published journal arti-
cles in biology and medicine and contains more
than nine million articles. Currently we have ex-
tracted a subset of MEDLINE based on a search
using the keywords human AND blood cell AND
transcription factor yielding about 3650 abstracts.
Of these 100 documents were NE tagged for our
experiments using a human domain expert. An
example of the annotated abstracts is shown in
Figure 2. In contrast to MUC-6 each article is
quite short and there are few pre-class clue words
making the task much more like terminology iden-
tification and classification than pure name find-
ing.
</bodyText>
<sectionHeader confidence="0.763893" genericHeader="method">
4 A first attempt at corpus
comparison based on simple
token frequency
</sectionHeader>
<bodyText confidence="0.999976166666666">
A simple and intuitive approach to NE task dif-
ficulty comparison used in some previous studies
such as (Palmer and Day, 1997) who studied cor-
pora in six different languages, compares class to
term-token ratios on the assumption that rarer
classes are more difficult to acquire. The relative
frequency counts from these ratios also give an in-
direct measure of the granularity of a class, i.e.
how wide it is. While this is appealing, we show
that this approach does not necessarily give the
best metric for comparison.
Tables 2 and 3 show the ratio of the number of
different words used in NEs to the total number
of words in the NE class vocabulary. The num-
ber of different tokens is influenced by the corpus
size and is not a suitable index that can uniformly
show the difficulty for different NE tasks, there-
fore it should be normalized. Here we use words
as tokens. A value close to zero indicates little
variation within the class and should imply that
the class is easier to acquire. We see that the NEs
in the biology domain seem overall to be easier
to acquire than those in the MUC-6 domain given
lexical variation.
The figures in the second columns of Tables 2
and 3 are normalized so that all numerals are re-
placed by a single token. It still seems though
that MUC-6 is a considerably more challenging
domain than biology. This is despite the fact that
the ratios for ENAMEX expressions such as Date,
</bodyText>
<page confidence="0.983303">
22
</page>
<bodyText confidence="0.967367111111111">
A graduate of &lt;ENAMEX TYPE=&amp;quot;ORGANIZATION&amp;quot;&gt;Harvard Law School&lt;/ENAMEX&gt;, Ms.
&lt;ENAMEX TYPE=&amp;quot;PERS014&amp;quot;&gt;Washington&lt;/ENAMEX&gt; worked as a laywer for the corporate fi-
nance division of the &lt;ENAMEX TYPE=&amp;quot;ORGANIZATION&amp;quot;&gt;SEC&lt;/ENAMEX&gt; in the late &lt;TIMEX
TYPE=&amp;quot;DATE&amp;quot;&gt;1970s&lt;/TIMEX&gt;. She has been a congressional staffer since &lt;TIMEX TYPE=
&amp;quot;DATE&amp;quot; &gt;1979&lt;/TIMEX&gt;. Separately, &lt;ENAMEX TYPE=&amp;quot;PERSON&amp;quot;&gt;Clinton&lt;/ENAMEX&gt; transi-
tion officials said that &lt;ENAMEX TYPE=&amp;quot;PERSON&amp;quot;&gt;Frank Newman&lt;/ENAMEX&gt;, 50, vice chairman
and chief financial officer of &lt;ENAMEX TYPE=&amp;quot;ORGANIZATION&amp;quot;&gt;BankAmerica. Corp.&lt;/ENAMEX&gt;,
is expected to be nominated as acsistant &lt;ENAMEX TYPE=&amp;quot;ORGANIZATION&amp;quot;&gt;Treasury&lt;/ENAMEX&gt;
secretary for domestic finance.
</bodyText>
<figureCaption confidence="0.926817333333333">
Figure 1: Example sentences taken from the annotated MUC-6 NE text
&lt;PROTEIN&gt;Sox-4&lt;/PROTEIN&gt;, an &lt;PROTEIN&gt;Sry-like HMG box protein&lt;/PROTElN&gt;, is
a transcriptional activator in &lt;SOURCE.cell-type&gt;lymphocytes&lt;/SOURCE&gt;. Previous studies in
&lt;SOURCE.cell-type&gt;lymphocytes&lt;/SOURCE&gt; have described two DNA-binding &lt;PROTEIN&gt;HMG
box proteins&lt;/PROTEIN&gt;, &lt;PROTEIN&gt;TCF-1&lt;/PROTEIN&gt; and &lt;PROTEIN&gt;LEF-1&lt;/PROTEIN&gt;,
with affinity for the &lt;DNA&gt;A/TA/TCAAAG motif&lt;/DNA&gt; found in several &lt;SOURCE.cell-type&gt;T
cell&lt;/SOURCE&gt;-specific enhancers. Evaluation of cotransfection experiments in &lt;SOURCE.cell-type&gt;non-
T cells&lt;/SOURCE&gt; and the observed inactivity of an &lt;DNA&gt;AACAAAG concatamer&lt;/DNA&gt; in the
&lt; PROTEIN&gt; TCF-1 &lt; /PROTEIN&gt; / &lt; PROTEIN&gt; LEF-1 &lt; /PROTEIN&gt;-expressing &lt;SOURCE.cell-line&gt;T
cell line BW5147&lt;/SOURCE&gt;, led us to conclude that these two proteins did not mediate the observed
enhancer effect.
Figure 2: Example sentences taken from the annotated biology text
</figureCaption>
<tableCaption confidence="0.744519">
Table 2: Frequency values for words in the MUC-6 Table 3: Frequency values for words in the biology
test corpus corpus
</tableCaption>
<table confidence="0.999408888888889">
Class Original Norm. numerals
Org. 0.28(=507 / 1783) 0.28(=507 / 1783)
Person 0.45(=381 / 838) 0.45(=381 / 838)
Loc. 0.38(=148 / 390) 0.38(=148 / 390)
Date 0.23(=123 / 542) 0.11(= 60 / 542)
Time 1.00(= 3 / 3) 1.00(= 3 / 3)
Money 0.33(=138 / 423) 0.05(= 20 / 423)
Percent 0.39(= 42 / 108) 0.03(= 3 / 108)
All 0.33(=1342/4087) 0.27(=1122/4087)
</table>
<bodyText confidence="0.994763769230769">
Money and Percent all fall significantly. Expres-
sions in the Time class are so rare however that it
is difficult to make any sort of meaningful compar-
ison. In the biology corpus, the ratios are not sig-
nificantly changed and the NE classes defined for
biology documents seem to have the same char-
acteristics as non-numeric ENAMEX classes in
MUC-6 documents.
Comparing between the biology documents and
the MUC-6 documents, we may say that identify-
ing entities in biology documents is easier than
identifying ENAMEX entities in MUC-6 docu-
ments.
</bodyText>
<sectionHeader confidence="0.999539" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.994145">
We evaluated the performance of our two systems
using a cross validation method. For the MUC-
6 corpus, 6-fold cross validation was performed
on the 60 texts and 5-fold cross validation was
performed for the 100 texts in the biology corpus.
</bodyText>
<table confidence="0.999363333333333">
Class Original Norm. numerals
DNA 0.21(=245 / 1140) 0.20(=228 / 1140)
Protein 0.15(=631 / 4125) 0.13(=540 / 4125)
RNA 0.43(= 30 / 70) 0.43(= 30 / 70)
Source 0.16(=248 / 1533) 0.16(=242 / 1533)
All 0.17(=1154/6868) 0.15(=1040/6868
</table>
<bodyText confidence="0.9984793">
We use &amp;quot;F-scores&amp;quot; for evaluation of our experi-
ments (Van Rijsbergen, 1979). &amp;quot;F-score&amp;quot; is a mea-
surement combining &amp;quot;Recall&amp;quot; and &amp;quot;Precision&amp;quot; and
defined in Equation 3. &amp;quot;Recall&amp;quot; is the percent-
age of answers proposed by the system that corre-
spond to those in the human-made key set. &amp;quot;Pre-
cision&amp;quot; is the percentage of correct answers among
the answers proposed by the system. The F-scores
presented here are automatically calculated using
a scoring program (Chinchor, 1995).
</bodyText>
<equation confidence="0.995677">
2 x Precision x Recall
F-score =
Precision + Recall
</equation>
<bodyText confidence="0.9996595">
In Table 4 we show the actual performance
of our term recognition systems, NE-DT and
NEHMM. We can see that corpus comparisons
based only on class-token ratios are inadequate to
explain why both systems&apos; performance was about
the same in both domains or why NEHMM did
better in both test corpora than NE-DT. The dif-
ference in performance is despite there being more
training examples in biology (3301 NEs) than in
MUC-6 (2182 NEs). Part of the reason for this is
</bodyText>
<equation confidence="0.280178">
(3)
</equation>
<tableCaption confidence="0.990511">
Table 4: Performance of the NE systems
</tableCaption>
<table confidence="0.95800225">
System MUC-6 Biology
NEHMM with Unity 78.4 75.0
NEHMM w/o Unity 74.2 73.1
NE-DT I 68.5 69.4
</table>
<bodyText confidence="0.987037333333333">
that the class-token ratios ignore individual sys-
tem knowledge, i.e. the types of features that
can be captured and useful in the corpus domain.
Among other considerations they also fail to con-
sider the overlap of words and features between
dasses in the same corpus domain.
</bodyText>
<sectionHeader confidence="0.9888375" genericHeader="method">
6 Corpus comparison based on
information theoretical measures
</sectionHeader>
<bodyText confidence="0.999259666666667">
In this section we attempt to present measures
that overcome some of the limitations of the class-
token method. We evaluate the contribution from
each feature used in our NE recognition systems
by calculating its entropy. There are three types of
feature information used by our two systems: lex-
ical information, character type information, and
part-of-speech information.
The entropy for NE classes H(C) is defined by
</bodyText>
<equation confidence="0.996253">
H(C)= — E p(c) log2 p(c)
cEC
where:
n(c)
p(c) =
</equation>
<bodyText confidence="0.997764833333333">
n(c): the number of words in class c
N: the total number of words in text
We can calculate the entropy for features in the
same way.
When a feature F is given, the conditional en-
tropy for NE elnsses H(C1F) is defined by
</bodyText>
<equation confidence="0.929726571428572">
= - E E p(c, f) log2 p(c)f)
cec feF
where:
n(c, f)
P(c, f)
n(c, f)
&amp;if) = n(f)
</equation>
<bodyText confidence="0.998235125">
n(c, f): the number of words in class c
with the feature value f
n(f): the number of words
with the feature value f
Using these entropies, we can calculate infor-
mation gain (Breiman et al., 1984) and gain ra-
tio (Quinlan, 1990). Information gain for NE
classes and a feature I(C; F) is given as follows:
</bodyText>
<equation confidence="0.994927">
I(C;F) = H(C) — H(CIF)
</equation>
<bodyText confidence="0.99979325">
The information gain I(C; F) shows how the fea-
ture F is related with NE classes C. When F is
completely independent of C, the value of /(C; F)
becomes the minimum value 0. The maximum
value of I(C; F) is equivalent to that of H(C),
when the feature F gives sufficient information to
recognize named entities. Information gain can
also be calculated by:
</bodyText>
<equation confidence="0.927246">
I(C; F) = H(C) H(F) H(C,F)
</equation>
<bodyText confidence="0.998674714285714">
We show the values of the above three entropies
in Table 5,6, and 7. In these tables, F is replaced
with single letters which represent each of the
model&apos;s features, i.e. character types (T), part-
of-speech (P), and lexical information (W).
Gain ratio is the normalized value of informa-
tion gain. The gain ratio GR(C; F) is defined by
</bodyText>
<equation confidence="0.98039925">
GR(C; F)
I (C; F)
=
H(C)
</equation>
<bodyText confidence="0.9999048">
The range of the gain ratio GR(C; F) is 0 5_
GR(C; F) &lt; 1 even when the class entropy is
different in various corpora, so we can compare
the values directly in the different NE recognition
tasks.
</bodyText>
<subsectionHeader confidence="0.997494">
6.1 Character types
</subsectionHeader>
<bodyText confidence="0.999537642857143">
Character type features are used to identify
named entities in the MUC-6 and biology corpus.
However, the distribution of the character types
are quite different between these two types of doc-
uments as we can see in Table 5. We see through
the gain-ratio score that character type informa-
tion has a greater predictive power for classes in
MUC-6 than biology due to the higher entropy
of character type and class sequences in the bi-
ology corpus, i.e. the greater disorder of this in-
formation. The result partially shows why iden-
tification and classification is harder in biological
documents than in newspaper articles such as the
MUC-6 corpus.
</bodyText>
<subsectionHeader confidence="0.999881">
6.2 Part-of-speech
</subsectionHeader>
<bodyText confidence="0.9998192">
Table 6 shows the entropy scores for part-of-
speech (POS) sequences in the two corpora. We
see through the gain ratio scores that POS infor-
mation is not so powerful for acquiring NEs in the
biology domain compared to the MUC-6 domain.
</bodyText>
<page confidence="0.999691">
24
</page>
<tableCaption confidence="0.998953">
Table 5: Values of Entropy for character type
</tableCaption>
<table confidence="0.9996795">
Entropy MUC-6 Biology
H(T) 1.880 2.013
H(C) 0.890 1.264
H(C,T) 2.345 2.974
I(C;T) 0.425 0.302
GR(C;T) 0.478 0.239
</table>
<tableCaption confidence="0.962051">
Table 6: Values of Entropy for POSs
</tableCaption>
<table confidence="0.992238">
Entropy MUC-6 Biology
H(P) 4.287 4.037
H(C) 0.890 1.264
H(C,P) 4.750 5.029
I(C;P) 0.426 0.272
GR(C;P) 0.479 0.216
</table>
<bodyText confidence="0.999554777777778">
In fact POS information for biology is far less use-
ful than character information when we compare
the results in Tables 5 and 6, whereas POS has
about the same predictive power as character in-
formation in the MUC-6 domain. One likely ex-
planation for this is that the POS tagger we use in
NE-DT is trained on a corpus based on newspaper
articles, therefore the assigned POS tags are often
incorrect in biology documents.
</bodyText>
<subsectionHeader confidence="0.999307">
6.3 Lexical information
</subsectionHeader>
<bodyText confidence="0.999445928571429">
Table 7 shows the entropy statistics for the two
domains. Although entropy for words in biology
is lower than MUC-6, the entropy for classes is
higher leading to a lower gain ratio in biology. We
also note that, as we would expect, in comparison
to the other two types of knowledge, surface word
forms are by far the most useful type of knowledge
with a gain ratio in MUC-6 of 0.897 compared to
0.479 for POS and 0.478 for character types in the
same domain. However, such knowledge is also
the least generalizable and runs the risk of data-
sparseness. It therefore has to be complemented
by more generalizable knowledge such as character
features and POS.
</bodyText>
<tableCaption confidence="0.996292">
Table 7: Values of Entropy for words
</tableCaption>
<table confidence="0.9995655">
Entropy MUC-6 Biology
H(W) 9.570 8.890
H(C) 0.890 1.264
H(C,W) 9.662 9.232
I(C;W) 0.798 0.921
GR(C;W) 0.897 0.729
</table>
<tableCaption confidence="0.95759">
Table 8: Values of Entropy for NEHMM features
in the MUC-6 corpus
</tableCaption>
<table confidence="0.999835285714286">
GR Cross Entropy Coverage Features.
0.994 5.38(4.08-9.68) 0.44(0.34-0.75) for Ao
0.898 7.69(6.97-9.32) 0.77(0.72-0.90) for At
0.967 7.73(7.07-9.30) 0.79(0.73-0.90) for A2
0.798 4.38(4.12-4.82) 0.99(0.98-1.00) for As
0.340 1.62(1.32-1.90) 1.00(1.00-1.00) Ct-1
0.806 7.65(7.11-8.65) 0.85(0.81-0.93) WI
0.461 2.64(2.41-2.97) 1.00(0.99-1.00) Ft
0.558 7.91(7.25-8.99) 0.83(0.79-0.92) WI -1
0.221 2.94(2.70-3.25) 1.00(1.00-1.00) F1-1
0.806 7.65(7.11-8.65) 0.85(0.81-0.93) Wt Ft
0.563 7.92(7.26-9.03) 0.83(0.79-0.92) W1-1 Ft-1
0.971 5.42(4.10-9.70) 0.44(0.34-0.75) Wt
0.633 4.18(3.91-4.60) 0.99(0.99-1.00) F1_ j.
</table>
<tableCaption confidence="0.9887665">
Table 9: Values of Entropy for NEHMM features
in the biology corpus
</tableCaption>
<table confidence="0.999696357142857">
GR Cross Entropy Coverage Features
0.977 5.83(5.66-6.14) 0.49(0.48-0.52) for Ao
0.793 7.93(7.77-8.08) 0.80(0.79-0.81) for At
0.929 7.79(7.65-7.85) 0.80(0.79-0.81) for A2
0.643 5.07(4.95-5.21) 0.98(0.98-0.98) for A3
0.315 2.26(2.24-2.28) 1.00(1.00-1.00) Cl _I
0.694 7.64(7.52-7.78) 0.89(0.87-0.89) Wt
0.257 3.12(3.06-3.19) 1.00(1.00-1.00) Ft
0.423 7.99(7.82-8.05) 0.87(0.86-0.88) W/-1
0.093 3.33(3.27-3.43) 1.00(1.00-1.00) F1_1
0.694 7.64(7.52-7.78) 0.89(0.87-0.89) Wt Ft
0.424 7.98(7.82-8.04) 0.87(0.86-0.88) W1-1 F1-1
0.904 5.96(5.78-6.24) 0.50(0.49-0.52) W2-1,1
0.339 4.65(4.53-4.78) 0.99(0.98-0.99) F1-1.2
</table>
<subsectionHeader confidence="0.9744975">
6.4 Comparison between the
combination of features
</subsectionHeader>
<bodyText confidence="0.98900125">
In this section we show a comparison of gain ra-
tio for the features used by both systems in each
corpus. Values of gain ratio for each feature set
are shown on the &apos;GR&apos; column in Tables 8, 9, 10
and 11&apos;. The values of GR show that surface
words have the best contribution in both corpora
for both systems. We can see that gain ratio for
all features in NE-DT is actually lower than the
top level model for NEHMM in biology, reflecting
the actual system performance that we observed.
We also see that in the biology corpus, the com-
bination of all features in NE-DT has a lower con-
tribution than in the MUC-6 corpus. This indi-
cates the limitation of the current feature set for
the biology corpus and shows that we need to uti-
lize other types of features in this domain.
Values for cross entropy between training and
test sets are shown in Tables 8, 9, 10 and 11 to-
&apos;On the &apos;Features&apos; column, &amp;quot;(Features) for Ate&apos;
means the features used in each HMM sub-
model which corresponds with the A# in Equa-
tion 2. And also, `ALL&apos; in Tables 10 and 11
means all the features used in decision tree, i.e.
{Pt-1,t,t+1,Ft -144+1 ,Wt -1,t,t+1}.
</bodyText>
<page confidence="0.553454">
&apos;IC
</page>
<tableCaption confidence="0.889818">
Table 10: Values of Entropy for NE-DT features
in the MUC-6 corpus
</tableCaption>
<table confidence="0.999277090909091">
GR Cross Entropy Coverage I Features
0.998 1.59(1.38-1.77) 0.12(0.10-1).13 ALL
0.402 5.22(5.09-5.32) 1.00(0.99-1.00) Ps
0.468 2.66(2.51-2.87) 1.00(0.99-1.00) Fs
0.844 7.36(7.19-7.57) 0.81(0.80-0.83) Ws
0.670 7.89(7.81-7.97) 0.98(0.96-0.98) Pt.-1,t
0.669 3.87(3.67-4.07) 0.99(0.98-1.00) Ft-53
0.977 4.42(4.10-4.88) 0.36(0.34-0.40) Ws-Ls
0.822 9.25(9.10-9.40) 0.89(0.87-0.91) Pe-2,t,t+1
0.807 4.92(4.72-5.08) 0.96(0.95-196) Ft-t,s,s+1
0.998 1.89(1.67-2.16) 0.15(0.13-0.17) Wt-13.:+1
</table>
<tableCaption confidence="0.9886195">
Table 11: Values of Entropy for NE-DT features
in the biology corpus
</tableCaption>
<table confidence="0.999468545454545">
CR Cross Entropy Coverage Features
0.937 2.31(2.00-2.50) 0.18(0.15-0.19 .1&amp;quot;
0.237 5.31(5.21-5.38) 1.00(0.99-1.00) Ps
0.262 3.27(3.14-3.41) 1.00(1.00-1.00) Fs
0.416 7.63(7.50-7.79) 0.87(0.85-0.88) Ws
0.370 7.78(7.69-7.86) 0.97(0.96-0.97) Pt-1,s
0.383 4.57(4.38-4.67) 0.98(0.98-0.99) Ft-1.:
0.586 5.71(5.37-5.93) 0.48(0.45-0.50) Ws... 1,,
0.541 8.92(8.82-9.02) 0.88(0.87-0.89) Ps.-133+1
0.502 5.46(5.26-5.64) 0.95(0.94-0.96) Ft-13,t+1
0.764 2.56(2.25-2.76) 0.20(0.17-0.21) Ws-1 t t t
</table>
<bodyText confidence="0.99972162962963">
gether with error bounds in parentheses. These
values are calculated for pairs of an NE class and
features, and averaged for the n-fold experiments.
In the MUC-6 corpus, 60 texts are separated into
6 subsets, and one of them is used as the test set
and the others are put together to form a train-
ing set. Similarly, 100 texts are separated into 5
subsets in the biology corpus. We also show the
coverage of the pairs on the &apos;Coverage&apos; column.
Coverage means that how many pairs which ap-
peared in a test set also appear in a training set.
In these columns, the greater the cross entropy
between features and a class, the more different
their occurrences between training and test sets.
On the other hand, as the coverage for class-
features pairs increases, so does the part of the
test set that is covered with the given feature set.
The results in both corpora for both systems
show a drawback of surface words, since their cov-
erage for a test set is lower than that of features
like POSs and character types in both corpora.
Also, the coverage of surface words in the biol-
ogy corpus is higher than in the MUC6 corpus
as opposed to other features. The result matches
our intuition that vocabulary in the biology corpus
is relatively restricted but has a variety of types
other than normal English words.
</bodyText>
<sectionHeader confidence="0.998773" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999984342105263">
The need for soundly-motivated metrics to com-
pare the usefulness of corpora for specific tasks
and systems is clearly necessary for the develop-
ment of robust and portable information extrac-
tion systems.
In this paper we have shown that measures for
comparing corpora based just on class-token ratios
have difficulty predicting system performance and
cannot adequately explain the difficulty of the NE
task either generally or for specific systems.
While we should be cautious in making sweep-
ing conclusions due to the small size of corpora in
our study, our results from gain ratio and cross
entropy indicate that counts from the features of
both systems will be more useful in the MUC6 cor-
pus than in the biology corpus. We can also see
that while the coverage is limited, surface words
play a leading role for both systems. Gain ra-
tio statistics for surface words in the two domains
were far closer than for any other type of feature,
and given that this is also the dominant knowl-
edge type this seems to be one likely reason that
the performance of systems is about the same in
both domains.
We have presented the results of applying two
supervised learning based models to the named
entity task in two widely different domains and
explained the performance through class-token ra-
tios, entropy and gain ratio. Measures such as
entropy and gain ratio have been found to have
the best predictive power, although the features
used to calculate gain ratio are not sufficient to
describe all the information that is necessary for
the named entity task. In future work we intend
to extend our study to new and larger NE corpora
in various domains and to try to reduce the error
factor in our calculations that is a result of corpus
size.
</bodyText>
<sectionHeader confidence="0.975669" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.76611675">
A. Bairoch and R. Apweiler. 1997. The SWISS-
PROT protein sequence data bank and its new
supplement TrEMBL. Nucleic Acids Research,
25:31-36.
</bodyText>
<reference confidence="0.71155125">
D. Bikel, S. Miller, R. Schwartz, and
R. Wesichedel. 1997. Nymble: a high-
performance learning name-finder. In Pro-
ceedings of the Fifth Confererence on Applied
Natural Language Processing, pages 194-201.
A. Borthwick, J. Sterling, E. Agichtein, and
R. Grishman. 1998. Exploiting diverse knowl-
edge sources via maximum entropy in named
entity recognition. In Proceedings of the Work-
shop on Very Large Corpora (WVLC&apos;98).
L. Breiman, R. Friedman, A. Olshen, and
C. Stone. 1984. Classification and regression
</reference>
<page confidence="0.953461">
26
</page>
<reference confidence="0.999040790697675">
trees. Belmont CA: Wadsworth International
Group.
S. Chen and J. Goodman. 1996. An empiri-
cal study of smoothing techniques for language
modeling. 3.4st Annual Meeting of the Associ-
ation of Computational Linguistics, California,
USA, 24-27 June.
N. Chinchor. 1995. MUC-5 evaluation metrics.
In In Proceedings of the Fifth Message Un-
derstanding Conference (MUC-5), Baltimore,
Maryland, USA., pages 69-78.
N. Collier, C. Nobata, and J. Tsujii. 2000. Ex-
tracting the names of genes and gene products
with a hidden Markov model. In Proceedings
of the .18th International Conference on Com-
putational Linguistics (COLING&apos;2000), Saar-
bracken, Germany, July 31st-August 4th.
M. Craven and J. Knrnlien. 1999. Constructing
biological knowledge bases by extracting infor-
mation from text sources. In Proceedings of the
7th International Conference on Intelligent Sys-
temps for Molecular Biology (ISMB-99), Hei-
delburg, Germany, August 6-10.
K. Fukuda, T. Tsunoda, A. Tamura, and T. Tak-
agi. 1998. Toward information extraction:
identifying protein names from biological pa-
pers. In Proceedings of the Pacific Symposium
on Biocomputing&apos;98 (PSB &apos;98), January.
MEDLINE. 1999. The PubMed
database can be found at:.
http://www.ncbi.nlm.nih.gov/PubMed/.
DARPA. 1995. Proceedings of the Sixth Message
Understanding Conference(MUC-6), Columbia,
MD, USA, November. Morgan Kaufmann.
C. Nobata, N. Collier, and J. Tsujii. 1999. Au-
tomatic term identification and classification
in biology texts. In Proceedings of the Nat-
ural Language Pacific Rim Symposium (NL-
PRS&apos;2000), November.
D. Palmer and D. Day. 1997. A statistical
profile of the named entity task. In Proceed-
ings of the Fifth Conference on Applied Natural
Language Processing (ANLP&apos;97), Washington
D.C., USA., 31 March -3 April.
J.R. Quinlan. 1990. Introduction to Decision
Trees. In J.W. Shavlik and T.G. Dietterich, ed-
itors, Readings in Machine Learning. Morgan
Kaufmann Publishers, Inc., San Mateo, Cali-
fornia.
J.R. Quinlan. 1993. c4.5 Programs for Machine
Learning. Morgan Kaufmann Publishers, Inc.,
San Mateo, California.
L. Rabiner and B. Juang. 1986. An introduction
to hidden Markov models. IEEE ASSP Maga-
zine, pages 4-16, January.
A. Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Confer-
ence on Empirical Methods in Natural Language
Processing, pages 133-142, University of Penn-
sylvania, May.
T. Rindflesch, L. Tanabe, N. Weinstein, and L..
Hunter. 2000. EDGAR.: Extraction of drugs,
genes and relations from the biomedical litera-
ture. In Pacific Symposium on Bio-informatics
(PSB&apos;2000), Hawaii, USA, January.
T. Sekimizu, H. Park, and J. Tsujii. 1998. Iden-
tifying the interaction between genes and gene
products based on frequently seen verbs in med-
line abstracts. In Genome Informatics. Univer-
sal Academy Press, Inc.
Satoshi Sekine, Ralph Grishman, and Hiroyuki
Shinnou. 1998. A Decision Tree Method for
Finding and Classifying Names in Japanese
Texts. In Proceedings of the Sixth Workshop
on Very Large Corpora, Montreal, Canada, Au-
gust.
C. Van Rijsbergen. 1979. Information Retrieval.
Butterworths, London.
A. J. Viterbi. 1967. Error bounds for convolutions
codes and an asymptotically optimum decoding
algorithm. IEEE Transactions on Information
Theory, IT-13(2):260-269.
E.M. Voorhees and D.K. Harman, editors.
2000. The Eighth Text REtrieval Confer-
ence (TREC-8), Electronic version available at
http://trec.nist.gov/pubs.html.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.175866">
<title confidence="0.915982">Comparison between Tagged Corpora for the Named Entity Task</title>
<author confidence="0.593788">Chikashi NOBATA Nigel COLLIER</author>
<author confidence="0.593788">Jun&apos;ichi TSUJII</author>
<affiliation confidence="0.7573715">Kansai Advanced Research Center Department of Information Science Communications Research Laboratory Graduate School of Science</affiliation>
<address confidence="0.992899">588-2 Iwaoka, Iwaoka-cho, Nishi-ku University of Tokyo, Hongo Kobe, Hyogo, 651-2492 JAPAN Bunkyo-ku, Tokyo, 113-0033 JAPAN</address>
<email confidence="0.498082">.go.jp</email>
<abstract confidence="0.99937975">We present two measures for comparing corpora based on information theory statistics such as gain ratio as well as simple term-class frequency counts. We tested the predictions made by these measures about corpus difficulty in two domains — news and molecular biology — using the result of two well-used paradigms for NE, decision trees and HMMs and found that gain ratio was the more reliable predictor.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Bikel</author>
<author>S Miller</author>
<author>R Schwartz</author>
<author>R Wesichedel</author>
</authors>
<title>Nymble: a highperformance learning name-finder.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Confererence on Applied Natural Language Processing,</booktitle>
<pages>194--201</pages>
<contexts>
<context position="2268" citStr="Bikel et al., 1997" startWordPosition="351" endWordPosition="354">This paper begins to address this issue, in particular the lowest level of TE task, defined in the TIPSTER sponsored MUC6 conference (MUC, 1995) as named entity (NE). This is emerging as a key technology in several other IE-related tasks such as question answering. We seek here to show theoretically motivated measures for comparing the difficulty of corpora for the NE task in two domains, newswire and molecular-biology. We then test the predictions made by these measures against actual system performance. Recently TE systems based on supervised learning paradigms such as hidden Markov models (Bikel et al., 1997), maximum entropy (Borthwick et al., 1998) and decision trees (Seidne et al., 1998) have emerged that should be easier to adapt to new domains than the dictionary-based systems of the past. Much of this work has taken advantage of smoothing techniques to overcome problems associated with data sparseness (Chen and Goodman, 1996). The two corpora we use in our NE experiments represent the following domains: • Newswire: acquisition of names of people, organizations and monetary units etc., from the MUC-6 data set. • Molecular-biology: acquisition of proteins, DNAs, RNAs etc. from a subset of the </context>
</contexts>
<marker>Bikel, Miller, Schwartz, Wesichedel, 1997</marker>
<rawString>D. Bikel, S. Miller, R. Schwartz, and R. Wesichedel. 1997. Nymble: a highperformance learning name-finder. In Proceedings of the Fifth Confererence on Applied Natural Language Processing, pages 194-201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
<author>J Sterling</author>
<author>E Agichtein</author>
<author>R Grishman</author>
</authors>
<title>Exploiting diverse knowledge sources via maximum entropy in named entity recognition.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora (WVLC&apos;98).</booktitle>
<contexts>
<context position="2310" citStr="Borthwick et al., 1998" startWordPosition="357" endWordPosition="361">e, in particular the lowest level of TE task, defined in the TIPSTER sponsored MUC6 conference (MUC, 1995) as named entity (NE). This is emerging as a key technology in several other IE-related tasks such as question answering. We seek here to show theoretically motivated measures for comparing the difficulty of corpora for the NE task in two domains, newswire and molecular-biology. We then test the predictions made by these measures against actual system performance. Recently TE systems based on supervised learning paradigms such as hidden Markov models (Bikel et al., 1997), maximum entropy (Borthwick et al., 1998) and decision trees (Seidne et al., 1998) have emerged that should be easier to adapt to new domains than the dictionary-based systems of the past. Much of this work has taken advantage of smoothing techniques to overcome problems associated with data sparseness (Chen and Goodman, 1996). The two corpora we use in our NE experiments represent the following domains: • Newswire: acquisition of names of people, organizations and monetary units etc., from the MUC-6 data set. • Molecular-biology: acquisition of proteins, DNAs, RNAs etc. from a subset of the MEDLINE database (MEDLINE, 1999). Informat</context>
</contexts>
<marker>Borthwick, Sterling, Agichtein, Grishman, 1998</marker>
<rawString>A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman. 1998. Exploiting diverse knowledge sources via maximum entropy in named entity recognition. In Proceedings of the Workshop on Very Large Corpora (WVLC&apos;98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
<author>R Friedman</author>
<author>A Olshen</author>
<author>C Stone</author>
</authors>
<title>Classification and regression trees. Belmont CA:</title>
<date>1984</date>
<publisher>Wadsworth International Group.</publisher>
<contexts>
<context position="17690" citStr="Breiman et al., 1984" startWordPosition="2907" endWordPosition="2910"> and part-of-speech information. The entropy for NE classes H(C) is defined by H(C)= — E p(c) log2 p(c) cEC where: n(c) p(c) = n(c): the number of words in class c N: the total number of words in text We can calculate the entropy for features in the same way. When a feature F is given, the conditional entropy for NE elnsses H(C1F) is defined by = - E E p(c, f) log2 p(c)f) cec feF where: n(c, f) P(c, f) n(c, f) &amp;if) = n(f) n(c, f): the number of words in class c with the feature value f n(f): the number of words with the feature value f Using these entropies, we can calculate information gain (Breiman et al., 1984) and gain ratio (Quinlan, 1990). Information gain for NE classes and a feature I(C; F) is given as follows: I(C;F) = H(C) — H(CIF) The information gain I(C; F) shows how the feature F is related with NE classes C. When F is completely independent of C, the value of /(C; F) becomes the minimum value 0. The maximum value of I(C; F) is equivalent to that of H(C), when the feature F gives sufficient information to recognize named entities. Information gain can also be calculated by: I(C; F) = H(C) H(F) H(C,F) We show the values of the above three entropies in Table 5,6, and 7. In these tables, F i</context>
</contexts>
<marker>Breiman, Friedman, Olshen, Stone, 1984</marker>
<rawString>L. Breiman, R. Friedman, A. Olshen, and C. Stone. 1984. Classification and regression trees. Belmont CA: Wadsworth International Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>3.4st Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>24--27</pages>
<location>California, USA,</location>
<contexts>
<context position="2597" citStr="Chen and Goodman, 1996" startWordPosition="404" endWordPosition="407">omparing the difficulty of corpora for the NE task in two domains, newswire and molecular-biology. We then test the predictions made by these measures against actual system performance. Recently TE systems based on supervised learning paradigms such as hidden Markov models (Bikel et al., 1997), maximum entropy (Borthwick et al., 1998) and decision trees (Seidne et al., 1998) have emerged that should be easier to adapt to new domains than the dictionary-based systems of the past. Much of this work has taken advantage of smoothing techniques to overcome problems associated with data sparseness (Chen and Goodman, 1996). The two corpora we use in our NE experiments represent the following domains: • Newswire: acquisition of names of people, organizations and monetary units etc., from the MUC-6 data set. • Molecular-biology: acquisition of proteins, DNAs, RNAs etc. from a subset of the MEDLINE database (MEDLINE, 1999). Information extraction in the molecular-biology domain (Selcimizu et al., 1998) (Craven and Kumlien, 1999) (Rindflesch et al., 2000) has recently become a topic of interest to the NLP community. This is a result of the need to formalise the huge number of research results that appear in free-te</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>S. Chen and J. Goodman. 1996. An empirical study of smoothing techniques for language modeling. 3.4st Annual Meeting of the Association of Computational Linguistics, California, USA, 24-27 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chinchor</author>
</authors>
<title>MUC-5 evaluation metrics. In</title>
<date>1995</date>
<booktitle>In Proceedings of the Fifth Message Understanding Conference (MUC-5),</booktitle>
<pages>69--78</pages>
<location>Baltimore, Maryland, USA.,</location>
<contexts>
<context position="15741" citStr="Chinchor, 1995" startWordPosition="2561" endWordPosition="2562">5(=631 / 4125) 0.13(=540 / 4125) RNA 0.43(= 30 / 70) 0.43(= 30 / 70) Source 0.16(=248 / 1533) 0.16(=242 / 1533) All 0.17(=1154/6868) 0.15(=1040/6868 We use &amp;quot;F-scores&amp;quot; for evaluation of our experiments (Van Rijsbergen, 1979). &amp;quot;F-score&amp;quot; is a measurement combining &amp;quot;Recall&amp;quot; and &amp;quot;Precision&amp;quot; and defined in Equation 3. &amp;quot;Recall&amp;quot; is the percentage of answers proposed by the system that correspond to those in the human-made key set. &amp;quot;Precision&amp;quot; is the percentage of correct answers among the answers proposed by the system. The F-scores presented here are automatically calculated using a scoring program (Chinchor, 1995). 2 x Precision x Recall F-score = Precision + Recall In Table 4 we show the actual performance of our term recognition systems, NE-DT and NEHMM. We can see that corpus comparisons based only on class-token ratios are inadequate to explain why both systems&apos; performance was about the same in both domains or why NEHMM did better in both test corpora than NE-DT. The difference in performance is despite there being more training examples in biology (3301 NEs) than in MUC-6 (2182 NEs). Part of the reason for this is (3) Table 4: Performance of the NE systems System MUC-6 Biology NEHMM with Unity 78</context>
</contexts>
<marker>Chinchor, 1995</marker>
<rawString>N. Chinchor. 1995. MUC-5 evaluation metrics. In In Proceedings of the Fifth Message Understanding Conference (MUC-5), Baltimore, Maryland, USA., pages 69-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Collier</author>
<author>C Nobata</author>
<author>J Tsujii</author>
</authors>
<title>Extracting the names of genes and gene products with a hidden Markov model.</title>
<date>2000</date>
<booktitle>In Proceedings of the .18th International Conference on Computational Linguistics (COLING&apos;2000),</booktitle>
<pages>4</pages>
<location>Saarbracken, Germany,</location>
<contexts>
<context position="4241" citStr="Collier et al., 2000" startWordPosition="684" endWordPosition="687">not hope to achieve performance limits. What we aim to do is to compare model performance against the predictions of corpus difficulty made by two different methods. In the rest of this paper we firstly introduce the NE models used for evaluation, the two corpora we 20 examined and then the difficulty comparison metrics. Predictive scores from the metrics are examined against the actual performance of the NE models. 2 Models Recent studies into the use of supervised learningbased models for the NE task in the molecularbiology domain have shown that models based on hidden Markov models (HMMs) (Collier et al., 2000) and decision trees (Nobata et al., 1999) are not only adaptable to this highly technical domain, but are also much more generalizable to new classes of words than systems based on traditional hand-built heuristic rules such as (Fukuda et al., 1998). We now describe two models used in our experiments based on the decision trees package C4.5 (Quinlan, 1993) and HMMs (Rabiner and Juang, 1986). 2.1 Decision tree named entity recogniser:NE-DT A decision tree is a type of classifier which has &amp;quot;leaf nodes&amp;quot; indicating classes and &amp;quot;decision nodes&amp;quot; that specify some test to be carried out, with one bra</context>
<context position="6500" citStr="Collier et al., 2000" startWordPosition="1058" endWordPosition="1061">raphic information is considered such as upper case, lower case, capitalization, numerical expressions, symbols. These character features are the same as those used by NEHMM described in the next section and shown in Table 1. Word lists specific to the domain: Word lists are made from the training corpus. Only the 200 highest frequency words are used. 2.2 Hidden Markov model named entity recog-niser: NEHMM HMMs are a widely used class of learning algorithms and can be considered to be stochastic finite state machines. In the following model, summarized here from the full description given in (Collier et al., 2000), we consider words to be ordered pairs consisting of a surface word, W, and a word feature, F, given as &lt; W,F &gt; . The word features themselves are discussed below. As is common practice, we need to calculate the probabilities for a word sequence for the first word&apos;s name class and every other word differently since we have no initial name-class to make a transition from. Accordingly we use the following equation to calculate the initial name class probability, Pr(NCil &lt; Wfirat, Ff ire; &gt;) = 0.0 f (NC firstj &lt; W first, Ffirst &gt;) cri f (NCfirst I &lt; Ff iret &gt;) 02 f (NCfirst) (1) and for all othe</context>
</contexts>
<marker>Collier, Nobata, Tsujii, 2000</marker>
<rawString>N. Collier, C. Nobata, and J. Tsujii. 2000. Extracting the names of genes and gene products with a hidden Markov model. In Proceedings of the .18th International Conference on Computational Linguistics (COLING&apos;2000), Saarbracken, Germany, July 31st-August 4th.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Craven</author>
<author>J Knrnlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In Proceedings of the 7th International Conference on Intelligent Systemps for Molecular Biology (ISMB-99),</booktitle>
<pages>6--10</pages>
<location>Heidelburg, Germany,</location>
<marker>Craven, Knrnlien, 1999</marker>
<rawString>M. Craven and J. Knrnlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In Proceedings of the 7th International Conference on Intelligent Systemps for Molecular Biology (ISMB-99), Heidelburg, Germany, August 6-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Fukuda</author>
<author>T Tsunoda</author>
<author>A Tamura</author>
<author>T Takagi</author>
</authors>
<title>Toward information extraction: identifying protein names from biological papers.</title>
<date>1998</date>
<booktitle>In Proceedings of the Pacific Symposium on Biocomputing&apos;98 (PSB &apos;98),</booktitle>
<contexts>
<context position="4490" citStr="Fukuda et al., 1998" startWordPosition="726" endWordPosition="729">e two corpora we 20 examined and then the difficulty comparison metrics. Predictive scores from the metrics are examined against the actual performance of the NE models. 2 Models Recent studies into the use of supervised learningbased models for the NE task in the molecularbiology domain have shown that models based on hidden Markov models (HMMs) (Collier et al., 2000) and decision trees (Nobata et al., 1999) are not only adaptable to this highly technical domain, but are also much more generalizable to new classes of words than systems based on traditional hand-built heuristic rules such as (Fukuda et al., 1998). We now describe two models used in our experiments based on the decision trees package C4.5 (Quinlan, 1993) and HMMs (Rabiner and Juang, 1986). 2.1 Decision tree named entity recogniser:NE-DT A decision tree is a type of classifier which has &amp;quot;leaf nodes&amp;quot; indicating classes and &amp;quot;decision nodes&amp;quot; that specify some test to be carried out, with one branch or subtree for each possible outcome of the test. A decision tree can be used to classify an object by starting at the root of the tree and moving through it until a leaf is encountered. When we can define suitable features for the decision tree</context>
</contexts>
<marker>Fukuda, Tsunoda, Tamura, Takagi, 1998</marker>
<rawString>K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi. 1998. Toward information extraction: identifying protein names from biological papers. In Proceedings of the Pacific Symposium on Biocomputing&apos;98 (PSB &apos;98), January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MEDLINE</author>
</authors>
<title>The PubMed database can be found at:.</title>
<date>1999</date>
<note>http://www.ncbi.nlm.nih.gov/PubMed/.</note>
<contexts>
<context position="2900" citStr="MEDLINE, 1999" startWordPosition="455" endWordPosition="456">(Borthwick et al., 1998) and decision trees (Seidne et al., 1998) have emerged that should be easier to adapt to new domains than the dictionary-based systems of the past. Much of this work has taken advantage of smoothing techniques to overcome problems associated with data sparseness (Chen and Goodman, 1996). The two corpora we use in our NE experiments represent the following domains: • Newswire: acquisition of names of people, organizations and monetary units etc., from the MUC-6 data set. • Molecular-biology: acquisition of proteins, DNAs, RNAs etc. from a subset of the MEDLINE database (MEDLINE, 1999). Information extraction in the molecular-biology domain (Selcimizu et al., 1998) (Craven and Kumlien, 1999) (Rindflesch et al., 2000) has recently become a topic of interest to the NLP community. This is a result of the need to formalise the huge number of research results that appear in free-text form in online collections of journal abstracts and papers such as MEDLINE for databases such as Swissprot (Bairoch and Apweiler, 1997) and also to search such collections for facts in an intelligent way. The purpose of our study is not to show a high level of absolute system performance. In fact si</context>
</contexts>
<marker>MEDLINE, 1999</marker>
<rawString>MEDLINE. 1999. The PubMed database can be found at:. http://www.ncbi.nlm.nih.gov/PubMed/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference(MUC-6),</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>Columbia, MD, USA,</location>
<marker>DARPA, 1995</marker>
<rawString>DARPA. 1995. Proceedings of the Sixth Message Understanding Conference(MUC-6), Columbia, MD, USA, November. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Nobata</author>
<author>N Collier</author>
<author>J Tsujii</author>
</authors>
<title>Automatic term identification and classification in biology texts.</title>
<date>1999</date>
<booktitle>In Proceedings of the Natural Language Pacific Rim Symposium (NLPRS&apos;2000),</booktitle>
<contexts>
<context position="4282" citStr="Nobata et al., 1999" startWordPosition="691" endWordPosition="694">at we aim to do is to compare model performance against the predictions of corpus difficulty made by two different methods. In the rest of this paper we firstly introduce the NE models used for evaluation, the two corpora we 20 examined and then the difficulty comparison metrics. Predictive scores from the metrics are examined against the actual performance of the NE models. 2 Models Recent studies into the use of supervised learningbased models for the NE task in the molecularbiology domain have shown that models based on hidden Markov models (HMMs) (Collier et al., 2000) and decision trees (Nobata et al., 1999) are not only adaptable to this highly technical domain, but are also much more generalizable to new classes of words than systems based on traditional hand-built heuristic rules such as (Fukuda et al., 1998). We now describe two models used in our experiments based on the decision trees package C4.5 (Quinlan, 1993) and HMMs (Rabiner and Juang, 1986). 2.1 Decision tree named entity recogniser:NE-DT A decision tree is a type of classifier which has &amp;quot;leaf nodes&amp;quot; indicating classes and &amp;quot;decision nodes&amp;quot; that specify some test to be carried out, with one branch or subtree for each possible outcome </context>
</contexts>
<marker>Nobata, Collier, Tsujii, 1999</marker>
<rawString>C. Nobata, N. Collier, and J. Tsujii. 1999. Automatic term identification and classification in biology texts. In Proceedings of the Natural Language Pacific Rim Symposium (NLPRS&apos;2000), November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Palmer</author>
<author>D Day</author>
</authors>
<title>A statistical profile of the named entity task.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP&apos;97),</booktitle>
<location>Washington D.C., USA., 31</location>
<contexts>
<context position="10858" citStr="Palmer and Day, 1997" startWordPosition="1817" endWordPosition="1820">he keywords human AND blood cell AND transcription factor yielding about 3650 abstracts. Of these 100 documents were NE tagged for our experiments using a human domain expert. An example of the annotated abstracts is shown in Figure 2. In contrast to MUC-6 each article is quite short and there are few pre-class clue words making the task much more like terminology identification and classification than pure name finding. 4 A first attempt at corpus comparison based on simple token frequency A simple and intuitive approach to NE task difficulty comparison used in some previous studies such as (Palmer and Day, 1997) who studied corpora in six different languages, compares class to term-token ratios on the assumption that rarer classes are more difficult to acquire. The relative frequency counts from these ratios also give an indirect measure of the granularity of a class, i.e. how wide it is. While this is appealing, we show that this approach does not necessarily give the best metric for comparison. Tables 2 and 3 show the ratio of the number of different words used in NEs to the total number of words in the NE class vocabulary. The number of different tokens is influenced by the corpus size and is not </context>
</contexts>
<marker>Palmer, Day, 1997</marker>
<rawString>D. Palmer and D. Day. 1997. A statistical profile of the named entity task. In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP&apos;97), Washington D.C., USA., 31 March -3 April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Introduction to Decision Trees.</title>
<date>1990</date>
<booktitle>Readings in Machine Learning.</booktitle>
<editor>In J.W. Shavlik and T.G. Dietterich, editors,</editor>
<publisher>Morgan Kaufmann Publishers, Inc.,</publisher>
<location>San Mateo, California.</location>
<contexts>
<context position="17721" citStr="Quinlan, 1990" startWordPosition="2915" endWordPosition="2916">ntropy for NE classes H(C) is defined by H(C)= — E p(c) log2 p(c) cEC where: n(c) p(c) = n(c): the number of words in class c N: the total number of words in text We can calculate the entropy for features in the same way. When a feature F is given, the conditional entropy for NE elnsses H(C1F) is defined by = - E E p(c, f) log2 p(c)f) cec feF where: n(c, f) P(c, f) n(c, f) &amp;if) = n(f) n(c, f): the number of words in class c with the feature value f n(f): the number of words with the feature value f Using these entropies, we can calculate information gain (Breiman et al., 1984) and gain ratio (Quinlan, 1990). Information gain for NE classes and a feature I(C; F) is given as follows: I(C;F) = H(C) — H(CIF) The information gain I(C; F) shows how the feature F is related with NE classes C. When F is completely independent of C, the value of /(C; F) becomes the minimum value 0. The maximum value of I(C; F) is equivalent to that of H(C), when the feature F gives sufficient information to recognize named entities. Information gain can also be calculated by: I(C; F) = H(C) H(F) H(C,F) We show the values of the above three entropies in Table 5,6, and 7. In these tables, F is replaced with single letters </context>
</contexts>
<marker>Quinlan, 1990</marker>
<rawString>J.R. Quinlan. 1990. Introduction to Decision Trees. In J.W. Shavlik and T.G. Dietterich, editors, Readings in Machine Learning. Morgan Kaufmann Publishers, Inc., San Mateo, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<date>1993</date>
<booktitle>c4.5 Programs for Machine Learning.</booktitle>
<publisher>Morgan Kaufmann Publishers, Inc.,</publisher>
<location>San Mateo, California.</location>
<contexts>
<context position="4599" citStr="Quinlan, 1993" startWordPosition="746" endWordPosition="747">mined against the actual performance of the NE models. 2 Models Recent studies into the use of supervised learningbased models for the NE task in the molecularbiology domain have shown that models based on hidden Markov models (HMMs) (Collier et al., 2000) and decision trees (Nobata et al., 1999) are not only adaptable to this highly technical domain, but are also much more generalizable to new classes of words than systems based on traditional hand-built heuristic rules such as (Fukuda et al., 1998). We now describe two models used in our experiments based on the decision trees package C4.5 (Quinlan, 1993) and HMMs (Rabiner and Juang, 1986). 2.1 Decision tree named entity recogniser:NE-DT A decision tree is a type of classifier which has &amp;quot;leaf nodes&amp;quot; indicating classes and &amp;quot;decision nodes&amp;quot; that specify some test to be carried out, with one branch or subtree for each possible outcome of the test. A decision tree can be used to classify an object by starting at the root of the tree and moving through it until a leaf is encountered. When we can define suitable features for the decision tree, the system can achieve good performance with only a small amount of training data. The system we used is ba</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J.R. Quinlan. 1993. c4.5 Programs for Machine Learning. Morgan Kaufmann Publishers, Inc., San Mateo, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rabiner</author>
<author>B Juang</author>
</authors>
<title>An introduction to hidden Markov models.</title>
<date>1986</date>
<journal>IEEE ASSP Magazine,</journal>
<pages>4--16</pages>
<contexts>
<context position="4634" citStr="Rabiner and Juang, 1986" startWordPosition="750" endWordPosition="753">performance of the NE models. 2 Models Recent studies into the use of supervised learningbased models for the NE task in the molecularbiology domain have shown that models based on hidden Markov models (HMMs) (Collier et al., 2000) and decision trees (Nobata et al., 1999) are not only adaptable to this highly technical domain, but are also much more generalizable to new classes of words than systems based on traditional hand-built heuristic rules such as (Fukuda et al., 1998). We now describe two models used in our experiments based on the decision trees package C4.5 (Quinlan, 1993) and HMMs (Rabiner and Juang, 1986). 2.1 Decision tree named entity recogniser:NE-DT A decision tree is a type of classifier which has &amp;quot;leaf nodes&amp;quot; indicating classes and &amp;quot;decision nodes&amp;quot; that specify some test to be carried out, with one branch or subtree for each possible outcome of the test. A decision tree can be used to classify an object by starting at the root of the tree and moving through it until a leaf is encountered. When we can define suitable features for the decision tree, the system can achieve good performance with only a small amount of training data. The system we used is based on one that was originally crea</context>
</contexts>
<marker>Rabiner, Juang, 1986</marker>
<rawString>L. Rabiner and B. Juang. 1986. An introduction to hidden Markov models. IEEE ASSP Magazine, pages 4-16, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="5843" citStr="Ratnaparkhi, 1996" startWordPosition="953" endWordPosition="954">ally created for Japanese documents (Sekine et al., 1998). It has two phases, one for creating the decision tree from training data and the other for generating the class-tagged text based on the decision tree. When generating decision trees, trigams of words were used. For this system, words are considered to be quadruple features. The following features are used to generate conditions in the decision tree: Part-of-speech information: There are 45 part-of-speech categories, whose definitions are based on Pennsylvania Treebank&apos;s categories. We use a tagger based on Adwait Ratnaparkhrs method (Ratnaparkhi, 1996). Character type information: Orthographic information is considered such as upper case, lower case, capitalization, numerical expressions, symbols. These character features are the same as those used by NEHMM described in the next section and shown in Table 1. Word lists specific to the domain: Word lists are made from the training corpus. Only the 200 highest frequency words are used. 2.2 Hidden Markov model named entity recog-niser: NEHMM HMMs are a widely used class of learning algorithms and can be considered to be stochastic finite state machines. In the following model, summarized here </context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Conference on Empirical Methods in Natural Language Processing, pages 133-142, University of Pennsylvania, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Rindflesch</author>
<author>L Tanabe</author>
<author>N Weinstein</author>
<author>L Hunter</author>
</authors>
<title>EDGAR.: Extraction of drugs, genes and relations from the biomedical literature.</title>
<date>2000</date>
<booktitle>In Pacific Symposium on Bio-informatics (PSB&apos;2000),</booktitle>
<location>Hawaii, USA,</location>
<contexts>
<context position="3034" citStr="Rindflesch et al., 2000" startWordPosition="472" endWordPosition="475"> than the dictionary-based systems of the past. Much of this work has taken advantage of smoothing techniques to overcome problems associated with data sparseness (Chen and Goodman, 1996). The two corpora we use in our NE experiments represent the following domains: • Newswire: acquisition of names of people, organizations and monetary units etc., from the MUC-6 data set. • Molecular-biology: acquisition of proteins, DNAs, RNAs etc. from a subset of the MEDLINE database (MEDLINE, 1999). Information extraction in the molecular-biology domain (Selcimizu et al., 1998) (Craven and Kumlien, 1999) (Rindflesch et al., 2000) has recently become a topic of interest to the NLP community. This is a result of the need to formalise the huge number of research results that appear in free-text form in online collections of journal abstracts and papers such as MEDLINE for databases such as Swissprot (Bairoch and Apweiler, 1997) and also to search such collections for facts in an intelligent way. The purpose of our study is not to show a high level of absolute system performance. In fact since we use only the MUC-6 executive succession data set of 60 articles and a new MEDLINE data set of 100 articles we cannot hope to ac</context>
</contexts>
<marker>Rindflesch, Tanabe, Weinstein, Hunter, 2000</marker>
<rawString>T. Rindflesch, L. Tanabe, N. Weinstein, and L.. Hunter. 2000. EDGAR.: Extraction of drugs, genes and relations from the biomedical literature. In Pacific Symposium on Bio-informatics (PSB&apos;2000), Hawaii, USA, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sekimizu</author>
<author>H Park</author>
<author>J Tsujii</author>
</authors>
<title>Identifying the interaction between genes and gene products based on frequently seen verbs in medline abstracts. In Genome Informatics.</title>
<date>1998</date>
<publisher>Universal Academy Press, Inc.</publisher>
<marker>Sekimizu, Park, Tsujii, 1998</marker>
<rawString>T. Sekimizu, H. Park, and J. Tsujii. 1998. Identifying the interaction between genes and gene products based on frequently seen verbs in medline abstracts. In Genome Informatics. Universal Academy Press, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
<author>Hiroyuki Shinnou</author>
</authors>
<title>A Decision Tree Method for Finding and Classifying Names in Japanese Texts.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<location>Montreal, Canada,</location>
<contexts>
<context position="5282" citStr="Sekine et al., 1998" startWordPosition="865" endWordPosition="868">tity recogniser:NE-DT A decision tree is a type of classifier which has &amp;quot;leaf nodes&amp;quot; indicating classes and &amp;quot;decision nodes&amp;quot; that specify some test to be carried out, with one branch or subtree for each possible outcome of the test. A decision tree can be used to classify an object by starting at the root of the tree and moving through it until a leaf is encountered. When we can define suitable features for the decision tree, the system can achieve good performance with only a small amount of training data. The system we used is based on one that was originally created for Japanese documents (Sekine et al., 1998). It has two phases, one for creating the decision tree from training data and the other for generating the class-tagged text based on the decision tree. When generating decision trees, trigams of words were used. For this system, words are considered to be quadruple features. The following features are used to generate conditions in the decision tree: Part-of-speech information: There are 45 part-of-speech categories, whose definitions are based on Pennsylvania Treebank&apos;s categories. We use a tagger based on Adwait Ratnaparkhrs method (Ratnaparkhi, 1996). Character type information: Orthograp</context>
</contexts>
<marker>Sekine, Grishman, Shinnou, 1998</marker>
<rawString>Satoshi Sekine, Ralph Grishman, and Hiroyuki Shinnou. 1998. A Decision Tree Method for Finding and Classifying Names in Japanese Texts. In Proceedings of the Sixth Workshop on Very Large Corpora, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<publisher>Butterworths,</publisher>
<location>London.</location>
<marker>Van Rijsbergen, 1979</marker>
<rawString>C. Van Rijsbergen. 1979. Information Retrieval. Butterworths, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<title>Error bounds for convolutions codes and an asymptotically optimum decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>13--2</pages>
<contexts>
<context position="8376" citStr="Viterbi, 1967" startWordPosition="1389" endWordPosition="1390">els designed to reduce the effects of data sparseness. 71 Table 1: Word features with examples Word Feature Example Feature Ex. TwoDigitNumber 25 OpenSquare [ FourDigitNumber 2000 CloseSquare [ DigitNumber 15012 Colon . SingleCap M SemiColon ; GreekLetter alpha Percent % CapsAndDigits 12 OpenParen ( TwoCaps RaIGDS CloseParen ) LettersAndDigits p52 Comma , InitCap Interleukin FullStop . LowCaps kappaB Determiner the Lowercase kinases Conjunction and Hyphon - Other *14 Backslash / Once the state transition probabilities have been calculated according to Equations 1 and 2, the Viterbi algorithm (Viterbi, 1967) is used to search the state space of possible name class assignments in linear time to find the highest probability path, i.e. to maximise Pr (W, NC). The final stage of our algorithm that is used after nameclass tagging is complete is to use a clean-up module called Unity. This creates a frequency list of words and name-classes and then re-tags the text using the most frequently used name class assigned by the HMM. We have generally found that this improves F-score performance by between 2 and 4%, both for re-tagging spuriously tagged words and for finding untagged words in unknown contexts </context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>A. J. Viterbi. 1967. Error bounds for convolutions codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory, IT-13(2):260-269.</rawString>
</citation>
<citation valid="true">
<date>2000</date>
<booktitle>The Eighth Text REtrieval Conference (TREC-8), Electronic version available at http://trec.nist.gov/pubs.html.</booktitle>
<editor>E.M. Voorhees and D.K. Harman, editors.</editor>
<marker>2000</marker>
<rawString>E.M. Voorhees and D.K. Harman, editors. 2000. The Eighth Text REtrieval Conference (TREC-8), Electronic version available at http://trec.nist.gov/pubs.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>