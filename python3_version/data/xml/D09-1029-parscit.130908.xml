<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000311">
<title confidence="0.967726">
Wikipedia as Frame Information Repository
</title>
<author confidence="0.970236">
Sara Tonelli Claudio Giuliano
</author>
<affiliation confidence="0.670815">
FBK-irst FBK-irst
</affiliation>
<address confidence="0.641681">
I-38100, Trento, Italy I-38100, Trento, Italy
</address>
<email confidence="0.983433">
satonelli@fbk.eu giuliano@fbk.eu
</email>
<sectionHeader confidence="0.993404" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999978684210526">
In this paper, we address the issue of au-
tomatic extending lexical resources by ex-
ploiting existing knowledge repositories.
In particular, we deal with the new task
of linking FrameNet and Wikipedia us-
ing a word sense disambiguation system
that, for a given pair frame – lexical unit
(F, l), finds the Wikipage that best ex-
presses the the meaning of l. The mapping
can be exploited to straightforwardly ac-
quire new example sentences and new lex-
ical units, both for English and for all lan-
guages available in Wikipedia. In this way,
it is possible to easily acquire good-quality
data as a starting point for the creation of
FrameNet in new languages. The evalua-
tion reported both for the monolingual and
the multilingual expansion of FrameNet
shows that the approach is promising.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968694915254">
Many applications in the context of natural lan-
guage processing or information retrieval have
proved to convey significant improvement by ex-
ploiting lexical databases with high-quality anno-
tation such as FrameNet (Fillmore et al., 2003)
and WordNet (Fellbaum, 1998). Nevertheless, the
practical use of similar resources is often biased
by their limited coverage because manual anno-
tation is time-consuming and requires a relevant
financial effort. For this reason, some research ac-
tivities have focused on the automatic enrichment
of such resources with annotated information in
(near) manual quality. The main strategy proposed
was the mapping between resources in order to
reciprocally enrich different lexical databases by
linking their information layers. This has proved
to be useful in several tasks, from verb classifica-
tion (Chow and Webster, 2007) to semantic role
labeling (Giuglea and Moschitti, 2006), open text
semantic parsing (Shi and Mihalcea, 2004) and
textual entailment (Burchardt and Frank, 2006).
In this work, we focus on the automatic enrich-
ment of the FrameNet database for English and we
propose a new framework to extend this procedure
to new languages. While similar works in the past
have mainly proposed to automatically extend the
FrameNet database by mapping frames and Word-
Net synsets (Shi and Mihalcea (2005), Johans-
son and Nugues (2007), and Tonelli and Pighin
(2009)), we present an explorative approach that
for the first time exploits Wikipedia to this pur-
pose. In particular, given a lexical unit l belong-
ing to a frame F, we devise a strategy to link
l to the Wikipedia article that best captures the
sense of l in F. This is basically a word disam-
biguation (WSD) problem (Erk, 2004) and to this
purpose we employ a state-of-the-art WSD sys-
tem (Gliozzo et al., 2005). The mapping between
(F, l) pairs and Wikipedia pages could then be ex-
ploited for three further subtasks: (a) automati-
cally extract from Wikipedia all sentences point-
ing to the Wikipage mapped with (F, l) and assign
them to F; (b) automatically expand the lexical
units sets in the English FrameNet by exploiting
the redirecting and linking strategy of Wikipedia;
and (c) since Wikipedia is available in 260 lan-
guages, use the English Wikipedia article linked to
(F, l) as a bridge to carry out sentence and lexical
unit retrieval in other languages. The set of auto-
matically collected data would represent the start-
ing point for the creation of FrameNet in new lan-
guages. In fact, having a repository of sentences
extracted from Wikipedia which have already been
divided by sense would significantly speed up the
annotation process. In this way, the annotators
would not need to extract all sentences in a cor-
pus containing l and classify them by sense. In-
stead, they should simply validate the given sen-
tences and assign the correct frame elements.
</bodyText>
<page confidence="0.973048">
276
</page>
<note confidence="0.9966035">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 276–285,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999933722222222">
In the following, we start by providing a brief
overview of FrameNet and Wikipedia and we
present their structure and organization. Next, we
describe the algorithm for mapping lexical units
and Wikipages and the word sense disambigua-
tion algorithm employed by the system. In Sec-
tion 5 we describe the dataset used in the first ex-
periment and report evaluation results of the map-
ping between (F, l) pairs and Wikipedia senses. In
Section 6 we describe an application of the map-
ping, i.e. the automatic enrichment of English
FrameNet. We describe the data extraction pro-
cess and evaluate the quality of the data. In Section
7 we describe and evaluate another application of
the mapping, i.e. the acquisition of data for the
automatic creation of Italian FrameNet using the
Italian Wikipedia. Finally, we draw conclusions
and present future research directions.
</bodyText>
<sectionHeader confidence="0.948063" genericHeader="introduction">
2 FrameNet and Wikipedia
</sectionHeader>
<bodyText confidence="0.999931833333333">
FrameNet (Fillmore et al., 2003) is a lexical re-
source for English based on corpus evidence,
whose conceptual model comprises a set of proto-
typical situations called frames, the frame-evoking
words or expressions called lexical units (LUs)
and the roles or participants involved in these situ-
ations, called frame elements. All lexical units be-
longing to the same frame have similar semantics
but, differently from WordNet synsets, they can
belong to different categories and present differ-
ent parts of speech. For example, the KILLING
frame is described in the FrameNet database1
as “A Killer or Cause causes the death of the
Victim”. The elements in capitals are the se-
mantic roles (frame elements) typically involved
in the KILLING situation. The frame definition
comes also with the list of frame-evoking lexical
units, namely annihilate.v, annihilation.n, butch-
ery.n, carnage.n, crucify.v, deadly.a, etc. Since
FrameNet is a corpus-based resource, every lexi-
cal unit should be instantiated by a set of exam-
ple sentences, where the frame elements are anno-
tated as well. Instead, FrameNet is still an ongoing
project and in the latest release (v. 1.3) there are
about 3,380 lexical units out of 10,195 that come
with no example sentences. In this work we focus
on these lexical units and propose how to automat-
ically collect the missing sentences. Anyhow, the
algorithm we propose is suitable also for expand-
ing sentence sets already present in FrameNet.
</bodyText>
<footnote confidence="0.909102">
1http://framenet.icsi.berkeley.edu
</footnote>
<bodyText confidence="0.999720805555556">
Wikipedia2 is one of the largest online reposito-
ries of encyclopedic knowledge, with millions of
articles available for a large number of languages
(&gt;2,800,000 for English). The article (or page)
is the basic entry in Wikipedia. Every article has
an unique reference, i.e., one or more words that
identify the page and are present in its URL. For
example, Ball (dance) identifies the page that de-
scribes several types of ball intended as formal
dance, while Dance (musical form) describes the
dance as musical genre. Every Wikipedia article
is linked to others, and in the body of every page
there are plenty of links to connect the most rel-
evant terms to other pages. Another important
attribute is the presence of about 3,000,000 redi-
rection pages, that given an identifier that is not
present in Wikipedia, automatically display the
page with the most semantically similar identi-
fier (for example Killing is redirected to the Mur-
der page). Wikipedia contains also more than
100,000 disambiguation pages listing all senses
(pages) for an ambiguous entity. For example,
Book has 9 senses, which correspond to 9 dif-
ferent articles. Wikipedia structure and quality
make this resource particularly suitable for infor-
mation extraction and word sense disambiguation
tasks (Csomai and Mihalcea (2008) and Milne and
Witten (2008)). In fact, page references can be
seen as senses and Wikipedia as a large sense in-
ventory. From this point of view, also linking
a lexical unit to the correct Wikipedia page is a
word sense disambiguation issue because it im-
plies recognizing what meaning the lexical unit
has in the given frame. For example, dance.n
in the SOCIAL EVENT frame should be linked to
Ball (dance) and not to Dance (musical form).
</bodyText>
<sectionHeader confidence="0.974773" genericHeader="method">
3 The Mapping Algorithm
</sectionHeader>
<bodyText confidence="0.995722833333333">
In this section, we describe how to map a frame
– lexical unit pair (F, l) into the Wikipedia arti-
cle that best captures the sense of l as defined in
F. The mapping problem is casted as a supervised
WSD problem, in which l must be disambiguated
using F to provide the context and Wikipedia to
provide the sense inventory and the training data.
Even if the idea of using Wikipedia links for dis-
ambiguation is not novel (Cucerzan, 2007), it is
applied for the first time to FrameNet lexical units,
considering a frame as a sense definition. The pro-
posed algorithm is summarized as follows:
</bodyText>
<footnote confidence="0.994876">
2http://en.wikipedia.org
</footnote>
<page confidence="0.997054">
277
</page>
<bodyText confidence="0.999989212121212">
Step 1 For each lexical unit l, we collect from
the English Wikipedia dump3 all contexts4 where l
is the anchor of an internal link (wiki link). The set
of targets represents the senses of l in Wikipedia
and the contexts are used as labelled training ex-
amples. For example, the lexical unit building.n in
the frame Buildings is an anchor in 708 different
contexts that point to 42 different Wikipedia pages
(senses).
Step 2 The set of contexts with their correspond-
ing senses is then used to train the WSD system
described in Section 4. For example, the context
“The building, which date from the mid-to-late
19th century, were built in a variety of High Victo-
rian architectural styles.” is a training example for
the sense defined by the Wikipedia page Building.
Step 3 Finally, the disambiguation model
learned in the previous step is used to map a pair
(F, l) to a Wikipedia article. (F, l) is represented
as a fictitious-context derived by aggregating the
frame definition and all lexical units associated to
F. We used the term “fictitious-context” to re-
mark the slight difference in structure compared
with the training contexts (i.e., the Wikipedia
paragraphs). For example, “... structures form-
ing an enclosure and providing protection from
the elements ... acropolis arena auditorium bar
building ... ” is the fictitious-context built for
the pair (Buildings, building.n). The sense, i.e.,
the Wikipedia article, assigned to the fictitious-
context by the disambiguation algorithm uniquely
defines the mapping. The previous example is as-
signed to the Wikipedia page Building.
</bodyText>
<sectionHeader confidence="0.981411" genericHeader="method">
4 The WSD Algorithm
</sectionHeader>
<bodyText confidence="0.999972181818182">
Gliozzo et al. (2005) proposed an elegant approach
to WSD based on kernel methods. The algorithm
proved effective at Senseval-3 (Mihalcea and Ed-
monds, 2004) and, nowadays, it still represents
the state-of-the-art in WSD (Pradhan et al., 2007).
Specifically, they addressed these issues: (i) inde-
pendently modeling domain and syntagmatic as-
pects of sense distinction to improve feature rep-
resentativeness; and (ii) exploiting external knowl-
edge acquired from unlabeled data, with the pur-
pose of drastically reducing the amount of labeled
</bodyText>
<footnote confidence="0.8834302">
3http://download.wikimedia.org/enwiki/
20090306
4A context corresponds to a line of text in the Wikipedia
dump and it is represented as a paragraph in a Wikipedia ar-
ticle.
</footnote>
<bodyText confidence="0.999945424242424">
training data. The first direction is based on the
linguistic assumption that syntagmatic and domain
(associative) relations are crucial for representing
sense distictions, but they are originated by differ-
ent phenomena. Regarding the second direction, it
is possible to obtain a more accurate prediction by
taking into account unlabeled data relevant for the
learning problem (Chapelle et al., 2006).
On the other hand, kernel methods are theoret-
ically well founded in statistical learning theory
and shown good empirical results in many appli-
cations (Shawe-Taylor and Cristianini, 2004). The
strategy adopted by kernel methods consists of
splitting the learning problem into two parts. They
first embed the input data in a suitable feature
space, and then use a linear algorithm (e.g., sup-
port vector machines) to discover nonlinear pat-
terns in the input space. The kernel function is
the only task-specific component of the learning
algorithm. Thus, to develop a WSD system, one
only needs to define appropriate kernel functions
to represent the domain and syntagmatic aspects
of sense distinction and to exploit the properties
of kernel functions in order to define a composite
kernel that combines and extends individual ker-
nels.
The WSD system described in the following
consists of a composite kernel (Section 4.3) that
combines the domain and syntagmatic kernels.
The former (Section 4.1) models the domain as-
pects of sense distinction, the latter (Section 4.2)
represents the syntagmatic aspects of sense dis-
tinction.
</bodyText>
<subsectionHeader confidence="0.995643">
4.1 Domain Kernel
</subsectionHeader>
<bodyText confidence="0.9993595">
It is been shown that domain information is fun-
damental for WSD (Magnini et al., 2002). For in-
stance, the (domain) polysemy between the com-
puter science and the medicine senses of the word
“virus” can be solved by considering the domain
of the context in which it appears.
In the context of kernel methods, domain infor-
mation can be exploited by defining a kernel func-
tion that estimates the domain similarity between
the contexts of the word to be disambiguated. The
simplest method to estimate the domain similarity
between two texts is to compute the cosine simi-
larity of their vector representations in the vector
space model (VSM). The VSM is a k-dimensional
space Rk, in which the text tj is represented by
a vector tj, where the ith component is the term
</bodyText>
<page confidence="0.984387">
278
</page>
<bodyText confidence="0.997252107142857">
frequency of the term wz in tj. However, such an
approach does not deal well with lexical variabil-
ity and ambiguity. For instance, despite the fact
that the sentences “he is affected by AIDS” and
“HIV is a virus” express concepts closely related,
their similarity is zero in the VSM because they
have no words in common (they are represented
by orthogonal vectors). On the other hand, due
to the ambiguity of the word “virus” , the simi-
larity between the sentences “the laptop has been
infected by a virus” and “HIV is a virus” is greater
than zero, even though they convey very different
messages.
To overcome this problem, Gliozzo et al. (2005)
introduced the domain model (DM) and show how
to define a domain VSM in which texts and terms
are represented in a uniform way. A DM is com-
posed of soft clusters of terms. Each cluster rep-
resents a semantic domain, that is, a set of terms
that often co-occur in texts having similar topics.
A DM is represented by a k x k&apos; rectangular ma-
trix D, containing the degree of association among
terms and domains.
The matrix D is used to define a function D :
Rk —* Rk0, that maps the vector t� represented in
�
the standard VSM, into the vector t&apos;j in the domain
VSM. D is defined by
</bodyText>
<equation confidence="0.999536">
D(~tj) = ~tj(IIDFD) = ~t0j, (1)
</equation>
<bodyText confidence="0.999961346153846">
where t� is represented as a row vector, IIDF is a
kxk diagonal matrix such that iIDF
z,z = IDF(wz),
and IDF(wz) is the inverse document frequency
of wz.
In the domain space, the similarity is esti-
mated by taking into account second order rela-
tions among terms. For example, the similarity of
the two sentences “He is affected by AIDS” and
“HIV is a virus” is very high, because the terms
AIDS, HIV and virus are strongly associated with
the domain medicine.
Singular valued decomposition (SVD) is used to
acquire in a unsupervised way the DM from a cor-
pus represented by its term-by-document matrix
T. SVD decomposes the term-by-document ma-
trix T into three matrixes T ^_ VEk0UT, where
V and U are orthogonal matrices (i.e., VTV = I
and UTU = I) whose columns are the eigenvec-
tors of TTT and TT T respectively, and Ek0 is
the diagonal k x k matrix containing the highest
k&apos; « k eigenvalues of T, and all the remaining
elements set to 0. The parameter k&apos; is the dimen-
sionality of the domain VSM and can be fixed in
advance. Under this setting, the domain matrix D
is defined by
</bodyText>
<equation confidence="0.787745">
1/
D = INV Ek (2)
where IN is a diagonal matrix such that iN;,; =
q i),
( �w0 i, �w0 �w&apos;z is the ith row of the matrix V&apos;\/Ek0.
The domain kernel is explicitly defined by
KD(ti, tj) = (D(ti), D(tj)), (3)
</equation>
<bodyText confidence="0.962710666666667">
where D is the domain mapping defined in Equa-
tion 1. Finally, the domain kernel is further ex-
tended to include the standard bag-of-word kernel.
</bodyText>
<subsectionHeader confidence="0.963082">
4.2 Syntagmatic Kernel
</subsectionHeader>
<bodyText confidence="0.999990941176471">
Kernel functions are not restricted to operate on
vectorial objects x� E Rk. In principle, kernels
can be defined for any kind of object representa-
tion, such as strings and trees. As syntagmatic re-
lations hold among words collocated in a partic-
ular temporal order, they can be modeled by ana-
lyzing sequences of words. Therefore, the string
kernel (Shawe-Taylor and Cristianini, 2004) is a
valid tool to represent such relations. It counts
how many times a (non-contiguous) subsequence
of symbols u of length n occurs in the input string
s, and penalizes non-contiguous occurrences ac-
cording to the number of gaps they contain. For-
mally, let V be the vocabulary, the feature space
associated with the string kernel of length n is in-
dexed by a set I of subsequences over V of length
n. The (explicit) mapping function is defined by
</bodyText>
<equation confidence="0.980222">
φnu(s) = X λl(i), u E V n, (4)
i:u=s(i)
</equation>
<bodyText confidence="0.9961424">
where u = s(i) is a subsequence of s in the posi-
tions given by the tuple i, l(i) is the length spanned
by u, and A E]0,1] is the decay factor used to pe-
nalize non-contiguous subsequences.
The associated string kernel is defined by
</bodyText>
<equation confidence="0.988478333333333">
Kn(si, sj) = (φn(si), φn(sj)) = X φn(si)φn(sj)
u∈V &amp;quot;
(5)
</equation>
<bodyText confidence="0.999583">
Gliozzo et al. (2005) modified the generic def-
inition of the string kernel in order to take into
account (sparse) collocations. Specifically, they
defined syntagmatic kernels as a combination of
string kernels applied to sequences of words in a
fixed-size window centered on the word to be dis-
ambiguated. This formulation allows estimating
the number of common (sparse) subsequences of
</bodyText>
<equation confidence="0.351039">
1
</equation>
<page confidence="0.988884">
279
</page>
<bodyText confidence="0.993279333333333">
words (i.e., collocations) between two examples,
in order to capture syntagmatic similarity. The
syntagmatic kernel is defined by
</bodyText>
<equation confidence="0.918058">
Kn(si, sj), (6)
</equation>
<bodyText confidence="0.9999505">
where Kn is the string kernel defined in Equation
5 and the parameter n represents the length of the
subsequences analyzed when estimating the sim-
ilarity between contexts. Notice that the syntag-
matic kernel is only effective for those fictitious
contexts in which the lexical units do occur in
meaningful sentences, however this is not guaran-
teed for the lexical units without examples.
</bodyText>
<subsectionHeader confidence="0.994046">
4.3 Composite Kernel
</subsectionHeader>
<bodyText confidence="0.9993885">
Finally, to combine domain and syntagmatic infor-
mation, the composite kernel is defined by
</bodyText>
<equation confidence="0.906815">
KWSD(ti, tj) = ˆKD(ti, tj) + ˆKS(ti, tj), (7)
</equation>
<bodyText confidence="0.9999716">
where ˆKD and ˆKS are normalized kernels defined
in Equation 3 and 6, respectively.5 It follows di-
rectly from the explicit construction of the feature
space and from closure properties of kernels that
it is a valid kernel.
</bodyText>
<sectionHeader confidence="0.970477" genericHeader="method">
5 Mapping task
</sectionHeader>
<bodyText confidence="0.9999056">
In this section we report the first experiment,
namely the mapping between (F, l) pairs and a
Wikipedia pages. We describe the experimental
setup and then present the corresponding evalua-
tion.
</bodyText>
<subsectionHeader confidence="0.946497">
5.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999978">
We applied our algorithm to all lexical units that
do not have any example sentence in the FrameNet
database. In principle, the proposed approach can
be applied to every lexical unit, and we expect the
algorithm performance to improve if some exam-
ple sentences are already available because they
could be added to the fictitious-context used to
represent (F, l) in the system. Nevertheless, in this
explorative study we wanted to focus on the harder
cases, even if results are likely to be worse than on
the whole FrameNet database.
In FrameNet, 3,305 (F, l) pairs have no exam-
ple sentences (536 pairs with adjectival LU, 1313
verbal LU, 1456 nominal LU). Since Wikipedia is
basically a resource organized by concepts, which
</bodyText>
<equation confidence="0.9700215">
5ˆK(xi, xj) = / K(xxj)
y K( „xj,xj)K(xi,xi)
</equation>
<bodyText confidence="0.999926590909091">
are generally expressed by nouns, we decided to
restrict our experiment to nominal lexical units.
Besides, many verbal and adjectival concepts in
Wikipedia are redirected to nominal identifiers.
So, we randomly selected 900 pairs with nominal
lexical units. For the moment, we decided to dis-
card lexical units expressed by multiwords (about
150), which will be taken into account in a future
version of our system. The average ambiguity of
the 900 LUs considered is 1.24 in FrameNet. In-
stead, every LU corresponds to about 35 candidate
senses in Wikipedia.
In order to perform WSD, we built the domain
model from the 200,000 most visited Wikipedia
articles. After removing terms that occur less than
5 times, the resulting dictionaries contain about
300,000 terms. We used the SVDLIBC pack-
age6 to compute the SVD, truncated to 100 di-
mensions. The experiments were performed using
the SVM package LIBSVM (Chang and Lin, 2001)
customized to embed the kernels described in Sec-
tion 4.
</bodyText>
<subsectionHeader confidence="0.981086">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999872923076923">
In this first evaluation step, we focus on the quality
of the mapping between (F, l) pairs and Wikipedia
articles. In order to evaluate the system output,
we created a gold standard where 250 (F, l) pairs
randomly extracted from the nominal subset de-
scribed above have been manually linked to the
Wikipedia page (if available) that best corresponds
to the meaning of l in F. The pairs have been cho-
sen in order to maximize the frame variability, i.e.
every pair corresponds to a different frame. Since
our gold standard contains 34% of all frames in
the FrameNet database, we believe that, despite its
limited size, it is well representative of FrameNet
characteristics. Evaluation was carried out com-
paring the system output against the gold stan-
dard. Results are reported in Table 1. The base-
line was computed considering the most frequent
sense of every lexical unit in Wikipedia. This ele-
ment is obtained by taking into account all occur-
rences in Wikipedia where the lexical unit LU we
consider is anchored to a given page. The most
frequent sense for LU is the page to which LU is
most frequently linked in Wikipedia. Since about
14% of the lexical units in the gold standard are
not present in Wikipedia, we also estimated an up-
per bound accuracy of 0.86. This confirms our in-
</bodyText>
<equation confidence="0.9204068">
6http://tedlab.mit.edu/˜dr/svdlibc/
p
X
n=1
KS(si, sj) =
</equation>
<page confidence="0.963794">
280
</page>
<bodyText confidence="0.995606">
tuition that FrameNet and Wikipedia are linkable
resources to a large extent and that our task is well-
founded.
</bodyText>
<table confidence="0.65927925">
Accuracy
Baseline 0.66
System output 0.71
Upper bound 0.86
</table>
<tableCaption confidence="0.998096">
Table 1: Accuracy evaluation.
</tableCaption>
<bodyText confidence="0.999960178571429">
Wrong assignments include also problematic
cases that are not directly connected to proper sys-
tem errors. One of the most relevant issues is the
different granularity between FrameNet frames
and Wikipages. For example, the NETWORK
frame is defined as “a set of entities of the same
or similar types (Nodes) are linked to each other
by Connections to form a Network allowing for
the flow of information, resources, etc.”. Even
if the listed lexical units (network.n and web.n)
and some examples refer to the informatics do-
main, the situation described in the FrameNet
database is more general. Wikipedia instead lists
several pages that may be seen as subdomains
of NETWORK such as Computer network, So-
cial network, Telecommunications network, etc.
In the future, it may be worth modifying the sys-
tem in order to allow multiple assignments of
Wikipages for every frame.
In other cases, frame definitions seem not to
be very consistent and it is very difficult to dis-
criminate between two frames even for a human
annotator. For example, ESTIMATED VALUE and
ESTIMATING include both estimation.n as lexical
unit, but since their frame definitions are almost
the same and the other lexical units in the same
frame are not discriminative, the system links both
(F, l) pairs to the same Wikipedia article.
</bodyText>
<sectionHeader confidence="0.991921" genericHeader="method">
6 English FrameNet expansion
</sectionHeader>
<bodyText confidence="0.999729345454546">
In the following part of the experiment, we want
to investigate to what extent the FrameNet –
Wikipedia mapping can be effectively applied to
automatically expand the FrameNet database with
new example sentences, and eventually to acquire
new lexical units. For every (F, l) pair, we con-
sider the linked Wikipedia sense s and extract all
sentences C3 in Wikipedia with a reference to
s. In this way, we can assume that, if s was
linked to (F, l), C3 can be included in the exam-
ple sentences of F. This repository of sentences
is already divided by sense and can significantly
speed-up manual annotation. On the other hand,
the extracted sentences could enrich the training
set of machine learning systems for frame annota-
tion to improve the frame identification step. In
fact, this task has raised growing interest in the
NLP community, with a devoted challenge at the
last SemEval campaign (Baker et al., 2007).
This retrieval process allows also to ex-
tract from C3 all words W3 that have an
embedded reference to s in the form &lt;a
href=“/wiki/Wiki Sense”...&gt;word&lt;/a&gt;. In this
way, W3 are automatically included in F as new
lexical units. In this phase, redirecting links are
very useful because they automatically connect a
word or expression to its nearest sense in case
there is no specific page for this word. The infor-
mation about redirecting allows also to account for
orthographic variations of the same lexical unit,
for example collectible is redirected to collectable.
We explain the data extraction process in
the light of an example from our dataset.
Our WSD system assigned to the (F, l) pair
(WORD RELATIONS – homonym.n) the Wikipage
http://en.wikipedia.org/wiki/Homonym.
So, we extracted from the English Wikipedia
dump all sentences where the anchor &lt;a
href=”/wiki/Homonym”... &gt; appears and as-
sumed that the word or multiword expression that
is linked to the Homonym site may be a good can-
didate as lexical unit for the WORD RELATIONS
frame. In this case, the example sentences were
186. Apart from homonym, the candidate lexical
units are homograph, homophone, homophonous,
homonymic, heteronym, same. Among them, only
the latter is not appropriate, even if the sentence
where it occurs is semantically connected to the
WORD RELATIONS frame: “In Hebrew the word
‘thus’ has the same triconsonantal root”. Instead,
homonymic and heteronym can be acquired as
new lexical units for WORD RELATIONS, and
homograph, for which no example sentences
are provided in FrameNet, can be automatically
instantiated by a set of examples.
</bodyText>
<subsectionHeader confidence="0.997354">
6.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.9998972">
We considered 893 frame – lexical unit pairs as-
signed to Wikipedia pages following the algorithm
described in Section 3. We discarded 7 pairs for
which the system reported an assignment failure,
i.e. the best sense delivered is the disambigua-
</bodyText>
<page confidence="0.99335">
281
</page>
<bodyText confidence="0.97958475">
tion page. Then we extracted a set of sentences
for every (F, l) pair as described in the previous
paragraph. Statistics about the retrieved data is re-
ported in Table 2.
</bodyText>
<figure confidence="0.432285333333333">
English Wikipedia
(F, l) pairs 893
N. of extracted sents 964,268
</figure>
<tableCaption confidence="0.805423">
Avg. sents per (F, l) 1,080
Table 2: Extracted data from English Wikipedia
</tableCaption>
<subsectionHeader confidence="0.99866">
6.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.99996593442623">
The dimension of the extracted corpus does not
allow to carry out a comprehensive evaluation.
For this reason, we manually evaluated 1,000 sen-
tences, i.e. we considered 20 (F, l) pairs, and for
each of them we evaluated 50 sentences extracted
from our large repository. Both (F, l) pairs and
the assigned sentences were randomly selected.
In particular, the 20 (F, l) pairs do not contain
only correctly assigned pairs, in fact three of them
are wrong. Anyhow, the 20 pairs seem to be
a representative subset of the 893 pairs consid-
ered in our experiment because they include both
monosemic lexical units (gynaecology.n in MED-
ICAL SPECIALTIES) and more ambiguous ones
(club.n in the WEAPON frame).
Our evaluation shows that 78% of the sentences
were correctly linked to (F, l) pairs. This value is
higher that the mapping accuracy between (F, l)
and Wikipages reported in Section 5.2. In fact,
we noticed that even if the Wikipage assigned to
(F, l) is not the article that best corresponds to the
meaning of l in F, some sentences pointing to it
may be appropriate to express l.
As we already mentioned in Section 5.2,
the different granularity of the information en-
coded by frames and Wikipages impacts on
the output quality. For example, conversion.n
in CAUSE CHANGE has a causative meaning,
while it implies a personal process in UN-
DERGO CHANGE. The mapping, instead, links
(CAUSE CHANGE – conversion.n) to the Reli-
gious conversion page, and all the sentences col-
lected point to religious conversion, regardless of
their causative form or not. Another characteristic
of this approach is that we can acquire new lexi-
cal units regardless of their part-of-speech, even if
we start from nominal lexical units. This proves
that we do not need to apply the initial mapping to
verbal or adjectival LUs to obtain new data for all
parts of speech. For example, we linked (MEDI-
CAL SPECIALTIES – gynaecology.n) to the Gynae-
cology Wikipage. Consequently, we could include
the adjective gynaecologic, pointing to the Gy-
naecology page, into the MEDICAL SPECIALTIES
frame for sentences like “Fellowship training in a
gynaecologic subspeciality can range from one to
four years”. However, this advantage can also turn
into a weakness, because gynaecologist is also
redirected to the Gynaecology page, but it belongs
to MEDICAL PROFESSIONALS and should not be
included into MEDICAL SPECIALTIES.
For the 20 (F, l) pairs considered in the given
sentences, it was possible also to retrieve 8 lex-
ical units that are not present in FrameNet, for
example billy-club for the WEAPON frame. Ex-
ploiting redirections and anchoring strategies, our
induction method can account for orthographical
variations, for example it acquires both memorize
and memorise. On the other hand, also misspelled
words may be collected, for instance gynaecolo-
gial instead of gynaecological.
</bodyText>
<sectionHeader confidence="0.960479" genericHeader="method">
7 Multilingual FrameNet expansion
</sectionHeader>
<bodyText confidence="0.999989615384616">
One of the great advantages of Wikipedia is its
availability in several languages. The English ver-
sion is by far the most extended, but a considerable
repository of pages is available also for other lan-
guages, esp. European ones. In general, articles on
the same object in different languages are edited
independently and do not have to be translations
of one another, but are linked to each other by their
authors. In this way, the multilingual versions of
Wikipedia can be easily exploited to build compa-
rable corpora, with connected Wikipages in differ-
ent languages dealing with the same contents.
In this research step, we focus on this aspect of
Wikipedia and propose a methodology that, using
the English Wikipages as a bridge, automatically
acquires new lexical units and example sentences
also for other languages. This would represent the
starting point towards the creation of FrameNet
for new languages. Indeed, FrameNet structure
comprises a language-independent level of infor-
mation, namely frame and frame element defini-
tions, and a language-dependent one, i.e. the lex-
ical units and the example sentences. This makes
the resource particularly suitable to corpus-based
(semi) automatic creation of FrameNet for new
languages, because the descriptive part can be pre-
</bodyText>
<page confidence="0.991501">
282
</page>
<bodyText confidence="0.999917666666667">
served and the language-dependent layer can be
populated with new instances in other languages
(Crespo and Buitelaar, 2008).
We apply our extraction algorithm to the Italian
Wikipedia. Since several approaches have been
experimented to (semi) automatically build Italian
FrameNet using WordNet (De Cao et al. (2008)
and Tonelli and Pighin (2009)), we believe that
our new proposal to exploit Wikipedia may be of
interest in the research community. Anyhow, the
approach can be exploited in principle for every
language available in Wikipedia.
</bodyText>
<subsectionHeader confidence="0.977714">
7.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999812457142857">
Similarly to the data extraction process described
in Section 6, we consider for every (F, l) pair in
English the linked Wikipedia sense s, in English
as well. Then, we retrieve the Italian Wikipedia
sense si linked to s and extract all sentences Ci
in the Italian Wikipedia dump7 with a reference to
si. In this way, we can assume that Ci are exam-
ple sentences of F and that the words or expres-
sions Wi in Ci containing an embedded reference
to si are good candidate lexical units of F in the
Italian FrameNet. For example, if we link http:
//en.wikipedia.org/wiki/Court to the JUDI-
CIAL BODY frame, we first retrieve the Italian
version of the site http://it.wikipedia.org/
wiki/Tribunale. Then, with a top-down strat-
egy, we further extract all Italian sentences point-
ing to the Tribunale page and acquire as lexi-
cal units all words with an embedded reference to
this concept, for example tribunale and corte. In
this way, we can include the extracted lexical units
and the sentences where they occur in the JUDI-
CIAL BODY frame for Italian.
Given the 893 (F, l) pairs in English and the
linked Wikipedia senses described in 6.2, we first
extracted the Italian Wikipages that are linked to
the English ones. Then for every linked Wikipage
in Italian, we retrieved all sentences with a refer-
ence pointing to that page in the Italian Wikipedia
dump. Statistics about the extracted data are re-
ported in Table 3.
Since the Italian Wikipedia is about one fifth of
the English one, it was not possible to map ev-
ery English Wikipage with an Italian article. In
fact, only 371 senses out of 893 in English were
linked to an Italian page. Also the average num-
</bodyText>
<footnote confidence="0.8935105">
7http://download.wikimedia.org/itwiki/
20090203
</footnote>
<table confidence="0.81196825">
Italian Wikipedia
Linked Wikipages in Italian 371
N. of extracted sents 23,078
Avg. sents per Italian sense 62
</table>
<tableCaption confidence="0.998866">
Table 3: Extracted data from Italian Wikipedia
</tableCaption>
<bodyText confidence="0.999970888888889">
ber of sentences extracted for every sense is much
smaller (62 vs. 1,080). Anyhow, this does not rep-
resent a problem because in the English FrameNet,
the lexical units whose annotation is considered
to be complete are usually instantiated by set of
20 annotated sentences on average. So, according
to the FrameNet standard, 60 sentences are more
than enough to represent the meaning of a lexical
unit in a frame.
</bodyText>
<subsectionHeader confidence="0.922922">
7.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999983185185185">
In this evaluation part, we took into account 1,000
sentences, in order to have a comparable dataset
w.r.t. the evaluation for English. However, the sets
of Italian sentences extracted for every (F, l), i.e.
for every Wikipedia article, were much smaller,
so we increased the number of randomly chosen
(F, l) pairs to 80. Our evaluation is focused on the
quality of the sentences and aims at assessing if the
given sentences are correctly assigned to the (F, l)
pairs. We report 69% accuracy, which is 9% lower
than for English. Apart from the same errors and
issues reported for English, a decrease in perfor-
mance can be explained by the fact that, since less
articles are present w.r.t. the English version, redi-
rections and internal links tend to be less precise
and fine-grained. For example, the word “diritti”
in the sense of “(human) rights” redirects to the ar-
ticle about Diritto, corresponding to Law as a sys-
tem of rules. On the contrary, Law and Rights have
two different pages in English. Besides, the differ-
ent quality of the two resources can also depend
on the smaller number of users that edit and check
the Italian articles. From the 1,000 sentences eval-
uated we extracted 145 new lexical units: since
Italian FrameNet does not exist yet, every lexical
unit in a sentence that is correct can be straightfor-
wardly included in the first version of the resource.
</bodyText>
<sectionHeader confidence="0.997146" genericHeader="conclusions">
8 Conclusions and Future work
</sectionHeader>
<bodyText confidence="0.99932125">
In this work, we have proposed to apply a
word sense disambiguation system to a new
task, namely the linking between FrameNet and
Wikipedia. Results are promising and show that
</bodyText>
<page confidence="0.993902">
283
</page>
<bodyText confidence="0.999941586206896">
the task is adequately substantiated. The proposed
approach can help enriching FrameNet with new
example sentences and lexical units and provide a
starting point for the creation of FrameNet-like re-
sources in all Wikipedia languages. On the one
hand, the retrieved data could speed up human
annotation, requiring only a manual validation.
On the other hand, the extracted sentences could
provide enough training data to machine learning
systems for frame assignment, since insufficient
frame attestations in the FrameNet database are a
major problem for such systems.
In the next research step, we plan to carry out an
extended evaluation process in order to compute
inter-annotator agreement and eventually point out
validation problems. Then, we want to extend
the mapping and the data extraction process to all
(F, l) pairs in FrameNet (about 10,000). The re-
trieved sentences will be made available as train-
ing or annotation material. Besides, we want
to create an online resource where the links be-
tween (F, l) pairs and Wikipages are made explicit
and where users can browse the retrieved sen-
tences. The resource can be produced and made
available with a reduced effort for every language
in Wikipedia. Anyway, the English version has
proved to be more precise, while the resource for
new languages would require a more accurate re-
vision.
</bodyText>
<sectionHeader confidence="0.998304" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.988388777777778">
Claudio Giuliano is supported by the ITCH
project (http://itch.fbk.eu), sponsored
by the Italian Ministry of University and Re-
search and by the Autonomous Province of
Trento and the X-Media project (http://www.
x-media-project.org), sponsored by the
European Commission as part of the Information
Society Technologies (IST) programme under EC
grant number IST-FP6-026978.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999828">
Collin F. Baker, Michael Ellsworth, and Katrin Erk.
2007. SemEval-2007 Task 10: Frame Semantic
Structure Extraction. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 99–104, Prague, CZ, June.
Aljoscha Burchardt and Anette Frank. 2006. Approxi-
mating Textual Entailment with LFG and FrameNet
Frames. In Proceedings of the 2nd PASCAL RTE
Workshop, pages 92–97, Venice, Italy.
Diego De Cao, Danilo Croce, Marco Pennacchiotti,
and Roberto Basili. 2008. Combining Word Sense
and Usage for modeling Frame Semantics. In Pro-
ceedings of STEP 2008, Venice, Italy.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/˜cjlin/libsvm.
Olivier Chapelle, Bernhard Sch¨olkopf, and Alexander
Zien. 2006. Semi-Supervised Learning. MIT Press,
Cambridge, MA.
Ian Chow and Jonathan Webster. 2007. Integra-
tion of Linguistic Resources for Verb Classification:
FrameNet Frame, WordNet Verb and Suggested Up-
per Merged Ontology. Computational Linguistics
and Intelligent Text Processing, pages 1–11.
Mario Crespo and Paul Buitelaar. 2008. Domain-
specific English-to-Spanish Translation of
FrameNet. In Proc. of LREC 2008, Marrakech.
Andras Csomai and Rada Mihalcea. 2008. Linking
Documents to Encyclopedic Knowledge. IEEE In-
telligent Systems, special issue on “Natural Lan-
guage Processing for the Web”.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 708–716, Prague, Czech Republic,
June. Association for Computational Linguistics.
Katrin Erk. 2004. Frame assignment as Word
Sense Disambiguation. In Proceedings of IWCS-6,
Tilburg, NL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
C.J. Fillmore, C.R. Johnson, and M. R. L. Petruck.
2003. Background to FrameNet. International
Journal of Lexicography, 16:235–250, September.
Ana-Maria Giuglea and Alessandro Moschitti. 2006.
Semantic role labeling via FrameNet, VerbNet and
PropBank. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual ACL meeting, pages 929–936, Morris-
town, US.
A. Gliozzo, C. Giuliano, and C. Strapparava. 2005.
Domain kernels for word sense disambiguation. In
Proceedings of the 43rd annual meeting of the As-
sociation for Computational Linguistics (ACL-05),
pages 403–410, Ann Arbor, Michigan, June.
R. Johansson and P. Nugues. 2007. Using Word-
Net to extend FrameNet coverage. In Proc. of the
Workshop on Building Frame-semantic Resources
for Scandinavian and Baltic Languages, at NODAL-
IDA, Tartu.
</reference>
<page confidence="0.976632">
284
</page>
<reference confidence="0.997658290322581">
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The Role of Domain Information
in Word Sense Disambiguation. Natural Language
Engineering, 8(4):359–373.
R. Mihalcea and P. Edmonds, editors. 2004. Proceed-
ings of SENSEVAL-3, Barcelona, Spain, July.
David Milne and Ian H. Witten. 2008. Learning to
link with Wikipedia. In CIKM ’08: Proceedings of
the 17th ACM conference on Information and knowl-
edge management, pages 509–518, NY, USA. ACM.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 Task-17: En-
glish Lexical Sample, SRL and All Words. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 87–
92, Prague, Czech Republic, June. Association for
Computational Linguistics.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge Univer-
sity Press.
Lei Shi and Rada Mihalcea. 2004. Open Text Seman-
tic Parsing Using FrameNet and WordNet. In Pro-
ceedings of HLT-NAACL 2004.
Lei Shi and Rada Mihalcea. 2005. Putting Pieces To-
gether: Combining FrameNet, VerbNet and Word-
Net for Robust Semantic Parsing. In Proceedings of
CICLing 2005, pages 100–111. Springer.
Sara Tonelli and Daniele Pighin. 2009. New features
for FrameNet - WordNet Mapping. In Proceedings
of the Thirteenth Conference on Computational Nat-
ural Language Learning, Boulder, CO, USA.
</reference>
<page confidence="0.998567">
285
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.639214">
<title confidence="0.985359">Wikipedia as Frame Information Repository</title>
<author confidence="0.999735">Sara Tonelli Claudio Giuliano</author>
<affiliation confidence="0.668872">FBK-irst FBK-irst</affiliation>
<address confidence="0.944051">I-38100, Trento, Italy I-38100, Trento, Italy</address>
<email confidence="0.957776">satonelli@fbk.eugiuliano@fbk.eu</email>
<abstract confidence="0.9991465">In this paper, we address the issue of automatic extending lexical resources by exploiting existing knowledge repositories. In particular, we deal with the new task of linking FrameNet and Wikipedia using a word sense disambiguation system that, for a given pair frame – lexical unit finds the Wikipage that best exthe the meaning of The mapping can be exploited to straightforwardly acquire new example sentences and new lexical units, both for English and for all languages available in Wikipedia. In this way, it is possible to easily acquire good-quality data as a starting point for the creation of FrameNet in new languages. The evaluation reported both for the monolingual and the multilingual expansion of FrameNet shows that the approach is promising.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Michael Ellsworth</author>
<author>Katrin Erk</author>
</authors>
<title>SemEval-2007 Task 10: Frame Semantic Structure Extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>99--104</pages>
<location>Prague, CZ,</location>
<contexts>
<context position="24307" citStr="Baker et al., 2007" startWordPosition="4059" endWordPosition="4062"> the linked Wikipedia sense s and extract all sentences C3 in Wikipedia with a reference to s. In this way, we can assume that, if s was linked to (F, l), C3 can be included in the example sentences of F. This repository of sentences is already divided by sense and can significantly speed-up manual annotation. On the other hand, the extracted sentences could enrich the training set of machine learning systems for frame annotation to improve the frame identification step. In fact, this task has raised growing interest in the NLP community, with a devoted challenge at the last SemEval campaign (Baker et al., 2007). This retrieval process allows also to extract from C3 all words W3 that have an embedded reference to s in the form &lt;a href=“/wiki/Wiki Sense”...&gt;word&lt;/a&gt;. In this way, W3 are automatically included in F as new lexical units. In this phase, redirecting links are very useful because they automatically connect a word or expression to its nearest sense in case there is no specific page for this word. The information about redirecting allows also to account for orthographic variations of the same lexical unit, for example collectible is redirected to collectable. We explain the data extraction p</context>
</contexts>
<marker>Baker, Ellsworth, Erk, 2007</marker>
<rawString>Collin F. Baker, Michael Ellsworth, and Katrin Erk. 2007. SemEval-2007 Task 10: Frame Semantic Structure Extraction. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 99–104, Prague, CZ, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Anette Frank</author>
</authors>
<title>Approximating Textual Entailment with LFG and FrameNet Frames.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2nd PASCAL RTE Workshop,</booktitle>
<pages>92--97</pages>
<location>Venice, Italy.</location>
<contexts>
<context position="1989" citStr="Burchardt and Frank, 2006" startWordPosition="301" endWordPosition="304"> time-consuming and requires a relevant financial effort. For this reason, some research activities have focused on the automatic enrichment of such resources with annotated information in (near) manual quality. The main strategy proposed was the mapping between resources in order to reciprocally enrich different lexical databases by linking their information layers. This has proved to be useful in several tasks, from verb classification (Chow and Webster, 2007) to semantic role labeling (Giuglea and Moschitti, 2006), open text semantic parsing (Shi and Mihalcea, 2004) and textual entailment (Burchardt and Frank, 2006). In this work, we focus on the automatic enrichment of the FrameNet database for English and we propose a new framework to extend this procedure to new languages. While similar works in the past have mainly proposed to automatically extend the FrameNet database by mapping frames and WordNet synsets (Shi and Mihalcea (2005), Johansson and Nugues (2007), and Tonelli and Pighin (2009)), we present an explorative approach that for the first time exploits Wikipedia to this purpose. In particular, given a lexical unit l belonging to a frame F, we devise a strategy to link l to the Wikipedia article</context>
</contexts>
<marker>Burchardt, Frank, 2006</marker>
<rawString>Aljoscha Burchardt and Anette Frank. 2006. Approximating Textual Entailment with LFG and FrameNet Frames. In Proceedings of the 2nd PASCAL RTE Workshop, pages 92–97, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diego De Cao</author>
<author>Danilo Croce</author>
<author>Marco Pennacchiotti</author>
<author>Roberto Basili</author>
</authors>
<title>Combining Word Sense and Usage for modeling Frame Semantics.</title>
<date>2008</date>
<booktitle>In Proceedings of STEP 2008,</booktitle>
<location>Venice, Italy.</location>
<marker>De Cao, Croce, Pennacchiotti, Basili, 2008</marker>
<rawString>Diego De Cao, Danilo Croce, Marco Pennacchiotti, and Roberto Basili. 2008. Combining Word Sense and Usage for modeling Frame Semantics. In Proceedings of STEP 2008, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.</title>
<date>2001</date>
<note>edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="20453" citStr="Chang and Lin, 2001" startWordPosition="3408" endWordPosition="3411">units expressed by multiwords (about 150), which will be taken into account in a future version of our system. The average ambiguity of the 900 LUs considered is 1.24 in FrameNet. Instead, every LU corresponds to about 35 candidate senses in Wikipedia. In order to perform WSD, we built the domain model from the 200,000 most visited Wikipedia articles. After removing terms that occur less than 5 times, the resulting dictionaries contain about 300,000 terms. We used the SVDLIBC package6 to compute the SVD, truncated to 100 dimensions. The experiments were performed using the SVM package LIBSVM (Chang and Lin, 2001) customized to embed the kernels described in Section 4. 5.2 Evaluation In this first evaluation step, we focus on the quality of the mapping between (F, l) pairs and Wikipedia articles. In order to evaluate the system output, we created a gold standard where 250 (F, l) pairs randomly extracted from the nominal subset described above have been manually linked to the Wikipedia page (if available) that best corresponds to the meaning of l in F. The pairs have been chosen in order to maximize the frame variability, i.e. every pair corresponds to a different frame. Since our gold standard contains</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu. edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Chapelle</author>
<author>Bernhard Sch¨olkopf</author>
<author>Alexander Zien</author>
</authors>
<title>Semi-Supervised Learning.</title>
<date>2006</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Chapelle, Sch¨olkopf, Zien, 2006</marker>
<rawString>Olivier Chapelle, Bernhard Sch¨olkopf, and Alexander Zien. 2006. Semi-Supervised Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Chow</author>
<author>Jonathan Webster</author>
</authors>
<title>Integration of Linguistic Resources for Verb Classification: FrameNet Frame, WordNet Verb and Suggested Upper Merged Ontology. Computational Linguistics and Intelligent Text Processing,</title>
<date>2007</date>
<pages>1--11</pages>
<contexts>
<context position="1829" citStr="Chow and Webster, 2007" startWordPosition="278" endWordPosition="281">03) and WordNet (Fellbaum, 1998). Nevertheless, the practical use of similar resources is often biased by their limited coverage because manual annotation is time-consuming and requires a relevant financial effort. For this reason, some research activities have focused on the automatic enrichment of such resources with annotated information in (near) manual quality. The main strategy proposed was the mapping between resources in order to reciprocally enrich different lexical databases by linking their information layers. This has proved to be useful in several tasks, from verb classification (Chow and Webster, 2007) to semantic role labeling (Giuglea and Moschitti, 2006), open text semantic parsing (Shi and Mihalcea, 2004) and textual entailment (Burchardt and Frank, 2006). In this work, we focus on the automatic enrichment of the FrameNet database for English and we propose a new framework to extend this procedure to new languages. While similar works in the past have mainly proposed to automatically extend the FrameNet database by mapping frames and WordNet synsets (Shi and Mihalcea (2005), Johansson and Nugues (2007), and Tonelli and Pighin (2009)), we present an explorative approach that for the firs</context>
</contexts>
<marker>Chow, Webster, 2007</marker>
<rawString>Ian Chow and Jonathan Webster. 2007. Integration of Linguistic Resources for Verb Classification: FrameNet Frame, WordNet Verb and Suggested Upper Merged Ontology. Computational Linguistics and Intelligent Text Processing, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Crespo</author>
<author>Paul Buitelaar</author>
</authors>
<title>Domainspecific English-to-Spanish Translation of FrameNet.</title>
<date>2008</date>
<booktitle>In Proc. of LREC 2008,</booktitle>
<location>Marrakech.</location>
<contexts>
<context position="30817" citStr="Crespo and Buitelaar, 2008" startWordPosition="5112" endWordPosition="5115">example sentences also for other languages. This would represent the starting point towards the creation of FrameNet for new languages. Indeed, FrameNet structure comprises a language-independent level of information, namely frame and frame element definitions, and a language-dependent one, i.e. the lexical units and the example sentences. This makes the resource particularly suitable to corpus-based (semi) automatic creation of FrameNet for new languages, because the descriptive part can be pre282 served and the language-dependent layer can be populated with new instances in other languages (Crespo and Buitelaar, 2008). We apply our extraction algorithm to the Italian Wikipedia. Since several approaches have been experimented to (semi) automatically build Italian FrameNet using WordNet (De Cao et al. (2008) and Tonelli and Pighin (2009)), we believe that our new proposal to exploit Wikipedia may be of interest in the research community. Anyhow, the approach can be exploited in principle for every language available in Wikipedia. 7.1 Experimental setup Similarly to the data extraction process described in Section 6, we consider for every (F, l) pair in English the linked Wikipedia sense s, in English as well</context>
</contexts>
<marker>Crespo, Buitelaar, 2008</marker>
<rawString>Mario Crespo and Paul Buitelaar. 2008. Domainspecific English-to-Spanish Translation of FrameNet. In Proc. of LREC 2008, Marrakech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andras Csomai</author>
<author>Rada Mihalcea</author>
</authors>
<title>Linking Documents to Encyclopedic Knowledge.</title>
<date>2008</date>
<journal>IEEE Intelligent Systems, special issue on “Natural Language Processing for the Web”.</journal>
<contexts>
<context position="7657" citStr="Csomai and Mihalcea (2008)" startWordPosition="1230" endWordPosition="1233">Another important attribute is the presence of about 3,000,000 redirection pages, that given an identifier that is not present in Wikipedia, automatically display the page with the most semantically similar identifier (for example Killing is redirected to the Murder page). Wikipedia contains also more than 100,000 disambiguation pages listing all senses (pages) for an ambiguous entity. For example, Book has 9 senses, which correspond to 9 different articles. Wikipedia structure and quality make this resource particularly suitable for information extraction and word sense disambiguation tasks (Csomai and Mihalcea (2008) and Milne and Witten (2008)). In fact, page references can be seen as senses and Wikipedia as a large sense inventory. From this point of view, also linking a lexical unit to the correct Wikipedia page is a word sense disambiguation issue because it implies recognizing what meaning the lexical unit has in the given frame. For example, dance.n in the SOCIAL EVENT frame should be linked to Ball (dance) and not to Dance (musical form). 3 The Mapping Algorithm In this section, we describe how to map a frame – lexical unit pair (F, l) into the Wikipedia article that best captures the sense of l as</context>
</contexts>
<marker>Csomai, Mihalcea, 2008</marker>
<rawString>Andras Csomai and Rada Mihalcea. 2008. Linking Documents to Encyclopedic Knowledge. IEEE Intelligent Systems, special issue on “Natural Language Processing for the Web”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on Wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>708--716</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="8553" citStr="Cucerzan, 2007" startWordPosition="1395" endWordPosition="1396">ng the lexical unit has in the given frame. For example, dance.n in the SOCIAL EVENT frame should be linked to Ball (dance) and not to Dance (musical form). 3 The Mapping Algorithm In this section, we describe how to map a frame – lexical unit pair (F, l) into the Wikipedia article that best captures the sense of l as defined in F. The mapping problem is casted as a supervised WSD problem, in which l must be disambiguated using F to provide the context and Wikipedia to provide the sense inventory and the training data. Even if the idea of using Wikipedia links for disambiguation is not novel (Cucerzan, 2007), it is applied for the first time to FrameNet lexical units, considering a frame as a sense definition. The proposed algorithm is summarized as follows: 2http://en.wikipedia.org 277 Step 1 For each lexical unit l, we collect from the English Wikipedia dump3 all contexts4 where l is the anchor of an internal link (wiki link). The set of targets represents the senses of l in Wikipedia and the contexts are used as labelled training examples. For example, the lexical unit building.n in the frame Buildings is an anchor in 708 different contexts that point to 42 different Wikipedia pages (senses). </context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 708–716, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Frame assignment as Word Sense Disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of IWCS-6,</booktitle>
<location>Tilburg, NL.</location>
<contexts>
<context position="2695" citStr="Erk, 2004" startWordPosition="428" endWordPosition="429"> propose a new framework to extend this procedure to new languages. While similar works in the past have mainly proposed to automatically extend the FrameNet database by mapping frames and WordNet synsets (Shi and Mihalcea (2005), Johansson and Nugues (2007), and Tonelli and Pighin (2009)), we present an explorative approach that for the first time exploits Wikipedia to this purpose. In particular, given a lexical unit l belonging to a frame F, we devise a strategy to link l to the Wikipedia article that best captures the sense of l in F. This is basically a word disambiguation (WSD) problem (Erk, 2004) and to this purpose we employ a state-of-the-art WSD system (Gliozzo et al., 2005). The mapping between (F, l) pairs and Wikipedia pages could then be exploited for three further subtasks: (a) automatically extract from Wikipedia all sentences pointing to the Wikipage mapped with (F, l) and assign them to F; (b) automatically expand the lexical units sets in the English FrameNet by exploiting the redirecting and linking strategy of Wikipedia; and (c) since Wikipedia is available in 260 languages, use the English Wikipedia article linked to (F, l) as a bridge to carry out sentence and lexical </context>
</contexts>
<marker>Erk, 2004</marker>
<rawString>Katrin Erk. 2004. Frame assignment as Word Sense Disambiguation. In Proceedings of IWCS-6, Tilburg, NL.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
<author>C R Johnson</author>
<author>M R L Petruck</author>
</authors>
<title>Background to FrameNet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<pages>16--235</pages>
<contexts>
<context position="1209" citStr="Fillmore et al., 2003" startWordPosition="186" endWordPosition="189">e new example sentences and new lexical units, both for English and for all languages available in Wikipedia. In this way, it is possible to easily acquire good-quality data as a starting point for the creation of FrameNet in new languages. The evaluation reported both for the monolingual and the multilingual expansion of FrameNet shows that the approach is promising. 1 Introduction Many applications in the context of natural language processing or information retrieval have proved to convey significant improvement by exploiting lexical databases with high-quality annotation such as FrameNet (Fillmore et al., 2003) and WordNet (Fellbaum, 1998). Nevertheless, the practical use of similar resources is often biased by their limited coverage because manual annotation is time-consuming and requires a relevant financial effort. For this reason, some research activities have focused on the automatic enrichment of such resources with annotated information in (near) manual quality. The main strategy proposed was the mapping between resources in order to reciprocally enrich different lexical databases by linking their information layers. This has proved to be useful in several tasks, from verb classification (Cho</context>
<context position="4902" citStr="Fillmore et al., 2003" startWordPosition="792" endWordPosition="795">be the dataset used in the first experiment and report evaluation results of the mapping between (F, l) pairs and Wikipedia senses. In Section 6 we describe an application of the mapping, i.e. the automatic enrichment of English FrameNet. We describe the data extraction process and evaluate the quality of the data. In Section 7 we describe and evaluate another application of the mapping, i.e. the acquisition of data for the automatic creation of Italian FrameNet using the Italian Wikipedia. Finally, we draw conclusions and present future research directions. 2 FrameNet and Wikipedia FrameNet (Fillmore et al., 2003) is a lexical resource for English based on corpus evidence, whose conceptual model comprises a set of prototypical situations called frames, the frame-evoking words or expressions called lexical units (LUs) and the roles or participants involved in these situations, called frame elements. All lexical units belonging to the same frame have similar semantics but, differently from WordNet synsets, they can belong to different categories and present different parts of speech. For example, the KILLING frame is described in the FrameNet database1 as “A Killer or Cause causes the death of the Victim</context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>C.J. Fillmore, C.R. Johnson, and M. R. L. Petruck. 2003. Background to FrameNet. International Journal of Lexicography, 16:235–250, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Giuglea</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Semantic role labeling via FrameNet, VerbNet and PropBank.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual ACL meeting,</booktitle>
<pages>929--936</pages>
<location>Morristown, US.</location>
<contexts>
<context position="1885" citStr="Giuglea and Moschitti, 2006" startWordPosition="286" endWordPosition="289"> practical use of similar resources is often biased by their limited coverage because manual annotation is time-consuming and requires a relevant financial effort. For this reason, some research activities have focused on the automatic enrichment of such resources with annotated information in (near) manual quality. The main strategy proposed was the mapping between resources in order to reciprocally enrich different lexical databases by linking their information layers. This has proved to be useful in several tasks, from verb classification (Chow and Webster, 2007) to semantic role labeling (Giuglea and Moschitti, 2006), open text semantic parsing (Shi and Mihalcea, 2004) and textual entailment (Burchardt and Frank, 2006). In this work, we focus on the automatic enrichment of the FrameNet database for English and we propose a new framework to extend this procedure to new languages. While similar works in the past have mainly proposed to automatically extend the FrameNet database by mapping frames and WordNet synsets (Shi and Mihalcea (2005), Johansson and Nugues (2007), and Tonelli and Pighin (2009)), we present an explorative approach that for the first time exploits Wikipedia to this purpose. In particular</context>
</contexts>
<marker>Giuglea, Moschitti, 2006</marker>
<rawString>Ana-Maria Giuglea and Alessandro Moschitti. 2006. Semantic role labeling via FrameNet, VerbNet and PropBank. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual ACL meeting, pages 929–936, Morristown, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gliozzo</author>
<author>C Giuliano</author>
<author>C Strapparava</author>
</authors>
<title>Domain kernels for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd annual meeting of the Association for Computational Linguistics (ACL-05),</booktitle>
<pages>403--410</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="2778" citStr="Gliozzo et al., 2005" startWordPosition="441" endWordPosition="444">e similar works in the past have mainly proposed to automatically extend the FrameNet database by mapping frames and WordNet synsets (Shi and Mihalcea (2005), Johansson and Nugues (2007), and Tonelli and Pighin (2009)), we present an explorative approach that for the first time exploits Wikipedia to this purpose. In particular, given a lexical unit l belonging to a frame F, we devise a strategy to link l to the Wikipedia article that best captures the sense of l in F. This is basically a word disambiguation (WSD) problem (Erk, 2004) and to this purpose we employ a state-of-the-art WSD system (Gliozzo et al., 2005). The mapping between (F, l) pairs and Wikipedia pages could then be exploited for three further subtasks: (a) automatically extract from Wikipedia all sentences pointing to the Wikipage mapped with (F, l) and assign them to F; (b) automatically expand the lexical units sets in the English FrameNet by exploiting the redirecting and linking strategy of Wikipedia; and (c) since Wikipedia is available in 260 languages, use the English Wikipedia article linked to (F, l) as a bridge to carry out sentence and lexical unit retrieval in other languages. The set of automatically collected data would re</context>
<context position="10360" citStr="Gliozzo et al. (2005)" startWordPosition="1688" endWordPosition="1691">ciated to F. We used the term “fictitious-context” to remark the slight difference in structure compared with the training contexts (i.e., the Wikipedia paragraphs). For example, “... structures forming an enclosure and providing protection from the elements ... acropolis arena auditorium bar building ... ” is the fictitious-context built for the pair (Buildings, building.n). The sense, i.e., the Wikipedia article, assigned to the fictitiouscontext by the disambiguation algorithm uniquely defines the mapping. The previous example is assigned to the Wikipedia page Building. 4 The WSD Algorithm Gliozzo et al. (2005) proposed an elegant approach to WSD based on kernel methods. The algorithm proved effective at Senseval-3 (Mihalcea and Edmonds, 2004) and, nowadays, it still represents the state-of-the-art in WSD (Pradhan et al., 2007). Specifically, they addressed these issues: (i) independently modeling domain and syntagmatic aspects of sense distinction to improve feature representativeness; and (ii) exploiting external knowledge acquired from unlabeled data, with the purpose of drastically reducing the amount of labeled 3http://download.wikimedia.org/enwiki/ 20090306 4A context corresponds to a line of </context>
<context position="14005" citStr="Gliozzo et al. (2005)" startWordPosition="2279" endWordPosition="2282">in tj. However, such an approach does not deal well with lexical variability and ambiguity. For instance, despite the fact that the sentences “he is affected by AIDS” and “HIV is a virus” express concepts closely related, their similarity is zero in the VSM because they have no words in common (they are represented by orthogonal vectors). On the other hand, due to the ambiguity of the word “virus” , the similarity between the sentences “the laptop has been infected by a virus” and “HIV is a virus” is greater than zero, even though they convey very different messages. To overcome this problem, Gliozzo et al. (2005) introduced the domain model (DM) and show how to define a domain VSM in which texts and terms are represented in a uniform way. A DM is composed of soft clusters of terms. Each cluster represents a semantic domain, that is, a set of terms that often co-occur in texts having similar topics. A DM is represented by a k x k&apos; rectangular matrix D, containing the degree of association among terms and domains. The matrix D is used to define a function D : Rk —* Rk0, that maps the vector t� represented in � the standard VSM, into the vector t&apos;j in the domain VSM. D is defined by D(~tj) = ~tj(IIDFD) =</context>
<context position="17244" citStr="Gliozzo et al. (2005)" startWordPosition="2884" endWordPosition="2887">contiguous occurrences according to the number of gaps they contain. Formally, let V be the vocabulary, the feature space associated with the string kernel of length n is indexed by a set I of subsequences over V of length n. The (explicit) mapping function is defined by φnu(s) = X λl(i), u E V n, (4) i:u=s(i) where u = s(i) is a subsequence of s in the positions given by the tuple i, l(i) is the length spanned by u, and A E]0,1] is the decay factor used to penalize non-contiguous subsequences. The associated string kernel is defined by Kn(si, sj) = (φn(si), φn(sj)) = X φn(si)φn(sj) u∈V &amp;quot; (5) Gliozzo et al. (2005) modified the generic definition of the string kernel in order to take into account (sparse) collocations. Specifically, they defined syntagmatic kernels as a combination of string kernels applied to sequences of words in a fixed-size window centered on the word to be disambiguated. This formulation allows estimating the number of common (sparse) subsequences of 1 279 words (i.e., collocations) between two examples, in order to capture syntagmatic similarity. The syntagmatic kernel is defined by Kn(si, sj), (6) where Kn is the string kernel defined in Equation 5 and the parameter n represents </context>
</contexts>
<marker>Gliozzo, Giuliano, Strapparava, 2005</marker>
<rawString>A. Gliozzo, C. Giuliano, and C. Strapparava. 2005. Domain kernels for word sense disambiguation. In Proceedings of the 43rd annual meeting of the Association for Computational Linguistics (ACL-05), pages 403–410, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Using WordNet to extend FrameNet coverage.</title>
<date>2007</date>
<booktitle>In Proc. of the Workshop on Building Frame-semantic Resources for Scandinavian and Baltic Languages, at NODALIDA,</booktitle>
<location>Tartu.</location>
<contexts>
<context position="2343" citStr="Johansson and Nugues (2007)" startWordPosition="360" endWordPosition="364">ormation layers. This has proved to be useful in several tasks, from verb classification (Chow and Webster, 2007) to semantic role labeling (Giuglea and Moschitti, 2006), open text semantic parsing (Shi and Mihalcea, 2004) and textual entailment (Burchardt and Frank, 2006). In this work, we focus on the automatic enrichment of the FrameNet database for English and we propose a new framework to extend this procedure to new languages. While similar works in the past have mainly proposed to automatically extend the FrameNet database by mapping frames and WordNet synsets (Shi and Mihalcea (2005), Johansson and Nugues (2007), and Tonelli and Pighin (2009)), we present an explorative approach that for the first time exploits Wikipedia to this purpose. In particular, given a lexical unit l belonging to a frame F, we devise a strategy to link l to the Wikipedia article that best captures the sense of l in F. This is basically a word disambiguation (WSD) problem (Erk, 2004) and to this purpose we employ a state-of-the-art WSD system (Gliozzo et al., 2005). The mapping between (F, l) pairs and Wikipedia pages could then be exploited for three further subtasks: (a) automatically extract from Wikipedia all sentences poi</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>R. Johansson and P. Nugues. 2007. Using WordNet to extend FrameNet coverage. In Proc. of the Workshop on Building Frame-semantic Resources for Scandinavian and Baltic Languages, at NODALIDA, Tartu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>C Strapparava</author>
<author>G Pezzulo</author>
<author>A Gliozzo</author>
</authors>
<date>2002</date>
<booktitle>The Role of Domain Information in Word Sense Disambiguation. Natural Language Engineering,</booktitle>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="12679" citStr="Magnini et al., 2002" startWordPosition="2046" endWordPosition="2049">iate kernel functions to represent the domain and syntagmatic aspects of sense distinction and to exploit the properties of kernel functions in order to define a composite kernel that combines and extends individual kernels. The WSD system described in the following consists of a composite kernel (Section 4.3) that combines the domain and syntagmatic kernels. The former (Section 4.1) models the domain aspects of sense distinction, the latter (Section 4.2) represents the syntagmatic aspects of sense distinction. 4.1 Domain Kernel It is been shown that domain information is fundamental for WSD (Magnini et al., 2002). For instance, the (domain) polysemy between the computer science and the medicine senses of the word “virus” can be solved by considering the domain of the context in which it appears. In the context of kernel methods, domain information can be exploited by defining a kernel function that estimates the domain similarity between the contexts of the word to be disambiguated. The simplest method to estimate the domain similarity between two texts is to compute the cosine similarity of their vector representations in the vector space model (VSM). The VSM is a k-dimensional space Rk, in which the</context>
</contexts>
<marker>Magnini, Strapparava, Pezzulo, Gliozzo, 2002</marker>
<rawString>B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo. 2002. The Role of Domain Information in Word Sense Disambiguation. Natural Language Engineering, 8(4):359–373.</rawString>
</citation>
<citation valid="true">
<date>2004</date>
<booktitle>Proceedings of SENSEVAL-3,</booktitle>
<editor>R. Mihalcea and P. Edmonds, editors.</editor>
<location>Barcelona, Spain,</location>
<marker>2004</marker>
<rawString>R. Mihalcea and P. Edmonds, editors. 2004. Proceedings of SENSEVAL-3, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>Learning to link with Wikipedia. In</title>
<date>2008</date>
<booktitle>CIKM ’08: Proceedings of the 17th ACM conference on Information and knowledge management,</booktitle>
<pages>509--518</pages>
<publisher>ACM.</publisher>
<location>NY, USA.</location>
<contexts>
<context position="7685" citStr="Milne and Witten (2008)" startWordPosition="1235" endWordPosition="1238">the presence of about 3,000,000 redirection pages, that given an identifier that is not present in Wikipedia, automatically display the page with the most semantically similar identifier (for example Killing is redirected to the Murder page). Wikipedia contains also more than 100,000 disambiguation pages listing all senses (pages) for an ambiguous entity. For example, Book has 9 senses, which correspond to 9 different articles. Wikipedia structure and quality make this resource particularly suitable for information extraction and word sense disambiguation tasks (Csomai and Mihalcea (2008) and Milne and Witten (2008)). In fact, page references can be seen as senses and Wikipedia as a large sense inventory. From this point of view, also linking a lexical unit to the correct Wikipedia page is a word sense disambiguation issue because it implies recognizing what meaning the lexical unit has in the given frame. For example, dance.n in the SOCIAL EVENT frame should be linked to Ball (dance) and not to Dance (musical form). 3 The Mapping Algorithm In this section, we describe how to map a frame – lexical unit pair (F, l) into the Wikipedia article that best captures the sense of l as defined in F. The mapping p</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne and Ian H. Witten. 2008. Learning to link with Wikipedia. In CIKM ’08: Proceedings of the 17th ACM conference on Information and knowledge management, pages 509–518, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Edward Loper</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Semeval-2007 Task-17: English Lexical Sample, SRL and All Words.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>87--92</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="10581" citStr="Pradhan et al., 2007" startWordPosition="1722" endWordPosition="1725">roviding protection from the elements ... acropolis arena auditorium bar building ... ” is the fictitious-context built for the pair (Buildings, building.n). The sense, i.e., the Wikipedia article, assigned to the fictitiouscontext by the disambiguation algorithm uniquely defines the mapping. The previous example is assigned to the Wikipedia page Building. 4 The WSD Algorithm Gliozzo et al. (2005) proposed an elegant approach to WSD based on kernel methods. The algorithm proved effective at Senseval-3 (Mihalcea and Edmonds, 2004) and, nowadays, it still represents the state-of-the-art in WSD (Pradhan et al., 2007). Specifically, they addressed these issues: (i) independently modeling domain and syntagmatic aspects of sense distinction to improve feature representativeness; and (ii) exploiting external knowledge acquired from unlabeled data, with the purpose of drastically reducing the amount of labeled 3http://download.wikimedia.org/enwiki/ 20090306 4A context corresponds to a line of text in the Wikipedia dump and it is represented as a paragraph in a Wikipedia article. training data. The first direction is based on the linguistic assumption that syntagmatic and domain (associative) relations are cruc</context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>Sameer Pradhan, Edward Loper, Dmitriy Dligach, and Martha Palmer. 2007. Semeval-2007 Task-17: English Lexical Sample, SRL and All Words. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 87– 92, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="11636" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="1878" endWordPosition="1881">esented as a paragraph in a Wikipedia article. training data. The first direction is based on the linguistic assumption that syntagmatic and domain (associative) relations are crucial for representing sense distictions, but they are originated by different phenomena. Regarding the second direction, it is possible to obtain a more accurate prediction by taking into account unlabeled data relevant for the learning problem (Chapelle et al., 2006). On the other hand, kernel methods are theoretically well founded in statistical learning theory and shown good empirical results in many applications (Shawe-Taylor and Cristianini, 2004). The strategy adopted by kernel methods consists of splitting the learning problem into two parts. They first embed the input data in a suitable feature space, and then use a linear algorithm (e.g., support vector machines) to discover nonlinear patterns in the input space. The kernel function is the only task-specific component of the learning algorithm. Thus, to develop a WSD system, one only needs to define appropriate kernel functions to represent the domain and syntagmatic aspects of sense distinction and to exploit the properties of kernel functions in order to define a composite kernel</context>
<context position="16448" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="2734" endWordPosition="2737">V&apos;\/Ek0. The domain kernel is explicitly defined by KD(ti, tj) = (D(ti), D(tj)), (3) where D is the domain mapping defined in Equation 1. Finally, the domain kernel is further extended to include the standard bag-of-word kernel. 4.2 Syntagmatic Kernel Kernel functions are not restricted to operate on vectorial objects x� E Rk. In principle, kernels can be defined for any kind of object representation, such as strings and trees. As syntagmatic relations hold among words collocated in a particular temporal order, they can be modeled by analyzing sequences of words. Therefore, the string kernel (Shawe-Taylor and Cristianini, 2004) is a valid tool to represent such relations. It counts how many times a (non-contiguous) subsequence of symbols u of length n occurs in the input string s, and penalizes non-contiguous occurrences according to the number of gaps they contain. Formally, let V be the vocabulary, the feature space associated with the string kernel of length n is indexed by a set I of subsequences over V of length n. The (explicit) mapping function is defined by φnu(s) = X λl(i), u E V n, (4) i:u=s(i) where u = s(i) is a subsequence of s in the positions given by the tuple i, l(i) is the length spanned by u, and </context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>J. Shawe-Taylor and N. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Shi</author>
<author>Rada Mihalcea</author>
</authors>
<title>Open Text Semantic Parsing Using FrameNet and WordNet.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<contexts>
<context position="1938" citStr="Shi and Mihalcea, 2004" startWordPosition="294" endWordPosition="297">ir limited coverage because manual annotation is time-consuming and requires a relevant financial effort. For this reason, some research activities have focused on the automatic enrichment of such resources with annotated information in (near) manual quality. The main strategy proposed was the mapping between resources in order to reciprocally enrich different lexical databases by linking their information layers. This has proved to be useful in several tasks, from verb classification (Chow and Webster, 2007) to semantic role labeling (Giuglea and Moschitti, 2006), open text semantic parsing (Shi and Mihalcea, 2004) and textual entailment (Burchardt and Frank, 2006). In this work, we focus on the automatic enrichment of the FrameNet database for English and we propose a new framework to extend this procedure to new languages. While similar works in the past have mainly proposed to automatically extend the FrameNet database by mapping frames and WordNet synsets (Shi and Mihalcea (2005), Johansson and Nugues (2007), and Tonelli and Pighin (2009)), we present an explorative approach that for the first time exploits Wikipedia to this purpose. In particular, given a lexical unit l belonging to a frame F, we d</context>
</contexts>
<marker>Shi, Mihalcea, 2004</marker>
<rawString>Lei Shi and Rada Mihalcea. 2004. Open Text Semantic Parsing Using FrameNet and WordNet. In Proceedings of HLT-NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Shi</author>
<author>Rada Mihalcea</author>
</authors>
<title>Putting Pieces Together: Combining FrameNet, VerbNet and WordNet for Robust Semantic Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of CICLing</booktitle>
<pages>100--111</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2314" citStr="Shi and Mihalcea (2005)" startWordPosition="356" endWordPosition="359">ases by linking their information layers. This has proved to be useful in several tasks, from verb classification (Chow and Webster, 2007) to semantic role labeling (Giuglea and Moschitti, 2006), open text semantic parsing (Shi and Mihalcea, 2004) and textual entailment (Burchardt and Frank, 2006). In this work, we focus on the automatic enrichment of the FrameNet database for English and we propose a new framework to extend this procedure to new languages. While similar works in the past have mainly proposed to automatically extend the FrameNet database by mapping frames and WordNet synsets (Shi and Mihalcea (2005), Johansson and Nugues (2007), and Tonelli and Pighin (2009)), we present an explorative approach that for the first time exploits Wikipedia to this purpose. In particular, given a lexical unit l belonging to a frame F, we devise a strategy to link l to the Wikipedia article that best captures the sense of l in F. This is basically a word disambiguation (WSD) problem (Erk, 2004) and to this purpose we employ a state-of-the-art WSD system (Gliozzo et al., 2005). The mapping between (F, l) pairs and Wikipedia pages could then be exploited for three further subtasks: (a) automatically extract fro</context>
</contexts>
<marker>Shi, Mihalcea, 2005</marker>
<rawString>Lei Shi and Rada Mihalcea. 2005. Putting Pieces Together: Combining FrameNet, VerbNet and WordNet for Robust Semantic Parsing. In Proceedings of CICLing 2005, pages 100–111. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Tonelli</author>
<author>Daniele Pighin</author>
</authors>
<title>New features for FrameNet - WordNet Mapping.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning,</booktitle>
<location>Boulder, CO, USA.</location>
<contexts>
<context position="2374" citStr="Tonelli and Pighin (2009)" startWordPosition="366" endWordPosition="369">to be useful in several tasks, from verb classification (Chow and Webster, 2007) to semantic role labeling (Giuglea and Moschitti, 2006), open text semantic parsing (Shi and Mihalcea, 2004) and textual entailment (Burchardt and Frank, 2006). In this work, we focus on the automatic enrichment of the FrameNet database for English and we propose a new framework to extend this procedure to new languages. While similar works in the past have mainly proposed to automatically extend the FrameNet database by mapping frames and WordNet synsets (Shi and Mihalcea (2005), Johansson and Nugues (2007), and Tonelli and Pighin (2009)), we present an explorative approach that for the first time exploits Wikipedia to this purpose. In particular, given a lexical unit l belonging to a frame F, we devise a strategy to link l to the Wikipedia article that best captures the sense of l in F. This is basically a word disambiguation (WSD) problem (Erk, 2004) and to this purpose we employ a state-of-the-art WSD system (Gliozzo et al., 2005). The mapping between (F, l) pairs and Wikipedia pages could then be exploited for three further subtasks: (a) automatically extract from Wikipedia all sentences pointing to the Wikipage mapped wi</context>
<context position="31039" citStr="Tonelli and Pighin (2009)" startWordPosition="5145" endWordPosition="5148"> frame and frame element definitions, and a language-dependent one, i.e. the lexical units and the example sentences. This makes the resource particularly suitable to corpus-based (semi) automatic creation of FrameNet for new languages, because the descriptive part can be pre282 served and the language-dependent layer can be populated with new instances in other languages (Crespo and Buitelaar, 2008). We apply our extraction algorithm to the Italian Wikipedia. Since several approaches have been experimented to (semi) automatically build Italian FrameNet using WordNet (De Cao et al. (2008) and Tonelli and Pighin (2009)), we believe that our new proposal to exploit Wikipedia may be of interest in the research community. Anyhow, the approach can be exploited in principle for every language available in Wikipedia. 7.1 Experimental setup Similarly to the data extraction process described in Section 6, we consider for every (F, l) pair in English the linked Wikipedia sense s, in English as well. Then, we retrieve the Italian Wikipedia sense si linked to s and extract all sentences Ci in the Italian Wikipedia dump7 with a reference to si. In this way, we can assume that Ci are example sentences of F and that the </context>
</contexts>
<marker>Tonelli, Pighin, 2009</marker>
<rawString>Sara Tonelli and Daniele Pighin. 2009. New features for FrameNet - WordNet Mapping. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning, Boulder, CO, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>