<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019825">
<title confidence="0.901742">
That’s What She Said: Double Entendre Identification
</title>
<author confidence="0.952488">
Chloe Kiddon and Yuriy Brun
</author>
<affiliation confidence="0.980921">
Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.852437">
Seattle WA 98195-2350
</address>
<email confidence="0.999363">
{chloe,brun}@cs.washington.edu
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997982615384615">
Humor identification is a hard natural lan-
guage understanding problem. We identify
a subproblem — the “that’s what she said”
problem — with two distinguishing character-
istics: (1) use of nouns that are euphemisms
for sexually explicit nouns and (2) structure
common in the erotic domain. We address
this problem in a classification approach that
includes features that model those two char-
acteristics. Experiments on web data demon-
strate that our approach improves precision by
12% over baseline techniques that use only
word-based features.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997495">
“That’s what she said” is a well-known family of
jokes, recently repopularized by the television show
“The Office” (Daniels et al., 2005). The jokes con-
sist of saying “that’s what she said” after someone
else utters a statement in a non-sexual context that
could also have been used in a sexual context. For
example, if Aaron refers to his late-evening basket-
ball practice, saying “I was trying all night, but I just
could not get it in!”, Betty could utter “that’s what
she said”, completing the joke. While somewhat ju-
venile, this joke presents an interesting natural lan-
guage understanding problem.
A “that’s what she said” (TWSS) joke is a type of
double entendre. A double entendre, or adianoeta,
is an expression that can be understood in two differ-
ent ways: an innocuous, straightforward way, given
the context, and a risqu´e way that indirectly alludes
to a different, indecent context. To our knowledge,
</bodyText>
<page confidence="0.802366">
89
</page>
<bodyText confidence="0.985796294736842">
related research has not studied the task of identify-
ing double entendres in text or speech. The task is
complex and would require both deep semantic and
cultural understanding to recognize the vast array of
double entendres. We focus on a subtask of double
entendre identification: TWSS recognition. We say
a sentence is a TWSS if it is funny to follow that
sentence with “that’s what she said”.
We frame the problem of TWSS recognition as
a type of metaphor identification. A metaphor is
a figure of speech that creates an analogical map-
ping between two conceptual domains so that the
terminology of one (source) domain can be used to
describe situations and objects in the other (target)
domain. Usage of the source domain’s terminol-
ogy in the source domain is literal and is nonliteral
in the target domain. Metaphor identification sys-
tems seek to differentiate between literal and nonlit-
eral expressions. Some computational approaches to
metaphor identification learn selectional preferences
of words in multiple domains to help identify nonlit-
eral usage (Mason, 2004; Shutova, 2010). Other ap-
proaches train support vector machine (SVM) mod-
els on labeled training data to distinguish metaphoric
language from literal language (Pasanek and Scul-
ley, 2008).
TWSSs also represent mappings between two do-
mains: the innocuous source domain and an erotic
target domain. Therefore, we can apply methods
from metaphor identification to TWSS identifica-
tion. In particular, we (1) compare the adjectival
selectional preferences of sexually explicit nouns to
those of other nouns to determine which nouns may
be euphemisms for sexually explicit nouns and (2)
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
examine the relationship between structures in the classification of erotic and nonerotic contexts to fu-
erotic domain and nonerotic contexts. We present ture work.
a novel approach — Double Entendre via Noun There are two interesting and important aspects
Transfer (DEviaNT) — that applies metaphor iden- of the TWSS problem that make solving it difficult.
tification techniques to solving the double entendre First, many domains in which a TWSS classifier
problem and evaluate it on the TWSS problem. DE- could be applied value high precision significantly
viaNT classifies individual sentences as either funny more than high recall. For example, in a social set-
if followed by “that’s what she said” or not, which ting, the cost of saying “that’s what she said” inap-
is a type of automatic humor recognition (Mihal- propriately is high, whereas the cost of not saying
cea and Strapparava, 2005; Mihalcea and Pulman, it when it might have been appropriate is negligible.
2007). For another example, in automated public tagging of
We argue that in the TWSS domain, high preci- twitter and facebook data, false positives are consid-
sion is important, while low recall may be tolerated. ered spam and violate usage policies, whereas false
In experiments on nearly 21K sentences, we find negatives go unnoticed. Second, the overwhelm-
that DEviaNT has 12% higher precision than that of ing majority of everyday sentences are not TWSSs,
baseline classifiers that use n-gram TWSS models. making achieving high precision even more difficult.
The rest of this paper is structured as follows: In this paper, we strive specifically to achieve high
Section 2 will outline the characteristics of the precision but are willing to sacrifice recall.
TWSS problem that we leverage in our approach. 3 The DEviaNT Approach
Section 3 will describe the DEviaNT approach. Sec- The TWSS problem has two identifying character-
tion 4 will evaluate DEviaNT on the TWSS problem. istics: (1) TWSSs are likely to contain nouns that
Finally, Section 5 will summarize our contributions. are euphemisms for sexually explicit nouns and (2)
2 The TWSS Problem TWSSs share common structure with sentences in
We observe two facts about the TWSS problem. the erotic domain. Our approach to solving the
First, sentences with nouns that are euphemisms for TWSS problem is centered around an SVM model
sexually explicit nouns are more likely to be TWSSs. that uses features designed to model those charac-
For example, containing the noun “banana” makes teristics. We call our approach Double Entendre via
a sentence more likely to be a TWSS than contain- Noun Transfer, or the DEviaNT approach.
ing the noun “door”. Second, TWSSs share com- We will use features that build on corpus statistics
mon structure with sentences in the erotic domain. computed for known erotic words, and their lexical
For example, a sentence of the form “[subject] stuck contexts, as described in the rest of this section.
[object] in” or “[subject] could eat [object] all day” 3.1 Data and word classes
is more likely to be a TWSS than not. Thus, we Let SN be an open set of sexually explicit nouns. We
hypothesize that machine learning with euphemism- manually approximated SN with a set of 76 nouns
and structure-based features is a promising approach that are predominantly used in sexual contexts. We
to solving the TWSS problem. Accordingly, apart clustered the nouns into 9 categories based on which
from a few basic features that define a TWSS joke sexual object, body part, or participant they identify.
(e.g., short sentence), all of our approach’s lexical Let SN− C SN be the set of sexually explicit nouns
features model a metaphorical mapping to objects that are likely targets for euphemism. We did not
and structures in the erotic domain. consider euphemisms for people since they rarely, if
Part of TWSS identification is recognizing that ever, are used in TWSS jokes. In our approximation,
the source context in which the potential TWSS is ��SN−I = 61. Let BP be an open set of body-part
uttered is not in an erotic one. If it is, then the map- nouns. Our approximation contains 98 body parts.
ping to the erotic domain is the identity and the state- DEviaNT uses two corpora. The erotica corpus
ment is not a TWSS. In this paper, we assume all test consists of 1.5M sentences from the erotica section
instances are from nonerotic domains and leave the
90
of textfiles.com/sex/EROTICA. We removed
headers, footers, URLs, and unparseable text. The
Brown corpus (Francis and Kucera, 1979) is 57K
sentences that represent standard (nonerotic) litera-
ture. We tagged the erotica corpus with the Stanford
Parser (Toutanova and Manning, 2000; Toutanova
et al., 2003); the Brown corpus is already tagged.
To make the corpora more generic, we replaced all
numbers with the CD tag, all proper nouns with the
NNP tag, all nouns E SN with an SN tag, and all
nouns E� BP with the NN tag. We ignored determin-
ers and punctuation.
</bodyText>
<subsectionHeader confidence="0.997038">
3.2 Word- and phrase-level analysis
</subsectionHeader>
<bodyText confidence="0.76582155">
We define three functions to measure how closely
related a noun, an adjective, and a verb phrase are to
the erotica domain.
1. The noun sexiness function NS(n) is a real-
valued measure of the maximum similarity a noun
n E� SN has to each of the nouns E SN−. For each
noun, let the adjective count vector be the vector of
the absolute frequencies of each adjective that mod-
ifies the noun in the union of the erotica and the
Brown corpora. We define NS(n) to be the maxi-
mum cosine similarity, over each noun E SN−, using
term frequency-inverse document frequency (tf-idf)
weights of the nouns’ adjective count vectors. For
nouns that occurred fewer that 200 times, occurred
fewer than 50 times with adjectives, or were asso-
ciated with 3 times as many adjectives that never
occurred with nouns in SN than adjectives that did,
NS(n) = 10−7 (smaller than all recorded similari-
ties). Example nouns with high NS are “rod” and
“meat”.
</bodyText>
<listItem confidence="0.762036357142857">
2. The adjective sexiness function AS(a) is a
real-valued measure of how likely an adjective a is
to modify a noun E SN. We define AS(a) to be the
relative frequency of a in sentences in the erotica
corpus that contain at least one noun E SN. Exam-
ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-
valued measure of how much more likely a verb
phrase v is to appear in an erotic context than a
nonerotic one. Let SE be the set of sentences in the
erotica corpus that contain nouns E SN. Let SB be
the set of all sentences in the Brown corpus. Given
a sentence s containing a verb v, the verb phrase v
is the contiguous substring of the sentence that con-
</listItem>
<bodyText confidence="0.9996782">
tains v and is bordered on each side by the closest
noun or one of the set of pronouns {I, you, it, me}.
(If neither a noun nor none of the pronouns occur on
a side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilities
of v appearing in an erotic and a nonerotic context
with counts in SE and SB, respectively. We normal-
ize the counts in SB such that P(s E SE) = P(s E SB).
Let VS(v) be the probability that (v E s) ==&gt;. (s is
in an erotic context). Then,
</bodyText>
<equation confidence="0.998452">
VS(v) = P(s E SE|v E s)
P(v E s|s E SE)P(s E SE)
P(v E s)
</equation>
<bodyText confidence="0.999667">
Intuitively, the verb sexiness is a measure of how
likely the action described in a sentence could be an
action (via some metaphoric mapping) to an action
in an erotic context.
</bodyText>
<subsectionHeader confidence="0.919149">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.986711">
DEviaNT uses the following features to identify po-
tential mappings of a sentence s into the erotic do-
main, organized into two categories: NOUN EU-
PHEMISMS and STRUCTURAL ELEMENTS.
</bodyText>
<listItem confidence="0.995031761904762">
NOUN EUPHEMISMS:
• (boolean) does s contain a noun E SN?,
• (boolean) does s contain a noun E BP?,
• (boolean) does s contain a noun n such that
NS(n) = 10−7,
• (real) average NS(n), for all nouns n E s such
that n E� SN U BP,
STRUCTURAL ELEMENTS:
• (boolean) does s contain a verb that never oc-
curs in SE?,
• (boolean) does s contain a verb phrase that
never occurs in SE?,
• (real) average VS(v) over all verb phrases v E s,
• (real) average AS(a) over all adjectives a E s,
• (boolean) does s contain an adjective a such
that a never occurs in a sentence s E SE U SB
with a noun E SN.
DEviaNT also uses the following features to iden-
tify the BASIC STRUCTURE of a TWSS:
• (int) number of non-punctuation tokens,
• (int) number of punctuation tokens,
</listItem>
<page confidence="0.805112">
91
</page>
<listItem confidence="0.962167571428571">
• ({0, 1, 2+}) for each pronoun and each part-of-
speech tag, number of times it occurs in s,
• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,
“she”, “it”), other pronoun}) the subject of s.
(We approximate the subject with the first noun
or pronoun.)
</listItem>
<subsectionHeader confidence="0.996987">
3.4 Learning algorithm
</subsectionHeader>
<bodyText confidence="0.999910846153846">
DEviaNT uses an SVM classifier from the WEKA
machine learning package (Hall et al., 2009) with
the features from Section 3.3. In our prototype im-
plementation, DEviaNT uses the default parameter
settings and has the option to fit logistic regression
curves to the outputs to allow for precision-recall
analysis. To minimize false positives, while toler-
ating false negatives, DEviaNT employs the Meta-
Cost metaclassifier (Domingos, 1999), which uses
bagging to reclassify the training data to produce
a single cost-sensitive classifier. DEviaNT sets the
cost of a false positive to be 100 times that of a false
negative.
</bodyText>
<sectionHeader confidence="0.998742" genericHeader="introduction">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9999776">
The goal of our evaluation is somewhat unusual.
DEviaNT explores a particular approach to solving
the TWSS problem: recognizing euphemistic and
structural relationships between the source domain
and an erotic domain. As such, DEviaNT is at a dis-
advantage to many potential solutions because DE-
viaNT does not aggressively explore features spe-
cific to TWSSs (e.g., DEviaNT does not use a lexical
n-gram model of the TWSS training data). Thus, the
goal of our evaluation is not to outperform the base-
lines in all aspects, but rather to show that by using
only euphemism-based and structure-based features,
DEviaNT can compete with the baselines, particu-
larly where it matters most, delivering high precision
and few false positives.
</bodyText>
<subsectionHeader confidence="0.917028">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.990336">
Our goals for DEviaNT’s training data were to
</bodyText>
<listItem confidence="0.94249125">
(1) include a wide range of negative samples to
distinguish TWSSs from arbitrary sentences while
(2) keeping negative and positive samples similar
enough in language to tackle difficult cases. DE-
</listItem>
<bodyText confidence="0.953483409090909">
viaNT’s positive training data are 2001 quoted sen-
tences from twssstories.com (TS), a website of
user-submitted TWSS jokes. DEviaNT’s negative
training data are 2001 sentences from three sources
(667 each): textsfromlastnight.com (TFLN), a
set of user-submitted, typically-racy text messages;
fmylife.com/intimacy (FML), a set of short (1–
2 sentence) user-submitted stories about their love
lives; and wikiquote.org (WQ), a set of quotations
from famous American speakers and films. We did
not carefully examine these sources for noise, but
given that TWSSs are rare, we assumed these data
are sufficiently negative. For testing, we used 262
other TS and 20,700 other TFLN, FML, and WQ
sentences (all the data from these sources that were
available at the time of the experiments). We cleaned
the data by splitting it into individual sentences, cap-
italizing the first letter of each sentence, tagging it
with the Stanford Parser (Toutanova and Manning,
2000; Toutanova et al., 2003), and fixing several tag-
ger errors (e.g., changing the tag of “i” from the for-
eign word tag FW to the correct pronoun tag PRP).
</bodyText>
<subsectionHeader confidence="0.985136">
4.2 Baselines
</subsectionHeader>
<bodyText confidence="0.999905347826087">
Our experiments compare DEviaNT to seven other
classifiers: (1) a Naive Bayes classifier on unigram
features, (2) an SVM model trained on unigram fea-
tures, (3) an SVM model trained on unigram and
bigram features, (4–6) MetaCost (Domingos, 1999)
(see Section 3.4) versions of (1–3), and (7) a version
of DEviaNT that uses just the BASIC STRUCTURE
features (as a feature ablation study). The SVM
models use the same parameters and kernel function
as DEviaNT.
The state-of-the-practice approach to TWSS iden-
tification is a naive Bayes model trained on a un-
igram model of instances of twitter tweets, some
tagged with #twss (VandenBos, 2011). While this
was the only existing classifier we were able to find,
this was not a rigorously approached solution to the
problem. In particular, its training data were noisy,
partially untaggable, and multilingual. Thus, we
reimplemented this approach more rigorously as one
of our baselines.
For completeness, we tested whether adding un-
igram features to DEviaNT improved its perfor-
mance but found that it did not.
</bodyText>
<page confidence="0.932437">
92
</page>
<figure confidence="0.94539695">
DEviaNT
Basic Structure
Unigram SVM w/ MetaCost
Unigram SVM w/o MetaCost
Bigram SVM w/ MetaCost
Bigram SVM w/o MetaCost
Naive Bayes w/ MetaCost
Naive Bayes w/o MetaCost
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Recall
Precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
</figure>
<figureCaption confidence="0.995739">
Figure 1: The precision-recall curves for DEviaNT and
baseline classifiers on TS, TFLN, FML, and WQ.
</figureCaption>
<subsectionHeader confidence="0.963085">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999966588235294">
Figure 1 shows the precision-recall curves for DE-
viaNT and the other seven classifiers. DEviaNT and
Basic Structure achieve the highest precisions. The
best competitor — Unigram SVM w/o MetaCost —
has the maximum precision of 59.2%. In contrast,
DEviaNT’s precision is over 71.4%. Note that the
addition of bigram features yields no improvement
in (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we compared
those sentences that DEviaNT, Basic Structure, and
Unigram SVM w/o MetaCost are most sure are
TWSSs. DEviaNT returned 28 such sentences (all
tied for most likely to be a TWSS), 20 of which
are true positives. However, 2 of the 8 false pos-
itives are in fact TWSSs (despite coming from the
negative testing data): “Yes give me all the cream
and he’s gone.” and “Yeah but his hole really smells
sometimes.” Basic Structure was most sure about 16
sentences, 11 of which are true positives. Of these,
7 were also in DEviaNT’s most-sure set. However,
DEviaNT was also able to identify TWSSs that deal
with noun euphemisms (e.g., “Don’t you think these
buns are a little too big for this meat?”), whereas Ba-
sic Structure could not. In contrast, Unigram SVM
w/o MetaCost is most sure about 130 sentences, 77
of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVM
w/o MetaCost, it accomplishes our goal of deliver-
ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low in
large because the testing data is predominantly neg-
ative. If DEviaNT classified a randomly selected,
balanced subset of the test data, DEviaNT’s preci-
sion would be 0.995.
</bodyText>
<sectionHeader confidence="0.997902" genericHeader="background">
5 Contributions
</sectionHeader>
<bodyText confidence="0.999987956521739">
We formally defined the TWSS problem, a sub-
problem of the double entendre problem. We then
identified two characteristics of the TWSS prob-
lem — (1) TWSSs are likely to contain nouns that
are euphemisms for sexually explicit nouns and (2)
TWSSs share common structure with sentences in
the erotic domain — that we used to construct
DEviaNT, an approach for TWSS classification.
DEviaNT identifies euphemism and erotic-domain
structure without relying heavily on structural fea-
tures specific to TWSSs. DEviaNT delivers sig-
nificantly higher precision than classifiers that use
n-gram TWSS models. Our experiments indicate
that euphemism- and erotic-domain-structure fea-
tures contribute to improving the precision of TWSS
identification.
While significant future work in improving DE-
viaNT remains, we have identified two character-
istics important to the TWSS problem and demon-
strated that an approach based on these character-
istics has promise. The technique of metaphorical
mapping may be generalized to identify other types
of double entendres and other forms of humor.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999960444444444">
The authors wish to thank Tony Fader and Mark
Yatskar for their insights and help with data, Bran-
don Lucia for his part in coming up with the name
DEviaNT, and Luke Zettlemoyer for helpful com-
ments. This material is based upon work supported
by the National Science Foundation Graduate Re-
search Fellowship under Grant #DGE-0718124 and
under Grant #0937060 to the Computing Research
Association for the CIFellows Project.
</bodyText>
<page confidence="0.998084">
93
</page>
<sectionHeader confidence="0.98971" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998959525423729">
Greg Daniels, Ricky Gervais, and Stephen Mer-
chant. 2005. The Office. Television series, the
National Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A general
method for making classifiers cost-sensitive. In
Proceedings of the 5th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and
Data Mining, pages 155–164. San Diego, CA,
USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-
dard Corpus of Present-Day Edited American En-
glish. Department of Linguistics, Brown Univer-
sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,
corpus-based conventional metaphor extraction
system. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-
acterizing humour: An exploration of features in
humorous texts. In Proceedings of the 8th Con-
ference on Intelligent Text Processing and Com-
putational Linguistics (CICLing07). Mexico City,
Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-
ing computers laugh: Investigations in auto-
matic humor recognition. In Human Language
Technology Conference / Conference on Empir-
ical Methods in Natural Language Processing
(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Mining
millions of metaphors. Literary and Linguistic
Computing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-
pretation as a paraphrasing task. In Proceedings
of Human Language Technologies: The 11th An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics
(HLT10), pages 1029–1037. Los Angeles, CA,
USA.
Kristina Toutanova, Dan Klein, Christopher Man-
ning, and Yoram Singer. 2003. Feature-rich part-
of-speech tagging with a cyclic dependency net-
work. In Proceedings of Human Language Tech-
nologies: The Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (HLT03), pages 252–259. Ed-
monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.
Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger. In Joint SIG-
DAT Conference on Empirical Methods in NLP
and Very Large Corpora (EMNLP/VLC00), pages
63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what she
said” bayes classifier. http://rubygems.org/
gems/twss.
</reference>
<page confidence="0.999542">
94
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.687468">
<title confidence="0.998637">That’s What She Said: Double Entendre Identification</title>
<author confidence="0.800192">Kiddon</author>
<affiliation confidence="0.9996195">Computer Science &amp; University of</affiliation>
<address confidence="0.917931">Seattle WA</address>
<abstract confidence="0.992595785714286">Humor identification is a hard natural language understanding problem. We identify a subproblem — the “that’s what she said” problem — with two distinguishing characteristics: (1) use of nouns that are euphemisms for sexually explicit nouns and (2) structure common in the erotic domain. We address this problem in a classification approach that includes features that model those two characteristics. Experiments on web data demonstrate that our approach improves precision by 12% over baseline techniques that use only word-based features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Greg Daniels</author>
<author>Ricky Gervais</author>
<author>Stephen Merchant</author>
</authors>
<date>2005</date>
<booktitle>The Office. Television series, the National Broadcasting Company (NBC).</booktitle>
<contexts>
<context position="893" citStr="Daniels et al., 2005" startWordPosition="128" endWordPosition="131"> identify a subproblem — the “that’s what she said” problem — with two distinguishing characteristics: (1) use of nouns that are euphemisms for sexually explicit nouns and (2) structure common in the erotic domain. We address this problem in a classification approach that includes features that model those two characteristics. Experiments on web data demonstrate that our approach improves precision by 12% over baseline techniques that use only word-based features. 1 Introduction “That’s what she said” is a well-known family of jokes, recently repopularized by the television show “The Office” (Daniels et al., 2005). The jokes consist of saying “that’s what she said” after someone else utters a statement in a non-sexual context that could also have been used in a sexual context. For example, if Aaron refers to his late-evening basketball practice, saying “I was trying all night, but I just could not get it in!”, Betty could utter “that’s what she said”, completing the joke. While somewhat juvenile, this joke presents an interesting natural language understanding problem. A “that’s what she said” (TWSS) joke is a type of double entendre. A double entendre, or adianoeta, is an expression that can be unders</context>
</contexts>
<marker>Daniels, Gervais, Merchant, 2005</marker>
<rawString>Greg Daniels, Ricky Gervais, and Stephen Merchant. 2005. The Office. Television series, the National Broadcasting Company (NBC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
</authors>
<title>MetaCost: A general method for making classifiers cost-sensitive.</title>
<date>1999</date>
<booktitle>In Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>155--164</pages>
<location>San Diego, CA, USA.</location>
<contexts>
<context position="12578" citStr="Domingos, 1999" startWordPosition="2138" endWordPosition="2139"> of pronouns that can be used as subjects (e.g., “she”, “it”), other pronoun}) the subject of s. (We approximate the subject with the first noun or pronoun.) 3.4 Learning algorithm DEviaNT uses an SVM classifier from the WEKA machine learning package (Hall et al., 2009) with the features from Section 3.3. In our prototype implementation, DEviaNT uses the default parameter settings and has the option to fit logistic regression curves to the outputs to allow for precision-recall analysis. To minimize false positives, while tolerating false negatives, DEviaNT employs the MetaCost metaclassifier (Domingos, 1999), which uses bagging to reclassify the training data to produce a single cost-sensitive classifier. DEviaNT sets the cost of a false positive to be 100 times that of a false negative. 4 Evaluation The goal of our evaluation is somewhat unusual. DEviaNT explores a particular approach to solving the TWSS problem: recognizing euphemistic and structural relationships between the source domain and an erotic domain. As such, DEviaNT is at a disadvantage to many potential solutions because DEviaNT does not aggressively explore features specific to TWSSs (e.g., DEviaNT does not use a lexical n-gram mo</context>
<context position="15128" citStr="Domingos, 1999" startWordPosition="2544" endWordPosition="2545">he time of the experiments). We cleaned the data by splitting it into individual sentences, capitalizing the first letter of each sentence, tagging it with the Stanford Parser (Toutanova and Manning, 2000; Toutanova et al., 2003), and fixing several tagger errors (e.g., changing the tag of “i” from the foreign word tag FW to the correct pronoun tag PRP). 4.2 Baselines Our experiments compare DEviaNT to seven other classifiers: (1) a Naive Bayes classifier on unigram features, (2) an SVM model trained on unigram features, (3) an SVM model trained on unigram and bigram features, (4–6) MetaCost (Domingos, 1999) (see Section 3.4) versions of (1–3), and (7) a version of DEviaNT that uses just the BASIC STRUCTURE features (as a feature ablation study). The SVM models use the same parameters and kernel function as DEviaNT. The state-of-the-practice approach to TWSS identification is a naive Bayes model trained on a unigram model of instances of twitter tweets, some tagged with #twss (VandenBos, 2011). While this was the only existing classifier we were able to find, this was not a rigorously approached solution to the problem. In particular, its training data were noisy, partially untaggable, and multil</context>
</contexts>
<marker>Domingos, 1999</marker>
<rawString>Pedro Domingos. 1999. MetaCost: A general method for making classifiers cost-sensitive. In Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 155–164. San Diego, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Nelson Francis</author>
<author>Henry Kucera</author>
</authors>
<title>A Standard</title>
<date>1979</date>
<institution>Corpus of Present-Day Edited American English. Department of Linguistics, Brown University.</institution>
<contexts>
<context position="8059" citStr="Francis and Kucera, 1979" startWordPosition="1294" endWordPosition="1297">jokes. In our approximation, the source context in which the potential TWSS is ��SN−I = 61. Let BP be an open set of body-part uttered is not in an erotic one. If it is, then the map- nouns. Our approximation contains 98 body parts. ping to the erotic domain is the identity and the state- DEviaNT uses two corpora. The erotica corpus ment is not a TWSS. In this paper, we assume all test consists of 1.5M sentences from the erotica section instances are from nonerotic domains and leave the 90 of textfiles.com/sex/EROTICA. We removed headers, footers, URLs, and unparseable text. The Brown corpus (Francis and Kucera, 1979) is 57K sentences that represent standard (nonerotic) literature. We tagged the erotica corpus with the Stanford Parser (Toutanova and Manning, 2000; Toutanova et al., 2003); the Brown corpus is already tagged. To make the corpora more generic, we replaced all numbers with the CD tag, all proper nouns with the NNP tag, all nouns E SN with an SN tag, and all nouns E� BP with the NN tag. We ignored determiners and punctuation. 3.2 Word- and phrase-level analysis We define three functions to measure how closely related a noun, an adjective, and a verb phrase are to the erotica domain. 1. The noun</context>
</contexts>
<marker>Francis, Kucera, 1979</marker>
<rawString>W. Nelson Francis and Henry Kucera. 1979. A Standard Corpus of Present-Day Edited American English. Department of Linguistics, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="12233" citStr="Hall et al., 2009" startWordPosition="2085" endWordPosition="2088"> a sentence s E SE U SB with a noun E SN. DEviaNT also uses the following features to identify the BASIC STRUCTURE of a TWSS: • (int) number of non-punctuation tokens, • (int) number of punctuation tokens, 91 • ({0, 1, 2+}) for each pronoun and each part-ofspeech tag, number of times it occurs in s, • ({noun, proper noun, each of a selected group of pronouns that can be used as subjects (e.g., “she”, “it”), other pronoun}) the subject of s. (We approximate the subject with the first noun or pronoun.) 3.4 Learning algorithm DEviaNT uses an SVM classifier from the WEKA machine learning package (Hall et al., 2009) with the features from Section 3.3. In our prototype implementation, DEviaNT uses the default parameter settings and has the option to fit logistic regression curves to the outputs to allow for precision-recall analysis. To minimize false positives, while tolerating false negatives, DEviaNT employs the MetaCost metaclassifier (Domingos, 1999), which uses bagging to reclassify the training data to produce a single cost-sensitive classifier. DEviaNT sets the cost of a false positive to be 100 times that of a false negative. 4 Evaluation The goal of our evaluation is somewhat unusual. DEviaNT ex</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Explorations, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zachary J Mason</author>
</authors>
<title>CorMet: A computational, corpus-based conventional metaphor extraction system.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="2744" citStr="Mason, 2004" startWordPosition="436" endWordPosition="437">dentification. A metaphor is a figure of speech that creates an analogical mapping between two conceptual domains so that the terminology of one (source) domain can be used to describe situations and objects in the other (target) domain. Usage of the source domain’s terminology in the source domain is literal and is nonliteral in the target domain. Metaphor identification systems seek to differentiate between literal and nonliteral expressions. Some computational approaches to metaphor identification learn selectional preferences of words in multiple domains to help identify nonliteral usage (Mason, 2004; Shutova, 2010). Other approaches train support vector machine (SVM) models on labeled training data to distinguish metaphoric language from literal language (Pasanek and Sculley, 2008). TWSSs also represent mappings between two domains: the innocuous source domain and an erotic target domain. Therefore, we can apply methods from metaphor identification to TWSS identification. In particular, we (1) compare the adjectival selectional preferences of sexually explicit nouns to those of other nouns to determine which nouns may be euphemisms for sexually explicit nouns and (2) Proceedings of the 4</context>
</contexts>
<marker>Mason, 2004</marker>
<rawString>Zachary J. Mason. 2004. CorMet: A computational, corpus-based conventional metaphor extraction system. Computational Linguistics, 30(1):23–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Stephen Pulman</author>
</authors>
<title>Characterizing humour: An exploration of features in humorous texts.</title>
<date>2007</date>
<booktitle>In Proceedings of the 8th Conference on Intelligent Text Processing and Computational Linguistics (CICLing07). Mexico City,</booktitle>
<location>Mexico.</location>
<marker>Mihalcea, Pulman, 2007</marker>
<rawString>Rada Mihalcea and Stephen Pulman. 2007. Characterizing humour: An exploration of features in humorous texts. In Proceedings of the 8th Conference on Intelligent Text Processing and Computational Linguistics (CICLing07). Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carlo Strapparava</author>
</authors>
<title>Making computers laugh: Investigations in automatic humor recognition.</title>
<date>2005</date>
<booktitle>In Human Language Technology Conference / Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP05).</booktitle>
<location>Vancouver, BC,</location>
<marker>Mihalcea, Strapparava, 2005</marker>
<rawString>Rada Mihalcea and Carlo Strapparava. 2005. Making computers laugh: Investigations in automatic humor recognition. In Human Language Technology Conference / Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP05). Vancouver, BC, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley M Pasanek</author>
<author>D Sculley</author>
</authors>
<title>Mining millions of metaphors.</title>
<date>2008</date>
<journal>Literary and Linguistic Computing,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="2930" citStr="Pasanek and Sculley, 2008" startWordPosition="461" endWordPosition="465">d to describe situations and objects in the other (target) domain. Usage of the source domain’s terminology in the source domain is literal and is nonliteral in the target domain. Metaphor identification systems seek to differentiate between literal and nonliteral expressions. Some computational approaches to metaphor identification learn selectional preferences of words in multiple domains to help identify nonliteral usage (Mason, 2004; Shutova, 2010). Other approaches train support vector machine (SVM) models on labeled training data to distinguish metaphoric language from literal language (Pasanek and Sculley, 2008). TWSSs also represent mappings between two domains: the innocuous source domain and an erotic target domain. Therefore, we can apply methods from metaphor identification to TWSS identification. In particular, we (1) compare the adjectival selectional preferences of sexually explicit nouns to those of other nouns to determine which nouns may be euphemisms for sexually explicit nouns and (2) Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics examine</context>
</contexts>
<marker>Pasanek, Sculley, 2008</marker>
<rawString>Bradley M. Pasanek and D. Sculley. 2008. Mining millions of metaphors. Literary and Linguistic Computing, 23(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
</authors>
<title>Automatic metaphor interpretation as a paraphrasing task.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT10),</booktitle>
<pages>1029--1037</pages>
<location>Los Angeles, CA, USA.</location>
<contexts>
<context position="2760" citStr="Shutova, 2010" startWordPosition="438" endWordPosition="439">. A metaphor is a figure of speech that creates an analogical mapping between two conceptual domains so that the terminology of one (source) domain can be used to describe situations and objects in the other (target) domain. Usage of the source domain’s terminology in the source domain is literal and is nonliteral in the target domain. Metaphor identification systems seek to differentiate between literal and nonliteral expressions. Some computational approaches to metaphor identification learn selectional preferences of words in multiple domains to help identify nonliteral usage (Mason, 2004; Shutova, 2010). Other approaches train support vector machine (SVM) models on labeled training data to distinguish metaphoric language from literal language (Pasanek and Sculley, 2008). TWSSs also represent mappings between two domains: the innocuous source domain and an erotic target domain. Therefore, we can apply methods from metaphor identification to TWSS identification. In particular, we (1) compare the adjectival selectional preferences of sexually explicit nouns to those of other nouns to determine which nouns may be euphemisms for sexually explicit nouns and (2) Proceedings of the 49th Annual Meeti</context>
</contexts>
<marker>Shutova, 2010</marker>
<rawString>Ekaterina Shutova. 2010. Automatic metaphor interpretation as a paraphrasing task. In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT10), pages 1029–1037. Los Angeles, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich partof-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT03),</booktitle>
<pages>252--259</pages>
<location>Edmonton, AB,</location>
<contexts>
<context position="8232" citStr="Toutanova et al., 2003" startWordPosition="1320" endWordPosition="1323">the map- nouns. Our approximation contains 98 body parts. ping to the erotic domain is the identity and the state- DEviaNT uses two corpora. The erotica corpus ment is not a TWSS. In this paper, we assume all test consists of 1.5M sentences from the erotica section instances are from nonerotic domains and leave the 90 of textfiles.com/sex/EROTICA. We removed headers, footers, URLs, and unparseable text. The Brown corpus (Francis and Kucera, 1979) is 57K sentences that represent standard (nonerotic) literature. We tagged the erotica corpus with the Stanford Parser (Toutanova and Manning, 2000; Toutanova et al., 2003); the Brown corpus is already tagged. To make the corpora more generic, we replaced all numbers with the CD tag, all proper nouns with the NNP tag, all nouns E SN with an SN tag, and all nouns E� BP with the NN tag. We ignored determiners and punctuation. 3.2 Word- and phrase-level analysis We define three functions to measure how closely related a noun, an adjective, and a verb phrase are to the erotica domain. 1. The noun sexiness function NS(n) is a realvalued measure of the maximum similarity a noun n E� SN has to each of the nouns E SN−. For each noun, let the adjective count vector be th</context>
<context position="14742" citStr="Toutanova et al., 2003" startWordPosition="2476" endWordPosition="2479">mitted stories about their love lives; and wikiquote.org (WQ), a set of quotations from famous American speakers and films. We did not carefully examine these sources for noise, but given that TWSSs are rare, we assumed these data are sufficiently negative. For testing, we used 262 other TS and 20,700 other TFLN, FML, and WQ sentences (all the data from these sources that were available at the time of the experiments). We cleaned the data by splitting it into individual sentences, capitalizing the first letter of each sentence, tagging it with the Stanford Parser (Toutanova and Manning, 2000; Toutanova et al., 2003), and fixing several tagger errors (e.g., changing the tag of “i” from the foreign word tag FW to the correct pronoun tag PRP). 4.2 Baselines Our experiments compare DEviaNT to seven other classifiers: (1) a Naive Bayes classifier on unigram features, (2) an SVM model trained on unigram features, (3) an SVM model trained on unigram and bigram features, (4–6) MetaCost (Domingos, 1999) (see Section 3.4) versions of (1–3), and (7) a version of DEviaNT that uses just the BASIC STRUCTURE features (as a feature ablation study). The SVM models use the same parameters and kernel function as DEviaNT. T</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich partof-speech tagging with a cyclic dependency network. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT03), pages 252–259. Edmonton, AB, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora (EMNLP/VLC00),</booktitle>
<pages>63--71</pages>
<location>Hong Kong, China.</location>
<contexts>
<context position="8207" citStr="Toutanova and Manning, 2000" startWordPosition="1316" endWordPosition="1319">n erotic one. If it is, then the map- nouns. Our approximation contains 98 body parts. ping to the erotic domain is the identity and the state- DEviaNT uses two corpora. The erotica corpus ment is not a TWSS. In this paper, we assume all test consists of 1.5M sentences from the erotica section instances are from nonerotic domains and leave the 90 of textfiles.com/sex/EROTICA. We removed headers, footers, URLs, and unparseable text. The Brown corpus (Francis and Kucera, 1979) is 57K sentences that represent standard (nonerotic) literature. We tagged the erotica corpus with the Stanford Parser (Toutanova and Manning, 2000; Toutanova et al., 2003); the Brown corpus is already tagged. To make the corpora more generic, we replaced all numbers with the CD tag, all proper nouns with the NNP tag, all nouns E SN with an SN tag, and all nouns E� BP with the NN tag. We ignored determiners and punctuation. 3.2 Word- and phrase-level analysis We define three functions to measure how closely related a noun, an adjective, and a verb phrase are to the erotica domain. 1. The noun sexiness function NS(n) is a realvalued measure of the maximum similarity a noun n E� SN has to each of the nouns E SN−. For each noun, let the adj</context>
<context position="14717" citStr="Toutanova and Manning, 2000" startWordPosition="2472" endWordPosition="2475">hort (1– 2 sentence) user-submitted stories about their love lives; and wikiquote.org (WQ), a set of quotations from famous American speakers and films. We did not carefully examine these sources for noise, but given that TWSSs are rare, we assumed these data are sufficiently negative. For testing, we used 262 other TS and 20,700 other TFLN, FML, and WQ sentences (all the data from these sources that were available at the time of the experiments). We cleaned the data by splitting it into individual sentences, capitalizing the first letter of each sentence, tagging it with the Stanford Parser (Toutanova and Manning, 2000; Toutanova et al., 2003), and fixing several tagger errors (e.g., changing the tag of “i” from the foreign word tag FW to the correct pronoun tag PRP). 4.2 Baselines Our experiments compare DEviaNT to seven other classifiers: (1) a Naive Bayes classifier on unigram features, (2) an SVM model trained on unigram features, (3) an SVM model trained on unigram and bigram features, (4–6) MetaCost (Domingos, 1999) (see Section 3.4) versions of (1–3), and (7) a version of DEviaNT that uses just the BASIC STRUCTURE features (as a feature ablation study). The SVM models use the same parameters and kern</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora (EMNLP/VLC00), pages 63–71. Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben VandenBos</author>
</authors>
<title>Pre-trained “that’s what she said” bayes classifier. http://rubygems.org/ gems/twss.</title>
<date>2011</date>
<contexts>
<context position="15521" citStr="VandenBos, 2011" startWordPosition="2609" endWordPosition="2610">re DEviaNT to seven other classifiers: (1) a Naive Bayes classifier on unigram features, (2) an SVM model trained on unigram features, (3) an SVM model trained on unigram and bigram features, (4–6) MetaCost (Domingos, 1999) (see Section 3.4) versions of (1–3), and (7) a version of DEviaNT that uses just the BASIC STRUCTURE features (as a feature ablation study). The SVM models use the same parameters and kernel function as DEviaNT. The state-of-the-practice approach to TWSS identification is a naive Bayes model trained on a unigram model of instances of twitter tweets, some tagged with #twss (VandenBos, 2011). While this was the only existing classifier we were able to find, this was not a rigorously approached solution to the problem. In particular, its training data were noisy, partially untaggable, and multilingual. Thus, we reimplemented this approach more rigorously as one of our baselines. For completeness, we tested whether adding unigram features to DEviaNT improved its performance but found that it did not. 92 DEviaNT Basic Structure Unigram SVM w/ MetaCost Unigram SVM w/o MetaCost Bigram SVM w/ MetaCost Bigram SVM w/o MetaCost Naive Bayes w/ MetaCost Naive Bayes w/o MetaCost 0.1 0.2 0.3 </context>
</contexts>
<marker>VandenBos, 2011</marker>
<rawString>Ben VandenBos. 2011. Pre-trained “that’s what she said” bayes classifier. http://rubygems.org/ gems/twss.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>