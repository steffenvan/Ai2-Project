<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000115">
<title confidence="0.961628">
Improving QA Accuracy by Question Inversion
</title>
<author confidence="0.859108">
John Prager
</author>
<bodyText confidence="0.3094575">
IBM T.J. Watson Res. Ctr.
Yorktown Heights
N.Y. 10598
jprager@us.ibm.com
Pablo Duboue
IBM T.J. Watson Res. Ctr.
Yorktown Heights
N.Y. 10598
duboue@us.ibm.com
Jennifer Chu-Carroll
IBM T.J. Watson Res. Ctr.
Yorktown Heights
N.Y. 10598
jencc@us.ibm.com
</bodyText>
<sectionHeader confidence="0.978432" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999885">
This paper demonstrates a conceptually simple
but effective method of increasing the accuracy
of QA systems on factoid-style questions. We
define the notion of an inverted question, and
show that by requiring that the answers to the
original and inverted questions be mutually con-
sistent, incorrect answers get demoted in confi-
dence and correct ones promoted. Additionally,
we show that lack of validation can be used to
assert no-answer (nil) conditions. We demon-
strate increases of performance on TREC and
other question-sets, and discuss the kinds of fu-
ture activities that can be particularly beneficial
to approaches such as ours.
</bodyText>
<sectionHeader confidence="0.998478" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999804830769231">
Most QA systems nowadays consist of the following
standard modules: QUESTION PROCESSING, to de-
termine the bag of words for a query and the desired
answer type (the type of the entity that will be of-
fered as a candidate answer); SEARCH, which will
use the query to extract a set of documents or pas-
sages from a corpus; and ANSWER SELECTION,
which will analyze the returned documents or pas-
sages for instances of the answer type in the most
favorable contexts. Each of these components im-
plements a set of heuristics or hypotheses, as de-
vised by their authors (cf. Clarke et al. 2001, Chu-
Carroll et al. 2003).
When we perform failure analysis on questions in-
correctly answered by our system, we find that there
are broadly speaking two kinds of failure. There are
errors (we might call them bugs) on the implementa-
tion of the said heuristics: errors in tagging, parsing,
named-entity recognition; omissions in synonym
lists; missing patterns, and just plain programming
errors. This class can be characterized by being fix-
able by identifying incorrect code and fixing it, or
adding more items, either explicitly or through train-
ing. The other class of errors (what we might call
unlucky) are at the boundaries of the heuristics;
situations were the system did not do anything
“wrong,” in the sense of bug, but circumstances con-
spired against finding the correct answer.
Usually when unlucky errors occur, the system gen-
erates a reasonable query and an appropriate answer
type, and at least one passage containing the right
answer is returned. However, there may be returned
passages that have a larger number of query terms
and an incorrect answer of the right type, or the
query terms might just be physically closer to the
incorrect answer than to the correct one. ANSWER
SELECTION modules typically work either by trying
to prove the answer is correct (Moldovan &amp; Rus,
2001) or by giving them a weight produced by
summing a collection of heuristic features (Radev et
al., 2000); in the latter case candidates having a lar-
ger number of matching query terms, even if they do
not exactly match the context in the question, might
generate a larger score than a correct passage with
fewer matching terms.
To be sure, unlucky errors are usually bugs when
considered from the standpoint of a system with a
more sophisticated heuristic, but any system at any
point in time will have limits on what it tries to do;
therefore the distinction is not absolute but is rela-
tive to a heuristic and system.
It has been argued (Prager, 2002) that the success of
a QA system is proportional to the impedance match
between the question and the knowledge sources
available. We argue here similarly. Moreover, we
believe that this is true not only in terms of the cor-
rect answer, but the distracters,1 or incorrect answers
too. In QA, an unlucky incorrect answer is not usu-
ally predictable in advance; it occurs because of a
coincidence of terms and syntactic contexts that
cause it to be preferred over the correct answer. It
has no connection with the correct answer and is
only returned because its enclosing passage so hap-
pens to exist in the same corpus as the correct an-
swer context. This would lead us to believe that if a
</bodyText>
<footnote confidence="0.874591">
1 We borrow the term from multiple-choice test design.
</footnote>
<page confidence="0.962214">
1073
</page>
<bodyText confidence="0.998403882352941">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1073–1080,
Sydney, July 2006. c�2006 Association for Computational Linguistics
different corpus containing the correct answer were
to be processed, while there would be no guarantee
that the correct answer would be found, it would be
unlikely (i.e. very unlucky) if the same incorrect an-
swer as before were returned.
We have demonstrated elsewhere (Prager et al.
2004b) how using multiple corpora can improve QA
performance, but in this paper we achieve similar
goals without using additional corpora. We note that
factoid questions are usually about relations between
entities, e.g. “What is the capital of France?”, where
one of the arguments of the relationship is sought
and the others given. We can invert the question by
substituting the candidate answer back into the ques-
tion, while making one of the given entities the so-
called wh-word, thus “Of what country is Paris the
capital?” We hypothesize that asking this question
(and those formed from other candidate answers)
will locate a largely different set of passages in the
corpus than the first time around. As will be ex-
plained in Section 3, this can be used to decrease the
confidence in the incorrect answers, and also in-
crease it for the correct answer, so that the latter be-
comes the answer the system ultimately proposes.
This work is part of a continuing program of demon-
strating how meta-heuristics, using what might be
called “collateral” information, can be used to con-
strain or adjust the results of the primary QA system.
In the next Section we review related work. In Sec-
tion 3 we describe our algorithm in detail, and in
Section 4 present evaluation results. In Section 5 we
discuss our conclusions and future work.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999927962264151">
Logic and inferencing have been a part of Question-
Answering since its earliest days. The first such
systems were natural-language interfaces to expert
systems, e.g., SHRDLU (Winograd, 1972), or to
databases, e.g., LIFER/LADDER (Hendrix et al.
1977). CHAT-80 (Warren &amp; Pereira, 1982), for in-
stance, was a DCG-based NL-query system about
world geography, entirely in Prolog. In these
systems, the NL question is transformed into a se-
mantic form, which is then processed further. Their
overall architecture and system operation is very
different from today’s systems, however, primarily
in that there was no text corpus to process.
Inferencing is a core requirement of systems that
participate in the current PASCAL Recognizing
Textual Entailment (RTE) challenge (see
http://www.pascal-network.org/Challenges/RTE and
.../RTE2). It is also used in at least two of the more
visible end-to-end QA systems of the present day.
The LCC system (Moldovan &amp; Rus, 2001) uses a
Logic Prover to establish the connection between a
candidate answer passage and the question. Text
terms are converted to logical forms, and the ques-
tion is treated as a goal which is “proven”, with real-
world knowledge being provided by Extended
WordNet. The IBM system PIQUANT (Chu-
Carroll et al., 2003) used Cyc (Lenat, 1995) in an-
swer verification. Cyc can in some cases confirm or
reject candidate answers based on its own store of
instance information; in other cases, primarily of a
numerical nature, Cyc can confirm whether candi-
dates are within a reasonable range established for
their subtype.
At a more abstract level, the use of inversions dis-
cussed in this paper can be viewed as simply an ex-
ample of finding support (or lack of it) for candidate
answers. Many current systems (see, e.g. (Clarke et
al., 2001; Prager et al. 2004b)) employ redundancy
as a significant feature of operation: if the same an-
swer appears multiple times in an internal top-n list,
whether from multiple sources or multiple algo-
rithms/agents, it is given a confidence boost, which
will affect whether and how it gets returned to the
end-user.
The work here is a continuation of previous work
described in (Prager et al. 2004a,b). In the former
we demonstrated that for a certain kind of question,
if the inverted question were given, we could im-
prove the F-measure of accuracy on a question set
by 75%. In this paper, by contrast, we do not manu-
ally provide the inverted question, and in the second
evaluation presented here we do not restrict the
question type.
</bodyText>
<sectionHeader confidence="0.998874" genericHeader="method">
3 Algorithm
</sectionHeader>
<subsectionHeader confidence="0.999066">
3.1 System Architecture
</subsectionHeader>
<bodyText confidence="0.989431083333333">
A simplified block-diagram of our PIQUANT sys-
tem is shown in Figure 1. The outer block on the
left, QS1, is our basic QA system, in which the
QUESTION PROCESSING (QP), SEARCH (S) and
ANSWER SELECTION (AS) subcomponents are indi-
cated. The outer block on the right, QS2, is another
QA-System that is used to answer the inverted ques-
tions. In principle QS2 could be QS1 but parameter-
ized differently, or even an entirely different system,
but we use another instance of QS1, as-is. The
block in the middle is our Constraints Module CM,
which is the subject of this paper.
</bodyText>
<page confidence="0.98392">
1074
</page>
<figure confidence="0.999555238095238">
Question
QS1
QA system
AS
answer selection
QP
question proc.
S
search
QS2
QA system
AS
answer selection
QP
question proc.
S
search
Answers
CM
constraints
module
</figure>
<figureCaption confidence="0.999973">
Figure 1. Constraints Architecture. QS1 and QS2 are (possibly identical) QA systems.
</figureCaption>
<bodyText confidence="0.99945225">
The Question Processing component of QS2 is not
used in this context since CM simulates its output by
modifying the output of QP in QS1, as described in
Section 3.3.
</bodyText>
<subsectionHeader confidence="0.999476">
3.2 Inverting Questions
</subsectionHeader>
<bodyText confidence="0.997552727272727">
Our open-domain QA system employs a named-
entity recognizer that identifies about a hundred
types. Any of these can be answer types, and there
are corresponding sets of patterns in the QUESTION
PROCESSING module to determine the answer type
sought by any question. When we wish to invert a
question, we must find an entity in the question
whose type we recognize; this entity then becomes
the sought answer for the inverted question. We call
this entity the inverted or pivot term.
Thus for the question:
</bodyText>
<listItem confidence="0.899722">
(1) “What was the capital of Germany in 1985?”
</listItem>
<bodyText confidence="0.912356333333333">
Germany is identified as a term with a known type
(COUNTRY). Then, given the candidate answer
&lt;CANDANS&gt;, the inverted question becomes
</bodyText>
<listItem confidence="0.936956">
(2) “Of what country was &lt; CANDANS&gt; the capital
in 1985?”
</listItem>
<bodyText confidence="0.600494">
Some questions have more than one invertible term.
Consider for example:
</bodyText>
<listItem confidence="0.990406714285714">
(3) “Who was the 33rd president of the U.S.?”
This question has 3 inversion points:
(4) “What number president of the U.S. was
&lt;CANDANS&gt;?”
(5) “Of what country was &lt;CANDANS&gt; the 33rd
president?”
(6) “&lt;CANDANS&gt; was the 33rd what of the U.S.?”
</listItem>
<bodyText confidence="0.999950366666667">
Having more than one possible inversion is in theory
a benefit, since it gives more opportunity for enforc-
ing consistency, but in our current implementation
we just pick one for simplicity. We observe on
training data that, in general, the smaller the number
of unique instances of an answer type, the more
likely it is that the inverted question will be correctly
answered. We generated a set NELIST of the most
frequently-occurring named-entity types in ques-
tions; this list is sorted in order of estimated cardi-
nality.
It might seem that the question inversion process can
be quite tricky and can generate possibly unnatural
phrasings, which in turn can be difficult to reparse.
However, the examples given above were simply
English renditions of internal inverted structures – as
we shall see the system does not need to use a natu-
ral language representation of the inverted questions.
Some questions are either not invertible, or, like
“How did X die?” have an inverted form (“Who died
of cancer?”) with so many correct answers that we
know our algorithm is unlikely to benefit us. How-
ever, as it is constituted it is unlikely to hurt us ei-
ther, and since it is difficult to automatically identify
such questions, we don’t attempt to intercept them.
As reported in (Prager et al. 2004a), an estimated
79% of the questions in TREC question sets can be
inverted meaningfully. This places an upper limit
on the gains to be achieved with our algorithm, but
is high enough to be worth pursuing.
</bodyText>
<page confidence="0.967423">
1075
</page>
<subsectionHeader confidence="0.984192">
3.3 Inversion Algorithm
</subsectionHeader>
<bodyText confidence="0.999113941176471">
As shown in the previous section, not all questions
have easily generated inverted forms (even by a hu-
man). However, we do not need to explicate the
inverted form in natural language in order to process
the inverted question.
In our system, a question is processed by the
QUESTION PROCESSING module, which produces a
structure called a QFrame, which is used by the sub-
sequent SEARCH and ANSWER SELECTION modules.
The QFrame contains the list of terms and phrases in
the question, along with their properties, such as
POS and NE-type (if it exists), and a list of syntactic
relationship tuples. When we have a candidate an-
swer in hand, we do not need to produce the inverted
English question, but merely the QFrame that would
have been generated from it. Figure 1 shows that
the CONSTRAINTS MODULE takes the QFrame as one
of its inputs, as shown by the link from QP in QS1
to CM. This inverted QFrame can be generated by a
set of simple transformations, substituting the pivot
term in the bag of words with a candidate answer
&lt;CANDANS&gt;, the original answer type with the type
of the pivot term, and in the relationships the pivot
term with its type and the original answer type with
&lt;CANDANS&gt;. When relationships are evaluated, a
type token will match any instance of that type. Fig-
ure 2 shows a simplified view of the original
QFrame for “What was the capital of Germany in
1945?”, and Figure 3 shows the corresponding In-
verted QFrame. COUNTRY is determined to be a
better type to invert than YEAR, so “Germany” be-
comes the pivot. In Figure 3, the token
&lt;CANDANS&gt; might take in turn “Berlin”, “Mos-
cow”, “Prague” etc.
</bodyText>
<table confidence="0.91387775">
Keywords: {1945, Germany, capital}
AnswerType: CAPITAL
Relationships: {(Germany, capital), (capital,
CAPITAL), (capital, 1945)}
</table>
<figureCaption confidence="0.692263">
Figure 2. Simplified QFrame
</figureCaption>
<table confidence="0.7721135">
Keywords: {1945, &lt;CANDANS&gt;, capital}
AnswerType: COUNTRY
Relationships: {(COUNTRY, capital), (capital,
&lt;CANDANS&gt;), (capital, 1945)}
</table>
<figureCaption confidence="0.991491">
Figure 3. Simplified Inverted QFrame.
</figureCaption>
<bodyText confidence="0.9956548">
The output of QS2 after processing the inverted
QFrame is a list of answers to the inverted question,
which by extension of the nomenclature we call “in-
verted answers.” If no term in the question has an
identifiable type, inversion is not possible.
</bodyText>
<subsectionHeader confidence="0.993831">
3.4 Profiting From Inversions
</subsectionHeader>
<bodyText confidence="0.999762272727273">
Broadly speaking, our goal is to keep or re-rank the
candidate answer hit-list on account of inversion
results. Suppose that a question Q is inverted
around pivot term T, and for each candidate answer
Ci, a list of “inverted” answers {Cij} is generated as
described in the previous section. If T is on one of
the {Cij}, then we say that Ci is validated. Valida-
tion is not a guarantee of keeping or improving Ci’s
position or score, but it helps. Most cases of failure
to validate are called refutation; similarly, refutation
of Ci is not a guarantee of lowering its score or posi-
tion.
It is an open question how to adjust the results of the
initial candidate answer list in light of the results of
the inversion. If the scores associated with candi-
date answers (in both directions) were true prob-
abilities, then a Bayesian approach would be easy to
develop. However, they are not in our system. In
addition, there are quite a few parameters that de-
scribe the inversion scenario.
Suppose Q generates a list of the top-N candidates
{Ci}, with scores {Si}. If this inversion method
were not to be used, the top candidate on this list,
C1, would be the emitted answer. The question gen-
erated by inverting about T and substituting Ci is
QTi. The system is fixed to find the top 10 passages
responsive to QTi, and generates an ordered list Cij
of candidate answers found in this set.
Each inverted question QTi is run through our sys-
tem, generating inverted answers {Cij}, with scores
{Sij}, and whether and where the pivot term T shows
up on this list, represented by a list of positions {Pi},
where Pi is defined as:
</bodyText>
<equation confidence="0.9325575">
Pi = j if Cij = T, for some j
Pi = -1 otherwise
</equation>
<bodyText confidence="0.999974214285714">
We added to the candidate list the special answer
nil, representing “no answer exists in the corpus.”
As described earlier, we had observed from training
data that failure to validate candidates of certain
types (such as Person) would not necessarily be a
real refutation, so we established a set of types
SOFTREFUTATION which would contain the broadest
of our types. At the other end of the spectrum, we
observed that certain narrow candidate types such as
UsState would definitely be refuted if validation
didn’t occur. These are put in set MUSTCONSTRAIN.
Our goal was to develop an algorithm for recomput-
ing all the original scores {Si} from some combina-
tion (based on either arithmetic or decision-trees) of
</bodyText>
<page confidence="0.98162">
1076
</page>
<bodyText confidence="0.9996991">
{Si} and {Sij} and membership of SOFTREFUTATION
and MUSTCONSTRAIN. Reliably learning all those
weights, along with set membership, was not possi-
ble given only several hundred questions of training
data. We therefore focused on a reduced problem.
We observed that when run on TREC question sets,
the frequency of the rank of our top answer fell off
rapidly, except with a second mode when the tail
was accumulated in a single bucket. Our numbers
for TRECs 11 and 12 are shown in Table 1.
</bodyText>
<table confidence="0.99799">
Top answer rank TREC11 TREC12
1 170 108
2 35 32
3 23 14
4 7 7
5 14 9
elsewhere 251 244
% correct 34 26
</table>
<tableCaption confidence="0.999899">
Table 1. Baseline statistics for TREC11-12.
</tableCaption>
<bodyText confidence="0.966829482758621">
We decided to focus on those questions where we
got the right answer in second place (for brevity,
we’ll call these second-place questions). Given that
TREC scoring only rewards first-place answers, it
seemed that with our incremental approach we
would get most benefit there. Also, we were keen to
limit the additional response time incurred by our
approach. Since evaluating the top N answers to the
original question with the Constraints process re-
quires calling the QA system another N times per
question, we were happy to limit N to 2. In addition,
this greatly reduced the number of parameters we
needed to learn.
For the evaluation, which consisted of determining if
the resulting top answer was right or wrong, it meant
ultimately deciding on one of three possible out-
comes: the original top answer, the original second
answer, or nil. We hoped to promote a significant
number of second-place finishers to top place and
introduce some nils, with minimal disturbance of
those already in first place.
We used TREC11 data for training, and established
a set of thresholds for a decision-tree approach to
determining the answer, using Weka (Witten &amp;
Frank, 2005). We populated sets SOFTREFUTATION
and MUSTCONSTRAIN by manual inspection.
The result is Algorithm A, where (i E {1,2}) and
o The Ci are the original candidate answers
o The ak are learned parameters (k E {1..13})
</bodyText>
<listItem confidence="0.7710289">
o Vi means the ith answer was validated
o Pi was the rank of the validating answer to ques-
tion QTi
o Ai was the score of the validating answer to QTi.
Algorithm A. Answer re-ranking using con-
straints validation data.
1. If C1 = nil and V2, return C2
2. If V1 and A1 &gt; a1, return C1
3. If not V1 and not V2 and
type(T) E MUSTCONSTRAIN,
</listItem>
<figure confidence="0.9100182">
return nil
4. If not V1 and not V2 and
type(T) ∉SOFTREFUTATION,
if S1 &gt; a2,, return C1 else nil
5. If not V2, return C1
6. If not V1 and V2 and
A2 &gt; a3 and P2 &lt; a4 and
S1-S2 &lt; a5 and S2 &gt; a6, return C2
7. If V1 and V2 and
(A2 - P2/a7) &gt; (A1 - P1/a7) and
A1 &lt; a8 and P1 &gt; ag and
A2 &lt; a10 and P2 &gt; a11 and
S1-S2 &lt; a12 and (S2 - P2/a7) &gt; a13,
return C2
8. else return C1
</figure>
<sectionHeader confidence="0.898359" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.998702333333333">
Due to the complexity of the learned algorithm, we
decided to evaluate in stages. We first performed an
evaluation with a fixed question type, to verify that
the purely arithmetic components of the algorithm
were performing reasonably. We then evaluated on
the entire TREC12 factoid question set.
</bodyText>
<subsectionHeader confidence="0.96411">
4.1 Evaluation 1
</subsectionHeader>
<bodyText confidence="0.999991466666667">
We created a fixed question set of 50 questions of
the form “What is the capital of X?”, for each state
in the U.S. The inverted question “What state is Z
the capital of?” was correctly generated in each
case. We evaluated against two corpora: the
AQUAINT corpus, of a little over a million news-
wire documents, and the CNS corpus, with about
37,000 documents from the Center for Nonprolifera-
tion Studies in Monterey, CA. We expected there to
be answers to most questions in the former corpus,
so we hoped there our method would be useful in
converting 2nd place answers to first place. The lat-
ter corpus is about WMDs, so we expected there to
be holes in the state capital coverage2, for which nil
identification would be useful.3
</bodyText>
<footnote confidence="0.99873">
2 We manually determined that only 23 state capitals were at-
tested to in the CNS corpus, compared with all in AQUAINT.
3 We added Tbilisi to the answer key for “What is the capi-
tal of Georgia?”, since there was nothing in the question to
disambiguate Georgia.
</footnote>
<page confidence="0.99796">
1077
</page>
<bodyText confidence="0.996000166666667">
The baseline is our regular search-based QA-System
without the Constraint process. In this baseline sys-
tem there was no special processing for nil ques-
tions, other than if the search (which always
contained some required terms) returned no docu-
ments. Our results are shown in Table 2.
</bodyText>
<table confidence="0.998764363636364">
AQUAINT AQUAINT CNS CNS
baseline w/con- baseline w/con-
straints straints
Firsts 39/50 43/50 7/23 4/23
(non-nil)
Total 0/0 0/0 0/27 16/27
nils
Total 39/50 43/50 7/50 20/50
firsts
% 78 86 14 40
correct
</table>
<tableCaption confidence="0.988141">
Table 2. Evaluation on AQUAINT and CNS
corpora.
</tableCaption>
<bodyText confidence="0.9997275">
On the AQUAINT corpus, four out of seven 2nd
place finishers went to first place. On the CNS cor-
pus 16 out of a possible 26 correct no-answer cases
were discovered, at a cost of losing three previously
correct answers. The percentage correct score in-
creased by a relative 10.3% for AQUAINT and
186% for CNS. In both cases, the error rate was
reduced by about a third.
</bodyText>
<subsectionHeader confidence="0.939088">
4.2 Evaluation 2
</subsectionHeader>
<bodyText confidence="0.999946433333333">
For the second evaluation, we processed the 414
factoid questions from TREC12. Of special interest
here are the questions initially in first and second
places, and in addition any questions for which nils
were found.
As seen in Table 1, there were 32 questions which
originally evaluated in rank 2. Of these, four ques-
tions were not invertible because they had no terms
that were annotated with any of our named-entity
types, e.g. #2285 “How much does it cost for gas-
tric bypass surgery?”
Of the remaining 28 questions, 12 were promoted to
first place. In addition, two new nils were found.
On the down side, four out of 108 previous first
place answers were lost. There was of course
movement in the ranks two and beyond whenever
nils were introduced in first place, but these do not
affect the current TREC-QA factoid correctness
measure, which is whether the top answer is correct
or not. These results are summarized in Table 3.
While the overall percentage improvement was
small, note that only second–place answers were
candidates for re-ranking, and 43% of these were
promoted to first place and hence judged correct.
Only 3.7% of originally correct questions were
casualties. To the extent that these percentages are
stable across other collections, as long as the size of
the set of second-place answers is at least about 1/10
of the set of first-place answers, this form of the
Constraint process can be applied effectively.
</bodyText>
<table confidence="0.998542">
Baseline Constraints
Firsts (non-nil) 105 113
nils 3 5
Total firsts 108 118
% correct 26.1 28.5
</table>
<tableCaption confidence="0.999795">
Table 3. Evaluation on TREC12 Factoids.
</tableCaption>
<sectionHeader confidence="0.897862" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999975903225807">
The experiments reported here pointed out many
areas of our system which previous failure analysis
of the basic QA system had not pinpointed as being
too problematic, but for which improvement should
help the Constraints process. In particular, this work
brought to light a matter of major significance, term
equivalence, which we had not previously focused
on too much (and neither had the QA community as
a whole). We will discuss that in Section 5.4.
Quantitatively, the results are very encouraging, but
it must be said that the number of questions that we
evaluated were rather small, as a result of the com-
putational expense of the approach.
From Table 1, we conclude that the most mileage is
to be achieved by our QA-System as a whole by ad-
dressing those questions which did not generate a
correct answer in the first one or two positions. We
have performed previous analyses of our system’s
failure modes, and have determined that the pas-
sages that are output from the SEARCH component
contain the correct answer 70-75% of the time. The
ANSWER SELECTION module takes these passages
and proposes a candidate answer list. Since the CON-
STRAINTS MODULE’s operation can be viewed as a
re-ranking of the output of ANSWER SELECTION, it
could in principle boost the system’s accuracy up to
that 70-75% level. However, this would either re-
quire a massive training set to establish all the pa-
rameters and weights required for all the possible re-
ranking decisions, or a new model of the answer-list
distribution.
</bodyText>
<subsectionHeader confidence="0.970529">
5.1 Probability-based Scores
</subsectionHeader>
<bodyText confidence="0.978085666666667">
Our ANSWER SELECTION component assigns scores
to candidate answers on the basis of the number of
terms and term-term syntactic relationships from the
</bodyText>
<page confidence="0.991826">
1078
</page>
<bodyText confidence="0.9999518">
original question found in the answer passage
(where the candidate answer and wh-word(s) in the
question are identified terms). The resulting num-
bers are in the range 0-1, but are not true probabili-
ties (e.g. where answers with a score of 0.7 would be
correct 70% of the time). While the generated
scores work well to rank candidates for a given
question, inter-question comparisons are not gener-
ally meaningful. This made the learning of a deci-
sion tree (Algorithm A) quite difficult, and we
expect that when addressed, will give better per-
formance to the Constraints process (and maybe a
simpler algorithm). This in turn will make it more
feasible to re-rank the top 10 (say) original answers,
instead of the current 2.
</bodyText>
<subsectionHeader confidence="0.995264">
5.2 Better confidences
</subsectionHeader>
<bodyText confidence="0.99999575">
Even if no changes to the ranking are produced by
the Constraints process, then the mere act of valida-
tion (or not) of existing answers can be used to ad-
just confidence scores. In TREC2002 (Voorhees,
2003), there was an evaluation of responses accord-
ing to systems’ confidences in their own answers,
using the Average Precision (AP) metric. This is an
important consideration, since it is generally better
for a system to say “I don’t know” than to give a
wrong answer. On the TREC12 questions set, our
AP score increased 2.1% with Constraints, using the
algorithm we presented in (Chu-Carroll et al. 2002).
</bodyText>
<subsectionHeader confidence="0.994559">
5.3 More complete NER
</subsectionHeader>
<bodyText confidence="0.999903263157895">
Except in pure pattern-based approaches, e.g. (Brill,
2002), answer types in QA systems typically corre-
spond to the types identifiable by their named-entity
recognizer (NER). There is no agreed-upon number
of classes for an NER system, even approximately.
It turns out that for best coverage by our
CONSTRAINTS MODULE, it is advantageous to have a
relatively large number of types. It was mentioned
in Section 4.2 that certain questions were not invert-
ible because no terms in them were of a recogniz-
able type. Even when questions did have typed
terms, if the types were very high-level then creating
a meaningful inverted question was problematic.
For example, for QA without Constraints it is not
necessary to know the type of “MTV” in “When
was MTV started?”, but if it is only known to be a
Name then the inverted question “What &lt;Name&gt;
was started in 1980?” could be too general to be ef-
fective.
</bodyText>
<subsectionHeader confidence="0.990981">
5.4 Establishing Term Equivalence
</subsectionHeader>
<bodyText confidence="0.999984705882353">
The somewhat surprising condition that emerged
from this effort was the need for a much more com-
plete ability than had previously been recognized for
the system to establish the equivalence of two terms.
Redundancy has always played a large role in QA
systems – the more occurrences of a candidate an-
swer in retrieved passages the higher the answer’s
score is made to be. Consequently, at the very least,
a string-matching operation is needed for checking
equivalence, but other techniques are used to vary-
ing degrees.
It has long been known in IR that stemming or lem-
matization is required for successful term matching,
and in NLP applications such as QA, resources such
as WordNet (Miller, 1995) are employed for check-
ing synonym and hypernym relationships; Extended
WordNet (Moldovan &amp; Novischi, 2002) has been
used to establish lexical chains between terms.
However, the Constraints work reported here has
highlighted the need for more extensive equivalence
testing.
In direct QA, when an ANSWER SELECTION module
generates two (or more) equivalent correct answers
to a question (e.g. “Ferdinand Marcos” vs. “Presi-
dent Marcos”; “French” vs. “France”), and fails to
combine them, it is observed that as long as either
one is in first place then the question is correct and
might not attract more attention from developers. It
is only when neither is initially in first place, but
combining the scores of correct candidates boosts
one to first place that the failure to merge them is
relevant. However, in the context of our system, we
are comparing the pivot term from the original ques-
tion to the answers to the inverted questions, and
failure here will directly impact validation and hence
the usefulness of the entire approach.
As a consequence, we have identified the need for a
component whose sole purpose is to establish the
equivalence, or generally the kind of relationship,
between two terms. It is clear that the processing
will be very type-dependent – for example, if two
populations are being compared, then a numerical
difference of 5% (say) might not be considered a
difference at all; for “Where” questions, there are
issues of granularity and physical proximity, and so
on. More examples of this problem were given in
(Prager et al. 2004a). Moriceau (2006) reports a
system that addresses part of this problem by trying
to rationalize different but “similar” answers to the
user, but does not extend to a general-purpose
equivalence identifier.
</bodyText>
<sectionHeader confidence="0.999183" genericHeader="method">
6 Summary
</sectionHeader>
<bodyText confidence="0.9999846">
We have extended earlier Constraints-based work
through the method of question inversion. The ap-
proach uses our QA system recursively, by taking
candidate answers and attempts to validate them
through asking the inverted questions. The outcome
</bodyText>
<page confidence="0.991024">
1079
</page>
<bodyText confidence="0.99990462962963">
is a re-ranking of the candidate answers, with the
possible insertion of nil (no answer in corpus) as the
top answer.
While we believe the approach is general, and can
work on any question and arbitrary candidate lists,
due to training limitations we focused on two re-
stricted evaluations. In the first we used a fixed
question type, and showed that the error rate was
reduced by 36% and 30% on two very different cor-
pora. In the second evaluation we focused on ques-
tions whose direct answers were correct in the
second position. 43% of these questions were sub-
sequently judged correct, at a cost of only 3.7% of
originally correct questions. While in the future we
would like to extend the Constraints process to the
entire answer candidate list, we have shown that ap-
plying it only to the top two can be beneficial as
long as the second-place answers are at least a tenth
as numerous as first-place answers. We also showed
that the application of Constraints can improve the
system’s confidence in its answers.
We have identified several areas where improve-
ment to our system would make the Constraints
process more effective, thus getting a double benefit.
In particular we feel that much more attention
should be paid to the problem of determining if two
entities are the same (or “close enough”).
</bodyText>
<sectionHeader confidence="0.999176" genericHeader="conclusions">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99987">
This work was supported in part by the Disruptive
Technology Office (DTO)’s Advanced Question
Answering for Intelligence (AQUAINT) Program
under contract number H98230-04-C-1577. We
would like to thank the anonymous reviewers
for their helpful comments.
</bodyText>
<sectionHeader confidence="0.999304" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99652993220339">
Brill, E., Dumais, S. and Banko M. “An analysis of
the AskMSR question-answering system.” In Pro-
ceedings of EMNLP 2002.
Chu-Carroll, J., J. Prager, C. Welty, K. Czuba and
D. Ferrucci. “A Multi-Strategy and Multi-Source
Approach to Question Answering”, Proceedings
of the 11th TREC, 2003.
Clarke, C., Cormack, G., Kisman, D. and Lynam, T.
“Question answering by passage selection
(Multitext experiments for TREC-9)” in Proceed-
ings of the 9th TREC, pp. 673-683, 2001.
Hendrix, G., Sacerdoti, E., Sagalowicz, D., Slocum
J.: Developing a Natural Language Interface to
Complex Data. VLDB 1977: 292
Lenat, D. 1995. &amp;quot;Cyc: A Large-Scale Investment in
Knowledge Infrastructure.&amp;quot; Communications of
the ACM 38, no. 11.
Miller, G. “WordNet: A Lexical Database for Eng-
lish”, Communications of the ACM 38(11) pp.
39-41, 1995.
Moldovan, D. and Novischi, A, “Lexical Chains for
Question Answering”, COLING 2002.
Moldovan, D. and Rus, V., “Logic Form Transfor-
mation of WordNet and its Applicability to Ques-
tion Answering”, Proceedings of the ACL, 2001.
Moriceau, V. “Numerical Data Integration for Co-
operative Question-Answering”, in EACL Work-
shop on Knowledge and Reasoning for Language
Processing (KRAQ’06), Trento, Italy, 2006.
Prager, J.M., Chu-Carroll, J. and Czuba, K. &amp;quot;Ques-
tion Answering using Constraint Satisfaction:
QA-by-Dossier-with-Constraints&amp;quot;, Proc. 42nd
ACL, pp. 575-582, Barcelona, Spain, 2004(a).
Prager, J.M., Chu-Carroll, J. and Czuba, K. &amp;quot;A
Multi-Strategy, Multi-Question Approach to
Question Answering&amp;quot; in New Directions in Ques-
tion-Answering, Maybury, M. (Ed.), AAAI Press,
2004(b).
Prager, J., &amp;quot;A Curriculum-Based Approach to a QA
Roadmap&amp;quot;&apos; LREC 2002 Workshop on Question
Answering: Strategy and Resources, Las Palmas,
May 2002.
Radev, D., Prager, J. and Samn, V. &amp;quot;Ranking Sus-
pected Answers to Natural Language Questions
using Predictive Annotation&amp;quot;, Proceedings of
ANLP 2000, pp. 150-157, Seattle, WA.
Voorhees, E. “Overview of the TREC 2002 Ques-
tion Answering Track”, Proceedings of the 11th
TREC, Gaithersburg, MD, 2003.
Warren, D., and F. Pereira &amp;quot;An efficient easily
adaptable system for interpreting natural language
queries,&amp;quot; Computational Linguistics, 8:3-4, 110-
122, 1982.
Winograd, T. Procedures as a representation for data
in a computer program for under-standing natural
language. Cognitive Psychology, 3(1), 1972.
Witten, I.H. &amp; Frank, E. Data Mining. Practical
Machine Learning Tools and Techniques. El-
sevier Press, 2005.
</reference>
<page confidence="0.993736">
1080
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.562273">
<title confidence="0.999823">Improving QA Accuracy by Question Inversion</title>
<author confidence="0.999907">John Prager</author>
<affiliation confidence="0.92217">IBM T.J. Watson Res. Ctr. Yorktown Heights</affiliation>
<address confidence="0.952529">N.Y. 10598</address>
<email confidence="0.999231">jprager@us.ibm.com</email>
<author confidence="0.999621">Pablo Duboue</author>
<affiliation confidence="0.928466">IBM T.J. Watson Res. Ctr. Yorktown Heights</affiliation>
<address confidence="0.956409">N.Y. 10598</address>
<email confidence="0.99849">duboue@us.ibm.com</email>
<author confidence="0.9994">Jennifer Chu-Carroll</author>
<affiliation confidence="0.92581">IBM T.J. Watson Res. Ctr. Yorktown Heights</affiliation>
<address confidence="0.952528">N.Y. 10598</address>
<email confidence="0.999662">jencc@us.ibm.com</email>
<abstract confidence="0.998583266666667">This paper demonstrates a conceptually simple but effective method of increasing the accuracy of QA systems on factoid-style questions. We define the notion of an inverted question, and show that by requiring that the answers to the original and inverted questions be mutually consistent, incorrect answers get demoted in confidence and correct ones promoted. Additionally, we show that lack of validation can be used to no-answer conditions. We demonstrate increases of performance on TREC and other question-sets, and discuss the kinds of future activities that can be particularly beneficial to approaches such as ours.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>S Dumais</author>
<author>M Banko</author>
</authors>
<title>An analysis of the AskMSR question-answering system.”</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Brill, Dumais, Banko, 2002</marker>
<rawString>Brill, E., Dumais, S. and Banko M. “An analysis of the AskMSR question-answering system.” In Proceedings of EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chu-Carroll</author>
<author>J Prager</author>
<author>C Welty</author>
<author>K Czuba</author>
<author>D Ferrucci</author>
</authors>
<title>A Multi-Strategy and Multi-Source Approach to Question Answering”,</title>
<date>2003</date>
<booktitle>Proceedings of the 11th TREC,</booktitle>
<marker>Chu-Carroll, Prager, Welty, Czuba, Ferrucci, 2003</marker>
<rawString>Chu-Carroll, J., J. Prager, C. Welty, K. Czuba and D. Ferrucci. “A Multi-Strategy and Multi-Source Approach to Question Answering”, Proceedings of the 11th TREC, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Clarke</author>
<author>G Cormack</author>
<author>D Kisman</author>
<author>T Lynam</author>
</authors>
<title>Question answering by passage selection (Multitext experiments for TREC-9)”</title>
<date>2001</date>
<booktitle>in Proceedings of the 9th TREC,</booktitle>
<pages>673--683</pages>
<contexts>
<context position="1547" citStr="Clarke et al. 2001" startWordPosition="246" endWordPosition="249">roaches such as ours. 1 Introduction Most QA systems nowadays consist of the following standard modules: QUESTION PROCESSING, to determine the bag of words for a query and the desired answer type (the type of the entity that will be offered as a candidate answer); SEARCH, which will use the query to extract a set of documents or passages from a corpus; and ANSWER SELECTION, which will analyze the returned documents or passages for instances of the answer type in the most favorable contexts. Each of these components implements a set of heuristics or hypotheses, as devised by their authors (cf. Clarke et al. 2001, ChuCarroll et al. 2003). When we perform failure analysis on questions incorrectly answered by our system, we find that there are broadly speaking two kinds of failure. There are errors (we might call them bugs) on the implementation of the said heuristics: errors in tagging, parsing, named-entity recognition; omissions in synonym lists; missing patterns, and just plain programming errors. This class can be characterized by being fixable by identifying incorrect code and fixing it, or adding more items, either explicitly or through training. The other class of errors (what we might call unlu</context>
<context position="7846" citStr="Clarke et al., 2001" startWordPosition="1299" endWordPosition="1302">ealworld knowledge being provided by Extended WordNet. The IBM system PIQUANT (ChuCarroll et al., 2003) used Cyc (Lenat, 1995) in answer verification. Cyc can in some cases confirm or reject candidate answers based on its own store of instance information; in other cases, primarily of a numerical nature, Cyc can confirm whether candidates are within a reasonable range established for their subtype. At a more abstract level, the use of inversions discussed in this paper can be viewed as simply an example of finding support (or lack of it) for candidate answers. Many current systems (see, e.g. (Clarke et al., 2001; Prager et al. 2004b)) employ redundancy as a significant feature of operation: if the same answer appears multiple times in an internal top-n list, whether from multiple sources or multiple algorithms/agents, it is given a confidence boost, which will affect whether and how it gets returned to the end-user. The work here is a continuation of previous work described in (Prager et al. 2004a,b). In the former we demonstrated that for a certain kind of question, if the inverted question were given, we could improve the F-measure of accuracy on a question set by 75%. In this paper, by contrast, w</context>
</contexts>
<marker>Clarke, Cormack, Kisman, Lynam, 2001</marker>
<rawString>Clarke, C., Cormack, G., Kisman, D. and Lynam, T. “Question answering by passage selection (Multitext experiments for TREC-9)” in Proceedings of the 9th TREC, pp. 673-683, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hendrix</author>
<author>E Sacerdoti</author>
<author>D Sagalowicz</author>
<author>J Slocum</author>
</authors>
<title>Developing a Natural Language Interface to Complex Data. VLDB</title>
<date>1977</date>
<pages>292</pages>
<contexts>
<context position="6311" citStr="Hendrix et al. 1977" startWordPosition="1048" endWordPosition="1051">demonstrating how meta-heuristics, using what might be called “collateral” information, can be used to constrain or adjust the results of the primary QA system. In the next Section we review related work. In Section 3 we describe our algorithm in detail, and in Section 4 present evaluation results. In Section 5 we discuss our conclusions and future work. 2 Related Work Logic and inferencing have been a part of QuestionAnswering since its earliest days. The first such systems were natural-language interfaces to expert systems, e.g., SHRDLU (Winograd, 1972), or to databases, e.g., LIFER/LADDER (Hendrix et al. 1977). CHAT-80 (Warren &amp; Pereira, 1982), for instance, was a DCG-based NL-query system about world geography, entirely in Prolog. In these systems, the NL question is transformed into a semantic form, which is then processed further. Their overall architecture and system operation is very different from today’s systems, however, primarily in that there was no text corpus to process. Inferencing is a core requirement of systems that participate in the current PASCAL Recognizing Textual Entailment (RTE) challenge (see http://www.pascal-network.org/Challenges/RTE and .../RTE2). It is also used in at l</context>
</contexts>
<marker>Hendrix, Sacerdoti, Sagalowicz, Slocum, 1977</marker>
<rawString>Hendrix, G., Sacerdoti, E., Sagalowicz, D., Slocum J.: Developing a Natural Language Interface to Complex Data. VLDB 1977: 292</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lenat</author>
</authors>
<title>Cyc: A Large-Scale Investment in Knowledge Infrastructure.&amp;quot;</title>
<date>1995</date>
<journal>Communications of the ACM</journal>
<volume>38</volume>
<pages>11</pages>
<contexts>
<context position="7353" citStr="Lenat, 1995" startWordPosition="1215" endWordPosition="1216"> participate in the current PASCAL Recognizing Textual Entailment (RTE) challenge (see http://www.pascal-network.org/Challenges/RTE and .../RTE2). It is also used in at least two of the more visible end-to-end QA systems of the present day. The LCC system (Moldovan &amp; Rus, 2001) uses a Logic Prover to establish the connection between a candidate answer passage and the question. Text terms are converted to logical forms, and the question is treated as a goal which is “proven”, with realworld knowledge being provided by Extended WordNet. The IBM system PIQUANT (ChuCarroll et al., 2003) used Cyc (Lenat, 1995) in answer verification. Cyc can in some cases confirm or reject candidate answers based on its own store of instance information; in other cases, primarily of a numerical nature, Cyc can confirm whether candidates are within a reasonable range established for their subtype. At a more abstract level, the use of inversions discussed in this paper can be viewed as simply an example of finding support (or lack of it) for candidate answers. Many current systems (see, e.g. (Clarke et al., 2001; Prager et al. 2004b)) employ redundancy as a significant feature of operation: if the same answer appears</context>
</contexts>
<marker>Lenat, 1995</marker>
<rawString>Lenat, D. 1995. &amp;quot;Cyc: A Large-Scale Investment in Knowledge Infrastructure.&amp;quot; Communications of the ACM 38, no. 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>WordNet: A Lexical Database for English”,</title>
<date>1995</date>
<journal>Communications of the ACM</journal>
<volume>38</volume>
<issue>11</issue>
<pages>39--41</pages>
<contexts>
<context position="28047" citStr="Miller, 1995" startWordPosition="4798" endWordPosition="4799">uch more complete ability than had previously been recognized for the system to establish the equivalence of two terms. Redundancy has always played a large role in QA systems – the more occurrences of a candidate answer in retrieved passages the higher the answer’s score is made to be. Consequently, at the very least, a string-matching operation is needed for checking equivalence, but other techniques are used to varying degrees. It has long been known in IR that stemming or lemmatization is required for successful term matching, and in NLP applications such as QA, resources such as WordNet (Miller, 1995) are employed for checking synonym and hypernym relationships; Extended WordNet (Moldovan &amp; Novischi, 2002) has been used to establish lexical chains between terms. However, the Constraints work reported here has highlighted the need for more extensive equivalence testing. In direct QA, when an ANSWER SELECTION module generates two (or more) equivalent correct answers to a question (e.g. “Ferdinand Marcos” vs. “President Marcos”; “French” vs. “France”), and fails to combine them, it is observed that as long as either one is in first place then the question is correct and might not attract more</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>Miller, G. “WordNet: A Lexical Database for English”, Communications of the ACM 38(11) pp. 39-41, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>A Novischi</author>
</authors>
<title>Lexical Chains for Question Answering”,</title>
<date>2002</date>
<location>COLING</location>
<contexts>
<context position="28154" citStr="Moldovan &amp; Novischi, 2002" startWordPosition="4811" endWordPosition="4814">equivalence of two terms. Redundancy has always played a large role in QA systems – the more occurrences of a candidate answer in retrieved passages the higher the answer’s score is made to be. Consequently, at the very least, a string-matching operation is needed for checking equivalence, but other techniques are used to varying degrees. It has long been known in IR that stemming or lemmatization is required for successful term matching, and in NLP applications such as QA, resources such as WordNet (Miller, 1995) are employed for checking synonym and hypernym relationships; Extended WordNet (Moldovan &amp; Novischi, 2002) has been used to establish lexical chains between terms. However, the Constraints work reported here has highlighted the need for more extensive equivalence testing. In direct QA, when an ANSWER SELECTION module generates two (or more) equivalent correct answers to a question (e.g. “Ferdinand Marcos” vs. “President Marcos”; “French” vs. “France”), and fails to combine them, it is observed that as long as either one is in first place then the question is correct and might not attract more attention from developers. It is only when neither is initially in first place, but combining the scores o</context>
</contexts>
<marker>Moldovan, Novischi, 2002</marker>
<rawString>Moldovan, D. and Novischi, A, “Lexical Chains for Question Answering”, COLING 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>V Rus</author>
</authors>
<title>Logic Form Transformation of WordNet and its Applicability to Question Answering”,</title>
<date>2001</date>
<booktitle>Proceedings of the ACL,</booktitle>
<contexts>
<context position="2841" citStr="Moldovan &amp; Rus, 2001" startWordPosition="460" endWordPosition="463">did not do anything “wrong,” in the sense of bug, but circumstances conspired against finding the correct answer. Usually when unlucky errors occur, the system generates a reasonable query and an appropriate answer type, and at least one passage containing the right answer is returned. However, there may be returned passages that have a larger number of query terms and an incorrect answer of the right type, or the query terms might just be physically closer to the incorrect answer than to the correct one. ANSWER SELECTION modules typically work either by trying to prove the answer is correct (Moldovan &amp; Rus, 2001) or by giving them a weight produced by summing a collection of heuristic features (Radev et al., 2000); in the latter case candidates having a larger number of matching query terms, even if they do not exactly match the context in the question, might generate a larger score than a correct passage with fewer matching terms. To be sure, unlucky errors are usually bugs when considered from the standpoint of a system with a more sophisticated heuristic, but any system at any point in time will have limits on what it tries to do; therefore the distinction is not absolute but is relative to a heuri</context>
<context position="7019" citStr="Moldovan &amp; Rus, 2001" startWordPosition="1156" endWordPosition="1159"> world geography, entirely in Prolog. In these systems, the NL question is transformed into a semantic form, which is then processed further. Their overall architecture and system operation is very different from today’s systems, however, primarily in that there was no text corpus to process. Inferencing is a core requirement of systems that participate in the current PASCAL Recognizing Textual Entailment (RTE) challenge (see http://www.pascal-network.org/Challenges/RTE and .../RTE2). It is also used in at least two of the more visible end-to-end QA systems of the present day. The LCC system (Moldovan &amp; Rus, 2001) uses a Logic Prover to establish the connection between a candidate answer passage and the question. Text terms are converted to logical forms, and the question is treated as a goal which is “proven”, with realworld knowledge being provided by Extended WordNet. The IBM system PIQUANT (ChuCarroll et al., 2003) used Cyc (Lenat, 1995) in answer verification. Cyc can in some cases confirm or reject candidate answers based on its own store of instance information; in other cases, primarily of a numerical nature, Cyc can confirm whether candidates are within a reasonable range established for their</context>
</contexts>
<marker>Moldovan, Rus, 2001</marker>
<rawString>Moldovan, D. and Rus, V., “Logic Form Transformation of WordNet and its Applicability to Question Answering”, Proceedings of the ACL, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Moriceau</author>
</authors>
<title>Numerical Data Integration for Cooperative Question-Answering”,</title>
<date>2006</date>
<booktitle>in EACL Workshop on Knowledge and Reasoning for Language Processing (KRAQ’06),</booktitle>
<location>Trento, Italy,</location>
<contexts>
<context position="29626" citStr="Moriceau (2006)" startWordPosition="5056" endWordPosition="5057">ctly impact validation and hence the usefulness of the entire approach. As a consequence, we have identified the need for a component whose sole purpose is to establish the equivalence, or generally the kind of relationship, between two terms. It is clear that the processing will be very type-dependent – for example, if two populations are being compared, then a numerical difference of 5% (say) might not be considered a difference at all; for “Where” questions, there are issues of granularity and physical proximity, and so on. More examples of this problem were given in (Prager et al. 2004a). Moriceau (2006) reports a system that addresses part of this problem by trying to rationalize different but “similar” answers to the user, but does not extend to a general-purpose equivalence identifier. 6 Summary We have extended earlier Constraints-based work through the method of question inversion. The approach uses our QA system recursively, by taking candidate answers and attempts to validate them through asking the inverted questions. The outcome 1079 is a re-ranking of the candidate answers, with the possible insertion of nil (no answer in corpus) as the top answer. While we believe the approach is g</context>
</contexts>
<marker>Moriceau, 2006</marker>
<rawString>Moriceau, V. “Numerical Data Integration for Cooperative Question-Answering”, in EACL Workshop on Knowledge and Reasoning for Language Processing (KRAQ’06), Trento, Italy, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Prager</author>
<author>J Chu-Carroll</author>
<author>K Czuba</author>
</authors>
<title>Question Answering using Constraint Satisfaction: QA-by-Dossier-with-Constraints&amp;quot;,</title>
<date>2004</date>
<booktitle>Proc. 42nd ACL,</booktitle>
<pages>575--582</pages>
<location>Barcelona,</location>
<contexts>
<context position="4723" citStr="Prager et al. 2004" startWordPosition="782" endWordPosition="785">as the correct answer context. This would lead us to believe that if a 1 We borrow the term from multiple-choice test design. 1073 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1073–1080, Sydney, July 2006. c�2006 Association for Computational Linguistics different corpus containing the correct answer were to be processed, while there would be no guarantee that the correct answer would be found, it would be unlikely (i.e. very unlucky) if the same incorrect answer as before were returned. We have demonstrated elsewhere (Prager et al. 2004b) how using multiple corpora can improve QA performance, but in this paper we achieve similar goals without using additional corpora. We note that factoid questions are usually about relations between entities, e.g. “What is the capital of France?”, where one of the arguments of the relationship is sought and the others given. We can invert the question by substituting the candidate answer back into the question, while making one of the given entities the socalled wh-word, thus “Of what country is Paris the capital?” We hypothesize that asking this question (and those formed from other candid</context>
<context position="7866" citStr="Prager et al. 2004" startWordPosition="1303" endWordPosition="1306">ing provided by Extended WordNet. The IBM system PIQUANT (ChuCarroll et al., 2003) used Cyc (Lenat, 1995) in answer verification. Cyc can in some cases confirm or reject candidate answers based on its own store of instance information; in other cases, primarily of a numerical nature, Cyc can confirm whether candidates are within a reasonable range established for their subtype. At a more abstract level, the use of inversions discussed in this paper can be viewed as simply an example of finding support (or lack of it) for candidate answers. Many current systems (see, e.g. (Clarke et al., 2001; Prager et al. 2004b)) employ redundancy as a significant feature of operation: if the same answer appears multiple times in an internal top-n list, whether from multiple sources or multiple algorithms/agents, it is given a confidence boost, which will affect whether and how it gets returned to the end-user. The work here is a continuation of previous work described in (Prager et al. 2004a,b). In the former we demonstrated that for a certain kind of question, if the inverted question were given, we could improve the F-measure of accuracy on a question set by 75%. In this paper, by contrast, we do not manually pr</context>
<context position="11971" citStr="Prager et al. 2004" startWordPosition="2003" endWordPosition="2006">o reparse. However, the examples given above were simply English renditions of internal inverted structures – as we shall see the system does not need to use a natural language representation of the inverted questions. Some questions are either not invertible, or, like “How did X die?” have an inverted form (“Who died of cancer?”) with so many correct answers that we know our algorithm is unlikely to benefit us. However, as it is constituted it is unlikely to hurt us either, and since it is difficult to automatically identify such questions, we don’t attempt to intercept them. As reported in (Prager et al. 2004a), an estimated 79% of the questions in TREC question sets can be inverted meaningfully. This places an upper limit on the gains to be achieved with our algorithm, but is high enough to be worth pursuing. 1075 3.3 Inversion Algorithm As shown in the previous section, not all questions have easily generated inverted forms (even by a human). However, we do not need to explicate the inverted form in natural language in order to process the inverted question. In our system, a question is processed by the QUESTION PROCESSING module, which produces a structure called a QFrame, which is used by the </context>
<context position="29607" citStr="Prager et al. 2004" startWordPosition="5052" endWordPosition="5055">failure here will directly impact validation and hence the usefulness of the entire approach. As a consequence, we have identified the need for a component whose sole purpose is to establish the equivalence, or generally the kind of relationship, between two terms. It is clear that the processing will be very type-dependent – for example, if two populations are being compared, then a numerical difference of 5% (say) might not be considered a difference at all; for “Where” questions, there are issues of granularity and physical proximity, and so on. More examples of this problem were given in (Prager et al. 2004a). Moriceau (2006) reports a system that addresses part of this problem by trying to rationalize different but “similar” answers to the user, but does not extend to a general-purpose equivalence identifier. 6 Summary We have extended earlier Constraints-based work through the method of question inversion. The approach uses our QA system recursively, by taking candidate answers and attempts to validate them through asking the inverted questions. The outcome 1079 is a re-ranking of the candidate answers, with the possible insertion of nil (no answer in corpus) as the top answer. While we believ</context>
</contexts>
<marker>Prager, Chu-Carroll, Czuba, 2004</marker>
<rawString>Prager, J.M., Chu-Carroll, J. and Czuba, K. &amp;quot;Question Answering using Constraint Satisfaction: QA-by-Dossier-with-Constraints&amp;quot;, Proc. 42nd ACL, pp. 575-582, Barcelona, Spain, 2004(a).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Prager</author>
<author>J Chu-Carroll</author>
<author>K Czuba</author>
</authors>
<title>A Multi-Strategy, Multi-Question Approach to Question Answering&amp;quot;</title>
<date>2004</date>
<publisher>AAAI Press,</publisher>
<note>in New Directions in</note>
<contexts>
<context position="4723" citStr="Prager et al. 2004" startWordPosition="782" endWordPosition="785">as the correct answer context. This would lead us to believe that if a 1 We borrow the term from multiple-choice test design. 1073 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1073–1080, Sydney, July 2006. c�2006 Association for Computational Linguistics different corpus containing the correct answer were to be processed, while there would be no guarantee that the correct answer would be found, it would be unlikely (i.e. very unlucky) if the same incorrect answer as before were returned. We have demonstrated elsewhere (Prager et al. 2004b) how using multiple corpora can improve QA performance, but in this paper we achieve similar goals without using additional corpora. We note that factoid questions are usually about relations between entities, e.g. “What is the capital of France?”, where one of the arguments of the relationship is sought and the others given. We can invert the question by substituting the candidate answer back into the question, while making one of the given entities the socalled wh-word, thus “Of what country is Paris the capital?” We hypothesize that asking this question (and those formed from other candid</context>
<context position="7866" citStr="Prager et al. 2004" startWordPosition="1303" endWordPosition="1306">ing provided by Extended WordNet. The IBM system PIQUANT (ChuCarroll et al., 2003) used Cyc (Lenat, 1995) in answer verification. Cyc can in some cases confirm or reject candidate answers based on its own store of instance information; in other cases, primarily of a numerical nature, Cyc can confirm whether candidates are within a reasonable range established for their subtype. At a more abstract level, the use of inversions discussed in this paper can be viewed as simply an example of finding support (or lack of it) for candidate answers. Many current systems (see, e.g. (Clarke et al., 2001; Prager et al. 2004b)) employ redundancy as a significant feature of operation: if the same answer appears multiple times in an internal top-n list, whether from multiple sources or multiple algorithms/agents, it is given a confidence boost, which will affect whether and how it gets returned to the end-user. The work here is a continuation of previous work described in (Prager et al. 2004a,b). In the former we demonstrated that for a certain kind of question, if the inverted question were given, we could improve the F-measure of accuracy on a question set by 75%. In this paper, by contrast, we do not manually pr</context>
<context position="11971" citStr="Prager et al. 2004" startWordPosition="2003" endWordPosition="2006">o reparse. However, the examples given above were simply English renditions of internal inverted structures – as we shall see the system does not need to use a natural language representation of the inverted questions. Some questions are either not invertible, or, like “How did X die?” have an inverted form (“Who died of cancer?”) with so many correct answers that we know our algorithm is unlikely to benefit us. However, as it is constituted it is unlikely to hurt us either, and since it is difficult to automatically identify such questions, we don’t attempt to intercept them. As reported in (Prager et al. 2004a), an estimated 79% of the questions in TREC question sets can be inverted meaningfully. This places an upper limit on the gains to be achieved with our algorithm, but is high enough to be worth pursuing. 1075 3.3 Inversion Algorithm As shown in the previous section, not all questions have easily generated inverted forms (even by a human). However, we do not need to explicate the inverted form in natural language in order to process the inverted question. In our system, a question is processed by the QUESTION PROCESSING module, which produces a structure called a QFrame, which is used by the </context>
<context position="29607" citStr="Prager et al. 2004" startWordPosition="5052" endWordPosition="5055">failure here will directly impact validation and hence the usefulness of the entire approach. As a consequence, we have identified the need for a component whose sole purpose is to establish the equivalence, or generally the kind of relationship, between two terms. It is clear that the processing will be very type-dependent – for example, if two populations are being compared, then a numerical difference of 5% (say) might not be considered a difference at all; for “Where” questions, there are issues of granularity and physical proximity, and so on. More examples of this problem were given in (Prager et al. 2004a). Moriceau (2006) reports a system that addresses part of this problem by trying to rationalize different but “similar” answers to the user, but does not extend to a general-purpose equivalence identifier. 6 Summary We have extended earlier Constraints-based work through the method of question inversion. The approach uses our QA system recursively, by taking candidate answers and attempts to validate them through asking the inverted questions. The outcome 1079 is a re-ranking of the candidate answers, with the possible insertion of nil (no answer in corpus) as the top answer. While we believ</context>
</contexts>
<marker>Prager, Chu-Carroll, Czuba, 2004</marker>
<rawString>Prager, J.M., Chu-Carroll, J. and Czuba, K. &amp;quot;A Multi-Strategy, Multi-Question Approach to Question Answering&amp;quot; in New Directions in Question-Answering, Maybury, M. (Ed.), AAAI Press, 2004(b).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
</authors>
<title>A Curriculum-Based Approach to a QA Roadmap&amp;quot;&apos;</title>
<date>2002</date>
<booktitle>LREC 2002 Workshop on Question Answering: Strategy and Resources,</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="3491" citStr="Prager, 2002" startWordPosition="578" endWordPosition="579">by summing a collection of heuristic features (Radev et al., 2000); in the latter case candidates having a larger number of matching query terms, even if they do not exactly match the context in the question, might generate a larger score than a correct passage with fewer matching terms. To be sure, unlucky errors are usually bugs when considered from the standpoint of a system with a more sophisticated heuristic, but any system at any point in time will have limits on what it tries to do; therefore the distinction is not absolute but is relative to a heuristic and system. It has been argued (Prager, 2002) that the success of a QA system is proportional to the impedance match between the question and the knowledge sources available. We argue here similarly. Moreover, we believe that this is true not only in terms of the correct answer, but the distracters,1 or incorrect answers too. In QA, an unlucky incorrect answer is not usually predictable in advance; it occurs because of a coincidence of terms and syntactic contexts that cause it to be preferred over the correct answer. It has no connection with the correct answer and is only returned because its enclosing passage so happens to exist in th</context>
</contexts>
<marker>Prager, 2002</marker>
<rawString>Prager, J., &amp;quot;A Curriculum-Based Approach to a QA Roadmap&amp;quot;&apos; LREC 2002 Workshop on Question Answering: Strategy and Resources, Las Palmas, May 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>J Prager</author>
<author>V Samn</author>
</authors>
<title>Ranking Suspected Answers to Natural Language Questions using Predictive Annotation&amp;quot;,</title>
<date>2000</date>
<booktitle>Proceedings of ANLP</booktitle>
<pages>150--157</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="2944" citStr="Radev et al., 2000" startWordPosition="478" endWordPosition="481"> answer. Usually when unlucky errors occur, the system generates a reasonable query and an appropriate answer type, and at least one passage containing the right answer is returned. However, there may be returned passages that have a larger number of query terms and an incorrect answer of the right type, or the query terms might just be physically closer to the incorrect answer than to the correct one. ANSWER SELECTION modules typically work either by trying to prove the answer is correct (Moldovan &amp; Rus, 2001) or by giving them a weight produced by summing a collection of heuristic features (Radev et al., 2000); in the latter case candidates having a larger number of matching query terms, even if they do not exactly match the context in the question, might generate a larger score than a correct passage with fewer matching terms. To be sure, unlucky errors are usually bugs when considered from the standpoint of a system with a more sophisticated heuristic, but any system at any point in time will have limits on what it tries to do; therefore the distinction is not absolute but is relative to a heuristic and system. It has been argued (Prager, 2002) that the success of a QA system is proportional to t</context>
</contexts>
<marker>Radev, Prager, Samn, 2000</marker>
<rawString>Radev, D., Prager, J. and Samn, V. &amp;quot;Ranking Suspected Answers to Natural Language Questions using Predictive Annotation&amp;quot;, Proceedings of ANLP 2000, pp. 150-157, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2002</date>
<booktitle>Proceedings of the 11th TREC,</booktitle>
<location>Gaithersburg, MD,</location>
<marker>Voorhees, 2002</marker>
<rawString>Voorhees, E. “Overview of the TREC 2002 Question Answering Track”, Proceedings of the 11th TREC, Gaithersburg, MD, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Warren</author>
<author>F Pereira</author>
</authors>
<title>An efficient easily adaptable system for interpreting natural language queries,&amp;quot;</title>
<date>1982</date>
<journal>Computational Linguistics,</journal>
<volume>8</volume>
<pages>110--122</pages>
<contexts>
<context position="6345" citStr="Warren &amp; Pereira, 1982" startWordPosition="1053" endWordPosition="1056">cs, using what might be called “collateral” information, can be used to constrain or adjust the results of the primary QA system. In the next Section we review related work. In Section 3 we describe our algorithm in detail, and in Section 4 present evaluation results. In Section 5 we discuss our conclusions and future work. 2 Related Work Logic and inferencing have been a part of QuestionAnswering since its earliest days. The first such systems were natural-language interfaces to expert systems, e.g., SHRDLU (Winograd, 1972), or to databases, e.g., LIFER/LADDER (Hendrix et al. 1977). CHAT-80 (Warren &amp; Pereira, 1982), for instance, was a DCG-based NL-query system about world geography, entirely in Prolog. In these systems, the NL question is transformed into a semantic form, which is then processed further. Their overall architecture and system operation is very different from today’s systems, however, primarily in that there was no text corpus to process. Inferencing is a core requirement of systems that participate in the current PASCAL Recognizing Textual Entailment (RTE) challenge (see http://www.pascal-network.org/Challenges/RTE and .../RTE2). It is also used in at least two of the more visible end-t</context>
</contexts>
<marker>Warren, Pereira, 1982</marker>
<rawString>Warren, D., and F. Pereira &amp;quot;An efficient easily adaptable system for interpreting natural language queries,&amp;quot; Computational Linguistics, 8:3-4, 110-122, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Procedures as a representation for data in a computer program for under-standing natural language.</title>
<date>1972</date>
<journal>Cognitive Psychology,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="6252" citStr="Winograd, 1972" startWordPosition="1041" endWordPosition="1042">roposes. This work is part of a continuing program of demonstrating how meta-heuristics, using what might be called “collateral” information, can be used to constrain or adjust the results of the primary QA system. In the next Section we review related work. In Section 3 we describe our algorithm in detail, and in Section 4 present evaluation results. In Section 5 we discuss our conclusions and future work. 2 Related Work Logic and inferencing have been a part of QuestionAnswering since its earliest days. The first such systems were natural-language interfaces to expert systems, e.g., SHRDLU (Winograd, 1972), or to databases, e.g., LIFER/LADDER (Hendrix et al. 1977). CHAT-80 (Warren &amp; Pereira, 1982), for instance, was a DCG-based NL-query system about world geography, entirely in Prolog. In these systems, the NL question is transformed into a semantic form, which is then processed further. Their overall architecture and system operation is very different from today’s systems, however, primarily in that there was no text corpus to process. Inferencing is a core requirement of systems that participate in the current PASCAL Recognizing Textual Entailment (RTE) challenge (see http://www.pascal-networ</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Winograd, T. Procedures as a representation for data in a computer program for under-standing natural language. Cognitive Psychology, 3(1), 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining. Practical Machine Learning Tools and Techniques.</title>
<date>2005</date>
<publisher>Elsevier Press,</publisher>
<contexts>
<context position="18606" citStr="Witten &amp; Frank, 2005" startWordPosition="3143" endWordPosition="3146">ddition, this greatly reduced the number of parameters we needed to learn. For the evaluation, which consisted of determining if the resulting top answer was right or wrong, it meant ultimately deciding on one of three possible outcomes: the original top answer, the original second answer, or nil. We hoped to promote a significant number of second-place finishers to top place and introduce some nils, with minimal disturbance of those already in first place. We used TREC11 data for training, and established a set of thresholds for a decision-tree approach to determining the answer, using Weka (Witten &amp; Frank, 2005). We populated sets SOFTREFUTATION and MUSTCONSTRAIN by manual inspection. The result is Algorithm A, where (i E {1,2}) and o The Ci are the original candidate answers o The ak are learned parameters (k E {1..13}) o Vi means the ith answer was validated o Pi was the rank of the validating answer to question QTi o Ai was the score of the validating answer to QTi. Algorithm A. Answer re-ranking using constraints validation data. 1. If C1 = nil and V2, return C2 2. If V1 and A1 &gt; a1, return C1 3. If not V1 and not V2 and type(T) E MUSTCONSTRAIN, return nil 4. If not V1 and not V2 and type(T) ∉SOF</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Witten, I.H. &amp; Frank, E. Data Mining. Practical Machine Learning Tools and Techniques. Elsevier Press, 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>