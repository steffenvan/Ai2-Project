<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000032">
<title confidence="0.9905525">
The PARADISE Evaluation Framework:
Issues and Findings
</title>
<author confidence="0.999316">
Melita Hajdinjak* France Miheliˇc*
</author>
<affiliation confidence="0.999001">
University of Ljubljana University of Ljubljana
</affiliation>
<bodyText confidence="0.993127454545455">
There has been a great deal of interest over the past 20 years in developing metrics and
frameworks for evaluating and comparing the performance of spoken-language dialogue sys-
tems. One of the results of this interest is a potential general methodology, known as the
PARADISE framework. This squib highlights some important issues concerning the application
of PARADISE that have, up to now, not been sufficiently emphasized or have even been neglected
by the dialogue-system community. These include considerations regarding the selection of
appropriate regression parameters, normalization effects on the accuracy of the prediction, the
influence of speech-recognition errors on the performance function, and the selection of an
appropriate user-satisfaction measure. In addition, it gives the results of an evaluation of data
from two Wizard-of-Oz experiments. These evaluations include different dependent variables
and examination of individual user-satisfaction measures.
</bodyText>
<sectionHeader confidence="0.995161" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999956047619048">
A long list of objective dialogue metrics (Danieli and Gerbino 1995; Smith and Gordon
1997) for dialogue evaluation, which can be calculated without recourse to human judg-
ment, and subjective dialogue metrics (Shriberg, Wade, and Price 1992; Danieli and
Gerbino 1995), which are based on human judgments, have been proposed. Their well-
known limitations led Walker, Litman, Kamm, and Abella (1997) to propose their para-
digm for dialogue system evaluation (PARADISE), a potentially general methodology
for evaluating spoken-language dialogue systems, the goal of which was to compare
and optimize different dialogue managers and task domains independently.
The PARADISE framework maintains that the system’s primary objective is to
maximize user satisfaction (Shriberg, Wade, and Price 1992), and it derives a combined
performance metric for a dialogue system as a weighted linear combination of task-
success measures and dialogue costs. The dialogue costs are of two types: dialogue-
efficiency costs (e.g., number of utterances, dialogue time), which are measures of
the system’s efficiency in helping the user to complete the task, and dialogue-quality
costs (e.g., system-response delay, mean recognition score), which are intended to cap-
ture other aspects that can have large effects on the user’s perception of the system’s
performance.
Applying PARADISE to dialogue data requires dialogue corpora to be collected
via controlled experiments during which users subjectively rate their satisfaction. Here,
user satisfaction is calculated with a survey (Walker et al. 1998) that asks users to
specify the degree to which they agree with several statements about the performance
</bodyText>
<note confidence="0.685312">
* Faculty of Electrical Engineering, Trˇzaˇska 25, 1000 Ljubljana, Slovenia
© 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 2
</note>
<bodyText confidence="0.999935866666667">
of the system. In addition, the other parameters of the model of performance, i.e., the
task-success measures and the dialogue costs, must be either automatically logged by
the system or be hand-labeled. The PARADISE model of performance posits that a
performance function can then be derived by applying multivariate linear regression
(MLR) with user satisfaction as the dependent variable and task-success measures and
dialogue costs as the independent variables.
The squib addresses some PARADISE issues (with most of them arising from the
application of MLR) that have, up to now, not been sufficiently emphasized or have
even been neglected by the dialogue-system community. Moreover, most of the con-
siderations about these issues are supported by the results of applying PARADISE to
the data from two Wizard-of-Oz (WOZ) experiments (Hajdinjak and Miheliˇc 2004a)
carried out during the development of a weather-information-providing, natural-
language spoken dialogue system (ˇZibert et al. 2003). In contrast to previous PARA-
DISE applications, we evaluated data that were collected in the early stages of a
dialogue system’s design where speech understanding was simulated by a human.
</bodyText>
<sectionHeader confidence="0.996179" genericHeader="method">
2. PARADISE-Framework Issues
</sectionHeader>
<bodyText confidence="0.969474">
The PARADISE model of performance (Walker et al. 1998) is defined as follows:
</bodyText>
<equation confidence="0.992074333333333">
n
Performance = (α * N (κ)) − wi * N (ci) (1)
i=1
</equation>
<bodyText confidence="0.99658075">
Here, α is the weight on the kappa coefficient κ, which is calculated from a confusion
matrix that summarizes how well the dialogue system achieves the information require-
ments of particular tasks within the dialogue and measures task success, wi are weights
on the costs ci, and N is a Z-score normalization function:
</bodyText>
<equation confidence="0.989174">
N (x) = x −x0 (2)
σx0
</equation>
<bodyText confidence="0.9999865">
where x0 and σx0 are the mean value and the standard deviation for x, respectively,
computed from the sample set of observations. Normalization guarantees that the
weights directly indicate the relative contributions to the performance function, which
can be used to predict user satisfaction, i.e., the value of the dependent variable.
</bodyText>
<subsectionHeader confidence="0.992026">
2.1 Relationships between MLR Variables
</subsectionHeader>
<bodyText confidence="0.999932454545455">
Nevertheless, MLR sets some constraints on the relationships between the parameters
(Seber 1977). Most obviously, there must exist at least an approximately linear relation-
ship between the dependent and the independent variables. On the other hand, it can
be shown (Patel and Bruce 1995) that dropping the predictors, i.e., the independent
variables, that are not statistically significant to the dependent variable (i.e., the p value
is greater than 0.05) can reduce the average error of the predictions.
Furthermore, the predictors must not correlate highly (i.e., the absolute values of
the correlation coefficients must not be greater than 0.7). If they do, small errors or
variations in the values of sample observations can have a large effect on the weights
in the performance function (1). Therefore, the redundant predictors must be removed
before applying MLR. Obviously, it is reasonable to remove those predictors that are
</bodyText>
<page confidence="0.976938">
264
</page>
<bodyText confidence="0.850141">
Hajdinjak and Miheliˇc The PARADISE Evaluation Framework
less statistically significant (i.e., with greater p values) to the dependent variable. Note,
high correlations may exist even between variables that seem unrelated (Section 3.2).
</bodyText>
<subsectionHeader confidence="0.998487">
2.2 Normalization Effects
</subsectionHeader>
<bodyText confidence="0.98487">
MLR is based on the least squares method, where the model minimizes the sum of the
squares of the differences between the observed and the predicted values. Hence,
</bodyText>
<equation confidence="0.976679666666667">
�
N (US) = N(US) + c (3)
�
</equation>
<bodyText confidence="0.9548745">
where N (US) is the normalized observed user-satisfaction value, N(US) is the predicted
normalized user-satisfaction value, and c is the error of the prediction. Further,
</bodyText>
<equation confidence="0.988772">
� �
US = N(US)6US0 + US0 + c6US0 = US + c6US0 (4)
</equation>
<bodyText confidence="0.991622166666667">
is the MLR model for the unnormalized user-satisfaction values, where US0 and 6US0
are the mean value and the standard deviation of the sample set of observed user-
satisfaction values, respectively. Note, the initial noise variable c is multiplied by 6US0.
There are several different measures of goodness of fit for a regression model, but
the most widely used is the coefficient of determination R2. In a regression model with
normalized variables, R2 turns out to be the variance of the predicted variable:
</bodyText>
<equation confidence="0.98475925">
�n i=1( N (USi) − N (US))2 n N(USi)2
R2 = i=1 = var(N � (US)) (5)
Eni=1(N (USi) − N (US))2 = n
�
</equation>
<bodyText confidence="0.875134333333333">
where USi and N(USi) are the ith component of the vector US of the observed values
�
and the ith component of the vector N(US) of the predicted normalized values, respec-
tively. One noteworthy consequence of the equality (5) is that the absolute values of the
weights in the performance function are not greater than 1.
Nevertheless, the accuracy of the prediction US for US is indicated by the q ratio:
</bodyText>
<equation confidence="0.923755666666667">
�
q(US, US) = |US −US |(6)
|US|
The definitions (2) and (6) and the equality (4) lead to the equality
q(N(US),N(US)) =
q(US, US)
|N(US)−N(US)|
|N(US)|
|US−us|
|US|
|US |=(7)
|US − US0|
|US−US0− N(US)σUS0|
|US−US0|
N�(US)σUS0 −US0|
|US|
=
|US−
</equation>
<bodyText confidence="0.997958333333333">
which shows that as soon as US &gt; US0 2the prediction for the normalized values is
(usually by a large margin) not as good as the prediction for the unnormalized values.
Therefore, after predicting the normalized user-satisfaction values, these values should
be transformed back to the original scale to guarantee more accurate predictions.
In previous work (Walker et al. 1997, 1998; Walker, Kamm, and Litman 2000; Litman
and Shimei 2002), not only was there no attention paid to these details, it was not
</bodyText>
<page confidence="0.968308">
265
</page>
<note confidence="0.284689">
Computational Linguistics Volume 32, Number 2
</note>
<bodyText confidence="0.999861">
mentioned that the observed user-satisfaction values need to be normalized as well if
one wants an acceptable error in the prediction.
</bodyText>
<subsectionHeader confidence="0.999767">
2.3 Choosing the Best Set of Predictors
</subsectionHeader>
<bodyText confidence="0.999995545454546">
A major problem in regression analysis is that of deciding which predictors should
be in the model. There are two conflicting criteria. First, the model chosen should
include as many predictors as possible if reliable predictions are to be obtained (also,
R2 increases with the number of predictors). Second, because of the costs involved
in determining a large number of predictors and the benefit of focusing only on the
most significant predictors, we would like the equation to include as few predictors
as possible.
An appropriate method of choosing the best set of predictors is backward elimination
(Seber 1977). Here, at each step, a single predictor is eliminated from the current regres-
sion model if its removal would increase the sum-of-squares differences between the
observed and the predicted values,
</bodyText>
<equation confidence="0.986575333333333">
n n
RSS = (N (USi) −N�(USi))2 = Ei2 (8)
i=1 i=1
</equation>
<bodyText confidence="0.9998755">
by not more than Fout (some properly chosen constant, typically between 2 and 4)
times the residual mean square RSS n−p,where n is the number of observations and p is
the number of predictors. That is, the predictor giving the smallest increase in RSS
is chosen.
</bodyText>
<subsectionHeader confidence="0.893377">
2.4 Why Model the Sum of User-Satisfaction Scores
</subsectionHeader>
<bodyText confidence="0.999885666666667">
Hone and Graham (2000) argued that the items chosen for the user-satisfaction survey
(Table 1) introduced within the PARADISE framework were based neither on theory
nor on well-conducted empirical research. Moreover, they said that the way that the
collected data was used would be inappropriate, i.e., the approach of summing all the
scores could only be justified on the basis of evidence that all of the items are measuring
the same construct, otherwise the overall score would be meaningless.
</bodyText>
<tableCaption confidence="0.885905">
Table 1
</tableCaption>
<bodyText confidence="0.844063">
The user-satisfaction survey used within the PARADISE framework.
</bodyText>
<listItem confidence="0.999147555555556">
1. Was the system easy to understand? (TTS Performance)
2. Did the system understand what you said? (ASR Performance)
3. Was it easy to find the message you wanted? (Task Ease)
4. Was the pace of interaction with the system appropriate? (Interaction Pace)
5. Did you know what you could say at each point of the dialogue? (User Expertise)
6. How often was the system sluggish and slow to reply to you? (System Response)
7. Did the system work the way you expected it? (Expected Behavior)
8. From your current experience with using the system, do you think you’d use the system regularly
when you are away from your desk? (Future Use)
</listItem>
<page confidence="0.974141">
266
</page>
<bodyText confidence="0.949650727272727">
Hajdinjak and Miheliˇc The PARADISE Evaluation Framework
If, in spite of Hone and Graham’s remarks, one wants to evaluate user satisfac-
tion with the survey given in Table 1, the question that arises is whether the target
to be predicted should really be the sum of all the user-satisfaction scores. How-
ever, our experiments (Section 2.3) showed a remarkable, but expected, difference in
the significance of the predictors when taking different satisfaction-measure sums or
even individual scores as the target to be predicted. Moreover, some individual user-
satisfaction measures could not be well modeled. Consequently, we think that it would
be more appropriate to take the sum of the user-satisfaction scores that are likely
to measure the selected aspect (e.g., dialogue-manager performance) of the system’s
performance.
</bodyText>
<subsectionHeader confidence="0.909791">
2.5 Speech-recognition Effects
</subsectionHeader>
<bodyText confidence="0.999918368421053">
It has often been reported (Walker et al. 1998; Kamm, Walker, and Litman 1999; Walker,
Kamm, and Litman 2000; Litman and Shimei 2002) that the mean concept accuracy,
often referred to as the mean recognition score, is the exceptional predictor of a dialogue
system’s performance. Moreover, it has been shown that as recognizer performance
improves the significance of the predictors can change (Walker, Borland, and Kamm
1999).
Thus, we would go so far as to claim that the influence of automatic speech
recognition hinders the other predictors from showing significance when evaluating
the performance of the dialogue-manager component, excluding the efficiency of its
clarification strategies. Only if the users are disencumbered from speech-recognition
errors are they able to reliably assess the mutual influence of the observable less sig-
nificant contributors to their satisfaction with the dialogue manager’s performance.
Therefore, in our WOZ experiments (Section 2), which were carried out in order to
evaluate the performance of the dialogue manager, speech understanding (i.e., speech
recognition and natural-language understanding) was performed by a human. As
expected, κ with mean values near 1 (0.94 and 0.98, respectively), which was the
only predictor reflecting speech-recognition performance (Section 2.1), did not show
the usual degree of significance in predicting the performance of our WOZ systems
(Section 2.3).
</bodyText>
<sectionHeader confidence="0.983643" genericHeader="method">
3. PARADISE Framework Application
</sectionHeader>
<bodyText confidence="0.999980230769231">
With the intention of involving the user in all the stages of the design of the spoken
natural language, weather-information-providing dialogue system (ˇZibert et al. 2003),
WOZ data were collected and evaluated even before the completion of all the
system’s components. The aim of the first two WOZ experiments (Hajdinjak and
Miheliˇc 2004a) was to evaluate the performance of the dialogue-manager component
(Hajdinjak and Miheliˇc 2004b). Therefore, while the task of the wizard in the first
WOZ experiment was to perform speech understanding and dialogue management,
the task in the second WOZ experiment was to perform only speech understand-
ing, and the dialogue-management task was assigned to the newly implemented
dialogue manager.
There were 76 and 68 users involved in the first and the second WOZ experiment,
respectively. The users were given two tasks: The first task was to obtain a particular
piece of weather-forecast information, and the second task was a given situation, the
</bodyText>
<page confidence="0.964432">
267
</page>
<note confidence="0.481003">
Computational Linguistics Volume 32, Number 2
</note>
<bodyText confidence="0.9997188">
aim of which was to stimulate them to ask context-specific questions. In addition,
the users were given the freedom to ask extra questions. User satisfaction was then
evaluated with the user-satisfaction survey given in Table 1, and a comprehensive user
satisfaction (US) was computed by summing each question’s score and thus ranged in
value from a low of 8 to a high of 40.
</bodyText>
<subsectionHeader confidence="0.999971">
3.1 Selection of Regression Parameters
</subsectionHeader>
<bodyText confidence="0.992480387096774">
The selection of regression parameters is crucial for the quality of the performance
equation, and it is usually the result of thorough considerations made during several
successive regression analyses. However, following the recommended Cohen’s method
(Di Eugenio and Glass 2004), we first computed the task-success measure
Kappa coefficient (κ), reflecting the wizard’s typing errors and
unauthorized corrections,
and the dialogue-efficiency costs
Mean elapsed time (MET), i.e., the mean elapsed time for the completion
of the tasks that occurred within the interaction, and
Mean user moves (MUM), i.e., the mean number of conversational moves
that the user needed to either fulfil or abandon the initiated
information-providing games.
Second, the following dialogue-quality costs were selected:
Task completion (Comp), i.e., the user’s perception of completing the
first task;
Number of user initiatives (NUI), i.e., the number of user’s moves
initiating information-providing games;
Mean words per turn (MWT), i.e., the mean number of words in each
of the user’s turns;
Mean response time (MRT), i.e., the mean system-response time;
Number of missing responses (NMR), i.e., the difference between the
number of turns by the system and the number of turns by the user;
Number of unsuitable requests (NUR) and unsuitable-request ratio
(URR), i.e., the number and the ratio of user’s initiating moves that were
out of context;
Number of inappropriate responses (NIR) and inappropriate-response
ratio (IRR), i.e., the number and the ratio of unexpected responses from
the system, including pardon moves;
Number of errors (Error), i.e., the number of system errors, e.g.,
interruptions of the telephone connection and unsuitable natural-
language sentences;
</bodyText>
<page confidence="0.972782">
268
</page>
<bodyText confidence="0.97424468">
Hajdinjak and Miheliˇc The PARADISE Evaluation Framework
Number of help messages (NHM) and help-message ratio (HMR), i.e.,
the number and the ratio of system’s help messages;
Number of check moves (NCM) and check-move ratio (CMR), i.e.,
the number and the ratio of system’s moves checking some information
regarding past dialogue events;
Number of given data (NGD) and given-data ratio (GDR), i.e., the
number and the ratio of system’s information-providing moves;
Number of relevant data (NRD) and relevant-data ratio (RDR), i.e.,
the number and the ratio of system’s moves directing the user to select
relevant, available data;
Number of no data (NND) and no-data ratio (NDR), i.e., the number and
the ratio of system’s moves stating that the requested information is not
available;
Number of abandoned requests (NAR) and abandoned-request ratio
(ARR), i.e., the number and the ratio of the information-providing games
abandoned by the user.
Note that special attention was given to the parameters NGD, GDR, NRD, RDR,
NND, and NDR, which have not so far been reported in the literature as costs for
user satisfaction. We will refer to them as database parameters. It has, however, been
argued (Walker et al. 1998) that the database size might be a relevant predictor of
performance. However, the decision to introduce the database parameters as dialogue
costs was based on the extremely sparse and dynamical weather-information source
(Hajdinjak and Miheliˇc 2004b) with a time-dependent data structure that we had at
our disposal.
</bodyText>
<subsectionHeader confidence="0.999719">
3.2 Correlations between MLR Parameters
</subsectionHeader>
<bodyText confidence="0.999984666666667">
In the data from both WOZ experiments, some high correlations between the regression
parameters were observed. Note that the high correlations were not found only
between the newly introduced or the obviously related parameters such as NUT and
NGD. In the first experiment, not entirely evident high correlations were observed
between MET and MRT (0.7), NMR and NHM (0.7), GDR and NDR (−0.8), but in
the second experiment between MET and MRT (0.8), NMR and NHM (0.7), GDR
and RDR (−0.7). The regression parameters GDR and NDR as well as GDR and
RDR were highly correlated only in one of the WOZ experiments, and moderately in
the other.
</bodyText>
<subsectionHeader confidence="0.998741">
3.3 Performance-function Results
</subsectionHeader>
<bodyText confidence="0.9980395">
We applied PARADISE to the data from both WOZ experiments (Hajdinjak and Miheliˇc
2004a). As the target to be predicted we first took user satisfaction (US) and afterwards
the sum of those user-satisfaction values that (in our opinion) measured the dialogue
manager’s performance (DM) and could, in addition, be well modeled, i.e., the sum
of the user-satisfaction-survey scores assigned to ASR Performance, Task Ease, System
Response, and Expected Behavior (Table 1).
</bodyText>
<page confidence="0.992487">
269
</page>
<note confidence="0.599071">
Computational Linguistics Volume 32, Number 2
</note>
<bodyText confidence="0.995037666666667">
After removing about 10% of the outliers,1 backward elimination for Fout = 2
was performed. Thus, the data from the first WOZ experiment gave the following
performance equations:
</bodyText>
<equation confidence="0.9999415">
N(U_S) = −0.69N (NND) − 0.16N (NRD)
N(DM) = −0.61N (NND) − 0.16N(NRD) + 0.21N(Comp)
</equation>
<bodyText confidence="0.99824925">
with 58% (R2 = 0.58) and 59% of the variance explained, respectively. To be able to
see the difference between these two equations, note that Comp was significant for US
(p &lt; 0.02), but removed by backward elimination. In contrast, the data from the second
WOZ experiment gave the following performance equations:
</bodyText>
<equation confidence="0.9998635">
N(U_S) = −0.30N (CMR) + 0.18N (K) − 0.23N (MET)
N(DM) = −0.35N (CMR) + 0.35N (K) + 0.35N(GDR) − 0.17N(ARR)
</equation>
<bodyText confidence="0.999939653846154">
with 26% and 46% of the variance explained, respectively. Note, MET was significant
for DM (p &lt; 0.02) and that GDR and ARR were significant for US (p &lt; 0.04), but they
were all removed by backward elimination. Moreover, MET was trivially correlated
with GDR and ARR (i.e., the correlation coefficients were lower than 0.1).
Let us compare both performance equations predicting DM. The first observation
we make is that none of the predictors is common to both performance equations.
All the predictors from the first performance equation (i.e., NND, NRD, Comp) were
insignificant (p &gt; 0.1) for DM in the second experiment. On the other hand, the only
predictor from the second performance equation that was significant for DM (p &lt; 0.004)
in the first experiment, but removed by backward elimination, was GDR. Unlike the
first performance equation with the database parameters NND and NRD as crucial
(negative) predictors, the second performance equation clearly shows their insignif-
icance to users’ satisfaction. Hence it follows that the developed dialogue manager
(Hajdinjak and Miheliˇc 2004b) with its rather consistent flexibility in directing the user
to select relevant, available data does not (negatively) influence users’ satisfaction.
Moreover, we thought that it would be very interesting to see which parameters
are significant for individual user-satisfaction measures (Table 1). However, the situ-
ation in which the Task Ease question was the only measure of user satisfaction, the
aim of which was to maximize the relationship between elapsed time and user satis-
faction, was considered before (Walker, Borland, and Kamm 1999). First, we discovered
that Future Use could not be well modeled in the first WOZ experiment and that User
Expertise and Interaction Pace could not be well modeled in the second WOZ experi-
ment, i.e., the corresponding MLR models explained less than 10% of the variance.
Second, the parameters that most significantly contributed to the remaining, in-
dividual user-satisfaction measures are given in Table 2. Surprisingly, the parameters
that were most significant to an individual user-satisfaction measure in the first WOZ
</bodyText>
<footnote confidence="0.972377">
1 Eliminating outliers, i.e., observations that lie at an abnormal distance from other values, is a common
practice in multivariate linear regression (Tabachnick and Fidell 1996). However, before the elimination of
the outliers in the data from the first WOZ experiment, the MLR models explained 44% and 39% of the
variance, respectively. In contrast, before the elimination of the outliers in the data from the second WOZ
experiment, the MLR models explained 18% and 34% of the variance, respectively.
</footnote>
<page confidence="0.988287">
270
</page>
<note confidence="0.833545">
Hajdinjak and Miheliˇc The PARADISE Evaluation Framework
</note>
<tableCaption confidence="0.99645">
Table 2
</tableCaption>
<table confidence="0.960621">
Most significant predictors of the individual user-satisfaction measures in the first (WOZ1) and
the second (WOZ2) WOZ experiment.
WOZ1 WOZ2
TTS Performance NND (p &lt; 0.00005) UMN (p &lt; 0.004)
ASR Performance NND (p &lt; 0.00005) CMR (p &lt; 0.012)
Task Ease NND (p &lt; 0.002) GDR (p &lt; 0.02)
System Response NND (p &lt; 0.0003) CMR (p &lt; 0.0002)
Expected Behavior NND (p &lt; 0.00005) Comp, RDR, CMR (p &lt; 0.04)
</table>
<bodyText confidence="0.999925111111111">
experiment were insignificant to the same measure in the second WOZ experiment
and vice versa. On the one hand, this could indicate that the selected individual
user-satisfaction measures really measure the performance of the dialogue manager
and consequently illustrate the obvious difference between both dialogue-management
manners. On the other hand, one could argue that this simply means that the individual
user-satisfaction measures are not appropriate measures of attitude because people
are likely to vary in the way they interpret the item wording (Hone and Graham
2000). However, due to the huge difference in significance the latter seems an unlikely
explanation.
</bodyText>
<sectionHeader confidence="0.996179" genericHeader="conclusions">
4. Conclusion
</sectionHeader>
<bodyText confidence="0.999975416666667">
The application of PARADISE to the data from two WOZ experiments led us to the
following conclusions. First, the identified high correlations between some dialogue
costs and the explained normalization effects on the accuracy of the prediction reinforce
the need for careful regression analysis. Second, if speech recognition is performed by a
human, the PARADISE evaluation will lead to the identification of the significant pre-
dictors of the dialogue-manager’s performance. Third, inaccurate MLR models of some
individual user-satisfaction measures and the observed differences between pairs of
performance equations predicting the same dependent variable require further empiri-
cal research. Not only does a reliable user-satisfaction measure that would capture the
performance measures of different dialogue-system components need to be established,
but the reasons for the possible differences between several performance equations also
need to be understood and properly assessed.
</bodyText>
<sectionHeader confidence="0.998777" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998319931034483">
Danieli, Morena and Elisabetta Gerbino.
1995. Metrics for evaluating dialogue
strategies in a spoken language system.
In Proceedings of the 1995 AAAI Spring
Symposium on Empirical Methods in
Discourse Interpretation and Generation,
pages 34–39, Stanford.
Di Eugenio, Barbara and Michael Glass.
2004. The Kappa statistic: A second look.
Computational Linguistics, 30(1):95–101.
Hajdinjak, Melita and France Miheliˇc.
2004a. Conducting the Wizard-of-Oz
experiment. Informatica, 28(4):
425–430.
Hajdinjak, Melita and France Miheliˇc.
2004b. Information-providing dialogue
management. In Proceedings of TSD,
pages 595–602, Brno.
Hone, Kate S. and Robert Graham.
2000. Towards a tool for the Subjective
Assessment of Speech System
Interfaces (SASSI). Natural Language
Engineering: Special Issue on Best
Practice in Spoken Dialogue Systems,
6(3–4):287–303.
Kamm, Candace A., Marilyn A. Walker, and
Diane J. Litman. 1999. Evaluating spoken
language systems. In Proceedings of American
Voice Input/Output Society, San Jose.
</reference>
<page confidence="0.987526">
271
</page>
<note confidence="0.59589">
Computational Linguistics Volume 32, Number 2
</note>
<reference confidence="0.989880779661017">
Litman, Diane J. and Pan Shimei. 2002.
Designing and evaluating an adaptive
spoken dialogue system. User Modeling
and User-Adapted Interaction, 12:111–137.
Patel, Nitin R. and Peter C. Bruce. 1995.
Resampling Stats—Data Mining in Excel:
Lecture Notes and Cases. Trafford Holdings
Ltd., Victoria, Canada.
Seber, George A. F. 1977. Linear Regression
Analysis. John Wiley &amp; Sons, New York.
Shriberg, Elizabeth, Elizabeth Wade,
and Patti Price. 1992. Human-machine
problem solving using Spoken
Language Systems (SLS): Factors
affecting performance and user
satisfaction. In Proceedings of the
DARPA Speech and NL Workshop,
pages 49–54, New York.
Smith, Ronnie W. and Steven A. Gordon.
1997. Effects of variable initiative
on linguistic behavior in human-
computer spoken natural language
dialog. Computational Linguistics,
23(1):141–168.
Tabachnick, Barbara G. and Linda S. Fidell.
1996. Using Multivariate Statistics, 3rd ed.
Harper Collins, New York.
Walker, Marilyn A., Julie Borland, and
Candace A. Kamm. 1999. The utility
of elapsed time as a usability metric
for spoken dialogue systems. In
Proceedings of ASRU, pages 317–320,
Keystone.
Walker, Marilyn A., Candace A. Kamm,
and Diane J. Litman. 2000. Towards
developing general models of usability
with PARADISE. Natural Language
Engineering: Special Issue on Best
Practice in Spoken Dialogue Systems,
6(3–4):363–377.
Walker, Marilyn A., Diane J. Litman,
Candace A. Kamm, and Alicia Abella.
1997. PARADISE: A general framework
for evaluating spoken dialogue agents.
In Proceedings of the 35th Annual Meeting
of ACL, pages 271–280, Madrid.
Walker, Marilyn A., Diane J. Litman,
Candace A. Kamm, and Alicia Abella.
1998. Evaluating spoken dialogue agents
with PARADISE: Two case studies.
Computer Speech and Language,
12(3):317–347.
ˇ
Zibert, Janez, Sanda Martinˇci´c-Ipˇsi´c, Melita
Hajdinjak, Ivo Ipˇsi´c, and France Miheliˇc.
2003. Development of a bilingual spoken
dialog system for weather information
retrieval. In Proceedings of EUROSPEECH,
pages 1917–1920, Geneva.
</reference>
<page confidence="0.997324">
272
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.782280">
<title confidence="0.9626025">The PARADISE Evaluation Framework: Issues and Findings</title>
<affiliation confidence="0.888907">University of Ljubljana University of Ljubljana</affiliation>
<abstract confidence="0.996182909090909">There has been a great deal of interest over the past 20 years in developing metrics and frameworks for evaluating and comparing the performance of spoken-language dialogue systems. One of the results of this interest is a potential general methodology, known as the PARADISE framework. This squib highlights some important issues concerning the application of PARADISE that have, up to now, not been sufficiently emphasized or have even been neglected by the dialogue-system community. These include considerations regarding the selection of appropriate regression parameters, normalization effects on the accuracy of the prediction, the influence of speech-recognition errors on the performance function, and the selection of an appropriate user-satisfaction measure. In addition, it gives the results of an evaluation of data from two Wizard-of-Oz experiments. These evaluations include different dependent variables and examination of individual user-satisfaction measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Morena Danieli</author>
<author>Elisabetta Gerbino</author>
</authors>
<title>Metrics for evaluating dialogue strategies in a spoken language system.</title>
<date>1995</date>
<booktitle>In Proceedings of the 1995 AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation,</booktitle>
<pages>34--39</pages>
<location>Stanford.</location>
<contexts>
<context position="1199" citStr="Danieli and Gerbino 1995" startWordPosition="163" endWordPosition="166"> been neglected by the dialogue-system community. These include considerations regarding the selection of appropriate regression parameters, normalization effects on the accuracy of the prediction, the influence of speech-recognition errors on the performance function, and the selection of an appropriate user-satisfaction measure. In addition, it gives the results of an evaluation of data from two Wizard-of-Oz experiments. These evaluations include different dependent variables and examination of individual user-satisfaction measures. 1. Introduction A long list of objective dialogue metrics (Danieli and Gerbino 1995; Smith and Gordon 1997) for dialogue evaluation, which can be calculated without recourse to human judgment, and subjective dialogue metrics (Shriberg, Wade, and Price 1992; Danieli and Gerbino 1995), which are based on human judgments, have been proposed. Their wellknown limitations led Walker, Litman, Kamm, and Abella (1997) to propose their paradigm for dialogue system evaluation (PARADISE), a potentially general methodology for evaluating spoken-language dialogue systems, the goal of which was to compare and optimize different dialogue managers and task domains independently. The PARADISE</context>
</contexts>
<marker>Danieli, Gerbino, 1995</marker>
<rawString>Danieli, Morena and Elisabetta Gerbino. 1995. Metrics for evaluating dialogue strategies in a spoken language system. In Proceedings of the 1995 AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, pages 34–39, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Michael Glass</author>
</authors>
<title>The Kappa statistic: A second look.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<marker>Di Eugenio, Glass, 2004</marker>
<rawString>Di Eugenio, Barbara and Michael Glass. 2004. The Kappa statistic: A second look. Computational Linguistics, 30(1):95–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melita Hajdinjak</author>
<author>France Miheliˇc</author>
</authors>
<title>Conducting the Wizard-of-Oz experiment.</title>
<date>2004</date>
<journal>Informatica,</journal>
<volume>28</volume>
<issue>4</issue>
<pages>425--430</pages>
<marker>Hajdinjak, Miheliˇc, 2004</marker>
<rawString>Hajdinjak, Melita and France Miheliˇc. 2004a. Conducting the Wizard-of-Oz experiment. Informatica, 28(4): 425–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melita Hajdinjak</author>
<author>France Miheliˇc</author>
</authors>
<title>Information-providing dialogue management.</title>
<date>2004</date>
<booktitle>In Proceedings of TSD,</booktitle>
<pages>595--602</pages>
<location>Brno.</location>
<marker>Hajdinjak, Miheliˇc, 2004</marker>
<rawString>Hajdinjak, Melita and France Miheliˇc. 2004b. Information-providing dialogue management. In Proceedings of TSD, pages 595–602, Brno.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate S Hone</author>
<author>Robert Graham</author>
</authors>
<title>Towards a tool for the Subjective Assessment of Speech System</title>
<date>2000</date>
<booktitle>Interfaces (SASSI). Natural Language Engineering: Special Issue on Best Practice in Spoken Dialogue Systems,</booktitle>
<pages>6--3</pages>
<contexts>
<context position="9839" citStr="Hone and Graham (2000)" startWordPosition="1542" endWordPosition="1545">ctors is backward elimination (Seber 1977). Here, at each step, a single predictor is eliminated from the current regression model if its removal would increase the sum-of-squares differences between the observed and the predicted values, n n RSS = (N (USi) −N�(USi))2 = Ei2 (8) i=1 i=1 by not more than Fout (some properly chosen constant, typically between 2 and 4) times the residual mean square RSS n−p,where n is the number of observations and p is the number of predictors. That is, the predictor giving the smallest increase in RSS is chosen. 2.4 Why Model the Sum of User-Satisfaction Scores Hone and Graham (2000) argued that the items chosen for the user-satisfaction survey (Table 1) introduced within the PARADISE framework were based neither on theory nor on well-conducted empirical research. Moreover, they said that the way that the collected data was used would be inappropriate, i.e., the approach of summing all the scores could only be justified on the basis of evidence that all of the items are measuring the same construct, otherwise the overall score would be meaningless. Table 1 The user-satisfaction survey used within the PARADISE framework. 1. Was the system easy to understand? (TTS Performan</context>
<context position="23723" citStr="Hone and Graham 2000" startWordPosition="3709" endWordPosition="3712">(p &lt; 0.00005) Comp, RDR, CMR (p &lt; 0.04) experiment were insignificant to the same measure in the second WOZ experiment and vice versa. On the one hand, this could indicate that the selected individual user-satisfaction measures really measure the performance of the dialogue manager and consequently illustrate the obvious difference between both dialogue-management manners. On the other hand, one could argue that this simply means that the individual user-satisfaction measures are not appropriate measures of attitude because people are likely to vary in the way they interpret the item wording (Hone and Graham 2000). However, due to the huge difference in significance the latter seems an unlikely explanation. 4. Conclusion The application of PARADISE to the data from two WOZ experiments led us to the following conclusions. First, the identified high correlations between some dialogue costs and the explained normalization effects on the accuracy of the prediction reinforce the need for careful regression analysis. Second, if speech recognition is performed by a human, the PARADISE evaluation will lead to the identification of the significant predictors of the dialogue-manager’s performance. Third, inaccur</context>
</contexts>
<marker>Hone, Graham, 2000</marker>
<rawString>Hone, Kate S. and Robert Graham. 2000. Towards a tool for the Subjective Assessment of Speech System Interfaces (SASSI). Natural Language Engineering: Special Issue on Best Practice in Spoken Dialogue Systems, 6(3–4):287–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Candace A Kamm</author>
<author>Marilyn A Walker</author>
<author>Diane J Litman</author>
</authors>
<title>Evaluating spoken language systems.</title>
<date>1999</date>
<booktitle>In Proceedings of American Voice Input/Output Society,</booktitle>
<location>San Jose.</location>
<marker>Kamm, Walker, Litman, 1999</marker>
<rawString>Kamm, Candace A., Marilyn A. Walker, and Diane J. Litman. 1999. Evaluating spoken language systems. In Proceedings of American Voice Input/Output Society, San Jose.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Pan Shimei</author>
</authors>
<title>Designing and evaluating an adaptive spoken dialogue system. User Modeling and User-Adapted Interaction,</title>
<date>2002</date>
<pages>12--111</pages>
<contexts>
<context position="8348" citStr="Litman and Shimei 2002" startWordPosition="1295" endWordPosition="1298">) and the equality (4) lead to the equality q(N(US),N(US)) = q(US, US) |N(US)−N(US)| |N(US)| |US−us| |US| |US |=(7) |US − US0| |US−US0− N(US)σUS0| |US−US0| N�(US)σUS0 −US0| |US| = |US− which shows that as soon as US &gt; US0 2the prediction for the normalized values is (usually by a large margin) not as good as the prediction for the unnormalized values. Therefore, after predicting the normalized user-satisfaction values, these values should be transformed back to the original scale to guarantee more accurate predictions. In previous work (Walker et al. 1997, 1998; Walker, Kamm, and Litman 2000; Litman and Shimei 2002), not only was there no attention paid to these details, it was not 265 Computational Linguistics Volume 32, Number 2 mentioned that the observed user-satisfaction values need to be normalized as well if one wants an acceptable error in the prediction. 2.3 Choosing the Best Set of Predictors A major problem in regression analysis is that of deciding which predictors should be in the model. There are two conflicting criteria. First, the model chosen should include as many predictors as possible if reliable predictions are to be obtained (also, R2 increases with the number of predictors). Second</context>
<context position="12013" citStr="Litman and Shimei 2002" startWordPosition="1892" endWordPosition="1895">pected, difference in the significance of the predictors when taking different satisfaction-measure sums or even individual scores as the target to be predicted. Moreover, some individual usersatisfaction measures could not be well modeled. Consequently, we think that it would be more appropriate to take the sum of the user-satisfaction scores that are likely to measure the selected aspect (e.g., dialogue-manager performance) of the system’s performance. 2.5 Speech-recognition Effects It has often been reported (Walker et al. 1998; Kamm, Walker, and Litman 1999; Walker, Kamm, and Litman 2000; Litman and Shimei 2002) that the mean concept accuracy, often referred to as the mean recognition score, is the exceptional predictor of a dialogue system’s performance. Moreover, it has been shown that as recognizer performance improves the significance of the predictors can change (Walker, Borland, and Kamm 1999). Thus, we would go so far as to claim that the influence of automatic speech recognition hinders the other predictors from showing significance when evaluating the performance of the dialogue-manager component, excluding the efficiency of its clarification strategies. Only if the users are disencumbered f</context>
</contexts>
<marker>Litman, Shimei, 2002</marker>
<rawString>Litman, Diane J. and Pan Shimei. 2002. Designing and evaluating an adaptive spoken dialogue system. User Modeling and User-Adapted Interaction, 12:111–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin R Patel</author>
<author>Peter C Bruce</author>
</authors>
<title>Resampling Stats—Data Mining in Excel: Lecture Notes and Cases. Trafford Holdings Ltd.,</title>
<date>1995</date>
<location>Victoria, Canada.</location>
<contexts>
<context position="5331" citStr="Patel and Bruce 1995" startWordPosition="792" endWordPosition="795"> standard deviation for x, respectively, computed from the sample set of observations. Normalization guarantees that the weights directly indicate the relative contributions to the performance function, which can be used to predict user satisfaction, i.e., the value of the dependent variable. 2.1 Relationships between MLR Variables Nevertheless, MLR sets some constraints on the relationships between the parameters (Seber 1977). Most obviously, there must exist at least an approximately linear relationship between the dependent and the independent variables. On the other hand, it can be shown (Patel and Bruce 1995) that dropping the predictors, i.e., the independent variables, that are not statistically significant to the dependent variable (i.e., the p value is greater than 0.05) can reduce the average error of the predictions. Furthermore, the predictors must not correlate highly (i.e., the absolute values of the correlation coefficients must not be greater than 0.7). If they do, small errors or variations in the values of sample observations can have a large effect on the weights in the performance function (1). Therefore, the redundant predictors must be removed before applying MLR. Obviously, it is</context>
</contexts>
<marker>Patel, Bruce, 1995</marker>
<rawString>Patel, Nitin R. and Peter C. Bruce. 1995. Resampling Stats—Data Mining in Excel: Lecture Notes and Cases. Trafford Holdings Ltd., Victoria, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A F Seber</author>
</authors>
<title>Linear Regression Analysis.</title>
<date>1977</date>
<publisher>John Wiley &amp; Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="5140" citStr="Seber 1977" startWordPosition="763" endWordPosition="764"> the dialogue and measures task success, wi are weights on the costs ci, and N is a Z-score normalization function: N (x) = x −x0 (2) σx0 where x0 and σx0 are the mean value and the standard deviation for x, respectively, computed from the sample set of observations. Normalization guarantees that the weights directly indicate the relative contributions to the performance function, which can be used to predict user satisfaction, i.e., the value of the dependent variable. 2.1 Relationships between MLR Variables Nevertheless, MLR sets some constraints on the relationships between the parameters (Seber 1977). Most obviously, there must exist at least an approximately linear relationship between the dependent and the independent variables. On the other hand, it can be shown (Patel and Bruce 1995) that dropping the predictors, i.e., the independent variables, that are not statistically significant to the dependent variable (i.e., the p value is greater than 0.05) can reduce the average error of the predictions. Furthermore, the predictors must not correlate highly (i.e., the absolute values of the correlation coefficients must not be greater than 0.7). If they do, small errors or variations in the </context>
<context position="9259" citStr="Seber 1977" startWordPosition="1444" endWordPosition="1445"> problem in regression analysis is that of deciding which predictors should be in the model. There are two conflicting criteria. First, the model chosen should include as many predictors as possible if reliable predictions are to be obtained (also, R2 increases with the number of predictors). Second, because of the costs involved in determining a large number of predictors and the benefit of focusing only on the most significant predictors, we would like the equation to include as few predictors as possible. An appropriate method of choosing the best set of predictors is backward elimination (Seber 1977). Here, at each step, a single predictor is eliminated from the current regression model if its removal would increase the sum-of-squares differences between the observed and the predicted values, n n RSS = (N (USi) −N�(USi))2 = Ei2 (8) i=1 i=1 by not more than Fout (some properly chosen constant, typically between 2 and 4) times the residual mean square RSS n−p,where n is the number of observations and p is the number of predictors. That is, the predictor giving the smallest increase in RSS is chosen. 2.4 Why Model the Sum of User-Satisfaction Scores Hone and Graham (2000) argued that the ite</context>
</contexts>
<marker>Seber, 1977</marker>
<rawString>Seber, George A. F. 1977. Linear Regression Analysis. John Wiley &amp; Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Elizabeth Wade</author>
<author>Patti Price</author>
</authors>
<title>Human-machine problem solving using Spoken Language Systems (SLS): Factors affecting performance and user satisfaction.</title>
<date>1992</date>
<booktitle>In Proceedings of the DARPA Speech and NL Workshop,</booktitle>
<pages>49--54</pages>
<location>New York.</location>
<marker>Shriberg, Wade, Price, 1992</marker>
<rawString>Shriberg, Elizabeth, Elizabeth Wade, and Patti Price. 1992. Human-machine problem solving using Spoken Language Systems (SLS): Factors affecting performance and user satisfaction. In Proceedings of the DARPA Speech and NL Workshop, pages 49–54, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronnie W Smith</author>
<author>Steven A Gordon</author>
</authors>
<title>Effects of variable initiative on linguistic behavior in humancomputer spoken natural language dialog.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="1223" citStr="Smith and Gordon 1997" startWordPosition="167" endWordPosition="170">logue-system community. These include considerations regarding the selection of appropriate regression parameters, normalization effects on the accuracy of the prediction, the influence of speech-recognition errors on the performance function, and the selection of an appropriate user-satisfaction measure. In addition, it gives the results of an evaluation of data from two Wizard-of-Oz experiments. These evaluations include different dependent variables and examination of individual user-satisfaction measures. 1. Introduction A long list of objective dialogue metrics (Danieli and Gerbino 1995; Smith and Gordon 1997) for dialogue evaluation, which can be calculated without recourse to human judgment, and subjective dialogue metrics (Shriberg, Wade, and Price 1992; Danieli and Gerbino 1995), which are based on human judgments, have been proposed. Their wellknown limitations led Walker, Litman, Kamm, and Abella (1997) to propose their paradigm for dialogue system evaluation (PARADISE), a potentially general methodology for evaluating spoken-language dialogue systems, the goal of which was to compare and optimize different dialogue managers and task domains independently. The PARADISE framework maintains tha</context>
</contexts>
<marker>Smith, Gordon, 1997</marker>
<rawString>Smith, Ronnie W. and Steven A. Gordon. 1997. Effects of variable initiative on linguistic behavior in humancomputer spoken natural language dialog. Computational Linguistics, 23(1):141–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara G Tabachnick</author>
<author>Linda S Fidell</author>
</authors>
<title>Using Multivariate Statistics, 3rd ed. Harper Collins,</title>
<date>1996</date>
<location>New York.</location>
<contexts>
<context position="22354" citStr="Tabachnick and Fidell 1996" startWordPosition="3488" endWordPosition="3491">rst WOZ experiment and that User Expertise and Interaction Pace could not be well modeled in the second WOZ experiment, i.e., the corresponding MLR models explained less than 10% of the variance. Second, the parameters that most significantly contributed to the remaining, individual user-satisfaction measures are given in Table 2. Surprisingly, the parameters that were most significant to an individual user-satisfaction measure in the first WOZ 1 Eliminating outliers, i.e., observations that lie at an abnormal distance from other values, is a common practice in multivariate linear regression (Tabachnick and Fidell 1996). However, before the elimination of the outliers in the data from the first WOZ experiment, the MLR models explained 44% and 39% of the variance, respectively. In contrast, before the elimination of the outliers in the data from the second WOZ experiment, the MLR models explained 18% and 34% of the variance, respectively. 270 Hajdinjak and Miheliˇc The PARADISE Evaluation Framework Table 2 Most significant predictors of the individual user-satisfaction measures in the first (WOZ1) and the second (WOZ2) WOZ experiment. WOZ1 WOZ2 TTS Performance NND (p &lt; 0.00005) UMN (p &lt; 0.004) ASR Performance</context>
</contexts>
<marker>Tabachnick, Fidell, 1996</marker>
<rawString>Tabachnick, Barbara G. and Linda S. Fidell. 1996. Using Multivariate Statistics, 3rd ed. Harper Collins, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Julie Borland</author>
<author>Candace A Kamm</author>
</authors>
<title>The utility of elapsed time as a usability metric for spoken dialogue systems.</title>
<date>1999</date>
<booktitle>In Proceedings of ASRU,</booktitle>
<pages>317--320</pages>
<location>Keystone.</location>
<marker>Walker, Borland, Kamm, 1999</marker>
<rawString>Walker, Marilyn A., Julie Borland, and Candace A. Kamm. 1999. The utility of elapsed time as a usability metric for spoken dialogue systems. In Proceedings of ASRU, pages 317–320, Keystone.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Candace A Kamm</author>
<author>Diane J Litman</author>
</authors>
<title>Towards developing general models of usability with PARADISE.</title>
<date>2000</date>
<booktitle>Natural Language Engineering: Special Issue on Best Practice in Spoken Dialogue Systems,</booktitle>
<pages>6--3</pages>
<marker>Walker, Kamm, Litman, 2000</marker>
<rawString>Walker, Marilyn A., Candace A. Kamm, and Diane J. Litman. 2000. Towards developing general models of usability with PARADISE. Natural Language Engineering: Special Issue on Best Practice in Spoken Dialogue Systems, 6(3–4):363–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Diane J Litman</author>
<author>Candace A Kamm</author>
<author>Alicia Abella</author>
</authors>
<title>PARADISE: A general framework for evaluating spoken dialogue agents.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of ACL,</booktitle>
<pages>271--280</pages>
<location>Madrid.</location>
<contexts>
<context position="8286" citStr="Walker et al. 1997" startWordPosition="1285" endWordPosition="1288"> q(US, US) = |US −US |(6) |US| The definitions (2) and (6) and the equality (4) lead to the equality q(N(US),N(US)) = q(US, US) |N(US)−N(US)| |N(US)| |US−us| |US| |US |=(7) |US − US0| |US−US0− N(US)σUS0| |US−US0| N�(US)σUS0 −US0| |US| = |US− which shows that as soon as US &gt; US0 2the prediction for the normalized values is (usually by a large margin) not as good as the prediction for the unnormalized values. Therefore, after predicting the normalized user-satisfaction values, these values should be transformed back to the original scale to guarantee more accurate predictions. In previous work (Walker et al. 1997, 1998; Walker, Kamm, and Litman 2000; Litman and Shimei 2002), not only was there no attention paid to these details, it was not 265 Computational Linguistics Volume 32, Number 2 mentioned that the observed user-satisfaction values need to be normalized as well if one wants an acceptable error in the prediction. 2.3 Choosing the Best Set of Predictors A major problem in regression analysis is that of deciding which predictors should be in the model. There are two conflicting criteria. First, the model chosen should include as many predictors as possible if reliable predictions are to be obtai</context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1997</marker>
<rawString>Walker, Marilyn A., Diane J. Litman, Candace A. Kamm, and Alicia Abella. 1997. PARADISE: A general framework for evaluating spoken dialogue agents. In Proceedings of the 35th Annual Meeting of ACL, pages 271–280, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Diane J Litman</author>
<author>Candace A Kamm</author>
<author>Alicia Abella</author>
</authors>
<title>Evaluating spoken dialogue agents with PARADISE: Two case studies.</title>
<date>1998</date>
<journal>Computer Speech and Language,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="2703" citStr="Walker et al. 1998" startWordPosition="384" endWordPosition="387">s are of two types: dialogueefficiency costs (e.g., number of utterances, dialogue time), which are measures of the system’s efficiency in helping the user to complete the task, and dialogue-quality costs (e.g., system-response delay, mean recognition score), which are intended to capture other aspects that can have large effects on the user’s perception of the system’s performance. Applying PARADISE to dialogue data requires dialogue corpora to be collected via controlled experiments during which users subjectively rate their satisfaction. Here, user satisfaction is calculated with a survey (Walker et al. 1998) that asks users to specify the degree to which they agree with several statements about the performance * Faculty of Electrical Engineering, Trˇzaˇska 25, 1000 Ljubljana, Slovenia © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 2 of the system. In addition, the other parameters of the model of performance, i.e., the task-success measures and the dialogue costs, must be either automatically logged by the system or be hand-labeled. The PARADISE model of performance posits that a performance function can then be derived by applying multivariate linear</context>
<context position="4252" citStr="Walker et al. 1998" startWordPosition="614" endWordPosition="617">ialogue-system community. Moreover, most of the considerations about these issues are supported by the results of applying PARADISE to the data from two Wizard-of-Oz (WOZ) experiments (Hajdinjak and Miheliˇc 2004a) carried out during the development of a weather-information-providing, naturallanguage spoken dialogue system (ˇZibert et al. 2003). In contrast to previous PARADISE applications, we evaluated data that were collected in the early stages of a dialogue system’s design where speech understanding was simulated by a human. 2. PARADISE-Framework Issues The PARADISE model of performance (Walker et al. 1998) is defined as follows: n Performance = (α * N (κ)) − wi * N (ci) (1) i=1 Here, α is the weight on the kappa coefficient κ, which is calculated from a confusion matrix that summarizes how well the dialogue system achieves the information requirements of particular tasks within the dialogue and measures task success, wi are weights on the costs ci, and N is a Z-score normalization function: N (x) = x −x0 (2) σx0 where x0 and σx0 are the mean value and the standard deviation for x, respectively, computed from the sample set of observations. Normalization guarantees that the weights directly indi</context>
<context position="11926" citStr="Walker et al. 1998" startWordPosition="1878" endWordPosition="1881">faction scores. However, our experiments (Section 2.3) showed a remarkable, but expected, difference in the significance of the predictors when taking different satisfaction-measure sums or even individual scores as the target to be predicted. Moreover, some individual usersatisfaction measures could not be well modeled. Consequently, we think that it would be more appropriate to take the sum of the user-satisfaction scores that are likely to measure the selected aspect (e.g., dialogue-manager performance) of the system’s performance. 2.5 Speech-recognition Effects It has often been reported (Walker et al. 1998; Kamm, Walker, and Litman 1999; Walker, Kamm, and Litman 2000; Litman and Shimei 2002) that the mean concept accuracy, often referred to as the mean recognition score, is the exceptional predictor of a dialogue system’s performance. Moreover, it has been shown that as recognizer performance improves the significance of the predictors can change (Walker, Borland, and Kamm 1999). Thus, we would go so far as to claim that the influence of automatic speech recognition hinders the other predictors from showing significance when evaluating the performance of the dialogue-manager component, excludin</context>
<context position="17781" citStr="Walker et al. 1998" startWordPosition="2768" endWordPosition="2771"> select relevant, available data; Number of no data (NND) and no-data ratio (NDR), i.e., the number and the ratio of system’s moves stating that the requested information is not available; Number of abandoned requests (NAR) and abandoned-request ratio (ARR), i.e., the number and the ratio of the information-providing games abandoned by the user. Note that special attention was given to the parameters NGD, GDR, NRD, RDR, NND, and NDR, which have not so far been reported in the literature as costs for user satisfaction. We will refer to them as database parameters. It has, however, been argued (Walker et al. 1998) that the database size might be a relevant predictor of performance. However, the decision to introduce the database parameters as dialogue costs was based on the extremely sparse and dynamical weather-information source (Hajdinjak and Miheliˇc 2004b) with a time-dependent data structure that we had at our disposal. 3.2 Correlations between MLR Parameters In the data from both WOZ experiments, some high correlations between the regression parameters were observed. Note that the high correlations were not found only between the newly introduced or the obviously related parameters such as NUT a</context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1998</marker>
<rawString>Walker, Marilyn A., Diane J. Litman, Candace A. Kamm, and Alicia Abella. 1998. Evaluating spoken dialogue agents with PARADISE: Two case studies. Computer Speech and Language, 12(3):317–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janez Zibert</author>
<author>Sanda Martinˇci´c-Ipˇsi´c</author>
<author>Melita Hajdinjak</author>
<author>Ivo Ipˇsi´c</author>
<author>France Miheliˇc</author>
</authors>
<title>Development of a bilingual spoken dialog system for weather information retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of EUROSPEECH,</booktitle>
<pages>1917--1920</pages>
<location>Geneva.</location>
<marker>Zibert, Martinˇci´c-Ipˇsi´c, Hajdinjak, Ipˇsi´c, Miheliˇc, 2003</marker>
<rawString>Zibert, Janez, Sanda Martinˇci´c-Ipˇsi´c, Melita Hajdinjak, Ivo Ipˇsi´c, and France Miheliˇc. 2003. Development of a bilingual spoken dialog system for weather information retrieval. In Proceedings of EUROSPEECH, pages 1917–1920, Geneva.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>