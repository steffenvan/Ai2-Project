<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.9985195">
For the sake of simplicity:
Unsupervised extraction of lexical simplifications from Wikipedia
</title>
<author confidence="0.991064">
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil and Lillian Lee
</author>
<email confidence="0.981742">
my89@cornell.edu, bopang@yahoo-inc.com, cristian@cs.cornell.edu, llee@cs.cornell.edu
</email>
<sectionHeader confidence="0.982534" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999327785714286">
We report on work in progress on extract-
ing lexical simplifications (e.g., “collaborate”
→ “work together”), focusing on utilizing
edit histories in Simple English Wikipedia for
this task. We consider two main approaches:
(1) deriving simplification probabilities via an
edit model that accounts for a mixture of dif-
ferent operations, and (2) using metadata to
focus on edits that are more likely to be sim-
plification operations. We find our methods
to outperform a reasonable baseline and yield
many high-quality lexical simplifications not
included in an independently-created manu-
ally prepared list.
</bodyText>
<sectionHeader confidence="0.998767" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987614478260869">
Nothing is more simple than greatness; indeed, to be
simple is to be great. —Emerson, Literary Ethics
Style is an important aspect of information pre-
sentation; indeed, different contexts call for differ-
ent styles. Here, we consider an important dimen-
sion of style, namely, simplicity. Systems that can
rewrite text into simpler versions promise to make
information available to a broader audience, such as
non-native speakers, children, laypeople, and so on.
One major effort to produce such text is the
Simple English Wikipedia (henceforth SimpleEW)1,
a sort of spin-off of the well-known English
Wikipedia (henceforth ComplexEW) where hu-
man editors enforce simplicity of language through
rewriting. The crux of our proposal is to learn lexical
simplifications from SimpleEW edit histories, thus
leveraging the efforts of the 18K pseudonymous in-
dividuals who work on SimpleEW. Importantly, not
all the changes on SimpleEW are simplifications; we
thus also make use of ComplexEW edits to filter out
non-simplifications.
Related work and related problems Previous
work usually involves general syntactic-level trans-
</bodyText>
<footnote confidence="0.979665">
1http://simple.wikipedia.org
</footnote>
<bodyText confidence="0.999967538461539">
formation rules [1, 9, 10].2 In contrast, we explore
data-driven methods to learn lexical simplifications
(e.g., “collaborate” → “work together”), which are
highly specific to the lexical items involved and thus
cannot be captured by a few general rules.
Simplification is strongly related to but distinct
from paraphrasing and machine translation (MT).
While it can be considered a directional form of
the former, it differs in spirit because simplification
must trade off meaning preservation (central to para-
phrasing) against complexity reduction (not a con-
sideration in paraphrasing). Simplification can also
be considered to be a form of MT in which the two
“languages” in question are highly related. How-
ever, note that ComplexEW and SimpleEW do not
together constitute a clean parallel corpus, but rather
an extremely noisy comparable corpus. For ex-
ample, Complex/Simple same-topic document pairs
are often written completely independently of each
other, and even when it is possible to get good
sentence alignments between them, the sentence
pairs may reflect operations other than simplifica-
tion, such as corrections, additions, or edit spam.
Our work joins others in using Wikipedia revi-
sions to learn interesting types of directional lexical
relations, e.g, “eggcorns”3 [7] and entailments [8].
</bodyText>
<sectionHeader confidence="0.944817" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.9995544">
As mentioned above, a key idea in our work is to
utilize SimpleEW edits. The primary difficulty in
working with these modifications is that they include
not only simplifications but also edits that serve
other functions, such as spam removal or correction
of grammar or factual content (“fixes”). We describe
two main approaches to this problem: a probabilis-
tic model that captures this mixture of different edit
operations (§2.1), and the use of metadata to filter
out undesirable revisions (§2.2).
</bodyText>
<footnote confidence="0.9986675">
2One exception [5] changes verb tense and replaces pro-
nouns. Other lexical-level work focuses on medical text [4, 2],
or uses frequency-filtered WordNet synonyms [3].
3A type of lexical corruption, e.g., “acorn”--+“eggcorn”.
</footnote>
<page confidence="0.930304">
365
</page>
<subsubsectionHeader confidence="0.475349">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 365–368,
</subsubsectionHeader>
<page confidence="0.221792">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</page>
<subsectionHeader confidence="0.950238">
2.1 Edit model
</subsectionHeader>
<bodyText confidence="0.922371255813953">
We say that the kth article in a Wikipedia corre-
sponds to (among other things) a title or topic (e.g.,
~
“Cat”) and a sequence dk of article versions caused
by successive edits. For a given lexical item or
~
phrase A, we write A E dk if there is any version
~ ~
in dk that contains A. From each dk we extract a
collection ek = (ek,1, ek,2, . . . , ek,nk) of lexical edit
instances, repeats allowed, where ek,i = A –* a
means that phrase A in one version was changed to
a in the next, A =� a; e.g., “stands for” –* “is the
same as”. (We defer detailed description of how we
extract lexical edit instances from data to §3.1.) We
denote the collection of ~dk in ComplexEW and Sim-
pleEW as C and S, respectively.
There are at least four possible edit operations: fix
(o1), simplify (o2), no-op (o3), or spam (o4). How-
ever, for this initial work we assume P(o4) = 0.4
Let P(oi  |A) be the probability that oi is applied
to A, and P(a  |A, oi) be the probability of A –* a
given that the operation is oi. The key quantities of
interest are P(o2  |A) in S, which is the probability
that A should be simplified, and P(a  |A, o2), which
yields proper simplifications of A. We start with an
equation that models the probability that a phrase A
is edited into a:
~
We similarly define fS(A) on dk in S. Note that we
count topics (version sequences), not individual ver-
sions: if A appears at some point and is not edited
until 50 revisions later, we should not conclude
that A is unlikely to be rewritten; for example, the
intervening revisions could all be minor additions,
or part of an edit war.
If we assume that the probability of any particular
fix operation being applied in SimpleEW is propor-
tional to that in ComplexEW— e.g., the SimpleEW
fix rate might be dampened because already-edited
ComplexEW articles are copied over — we have6
Pb(o1  |A) = αfC(A)
where 0 &lt; α &lt; 1. Note that in SimpleEW,
</bodyText>
<equation confidence="0.966934">
P(o1 V o2  |A) = P(o1  |A) + P(o2  |A),
</equation>
<bodyText confidence="0.999888333333333">
where P(o1 V o2  |A) is the probability that A is
changed to abdifferent word in SimpleEW, which we
estimate as P(o1 V o2  |A) = fS(A). We then set
</bodyText>
<equation confidence="0.973799">
bP(o2  |A) = max (0, fS(A) − αfC(A)).
</equation>
<bodyText confidence="0.99877775">
Next, under our working assumption, we estimate
the probability of A being changed to a as a fix
by the proportion of ComplexEW edit instances that
rewrite A to a:
</bodyText>
<equation confidence="0.9782518">
X
P(a  |A) =
oi∈Q
P(oi  |A)P(a  |A, oi), (1)
Pb(a  |A, o1) = |{(k, i) pairs  |ek,i = A –* a ∧
~dk E C}|
.
~dk E C}|
P
a, |{(k, i) pairs  |ek,i = A –* a&apos; ∧
</equation>
<bodyText confidence="0.997623923076923">
where Q is the set of edit operations. This involves
the desired parameters, which we solve for by esti-
mating the others from data, as described next.
Estimation Note that P(a  |A, o3) = 0 if A =� a.
Thus, if we have estimates for o1-related probabili-
ties, we can derive o2-related probabilities via Equa-
tion 1. To begin with, we make the working as-
sumption that occurrences of simplification in Com-
plexEW are negligible in comparison to fixes. Since
we are also currently ignoring edit spam, we thus
assume that only o1 edits occur in ComplexEW.5
Let fC(A) be the fraction of ~dk in C
containing A in which A is modified:
</bodyText>
<equation confidence="0.9775265">
fC(A) = |{ ~dk∈C|∃a,i such that ek,i=A→a}|
|{~dk∈C|A∈ ~dk} |.
</equation>
<footnote confidence="0.799227">
4Spam/vandalism detection is a direction for future work.
5This assumption also provides useful constraints to EM,
which we plan to apply in the future, by reducing the number of
parameter settings yielding the same likelihood.
</footnote>
<bodyText confidence="0.970663333333333">
A natural estimate for the conditional probability
of A being rewritten to a under any operation type
is based on observations of A –* a in SimpleEW,
since that is the corpus wherein both operations are
assumed to occur:
Thus, from (1) we get that for A =� a:
</bodyText>
<equation confidence="0.311213">
bP(a  |A, o2) = bP(a  |A) − bP(o1  |A)bP(a  |A,o1)
bP(o2  |A) .
</equation>
<sectionHeader confidence="0.732679" genericHeader="method">
2.2 Metadata-based methods
</sectionHeader>
<bodyText confidence="0.99978">
Wiki editors have the option of associating a com-
ment with each revision, and such comments some-
times indicate the intent of the revision. We there-
fore sought to use comments to identify “trusted”
</bodyText>
<equation confidence="0.8723632">
6Throughout, “hats” denote estimates.
Pb(a  |A) = |{(k, i) pairs  |ek,i = A –* a ∧
~dk E S}|
P .
a, |{(k, i) pairs  |ek,i = A –* a&apos; ∧ ~dk E S}|
</equation>
<page confidence="0.988366">
366
</page>
<bodyText confidence="0.998239954545455">
revisions wherein the extracted lexical edit instances
(see §3.1) would be likely to be simplifications.
Let ik = (rk, ... , rk, ...) be the sequence of revi-
sions for the kth article in SimpleEW, where rk is the
set of lexical edit instances (A —* a) extracted from
the ith modification of the document. Let ck be the
comment that accompanies rk, and conversely, let
R(Set) = {rk|ck E Set}.
We start with a seed set of trusted comments,
Seed. To initialize it, we manually inspected a small
sample of the 700K+ SimpleEW revisions that bear
comments, and found that comments containing a
word matching the regular expression *simpl* (e.g,
“simplify”) seem promising. We thus set Seed �=
{ * simpl*} (abusing notation).
The SIMPL method Given a set of trusted revi-
sions TRev (in our case TRev = R(Seed)), we
score each A —* a E TRev by the point-wise mu-
tual information (PMI) between A and a.7 We write
RANK(TRev) to denote the PMI-based ranking of
A —* a E TRev, and use SIMPL to denote our most
basic ranking method, RANK(R(Seed)).
Two ideas for bootstrapping We also considered
bootstrapping as a way to be able to utilize revisions
whose comments are not in the initial Seed set.
Our first idea was to iteratively expand the set
of trusted comments to include those that most of-
ten accompany already highly ranked simplifica-
tions. Unfortunately, our initial implementations in-
volved many parameters (upper and lower comment-
frequency thresholds, number of highly ranked sim-
plifications to consider, number of comments to add
per iteration), making it relatively difficult to tune;
we thus omit its results.
Our second idea was to iteratively expand the
set of trusted revisions, adding those that contain
already highly ranked simplifications. While our
initial implementation had fewer parameters than
the method sketched above, it tended to terminate
quickly, so that not many new simplifications were
found; so, again, we do not report results here.
An important direction for future work is to differ-
entially weight the edit instances within a revision,
as opposed to placing equal trust in all of them; this
</bodyText>
<footnote confidence="0.6928585">
7PMI seemed to outperform raw frequency and conditional
probability.
</footnote>
<bodyText confidence="0.9986595">
could prevent our bootstrapping methods from giv-
ing common fixes (e.g., “a” —* “the”) high scores.
</bodyText>
<sectionHeader confidence="0.820995" genericHeader="method">
3 Evaluation8
</sectionHeader>
<subsectionHeader confidence="0.982555">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.9999882">
We obtained the revision histories of both Sim-
pleEW (November 2009 snapshot) and ComplexEW
(January 2008 snapshot). In total, —1.5M revisions
for 81733 SimpleEW articles were processed (only
30% involved textual changes). For ComplexEW,
we processed —16M revisions for 19407 articles.
Extracting lexical edit instances. For each ar-
ticle, we aligned sentences in each pair of adja-
cent versions using tf-idf scores in a way simi-
lar to Nelken and Shieber [6] (this produced sat-
isfying results because revisions tended to repre-
sent small changes). From the aligned sentence
pairs, we obtained the aforementioned lexical edit
instances A —* a. Since the focus of our study
was not word alignment, we used a simple method
that identified the longest differing segments (based
on word boundaries) between each sentence, except
that to prevent the extraction of entire (highly non-
matching) sentences, we filtered out A —* a pairs if
either A or a contained more than five words.
</bodyText>
<subsectionHeader confidence="0.999845">
3.2 Comparison points
</subsectionHeader>
<bodyText confidence="0.9997148125">
Baselines RANDOM returns lexical edit instances
drawn uniformly at random from among those ex-
tracted from SimpleEW. FREQUENT returns the
most frequent lexical edit instances extracted from
SimpleEW.
Dictionary of simplifications The SimpleEW ed-
itor “Spencerk” (Spencer Kelly) has assembled a list
of simple words and simplifications using a combi-
nation of dictionaries and manual effort9. He pro-
vides a list of 17,900 simple words — words that do
not need further simplification — and a list of 2000
transformation pairs. We did not use Spencerk’s set
as the gold standard because many transformations
we found to be reasonable were not on his list. In-
stead, we measured our agreement with the list of
transformations he assembled (SPLIST).
</bodyText>
<footnote confidence="0.99997">
8Results at http://www.cs.cornell.edu/home/llee/data/simple
9http://www.spencerwaterbed.com/soft/simple/about.html
</footnote>
<page confidence="0.992368">
367
</page>
<subsectionHeader confidence="0.998708">
3.3 Preliminary results
</subsectionHeader>
<bodyText confidence="0.999459416666667">
The top 100 pairs from each system (edit model10
and SIMPL and the two baselines) plus 100 ran-
domly selected pairs from SPLIST were mixed and
all presented in random order to three native English
speakers and three non-native English speakers (all
non-authors). Each pair was presented in random
orientation (i.e., either as A → a or as a → A),
and the labels included “simpler”, “more complex”,
“equal”, “unrelated”, and “?” (“hard to judge”). The
first two labels correspond to simplifications for the
orientations A → a and a → A, respectively. Col-
lapsing the 5 labels into “simplification”, “not a sim-
plification”, and “?” yields reasonable agreement
among the 3 native speakers (n = 0.69; 75.3% of the
time all three agreed on the same label). While we
postulated that non-native speakers11 might be more
sensitive to what was simpler, we note that they dis-
agreed more than the native speakers (n = 0.49) and
reported having to consult a dictionary. The native-
speaker majority label was used in our evaluations.
Here are the results; “-x-y” means that x and y are
the number of instances discarded from the precision
calculation for having no majority label or majority
label “?”, respectively:
</bodyText>
<table confidence="0.979861333333333">
Method Prec@100 # of pairs
SPLIST 86% (-0-0) 2000
Edit model 77% (-0-1) 1079
SIMPL 66% (-0-0) 2970
FREQUENT 17% (-1-7) -
RANDOM 17% (-1-4) -
</table>
<bodyText confidence="0.999078416666667">
Both baselines yielded very low precisions —
clearly not all (frequent) edits in SimpleEW were
simplifications. Furthermore, the edit model yielded
higher precision than SIMPL for the top 100 pairs.
(Note that we only examined one simplification per
A for those A where P�(o2  |A) was well-defined;
thus “# of pairs” does not directly reflect the full
potential recall that either method can achieve.)
Both, however, produced many high-quality pairs
(62% and 71% of the correct pairs) not included in
SPLIST. We also found the pairs produced by these
two systems to be complementary to each other. We
</bodyText>
<footnote confidence="0.7551556">
10We only considered those A such that freq(A --+ *) &gt;
1 n freq(A) &gt; 100 on both SimpleEW and ComplexEW. The
final top 100 A --+ a pairs were those with As with the highest
P(o2  |A). We set a = 1.
11Native languages: Russian; Russian; Russian and Kazakh.
</footnote>
<bodyText confidence="0.999895571428571">
believe that these two approaches provide a good
starting point for further explorations.
Finally, some examples of simplifications found
by our methods: “stands for” → “is the same
as”, “indigenous” → “native”, “permitted” → “al-
lowed”, “concealed” → “hidden”, “collapsed” →
“fell down”, “annually” → “every year”.
</bodyText>
<sectionHeader confidence="0.694198" genericHeader="method">
3.4 Future work
</sectionHeader>
<bodyText confidence="0.990568888888889">
Further evaluation could include comparison with
machine-translation and paraphrasing algorithms. It
would be interesting to use our proposed estimates
as initialization for EM-style iterative re-estimation.
Another idea would be to estimate simplification pri-
ors based on a model of inherent lexical complexity;
some possible starting points are number of sylla-
bles (which is used in various readability formulae)
or word length.
</bodyText>
<sectionHeader confidence="0.360842" genericHeader="method">
Acknowledgments We first wish to thank Ainur Yessenalina
</sectionHeader>
<reference confidence="0.690419666666667">
for initial investigations and helpful comments. We are
also thankful to R. Barzilay, T. Bruce, C. Callison-Burch, J.
Cantwell, M. Dredze, C. Napoles, E. Gabrilovich, &amp; the review-
ers for helpful comments; W. Arms and L. Walle for access to
the Cornell Hadoop cluster; J. Cantwell for access to computa-
tional resources; R. Hwa &amp; A. Owens for annotation software;
M. Ulinski for preliminary explorations; J. Cantwell, M. Ott, J.
Silverstein, J. Yatskar, Y. Yatskar, &amp; A. Yessenalina for annota-
tions. Supported by NSF grant IIS-0910664.
</reference>
<sectionHeader confidence="0.938476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999813192307692">
[1] R. Chandrasekar, B. Srinivas. Automatic induction of rules
for text simplification. Knowledge-Based Systems, 1997.
[2] L. Del´eger, P. Zweigenbaum. Extracting lay paraphrases
of specialized expressions from monolingual comparable
medical corpora. Workshop on Building and Using Com-
parable Corpora, 2009.
[3] S. Devlin, J. Tait. The use of a psycholinguistic database in
the simplification of text for aphasic readers. In Linguistic
Databases, 1998.
[4] N. Elhadad, K. Sutaria. Mining a lexicon of technical terms
and lay equivalents. Workshop on BioNLP, 2007.
[5] B. Beigman Klebanov, K. Knight, D. Marcu. Text simplifi-
cation for information-seeking applications. OTM Confer-
ences, 2004.
[6] R. Nelken, S. M. Shieber. Towards robust context-sensitive
sentence alignment for monolingual corpora. EACL, 2006.
[7] R. Nelken, E. Yamangil. Mining Wikipedia’s article re-
vision history for training computational linguistics algo-
rithms. WikiAI, 2008.
[8] E. Shnarch, L. Barak, I. Dagan. Extracting lexical reference
rules from Wikipedia. ACL, 2009.
[9] A. Siddharthan, A. Nenkova, K. McKeown. Syntactic
simplification for improving content selection in multi-
document summarization. COLING, 2004.
[10] D. Vickrey, D. Koller. Sentence simplification for seman-
tic role labeling/ ACL, 2008.
</reference>
<page confidence="0.99826">
368
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.927302">
<title confidence="0.984951">For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia</title>
<author confidence="0.999164">Mark Yatskar</author>
<author confidence="0.999164">Bo Pang</author>
<author confidence="0.999164">Cristian Danescu-Niculescu-Mizil</author>
<author confidence="0.999164">Lillian</author>
<email confidence="0.998423">my89@cornell.edu,bopang@yahoo-inc.com,cristian@cs.cornell.edu,llee@cs.cornell.edu</email>
<abstract confidence="0.996257">We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task. We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations. We find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>R Barzilay</author>
<author>T Bruce</author>
<author>C Callison-Burch</author>
<author>J Cantwell</author>
<author>M Dredze</author>
<author>C Napoles</author>
<author>E Gabrilovich</author>
</authors>
<title>for initial investigations and helpful comments. We are also thankful to</title>
<marker>Barzilay, Bruce, Callison-Burch, Cantwell, Dredze, Napoles, Gabrilovich, </marker>
<rawString> for initial investigations and helpful comments. We are also thankful to R. Barzilay, T. Bruce, C. Callison-Burch, J. Cantwell, M. Dredze, C. Napoles, E. Gabrilovich, &amp; the reviewers for helpful comments; W. Arms and L. Walle for access to the Cornell Hadoop cluster; J. Cantwell for access to computational resources; R. Hwa &amp; A. Owens for annotation software; M. Ulinski for preliminary explorations; J. Cantwell, M. Ott, J. Silverstein, J. Yatskar, Y. Yatskar, &amp; A. Yessenalina for annotations. Supported by NSF grant IIS-0910664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chandrasekar</author>
<author>B Srinivas</author>
</authors>
<title>Automatic induction of rules for text simplification. Knowledge-Based Systems,</title>
<date>1997</date>
<contexts>
<context position="2047" citStr="[1, 9, 10]" startWordPosition="289" endWordPosition="291"> of spin-off of the well-known English Wikipedia (henceforth ComplexEW) where human editors enforce simplicity of language through rewriting. The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories, thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW. Importantly, not all the changes on SimpleEW are simplifications; we thus also make use of ComplexEW edits to filter out non-simplifications. Related work and related problems Previous work usually involves general syntactic-level trans1http://simple.wikipedia.org formation rules [1, 9, 10].2 In contrast, we explore data-driven methods to learn lexical simplifications (e.g., “collaborate” → “work together”), which are highly specific to the lexical items involved and thus cannot be captured by a few general rules. Simplification is strongly related to but distinct from paraphrasing and machine translation (MT). While it can be considered a directional form of the former, it differs in spirit because simplification must trade off meaning preservation (central to paraphrasing) against complexity reduction (not a consideration in paraphrasing). Simplification can also be considered</context>
</contexts>
<marker>[1]</marker>
<rawString>R. Chandrasekar, B. Srinivas. Automatic induction of rules for text simplification. Knowledge-Based Systems, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Del´eger</author>
<author>P Zweigenbaum</author>
</authors>
<title>Extracting lay paraphrases of specialized expressions from monolingual comparable medical corpora. Workshop on Building and Using Comparable Corpora,</title>
<date>2009</date>
<contexts>
<context position="3952" citStr="[4, 2]" startWordPosition="584" endWordPosition="585">ed above, a key idea in our work is to utilize SimpleEW edits. The primary difficulty in working with these modifications is that they include not only simplifications but also edits that serve other functions, such as spam removal or correction of grammar or factual content (“fixes”). We describe two main approaches to this problem: a probabilistic model that captures this mixture of different edit operations (§2.1), and the use of metadata to filter out undesirable revisions (§2.2). 2One exception [5] changes verb tense and replaces pronouns. Other lexical-level work focuses on medical text [4, 2], or uses frequency-filtered WordNet synonyms [3]. 3A type of lexical corruption, e.g., “acorn”--+“eggcorn”. 365 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 365–368, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics 2.1 Edit model We say that the kth article in a Wikipedia corresponds to (among other things) a title or topic (e.g., ~ “Cat”) and a sequence dk of article versions caused by successive edits. For a given lexical item or ~ phrase A, we write A E dk if there is any version ~ ~ in dk that c</context>
</contexts>
<marker>[2]</marker>
<rawString>L. Del´eger, P. Zweigenbaum. Extracting lay paraphrases of specialized expressions from monolingual comparable medical corpora. Workshop on Building and Using Comparable Corpora, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Devlin</author>
<author>J Tait</author>
</authors>
<title>The use of a psycholinguistic database in the simplification of text for aphasic readers. In Linguistic Databases,</title>
<date>1998</date>
<contexts>
<context position="4001" citStr="[3]" startWordPosition="591" endWordPosition="591">eEW edits. The primary difficulty in working with these modifications is that they include not only simplifications but also edits that serve other functions, such as spam removal or correction of grammar or factual content (“fixes”). We describe two main approaches to this problem: a probabilistic model that captures this mixture of different edit operations (§2.1), and the use of metadata to filter out undesirable revisions (§2.2). 2One exception [5] changes verb tense and replaces pronouns. Other lexical-level work focuses on medical text [4, 2], or uses frequency-filtered WordNet synonyms [3]. 3A type of lexical corruption, e.g., “acorn”--+“eggcorn”. 365 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 365–368, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics 2.1 Edit model We say that the kth article in a Wikipedia corresponds to (among other things) a title or topic (e.g., ~ “Cat”) and a sequence dk of article versions caused by successive edits. For a given lexical item or ~ phrase A, we write A E dk if there is any version ~ ~ in dk that contains A. From each dk we extract a collection e</context>
</contexts>
<marker>[3]</marker>
<rawString>S. Devlin, J. Tait. The use of a psycholinguistic database in the simplification of text for aphasic readers. In Linguistic Databases, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Elhadad</author>
<author>K Sutaria</author>
</authors>
<title>Mining a lexicon of technical terms and lay equivalents. Workshop on BioNLP,</title>
<date>2007</date>
<contexts>
<context position="3952" citStr="[4, 2]" startWordPosition="584" endWordPosition="585">ed above, a key idea in our work is to utilize SimpleEW edits. The primary difficulty in working with these modifications is that they include not only simplifications but also edits that serve other functions, such as spam removal or correction of grammar or factual content (“fixes”). We describe two main approaches to this problem: a probabilistic model that captures this mixture of different edit operations (§2.1), and the use of metadata to filter out undesirable revisions (§2.2). 2One exception [5] changes verb tense and replaces pronouns. Other lexical-level work focuses on medical text [4, 2], or uses frequency-filtered WordNet synonyms [3]. 3A type of lexical corruption, e.g., “acorn”--+“eggcorn”. 365 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 365–368, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics 2.1 Edit model We say that the kth article in a Wikipedia corresponds to (among other things) a title or topic (e.g., ~ “Cat”) and a sequence dk of article versions caused by successive edits. For a given lexical item or ~ phrase A, we write A E dk if there is any version ~ ~ in dk that c</context>
</contexts>
<marker>[4]</marker>
<rawString>N. Elhadad, K. Sutaria. Mining a lexicon of technical terms and lay equivalents. Workshop on BioNLP, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Beigman Klebanov</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Text simplification for information-seeking applications. OTM Conferences,</title>
<date>2004</date>
<contexts>
<context position="3854" citStr="[5]" startWordPosition="569" endWordPosition="569">of directional lexical relations, e.g, “eggcorns”3 [7] and entailments [8]. 2 Method As mentioned above, a key idea in our work is to utilize SimpleEW edits. The primary difficulty in working with these modifications is that they include not only simplifications but also edits that serve other functions, such as spam removal or correction of grammar or factual content (“fixes”). We describe two main approaches to this problem: a probabilistic model that captures this mixture of different edit operations (§2.1), and the use of metadata to filter out undesirable revisions (§2.2). 2One exception [5] changes verb tense and replaces pronouns. Other lexical-level work focuses on medical text [4, 2], or uses frequency-filtered WordNet synonyms [3]. 3A type of lexical corruption, e.g., “acorn”--+“eggcorn”. 365 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 365–368, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics 2.1 Edit model We say that the kth article in a Wikipedia corresponds to (among other things) a title or topic (e.g., ~ “Cat”) and a sequence dk of article versions caused by successive edits</context>
</contexts>
<marker>[5]</marker>
<rawString>B. Beigman Klebanov, K. Knight, D. Marcu. Text simplification for information-seeking applications. OTM Conferences, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nelken</author>
<author>S M Shieber</author>
</authors>
<title>Towards robust context-sensitive sentence alignment for monolingual corpora. EACL,</title>
<date>2006</date>
<contexts>
<context position="11057" citStr="[6]" startWordPosition="1859" endWordPosition="1859"> conditional probability. could prevent our bootstrapping methods from giving common fixes (e.g., “a” —* “the”) high scores. 3 Evaluation8 3.1 Data We obtained the revision histories of both SimpleEW (November 2009 snapshot) and ComplexEW (January 2008 snapshot). In total, —1.5M revisions for 81733 SimpleEW articles were processed (only 30% involved textual changes). For ComplexEW, we processed —16M revisions for 19407 articles. Extracting lexical edit instances. For each article, we aligned sentences in each pair of adjacent versions using tf-idf scores in a way similar to Nelken and Shieber [6] (this produced satisfying results because revisions tended to represent small changes). From the aligned sentence pairs, we obtained the aforementioned lexical edit instances A —* a. Since the focus of our study was not word alignment, we used a simple method that identified the longest differing segments (based on word boundaries) between each sentence, except that to prevent the extraction of entire (highly nonmatching) sentences, we filtered out A —* a pairs if either A or a contained more than five words. 3.2 Comparison points Baselines RANDOM returns lexical edit instances drawn uniforml</context>
</contexts>
<marker>[6]</marker>
<rawString>R. Nelken, S. M. Shieber. Towards robust context-sensitive sentence alignment for monolingual corpora. EACL, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nelken</author>
<author>E Yamangil</author>
</authors>
<title>Mining Wikipedia’s article revision history for training computational linguistics algorithms.</title>
<date>2008</date>
<location>WikiAI,</location>
<contexts>
<context position="3305" citStr="[7]" startWordPosition="481" endWordPosition="481">stion are highly related. However, note that ComplexEW and SimpleEW do not together constitute a clean parallel corpus, but rather an extremely noisy comparable corpus. For example, Complex/Simple same-topic document pairs are often written completely independently of each other, and even when it is possible to get good sentence alignments between them, the sentence pairs may reflect operations other than simplification, such as corrections, additions, or edit spam. Our work joins others in using Wikipedia revisions to learn interesting types of directional lexical relations, e.g, “eggcorns”3 [7] and entailments [8]. 2 Method As mentioned above, a key idea in our work is to utilize SimpleEW edits. The primary difficulty in working with these modifications is that they include not only simplifications but also edits that serve other functions, such as spam removal or correction of grammar or factual content (“fixes”). We describe two main approaches to this problem: a probabilistic model that captures this mixture of different edit operations (§2.1), and the use of metadata to filter out undesirable revisions (§2.2). 2One exception [5] changes verb tense and replaces pronouns. Other le</context>
</contexts>
<marker>[7]</marker>
<rawString>R. Nelken, E. Yamangil. Mining Wikipedia’s article revision history for training computational linguistics algorithms. WikiAI, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shnarch</author>
<author>L Barak</author>
<author>I Dagan</author>
</authors>
<title>Extracting lexical reference rules from Wikipedia.</title>
<date>2009</date>
<publisher>ACL,</publisher>
<contexts>
<context position="3325" citStr="[8]" startWordPosition="484" endWordPosition="484">ated. However, note that ComplexEW and SimpleEW do not together constitute a clean parallel corpus, but rather an extremely noisy comparable corpus. For example, Complex/Simple same-topic document pairs are often written completely independently of each other, and even when it is possible to get good sentence alignments between them, the sentence pairs may reflect operations other than simplification, such as corrections, additions, or edit spam. Our work joins others in using Wikipedia revisions to learn interesting types of directional lexical relations, e.g, “eggcorns”3 [7] and entailments [8]. 2 Method As mentioned above, a key idea in our work is to utilize SimpleEW edits. The primary difficulty in working with these modifications is that they include not only simplifications but also edits that serve other functions, such as spam removal or correction of grammar or factual content (“fixes”). We describe two main approaches to this problem: a probabilistic model that captures this mixture of different edit operations (§2.1), and the use of metadata to filter out undesirable revisions (§2.2). 2One exception [5] changes verb tense and replaces pronouns. Other lexical-level work foc</context>
</contexts>
<marker>[8]</marker>
<rawString>E. Shnarch, L. Barak, I. Dagan. Extracting lexical reference rules from Wikipedia. ACL, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Siddharthan</author>
<author>A Nenkova</author>
<author>K McKeown</author>
</authors>
<title>Syntactic simplification for improving content selection in multidocument summarization.</title>
<date>2004</date>
<publisher>COLING,</publisher>
<contexts>
<context position="2047" citStr="[1, 9, 10]" startWordPosition="289" endWordPosition="291"> of spin-off of the well-known English Wikipedia (henceforth ComplexEW) where human editors enforce simplicity of language through rewriting. The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories, thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW. Importantly, not all the changes on SimpleEW are simplifications; we thus also make use of ComplexEW edits to filter out non-simplifications. Related work and related problems Previous work usually involves general syntactic-level trans1http://simple.wikipedia.org formation rules [1, 9, 10].2 In contrast, we explore data-driven methods to learn lexical simplifications (e.g., “collaborate” → “work together”), which are highly specific to the lexical items involved and thus cannot be captured by a few general rules. Simplification is strongly related to but distinct from paraphrasing and machine translation (MT). While it can be considered a directional form of the former, it differs in spirit because simplification must trade off meaning preservation (central to paraphrasing) against complexity reduction (not a consideration in paraphrasing). Simplification can also be considered</context>
</contexts>
<marker>[9]</marker>
<rawString>A. Siddharthan, A. Nenkova, K. McKeown. Syntactic simplification for improving content selection in multidocument summarization. COLING, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vickrey</author>
<author>D Koller</author>
</authors>
<title>Sentence simplification for semantic role labeling/ ACL,</title>
<date>2008</date>
<contexts>
<context position="2047" citStr="[1, 9, 10]" startWordPosition="289" endWordPosition="291"> of spin-off of the well-known English Wikipedia (henceforth ComplexEW) where human editors enforce simplicity of language through rewriting. The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories, thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW. Importantly, not all the changes on SimpleEW are simplifications; we thus also make use of ComplexEW edits to filter out non-simplifications. Related work and related problems Previous work usually involves general syntactic-level trans1http://simple.wikipedia.org formation rules [1, 9, 10].2 In contrast, we explore data-driven methods to learn lexical simplifications (e.g., “collaborate” → “work together”), which are highly specific to the lexical items involved and thus cannot be captured by a few general rules. Simplification is strongly related to but distinct from paraphrasing and machine translation (MT). While it can be considered a directional form of the former, it differs in spirit because simplification must trade off meaning preservation (central to paraphrasing) against complexity reduction (not a consideration in paraphrasing). Simplification can also be considered</context>
</contexts>
<marker>[10]</marker>
<rawString>D. Vickrey, D. Koller. Sentence simplification for semantic role labeling/ ACL, 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>