<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000351">
<title confidence="0.977719">
Reformatting Web Documents via Header Trees
</title>
<author confidence="0.979818">
Minoru Yoshida and Hiroshi Nakagawa
</author>
<affiliation confidence="0.997401">
Information Technology Center, University of Tokyo
</affiliation>
<address confidence="0.900161">
7-3-1, Hongo, Bunkyo-ku, Tokyo 113-0033, Japan
CREST, JST
</address>
<email confidence="0.99849">
mino@r.dl.itc.u-tokyo.ac.jp, nakagawa@dl.itc.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.995631" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999469">
We propose a new method for reformat-
ting web documents by extracting seman-
tic structures from web pages. Our ap-
proach is to extract trees that describe hier-
archical relations in documents. We devel-
oped an algorithm for this task by employ-
ing the EM algorithm and clustering tech-
niques. Preliminary experiments showed
that our approach was more effective than
baseline methods.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955875">
This paper proposes a novel method for reformat-
ting (i.e., changing visual representations,) of web
documents. Our final goal is to implement the sys-
tem that appropriately reformats layouts of web doc-
uments by separating semantic aspects (like XML)
from layout aspects (like CSS) of web documents,
and changing the layout aspects while retaining the
semantic aspects.
We propose a header tree, which is a reasonable
choice as a semantic representation of web docu-
ments for this goal. Header trees can be seen as vari-
ants of XML trees where each internal node is not an
XML tag, but a header which is a part of document
that can be regarded as tags annotated to other parts
of the document. Titles, headlines, and attributes are
examples of headers. The left part of Figure 1 shows
an example web document. In this document, the
headers are About Me, which is a title, and NAME
and AGE, which are attributes. (For example, NAME
can be seen as a tag annotated to John Smith.)
Figure 2 shows a header tree for the example docu-
ment. It should be noted that each node is labeled
with parts of HTML pages, not abstract categories
such as XML tags.
</bodyText>
<page confidence="0.987256">
121
</page>
<table confidence="0.9446567">
About Me ...&lt;h1&gt;About Me&lt;/h1&gt;&lt;center&gt;&lt;br&gt;&lt;br&gt;
* NAME *&lt;br&gt;...
* NAME * HTML Source
John Smith
* AGE *
25
[ About, Me, NAME, John, Smith, ... ]
List of Blocks
Back to Home Page &lt;/h1&gt;&lt;center&gt;&lt;br&gt;&lt;br&gt;*
Web Page Separator
</table>
<figureCaption confidence="0.9618745">
Figure 1: An Example Web Document and Conver-
sion from HTML Documents to Block Lists.
</figureCaption>
<bodyText confidence="0.999876458333333">
Therefore, the required task is to extract header
trees from given web documents. Web documents
can be reformatted by converting their header trees
into various forms including Powerpoint-like in-
dented lists, HTML tables1, and Tree-class objects
of Java. We implemented the system that produces
these representations by extracting header trees from
given web documents.
One application of such reformatting is a web
browser on small devices that shows extracted
header trees regardless of original HTML visual ren-
dering. Trees can be used as compact representa-
tions of web documents because they show internal
structures of web documents concisely, and they can
be further augmented with open/close operations on
each node for the purpose of closing unnecessary
nodes, or sentence summarization on leaf nodes con-
taining long sentences. Another application is a lay-
out changer, which change a layout (i.e., HTML tag
usage) of one web page to another, by aligning ex-
tracted header trees of two web documents. Other
applications include HTML to XML transformation
and audio-browsable web content (Mukherjee et al.,
2003).
</bodyText>
<footnote confidence="0.999013">
1For example, the first column represents the root, the sec-
ond column represents its children, etc.
</footnote>
<note confidence="0.714066">
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
</note>
<page confidence="0.228795">
pages 121–124, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</page>
<figure confidence="0.9861505">
About Me
NAME
John Smith
AGE
25
Back to Home Page
</figure>
<figureCaption confidence="0.9557235">
Figure 2: A Header Tree for the Example Web Doc-
ument
</figureCaption>
<subsectionHeader confidence="0.523811">
1.1 Related Work
</subsectionHeader>
<bodyText confidence="0.999969037037037">
Several studies have addressed the problem of ex-
tracting logical structures from general HTML doc-
uments without labeled training examples. One
of these studies used domain-specific knowledge to
extract information used to organize logical struc-
tures (Chung et al., 2002). However, their ap-
proach cannot be applied to domains for which
any knowledge is not provided. Another type of
study employed algorithms to detect repeated pat-
terns in a list of HTML tags and texts (Yang and
Zhang, 2001; Nanno et al., 2003), or more struc-
tured forms (Mukherjee et al., 2003; Crescenzi et
al., 2001; Chang and Lui, 2001) such as DOM
trees. This approach might be useful for certain
types of web documents, particularly those with
highly regular formats such as www.yahoo.com
and www.amazon.com. However, in many cases,
HTML tag usage does not have so much regularity,
and, there are even the case where headers do not
repeat at all. Therefore, this type of algorithm may
be inadequate for the task of header extraction from
arbitrary web documents.
The remainder of this paper is organized as fol-
lows. Section 2 defines the terms used in this paper.
Section 3 provides the details of our algorithm. Sec-
tion 4 lists the experimental results and Section 5
concludes this paper.
</bodyText>
<sectionHeader confidence="0.998049" genericHeader="introduction">
2 Definitions
</sectionHeader>
<subsectionHeader confidence="0.999156">
2.1 Definition of Terms
</subsectionHeader>
<bodyText confidence="0.998889125">
Our system decomposes an HTML document into a
list of blocks. A block is defined as the part of a
web document that is separated by a separator. A
separator is a sequence of HTML tags and symbols.
Symbols are defined as characters in texts that are
neither numbers nor letters. Figure 1 shows an ex-
ample of the conversion of an HTML document to a
list of blocks.
</bodyText>
<figure confidence="0.4918065">
[ [About Me, [NAME, John Smith], [AGE, 25] ], Back to Home
Page] ]
</figure>
<figureCaption confidence="0.8829555">
Figure 3: A List Representation of the Example Web
Document
</figureCaption>
<bodyText confidence="0.998708285714286">
A header is defined as a block that modifies sub-
sequent blocks. In other words, a block that can be
a tag annotated to subsequent blocks is defined as a
header. Some examples of headers are Titles (e.g.,
“About Me”), Headlines (e.g., “Here is my pro-
file:”), Attributes (e.g., “Name”, “Age”, etc.), and
Dates.
</bodyText>
<subsectionHeader confidence="0.998447">
2.2 Definition of the Task
</subsectionHeader>
<bodyText confidence="0.999967888888889">
The system produces header trees for given web
documents. A header tree can be seen as an indented
list of blocks where the level of each node’s indent
is equal to the depth of the node, as shown in Figure
2. Therefore, the main part of our task is to give a
depth to each block in a given web document. After
that, some heuristic rules are employed to construct
header trees from a list of depths. In the next sec-
tion, we discuss the task of assigning a depth to each
block. Therefore, an input to the system is a list of
blocks and the output is a list of depths.
The system also produces nested-list representa-
tion of header trees for the purpose of evaluation. In
nested-list representation, each node that has chil-
dren is represented by the list whose first element
represents the parent and remaining elements repre-
sent the children. Figure 3 shows list representation
of the tree in Figure 2.
</bodyText>
<sectionHeader confidence="0.992842" genericHeader="method">
3 Header Extraction Algorithm
</sectionHeader>
<bodyText confidence="0.9990925">
In this section, we describe our algorithm that re-
ceives a list of blocks and returns a list of depths.
</bodyText>
<subsectionHeader confidence="0.997442">
3.1 Basic Concepts
</subsectionHeader>
<bodyText confidence="0.999989272727273">
The algorithm proceeds in two steps: separator cat-
egorization and block clustering. The first step
estimates local block relations (i.e., relations be-
tween neighboring blocks) via probabilistic models
for characters and tags that appear around separa-
tors. The second step supplements the first by ex-
tracting the undetermined relations between blocks
by focusing on global features, i.e., regularities in
HTML tag sequences. We employed a clustering
framework to implement a flexible regularity detec-
tion system that is robust to noise.
</bodyText>
<subsectionHeader confidence="0.976616">
3.2 STEP 1: Separator Categorization
</subsectionHeader>
<bodyText confidence="0.9988405">
The algorithm classifies each block relation into one
of three classes: NON-BOUNDARY, RELATING,
</bodyText>
<page confidence="0.953796">
122
</page>
<figure confidence="0.870114666666667">
NON-BOUNDARY RELATING UNRELATING
[ About, Me, NAME, John, Smith, AGE, ... ]
List of Blocks
Based on this model, each class of separators is
determined as follows:
c=
arg max P(s)P(c1s)P(11c)P(r1c).
�
RELATING NON-BOUNDARY
</figure>
<figureCaption confidence="0.999762">
Figure 4: An Example of Separator Categorization.
</figureCaption>
<bodyText confidence="0.994122692307692">
and UNRELATING. Both RELATING and UNRE-
LATING can be considered to be boundaries; how-
ever, blocks that sandwich RELATING separators
are regarded to consist of a header and its modified
block. Figure 4 shows an example of separator cate-
gorization for the list of blocks in Figure 1.
The left block of a RELATING separator must be
in the smaller depth than the right block. Figure 2
shows an example. In this tree, NAME is in a smaller
depth than John. On the other hand, both the left
and right blocks in a NON-BOUNDARY separator
must be in the same depth in a tree representation,
for example, John and Smith in Figure 2.
</bodyText>
<sectionHeader confidence="0.475104" genericHeader="method">
3.2.1 Local Model
</sectionHeader>
<bodyText confidence="0.97550335">
We use a probabilistic model that assumes the lo-
cality of relations among separators and blocks. In
this model, each separator s and the strings around
it, l and r, are modeled by means of the hidden vari-
able c, which indicates the class in which s is cate-
gorized. We use the character zerogram, unigram, or
bigram (changed according to the number of appear-
ances2) for l and r to avoid data sparseness prob-
lems.
For example, let us consider the following part of
the example document:
NAME: John Smith.
In this case, : is a separator, ME is the left string and
Jo is the right string.
Assuming the locality of separator appearances,
the model for all separators in a given document set
is defined as P(1, s, r) = F1 P(1, s, r) where 1 is a
vector of left strings, s is a vector of separators, and
r is a vector of right strings.
The joint probability of obtaining 1, s, and r is
</bodyText>
<equation confidence="0.976483">
P(1, s, r) = P(8)P(c18)P(11c)P(r1c)
</equation>
<bodyText confidence="0.999458">
assuming that 1 and r depend only on c: a class of
relation between the blocks around s.34
</bodyText>
<footnote confidence="0.848057571428572">
2This generalization is performed by a heuristic algorithm.
The main idea is to use a bigram if its number of appearances is
over a threshold, and unigrams or zerograms otherwise.
3If the frequency for (1, r) is over a threthold, P(1, rIc) is
used instead of P (1Ic)P (rIc).
4If the frequency for s is under a threthold, s is replaced by
its longest prefix whose frequency is over the threthold.
</footnote>
<bodyText confidence="0.9997793">
The hidden parameters P(cls), P(11c), and
P (rIc), are estimated by the EM algorithm (Demp-
ster et al., 1977). Starting with arbitrary initial pa-
rameters, the EM algorithm iterates E-STEPs and
M-STEPs in order to increase the (log-)likelihood
function log P(1, s, r) = E log P(1, s, r).
To characterize each class of separators, we use a
set of typical symbols and HTML tags, called rep-
resentatives from each class. This constraint con-
tributes to give a structure to the parameter space.
</bodyText>
<subsectionHeader confidence="0.99375">
3.3 STEP 2: Block Clustering
</subsectionHeader>
<bodyText confidence="0.951505619047619">
The purpose of block clustering is to take advantage
of the regularity in visual representations. For exam-
ple, we can observe regularity between NAME and
AGE in Figure 1 because both are sandwiched by the
character * and preceded by a null line. This visual
representation is described in the HTML source as,
for example,
... &lt;br&gt;&lt;br&gt;* NAME *&lt;br&gt; ...
... &lt;br&gt;&lt;br&gt;* AGE *&lt;br&gt; ...
Our idea is to define the similarities between (con-
text of) blocks based on the similarities between
their surrounding separators. Each separator is rep-
resented by the vector that consist of symbols and
HTML tags included in it, and the similarity be-
tween separators are calculated as cosine values.
The algorithm proceeds in a bottom-up manner by
examining a given block list from tail to head, find-
ing the block that is the most similar to the current
block, and collecting them into the same cluster. Af-
ter that, all blocks in the same cluster is assigned the
same depth.
</bodyText>
<sectionHeader confidence="0.979511" genericHeader="method">
4 Preliminary Experiments
</sectionHeader>
<bodyText confidence="0.9996652">
We used a training data that consists of 1,418 web
documents5 of moderate file size6 that did not have
“src” or “script” tags7. The former criteria is based
on the observation that too small or too large doc-
uments are hard to use for measuring performance
of algorithms, and the latter criteria is caused by the
fact our system currently has no module to handle
image files as blocks.
We randomly selected 20 documents as test doc-
uments. Each test document was bracketed by hand
</bodyText>
<footnote confidence="0.9910735">
5They are collected by retrieving all user pages on one server
of a Japanese ISP.
6from 1,000 to 10,000 bytes
7Src tags indicate inclusion of image files, java codes, etc
</footnote>
<page confidence="0.991475">
123
</page>
<table confidence="0.9996704">
Algorithm Recall Precision F-measure
OUR ALGORITHM 0.477 0.266 0.329
NO-CL 0.178 0.119 0.139
NO-EM 0.389 0.211 0.265
PREV 0.144 0.615 0.202
</table>
<tableCaption confidence="0.9299345">
Table 1: Macro-Averaged Recall, Precision, and F-
measure on Test Documents
</tableCaption>
<bodyText confidence="0.998729853658536">
to evaluate machine-made bracketings. The per-
formance of web-page structuring algorithms can
be evaluated via the nested-list form of tree by
bracketed recall and bracketed precision (Goodman,
1996). Recall is the rate that bracketing given by
hand are also given by machine, and precision is the
rate that bracketing given by machine are also given
by hand. F-measure is a harmonic mean of recall and
precision that is used as a combined measure. Recall
and precision were evaluated for each test document
and they were averaged across all test documents.
These averaged values are called macro-average re-
call, precision, and f-measure (Yang, 1999).
We implemented our algorithm and the following
three ones as baselines.
NO-CL does not perform block clustering.
NO-EM does not perform the EM-parameter-
estimation. Every boundary but representatives
is defined to be categorized as “UNRELAT-
ING”.
PREV performs neither the EM-learning nor the
block clustering. Every boundary but represen-
tatives is defined to be categorized as “NON-
BOUNDARY”8. It uses the heuristics that “ev-
ery block depends on its previous block.”
Table 1 shows the result. We observed that use of
both the EM-learning and block clustering resulted
in the best performance. NO-EM performs the best
among the three baselines. It suggests that only rely-
ing on HTML tag information is not a so bad strat-
egy when the EM-training is not available because
of, for example, the lack of a sufficient number of
training examples.
Results on the documents that were rich in HTML
tags with highly coherent layouts were better than
those on the others like the documents with poor
separators such as only one space character or one
line feed. Some of the current results on the doc-
uments with such poor visual cues seemed difficult
for use in practical systems, which indicates our sys-
tem still leaves room for improvement.
</bodyText>
<footnote confidence="0.9493675">
8This strategy is based on the fact that it maximized the per-
formance in a preliminary investigation.
</footnote>
<sectionHeader confidence="0.989349" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999979416666667">
This paper proposed a method for reformatting web
documents by extracting header trees that give hi-
erarchical structures of web documents. Prelimi-
nary experiments showed that the proposed algo-
rithm was effective compared with some baseline
methods. However, the performance of the algo-
rithm on some of the test documents was not suf-
ficient for practical use. We plan to improve the
performance by, for example, using larger amount
of training examples. Finding other reformatting
strategies in addition to the ones proposed in this pa-
per is also important future work.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999772857142857">
Chia-Hui Chang and Shao-Chen Lui. 2001. IEPAD: In-
formation extraction based on pattern discovery. In
Proceedings of WWW2001, pages 681–688.
Christina Yip Chung, Michael Gertz, and Neel Sundare-
san. 2002. Reverse engineering for web data: From
visual to semantic structures. In ICDE.
Valter Crescenzi, Giansalvatore Mecca, and Paolo Meri-
aldo. 2001. ROADRUNNER: Towards automatic data
extraction from large web sites. In Proceedings of
VLDB ’01, pages 109–118.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM al-
gorithm. Journal of Royal Statistical Society: Series
B, 39:1–38.
Joshua Goodman. 1996. Parsing algorithms and metrics.
In Proceedings ofACL96, pages 177–183.
Saikat Mukherjee, Guizhen Yang, Wenfang Tan, and I.V.
Ramakrishnan. 2003. Automatic discovery of seman-
tic structures in HTML documents. In Proceedings of
ICDAR 2003.
Tomoyuki Nanno, Suguru Saito, and Manabu Okumura.
2003. Structuring web pages based on repetition of
elements. In Proceedings of WDA2003.
Yudong Yang and Hongjiang Zhang. 2001. HTML page
analysis based on visual cues. In Proceedings of IC-
DAR01.
Yiming Yang. 1999. An evaluation of statistical ap-
proaches to text categorization. INRT, 1:69–90.
</reference>
<page confidence="0.998312">
124
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.555950">
<title confidence="0.9996">Reformatting Web Documents via Header Trees</title>
<author confidence="0.994414">Minoru Yoshida</author>
<author confidence="0.994414">Hiroshi Nakagawa</author>
<affiliation confidence="0.999429">Information Technology Center, University of Tokyo</affiliation>
<address confidence="0.798945">7-3-1, Hongo, Bunkyo-ku, Tokyo 113-0033, Japan CREST, JST</address>
<email confidence="0.899533">mino@r.dl.itc.u-tokyo.ac.jp,nakagawa@dl.itc.u-tokyo.ac.jp</email>
<abstract confidence="0.999254181818182">We propose a new method for reformatting web documents by extracting semantic structures from web pages. Our approach is to extract trees that describe hierarchical relations in documents. We developed an algorithm for this task by employing the EM algorithm and clustering techniques. Preliminary experiments showed that our approach was more effective than baseline methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chia-Hui Chang</author>
<author>Shao-Chen Lui</author>
</authors>
<title>IEPAD: Information extraction based on pattern discovery.</title>
<date>2001</date>
<booktitle>In Proceedings of WWW2001,</booktitle>
<pages>681--688</pages>
<contexts>
<context position="4200" citStr="Chang and Lui, 2001" startWordPosition="678" endWordPosition="681">Related Work Several studies have addressed the problem of extracting logical structures from general HTML documents without labeled training examples. One of these studies used domain-specific knowledge to extract information used to organize logical structures (Chung et al., 2002). However, their approach cannot be applied to domains for which any knowledge is not provided. Another type of study employed algorithms to detect repeated patterns in a list of HTML tags and texts (Yang and Zhang, 2001; Nanno et al., 2003), or more structured forms (Mukherjee et al., 2003; Crescenzi et al., 2001; Chang and Lui, 2001) such as DOM trees. This approach might be useful for certain types of web documents, particularly those with highly regular formats such as www.yahoo.com and www.amazon.com. However, in many cases, HTML tag usage does not have so much regularity, and, there are even the case where headers do not repeat at all. Therefore, this type of algorithm may be inadequate for the task of header extraction from arbitrary web documents. The remainder of this paper is organized as follows. Section 2 defines the terms used in this paper. Section 3 provides the details of our algorithm. Section 4 lists the e</context>
</contexts>
<marker>Chang, Lui, 2001</marker>
<rawString>Chia-Hui Chang and Shao-Chen Lui. 2001. IEPAD: Information extraction based on pattern discovery. In Proceedings of WWW2001, pages 681–688.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Yip Chung</author>
<author>Michael Gertz</author>
<author>Neel Sundaresan</author>
</authors>
<title>Reverse engineering for web data: From visual to semantic structures.</title>
<date>2002</date>
<booktitle>In ICDE.</booktitle>
<contexts>
<context position="3863" citStr="Chung et al., 2002" startWordPosition="618" endWordPosition="621">umn represents the root, the second column represents its children, etc. Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 121–124, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics About Me NAME John Smith AGE 25 Back to Home Page Figure 2: A Header Tree for the Example Web Document 1.1 Related Work Several studies have addressed the problem of extracting logical structures from general HTML documents without labeled training examples. One of these studies used domain-specific knowledge to extract information used to organize logical structures (Chung et al., 2002). However, their approach cannot be applied to domains for which any knowledge is not provided. Another type of study employed algorithms to detect repeated patterns in a list of HTML tags and texts (Yang and Zhang, 2001; Nanno et al., 2003), or more structured forms (Mukherjee et al., 2003; Crescenzi et al., 2001; Chang and Lui, 2001) such as DOM trees. This approach might be useful for certain types of web documents, particularly those with highly regular formats such as www.yahoo.com and www.amazon.com. However, in many cases, HTML tag usage does not have so much regularity, and, there are </context>
</contexts>
<marker>Chung, Gertz, Sundaresan, 2002</marker>
<rawString>Christina Yip Chung, Michael Gertz, and Neel Sundaresan. 2002. Reverse engineering for web data: From visual to semantic structures. In ICDE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valter Crescenzi</author>
<author>Giansalvatore Mecca</author>
<author>Paolo Merialdo</author>
</authors>
<title>ROADRUNNER: Towards automatic data extraction from large web sites.</title>
<date>2001</date>
<booktitle>In Proceedings of VLDB ’01,</booktitle>
<pages>109--118</pages>
<contexts>
<context position="4178" citStr="Crescenzi et al., 2001" startWordPosition="674" endWordPosition="677">xample Web Document 1.1 Related Work Several studies have addressed the problem of extracting logical structures from general HTML documents without labeled training examples. One of these studies used domain-specific knowledge to extract information used to organize logical structures (Chung et al., 2002). However, their approach cannot be applied to domains for which any knowledge is not provided. Another type of study employed algorithms to detect repeated patterns in a list of HTML tags and texts (Yang and Zhang, 2001; Nanno et al., 2003), or more structured forms (Mukherjee et al., 2003; Crescenzi et al., 2001; Chang and Lui, 2001) such as DOM trees. This approach might be useful for certain types of web documents, particularly those with highly regular formats such as www.yahoo.com and www.amazon.com. However, in many cases, HTML tag usage does not have so much regularity, and, there are even the case where headers do not repeat at all. Therefore, this type of algorithm may be inadequate for the task of header extraction from arbitrary web documents. The remainder of this paper is organized as follows. Section 2 defines the terms used in this paper. Section 3 provides the details of our algorithm.</context>
</contexts>
<marker>Crescenzi, Mecca, Merialdo, 2001</marker>
<rawString>Valter Crescenzi, Giansalvatore Mecca, and Paolo Merialdo. 2001. ROADRUNNER: Towards automatic data extraction from large web sites. In Proceedings of VLDB ’01, pages 109–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of Royal Statistical Society: Series B,</journal>
<pages>39--1</pages>
<contexts>
<context position="9867" citStr="Dempster et al., 1977" startWordPosition="1674" endWordPosition="1678">) = P(8)P(c18)P(11c)P(r1c) assuming that 1 and r depend only on c: a class of relation between the blocks around s.34 2This generalization is performed by a heuristic algorithm. The main idea is to use a bigram if its number of appearances is over a threshold, and unigrams or zerograms otherwise. 3If the frequency for (1, r) is over a threthold, P(1, rIc) is used instead of P (1Ic)P (rIc). 4If the frequency for s is under a threthold, s is replaced by its longest prefix whose frequency is over the threthold. The hidden parameters P(cls), P(11c), and P (rIc), are estimated by the EM algorithm (Dempster et al., 1977). Starting with arbitrary initial parameters, the EM algorithm iterates E-STEPs and M-STEPs in order to increase the (log-)likelihood function log P(1, s, r) = E log P(1, s, r). To characterize each class of separators, we use a set of typical symbols and HTML tags, called representatives from each class. This constraint contributes to give a structure to the parameter space. 3.3 STEP 2: Block Clustering The purpose of block clustering is to take advantage of the regularity in visual representations. For example, we can observe regularity between NAME and AGE in Figure 1 because both are sandw</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of Royal Statistical Society: Series B, 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In Proceedings ofACL96,</booktitle>
<pages>177--183</pages>
<contexts>
<context position="12321" citStr="Goodman, 1996" startWordPosition="2090" endWordPosition="2091">ment was bracketed by hand 5They are collected by retrieving all user pages on one server of a Japanese ISP. 6from 1,000 to 10,000 bytes 7Src tags indicate inclusion of image files, java codes, etc 123 Algorithm Recall Precision F-measure OUR ALGORITHM 0.477 0.266 0.329 NO-CL 0.178 0.119 0.139 NO-EM 0.389 0.211 0.265 PREV 0.144 0.615 0.202 Table 1: Macro-Averaged Recall, Precision, and Fmeasure on Test Documents to evaluate machine-made bracketings. The performance of web-page structuring algorithms can be evaluated via the nested-list form of tree by bracketed recall and bracketed precision (Goodman, 1996). Recall is the rate that bracketing given by hand are also given by machine, and precision is the rate that bracketing given by machine are also given by hand. F-measure is a harmonic mean of recall and precision that is used as a combined measure. Recall and precision were evaluated for each test document and they were averaged across all test documents. These averaged values are called macro-average recall, precision, and f-measure (Yang, 1999). We implemented our algorithm and the following three ones as baselines. NO-CL does not perform block clustering. NO-EM does not perform the EM-para</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Joshua Goodman. 1996. Parsing algorithms and metrics. In Proceedings ofACL96, pages 177–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saikat Mukherjee</author>
<author>Guizhen Yang</author>
<author>Wenfang Tan</author>
<author>I V Ramakrishnan</author>
</authors>
<title>Automatic discovery of semantic structures in HTML documents.</title>
<date>2003</date>
<booktitle>In Proceedings of ICDAR</booktitle>
<contexts>
<context position="3215" citStr="Mukherjee et al., 2003" startWordPosition="517" endWordPosition="520">nal HTML visual rendering. Trees can be used as compact representations of web documents because they show internal structures of web documents concisely, and they can be further augmented with open/close operations on each node for the purpose of closing unnecessary nodes, or sentence summarization on leaf nodes containing long sentences. Another application is a layout changer, which change a layout (i.e., HTML tag usage) of one web page to another, by aligning extracted header trees of two web documents. Other applications include HTML to XML transformation and audio-browsable web content (Mukherjee et al., 2003). 1For example, the first column represents the root, the second column represents its children, etc. Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 121–124, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics About Me NAME John Smith AGE 25 Back to Home Page Figure 2: A Header Tree for the Example Web Document 1.1 Related Work Several studies have addressed the problem of extracting logical structures from general HTML documents without labeled training examples. One of these studies used domain-specific knowledge to extract information used to </context>
</contexts>
<marker>Mukherjee, Yang, Tan, Ramakrishnan, 2003</marker>
<rawString>Saikat Mukherjee, Guizhen Yang, Wenfang Tan, and I.V. Ramakrishnan. 2003. Automatic discovery of semantic structures in HTML documents. In Proceedings of ICDAR 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoyuki Nanno</author>
<author>Suguru Saito</author>
<author>Manabu Okumura</author>
</authors>
<title>Structuring web pages based on repetition of elements.</title>
<date>2003</date>
<booktitle>In Proceedings of WDA2003.</booktitle>
<contexts>
<context position="4104" citStr="Nanno et al., 2003" startWordPosition="661" endWordPosition="664">E John Smith AGE 25 Back to Home Page Figure 2: A Header Tree for the Example Web Document 1.1 Related Work Several studies have addressed the problem of extracting logical structures from general HTML documents without labeled training examples. One of these studies used domain-specific knowledge to extract information used to organize logical structures (Chung et al., 2002). However, their approach cannot be applied to domains for which any knowledge is not provided. Another type of study employed algorithms to detect repeated patterns in a list of HTML tags and texts (Yang and Zhang, 2001; Nanno et al., 2003), or more structured forms (Mukherjee et al., 2003; Crescenzi et al., 2001; Chang and Lui, 2001) such as DOM trees. This approach might be useful for certain types of web documents, particularly those with highly regular formats such as www.yahoo.com and www.amazon.com. However, in many cases, HTML tag usage does not have so much regularity, and, there are even the case where headers do not repeat at all. Therefore, this type of algorithm may be inadequate for the task of header extraction from arbitrary web documents. The remainder of this paper is organized as follows. Section 2 defines the </context>
</contexts>
<marker>Nanno, Saito, Okumura, 2003</marker>
<rawString>Tomoyuki Nanno, Suguru Saito, and Manabu Okumura. 2003. Structuring web pages based on repetition of elements. In Proceedings of WDA2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yudong Yang</author>
<author>Hongjiang Zhang</author>
</authors>
<title>HTML page analysis based on visual cues.</title>
<date>2001</date>
<booktitle>In Proceedings of ICDAR01.</booktitle>
<contexts>
<context position="4083" citStr="Yang and Zhang, 2001" startWordPosition="657" endWordPosition="660">nguistics About Me NAME John Smith AGE 25 Back to Home Page Figure 2: A Header Tree for the Example Web Document 1.1 Related Work Several studies have addressed the problem of extracting logical structures from general HTML documents without labeled training examples. One of these studies used domain-specific knowledge to extract information used to organize logical structures (Chung et al., 2002). However, their approach cannot be applied to domains for which any knowledge is not provided. Another type of study employed algorithms to detect repeated patterns in a list of HTML tags and texts (Yang and Zhang, 2001; Nanno et al., 2003), or more structured forms (Mukherjee et al., 2003; Crescenzi et al., 2001; Chang and Lui, 2001) such as DOM trees. This approach might be useful for certain types of web documents, particularly those with highly regular formats such as www.yahoo.com and www.amazon.com. However, in many cases, HTML tag usage does not have so much regularity, and, there are even the case where headers do not repeat at all. Therefore, this type of algorithm may be inadequate for the task of header extraction from arbitrary web documents. The remainder of this paper is organized as follows. S</context>
</contexts>
<marker>Yang, Zhang, 2001</marker>
<rawString>Yudong Yang and Hongjiang Zhang. 2001. HTML page analysis based on visual cues. In Proceedings of ICDAR01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
</authors>
<title>An evaluation of statistical approaches to text categorization.</title>
<date>1999</date>
<journal>INRT,</journal>
<pages>1--69</pages>
<contexts>
<context position="12772" citStr="Yang, 1999" startWordPosition="2165" endWordPosition="2166">The performance of web-page structuring algorithms can be evaluated via the nested-list form of tree by bracketed recall and bracketed precision (Goodman, 1996). Recall is the rate that bracketing given by hand are also given by machine, and precision is the rate that bracketing given by machine are also given by hand. F-measure is a harmonic mean of recall and precision that is used as a combined measure. Recall and precision were evaluated for each test document and they were averaged across all test documents. These averaged values are called macro-average recall, precision, and f-measure (Yang, 1999). We implemented our algorithm and the following three ones as baselines. NO-CL does not perform block clustering. NO-EM does not perform the EM-parameterestimation. Every boundary but representatives is defined to be categorized as “UNRELATING”. PREV performs neither the EM-learning nor the block clustering. Every boundary but representatives is defined to be categorized as “NONBOUNDARY”8. It uses the heuristics that “every block depends on its previous block.” Table 1 shows the result. We observed that use of both the EM-learning and block clustering resulted in the best performance. NO-EM p</context>
</contexts>
<marker>Yang, 1999</marker>
<rawString>Yiming Yang. 1999. An evaluation of statistical approaches to text categorization. INRT, 1:69–90.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>