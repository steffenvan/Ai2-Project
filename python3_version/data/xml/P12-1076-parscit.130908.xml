<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.612155">
Reducing Wrong Labels in Distant Supervision for Relation Extraction
</title>
<author confidence="0.788521">
Shingo Takamatsu Issei Sato and Hiroshi Nakagawa
</author>
<affiliation confidence="0.906129">
System Technologies Laboratories Information Technology Center
Sony Corporation The University of Tokyo
</affiliation>
<address confidence="0.957992">
5-1-12 Kitashinagawa, Shinagawa-ku, Tokyo 7-3-1 Hongo, Bunkyo-ku, Tokyo
</address>
<email confidence="0.995627">
Shingo.Takamatsu@jp.sony.com {sato@r., n3@}dl.itc.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.998576" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999709739130435">
In relation extraction, distant supervision
seeks to extract relations between entities
from text by using a knowledge base, such as
Freebase, as a source of supervision. When
a sentence and a knowledge base refer to the
same entity pair, this approach heuristically la-
bels the sentence with the corresponding re-
lation in the knowledge base. However, this
heuristic can fail with the result that some sen-
tences are labeled wrongly. This noisy labeled
data causes poor extraction performance. In
this paper, we propose a method to reduce
the number of wrong labels. We present a
novel generative model that directly models
the heuristic labeling process of distant super-
vision. The model predicts whether assigned
labels are correct or wrong via its hidden vari-
ables. Our experimental results show that this
model detected wrong labels with higher per-
formance than baseline methods. In the ex-
periment, we also found that our wrong label
reduction boosted the performance of relation
extraction.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999308333333333">
Machine learning approaches have been developed
to address relation extraction, which is the task of
extracting semantic relations between entities ex-
pressed in text. Supervised approaches are limited in
scalability because labeled data is expensive to pro-
duce. A particularly attractive approach, called dis-
tant supervision (DS), creates labeled data by heuris-
tically aligning entities in text with those in a knowl-
edge base, such as Freebase (Mintz et al., 2009).
</bodyText>
<figureCaption confidence="0.996149333333333">
Figure 1: Automatic labeling by distant supervision. Up-
per sentence: correct labeling; lower sentence: incorrect
labeling.
</figureCaption>
<bodyText confidence="0.998084714285714">
With DS it is assumed that if a sentence contains
an entity pair in a knowledge base, such a sentence
actually expresses the corresponding relation in the
knowledge base.
However, the DS assumption can fail, which re-
sults in noisy labeled data and this causes poor ex-
traction performance. An entity pair in a target text
generally expresses more than one relation while
a knowledge base stores a subset of the relations.
The assumption ignores this possibility. For in-
stance, consider the place of birth relation between
Michael Jackson and Gary in Figure 1. The upper
sentence indeed expresses the place of birth relation
between the two entities. In DS place of birth is as-
signed to the sentence, and it becomes a useful train-
ing example. On the other hand, the lower sentence
does not express this relation between the two enti-
ties, but the DS heuristic wrongly labels the sentence
as expressing it.
Riedel et al. (2010) relax the DS assumption as
at least one sentence containing an entity pair ex-
</bodyText>
<page confidence="0.96324">
721
</page>
<note confidence="0.9857885">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 721–729,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999328210526316">
pressing the corresponding relation in the knowl-
edge base. They cast the relaxed assumption as
multi-instance learning. However, even the relaxed
assumption can fail. The relaxation is equivalent to
the DS assumption when a labeled pair of entities
is mentioned once in a target corpus (Riedel et al.,
2010). In fact, 91.7% of entity pairs appear only
once in Wikipedia articles (see Section 7).
In this paper, we propose a method to reduce the
number of wrong labels generated by DS without
using either of these assumptions. Given the labeled
corpus created with the DS assumption, we first pre-
dict whether each pattern, which frequently appears
in text to express a relation (see Section 4), expresses
a target relation. Patterns that are predicted not to ex-
press the relation are used to form a negative pattern
list for removing wrong labels of the relation.
The main contributions of this paper are as fol-
lows:
</bodyText>
<listItem confidence="0.916517571428571">
• To make the pattern prediction, we propose a
generative model that directly models the pro-
cess of automatic labeling in DS. Without any
strong assumptions like Riedel et al. (2010)’s,
the model predicts whether each pattern ex-
presses each relation via hidden variables (see
Section 5).
• Our variational inference for our generative
model lets us automatically calibrate parame-
ters for each relation, which are sensitive to the
performance (see Section 6).
• We applied our method to Wikipedia articles
using Freebase as a knowledge base and found
that (i) our model identified patterns express-
ing a given relation more accurately than base-
line methods and (ii) our method led to bet-
ter extraction performance than the original DS
(Mintz et al., 2009) and MultiR (Hoffmann et
al., 2011), which is a state-of-the-art multi-
instance learning system for relation extraction
(see Section 7).
</listItem>
<sectionHeader confidence="0.999754" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99997525">
The increasingly popular approach, called distant
supervision (DS), or weak supervision, utilizes a
knowledge base to heuristically label a corpus (Wu
and Weld, 2007; Bellare and McCallum, 2007; Pal
et al., 2007). Our work was inspired by Mintz et al.
(2009) who used Freebase as a knowledge base by
making the DS assumption and trained relation ex-
tractors on Wikipedia. Previous works (Hoffmann
et al., 2010; Yao et al., 2010) have pointed out that
the DS assumption generates noisy labeled data, but
did not directly address the problem. Wang et al.
(2011) applied a rule-based method to the problem
by using popular entity types and keywords for each
relation. In (Bellare and McCallum, 2007; Riedel et
al., 2010; Hoffmann et al., 2011), they used multi-
instance learning, which deals with uncertainty of
labels, to relax the DS assumption. However, the re-
laxed assumption can fail when a labeled entity pair
is mentioned only once in a corpus (Riedel et al.,
2010). Our approach relies on neither of these as-
sumptions.
Bootstrapping for relation extraction (Riloff and
Jones, 1999; Pantel and Pennacchiotti, 2006; Carl-
son et al., 2010) is related to our method. In boot-
strapping, seed entity pairs of the target relation are
given in order to select reliable patterns, which are
used to extract new entity pairs. To avoid the selec-
tion of unreliable patterns, bootstrapping introduces
scoring functions for each pattern candidate. This
can be applied to our approach, which seeks to re-
duce the number of unreliable patterns by using a set
of given entity pairs. However, the bootstrapping-
like approach suffers from sensitive parameters that
are critical to its performance. Ideally, the parame-
ters such as a threshold for scoring function should
be determined for each relation, but there are no
principled methods (Komachi et al., 2008). In our
approach, parameters are calibrated for each rela-
tion by maximizing the likelihood of our generative
model.
</bodyText>
<sectionHeader confidence="0.986776" genericHeader="method">
3 Knowledge-based Distant Supervision
</sectionHeader>
<bodyText confidence="0.992747">
In this section, we describe DS for relation extrac-
tion. We use the term relation as the relation be-
tween two entities. A relation instance is a tuple
consisting of two entities and relation r. For exam-
ple, place of birth(Michael Jackson, Gary) in Fig-
ure 1 is a relation instance.
Relation extraction seeks to extract relation in-
stances from text. An entity is mentioned as a named
entity in text. We extract a relation instance from a
</bodyText>
<page confidence="0.99502">
722
</page>
<bodyText confidence="0.999935457142857">
single sentence. For example, from the upper sen-
tence in Figure 1 we extract place of birth(Michael
Jackson, Gary). Since two entities mentioned in a
sentence do not always have a relation, we select en-
tity pairs from a corpus when: (i) the path of the de-
pendency parse tree between the corresponding two
named entities in the sentence is no longer than 4
and (ii) the path does not contain a sentence-like
boundary, such as a relative clause1 (Banko et al.,
2007; Banko and Etzioni, 2008). Banko and Et-
zioni (2008) found that a set of eight lexico-syntactic
forms covers nearly 95% of relation phrases in their
corpus. (Fader et al. (2011) found that this set covers
69% of their corpus). Our rule is designed to cover
at least the eight lexico-syntactic forms. We use the
entity pairs extracted by this rule.
DS uses a knowledge base to create labeled data
for relation extraction by heuristically matching en-
tity pairs. A knowledge base is a set of relation
instances about predefined relations. For each sen-
tence in the corpus, we extract all of its entity pairs.
Then, for each entity pair, we try to retrieve the rela-
tion instances about the entity pair from the knowl-
edge base. If we found such a relation instance, then
the set of its relation, the entity pair, and the sentence
is stored as a positive example. If not, then the set of
the entity pair and the sentence is stored as a nega-
tive example. Features of an entity pair are extracted
from the sentence containing the entity pair.
As mentioned in Section 1, the assumption of DS
can fail, resulting in wrong assignments of a relation
to sentences that do not express the relation. We call
such assignments wrong labels. An example of a
wrong label is place of birth assigned to the lower
sentence in Figure 1.
</bodyText>
<sectionHeader confidence="0.969971" genericHeader="method">
4 Wrong Label Reduction
</sectionHeader>
<bodyText confidence="0.999302333333333">
We define a pattern as the entity types of an entity
pair2 as well as the sequence of words on the path
of the dependency parse tree from the first entity to
the second one. For example, from “Michael Jack-
son was born in Gary” in Figure 1, the pattern “[Per-
son] born in [Location]” is extracted. We use entity
</bodyText>
<footnote confidence="0.98919925">
1We reject sentence-like dependencies such as ccomp, com-
plm and mark
2If we use a standard named entity tagger, the entity types
are Person, Location, and Organization.
</footnote>
<figure confidence="0.6309956">
Algorithm 1 Wrong Label Reduction
labeled data generated by DS: LD
negative patterns for relation r: NegPat(r)
for each entry (r, Pair, Sentence) in LD do
pattern Pat ← the pattern from (Pair, Sentence)
if Pat E NegPat(r) then
remove (r, Pair, Sentence) from LD
end if
end for
return LD
</figure>
<bodyText confidence="0.999958615384615">
types to distinguish the sentences that express differ-
ent relations with the same dependency path, such
as “ABBA was formed in Stockholm.” and “ABBA
was formed in 1970.”
Our aim is to remove wrong labels assigned to
frequent patterns, which cause poor precision. In-
deed, in our Wikipedia corpus, more than 6% of the
sentences containing the pattern “[Person] moved to
[Location]”, which does not express place of death,
are labeled as place of death, and the labels as-
signed to these sentences hurt extraction perfor-
mance (see Section 7.3.3). We would like to remove
place of death from the sentences that contain this
pattern.
In our method, we reduce the number of wrong
labels as follows: (i) given a labeled corpus with the
DS assumption, we first predict whether a pattern
expresses a relation and then (ii) remove wrong la-
bels using the negative pattern list, which is defined
as patterns that are predicted not to express the rela-
tion. In the first step, we introduce the novel gener-
ative model that directly models DS’s labeling pro-
cess and make the prediction (see Section 5). The
second step is formally described in Algorithm 1.
For relation extraction, we train a classifier for en-
tity pairs using the resultant labeled data.
</bodyText>
<sectionHeader confidence="0.999871" genericHeader="method">
5 Generative Model
</sectionHeader>
<bodyText confidence="0.995198111111111">
We now describe our generative model, which pre-
dicts whether a pattern expresses relation r or not
via hidden variables. In this section, we consider re-
lation r since parameters are conditionally indepen-
dent if relation r and the hyperparameter are given.
An observation of our model is whether entity
pair i appearing with pattern s in the corpus is la-
beled with relation r or not. Our binary observa-
tions are written as Xr = {(xrsi)|s = 1, ... , S, i =
</bodyText>
<page confidence="0.997851">
723
</page>
<figureCaption confidence="0.973592333333333">
Figure 2: Graphical model representation of our model.
R indicates the number of relations. S is the number of
patterns. Ns is the number of entity pairs that appear
with pattern s in the corpus. xrsi is the observed vari-
ables. The circled variables except xrsi are parameters
or hidden variables. A is the hyperparameter and mst is
constant. The boxes are “plates” representing replicates.
Figure 3: Venn diagram-like description. E1 and E2 are
sets of entity pairs. E1/E2 has 6/4 entity pairs because
the 6/4 entity pairs appear with pattern 1/2 in the target
corpus. Pattern 1 expresses relation r and pattern 2 does
not. Elements in E1 are labeled with probability ar =
</figureCaption>
<equation confidence="0.6978885">
3/6 = 0.5. Those in E2 are labeled with probability
br2 = ar(|E1 n E2|/|E2|) = 0.5(2/4) = 0.25.
</equation>
<bodyText confidence="0.997006111111111">
1, ... , Ns},3 where we define S to be the number of
patterns and Ns to be the number of entity pairs ap-
pearing with pattern s. Note that we count an entity
pair for given pattern s once even if the entity pair
is mentioned with pattern s more than once in the
corpus, because DS assigns the same relation to all
mentions of the entity pair.
Given relation r, our model assumes the follow-
ing generative process:
</bodyText>
<listItem confidence="0.763845333333333">
1. For each pattern s
Choose whether s expresses relation r or not
zrs — Be(0r)
2. For each entity pair i appearing with pattern s
Choose whether i is labeled or not
xrsi — P(xrsi|Zr, ar, dr, A, M),
</listItem>
<bodyText confidence="0.999842583333333">
where Be(0r) is a Bernoulli distribution with pa-
rameter 0r, zrs is a binary hidden variable that is 1
if pattern s expresses relation r and 0 otherwise, and
Zr = {(zrs)|s = 1, ... , S}. Given a value of zrs,
we model two kinds of probabilities: one for pat-
terns that actually express relation r, i.e., P(xrsi =
1|zrs = 1), and one for patterns that do not express
r, i.e., P(xrsi = 1|zrs = 0). The former is simply
parameterized as 0 &lt; ar &lt; 1. We express the lat-
ter as brs = P(xrsi = 1|Zr, ar, dr, A, M), which is
a function of Zr, ar, dr, A and M; we explain its
modeling in the following two subsections.
</bodyText>
<footnote confidence="0.367784">
3Since a set of entity pairs appearing with pattern s is differ-
ent, i should be written as is. For simplicity, however, we use i
for each pattern.
</footnote>
<figureCaption confidence="0.983059">
The graphical model of our model is shown in
Figure 2.
</figureCaption>
<subsectionHeader confidence="0.998197">
5.1 Example of Wrong Labeling
</subsectionHeader>
<bodyText confidence="0.999967272727273">
Using a simple example, we describe how we model
brs, the probability with which DS assigns relation r
to pattern s via entity pairs when pattern s does not
express relation r.
Consider two patterns: pattern 1 that expresses re-
lation r and pattern 2 that does not (i.e., zr1 = 1 and
zr2 = 0). We also assume that there are entity pairs
that appear with pattern 1 as well as with pattern 2 in
different places in the corpus (for example, Michael
Jackson and Gary in Figure 1). When such entity
pairs are labeled, relation r is assigned to pattern 1
and at the same time to wrong pattern 2. Such entity
pairs are observed as elements in the intersection of
the two sets of entity pairs, E1 and E2. Here, Es is
the set of entity pairs that appear with pattern s in
the corpus. This situation is described in Figure 3.
We model probability br2 as follows. In E1, an
entity pair is labeled with probability ar. We as-
sume that entity pairs in the intersection, E1 n E2,
are also labeled with ar. From the viewpoint of E2,
entity pairs in its subset, E1 n E2, are labeled with
ar. Therefore, br2 is modeled as
</bodyText>
<equation confidence="0.9971105">
|E1 n E2|
br2 = ar |E2 |,
</equation>
<bodyText confidence="0.9997395">
where |E |denotes the number of elements in set E.
An example of this calculation is shown in Figure 3.
</bodyText>
<page confidence="0.990465">
724
</page>
<bodyText confidence="0.988755">
We generalize the example in the next subsection.
</bodyText>
<subsectionHeader confidence="0.999225">
5.2 Modeling of Probability brs
</subsectionHeader>
<bodyText confidence="0.998916333333333">
We model brs so that it is proportional to the number
of entity pairs that are shared with correct patterns
whose zrs = 1, i.e.,
</bodyText>
<equation confidence="0.960224666666667">
~~~T � ���
� �t�zrt=1,t�=s� Et ∩ Es ,(1)
|Es|
</equation>
<bodyText confidence="0.803975166666667">
where T indicates set intersections. However, the
enumeration in Eq.1 requires O(SN2s ) computa-
tional cost and a huge amount of memory to store
all of the entity pairs. We approximate the right-
hand side of Eq.1 as
use less memory. We define S
S matrix M whose
elements are mst =
In reality, factors other than the process described
in the previous subsection can cause wrong labeling
(for example, errors in the knowledge base). We in-
troduce aparameter 0
</bodyText>
<equation confidence="0.552141">
1 that covers such
×
|Et∩Es|/|Es|.
≤dr≤
</equation>
<bodyText confidence="0.743167">
y, we define brs as
</bodyText>
<equation confidence="0.975647">
   
YS
brs≡ar λ 1 − (1−mst)zrt +(1−λ) dr, (2)
t=1,t#s
</equation>
<bodyText confidence="0.999961333333333">
where 0 ≤ λ ≤ 1 is the hyperparameter that con-
trols how strongly brs is affected by the main label-
ing process explained in the previous subsection.
</bodyText>
<subsectionHeader confidence="0.986287">
5.3 Likelihood
</subsectionHeader>
<bodyText confidence="0.933525">
Given observation Xr, the likelihood of our model
is
P(Xr|θr, ar, dr, λ, M)
where
</bodyText>
<equation confidence="0.946489727272727">
P (Zr|θr)= YS θzrs r (1 − θr)1−zrs .
s=1
For each pattern s, we define nrs as the number
of entity pairs to which relation r is assigned (i.e.,
nrs = Pi xrsi).
p (Xr|Zr, ar, dr,
, M) =
S
nanrs
r (1 − ar)Ns−nrsozrs
nbrss (1 − brs)Ns−nrso1−zrs , (3)
</equation>
<bodyText confidence="0.974669">
where brs is in Eq.2.
</bodyText>
<sectionHeader confidence="0.997765" genericHeader="method">
6 Learning
</sectionHeader>
<bodyText confidence="0.999970166666667">
We learn parameters ar, θr, and dr and infer hidden
variables Zr by maximizing the log likelihood given
Xr. Estimated Zr is used to predict which patterns
express relation r.
To infer zrs, we would like to calculate the pos-
terior probability of zrs. However, this calculation
is intractable because each zrs depends on the oth-
ers, {(zrt)|t =6 s}, as shown in Eqs.2 and 3. This
prevents us from using the EM algorithm. Instead,
we apply variational approximation to the posterior
distribution by using the following trial distribution:
where 0 ≤ φrs ≤ 1 is a parameter for the trial dis-
tribution.
The following function Fr is a lower bound of the
log likelihood, and maximizing this function with
respect to Φr is equivalent to minimizing the KL di-
vergence between the trial distribution and the pos-
terior distribution ofZr.
</bodyText>
<equation confidence="0.929956666666667">
=
EQ [log Q
(4)
</equation>
<bodyText confidence="0.916315625">
represents the expectation over trial distribu-
tion Q. We maximize function
with respect to
the parameters instead of the log likelihood.
However, we need further approximation for two
terms on expanding Eq.4. Both of the terms are ex-
pressed as
where
</bodyText>
<equation confidence="0.885903454545454">
is a func-
tion of
Fr
EQ[log P(Zr,Xr|θr,ar,dr,λ,M)]−
(Zr|Φr)].
EQ[•]
Fr
EQ[log(f(Zr))],
f(Zr)
Zr. We apply the following approximation
(Asuncion et al., 2009).
</equation>
<bodyText confidence="0.60822325">
This approximation is made, given the sizes of all
Ess and those of all intersections of two Ess. This
has a lower computational cost of O(S) and let us
factors. Finall
</bodyText>
<equation confidence="0.982186761904762">
P (Zr|θr) P (Xr|Zr, ar, dr, λ,M),
725
YS
s=1
φzrs 1−zrs
rs (1 − φrs) ,
EQ [log(f(Zr))] ≈ log (EQ [f(Zr)]) .
 brs ≈ ar 1 − Y
1
t=
(1− |Et ∩ Es|1zrt
,t�s \ J
|
EsI
.
brs = ar
X=
Zr
Y
s=1
Q (Zr|Φr) =
</equation>
<bodyText confidence="0.999852944444445">
This is based on the Taylor series of log at
EQ[f(Zr)]. In our problem, since the second deriva-
tive is sufficiently small, we use the zeroth-order ap-
proximation.4
Our learning algorithm is derived by calculating
the stationary condition of the resultant evaluation
function with respect to each parameter. We have the
exact solution for Br. For each Drs and dr, we derive
a fixed point iteration. We update ar by using the
steepest ascent. We update each parameter in turn
while keeping the other parameters fixed. Parameter
updating proceeds until a termination condition is
met.
After learning, we have Drs for each pair of rela-
tion r and pattern s. The greater the value of Drs is,
the more likely it is that pattern s expresses relation
r. We set a threshold and determine zrs = 0 when
Drs is less than the threshold.
</bodyText>
<sectionHeader confidence="0.999689" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999916272727273">
We performed two sets of experiments.
Experiment 1 aimed to evaluate the performance of
our generative model itself, which predicts whether
a pattern expresses a relation, given a labeled corpus
created with the DS assumption.
Experiment 2 aimed to evaluate how much our
wrong label reduction in Section 4 improved the per-
formance of relation extraction. In our method, we
trained a classifier with a labeled corpus cleaned by
Algorithm 1 using the negative pattern list predicted
by the generative model.
</bodyText>
<subsectionHeader confidence="0.930647">
7.1 Dataset
</subsectionHeader>
<bodyText confidence="0.99269575">
Following Mintz et al. (2009), we carried out our
experiments using Wikipedia as the target corpus
and Freebase (September, 2009, (Google, 2009)) as
the knowledge base. We used more than 1,300,000
Wikipedia articles in the wex dump data (September,
2009, (Metaweb Technologies, 2009)). The proper-
ties of our data are shown in Table 1.
In Wikipedia articles, named entities were iden-
tified by anchor text linking to another article and
starting with a capital letter (Yan et al., 2009). We
applied Open NLP POS tagger5 and MaltParser
(Nivre et al., 2007) to sentences containing more
</bodyText>
<footnote confidence="0.999377">
4The first-order information becomes zero in this case.
5http://opennlp.sourceforge.net/
</footnote>
<tableCaption confidence="0.998625">
Table 1: Properties of Wikipedia dataset
</tableCaption>
<table confidence="0.9738245">
documents 1,303,000
entity pairs 2,017,000
(matched to Freebase) 129,000
(with entity types) 913,000
frequent patterns 3,084
relations 24
</table>
<bodyText confidence="0.9774908">
than one named entity. We then extracted sentences
containing related entity pairs with the method ex-
plained in Section 3. To match entity pairs, we used
ID mapping between the dump data and Freebase.
We used the most frequent 24 relations.
</bodyText>
<subsectionHeader confidence="0.990515">
7.2 Experiment 1: Pattern Prediction
</subsectionHeader>
<bodyText confidence="0.999283181818182">
We compared our model with baseline methods in
terms of ability to predict patterns that express a
given relation.
The input of this task was Xrs, which expresses
whether or not each entity pair appearing with each
pattern is labeled with relation r, as explained in
Section 5. In Experiment 1, since we needed entity
types for patterns, we restricted ourselves to entities
matched with Freebase, which also provides entity
types for entities. We used patterns that appear more
than 20 times in the corpus.
</bodyText>
<subsectionHeader confidence="0.717751">
7.2.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.999969625">
We split the data into training data and test data.
The training data was Xrs for 12 relations and the
test data was that for the remaining 12 relations. The
training data was used to calibrate parameters (see
the following subsection for details). The test data
was used for evaluation. We randomly split the data
five times and took the average of the following eval-
uation values.
We evaluated the performance by precision, re-
call, and F value. They were calculated using gold
standard data, which was constructed by hand. We
manually selected patterns that actually express a
target relation as positive patterns for the relation. 6
We averaged the evaluation values in terms of macro
average over relations before averaging over the data
splits.
</bodyText>
<footnote confidence="0.990373666666667">
6Patterns that ambiguously express the relation, for instance
“[Person] in [Location]” for place of birth, were not selected as
positive patterns.
</footnote>
<page confidence="0.996688">
726
</page>
<tableCaption confidence="0.912444777777778">
Table 2: Averages of precision, recall, and F value in Ex-
periment 1. The averages of threshold of RS(rank) and
RS(value) were 6.2 f 3.2 and 0.10 f 0.06, respectively.
The averages of hyperparameters of PROP were 0.84 f
0.05 for A and 0.85 f 0.10 for the threshold.
Table 3: Example of estimated Ors for r =
place of birth. Entity types are omitted in patterns.
nrs/Ns is the ratio of the number of labeled entity pairs
to the number of entity pairs appearing with pattern s.
</tableCaption>
<table confidence="0.978913307692308">
Precision Recall F value
Baseline 0.339 1.000 0.458
RS(rank) 0.749 0.549 0.467
RS(value) 0.601 0.647 0.545
PROP 0.782 0.688 0.667
pattern s
born in
actor from
elected Mayor of
family moved from
native of
grew in
nrs/Ns Ors expresses r?
</table>
<footnote confidence="0.721652666666667">
0.512 0.999 true
0.480 0.999 true
0.384 0.855 false
0.344 0.055 false
0.327 0.999 true
0.162 0.000 false
</footnote>
<subsectionHeader confidence="0.688918">
7.2.2 Methods
</subsectionHeader>
<bodyText confidence="0.999748761904762">
We compared the following methods:
Baseline: This method assigns relation r to a pat-
tern when the pattern is mentioned with at least one
entity pair corresponding to relation r in Freebase.
This method is based on the DS assumption.
Ratio-based Selection (RS): Given relation r and
pattern s, this method calculates nrs/Ns, which is
the ratio of the number of labeled entity pairs ap-
pearing with pattern s to the number of entity pairs
including unlabeled ones. RS then selects the top
n patterns (RS(rank)). We also tested a version us-
ing a real-valued threshold (RS(value)). In train-
ing, we selected the threshold that maximized the
F value. Some bootstrapping approaches (Carlson et
al., 2010) use a rank-based threshold like RS(rank).
Proposed Model (PROP): Using the training data,
we determined the two hyperparameters, A and the
threshold to round Ors to 1 or 0, so that they max-
imized the F value. When Ors is greater than the
threshold, we select pattern s as one expressing re-
lation r.
</bodyText>
<subsectionHeader confidence="0.748409">
7.2.3 Result and Discussion
</subsectionHeader>
<bodyText confidence="0.999308678571429">
The results of Experiment 1 are shown in Table 2.
Our model achieved the best precision, recall, and F
value. RS(value) had the second best F value, but it
completely removed more than one infrequent rela-
tion on average in test sets. This is problematic for
real situations. RS(rank) achieved the second high-
est precision. However, its recall, which is also im-
portant in our task, was the lowest and its F value
was almost the same as naive Baseline.
The thresholds of RS, which directly affect their
performance, should be calibrated for each relation,
but it is hard to do this in advance. On the other
hand, our model learns parameters such as ar for
each relation and thus the hyperparameter of our
model does not directly affect its performance. This
results in a high prediction performance.
Examples of estimated Ors, the probability with
which pattern s expresses relation r, are shown in
Table 3. The pattern, “[Person] family moved from
[Location]”, which does not express place of birth,
had low Ors in spite of having higher nrs/Ns than
the valid pattern “[Person] native of [Location]”.
The former pattern had higher brs, the probability
with which relation r is wrongly assigned to pat-
tern s via entity pairs, because there were more en-
tity pairs that appeared not only with this pattern
but also with patterns that was predicted to express
place of birth.
</bodyText>
<subsectionHeader confidence="0.977782">
7.3 Experiment 2: Relation Extraction
</subsectionHeader>
<bodyText confidence="0.999967714285714">
We investigated the performance of relation extrac-
tion using our wrong label reduction, which uses the
results of the pattern prediction.
Following Mintz et al. (2009), we performed an
automatic held-out evaluation and a manual evalu-
ation. In both cases, we used 400,000 articles for
testing and the remaining 903,000 for training.
</bodyText>
<subsectionHeader confidence="0.978942">
7.3.1 Configuration of Classifiers
</subsectionHeader>
<bodyText confidence="0.99984075">
Following Mintz et al. (2009), we used a multi-
class logistic classifier optimized using L-BFGS
with Gaussian regularization to classify entity pairs
to the predefined 24 relations and NONE. In order to
train the NONE class, we randomly picked 100,000
examples that did not match to Freebase as pairs.
(Several entities in the examples matched and had
entity types of Freebase.) In this experiment, we
</bodyText>
<page confidence="0.991998">
727
</page>
<figureCaption confidence="0.999576">
Figure 4: Precision-recall curves in held-out evaluation.
Precision is reported at recall levels from 5 to 50,000.
</figureCaption>
<bodyText confidence="0.998892764705882">
used not only entity pairs matched to Freebase but
also ones not matched to Freebase (i.e., entity pairs
that do not have entity types). We used syntactic
features (i.e., features obtained from the dependency
parse tree of a sentence) and lexical features, and en-
tity types, which essentially correspond to the ones
developed by Mintz et al. (2009).
We compared the following methods: logistic re-
gression with the labeled data cleaned by the pro-
posed method (PROP), logistic regression with the
standard DS labeled data (LR), and MultiR proposed
in (Hoffmann et al., 2011) as a state-of-the-art multi-
instance learning system.7 For logistic regression,
when more than one relation is assigned to a sen-
tence, we simply copied the feature vector and cre-
ated a training example for each relation. In PROP,
we used training articles for pattern prediction.8
</bodyText>
<subsectionHeader confidence="0.40173">
7.3.2 Held-out Evaluation
</subsectionHeader>
<bodyText confidence="0.999887111111111">
In the held-out evaluation, relation instances dis-
covered from testing articles were automatically
compared with those in Freebase. This let us calcu-
late the precision of each method for the best n re-
lation instances. The precisions are underestimated
because this evaluation suffers from false negatives
due to the incompleteness of Freebase. We changed
n from 5 to 50,000 and measured precision and re-
call. Precision-recall curves for the held-out data are
</bodyText>
<footnote confidence="0.995328333333333">
7For MultiR, we used the authors’ implementation from
http://www.cs.washington.edu/homes/raphaelh/mr/
8In Experiment 2 we set A = 0.85 and the threshold at 0.95.
</footnote>
<tableCaption confidence="0.9777515">
Table 4: Averages of precisions at 50 for the most fre-
quent 15 relations as well as example relations.
</tableCaption>
<table confidence="0.84922875">
PROP MultiR LR
place of birth 1.0 1.0 0.56
place of death 1.0 0.7 0.84
average 0.89±0.14 0.83±0.21 0.82±0.23
</table>
<bodyText confidence="0.991235">
shown in Figure 4.
PROP achieved comparable or higher precision at
most recall levels compared with LR and MultiR. Its
performance at n = 50,000 is much higher than that
of the others. While our generative model does not
use unlabeled examples as negative ones in detecting
wrong labels, classifier-based approaches including
MultiR do, suffering from false negatives.
</bodyText>
<subsubsectionHeader confidence="0.459304">
7.3.3 Manual Evaluation
</subsubsectionHeader>
<bodyText confidence="0.999926222222222">
For manual evaluation, we picked the top ranked
50 relation instances for the most frequent 15 rela-
tions. The manually evaluated precisions averaged
over the 15 relations are shown in table 4.
PROP achieved the best average precision. For
place of birth, LR wrongly extracted entity pairs
with “[Person] played with club [Location]”, which
does not express the relation. PROP and MultiR
avoided this mistake. For place of death, LR and
MultiR wrongly extracted entity pairs with “[Per-
son] moved to [Location]”. Multi-instance learning
does not work for wrong labels assigned to entity
pairs that appear only once in a corpus. In fact, 72%
of entity pairs that appeared with this pattern and
were wrongly labeled as place of death appeared
only once in the corpus. Only PROP avoided mis-
takes of this kind because our method works in such
situations.
</bodyText>
<sectionHeader confidence="0.998583" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999918777777778">
We proposed a method that reduces the number of
wrong labels created with the DS assumption, which
is widely applied. Our generative model directly
models the labeling process of DS and predicts pat-
terns that are wrongly labeled with a relation. The
predicted patterns are used for wrong label reduc-
tion. The experimental results show that this method
successfully reduced the number of wrong labels
and boosted the performance of relation extraction.
</bodyText>
<page confidence="0.996066">
728
</page>
<sectionHeader confidence="0.958771" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994688226415094">
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee W. Teh. 2009. On smoothing and inference
for topic models. In Proceedings of the 25th Con-
ference on Uncertainty in Artificial Intelligence (UAI
’9), pages 27–34.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT ’08), pages 28–36.
Michele Banko, Michael J Cafarella, Stephen Soderl,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
the International Joint Conferences on Artificial Intel-
ligence (IJCAI ’07), pages 2670–2676.
Kedar Bellare and Andrew McCallum. 2007. Learn-
ing Extractors from Unlabeled Text using Relevant
Databases. In Sixth International Workshop on Infor-
mation Integration on the Web (IIWeb ’07).
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the 3rd ACM International
Conference on Web Search and Data Mining (WSDM
’10), pages 101–110.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP ’11), pages 1535–1545.
Google. 2009. Freebase data dumps. http://
download.freebase.com/datadumps/.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL ’10), pages
286–295.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT ’11), pages 541–550.
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of se-
mantic drift in Espresso-like bootstrapping algorithms.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP
’08), pages 1011–1020.
Metaweb Technologies. 2009. Freebase wikipedia ex-
traction (wex). http://download.freebase.
com/wex/.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP (ACL-IJCNLP ’09),
pages 1003–1011.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2007.
Maltparser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 37:95–135.
Chris Pal, Gideon Mann, and Richard Minerich. 2007.
Putting semantic information extraction on the map:
Noisy label models for fact extraction. In Sixth Inter-
national Workshop on Information Integration on the
Web (IIWeb ’07).
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL ’06), pages
113–120.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In In Proceedings of the European Con-
ference on Machine Learning and Knowledge Discov-
ery in Databases (ECML-PKDD ’10), pages 148–163.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for information extraction by multi-level bootstrap-
ping. In AAAI/IAAI, pages 474–479.
Chang Wang, James Fan, Aditya Kalyanpur, and David
Gondek. 2011. Relation extraction with relation
topics. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP ’11), pages 1426–1436.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the 16th
ACM Conference on Conference on Information and
Knowledge Management (CIKM ’07), pages 41–50.
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu
Yang, and Mitsuru Ishizuka. 2009. Unsupervised re-
lation extraction by mining wikipedia texts using in-
formation from the web. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP (ACL-IJCNLP
’09), pages 1021–1029.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP ’10), pages 1013–1023.
</reference>
<page confidence="0.998499">
729
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.832366">
<title confidence="0.999417">Reducing Wrong Labels in Distant Supervision for Relation Extraction</title>
<author confidence="0.993582">Takamatsu Issei Sato Nakagawa</author>
<affiliation confidence="0.9804785">System Technologies Laboratories Information Technology Center Sony Corporation The University of</affiliation>
<address confidence="0.956552">5-1-12 Kitashinagawa, Shinagawa-ku, Tokyo 7-3-1 Hongo, Bunkyo-ku, Tokyo</address>
<abstract confidence="0.995847791666666">In relation extraction, distant supervision seeks to extract relations between entities from text by using a knowledge base, such as Freebase, as a source of supervision. When a sentence and a knowledge base refer to the same entity pair, this approach heuristically labels the sentence with the corresponding relation in the knowledge base. However, this heuristic can fail with the result that some sentences are labeled wrongly. This noisy labeled data causes poor extraction performance. In this paper, we propose a method to reduce the number of wrong labels. We present a novel generative model that directly models the heuristic labeling process of distant supervision. The model predicts whether assigned labels are correct or wrong via its hidden variables. Our experimental results show that this model detected wrong labels with higher performance than baseline methods. In the experiment, we also found that our wrong label reduction boosted the performance of relation extraction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arthur Asuncion</author>
<author>Max Welling</author>
<author>Padhraic Smyth</author>
<author>Yee W Teh</author>
</authors>
<title>On smoothing and inference for topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI ’9),</booktitle>
<pages>27--34</pages>
<contexts>
<context position="17765" citStr="Asuncion et al., 2009" startWordPosition="3075" endWordPosition="3078"> a lower bound of the log likelihood, and maximizing this function with respect to Φr is equivalent to minimizing the KL divergence between the trial distribution and the posterior distribution ofZr. = EQ [log Q (4) represents the expectation over trial distribution Q. We maximize function with respect to the parameters instead of the log likelihood. However, we need further approximation for two terms on expanding Eq.4. Both of the terms are expressed as where is a function of Fr EQ[log P(Zr,Xr|θr,ar,dr,λ,M)]− (Zr|Φr)]. EQ[•] Fr EQ[log(f(Zr))], f(Zr) Zr. We apply the following approximation (Asuncion et al., 2009). This approximation is made, given the sizes of all Ess and those of all intersections of two Ess. This has a lower computational cost of O(S) and let us factors. Finall P (Zr|θr) P (Xr|Zr, ar, dr, λ,M), 725 YS s=1 φzrs 1−zrs rs (1 − φrs) , EQ [log(f(Zr))] ≈ log (EQ [f(Zr)]) .  brs ≈ ar 1 − Y 1 t= (1− |Et ∩ Es|1zrt ,t�s \ J | EsI . brs = ar X= Zr Y s=1 Q (Zr|Φr) = This is based on the Taylor series of log at EQ[f(Zr)]. In our problem, since the second derivative is sufficiently small, we use the zeroth-order approximation.4 Our learning algorithm is derived by calculating the stationary cond</context>
</contexts>
<marker>Asuncion, Welling, Smyth, Teh, 2009</marker>
<rawString>Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee W. Teh. 2009. On smoothing and inference for topic models. In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI ’9), pages 27–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
</authors>
<title>The tradeoffs between open and traditional relation extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT ’08),</booktitle>
<pages>28--36</pages>
<contexts>
<context position="7909" citStr="Banko and Etzioni, 2008" startWordPosition="1274" endWordPosition="1277">s to extract relation instances from text. An entity is mentioned as a named entity in text. We extract a relation instance from a 722 single sentence. For example, from the upper sentence in Figure 1 we extract place of birth(Michael Jackson, Gary). Since two entities mentioned in a sentence do not always have a relation, we select entity pairs from a corpus when: (i) the path of the dependency parse tree between the corresponding two named entities in the sentence is no longer than 4 and (ii) the path does not contain a sentence-like boundary, such as a relative clause1 (Banko et al., 2007; Banko and Etzioni, 2008). Banko and Etzioni (2008) found that a set of eight lexico-syntactic forms covers nearly 95% of relation phrases in their corpus. (Fader et al. (2011) found that this set covers 69% of their corpus). Our rule is designed to cover at least the eight lexico-syntactic forms. We use the entity pairs extracted by this rule. DS uses a knowledge base to create labeled data for relation extraction by heuristically matching entity pairs. A knowledge base is a set of relation instances about predefined relations. For each sentence in the corpus, we extract all of its entity pairs. Then, for each entity</context>
</contexts>
<marker>Banko, Etzioni, 2008</marker>
<rawString>Michele Banko and Oren Etzioni. 2008. The tradeoffs between open and traditional relation extraction. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT ’08), pages 28–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderl</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI ’07),</booktitle>
<pages>2670--2676</pages>
<contexts>
<context position="7883" citStr="Banko et al., 2007" startWordPosition="1270" endWordPosition="1273">tion extraction seeks to extract relation instances from text. An entity is mentioned as a named entity in text. We extract a relation instance from a 722 single sentence. For example, from the upper sentence in Figure 1 we extract place of birth(Michael Jackson, Gary). Since two entities mentioned in a sentence do not always have a relation, we select entity pairs from a corpus when: (i) the path of the dependency parse tree between the corresponding two named entities in the sentence is no longer than 4 and (ii) the path does not contain a sentence-like boundary, such as a relative clause1 (Banko et al., 2007; Banko and Etzioni, 2008). Banko and Etzioni (2008) found that a set of eight lexico-syntactic forms covers nearly 95% of relation phrases in their corpus. (Fader et al. (2011) found that this set covers 69% of their corpus). Our rule is designed to cover at least the eight lexico-syntactic forms. We use the entity pairs extracted by this rule. DS uses a knowledge base to create labeled data for relation extraction by heuristically matching entity pairs. A knowledge base is a set of relation instances about predefined relations. For each sentence in the corpus, we extract all of its entity pa</context>
</contexts>
<marker>Banko, Cafarella, Soderl, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J Cafarella, Stephen Soderl, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI ’07), pages 2670–2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kedar Bellare</author>
<author>Andrew McCallum</author>
</authors>
<title>Learning Extractors from Unlabeled Text using Relevant Databases.</title>
<date>2007</date>
<booktitle>In Sixth International Workshop on Information Integration on the Web</booktitle>
<contexts>
<context position="5178" citStr="Bellare and McCallum, 2007" startWordPosition="813" endWordPosition="816"> our method to Wikipedia articles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS (Mintz et al., 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multiinstance learning system for relation extraction (see Section 7). 2 Related Work The increasingly popular approach, called distant supervision (DS), or weak supervision, utilizes a knowledge base to heuristically label a corpus (Wu and Weld, 2007; Bellare and McCallum, 2007; Pal et al., 2007). Our work was inspired by Mintz et al. (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia. Previous works (Hoffmann et al., 2010; Yao et al., 2010) have pointed out that the DS assumption generates noisy labeled data, but did not directly address the problem. Wang et al. (2011) applied a rule-based method to the problem by using popular entity types and keywords for each relation. In (Bellare and McCallum, 2007; Riedel et al., 2010; Hoffmann et al., 2011), they used multiinstance learning, which deals with u</context>
</contexts>
<marker>Bellare, McCallum, 2007</marker>
<rawString>Kedar Bellare and Andrew McCallum. 2007. Learning Extractors from Unlabeled Text using Relevant Databases. In Sixth International Workshop on Information Integration on the Web (IIWeb ’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Richard C Wang</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Coupled semi-supervised learning for information extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining (WSDM ’10),</booktitle>
<pages>101--110</pages>
<contexts>
<context position="6123" citStr="Carlson et al., 2010" startWordPosition="971" endWordPosition="975">y address the problem. Wang et al. (2011) applied a rule-based method to the problem by using popular entity types and keywords for each relation. In (Bellare and McCallum, 2007; Riedel et al., 2010; Hoffmann et al., 2011), they used multiinstance learning, which deals with uncertainty of labels, to relax the DS assumption. However, the relaxed assumption can fail when a labeled entity pair is mentioned only once in a corpus (Riedel et al., 2010). Our approach relies on neither of these assumptions. Bootstrapping for relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006; Carlson et al., 2010) is related to our method. In bootstrapping, seed entity pairs of the target relation are given in order to select reliable patterns, which are used to extract new entity pairs. To avoid the selection of unreliable patterns, bootstrapping introduces scoring functions for each pattern candidate. This can be applied to our approach, which seeks to reduce the number of unreliable patterns by using a set of given entity pairs. However, the bootstrappinglike approach suffers from sensitive parameters that are critical to its performance. Ideally, the parameters such as a threshold for scoring funct</context>
<context position="23568" citStr="Carlson et al., 2010" startWordPosition="4060" endWordPosition="4063"> to a pattern when the pattern is mentioned with at least one entity pair corresponding to relation r in Freebase. This method is based on the DS assumption. Ratio-based Selection (RS): Given relation r and pattern s, this method calculates nrs/Ns, which is the ratio of the number of labeled entity pairs appearing with pattern s to the number of entity pairs including unlabeled ones. RS then selects the top n patterns (RS(rank)). We also tested a version using a real-valued threshold (RS(value)). In training, we selected the threshold that maximized the F value. Some bootstrapping approaches (Carlson et al., 2010) use a rank-based threshold like RS(rank). Proposed Model (PROP): Using the training data, we determined the two hyperparameters, A and the threshold to round Ors to 1 or 0, so that they maximized the F value. When Ors is greater than the threshold, we select pattern s as one expressing relation r. 7.2.3 Result and Discussion The results of Experiment 1 are shown in Table 2. Our model achieved the best precision, recall, and F value. RS(value) had the second best F value, but it completely removed more than one infrequent relation on average in test sets. This is problematic for real situation</context>
</contexts>
<marker>Carlson, Betteridge, Wang, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Richard C. Wang, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Coupled semi-supervised learning for information extraction. In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining (WSDM ’10), pages 101–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP ’11),</booktitle>
<pages>1535--1545</pages>
<contexts>
<context position="8060" citStr="Fader et al. (2011)" startWordPosition="1300" endWordPosition="1303">example, from the upper sentence in Figure 1 we extract place of birth(Michael Jackson, Gary). Since two entities mentioned in a sentence do not always have a relation, we select entity pairs from a corpus when: (i) the path of the dependency parse tree between the corresponding two named entities in the sentence is no longer than 4 and (ii) the path does not contain a sentence-like boundary, such as a relative clause1 (Banko et al., 2007; Banko and Etzioni, 2008). Banko and Etzioni (2008) found that a set of eight lexico-syntactic forms covers nearly 95% of relation phrases in their corpus. (Fader et al. (2011) found that this set covers 69% of their corpus). Our rule is designed to cover at least the eight lexico-syntactic forms. We use the entity pairs extracted by this rule. DS uses a knowledge base to create labeled data for relation extraction by heuristically matching entity pairs. A knowledge base is a set of relation instances about predefined relations. For each sentence in the corpus, we extract all of its entity pairs. Then, for each entity pair, we try to retrieve the relation instances about the entity pair from the knowledge base. If we found such a relation instance, then the set of i</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP ’11), pages 1535–1545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Google</author>
</authors>
<title>Freebase data dumps.</title>
<date>2009</date>
<note>http:// download.freebase.com/datadumps/.</note>
<contexts>
<context position="19632" citStr="Google, 2009" startWordPosition="3412" endWordPosition="3413">1 aimed to evaluate the performance of our generative model itself, which predicts whether a pattern expresses a relation, given a labeled corpus created with the DS assumption. Experiment 2 aimed to evaluate how much our wrong label reduction in Section 4 improved the performance of relation extraction. In our method, we trained a classifier with a labeled corpus cleaned by Algorithm 1 using the negative pattern list predicted by the generative model. 7.1 Dataset Following Mintz et al. (2009), we carried out our experiments using Wikipedia as the target corpus and Freebase (September, 2009, (Google, 2009)) as the knowledge base. We used more than 1,300,000 Wikipedia articles in the wex dump data (September, 2009, (Metaweb Technologies, 2009)). The properties of our data are shown in Table 1. In Wikipedia articles, named entities were identified by anchor text linking to another article and starting with a capital letter (Yan et al., 2009). We applied Open NLP POS tagger5 and MaltParser (Nivre et al., 2007) to sentences containing more 4The first-order information becomes zero in this case. 5http://opennlp.sourceforge.net/ Table 1: Properties of Wikipedia dataset documents 1,303,000 entity pair</context>
</contexts>
<marker>Google, 2009</marker>
<rawString>Google. 2009. Freebase data dumps. http:// download.freebase.com/datadumps/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Daniel S Weld</author>
</authors>
<title>Learning 5000 relational extractors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL ’10),</booktitle>
<pages>286--295</pages>
<contexts>
<context position="5393" citStr="Hoffmann et al., 2010" startWordPosition="851" endWordPosition="854">traction performance than the original DS (Mintz et al., 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multiinstance learning system for relation extraction (see Section 7). 2 Related Work The increasingly popular approach, called distant supervision (DS), or weak supervision, utilizes a knowledge base to heuristically label a corpus (Wu and Weld, 2007; Bellare and McCallum, 2007; Pal et al., 2007). Our work was inspired by Mintz et al. (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia. Previous works (Hoffmann et al., 2010; Yao et al., 2010) have pointed out that the DS assumption generates noisy labeled data, but did not directly address the problem. Wang et al. (2011) applied a rule-based method to the problem by using popular entity types and keywords for each relation. In (Bellare and McCallum, 2007; Riedel et al., 2010; Hoffmann et al., 2011), they used multiinstance learning, which deals with uncertainty of labels, to relax the DS assumption. However, the relaxed assumption can fail when a labeled entity pair is mentioned only once in a corpus (Riedel et al., 2010). Our approach relies on neither of these</context>
</contexts>
<marker>Hoffmann, Zhang, Weld, 2010</marker>
<rawString>Raphael Hoffmann, Congle Zhang, and Daniel S. Weld. 2010. Learning 5000 relational extractors. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL ’10), pages 286–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledgebased weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACLHLT ’11),</booktitle>
<pages>541--550</pages>
<contexts>
<context position="4869" citStr="Hoffmann et al., 2011" startWordPosition="768" endWordPosition="771">el et al. (2010)’s, the model predicts whether each pattern expresses each relation via hidden variables (see Section 5). • Our variational inference for our generative model lets us automatically calibrate parameters for each relation, which are sensitive to the performance (see Section 6). • We applied our method to Wikipedia articles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS (Mintz et al., 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multiinstance learning system for relation extraction (see Section 7). 2 Related Work The increasingly popular approach, called distant supervision (DS), or weak supervision, utilizes a knowledge base to heuristically label a corpus (Wu and Weld, 2007; Bellare and McCallum, 2007; Pal et al., 2007). Our work was inspired by Mintz et al. (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia. Previous works (Hoffmann et al., 2010; Yao et al., 2010) have pointed out that the DS assumption generates noisy </context>
<context position="26764" citStr="Hoffmann et al., 2011" startWordPosition="4591" endWordPosition="4594">on is reported at recall levels from 5 to 50,000. used not only entity pairs matched to Freebase but also ones not matched to Freebase (i.e., entity pairs that do not have entity types). We used syntactic features (i.e., features obtained from the dependency parse tree of a sentence) and lexical features, and entity types, which essentially correspond to the ones developed by Mintz et al. (2009). We compared the following methods: logistic regression with the labeled data cleaned by the proposed method (PROP), logistic regression with the standard DS labeled data (LR), and MultiR proposed in (Hoffmann et al., 2011) as a state-of-the-art multiinstance learning system.7 For logistic regression, when more than one relation is assigned to a sentence, we simply copied the feature vector and created a training example for each relation. In PROP, we used training articles for pattern prediction.8 7.3.2 Held-out Evaluation In the held-out evaluation, relation instances discovered from testing articles were automatically compared with those in Freebase. This let us calculate the precision of each method for the best n relation instances. The precisions are underestimated because this evaluation suffers from fals</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledgebased weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACLHLT ’11), pages 541–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mamoru Komachi</author>
<author>Taku Kudo</author>
<author>Masashi Shimbo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Graph-based analysis of semantic drift in Espresso-like bootstrapping algorithms.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP ’08),</booktitle>
<pages>1011--1020</pages>
<contexts>
<context position="6825" citStr="Komachi et al., 2008" startWordPosition="1086" endWordPosition="1089">tion are given in order to select reliable patterns, which are used to extract new entity pairs. To avoid the selection of unreliable patterns, bootstrapping introduces scoring functions for each pattern candidate. This can be applied to our approach, which seeks to reduce the number of unreliable patterns by using a set of given entity pairs. However, the bootstrappinglike approach suffers from sensitive parameters that are critical to its performance. Ideally, the parameters such as a threshold for scoring function should be determined for each relation, but there are no principled methods (Komachi et al., 2008). In our approach, parameters are calibrated for each relation by maximizing the likelihood of our generative model. 3 Knowledge-based Distant Supervision In this section, we describe DS for relation extraction. We use the term relation as the relation between two entities. A relation instance is a tuple consisting of two entities and relation r. For example, place of birth(Michael Jackson, Gary) in Figure 1 is a relation instance. Relation extraction seeks to extract relation instances from text. An entity is mentioned as a named entity in text. We extract a relation instance from a 722 singl</context>
</contexts>
<marker>Komachi, Kudo, Shimbo, Matsumoto, 2008</marker>
<rawString>Mamoru Komachi, Taku Kudo, Masashi Shimbo, and Yuji Matsumoto. 2008. Graph-based analysis of semantic drift in Espresso-like bootstrapping algorithms. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP ’08), pages 1011–1020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Metaweb Technologies</author>
</authors>
<title>Freebase wikipedia extraction (wex).</title>
<date>2009</date>
<note>http://download.freebase. com/wex/.</note>
<contexts>
<context position="19771" citStr="Technologies, 2009" startWordPosition="3433" endWordPosition="3434">beled corpus created with the DS assumption. Experiment 2 aimed to evaluate how much our wrong label reduction in Section 4 improved the performance of relation extraction. In our method, we trained a classifier with a labeled corpus cleaned by Algorithm 1 using the negative pattern list predicted by the generative model. 7.1 Dataset Following Mintz et al. (2009), we carried out our experiments using Wikipedia as the target corpus and Freebase (September, 2009, (Google, 2009)) as the knowledge base. We used more than 1,300,000 Wikipedia articles in the wex dump data (September, 2009, (Metaweb Technologies, 2009)). The properties of our data are shown in Table 1. In Wikipedia articles, named entities were identified by anchor text linking to another article and starting with a capital letter (Yan et al., 2009). We applied Open NLP POS tagger5 and MaltParser (Nivre et al., 2007) to sentences containing more 4The first-order information becomes zero in this case. 5http://opennlp.sourceforge.net/ Table 1: Properties of Wikipedia dataset documents 1,303,000 entity pairs 2,017,000 (matched to Freebase) 129,000 (with entity types) 913,000 frequent patterns 3,084 relations 24 than one named entity. We then e</context>
</contexts>
<marker>Technologies, 2009</marker>
<rawString>Metaweb Technologies. 2009. Freebase wikipedia extraction (wex). http://download.freebase. com/wex/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP ’09),</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="1840" citStr="Mintz et al., 2009" startWordPosition="268" endWordPosition="271">ance than baseline methods. In the experiment, we also found that our wrong label reduction boosted the performance of relation extraction. 1 Introduction Machine learning approaches have been developed to address relation extraction, which is the task of extracting semantic relations between entities expressed in text. Supervised approaches are limited in scalability because labeled data is expensive to produce. A particularly attractive approach, called distant supervision (DS), creates labeled data by heuristically aligning entities in text with those in a knowledge base, such as Freebase (Mintz et al., 2009). Figure 1: Automatic labeling by distant supervision. Upper sentence: correct labeling; lower sentence: incorrect labeling. With DS it is assumed that if a sentence contains an entity pair in a knowledge base, such a sentence actually expresses the corresponding relation in the knowledge base. However, the DS assumption can fail, which results in noisy labeled data and this causes poor extraction performance. An entity pair in a target text generally expresses more than one relation while a knowledge base stores a subset of the relations. The assumption ignores this possibility. For instance,</context>
<context position="4834" citStr="Mintz et al., 2009" startWordPosition="762" endWordPosition="765">any strong assumptions like Riedel et al. (2010)’s, the model predicts whether each pattern expresses each relation via hidden variables (see Section 5). • Our variational inference for our generative model lets us automatically calibrate parameters for each relation, which are sensitive to the performance (see Section 6). • We applied our method to Wikipedia articles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS (Mintz et al., 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multiinstance learning system for relation extraction (see Section 7). 2 Related Work The increasingly popular approach, called distant supervision (DS), or weak supervision, utilizes a knowledge base to heuristically label a corpus (Wu and Weld, 2007; Bellare and McCallum, 2007; Pal et al., 2007). Our work was inspired by Mintz et al. (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia. Previous works (Hoffmann et al., 2010; Yao et al., 2010) have pointed out that</context>
<context position="19517" citStr="Mintz et al. (2009)" startWordPosition="3393" endWordPosition="3396">nd determine zrs = 0 when Drs is less than the threshold. 7 Experiments We performed two sets of experiments. Experiment 1 aimed to evaluate the performance of our generative model itself, which predicts whether a pattern expresses a relation, given a labeled corpus created with the DS assumption. Experiment 2 aimed to evaluate how much our wrong label reduction in Section 4 improved the performance of relation extraction. In our method, we trained a classifier with a labeled corpus cleaned by Algorithm 1 using the negative pattern list predicted by the generative model. 7.1 Dataset Following Mintz et al. (2009), we carried out our experiments using Wikipedia as the target corpus and Freebase (September, 2009, (Google, 2009)) as the knowledge base. We used more than 1,300,000 Wikipedia articles in the wex dump data (September, 2009, (Metaweb Technologies, 2009)). The properties of our data are shown in Table 1. In Wikipedia articles, named entities were identified by anchor text linking to another article and starting with a capital letter (Yan et al., 2009). We applied Open NLP POS tagger5 and MaltParser (Nivre et al., 2007) to sentences containing more 4The first-order information becomes zero in t</context>
<context position="25472" citStr="Mintz et al. (2009)" startWordPosition="4384" endWordPosition="4387">”, which does not express place of birth, had low Ors in spite of having higher nrs/Ns than the valid pattern “[Person] native of [Location]”. The former pattern had higher brs, the probability with which relation r is wrongly assigned to pattern s via entity pairs, because there were more entity pairs that appeared not only with this pattern but also with patterns that was predicted to express place of birth. 7.3 Experiment 2: Relation Extraction We investigated the performance of relation extraction using our wrong label reduction, which uses the results of the pattern prediction. Following Mintz et al. (2009), we performed an automatic held-out evaluation and a manual evaluation. In both cases, we used 400,000 articles for testing and the remaining 903,000 for training. 7.3.1 Configuration of Classifiers Following Mintz et al. (2009), we used a multiclass logistic classifier optimized using L-BFGS with Gaussian regularization to classify entity pairs to the predefined 24 relations and NONE. In order to train the NONE class, we randomly picked 100,000 examples that did not match to Freebase as pairs. (Several entities in the examples matched and had entity types of Freebase.) In this experiment, we</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP ’09), pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A language-independent system for datadriven dependency parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<pages>37--95</pages>
<contexts>
<context position="20041" citStr="Nivre et al., 2007" startWordPosition="3479" endWordPosition="3482">ative pattern list predicted by the generative model. 7.1 Dataset Following Mintz et al. (2009), we carried out our experiments using Wikipedia as the target corpus and Freebase (September, 2009, (Google, 2009)) as the knowledge base. We used more than 1,300,000 Wikipedia articles in the wex dump data (September, 2009, (Metaweb Technologies, 2009)). The properties of our data are shown in Table 1. In Wikipedia articles, named entities were identified by anchor text linking to another article and starting with a capital letter (Yan et al., 2009). We applied Open NLP POS tagger5 and MaltParser (Nivre et al., 2007) to sentences containing more 4The first-order information becomes zero in this case. 5http://opennlp.sourceforge.net/ Table 1: Properties of Wikipedia dataset documents 1,303,000 entity pairs 2,017,000 (matched to Freebase) 129,000 (with entity types) 913,000 frequent patterns 3,084 relations 24 than one named entity. We then extracted sentences containing related entity pairs with the method explained in Section 3. To match entity pairs, we used ID mapping between the dump data and Freebase. We used the most frequent 24 relations. 7.2 Experiment 1: Pattern Prediction We compared our model wi</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2007. Maltparser: A language-independent system for datadriven dependency parsing. Natural Language Engineering, 37:95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Pal</author>
<author>Gideon Mann</author>
<author>Richard Minerich</author>
</authors>
<title>Putting semantic information extraction on the map: Noisy label models for fact extraction.</title>
<date>2007</date>
<booktitle>In Sixth International Workshop on Information Integration on the Web</booktitle>
<contexts>
<context position="5197" citStr="Pal et al., 2007" startWordPosition="817" endWordPosition="820">icles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS (Mintz et al., 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multiinstance learning system for relation extraction (see Section 7). 2 Related Work The increasingly popular approach, called distant supervision (DS), or weak supervision, utilizes a knowledge base to heuristically label a corpus (Wu and Weld, 2007; Bellare and McCallum, 2007; Pal et al., 2007). Our work was inspired by Mintz et al. (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia. Previous works (Hoffmann et al., 2010; Yao et al., 2010) have pointed out that the DS assumption generates noisy labeled data, but did not directly address the problem. Wang et al. (2011) applied a rule-based method to the problem by using popular entity types and keywords for each relation. In (Bellare and McCallum, 2007; Riedel et al., 2010; Hoffmann et al., 2011), they used multiinstance learning, which deals with uncertainty of label</context>
</contexts>
<marker>Pal, Mann, Minerich, 2007</marker>
<rawString>Chris Pal, Gideon Mann, and Richard Minerich. 2007. Putting semantic information extraction on the map: Noisy label models for fact extraction. In Sixth International Workshop on Information Integration on the Web (IIWeb ’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL ’06),</booktitle>
<pages>113--120</pages>
<contexts>
<context position="6100" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="967" endWordPosition="970">abeled data, but did not directly address the problem. Wang et al. (2011) applied a rule-based method to the problem by using popular entity types and keywords for each relation. In (Bellare and McCallum, 2007; Riedel et al., 2010; Hoffmann et al., 2011), they used multiinstance learning, which deals with uncertainty of labels, to relax the DS assumption. However, the relaxed assumption can fail when a labeled entity pair is mentioned only once in a corpus (Riedel et al., 2010). Our approach relies on neither of these assumptions. Bootstrapping for relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006; Carlson et al., 2010) is related to our method. In bootstrapping, seed entity pairs of the target relation are given in order to select reliable patterns, which are used to extract new entity pairs. To avoid the selection of unreliable patterns, bootstrapping introduces scoring functions for each pattern candidate. This can be applied to our approach, which seeks to reduce the number of unreliable patterns by using a set of given entity pairs. However, the bootstrappinglike approach suffers from sensitive parameters that are critical to its performance. Ideally, the parameters such as a thre</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL ’06), pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text. In</title>
<date>2010</date>
<booktitle>In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML-PKDD ’10),</booktitle>
<pages>148--163</pages>
<contexts>
<context position="2888" citStr="Riedel et al. (2010)" startWordPosition="443" endWordPosition="446"> in a target text generally expresses more than one relation while a knowledge base stores a subset of the relations. The assumption ignores this possibility. For instance, consider the place of birth relation between Michael Jackson and Gary in Figure 1. The upper sentence indeed expresses the place of birth relation between the two entities. In DS place of birth is assigned to the sentence, and it becomes a useful training example. On the other hand, the lower sentence does not express this relation between the two entities, but the DS heuristic wrongly labels the sentence as expressing it. Riedel et al. (2010) relax the DS assumption as at least one sentence containing an entity pair ex721 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 721–729, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics pressing the corresponding relation in the knowledge base. They cast the relaxed assumption as multi-instance learning. However, even the relaxed assumption can fail. The relaxation is equivalent to the DS assumption when a labeled pair of entities is mentioned once in a target corpus (Riedel et al., 2010). In fact, 91.7% </context>
<context position="4263" citStr="Riedel et al. (2010)" startWordPosition="669" endWordPosition="672">y DS without using either of these assumptions. Given the labeled corpus created with the DS assumption, we first predict whether each pattern, which frequently appears in text to express a relation (see Section 4), expresses a target relation. Patterns that are predicted not to express the relation are used to form a negative pattern list for removing wrong labels of the relation. The main contributions of this paper are as follows: • To make the pattern prediction, we propose a generative model that directly models the process of automatic labeling in DS. Without any strong assumptions like Riedel et al. (2010)’s, the model predicts whether each pattern expresses each relation via hidden variables (see Section 5). • Our variational inference for our generative model lets us automatically calibrate parameters for each relation, which are sensitive to the performance (see Section 6). • We applied our method to Wikipedia articles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS (Mintz et al., 2009) and MultiR (Hoffmann et al.,</context>
<context position="5700" citStr="Riedel et al., 2010" startWordPosition="903" endWordPosition="906">s a knowledge base to heuristically label a corpus (Wu and Weld, 2007; Bellare and McCallum, 2007; Pal et al., 2007). Our work was inspired by Mintz et al. (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia. Previous works (Hoffmann et al., 2010; Yao et al., 2010) have pointed out that the DS assumption generates noisy labeled data, but did not directly address the problem. Wang et al. (2011) applied a rule-based method to the problem by using popular entity types and keywords for each relation. In (Bellare and McCallum, 2007; Riedel et al., 2010; Hoffmann et al., 2011), they used multiinstance learning, which deals with uncertainty of labels, to relax the DS assumption. However, the relaxed assumption can fail when a labeled entity pair is mentioned only once in a corpus (Riedel et al., 2010). Our approach relies on neither of these assumptions. Bootstrapping for relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006; Carlson et al., 2010) is related to our method. In bootstrapping, seed entity pairs of the target relation are given in order to select reliable patterns, which are used to extract new entity pairs.</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML-PKDD ’10), pages 148–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In AAAI/IAAI,</booktitle>
<pages>474--479</pages>
<contexts>
<context position="6068" citStr="Riloff and Jones, 1999" startWordPosition="963" endWordPosition="966">mption generates noisy labeled data, but did not directly address the problem. Wang et al. (2011) applied a rule-based method to the problem by using popular entity types and keywords for each relation. In (Bellare and McCallum, 2007; Riedel et al., 2010; Hoffmann et al., 2011), they used multiinstance learning, which deals with uncertainty of labels, to relax the DS assumption. However, the relaxed assumption can fail when a labeled entity pair is mentioned only once in a corpus (Riedel et al., 2010). Our approach relies on neither of these assumptions. Bootstrapping for relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006; Carlson et al., 2010) is related to our method. In bootstrapping, seed entity pairs of the target relation are given in order to select reliable patterns, which are used to extract new entity pairs. To avoid the selection of unreliable patterns, bootstrapping introduces scoring functions for each pattern candidate. This can be applied to our approach, which seeks to reduce the number of unreliable patterns by using a set of given entity pairs. However, the bootstrappinglike approach suffers from sensitive parameters that are critical to its performance. Ideall</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In AAAI/IAAI, pages 474–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Wang</author>
<author>James Fan</author>
<author>Aditya Kalyanpur</author>
<author>David Gondek</author>
</authors>
<title>Relation extraction with relation topics.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP ’11),</booktitle>
<pages>1426--1436</pages>
<contexts>
<context position="5543" citStr="Wang et al. (2011)" startWordPosition="877" endWordPosition="880">tem for relation extraction (see Section 7). 2 Related Work The increasingly popular approach, called distant supervision (DS), or weak supervision, utilizes a knowledge base to heuristically label a corpus (Wu and Weld, 2007; Bellare and McCallum, 2007; Pal et al., 2007). Our work was inspired by Mintz et al. (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia. Previous works (Hoffmann et al., 2010; Yao et al., 2010) have pointed out that the DS assumption generates noisy labeled data, but did not directly address the problem. Wang et al. (2011) applied a rule-based method to the problem by using popular entity types and keywords for each relation. In (Bellare and McCallum, 2007; Riedel et al., 2010; Hoffmann et al., 2011), they used multiinstance learning, which deals with uncertainty of labels, to relax the DS assumption. However, the relaxed assumption can fail when a labeled entity pair is mentioned only once in a corpus (Riedel et al., 2010). Our approach relies on neither of these assumptions. Bootstrapping for relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006; Carlson et al., 2010) is related to our m</context>
</contexts>
<marker>Wang, Fan, Kalyanpur, Gondek, 2011</marker>
<rawString>Chang Wang, James Fan, Aditya Kalyanpur, and David Gondek. 2011. Relation extraction with relation topics. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP ’11), pages 1426–1436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Autonomously semantifying wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th ACM Conference on Conference on Information and Knowledge Management (CIKM ’07),</booktitle>
<pages>41--50</pages>
<contexts>
<context position="5150" citStr="Wu and Weld, 2007" startWordPosition="809" endWordPosition="812">on 6). • We applied our method to Wikipedia articles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS (Mintz et al., 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multiinstance learning system for relation extraction (see Section 7). 2 Related Work The increasingly popular approach, called distant supervision (DS), or weak supervision, utilizes a knowledge base to heuristically label a corpus (Wu and Weld, 2007; Bellare and McCallum, 2007; Pal et al., 2007). Our work was inspired by Mintz et al. (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia. Previous works (Hoffmann et al., 2010; Yao et al., 2010) have pointed out that the DS assumption generates noisy labeled data, but did not directly address the problem. Wang et al. (2011) applied a rule-based method to the problem by using popular entity types and keywords for each relation. In (Bellare and McCallum, 2007; Riedel et al., 2010; Hoffmann et al., 2011), they used multiinstance </context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Daniel S. Weld. 2007. Autonomously semantifying wikipedia. In Proceedings of the 16th ACM Conference on Conference on Information and Knowledge Management (CIKM ’07), pages 41–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan Yan</author>
<author>Naoaki Okazaki</author>
<author>Yutaka Matsuo</author>
<author>Zhenglu Yang</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Unsupervised relation extraction by mining wikipedia texts using information from the web.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP ’09),</booktitle>
<pages>1021--1029</pages>
<contexts>
<context position="19972" citStr="Yan et al., 2009" startWordPosition="3467" endWordPosition="3470">assifier with a labeled corpus cleaned by Algorithm 1 using the negative pattern list predicted by the generative model. 7.1 Dataset Following Mintz et al. (2009), we carried out our experiments using Wikipedia as the target corpus and Freebase (September, 2009, (Google, 2009)) as the knowledge base. We used more than 1,300,000 Wikipedia articles in the wex dump data (September, 2009, (Metaweb Technologies, 2009)). The properties of our data are shown in Table 1. In Wikipedia articles, named entities were identified by anchor text linking to another article and starting with a capital letter (Yan et al., 2009). We applied Open NLP POS tagger5 and MaltParser (Nivre et al., 2007) to sentences containing more 4The first-order information becomes zero in this case. 5http://opennlp.sourceforge.net/ Table 1: Properties of Wikipedia dataset documents 1,303,000 entity pairs 2,017,000 (matched to Freebase) 129,000 (with entity types) 913,000 frequent patterns 3,084 relations 24 than one named entity. We then extracted sentences containing related entity pairs with the method explained in Section 3. To match entity pairs, we used ID mapping between the dump data and Freebase. We used the most frequent 24 rel</context>
</contexts>
<marker>Yan, Okazaki, Matsuo, Yang, Ishizuka, 2009</marker>
<rawString>Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu Yang, and Mitsuru Ishizuka. 2009. Unsupervised relation extraction by mining wikipedia texts using information from the web. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP ’09), pages 1021–1029.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Collective cross-document relation extraction without labelled data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP ’10),</booktitle>
<pages>1013--1023</pages>
<contexts>
<context position="5412" citStr="Yao et al., 2010" startWordPosition="855" endWordPosition="858">an the original DS (Mintz et al., 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multiinstance learning system for relation extraction (see Section 7). 2 Related Work The increasingly popular approach, called distant supervision (DS), or weak supervision, utilizes a knowledge base to heuristically label a corpus (Wu and Weld, 2007; Bellare and McCallum, 2007; Pal et al., 2007). Our work was inspired by Mintz et al. (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia. Previous works (Hoffmann et al., 2010; Yao et al., 2010) have pointed out that the DS assumption generates noisy labeled data, but did not directly address the problem. Wang et al. (2011) applied a rule-based method to the problem by using popular entity types and keywords for each relation. In (Bellare and McCallum, 2007; Riedel et al., 2010; Hoffmann et al., 2011), they used multiinstance learning, which deals with uncertainty of labels, to relax the DS assumption. However, the relaxed assumption can fail when a labeled entity pair is mentioned only once in a corpus (Riedel et al., 2010). Our approach relies on neither of these assumptions. Boots</context>
</contexts>
<marker>Yao, Riedel, McCallum, 2010</marker>
<rawString>Limin Yao, Sebastian Riedel, and Andrew McCallum. 2010. Collective cross-document relation extraction without labelled data. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP ’10), pages 1013–1023.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>