<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.102218">
<title confidence="0.998929">
Discriminative Approach to Predicate-Argument Structure Analysis
with Zero-Anaphora Resolution
</title>
<author confidence="0.89947">
Kenji Imamura, Kuniko Saito, and Tomoko Izumi
</author>
<affiliation confidence="0.731105">
NTT Cyber Space Laboratories, NTT Corporation
</affiliation>
<address confidence="0.897256">
1-1 Hikarinooka, Yokosuka, Kanagawa, 239-0847, Japan
</address>
<email confidence="0.999565">
{imamura.kenji,saito.kuniko,izumi.tomoko}@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.997398" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999925133333333">
This paper presents a predicate-argument
structure analysis that simultaneously con-
ducts zero-anaphora resolution. By adding
noun phrases as candidate arguments that
are not only in the sentence of the target
predicate but also outside of the sentence,
our analyzer identifies arguments regard-
less of whether they appear in the sen-
tence or not. Because we adopt discrimi-
native models based on maximum entropy
for argument identification, we can easily
add new features. We add language model
scores as well as contextual features. We
also use contextual information to restrict
candidate arguments.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99982825">
Predicate-argument structure analysis is a type of
semantic role labeling, which is an important mod-
ule to extract event information such as “who did
what to whom” from a sentence. There are many
arguments called zero pronouns that do not appear
in the surface of a sentence in Japanese. In this
case, predicate-argument structures cannot be con-
structed if we only rely on the syntactic informa-
tion of a single sentence. Similar phenomena also
happen in English noun predicates, in which ar-
guments of noun predicates sometimes do not ex-
ist in the sentence due to things such as ellipses
(Jiang and Ng, 2006). To correctly extract the
structures from such sentences, it is necessary to
resolve what zero pronouns refer to by using other
information such as context.
Although predicate-argument structure analysis
and zero-anaphora resolution are closely related,
it was not until recently that these two tasks were
lumped together. Due to the developments of
large annotated corpora with predicate-argument
and coreference relations (e.g.,(Iida et al., 2007))
and with case frames, several works using statisti-
cal models have been proposed to solve these two
tasks simultaneously (Sasano et al., 2008; Taira et
al., 2008).
In this paper, we present a predicate-argument
structure analysis that simultaneously resolves the
anaphora of zero pronouns in Japanese, based on
supervised learning. The analyzer obtains candi-
date arguments not only from the sentence of the
target predicate but also from the previous sen-
tences. It then identifies the most likely argu-
ments based on discriminative models. To iden-
tify arguments that appear in the sentence and are
represented by zero pronouns without distinction,
the analyzer introduces the following features and
techniques: the language model features of noun
phrases, contextual features, and restrictions of
candidate arguments.
</bodyText>
<sectionHeader confidence="0.995218" genericHeader="method">
2 Predicate-Argument Structure
Analyzer
</sectionHeader>
<subsectionHeader confidence="0.998901">
2.1 Procedure and Models
</subsectionHeader>
<bodyText confidence="0.9986235">
The procedure of our predicate-argument structure
analyzer is as follows. The input to the analyzer is
an article (multiple sentences) because our target
is to identify arguments spread across sentences.
</bodyText>
<listItem confidence="0.999067615384616">
1. First, each sentence is individually analyzed
and segmented into base phrases by a morpho-
logical analyzer and a base phrase chunker. In
Japanese, a base phrase is usually constructed
by one or more content words (such as base
noun phrases) and function words (such as case
particles). In addition, dependency relations
among base phrases are parsed by a depen-
dency parser. In this paper, base phrases and
dependency relations are acquired from an an-
notated corpus (i.e., correct parses).
2. Next, predicates are extracted from the base
phrases. In general, a predicate is determined
</listItem>
<page confidence="0.995317">
85
</page>
<note confidence="0.9720355">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 85–88,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<table confidence="0.959037074074074">
Name Note
Baseline Predicate Form and POS of the predi-
Features cate
Noun Form and POS of the head-
word of the candidate phrase
Particle Form and POS of the particle
of the candidate phrase
Path Dependency relation between
the predicate and the candi-
date phrase
Passive Passive auxiliary verbs that
the predicate contains
PhPosit Relative phrase position be-
tween the predicate and the
candidate phrase
SentPosit Relative sentence position be-
tween the predicate and the
candidate phrase
Additional LangModel Language model scores
Features
(c.f.,
Sec. 2.2
and 2.3)
Used Flag whether the candidate
phrase was used as arguments
of previous predicates
SRLOrder Order in Salient Referent List
</table>
<figureCaption confidence="0.999574">
Figure 1: Summary of Argument Identification
</figureCaption>
<bodyText confidence="0.903048">
satisfies the following equations from the candi-
date arguments:
</bodyText>
<figure confidence="0.999521103448276">
Candidate Arguments Candidate Arguments
before Sentences of Predicate in Sentence of Predicate
NULL
Phrase 1
Phrase 2
Candidate Arguments
Phrase 3
Phrase 4
...
Phrase 1 Phrase 3 NULL
exophoric
or no argument
zero-anaphoric
(inter-sentential)
Select
Best
Phrase
Nom.
Model
Select
Best
Phrase
Acc.
Model
Select
Best
Phrase
Dat.
Model
</figure>
<tableCaption confidence="0.981713">
Table 1: Features Used in this Paper nˆ = argmax P(d(nj) = 1|Xj; Mc) (1)
</tableCaption>
<equation confidence="0.99662">
nj∈N
∑
Zc(X) exp {λckfk(d(nj) = 1, Xj)}(2)
k
Zc(X) =
∑ ∑exp {λckfk(d(nj) = 1, Xj)} (3)
nj∈N k
Xj = hnj, v, Ai (4)
P(d(nj) = 1|Xj; Mc) =
1
</equation>
<bodyText confidence="0.992818666666667">
based on parts of speech such as verbs and ad-
jectives. In this paper, the predicates are also
provided from an annotated corpus.
</bodyText>
<listItem confidence="0.970976173913043">
3. Concurrently, noun phrases and their head-
words are extracted as candidate arguments
from base phrases. If an argument of a predi-
cate is a zero pronoun, it is likely that the argu-
ment itself has appeared in previous sentences.
Therefore, the analyzer collects not only all
phrases in the sentence but also some phrases
in the previous sentences. We also add the spe-
cial noun phrase NULL, which denotes that the
argument of the predicate is not required or did
not appear in the article (i.e., exophoric).
4. Next, features needed for an argument iden-
tifier are extracted from each pair of a predi-
cate and a candidate argument. Features used
in this paper are shown in Table 1. Base-
line features are roughly those of the predi-
cate, the noun phrase, and their relations (on
the phrasal/sentential sequence and the depen-
dency tree). For binary features, we use all
combinations of these features listed above.
5. Finally, the argument identifier selects the best
phrases for nominative, accusative, and dative
cases from the candidate arguments (Figure 1).
</listItem>
<bodyText confidence="0.999949423076923">
In this paper, we use maximum entropy models
normalized for each predicate to each case. That
is, the identifier directly selects the best phrase that
where n, c, and v denote a noun phrase of an argu-
ment, the case, and the target predicate, respec-
tively, N denotes a set of candidate arguments,
d(n) is a function that returns 1 iff the phrase n
becomes the argument, and Mc denotes the model
of the case c. In addition, fk(d(nj) = 1, Xj) is a
feature function, λck denotes a weight parameter
of the feature function, and A denotes an article in
which all sentences are parsed.
As shown, our analyzer can assign the best noun
phrases to arguments regardless of whether they
appear in the sentence or not by collecting candi-
dates spread across multiple sentences. Further-
more, because the identifier is regarded as a selec-
tor based on the discriminative models, our ana-
lyzer has two properties: 1) New features can be
easily added. 2) The precision can be improved by
restricting the candidate arguments appropriately.
When we analyze predicate-argument struc-
tures and zero-anaphora resolution, syntactic in-
formation sometimes does not help because refer-
ents of zero pronouns do not appear in the sen-
tence of the predicate. To overcome this problem,
</bodyText>
<page confidence="0.992655">
86
</page>
<bodyText confidence="0.9992905">
we introduce additional information, i.e., language
model scores and contextual information.
</bodyText>
<subsectionHeader confidence="0.996165">
2.2 Language Models
</subsectionHeader>
<bodyText confidence="0.999985441176471">
Even if syntactic information does not help to
identify arguments, we can expect that a certain
noun phrase might be the correct argument of the
predicate when we put it in place of the zero
pronoun and the sentence becomes meaningful.
Therefore, we add language model scores as fea-
tures of the identifier. Because the appearance or-
der of argument phrases is not strongly constricted
in Japanese, we construct generation models that
reflect dependency relations among a predicate, its
case and a noun phrase. That is, we regard gen-
eration probabilities P(nlc, v) acquired from the
dependency tree as the scores of language models.
The language models are built from large plain
texts by using a dependency parser. First, predi-
cates and the base phrases that directly depend on
the predicates are aquired from parsed sentences.
Next, case particles and headwords are extracted
from the base phrases. Finally, generation prob-
abilities are computed using maximum likelihood
estimation. Good-Turing discounting and backoff
smoothing are also applied. Here, it is necessary
to assign generation probabilities to NULLs. Re-
garding the training corpus that will be described
in Section 3, the NULL rates of the nominative,
accusative, and dative cases were 16.7%, 59.9%,
and 81.6%, respectively. We assign these rates to
the backoff term P(NULL c).
Using the language models, generation proba-
bilities of the noun phrases are computed for ev-
ery case of the predicate, and features that main-
tain the logarithms of language model scores are
added (‘LangModel’ features in Table 1). Thus,
the values of these feature functions are real.
</bodyText>
<subsectionHeader confidence="0.999878">
2.3 Usage of Context
</subsectionHeader>
<bodyText confidence="0.999582454545454">
Centering theory claims that noun phrases that
have been used once tend to be used again within
the same context. We adopt this claim and add two
different kinds of features. One is the feature that
indicates whether a candidate has been used as an
argument of predicates in the preceding sentences
(‘Used’ features). However, the Used features are
affected by the accuracy of the previous analyses.
Thus, we also adopt the Salience Reference List
(Nariyama, 2002), which only uses explicit sur-
face case markers or a topic marker, and added
</bodyText>
<table confidence="0.99980375">
Training Development Test
# of Articles 1,751 480 695
# of Sentences 24,225 4,833 9,272
# of Predicates 67,145 13,594 25,500
# of Arguments
Nom. 56,132 11,969 21,931
Acc. 26,899 5,566 10,329
Dat. 12,332 3,147 5,944
</table>
<tableCaption confidence="0.994347">
Table 2: Corpus Statistics
</tableCaption>
<bodyText confidence="0.999187142857143">
their priority order to the List as another feature
(‘SRLOrder’ feature).
Another way to adopt contextual information
is to restrict the candidate arguments. When we
analyzed the training corpus from the viewpoint
of zero pronouns, it was found that 102.2 noun
phrases on average were required as candidate ar-
guments if we did not stipulate any restrictions.
When the candidate arguments we had restricted
to those that had been used as arguments of the
predicate appeared in a previous one sentence
(namely, noun phrases appeared in more than one
sentence before have a chance to remain), then the
number of candidate arguments significantly de-
creased to an average of 3.2 but they covered the
62.5% of the referents of zero pronouns.
By using these characteristics, our analyzer re-
stricts the candidate arguments to those that are of
the same sentence, and those that were used as the
arguments of another predicate in a previous sen-
tence.
</bodyText>
<sectionHeader confidence="0.999878" genericHeader="evaluation">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999767">
3.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999986875">
Corpora: We used the NAIST Text Corpus ver-
sion 1.4b (Iida et al., 2007) and the Kyoto Text
Corpus 4.0 as the annotated corpora. We could
obtain dependency and predicate-argument struc-
tures because these corpora were annotated to al-
most the same newspaper articles. We divided
them into training, development, and test sets as
shown in Table 2.
</bodyText>
<subsectionHeader confidence="0.478917">
Argument Identification Models: Maximum
</subsectionHeader>
<bodyText confidence="0.9797015">
entropy models were trained using the training set.
In these experiments, we used the Gaussian prior,
and the variance was tuned using the development
set. Candidate argument restrictions were applied
during both training and decoding.
Language Models: Language models were
trained from twelve years of newspaper articles
(Mainichi Shinbun newspaper 1991-2002, about
</bodyText>
<page confidence="0.998124">
87
</page>
<table confidence="0.999777285714286">
Case Type # of Prec. Rec. F
Args.
Nom. Dep. 14,287 85.2% 88.8% 87.0%
Zero-Intra 4,581 58.8% 43.4% 50.0%
Zero-Inter 3,063 47.5% 7.6% 13.1%
Total 21,931 79.4% 68.0% 73.2%
Acc. Dep. 9,316 95.6% 92.2% 93.9%
Zero-Intra 742 53.7% 21.6% 30.8%
Zero-Inter 271 25.0% 0.4% 0.7%
Total 10,329 94.3% 84.7% 89.2%
Dat. Dep. 5,409 91.1% 72.6% 80.8%
Zero-Intra 396 0.0% 0.0% 0.0%
Zero-Inter 139 0.0% 0.0% 0.0%
Total 5,944 91.1% 66.1% 76.6%
</table>
<tableCaption confidence="0.999959">
Table 3: Results on the Test Set
</tableCaption>
<bodyText confidence="0.9957251">
5.5M sentences) using the method described in
Section 2.2. However, we eliminated articles that
overlap the NAIST Corpus.
Evaluation: We evaluated the precision and re-
call rates, and F scores, all of which were com-
puted by comparing system output and the correct
answer of each argument. We also evaluated the
rate at which all arguments of a predicate were
completely identified as predicate-argument accu-
racy.
</bodyText>
<subsectionHeader confidence="0.723374">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999995111111111">
The results are shown in Table 3. This table
shows accuracies of the argument identification
according to each case and each dependency re-
lation between predicates and arguments. The
predicate-argument accuracy on the test set was
59.4% (15,140/25,500).
First, focusing on the F scores of the Dep. rela-
tions, which denote a predicate and an argument in
the same sentence and directly depend upon each
other, scores of over 80% were obtained for all
cases. Compared with Taira et al. (2008), they
were higher in the nominative and accusative cases
but were lower in the dative case. Overall, we ob-
tained F scores between 73.2% and 89.2%.
Next, focusing on the intra-sentential (Zero-
Intra) and inter-sentential (Zero-Intra) zero-
anaphora, the analyzer identified arguments at
some level from the viewpoint of precision. How-
ever, the recall rates and F scores were very
low. The Zero-Inter recall rate for the nominative
case, in which zero pronouns are centered, was
only 7.6%. This is because our method preferred
NULL phrases over unreliable phrases appearing
before the predicate sentence. In fact, the analyzer
output only 488 arguments, although the answer
was 3,063. To control the NULL preference is a
future work for our analyzer.
</bodyText>
<sectionHeader confidence="0.997295" genericHeader="conclusions">
4 Discussions and Conclusions
</sectionHeader>
<bodyText confidence="0.999990074074074">
We proposed a predicate-argument structure anal-
ysis that simultaneously conducts zero-anaphora
resolution. By adding noun phrases as candidate
arguments that are not only in the sentence of
the target predicate but also outside of the sen-
tence, our analyzer identified arguments regard-
less of whether they appear in the sentence or
not. Because we adopted discriminative models
for argument identification, we can easily add new
features. By using this property, we added lan-
guage model scores as well as contextual features.
We also used contextual information to restrict
candidate arguments. As a result, we achieved
predicate-argument accuracy of 59.4%, and accu-
racies of argument identification were F-scores of
73.2%–89.2%.
Verifying argument structures by language
models evokes selectional preference of case
frames. Sasano et al. (2008) has proposed statis-
tical models using case frames built from 1.6 B
sentences. Because the amount of the resources
used in our study is quite different, we cannot di-
rectly compare the methods and results. However,
because our analyzer has scalability that can freely
add new features, for our future work, we hope to
adopt the case frames as new features and compare
their effect.
</bodyText>
<sectionHeader confidence="0.999552" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99988995">
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text cor-
pus with predicate-argument and coreference rela-
tions. In Proceedings of the Linguistic Annotation
Workshop in ACL-2007, pages 132–139.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Seman-
tic role labeling of nombank: A maximum entropy
approach. In Proceedings of EMNLP-2006, pages
138–145.
Shigeko Nariyama. 2002. Grammar for ellipsis res-
olution in Japanese. In Proceedings of TMI-2002,
pages 135–145.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2008. A fully-lexicalized probabilistic model
for Japanese zero anaphora resolution. In Proceed-
ings of COLING-2008, pages 769–776.
Hirotoshi Taira, Sanae Fujita, and Masaaki Nagata.
2008. A Japanese predicate argument structure anal-
ysis using decision lists. In Proceedings ofEMNLP-
2008, pages 523–532.
</reference>
<page confidence="0.999409">
88
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.855716">
<title confidence="0.986623">Discriminative Approach to Predicate-Argument Structure Analysis with Zero-Anaphora Resolution</title>
<author confidence="0.95191">Kuniko Saito Imamura</author>
<author confidence="0.95191">Izumi</author>
<affiliation confidence="0.997731">NTT Cyber Space Laboratories, NTT Corporation</affiliation>
<address confidence="0.916608">1-1 Hikarinooka, Yokosuka, Kanagawa, 239-0847, Japan</address>
<abstract confidence="0.9995439375">This paper presents a predicate-argument structure analysis that simultaneously conducts zero-anaphora resolution. By adding noun phrases as candidate arguments that are not only in the sentence of the target predicate but also outside of the sentence, our analyzer identifies arguments regardless of whether they appear in the sentence or not. Because we adopt discriminative models based on maximum entropy for argument identification, we can easily add new features. We add language model scores as well as contextual features. We also use contextual information to restrict candidate arguments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Mamoru Komachi</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Annotating a Japanese text corpus with predicate-argument and coreference relations.</title>
<date>2007</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop in ACL-2007,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1976" citStr="Iida et al., 2007" startWordPosition="287" endWordPosition="290">also happen in English noun predicates, in which arguments of noun predicates sometimes do not exist in the sentence due to things such as ellipses (Jiang and Ng, 2006). To correctly extract the structures from such sentences, it is necessary to resolve what zero pronouns refer to by using other information such as context. Although predicate-argument structure analysis and zero-anaphora resolution are closely related, it was not until recently that these two tasks were lumped together. Due to the developments of large annotated corpora with predicate-argument and coreference relations (e.g.,(Iida et al., 2007)) and with case frames, several works using statistical models have been proposed to solve these two tasks simultaneously (Sasano et al., 2008; Taira et al., 2008). In this paper, we present a predicate-argument structure analysis that simultaneously resolves the anaphora of zero pronouns in Japanese, based on supervised learning. The analyzer obtains candidate arguments not only from the sentence of the target predicate but also from the previous sentences. It then identifies the most likely arguments based on discriminative models. To identify arguments that appear in the sentence and are re</context>
<context position="11150" citStr="Iida et al., 2007" startWordPosition="1782" endWordPosition="1785">s arguments of the predicate appeared in a previous one sentence (namely, noun phrases appeared in more than one sentence before have a chance to remain), then the number of candidate arguments significantly decreased to an average of 3.2 but they covered the 62.5% of the referents of zero pronouns. By using these characteristics, our analyzer restricts the candidate arguments to those that are of the same sentence, and those that were used as the arguments of another predicate in a previous sentence. 3 Experiments 3.1 Experimental Settings Corpora: We used the NAIST Text Corpus version 1.4b (Iida et al., 2007) and the Kyoto Text Corpus 4.0 as the annotated corpora. We could obtain dependency and predicate-argument structures because these corpora were annotated to almost the same newspaper articles. We divided them into training, development, and test sets as shown in Table 2. Argument Identification Models: Maximum entropy models were trained using the training set. In these experiments, we used the Gaussian prior, and the variance was tuned using the development set. Candidate argument restrictions were applied during both training and decoding. Language Models: Language models were trained from </context>
</contexts>
<marker>Iida, Komachi, Inui, Matsumoto, 2007</marker>
<rawString>Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Matsumoto. 2007. Annotating a Japanese text corpus with predicate-argument and coreference relations. In Proceedings of the Linguistic Annotation Workshop in ACL-2007, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Ping Jiang</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Semantic role labeling of nombank: A maximum entropy approach.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP-2006,</booktitle>
<pages>138--145</pages>
<contexts>
<context position="1526" citStr="Jiang and Ng, 2006" startWordPosition="223" endWordPosition="226">ntroduction Predicate-argument structure analysis is a type of semantic role labeling, which is an important module to extract event information such as “who did what to whom” from a sentence. There are many arguments called zero pronouns that do not appear in the surface of a sentence in Japanese. In this case, predicate-argument structures cannot be constructed if we only rely on the syntactic information of a single sentence. Similar phenomena also happen in English noun predicates, in which arguments of noun predicates sometimes do not exist in the sentence due to things such as ellipses (Jiang and Ng, 2006). To correctly extract the structures from such sentences, it is necessary to resolve what zero pronouns refer to by using other information such as context. Although predicate-argument structure analysis and zero-anaphora resolution are closely related, it was not until recently that these two tasks were lumped together. Due to the developments of large annotated corpora with predicate-argument and coreference relations (e.g.,(Iida et al., 2007)) and with case frames, several works using statistical models have been proposed to solve these two tasks simultaneously (Sasano et al., 2008; Taira </context>
</contexts>
<marker>Jiang, Ng, 2006</marker>
<rawString>Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic role labeling of nombank: A maximum entropy approach. In Proceedings of EMNLP-2006, pages 138–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shigeko Nariyama</author>
</authors>
<title>Grammar for ellipsis resolution in Japanese.</title>
<date>2002</date>
<booktitle>In Proceedings of TMI-2002,</booktitle>
<pages>135--145</pages>
<contexts>
<context position="9778" citStr="Nariyama, 2002" startWordPosition="1555" endWordPosition="1556">ithms of language model scores are added (‘LangModel’ features in Table 1). Thus, the values of these feature functions are real. 2.3 Usage of Context Centering theory claims that noun phrases that have been used once tend to be used again within the same context. We adopt this claim and add two different kinds of features. One is the feature that indicates whether a candidate has been used as an argument of predicates in the preceding sentences (‘Used’ features). However, the Used features are affected by the accuracy of the previous analyses. Thus, we also adopt the Salience Reference List (Nariyama, 2002), which only uses explicit surface case markers or a topic marker, and added Training Development Test # of Articles 1,751 480 695 # of Sentences 24,225 4,833 9,272 # of Predicates 67,145 13,594 25,500 # of Arguments Nom. 56,132 11,969 21,931 Acc. 26,899 5,566 10,329 Dat. 12,332 3,147 5,944 Table 2: Corpus Statistics their priority order to the List as another feature (‘SRLOrder’ feature). Another way to adopt contextual information is to restrict the candidate arguments. When we analyzed the training corpus from the viewpoint of zero pronouns, it was found that 102.2 noun phrases on average w</context>
</contexts>
<marker>Nariyama, 2002</marker>
<rawString>Shigeko Nariyama. 2002. Grammar for ellipsis resolution in Japanese. In Proceedings of TMI-2002, pages 135–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A fully-lexicalized probabilistic model for Japanese zero anaphora resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING-2008,</booktitle>
<pages>769--776</pages>
<contexts>
<context position="2118" citStr="Sasano et al., 2008" startWordPosition="310" endWordPosition="313">lipses (Jiang and Ng, 2006). To correctly extract the structures from such sentences, it is necessary to resolve what zero pronouns refer to by using other information such as context. Although predicate-argument structure analysis and zero-anaphora resolution are closely related, it was not until recently that these two tasks were lumped together. Due to the developments of large annotated corpora with predicate-argument and coreference relations (e.g.,(Iida et al., 2007)) and with case frames, several works using statistical models have been proposed to solve these two tasks simultaneously (Sasano et al., 2008; Taira et al., 2008). In this paper, we present a predicate-argument structure analysis that simultaneously resolves the anaphora of zero pronouns in Japanese, based on supervised learning. The analyzer obtains candidate arguments not only from the sentence of the target predicate but also from the previous sentences. It then identifies the most likely arguments based on discriminative models. To identify arguments that appear in the sentence and are represented by zero pronouns without distinction, the analyzer introduces the following features and techniques: the language model features of </context>
</contexts>
<marker>Sasano, Kawahara, Kurohashi, 2008</marker>
<rawString>Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi. 2008. A fully-lexicalized probabilistic model for Japanese zero anaphora resolution. In Proceedings of COLING-2008, pages 769–776.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirotoshi Taira</author>
<author>Sanae Fujita</author>
<author>Masaaki Nagata</author>
</authors>
<title>A Japanese predicate argument structure analysis using decision lists.</title>
<date>2008</date>
<booktitle>In Proceedings ofEMNLP2008,</booktitle>
<pages>523--532</pages>
<contexts>
<context position="2139" citStr="Taira et al., 2008" startWordPosition="314" endWordPosition="317"> 2006). To correctly extract the structures from such sentences, it is necessary to resolve what zero pronouns refer to by using other information such as context. Although predicate-argument structure analysis and zero-anaphora resolution are closely related, it was not until recently that these two tasks were lumped together. Due to the developments of large annotated corpora with predicate-argument and coreference relations (e.g.,(Iida et al., 2007)) and with case frames, several works using statistical models have been proposed to solve these two tasks simultaneously (Sasano et al., 2008; Taira et al., 2008). In this paper, we present a predicate-argument structure analysis that simultaneously resolves the anaphora of zero pronouns in Japanese, based on supervised learning. The analyzer obtains candidate arguments not only from the sentence of the target predicate but also from the previous sentences. It then identifies the most likely arguments based on discriminative models. To identify arguments that appear in the sentence and are represented by zero pronouns without distinction, the analyzer introduces the following features and techniques: the language model features of noun phrases, context</context>
<context position="13201" citStr="Taira et al. (2008)" startWordPosition="2110" endWordPosition="2113">o evaluated the rate at which all arguments of a predicate were completely identified as predicate-argument accuracy. 3.2 Results The results are shown in Table 3. This table shows accuracies of the argument identification according to each case and each dependency relation between predicates and arguments. The predicate-argument accuracy on the test set was 59.4% (15,140/25,500). First, focusing on the F scores of the Dep. relations, which denote a predicate and an argument in the same sentence and directly depend upon each other, scores of over 80% were obtained for all cases. Compared with Taira et al. (2008), they were higher in the nominative and accusative cases but were lower in the dative case. Overall, we obtained F scores between 73.2% and 89.2%. Next, focusing on the intra-sentential (ZeroIntra) and inter-sentential (Zero-Intra) zeroanaphora, the analyzer identified arguments at some level from the viewpoint of precision. However, the recall rates and F scores were very low. The Zero-Inter recall rate for the nominative case, in which zero pronouns are centered, was only 7.6%. This is because our method preferred NULL phrases over unreliable phrases appearing before the predicate sentence.</context>
</contexts>
<marker>Taira, Fujita, Nagata, 2008</marker>
<rawString>Hirotoshi Taira, Sanae Fujita, and Masaaki Nagata. 2008. A Japanese predicate argument structure analysis using decision lists. In Proceedings ofEMNLP2008, pages 523–532.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>