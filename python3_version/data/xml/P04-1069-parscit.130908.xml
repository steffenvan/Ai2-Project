<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000066">
<title confidence="0.988979">
Probabilistic Parsing Strategies
</title>
<author confidence="0.981063">
Mark-Jan Nederhof
</author>
<affiliation confidence="0.9813325">
Faculty of Arts
University of Groningen
</affiliation>
<note confidence="0.592528333333333">
P.O. Box 716
NL-9700 AS Groningen
The Netherlands
</note>
<email confidence="0.994228">
markjan@let.rug.nl
</email>
<author confidence="0.99666">
Giorgio Satta
</author>
<affiliation confidence="0.997938">
Dept. of Information Engineering
University of Padua
</affiliation>
<address confidence="0.863156666666667">
via Gradenigo, 6/A
I-35131 Padova
Italy
</address>
<email confidence="0.996326">
satta@dei.unipd.it
</email>
<sectionHeader confidence="0.993797" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999844714285714">
We present new results on the relation between
context-free parsing strategies and their probabilis-
tic counter-parts. We provide a necessary condition
and a sufficient condition for the probabilistic exten-
sion of parsing strategies. These results generalize
existing results in the literature that were obtained
by considering parsing strategies in isolation.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969186666667">
Context-free grammars (CFGs) are standardly used
in computational linguistics as formal models of the
syntax of natural language, associating sentences
with all their possible derivations. Other computa-
tional models with the same generative capacity as
CFGs are also adopted, as for instance push-down
automata (PDAs). One of the advantages of the use
of PDAs is that these devices provide an operational
specification that determines which steps must be
performed when parsing an input string, something
that is not offered by CFGs. In other words, PDAs
can be associated to parsing strategies for context-
free languages. More precisely, parsing strategies
are traditionally specified as constructions that map
CFGs to language-equivalent PDAs. Popular ex-
amples of parsing strategies are the standard con-
structions of top-down PDAs (Harrison, 1978), left-
corner PDAs (Rosenkrantz and Lewis II, 1970),
shift-reduce PDAs (Aho and Ullman, 1972) and LR
PDAs (Sippu and Soisalon-Soininen, 1990).
CFGs and PDAs have probabilistic counterparts,
called probabilistic CFGs (PCFGs) and probabilis-
tic PDAs (PPDAs). These models are very popular
in natural language processing applications, where
they are used to define a probability distribution
function on the domain of all derivations for sen-
tences in the language of interest. In PCFGs and
PPDAs, probabilities are assigned to rules or tran-
sitions, respectively. However, these probabilities
cannot be chosen entirely arbitrarily. For example,
for a given nonterminal A in a PCFG, the sum of the
probabilities of all rules rewriting A must be 1. This
means that, out of a total of say m rules rewriting A,
only m − 1 rules represent “free” parameters.
Depending on the choice of the parsing strategy,
the constructed PDA may allow different probabil-
ity distributions than the underlying CFG, since the
set of free parameters may differ between the CFG
and the PDA, both quantitatively and qualitatively.
For example, (Sornlertlamvanich et al., 1999) and
(Roark and Johnson, 1999) have shown that a prob-
ability distribution that can be obtained by training
the probabilities of a CFG on the basis of a corpus
can be less accurate than the probability distribution
obtained by training the probabilities of a PDA con-
structed by a particular parsing strategy, on the basis
of the same corpus. Also the results from (Chitrao
and Grishman, 1990), (Charniak and Carroll, 1994)
and (Manning and Carpenter, 2000) could be seen
in this light.
The question arises of whether parsing strate-
gies can be extended probabilistically, i.e., whether
a given construction of PDAs from CFGs can be
“augmented” with a function defining the probabili-
ties for the target PDA, given the probabilities asso-
ciated with the input CFG, in such a way that the ob-
tained probabilistic distributions on the CFG deriva-
tions and the corresponding PDA computations are
equivalent. Some first results on this issue have been
presented by (Tendeau, 1995), who shows that the
already mentioned left-corner parsing strategy can
be extended probabilistically, and later by (Abney et
al., 1999) who show that the pure top-down parsing
strategy and a specific type of shift-reduce parsing
strategy can be probabilistically extended.
One might think that any “practical” parsing
strategy can be probabilistically extended, but this
turns out not to be the case. We briefly discuss
here a counter-example, in order to motivate the ap-
proach we have taken in this paper. Probabilistic
LR parsing has been investigated in the literature
(Wright and Wrigley, 1991; Briscoe and Carroll,
1993; Inui et al., 2000) under the assumption that
it would allow more fine-grained probability distri-
butions than the underlying PCFGs. However, this
</bodyText>
<figure confidence="0.737903285714286">
is not the case in general. Consider a PCFG with
rule/probability pairs: B → bC, 23
5 → AB, 1 B → bD, 1
A → aC, 1 3
3 C → xc, 1
A → aD, 2 D → xd, 1
3
</figure>
<bodyText confidence="0.9994415">
There are two key transitions in the associated LR
automaton, which represent shift actions over c and
d (we denote LR states by their sets of kernel items
and encode these states into stack symbols):
</bodyText>
<equation confidence="0.999817833333333">
Tc : {C → x • c,D → x • d} c
7→
{C → x • c,D → x • d} {C → xc •}
Td : {C → x • c,D → x • d} d
7→
{C → x • c,D → x • d} {D → xd •}
</equation>
<bodyText confidence="0.996354142857143">
Assume a proper assignment of probabilities to the
transitions of the LR automaton, i.e., the sum of
transition probabilities for a given LR state is 1. It
can be easily seen that we must assign probabil-
ity 1 to all transitions except Tc and Td, since this
is the only pair of distinct transitions that can be ap-
plied for one and the same top-of-stack symbol, viz.
</bodyText>
<equation confidence="0.656264">
{C → x • c, D → x • d}. However, in the PCFG
</equation>
<bodyText confidence="0.9274685">
model we have
whereas in the LR PPDA model we have
</bodyText>
<equation confidence="0.9959325">
Pr(τc)·Pr(τd) 1 1
Pr(τd)·Pr(τ _ c) T_ 4.
</equation>
<bodyText confidence="0.997233617021277">
Thus we conclude that there is no proper assignment
of probabilities to the transitions of the LR automa-
ton that would result in a distribution on the gener-
ated language that is equivalent to the one induced
by the source PCFG. Therefore the LR strategy does
not allow probabilistic extension.
One may seemingly solve this problem by drop-
ping the constraint of properness, letting each tran-
sition that outputs a rule have the same probability
as that rule in the PCFG, and letting other transitions
have probability 1. However, the properness condi-
tion for PDAs has been heavily exploited in pars-
ing applications, in doing incremental left-to-right
probability computation for beam search (Roark
and Johnson, 1999; Manning and Carpenter, 2000),
and more generally in integration with other lin-
ear probabilistic models. Furthermore, commonly
used training algorithms for PCFGS/PPDAs always
produce proper probability assignments, and many
desired mathematical properties of these methods
are based on such an assumption (Chi and Geman,
1998; S´anchez and Benedi, 1997). We may there-
fore discard non-proper probability assignments in
the current study.
However, such probability assignments are out-
side the reach of the usual training algorithms for
PDAs, which always produce proper PDAs. There-
fore, we may discard such assignments in the cur-
rent study, which investigates aspects of the poten-
tial of training algorithms for CFGs and PDAs.
What has been lacking in the literature is a theo-
retical framework to relate the parameter space of a
CFG to that of a PDA constructed from the CFG by
a particular parsing strategy, in terms of the set of
allowable probability distributions over derivations.
Note that the number of free parameters alone is
not a satisfactory characterization of the parameter
space. In fact, if the “nature” of the parameters is
ill-chosen, then an increase in the number of param-
eters may lead to a deterioration of the accuracy of
the model, due to sparseness of data.
In this paper we extend previous results, where
only a few specific parsing strategies were consid-
ered in isolation, and provide some general char-
acterization of parsing strategies that can be prob-
abilistically extended. Our main contribution can
be stated as follows.
</bodyText>
<listItem confidence="0.992977285714286">
• We define a theoretical framework to relate the
parameter space defined by a CFG and that de-
fined by a PDA constructed from the CFG by a
particular parsing strategy.
• We provide a necessary condition and a suffi-
cient condition for the probabilistic extension
of parsing strategies.
</listItem>
<bodyText confidence="0.999856375">
We use the above findings to establish new results
about probabilistic extensions of parsing strategies
that are used in standard practice in computational
linguistics, as well as to provide simpler proofs of
already known results.
We introduce our framework in Section 3 and re-
port our main results in Sections 4 and 5. We discuss
applications of our results in Section 6.
</bodyText>
<sectionHeader confidence="0.988991" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.9870203">
In this paper we assume some familiarity with def-
initions of (P)CFGs and (P)PDAs. We refer the
reader to standard textbooks and publications as for
instance (Harrison, 1978; Booth and Thompson,
1973; Santos, 1972).
A CFG G is a tuple (E, N, 5, R), with E and N
the sets of terminals and nonterminals, respectively,
5 the start symbol and R the set of rules. In this
paper we only consider left-most derivations, repre-
sented as strings d ∈ R* and simply called deriva-
</bodyText>
<figure confidence="0.96829505882353">
Pr(axcbxd) _
Pr(axdbxc)
Pr(A→aC)·Pr(B→bD) _
Pr(A→aD)·Pr(B→bC)
1
1 ·
3
3
2
_
2 ·
1
4
3
3
Pr(axcbxd) _
Pr(axdbxc)
</figure>
<bodyText confidence="0.998066095238095">
tions. For α, β E (E U N)∗, we write α =�-d β with
the usual meaning. If α = S and β = w E E∗, we
call d a complete derivation of w. We say a CFG is
reduced if each rule in R occurs in some complete
derivation.
A PCFG is a pair (!9, p) consisting of a CFG !9
and a probability function p from R to real num-
bers in the interval [0, 1]. A PCFG is proper
if Eπ=(A→α)∈R p(π) = 1 for each A E N.
The probability of a (left-most) derivation d =
π1 • • • πm, πi E R for 1 G i G m, is p(d) =
Hmi=1 p(πi). The probability of a string w E E∗
is p(w) = ES⇒dw p(d). A PCFG is consistent if
Ew∈Σ* p(w) = 1. A PCFG (!9, p) is reduced if !9 is
reduced.
In this paper we will mainly consider push-down
transducers rather than push-down automata. Push-
down transducers not only compute derivations of
the grammar while processing an input string, but
they also explicitly produce output strings from
which these derivations can be obtained. We use
transducers for two reasons. First, constraints on
the output strings allow us to restrict our attention
to “reasonable” parsing strategies. Those strategies
that cannot be formalized within these constraints
are unlikely to be of practical interest. Secondly,
mappings from input strings to derivations, such as
those realized by push-down transducers, turn out
to be a very powerful abstraction and allow direct
proofs of several general results.
Contrary to many textbooks, our push-down de-
vices do not possess states next to stack symbols.
This is without loss of generality, since states can
be encoded into the stack symbols, given the types
of transitions that we allow. Thus, a PDT A is a
6-tuple (E, E, Q, Xin, Xfin, Δ), with E and
E the input and output alphabets, respectively, Q
the set of stack symbols, including the initial and fi-
nal stack symbols Xin and Xfin, respectively, and
Δ the set of transitions. Each transition has one
of the following three forms: X H XY , called a
push transition, YX H Z, called a pop transition,
</bodyText>
<equation confidence="0.944455666666667">
or X x,y
�� Y , called a swap transition; here X, Y ,
Z E Q, x E E U {ε} is the input read by the tran-
</equation>
<bodyText confidence="0.999214304347826">
sition and y E E∗ is the written output. Note that
in our notation, stacks grow from left to right, i.e.,
the top-most stack symbol will be found at the right
end. A configuration of a PDT is a triple (α, w, v),
where α E Q∗ is a stack, w E E∗ is the remain-
ing input, and v E E∗ is the output generated so
far. Computations are represented as strings c E
Δ∗. For configurations (α, w, v) and (β, w0, v0), we
write (α, w, v) �-c (β, w0, v0) with the usual mean-
ing, and write (α, w, v) �-∗ (β, w0, v0) when c is of
no importance. If (Xin, w, ε) �-c (Xfin, ε, v), then
c is a complete computation of w, and the output
string v is denoted out(c). A PDT is reduced if
each transition in Δ occurs in some complete com-
putation.
Without loss of generality, we assume that com-
binations of different types of transitions are not al-
lowed for a given stack symbol. More precisely,
for each stack symbol X =� Xfin, the PDA can
only take transitions of a single type (push, pop or
swap). A PDT can easily be brought in this form
by introducing for each X three new stack symbols
Xpush, Xpop and Xswap and new swap transitions
</bodyText>
<equation confidence="0.577815">
X ε,ε
�� Xpush, X ε,ε
�� Xpop and X ε,ε
</equation>
<bodyText confidence="0.9937343">
�� Xswap. In
each existing transition that operates on top-of-stack
X, we then replace X by one from Xpush, Xpop or
Xswap, depending on the type of that transition. We
also assume that Xfin does not occur in the left-
hand side of a transition, again without loss of gen-
erality.
A PPDT is a pair (A, p) consisting of a PDT A
and a probability function p from Δ to real numbers
in the interval [0, 1]. A PPDT is proper if
</bodyText>
<listItem confidence="0.99886625">
• Eτ=(X7→XY )∈Δ p(T) = 1 for each X E Q
such that there is at least one transition X H
XY,YEQ;
• Eτ=(Xx,y
</listItem>
<equation confidence="0.66591">
7→Y )∈Δ p(T) = 1 for each X E Q such
that there is at least one transition X x,y
�� Y ,
x E E U {ε},y E E∗,Y E Q; and
</equation>
<listItem confidence="0.878613333333333">
• Eτ=(Y X7→Z)∈Δ p(T) = 1, for each X, Y E Q
such that there is at least one transition Y X H
Z,ZEQ.
</listItem>
<bodyText confidence="0.997676833333333">
The probability of a computation c = T1 • • • Tm,
Ti E Δ for 1 G i G m, is p(c) =
Hz ` 1 p(Ti). The probability of a string w is p(w) =
E(Xin,w,ε)`c(Xfin,ε,v) p(c). A PPDT is consistent
if Ew∈Σ* p(w) = 1. A PPDT (A, p) is reduced if
A is reduced.
</bodyText>
<sectionHeader confidence="0.971944" genericHeader="method">
3 Parsing Strategies
</sectionHeader>
<bodyText confidence="0.9999822">
The term “parsing strategy” is often used informally
to refer to a class of parsing algorithms that behave
similarly in some way. In this paper, we assign a
formal meaning to this term, relying on the obser-
vation by (Lang, 1974) and (Billot and Lang, 1989)
that many parsing algorithms for CFGs can be de-
scribed in two steps. The first is a construction of
push-down devices from CFGs, and the second is
a method for handling nondeterminism (e.g. back-
tracking or dynamic programming). Parsing algo-
rithms that handle nondeterminism in different ways
but apply the same construction of push-down de-
vices from CFGs are seen as realizations of the same
parsing strategy.
Thus, we define a parsing strategy to be a func-
tion S that maps a reduced CFG 9 = (Σ, N, S,
R) to a pair S(9) = (A, f) consisting of a reduced
PDT A = (Σ, Σ, Q, Xin, Xfin, Δ), and a func-
tion f that maps a subset of Σ* to a subset of R*,
with the following properties:
</bodyText>
<listItem confidence="0.9807464">
• R C Σ.
• For each string w E Σ* and each complete
computation c on w, f(out(c)) = d is a (left-
most) derivation of w. Furthermore, each sym-
bol from R occurs as often in out(c) as it oc-
curs in d.
• Conversely, for each string w E Σ* and
each derivation d of w, there is precisely
one complete computation c on w such that
f(out(c)) = d.
</listItem>
<bodyText confidence="0.990501666666667">
If c is a complete computation, we will write f(c)
to denote f(out(c)). The conditions above then im-
ply that f is a bijection from complete computations
to complete derivations. Note that output strings of
(complete) computations may contain symbols that
are not in R, and the symbols that are in R may
occur in a different order in v than in f(v) = d.
The purpose of the symbols in Σ − R is to help
this process of reordering of symbols from R in v,
as needed for instance in the case of the left-corner
parsing strategy (see (Nijholt, 1980, pp. 22–23) for
discussion).
A probabilistic parsing strategy is defined to
be a function S that maps a reduced, proper and
consistent PCFG (9, pg) to a triple S(9, pg) =
(A, pA, f), where (A, pA) is a reduced, proper and
consistent PPDT, with the same properties as a
(non-probabilistic) parsing strategy, and in addition:
</bodyText>
<listItem confidence="0.932974">
• For each complete derivation d and each com-
plete computation c such that f(c) = d, pg(d)
equals pA(c).
</listItem>
<bodyText confidence="0.999891090909091">
In other words, a complete computation has the
same probability as the complete derivation that it
is mapped to by function f. An implication of
this property is that for each string w E Σ*, the
probabilities assigned to that string by (9, pg) and
(A, pA) are equal.
We say that probabilistic parsing strategy S&apos; is an
extension of parsing strategy S if for each reduced
CFG 9 and probability function pg we have S(9) =
(A, f) if and only if S&apos;(9, pg) = (A, pA, f) for
some pA.
</bodyText>
<sectionHeader confidence="0.996898" genericHeader="method">
4 Correct-Prefix Property
</sectionHeader>
<bodyText confidence="0.991537866666667">
In this section we present a necessary condition for
the probabilistic extension of a parsing strategy. For
a given PDT, we say a computation c is dead if
(Xin, w1, ε) �-c (α, ε, v1), for some α E Q*, w1 E
Σ* and v1 E Σ*, and there are no w2 E Σ* and
v2 E Σ* such that (α, w2, ε) �_* (Xfin, ε, v2). In-
formally, a dead computation is a computation that
cannot be continued to become a complete compu-
tation. We say that a PDT has the correct-prefix
property (CPP) if it does not allow any dead com-
putations. We also say that a parsing strategy has
the CPP if it maps each reduced CFG to a PDT that
has the CPP.
Lemma 1 For each reduced CFG 9, there is a
probability function pg such that PCFG (9, pg) is
proper and consistent, and pg(d) &gt; 0 for all com-
plete derivations d.
Proof. Since 9 is reduced, there is a finite set D
consisting of complete derivations d, such that for
each rule π in 9 there is at least one d E D in
which π occurs. Let nπ,d be the number of occur-
rences of rule π in derivation d E D, and let nπ be
EdED nπ,d, the total number of occurrences of π in
D. Let nA be the sum of nπ for all rules π with A in
the left-hand side. A probability function pg can be
defined through “maximum-likelihood estimation”
such that pg(π) = n�
n� for each rule π = A → α.
For all nonterminals A, Eπ=A→α pg(π) =
Eπ=A→α n�
</bodyText>
<equation confidence="0.828113">
n� = n�
n� = 1, which means that the
PCFG (9,pg) is proper. Furthermore, it has been
</equation>
<bodyText confidence="0.975213765957447">
shown in (Chi and Geman, 1998; S´anchez and
Benedi, 1997) that a PCFG (9,pg) is consistent if
pg was obtained by maximum-likelihood estimation
using a set of derivations. Finally, since nπ &gt; 0 for
each π, also pg(π) &gt; 0 for each π, and pg(d) &gt; 0
for all complete derivations d.
We say a computation is a shortest dead compu-
tation if it is dead and none of its proper prefixes is
dead. Note that each dead computation has a unique
prefix that is a shortest dead computation. For a
PDT A, let TA be the union of the set of all com-
plete computations and the set of all shortest dead
computations.
Lemma 2 For each proper PPDT (A, pA),
EcETA pA(c) G 1.
Proof. The proof is a trivial variant of the proof
that for a proper PCFG (9, pg), the sum of pg(d) for
all derivations d cannot exceed 1, which is shown by
(Booth and Thompson, 1973).
From this, the main result of this section follows.
Theorem 3 A parsing strategy that lacks the CPP
cannot be extended to become a probabilistic pars-
ing strategy.
Proof. Take a parsing strategy S that does not have
the CPP. Then there is a reduced CFG G = (Σ, N,
S, R), with S(G) = (A, f) for some A and f, and
a shortest dead computation c allowed by A.
It follows from Lemma 1 that there is a proba-
bility function pG such that (G, pG) is a proper and
consistent PCFG and pG(d) &gt; 0 for all complete
derivations d. Assume we also have a probability
function pA such that (A, pA) is a proper and con-
sistent PPDT and pA(c0) = pG(f(c0)) for each com-
plete computation c0. Since A is reduced, each tran-
sition τ must occur in some complete computation
c0. Furthermore, for each complete computation c0
there is a complete derivation d such that f(c0) = d,
and pA(c0) = pG(d) &gt; 0. Therefore, pA(τ) &gt; 0
for each transition τ, and pA(c) &gt; 0, where c is the
above-mentioned dead computation.
Due to Lemma 2, 1 ≥ Σc0∈TA pA(c0) ≥
Σw∈Σ∗ pA(w) + pA(c) &gt; Σw∈Σ∗ pA(w) =
Σw∈Σ∗ pG(w). This is in contradiction with the con-
sistency of (G, pG). Hence, a probability function
pA with the properties we required above cannot ex-
ist, and therefore S cannot be extended to become a
probabilistic parsing strategy.
</bodyText>
<sectionHeader confidence="0.990711" genericHeader="method">
5 Strong Predictiveness
</sectionHeader>
<bodyText confidence="0.998072018518519">
In this section we present our main result, which is a
sufficient condition allowing the probabilistic exten-
sion of a parsing strategy. We start with a technical
result that was proven in (Abney et al., 1999; Chi,
1999; Nederhof and Satta, 2003).
Lemma 4 Given a non-proper PCFG (G, pG), G =
(Σ, N, S, R), there is a probability function p0G such
that PCFG (G, p0G) is proper and, for every com-
plete derivation d, p0 G(d) = 1C · pG(d), where C =
rS⇒d0w,w∈Σ∗ pG(d0).
Note that if PCFG (G,pG) in the above lemma is
consistent, then C = 1 and (G, p0G) and (G, pG) de-
fine the same distribution on derivations. The nor-
malization procedure underlying Lemma 4 makes
use of quantities EA⇒dw,w∈Σ∗ pG(d) for each A ∈
N. These quantities can be computed to any degree
of precision, as discussed for instance in (Booth and
Thompson, 1973) and (Stolcke, 1995). Thus nor-
malization of a PCFG can be effectively computed.
For a fixed PDT, we define the binary relation --*
on stack symbols by: Y --* Y 0 if and only if
(Y, w, ε) `∗ (Y 0, ε, v) for some w ∈ Σ∗ and
v ∈ Σ∗. In words, some subcomputation of the
PDT may start with stack Y and end with stack Y 0.
Note that all stacks that occur in such a subcompu-
tation must have height of 1 or more. We say that a
(P)PDA or a (P)PDT has the strong predictiveness
property (SPP) if the existence of three transitions
X 7→ XY , XY1 7→ Z1 and XY2 7→ Z2 such that
Y --* Y1 and Y --* Y2 implies Z1 = Z2. Infor-
mally, this means that when a subcomputation starts
with some stack α and some push transition τ, then
solely on the basis of τ we can uniquely determine
what stack symbol Z1 = Z2 will be on top of the
stack in the firstly reached configuration with stack
height equal to |α|. Another way of looking at it is
that no information may flow from higher stack el-
ements to lower stack elements that was not already
predicted before these higher stack elements came
into being, hence the term “strong predictiveness”.
We say that a parsing strategy has the SPP if it maps
each reduced CFG to a PDT with the SPP.
Theorem 5 Any parsing strategy that has the CPP
and the SPP can be extended to become a proba-
bilistic parsing strategy.
Proof. Consider a parsing strategy S that has the
CPP and the SPP, and a proper, consistent and re-
duced PCFG (G, pG), G = (Σ, N, S, R). Let
S(G) = (A, f), A = (Σ, Σ, Q, Xin, Xfin, Δ).
We will show that there is a probability function pA
such that (A, pA) is a proper and consistent PPDT,
and pA(c) = pG(f(c)) for all complete computa-
tions c.
We first construct a PPDT (A, p0A) as follows.
</bodyText>
<equation confidence="0.650026333333333">
For each scan transition τ = X x,y
7→ Y in Δ, let
p0A(τ) = pG(y) in case y ∈ R, and p0A(τ) = 1
</equation>
<bodyText confidence="0.90806825">
otherwise. For all remaining transitions τ ∈ Δ, let
p0A(τ) = 1. Note that (A, p0A) may be non-proper.
Still, from the definition of f it follows that, for each
complete computation c, we have
</bodyText>
<equation confidence="0.988185">
p0A(c) = pG(f(c)), (1)
</equation>
<bodyText confidence="0.9898632">
and so our PPDT is consistent.
We now map (A, p0A) to a language-equivalent
PCFG (G0, pG0), G0 = (Σ, Q, Xin, R0), where R0
contains the following rules with the specified asso-
ciated probabilities:
</bodyText>
<listItem confidence="0.998804">
• X → YZ with pG0(X → YZ) = p0A(X 7→
XY ), for each X 7→ XY ∈ Δ with Z the
unique stack symbol such that there is at least
one transition XY 0 7→ Z with Y --* Y 0;
• X → xY with pG0(X → xY ) = p0A(X x
</listItem>
<page confidence="0.541372">
7→
</page>
<bodyText confidence="0.494897">
Y ), for each transition X x
</bodyText>
<listItem confidence="0.863001">
7→ Y ∈ Δ;
• Y → ε with pG0(X → ε) = 1, for each stack
symbol Y such that there is at least one transi-
tion XY H Z E Δ or such that Y = Xfin.
</listItem>
<bodyText confidence="0.997828333333333">
It is not difficult to see that there exists a bijection
f0 from complete computations of A to complete
derivations of g0, and that we have
</bodyText>
<equation confidence="0.999035">
pG0(f0(c)) = p0A(c), (2)
</equation>
<bodyText confidence="0.99780225">
for each complete computation c. Thus (g0, pG0)
is consistent. However, note that (g0,pG0) is not
proper.
By Lemma 4, we can construct a new PCFG
(g0, p0G0) that is proper and consistent, and such that
pG0(d) = p0G0(d), for each complete derivation d of
g0. Thus, for each complete computation c of A, we
have
</bodyText>
<equation confidence="0.99826">
p0G0(f0(c)) = pG0(f0(c)). (3)
</equation>
<bodyText confidence="0.999790125">
We now transfer back the probabilities of rules of
(g0, p0G0) to the transitions of A. Formally, we define
a new probability function pA such that, for each
τ E Δ, pA(τ) = p0 G0(π), where π is the rule in R0
that has been constructed from τ as specified above.
It is easy to see that PPDT (A, pA) is now proper.
Furthermore, for each complete computation c of A
we have
</bodyText>
<equation confidence="0.994211">
pA(c) = p0G0(f0(c)), (4)
</equation>
<bodyText confidence="0.99923746875">
and so (A, pA) is also consistent. By combining
equations (1) to (4) we conclude that, for each com-
plete computation c of A, pA(c) = p0G0(f0(c)) =
pG0(f0(c)) = p0A(c) = pG(f(c)). Thus our parsing
strategy S can be probabilistically extended.
Note that the construction in the proof above can
be effectively computed (see discussion in Section 4
for effective computation of normalized PCFGs).
The definition of p0A in the proof of Theorem 5
relies on the strings output by A. This is the main
reason why we needed to consider PDTs rather
than PDAs. Now assume an appropriate probabil-
ity function pA has been computed, such that the
source PCFG and (A, pA) define equivalent dis-
tributions on derivations/computations. Then the
probabilities assigned to strings over the input al-
phabet are also equal. We may subsequently ignore
the output strings if the application at hand merely
requires probabilistic recognition rather than proba-
bilistic transduction, or in other words, we may sim-
plify PDTs to PDAs.
The proof of Theorem 5 also leads to the obser-
vation that parsing strategies with the CPP and the
SPP as well as their probabilistic extensions can be
described as grammar transformations, as follows.
A given (P)CFG is mapped to an equivalent (P)PDT
by a (probabilistic) parsing strategy. By ignoring
the output components of swap transitions we ob-
tain a (P)PDA, which can be mapped to an equiva-
lent (P)CFG as shown above. This observation gives
rise to an extension with probabilities of the work on
covers by (Nijholt, 1980; Leermakers, 1989).
</bodyText>
<sectionHeader confidence="0.988958" genericHeader="method">
6 Applications
</sectionHeader>
<bodyText confidence="0.999962962264151">
Many well-known parsing strategies with the CPP
also have the SPP. This is for instance the case
for top-down parsing and left-corner parsing. As
discussed in the introduction, it has already been
shown that for any PCFG g, there are equiva-
lent PPDTs implementing these strategies, as re-
ported in (Abney et al., 1999) and (Tendeau, 1995),
respectively. Those results more simply follow
now from our general characterization. Further-
more, PLR parsing (Soisalon-Soininen and Ukko-
nen, 1979; Nederhof, 1994) can be expressed in our
framework as a parsing strategy with the CPP and
the SPP, and thus we obtain as a new result that this
strategy allows probabilistic extension.
The above strategies are in contrast to the LR
parsing strategy, which has the CPP but lacks the
SPP, and therefore falls outside our sufficient condi-
tion. As we have already seen in the introduction, it
turns out that LR parsing cannot be extended to be-
come a probabilistic parsing strategy. Related to LR
parsing is ELR parsing (Purdom and Brown, 1981;
Nederhof, 1994), which also lacks the SPP. By an
argument similar to the one provided for LR, we can
show that also ELR parsing cannot be extended to
become a probabilistic parsing strategy. (See (Ten-
deau, 1997) for earlier observations related to this.)
These two cases might suggest that the sufficient
condition in Theorem 5 is tight in practice.
Decidability of the CPP and the SPP obviously
depends on how a parsing strategy is specified. As
far as we know, in all practical cases of parsing
strategies these properties can be easily decided.
Also, observe that our results do not depend on the
general behaviour of a parsing strategy S, but just
on its “point-wise” behaviour on each input CFG.
Specifically, if S does not have the CPP and the
SPP, but for some fixed CFG g of interest we ob-
tain a PDT A that has the CPP and the SPP, then
we can still apply the construction in Theorem 5.
In this way, any probability function pG associated
with g can be converted into a probability function
pA, such that the resulting PCFG and PPDT induce
equivalent distributions. We point out that decid-
ability of the CPP and the SPP for a fixed PDT can
be efficiently decided using dynamic programming.
One more consequence of our results is this. As
discussed in the introduction, the properness condi-
tion reduces the number of parameters of a PPDT.
However, our results show that if the PPDT has the
CPP and the SPP then the properness assumption is
not restrictive, i.e., by lifting properness we do not
gain new distributions with respect to those induced
by the underlying PCFG.
</bodyText>
<sectionHeader confidence="0.999086" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999955">
We have formalized the notion of CFG parsing strat-
egy as a mapping from CFGs to PDTs, and have in-
vestigated the extension to probabilities. We have
shown that the question of which parsing strategies
can be extended to become probabilistic heavily re-
lies on two properties, the correct-prefix property
and the strong predictiveness property. As far as we
know, this is the first general characterization that
has been provided in the literature for probabilistic
extension of CFG parsing strategies. We have also
shown that there is at least one strategy of practical
interest with the CPP but without the SPP, namely
LR parsing, that cannot be extended to become a
probabilistic parsing strategy.
</bodyText>
<sectionHeader confidence="0.994728" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999923333333333">
The first author is supported by the PIO-
NIER Project Algorithms for Linguistic Process-
ing, funded by NWO (Dutch Organization for
Scientific Research). The second author is par-
tially supported by MIUR under project PRIN No.
2003091149 005.
</bodyText>
<sectionHeader confidence="0.996256" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997715422764228">
S. Abney, D. McAllester, and F. Pereira. 1999. Re-
lating probabilistic grammars and automata. In
37th Annual Meeting of the Association for Com-
putational Linguistics, Proceedings of the Con-
ference, pages 542–549, Maryland, USA, June.
A.V. Aho and J.D. Ullman. 1972. Parsing, vol-
ume 1 of The Theory ofParsing, Translation and
Compiling. Prentice-Hall.
S. Billot and B. Lang. 1989. The structure of
shared forests in ambiguous parsing. In 27th
Annual Meeting of the Association for Com-
putational Linguistics, Proceedings of the Con-
ference, pages 143–151, Vancouver, British
Columbia, Canada, June.
T.L. Booth and R.A. Thompson. 1973. Apply-
ing probabilistic measures to abstract languages.
IEEE Transactions on Computers, C-22(5):442–
450, May.
T. Briscoe and J. Carroll. 1993. Generalized prob-
abilistic LR parsing of natural language (cor-
pora) with unification-based grammars. Compu-
tational Linguistics, 19(1):25–59.
E. Charniak and G. Carroll. 1994. Context-
sensitive statistics for improved grammatical lan-
guage models. In Proceedings Twelfth National
Conference on Artificial Intelligence, volume 1,
pages 728–733, Seattle, Washington.
Z. Chi and S. Geman. 1998. Estimation of prob-
abilistic context-free grammars. Computational
Linguistics, 24(2):299–305.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguis-
tics, 25(1):131–160.
M.V. Chitrao and R. Grishman. 1990. Statistical
parsing of messages. In Speech and Natural Lan-
guage, Proceedings, pages 263–266, Hidden Val-
ley, Pennsylvania, June.
M.A. Harrison. 1978. Introduction to Formal Lan-
guage Theory. Addison-Wesley.
K. Inui, V. Sornlertlamvanich, H. Tanaka, and
T. Tokunaga. 2000. Probabilistic GLR parsing.
In H. Bunt and A. Nijholt, editors, Advances
in Probabilistic and other Parsing Technologies,
chapter 5, pages 85–104. Kluwer Academic Pub-
lishers.
B. Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In Automata,
Languages and Programming, 2nd Colloquium,
volume 14 of Lecture Notes in Computer Science,
pages 255–269, Saarbr¨ucken. Springer-Verlag.
R. Leermakers. 1989. How to cover a grammar.
In 27th Annual Meeting of the Association for
Computational Linguistics, Proceedings of the
Conference, pages 135–142, Vancouver, British
Columbia, Canada, June.
C.D. Manning and B. Carpenter. 2000. Proba-
bilistic parsing using left corner language mod-
els. In H. Bunt and A. Nijholt, editors, Ad-
vances in Probabilistic and other Parsing Tech-
nologies, chapter 6, pages 105–124. Kluwer Aca-
demic Publishers.
M.-J. Nederhof and G. Satta. 2003. Probabilis-
tic parsing as intersection. In 8th International
Workshop on Parsing Technologies, pages 137–
148, LORIA, Nancy, France, April.
M.-J. Nederhof. 1994. An optimal tabular parsing
algorithm. In 32nd Annual Meeting of the Associ-
ation for Computational Linguistics, Proceedings
of the Conference, pages 117–124, Las Cruces,
New Mexico, USA, June.
A. Nijholt. 1980. Context-Free Grammars: Cov-
ers, Normal Forms, and Parsing, volume 93 of
Lecture Notes in Computer Science. Springer-
Verlag.
P.W. Purdom, Jr. and C.A. Brown. 1981. Pars-
ing extended LR(k) grammars. Acta Informatica,
15:115–127.
B. Roark and M. Johnson. 1999. Efficient proba-
bilistic top-down and left-corner parsing. In 37th
Annual Meeting of the Association for Compu-
tational Linguistics, Proceedings of the Confer-
ence, pages 421–428, Maryland, USA, June.
D.J. Rosenkrantz and P.M. Lewis II. 1970. Deter-
ministic left corner parsing. In IEEE Conference
Record of the 11th Annual Symposium on Switch-
ing and Automata Theory, pages 139–152.
J.-A. S´anchez and J.-M. Benedi. 1997. Consis-
tency of stochastic context-free grammars from
probabilistic estimation based on growth trans-
formations. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 19(9):1052–1055,
September.
E.S. Santos. 1972. Probabilistic grammars and au-
tomata. Information and Control, 21:27–47.
S. Sippu and E. Soisalon-Soininen. 1990. Parsing
Theory, Vol. II: LR(k) and LL(k) Parsing, vol-
ume 20 of EATCS Monographs on Theoretical
Computer Science. Springer-Verlag.
E. Soisalon-Soininen and E. Ukkonen. 1979. A
method for transforming grammars into LL(k)
form. Acta Informatica, 12:339–369.
V. Sornlertlamvanich, K. Inui, H. Tanaka, T. Toku-
naga, and T. Takezawa. 1999. Empirical sup-
port for new probabilistic generalized LR pars-
ing. Journal of Natural Language Processing,
6(3):3–22.
A. Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2):167–201.
F. Tendeau. 1995. Stochastic parse-tree recognition
by a pushdown automaton. In Fourth Interna-
tional Workshop on Parsing Technologies, pages
234–249, Prague and Karlovy Vary, Czech Re-
public, September.
F. Tendeau. 1997. Analyse syntaxique et
s´emantique avec ´evaluation d’attributs dans
un demi-anneau. Ph.D. thesis, University of
Orl´eans.
J.H. Wright and E.N. Wrigley. 1991. GLR pars-
ing with probability. In M. Tomita, editor, Gen-
eralized LR Parsing, chapter 8, pages 113–128.
Kluwer Academic Publishers.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.841466">
<title confidence="0.999985">Probabilistic Parsing Strategies</title>
<author confidence="0.986059">Mark-Jan Nederhof</author>
<affiliation confidence="0.998941">Faculty of Arts University of Groningen</affiliation>
<address confidence="0.976752333333333">P.O. Box 716 NL-9700 AS Groningen The Netherlands</address>
<email confidence="0.989973">markjan@let.rug.nl</email>
<author confidence="0.999152">Giorgio Satta</author>
<affiliation confidence="0.999687">Dept. of Information Engineering University of Padua</affiliation>
<address confidence="0.977764666666667">via Gradenigo, 6/A I-35131 Padova Italy</address>
<email confidence="0.998894">satta@dei.unipd.it</email>
<abstract confidence="0.998140375">We present new results on the relation between context-free parsing strategies and their probabilistic counter-parts. We provide a necessary condition and a sufficient condition for the probabilistic extension of parsing strategies. These results generalize existing results in the literature that were obtained by considering parsing strategies in isolation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
<author>D McAllester</author>
<author>F Pereira</author>
</authors>
<title>Relating probabilistic grammars and automata.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>542--549</pages>
<location>Maryland, USA,</location>
<contexts>
<context position="3767" citStr="Abney et al., 1999" startWordPosition="571" endWordPosition="574">he question arises of whether parsing strategies can be extended probabilistically, i.e., whether a given construction of PDAs from CFGs can be “augmented” with a function defining the probabilities for the target PDA, given the probabilities associated with the input CFG, in such a way that the obtained probabilistic distributions on the CFG derivations and the corresponding PDA computations are equivalent. Some first results on this issue have been presented by (Tendeau, 1995), who shows that the already mentioned left-corner parsing strategy can be extended probabilistically, and later by (Abney et al., 1999) who show that the pure top-down parsing strategy and a specific type of shift-reduce parsing strategy can be probabilistically extended. One might think that any “practical” parsing strategy can be probabilistically extended, but this turns out not to be the case. We briefly discuss here a counter-example, in order to motivate the approach we have taken in this paper. Probabilistic LR parsing has been investigated in the literature (Wright and Wrigley, 1991; Briscoe and Carroll, 1993; Inui et al., 2000) under the assumption that it would allow more fine-grained probability distributions than </context>
<context position="19678" citStr="Abney et al., 1999" startWordPosition="3606" endWordPosition="3609">ransition τ, and pA(c) &gt; 0, where c is the above-mentioned dead computation. Due to Lemma 2, 1 ≥ Σc0∈TA pA(c0) ≥ Σw∈Σ∗ pA(w) + pA(c) &gt; Σw∈Σ∗ pA(w) = Σw∈Σ∗ pG(w). This is in contradiction with the consistency of (G, pG). Hence, a probability function pA with the properties we required above cannot exist, and therefore S cannot be extended to become a probabilistic parsing strategy. 5 Strong Predictiveness In this section we present our main result, which is a sufficient condition allowing the probabilistic extension of a parsing strategy. We start with a technical result that was proven in (Abney et al., 1999; Chi, 1999; Nederhof and Satta, 2003). Lemma 4 Given a non-proper PCFG (G, pG), G = (Σ, N, S, R), there is a probability function p0G such that PCFG (G, p0G) is proper and, for every complete derivation d, p0 G(d) = 1C · pG(d), where C = rS⇒d0w,w∈Σ∗ pG(d0). Note that if PCFG (G,pG) in the above lemma is consistent, then C = 1 and (G, p0G) and (G, pG) define the same distribution on derivations. The normalization procedure underlying Lemma 4 makes use of quantities EA⇒dw,w∈Σ∗ pG(d) for each A ∈ N. These quantities can be computed to any degree of precision, as discussed for instance in (Booth </context>
<context position="25690" citStr="Abney et al., 1999" startWordPosition="4753" endWordPosition="4756">babilistic) parsing strategy. By ignoring the output components of swap transitions we obtain a (P)PDA, which can be mapped to an equivalent (P)CFG as shown above. This observation gives rise to an extension with probabilities of the work on covers by (Nijholt, 1980; Leermakers, 1989). 6 Applications Many well-known parsing strategies with the CPP also have the SPP. This is for instance the case for top-down parsing and left-corner parsing. As discussed in the introduction, it has already been shown that for any PCFG g, there are equivalent PPDTs implementing these strategies, as reported in (Abney et al., 1999) and (Tendeau, 1995), respectively. Those results more simply follow now from our general characterization. Furthermore, PLR parsing (Soisalon-Soininen and Ukkonen, 1979; Nederhof, 1994) can be expressed in our framework as a parsing strategy with the CPP and the SPP, and thus we obtain as a new result that this strategy allows probabilistic extension. The above strategies are in contrast to the LR parsing strategy, which has the CPP but lacks the SPP, and therefore falls outside our sufficient condition. As we have already seen in the introduction, it turns out that LR parsing cannot be exten</context>
</contexts>
<marker>Abney, McAllester, Pereira, 1999</marker>
<rawString>S. Abney, D. McAllester, and F. Pereira. 1999. Relating probabilistic grammars and automata. In 37th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages 542–549, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<date>1972</date>
<journal>Parsing,</journal>
<booktitle>of The Theory ofParsing, Translation and Compiling.</booktitle>
<volume>1</volume>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="1610" citStr="Aho and Ullman, 1972" startWordPosition="225" endWordPosition="228">One of the advantages of the use of PDAs is that these devices provide an operational specification that determines which steps must be performed when parsing an input string, something that is not offered by CFGs. In other words, PDAs can be associated to parsing strategies for contextfree languages. More precisely, parsing strategies are traditionally specified as constructions that map CFGs to language-equivalent PDAs. Popular examples of parsing strategies are the standard constructions of top-down PDAs (Harrison, 1978), leftcorner PDAs (Rosenkrantz and Lewis II, 1970), shift-reduce PDAs (Aho and Ullman, 1972) and LR PDAs (Sippu and Soisalon-Soininen, 1990). CFGs and PDAs have probabilistic counterparts, called probabilistic CFGs (PCFGs) and probabilistic PDAs (PPDAs). These models are very popular in natural language processing applications, where they are used to define a probability distribution function on the domain of all derivations for sentences in the language of interest. In PCFGs and PPDAs, probabilities are assigned to rules or transitions, respectively. However, these probabilities cannot be chosen entirely arbitrarily. For example, for a given nonterminal A in a PCFG, the sum of the p</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>A.V. Aho and J.D. Ullman. 1972. Parsing, volume 1 of The Theory ofParsing, Translation and Compiling. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Billot</author>
<author>B Lang</author>
</authors>
<title>The structure of shared forests in ambiguous parsing.</title>
<date>1989</date>
<booktitle>In 27th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>143--151</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="13407" citStr="Billot and Lang, 1989" startWordPosition="2390" endWordPosition="2393">Eτ=(Y X7→Z)∈Δ p(T) = 1, for each X, Y E Q such that there is at least one transition Y X H Z,ZEQ. The probability of a computation c = T1 • • • Tm, Ti E Δ for 1 G i G m, is p(c) = Hz ` 1 p(Ti). The probability of a string w is p(w) = E(Xin,w,ε)`c(Xfin,ε,v) p(c). A PPDT is consistent if Ew∈Σ* p(w) = 1. A PPDT (A, p) is reduced if A is reduced. 3 Parsing Strategies The term “parsing strategy” is often used informally to refer to a class of parsing algorithms that behave similarly in some way. In this paper, we assign a formal meaning to this term, relying on the observation by (Lang, 1974) and (Billot and Lang, 1989) that many parsing algorithms for CFGs can be described in two steps. The first is a construction of push-down devices from CFGs, and the second is a method for handling nondeterminism (e.g. backtracking or dynamic programming). Parsing algorithms that handle nondeterminism in different ways but apply the same construction of push-down devices from CFGs are seen as realizations of the same parsing strategy. Thus, we define a parsing strategy to be a function S that maps a reduced CFG 9 = (Σ, N, S, R) to a pair S(9) = (A, f) consisting of a reduced PDT A = (Σ, Σ, Q, Xin, Xfin, Δ), and a func</context>
</contexts>
<marker>Billot, Lang, 1989</marker>
<rawString>S. Billot and B. Lang. 1989. The structure of shared forests in ambiguous parsing. In 27th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages 143–151, Vancouver, British Columbia, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Booth</author>
<author>R A Thompson</author>
</authors>
<title>Applying probabilistic measures to abstract languages.</title>
<date>1973</date>
<journal>IEEE Transactions on Computers,</journal>
<volume>22</volume>
<issue>5</issue>
<pages>450</pages>
<contexts>
<context position="8523" citStr="Booth and Thompson, 1973" startWordPosition="1408" endWordPosition="1411">ic extension of parsing strategies. We use the above findings to establish new results about probabilistic extensions of parsing strategies that are used in standard practice in computational linguistics, as well as to provide simpler proofs of already known results. We introduce our framework in Section 3 and report our main results in Sections 4 and 5. We discuss applications of our results in Section 6. 2 Preliminaries In this paper we assume some familiarity with definitions of (P)CFGs and (P)PDAs. We refer the reader to standard textbooks and publications as for instance (Harrison, 1978; Booth and Thompson, 1973; Santos, 1972). A CFG G is a tuple (E, N, 5, R), with E and N the sets of terminals and nonterminals, respectively, 5 the start symbol and R the set of rules. In this paper we only consider left-most derivations, represented as strings d ∈ R* and simply called derivaPr(axcbxd) _ Pr(axdbxc) Pr(A→aC)·Pr(B→bD) _ Pr(A→aD)·Pr(B→bC) 1 1 · 3 3 2 _ 2 · 1 4 3 3 Pr(axcbxd) _ Pr(axdbxc) tions. For α, β E (E U N)∗, we write α =�-d β with the usual meaning. If α = S and β = w E E∗, we call d a complete derivation of w. We say a CFG is reduced if each rule in R occurs in some complete derivation. A PCFG is</context>
<context position="18152" citStr="Booth and Thompson, 1973" startWordPosition="3322" endWordPosition="3325">so pg(π) &gt; 0 for each π, and pg(d) &gt; 0 for all complete derivations d. We say a computation is a shortest dead computation if it is dead and none of its proper prefixes is dead. Note that each dead computation has a unique prefix that is a shortest dead computation. For a PDT A, let TA be the union of the set of all complete computations and the set of all shortest dead computations. Lemma 2 For each proper PPDT (A, pA), EcETA pA(c) G 1. Proof. The proof is a trivial variant of the proof that for a proper PCFG (9, pg), the sum of pg(d) for all derivations d cannot exceed 1, which is shown by (Booth and Thompson, 1973). From this, the main result of this section follows. Theorem 3 A parsing strategy that lacks the CPP cannot be extended to become a probabilistic parsing strategy. Proof. Take a parsing strategy S that does not have the CPP. Then there is a reduced CFG G = (Σ, N, S, R), with S(G) = (A, f) for some A and f, and a shortest dead computation c allowed by A. It follows from Lemma 1 that there is a probability function pG such that (G, pG) is a proper and consistent PCFG and pG(d) &gt; 0 for all complete derivations d. Assume we also have a probability function pA such that (A, pA) is a proper and co</context>
<context position="20297" citStr="Booth and Thompson, 1973" startWordPosition="3722" endWordPosition="3725">, 1999; Chi, 1999; Nederhof and Satta, 2003). Lemma 4 Given a non-proper PCFG (G, pG), G = (Σ, N, S, R), there is a probability function p0G such that PCFG (G, p0G) is proper and, for every complete derivation d, p0 G(d) = 1C · pG(d), where C = rS⇒d0w,w∈Σ∗ pG(d0). Note that if PCFG (G,pG) in the above lemma is consistent, then C = 1 and (G, p0G) and (G, pG) define the same distribution on derivations. The normalization procedure underlying Lemma 4 makes use of quantities EA⇒dw,w∈Σ∗ pG(d) for each A ∈ N. These quantities can be computed to any degree of precision, as discussed for instance in (Booth and Thompson, 1973) and (Stolcke, 1995). Thus normalization of a PCFG can be effectively computed. For a fixed PDT, we define the binary relation --* on stack symbols by: Y --* Y 0 if and only if (Y, w, ε) `∗ (Y 0, ε, v) for some w ∈ Σ∗ and v ∈ Σ∗. In words, some subcomputation of the PDT may start with stack Y and end with stack Y 0. Note that all stacks that occur in such a subcomputation must have height of 1 or more. We say that a (P)PDA or a (P)PDT has the strong predictiveness property (SPP) if the existence of three transitions X 7→ XY , XY1 7→ Z1 and XY2 7→ Z2 such that Y --* Y1 and Y --* Y2 implies Z1</context>
</contexts>
<marker>Booth, Thompson, 1973</marker>
<rawString>T.L. Booth and R.A. Thompson. 1973. Applying probabilistic measures to abstract languages. IEEE Transactions on Computers, C-22(5):442– 450, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="4256" citStr="Briscoe and Carroll, 1993" startWordPosition="648" endWordPosition="651">, who shows that the already mentioned left-corner parsing strategy can be extended probabilistically, and later by (Abney et al., 1999) who show that the pure top-down parsing strategy and a specific type of shift-reduce parsing strategy can be probabilistically extended. One might think that any “practical” parsing strategy can be probabilistically extended, but this turns out not to be the case. We briefly discuss here a counter-example, in order to motivate the approach we have taken in this paper. Probabilistic LR parsing has been investigated in the literature (Wright and Wrigley, 1991; Briscoe and Carroll, 1993; Inui et al., 2000) under the assumption that it would allow more fine-grained probability distributions than the underlying PCFGs. However, this is not the case in general. Consider a PCFG with rule/probability pairs: B → bC, 23 5 → AB, 1 B → bD, 1 A → aC, 1 3 3 C → xc, 1 A → aD, 2 D → xd, 1 3 There are two key transitions in the associated LR automaton, which represent shift actions over c and d (we denote LR states by their sets of kernel items and encode these states into stack symbols): Tc : {C → x • c,D → x • d} c 7→ {C → x • c,D → x • d} {C → xc •} Td : {C → x • c,D → x • d} d 7→ {C → </context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>T. Briscoe and J. Carroll. 1993. Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19(1):25–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>G Carroll</author>
</authors>
<title>Contextsensitive statistics for improved grammatical language models.</title>
<date>1994</date>
<booktitle>In Proceedings Twelfth National Conference on Artificial Intelligence,</booktitle>
<volume>1</volume>
<pages>728--733</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="3083" citStr="Charniak and Carroll, 1994" startWordPosition="462" endWordPosition="465">obability distributions than the underlying CFG, since the set of free parameters may differ between the CFG and the PDA, both quantitatively and qualitatively. For example, (Sornlertlamvanich et al., 1999) and (Roark and Johnson, 1999) have shown that a probability distribution that can be obtained by training the probabilities of a CFG on the basis of a corpus can be less accurate than the probability distribution obtained by training the probabilities of a PDA constructed by a particular parsing strategy, on the basis of the same corpus. Also the results from (Chitrao and Grishman, 1990), (Charniak and Carroll, 1994) and (Manning and Carpenter, 2000) could be seen in this light. The question arises of whether parsing strategies can be extended probabilistically, i.e., whether a given construction of PDAs from CFGs can be “augmented” with a function defining the probabilities for the target PDA, given the probabilities associated with the input CFG, in such a way that the obtained probabilistic distributions on the CFG derivations and the corresponding PDA computations are equivalent. Some first results on this issue have been presented by (Tendeau, 1995), who shows that the already mentioned left-corner p</context>
</contexts>
<marker>Charniak, Carroll, 1994</marker>
<rawString>E. Charniak and G. Carroll. 1994. Contextsensitive statistics for improved grammatical language models. In Proceedings Twelfth National Conference on Artificial Intelligence, volume 1, pages 728–733, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chi</author>
<author>S Geman</author>
</authors>
<title>Estimation of probabilistic context-free grammars.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="6425" citStr="Chi and Geman, 1998" startWordPosition="1061" endWordPosition="1064">e the same probability as that rule in the PCFG, and letting other transitions have probability 1. However, the properness condition for PDAs has been heavily exploited in parsing applications, in doing incremental left-to-right probability computation for beam search (Roark and Johnson, 1999; Manning and Carpenter, 2000), and more generally in integration with other linear probabilistic models. Furthermore, commonly used training algorithms for PCFGS/PPDAs always produce proper probability assignments, and many desired mathematical properties of these methods are based on such an assumption (Chi and Geman, 1998; S´anchez and Benedi, 1997). We may therefore discard non-proper probability assignments in the current study. However, such probability assignments are outside the reach of the usual training algorithms for PDAs, which always produce proper PDAs. Therefore, we may discard such assignments in the current study, which investigates aspects of the potential of training algorithms for CFGs and PDAs. What has been lacking in the literature is a theoretical framework to relate the parameter space of a CFG to that of a PDA constructed from the CFG by a particular parsing strategy, in terms of the se</context>
<context position="17349" citStr="Chi and Geman, 1998" startWordPosition="3166" endWordPosition="3169"> D consisting of complete derivations d, such that for each rule π in 9 there is at least one d E D in which π occurs. Let nπ,d be the number of occurrences of rule π in derivation d E D, and let nπ be EdED nπ,d, the total number of occurrences of π in D. Let nA be the sum of nπ for all rules π with A in the left-hand side. A probability function pg can be defined through “maximum-likelihood estimation” such that pg(π) = n� n� for each rule π = A → α. For all nonterminals A, Eπ=A→α pg(π) = Eπ=A→α n� n� = n� n� = 1, which means that the PCFG (9,pg) is proper. Furthermore, it has been shown in (Chi and Geman, 1998; S´anchez and Benedi, 1997) that a PCFG (9,pg) is consistent if pg was obtained by maximum-likelihood estimation using a set of derivations. Finally, since nπ &gt; 0 for each π, also pg(π) &gt; 0 for each π, and pg(d) &gt; 0 for all complete derivations d. We say a computation is a shortest dead computation if it is dead and none of its proper prefixes is dead. Note that each dead computation has a unique prefix that is a shortest dead computation. For a PDT A, let TA be the union of the set of all complete computations and the set of all shortest dead computations. Lemma 2 For each proper PPDT (A, pA</context>
</contexts>
<marker>Chi, Geman, 1998</marker>
<rawString>Z. Chi and S. Geman. 1998. Estimation of probabilistic context-free grammars. Computational Linguistics, 24(2):299–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chi</author>
</authors>
<title>Statistical properties of probabilistic context-free grammars.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="19689" citStr="Chi, 1999" startWordPosition="3610" endWordPosition="3611">c) &gt; 0, where c is the above-mentioned dead computation. Due to Lemma 2, 1 ≥ Σc0∈TA pA(c0) ≥ Σw∈Σ∗ pA(w) + pA(c) &gt; Σw∈Σ∗ pA(w) = Σw∈Σ∗ pG(w). This is in contradiction with the consistency of (G, pG). Hence, a probability function pA with the properties we required above cannot exist, and therefore S cannot be extended to become a probabilistic parsing strategy. 5 Strong Predictiveness In this section we present our main result, which is a sufficient condition allowing the probabilistic extension of a parsing strategy. We start with a technical result that was proven in (Abney et al., 1999; Chi, 1999; Nederhof and Satta, 2003). Lemma 4 Given a non-proper PCFG (G, pG), G = (Σ, N, S, R), there is a probability function p0G such that PCFG (G, p0G) is proper and, for every complete derivation d, p0 G(d) = 1C · pG(d), where C = rS⇒d0w,w∈Σ∗ pG(d0). Note that if PCFG (G,pG) in the above lemma is consistent, then C = 1 and (G, p0G) and (G, pG) define the same distribution on derivations. The normalization procedure underlying Lemma 4 makes use of quantities EA⇒dw,w∈Σ∗ pG(d) for each A ∈ N. These quantities can be computed to any degree of precision, as discussed for instance in (Booth and Thompso</context>
</contexts>
<marker>Chi, 1999</marker>
<rawString>Z. Chi. 1999. Statistical properties of probabilistic context-free grammars. Computational Linguistics, 25(1):131–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M V Chitrao</author>
<author>R Grishman</author>
</authors>
<title>Statistical parsing of messages.</title>
<date>1990</date>
<booktitle>In Speech and Natural Language, Proceedings,</booktitle>
<pages>263--266</pages>
<location>Hidden Valley, Pennsylvania,</location>
<contexts>
<context position="3053" citStr="Chitrao and Grishman, 1990" startWordPosition="458" endWordPosition="461">ted PDA may allow different probability distributions than the underlying CFG, since the set of free parameters may differ between the CFG and the PDA, both quantitatively and qualitatively. For example, (Sornlertlamvanich et al., 1999) and (Roark and Johnson, 1999) have shown that a probability distribution that can be obtained by training the probabilities of a CFG on the basis of a corpus can be less accurate than the probability distribution obtained by training the probabilities of a PDA constructed by a particular parsing strategy, on the basis of the same corpus. Also the results from (Chitrao and Grishman, 1990), (Charniak and Carroll, 1994) and (Manning and Carpenter, 2000) could be seen in this light. The question arises of whether parsing strategies can be extended probabilistically, i.e., whether a given construction of PDAs from CFGs can be “augmented” with a function defining the probabilities for the target PDA, given the probabilities associated with the input CFG, in such a way that the obtained probabilistic distributions on the CFG derivations and the corresponding PDA computations are equivalent. Some first results on this issue have been presented by (Tendeau, 1995), who shows that the a</context>
</contexts>
<marker>Chitrao, Grishman, 1990</marker>
<rawString>M.V. Chitrao and R. Grishman. 1990. Statistical parsing of messages. In Speech and Natural Language, Proceedings, pages 263–266, Hidden Valley, Pennsylvania, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Harrison</author>
</authors>
<title>Introduction to Formal Language Theory.</title>
<date>1978</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="1518" citStr="Harrison, 1978" startWordPosition="213" endWordPosition="214">erative capacity as CFGs are also adopted, as for instance push-down automata (PDAs). One of the advantages of the use of PDAs is that these devices provide an operational specification that determines which steps must be performed when parsing an input string, something that is not offered by CFGs. In other words, PDAs can be associated to parsing strategies for contextfree languages. More precisely, parsing strategies are traditionally specified as constructions that map CFGs to language-equivalent PDAs. Popular examples of parsing strategies are the standard constructions of top-down PDAs (Harrison, 1978), leftcorner PDAs (Rosenkrantz and Lewis II, 1970), shift-reduce PDAs (Aho and Ullman, 1972) and LR PDAs (Sippu and Soisalon-Soininen, 1990). CFGs and PDAs have probabilistic counterparts, called probabilistic CFGs (PCFGs) and probabilistic PDAs (PPDAs). These models are very popular in natural language processing applications, where they are used to define a probability distribution function on the domain of all derivations for sentences in the language of interest. In PCFGs and PPDAs, probabilities are assigned to rules or transitions, respectively. However, these probabilities cannot be cho</context>
<context position="8497" citStr="Harrison, 1978" startWordPosition="1406" endWordPosition="1407"> the probabilistic extension of parsing strategies. We use the above findings to establish new results about probabilistic extensions of parsing strategies that are used in standard practice in computational linguistics, as well as to provide simpler proofs of already known results. We introduce our framework in Section 3 and report our main results in Sections 4 and 5. We discuss applications of our results in Section 6. 2 Preliminaries In this paper we assume some familiarity with definitions of (P)CFGs and (P)PDAs. We refer the reader to standard textbooks and publications as for instance (Harrison, 1978; Booth and Thompson, 1973; Santos, 1972). A CFG G is a tuple (E, N, 5, R), with E and N the sets of terminals and nonterminals, respectively, 5 the start symbol and R the set of rules. In this paper we only consider left-most derivations, represented as strings d ∈ R* and simply called derivaPr(axcbxd) _ Pr(axdbxc) Pr(A→aC)·Pr(B→bD) _ Pr(A→aD)·Pr(B→bC) 1 1 · 3 3 2 _ 2 · 1 4 3 3 Pr(axcbxd) _ Pr(axdbxc) tions. For α, β E (E U N)∗, we write α =�-d β with the usual meaning. If α = S and β = w E E∗, we call d a complete derivation of w. We say a CFG is reduced if each rule in R occurs in some comp</context>
</contexts>
<marker>Harrison, 1978</marker>
<rawString>M.A. Harrison. 1978. Introduction to Formal Language Theory. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Inui</author>
<author>V Sornlertlamvanich</author>
<author>H Tanaka</author>
<author>T Tokunaga</author>
</authors>
<title>Probabilistic GLR parsing.</title>
<date>2000</date>
<booktitle>Advances in Probabilistic and other Parsing Technologies, chapter 5,</booktitle>
<pages>85--104</pages>
<editor>In H. Bunt and A. Nijholt, editors,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="4276" citStr="Inui et al., 2000" startWordPosition="652" endWordPosition="655">y mentioned left-corner parsing strategy can be extended probabilistically, and later by (Abney et al., 1999) who show that the pure top-down parsing strategy and a specific type of shift-reduce parsing strategy can be probabilistically extended. One might think that any “practical” parsing strategy can be probabilistically extended, but this turns out not to be the case. We briefly discuss here a counter-example, in order to motivate the approach we have taken in this paper. Probabilistic LR parsing has been investigated in the literature (Wright and Wrigley, 1991; Briscoe and Carroll, 1993; Inui et al., 2000) under the assumption that it would allow more fine-grained probability distributions than the underlying PCFGs. However, this is not the case in general. Consider a PCFG with rule/probability pairs: B → bC, 23 5 → AB, 1 B → bD, 1 A → aC, 1 3 3 C → xc, 1 A → aD, 2 D → xd, 1 3 There are two key transitions in the associated LR automaton, which represent shift actions over c and d (we denote LR states by their sets of kernel items and encode these states into stack symbols): Tc : {C → x • c,D → x • d} c 7→ {C → x • c,D → x • d} {C → xc •} Td : {C → x • c,D → x • d} d 7→ {C → x • c,D → x • d} {D </context>
</contexts>
<marker>Inui, Sornlertlamvanich, Tanaka, Tokunaga, 2000</marker>
<rawString>K. Inui, V. Sornlertlamvanich, H. Tanaka, and T. Tokunaga. 2000. Probabilistic GLR parsing. In H. Bunt and A. Nijholt, editors, Advances in Probabilistic and other Parsing Technologies, chapter 5, pages 85–104. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lang</author>
</authors>
<title>Deterministic techniques for efficient non-deterministic parsers.</title>
<date>1974</date>
<booktitle>In Automata, Languages and Programming, 2nd Colloquium,</booktitle>
<volume>14</volume>
<pages>255--269</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="13379" citStr="Lang, 1974" startWordPosition="2387" endWordPosition="2388">E∗,Y E Q; and • Eτ=(Y X7→Z)∈Δ p(T) = 1, for each X, Y E Q such that there is at least one transition Y X H Z,ZEQ. The probability of a computation c = T1 • • • Tm, Ti E Δ for 1 G i G m, is p(c) = Hz ` 1 p(Ti). The probability of a string w is p(w) = E(Xin,w,ε)`c(Xfin,ε,v) p(c). A PPDT is consistent if Ew∈Σ* p(w) = 1. A PPDT (A, p) is reduced if A is reduced. 3 Parsing Strategies The term “parsing strategy” is often used informally to refer to a class of parsing algorithms that behave similarly in some way. In this paper, we assign a formal meaning to this term, relying on the observation by (Lang, 1974) and (Billot and Lang, 1989) that many parsing algorithms for CFGs can be described in two steps. The first is a construction of push-down devices from CFGs, and the second is a method for handling nondeterminism (e.g. backtracking or dynamic programming). Parsing algorithms that handle nondeterminism in different ways but apply the same construction of push-down devices from CFGs are seen as realizations of the same parsing strategy. Thus, we define a parsing strategy to be a function S that maps a reduced CFG 9 = (Σ, N, S, R) to a pair S(9) = (A, f) consisting of a reduced PDT A = (Σ, Σ, </context>
</contexts>
<marker>Lang, 1974</marker>
<rawString>B. Lang. 1974. Deterministic techniques for efficient non-deterministic parsers. In Automata, Languages and Programming, 2nd Colloquium, volume 14 of Lecture Notes in Computer Science, pages 255–269, Saarbr¨ucken. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Leermakers</author>
</authors>
<title>How to cover a grammar.</title>
<date>1989</date>
<booktitle>In 27th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>135--142</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="25356" citStr="Leermakers, 1989" startWordPosition="4699" endWordPosition="4700">abilistic transduction, or in other words, we may simplify PDTs to PDAs. The proof of Theorem 5 also leads to the observation that parsing strategies with the CPP and the SPP as well as their probabilistic extensions can be described as grammar transformations, as follows. A given (P)CFG is mapped to an equivalent (P)PDT by a (probabilistic) parsing strategy. By ignoring the output components of swap transitions we obtain a (P)PDA, which can be mapped to an equivalent (P)CFG as shown above. This observation gives rise to an extension with probabilities of the work on covers by (Nijholt, 1980; Leermakers, 1989). 6 Applications Many well-known parsing strategies with the CPP also have the SPP. This is for instance the case for top-down parsing and left-corner parsing. As discussed in the introduction, it has already been shown that for any PCFG g, there are equivalent PPDTs implementing these strategies, as reported in (Abney et al., 1999) and (Tendeau, 1995), respectively. Those results more simply follow now from our general characterization. Furthermore, PLR parsing (Soisalon-Soininen and Ukkonen, 1979; Nederhof, 1994) can be expressed in our framework as a parsing strategy with the CPP and the SP</context>
</contexts>
<marker>Leermakers, 1989</marker>
<rawString>R. Leermakers. 1989. How to cover a grammar. In 27th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages 135–142, Vancouver, British Columbia, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>B Carpenter</author>
</authors>
<title>Probabilistic parsing using left corner language models.</title>
<date>2000</date>
<booktitle>Advances in Probabilistic and other Parsing Technologies, chapter 6,</booktitle>
<pages>105--124</pages>
<editor>In H. Bunt and A. Nijholt, editors,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="3117" citStr="Manning and Carpenter, 2000" startWordPosition="467" endWordPosition="470">underlying CFG, since the set of free parameters may differ between the CFG and the PDA, both quantitatively and qualitatively. For example, (Sornlertlamvanich et al., 1999) and (Roark and Johnson, 1999) have shown that a probability distribution that can be obtained by training the probabilities of a CFG on the basis of a corpus can be less accurate than the probability distribution obtained by training the probabilities of a PDA constructed by a particular parsing strategy, on the basis of the same corpus. Also the results from (Chitrao and Grishman, 1990), (Charniak and Carroll, 1994) and (Manning and Carpenter, 2000) could be seen in this light. The question arises of whether parsing strategies can be extended probabilistically, i.e., whether a given construction of PDAs from CFGs can be “augmented” with a function defining the probabilities for the target PDA, given the probabilities associated with the input CFG, in such a way that the obtained probabilistic distributions on the CFG derivations and the corresponding PDA computations are equivalent. Some first results on this issue have been presented by (Tendeau, 1995), who shows that the already mentioned left-corner parsing strategy can be extended pr</context>
<context position="6129" citStr="Manning and Carpenter, 2000" startWordPosition="1020" endWordPosition="1023">uld result in a distribution on the generated language that is equivalent to the one induced by the source PCFG. Therefore the LR strategy does not allow probabilistic extension. One may seemingly solve this problem by dropping the constraint of properness, letting each transition that outputs a rule have the same probability as that rule in the PCFG, and letting other transitions have probability 1. However, the properness condition for PDAs has been heavily exploited in parsing applications, in doing incremental left-to-right probability computation for beam search (Roark and Johnson, 1999; Manning and Carpenter, 2000), and more generally in integration with other linear probabilistic models. Furthermore, commonly used training algorithms for PCFGS/PPDAs always produce proper probability assignments, and many desired mathematical properties of these methods are based on such an assumption (Chi and Geman, 1998; S´anchez and Benedi, 1997). We may therefore discard non-proper probability assignments in the current study. However, such probability assignments are outside the reach of the usual training algorithms for PDAs, which always produce proper PDAs. Therefore, we may discard such assignments in the curre</context>
</contexts>
<marker>Manning, Carpenter, 2000</marker>
<rawString>C.D. Manning and B. Carpenter. 2000. Probabilistic parsing using left corner language models. In H. Bunt and A. Nijholt, editors, Advances in Probabilistic and other Parsing Technologies, chapter 6, pages 105–124. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>Probabilistic parsing as intersection.</title>
<date>2003</date>
<booktitle>In 8th International Workshop on Parsing Technologies,</booktitle>
<pages>137--148</pages>
<location>LORIA, Nancy, France,</location>
<contexts>
<context position="19716" citStr="Nederhof and Satta, 2003" startWordPosition="3612" endWordPosition="3615">re c is the above-mentioned dead computation. Due to Lemma 2, 1 ≥ Σc0∈TA pA(c0) ≥ Σw∈Σ∗ pA(w) + pA(c) &gt; Σw∈Σ∗ pA(w) = Σw∈Σ∗ pG(w). This is in contradiction with the consistency of (G, pG). Hence, a probability function pA with the properties we required above cannot exist, and therefore S cannot be extended to become a probabilistic parsing strategy. 5 Strong Predictiveness In this section we present our main result, which is a sufficient condition allowing the probabilistic extension of a parsing strategy. We start with a technical result that was proven in (Abney et al., 1999; Chi, 1999; Nederhof and Satta, 2003). Lemma 4 Given a non-proper PCFG (G, pG), G = (Σ, N, S, R), there is a probability function p0G such that PCFG (G, p0G) is proper and, for every complete derivation d, p0 G(d) = 1C · pG(d), where C = rS⇒d0w,w∈Σ∗ pG(d0). Note that if PCFG (G,pG) in the above lemma is consistent, then C = 1 and (G, p0G) and (G, pG) define the same distribution on derivations. The normalization procedure underlying Lemma 4 makes use of quantities EA⇒dw,w∈Σ∗ pG(d) for each A ∈ N. These quantities can be computed to any degree of precision, as discussed for instance in (Booth and Thompson, 1973) and (Stolcke, 1995</context>
</contexts>
<marker>Nederhof, Satta, 2003</marker>
<rawString>M.-J. Nederhof and G. Satta. 2003. Probabilistic parsing as intersection. In 8th International Workshop on Parsing Technologies, pages 137– 148, LORIA, Nancy, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
</authors>
<title>An optimal tabular parsing algorithm.</title>
<date>1994</date>
<booktitle>In 32nd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>117--124</pages>
<location>Las Cruces, New Mexico, USA,</location>
<contexts>
<context position="25876" citStr="Nederhof, 1994" startWordPosition="4780" endWordPosition="4781">e to an extension with probabilities of the work on covers by (Nijholt, 1980; Leermakers, 1989). 6 Applications Many well-known parsing strategies with the CPP also have the SPP. This is for instance the case for top-down parsing and left-corner parsing. As discussed in the introduction, it has already been shown that for any PCFG g, there are equivalent PPDTs implementing these strategies, as reported in (Abney et al., 1999) and (Tendeau, 1995), respectively. Those results more simply follow now from our general characterization. Furthermore, PLR parsing (Soisalon-Soininen and Ukkonen, 1979; Nederhof, 1994) can be expressed in our framework as a parsing strategy with the CPP and the SPP, and thus we obtain as a new result that this strategy allows probabilistic extension. The above strategies are in contrast to the LR parsing strategy, which has the CPP but lacks the SPP, and therefore falls outside our sufficient condition. As we have already seen in the introduction, it turns out that LR parsing cannot be extended to become a probabilistic parsing strategy. Related to LR parsing is ELR parsing (Purdom and Brown, 1981; Nederhof, 1994), which also lacks the SPP. By an argument similar to the one</context>
</contexts>
<marker>Nederhof, 1994</marker>
<rawString>M.-J. Nederhof. 1994. An optimal tabular parsing algorithm. In 32nd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages 117–124, Las Cruces, New Mexico, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nijholt</author>
</authors>
<title>Context-Free Grammars: Covers, Normal Forms, and Parsing,</title>
<date>1980</date>
<booktitle>of Lecture Notes in Computer Science.</booktitle>
<volume>93</volume>
<publisher>SpringerVerlag.</publisher>
<contexts>
<context position="14972" citStr="Nijholt, 1980" startWordPosition="2697" endWordPosition="2698"> is precisely one complete computation c on w such that f(out(c)) = d. If c is a complete computation, we will write f(c) to denote f(out(c)). The conditions above then imply that f is a bijection from complete computations to complete derivations. Note that output strings of (complete) computations may contain symbols that are not in R, and the symbols that are in R may occur in a different order in v than in f(v) = d. The purpose of the symbols in Σ − R is to help this process of reordering of symbols from R in v, as needed for instance in the case of the left-corner parsing strategy (see (Nijholt, 1980, pp. 22–23) for discussion). A probabilistic parsing strategy is defined to be a function S that maps a reduced, proper and consistent PCFG (9, pg) to a triple S(9, pg) = (A, pA, f), where (A, pA) is a reduced, proper and consistent PPDT, with the same properties as a (non-probabilistic) parsing strategy, and in addition: • For each complete derivation d and each complete computation c such that f(c) = d, pg(d) equals pA(c). In other words, a complete computation has the same probability as the complete derivation that it is mapped to by function f. An implication of this property is that for</context>
<context position="25337" citStr="Nijholt, 1980" startWordPosition="4697" endWordPosition="4698">ather than probabilistic transduction, or in other words, we may simplify PDTs to PDAs. The proof of Theorem 5 also leads to the observation that parsing strategies with the CPP and the SPP as well as their probabilistic extensions can be described as grammar transformations, as follows. A given (P)CFG is mapped to an equivalent (P)PDT by a (probabilistic) parsing strategy. By ignoring the output components of swap transitions we obtain a (P)PDA, which can be mapped to an equivalent (P)CFG as shown above. This observation gives rise to an extension with probabilities of the work on covers by (Nijholt, 1980; Leermakers, 1989). 6 Applications Many well-known parsing strategies with the CPP also have the SPP. This is for instance the case for top-down parsing and left-corner parsing. As discussed in the introduction, it has already been shown that for any PCFG g, there are equivalent PPDTs implementing these strategies, as reported in (Abney et al., 1999) and (Tendeau, 1995), respectively. Those results more simply follow now from our general characterization. Furthermore, PLR parsing (Soisalon-Soininen and Ukkonen, 1979; Nederhof, 1994) can be expressed in our framework as a parsing strategy with</context>
</contexts>
<marker>Nijholt, 1980</marker>
<rawString>A. Nijholt. 1980. Context-Free Grammars: Covers, Normal Forms, and Parsing, volume 93 of Lecture Notes in Computer Science. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C A Brown</author>
</authors>
<title>Parsing extended LR(k) grammars.</title>
<date>1981</date>
<journal>Acta Informatica,</journal>
<pages>15--115</pages>
<contexts>
<context position="26398" citStr="Brown, 1981" startWordPosition="4872" endWordPosition="4873">erization. Furthermore, PLR parsing (Soisalon-Soininen and Ukkonen, 1979; Nederhof, 1994) can be expressed in our framework as a parsing strategy with the CPP and the SPP, and thus we obtain as a new result that this strategy allows probabilistic extension. The above strategies are in contrast to the LR parsing strategy, which has the CPP but lacks the SPP, and therefore falls outside our sufficient condition. As we have already seen in the introduction, it turns out that LR parsing cannot be extended to become a probabilistic parsing strategy. Related to LR parsing is ELR parsing (Purdom and Brown, 1981; Nederhof, 1994), which also lacks the SPP. By an argument similar to the one provided for LR, we can show that also ELR parsing cannot be extended to become a probabilistic parsing strategy. (See (Tendeau, 1997) for earlier observations related to this.) These two cases might suggest that the sufficient condition in Theorem 5 is tight in practice. Decidability of the CPP and the SPP obviously depends on how a parsing strategy is specified. As far as we know, in all practical cases of parsing strategies these properties can be easily decided. Also, observe that our results do not depend on th</context>
</contexts>
<marker>Brown, 1981</marker>
<rawString>P.W. Purdom, Jr. and C.A. Brown. 1981. Parsing extended LR(k) grammars. Acta Informatica, 15:115–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>M Johnson</author>
</authors>
<title>Efficient probabilistic top-down and left-corner parsing.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>421--428</pages>
<location>Maryland, USA,</location>
<contexts>
<context position="2692" citStr="Roark and Johnson, 1999" startWordPosition="396" endWordPosition="399">tively. However, these probabilities cannot be chosen entirely arbitrarily. For example, for a given nonterminal A in a PCFG, the sum of the probabilities of all rules rewriting A must be 1. This means that, out of a total of say m rules rewriting A, only m − 1 rules represent “free” parameters. Depending on the choice of the parsing strategy, the constructed PDA may allow different probability distributions than the underlying CFG, since the set of free parameters may differ between the CFG and the PDA, both quantitatively and qualitatively. For example, (Sornlertlamvanich et al., 1999) and (Roark and Johnson, 1999) have shown that a probability distribution that can be obtained by training the probabilities of a CFG on the basis of a corpus can be less accurate than the probability distribution obtained by training the probabilities of a PDA constructed by a particular parsing strategy, on the basis of the same corpus. Also the results from (Chitrao and Grishman, 1990), (Charniak and Carroll, 1994) and (Manning and Carpenter, 2000) could be seen in this light. The question arises of whether parsing strategies can be extended probabilistically, i.e., whether a given construction of PDAs from CFGs can be </context>
<context position="6099" citStr="Roark and Johnson, 1999" startWordPosition="1016" endWordPosition="1019"> the LR automaton that would result in a distribution on the generated language that is equivalent to the one induced by the source PCFG. Therefore the LR strategy does not allow probabilistic extension. One may seemingly solve this problem by dropping the constraint of properness, letting each transition that outputs a rule have the same probability as that rule in the PCFG, and letting other transitions have probability 1. However, the properness condition for PDAs has been heavily exploited in parsing applications, in doing incremental left-to-right probability computation for beam search (Roark and Johnson, 1999; Manning and Carpenter, 2000), and more generally in integration with other linear probabilistic models. Furthermore, commonly used training algorithms for PCFGS/PPDAs always produce proper probability assignments, and many desired mathematical properties of these methods are based on such an assumption (Chi and Geman, 1998; S´anchez and Benedi, 1997). We may therefore discard non-proper probability assignments in the current study. However, such probability assignments are outside the reach of the usual training algorithms for PDAs, which always produce proper PDAs. Therefore, we may discard</context>
</contexts>
<marker>Roark, Johnson, 1999</marker>
<rawString>B. Roark and M. Johnson. 1999. Efficient probabilistic top-down and left-corner parsing. In 37th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages 421–428, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Rosenkrantz</author>
<author>P M Lewis</author>
</authors>
<title>Deterministic left corner parsing.</title>
<date>1970</date>
<booktitle>In IEEE Conference Record of the 11th Annual Symposium on Switching and Automata Theory,</booktitle>
<pages>139--152</pages>
<marker>Rosenkrantz, Lewis, 1970</marker>
<rawString>D.J. Rosenkrantz and P.M. Lewis II. 1970. Deterministic left corner parsing. In IEEE Conference Record of the 11th Annual Symposium on Switching and Automata Theory, pages 139–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-A S´anchez</author>
<author>J-M Benedi</author>
</authors>
<title>Consistency of stochastic context-free grammars from probabilistic estimation based on growth transformations.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>9</issue>
<marker>S´anchez, Benedi, 1997</marker>
<rawString>J.-A. S´anchez and J.-M. Benedi. 1997. Consistency of stochastic context-free grammars from probabilistic estimation based on growth transformations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(9):1052–1055, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E S Santos</author>
</authors>
<title>Probabilistic grammars and automata.</title>
<date>1972</date>
<journal>Information and Control,</journal>
<pages>21--27</pages>
<contexts>
<context position="8538" citStr="Santos, 1972" startWordPosition="1412" endWordPosition="1413">rategies. We use the above findings to establish new results about probabilistic extensions of parsing strategies that are used in standard practice in computational linguistics, as well as to provide simpler proofs of already known results. We introduce our framework in Section 3 and report our main results in Sections 4 and 5. We discuss applications of our results in Section 6. 2 Preliminaries In this paper we assume some familiarity with definitions of (P)CFGs and (P)PDAs. We refer the reader to standard textbooks and publications as for instance (Harrison, 1978; Booth and Thompson, 1973; Santos, 1972). A CFG G is a tuple (E, N, 5, R), with E and N the sets of terminals and nonterminals, respectively, 5 the start symbol and R the set of rules. In this paper we only consider left-most derivations, represented as strings d ∈ R* and simply called derivaPr(axcbxd) _ Pr(axdbxc) Pr(A→aC)·Pr(B→bD) _ Pr(A→aD)·Pr(B→bC) 1 1 · 3 3 2 _ 2 · 1 4 3 3 Pr(axcbxd) _ Pr(axdbxc) tions. For α, β E (E U N)∗, we write α =�-d β with the usual meaning. If α = S and β = w E E∗, we call d a complete derivation of w. We say a CFG is reduced if each rule in R occurs in some complete derivation. A PCFG is a pair (!9, p)</context>
</contexts>
<marker>Santos, 1972</marker>
<rawString>E.S. Santos. 1972. Probabilistic grammars and automata. Information and Control, 21:27–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sippu</author>
<author>E Soisalon-Soininen</author>
</authors>
<title>Parsing Theory, Vol. II: LR(k) and LL(k) Parsing,</title>
<date>1990</date>
<journal>of EATCS Monographs on Theoretical Computer Science.</journal>
<volume>20</volume>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="1658" citStr="Sippu and Soisalon-Soininen, 1990" startWordPosition="232" endWordPosition="235"> PDAs is that these devices provide an operational specification that determines which steps must be performed when parsing an input string, something that is not offered by CFGs. In other words, PDAs can be associated to parsing strategies for contextfree languages. More precisely, parsing strategies are traditionally specified as constructions that map CFGs to language-equivalent PDAs. Popular examples of parsing strategies are the standard constructions of top-down PDAs (Harrison, 1978), leftcorner PDAs (Rosenkrantz and Lewis II, 1970), shift-reduce PDAs (Aho and Ullman, 1972) and LR PDAs (Sippu and Soisalon-Soininen, 1990). CFGs and PDAs have probabilistic counterparts, called probabilistic CFGs (PCFGs) and probabilistic PDAs (PPDAs). These models are very popular in natural language processing applications, where they are used to define a probability distribution function on the domain of all derivations for sentences in the language of interest. In PCFGs and PPDAs, probabilities are assigned to rules or transitions, respectively. However, these probabilities cannot be chosen entirely arbitrarily. For example, for a given nonterminal A in a PCFG, the sum of the probabilities of all rules rewriting A must be 1.</context>
</contexts>
<marker>Sippu, Soisalon-Soininen, 1990</marker>
<rawString>S. Sippu and E. Soisalon-Soininen. 1990. Parsing Theory, Vol. II: LR(k) and LL(k) Parsing, volume 20 of EATCS Monographs on Theoretical Computer Science. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Soisalon-Soininen</author>
<author>E Ukkonen</author>
</authors>
<title>A method for transforming grammars into LL(k) form.</title>
<date>1979</date>
<journal>Acta Informatica,</journal>
<pages>12--339</pages>
<contexts>
<context position="25859" citStr="Soisalon-Soininen and Ukkonen, 1979" startWordPosition="4775" endWordPosition="4779">own above. This observation gives rise to an extension with probabilities of the work on covers by (Nijholt, 1980; Leermakers, 1989). 6 Applications Many well-known parsing strategies with the CPP also have the SPP. This is for instance the case for top-down parsing and left-corner parsing. As discussed in the introduction, it has already been shown that for any PCFG g, there are equivalent PPDTs implementing these strategies, as reported in (Abney et al., 1999) and (Tendeau, 1995), respectively. Those results more simply follow now from our general characterization. Furthermore, PLR parsing (Soisalon-Soininen and Ukkonen, 1979; Nederhof, 1994) can be expressed in our framework as a parsing strategy with the CPP and the SPP, and thus we obtain as a new result that this strategy allows probabilistic extension. The above strategies are in contrast to the LR parsing strategy, which has the CPP but lacks the SPP, and therefore falls outside our sufficient condition. As we have already seen in the introduction, it turns out that LR parsing cannot be extended to become a probabilistic parsing strategy. Related to LR parsing is ELR parsing (Purdom and Brown, 1981; Nederhof, 1994), which also lacks the SPP. By an argument s</context>
</contexts>
<marker>Soisalon-Soininen, Ukkonen, 1979</marker>
<rawString>E. Soisalon-Soininen and E. Ukkonen. 1979. A method for transforming grammars into LL(k) form. Acta Informatica, 12:339–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sornlertlamvanich</author>
<author>K Inui</author>
<author>H Tanaka</author>
<author>T Tokunaga</author>
<author>T Takezawa</author>
</authors>
<title>Empirical support for new probabilistic generalized LR parsing.</title>
<date>1999</date>
<journal>Journal of Natural Language Processing,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="2662" citStr="Sornlertlamvanich et al., 1999" startWordPosition="391" endWordPosition="394">igned to rules or transitions, respectively. However, these probabilities cannot be chosen entirely arbitrarily. For example, for a given nonterminal A in a PCFG, the sum of the probabilities of all rules rewriting A must be 1. This means that, out of a total of say m rules rewriting A, only m − 1 rules represent “free” parameters. Depending on the choice of the parsing strategy, the constructed PDA may allow different probability distributions than the underlying CFG, since the set of free parameters may differ between the CFG and the PDA, both quantitatively and qualitatively. For example, (Sornlertlamvanich et al., 1999) and (Roark and Johnson, 1999) have shown that a probability distribution that can be obtained by training the probabilities of a CFG on the basis of a corpus can be less accurate than the probability distribution obtained by training the probabilities of a PDA constructed by a particular parsing strategy, on the basis of the same corpus. Also the results from (Chitrao and Grishman, 1990), (Charniak and Carroll, 1994) and (Manning and Carpenter, 2000) could be seen in this light. The question arises of whether parsing strategies can be extended probabilistically, i.e., whether a given construc</context>
</contexts>
<marker>Sornlertlamvanich, Inui, Tanaka, Tokunaga, Takezawa, 1999</marker>
<rawString>V. Sornlertlamvanich, K. Inui, H. Tanaka, T. Tokunaga, and T. Takezawa. 1999. Empirical support for new probabilistic generalized LR parsing. Journal of Natural Language Processing, 6(3):3–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="20317" citStr="Stolcke, 1995" startWordPosition="3727" endWordPosition="3728"> Satta, 2003). Lemma 4 Given a non-proper PCFG (G, pG), G = (Σ, N, S, R), there is a probability function p0G such that PCFG (G, p0G) is proper and, for every complete derivation d, p0 G(d) = 1C · pG(d), where C = rS⇒d0w,w∈Σ∗ pG(d0). Note that if PCFG (G,pG) in the above lemma is consistent, then C = 1 and (G, p0G) and (G, pG) define the same distribution on derivations. The normalization procedure underlying Lemma 4 makes use of quantities EA⇒dw,w∈Σ∗ pG(d) for each A ∈ N. These quantities can be computed to any degree of precision, as discussed for instance in (Booth and Thompson, 1973) and (Stolcke, 1995). Thus normalization of a PCFG can be effectively computed. For a fixed PDT, we define the binary relation --* on stack symbols by: Y --* Y 0 if and only if (Y, w, ε) `∗ (Y 0, ε, v) for some w ∈ Σ∗ and v ∈ Σ∗. In words, some subcomputation of the PDT may start with stack Y and end with stack Y 0. Note that all stacks that occur in such a subcomputation must have height of 1 or more. We say that a (P)PDA or a (P)PDT has the strong predictiveness property (SPP) if the existence of three transitions X 7→ XY , XY1 7→ Z1 and XY2 7→ Z2 such that Y --* Y1 and Y --* Y2 implies Z1 = Z2. Informally, t</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>A. Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):167–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Tendeau</author>
</authors>
<title>Stochastic parse-tree recognition by a pushdown automaton.</title>
<date>1995</date>
<booktitle>In Fourth International Workshop on Parsing Technologies,</booktitle>
<pages>234--249</pages>
<location>Prague</location>
<contexts>
<context position="3631" citStr="Tendeau, 1995" startWordPosition="553" endWordPosition="554">ts from (Chitrao and Grishman, 1990), (Charniak and Carroll, 1994) and (Manning and Carpenter, 2000) could be seen in this light. The question arises of whether parsing strategies can be extended probabilistically, i.e., whether a given construction of PDAs from CFGs can be “augmented” with a function defining the probabilities for the target PDA, given the probabilities associated with the input CFG, in such a way that the obtained probabilistic distributions on the CFG derivations and the corresponding PDA computations are equivalent. Some first results on this issue have been presented by (Tendeau, 1995), who shows that the already mentioned left-corner parsing strategy can be extended probabilistically, and later by (Abney et al., 1999) who show that the pure top-down parsing strategy and a specific type of shift-reduce parsing strategy can be probabilistically extended. One might think that any “practical” parsing strategy can be probabilistically extended, but this turns out not to be the case. We briefly discuss here a counter-example, in order to motivate the approach we have taken in this paper. Probabilistic LR parsing has been investigated in the literature (Wright and Wrigley, 1991; </context>
<context position="25710" citStr="Tendeau, 1995" startWordPosition="4758" endWordPosition="4759">egy. By ignoring the output components of swap transitions we obtain a (P)PDA, which can be mapped to an equivalent (P)CFG as shown above. This observation gives rise to an extension with probabilities of the work on covers by (Nijholt, 1980; Leermakers, 1989). 6 Applications Many well-known parsing strategies with the CPP also have the SPP. This is for instance the case for top-down parsing and left-corner parsing. As discussed in the introduction, it has already been shown that for any PCFG g, there are equivalent PPDTs implementing these strategies, as reported in (Abney et al., 1999) and (Tendeau, 1995), respectively. Those results more simply follow now from our general characterization. Furthermore, PLR parsing (Soisalon-Soininen and Ukkonen, 1979; Nederhof, 1994) can be expressed in our framework as a parsing strategy with the CPP and the SPP, and thus we obtain as a new result that this strategy allows probabilistic extension. The above strategies are in contrast to the LR parsing strategy, which has the CPP but lacks the SPP, and therefore falls outside our sufficient condition. As we have already seen in the introduction, it turns out that LR parsing cannot be extended to become a prob</context>
</contexts>
<marker>Tendeau, 1995</marker>
<rawString>F. Tendeau. 1995. Stochastic parse-tree recognition by a pushdown automaton. In Fourth International Workshop on Parsing Technologies, pages 234–249, Prague and Karlovy Vary, Czech Republic, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Tendeau</author>
</authors>
<title>Analyse syntaxique et s´emantique avec ´evaluation d’attributs dans un demi-anneau.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Orl´eans.</institution>
<contexts>
<context position="26611" citStr="Tendeau, 1997" startWordPosition="4908" endWordPosition="4910">t this strategy allows probabilistic extension. The above strategies are in contrast to the LR parsing strategy, which has the CPP but lacks the SPP, and therefore falls outside our sufficient condition. As we have already seen in the introduction, it turns out that LR parsing cannot be extended to become a probabilistic parsing strategy. Related to LR parsing is ELR parsing (Purdom and Brown, 1981; Nederhof, 1994), which also lacks the SPP. By an argument similar to the one provided for LR, we can show that also ELR parsing cannot be extended to become a probabilistic parsing strategy. (See (Tendeau, 1997) for earlier observations related to this.) These two cases might suggest that the sufficient condition in Theorem 5 is tight in practice. Decidability of the CPP and the SPP obviously depends on how a parsing strategy is specified. As far as we know, in all practical cases of parsing strategies these properties can be easily decided. Also, observe that our results do not depend on the general behaviour of a parsing strategy S, but just on its “point-wise” behaviour on each input CFG. Specifically, if S does not have the CPP and the SPP, but for some fixed CFG g of interest we obtain a PDT A t</context>
</contexts>
<marker>Tendeau, 1997</marker>
<rawString>F. Tendeau. 1997. Analyse syntaxique et s´emantique avec ´evaluation d’attributs dans un demi-anneau. Ph.D. thesis, University of Orl´eans.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Wright</author>
<author>E N Wrigley</author>
</authors>
<title>GLR parsing with probability.</title>
<date>1991</date>
<booktitle>Generalized LR Parsing, chapter 8,</booktitle>
<pages>113--128</pages>
<editor>In M. Tomita, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="4229" citStr="Wright and Wrigley, 1991" startWordPosition="644" endWordPosition="647">esented by (Tendeau, 1995), who shows that the already mentioned left-corner parsing strategy can be extended probabilistically, and later by (Abney et al., 1999) who show that the pure top-down parsing strategy and a specific type of shift-reduce parsing strategy can be probabilistically extended. One might think that any “practical” parsing strategy can be probabilistically extended, but this turns out not to be the case. We briefly discuss here a counter-example, in order to motivate the approach we have taken in this paper. Probabilistic LR parsing has been investigated in the literature (Wright and Wrigley, 1991; Briscoe and Carroll, 1993; Inui et al., 2000) under the assumption that it would allow more fine-grained probability distributions than the underlying PCFGs. However, this is not the case in general. Consider a PCFG with rule/probability pairs: B → bC, 23 5 → AB, 1 B → bD, 1 A → aC, 1 3 3 C → xc, 1 A → aD, 2 D → xd, 1 3 There are two key transitions in the associated LR automaton, which represent shift actions over c and d (we denote LR states by their sets of kernel items and encode these states into stack symbols): Tc : {C → x • c,D → x • d} c 7→ {C → x • c,D → x • d} {C → xc •} Td : {C → </context>
</contexts>
<marker>Wright, Wrigley, 1991</marker>
<rawString>J.H. Wright and E.N. Wrigley. 1991. GLR parsing with probability. In M. Tomita, editor, Generalized LR Parsing, chapter 8, pages 113–128. Kluwer Academic Publishers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>