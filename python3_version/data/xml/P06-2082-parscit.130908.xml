<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000117">
<title confidence="0.980525">
Analysis of Selective Strategies to Build a Dependency-Analyzed Corpus
</title>
<author confidence="0.981603">
Kiyonori Ohtake
</author>
<affiliation confidence="0.665707">
National Institute of Information and Communications Technology (NICT),
ATR Spoken Language Communication Research Labs.
2-2-2 Hikaridai “Keihanna Science City” Kyoto 619-0288 Japan
</affiliation>
<email confidence="0.785338">
kiyonori.ohtake [at] nict.go.jp
</email>
<sectionHeader confidence="0.986479" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999770266666667">
This paper discusses sampling strategies
for building a dependency-analyzed cor-
pus and analyzes them with different kinds
of corpora. We used the Kyoto Text
Corpus, a dependency-analyzed corpus of
newspaper articles, and prepared the IPAL
corpus, a dependency-analyzed corpus of
example sentences in dictionaries, as a
new and different kind of corpus. The ex-
perimental results revealed that the length
of the test set controlled the accuracy and
that the longest-first strategy was good
for an expanding corpus, but this was not
the case when constructing a corpus from
scratch.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998003508772">
Dependency-structure analysis plays a very impor-
tant role in natural language processing (NLP).
Thus, so far, much research has been done on
this subject, with many analyzers being developed
such as rule-based analyzers and corpus-based
analyzers that use machine-learning techniques.
However, the maximum accuracy achieved by
state-of-the art analyzers is almost 90% for news-
paper articles; it seems very difficult to exceed this
figure of 90%. To improve our analyzers, we have
to write more rules for rule-based analyzers or pre-
pare more corpora for corpus-based analyzers.
If we take a machine-learning approach, it
is important to consider what features are used.
However, there are several machine-learning tech-
niques, such as support vector machines (SVMs)
with a kernel function, that have strong general-
ization ability and are very robust for choosing the
right features. If we use such machine-learning
techniques, we will be free from choosing a fea-
ture set because it will be possible to use all pos-
sible features with little or no decline in perfor-
mance. Actually, Sasano tried to expand the fea-
ture set for a Japanese dependency analyzer using
SVMs in (Sasano, 2004), with a small improve-
ment in accuracy.
To write rules for a rule-based analyzer, and to
produce an analyzer using machine-learning tech-
niques, it is crucial to construct a dependency-
analyzed corpus. Such a corpus is very useful not
only for constructing a dependency analyzer but
also for other natural language processing appli-
cations. However, building this kind of resource
is very expensive and labor-intensive because it is
difficult to annotate a large amount of dependency-
analyzed corpus in short time.
At present, one promising approach to mitigat-
ing the annotation bottleneck problem is to use
selective sampling, a variant of active learning
(Cohn et al., 1994; Fujii et al., 1998; Hwa, 2004).
In general, selective sampling is an interactive
learning method in which the machine takes the
initiative in selecting unlabeled data for the human
to annotate. Under this framework, the system has
access to a large pool of unlabeled data, and it has
to predict how much it can learn from each candi-
date in the pool if that candidate is labeled.
Most of the experiments that had been carried
out in the previous works for selective sampling
used an annotated corpus in a limited domain. The
most typical corpus is WSJ of Penn Treebank. The
reason why the domain was so limited is very sim-
ple; corpus annotation is very expensive. How-
ever, we want to know the effects of selective sam-
pling for corpora in various domains because a de-
pendency analyzer constructed from a corpus does
not always analyze a text in limited domain.
</bodyText>
<page confidence="0.984581">
635
</page>
<note confidence="0.7247395">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 635–642,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.9991699">
On the other hand, there is no clear guide-
line nor development strategy for constructing a
dependency-analyzed corpus to produce a highly
accurate dependency analyzer. Thus in this paper,
we discuss fundamental sampling strategies for
a dependency-analyzed corpus for corpus-based
dependency analyzers with several types of cor-
pora. This paper unveils the essential characteris-
tics of basic sampling strategies for a dependency-
analyzed corpus.
</bodyText>
<sectionHeader confidence="0.996045" genericHeader="method">
2 Dependency-Analyzed Corpora
</sectionHeader>
<bodyText confidence="0.999853375">
We use two dependency-analyzed corpora. One is
the Kyoto Text Corpus, which consists of news-
paper articles, and the other one is the IPAL cor-
pus, which contains sentences extracted from the
“example of use” section of the enties in several
dictionaries for computers. The IPAL corpus was
recently annotated for this study as a different kind
of corpus.
</bodyText>
<subsectionHeader confidence="0.990574">
2.1 Kyoto Text Corpus
</subsectionHeader>
<bodyText confidence="0.999782782608696">
In this study we use Kyoto Text Corpus version
3.0. The corpus consists of newspaper articles
from Mainichi Newspapers from January 1st to
January 17th, 1995 (almost 20,000 sentences) and
all editorials of the year 1995 (almost 20,000 sen-
tences). All of the articles were analyzed by mor-
phological analyzer JUMAN and dependency an-
alyzer KNP1. After that, the analyzed results were
manually corrected. Kyoto Text Corpus version
4.0 is now available, holding on additional 5,000
annotated sentences in the corpus to version 3.0
for case relations, anaphoric relations, omission
information and co-reference information2.
The original POS system used in the Kyoto
Text Corpus is JUMAN’s POS system. We con-
verted the POS system used in the Kyoto Text Cor-
pus into ChaSen’s POS system because we used
ChaSen, a Japanese morphological analyzer, and
CaboCha3 (Kudo and Matsumoto, 2002), a depen-
dency analyzer incorporating SVMs, as a state-of-
the art corpus-based Japanese dependency struc-
ture analyzer that prefers ChaSen’s POS system to
that of JUMAN. In addition, we modified some
</bodyText>
<footnote confidence="0.985308">
1http://www.kc.t.u-tokyo.ac.jp/
nl-resource
2http://www.kc.t.u-tokyo.ac.jp/
nl-resource/corpus.html
3http://chasen.org/˜taku/
software/cabocha/
</footnote>
<bodyText confidence="0.5338025">
bunsetu segmentations because there were several
inconsistencies in bunsetu segmentation.
Table 1 shows the details of the Kyoto Text Cor-
pus.
</bodyText>
<table confidence="0.998410857142857">
Kyoto Text Corpus
(General) (Editorial)
# of sentences 19,669 18,714
# of bunsetu 192,154 171,461
# of morphemes 542,334 480,005
vocabulary size 29,542 17,730
bunsetu / sentence 9.769 9.162
</table>
<tableCaption confidence="0.999564">
Table 1: Kyoto Text Corpus
</tableCaption>
<subsectionHeader confidence="0.966334">
2.2 IPAL corpus
</subsectionHeader>
<bodyText confidence="0.999027875">
IPAL (IPA, Information-technology Promotion
Agency, Lexicon of the Japanese language for
computers) dictionaries consist of three dictionar-
ies, the IPAL noun dictionary, the IPAL verb dic-
tionary and the IPAL adjective dictionary. Each of
the dictionaries includes example sentences. We
extracted 7,720 sentences from IPAL Noun, 5,244
sentences from IPAL Verb, and 2,366 sentences
from IPAL Adjective. We analyzed them using
CaboCha and manually corrected the errors. We
named this dependency-analyzed corpus the IPAL
corpus. Table 2 presents the details of the IPAL
corpus. One characteristic of the IPAL corpus is
that the average sentence length is very short; in
other words, the sentences in the IPAL corpus are
very simple.
</bodyText>
<table confidence="0.9956044">
# of sentences 15,330
# of bunsetu 67,170
# of morphemes 156,131
vocabulary size 11,895
bunsetu/sentence 4.382
</table>
<tableCaption confidence="0.997828">
Table 2: IPAL corpus
</tableCaption>
<sectionHeader confidence="0.99901" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999961285714286">
We carried out several experiments to determine
the basic characteristics of several selective strate-
gies for a Japanese dependency-analyzed corpus.
First, we briefly introduce Japanese dependency
structure. Second, we carry out basic experiments
with our dependency-analyzed corpora and ana-
lyze the errors. Finally, we conduct simulations to
</bodyText>
<page confidence="0.995446">
636
</page>
<bodyText confidence="0.9994655">
ascertain the fundamental characteristics of these
strategies.
</bodyText>
<subsectionHeader confidence="0.999178">
3.1 Japanese dependency structure
</subsectionHeader>
<bodyText confidence="0.998927">
The Japanese dependency structure is usually de-
fined in terms of the relationship between phrasal
units called bunsetu segments. Conventional
methods of dependency analysis have assumed the
following three syntactic constraints (Kurohashi
and Nagao, 1994a):
</bodyText>
<listItem confidence="0.9773052">
1. All dependencies are directed from left to
right.
2. Dependencies do not cross each other.
3. Each bunsetu segment, except the last one,
depends on only one bunsetu segment.
</listItem>
<bodyText confidence="0.892810833333333">
Figure 1 shows examples of Japanese dependency
structure.
Jack-wa Kim-ni atsui hon-o okutta
Jack to Kim thick a book presented
(Jack presented a thick book to Kim.)
SVM of CaboCha, and tested with second- and
third-degree polynomial kernels. The input data
for each test were correct for morphological anal-
ysis and bunsetu segmentation, though in practical
situations we have to expect some morphological
analysis errors and bunsetu mis-segmentations.
In Table 3 “Learning” indicates the learning cor-
pus, “Test” represents the test corpus, and “De-
gree” denotes the degree of the polynomial func-
tion. In addition, “Acc.” indicates the accuracy
of dependency-analyzed results and “S-acc.” in-
dicates the sentence accuracy that is the ratio of
sentences that were analyzed without errors.
</bodyText>
<table confidence="0.999793846153846">
Learning Test Degree Acc.(%) S-acc.(%)
KG0 KG0 2 94.06 65.51
KG0 KG0 3 99.96 99.71
KG0 KG1 2 89.50 50.35
KG0 KG1 3 89.23 49.33
KG1 KG0 2 89.60 49.89
KG1 KG0 3 89.21 49.05
ED0 ED1 2 90.77 55.58
ED1 ED0 2 90.52 54.62
IPAL0 IPAL1 2 97.43 92.25
IPAL1 IPAL0 2 97.69 93.06
KG0 IPAL0 2 97.76 93.15
ED0 IPAL0 2 97.56 92.81
</table>
<tableCaption confidence="0.999405">
Table 3: Results of cross-validation tests
</tableCaption>
<figure confidence="0.642843666666667">
Kim-wa Jack-ga kureta hon-o nakushita
Kim Jack gave a book lost
(Kim lost the book Jack gave her.)
</figure>
<figureCaption confidence="0.9186385">
Figure 1: Examples of Japanese dependency struc-
ture
</figureCaption>
<bodyText confidence="0.999926333333333">
In this paper, we refer to the beginning of a de-
pendency direction as a “modifier” and the end of
that as a “head.”
</bodyText>
<subsectionHeader confidence="0.999967">
3.2 Analyzing errors
</subsectionHeader>
<bodyText confidence="0.999945487179487">
We performed a cross-validation test with our
dependency-analyzed corpora by using the SVM-
based dependency analyzer CaboCha. The feature
set used for SVM in CaboCha followed the default
settings of CaboCha.
First, we arbitrarily divided each corpus into
two parts. General articles of the Kyoto Text Cor-
pus were arbitrarily divided into KG0 and KG1,
while editorials were also divided into ED0 and
ED1. The IPAL corpus was arbitrarily divided into
IPAL0 and IPAL1. Second, we carried out cross-
validation tests on these divided corpora.
Table 3 shows the results of the cross-validation
tests. We employed a polynomial kernel for the
Table 3 also shows the biased evaluation (closed
test; the test was the training set itself) results. In
the cross-validation results of KG0 and KG1, the
average accuracy of the second-degree kernel was
89.55 (154,455 / 172,485)% and the average sen-
tence accuracy was 50.12 (9,858 / 19,669)%. In
other words, there were 18,030 dependency errors
in the cross validation test. We analyzed these er-
rors.
Against the average length (9.769) of the cor-
pus shown in Table 1, the average length of the
sentences with errors in the cross-validation test is
12.53 (bunsetu / sentence). These results confirm
that longer sentences tend to be analyzed incor-
rectly.
Next we analyzed modifier bunsetu that were
mis-analyzed. Table 4 shows the top ten POS se-
quences that consisted of modifier mis-analyzed
bunsetu.
We also analyzed the distance between modi-
fier bunsetu and head bunsetu of the mis-analyzed
dependencies. Table 5 shows top ten cases of
the distance. In Table 5 “Err.” indicates the dis-
tance between a modifier and a head bunsetu of
mis-analyzed dependencies, “Correct” indicates
</bodyText>
<page confidence="0.991451">
637
</page>
<table confidence="0.901141818181818">
POS sequence Frequency
noun, case marker 835
verb, comma 576
noun, topic marker 444
adverbial noun, comma 370
verb 336
number, numeral classifier, comma 318
noun, adnominal particle 304
adverb 304
verb, verbal auxiliary 281
verb, conjunctive particle, comma 265
</table>
<tableCaption confidence="0.971062">
Table 4: Modifier POS sequences of mis-analyzed
</tableCaption>
<bodyText confidence="0.854109">
dependencies and their frequencies in the cross-
validation test (top 10)
the distance between a modifier and a correct
(should modify) head bunsetu in each case of mis-
analyzed dependencies, and “Freq.” denotes their
frequency.
</bodyText>
<table confidence="0.997400666666667">
Err. Correct Freq.
1 2 3,117
2 1 1,362
3 1 919
1 3 863
2 3 482
</table>
<tableCaption confidence="0.97239">
Table 5: Frequencies of dependency distances at
error and correct cases in the cross-validation test
(top 10)
</tableCaption>
<subsectionHeader confidence="0.997782">
3.3 Selective sampling simulation
</subsectionHeader>
<bodyText confidence="0.999978">
In this section, we discuss selective strategies
through two simulations. One is expanding a
dependency-analyzed corpus to construct a more
accurate dependency analyzer, and the other is an
initial situation just beginning to build a corpus.
</bodyText>
<subsectionHeader confidence="0.87606">
3.3.1 Expanding situation
</subsectionHeader>
<bodyText confidence="0.999224454545454">
The situation is as follows. First, the corpus,
Kyoto Text Corpus KG1, is given. Second, we ex-
pand the corpus using the editorials component of
the Kyoto Text Corpus. Then we consider the fol-
lowing six strategies: (1) Longest first, (2) Max-
imizing vocabulary size first, (3) Maximizing un-
seen dependencies first, (4) Maximizing average
distance of dependencies first, (5) Chronological
order, and (6) Random.
We briefly introduce these six strategies as fol-
lows:
</bodyText>
<listItem confidence="0.949219">
1. Longest first (Long)
</listItem>
<bodyText confidence="0.9989176">
Since longer sentences tend to have com-
plex structures and be analyzed incorrectly,
we prepare the corpus in descending order of
length. The length is measured by the num-
ber of bunsetu in a sentence.
</bodyText>
<listItem confidence="0.997698833333333">
2. Maximizing vocabulary size first (VSort)
Unknown words cause unknown dependen-
cies, thus we sort the corpus to maximize its
vocabulary size.
3. Maximizing unseen dependencies first
(UDep)
</listItem>
<bodyText confidence="0.999945">
This is similar to (2). However, we cannot
know the true dependencies. The analyzed
results by the dependency analyzer based
on the current corpus are used to estimate
the unseen dependencies. The accuracy of
the estimated results was 90.25% and the
sentence accuracy was 54.03%.
</bodyText>
<listItem confidence="0.8114775">
4. Maximizing average distance of dependen-
cies first (ADist)
</listItem>
<bodyText confidence="0.82982325">
It is difficult to analyze long-distance depen-
dencies correctly. Thus, the average distance
of dependencies is an approximation for the
difficulty of analysis.
</bodyText>
<listItem confidence="0.746244">
5. Chronological order (Chrono)
</listItem>
<bodyText confidence="0.855349666666667">
Since there is a chronological order in news-
paper articles, this strategy should feel quite
natural.
</bodyText>
<listItem confidence="0.600782">
6. Random (ED0)
</listItem>
<bodyText confidence="0.9994573125">
Chronological order seems natural, but news-
paper articles also have cohesion. Thus, the
vocabulary might be unbalanced when we
consider the chronological order. We also try
randomized order; actually, we used the cor-
pus ED0 as the randomized corpus.
We sorted the editorial component of the Kyoto
Text Corpus by each strategy mentioned above.
After sorting, corpora were constructed by taking
the top N sentences of each corpus sorted by each
strategy. The size of each corpus was balanced
with the number dependencies.
We constructed dependency analyzers based on
each corpus, KG1 plus each prepared corpus, then
tested them by using the following corpora: (a) K-
mag, (b) IPAL0, and (c) KG0.
</bodyText>
<figure confidence="0.9984865">
Err. Correct Freq.
2 4 478
3 2 436
4 1 434
4 2 379
1 4 329
</figure>
<page confidence="0.983773">
638
</page>
<table confidence="0.985829833333333">
Corpus # of sent. # of bunsetu vocabulary size # of dependencies # of bunsetu / sent.
Long 5,490 81,759 13,266 76,269 14.89
VSort 8,762 85,031 16,428 76,269 9.705
UDep 5,524 81,793 13,371 76,269 14.81
ADist 6,950 83,223 13,074 76,273 11.97
Chrono 9,342 85,609 13,278 76,267 9.164
ED0 9,357 85,628 13,561 76,271 9.151
K-mag 489 4,851 2,501 4,362 9.920
IPAL0 7,665 33,484 8,617 25,819 4.368
KG0 9,835 96,283 21,616 86,448 9.790
I-Long 5,523 91,972 20,068 86,449 16.65
I-VSort 8,437 94,881 28,867 86,444 11.25
</table>
<tableCaption confidence="0.999614">
Table 6: Detailed information of corpora
</tableCaption>
<bodyText confidence="0.9994166">
K-mag consists of articles from the Koizumi
Cabinet’s E-Mail Magazine. This magazine was
first published on May 29th 1999 and is still re-
leased weekly. K-mag consists of articles of the
magazine published from May 29th 1999 to July
19th 1999. In addition, since March 25th 2004 an
English version of this E-Mail Magazine has been
available. Thus, currently this E-mail Magazine is
bilingual. The articles of this magazine were an-
alyzed by the dependency analyzer CaboCha, and
we manually corrected the errors.
K-mag includes a wide variety articles, and the
average sentence length is longer than in newspa-
pers. Basic information on K-mag is also provided
in Table 6.
</bodyText>
<table confidence="0.999597125">
Learning corpus Acc.(%) S-acc.(%)
KG1 87.25 49.69
KG1+LONG 87.67 51.53
KG1+Vsort 87.25 50.10
KG1+UDep 87.57 51.12
KG1+ADist 87.67 50.72
KG1+Chrono 87.57 50.31
KG1+Rand 87.60 49.69
</table>
<tableCaption confidence="0.988309">
Table 7: Analyzed results of K-mag (which is
</tableCaption>
<bodyText confidence="0.767298">
different domain and has long average sentence
length) with these learning corpora
</bodyText>
<subsectionHeader confidence="0.428389">
3.3.2 Simulation for initial situation
</subsectionHeader>
<bodyText confidence="0.9998158">
The results revealed that the longest-first strat-
egy seems the best way. Here, however, a question
arises: “Does the longest-first strategy always pro-
vide good predictions?” We carried out an exper-
iment to answer the question. The experimental
</bodyText>
<table confidence="0.99893475">
Learning corpus Acc. (%) S-acc.(%)
KG1 97.68 93.02
KG1+LONG 97.75 93.22
KG1+Vsort 97.70 93.06
KG1+UDep 97.75 93.18
KG1+ADist 97.70 93.10
KG1+Chrono 97.71 93.06
KG1+Rand 97.69 93.06
</table>
<tableCaption confidence="0.979785">
Table 8: Analyzed results of IPAL0 (which is
</tableCaption>
<bodyText confidence="0.997401666666667">
different domain and has short average sentence
length) with these learning corpora
results we presented above were simulations of an
expanding corpus. On the other hand, it is also
possible to consider an initial situation for build-
ing a dependency-analyzed corpus. In such a situ-
ation, which would be the best strategy to take?
We carried out a simulation experiment in
which there was no annotated corpus; instead we
began to construct a new one. We used general
articles from the Kyoto Text Corpus and tried the
following three strategies: (a) Random (actually,
KG0 was used), (b) Longest first (I-Long), and (c)
maximizing vocabulary size first (I-VSort). Three
corpora were prepared by these strategies. Table
6 also shows the corpora information. In this ex-
periment, the corpora were balanced with respect
to the number of dependencies. We used CaboCha
with these corpora and tested them with K-mag,
ED0, and IPAL0. Table 10 shows the results of
the experiment.
</bodyText>
<page confidence="0.997216">
639
</page>
<table confidence="0.9997254">
K-mag ED0 IPAL0
Corpus Acc. (%) S-acc. (%) Acc. (%) S-acc. (%) Acc. (%) s-acc(%)
Random (KG0) 87.87 49.69 90.17 53.64 97.76 93.15
I-Long 87.41 49.28 90.11 52.96 97.66 92.94
I-VSort 87.92 50.31 90.14 53.86 97.72 93.06
</table>
<tableCaption confidence="0.991684">
Table 10: Results of initial situation experiment
</tableCaption>
<table confidence="0.999863125">
Learning corpus Acc. (%) S-acc. (%)
KG1 89.60 49.89
KG1+LONG 89.99 51.25
KG1+Vsort 89.97 51.31
KG1+UDep 89.98 51.39
KG1+ADist 89.98 51.01
KG1+Chrono 89.86 51.09
KG1+Rand 89.95 51.20
</table>
<tableCaption confidence="0.986796">
Table 9: Analyzed results of KG0 (which is the
</tableCaption>
<bodyText confidence="0.958134">
same domain and has almost the same average
sentence length) with these learning corpora
</bodyText>
<sectionHeader confidence="0.999677" genericHeader="method">
4 Discussion
</sectionHeader>
<subsectionHeader confidence="0.99958">
4.1 Error analysis
</subsectionHeader>
<bodyText confidence="0.999925215686275">
To analyze corpora, we employed the dependency
analyzer CaboCha, an SVM-based system. In gen-
eral, when one attempts to solve a classification
problem with kernel functions, it is difficult to
know the kernel function that best fits the prob-
lem. To date, second- and third-degree polynomial
kernels have been empirically used in Japanese de-
pendency analysis with SVMs.
In the biased evaluation (the test corpus was the
learning corpus), the third-degree polynomial ker-
nel produced very accurate results, almost 100%.
On the other hand, in the open test, however, the
third-degree polynomial kernel did not produce re-
sults as good as the second-degree one. We con-
clude from these results that the third-degree poly-
nomial kernel suffered the over-fitting problem.
The second-degree polynomial kernel produced
on accuracy of almost 94% in the biased evalua-
tion, and this can be considered as the upper bound
for the second degree polynomial kernel to ana-
lyze Japanese dependency structure. The accuracy
was stable when we adjusted the soft-margin pa-
rameter of the SVM. However, there were several
annotation errors in the corpus. Thus, if we cor-
rect such annotation errors, the accuracy would
improve.
Table 4 indicates that case elements consisting
of nouns and case markers were frequently mis-
analyzed. From a grammatical point of view, a
case element should depend on a verb. However,
the number of relations between verbs and case el-
ements is combinatorial explosion. Thus, we can
conclude that the learning data were not sufficient
for relations between verbs and case elements to
analyze unseen relations.
On the other hand, in Table 4, verbs take many
places in comparison to their distribution in the
test set corpus. These verbs tend to form conjunc-
tive structures and it is known that analyzing con-
junctive structure is difficult (Kurohashi and Na-
gao, 1994b). Particularly when a verb is a head of
an adverbial clause, it seems very difficult to de-
tect a head bunsetu, which is modified by the verb.
From Table 5, we can conclude that the ana-
lyzed errors centered on short-distance relations;
the analyzer especially tends to mis-analyze the
correct distance of two as one. Typical cases
of such mis-analysis are “N1-no N2-no N3” and
“[adnominal clause] N1-no N2.” In some cases, it
is also difficult for humans to analyze these pat-
terns correctly.
</bodyText>
<subsectionHeader confidence="0.995517">
4.2 Selective sampling simulation
</subsectionHeader>
<bodyText confidence="0.9998610625">
The results revealed very small differences be-
tween strategies possibly due to insufficient cor-
pus size. However, there was an overall tendency
that the accuracy depended heavily whether how
many long sentences with very long dependencies
were included in the test set. Table 3 shows a sim-
ple example of this. In the cross-validation tests
the accuracy of the general articles, the average
length of which was 9.769 bunsetu / sentence, was
almost 1% lower than that of the editorial articles,
whose average length was 9.162 bunsetu / sen-
tence. The reason why sentence length controlled
the accuracy was that an error in the long-distance
dependency may have caused other errors in order
to satisfy the condition that dependencies do not
cross each other in Japanese dependencies. Thus,
</bodyText>
<page confidence="0.991966">
640
</page>
<bodyText confidence="0.999980527777778">
many errors occurred in longer sentences. To im-
prove the accuracy, it is vital to analyze very long-
distance dependencies correctly.
From Tables 7, 8 and 9, the strategy of longest
first appears good for the expanding situation even
if the average length of the test set is very short like
in IPAL0. However, in the initial situation, since
there is no labeled data, the longest-first strategy
is not a good method. Table 10 shows that the
random strategy (KG0) and the strategy of max-
imizing vocabulary size first (I-VSort) were bet-
ter than the longest-first strategy (I-Long). This
is because the test sets comprised short sentences
and we can imagine that there were dependen-
cies included only in such short sentences. In
other words, the longest-first strategy was heav-
ily biased toward long sentences and the strategy
could not cover the dependencies that were only
included in short sentences.
On the other hand, the number of such depen-
dencies that were only included in short sentences
was quite small, and this number would soon be
saturated when we built a dependency analyzed
corpus. Thus, in the initial situation, the random
strategy was better, whereas after we prepared a
corpus to some extent, the longest-first strategy
would be better because analyzing long sentences
is difficult.
In the case of expansion, the longest-first strat-
egy was good, though we have to consider the ac-
tual time required to annotate such long sentences
because in general longer sentences tend to have
more complex structures and introduce more op-
portunities for ambiguous parses. This means it
is difficult for humans to annotate such long sen-
tences.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="method">
5 Related works
</sectionHeader>
<bodyText confidence="0.999970481481481">
To date, many works on selective sampling were
conducted in the field related to natural language
processing (Fujii et al., 1998; Hwa, 2004; Kamm
and Meyer, 2002; Riccardi and Hakkani-T¨ur,
2005; Ngai and Yarowsky, 2000; Banko and Brill,
2001; Engelson and Dagan, 1996). The basic con-
cepts are the same and it is important to predict the
training utility value of each candidate with high
accuracy. The work most closely related to this
paper is Hwa’s (Hwa, 2004), which proposed a so-
phisticated method for selective sampling for sta-
tistical parsing. However, the experiments carried
out in that paper were done with just one corpus,
WSJ Treebank. The study by Baldridge and Os-
borne (Baldridge and Osborne, 2004) is also very
close to this paper. They used the Redwoods tree-
bank environment (Oepen et al., 2002) and dis-
cussed the reduction in annotation cost by an ac-
tive learning approach.
In this paper, we focused on the analysis of sev-
eral fundamental sampling strategies for building
a Japanese dependency-analyzed corpus. A com-
plete estimating function of training utility value
was not shown in this paper. However, we tested
several strategies with different types of corpora,
and these results can be used to design such a func-
tion for selective sampling.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999977608695652">
This paper discussed several sampling strategies
for Japanese dependency-analyzed corpora, test-
ing them with the Kyoto Text Corpus and the
IPAL corpus. The IPAL corpus was constructed
especially for this study. In addition, although it
was quite small, we prepared the K-mag corpus to
test the strategies. The experimental results using
these corpora revealed that the average length of a
test set controlled the accuracy in case of expan-
sion; thus the longest-first strategy outperformed
other strategies. On the other hand, in the initial
situation, the longest-first strategy was not suitable
for any test set.
The current work points us in several future
directions. First, we shall continue to build
dependency-analyzed corpora. While newspaper
articles may be sufficient for our purpose, other
resources seem still inadequate. Second, while
in this work we focused on analysis using several
fundamental selective strategies for a dependency-
analyzed corpus, it is necessary to provide a func-
tion to build a selective sampling framework to
construct a dependency-analyzed corpus.
</bodyText>
<sectionHeader confidence="0.998717" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.978229444444444">
Jason Baldridge and Miles Osborne. 2004. Active
learning and the total cost of annotation. In Pro-
ceedings ofEMNLP.
Michele Banko and Eric Brill. 2001. Scaling to
very very large corpora for natural language disam-
biguation. In Proceedings of the 39th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-2001), pages 26–33.
David A. Cohn, Les Atlas, and Richard E. Ladner.
</reference>
<page confidence="0.98029">
641
</page>
<reference confidence="0.999834531914894">
1994. Improving generalization with active learn-
ing. Machine Learning, 15(2):201–221.
Sean P. Engelson and Ido Dagan. 1996. Minimizing
manual annotation cost in supervised training from
corpora. In Proceedings of the 34th Annual meeting
ofAssociation for Computational Linguistics, pages
319–326.
Atsushi Fujii, Kentaro Inui, Takenobu Tokunaga, and
Hozumi Tanaka. 1998. Selective sampling for
example-based word sense disambiguation. Com-
putational Linguistics, 24(4):573–598.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253–276.
Teresa M. Kamm and Gerard G. L. Meyer. 2002. Se-
lective sampling of training data for speech recogni-
tion. In Proceedings of Human Language Technol-
ogy.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
CoNLL 2002: Proceedings of the 6th Conference on
Natural Language Learning 2002 (COLING 2002
Post-Conference Workshops), pages 63–69.
Sadao Kurohashi and Makoto Nagao. 1994a. KN
Parser: Japanese dependency/case structure ana-
lyzer. In Proceedings of Workshop on Sharable Nat-
ural Language Resources, pages 48–55.
Sadao Kurohashi and Makoto Nagao. 1994b. A syn-
tactic analysis method of long Japanese sentences
based on the detection of conjunctive structures.
Computational Linguistics, 20(4):507–534.
Grace Ngai and David Yarowsky. 2000. Rule writ-
ing or annotation: Cost-efficient resource usage for
base noun phrase chunking. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics, pages 117–125.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and Thorsten
Brants. 2002. The LinGO Redwoods treebank: Mo-
tivation and preliminary applicatoins. In Proceed-
ings of COLING 2002, pages 1–5.
Giuseppe Riccardi and Dilek Hakkani-T¨ur. 2005. Ac-
tive learning: Theory and applications to automatic
speech recognition. IEEE Transactions on Speech
and Audio Processing, 13(4):504–511.
Manabu Sasano. 2004. Linear-time dependency anal-
ysis for Japanese. In Proceedings of Coling 2004,
pages 8–14.
</reference>
<page confidence="0.998">
642
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.725638">
<title confidence="0.990236">Analysis of Selective Strategies to Build a Dependency-Analyzed Corpus</title>
<author confidence="0.973734">Kiyonori Ohtake</author>
<affiliation confidence="0.997017">National Institute of Information and Communications Technology (NICT), ATR Spoken Language Communication Research Labs.</affiliation>
<address confidence="0.923185">2-2-2 Hikaridai “Keihanna Science City” Kyoto 619-0288 Japan</address>
<email confidence="0.929832">kiyonori.ohtake[at]nict.go.jp</email>
<abstract confidence="0.9903874375">This paper discusses sampling strategies for building a dependency-analyzed corpus and analyzes them with different kinds of corpora. We used the Kyoto Text Corpus, a dependency-analyzed corpus of newspaper articles, and prepared the IPAL corpus, a dependency-analyzed corpus of example sentences in dictionaries, as a new and different kind of corpus. The experimental results revealed that the length of the test set controlled the accuracy and that the longest-first strategy was good for an expanding corpus, but this was not the case when constructing a corpus from scratch.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Miles Osborne</author>
</authors>
<title>Active learning and the total cost of annotation.</title>
<date>2004</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="23585" citStr="Baldridge and Osborne, 2004" startWordPosition="3760" endWordPosition="3763">ural language processing (Fujii et al., 1998; Hwa, 2004; Kamm and Meyer, 2002; Riccardi and Hakkani-T¨ur, 2005; Ngai and Yarowsky, 2000; Banko and Brill, 2001; Engelson and Dagan, 1996). The basic concepts are the same and it is important to predict the training utility value of each candidate with high accuracy. The work most closely related to this paper is Hwa’s (Hwa, 2004), which proposed a sophisticated method for selective sampling for statistical parsing. However, the experiments carried out in that paper were done with just one corpus, WSJ Treebank. The study by Baldridge and Osborne (Baldridge and Osborne, 2004) is also very close to this paper. They used the Redwoods treebank environment (Oepen et al., 2002) and discussed the reduction in annotation cost by an active learning approach. In this paper, we focused on the analysis of several fundamental sampling strategies for building a Japanese dependency-analyzed corpus. A complete estimating function of training utility value was not shown in this paper. However, we tested several strategies with different types of corpora, and these results can be used to design such a function for selective sampling. 6 Conclusion This paper discussed several sampl</context>
</contexts>
<marker>Baldridge, Osborne, 2004</marker>
<rawString>Jason Baldridge and Miles Osborne. 2004. Active learning and the total cost of annotation. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001),</booktitle>
<pages>26--33</pages>
<contexts>
<context position="23115" citStr="Banko and Brill, 2001" startWordPosition="3681" endWordPosition="3684">es is difficult. In the case of expansion, the longest-first strategy was good, though we have to consider the actual time required to annotate such long sentences because in general longer sentences tend to have more complex structures and introduce more opportunities for ambiguous parses. This means it is difficult for humans to annotate such long sentences. 5 Related works To date, many works on selective sampling were conducted in the field related to natural language processing (Fujii et al., 1998; Hwa, 2004; Kamm and Meyer, 2002; Riccardi and Hakkani-T¨ur, 2005; Ngai and Yarowsky, 2000; Banko and Brill, 2001; Engelson and Dagan, 1996). The basic concepts are the same and it is important to predict the training utility value of each candidate with high accuracy. The work most closely related to this paper is Hwa’s (Hwa, 2004), which proposed a sophisticated method for selective sampling for statistical parsing. However, the experiments carried out in that paper were done with just one corpus, WSJ Treebank. The study by Baldridge and Osborne (Baldridge and Osborne, 2004) is also very close to this paper. They used the Redwoods treebank environment (Oepen et al., 2002) and discussed the reduction in</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001), pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Cohn</author>
<author>Les Atlas</author>
<author>Richard E Ladner</author>
</authors>
<title>Improving generalization with active learning.</title>
<date>1994</date>
<booktitle>Machine Learning,</booktitle>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="2756" citStr="Cohn et al., 1994" startWordPosition="420" endWordPosition="423"> for a rule-based analyzer, and to produce an analyzer using machine-learning techniques, it is crucial to construct a dependencyanalyzed corpus. Such a corpus is very useful not only for constructing a dependency analyzer but also for other natural language processing applications. However, building this kind of resource is very expensive and labor-intensive because it is difficult to annotate a large amount of dependencyanalyzed corpus in short time. At present, one promising approach to mitigating the annotation bottleneck problem is to use selective sampling, a variant of active learning (Cohn et al., 1994; Fujii et al., 1998; Hwa, 2004). In general, selective sampling is an interactive learning method in which the machine takes the initiative in selecting unlabeled data for the human to annotate. Under this framework, the system has access to a large pool of unlabeled data, and it has to predict how much it can learn from each candidate in the pool if that candidate is labeled. Most of the experiments that had been carried out in the previous works for selective sampling used an annotated corpus in a limited domain. The most typical corpus is WSJ of Penn Treebank. The reason why the domain was</context>
</contexts>
<marker>Cohn, Atlas, Ladner, 1994</marker>
<rawString>David A. Cohn, Les Atlas, and Richard E. Ladner. 1994. Improving generalization with active learning. Machine Learning, 15(2):201–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean P Engelson</author>
<author>Ido Dagan</author>
</authors>
<title>Minimizing manual annotation cost in supervised training from corpora.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual meeting ofAssociation for Computational Linguistics,</booktitle>
<pages>319--326</pages>
<contexts>
<context position="23142" citStr="Engelson and Dagan, 1996" startWordPosition="3685" endWordPosition="3688"> case of expansion, the longest-first strategy was good, though we have to consider the actual time required to annotate such long sentences because in general longer sentences tend to have more complex structures and introduce more opportunities for ambiguous parses. This means it is difficult for humans to annotate such long sentences. 5 Related works To date, many works on selective sampling were conducted in the field related to natural language processing (Fujii et al., 1998; Hwa, 2004; Kamm and Meyer, 2002; Riccardi and Hakkani-T¨ur, 2005; Ngai and Yarowsky, 2000; Banko and Brill, 2001; Engelson and Dagan, 1996). The basic concepts are the same and it is important to predict the training utility value of each candidate with high accuracy. The work most closely related to this paper is Hwa’s (Hwa, 2004), which proposed a sophisticated method for selective sampling for statistical parsing. However, the experiments carried out in that paper were done with just one corpus, WSJ Treebank. The study by Baldridge and Osborne (Baldridge and Osborne, 2004) is also very close to this paper. They used the Redwoods treebank environment (Oepen et al., 2002) and discussed the reduction in annotation cost by an acti</context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>Sean P. Engelson and Ido Dagan. 1996. Minimizing manual annotation cost in supervised training from corpora. In Proceedings of the 34th Annual meeting ofAssociation for Computational Linguistics, pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
</authors>
<title>Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<marker>Fujii, 1998</marker>
<rawString>Atsushi Fujii, Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka. 1998. Selective sampling for example-based word sense disambiguation. Computational Linguistics, 24(4):573–598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Sample selection for statistical parsing.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="2788" citStr="Hwa, 2004" startWordPosition="428" endWordPosition="429">uce an analyzer using machine-learning techniques, it is crucial to construct a dependencyanalyzed corpus. Such a corpus is very useful not only for constructing a dependency analyzer but also for other natural language processing applications. However, building this kind of resource is very expensive and labor-intensive because it is difficult to annotate a large amount of dependencyanalyzed corpus in short time. At present, one promising approach to mitigating the annotation bottleneck problem is to use selective sampling, a variant of active learning (Cohn et al., 1994; Fujii et al., 1998; Hwa, 2004). In general, selective sampling is an interactive learning method in which the machine takes the initiative in selecting unlabeled data for the human to annotate. Under this framework, the system has access to a large pool of unlabeled data, and it has to predict how much it can learn from each candidate in the pool if that candidate is labeled. Most of the experiments that had been carried out in the previous works for selective sampling used an annotated corpus in a limited domain. The most typical corpus is WSJ of Penn Treebank. The reason why the domain was so limited is very simple; corp</context>
<context position="23012" citStr="Hwa, 2004" startWordPosition="3667" endWordPosition="3668">s to some extent, the longest-first strategy would be better because analyzing long sentences is difficult. In the case of expansion, the longest-first strategy was good, though we have to consider the actual time required to annotate such long sentences because in general longer sentences tend to have more complex structures and introduce more opportunities for ambiguous parses. This means it is difficult for humans to annotate such long sentences. 5 Related works To date, many works on selective sampling were conducted in the field related to natural language processing (Fujii et al., 1998; Hwa, 2004; Kamm and Meyer, 2002; Riccardi and Hakkani-T¨ur, 2005; Ngai and Yarowsky, 2000; Banko and Brill, 2001; Engelson and Dagan, 1996). The basic concepts are the same and it is important to predict the training utility value of each candidate with high accuracy. The work most closely related to this paper is Hwa’s (Hwa, 2004), which proposed a sophisticated method for selective sampling for statistical parsing. However, the experiments carried out in that paper were done with just one corpus, WSJ Treebank. The study by Baldridge and Osborne (Baldridge and Osborne, 2004) is also very close to this</context>
</contexts>
<marker>Hwa, 2004</marker>
<rawString>Rebecca Hwa. 2004. Sample selection for statistical parsing. Computational Linguistics, 30(3):253–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teresa M Kamm</author>
<author>Gerard G L Meyer</author>
</authors>
<title>Selective sampling of training data for speech recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of Human Language Technology.</booktitle>
<contexts>
<context position="23034" citStr="Kamm and Meyer, 2002" startWordPosition="3669" endWordPosition="3672">xtent, the longest-first strategy would be better because analyzing long sentences is difficult. In the case of expansion, the longest-first strategy was good, though we have to consider the actual time required to annotate such long sentences because in general longer sentences tend to have more complex structures and introduce more opportunities for ambiguous parses. This means it is difficult for humans to annotate such long sentences. 5 Related works To date, many works on selective sampling were conducted in the field related to natural language processing (Fujii et al., 1998; Hwa, 2004; Kamm and Meyer, 2002; Riccardi and Hakkani-T¨ur, 2005; Ngai and Yarowsky, 2000; Banko and Brill, 2001; Engelson and Dagan, 1996). The basic concepts are the same and it is important to predict the training utility value of each candidate with high accuracy. The work most closely related to this paper is Hwa’s (Hwa, 2004), which proposed a sophisticated method for selective sampling for statistical parsing. However, the experiments carried out in that paper were done with just one corpus, WSJ Treebank. The study by Baldridge and Osborne (Baldridge and Osborne, 2004) is also very close to this paper. They used the </context>
</contexts>
<marker>Kamm, Meyer, 2002</marker>
<rawString>Teresa M. Kamm and Gerard G. L. Meyer. 2002. Selective sampling of training data for speech recognition. In Proceedings of Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In CoNLL 2002: Proceedings of the 6th Conference on Natural Language Learning</booktitle>
<pages>63--69</pages>
<contexts>
<context position="5498" citStr="Kudo and Matsumoto, 2002" startWordPosition="861" endWordPosition="864">f the articles were analyzed by morphological analyzer JUMAN and dependency analyzer KNP1. After that, the analyzed results were manually corrected. Kyoto Text Corpus version 4.0 is now available, holding on additional 5,000 annotated sentences in the corpus to version 3.0 for case relations, anaphoric relations, omission information and co-reference information2. The original POS system used in the Kyoto Text Corpus is JUMAN’s POS system. We converted the POS system used in the Kyoto Text Corpus into ChaSen’s POS system because we used ChaSen, a Japanese morphological analyzer, and CaboCha3 (Kudo and Matsumoto, 2002), a dependency analyzer incorporating SVMs, as a state-ofthe art corpus-based Japanese dependency structure analyzer that prefers ChaSen’s POS system to that of JUMAN. In addition, we modified some 1http://www.kc.t.u-tokyo.ac.jp/ nl-resource 2http://www.kc.t.u-tokyo.ac.jp/ nl-resource/corpus.html 3http://chasen.org/˜taku/ software/cabocha/ bunsetu segmentations because there were several inconsistencies in bunsetu segmentation. Table 1 shows the details of the Kyoto Text Corpus. Kyoto Text Corpus (General) (Editorial) # of sentences 19,669 18,714 # of bunsetu 192,154 171,461 # of morphemes 542</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In CoNLL 2002: Proceedings of the 6th Conference on Natural Language Learning 2002 (COLING 2002 Post-Conference Workshops), pages 63–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>KN Parser: Japanese dependency/case structure analyzer.</title>
<date>1994</date>
<booktitle>In Proceedings of Workshop on Sharable Natural Language Resources,</booktitle>
<pages>48--55</pages>
<contexts>
<context position="7788" citStr="Kurohashi and Nagao, 1994" startWordPosition="1184" endWordPosition="1187"> of several selective strategies for a Japanese dependency-analyzed corpus. First, we briefly introduce Japanese dependency structure. Second, we carry out basic experiments with our dependency-analyzed corpora and analyze the errors. Finally, we conduct simulations to 636 ascertain the fundamental characteristics of these strategies. 3.1 Japanese dependency structure The Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetu segments. Conventional methods of dependency analysis have assumed the following three syntactic constraints (Kurohashi and Nagao, 1994a): 1. All dependencies are directed from left to right. 2. Dependencies do not cross each other. 3. Each bunsetu segment, except the last one, depends on only one bunsetu segment. Figure 1 shows examples of Japanese dependency structure. Jack-wa Kim-ni atsui hon-o okutta Jack to Kim thick a book presented (Jack presented a thick book to Kim.) SVM of CaboCha, and tested with second- and third-degree polynomial kernels. The input data for each test were correct for morphological analysis and bunsetu segmentation, though in practical situations we have to expect some morphological analysis error</context>
<context position="19895" citStr="Kurohashi and Nagao, 1994" startWordPosition="3148" endWordPosition="3152">nsisting of nouns and case markers were frequently misanalyzed. From a grammatical point of view, a case element should depend on a verb. However, the number of relations between verbs and case elements is combinatorial explosion. Thus, we can conclude that the learning data were not sufficient for relations between verbs and case elements to analyze unseen relations. On the other hand, in Table 4, verbs take many places in comparison to their distribution in the test set corpus. These verbs tend to form conjunctive structures and it is known that analyzing conjunctive structure is difficult (Kurohashi and Nagao, 1994b). Particularly when a verb is a head of an adverbial clause, it seems very difficult to detect a head bunsetu, which is modified by the verb. From Table 5, we can conclude that the analyzed errors centered on short-distance relations; the analyzer especially tends to mis-analyze the correct distance of two as one. Typical cases of such mis-analysis are “N1-no N2-no N3” and “[adnominal clause] N1-no N2.” In some cases, it is also difficult for humans to analyze these patterns correctly. 4.2 Selective sampling simulation The results revealed very small differences between strategies possibly d</context>
</contexts>
<marker>Kurohashi, Nagao, 1994</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1994a. KN Parser: Japanese dependency/case structure analyzer. In Proceedings of Workshop on Sharable Natural Language Resources, pages 48–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="7788" citStr="Kurohashi and Nagao, 1994" startWordPosition="1184" endWordPosition="1187"> of several selective strategies for a Japanese dependency-analyzed corpus. First, we briefly introduce Japanese dependency structure. Second, we carry out basic experiments with our dependency-analyzed corpora and analyze the errors. Finally, we conduct simulations to 636 ascertain the fundamental characteristics of these strategies. 3.1 Japanese dependency structure The Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetu segments. Conventional methods of dependency analysis have assumed the following three syntactic constraints (Kurohashi and Nagao, 1994a): 1. All dependencies are directed from left to right. 2. Dependencies do not cross each other. 3. Each bunsetu segment, except the last one, depends on only one bunsetu segment. Figure 1 shows examples of Japanese dependency structure. Jack-wa Kim-ni atsui hon-o okutta Jack to Kim thick a book presented (Jack presented a thick book to Kim.) SVM of CaboCha, and tested with second- and third-degree polynomial kernels. The input data for each test were correct for morphological analysis and bunsetu segmentation, though in practical situations we have to expect some morphological analysis error</context>
<context position="19895" citStr="Kurohashi and Nagao, 1994" startWordPosition="3148" endWordPosition="3152">nsisting of nouns and case markers were frequently misanalyzed. From a grammatical point of view, a case element should depend on a verb. However, the number of relations between verbs and case elements is combinatorial explosion. Thus, we can conclude that the learning data were not sufficient for relations between verbs and case elements to analyze unseen relations. On the other hand, in Table 4, verbs take many places in comparison to their distribution in the test set corpus. These verbs tend to form conjunctive structures and it is known that analyzing conjunctive structure is difficult (Kurohashi and Nagao, 1994b). Particularly when a verb is a head of an adverbial clause, it seems very difficult to detect a head bunsetu, which is modified by the verb. From Table 5, we can conclude that the analyzed errors centered on short-distance relations; the analyzer especially tends to mis-analyze the correct distance of two as one. Typical cases of such mis-analysis are “N1-no N2-no N3” and “[adnominal clause] N1-no N2.” In some cases, it is also difficult for humans to analyze these patterns correctly. 4.2 Selective sampling simulation The results revealed very small differences between strategies possibly d</context>
</contexts>
<marker>Kurohashi, Nagao, 1994</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1994b. A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures. Computational Linguistics, 20(4):507–534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>David Yarowsky</author>
</authors>
<title>Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>117--125</pages>
<contexts>
<context position="23092" citStr="Ngai and Yarowsky, 2000" startWordPosition="3677" endWordPosition="3680">se analyzing long sentences is difficult. In the case of expansion, the longest-first strategy was good, though we have to consider the actual time required to annotate such long sentences because in general longer sentences tend to have more complex structures and introduce more opportunities for ambiguous parses. This means it is difficult for humans to annotate such long sentences. 5 Related works To date, many works on selective sampling were conducted in the field related to natural language processing (Fujii et al., 1998; Hwa, 2004; Kamm and Meyer, 2002; Riccardi and Hakkani-T¨ur, 2005; Ngai and Yarowsky, 2000; Banko and Brill, 2001; Engelson and Dagan, 1996). The basic concepts are the same and it is important to predict the training utility value of each candidate with high accuracy. The work most closely related to this paper is Hwa’s (Hwa, 2004), which proposed a sophisticated method for selective sampling for statistical parsing. However, the experiments carried out in that paper were done with just one corpus, WSJ Treebank. The study by Baldridge and Osborne (Baldridge and Osborne, 2004) is also very close to this paper. They used the Redwoods treebank environment (Oepen et al., 2002) and dis</context>
</contexts>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>Grace Ngai and David Yarowsky. 2000. Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 117–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Kristina Toutanova</author>
<author>Stuart Shieber</author>
<author>Christopher Manning</author>
<author>Dan Flickinger</author>
<author>Thorsten Brants</author>
</authors>
<title>The LinGO Redwoods treebank: Motivation and preliminary applicatoins.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>1--5</pages>
<contexts>
<context position="23684" citStr="Oepen et al., 2002" startWordPosition="3778" endWordPosition="3781">05; Ngai and Yarowsky, 2000; Banko and Brill, 2001; Engelson and Dagan, 1996). The basic concepts are the same and it is important to predict the training utility value of each candidate with high accuracy. The work most closely related to this paper is Hwa’s (Hwa, 2004), which proposed a sophisticated method for selective sampling for statistical parsing. However, the experiments carried out in that paper were done with just one corpus, WSJ Treebank. The study by Baldridge and Osborne (Baldridge and Osborne, 2004) is also very close to this paper. They used the Redwoods treebank environment (Oepen et al., 2002) and discussed the reduction in annotation cost by an active learning approach. In this paper, we focused on the analysis of several fundamental sampling strategies for building a Japanese dependency-analyzed corpus. A complete estimating function of training utility value was not shown in this paper. However, we tested several strategies with different types of corpora, and these results can be used to design such a function for selective sampling. 6 Conclusion This paper discussed several sampling strategies for Japanese dependency-analyzed corpora, testing them with the Kyoto Text Corpus an</context>
</contexts>
<marker>Oepen, Toutanova, Shieber, Manning, Flickinger, Brants, 2002</marker>
<rawString>Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christopher Manning, Dan Flickinger, and Thorsten Brants. 2002. The LinGO Redwoods treebank: Motivation and preliminary applicatoins. In Proceedings of COLING 2002, pages 1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Riccardi</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Active learning: Theory and applications to automatic speech recognition.</title>
<date>2005</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>13</volume>
<issue>4</issue>
<marker>Riccardi, Hakkani-T¨ur, 2005</marker>
<rawString>Giuseppe Riccardi and Dilek Hakkani-T¨ur. 2005. Active learning: Theory and applications to automatic speech recognition. IEEE Transactions on Speech and Audio Processing, 13(4):504–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Sasano</author>
</authors>
<title>Linear-time dependency analysis for Japanese.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>8--14</pages>
<contexts>
<context position="2085" citStr="Sasano, 2004" startWordPosition="315" endWordPosition="316">nalyzers. If we take a machine-learning approach, it is important to consider what features are used. However, there are several machine-learning techniques, such as support vector machines (SVMs) with a kernel function, that have strong generalization ability and are very robust for choosing the right features. If we use such machine-learning techniques, we will be free from choosing a feature set because it will be possible to use all possible features with little or no decline in performance. Actually, Sasano tried to expand the feature set for a Japanese dependency analyzer using SVMs in (Sasano, 2004), with a small improvement in accuracy. To write rules for a rule-based analyzer, and to produce an analyzer using machine-learning techniques, it is crucial to construct a dependencyanalyzed corpus. Such a corpus is very useful not only for constructing a dependency analyzer but also for other natural language processing applications. However, building this kind of resource is very expensive and labor-intensive because it is difficult to annotate a large amount of dependencyanalyzed corpus in short time. At present, one promising approach to mitigating the annotation bottleneck problem is to </context>
</contexts>
<marker>Sasano, 2004</marker>
<rawString>Manabu Sasano. 2004. Linear-time dependency analysis for Japanese. In Proceedings of Coling 2004, pages 8–14.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>