<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000951">
<title confidence="0.987553">
Shallow Discourse Parsing Using Constituent Parsing Tree∗
</title>
<author confidence="0.994119">
Changge Chen1,2, Peilu Wang1,2, Hai Zhao1,2†
</author>
<affiliation confidence="0.9984884">
1Department of Computer Science and Engineering,
Shanghai Jiao Tong University, Shanghai 200240, China
2Key Laboratory of Shanghai Education Commission
for Intelligent Interaction and Cognitive Engineering,
Shanghai Jiao Tong University, Shanghai 200240, China
</affiliation>
<email confidence="0.995836">
{changge.chen.cc,plwang1990}@gmail.com,zhaohai@cs.sjtu.edu.cn
</email>
<sectionHeader confidence="0.993843" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997575">
This paper describes our system in the
closed track of the shared task of CoNLL-
2015. We formulize the discourse pars-
ing work into a series of classification sub-
tasks. The official evaluation shows that
the proposed framework can give compet-
itive results and we give a few discussions
over latent improvement as well.
</bodyText>
<sectionHeader confidence="0.946624" genericHeader="method">
1 System Overview
</sectionHeader>
<bodyText confidence="0.999787333333333">
We design our shallow discourse parser as a se-
quential pipeline to mimic the annotation proce-
dure as the Penn Discourse Treebank (we will use
PDTB instead in the rest of this paper) annotator
(Lin et al., 2014). Figure 1 gives the pipeline of
the system. The system can be roughly split into
two parts: the explicit and the non-explicit. The
first part consists of three steps, which sequen-
tially are Explicit Classifier, Explicit Argument
Labeler, and Explicit Sense Classifier. While the
non-explicit part consists of Filter, Non-explicit
and Non-explicit Sense Classifiers. Non-explicit
relations include ‘Implicit’, ‘AltLex’, ‘EntRel’, but
not ‘NoRel’.
We adopt an adapted maximum entropy model
as the classification algorithm for every steps. Our
system only exploits resources provided by the or-
ganizer.
</bodyText>
<footnote confidence="0.669705866666667">
∗This work of C. Chen, P. Wang, and H. Zhao was
supported in part by the National Natural Science Foun-
dation of China under Grants 60903119, 61170114, and
61272248, the National Basic Research Program of China
under Grant 2013CB329401, the Science and Technol-
ogy Commission of Shanghai Municipality under Grant
13511500200, the European Union Seventh Framework Pro-
gram under Grant 247619, the Cai Yuanpei Program (CSC
fund 201304490199, 201304490171, and the art and sci-
ence interdiscipline funds of Shanghai Jiao Tong University
(a study on mobilization mechanism and alerting threshold
setting for online community, and media image and psychol-
ogy evaluation: a computational intelligence approach under
Grant 14X190040031(14JCRZ04).
†Corresponding author
</footnote>
<figureCaption confidence="0.999551">
Figure 1: Pipeline of the system
</figureCaption>
<table confidence="0.982208">
Explicit connectives in train set 14722
Level-order Scan 13911
</table>
<tableCaption confidence="0.999676">
Table 1: Performance of level-order scan
</tableCaption>
<bodyText confidence="0.9999091875">
We first give a brief introduction over each step
of the entire system as the following. After the
Explicit Classifier detects explicit connectives, the
Explicit Argument Labeler then prunes and clas-
sifies the ‘Arg1’ and ‘Arg2’ of the detected con-
nective. Then, Explicit Sense Classifier integrates
results of previous two steps when trying to dis-
tinguish different senses. The second part of the
system starts with filtering out obvious false cases.
Then the Non-explicit classifier classifies the non-
relations into three classes, i.e., ‘Implicit’, ‘Al-
tLex’, ‘EntRel’. Finally, the Non-explicit Sense
classifier determines the sense of the non-explicit
relation. In the last two steps, we take the ‘En-
tRel’ as a sense of implicit relation, which we will
explain later.
</bodyText>
<page confidence="0.995831">
37
</page>
<note confidence="0.913219">
Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 37–41,
Beijing, China, July 26-31, 2015. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.999699">
Tag P R F
Arg1 0.7748 0.7544 0.7645
Arg2 0.8102 0.9658 0.8812
NULL 0.9922 0.9781 0.9851
</table>
<tableCaption confidence="0.998772">
Table 2: Performance of Argument Labler
</tableCaption>
<sectionHeader confidence="0.955411" genericHeader="method">
2 System Modules
</sectionHeader>
<subsectionHeader confidence="0.934126">
2.1 Explicit Part
</subsectionHeader>
<bodyText confidence="0.994836">
In this part, our parser extracts the explicit rela-
tions. An explicit example is given below.
He added that ”having just one firm do this
isn’t going to mean a hill of beans. But if this
prompts others to consider the same thing,
then it may become much more important.
‘Arg1’ is shown in italic, and ‘Arg2’ is shown
in bold. The discourse connective is underlined
and the sense of this explicit relation is ‘Compari-
son.Concession’.
</bodyText>
<subsectionHeader confidence="0.905445">
2.1.1 Explicit Classifier
</subsectionHeader>
<bodyText confidence="0.995475111111111">
There are 100 explicit connectives in PDTB anno-
tation (Prasad et al., 2008). However, some con-
nectives, e.g., ‘and’, do not express a discourse re-
lation. We use a level-order traverse to scan every
node in the constituent parse tree to select the con-
nective candidates. This method gives us a high
recall in the train set as shown in Table 1.
Seven features are considered (Pitler and
Nenkova, 2009):
</bodyText>
<listItem confidence="0.984497">
a) Self Category The highest dominated node
which covers the connective.
b) Parent Caterogy The category of the parent of
the self category.
c) Left Sibling Category The syntactic category
of immediate left sibling of the self-category. It
would be ‘NONE’ if the connective is the leftmost
node.
d) Right Sibling Category The immediate right
sibling of the self category. It also would be as-
signed ‘NONE’ if the self-category has been the
rightmost node.
e) VP Existence We set a binary feature to indi-
cate whether the right sibling contains a VP.
f) Connective In addition to those features pro-
posed by Pilter and Nenvoda, we introduce con-
nective feature. The potential connective itself
would be a strong sign of its function. A few of
discourse connectives that are deterministic. For
</listItem>
<bodyText confidence="0.989884727272727">
example, ’in addition’ will always be ’Expan-
sion.Conjunction’.
Maximum Entropy classifier has shown good
performance in various previous works (Wang et
al., 2014; Jia et al., 2013; Zhao and Kit, 2008).
Based on these features, we trained a Maximum
Entropy classifier. In order to check the perfor-
mance of the classifier only, we evaluate the clas-
sifier on connective candidates that selected by a
level-order traverse. This gives 93.87% accuracy
and 90.1% F1 score on dev set.
</bodyText>
<subsectionHeader confidence="0.606052">
2.1.2 Explicit Argument Labeler
</subsectionHeader>
<bodyText confidence="0.9985809">
With all explicit connectives detected, we exploit
a constituent-based approach to perform argument
labeling (Kong et al., 2014). Along the path from
the connective node to the root node in the con-
stituent parse tree, all the siblings of every node
on the path are selected as candidates for ‘Arg1’
and ‘Arg2’. For these candidates, we compare
them with PDTB to label them as ‘Arg1’, ‘Arg2’,
or ‘NULL’. However, this argument prune strategy
focuses on intra sentence. In addition, Kong et al.
unified the intra- and inter-sentence cases by treat-
ing the immediate preceding sentence as a spe-
cial constituent. Based on our empirical results,
the inter-sentences only contribute to the augment
candidate Arg1. Kong et al. also reported a very
high recalls (80-90%) on ‘Arg1’ and ‘Arg2’ ex-
traction, though our re-implementation only re-
ceive recalls 37.5% and 51.3% of the ‘Arg1’ and
‘Arg2’, respectively. And about 87.75% of all the
pruning out constituents are labeled as ‘NULL’.
Similar to treating the immediate preceding sen-
tence as ‘Arg1’ candidate, we take the remaining
part of the sentence that is adjacent to the connec-
tive as ‘Arg2’ candidate. This approach gives a
boost in ‘Arg2’ recall, as high as 93.1%.
We extract features from constituent parser tree
(Zhao and Kit, 2008; Zhao et al., 2009). The ex-
tracted features can be divided into two parts. The
first part captures information about the connec-
tive itself:
</bodyText>
<listItem confidence="0.967327111111111">
a) Con-str Case-sensitive string of the given con-
nective.
b) Con-Lstr The lowercase string of the connec-
tive.
c) Con-iLSib Number of left sibling of the con-
nective.
d) Con-iRSib Number of right sibling of the con-
nective.
The second part consists of features from the
</listItem>
<page confidence="0.925046">
38
</page>
<listItem confidence="0.751511333333333">
syntactic constituent:
e) NT-CtxContext of the constituent. We use POS
combination of the constituent, its parent, left sib-
ling and right sibling to represent the context.
f) Con-NT-Path The path from the parent of the
connective to the node of the constituent.
g) Con-NT-Position The positive of the con-
stituent relative to the connective: left, right, or
previous.
</listItem>
<bodyText confidence="0.9999378125">
After the parser categories all the candidates
constituent into ‘Arg1’, ‘Arg2’, and ’NULL’, Kong
et al. adopted a Linear Integer Programming to
impose constraints that the number of ‘Arg1’ and
‘Arg2’ should no less than one, The extracted
arguments should not overlap with the connec-
tive. Our experiments also show that some con-
straints are useless. For example, constraint that
the pruned out candidates should not overlap with
the connective. The pruning algorithm considers
the siblings of the node along the path, there is no
chance that the pruned out candidate would over-
lap with the connective node.
Without considering the error propagated by the
pruning process, the argument labeler gives results
as Table 2.
</bodyText>
<subsectionHeader confidence="0.869309">
2.1.3 Explicit Sense Classifier
</subsectionHeader>
<bodyText confidence="0.999900833333333">
In this part we only take a naive approach that take
the most frequent sense of the detected explicit
connective. A better approach needs to build a
sense classifier with syntactic features of the con-
nective such as POS, and position and length of
arguments.
</bodyText>
<subsectionHeader confidence="0.960968">
2.2 Non-Explicit Part
</subsectionHeader>
<bodyText confidence="0.999971625">
This part is based on the result of explicit part.
We assume that Explicit and Non-explicit relations
cannot exist in the same sentence simultaneously.
So we take out sentences which have been labeled
as Explicit in the first part. Then, we take all the
adjacent sentences left in the article as candidate
implicit relations. There are 13,155 implicit rela-
tions given in the train set.
</bodyText>
<subsectionHeader confidence="0.560161">
2.2.1 Filter
</subsectionHeader>
<bodyText confidence="0.999944">
Apart form filtering out the explicit connective, we
also discard sentences between two paragraphs.
After these two filtering steps we get 8,728 non-
explicit relations.
</bodyText>
<subsubsectionHeader confidence="0.613566">
2.2.2 Non-explicit Classifier
</subsubsectionHeader>
<bodyText confidence="0.99936375">
At first glance, we should build a classifier that can
distinguish the relations ‘Implicit’, ‘AltLex’, and
‘EntRel’. We give the distribution of each relations
in the train set in Table 3.
</bodyText>
<table confidence="0.997239">
Implicit AltLex EntRel
# 13,155 524 4,133
% 73.85 2.94 23.2
</table>
<tableCaption confidence="0.975741">
Table 3: Distribution of Non-Explicit Relations in
train set
</tableCaption>
<table confidence="0.999533666666667">
Sense # %
*Ent Rel 4,133 23.2
*Expn..Conj. 3,321 18.64
*Expn..Rest. 2,543 14.28
*Cont. Cause. Reason 2,135 11.99
*Comp..Cont. 1,646 9.24
*Cont..Cause.Result 1,519 8.53
* Expn..Inst. 1,165 6.54
*Temp..Asyn..Prec. 460 2.58
*Comp..Conc. 197 1.11
*Temp..Sync. 169 0.95
Comparison 145 0.81
*Temp..Asyn..Suc. 143 0.8
*Expn..Alt..Chosen alt. 142 0.8
Expn. 75 0.42
*Expn.Alt. 11 0.06
*Cont.Cond. 4 0.02
*Expn..Exc. 2 0.01
Cont..Cause 1 0.00
Temp. 1 0.00
Cont. 1 0.00
</table>
<tableCaption confidence="0.987325">
Table 4: Distribution of Non-explicit Senses in
train set.∗
</tableCaption>
<bodyText confidence="0.999977875">
We can see the ‘AltLex’ only covers about
2.94%, which is relatively negligible comparing
with ‘Implicit’( 73.85%) and ‘EntRel’(23.2%). So
we decide to focus only on the latter two relations,
and the classifier only works on these two rela-
tions. Instead of building a single classifier, we set
all the non-explicit relations as ‘Implicit’ here, and
view ‘EntRel’ as a sense of implicit relation.
</bodyText>
<subsectionHeader confidence="0.838275">
2.2.3 Non-explicit Sense Classifier
</subsectionHeader>
<bodyText confidence="0.9939015">
The distribution of all senses in the train set is
given in Table 4. The current shared task only asks
</bodyText>
<reference confidence="0.940007666666667">
∗Abbreviations: Expansionz(Expn.), Conjunc-
tion(Conj.), Restatement(Rest.), Contingency(Cont.), In-
stantiation(Inst.), Temporal(Temp.), Asynchronous(Asyn.),
Precedence(Prec.), Comparison(Comp.),Concession(Conc.),
Synchrony(Sync.), Asynchronous(Asyn.), Succession(Suc.),
alternative(alt.), Condition(Cond.), Exception(Exc.)
</reference>
<page confidence="0.998978">
39
</page>
<table confidence="0.999808625">
Blind Test Dev
Explicit Conn. F 0.8168 0.7867 0.8609
Explicit Conn. P 0.8117 0.7784 0.8528
Explicit Conn. R 0.8219 0.7952 0.8691
Arg1 Arg2 Ext. F 0.047 0.0457 0.0681
Arg1 Arg2 Ext. P 0.0453 0.0424 0.0643
Arg1 Arg2 Ext. R 0.0488 0.0495 0.0724
Arg1 Ext. F 0.0629 0.0657 0.0845
Arg1 Ext. P 0.0607 0.061 0.0798
Arg1 Ext. R 0.0653 0.0712 0.0898
Arg2 Ext. F 0.2182 0.2166 0.264
Arg2 Ext. P 0.2104 0.2011 0.2492
Arg2 Ext. R 0.2266 0.2347 0.2806
Parser F 0.0358 0.0443 0.0655
Parser P 0.0346 0.0411 0.0618
Parser R 0.0372 0.048 0.0696
</table>
<tableCaption confidence="0.998358">
Table 5: Detailed Results
</tableCaption>
<bodyText confidence="0.999943142857143">
us to detect 15 senses, which are marked by star.
We can see that senses below the double line ac-
count less than 1%. Based on this observation, we
decide only consider those significant sense.
What’s more, we can see that the most frequent
sense is ‘EntRel’. This leads to our another strat-
egy: At first we set all the candidate non-explicit
senses as ‘Implicit’ and view ‘EntRel’ as a sense.
Then when the Non-explicit Sense Classifier la-
bels the sense as ‘EntRel’, the Non-explicit Sense
Classifier re-labels the type of corresponding rela-
tion as ‘EntRel’.
Previous studies attempt to predict the missing
connective of implicit relations (Zhou et al., 2010;
Pitler et al., 2009) . It has been shown that con-
nective is very predictive for the sense of the rela-
tion (Kong et al., 2014). Consequently, we can get
the intuition that features for predicting the miss-
ing connective are also useful for predicting the
implicit sense. Thus we use word-pair features to
train our Non-explicit Sense Classifier:
</bodyText>
<listItem confidence="0.936021777777778">
b) Arg1Last The last word of ‘Arg1’.
a) Arg1First The first word of ‘Arg1’.
c) Arg2First The first word of ‘Arg2’.
d) Arg2Last The last word of ‘Arg2’.
e) FirstS Arg1First + Arg2First.
f) LastS Arg1Last + Arg2Last.
g) Arg1First3 The first three words of ‘Arg1’.
h) Arg1Last3 The last three words of ‘Arg2’.
i) Arg2First3 The first three words of ‘Arg2’.
</listItem>
<sectionHeader confidence="0.99821" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999981571428571">
A comprehensive evaluation towards our parser
has been given in Table 5. We can see that the first
step of our parser, i.e., Explicit Classifier, does a
moderate job. However, our work to extract the
‘Arg1’ and ‘Arg2’ cannot be regarded as success.
Since our parser is in a sequential mode, all steps
after that receive negative impacts.
</bodyText>
<sectionHeader confidence="0.996341" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999968">
In this paper, a sequential system is proposed to do
shallow discourse parsing. We demonstrate that
the whole task can be worked out by a pipeline
consists of several subtasks.
In future, we will tune our Argument Labeler in
order to gain a better result in the explicit part .
</bodyText>
<sectionHeader confidence="0.986535" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.915350666666667">
Zhongye Jia, Peilu Wang, and Hai Zhao. 2013. Gram-
matical error correction as multiclass classification
with single model. pages 74–81, Sofia, Bulgaria,
August.
Fang Kong, Hwee Tou Ng, and Guodong Zhou. 2014.
A constituent-based approach to argument labeling
with joint inference in discourse parsing. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing, pages 68–77,
</bodyText>
<reference confidence="0.98213375">
Doha, Qatar, October.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20:151–184, April.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 13–16, Suntec, Singapore, Au-
gust.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 683–691,
Suntec, Singapore, August.
Rashmi Prasad, Dinesh Nikhil, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The Penn discourse tree-
bank 2.0. In The International Conference on Lan-
guage Resources and Evaluation, Marrakech, Mo-
rocco, May.
</reference>
<page confidence="0.966963">
40
</page>
<reference confidence="0.999738">
Peilu Wang, Zhongye Jia, and Hai Zhao. 2014. Gram-
matical error detection and correction using a single
maximum entropy model. pages 74–82, Baltimore,
Maryland, USA, July.
Hai Zhao and Chunyu Kit. 2008. Parsing syntac-
tic and semantic dependencies with two single-stage
maximum entropy models. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, pages 203–207, Manchester, Au-
gust.
Hai Zhao, Wenliang Chen, Jun’ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009. Multi-
lingual dependency learning: Exploiting rich fea-
tures for tagging syntactic and semantic dependen-
cies. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task, pages 61–66, Boulder, Colorado,
June.
Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian
Su, and Chew Lim Tan. 2010. Predicting discourse
connectives for implicit discourse relation recogni-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 1507–
1514, Beijing, China, August.
</reference>
<page confidence="0.999448">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.088603">
<title confidence="0.999796">Discourse Parsing Using Constituent Parsing</title>
<author confidence="0.998478">Peilu Hai</author>
<affiliation confidence="0.784166666666667">of Computer Science and Shanghai Jiao Tong University, Shanghai 200240, Laboratory of Shanghai Education</affiliation>
<title confidence="0.866433">for Intelligent Interaction and Cognitive</title>
<author confidence="0.884656">Shanghai Jiao Tong University</author>
<author confidence="0.884656">Shanghai</author>
<abstract confidence="0.984963175">This paper describes our system in the closed track of the shared task of CoNLL- 2015. We formulize the discourse parsing work into a series of classification subtasks. The official evaluation shows that the proposed framework can give competitive results and we give a few discussions over latent improvement as well. 1 System Overview We design our shallow discourse parser as a sequential pipeline to mimic the annotation procedure as the Penn Discourse Treebank (we will use PDTB instead in the rest of this paper) annotator (Lin et al., 2014). Figure 1 gives the pipeline of the system. The system can be roughly split into two parts: the explicit and the non-explicit. The first part consists of three steps, which sequentially are Explicit Classifier, Explicit Argument Labeler, and Explicit Sense Classifier. While the non-explicit part consists of Filter, Non-explicit and Non-explicit Sense Classifiers. Non-explicit include but We adopt an adapted maximum entropy model as the classification algorithm for every steps. Our system only exploits resources provided by the organizer. work of C. Chen, P. Wang, and H. Zhao was supported in part by the National Natural Science Foundation of China under Grants 60903119, 61170114, and 61272248, the National Basic Research Program of China under Grant 2013CB329401, the Science and Technology Commission of Shanghai Municipality under Grant 13511500200, the European Union Seventh Framework Program under Grant 247619, the Cai Yuanpei Program (CSC fund 201304490199, 201304490171, and the art and science interdiscipline funds of Shanghai Jiao Tong University (a study on mobilization mechanism and alerting threshold setting for online community, and media image and psychology evaluation: a computational intelligence approach under</abstract>
<note confidence="0.829585833333333">Grant 14X190040031(14JCRZ04). author Figure 1: Pipeline of the system Explicit connectives in train set 14722 Level-order Scan 13911 Table 1: Performance of level-order scan</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>∗Abbreviations Expansionz</author>
<author>Restatement Conjunction</author>
</authors>
<date></date>
<location>Contingency(Cont.), Instantiation(Inst.), Temporal(Temp.), Asynchronous(Asyn.), Precedence(Prec.), Comparison(Comp.),Concession(Conc.), Synchrony(Sync.), Asynchronous(Asyn.), Succession(Suc.), alternative(alt.), Condition(Cond.), Exception(Exc.) Doha, Qatar,</location>
<marker>Expansionz, Conjunction, </marker>
<rawString>∗Abbreviations: Expansionz(Expn.), Conjunction(Conj.), Restatement(Rest.), Contingency(Cont.), Instantiation(Inst.), Temporal(Temp.), Asynchronous(Asyn.), Precedence(Prec.), Comparison(Comp.),Concession(Conc.), Synchrony(Sync.), Asynchronous(Asyn.), Succession(Suc.), alternative(alt.), Condition(Cond.), Exception(Exc.) Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A PDTB-styled end-to-end discourse parser.</title>
<date>2014</date>
<journal>Natural Language Engineering,</journal>
<pages>20--151</pages>
<contexts>
<context position="980" citStr="Lin et al., 2014" startWordPosition="138" endWordPosition="141"> {changge.chen.cc,plwang1990}@gmail.com,zhaohai@cs.sjtu.edu.cn Abstract This paper describes our system in the closed track of the shared task of CoNLL2015. We formulize the discourse parsing work into a series of classification subtasks. The official evaluation shows that the proposed framework can give competitive results and we give a few discussions over latent improvement as well. 1 System Overview We design our shallow discourse parser as a sequential pipeline to mimic the annotation procedure as the Penn Discourse Treebank (we will use PDTB instead in the rest of this paper) annotator (Lin et al., 2014). Figure 1 gives the pipeline of the system. The system can be roughly split into two parts: the explicit and the non-explicit. The first part consists of three steps, which sequentially are Explicit Classifier, Explicit Argument Labeler, and Explicit Sense Classifier. While the non-explicit part consists of Filter, Non-explicit and Non-explicit Sense Classifiers. Non-explicit relations include ‘Implicit’, ‘AltLex’, ‘EntRel’, but not ‘NoRel’. We adopt an adapted maximum entropy model as the classification algorithm for every steps. Our system only exploits resources provided by the organizer. </context>
</contexts>
<marker>Lin, Ng, Kan, 2014</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A PDTB-styled end-to-end discourse parser. Natural Language Engineering, 20:151–184, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Using syntax to disambiguate explicit discourse connectives in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>13--16</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="4462" citStr="Pitler and Nenkova, 2009" startWordPosition="678" endWordPosition="681">n it may become much more important. ‘Arg1’ is shown in italic, and ‘Arg2’ is shown in bold. The discourse connective is underlined and the sense of this explicit relation is ‘Comparison.Concession’. 2.1.1 Explicit Classifier There are 100 explicit connectives in PDTB annotation (Prasad et al., 2008). However, some connectives, e.g., ‘and’, do not express a discourse relation. We use a level-order traverse to scan every node in the constituent parse tree to select the connective candidates. This method gives us a high recall in the train set as shown in Table 1. Seven features are considered (Pitler and Nenkova, 2009): a) Self Category The highest dominated node which covers the connective. b) Parent Caterogy The category of the parent of the self category. c) Left Sibling Category The syntactic category of immediate left sibling of the self-category. It would be ‘NONE’ if the connective is the leftmost node. d) Right Sibling Category The immediate right sibling of the self category. It also would be assigned ‘NONE’ if the self-category has been the rightmost node. e) VP Existence We set a binary feature to indicate whether the right sibling contains a VP. f) Connective In addition to those features propos</context>
</contexts>
<marker>Pitler, Nenkova, 2009</marker>
<rawString>Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 13–16, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>683--691</pages>
<location>Suntec, Singapore,</location>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 683–691, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Dinesh Nikhil</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind K Joshi</author>
<author>Bonnie L Webber</author>
</authors>
<title>The Penn discourse treebank 2.0.</title>
<date>2008</date>
<booktitle>In The International Conference on Language Resources and Evaluation,</booktitle>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="4138" citStr="Prasad et al., 2008" startWordPosition="621" endWordPosition="624">781 0.9851 Table 2: Performance of Argument Labler 2 System Modules 2.1 Explicit Part In this part, our parser extracts the explicit relations. An explicit example is given below. He added that ”having just one firm do this isn’t going to mean a hill of beans. But if this prompts others to consider the same thing, then it may become much more important. ‘Arg1’ is shown in italic, and ‘Arg2’ is shown in bold. The discourse connective is underlined and the sense of this explicit relation is ‘Comparison.Concession’. 2.1.1 Explicit Classifier There are 100 explicit connectives in PDTB annotation (Prasad et al., 2008). However, some connectives, e.g., ‘and’, do not express a discourse relation. We use a level-order traverse to scan every node in the constituent parse tree to select the connective candidates. This method gives us a high recall in the train set as shown in Table 1. Seven features are considered (Pitler and Nenkova, 2009): a) Self Category The highest dominated node which covers the connective. b) Parent Caterogy The category of the parent of the self category. c) Left Sibling Category The syntactic category of immediate left sibling of the self-category. It would be ‘NONE’ if the connective </context>
</contexts>
<marker>Prasad, Nikhil, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Dinesh Nikhil, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L Webber. 2008. The Penn discourse treebank 2.0. In The International Conference on Language Resources and Evaluation, Marrakech, Morocco, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peilu Wang</author>
<author>Zhongye Jia</author>
<author>Hai Zhao</author>
</authors>
<title>Grammatical error detection and correction using a single maximum entropy model.</title>
<date>2014</date>
<pages>74--82</pages>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="5413" citStr="Wang et al., 2014" startWordPosition="833" endWordPosition="836">ate right sibling of the self category. It also would be assigned ‘NONE’ if the self-category has been the rightmost node. e) VP Existence We set a binary feature to indicate whether the right sibling contains a VP. f) Connective In addition to those features proposed by Pilter and Nenvoda, we introduce connective feature. The potential connective itself would be a strong sign of its function. A few of discourse connectives that are deterministic. For example, ’in addition’ will always be ’Expansion.Conjunction’. Maximum Entropy classifier has shown good performance in various previous works (Wang et al., 2014; Jia et al., 2013; Zhao and Kit, 2008). Based on these features, we trained a Maximum Entropy classifier. In order to check the performance of the classifier only, we evaluate the classifier on connective candidates that selected by a level-order traverse. This gives 93.87% accuracy and 90.1% F1 score on dev set. 2.1.2 Explicit Argument Labeler With all explicit connectives detected, we exploit a constituent-based approach to perform argument labeling (Kong et al., 2014). Along the path from the connective node to the root node in the constituent parse tree, all the siblings of every node on </context>
</contexts>
<marker>Wang, Jia, Zhao, 2014</marker>
<rawString>Peilu Wang, Zhongye Jia, and Hai Zhao. 2014. Grammatical error detection and correction using a single maximum entropy model. pages 74–82, Baltimore, Maryland, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Parsing syntactic and semantic dependencies with two single-stage maximum entropy models.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>203--207</pages>
<location>Manchester,</location>
<contexts>
<context position="5452" citStr="Zhao and Kit, 2008" startWordPosition="841" endWordPosition="844">y. It also would be assigned ‘NONE’ if the self-category has been the rightmost node. e) VP Existence We set a binary feature to indicate whether the right sibling contains a VP. f) Connective In addition to those features proposed by Pilter and Nenvoda, we introduce connective feature. The potential connective itself would be a strong sign of its function. A few of discourse connectives that are deterministic. For example, ’in addition’ will always be ’Expansion.Conjunction’. Maximum Entropy classifier has shown good performance in various previous works (Wang et al., 2014; Jia et al., 2013; Zhao and Kit, 2008). Based on these features, we trained a Maximum Entropy classifier. In order to check the performance of the classifier only, we evaluate the classifier on connective candidates that selected by a level-order traverse. This gives 93.87% accuracy and 90.1% F1 score on dev set. 2.1.2 Explicit Argument Labeler With all explicit connectives detected, we exploit a constituent-based approach to perform argument labeling (Kong et al., 2014). Along the path from the connective node to the root node in the constituent parse tree, all the siblings of every node on the path are selected as candidates for</context>
<context position="7043" citStr="Zhao and Kit, 2008" startWordPosition="1102" endWordPosition="1105"> contribute to the augment candidate Arg1. Kong et al. also reported a very high recalls (80-90%) on ‘Arg1’ and ‘Arg2’ extraction, though our re-implementation only receive recalls 37.5% and 51.3% of the ‘Arg1’ and ‘Arg2’, respectively. And about 87.75% of all the pruning out constituents are labeled as ‘NULL’. Similar to treating the immediate preceding sentence as ‘Arg1’ candidate, we take the remaining part of the sentence that is adjacent to the connective as ‘Arg2’ candidate. This approach gives a boost in ‘Arg2’ recall, as high as 93.1%. We extract features from constituent parser tree (Zhao and Kit, 2008; Zhao et al., 2009). The extracted features can be divided into two parts. The first part captures information about the connective itself: a) Con-str Case-sensitive string of the given connective. b) Con-Lstr The lowercase string of the connective. c) Con-iLSib Number of left sibling of the connective. d) Con-iRSib Number of right sibling of the connective. The second part consists of features from the 38 syntactic constituent: e) NT-CtxContext of the constituent. We use POS combination of the constituent, its parent, left sibling and right sibling to represent the context. f) Con-NT-Path Th</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and semantic dependencies with two single-stage maximum entropy models. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 203–207, Manchester, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Wenliang Chen</author>
<author>Jun’ichi Kazama</author>
<author>Kiyotaka Uchimoto</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Multilingual dependency learning: Exploiting rich features for tagging syntactic and semantic dependencies.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>61--66</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="7063" citStr="Zhao et al., 2009" startWordPosition="1106" endWordPosition="1109">ugment candidate Arg1. Kong et al. also reported a very high recalls (80-90%) on ‘Arg1’ and ‘Arg2’ extraction, though our re-implementation only receive recalls 37.5% and 51.3% of the ‘Arg1’ and ‘Arg2’, respectively. And about 87.75% of all the pruning out constituents are labeled as ‘NULL’. Similar to treating the immediate preceding sentence as ‘Arg1’ candidate, we take the remaining part of the sentence that is adjacent to the connective as ‘Arg2’ candidate. This approach gives a boost in ‘Arg2’ recall, as high as 93.1%. We extract features from constituent parser tree (Zhao and Kit, 2008; Zhao et al., 2009). The extracted features can be divided into two parts. The first part captures information about the connective itself: a) Con-str Case-sensitive string of the given connective. b) Con-Lstr The lowercase string of the connective. c) Con-iLSib Number of left sibling of the connective. d) Con-iRSib Number of right sibling of the connective. The second part consists of features from the 38 syntactic constituent: e) NT-CtxContext of the constituent. We use POS combination of the constituent, its parent, left sibling and right sibling to represent the context. f) Con-NT-Path The path from the pare</context>
</contexts>
<marker>Zhao, Chen, Kazama, Uchimoto, Torisawa, 2009</marker>
<rawString>Hai Zhao, Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Multilingual dependency learning: Exploiting rich features for tagging syntactic and semantic dependencies. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, pages 61–66, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi-Min Zhou</author>
<author>Yu Xu</author>
<author>Zheng-Yu Niu</author>
<author>Man Lan</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Predicting discourse connectives for implicit discourse relation recognition.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1507--1514</pages>
<location>Beijing, China,</location>
<marker>Zhou, Xu, Niu, Lan, Su, Tan, 2010</marker>
<rawString>Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian Su, and Chew Lim Tan. 2010. Predicting discourse connectives for implicit discourse relation recognition. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1507– 1514, Beijing, China, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>