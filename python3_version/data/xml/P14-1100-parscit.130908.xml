<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996596">
Spectral Unsupervised Parsing with Additive Tree Metrics
</title>
<author confidence="0.981698">
Ankur P. Parikh
</author>
<affiliation confidence="0.9849555">
School of Computer Science
Carnegie Mellon University
</affiliation>
<email confidence="0.984758">
apparikh@cs.cmu.edu
</email>
<author confidence="0.976509">
Shay B. Cohen
</author>
<affiliation confidence="0.997761">
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.982743">
scohen@inf.ed.ac.uk
</email>
<author confidence="0.995085">
Eric P. Xing
</author>
<affiliation confidence="0.9849925">
School of Computer Science
Carnegie Mellon University
</affiliation>
<email confidence="0.995766">
epxing@cs.cmu.edu
</email>
<sectionHeader confidence="0.994977" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999813">
We propose a spectral approach for un-
supervised constituent parsing that comes
with theoretical guarantees on latent struc-
ture recovery. Our approach is grammar-
less – we directly learn the bracketing
structure of a given sentence without us-
ing a grammar model. The main algorithm
is based on lifting the concept of additive
tree metrics for structure learning of la-
tent trees in the phylogenetic and machine
learning communities to the case where
the tree structure varies across examples.
Although finding the “minimal” latent tree
is NP-hard in general, for the case of pro-
jective trees we find that it can be found
using bilexical parsing algorithms. Empir-
ically, our algorithm performs favorably
compared to the constituent context model
of Klein and Manning (2002) without the
need for careful initialization.
</bodyText>
<sectionHeader confidence="0.998127" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986356045454546">
Solutions to the problem of grammar induction
have been long sought after since the early days of
computational linguistics and are interesting both
from cognitive and engineering perspectives. Cog-
nitively, it is more plausible to assume that chil-
dren obtain only terminal strings of parse trees and
not the actual parse trees. This means the unsu-
pervised setting is a better model for studying lan-
guage acquisition. From the engineering perspec-
tive, training data for unsupervised parsing exists
in abundance (i.e. sentences and part-of-speech
tags), and is much cheaper than the syntactically
annotated data required for supervised training.
Most existing solutions treat the problem of un-
supervised parsing by assuming a generative pro-
cess over parse trees e.g. probabilistic context
free grammars (Jelinek et al., 1992), and the con-
stituent context model (Klein and Manning, 2002).
Learning then reduces to finding a set of parame-
ters that are estimated by identifying a local max-
imum of an objective function such as the likeli-
hood (Klein and Manning, 2002) or a variant of it
(Smith and Eisner, 2005; Cohen and Smith, 2009;
Headden et al., 2009; Spitkovsky et al., 2010b;
Gillenwater et al., 2010; Golland et al., 2012). Un-
fortunately, finding the global maximum for these
objective functions is usually intractable (Cohen
and Smith, 2012) which often leads to severe lo-
cal optima problems (but see Gormley and Eisner,
2013). Thus, strong experimental results are often
achieved by initialization techniques (Klein and
Manning, 2002; Gimpel and Smith, 2012), incre-
mental dataset use (Spitkovsky et al., 2010a) and
other specialized techniques to avoid local optima
such as count transforms (Spitkovsky et al., 2013).
These approaches, while empirically promising,
generally lack theoretical justification.
On the other hand, recently proposed spectral
methods approach the problem via restriction of
the PCFG model (Hsu et al., 2012) or matrix com-
pletion (Bailly et al., 2013). These novel perspec-
tives offer strong theoretical guarantees but are not
designed to achieve competitive empirical results.
In this paper, we suggest a different approach,
to provide a first step to bridging this theory-
experiment gap. More specifically, we approach
unsupervised constituent parsing from the per-
spective of structure learning as opposed to pa-
rameter learning. We associate each sentence with
an undirected latent tree graphical model, which is
a tree consisting of both observed variables (corre-
sponding to the words in the sentence) and an ad-
ditional set of latent variables that are unobserved
in the data. This undirected latent tree is then di-
rected via a direction mapping to give the final
constituent parse.
In our framework, parsing reduces to finding the
best latent structure for a given sentence. How-
ever, due to the presence of latent variables, struc-
ture learning of latent trees is substantially more
complicated than in observed models. As before,
one solution would be local search heuristics.
Intuitively, however, latent tree models en-
code low rank dependencies among the observed
variables permitting the development of “spec-
1062
</bodyText>
<note confidence="0.9634535">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1062–1072,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999733809523809">
tral” methods that can lead to provably correct
solutions. In particular we leverage the con-
cept of additive tree metrics (Buneman, 1971;
Buneman, 1974) in phylogenetics and machine
learning that can create a special distance met-
ric among the observed variables as a function
of the underlying spectral dependencies (Choi et
al., 2011; Song et al., 2011; Anandkumar et al.,
2011; Ishteva et al., 2012). Additive tree met-
rics can be leveraged by “meta-algorithms” such
as neighbor-joining (Saitou and Nei, 1987) and
recursive grouping (Choi et al., 2011) to provide
consistent learning algorithms for latent trees.
Moreover, we show that it is desirable to learn
the “minimal” latent tree based on the tree metric
(“minimum evolution” in phylogenetics). While
this criterion is in general NP-hard (Desper and
Gascuel, 2005), for projective trees we find that a
bilexical parsing algorithm can be used to find an
exact solution efficiently (Eisner and Satta, 1999).
Unlike in phylogenetics and graphical models,
where a single latent tree is constructed for all the
data, in our case, each part of speech sequence is
associated with its own parse tree. This leads to a
severe data sparsity problem even for moderately
long sentences. To handle this issue, we present
a strategy that is inspired by ideas from kernel
smoothing in the statistics community (Zhou et al.,
2010; Kolar et al., 2010b; Kolar et al., 2010a).
This allows principled sharing of samples from
different but similar underlying distributions.
We provide theoretical guarantees on the re-
covery of the correct underlying latent tree and
characterize the associated sample complexity un-
der our technique. Empirically we evaluate our
method on data in English, German and Chi-
nese. Our algorithm performs favorably to Klein
and Manning’s (2002) constituent-context model
(CCM), without the need for careful initialization.
In addition, we also analyze CCM’s sensitivity to
initialization, and compare our results to Seginer’s
algorithm (Seginer, 2007).
</bodyText>
<subsectionHeader confidence="0.726405">
2 Learning Setting and Model
</subsectionHeader>
<bodyText confidence="0.9987495">
In this section, we detail the learning setting and a
conditional tree model we learn the structure for.
</bodyText>
<subsectionHeader confidence="0.999609">
2.1 Learning Setting
</subsectionHeader>
<bodyText confidence="0.99978675">
Let w = (w1,..., wt) be a vector of words corre-
sponding to a sentence of length E. Each wi is rep-
resented by a vector in Rp for p E N. The vector
is an embedding of the word in some space, cho-
</bodyText>
<figureCaption confidence="0.989621">
Figure 2: Candidate constituent parses for x = (VBD, DT, NN)
(left-correct, right -incorrect)
</figureCaption>
<bodyText confidence="0.999982259259259">
sen from a fixed dictionary that maps word types
to Rp. In addition, let x = (x1,..., xt) be the as-
sociated vector of part-of-speech (POS) tags (i.e.
xi is the POS tag of wi).
In our learning algorithm, we assume that ex-
amples of the form (w(i), x(i)) for i E [N] =
{1, ... , N} are given, and the goal is to predict
a bracketing parse tree for each of these examples.
The word embeddings are used during the learn-
ing process, but the final decoder that the learning
algorithm outputs maps a POS tag sequence x to
a parse tree. While ideally we would want to use
the word information in decoding as well, much of
the syntax of a sentence is determined by the POS
tags, and relatively high level of accuracy can be
achieved by learning, for example, a supervised
parser from POS tag sequences.
Just like our decoder, our model assumes that
the bracketing of a given sentence is a function
of its POS tags. The POS tags are generated
from some distribution, followed by a determin-
istic generation of the bracketing parse tree. Then,
latent states are generated for each bracket, and
finally, the latent states at the yield of the bracket-
ing parse tree generate the words of the sentence
(in the form of embeddings). The latent states are
represented by vectors z E R&apos; where m &lt; p.
</bodyText>
<subsectionHeader confidence="0.99468">
2.2 Intuition
</subsectionHeader>
<bodyText confidence="0.9999744">
For intuition, consider the simple tag sequence
x = (VBD, DT, NN). Two candidate constituent
parse structures are shown in Figure 2 and the cor-
rect one is boxed in green (the other in red). Re-
call that our training data contains word phrases
that have the tag sequence x e.g. w(1) =
(hit, the, ball), w(2) = (ate, an, apple).
Intuitively, the words in the above phrases ex-
hibit dependencies that can reveal the parse struc-
ture. The determiner (w2) and the direct object
(w3) are correlated in that the choice of deter-
miner depends on the plurality of w3. However,
the choice of verb (w1) is mostly independent of
the determiner. We could thus conclude that w2
and w3 should be closer in the parse tree than w1
</bodyText>
<figure confidence="0.37624">
VBD DT NN VBD DT NN
1063
The bear ate the fish
</figure>
<figureCaption confidence="0.905653">
Figure 1: Example for the tag
</figureCaption>
<bodyText confidence="0.965982333333333">
sequence (DT, NN, VBD, DT, NN)
showing the overview of our
approach. We first learn a undi-
rected latent tree for the se-
quence (left). We then ap-
ply a direction mapping hdir to
direct the latent tree (center).
This can then easily be con-
verted into a bracketing (right).
</bodyText>
<equation confidence="0.998592">
x = (DT, NN, VBD, DT, NN)
U(x)
z2
z1
z,
((DT NN) (VBD (DT NN)))
z3
w1 w2 w3
w4 w5
w1 w3 w3
w4 wS
W1 , W2 , W3 , W4 , W5, Z1, Z2, Z3
z1 z3
</equation>
<bodyText confidence="0.999970666666667">
and w2, giving us the correct structure. Informally,
the latent state z corresponding to the (w2, w3)
bracket would store information about the plural-
ity of z, the key to the dependence between w2 and
w3. It would then be reasonable to assume that w2
and w3 are independent given z.
</bodyText>
<subsectionHeader confidence="0.998512">
2.3 A Conditional Latent Tree Model
</subsectionHeader>
<bodyText confidence="0.999826">
Following this intuition, we propose to model the
distribution over the latent bracketing states and
words for each tag sequence x as a latent tree
graphical model, which encodes conditional inde-
pendences among the words given the latent states.
Let V := {w1, ..., wt, z1, ..., zH}, with wi rep-
resenting the word embeddings, and zi represent-
ing the latent states of the bracketings. Then, ac-
cording to our base model it holds that:
</bodyText>
<equation confidence="0.9917605">
p(zi|π,,(zi), θ(x))
p(wi|π,,(wi),θ(x)) (1)
</equation>
<bodyText confidence="0.98082390625">
where π,,(·) returns the parent node index of the
argument in the latent tree corresponding to tag
sequence x.1 If z is the root, then π,,(z) = ∅.
All the wi are assumed to be leaves while all the
zi are internal (i.e. non-leaf) nodes. The param-
eters θ(x) control the conditional probability ta-
bles. We do not commit to a certain parametric
family, but see more about the assumptions we
make about θ in §3.2. The parameter space is de-
noted O. The model assumes a factorization ac-
cording to a latent-variable tree. The latent vari-
ables can incorporate various linguistic properties,
such as head information, valence of dependency
being generated, and so on. This information is
expected to be learned automatically from data.
Our generative model deterministically maps a
POS sequence to a bracketing via an undirected
1At this point, it refers to an arbitrary direction of the
undirected tree u(x).
latent-variable tree. The orientation of the tree is
determined by a direction mapping hdir(u), which
is fixed during learning and decoding. This means
our decoder first identifies (given a POS sequence)
an undirected tree, and then orients it by applying
hdir on the resulting tree (see below).
Define U to be the set of undirected latent trees
where all internal nodes have degree exactly 3 (i.e.
they correspond to binary bracketing), and in addi-
tion hdir(u) for any u E U is projective (explained
in the hdir section). In addition, let T be the set
of binary bracketings. The complete generative
model that we follow is then:
</bodyText>
<listItem confidence="0.944335714285714">
• Generate a tag sequence x = (x1, ... , xt)
• Decide on u(x) E U, the undirected latent tree
that x maps to.
• Set t E T by computing t = hdir(u).
• Set θ E O by computing θ = θ(x).
• Generate a tuple v = (w1, ... , wt, z1, ..., zH)
where wi E Rp, zj E Rm according to Eq. 1.
</listItem>
<bodyText confidence="0.995139909090909">
See Figure 1 (left) for an example.
The Direction Mapping hdir. Generating a
bracketing via an undirected tree enables us to
build on existing methods for structure learning
of latent-tree graphical models (Choi et al., 2011;
Anandkumar et al., 2011). Our learning algorithm
focuses on recovering the undirected tree based
for the generative model that was described above.
This undirected tree is converted into a directed
tree by applying hdir. The mapping hdir works in
three steps:
</bodyText>
<listItem confidence="0.952553625">
• It first chooses a top bracket ([1, R − 1], [R, `])
where R is the mid-point of the bracket and ` is
the length of the sentence.
• It marks the edge ei,j that splits the tree accord-
ing to the top bracket as the “root edge” (marked
in red in Figure 1(center))
• It then creates t from u by directing the tree out-
ward from ei,j as shown in Figure 1(center)
</listItem>
<equation confidence="0.9792488">
H
p(w, z|x) = ri
i=1
× t(,,) i=1
1064
</equation>
<bodyText confidence="0.999942461538462">
The resulting t is a binary bracketing parse tree.
As implied by the above definition of hdir, se-
lecting which edge is the root can be interpreted
as determining the top bracket of the constituent
parse. For example, in Figure 1, the top bracket
is ([1, 2], [3, 5]) = ([DT, NN], [VBD, DT, NN]). Note
that the “root” edge ez1,z2 partitions the leaves
into precisely this bracketing. As indicated in the
above section, we restrict the set of undirected
trees to be those such that after applying hdir the
resulting t is projective i.e. there are no crossing
brackets. In §4.1, we discuss an effective heuristic
to find the top bracket without supervision.
</bodyText>
<sectionHeader confidence="0.9790215" genericHeader="method">
3 Spectral Learning Algorithm based on
Additive Tree Metrics
</sectionHeader>
<bodyText confidence="0.999986075">
Our goal is to recover t E T for tag sequence x
using the data D = [(w(i), x(i))]Ni=1. To get an in-
tuition about the algorithm, consider a partition of
the set of examples D into D(x) = {(w(i), x(i)) E
D|x(i) = x}, i.e. each section in the partition has
an identical sequence of part of speech tags. As-
sume for this section |D(x) |is large (we address
the data sparsity issue in §3.4).
We can then proceed by learning how to map a
POS sequence x to a tree t E T (through u E U)
by focusing only on examples in D(x).
Directly attempting to maximize the likelihood
unfortunately results in an intractable optimiza-
tion problem and greedy heuristics are often em-
ployed (Harmeling and Williams, 2011). Instead
we propose a method that is provably consistent
and returns a tree that can be mapped to a bracket-
ing using hdir.
If all the variables were observed, then the
Chow-Liu algorithm (Chow and Liu, 1968) could
be used to find the most likely tree structure u E
U. The Chow-Liu algorithm essentially computes
the distances among all pairs of variables (the neg-
ative of the mutual information) and then finds the
minimum cost tree. However, the fact that the zi
are latent variables makes this strategy substan-
tially more complicated. In particular, it becomes
challenging to compute the distances among pairs
of latent variables. What is needed is a “special”
distance function that allows us to reverse engineer
the distances among the latent variables given the
distances among the observed variables. This is
the key idea behind additive tree metrics that are
the basis of our approach.
In the following sections, we describe the key
steps to our method. §3.1 and §3.2 largely describe
existing background on additive tree metrics and
latent tree structure learning, while §3.3 and §3.4
discuss novel aspects that are unique to our prob-
lem.
</bodyText>
<subsectionHeader confidence="0.999638">
3.1 Additive Tree Metrics
</subsectionHeader>
<bodyText confidence="0.998554714285714">
Let u(x) be the true undirected tree of sentence x
and assume the nodes V to be indexed by [M] =
{1, . . . ,M} such that M = |V |= H + f. Fur-
thermore, let v E V refer to a node in the undi-
rected tree (either observed or latent). We assume
the existence of a distance function that allows us
to compute distances between pairs of nodes. For
example, as we see in §3.2 we will define the dis-
tance d(i, j) to be a function of the covariance ma-
trix E[vivTj |u(x), θ(x)]. Thus if vi and vj are both
observed variables, the distance can be directly
computed from the data.
Moreover, the metrics we construct are such
that they are tree additive, defined below:
</bodyText>
<construct confidence="0.9743264">
Definition 1 A function du(x) : [M]x[M] → R is
an additive tree metric (Erd˜os et al., 1999) for the
undirected tree u(x) if it is a distance metric,2 and
furthermore, ∀i, j E [M] the following relation
holds:
</construct>
<equation confidence="0.9918605">
du(x)(i,j) = � du(x)(a,b) (2)
(a,b)∈pathu(,)(i,j)
</equation>
<bodyText confidence="0.998323523809524">
where pathu(x)(i, j) is the set of all the edges in
the (undirected) path from i to j in the tree u(x).
As we describe below, given the tree structure,
the additive tree metric property allows us to com-
pute “backwards” the distances among the latent
variables as a function of the distances among the
observed variables.
Define D to be the M x M distance matrix
among the M variables, i.e. Dij = du(x)(i, j).
Let DWW, DZW (equal to DTZ), and DZZ indi-
cate the word-word, latent-word and latent-latent
sub-blocks of D respectively. In addition, since
u(x) is assumed to be known from context, we
denote du(x)(i, j) just by d(i, j).
Given the fact that the distance between a pair
of nodes is a function of the random variables
they represent (according to the true model), only
DWW can be empirically estimated from data.
However, if the underlying tree structure is known,
then Definition 1 can be leveraged to compute
DZZ and DZW as we show below.
</bodyText>
<footnote confidence="0.4528285">
2This means that it satisfies d(i, j) = 0 if and only if
i = j, the triangle inequality and is also symmetric.
</footnote>
<figure confidence="0.980798666666667">
1065
ei,j
vi
vj
ei,j
vi
vj
(b)
(a)
</figure>
<figureCaption confidence="0.9990055">
Figure 3: Two types of edges in general undirected latent
trees. (a) leaf edge, (b) internal edge
</figureCaption>
<bodyText confidence="0.999864388888889">
We first show how to compute d(i, j) for all i, j
such that i and j are adjacent to each other in u(x),
based only on observed nodes. It then follows that
the other elements of the distance matrix can be
computed based on Definition 1. To show how to
compute distances between adjacent nodes, con-
sider the two cases: (1) (i, j) is a leaf edge; (2)
(i, j) is an internal edge.
Case 1 (leaf edge, figure 3(a)) Assume without
loss of generality that j is the leaf and i is an in-
ternal latent node. Then i must have exactly two
other neighbors a E [M] and b E [M]. Let A
denote the set of nodes that are closer to a than
i and similarly let B denote the set of nodes that
are closer to b than i. Let A∗ and B∗ denote all
the leaves (word nodes) in A and B respectively.
Then using path additivity (Definition 1), it can be
shown that for any a∗ E A∗, b∗ E B∗ it holds that:
</bodyText>
<equation confidence="0.990185">
d(i, j) = 2 (d(j, a∗) + d(j, b∗) − d(a∗, b∗)) (3)
1
</equation>
<bodyText confidence="0.980833416666667">
Note that the right-hand side only depends on
distances between observed random variables.
Case 2 (internal edge, figure 3(b)) Both i and
j are internal nodes. In this case, i has exactly
two other neighbors a E [M] and b E [M], and
similarly, j has exactly other two neighbors g E
[M] and h E [M]. Let A denote the set of nodes
closer to a than i, and analogously for B, G, and
H. Let A∗, B∗, G∗, and H∗ refer to the leaves in
A, B, G, and H respectively. Then for any a∗ E
A∗, b∗ E B∗, g∗ E G∗, and h∗ E H∗ it can be
shown that:
</bodyText>
<equation confidence="0.983079">
d(i, j) = 4 (d(a∗, g∗) + d(a∗, h∗) + d(b∗, g∗)
)+d(b∗, h∗) − 2d(a∗, b∗) − 2d(g∗, h∗) (4)
</equation>
<bodyText confidence="0.95821425">
Empirically, one can obtain a more robust em-
pirical estimate d(i, j) by averaging over all valid
choices of a∗, b∗ in Eq. 3 and all valid choices of
a∗, b∗, g∗, h∗ in Eq. 4 (Desper and Gascuel, 2005).
</bodyText>
<subsectionHeader confidence="0.999772">
3.2 Constructing a Spectral Additive Metric
</subsectionHeader>
<bodyText confidence="0.99345975">
In constructing our distance metric, we begin with
the following assumption on the distribution in
Eq. 1 (analogous to the assumptions made in
Anandkumar et al., 2011).
</bodyText>
<equation confidence="0.78023325">
Assumption 1(Linear, Rank m, Means)
E[zi|πx(zi), x] = A(zi|zπx(zi),x)πx(zi) Vi E [H]
where A(zi|πx(zi),x) E Rm×m has rank m.
E[wi|πx(wi),x] = C(wi|πx(wi),x)πx(wi) Vi E [f(x)]
</equation>
<bodyText confidence="0.965214913043478">
where C(wi|πx(wi) x) E Rp×m has rank m.
Also assume that E[ziz&gt;i |x] has rank m Vi E
[H].
Note that the matrices A and C are a direct
function of θ(x), but we do not specify a model
family for θ(x). The only restriction is in the form
of the above assumption. If wi and zi were dis-
crete, represented as binary vectors, the above as-
sumption would correspond to requiring all con-
ditional probability tables in the latent tree to have
rank m. Assumption 1 allows for the wi to be high
dimensional features, as long as the expectation
requirement above is satisfied. Similar assump-
tions are made with spectral parameter learning
methods e.g. Hsu et al. (2009), Bailly et al. (2009),
Parikh et al. (2011), and Cohen et al. (2012).
Furthermore, Assumption 1 makes it explicit
that regardless of the size of p, the relationships
among the variables in the latent tree are restricted
to be of rank m, and are thus low rank since p &gt;
m. To leverage this low rank structure, we propose
using the following additive metric, a normalized
variant of that in Anandkumar et al. (2011):
</bodyText>
<equation confidence="0.995899">
dspectral(i, j) = − log Λm(Σx(i,j))
+12 log Λm(Σx(i,i)) + 2 1 log Λm(Σx(j,j)) (5)
</equation>
<bodyText confidence="0.923605545454545">
where Λm(A) denotes the product of the top m
singular values of A and Σx(i, j) := E[viv&gt;j |x],
i.e. the uncentered cross-covariance matrix.
We can then show that this metric is additive:
pectral is
Lemma 1 If Assumption 1 holds then, ds
an additive tree metric (Definition 1).
A proof is in the supplementary for completeness.
From here, we use d to denote dspectral, since that
is the metric we use for our learning algorithm.
1066
</bodyText>
<subsectionHeader confidence="0.998295">
3.3 Recovering the Minimal Projective
Latent Tree
</subsectionHeader>
<bodyText confidence="0.998307666666667">
It has been shown (Rzhetsky and Nei, 1993) that
for any additive tree metric, u(x) can be recovered
by solving arg minuEU c(u) for c(u):
</bodyText>
<equation confidence="0.866972">
Xc(u) = d(i, j). (6)
(i,j)EEu
</equation>
<bodyText confidence="0.99313520754717">
where Eu is the set of pairs of nodes which are
adjacent to each other in u and d(i, j) is computed
using Eq. 3 and Eq. 4.
Note that the metric d we use in defining c(u)
is based on the expectations from the true distri-
bution. In practice, the true distribution is un-
known, and therefore we use an approximation for
the distance metric ˆd. As we discussed in §3.1
all elements of the distance matrix are functions
of observable quantities if the underlying tree u is
known. However, only the word-word sub-block
DWW can be directly estimated from the data
without knowledge of the tree structure.
This subtlety makes solving the minimization
problem in Eq. 6 NP-hard (Desper and Gascuel,
2005) if u is allowed to be an arbitrary undirected
tree. However, if we restrict u to be in U, as we do
in the above, then maximizing ˆc(u) over U can be
solved using the bilexical parsing algorithm from
Eisner and Satta (1999). This is because the com-
putation of the other sub-blocks of the distance
matrix only depend on the partitions of the nodes
shown in Figure 3 into A, B, G, and H, and not
on the entire tree structure.
Therefore, the procedure to find a bracketing
for a given POS tag x isb Dto first estimate the dis-
tance matrix sub-block WW from raw text data
(see §3.4), and then solve the optimization prob-
lem arg minuEU ˆc(u) using a variant of the Eisner-
Satta algorithm where ˆc(u) is identical to c(u) in
Eq. 6, with d replaced with ˆd.
Summary. We first defined a generative model
that describes how a sentence, its sequence of POS
tags, and its bracketing is generated (§2.3). First
an undirected u E U is generated (only as a func-
tion of the POS tags), and then u is mapped to
a bracketing using a direction mapping hdir. We
then showed that we can define a distance met-
ric between nodes in the undirected tree, such that
minimizing it leads to a recovery of u. This dis-
tance metric can be computed based only on the
text, without needing to identify the latent infor-
mation (§3.2). If the true distance metric is known,
Algorithm 1 The learning algorithm for find-
ing the latent structure from a set of examples
(w(i), x(i)), i E [N].
Inputs: Set of examples (w(i), x(i)) for i E [N],
a kernel Kγ(j, k, j&apos;, k&apos;Jx, x&apos;), an integer m
Data structures: For each i E [N], j, k E
`(x(i)) there is a (uncentered) covariance matrix
bEx(i)(j, k) E II p×p, and a distance ˆdspectral(j, k).
Algorithm:
(Covariance estimation) ∀i E [N], j, k E `(x(i))
</bodyText>
<listItem confidence="0.608110666666667">
• Let Cj0,k0|i0 = wj00)(wk00))T, kj,k,j0,k0,i,i0 =
Kγ(j, k, j&apos;, k&apos;Jx(i), x(i0)) and `i0 = `(x(i0)),
and estimate each p x p covariance matrix as:
</listItem>
<equation confidence="0.9974146">
bEx(j, k) =
PN `i0 `i0
i0=1 Pj0=1 Pk0 =1 kj,k,j0,k0,i,i0Cj0,k0|i0
PN `i0 `i0
i0=1 Pj0=1 Pk0=1 kj,k,j0,k0,i,i0
</equation>
<listItem confidence="0.987127">
• Compute ˆdspectral(j, k) ∀j, k E `(x(i)) using
Eq. 5.
(Uncover structure) ∀i E [N]
• Find ˆu(i) = arg minuEU ˆc(u), and for the ith
example, return the structure hdir(ˆu(i)).
</listItem>
<bodyText confidence="0.999145">
with respect to the true distribution that generates
the words in a sentence, then u can be fully recov-
ered by optimizing the cost function c(u). How-
ever, in practice the distance metric must be esti-
mated from data, as discussed below.
</bodyText>
<subsectionHeader confidence="0.991782">
3.4 Estimation of d from Sparse Data
</subsectionHeader>
<bodyText confidence="0.997176875">
We now address the data sparsity problem, in par-
ticular that D(x) can be very small, and therefore
estimating d for each POS sequence separately can
be problematic.3
In order to estimate d from data, we need to es-
timate the covariance matrices Ex(i, j) (for i, j E
{1, ... , `(x)}) from Eq. 5.
To give some motivation to our solu-
tion, consider estimating the covariance
matrix Ex(1, 2) for the tag sequence
x = (DT1, NN2, VBD3, DT4, NN5). D(x) may
be insufficient for an accurate empirical es-
3This data sparsity problem is quite severe – for example,
the Penn treebank (Marcus et al., 1993) has a total number
of 43,498 sentences, with 42,246 unique POS tag sequences,
averaging |D(x) |to be 1.04.
</bodyText>
<equation confidence="0.68103">
1067
= (RB1, DT2, NN3, VBD4, DT5, ADJ6, NN7).
</equation>
<bodyText confidence="0.994051341463414">
Although x and x0 are not identical, it is likely
that Σx0(2, 3) is similar to Σx(1, 2) because the
determiner and the noun appear in similar syn-
tactic context. Σx0(5, 7) also may be somewhat
similar, but Σx0(2, 7) should not be very similar
to Σx(1, 2) because the noun and the determiner
appear in a different syntactic context.
The observation that the covariance matrices
depend on local syntactic context is the main driv-
ing force behind our solution. The local syntactic
context acts as an “anchor,” which enhances or re-
places a word index in a sentence with local syn-
tactic context. More formally, an anchor is a func-
tion G that maps a word index j and a sequence of
POS tags x to a local context G(j, x). The anchor
we use is G(j, x) = (j, xj). Then, the covariance
matrices Σx are estimated using kernel smooth-
ing (Hastie et al., 2009), where the smoother tests
similarity between the different anchors G(j, x).
The full learning algorithm is given in Figure 1.
The first step in the algorithm is to estimate the
covariance matrix block ix(i)(j, k) for each train-
ing example x(&apos;) and each pair of preterminal po-
sitions (j, k) in x(&apos;). Instead of computing this
block by computing the empirical covariance ma-
trix for positions (j, k) in the data D(x), the al-
gorithm uses all of the pairs (j0, k0) from all of
N training examples. It averages the empirical
covariance matrices from these contexts using a
kernel weight, which gives a similarity measure
for the position (j, k) in x(&apos;) and (j0, k0) in an-
other example x(&apos;0). γ is the kernel “bandwidth”,
a user-specified parameter that controls how in-
clusive the kernel will be with respect to exam-
ples in D (see § 4.1 for a concrete example). Note
that the learning algorithm is such that it ensures
�Σx(i)(j, k) = �Σx(i0)(j0,k0) if G(j, x(&apos;)) =
G(j0, x(&apos;0)) and G(k, x(&apos;)) = G(k0, x(&apos;0)).
Once the empirical estimates for the covariance
matrices are obtained, a variant of the Eisner-Satta
algorithm is used, as mentioned in §3.3.
</bodyText>
<subsectionHeader confidence="0.962762">
3.5 Theoretical Guarantees
</subsectionHeader>
<bodyText confidence="0.902653916666667">
Our main theoretical guarantee is that Algorithm 1
will recover the correct tree u E U with high prob-
ability, if the given top bracket is correct and if
we obtain enough examples (w(&apos;), x(&apos;)) from the
model in §2. We give the theorem statement be-
low. The constants lurking in the O-notation and
the full proof are in the supplementary.
Denote σx(j, k)(r) as the rth singu-
lar value of Σx(j, k). Let σ∗ (x) :=
minj k∈t(x) min (σx (j, k) (m)) .
Theorem 1 Define uˆ as the estimated tree for tag
sequence x and u(x) as the correct tree. Let
</bodyText>
<equation confidence="0.989884428571429">
A(x) := min (c(u(x)) − c(u0))/(8|`(x)|)
u0∈U:u06=u(x)
Assume that
m2 log �p2�(x)2 �
δ
min(σ∗(x)2A(x)2, σ∗(x)2)νx(γ)2
Then with probability 1 − δ, uˆ = u(x).
</equation>
<bodyText confidence="0.999981">
where νx(γ), defined in the supplementary, is a
function of the underlying distribution over the tag
sequences x and the kernel bandwidth γ.
Thus, the sample complexity of our approach
depends on the dimensionality of the latent and
observed states (m and p), the underlying singu-
lar values of the cross-covariance matrices (σ∗(x))
and the difference in the cost of the true tree com-
pared to the cost of the incorrect trees (A(x)).
</bodyText>
<sectionHeader confidence="0.998869" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999948153846154">
We report results on three different languages: En-
glish, German, and Chinese. For English we use
the Penn treebank (Marcus et al., 1993), with sec-
tions 2–21 for training and section 23 for final
testing. For German and Chinese we use the Ne-
gra treebank and the Chinese treebank respectively
and the first 80% of the sentences are used for
training and the last 20% for testing. All punc-
tuation from the data is removed.4
We primarily compare our method to the
constituent-context model (CCM) of Klein and
Manning (2002). We also compare our method to
the algorithm of Seginer (2007).
</bodyText>
<subsectionHeader confidence="0.996415">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999959">
Top bracket heuristic Our algorithm requires
the top bracket in order to direct the latent tree.
In practice, we employ the following heuristic to
find the bracket using the following three steps:
</bodyText>
<listItem confidence="0.8575455">
• If there exists a comma/semicolon/colon at in-
dex i that has at least a verb before i and both
a noun followed by a verb after i, then return
([0, i − 1], [i, `(x)]) as the top bracket. (Pick
the rightmost comma/semicolon/colon if multi-
ple satisfy the criterion).
</listItem>
<footnote confidence="0.52699">
4We make brief use of punctuation for our top bracket
heuristic detailed below before removing it.
</footnote>
<table confidence="0.898675066666667">
timate. However, consider another sequence
x0
that
⎛
N ≥ O ⎝
⎞
⎠
1068
Length CCM CCM-U CCM-OB CCM-UB
≤ 10 72.5 57.1 58.2 62.9
≤ 15 54.1 36 24 23.7
≤ 20 50 34.7 19.3 19.1
≤ 25 47.2 30.7 16.8 16.6
≤ 30 44.8 29.6 15.3 15.2
≤ 40 26.3 13.5 13.9 13.8
</table>
<tableCaption confidence="0.999892">
Table 1: Comparison of different CCM variants on English
</tableCaption>
<bodyText confidence="0.69543075">
(training). U stands for universal POS tagset, OB stands for
conjoining original POS tags with Brown clusters and UB
stands for conjoining universal POS tags with Brown clusters.
The best setting is just the vanilla setting, CCM.
</bodyText>
<listItem confidence="0.989498333333333">
• Otherwise find the first non-participle verb (say
at index j) and return ([0, j − 1], [j, `(x)]).
• If no verb exists, return ([0,1], [1, `(x)]).
</listItem>
<bodyText confidence="0.9988339375">
Word embeddings As mentioned earlier, each
wi can be an arbitrary feature vector. For all lan-
guages we use Brown clustering (Brown et al.,
1992) to construct a log(C) + C feature vector
where the first log(C) elements indicate which
mergable cluster the word belongs to, and the last
C elements indicate the cluster identity. For En-
glish, more sophisticated word embeddings are
easily obtainable, and we experiment with neural
word embeddings Turian et al. (2010) of length
50. We also explored two types of CCA embed-
dings: OSCCA and TSCCA, given in Dhillon et
al. (2012). The OSCCA embeddings behaved bet-
ter, so we only report its results.
Choice of kernel For our experiments, we use
the kernel
</bodyText>
<equation confidence="0.612353">
Kγ(j, k, j(&apos;, k&apos; |x, x&apos;)
= max { 0,1 − κ (j, k, j&apos;, k&apos; |x, x&apos;) l
l &apos;Y f
</equation>
<bodyText confidence="0.998793">
where γ denotes the user-specified bandwidth,
and κ(j,k,j&apos;,k&apos;|x,x&apos;) = |j − k |− |j&apos; − k&apos; |if
</bodyText>
<equation confidence="0.882491666666667">
|j − k |+ |j&apos; − k&apos;|
x(j) = x(j&apos;) and x(k&apos;) = x(k), and sign(j −
k) = sign(j&apos; − k&apos;) (and oc otherwise).
</equation>
<bodyText confidence="0.999896274509804">
The kernel is non-zero if and only if the tags at
position j and k in x are identical to the ones in
position j&apos; and k&apos; in x&apos;, and if the direction be-
tween j and k is identical to the one between j&apos;
and k&apos;. Note that the kernel is not binary, as op-
posed to the theoretical kernel in the supplemen-
tary material. Our experiments show that using a
non-zero value different than 1 that is a function
of the distance between j and k compared to the
distance between j&apos; and k&apos; does better in practice.
Choice of data For CCM, we found that if the
full dataset (all sentence lengths) is used in train-
ing, then performance degrades when evaluating
on sentences of length &lt; 10. We therefore restrict
the data used with CCM to sentences of length
&lt; `, where ` is the maximal sentence length being
evaluated. This does not happen with our algo-
rithm, which manages to leverage lexical informa-
tion whenever more data is available. We therefore
use the full data for our method for all lengths.
We also experimented with the original POS
tags and the universal POS tags of Petrov et al.
(2011). Here, we found out that our method
does better with the universal part of speech tags.
For CCM, we also experimented with the origi-
nal parts of speech, universal tags (CCM-U), the
cross-product of the original parts of speech with
the Brown clusters (CCM-OB), and the cross-
product of the universal tags with the Brown clus-
ters (CCM-UB). The results in Table 1 indicate
that the vanilla setting is the best for CCM.
Thus, for all results, we use universal tags for
our method and the original POS tags for CCM.
We believe that our approach substitutes the need
for fine-grained POS tags with the lexical informa-
tion. CCM, on the other hand, is fully unlexical-
ized.
Parameter Selection Our method requires two
parameters, the latent dimension m and the band-
width γ. CCM also has two parameters, the num-
ber of extra constituent/distituent counts used for
smoothing. For both methods we chose the best
parameters for sentences of length ` &lt; 10 on the
English Penn Treebank (training) and used this
set for all other experiments. This resulted in
m = 7,γ = 0.4 for our method and 2, 8 for
CCM’s extra constituent/distituent counts respec-
tively. We also tried letting CCM choose differ-
ent hyperparameters for different sentence lengths
based on dev-set likelihood, but this gave worse
results than holding them fixed.
</bodyText>
<sectionHeader confidence="0.764211" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.995257166666667">
Test I: Accuracy Table 2 summarizes our re-
sults. CCM is used with the initializer proposed
in Klein and Manning (2002).5 NN, CC, and BC
indicate the performance of our method for neural
embeddings, CCA embeddings, and Brown clus-
tering respectively, using the heuristic for hdir de-
</bodyText>
<footnote confidence="0.944807">
5We used the implementation available at
http://tinyurl.com/lhwk5n6.
</footnote>
<table confidence="0.993552833333333">
1069
f NN-O NN CC-O English BC-O BC CCM BC-O German CCM BC-O Chinese CCM
CC BC BC
train &lt; 10 70.9 69.2 70.4 68.7 71.1 69.3 72.5 64.6 59.9 62.6 64.9 57.3 46.1
&lt; 20 55.1 53.5 53.2 51.6 53.0 51.5 50 52.7 48.7 47.9 51.4 46 22.4
&lt; 40 46.1 44.5 43.6 41.9 43.3 41.8 26.3 46.7 43.6 19.8 42.6 38.6 15
test &lt; 10 69.2 66.7 68.3 65.5 68.9 66.1 70.5 66.4 61.6 64.7 58.0 53.2 40.7
&lt; 15 60.3 58.3 58.6 56.4 58.6 56.5 53.8 57.5 53.5 49.6 54.3 49.4 35.9
&lt; 20 54.1 52.3 52.3 50.3 51.9 50.2 50.4 52.8 49.2 48.9 49.7 45.5 20.1
&lt; 25 50.8 49.0 48.6 46.6 48.3 46.6 47.4 50.0 46.8 45.6 46.7 42.7 17.8
&lt; 30 48.1 46.3 45.6 43.7 45.4 43.8 44.9 48.3 45.4 21.9 44.6 40.7 16.1
&lt; 40 45.5 43.8 43.0 41.1 42.7 41.1 26.1 46.9 44.1 20.1 42.2 38.6 14.3
</table>
<tableCaption confidence="0.972695333333333">
Table 2: F1 bracketing measure for the test sets and train sets in three languages. NN, CC, and BC indicate the performance of
our method for neural embeddings, CCA embeddings, and Brown clustering respectively, using the heuristic for hdir described
in § 4.1. NN-O, CC-O, and BC-O indicate that the oracle (i.e. true top bracket) was used for hdir.
</tableCaption>
<figure confidence="0.9864932">
35 CCM Random Restarts (Length &lt;= 10)
30
25
20
15
10
5
0
20-30 31-40 41-50 51-60 61-70 71-80
Bracketing F1
</figure>
<figureCaption confidence="0.941035">
Figure 4: Histogram showing performance of CCM across
100 random restarts for sentences of length &lt; 10.
</figureCaption>
<bodyText confidence="0.999738037735849">
scribed in § 4.1. NN-O, CC-O, and BC-O indicate
that the oracle (i.e. true top bracket) was used for
hdir. For our method, test set results can be ob-
tained by using Algorithm 1 (except the distances
are computed using the training data).
For English, while CCM behaves better for
short sentences (E &lt; 10), our algorithm is more
robust with longer sentences. This is especially
noticeable for length &lt; 40, where CCM breaks
down and our algorithm is more stable. We find
that the neural embeddings modestly outperform
the CCA and Brown cluster embeddings.
The results for German are similar, except CCM
breaks down earlier at sentences of E &lt; 30. For
Chinese, our method substantially outperforms
CCM for all lengths. Note that CCM performs
very poorly, obtaining only around 20% accu-
racy even for sentences of E &lt; 20. We didn’t
have neural embeddings for German and Chinese
(which worked best for English) and thus only
used Brown cluster embeddings.
For English, the disparity between NN-O (ora-
cle top bracket) and NN (heuristic top bracket) is
rather low suggesting that our top bracket heuris-
tic is rather effective. However, for German and
Chinese note that the “BC-O” performs substan-
tially better, suggesting that if we had a better top
bracket heuristic our performance would increase.
Test II: Sensitivity to initialization The EM al-
gorithm with the CCM requires very careful ini-
tialization, which is described in Klein and Man-
ning (2002). If, on the other hand, random ini-
tialization is used, the variance of the performance
of the CCM varies greatly. Figure 4 shows a his-
togram of the performance level for sentences of
length &lt; 10 for different random initializers. As
one can see, for some restarts, CCM obtains ac-
curacies lower than 30% due to local optima. Our
method does not suffer from local optima and thus
does not require careful initialization.
Test III: Comparison to Seginer’s algorithm
Our approach is not directly comparable to
Seginer’s because he uses punctuation, while we
use POS tags. Using Seginer’s parser we were
able to get results on the training sets. On English:
75.2% (E &lt; 10), 64.2% (E &lt; 20), 56.7% (E &lt; 40).
On German: 57.8% (E &lt; 10), 45.0% (E &lt; 20), and
39.9% (E &lt; 40). On Chinese: 56.6% (E &lt; 10),
45.1% (E &lt; 20), and 38.9% (E &lt; 40).
Thus, while Seginer’s method performs better
on English, our approach performs 2-3 points bet-
ter on German, and both methods give similar per-
formance on Chinese.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.957487083333333">
We described a spectral approach for unsu-
pervised constituent parsing that comes with
theoretical guarantees on latent structure recovery.
Empirically, our algorithm performs favorably to
the CCM of Klein and Manning (2002) without
the need for careful initialization.
Acknowledgements: This work is supported
by NSF IIS1218282, NSF IIS1111142, NIH
R01GM093156, and the NSF Graduate Research
Fellowship Program under Grant No. 0946825
(NSF Fellowship to APP).
Frequency
</bodyText>
<page confidence="0.561218">
1070
</page>
<sectionHeader confidence="0.977578" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99846686013986">
A. Anandkumar, K. Chaudhuri, D. Hsu, S. M. Kakade,
L. Song, and T. Zhang. 2011. Spectral methods
for learning multivariate latent tree structure. arXiv
preprint arXiv:1107.1283.
R. Bailly, F. Denis, and L. Ralaivola. 2009. Gram-
matical inference as a principal component analysis
problem. In Proceedings of ICML.
R. Bailly, X. Carreras, F. M. Luque, and A. Quattoni.
2013. Unsupervised spectral learning of WCFG
as low-rank matrix completion. In Proceedings of
EMNLP.
P. F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra,
and J.C. Lai. 1992. Class-based n-gram mod-
els of natural language. Computational linguistics,
18(4):467–479.
O. P. Buneman. 1971. The recovery of trees from mea-
sures of dissimilarity. Mathematics in the archaeo-
logical and historical sciences.
P. Buneman. 1974. A note on the metric properties of
trees. Journal of Combinatorial Theory, Series B,
17(1):48–50.
M.J. Choi, V. YF Tan, A. Anandkumar, and A.S. Will-
sky. 2011. Learning latent tree graphical mod-
els. The Journal of Machine Learning Research,
12:1771–1812.
C. K. Chow and C. N. Liu. 1968. Approximating
Discrete Probability Distributions With Dependence
Trees. IEEE Transactions on Information Theory,
IT-14:462–467.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in un-
supervised grammar induction. In Proceedings of
HLT-NAACL.
S. B. Cohen and N. A. Smith. 2012. Empirical risk
minimization for probabilistic grammars: Sample
complexity and hardness of learning. Computa-
tional Linguistics, 38(3):479–526.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
R. Desper and O. Gascuel. 2005. The minimum evo-
lution distance-based approach to phylogenetic in-
ference. Mathematics of evolution and phylogeny,
pages 1–32.
P. S. Dhillon, J. Rodu, D. P. Foster, and L. H. Ungar.
2012. Two step cca: A new spectral method for es-
timating vector models of words. In Proceedings of
ICML.
J. Eisner and G. Satta. 1999. Efficient parsing for
bilexical context-free grammars and head automaton
grammars. In Proceedings ofACL.
P. Erd˜os, M. Steel, L. Sz´ekely, and T. Warnow. 1999.
A few logs suffice to build (almost) all trees: Part ii.
Theoretical Computer Science, 221(1):77–118.
J. Gillenwater, K. Ganchev, J. Grac¸a, F. Pereira, and
B. Taskar. 2010. Sparsity in dependency grammar
induction. In Proceedings of ACL.
K. Gimpel and N.A. Smith. 2012. Concavity and ini-
tialization for unsupervised dependency parsing. In
Proceedings of NAACL.
D. Golland, J. DeNero, and J. Uszkoreit. 2012. A
feature-rich constituent context model for grammar
induction. In Proceedings of ACL.
M. Gormley and J. Eisner. 2013. Nonconvex global
optimization for latent-variable models. In Proceed-
ings of ACL.
S. Harmeling and C. KI Williams. 2011. Greedy
learning of binary latent trees. Pattern Analysis
and Machine Intelligence, IEEE Transactions on,
33(6):1087–1097.
T. Hastie, R. Tibshirani, and J. Friedman. 2009. The
Elements of Statistical Learning: Data Mining, In-
ference, and Prediction. Springer Series in Statis-
tics. Springer Verlag.
W. P. Headden, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proceedings of
NAACL-HLT.
D. Hsu, S. Kakade, and T. Zhang. 2009. A spectral
algorithm for learning hidden Markov models. In
Proceedings of COLT.
D. Hsu, S. M. Kakade, and P. Liang. 2012. Identi-
fiability and unmixing of latent parse trees. arXiv
preprint arXiv:1206.3137.
M. Ishteva, H. Park, and L. Song. 2012. Unfolding
latent tree structures using 4th order tensors. arXiv
preprint arXiv:1210.1258.
F. Jelinek, J. D. Lafferty, and R. L. Mercer. 1992. Ba-
sic methods of probabilistic context free grammars.
Springer.
D. Klein and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proceedings of ACL.
M. Kolar, A. P. Parikh, and E. P. Xing. 2010a. On
sparse nonparametric conditional covariance selec-
tion. In Proceedings of ICML.
M. Kolar, L. Song, A. Ahmed, and E. P. Xing. 2010b.
Estimating time-varying networks. The Annals of
Applied Statistics, 4(1):94–123.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguis-
tics, 19:313–330.
1071
A.P. Parikh, L. Song, and E.P. Xing. 2011. A spectral
algorithm for latent tree graphical models. In Pro-
ceedings of ICML.
S. Petrov, D. Das, and R. McDonald. 2011. A univer-
sal part-of-speech tagset. ArXiv:1104.2086.
A. Rzhetsky and M. Nei. 1993. Theoretical founda-
tion of the minimum-evolution method of phyloge-
netic inference. Molecular Biology and Evolution,
10(5):1073–1095.
N. Saitou and M. Nei. 1987. The neighbor-joining
method: a new method for reconstructing phylo-
genetic trees. Molecular biology and evolution,
4(4):406–425.
Y. Seginer. 2007. Fast unsupervised incremental pars-
ing. In Proceedings ofACL.
N. A. Smith and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proceedings of ACL.
L. Song, A.P. Parikh, and E.P. Xing. 2011. Kernel
embeddings of latent tree graphical models. In Pro-
ceedings of NIPS.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a.
From baby steps to leapfrog: how less is more in
unsupervised dependency parsing. In Proceedings
of NAACL.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D.
Manning. 2010b. Viterbi training improves un-
supervised dependency parsing. In Proceedings of
CoNLL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2013.
Breaking out of local optima with count transforms
and model recombination: A study in grammar in-
duction. In Proceedings of EMNLP.
J. P. Turian, L.-A. Ratinov, and Y. Bengio. 2010. Word
representations: A simple and general method for
semi-supervised learning. In Proceedings of ACL.
S. Zhou, J. Lafferty, and L. Wasserman. 2010. Time
varying undirected graphs. Machine Learning,
80(2-3):295–319.
</reference>
<page confidence="0.869523">
1072
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.903515">
<title confidence="0.999164">Spectral Unsupervised Parsing with Additive Tree Metrics</title>
<author confidence="0.999957">Ankur P Parikh</author>
<affiliation confidence="0.9999045">School of Computer Science Carnegie Mellon University</affiliation>
<email confidence="0.996129">apparikh@cs.cmu.edu</email>
<author confidence="0.999534">B Shay</author>
<affiliation confidence="0.998998">School of University of</affiliation>
<email confidence="0.961884">scohen@inf.ed.ac.uk</email>
<author confidence="0.998879">P Eric</author>
<affiliation confidence="0.997915">School of Computer Carnegie Mellon</affiliation>
<email confidence="0.999462">epxing@cs.cmu.edu</email>
<abstract confidence="0.996731">We propose a spectral approach for unsupervised constituent parsing that comes with theoretical guarantees on latent structure recovery. Our approach is grammarless – we directly learn the bracketing structure of a given sentence without using a grammar model. The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples. Although finding the “minimal” latent tree is NP-hard in general, for the case of projective trees we find that it can be found using bilexical parsing algorithms. Empirically, our algorithm performs favorably compared to the constituent context model of Klein and Manning (2002) without the need for careful initialization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Anandkumar</author>
<author>K Chaudhuri</author>
<author>D Hsu</author>
<author>S M Kakade</author>
<author>L Song</author>
<author>T Zhang</author>
</authors>
<title>Spectral methods for learning multivariate latent tree structure. arXiv preprint arXiv:1107.1283.</title>
<date>2011</date>
<contexts>
<context position="4876" citStr="Anandkumar et al., 2011" startWordPosition="746" endWordPosition="749">iables permitting the development of “spec1062 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1062–1072, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tral” methods that can lead to provably correct solutions. In particular we leverage the concept of additive tree metrics (Buneman, 1971; Buneman, 1974) in phylogenetics and machine learning that can create a special distance metric among the observed variables as a function of the underlying spectral dependencies (Choi et al., 2011; Song et al., 2011; Anandkumar et al., 2011; Ishteva et al., 2012). Additive tree metrics can be leveraged by “meta-algorithms” such as neighbor-joining (Saitou and Nei, 1987) and recursive grouping (Choi et al., 2011) to provide consistent learning algorithms for latent trees. Moreover, we show that it is desirable to learn the “minimal” latent tree based on the tree metric (“minimum evolution” in phylogenetics). While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bilexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). Unlike in phyl</context>
<context position="12293" citStr="Anandkumar et al., 2011" startWordPosition="2067" endWordPosition="2070">n, let T be the set of binary bracketings. The complete generative model that we follow is then: • Generate a tag sequence x = (x1, ... , xt) • Decide on u(x) E U, the undirected latent tree that x maps to. • Set t E T by computing t = hdir(u). • Set θ E O by computing θ = θ(x). • Generate a tuple v = (w1, ... , wt, z1, ..., zH) where wi E Rp, zj E Rm according to Eq. 1. See Figure 1 (left) for an example. The Direction Mapping hdir. Generating a bracketing via an undirected tree enables us to build on existing methods for structure learning of latent-tree graphical models (Choi et al., 2011; Anandkumar et al., 2011). Our learning algorithm focuses on recovering the undirected tree based for the generative model that was described above. This undirected tree is converted into a directed tree by applying hdir. The mapping hdir works in three steps: • It first chooses a top bracket ([1, R − 1], [R, `]) where R is the mid-point of the bracket and ` is the length of the sentence. • It marks the edge ei,j that splits the tree according to the top bracket as the “root edge” (marked in red in Figure 1(center)) • It then creates t from u by directing the tree outward from ei,j as shown in Figure 1(center) H p(w, </context>
<context position="19569" citStr="Anandkumar et al., 2011" startWordPosition="3436" endWordPosition="3439">er to the leaves in A, B, G, and H respectively. Then for any a∗ E A∗, b∗ E B∗, g∗ E G∗, and h∗ E H∗ it can be shown that: d(i, j) = 4 (d(a∗, g∗) + d(a∗, h∗) + d(b∗, g∗) )+d(b∗, h∗) − 2d(a∗, b∗) − 2d(g∗, h∗) (4) Empirically, one can obtain a more robust empirical estimate d(i, j) by averaging over all valid choices of a∗, b∗ in Eq. 3 and all valid choices of a∗, b∗, g∗, h∗ in Eq. 4 (Desper and Gascuel, 2005). 3.2 Constructing a Spectral Additive Metric In constructing our distance metric, we begin with the following assumption on the distribution in Eq. 1 (analogous to the assumptions made in Anandkumar et al., 2011). Assumption 1(Linear, Rank m, Means) E[zi|πx(zi), x] = A(zi|zπx(zi),x)πx(zi) Vi E [H] where A(zi|πx(zi),x) E Rm×m has rank m. E[wi|πx(wi),x] = C(wi|πx(wi),x)πx(wi) Vi E [f(x)] where C(wi|πx(wi) x) E Rp×m has rank m. Also assume that E[ziz&gt;i |x] has rank m Vi E [H]. Note that the matrices A and C are a direct function of θ(x), but we do not specify a model family for θ(x). The only restriction is in the form of the above assumption. If wi and zi were discrete, represented as binary vectors, the above assumption would correspond to requiring all conditional probability tables in the latent tree</context>
<context position="20815" citStr="Anandkumar et al. (2011)" startWordPosition="3655" endWordPosition="3658">ption 1 allows for the wi to be high dimensional features, as long as the expectation requirement above is satisfied. Similar assumptions are made with spectral parameter learning methods e.g. Hsu et al. (2009), Bailly et al. (2009), Parikh et al. (2011), and Cohen et al. (2012). Furthermore, Assumption 1 makes it explicit that regardless of the size of p, the relationships among the variables in the latent tree are restricted to be of rank m, and are thus low rank since p &gt; m. To leverage this low rank structure, we propose using the following additive metric, a normalized variant of that in Anandkumar et al. (2011): dspectral(i, j) = − log Λm(Σx(i,j)) +12 log Λm(Σx(i,i)) + 2 1 log Λm(Σx(j,j)) (5) where Λm(A) denotes the product of the top m singular values of A and Σx(i, j) := E[viv&gt;j |x], i.e. the uncentered cross-covariance matrix. We can then show that this metric is additive: pectral is Lemma 1 If Assumption 1 holds then, ds an additive tree metric (Definition 1). A proof is in the supplementary for completeness. From here, we use d to denote dspectral, since that is the metric we use for our learning algorithm. 1066 3.3 Recovering the Minimal Projective Latent Tree It has been shown (Rzhetsky and N</context>
</contexts>
<marker>Anandkumar, Chaudhuri, Hsu, Kakade, Song, Zhang, 2011</marker>
<rawString>A. Anandkumar, K. Chaudhuri, D. Hsu, S. M. Kakade, L. Song, and T. Zhang. 2011. Spectral methods for learning multivariate latent tree structure. arXiv preprint arXiv:1107.1283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bailly</author>
<author>F Denis</author>
<author>L Ralaivola</author>
</authors>
<title>Grammatical inference as a principal component analysis problem.</title>
<date>2009</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="20423" citStr="Bailly et al. (2009)" startWordPosition="3585" endWordPosition="3588">E[ziz&gt;i |x] has rank m Vi E [H]. Note that the matrices A and C are a direct function of θ(x), but we do not specify a model family for θ(x). The only restriction is in the form of the above assumption. If wi and zi were discrete, represented as binary vectors, the above assumption would correspond to requiring all conditional probability tables in the latent tree to have rank m. Assumption 1 allows for the wi to be high dimensional features, as long as the expectation requirement above is satisfied. Similar assumptions are made with spectral parameter learning methods e.g. Hsu et al. (2009), Bailly et al. (2009), Parikh et al. (2011), and Cohen et al. (2012). Furthermore, Assumption 1 makes it explicit that regardless of the size of p, the relationships among the variables in the latent tree are restricted to be of rank m, and are thus low rank since p &gt; m. To leverage this low rank structure, we propose using the following additive metric, a normalized variant of that in Anandkumar et al. (2011): dspectral(i, j) = − log Λm(Σx(i,j)) +12 log Λm(Σx(i,i)) + 2 1 log Λm(Σx(j,j)) (5) where Λm(A) denotes the product of the top m singular values of A and Σx(i, j) := E[viv&gt;j |x], i.e. the uncentered cross-cov</context>
</contexts>
<marker>Bailly, Denis, Ralaivola, 2009</marker>
<rawString>R. Bailly, F. Denis, and L. Ralaivola. 2009. Grammatical inference as a principal component analysis problem. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bailly</author>
<author>X Carreras</author>
<author>F M Luque</author>
<author>A Quattoni</author>
</authors>
<title>Unsupervised spectral learning of WCFG as low-rank matrix completion.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3133" citStr="Bailly et al., 2013" startWordPosition="477" endWordPosition="480">s to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or matrix completion (Bailly et al., 2013). These novel perspectives offer strong theoretical guarantees but are not designed to achieve competitive empirical results. In this paper, we suggest a different approach, to provide a first step to bridging this theoryexperiment gap. More specifically, we approach unsupervised constituent parsing from the perspective of structure learning as opposed to parameter learning. We associate each sentence with an undirected latent tree graphical model, which is a tree consisting of both observed variables (corresponding to the words in the sentence) and an additional set of latent variables that a</context>
</contexts>
<marker>Bailly, Carreras, Luque, Quattoni, 2013</marker>
<rawString>R. Bailly, X. Carreras, F. M. Luque, and A. Quattoni. 2013. Unsupervised spectral learning of WCFG as low-rank matrix completion. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>P V Desouza</author>
<author>R L Mercer</author>
<author>V J D Pietra</author>
<author>J C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="30596" citStr="Brown et al., 1992" startWordPosition="5418" endWordPosition="5421">29.6 15.3 15.2 ≤ 40 26.3 13.5 13.9 13.8 Table 1: Comparison of different CCM variants on English (training). U stands for universal POS tagset, OB stands for conjoining original POS tags with Brown clusters and UB stands for conjoining universal POS tags with Brown clusters. The best setting is just the vanilla setting, CCM. • Otherwise find the first non-participle verb (say at index j) and return ([0, j − 1], [j, `(x)]). • If no verb exists, return ([0,1], [1, `(x)]). Word embeddings As mentioned earlier, each wi can be an arbitrary feature vector. For all languages we use Brown clustering (Brown et al., 1992) to construct a log(C) + C feature vector where the first log(C) elements indicate which mergable cluster the word belongs to, and the last C elements indicate the cluster identity. For English, more sophisticated word embeddings are easily obtainable, and we experiment with neural word embeddings Turian et al. (2010) of length 50. We also explored two types of CCA embeddings: OSCCA and TSCCA, given in Dhillon et al. (2012). The OSCCA embeddings behaved better, so we only report its results. Choice of kernel For our experiments, we use the kernel Kγ(j, k, j(&apos;, k&apos; |x, x&apos;) = max { 0,1 − κ (j, k,</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>P. F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra, and J.C. Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O P Buneman</author>
</authors>
<title>The recovery of trees from measures of dissimilarity. Mathematics in the archaeological and historical sciences.</title>
<date>1971</date>
<contexts>
<context position="4634" citStr="Buneman, 1971" startWordPosition="709" endWordPosition="710">arning of latent trees is substantially more complicated than in observed models. As before, one solution would be local search heuristics. Intuitively, however, latent tree models encode low rank dependencies among the observed variables permitting the development of “spec1062 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1062–1072, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tral” methods that can lead to provably correct solutions. In particular we leverage the concept of additive tree metrics (Buneman, 1971; Buneman, 1974) in phylogenetics and machine learning that can create a special distance metric among the observed variables as a function of the underlying spectral dependencies (Choi et al., 2011; Song et al., 2011; Anandkumar et al., 2011; Ishteva et al., 2012). Additive tree metrics can be leveraged by “meta-algorithms” such as neighbor-joining (Saitou and Nei, 1987) and recursive grouping (Choi et al., 2011) to provide consistent learning algorithms for latent trees. Moreover, we show that it is desirable to learn the “minimal” latent tree based on the tree metric (“minimum evolution” in</context>
</contexts>
<marker>Buneman, 1971</marker>
<rawString>O. P. Buneman. 1971. The recovery of trees from measures of dissimilarity. Mathematics in the archaeological and historical sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buneman</author>
</authors>
<title>A note on the metric properties of trees.</title>
<date>1974</date>
<journal>Journal of Combinatorial Theory, Series B,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="4650" citStr="Buneman, 1974" startWordPosition="711" endWordPosition="712">t trees is substantially more complicated than in observed models. As before, one solution would be local search heuristics. Intuitively, however, latent tree models encode low rank dependencies among the observed variables permitting the development of “spec1062 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1062–1072, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tral” methods that can lead to provably correct solutions. In particular we leverage the concept of additive tree metrics (Buneman, 1971; Buneman, 1974) in phylogenetics and machine learning that can create a special distance metric among the observed variables as a function of the underlying spectral dependencies (Choi et al., 2011; Song et al., 2011; Anandkumar et al., 2011; Ishteva et al., 2012). Additive tree metrics can be leveraged by “meta-algorithms” such as neighbor-joining (Saitou and Nei, 1987) and recursive grouping (Choi et al., 2011) to provide consistent learning algorithms for latent trees. Moreover, we show that it is desirable to learn the “minimal” latent tree based on the tree metric (“minimum evolution” in phylogenetics).</context>
</contexts>
<marker>Buneman, 1974</marker>
<rawString>P. Buneman. 1974. A note on the metric properties of trees. Journal of Combinatorial Theory, Series B, 17(1):48–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Choi</author>
<author>V YF Tan</author>
<author>A Anandkumar</author>
<author>A S Willsky</author>
</authors>
<title>Learning latent tree graphical models.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--1771</pages>
<contexts>
<context position="4832" citStr="Choi et al., 2011" startWordPosition="738" endWordPosition="741">nk dependencies among the observed variables permitting the development of “spec1062 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1062–1072, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tral” methods that can lead to provably correct solutions. In particular we leverage the concept of additive tree metrics (Buneman, 1971; Buneman, 1974) in phylogenetics and machine learning that can create a special distance metric among the observed variables as a function of the underlying spectral dependencies (Choi et al., 2011; Song et al., 2011; Anandkumar et al., 2011; Ishteva et al., 2012). Additive tree metrics can be leveraged by “meta-algorithms” such as neighbor-joining (Saitou and Nei, 1987) and recursive grouping (Choi et al., 2011) to provide consistent learning algorithms for latent trees. Moreover, we show that it is desirable to learn the “minimal” latent tree based on the tree metric (“minimum evolution” in phylogenetics). While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bilexical parsing algorithm can be used to find an exact solution efficien</context>
<context position="12267" citStr="Choi et al., 2011" startWordPosition="2063" endWordPosition="2066">ection). In addition, let T be the set of binary bracketings. The complete generative model that we follow is then: • Generate a tag sequence x = (x1, ... , xt) • Decide on u(x) E U, the undirected latent tree that x maps to. • Set t E T by computing t = hdir(u). • Set θ E O by computing θ = θ(x). • Generate a tuple v = (w1, ... , wt, z1, ..., zH) where wi E Rp, zj E Rm according to Eq. 1. See Figure 1 (left) for an example. The Direction Mapping hdir. Generating a bracketing via an undirected tree enables us to build on existing methods for structure learning of latent-tree graphical models (Choi et al., 2011; Anandkumar et al., 2011). Our learning algorithm focuses on recovering the undirected tree based for the generative model that was described above. This undirected tree is converted into a directed tree by applying hdir. The mapping hdir works in three steps: • It first chooses a top bracket ([1, R − 1], [R, `]) where R is the mid-point of the bracket and ` is the length of the sentence. • It marks the edge ei,j that splits the tree according to the top bracket as the “root edge” (marked in red in Figure 1(center)) • It then creates t from u by directing the tree outward from ei,j as shown i</context>
</contexts>
<marker>Choi, Tan, Anandkumar, Willsky, 2011</marker>
<rawString>M.J. Choi, V. YF Tan, A. Anandkumar, and A.S. Willsky. 2011. Learning latent tree graphical models. The Journal of Machine Learning Research, 12:1771–1812.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K Chow</author>
<author>C N Liu</author>
</authors>
<title>Approximating Discrete Probability Distributions With Dependence Trees.</title>
<date>1968</date>
<booktitle>IEEE Transactions on Information Theory, IT-14:462–467.</booktitle>
<contexts>
<context position="14542" citStr="Chow and Liu, 1968" startWordPosition="2480" endWordPosition="2483">of speech tags. Assume for this section |D(x) |is large (we address the data sparsity issue in §3.4). We can then proceed by learning how to map a POS sequence x to a tree t E T (through u E U) by focusing only on examples in D(x). Directly attempting to maximize the likelihood unfortunately results in an intractable optimization problem and greedy heuristics are often employed (Harmeling and Williams, 2011). Instead we propose a method that is provably consistent and returns a tree that can be mapped to a bracketing using hdir. If all the variables were observed, then the Chow-Liu algorithm (Chow and Liu, 1968) could be used to find the most likely tree structure u E U. The Chow-Liu algorithm essentially computes the distances among all pairs of variables (the negative of the mutual information) and then finds the minimum cost tree. However, the fact that the zi are latent variables makes this strategy substantially more complicated. In particular, it becomes challenging to compute the distances among pairs of latent variables. What is needed is a “special” distance function that allows us to reverse engineer the distances among the latent variables given the distances among the observed variables. </context>
</contexts>
<marker>Chow, Liu, 1968</marker>
<rawString>C. K. Chow and C. N. Liu. 1968. Approximating Discrete Probability Distributions With Dependence Trees. IEEE Transactions on Information Theory, IT-14:462–467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="2279" citStr="Cohen and Smith, 2009" startWordPosition="349" endWordPosition="352">ce (i.e. sentences and part-of-speech tags), and is much cheaper than the syntactically annotated data required for supervised training. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likelihood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>S. B. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Empirical risk minimization for probabilistic grammars: Sample complexity and hardness of learning.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>3</issue>
<contexts>
<context position="2496" citStr="Cohen and Smith, 2012" startWordPosition="382" endWordPosition="385"> generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likelihood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or ma</context>
</contexts>
<marker>Cohen, Smith, 2012</marker>
<rawString>S. B. Cohen and N. A. Smith. 2012. Empirical risk minimization for probabilistic grammars: Sample complexity and hardness of learning. Computational Linguistics, 38(3):479–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>K Stratos</author>
<author>M Collins</author>
<author>D P Foster</author>
<author>L Ungar</author>
</authors>
<title>Spectral learning of latent-variable PCFGs.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="20470" citStr="Cohen et al. (2012)" startWordPosition="3594" endWordPosition="3597">atrices A and C are a direct function of θ(x), but we do not specify a model family for θ(x). The only restriction is in the form of the above assumption. If wi and zi were discrete, represented as binary vectors, the above assumption would correspond to requiring all conditional probability tables in the latent tree to have rank m. Assumption 1 allows for the wi to be high dimensional features, as long as the expectation requirement above is satisfied. Similar assumptions are made with spectral parameter learning methods e.g. Hsu et al. (2009), Bailly et al. (2009), Parikh et al. (2011), and Cohen et al. (2012). Furthermore, Assumption 1 makes it explicit that regardless of the size of p, the relationships among the variables in the latent tree are restricted to be of rank m, and are thus low rank since p &gt; m. To leverage this low rank structure, we propose using the following additive metric, a normalized variant of that in Anandkumar et al. (2011): dspectral(i, j) = − log Λm(Σx(i,j)) +12 log Λm(Σx(i,i)) + 2 1 log Λm(Σx(j,j)) (5) where Λm(A) denotes the product of the top m singular values of A and Σx(i, j) := E[viv&gt;j |x], i.e. the uncentered cross-covariance matrix. We can then show that this metr</context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2012</marker>
<rawString>S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Ungar. 2012. Spectral learning of latent-variable PCFGs. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Desper</author>
<author>O Gascuel</author>
</authors>
<title>The minimum evolution distance-based approach to phylogenetic inference. Mathematics of evolution and phylogeny,</title>
<date>2005</date>
<pages>1--32</pages>
<contexts>
<context position="5320" citStr="Desper and Gascuel, 2005" startWordPosition="814" endWordPosition="817">an create a special distance metric among the observed variables as a function of the underlying spectral dependencies (Choi et al., 2011; Song et al., 2011; Anandkumar et al., 2011; Ishteva et al., 2012). Additive tree metrics can be leveraged by “meta-algorithms” such as neighbor-joining (Saitou and Nei, 1987) and recursive grouping (Choi et al., 2011) to provide consistent learning algorithms for latent trees. Moreover, we show that it is desirable to learn the “minimal” latent tree based on the tree metric (“minimum evolution” in phylogenetics). While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bilexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). Unlike in phylogenetics and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own parse tree. This leads to a severe data sparsity problem even for moderately long sentences. To handle this issue, we present a strategy that is inspired by ideas from kernel smoothing in the statistics community (Zhou et al., 2010; Kolar et al., 2010b; Kolar et al., 2010a). This al</context>
<context position="19356" citStr="Desper and Gascuel, 2005" startWordPosition="3403" endWordPosition="3406">her neighbors a E [M] and b E [M], and similarly, j has exactly other two neighbors g E [M] and h E [M]. Let A denote the set of nodes closer to a than i, and analogously for B, G, and H. Let A∗, B∗, G∗, and H∗ refer to the leaves in A, B, G, and H respectively. Then for any a∗ E A∗, b∗ E B∗, g∗ E G∗, and h∗ E H∗ it can be shown that: d(i, j) = 4 (d(a∗, g∗) + d(a∗, h∗) + d(b∗, g∗) )+d(b∗, h∗) − 2d(a∗, b∗) − 2d(g∗, h∗) (4) Empirically, one can obtain a more robust empirical estimate d(i, j) by averaging over all valid choices of a∗, b∗ in Eq. 3 and all valid choices of a∗, b∗, g∗, h∗ in Eq. 4 (Desper and Gascuel, 2005). 3.2 Constructing a Spectral Additive Metric In constructing our distance metric, we begin with the following assumption on the distribution in Eq. 1 (analogous to the assumptions made in Anandkumar et al., 2011). Assumption 1(Linear, Rank m, Means) E[zi|πx(zi), x] = A(zi|zπx(zi),x)πx(zi) Vi E [H] where A(zi|πx(zi),x) E Rm×m has rank m. E[wi|πx(wi),x] = C(wi|πx(wi),x)πx(wi) Vi E [f(x)] where C(wi|πx(wi) x) E Rp×m has rank m. Also assume that E[ziz&gt;i |x] has rank m Vi E [H]. Note that the matrices A and C are a direct function of θ(x), but we do not specify a model family for θ(x). The only re</context>
<context position="22242" citStr="Desper and Gascuel, 2005" startWordPosition="3909" endWordPosition="3912">er in u and d(i, j) is computed using Eq. 3 and Eq. 4. Note that the metric d we use in defining c(u) is based on the expectations from the true distribution. In practice, the true distribution is unknown, and therefore we use an approximation for the distance metric ˆd. As we discussed in §3.1 all elements of the distance matrix are functions of observable quantities if the underlying tree u is known. However, only the word-word sub-block DWW can be directly estimated from the data without knowledge of the tree structure. This subtlety makes solving the minimization problem in Eq. 6 NP-hard (Desper and Gascuel, 2005) if u is allowed to be an arbitrary undirected tree. However, if we restrict u to be in U, as we do in the above, then maximizing ˆc(u) over U can be solved using the bilexical parsing algorithm from Eisner and Satta (1999). This is because the computation of the other sub-blocks of the distance matrix only depend on the partitions of the nodes shown in Figure 3 into A, B, G, and H, and not on the entire tree structure. Therefore, the procedure to find a bracketing for a given POS tag x isb Dto first estimate the distance matrix sub-block WW from raw text data (see §3.4), and then solve the op</context>
</contexts>
<marker>Desper, Gascuel, 2005</marker>
<rawString>R. Desper and O. Gascuel. 2005. The minimum evolution distance-based approach to phylogenetic inference. Mathematics of evolution and phylogeny, pages 1–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Dhillon</author>
<author>J Rodu</author>
<author>D P Foster</author>
<author>L H Ungar</author>
</authors>
<title>Two step cca: A new spectral method for estimating vector models of words.</title>
<date>2012</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="31023" citStr="Dhillon et al. (2012)" startWordPosition="5490" endWordPosition="5493">. • If no verb exists, return ([0,1], [1, `(x)]). Word embeddings As mentioned earlier, each wi can be an arbitrary feature vector. For all languages we use Brown clustering (Brown et al., 1992) to construct a log(C) + C feature vector where the first log(C) elements indicate which mergable cluster the word belongs to, and the last C elements indicate the cluster identity. For English, more sophisticated word embeddings are easily obtainable, and we experiment with neural word embeddings Turian et al. (2010) of length 50. We also explored two types of CCA embeddings: OSCCA and TSCCA, given in Dhillon et al. (2012). The OSCCA embeddings behaved better, so we only report its results. Choice of kernel For our experiments, we use the kernel Kγ(j, k, j(&apos;, k&apos; |x, x&apos;) = max { 0,1 − κ (j, k, j&apos;, k&apos; |x, x&apos;) l l &apos;Y f where γ denotes the user-specified bandwidth, and κ(j,k,j&apos;,k&apos;|x,x&apos;) = |j − k |− |j&apos; − k&apos; |if |j − k |+ |j&apos; − k&apos;| x(j) = x(j&apos;) and x(k&apos;) = x(k), and sign(j − k) = sign(j&apos; − k&apos;) (and oc otherwise). The kernel is non-zero if and only if the tags at position j and k in x are identical to the ones in position j&apos; and k&apos; in x&apos;, and if the direction between j and k is identical to the one between j&apos; and k&apos;.</context>
</contexts>
<marker>Dhillon, Rodu, Foster, Ungar, 2012</marker>
<rawString>P. S. Dhillon, J. Rodu, D. P. Foster, and L. H. Ungar. 2012. Two step cca: A new spectral method for estimating vector models of words. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>G Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="5460" citStr="Eisner and Satta, 1999" startWordPosition="837" endWordPosition="840">g et al., 2011; Anandkumar et al., 2011; Ishteva et al., 2012). Additive tree metrics can be leveraged by “meta-algorithms” such as neighbor-joining (Saitou and Nei, 1987) and recursive grouping (Choi et al., 2011) to provide consistent learning algorithms for latent trees. Moreover, we show that it is desirable to learn the “minimal” latent tree based on the tree metric (“minimum evolution” in phylogenetics). While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bilexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). Unlike in phylogenetics and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own parse tree. This leads to a severe data sparsity problem even for moderately long sentences. To handle this issue, we present a strategy that is inspired by ideas from kernel smoothing in the statistics community (Zhou et al., 2010; Kolar et al., 2010b; Kolar et al., 2010a). This allows principled sharing of samples from different but similar underlying distributions. We provide theoretical guarantees on the recovery of</context>
<context position="22465" citStr="Eisner and Satta (1999)" startWordPosition="3952" endWordPosition="3955">e an approximation for the distance metric ˆd. As we discussed in §3.1 all elements of the distance matrix are functions of observable quantities if the underlying tree u is known. However, only the word-word sub-block DWW can be directly estimated from the data without knowledge of the tree structure. This subtlety makes solving the minimization problem in Eq. 6 NP-hard (Desper and Gascuel, 2005) if u is allowed to be an arbitrary undirected tree. However, if we restrict u to be in U, as we do in the above, then maximizing ˆc(u) over U can be solved using the bilexical parsing algorithm from Eisner and Satta (1999). This is because the computation of the other sub-blocks of the distance matrix only depend on the partitions of the nodes shown in Figure 3 into A, B, G, and H, and not on the entire tree structure. Therefore, the procedure to find a bracketing for a given POS tag x isb Dto first estimate the distance matrix sub-block WW from raw text data (see §3.4), and then solve the optimization problem arg minuEU ˆc(u) using a variant of the EisnerSatta algorithm where ˆc(u) is identical to c(u) in Eq. 6, with d replaced with ˆd. Summary. We first defined a generative model that describes how a sentence</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>J. Eisner and G. Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Erd˜os</author>
<author>M Steel</author>
<author>L Sz´ekely</author>
<author>T Warnow</author>
</authors>
<title>A few logs suffice to build (almost) all trees: Part ii.</title>
<date>1999</date>
<journal>Theoretical Computer Science,</journal>
<volume>221</volume>
<issue>1</issue>
<marker>Erd˜os, Steel, Sz´ekely, Warnow, 1999</marker>
<rawString>P. Erd˜os, M. Steel, L. Sz´ekely, and T. Warnow. 1999. A few logs suffice to build (almost) all trees: Part ii. Theoretical Computer Science, 221(1):77–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gillenwater</author>
<author>K Ganchev</author>
<author>J Grac¸a</author>
<author>F Pereira</author>
<author>B Taskar</author>
</authors>
<title>Sparsity in dependency grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Gillenwater, Ganchev, Grac¸a, Pereira, Taskar, 2010</marker>
<rawString>J. Gillenwater, K. Ganchev, J. Grac¸a, F. Pereira, and B. Taskar. 2010. Sparsity in dependency grammar induction. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Concavity and initialization for unsupervised dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="2714" citStr="Gimpel and Smith, 2012" startWordPosition="415" endWordPosition="418">hat are estimated by identifying a local maximum of an objective function such as the likelihood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or matrix completion (Bailly et al., 2013). These novel perspectives offer strong theoretical guarantees but are not designed to achieve competitive empirical results. In this paper, we suggest a different approach, to prov</context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>K. Gimpel and N.A. Smith. 2012. Concavity and initialization for unsupervised dependency parsing. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Golland</author>
<author>J DeNero</author>
<author>J Uszkoreit</author>
</authors>
<title>A feature-rich constituent context model for grammar induction.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2376" citStr="Golland et al., 2012" startWordPosition="365" endWordPosition="368"> data required for supervised training. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likelihood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other </context>
</contexts>
<marker>Golland, DeNero, Uszkoreit, 2012</marker>
<rawString>D. Golland, J. DeNero, and J. Uszkoreit. 2012. A feature-rich constituent context model for grammar induction. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gormley</author>
<author>J Eisner</author>
</authors>
<title>Nonconvex global optimization for latent-variable models.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2581" citStr="Gormley and Eisner, 2013" startWordPosition="397" endWordPosition="400">inek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likelihood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or matrix completion (Bailly et al., 2013). These novel perspectives offer strong theoreti</context>
</contexts>
<marker>Gormley, Eisner, 2013</marker>
<rawString>M. Gormley and J. Eisner. 2013. Nonconvex global optimization for latent-variable models. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harmeling</author>
<author>C KI Williams</author>
</authors>
<title>Greedy learning of binary latent trees.</title>
<date>2011</date>
<journal>Pattern Analysis and Machine Intelligence, IEEE Transactions on,</journal>
<volume>33</volume>
<issue>6</issue>
<contexts>
<context position="14334" citStr="Harmeling and Williams, 2011" startWordPosition="2443" endWordPosition="2446"> [(w(i), x(i))]Ni=1. To get an intuition about the algorithm, consider a partition of the set of examples D into D(x) = {(w(i), x(i)) E D|x(i) = x}, i.e. each section in the partition has an identical sequence of part of speech tags. Assume for this section |D(x) |is large (we address the data sparsity issue in §3.4). We can then proceed by learning how to map a POS sequence x to a tree t E T (through u E U) by focusing only on examples in D(x). Directly attempting to maximize the likelihood unfortunately results in an intractable optimization problem and greedy heuristics are often employed (Harmeling and Williams, 2011). Instead we propose a method that is provably consistent and returns a tree that can be mapped to a bracketing using hdir. If all the variables were observed, then the Chow-Liu algorithm (Chow and Liu, 1968) could be used to find the most likely tree structure u E U. The Chow-Liu algorithm essentially computes the distances among all pairs of variables (the negative of the mutual information) and then finds the minimum cost tree. However, the fact that the zi are latent variables makes this strategy substantially more complicated. In particular, it becomes challenging to compute the distances</context>
</contexts>
<marker>Harmeling, Williams, 2011</marker>
<rawString>S. Harmeling and C. KI Williams. 2011. Greedy learning of binary latent trees. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(6):1087–1097.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hastie</author>
<author>R Tibshirani</author>
<author>J Friedman</author>
</authors>
<title>The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Series in Statistics.</title>
<date>2009</date>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="26289" citStr="Hastie et al., 2009" startWordPosition="4654" endWordPosition="4657">t be very similar to Σx(1, 2) because the noun and the determiner appear in a different syntactic context. The observation that the covariance matrices depend on local syntactic context is the main driving force behind our solution. The local syntactic context acts as an “anchor,” which enhances or replaces a word index in a sentence with local syntactic context. More formally, an anchor is a function G that maps a word index j and a sequence of POS tags x to a local context G(j, x). The anchor we use is G(j, x) = (j, xj). Then, the covariance matrices Σx are estimated using kernel smoothing (Hastie et al., 2009), where the smoother tests similarity between the different anchors G(j, x). The full learning algorithm is given in Figure 1. The first step in the algorithm is to estimate the covariance matrix block ix(i)(j, k) for each training example x(&apos;) and each pair of preterminal positions (j, k) in x(&apos;). Instead of computing this block by computing the empirical covariance matrix for positions (j, k) in the data D(x), the algorithm uses all of the pairs (j0, k0) from all of N training examples. It averages the empirical covariance matrices from these contexts using a kernel weight, which gives a sim</context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2009</marker>
<rawString>T. Hastie, R. Tibshirani, and J. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Series in Statistics. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W P Headden</author>
<author>M Johnson</author>
<author>D McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="2301" citStr="Headden et al., 2009" startWordPosition="353" endWordPosition="356">part-of-speech tags), and is much cheaper than the syntactically annotated data required for supervised training. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likelihood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while emp</context>
</contexts>
<marker>Headden, Johnson, McClosky, 2009</marker>
<rawString>W. P. Headden, M. Johnson, and D. McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hsu</author>
<author>S Kakade</author>
<author>T Zhang</author>
</authors>
<title>A spectral algorithm for learning hidden Markov models.</title>
<date>2009</date>
<booktitle>In Proceedings of COLT.</booktitle>
<contexts>
<context position="20401" citStr="Hsu et al. (2009)" startWordPosition="3581" endWordPosition="3584">. Also assume that E[ziz&gt;i |x] has rank m Vi E [H]. Note that the matrices A and C are a direct function of θ(x), but we do not specify a model family for θ(x). The only restriction is in the form of the above assumption. If wi and zi were discrete, represented as binary vectors, the above assumption would correspond to requiring all conditional probability tables in the latent tree to have rank m. Assumption 1 allows for the wi to be high dimensional features, as long as the expectation requirement above is satisfied. Similar assumptions are made with spectral parameter learning methods e.g. Hsu et al. (2009), Bailly et al. (2009), Parikh et al. (2011), and Cohen et al. (2012). Furthermore, Assumption 1 makes it explicit that regardless of the size of p, the relationships among the variables in the latent tree are restricted to be of rank m, and are thus low rank since p &gt; m. To leverage this low rank structure, we propose using the following additive metric, a normalized variant of that in Anandkumar et al. (2011): dspectral(i, j) = − log Λm(Σx(i,j)) +12 log Λm(Σx(i,i)) + 2 1 log Λm(Σx(j,j)) (5) where Λm(A) denotes the product of the top m singular values of A and Σx(i, j) := E[viv&gt;j |x], i.e. th</context>
</contexts>
<marker>Hsu, Kakade, Zhang, 2009</marker>
<rawString>D. Hsu, S. Kakade, and T. Zhang. 2009. A spectral algorithm for learning hidden Markov models. In Proceedings of COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hsu</author>
<author>S M Kakade</author>
<author>P Liang</author>
</authors>
<title>Identifiability and unmixing of latent parse trees. arXiv preprint arXiv:1206.3137.</title>
<date>2012</date>
<contexts>
<context position="3090" citStr="Hsu et al., 2012" startWordPosition="469" endWordPosition="472">(Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or matrix completion (Bailly et al., 2013). These novel perspectives offer strong theoretical guarantees but are not designed to achieve competitive empirical results. In this paper, we suggest a different approach, to provide a first step to bridging this theoryexperiment gap. More specifically, we approach unsupervised constituent parsing from the perspective of structure learning as opposed to parameter learning. We associate each sentence with an undirected latent tree graphical model, which is a tree consisting of both observed variables (corresponding to the words in the sentence) and a</context>
</contexts>
<marker>Hsu, Kakade, Liang, 2012</marker>
<rawString>D. Hsu, S. M. Kakade, and P. Liang. 2012. Identifiability and unmixing of latent parse trees. arXiv preprint arXiv:1206.3137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ishteva</author>
<author>H Park</author>
<author>L Song</author>
</authors>
<title>Unfolding latent tree structures using 4th order tensors. arXiv preprint arXiv:1210.1258.</title>
<date>2012</date>
<contexts>
<context position="4899" citStr="Ishteva et al., 2012" startWordPosition="750" endWordPosition="753">elopment of “spec1062 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1062–1072, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tral” methods that can lead to provably correct solutions. In particular we leverage the concept of additive tree metrics (Buneman, 1971; Buneman, 1974) in phylogenetics and machine learning that can create a special distance metric among the observed variables as a function of the underlying spectral dependencies (Choi et al., 2011; Song et al., 2011; Anandkumar et al., 2011; Ishteva et al., 2012). Additive tree metrics can be leveraged by “meta-algorithms” such as neighbor-joining (Saitou and Nei, 1987) and recursive grouping (Choi et al., 2011) to provide consistent learning algorithms for latent trees. Moreover, we show that it is desirable to learn the “minimal” latent tree based on the tree metric (“minimum evolution” in phylogenetics). While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bilexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). Unlike in phylogenetics and graphical</context>
</contexts>
<marker>Ishteva, Park, Song, 2012</marker>
<rawString>M. Ishteva, H. Park, and L. Song. 2012. Unfolding latent tree structures using 4th order tensors. arXiv preprint arXiv:1210.1258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
</authors>
<title>Basic methods of probabilistic context free grammars.</title>
<date>1992</date>
<publisher>Springer.</publisher>
<contexts>
<context position="1974" citStr="Jelinek et al., 1992" startWordPosition="294" endWordPosition="297">ely, it is more plausible to assume that children obtain only terminal strings of parse trees and not the actual parse trees. This means the unsupervised setting is a better model for studying language acquisition. From the engineering perspective, training data for unsupervised parsing exists in abundance (i.e. sentences and part-of-speech tags), and is much cheaper than the syntactically annotated data required for supervised training. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likelihood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner</context>
</contexts>
<marker>Jelinek, Lafferty, Mercer, 1992</marker>
<rawString>F. Jelinek, J. D. Lafferty, and R. L. Mercer. 1992. Basic methods of probabilistic context free grammars. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>A generative constituent-context model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1090" citStr="Klein and Manning (2002)" startWordPosition="160" endWordPosition="163">oach is grammarless – we directly learn the bracketing structure of a given sentence without using a grammar model. The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples. Although finding the “minimal” latent tree is NP-hard in general, for the case of projective trees we find that it can be found using bilexical parsing algorithms. Empirically, our algorithm performs favorably compared to the constituent context model of Klein and Manning (2002) without the need for careful initialization. 1 Introduction Solutions to the problem of grammar induction have been long sought after since the early days of computational linguistics and are interesting both from cognitive and engineering perspectives. Cognitively, it is more plausible to assume that children obtain only terminal strings of parse trees and not the actual parse trees. This means the unsupervised setting is a better model for studying language acquisition. From the engineering perspective, training data for unsupervised parsing exists in abundance (i.e. sentences and part-of-s</context>
<context position="2689" citStr="Klein and Manning, 2002" startWordPosition="411" endWordPosition="414">ing a set of parameters that are estimated by identifying a local maximum of an objective function such as the likelihood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or matrix completion (Bailly et al., 2013). These novel perspectives offer strong theoretical guarantees but are not designed to achieve competitive empirical results. In this paper, we suggest a di</context>
<context position="29120" citStr="Klein and Manning (2002)" startWordPosition="5151" endWordPosition="5154">ifference in the cost of the true tree compared to the cost of the incorrect trees (A(x)). 4 Experiments We report results on three different languages: English, German, and Chinese. For English we use the Penn treebank (Marcus et al., 1993), with sections 2–21 for training and section 23 for final testing. For German and Chinese we use the Negra treebank and the Chinese treebank respectively and the first 80% of the sentences are used for training and the last 20% for testing. All punctuation from the data is removed.4 We primarily compare our method to the constituent-context model (CCM) of Klein and Manning (2002). We also compare our method to the algorithm of Seginer (2007). 4.1 Experimental Settings Top bracket heuristic Our algorithm requires the top bracket in order to direct the latent tree. In practice, we employ the following heuristic to find the bracket using the following three steps: • If there exists a comma/semicolon/colon at index i that has at least a verb before i and both a noun followed by a verb after i, then return ([0, i − 1], [i, `(x)]) as the top bracket. (Pick the rightmost comma/semicolon/colon if multiple satisfy the criterion). 4We make brief use of punctuation for our top b</context>
<context position="33940" citStr="Klein and Manning (2002)" startWordPosition="6027" endWordPosition="6030">ituent/distituent counts used for smoothing. For both methods we chose the best parameters for sentences of length ` &lt; 10 on the English Penn Treebank (training) and used this set for all other experiments. This resulted in m = 7,γ = 0.4 for our method and 2, 8 for CCM’s extra constituent/distituent counts respectively. We also tried letting CCM choose different hyperparameters for different sentence lengths based on dev-set likelihood, but this gave worse results than holding them fixed. 4.2 Results Test I: Accuracy Table 2 summarizes our results. CCM is used with the initializer proposed in Klein and Manning (2002).5 NN, CC, and BC indicate the performance of our method for neural embeddings, CCA embeddings, and Brown clustering respectively, using the heuristic for hdir de5We used the implementation available at http://tinyurl.com/lhwk5n6. 1069 f NN-O NN CC-O English BC-O BC CCM BC-O German CCM BC-O Chinese CCM CC BC BC train &lt; 10 70.9 69.2 70.4 68.7 71.1 69.3 72.5 64.6 59.9 62.6 64.9 57.3 46.1 &lt; 20 55.1 53.5 53.2 51.6 53.0 51.5 50 52.7 48.7 47.9 51.4 46 22.4 &lt; 40 46.1 44.5 43.6 41.9 43.3 41.8 26.3 46.7 43.6 19.8 42.6 38.6 15 test &lt; 10 69.2 66.7 68.3 65.5 68.9 66.1 70.5 66.4 61.6 64.7 58.0 53.2 40.7 &lt; </context>
<context position="36894" citStr="Klein and Manning (2002)" startWordPosition="6555" endWordPosition="6559">idn’t have neural embeddings for German and Chinese (which worked best for English) and thus only used Brown cluster embeddings. For English, the disparity between NN-O (oracle top bracket) and NN (heuristic top bracket) is rather low suggesting that our top bracket heuristic is rather effective. However, for German and Chinese note that the “BC-O” performs substantially better, suggesting that if we had a better top bracket heuristic our performance would increase. Test II: Sensitivity to initialization The EM algorithm with the CCM requires very careful initialization, which is described in Klein and Manning (2002). If, on the other hand, random initialization is used, the variance of the performance of the CCM varies greatly. Figure 4 shows a histogram of the performance level for sentences of length &lt; 10 for different random initializers. As one can see, for some restarts, CCM obtains accuracies lower than 30% due to local optima. Our method does not suffer from local optima and thus does not require careful initialization. Test III: Comparison to Seginer’s algorithm Our approach is not directly comparable to Seginer’s because he uses punctuation, while we use POS tags. Using Seginer’s parser we were </context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>D. Klein and C. D. Manning. 2002. A generative constituent-context model for improved grammar induction. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kolar</author>
<author>A P Parikh</author>
<author>E P Xing</author>
</authors>
<title>On sparse nonparametric conditional covariance selection.</title>
<date>2010</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="5888" citStr="Kolar et al., 2010" startWordPosition="910" endWordPosition="913">n is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bilexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). Unlike in phylogenetics and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own parse tree. This leads to a severe data sparsity problem even for moderately long sentences. To handle this issue, we present a strategy that is inspired by ideas from kernel smoothing in the statistics community (Zhou et al., 2010; Kolar et al., 2010b; Kolar et al., 2010a). This allows principled sharing of samples from different but similar underlying distributions. We provide theoretical guarantees on the recovery of the correct underlying latent tree and characterize the associated sample complexity under our technique. Empirically we evaluate our method on data in English, German and Chinese. Our algorithm performs favorably to Klein and Manning’s (2002) constituent-context model (CCM), without the need for careful initialization. In addition, we also analyze CCM’s sensitivity to initialization, and compare our results to Seginer’s al</context>
</contexts>
<marker>Kolar, Parikh, Xing, 2010</marker>
<rawString>M. Kolar, A. P. Parikh, and E. P. Xing. 2010a. On sparse nonparametric conditional covariance selection. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kolar</author>
<author>L Song</author>
<author>A Ahmed</author>
<author>E P Xing</author>
</authors>
<title>Estimating time-varying networks.</title>
<date>2010</date>
<journal>The Annals of Applied Statistics,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="5888" citStr="Kolar et al., 2010" startWordPosition="910" endWordPosition="913">n is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bilexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). Unlike in phylogenetics and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own parse tree. This leads to a severe data sparsity problem even for moderately long sentences. To handle this issue, we present a strategy that is inspired by ideas from kernel smoothing in the statistics community (Zhou et al., 2010; Kolar et al., 2010b; Kolar et al., 2010a). This allows principled sharing of samples from different but similar underlying distributions. We provide theoretical guarantees on the recovery of the correct underlying latent tree and characterize the associated sample complexity under our technique. Empirically we evaluate our method on data in English, German and Chinese. Our algorithm performs favorably to Klein and Manning’s (2002) constituent-context model (CCM), without the need for careful initialization. In addition, we also analyze CCM’s sensitivity to initialization, and compare our results to Seginer’s al</context>
</contexts>
<marker>Kolar, Song, Ahmed, Xing, 2010</marker>
<rawString>M. Kolar, L. Song, A. Ahmed, and E. P. Xing. 2010b. Estimating time-varying networks. The Annals of Applied Statistics, 4(1):94–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="25291" citStr="Marcus et al., 1993" startWordPosition="4470" endWordPosition="4473">imation of d from Sparse Data We now address the data sparsity problem, in particular that D(x) can be very small, and therefore estimating d for each POS sequence separately can be problematic.3 In order to estimate d from data, we need to estimate the covariance matrices Ex(i, j) (for i, j E {1, ... , `(x)}) from Eq. 5. To give some motivation to our solution, consider estimating the covariance matrix Ex(1, 2) for the tag sequence x = (DT1, NN2, VBD3, DT4, NN5). D(x) may be insufficient for an accurate empirical es3This data sparsity problem is quite severe – for example, the Penn treebank (Marcus et al., 1993) has a total number of 43,498 sentences, with 42,246 unique POS tag sequences, averaging |D(x) |to be 1.04. 1067 = (RB1, DT2, NN3, VBD4, DT5, ADJ6, NN7). Although x and x0 are not identical, it is likely that Σx0(2, 3) is similar to Σx(1, 2) because the determiner and the noun appear in similar syntactic context. Σx0(5, 7) also may be somewhat similar, but Σx0(2, 7) should not be very similar to Σx(1, 2) because the noun and the determiner appear in a different syntactic context. The observation that the covariance matrices depend on local syntactic context is the main driving force behind our</context>
<context position="28737" citStr="Marcus et al., 1993" startWordPosition="5084" endWordPosition="5087">en with probability 1 − δ, uˆ = u(x). where νx(γ), defined in the supplementary, is a function of the underlying distribution over the tag sequences x and the kernel bandwidth γ. Thus, the sample complexity of our approach depends on the dimensionality of the latent and observed states (m and p), the underlying singular values of the cross-covariance matrices (σ∗(x)) and the difference in the cost of the true tree compared to the cost of the incorrect trees (A(x)). 4 Experiments We report results on three different languages: English, German, and Chinese. For English we use the Penn treebank (Marcus et al., 1993), with sections 2–21 for training and section 23 for final testing. For German and Chinese we use the Negra treebank and the Chinese treebank respectively and the first 80% of the sentences are used for training and the last 20% for testing. All punctuation from the data is removed.4 We primarily compare our method to the constituent-context model (CCM) of Klein and Manning (2002). We also compare our method to the algorithm of Seginer (2007). 4.1 Experimental Settings Top bracket heuristic Our algorithm requires the top bracket in order to direct the latent tree. In practice, we employ the fo</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Parikh</author>
<author>L Song</author>
<author>E P Xing</author>
</authors>
<title>A spectral algorithm for latent tree graphical models.</title>
<date>2011</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="20445" citStr="Parikh et al. (2011)" startWordPosition="3589" endWordPosition="3592"> Vi E [H]. Note that the matrices A and C are a direct function of θ(x), but we do not specify a model family for θ(x). The only restriction is in the form of the above assumption. If wi and zi were discrete, represented as binary vectors, the above assumption would correspond to requiring all conditional probability tables in the latent tree to have rank m. Assumption 1 allows for the wi to be high dimensional features, as long as the expectation requirement above is satisfied. Similar assumptions are made with spectral parameter learning methods e.g. Hsu et al. (2009), Bailly et al. (2009), Parikh et al. (2011), and Cohen et al. (2012). Furthermore, Assumption 1 makes it explicit that regardless of the size of p, the relationships among the variables in the latent tree are restricted to be of rank m, and are thus low rank since p &gt; m. To leverage this low rank structure, we propose using the following additive metric, a normalized variant of that in Anandkumar et al. (2011): dspectral(i, j) = − log Λm(Σx(i,j)) +12 log Λm(Σx(i,i)) + 2 1 log Λm(Σx(j,j)) (5) where Λm(A) denotes the product of the top m singular values of A and Σx(i, j) := E[viv&gt;j |x], i.e. the uncentered cross-covariance matrix. We can</context>
</contexts>
<marker>Parikh, Song, Xing, 2011</marker>
<rawString>A.P. Parikh, L. Song, and E.P. Xing. 2011. A spectral algorithm for latent tree graphical models. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Das</author>
<author>R McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2011</date>
<journal>ArXiv:1104.2086.</journal>
<contexts>
<context position="32496" citStr="Petrov et al. (2011)" startWordPosition="5780" endWordPosition="5783">n j&apos; and k&apos; does better in practice. Choice of data For CCM, we found that if the full dataset (all sentence lengths) is used in training, then performance degrades when evaluating on sentences of length &lt; 10. We therefore restrict the data used with CCM to sentences of length &lt; `, where ` is the maximal sentence length being evaluated. This does not happen with our algorithm, which manages to leverage lexical information whenever more data is available. We therefore use the full data for our method for all lengths. We also experimented with the original POS tags and the universal POS tags of Petrov et al. (2011). Here, we found out that our method does better with the universal part of speech tags. For CCM, we also experimented with the original parts of speech, universal tags (CCM-U), the cross-product of the original parts of speech with the Brown clusters (CCM-OB), and the crossproduct of the universal tags with the Brown clusters (CCM-UB). The results in Table 1 indicate that the vanilla setting is the best for CCM. Thus, for all results, we use universal tags for our method and the original POS tags for CCM. We believe that our approach substitutes the need for fine-grained POS tags with the lex</context>
</contexts>
<marker>Petrov, Das, McDonald, 2011</marker>
<rawString>S. Petrov, D. Das, and R. McDonald. 2011. A universal part-of-speech tagset. ArXiv:1104.2086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rzhetsky</author>
<author>M Nei</author>
</authors>
<title>Theoretical foundation of the minimum-evolution method of phylogenetic inference.</title>
<date>1993</date>
<journal>Molecular Biology and Evolution,</journal>
<volume>10</volume>
<issue>5</issue>
<contexts>
<context position="21424" citStr="Rzhetsky and Nei, 1993" startWordPosition="3762" endWordPosition="3765"> et al. (2011): dspectral(i, j) = − log Λm(Σx(i,j)) +12 log Λm(Σx(i,i)) + 2 1 log Λm(Σx(j,j)) (5) where Λm(A) denotes the product of the top m singular values of A and Σx(i, j) := E[viv&gt;j |x], i.e. the uncentered cross-covariance matrix. We can then show that this metric is additive: pectral is Lemma 1 If Assumption 1 holds then, ds an additive tree metric (Definition 1). A proof is in the supplementary for completeness. From here, we use d to denote dspectral, since that is the metric we use for our learning algorithm. 1066 3.3 Recovering the Minimal Projective Latent Tree It has been shown (Rzhetsky and Nei, 1993) that for any additive tree metric, u(x) can be recovered by solving arg minuEU c(u) for c(u): Xc(u) = d(i, j). (6) (i,j)EEu where Eu is the set of pairs of nodes which are adjacent to each other in u and d(i, j) is computed using Eq. 3 and Eq. 4. Note that the metric d we use in defining c(u) is based on the expectations from the true distribution. In practice, the true distribution is unknown, and therefore we use an approximation for the distance metric ˆd. As we discussed in §3.1 all elements of the distance matrix are functions of observable quantities if the underlying tree u is known. H</context>
</contexts>
<marker>Rzhetsky, Nei, 1993</marker>
<rawString>A. Rzhetsky and M. Nei. 1993. Theoretical foundation of the minimum-evolution method of phylogenetic inference. Molecular Biology and Evolution, 10(5):1073–1095.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Saitou</author>
<author>M Nei</author>
</authors>
<title>The neighbor-joining method: a new method for reconstructing phylogenetic trees. Molecular biology and evolution,</title>
<date>1987</date>
<pages>4--4</pages>
<contexts>
<context position="5008" citStr="Saitou and Nei, 1987" startWordPosition="766" endWordPosition="769">, pages 1062–1072, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tral” methods that can lead to provably correct solutions. In particular we leverage the concept of additive tree metrics (Buneman, 1971; Buneman, 1974) in phylogenetics and machine learning that can create a special distance metric among the observed variables as a function of the underlying spectral dependencies (Choi et al., 2011; Song et al., 2011; Anandkumar et al., 2011; Ishteva et al., 2012). Additive tree metrics can be leveraged by “meta-algorithms” such as neighbor-joining (Saitou and Nei, 1987) and recursive grouping (Choi et al., 2011) to provide consistent learning algorithms for latent trees. Moreover, we show that it is desirable to learn the “minimal” latent tree based on the tree metric (“minimum evolution” in phylogenetics). While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bilexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). Unlike in phylogenetics and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequenc</context>
</contexts>
<marker>Saitou, Nei, 1987</marker>
<rawString>N. Saitou and M. Nei. 1987. The neighbor-joining method: a new method for reconstructing phylogenetic trees. Molecular biology and evolution, 4(4):406–425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Seginer</author>
</authors>
<title>Fast unsupervised incremental parsing.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="6511" citStr="Seginer, 2007" startWordPosition="1001" endWordPosition="1002">et al., 2010a). This allows principled sharing of samples from different but similar underlying distributions. We provide theoretical guarantees on the recovery of the correct underlying latent tree and characterize the associated sample complexity under our technique. Empirically we evaluate our method on data in English, German and Chinese. Our algorithm performs favorably to Klein and Manning’s (2002) constituent-context model (CCM), without the need for careful initialization. In addition, we also analyze CCM’s sensitivity to initialization, and compare our results to Seginer’s algorithm (Seginer, 2007). 2 Learning Setting and Model In this section, we detail the learning setting and a conditional tree model we learn the structure for. 2.1 Learning Setting Let w = (w1,..., wt) be a vector of words corresponding to a sentence of length E. Each wi is represented by a vector in Rp for p E N. The vector is an embedding of the word in some space, choFigure 2: Candidate constituent parses for x = (VBD, DT, NN) (left-correct, right -incorrect) sen from a fixed dictionary that maps word types to Rp. In addition, let x = (x1,..., xt) be the associated vector of part-of-speech (POS) tags (i.e. xi is t</context>
<context position="29183" citStr="Seginer (2007)" startWordPosition="5164" endWordPosition="5165">ect trees (A(x)). 4 Experiments We report results on three different languages: English, German, and Chinese. For English we use the Penn treebank (Marcus et al., 1993), with sections 2–21 for training and section 23 for final testing. For German and Chinese we use the Negra treebank and the Chinese treebank respectively and the first 80% of the sentences are used for training and the last 20% for testing. All punctuation from the data is removed.4 We primarily compare our method to the constituent-context model (CCM) of Klein and Manning (2002). We also compare our method to the algorithm of Seginer (2007). 4.1 Experimental Settings Top bracket heuristic Our algorithm requires the top bracket in order to direct the latent tree. In practice, we employ the following heuristic to find the bracket using the following three steps: • If there exists a comma/semicolon/colon at index i that has at least a verb before i and both a noun followed by a verb after i, then return ([0, i − 1], [i, `(x)]) as the top bracket. (Pick the rightmost comma/semicolon/colon if multiple satisfy the criterion). 4We make brief use of punctuation for our top bracket heuristic detailed below before removing it. timate. How</context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Y. Seginer. 2007. Fast unsupervised incremental parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2256" citStr="Smith and Eisner, 2005" startWordPosition="345" endWordPosition="348">arsing exists in abundance (i.e. sentences and part-of-speech tags), and is much cheaper than the syntactically annotated data required for supervised training. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likelihood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovs</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N. A. Smith and J. Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Song</author>
<author>A P Parikh</author>
<author>E P Xing</author>
</authors>
<title>Kernel embeddings of latent tree graphical models.</title>
<date>2011</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="4851" citStr="Song et al., 2011" startWordPosition="742" endWordPosition="745">ng the observed variables permitting the development of “spec1062 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1062–1072, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tral” methods that can lead to provably correct solutions. In particular we leverage the concept of additive tree metrics (Buneman, 1971; Buneman, 1974) in phylogenetics and machine learning that can create a special distance metric among the observed variables as a function of the underlying spectral dependencies (Choi et al., 2011; Song et al., 2011; Anandkumar et al., 2011; Ishteva et al., 2012). Additive tree metrics can be leveraged by “meta-algorithms” such as neighbor-joining (Saitou and Nei, 1987) and recursive grouping (Choi et al., 2011) to provide consistent learning algorithms for latent trees. Moreover, we show that it is desirable to learn the “minimal” latent tree based on the tree metric (“minimum evolution” in phylogenetics). While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bilexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Sat</context>
</contexts>
<marker>Song, Parikh, Xing, 2011</marker>
<rawString>L. Song, A.P. Parikh, and E.P. Xing. 2011. Kernel embeddings of latent tree graphical models. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>From baby steps to leapfrog: how less is more in unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="2326" citStr="Spitkovsky et al., 2010" startWordPosition="357" endWordPosition="360">and is much cheaper than the syntactically annotated data required for supervised training. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likelihood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, gener</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2010</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a. From baby steps to leapfrog: how less is more in unsupervised dependency parsing. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Viterbi training improves unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="2326" citStr="Spitkovsky et al., 2010" startWordPosition="357" endWordPosition="360">and is much cheaper than the syntactically annotated data required for supervised training. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likelihood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, gener</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, Manning, 2010</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning. 2010b. Viterbi training improves unsupervised dependency parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Breaking out of local optima with count transforms and model recombination: A study in grammar induction.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2872" citStr="Spitkovsky et al., 2013" startWordPosition="439" endWordPosition="442">er, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or matrix completion (Bailly et al., 2013). These novel perspectives offer strong theoretical guarantees but are not designed to achieve competitive empirical results. In this paper, we suggest a different approach, to provide a first step to bridging this theoryexperiment gap. More specifically, we approach unsupervised constituent parsing from the perspective of structure lear</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2013</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2013. Breaking out of local optima with count transforms and model recombination: A study in grammar induction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Turian</author>
<author>L-A Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="30915" citStr="Turian et al. (2010)" startWordPosition="5469" endWordPosition="5472">ng, CCM. • Otherwise find the first non-participle verb (say at index j) and return ([0, j − 1], [j, `(x)]). • If no verb exists, return ([0,1], [1, `(x)]). Word embeddings As mentioned earlier, each wi can be an arbitrary feature vector. For all languages we use Brown clustering (Brown et al., 1992) to construct a log(C) + C feature vector where the first log(C) elements indicate which mergable cluster the word belongs to, and the last C elements indicate the cluster identity. For English, more sophisticated word embeddings are easily obtainable, and we experiment with neural word embeddings Turian et al. (2010) of length 50. We also explored two types of CCA embeddings: OSCCA and TSCCA, given in Dhillon et al. (2012). The OSCCA embeddings behaved better, so we only report its results. Choice of kernel For our experiments, we use the kernel Kγ(j, k, j(&apos;, k&apos; |x, x&apos;) = max { 0,1 − κ (j, k, j&apos;, k&apos; |x, x&apos;) l l &apos;Y f where γ denotes the user-specified bandwidth, and κ(j,k,j&apos;,k&apos;|x,x&apos;) = |j − k |− |j&apos; − k&apos; |if |j − k |+ |j&apos; − k&apos;| x(j) = x(j&apos;) and x(k&apos;) = x(k), and sign(j − k) = sign(j&apos; − k&apos;) (and oc otherwise). The kernel is non-zero if and only if the tags at position j and k in x are identical to the ones </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>J. P. Turian, L.-A. Ratinov, and Y. Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zhou</author>
<author>J Lafferty</author>
<author>L Wasserman</author>
</authors>
<title>Time varying undirected graphs.</title>
<date>2010</date>
<booktitle>Machine Learning,</booktitle>
<pages>80--2</pages>
<contexts>
<context position="5868" citStr="Zhou et al., 2010" startWordPosition="906" endWordPosition="909">While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bilexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). Unlike in phylogenetics and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own parse tree. This leads to a severe data sparsity problem even for moderately long sentences. To handle this issue, we present a strategy that is inspired by ideas from kernel smoothing in the statistics community (Zhou et al., 2010; Kolar et al., 2010b; Kolar et al., 2010a). This allows principled sharing of samples from different but similar underlying distributions. We provide theoretical guarantees on the recovery of the correct underlying latent tree and characterize the associated sample complexity under our technique. Empirically we evaluate our method on data in English, German and Chinese. Our algorithm performs favorably to Klein and Manning’s (2002) constituent-context model (CCM), without the need for careful initialization. In addition, we also analyze CCM’s sensitivity to initialization, and compare our res</context>
</contexts>
<marker>Zhou, Lafferty, Wasserman, 2010</marker>
<rawString>S. Zhou, J. Lafferty, and L. Wasserman. 2010. Time varying undirected graphs. Machine Learning, 80(2-3):295–319.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>