<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000111">
<title confidence="0.9863515">
Machine Reading Tea Leaves: Automatically Evaluating Topic Coherence
and Topic Model Quality
</title>
<author confidence="0.812263">
Jey Han Lau David Newman Timothy Baldwin
</author>
<affiliation confidence="0.820516666666667">
Dept of Philosophy Google Dept of Computing and
King’s College London dnewman@google.com Information Systems
jeyhan.lau@gmail.com The University of Melbourne
</affiliation>
<email confidence="0.93583">
tb@ldwin.net
</email>
<sectionHeader confidence="0.992587" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999419285714286">
Topic models based on latent Dirichlet al-
location and related methods are used in a
range of user-focused tasks including doc-
ument navigation and trend analysis, but
evaluation of the intrinsic quality of the
topic model and topics remains an open
research area. In this work, we explore
the two tasks of automatic evaluation of
single topics and automatic evaluation of
whole topic models, and provide recom-
mendations on the best strategy for per-
forming the two tasks, in addition to pro-
viding an open-source toolkit for topic and
topic model evaluation.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999802333333333">
Topic modelling based on Latent Dirichlet Alloca-
tion (LDA: Blei et al. (2003)) and related methods
is increasingly being used in user-focused tasks, in
contexts such as the evaluation of scientific impact
(McCallum et al., 2006; Hall et al., 2008), trend
analysis (Bolelli et al., 2009; Lau et al., 2012a)
and document search (Wang et al., 2007). The
LDA model is based on the assumption that doc-
ument collections have latent topics, in the form
of a multinomial distribution of words, which is
typically presented to users via its top-N highest-
probability words. In NLP, topic models are gener-
ally used as a means of preprocessing a document
collection, and the topics and per-document topic
allocations are fed into downstream applications
such as document summarisation (Haghighi and
Vanderwende, 2009), novel word sense detection
methods (Lau et al., 2012b) and machine transla-
tion (Zhao and Xing, 2007). In fields such as the
digital humanities, on the other hand, human users
interact directly with the output of topic models. It
is this context of topic modelling for direct human
consumption that we target in this paper.
The topics produced by topic models have a
varying degree of human-interpretability. To il-
lustrate this, we present two topics automatically
learnt from a collection of news articles:
</bodyText>
<listItem confidence="0.9715155">
1. (farmers, farm, food, rice, agriculture)
2. (stories, undated, receive, scheduled, clients)
</listItem>
<bodyText confidence="0.99992793939394">
The first topic is clearly related to agriculture.
The subject of the second topic, however, is less
clear, and may confuse users if presented to them
as part of a larger topic model. Measuring the
human-interpretability of topics and the overall
topic model is the core topic of this paper.
Various methodologies have been proposed for
measuring the semantic interpretability of topics.
In Chang et al. (2009), the authors proposed an
indirect approach based on word intrusion, where
“intruder words” are randomly injected into topics
and human users are asked to identify the intruder
words. The word intrusion task builds on the as-
sumption that the intruder words are more iden-
tifiable in coherent topics than in incoherent top-
ics, and thus the interpretability of a topic can be
estimated by measuring how readily the intruder
words can be manually identified by annotators.
Since its inception, the method of Chang et
al. (2009) has been used variously as a means
of assessing topic models (Paul and Girju, 2010;
Reisinger et al., 2010; Hall et al., 2012). Despite
its wide acceptance, the method relies on manual
annotation and has never been automated. This is
one of the primary contributions of this work: the
demonstration that we can automate the method of
Chang et al. (2009) at near-human levels of accu-
racy, as a result of which we can perform auto-
matic evaluation of the human-interpretability of
topics, as well as topic models.
There has been prior work to directly estimate
the human-interpretability of topics through au-
tomatic means. For example, Newman et al.
</bodyText>
<page confidence="0.951579">
530
</page>
<note confidence="0.9928275">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 530–539,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999800266666667">
(2010) introduced the notion of topic “coher-
ence”, and proposed an automatic method for es-
timating topic coherence based on pairwise point-
wise mutual information (PMI) between the topic
words. Mimno et al. (2011) similarly introduced
a methodology for computing coherence, replac-
ing PMI with log conditional probability. Musat
et al. (2011) incorporated the WordNet hierarchy
to capture the relevance of topics, and in Aletras
and Stevenson (2013a), the authors proposed the
use of distributional similarity for computing the
pairwise association of the topic words. One ap-
plication of these methods has been to remove in-
coherent topics before generating labels for topics
(Lau et al., 2011; Aletras and Stevenson, 2013b).
Ultimately, all these methodologies, and also
the word intrusion approach, attempt to assess the
same quality: the human-interpretability of top-
ics. The relationship between these methodolo-
gies, however, is poorly understood, and there is
no consensus on what is the best approach for
computing the semantic interpretability of topic
models. This is a second contribution of this pa-
per: we perform a systematic empirical compar-
ison of the different methods and find apprecia-
ble differences between them. We further go on to
propose an improved formulation of Newman et
al. (2010) based on normalised PMI. Finally, we
release a toolkit which implements the topic inter-
pretability measures described in this paper.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999934587301588">
Chang et al. (2009) challenged the conventional
wisdom that held-out likelihood — often com-
puted as the perplexity of test data or unseen doc-
uments — is the only way to evaluate topic mod-
els. To measure the human-interpretability of top-
ics, the authors proposed a word intrusion task
and conducted experiments using three topic mod-
els: Latent Dirichlet Allocation (LDA: Blei et al.
(2003)), Probabilistic Latent Semantic Indexing
(PLSI: Hofmann (1999)) and the Correlated Topic
Model (CTM: Blei and Lafferty (2005)). Contrary
to expectation, they found that perplexity corre-
lates negatively with topic interpretability.
In the word intrusion task, each topic is pre-
sented as a list of six words — the five most proba-
ble topic words and a randomly-selected “intruder
word”, which has low probability in the topic of
interest, but high probability in other topics —
and human users are asked to identify the intruder
word that does not belong to the topic in question.
Newman et al. (2010) capture topic inter-
pretability using a more direct approach, by asking
human users to rate topics (represented by their
top-10 topic words) on a 3-point scale based on
how coherent the topic words are (i.e. their ob-
served coherence). They proposed several ways of
automating the estimation of the observed coher-
ence, and ultimately found that a simple method
based on PMI term co-occurrence within a sliding
context window over English Wikipedia produces
the consistently best result, nearing levels of inter-
annotator agreement over topics learnt from two
distinct document collections.
Mimno et al. (2011) proposed a closely-related
method for evaluating semantic coherence, replac-
ing PMI with log conditional probability. Rather
than using Wikipedia for sampling the word co-
occurrence counts, Mimno et al. (2011) used the
topic-modelled documents, and found that their
measure correlates well with human judgements
of observed coherence (where topics were rated
in the same manner as Newman et al. (2010),
based on a 3-point ordinal scale). To incorpo-
rate the evaluation of semantic coherence into the
topic model, the authors proposed to record words
that co-occur together frequently, and update the
counts of all associated words before and after the
sampling of a new topic assignment in the Gibbs
sampler. This variant of topic model was shown to
produce more coherent topics than LDA based on
the log conditional probability coherence measure.
Aletras and Stevenson (2013a) introduced dis-
tributional semantic similarity methods for com-
puting coherence, calculating the distributional
similarity between semantic vectors for the top-N
topic words using a range of distributional similar-
ity measures such as cosine similarity and the Dice
coefficient. To construct the semantic vector space
for the topic words, they used English Wikipedia
as the reference corpus, and collected words that
co-occur in a window of ±5 words. They showed
that their method correlates well with the observed
coherence rated by human judges.
</bodyText>
<sectionHeader confidence="0.99887" genericHeader="method">
3 Dataset
</sectionHeader>
<bodyText confidence="0.999674">
As one of the primary foci of this paper is the au-
tomation of the intruder word task of Chang et
al. (2009), our primary dataset is that used in the
original paper by Chang et al. (2009), which pro-
vides topics and human annotations for a range of
</bodyText>
<page confidence="0.99597">
531
</page>
<bodyText confidence="0.987692340425532">
domains and topic model types. In the dataset,
two text collections were used: (1) 10,000 articles
from English Wikipedia (WIKI); and (2) 8,447 arti-
cles from the New York Times dating from 1987 to
2007 (NEWS). For each document collection, top-
ics were generated by three topic modelling meth-
ods: LDA, PLSI and CTM (see Section 2). For
each topic model, three settings of T (the num-
ber of topics) were used: T = 50, T = 100
and T = 150. In total, there were 9 topic mod-
els (3 models x 3 T) and 900 topics (3 models x
(50 + 100 + 150)) for each dataset.1
For some of topic interpretability estimation
methods, we require a reference corpus to sam-
ple lexical probabilities. We use two reference
corpora: (1) NEWS-FULL, which contains 1.2 mil-
lion New York Times articles from 1994 to 2004
(from the English Gigaword); and (2) WIKI-FULL,
which contains 3.3 million English Wikipedia ar-
ticles (retrieved November 28th 2009).2 The ratio-
nale for choosing the New York Times and English
Wikipedia as the reference corpora is to ensure do-
main consistency with the word intrusion dataset;
the full collections are used to more robustly esti-
mate lexical probabilities.
task is that it requires human annotations, there-
fore preventing large-scale evaluation. We begin
by proposing a methodology to fully automate the
word intrusion task.
Lau et al. (2010) proposed a methodology that
learns the most representative or best topic word
that summarises the semantics of the topic. Ob-
serving that the word intrusion task — the task
of detecting the least representative word — is
the converse of the best topic word selection task,
we adapt their methodology to automatically iden-
tify the intruder word for the word intrusion task,
based on the knowledge that there is a unique in-
truder word per topic.
The methodology works as follows: given a set
of topics (including intruder words), we compute
the word association features for each of the top-
N topic words of a topic,3 and combine the fea-
tures in a ranking support vector regression model
(SVMrank: Joachims (2006)) to learn the intruder
words. Following Lau et al. (2010), we use three
word association measures:
</bodyText>
<equation confidence="0.829261166666666">
P(wi, wj)
log P(wi)P(wj)
PMI(wi) = N−1�
j
4 Human-Interpretability at the Model CP1(wi) = N−1� P(wi, wj)
Level j P(wj)
</equation>
<bodyText confidence="0.999956583333334">
In this section, we evaluate measures for estimat-
ing human-interpretability at the model level. That
is, for a measure — human-judged or automated
— we first aggregate its coherence/interpretability
scores for all topics from a given topic model to
obtain the topic model’s average coherence score.
We then calculate the Pearson correlation coeffi-
cients between the two measures using the topic
models’ average coherence scores. In summary,
the correlation is computed over nine sets of top-
ics (3 topic modellers x 3 settings of T) for each
of WIKI and NEWS.
</bodyText>
<subsectionHeader confidence="0.97778">
4.1 Indirect Approach: Word Intrusion
</subsectionHeader>
<bodyText confidence="0.99802375">
The word intrusion task measures topic inter-
pretability indirectly, by computing the fraction
of annotators who successfully identify the in-
truder word. A limitation of the word intrusion
</bodyText>
<footnote confidence="0.991198">
1In the WIKI topics there were corrupted symbols in the
topic words for 24 topics. We removed these topics, reducing
the total number of topics to 876.
2For both corpora we perform tokenisation and POS tag-
ging using OpenNLP and lemmatisation using Morpha (Min-
nen et al., 2001).
</footnote>
<equation confidence="0.9890535">
P(wi, wj)
P(wi)
</equation>
<bodyText confidence="0.999310666666667">
We additionally experiment with normalised
pointwise mutual information (NPMI: Bouma
(2009)):
</bodyText>
<equation confidence="0.968486">
NPMI(wi) =
</equation>
<bodyText confidence="0.99996525">
In the dataset of Chang et al. (2009) (see Sec-
tion 3), each topic was presented to 8 annota-
tors, with small variations in the displayed topic
words (including the intruder word) for each an-
notator. That is, each topic has essentially 8 subtly
different representations. To measure topic inter-
pretability, the authors defined “model precision”:
the relative success of human annotators at identi-
fying the intruder word, across all representations
of the different topics. The model precision scores
produced by human judges are henceforth referred
to as WI-Human, and the scores produced by our
</bodyText>
<footnote confidence="0.9837565">
3N is the number of topic words displayed to the human
users in the word intrusion task, including the intruder word.
</footnote>
<equation confidence="0.986707142857143">
CP2(wi) = N−1�
j
log P (wi,wj)
P(wi)P(wj)
− log P(wi, wj)
N−1�
j
</equation>
<page confidence="0.98762">
532
</page>
<table confidence="0.999649714285714">
Topic Ref. Pearson’s r with WI-Human
Domain Corpus
WI-Auto-PMI WI-Auto-NPMI
WIKI WIKI-FULL 0.947 0.936
NEWS-FULL 0.801 0.835
NEWS NEWS-FULL 0.913 0.831
WIKI-FULL 0.811 0.750
</table>
<tableCaption confidence="0.999967">
Table 1: Pearson correlation of WI-Human and WI-Auto-PMI/WI-Auto-NPMI at the model level.
</tableCaption>
<bodyText confidence="0.999810041666667">
automated method for the PMI and NPMI vari-
ants as WI-Auto-PMI and WI-Auto-NPMI respec-
tively.4
The Pearson correlation coefficients between
WI-Human and WI-Auto-PMI/WI-Auto-NPMI at
the model level are presented in Table 1. Note
that our two reference corpora are used to inde-
pendently sample the lexical probabilities for the
word association features.
We see very strong correlation for in-domain
pairings (i.e. WIKI+WIKI-FULL and NEWS+NEWS-
FULL), achieving r &gt; 0.9 in most cases for both
WI-Auto-PMI or WI-Auto-NPMI, demonstrating
the effectiveness of our methodology at automat-
ing the word intrusion task for estimating human-
interpretability at the model level. Overall, WI-
Auto-PMI outperforms WI-Auto-NPMI.
Note that although our proposed methodology
is supervised, as intruder words are synthetically
generated and no annotation is needed for the su-
pervised learning, the whole process of computing
topic coherence via word intrusion is fully auto-
matic, without the need for hand-labelled training
data.
</bodyText>
<subsectionHeader confidence="0.988602">
4.2 Direct Approach: Observed Coherence
</subsectionHeader>
<bodyText confidence="0.9999535">
Newman et al. (2010) defined topic interpretabil-
ity based on a more direct approach, by asking hu-
man judges to rate topics based on the observed
coherence of the top-N topic words, and various
methodologies have since been proposed to auto-
mate the computation of the observed coherence.
In this section, we present all these methods and
compare them.
The word intrusion dataset is not annotated with
human ratings of observed coherence. To cre-
ate gold-standard coherence judgements, we used
Amazon Mechanical Turk:5 we presented the top-
ics (with intruder words removed) to the Turkers
and asked them to rate the topics using on a 3-point
</bodyText>
<footnote confidence="0.99612225">
4Note that both variants use CP1 and CP2 features, i.e.
WI-Auto-PMI uses PMI+CP1+C2 while WI-Auto-NPMI
uses NPMI+CP1+C2 features.
5https://www.mturk.com/mturk/
</footnote>
<bodyText confidence="0.997529363636364">
ordinal scale, following Newman et al. (2010). In
total, we collected six to fourteen annotations per
topic (an average of 8.4 annotations per topic).
The observed coherence of a topic is computed
as the arithmetic mean of the annotators’ ratings,
once again following Newman et al. (2010). The
human-judged observed topic coherence is hence-
forth referred to as OC-Human.
For the automated methods, we experimented
with the following methods for estimating the
human-interpretability of a topic t:
</bodyText>
<listItem confidence="0.9848385">
1. OC-Auto-PMI: Pairwise PMI of top-N
topic words (Newman et al., 2010):
</listItem>
<equation confidence="0.879878625">
P(wj, wi)
log
P(wi)P(wj)
2. OC-Auto-NPMI: NPMI variant of OC-
Auto-PMI:
log P (wj,wi)
P (wi)P(wj)
− log P(wi, wj)
</equation>
<listItem confidence="0.618448333333333">
4. OC-Auto-DS: Pairwise distributional simi-
larity of the top-N topic words, as described
in Aletras and Stevenson (2013a).
</listItem>
<bodyText confidence="0.970216">
For OC-Auto-PMI, OC-Auto-NPMI and OC-
Auto-LCP, all topics are lemmatised and intruder
words are removed before coherence is com-
puted.7 In-domain and cross-domain pairings of
</bodyText>
<footnote confidence="0.602507583333333">
6Although the original method uses the topic-modelled
document collection and document co-occurrence for sam-
pling word counts, for a fairer comparison we use log condi-
tional probability only as a replacement to the PMI compo-
nent of the coherence computation (i.e. words are still sam-
pled using a reference corpus and a sliding window). For ad-
ditional evidence that the original method performs at a sub-
par level, see Lau et al. (2013) and Aletras and Stevenson
(2013a).
7We once again use Morpha to do the lemmatisation, and
determine POS via the majority POS for a given word, aggre-
gated over all its occurrences in English Wikipedia.
</footnote>
<equation confidence="0.920403619047619">
OC-Auto-PMI(t) =
N
j=2
j−1 �
i=1
OC-Auto-NPMI(t) =
N
j=2
j−1 �
i=1
3. OC-Auto-LCP: Pairwise log conditional
probability of top-N topic words (Mimno et
al., 2011):6
OC-Auto-LCP(t) =
P(wj, wi)
log
P(wi)
N
j=2
j−1 �
i=1
</equation>
<page confidence="0.993127">
533
</page>
<table confidence="0.999903">
Topic Ref. Pearson’s r with OC-Human
Domain Corpus
OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS
WIKI WIKI-FULL 0.490 0.903 0.959 0.859
NEWS-FULL 0.696 0.844 0.913
NEWS NEWS-FULL 0.965 0.979 0.887 0.941
WIKI-FULL 0.931 0.964 0.872
</table>
<tableCaption confidence="0.916908">
Table 2: Pearson correlation of OC-Human and the automated methods — OC-Auto-PMI, OC-Auto-
NPMI, OC-Auto-LCP and OC-Auto-DS — at the model level.
</tableCaption>
<bodyText confidence="0.999298805555556">
the topic domain and reference corpus are experi-
mented with for these measures.
For OC-Auto-DS, all topics are lemmatised, in-
truder words are removed and English Wikipedia
is used to generate the vector space for the topic
words. The size of the context window is set to
±5 word (i.e. 5 words to either side of the tar-
get word). We use PMI to weight the vectors,
cosine similarity for measuring the distributional
similarity between the top-N topic words, and the
“Topic Word Space” approach to reduce the di-
mensionality of the vector space. A complete de-
scription of the parameters can be found in Aletras
and Stevenson (2013a). Note that cross-domain
pairings of the topic domain and reference corpus
are not tested: in line with the original paper, we
use only English Wikipedia to generate the vector
space before distributional similarity.
We present the Pearson correlation coefficient
of OC-Human and the four automated methods at
the model level in Table 2. For OC-Auto-NPMI,
OC-Auto-LCP and OC-Auto-DS, we see that they
correlate strongly with the human-judged coher-
ence. Overall, OC-Auto-NPMI has the best per-
formance among the methods, and in-domain pair-
ings generally produce the best results for OC-
Auto-NPMI and OC-Auto-LCP. The results are
comparable to those for the automated intruder
word detection method in Section 4.1.
The non-normalised variant OC-Auto-PMI cor-
relates well for NEWS but performs poorly for WIKI,
producing a correlation of only 0.490 for the in-
domain pairing. We revisit this in Section 6, and
provide a qualitative analysis to explain the dis-
crepancy in results between OC-Auto-PMI and
OC-Auto-NPMI.
</bodyText>
<subsectionHeader confidence="0.975079">
4.3 Word Intrusion vs. Observed Coherence
</subsectionHeader>
<bodyText confidence="0.993999416666667">
In the previous sections, we showed for both the
direct and indirect approaches that the automated
methods correlate strongly with the manually-
annotated human-interpretability of topics at the
model level (with the exception of OC-Auto-PMI).
One question that remains unanswered, however,
is whether word intrusion measures topic inter-
pretability differently to observed coherence. This
is the focus of this section.
From the results in Table 3 for the intruder
word model vs. observed coherence, we see a
strong correlation between WI-Human and OC-
Human. This observation is insightful: it shows
that the topic interpretability estimated by the two
approaches is almost identical at the model level.
Between WI-Human and the observed coher-
ence methods automated methods, overall we see
a strong correlation for the OC-Auto-NPMI, OC-
Auto-LCP and OC-Auto-DS methods. OC-Auto-
PMI once again performs poorly over WIKI, but
this is unsurprising given its previous results (i.e.
its poor correlation with OC-Human). In-domain
pairings tend to perform better, and the per-
formance of OC-Auto-NPMI, OC-Auto-LCP and
OC-Auto-DS is comparable, with no one clearly
best method.
5 Human-Interpretability at the Topic
Level
In this section, we evaluate the various methods
at the topic level. We group together all topics
for each dataset (without distinguishing the topic
models that produce them) and calculate the cor-
relation of one measure against another. That is,
the correlation coefficient is computed for 900 top-
ics/data points in the case of each of WIKI and
NEWS.
</bodyText>
<subsectionHeader confidence="0.931557">
5.1 Indirect Approach: Word Intrusion
</subsectionHeader>
<bodyText confidence="0.998523888888889">
In Section 4.1, we proposed a novel methodol-
ogy to automate the word intrusion task (WI-Auto-
PMI and WI-Auto-NPMI). We now evaluate its
performance at the topic level, and present its
correlation with the human gold standard (WI-
Human) in Table 4.
The correlation of WI-Human and WI-Auto-
PMI/WI-Auto-NPMI at the topic level is consid-
erably worse, compared to its results at the model
</bodyText>
<page confidence="0.997328">
534
</page>
<table confidence="0.983044375">
Topic Ref. Pearson’s r with WI-Human
Domain Corpus OC-Human OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS
WIKI-FULL 0.638 0.927 0.911
WIKI 0.900 0.907
NEWS-FULL 0.614 0.757 0.821
NEWS-FULL 0.865 0.866 0.867
NEWS 0.915 0.925
WIKI-FULL 0.838 0.874 0.893
</table>
<tableCaption confidence="0.984985">
Table 3: Word intrusion vs. observed coherence: Pearson correlation coefficient at the model level.
</tableCaption>
<table confidence="0.952280875">
Topic Ref. Pearson’s r with WI-Human Human
Domain Corpus WI-Auto-PMI WI-Auto-NPMI Agreement
WIKI-FULL 0.554 0.573
WIKI 0.735
NEWS-FULL 0.622 0.592
NEWS-FULL 0.602 0.612
NEWS 0.770
WIKI-FULL 0.638 0.648
</table>
<tableCaption confidence="0.998267">
Table 4: Pearson correlation coefficient of WI-Human and WI-Auto-PMI/WI-Auto-NPMI at the topic
</tableCaption>
<bodyText confidence="0.965334909090909">
level.
level (Table 1). The performance between WI-
Auto-PMI and WI-Auto-NPMI is not very differ-
ent, and the cross-domain pairing slightly outper-
forms the in-domain pairing.
To better understand the difficulty of the task,
we compute the agreement between human anno-
tators by calculating the Pearson correlation co-
efficient of model precisions produced by ran-
domised sub-group pairs in the topics.8 That is, for
each topic, we randomly split the annotations into
two sub-groups, and compute the Pearson correla-
tion coefficient of the model precisions produced
by the first sub-group and that of the second sub-
group.
The original dataset has 8 annotations per topic.
Splitting the annotations into two sub-groups re-
duces the number of annotations to 4 per group,
which is not ideal for computing model precision.
We thus chose to expand the number of annota-
tions by sampling 300 random topics from each
domain (for a total of 600 topics) and following
the same process as Chang et al. (2009) to get in-
truder word annotations using Amazon Mechani-
cal Turk. On average, we obtained 11.7 additional
annotations per topic for these 600 topics. The hu-
man agreement scores (i.e. the Pearson correlation
coefficient of randomised sub-group pairs) for the
sampled 600 topics are presented in the last col-
umn of Table 4.
The sub-group correlation is around r = 0.75
for the topics from both datasets. As such, esti-
mating topic interpretability at the topic level is a
much harder task than model-level evaluation. Our
automated methods perform at a highly credible
8To counter for the fact that annotators labelled varying
numbers of topics.
r = 0.6, but there is certainly room for improve-
ment. Note that the correlation values reported in
Newman et al. (2010) are markedly higher than
ours, as they evaluated based on Spearman rank
correlation, which isn’t attuned to the relative dif-
ferences in coherence values and returns higher
values for the task.
</bodyText>
<subsectionHeader confidence="0.993842">
5.2 Direct Approach: Observed Coherence
</subsectionHeader>
<bodyText confidence="0.99997337037037">
We repeat the experiments of observed coherence
in Section 4.2, and evaluate the correlation of
the automated methods (OC-Auto-PMI, OC-Auto-
NPMI, OC-Auto-LCP and OC-Auto-DS) on the
human gold standard (OC-Human) at the topic
level. Results are summarised in Table 5.
OC-Auto-PMI performs poorly at the topic
level in the WIKI domain, similar to what was
seen at the model level in Section 4.2. Over-
all, both OC-Auto-NPMI and OC-Auto-DS are the
most consistent methods. OC-Auto-LCP performs
markedly worse than these two methods.
To get a better understanding of how well hu-
man annotators perform at the task, we compute
the one-vs-rest Pearson correlation coefficient us-
ing the gold standard annotations. That is, for
each topic, we single out each rating/annotation
and compare it to the average of all other rat-
ings/annotations. The one-vs-rest correlation re-
sult is displayed in the last column (titled “Hu-
man Agreement”) in Table 5. The best auto-
mated methods surpass the single-annotator per-
formance, indicating that they are able to per-
form the task as well as human annotators (unlike
the topic-level results for the word intrusion task
where humans were markedly better at the task
than the automated methods).
</bodyText>
<page confidence="0.99681">
535
</page>
<table confidence="0.965803">
Topic Ref. Pearson’s r with OC-Human Human
Domain Corpus OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS Agreement
WIKI-FULL 0.533 0.638 0.579
WIKI 0.682 0.624
NEWS-FULL 0.582 0.667 0.496
NEWS-FULL 0.719 0.741 0.471
NEWS 0.682 0.634
WIKI-FULL 0.671 0.722 0.452
</table>
<tableCaption confidence="0.995936">
Table 5: Pearson correlation of OC-Human and the automated methods at the topic level.
</tableCaption>
<table confidence="0.981244625">
Topic Ref. Pearson’s r with WI-Human
Domain Corpus OC-Human OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS
WIKI-FULL 0.472 0.557 0.547
WIKI 0.665 0.639
NEWS-FULL 0.504 0.571 0.455
NEWS-FULL 0.629 0.634 0.407
NEWS 0.641 0.649
WIKI-FULL 0.604 0.633 0.390
</table>
<tableCaption confidence="0.997079">
Table 6: Word intrusion vs. observed coherence: pearson correlation results at the topic level.
</tableCaption>
<subsectionHeader confidence="0.828887">
5.3 Word Intrusion vs. Observed Coherence
</subsectionHeader>
<bodyText confidence="0.99995575">
In this section, we bring together the indirect ap-
proach of word intrusion and the direct approach
of observed coherence, and evaluate them against
each other at the topic level. Results are sum-
marised in Table 6.
We see that the correlation between the human
ratings of intruder words and observed coherence
is only modest, implying that there are topic-level
differences in the output of the two approaches. In
Section 6, we provide a qualitative analysis and
explanation as to what constitutes the differences
between the approaches.
For the automated methods, OC-Auto-DS has
the best performance, with OC-Auto-NPMI per-
forming relatively well (in particularly in the NEWS
domain).
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999975528301887">
Normalised PMI (NPMI) was first introduced by
Bouma (2009) as a means of reducing the bias for
PMI towards words of lower frequency, in addition
to providing a standardised range of [−1, 1] for the
calculated values.
We introduced NPMI to the automated meth-
ods of word intrusion (WI-Auto-NPMI) and ob-
served coherence (OC-Auto-NPMI) to explore its
suitability for the task. For the latter, we saw
that NPMI achieves markedly higher correlation
than OC-Human (in particular, at the model level).
To better understand the impact of normalisation,
we inspected a list of WIKI topics that have simi-
lar scores for OC-Human and OC-Auto-NPMI but
very different OC-Auto-PMI scores. A sample of
these topics is presented in Table 7. WIKI-FULL
is used as the reference corpus for computing the
scores. Note that the presented OC-Auto-NPMI*
and OC-Auto-PMI* scores are post-normalised to
the range [0, 1] for ease of interpretation. To give
a sense of how readily these topic words occur in
the reference corpus, we additionally display the
frequency of the first topic word in the reference
corpus (last column).
All topics presented have an OC-Human score
of 3.0 (i.e. these topics are rated as being very co-
herent by human judges) and similar OC-Auto-
NPMI values. Their OC-Auto-PMI scores, how-
ever, are very different between the top-3 and
bottom-3 topics. The bias of PMI towards lower
frequency words is clear: topic words that occur
frequently in the corpus receive a lower OC-Auto-
PMI score compared to those that occur less fre-
quently, even though the human-judged observed
coherence is the same. OC-Auto-NPMI on the
other hand, correctly estimates the coherence.
We observed, however, that the impact of nor-
malising PMI is less in the word intrusion task.
One possible explanation is that for the automated
methods WI-Auto-PMI and WI-Auto-NPMI, the
PMI/NPMI scores are used indirectly as a feature
to a machine learning framework, and the bias
could be reduced/compensated by other features.
On the subject of the difference between ob-
served coherence and word intrusion in estimat-
ing topic interpretability, we observed that WI-
Human and OC-Human correlate only moderately
(r ≈ 0.6) at the topic level (Table 6). To better
understand this effect, we manually analysed top-
ics that have differing WI-Human and OC-Human
scores. A sample of topics with high divergence
in estimated coherence score is given in Table 8.
As before, the presented the OC-Human* and WI-
</bodyText>
<page confidence="0.994111">
536
</page>
<table confidence="0.975406133333333">
OC- OC- OC- Word
Human Auto-NPMI* Auto-PMI* Count
3.0 0.59 0.61 #(cell) = 1.1M
3.0 0.52 0.54 #(electron) = 0.3M
3.0 0.55 0.55 #(magnetic) = 0.4M
3.0 0.56 0.37 #(album) = 12.5M
3.0 0.57 0.38 #(college) = 9.8M
3.0 0.52 0.34 #(city) = 22.0M
Topic
cell hormone insulin muscle receptor
electron laser magnetic voltage wavelength
magnetic neutrino particle quantum universe
album band music release song
college education school student university
city county district population town
</table>
<tableCaption confidence="0.995531">
Table 7: A list of WIKI topics to illustrate the impact of NPMI.
</tableCaption>
<figure confidence="0.820114125">
Topic # Topic OC-Human* WI-Human*
1 business company corporation cluster loch shareholder 0.94 0.25
2 song actor clown play role theatre 1.00 0.50
3 census ethnic female male population village 0.92 0.25
1.00 0.63
4 composer singer jazz music opera piano
5 choice count give i.e. simply unionist 0.14 1.00
6 digital clown friend love mother wife 0.17 1.00
</figure>
<tableCaption confidence="0.910172">
Table 8: A list of WIKI topics to illustrate the difference between observed coherence and word intrusion.
Boxes denote human chosen intruder words, and boldface denotes true intruder words.
</tableCaption>
<bodyText confidence="0.999444931034483">
Human* scores in the table are post-normalised to
the range [0, 1] for ease of comparison.
In general, there are two reasons for topics to
have high OC-Human and low WI-Human scores.
First, if a topic has an outlier word that is mildly
related to the topic, users tend to choose this word
as the intruder word in the word intrusion task,
yielding a low WI-Human score. If they are asked
to rate the observed coherence, however, the single
outlier word often does not affect its overall coher-
ence, resulting in a high OC-Human score. This is
observed in topics 1 and 2 in Table 8, where loch
and clown are chosen by annotators in the word in-
trusion task, as they detract from the semantics of
the topic. This results in low WI-Human scores,
but high observed coherence scores (OC-Human).
The second reason is the random selection of
intruder words related to the original topic. We
see this in topics 3 and 4, where related intruder
words (village and singer) were selected.
For topics with low OC-Human and high WI-
Human scores, the true intruder words are often
very different to the domain/focus of other topic
words. As such, annotators are consistently able
to single them out to yield high WI-Human scores,
even though the topic as a whole is not coherent.
Topics 5 and 6 in Table 8 exhibit this.
All topic evaluation measures described in this
paper are implemented in an open-source toolkit.9
</bodyText>
<footnote confidence="0.922599">
9https://github.com/jhlau/topic_
interpretability
</footnote>
<sectionHeader confidence="0.998113" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999968263157895">
In this paper, we examined various methodologies
that estimate the semantic interpretability of top-
ics, at two levels: the model level and the topic
level. We looked first at the word intrusion task
proposed by Chang et al. (2009), and proposed
a method that fully automates the task. Next we
turned to observed coherence, a more direct ap-
proach to estimate topic interpretability. At the
model level, results were very positive for both the
word intrusion and observed coherence methods.
At the topic level, however, the results were more
mixed. For observed coherence, our best methods
(OC-Auto-NPMI and OC-Auto-DS) were able to
emulate human performance. For word intrusion,
the automated methods were slightly below human
performance, with some room for improvement.
We finally observed that there are systematic dif-
ferences in the topic-level scores derived from the
two task formulations.
</bodyText>
<sectionHeader confidence="0.994987" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999319">
This work was supported in part by the Australian
Research Council, and for author JHL, also partly
funded by grant ES/J022969/1 from the Economic
and Social Research Council of the UK. The au-
thors acknowledge the generosity of Nikos Ale-
tras and Mark Stevenson in providing their code
for OC-Auto-DS, and Jordan Boyd-Graber in pro-
viding the data used in Chang et al. (2009).
</bodyText>
<page confidence="0.995111">
537
</page>
<sectionHeader confidence="0.989963" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999757183486239">
N. Aletras and M. Stevenson. 2013a. Evaluating
topic coherence using distributional semantics. In
Proceedings of the Tenth International Workshop on
Computational Semantics (IWCS-10), pages 13–22,
Potsdam, Germany.
N. Aletras and M. Stevenson. 2013b. Representing
topics using images. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL HLT 2013), pages
158–167, Atlanta, USA.
D. Blei and J. Lafferty. 2005. Correlated topic mod-
els. In Advances in Neural Information Processing
Systems 17 (NIPS-05), pages 147–154, Vancouver,
Canada.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022.
L. Bolelli, S¸. Ertekin, and C.L. Giles. 2009. Topic
and trend detection in text collections using Latent
Dirichlet Allocation. In Proceedings of ECIR 2009,
pages 776–780, Toulouse, France.
G. Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. In Proceed-
ings of the Biennial GSCL Conference, pages 31–40,
Potsdam, Germany.
J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and
D. Blei. 2009. Reading tea leaves: How humans
interpret topic models. In Advances in Neural In-
formation Processing Systems 21 (NIPS-09), pages
288–296, Vancouver, Canada.
A. Haghighi and L. Vanderwende. 2009. Exploring
content models for multi-document summarization.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics – Human
Language Technologies 2009 (NAACL HLT 2009),
pages 362–370.
D. Hall, D. Jurafsky, and C.D. Manning. 2008. Study-
ing the history of ideas using topic models. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2008), pages 363–371, Honolulu, USA.
M. Hall, P. Clough, and M. Stevenson. 2012. Evalu-
ating the use of clustering for automatically organ-
ising digital library collections. In Proceedings of
the Second International Conference on Theory and
Practice of Digital Libraries, pages 323–334, Pa-
phos, Cyprus.
T. Hofmann. 1999. Probabilistic latent semantic in-
dexing. In Proceedings of 22nd International ACM-
SIGIR Conference on Research and Development
in Information Retrieval (SIGIR’99), pages 50–57,
Berkeley, USA.
T. Joachims. 2006. Training linear SVMs in linear
time. In Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining (KDD 2006), Philadelphia, USA.
J.H. Lau, D. Newman, S. Karimi, and T. Baldwin.
2010. Best topic word selection for topic labelling.
In Proceedings of the 23rd International Confer-
ence on Computational Linguistics (COLING 2010),
Posters Volume, pages 605–613, Beijing, China.
J.H. Lau, K. Grieser, D. Newman, and T. Baldwin.
2011. Automatic labelling of topic models. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL HLT 2011), pages 1536–
1545, Portland, USA.
J.H. Lau, N. Collier, and T. Baldwin. 2012a. On-
line trend analysis with topic models: #twitter
trends detection topic model online. In Proceedings
of the 24th International Conference on Compu-
tational Linguistics (COLING 2012), pages 1519–
1534, Mumbai, India.
J.H. Lau, P. Cook, D. McCarthy, D. Newman, and
T. Baldwin. 2012b. Word sense induction for novel
sense detection. In Proceedings of the 13th Con-
ference of the EACL (EACL 2012), pages 591–601,
Avignon, France.
J.H. Lau, T. Baldwin, and D. Newman. 2013. On
collocations and topic models. ACM Transactions
on Speech and Language Processing, 10(3):10:1–
10:14.
A McCallum, G.S. Mann, and D Mimno. 2006. Bib-
liometric impact measures leveraging topic analysis.
In Proceedings of the 6th ACM/IEEE-CS Joint Con-
ference on Digital Libraries 2006 (JCDL’06), pages
65–74, Chapel Hill, USA.
D. Mimno, H. Wallach, E. Talley, M. Leenders, and
A. McCallum. 2011. Optimizing semantic coher-
ence in topic models. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2011), pages 262–272,
Edinburgh, UK.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of English. Natural Lan-
guage Engineering, 7(3):207–223.
C. Musat, J. Velcin, S. Trausan-Matu, and M.A. Rizoiu.
2011. Improving topic evaluation using concep-
tual knowledge. In Proceedings of the 22nd Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-2011), pages 1866–1871, Barcelona, Spain.
D. Newman, J.H. Lau, K. Grieser, and T. Baldwin.
2010. Automatic evaluation of topic coherence.
In Proceedings of Human Language Technologies:
The 11th Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL HLT 2010), pages 100–108, Los
Angeles, USA.
</reference>
<page confidence="0.977461">
538
</page>
<reference confidence="0.99966845">
M. Paul and R. Girju. 2010. A two-dimensional topic-
aspect model for discovering multi-faceted topics.
In Proceedings of the 24th Annual Conference on
Artificial Intelligence (AAAI-10), Atlanta, USA.
J. Reisinger, A. Waters, B. Silverthorn, and R.J.
Mooney. 2010. Spherical topic models. In Proceed-
ings of the 27th International Conference on Ma-
chine Learning (ICML 2010), pages 903–910, Haifa,
Israel.
X. Wang, A. McCallum, and X. Wei. 2007. Topical
n-grams: Phrase and topic discovery, with an ap-
plication to information retrieval. In Proceedings
of the Seventh IEEE International Conference on
Data Mining (ICDM 2007), pages 697–702, Omaha,
USA.
B. Zhao and E.P. Xing. 2007. HM-BiTAM: Bilin-
gual topic exploration, word alignment, and transla-
tion. In Advances in Neural Information Processing
Systems (NIPS 2007), pages 1689–1696, Vancouver,
Canada.
</reference>
<page confidence="0.998621">
539
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.318805">
<title confidence="0.770561">Machine Reading Tea Leaves: Automatically Evaluating Topic Coherence and Topic Model Quality</title>
<author confidence="0.999894">Jey Han David Timothy</author>
<affiliation confidence="0.863385333333333">Dept of Google Dept of Computing King’s College dnewman@google.com Information jeyhan.lau@gmail.com The University of</affiliation>
<email confidence="0.820852">tb@ldwin.net</email>
<abstract confidence="0.9985724">Topic models based on latent Dirichlet allocation and related methods are used in a range of user-focused tasks including document navigation and trend analysis, but evaluation of the intrinsic quality of the topic model and topics remains an open research area. In this work, we explore the two tasks of automatic evaluation of single topics and automatic evaluation of whole topic models, and provide recommendations on the best strategy for performing the two tasks, in addition to providing an open-source toolkit for topic and topic model evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Aletras</author>
<author>M Stevenson</author>
</authors>
<title>Evaluating topic coherence using distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the Tenth International Workshop on Computational Semantics (IWCS-10),</booktitle>
<pages>13--22</pages>
<location>Potsdam, Germany.</location>
<contexts>
<context position="4536" citStr="Aletras and Stevenson (2013" startWordPosition="712" endWordPosition="715">ence of the European Chapter of the Association for Computational Linguistics, pages 530–539, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics (2010) introduced the notion of topic “coherence”, and proposed an automatic method for estimating topic coherence based on pairwise pointwise mutual information (PMI) between the topic words. Mimno et al. (2011) similarly introduced a methodology for computing coherence, replacing PMI with log conditional probability. Musat et al. (2011) incorporated the WordNet hierarchy to capture the relevance of topics, and in Aletras and Stevenson (2013a), the authors proposed the use of distributional similarity for computing the pairwise association of the topic words. One application of these methods has been to remove incoherent topics before generating labels for topics (Lau et al., 2011; Aletras and Stevenson, 2013b). Ultimately, all these methodologies, and also the word intrusion approach, attempt to assess the same quality: the human-interpretability of topics. The relationship between these methodologies, however, is poorly understood, and there is no consensus on what is the best approach for computing the semantic interpretabilit</context>
<context position="8008" citStr="Aletras and Stevenson (2013" startWordPosition="1263" endWordPosition="1266">d found that their measure correlates well with human judgements of observed coherence (where topics were rated in the same manner as Newman et al. (2010), based on a 3-point ordinal scale). To incorporate the evaluation of semantic coherence into the topic model, the authors proposed to record words that co-occur together frequently, and update the counts of all associated words before and after the sampling of a new topic assignment in the Gibbs sampler. This variant of topic model was shown to produce more coherent topics than LDA based on the log conditional probability coherence measure. Aletras and Stevenson (2013a) introduced distributional semantic similarity methods for computing coherence, calculating the distributional similarity between semantic vectors for the top-N topic words using a range of distributional similarity measures such as cosine similarity and the Dice coefficient. To construct the semantic vector space for the topic words, they used English Wikipedia as the reference corpus, and collected words that co-occur in a window of ±5 words. They showed that their method correlates well with the observed coherence rated by human judges. 3 Dataset As one of the primary foci of this paper i</context>
<context position="15964" citStr="Aletras and Stevenson (2013" startWordPosition="2548" endWordPosition="2551">f a topic is computed as the arithmetic mean of the annotators’ ratings, once again following Newman et al. (2010). The human-judged observed topic coherence is henceforth referred to as OC-Human. For the automated methods, we experimented with the following methods for estimating the human-interpretability of a topic t: 1. OC-Auto-PMI: Pairwise PMI of top-N topic words (Newman et al., 2010): P(wj, wi) log P(wi)P(wj) 2. OC-Auto-NPMI: NPMI variant of OCAuto-PMI: log P (wj,wi) P (wi)P(wj) − log P(wi, wj) 4. OC-Auto-DS: Pairwise distributional similarity of the top-N topic words, as described in Aletras and Stevenson (2013a). For OC-Auto-PMI, OC-Auto-NPMI and OCAuto-LCP, all topics are lemmatised and intruder words are removed before coherence is computed.7 In-domain and cross-domain pairings of 6Although the original method uses the topic-modelled document collection and document co-occurrence for sampling word counts, for a fairer comparison we use log conditional probability only as a replacement to the PMI component of the coherence computation (i.e. words are still sampled using a reference corpus and a sliding window). For additional evidence that the original method performs at a subpar level, see Lau et</context>
<context position="18001" citStr="Aletras and Stevenson (2013" startWordPosition="2883" endWordPosition="2886"> the topic domain and reference corpus are experimented with for these measures. For OC-Auto-DS, all topics are lemmatised, intruder words are removed and English Wikipedia is used to generate the vector space for the topic words. The size of the context window is set to ±5 word (i.e. 5 words to either side of the target word). We use PMI to weight the vectors, cosine similarity for measuring the distributional similarity between the top-N topic words, and the “Topic Word Space” approach to reduce the dimensionality of the vector space. A complete description of the parameters can be found in Aletras and Stevenson (2013a). Note that cross-domain pairings of the topic domain and reference corpus are not tested: in line with the original paper, we use only English Wikipedia to generate the vector space before distributional similarity. We present the Pearson correlation coefficient of OC-Human and the four automated methods at the model level in Table 2. For OC-Auto-NPMI, OC-Auto-LCP and OC-Auto-DS, we see that they correlate strongly with the human-judged coherence. Overall, OC-Auto-NPMI has the best performance among the methods, and in-domain pairings generally produce the best results for OCAuto-NPMI and O</context>
</contexts>
<marker>Aletras, Stevenson, 2013</marker>
<rawString>N. Aletras and M. Stevenson. 2013a. Evaluating topic coherence using distributional semantics. In Proceedings of the Tenth International Workshop on Computational Semantics (IWCS-10), pages 13–22, Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Aletras</author>
<author>M Stevenson</author>
</authors>
<title>Representing topics using images.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2013),</booktitle>
<pages>158--167</pages>
<location>Atlanta, USA.</location>
<contexts>
<context position="4536" citStr="Aletras and Stevenson (2013" startWordPosition="712" endWordPosition="715">ence of the European Chapter of the Association for Computational Linguistics, pages 530–539, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics (2010) introduced the notion of topic “coherence”, and proposed an automatic method for estimating topic coherence based on pairwise pointwise mutual information (PMI) between the topic words. Mimno et al. (2011) similarly introduced a methodology for computing coherence, replacing PMI with log conditional probability. Musat et al. (2011) incorporated the WordNet hierarchy to capture the relevance of topics, and in Aletras and Stevenson (2013a), the authors proposed the use of distributional similarity for computing the pairwise association of the topic words. One application of these methods has been to remove incoherent topics before generating labels for topics (Lau et al., 2011; Aletras and Stevenson, 2013b). Ultimately, all these methodologies, and also the word intrusion approach, attempt to assess the same quality: the human-interpretability of topics. The relationship between these methodologies, however, is poorly understood, and there is no consensus on what is the best approach for computing the semantic interpretabilit</context>
<context position="8008" citStr="Aletras and Stevenson (2013" startWordPosition="1263" endWordPosition="1266">d found that their measure correlates well with human judgements of observed coherence (where topics were rated in the same manner as Newman et al. (2010), based on a 3-point ordinal scale). To incorporate the evaluation of semantic coherence into the topic model, the authors proposed to record words that co-occur together frequently, and update the counts of all associated words before and after the sampling of a new topic assignment in the Gibbs sampler. This variant of topic model was shown to produce more coherent topics than LDA based on the log conditional probability coherence measure. Aletras and Stevenson (2013a) introduced distributional semantic similarity methods for computing coherence, calculating the distributional similarity between semantic vectors for the top-N topic words using a range of distributional similarity measures such as cosine similarity and the Dice coefficient. To construct the semantic vector space for the topic words, they used English Wikipedia as the reference corpus, and collected words that co-occur in a window of ±5 words. They showed that their method correlates well with the observed coherence rated by human judges. 3 Dataset As one of the primary foci of this paper i</context>
<context position="15964" citStr="Aletras and Stevenson (2013" startWordPosition="2548" endWordPosition="2551">f a topic is computed as the arithmetic mean of the annotators’ ratings, once again following Newman et al. (2010). The human-judged observed topic coherence is henceforth referred to as OC-Human. For the automated methods, we experimented with the following methods for estimating the human-interpretability of a topic t: 1. OC-Auto-PMI: Pairwise PMI of top-N topic words (Newman et al., 2010): P(wj, wi) log P(wi)P(wj) 2. OC-Auto-NPMI: NPMI variant of OCAuto-PMI: log P (wj,wi) P (wi)P(wj) − log P(wi, wj) 4. OC-Auto-DS: Pairwise distributional similarity of the top-N topic words, as described in Aletras and Stevenson (2013a). For OC-Auto-PMI, OC-Auto-NPMI and OCAuto-LCP, all topics are lemmatised and intruder words are removed before coherence is computed.7 In-domain and cross-domain pairings of 6Although the original method uses the topic-modelled document collection and document co-occurrence for sampling word counts, for a fairer comparison we use log conditional probability only as a replacement to the PMI component of the coherence computation (i.e. words are still sampled using a reference corpus and a sliding window). For additional evidence that the original method performs at a subpar level, see Lau et</context>
<context position="18001" citStr="Aletras and Stevenson (2013" startWordPosition="2883" endWordPosition="2886"> the topic domain and reference corpus are experimented with for these measures. For OC-Auto-DS, all topics are lemmatised, intruder words are removed and English Wikipedia is used to generate the vector space for the topic words. The size of the context window is set to ±5 word (i.e. 5 words to either side of the target word). We use PMI to weight the vectors, cosine similarity for measuring the distributional similarity between the top-N topic words, and the “Topic Word Space” approach to reduce the dimensionality of the vector space. A complete description of the parameters can be found in Aletras and Stevenson (2013a). Note that cross-domain pairings of the topic domain and reference corpus are not tested: in line with the original paper, we use only English Wikipedia to generate the vector space before distributional similarity. We present the Pearson correlation coefficient of OC-Human and the four automated methods at the model level in Table 2. For OC-Auto-NPMI, OC-Auto-LCP and OC-Auto-DS, we see that they correlate strongly with the human-judged coherence. Overall, OC-Auto-NPMI has the best performance among the methods, and in-domain pairings generally produce the best results for OCAuto-NPMI and O</context>
</contexts>
<marker>Aletras, Stevenson, 2013</marker>
<rawString>N. Aletras and M. Stevenson. 2013b. Representing topics using images. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2013), pages 158–167, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>J Lafferty</author>
</authors>
<title>Correlated topic models.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems 17 (NIPS-05),</booktitle>
<pages>147--154</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="6055" citStr="Blei and Lafferty (2005)" startWordPosition="951" endWordPosition="954">elease a toolkit which implements the topic interpretability measures described in this paper. 2 Related Work Chang et al. (2009) challenged the conventional wisdom that held-out likelihood — often computed as the perplexity of test data or unseen documents — is the only way to evaluate topic models. To measure the human-interpretability of topics, the authors proposed a word intrusion task and conducted experiments using three topic models: Latent Dirichlet Allocation (LDA: Blei et al. (2003)), Probabilistic Latent Semantic Indexing (PLSI: Hofmann (1999)) and the Correlated Topic Model (CTM: Blei and Lafferty (2005)). Contrary to expectation, they found that perplexity correlates negatively with topic interpretability. In the word intrusion task, each topic is presented as a list of six words — the five most probable topic words and a randomly-selected “intruder word”, which has low probability in the topic of interest, but high probability in other topics — and human users are asked to identify the intruder word that does not belong to the topic in question. Newman et al. (2010) capture topic interpretability using a more direct approach, by asking human users to rate topics (represented by their top-10</context>
</contexts>
<marker>Blei, Lafferty, 2005</marker>
<rawString>D. Blei and J. Lafferty. 2005. Correlated topic models. In Advances in Neural Information Processing Systems 17 (NIPS-05), pages 147–154, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="962" citStr="Blei et al. (2003)" startWordPosition="143" endWordPosition="146">irichlet allocation and related methods are used in a range of user-focused tasks including document navigation and trend analysis, but evaluation of the intrinsic quality of the topic model and topics remains an open research area. In this work, we explore the two tasks of automatic evaluation of single topics and automatic evaluation of whole topic models, and provide recommendations on the best strategy for performing the two tasks, in addition to providing an open-source toolkit for topic and topic model evaluation. 1 Introduction Topic modelling based on Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and related methods is increasingly being used in user-focused tasks, in contexts such as the evaluation of scientific impact (McCallum et al., 2006; Hall et al., 2008), trend analysis (Bolelli et al., 2009; Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per</context>
<context position="5929" citStr="Blei et al. (2003)" startWordPosition="934" endWordPosition="937">them. We further go on to propose an improved formulation of Newman et al. (2010) based on normalised PMI. Finally, we release a toolkit which implements the topic interpretability measures described in this paper. 2 Related Work Chang et al. (2009) challenged the conventional wisdom that held-out likelihood — often computed as the perplexity of test data or unseen documents — is the only way to evaluate topic models. To measure the human-interpretability of topics, the authors proposed a word intrusion task and conducted experiments using three topic models: Latent Dirichlet Allocation (LDA: Blei et al. (2003)), Probabilistic Latent Semantic Indexing (PLSI: Hofmann (1999)) and the Correlated Topic Model (CTM: Blei and Lafferty (2005)). Contrary to expectation, they found that perplexity correlates negatively with topic interpretability. In the word intrusion task, each topic is presented as a list of six words — the five most probable topic words and a randomly-selected “intruder word”, which has low probability in the topic of interest, but high probability in other topics — and human users are asked to identify the intruder word that does not belong to the topic in question. Newman et al. (2010) </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bolelli</author>
<author>S¸ Ertekin</author>
<author>C L Giles</author>
</authors>
<title>Topic and trend detection in text collections using Latent Dirichlet Allocation.</title>
<date>2009</date>
<booktitle>In Proceedings of ECIR 2009,</booktitle>
<pages>776--780</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="1170" citStr="Bolelli et al., 2009" startWordPosition="176" endWordPosition="179">ins an open research area. In this work, we explore the two tasks of automatic evaluation of single topics and automatic evaluation of whole topic models, and provide recommendations on the best strategy for performing the two tasks, in addition to providing an open-source toolkit for topic and topic model evaluation. 1 Introduction Topic modelling based on Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and related methods is increasingly being used in user-focused tasks, in contexts such as the evaluation of scientific impact (McCallum et al., 2006; Hall et al., 2008), trend analysis (Bolelli et al., 2009; Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per-document topic allocations are fed into downstream applications such as document summarisation (Haghighi and Vanderwende, 2009), novel word sense detection methods (Lau et al., 2012b) and machine translation</context>
</contexts>
<marker>Bolelli, Ertekin, Giles, 2009</marker>
<rawString>L. Bolelli, S¸. Ertekin, and C.L. Giles. 2009. Topic and trend detection in text collections using Latent Dirichlet Allocation. In Proceedings of ECIR 2009, pages 776–780, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouma</author>
</authors>
<title>Normalized (pointwise) mutual information in collocation extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Biennial GSCL Conference,</booktitle>
<pages>31--40</pages>
<location>Potsdam, Germany.</location>
<contexts>
<context position="12258" citStr="Bouma (2009)" startWordPosition="1976" endWordPosition="1977"> 4.1 Indirect Approach: Word Intrusion The word intrusion task measures topic interpretability indirectly, by computing the fraction of annotators who successfully identify the intruder word. A limitation of the word intrusion 1In the WIKI topics there were corrupted symbols in the topic words for 24 topics. We removed these topics, reducing the total number of topics to 876. 2For both corpora we perform tokenisation and POS tagging using OpenNLP and lemmatisation using Morpha (Minnen et al., 2001). P(wi, wj) P(wi) We additionally experiment with normalised pointwise mutual information (NPMI: Bouma (2009)): NPMI(wi) = In the dataset of Chang et al. (2009) (see Section 3), each topic was presented to 8 annotators, with small variations in the displayed topic words (including the intruder word) for each annotator. That is, each topic has essentially 8 subtly different representations. To measure topic interpretability, the authors defined “model precision”: the relative success of human annotators at identifying the intruder word, across all representations of the different topics. The model precision scores produced by human judges are henceforth referred to as WI-Human, and the scores produced</context>
<context position="26386" citStr="Bouma (2009)" startWordPosition="4192" endWordPosition="4193">inst each other at the topic level. Results are summarised in Table 6. We see that the correlation between the human ratings of intruder words and observed coherence is only modest, implying that there are topic-level differences in the output of the two approaches. In Section 6, we provide a qualitative analysis and explanation as to what constitutes the differences between the approaches. For the automated methods, OC-Auto-DS has the best performance, with OC-Auto-NPMI performing relatively well (in particularly in the NEWS domain). 6 Discussion Normalised PMI (NPMI) was first introduced by Bouma (2009) as a means of reducing the bias for PMI towards words of lower frequency, in addition to providing a standardised range of [−1, 1] for the calculated values. We introduced NPMI to the automated methods of word intrusion (WI-Auto-NPMI) and observed coherence (OC-Auto-NPMI) to explore its suitability for the task. For the latter, we saw that NPMI achieves markedly higher correlation than OC-Human (in particular, at the model level). To better understand the impact of normalisation, we inspected a list of WIKI topics that have similar scores for OC-Human and OC-Auto-NPMI but very different OC-Au</context>
</contexts>
<marker>Bouma, 2009</marker>
<rawString>G. Bouma. 2009. Normalized (pointwise) mutual information in collocation extraction. In Proceedings of the Biennial GSCL Conference, pages 31–40, Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chang</author>
<author>J Boyd-Graber</author>
<author>S Gerrish</author>
<author>C Wang</author>
<author>D Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 21 (NIPS-09),</booktitle>
<pages>288--296</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="2704" citStr="Chang et al. (2009)" startWordPosition="421" endWordPosition="424">erpretability. To illustrate this, we present two topics automatically learnt from a collection of news articles: 1. (farmers, farm, food, rice, agriculture) 2. (stories, undated, receive, scheduled, clients) The first topic is clearly related to agriculture. The subject of the second topic, however, is less clear, and may confuse users if presented to them as part of a larger topic model. Measuring the human-interpretability of topics and the overall topic model is the core topic of this paper. Various methodologies have been proposed for measuring the semantic interpretability of topics. In Chang et al. (2009), the authors proposed an indirect approach based on word intrusion, where “intruder words” are randomly injected into topics and human users are asked to identify the intruder words. The word intrusion task builds on the assumption that the intruder words are more identifiable in coherent topics than in incoherent topics, and thus the interpretability of a topic can be estimated by measuring how readily the intruder words can be manually identified by annotators. Since its inception, the method of Chang et al. (2009) has been used variously as a means of assessing topic models (Paul and Girju</context>
<context position="5560" citStr="Chang et al. (2009)" startWordPosition="873" endWordPosition="876">etability of topics. The relationship between these methodologies, however, is poorly understood, and there is no consensus on what is the best approach for computing the semantic interpretability of topic models. This is a second contribution of this paper: we perform a systematic empirical comparison of the different methods and find appreciable differences between them. We further go on to propose an improved formulation of Newman et al. (2010) based on normalised PMI. Finally, we release a toolkit which implements the topic interpretability measures described in this paper. 2 Related Work Chang et al. (2009) challenged the conventional wisdom that held-out likelihood — often computed as the perplexity of test data or unseen documents — is the only way to evaluate topic models. To measure the human-interpretability of topics, the authors proposed a word intrusion task and conducted experiments using three topic models: Latent Dirichlet Allocation (LDA: Blei et al. (2003)), Probabilistic Latent Semantic Indexing (PLSI: Hofmann (1999)) and the Correlated Topic Model (CTM: Blei and Lafferty (2005)). Contrary to expectation, they found that perplexity correlates negatively with topic interpretability.</context>
<context position="8673" citStr="Chang et al. (2009)" startWordPosition="1370" endWordPosition="1373">ty methods for computing coherence, calculating the distributional similarity between semantic vectors for the top-N topic words using a range of distributional similarity measures such as cosine similarity and the Dice coefficient. To construct the semantic vector space for the topic words, they used English Wikipedia as the reference corpus, and collected words that co-occur in a window of ±5 words. They showed that their method correlates well with the observed coherence rated by human judges. 3 Dataset As one of the primary foci of this paper is the automation of the intruder word task of Chang et al. (2009), our primary dataset is that used in the original paper by Chang et al. (2009), which provides topics and human annotations for a range of 531 domains and topic model types. In the dataset, two text collections were used: (1) 10,000 articles from English Wikipedia (WIKI); and (2) 8,447 articles from the New York Times dating from 1987 to 2007 (NEWS). For each document collection, topics were generated by three topic modelling methods: LDA, PLSI and CTM (see Section 2). For each topic model, three settings of T (the number of topics) were used: T = 50, T = 100 and T = 150. In total, there were</context>
<context position="12309" citStr="Chang et al. (2009)" startWordPosition="1984" endWordPosition="1987">ord intrusion task measures topic interpretability indirectly, by computing the fraction of annotators who successfully identify the intruder word. A limitation of the word intrusion 1In the WIKI topics there were corrupted symbols in the topic words for 24 topics. We removed these topics, reducing the total number of topics to 876. 2For both corpora we perform tokenisation and POS tagging using OpenNLP and lemmatisation using Morpha (Minnen et al., 2001). P(wi, wj) P(wi) We additionally experiment with normalised pointwise mutual information (NPMI: Bouma (2009)): NPMI(wi) = In the dataset of Chang et al. (2009) (see Section 3), each topic was presented to 8 annotators, with small variations in the displayed topic words (including the intruder word) for each annotator. That is, each topic has essentially 8 subtly different representations. To measure topic interpretability, the authors defined “model precision”: the relative success of human annotators at identifying the intruder word, across all representations of the different topics. The model precision scores produced by human judges are henceforth referred to as WI-Human, and the scores produced by our 3N is the number of topic words displayed t</context>
<context position="22675" citStr="Chang et al. (2009)" startWordPosition="3607" endWordPosition="3610">group pairs in the topics.8 That is, for each topic, we randomly split the annotations into two sub-groups, and compute the Pearson correlation coefficient of the model precisions produced by the first sub-group and that of the second subgroup. The original dataset has 8 annotations per topic. Splitting the annotations into two sub-groups reduces the number of annotations to 4 per group, which is not ideal for computing model precision. We thus chose to expand the number of annotations by sampling 300 random topics from each domain (for a total of 600 topics) and following the same process as Chang et al. (2009) to get intruder word annotations using Amazon Mechanical Turk. On average, we obtained 11.7 additional annotations per topic for these 600 topics. The human agreement scores (i.e. the Pearson correlation coefficient of randomised sub-group pairs) for the sampled 600 topics are presented in the last column of Table 4. The sub-group correlation is around r = 0.75 for the topics from both datasets. As such, estimating topic interpretability at the topic level is a much harder task than model-level evaluation. Our automated methods perform at a highly credible 8To counter for the fact that annota</context>
<context position="31570" citStr="Chang et al. (2009)" startWordPosition="5047" endWordPosition="5050">ent to the domain/focus of other topic words. As such, annotators are consistently able to single them out to yield high WI-Human scores, even though the topic as a whole is not coherent. Topics 5 and 6 in Table 8 exhibit this. All topic evaluation measures described in this paper are implemented in an open-source toolkit.9 9https://github.com/jhlau/topic_ interpretability 7 Conclusion In this paper, we examined various methodologies that estimate the semantic interpretability of topics, at two levels: the model level and the topic level. We looked first at the word intrusion task proposed by Chang et al. (2009), and proposed a method that fully automates the task. Next we turned to observed coherence, a more direct approach to estimate topic interpretability. At the model level, results were very positive for both the word intrusion and observed coherence methods. At the topic level, however, the results were more mixed. For observed coherence, our best methods (OC-Auto-NPMI and OC-Auto-DS) were able to emulate human performance. For word intrusion, the automated methods were slightly below human performance, with some room for improvement. We finally observed that there are systematic differences i</context>
</contexts>
<marker>Chang, Boyd-Graber, Gerrish, Wang, Blei, 2009</marker>
<rawString>J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and D. Blei. 2009. Reading tea leaves: How humans interpret topic models. In Advances in Neural Information Processing Systems 21 (NIPS-09), pages 288–296, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>L Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies</booktitle>
<pages>362--370</pages>
<contexts>
<context position="1690" citStr="Haghighi and Vanderwende, 2009" startWordPosition="259" endWordPosition="262">valuation of scientific impact (McCallum et al., 2006; Hall et al., 2008), trend analysis (Bolelli et al., 2009; Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per-document topic allocations are fed into downstream applications such as document summarisation (Haghighi and Vanderwende, 2009), novel word sense detection methods (Lau et al., 2012b) and machine translation (Zhao and Xing, 2007). In fields such as the digital humanities, on the other hand, human users interact directly with the output of topic models. It is this context of topic modelling for direct human consumption that we target in this paper. The topics produced by topic models have a varying degree of human-interpretability. To illustrate this, we present two topics automatically learnt from a collection of news articles: 1. (farmers, farm, food, rice, agriculture) 2. (stories, undated, receive, scheduled, clien</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>A. Haghighi and L. Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies 2009 (NAACL HLT 2009), pages 362–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hall</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Studying the history of ideas using topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>363--371</pages>
<location>Honolulu, USA.</location>
<contexts>
<context position="1132" citStr="Hall et al., 2008" startWordPosition="170" endWordPosition="173">y of the topic model and topics remains an open research area. In this work, we explore the two tasks of automatic evaluation of single topics and automatic evaluation of whole topic models, and provide recommendations on the best strategy for performing the two tasks, in addition to providing an open-source toolkit for topic and topic model evaluation. 1 Introduction Topic modelling based on Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and related methods is increasingly being used in user-focused tasks, in contexts such as the evaluation of scientific impact (McCallum et al., 2006; Hall et al., 2008), trend analysis (Bolelli et al., 2009; Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per-document topic allocations are fed into downstream applications such as document summarisation (Haghighi and Vanderwende, 2009), novel word sense detection methods (Lau </context>
</contexts>
<marker>Hall, Jurafsky, Manning, 2008</marker>
<rawString>D. Hall, D. Jurafsky, and C.D. Manning. 2008. Studying the history of ideas using topic models. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP 2008), pages 363–371, Honolulu, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hall</author>
<author>P Clough</author>
<author>M Stevenson</author>
</authors>
<title>Evaluating the use of clustering for automatically organising digital library collections.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second International Conference on Theory and Practice of Digital Libraries,</booktitle>
<pages>323--334</pages>
<location>Paphos, Cyprus.</location>
<contexts>
<context position="3354" citStr="Hall et al., 2012" startWordPosition="530" endWordPosition="533">ct approach based on word intrusion, where “intruder words” are randomly injected into topics and human users are asked to identify the intruder words. The word intrusion task builds on the assumption that the intruder words are more identifiable in coherent topics than in incoherent topics, and thus the interpretability of a topic can be estimated by measuring how readily the intruder words can be manually identified by annotators. Since its inception, the method of Chang et al. (2009) has been used variously as a means of assessing topic models (Paul and Girju, 2010; Reisinger et al., 2010; Hall et al., 2012). Despite its wide acceptance, the method relies on manual annotation and has never been automated. This is one of the primary contributions of this work: the demonstration that we can automate the method of Chang et al. (2009) at near-human levels of accuracy, as a result of which we can perform automatic evaluation of the human-interpretability of topics, as well as topic models. There has been prior work to directly estimate the human-interpretability of topics through automatic means. For example, Newman et al. 530 Proceedings of the 14th Conference of the European Chapter of the Associati</context>
</contexts>
<marker>Hall, Clough, Stevenson, 2012</marker>
<rawString>M. Hall, P. Clough, and M. Stevenson. 2012. Evaluating the use of clustering for automatically organising digital library collections. In Proceedings of the Second International Conference on Theory and Practice of Digital Libraries, pages 323–334, Paphos, Cyprus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of 22nd International ACMSIGIR Conference on Research and Development in Information Retrieval (SIGIR’99),</booktitle>
<pages>50--57</pages>
<location>Berkeley, USA.</location>
<contexts>
<context position="5992" citStr="Hofmann (1999)" startWordPosition="943" endWordPosition="944"> et al. (2010) based on normalised PMI. Finally, we release a toolkit which implements the topic interpretability measures described in this paper. 2 Related Work Chang et al. (2009) challenged the conventional wisdom that held-out likelihood — often computed as the perplexity of test data or unseen documents — is the only way to evaluate topic models. To measure the human-interpretability of topics, the authors proposed a word intrusion task and conducted experiments using three topic models: Latent Dirichlet Allocation (LDA: Blei et al. (2003)), Probabilistic Latent Semantic Indexing (PLSI: Hofmann (1999)) and the Correlated Topic Model (CTM: Blei and Lafferty (2005)). Contrary to expectation, they found that perplexity correlates negatively with topic interpretability. In the word intrusion task, each topic is presented as a list of six words — the five most probable topic words and a randomly-selected “intruder word”, which has low probability in the topic of interest, but high probability in other topics — and human users are asked to identify the intruder word that does not belong to the topic in question. Newman et al. (2010) capture topic interpretability using a more direct approach, by</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>T. Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of 22nd International ACMSIGIR Conference on Research and Development in Information Retrieval (SIGIR’99), pages 50–57, Berkeley, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Training linear SVMs in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="10869" citStr="Joachims (2006)" startWordPosition="1753" endWordPosition="1754">the semantics of the topic. Observing that the word intrusion task — the task of detecting the least representative word — is the converse of the best topic word selection task, we adapt their methodology to automatically identify the intruder word for the word intrusion task, based on the knowledge that there is a unique intruder word per topic. The methodology works as follows: given a set of topics (including intruder words), we compute the word association features for each of the topN topic words of a topic,3 and combine the features in a ranking support vector regression model (SVMrank: Joachims (2006)) to learn the intruder words. Following Lau et al. (2010), we use three word association measures: P(wi, wj) log P(wi)P(wj) PMI(wi) = N−1� j 4 Human-Interpretability at the Model CP1(wi) = N−1� P(wi, wj) Level j P(wj) In this section, we evaluate measures for estimating human-interpretability at the model level. That is, for a measure — human-judged or automated — we first aggregate its coherence/interpretability scores for all topics from a given topic model to obtain the topic model’s average coherence score. We then calculate the Pearson correlation coefficients between the two measures us</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>T. Joachims. 2006. Training linear SVMs in linear time. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2006), Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Lau</author>
<author>D Newman</author>
<author>S Karimi</author>
<author>T Baldwin</author>
</authors>
<title>Best topic word selection for topic labelling.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), Posters Volume,</booktitle>
<pages>605--613</pages>
<location>Beijing, China.</location>
<contexts>
<context position="10159" citStr="Lau et al. (2010)" startWordPosition="1630" endWordPosition="1633">tains 1.2 million New York Times articles from 1994 to 2004 (from the English Gigaword); and (2) WIKI-FULL, which contains 3.3 million English Wikipedia articles (retrieved November 28th 2009).2 The rationale for choosing the New York Times and English Wikipedia as the reference corpora is to ensure domain consistency with the word intrusion dataset; the full collections are used to more robustly estimate lexical probabilities. task is that it requires human annotations, therefore preventing large-scale evaluation. We begin by proposing a methodology to fully automate the word intrusion task. Lau et al. (2010) proposed a methodology that learns the most representative or best topic word that summarises the semantics of the topic. Observing that the word intrusion task — the task of detecting the least representative word — is the converse of the best topic word selection task, we adapt their methodology to automatically identify the intruder word for the word intrusion task, based on the knowledge that there is a unique intruder word per topic. The methodology works as follows: given a set of topics (including intruder words), we compute the word association features for each of the topN topic word</context>
</contexts>
<marker>Lau, Newman, Karimi, Baldwin, 2010</marker>
<rawString>J.H. Lau, D. Newman, S. Karimi, and T. Baldwin. 2010. Best topic word selection for topic labelling. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), Posters Volume, pages 605–613, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Lau</author>
<author>K Grieser</author>
<author>D Newman</author>
<author>T Baldwin</author>
</authors>
<title>Automatic labelling of topic models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT 2011),</booktitle>
<pages>1536--1545</pages>
<location>Portland, USA.</location>
<contexts>
<context position="4780" citStr="Lau et al., 2011" startWordPosition="751" endWordPosition="754">ic method for estimating topic coherence based on pairwise pointwise mutual information (PMI) between the topic words. Mimno et al. (2011) similarly introduced a methodology for computing coherence, replacing PMI with log conditional probability. Musat et al. (2011) incorporated the WordNet hierarchy to capture the relevance of topics, and in Aletras and Stevenson (2013a), the authors proposed the use of distributional similarity for computing the pairwise association of the topic words. One application of these methods has been to remove incoherent topics before generating labels for topics (Lau et al., 2011; Aletras and Stevenson, 2013b). Ultimately, all these methodologies, and also the word intrusion approach, attempt to assess the same quality: the human-interpretability of topics. The relationship between these methodologies, however, is poorly understood, and there is no consensus on what is the best approach for computing the semantic interpretability of topic models. This is a second contribution of this paper: we perform a systematic empirical comparison of the different methods and find appreciable differences between them. We further go on to propose an improved formulation of Newman e</context>
</contexts>
<marker>Lau, Grieser, Newman, Baldwin, 2011</marker>
<rawString>J.H. Lau, K. Grieser, D. Newman, and T. Baldwin. 2011. Automatic labelling of topic models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT 2011), pages 1536– 1545, Portland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Lau</author>
<author>N Collier</author>
<author>T Baldwin</author>
</authors>
<title>Online trend analysis with topic models: #twitter trends detection topic model online.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012),</booktitle>
<pages>1519--1534</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="1188" citStr="Lau et al., 2012" startWordPosition="180" endWordPosition="183">rea. In this work, we explore the two tasks of automatic evaluation of single topics and automatic evaluation of whole topic models, and provide recommendations on the best strategy for performing the two tasks, in addition to providing an open-source toolkit for topic and topic model evaluation. 1 Introduction Topic modelling based on Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and related methods is increasingly being used in user-focused tasks, in contexts such as the evaluation of scientific impact (McCallum et al., 2006; Hall et al., 2008), trend analysis (Bolelli et al., 2009; Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per-document topic allocations are fed into downstream applications such as document summarisation (Haghighi and Vanderwende, 2009), novel word sense detection methods (Lau et al., 2012b) and machine translation (Zhao and Xing, 2</context>
</contexts>
<marker>Lau, Collier, Baldwin, 2012</marker>
<rawString>J.H. Lau, N. Collier, and T. Baldwin. 2012a. Online trend analysis with topic models: #twitter trends detection topic model online. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 1519– 1534, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Lau</author>
<author>P Cook</author>
<author>D McCarthy</author>
<author>D Newman</author>
<author>T Baldwin</author>
</authors>
<title>Word sense induction for novel sense detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the EACL (EACL 2012),</booktitle>
<pages>591--601</pages>
<location>Avignon, France.</location>
<contexts>
<context position="1188" citStr="Lau et al., 2012" startWordPosition="180" endWordPosition="183">rea. In this work, we explore the two tasks of automatic evaluation of single topics and automatic evaluation of whole topic models, and provide recommendations on the best strategy for performing the two tasks, in addition to providing an open-source toolkit for topic and topic model evaluation. 1 Introduction Topic modelling based on Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and related methods is increasingly being used in user-focused tasks, in contexts such as the evaluation of scientific impact (McCallum et al., 2006; Hall et al., 2008), trend analysis (Bolelli et al., 2009; Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per-document topic allocations are fed into downstream applications such as document summarisation (Haghighi and Vanderwende, 2009), novel word sense detection methods (Lau et al., 2012b) and machine translation (Zhao and Xing, 2</context>
</contexts>
<marker>Lau, Cook, McCarthy, Newman, Baldwin, 2012</marker>
<rawString>J.H. Lau, P. Cook, D. McCarthy, D. Newman, and T. Baldwin. 2012b. Word sense induction for novel sense detection. In Proceedings of the 13th Conference of the EACL (EACL 2012), pages 591–601, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Lau</author>
<author>T Baldwin</author>
<author>D Newman</author>
</authors>
<title>On collocations and topic models.</title>
<date>2013</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>10</volume>
<issue>3</issue>
<pages>10--14</pages>
<contexts>
<context position="16575" citStr="Lau et al. (2013)" startWordPosition="2646" endWordPosition="2649"> (2013a). For OC-Auto-PMI, OC-Auto-NPMI and OCAuto-LCP, all topics are lemmatised and intruder words are removed before coherence is computed.7 In-domain and cross-domain pairings of 6Although the original method uses the topic-modelled document collection and document co-occurrence for sampling word counts, for a fairer comparison we use log conditional probability only as a replacement to the PMI component of the coherence computation (i.e. words are still sampled using a reference corpus and a sliding window). For additional evidence that the original method performs at a subpar level, see Lau et al. (2013) and Aletras and Stevenson (2013a). 7We once again use Morpha to do the lemmatisation, and determine POS via the majority POS for a given word, aggregated over all its occurrences in English Wikipedia. OC-Auto-PMI(t) = N j=2 j−1 � i=1 OC-Auto-NPMI(t) = N j=2 j−1 � i=1 3. OC-Auto-LCP: Pairwise log conditional probability of top-N topic words (Mimno et al., 2011):6 OC-Auto-LCP(t) = P(wj, wi) log P(wi) N j=2 j−1 � i=1 533 Topic Ref. Pearson’s r with OC-Human Domain Corpus OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS WIKI WIKI-FULL 0.490 0.903 0.959 0.859 NEWS-FULL 0.696 0.844 0.913 NEWS NEWS-F</context>
</contexts>
<marker>Lau, Baldwin, Newman, 2013</marker>
<rawString>J.H. Lau, T. Baldwin, and D. Newman. 2013. On collocations and topic models. ACM Transactions on Speech and Language Processing, 10(3):10:1– 10:14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>G S Mann</author>
<author>D Mimno</author>
</authors>
<title>Bibliometric impact measures leveraging topic analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital Libraries</booktitle>
<pages>65--74</pages>
<location>Chapel Hill, USA.</location>
<contexts>
<context position="1112" citStr="McCallum et al., 2006" startWordPosition="166" endWordPosition="169">of the intrinsic quality of the topic model and topics remains an open research area. In this work, we explore the two tasks of automatic evaluation of single topics and automatic evaluation of whole topic models, and provide recommendations on the best strategy for performing the two tasks, in addition to providing an open-source toolkit for topic and topic model evaluation. 1 Introduction Topic modelling based on Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and related methods is increasingly being used in user-focused tasks, in contexts such as the evaluation of scientific impact (McCallum et al., 2006; Hall et al., 2008), trend analysis (Bolelli et al., 2009; Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per-document topic allocations are fed into downstream applications such as document summarisation (Haghighi and Vanderwende, 2009), novel word sense det</context>
</contexts>
<marker>McCallum, Mann, Mimno, 2006</marker>
<rawString>A McCallum, G.S. Mann, and D Mimno. 2006. Bibliometric impact measures leveraging topic analysis. In Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital Libraries 2006 (JCDL’06), pages 65–74, Chapel Hill, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mimno</author>
<author>H Wallach</author>
<author>E Talley</author>
<author>M Leenders</author>
<author>A McCallum</author>
</authors>
<title>Optimizing semantic coherence in topic models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011),</booktitle>
<pages>262--272</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="4302" citStr="Mimno et al. (2011)" startWordPosition="678" endWordPosition="681">an-interpretability of topics, as well as topic models. There has been prior work to directly estimate the human-interpretability of topics through automatic means. For example, Newman et al. 530 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 530–539, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics (2010) introduced the notion of topic “coherence”, and proposed an automatic method for estimating topic coherence based on pairwise pointwise mutual information (PMI) between the topic words. Mimno et al. (2011) similarly introduced a methodology for computing coherence, replacing PMI with log conditional probability. Musat et al. (2011) incorporated the WordNet hierarchy to capture the relevance of topics, and in Aletras and Stevenson (2013a), the authors proposed the use of distributional similarity for computing the pairwise association of the topic words. One application of these methods has been to remove incoherent topics before generating labels for topics (Lau et al., 2011; Aletras and Stevenson, 2013b). Ultimately, all these methodologies, and also the word intrusion approach, attempt to ass</context>
<context position="7135" citStr="Mimno et al. (2011)" startWordPosition="1127" endWordPosition="1130">t al. (2010) capture topic interpretability using a more direct approach, by asking human users to rate topics (represented by their top-10 topic words) on a 3-point scale based on how coherent the topic words are (i.e. their observed coherence). They proposed several ways of automating the estimation of the observed coherence, and ultimately found that a simple method based on PMI term co-occurrence within a sliding context window over English Wikipedia produces the consistently best result, nearing levels of interannotator agreement over topics learnt from two distinct document collections. Mimno et al. (2011) proposed a closely-related method for evaluating semantic coherence, replacing PMI with log conditional probability. Rather than using Wikipedia for sampling the word cooccurrence counts, Mimno et al. (2011) used the topic-modelled documents, and found that their measure correlates well with human judgements of observed coherence (where topics were rated in the same manner as Newman et al. (2010), based on a 3-point ordinal scale). To incorporate the evaluation of semantic coherence into the topic model, the authors proposed to record words that co-occur together frequently, and update the co</context>
<context position="16938" citStr="Mimno et al., 2011" startWordPosition="2708" endWordPosition="2711">ability only as a replacement to the PMI component of the coherence computation (i.e. words are still sampled using a reference corpus and a sliding window). For additional evidence that the original method performs at a subpar level, see Lau et al. (2013) and Aletras and Stevenson (2013a). 7We once again use Morpha to do the lemmatisation, and determine POS via the majority POS for a given word, aggregated over all its occurrences in English Wikipedia. OC-Auto-PMI(t) = N j=2 j−1 � i=1 OC-Auto-NPMI(t) = N j=2 j−1 � i=1 3. OC-Auto-LCP: Pairwise log conditional probability of top-N topic words (Mimno et al., 2011):6 OC-Auto-LCP(t) = P(wj, wi) log P(wi) N j=2 j−1 � i=1 533 Topic Ref. Pearson’s r with OC-Human Domain Corpus OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS WIKI WIKI-FULL 0.490 0.903 0.959 0.859 NEWS-FULL 0.696 0.844 0.913 NEWS NEWS-FULL 0.965 0.979 0.887 0.941 WIKI-FULL 0.931 0.964 0.872 Table 2: Pearson correlation of OC-Human and the automated methods — OC-Auto-PMI, OC-AutoNPMI, OC-Auto-LCP and OC-Auto-DS — at the model level. the topic domain and reference corpus are experimented with for these measures. For OC-Auto-DS, all topics are lemmatised, intruder words are removed and English W</context>
</contexts>
<marker>Mimno, Wallach, Talley, Leenders, McCallum, 2011</marker>
<rawString>D. Mimno, H. Wallach, E. Talley, M. Leenders, and A. McCallum. 2011. Optimizing semantic coherence in topic models. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011), pages 262–272, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>J Carroll</author>
<author>D Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="12149" citStr="Minnen et al., 2001" startWordPosition="1959" endWordPosition="1963">the correlation is computed over nine sets of topics (3 topic modellers x 3 settings of T) for each of WIKI and NEWS. 4.1 Indirect Approach: Word Intrusion The word intrusion task measures topic interpretability indirectly, by computing the fraction of annotators who successfully identify the intruder word. A limitation of the word intrusion 1In the WIKI topics there were corrupted symbols in the topic words for 24 topics. We removed these topics, reducing the total number of topics to 876. 2For both corpora we perform tokenisation and POS tagging using OpenNLP and lemmatisation using Morpha (Minnen et al., 2001). P(wi, wj) P(wi) We additionally experiment with normalised pointwise mutual information (NPMI: Bouma (2009)): NPMI(wi) = In the dataset of Chang et al. (2009) (see Section 3), each topic was presented to 8 annotators, with small variations in the displayed topic words (including the intruder word) for each annotator. That is, each topic has essentially 8 subtly different representations. To measure topic interpretability, the authors defined “model precision”: the relative success of human annotators at identifying the intruder word, across all representations of the different topics. The mo</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>G. Minnen, J. Carroll, and D. Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Musat</author>
<author>J Velcin</author>
<author>S Trausan-Matu</author>
<author>M A Rizoiu</author>
</authors>
<title>Improving topic evaluation using conceptual knowledge.</title>
<date>2011</date>
<booktitle>In Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI-2011),</booktitle>
<pages>1866--1871</pages>
<location>Barcelona,</location>
<contexts>
<context position="4430" citStr="Musat et al. (2011)" startWordPosition="696" endWordPosition="699">y of topics through automatic means. For example, Newman et al. 530 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 530–539, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics (2010) introduced the notion of topic “coherence”, and proposed an automatic method for estimating topic coherence based on pairwise pointwise mutual information (PMI) between the topic words. Mimno et al. (2011) similarly introduced a methodology for computing coherence, replacing PMI with log conditional probability. Musat et al. (2011) incorporated the WordNet hierarchy to capture the relevance of topics, and in Aletras and Stevenson (2013a), the authors proposed the use of distributional similarity for computing the pairwise association of the topic words. One application of these methods has been to remove incoherent topics before generating labels for topics (Lau et al., 2011; Aletras and Stevenson, 2013b). Ultimately, all these methodologies, and also the word intrusion approach, attempt to assess the same quality: the human-interpretability of topics. The relationship between these methodologies, however, is poorly und</context>
</contexts>
<marker>Musat, Velcin, Trausan-Matu, Rizoiu, 2011</marker>
<rawString>C. Musat, J. Velcin, S. Trausan-Matu, and M.A. Rizoiu. 2011. Improving topic evaluation using conceptual knowledge. In Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI-2011), pages 1866–1871, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Newman</author>
<author>J H Lau</author>
<author>K Grieser</author>
<author>T Baldwin</author>
</authors>
<title>Automatic evaluation of topic coherence.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2010),</booktitle>
<pages>100--108</pages>
<location>Los Angeles, USA.</location>
<contexts>
<context position="5392" citStr="Newman et al. (2010)" startWordPosition="846" endWordPosition="849">l., 2011; Aletras and Stevenson, 2013b). Ultimately, all these methodologies, and also the word intrusion approach, attempt to assess the same quality: the human-interpretability of topics. The relationship between these methodologies, however, is poorly understood, and there is no consensus on what is the best approach for computing the semantic interpretability of topic models. This is a second contribution of this paper: we perform a systematic empirical comparison of the different methods and find appreciable differences between them. We further go on to propose an improved formulation of Newman et al. (2010) based on normalised PMI. Finally, we release a toolkit which implements the topic interpretability measures described in this paper. 2 Related Work Chang et al. (2009) challenged the conventional wisdom that held-out likelihood — often computed as the perplexity of test data or unseen documents — is the only way to evaluate topic models. To measure the human-interpretability of topics, the authors proposed a word intrusion task and conducted experiments using three topic models: Latent Dirichlet Allocation (LDA: Blei et al. (2003)), Probabilistic Latent Semantic Indexing (PLSI: Hofmann (1999)</context>
<context position="7535" citStr="Newman et al. (2010)" startWordPosition="1187" endWordPosition="1190">ence within a sliding context window over English Wikipedia produces the consistently best result, nearing levels of interannotator agreement over topics learnt from two distinct document collections. Mimno et al. (2011) proposed a closely-related method for evaluating semantic coherence, replacing PMI with log conditional probability. Rather than using Wikipedia for sampling the word cooccurrence counts, Mimno et al. (2011) used the topic-modelled documents, and found that their measure correlates well with human judgements of observed coherence (where topics were rated in the same manner as Newman et al. (2010), based on a 3-point ordinal scale). To incorporate the evaluation of semantic coherence into the topic model, the authors proposed to record words that co-occur together frequently, and update the counts of all associated words before and after the sampling of a new topic assignment in the Gibbs sampler. This variant of topic model was shown to produce more coherent topics than LDA based on the log conditional probability coherence measure. Aletras and Stevenson (2013a) introduced distributional semantic similarity methods for computing coherence, calculating the distributional similarity bet</context>
<context position="14384" citStr="Newman et al. (2010)" startWordPosition="2300" endWordPosition="2303">achieving r &gt; 0.9 in most cases for both WI-Auto-PMI or WI-Auto-NPMI, demonstrating the effectiveness of our methodology at automating the word intrusion task for estimating humaninterpretability at the model level. Overall, WIAuto-PMI outperforms WI-Auto-NPMI. Note that although our proposed methodology is supervised, as intruder words are synthetically generated and no annotation is needed for the supervised learning, the whole process of computing topic coherence via word intrusion is fully automatic, without the need for hand-labelled training data. 4.2 Direct Approach: Observed Coherence Newman et al. (2010) defined topic interpretability based on a more direct approach, by asking human judges to rate topics based on the observed coherence of the top-N topic words, and various methodologies have since been proposed to automate the computation of the observed coherence. In this section, we present all these methods and compare them. The word intrusion dataset is not annotated with human ratings of observed coherence. To create gold-standard coherence judgements, we used Amazon Mechanical Turk:5 we presented the topics (with intruder words removed) to the Turkers and asked them to rate the topics u</context>
<context position="15731" citStr="Newman et al., 2010" startWordPosition="2510" endWordPosition="2513">PMI+CP1+C2 features. 5https://www.mturk.com/mturk/ ordinal scale, following Newman et al. (2010). In total, we collected six to fourteen annotations per topic (an average of 8.4 annotations per topic). The observed coherence of a topic is computed as the arithmetic mean of the annotators’ ratings, once again following Newman et al. (2010). The human-judged observed topic coherence is henceforth referred to as OC-Human. For the automated methods, we experimented with the following methods for estimating the human-interpretability of a topic t: 1. OC-Auto-PMI: Pairwise PMI of top-N topic words (Newman et al., 2010): P(wj, wi) log P(wi)P(wj) 2. OC-Auto-NPMI: NPMI variant of OCAuto-PMI: log P (wj,wi) P (wi)P(wj) − log P(wi, wj) 4. OC-Auto-DS: Pairwise distributional similarity of the top-N topic words, as described in Aletras and Stevenson (2013a). For OC-Auto-PMI, OC-Auto-NPMI and OCAuto-LCP, all topics are lemmatised and intruder words are removed before coherence is computed.7 In-domain and cross-domain pairings of 6Although the original method uses the topic-modelled document collection and document co-occurrence for sampling word counts, for a fairer comparison we use log conditional probability only</context>
<context position="23435" citStr="Newman et al. (2010)" startWordPosition="3734" endWordPosition="3737">topics. The human agreement scores (i.e. the Pearson correlation coefficient of randomised sub-group pairs) for the sampled 600 topics are presented in the last column of Table 4. The sub-group correlation is around r = 0.75 for the topics from both datasets. As such, estimating topic interpretability at the topic level is a much harder task than model-level evaluation. Our automated methods perform at a highly credible 8To counter for the fact that annotators labelled varying numbers of topics. r = 0.6, but there is certainly room for improvement. Note that the correlation values reported in Newman et al. (2010) are markedly higher than ours, as they evaluated based on Spearman rank correlation, which isn’t attuned to the relative differences in coherence values and returns higher values for the task. 5.2 Direct Approach: Observed Coherence We repeat the experiments of observed coherence in Section 4.2, and evaluate the correlation of the automated methods (OC-Auto-PMI, OC-AutoNPMI, OC-Auto-LCP and OC-Auto-DS) on the human gold standard (OC-Human) at the topic level. Results are summarised in Table 5. OC-Auto-PMI performs poorly at the topic level in the WIKI domain, similar to what was seen at the m</context>
</contexts>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>D. Newman, J.H. Lau, K. Grieser, and T. Baldwin. 2010. Automatic evaluation of topic coherence. In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2010), pages 100–108, Los Angeles, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paul</author>
<author>R Girju</author>
</authors>
<title>A two-dimensional topicaspect model for discovering multi-faceted topics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 24th Annual Conference on Artificial Intelligence (AAAI-10),</booktitle>
<location>Atlanta, USA.</location>
<contexts>
<context position="3310" citStr="Paul and Girju, 2010" startWordPosition="522" endWordPosition="525"> et al. (2009), the authors proposed an indirect approach based on word intrusion, where “intruder words” are randomly injected into topics and human users are asked to identify the intruder words. The word intrusion task builds on the assumption that the intruder words are more identifiable in coherent topics than in incoherent topics, and thus the interpretability of a topic can be estimated by measuring how readily the intruder words can be manually identified by annotators. Since its inception, the method of Chang et al. (2009) has been used variously as a means of assessing topic models (Paul and Girju, 2010; Reisinger et al., 2010; Hall et al., 2012). Despite its wide acceptance, the method relies on manual annotation and has never been automated. This is one of the primary contributions of this work: the demonstration that we can automate the method of Chang et al. (2009) at near-human levels of accuracy, as a result of which we can perform automatic evaluation of the human-interpretability of topics, as well as topic models. There has been prior work to directly estimate the human-interpretability of topics through automatic means. For example, Newman et al. 530 Proceedings of the 14th Confere</context>
</contexts>
<marker>Paul, Girju, 2010</marker>
<rawString>M. Paul and R. Girju. 2010. A two-dimensional topicaspect model for discovering multi-faceted topics. In Proceedings of the 24th Annual Conference on Artificial Intelligence (AAAI-10), Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reisinger</author>
<author>A Waters</author>
<author>B Silverthorn</author>
<author>R J Mooney</author>
</authors>
<title>Spherical topic models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 27th International Conference on Machine Learning (ICML 2010),</booktitle>
<pages>903--910</pages>
<location>Haifa,</location>
<contexts>
<context position="3334" citStr="Reisinger et al., 2010" startWordPosition="526" endWordPosition="529">thors proposed an indirect approach based on word intrusion, where “intruder words” are randomly injected into topics and human users are asked to identify the intruder words. The word intrusion task builds on the assumption that the intruder words are more identifiable in coherent topics than in incoherent topics, and thus the interpretability of a topic can be estimated by measuring how readily the intruder words can be manually identified by annotators. Since its inception, the method of Chang et al. (2009) has been used variously as a means of assessing topic models (Paul and Girju, 2010; Reisinger et al., 2010; Hall et al., 2012). Despite its wide acceptance, the method relies on manual annotation and has never been automated. This is one of the primary contributions of this work: the demonstration that we can automate the method of Chang et al. (2009) at near-human levels of accuracy, as a result of which we can perform automatic evaluation of the human-interpretability of topics, as well as topic models. There has been prior work to directly estimate the human-interpretability of topics through automatic means. For example, Newman et al. 530 Proceedings of the 14th Conference of the European Chap</context>
</contexts>
<marker>Reisinger, Waters, Silverthorn, Mooney, 2010</marker>
<rawString>J. Reisinger, A. Waters, B. Silverthorn, and R.J. Mooney. 2010. Spherical topic models. In Proceedings of the 27th International Conference on Machine Learning (ICML 2010), pages 903–910, Haifa, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wang</author>
<author>A McCallum</author>
<author>X Wei</author>
</authors>
<title>Topical n-grams: Phrase and topic discovery, with an application to information retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the Seventh IEEE International Conference on Data Mining (ICDM</booktitle>
<pages>697--702</pages>
<location>Omaha, USA.</location>
<contexts>
<context position="1230" citStr="Wang et al., 2007" startWordPosition="187" endWordPosition="190">sks of automatic evaluation of single topics and automatic evaluation of whole topic models, and provide recommendations on the best strategy for performing the two tasks, in addition to providing an open-source toolkit for topic and topic model evaluation. 1 Introduction Topic modelling based on Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and related methods is increasingly being used in user-focused tasks, in contexts such as the evaluation of scientific impact (McCallum et al., 2006; Hall et al., 2008), trend analysis (Bolelli et al., 2009; Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per-document topic allocations are fed into downstream applications such as document summarisation (Haghighi and Vanderwende, 2009), novel word sense detection methods (Lau et al., 2012b) and machine translation (Zhao and Xing, 2007). In fields such as the digital humani</context>
</contexts>
<marker>Wang, McCallum, Wei, 2007</marker>
<rawString>X. Wang, A. McCallum, and X. Wei. 2007. Topical n-grams: Phrase and topic discovery, with an application to information retrieval. In Proceedings of the Seventh IEEE International Conference on Data Mining (ICDM 2007), pages 697–702, Omaha, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zhao</author>
<author>E P Xing</author>
</authors>
<title>HM-BiTAM: Bilingual topic exploration, word alignment, and translation.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS</booktitle>
<pages>1689--1696</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="1792" citStr="Zhao and Xing, 2007" startWordPosition="276" endWordPosition="279">Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per-document topic allocations are fed into downstream applications such as document summarisation (Haghighi and Vanderwende, 2009), novel word sense detection methods (Lau et al., 2012b) and machine translation (Zhao and Xing, 2007). In fields such as the digital humanities, on the other hand, human users interact directly with the output of topic models. It is this context of topic modelling for direct human consumption that we target in this paper. The topics produced by topic models have a varying degree of human-interpretability. To illustrate this, we present two topics automatically learnt from a collection of news articles: 1. (farmers, farm, food, rice, agriculture) 2. (stories, undated, receive, scheduled, clients) The first topic is clearly related to agriculture. The subject of the second topic, however, is le</context>
</contexts>
<marker>Zhao, Xing, 2007</marker>
<rawString>B. Zhao and E.P. Xing. 2007. HM-BiTAM: Bilingual topic exploration, word alignment, and translation. In Advances in Neural Information Processing Systems (NIPS 2007), pages 1689–1696, Vancouver, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>