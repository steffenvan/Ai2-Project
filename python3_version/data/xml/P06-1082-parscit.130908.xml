<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010091">
<title confidence="0.99927">
Word Alignment in English-Hindi Parallel Corpus Using Recency-Vector
Approach: Some Studies
</title>
<author confidence="0.996933">
Niladri Chatterjee
</author>
<affiliation confidence="0.998706">
Department of Mathematics
Indian Institute of Technology Delhi
</affiliation>
<address confidence="0.574684">
Hauz Khas, New Delhi
INDIA - 110016
</address>
<email confidence="0.989634">
niladri iitd@yahoo.com
</email>
<author confidence="0.98624">
Saumya Agrawal
</author>
<affiliation confidence="0.920059333333333">
Department of Mathematics
Indian Institute of Technology
Kharagpur, West Bengal
</affiliation>
<address confidence="0.742943">
INDIA - 721302
</address>
<email confidence="0.990511">
saumya agrawal2000@yahoo.co.in
</email>
<sectionHeader confidence="0.993422" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999911454545455">
Word alignment using recency-vector
based approach has recently become pop-
ular. One major advantage of these tech-
niques is that unlike other approaches they
perform well even if the size of the par-
allel corpora is small. This makes these
algorithms worth-studying for languages
where resources are scarce. In this work
we studied the performance of two very
popular recency-vector based approaches,
proposed in (Fung and McKeown, 1994)
and (Somers, 1998), respectively, for word
alignment in English-Hindi parallel cor-
pus. But performance of the above al-
gorithms was not found to be satisfac-
tory. However, subsequent addition of
some new constraints improved the perfor-
mance of the recency-vector based align-
ment technique significantly for the said
corpus. The present paper discusses the
new version of the algorithm and its per-
formance in detail.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99986858">
Several approaches including statistical tech-
niques (Gale and Church, 1991; Brown et al.,
1993), lexical techniques (Huang and Choi, 2000;
Tiedemann, 2003) and hybrid techniques (Ahren-
berg et al., 2000), have been pursued to design
schemes for word alignment which aims at estab-
lishing links between words of a source language
and a target language in a parallel corpus. All
these schemes rely heavily on rich linguistic re-
sources, either in the form of huge data of parallel
texts or various language/grammar related tools,
such as parser, tagger, morphological analyser etc.
Recency vector based approach has been pro-
posed as an alternative strategy for word align-
ment. Approaches based on recency vectors typ-
ically consider the positions of the word in the
corresponding texts rather than sentence bound-
aries. Two algorithms of this type can be found in
(Fung and McKeown, 1994) and (Somers, 1998).
The algorithms first compute the position vector
V„ for the word w in the text. Typically, V� is
of the form (p1p2 ... pk), where the pis indicate
the positions of the word w in a text T. A new
vector R,,,, called the recency vector, is computed
using the position vector V�, and is defined as
(p2 −p1, p3 −p2, . . . , pk −pk−1). In order to com-
pute the alignment of a given word in the source
language text, the recency vector of the word is
compared with the recency vector of each target
language word and the similarity between them is
measured by computing a matching cost associ-
ated with the recency vectors using dynamic pro-
gramming. The target language word having the
least cost is selected as the aligned word.
The results given in the above references show
that the algorithms worked quite well in aligning
words in parallel corpora of language pairs con-
sisting of various European languages and Chi-
nese, Japanese, taken pair-wise. Precision of about
70% could be achieved using these algorithms.
The major advantage of this approach is that it can
work even on a relatively small dataset and it does
not rely on rich language resources.
The above advantage motivated us to study
the effectiveness of these algorithms for aligning
words in English-Hindi parallel texts. The corpus
used for this work is described in Table 1. It has
been made manually from three different sources:
children’s storybooks, English to Hindi translation
book material, and advertisements. We shall call
</bodyText>
<page confidence="0.989485">
649
</page>
<bodyText confidence="0.98573625">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 649–656,
Sydney, July 2006. c�2006 Association for Computational Linguistics
the three corpora as Storybook corpus, Sentence
corpus and Advertisement corpus, respectively.
</bodyText>
<sectionHeader confidence="0.62941" genericHeader="method">
2 Word Alignment Algorithm: Recency
Vector Based Approach
</sectionHeader>
<bodyText confidence="0.9990785">
DK-vec algorithm given in (Fung and McKeown,
1994) uses the following dynamic programming
based approach to compute the matching cost
C(n, m) of two vectors v1 and v2 of lengths n and
m, respectively. The cost is calculated recursively
using the following formula,
</bodyText>
<equation confidence="0.9982675">
C(i,j) = |(v1(i) − v2(j) |+ min{C(i − 1,j),
C(i − 1,j − 1),C(i,j − 1)}
</equation>
<bodyText confidence="0.999804666666667">
where i and j have values from 2 to n and 2 to
m respectively, n and m being the number of dis-
tinct words in source and target language corpus
respectively. Note that vl(k) denotes the kth entry
of the vector vl, for l = 1 and 2. The costs are
initialised as follows.
</bodyText>
<equation confidence="0.999929333333333">
C(1, 1) = |v1(1) − v2(1)|;
C(i, 1) = |v1(i) − v2(1) |+ C(i − 1,1);
C(1,j) = |v1(1) − v2(j) |+ C(1,j − 1);
</equation>
<bodyText confidence="0.998672090909091">
The word in the target language that has the
minimum normalized cost (C(n, m)/(n + m)) is
taken as the translation of the word considered in
the source text.
One major shortcoming of the above scheme is
its high computational complexity i.e. O(mn). A
variation of the above scheme has been proposed
in (Somers, 1998) which has a much lower com-
putational complexity O(min(m, n)). In this new
scheme, a distance called Levenshtein distance(S)
is successively measured using:
</bodyText>
<equation confidence="0.918433">
S = S + min{|v1(i + 1) − v2(j)|,
</equation>
<bodyText confidence="0.962144">
|v1(i+1)−v2(j+1)|, |v1(i)−v2(j+1)|}
The word in the target text having the minimum
value of S (Levenshtein difference) is considered
to be the translation of the word in the source text.
</bodyText>
<subsectionHeader confidence="0.9996835">
2.1 Constraints Used in the Dynamic
Programming Algorithms
</subsectionHeader>
<bodyText confidence="0.998043">
In order to reduce the complexity of the dynamic
programming algorithm certain constraints have
been proposed in (Fung and McKeown, 1994).
</bodyText>
<listItem confidence="0.979457142857143">
1. Starting Point Constraint: The constraint im-
posed is: |first-occurrence of source language
word (w1) - first-occurrence of target lan-
guage word w2 |&lt; 12∗(length of the text).
2. Euclidean distance constraint: The con-
straint imposed is:
�/(m1 − m2)2 + (s1 − s2)2 &lt; T, where mj
and sj are the mean and standard deviation,
respectively, of the recency vector of wj, j =
1 or 2. Here, T is some predefined threshold:
3. Length Constraint: The constraint imposed
is: 21 ∗ f2 &lt; f1 &lt; 2 ∗ f2, where f1 and f2 are
the frequencies of occurrence of w1 and w2,
in their respective texts.
</listItem>
<subsectionHeader confidence="0.99928">
2.2 Experiments with DK-vec Algorithm
</subsectionHeader>
<bodyText confidence="0.999986111111111">
The results of the application of this algorithm
have been very poor when applied on the three
English to Hindi parallel corpora mentioned above
without imposing any constraints.
We then experimented by varying the values of
the parameters in the constraints in order to ob-
serve their effects on the accuracy of alignment.
As was suggested in (Somers, 1998), we also ob-
served that the Euclidean distance constraint is
not very beneficial when the corpus size is small.
So this constraint has not been considered in our
subsequent experiments. Starting point constraint
imposes a range within which the search for the
matching word is restricted. Although Fung and
McKeown suggested the range to be half of the
length of the text, we felt that the optimum value
of this range will vary from text to text depend-
ing on the type of corpus, length ratio of the two
texts etc. Table 2 shows the results obtained on
applying the DK vec algorithm on Sentence cor-
pus for different lower values of range. Similar
results were obtained for the other two corpora.
The maximum increase observed in the F-score is
around 0.062 for the Sentence corpus, 0.03 for the
Story book corpus and 0.05 for the Advertisement
corpus. None of these improvements can be con-
sidered to be significant.
</bodyText>
<subsectionHeader confidence="0.999497">
2.3 Experiments with Somers’ Algorithm
</subsectionHeader>
<bodyText confidence="0.9974631">
The algorithm provided by Somers works by first
finding all the minimum score word pairs using
dynamic programming, and then applying three
filters Multiple Alignment Selection filter, Best
Alignment Score Selection filter and Frequency
Range constraint to the raw results to increase the
accuracy of alignment.
The Multiple Alignment Selection(MAS) filter
takes care of situations where a single target lan-
guage word is aligned with the number of source
</bodyText>
<page confidence="0.993855">
650
</page>
<table confidence="0.8122886">
Corpora English corpus Hindi corpus
Total words Distinct words Total words Distinct words
Storybook corpus 6545 1079 7381 1587
Sentence corpus 8541 1186 9070 1461
Advertisement corpus 3709 1307 4009 1410
</table>
<tableCaption confidence="0.993041">
Table 1: Details of English-Hindi Parallel Corpora
</tableCaption>
<table confidence="0.999895272727273">
Range Available Proposed Correct P% R% F-score
50 516 430 34 7.91 6.59 0.077
150 516 481 51 10.60 09.88 0.102
250 516 506 98 19.37 18.99 0.192
500 516 514 100 19.46 19.38 0.194
700 516 515 94 18.25 18.22 0.182
800 516 515 108 20.97 20.93 0.209
900 516 515 88 17.09 17.05 0.171
1000 516 516 100 19.38 19.38 0.194
2000 516 516 81 15.70 15.70 0.157
4535 516 516 76 14.73 14.73 0.147
</table>
<tableCaption confidence="0.999739">
Table 2: Results of DK-vec Algorithm on Sentence Corpus for different range
</tableCaption>
<bodyText confidence="0.999868">
language words. Somers has suggested that in
such cases only the word pair that has the mini-
mum alignment score should be considered. Table
3 provides results (see column F-score old) when
the raw output is passed through the MAS filters
for the three corpora. Note that for all the three
corpora a variety of frequency ranges have been
considered, and we have observed that the results
obtained are slightly better when the MAS filter
has been used.
The best F-score is obtained when frequency
range is high i.e. 100-150, 100-200. But here
the words are very few in number and are primar-
ily pronoun, determiner or conjunction which are
not significant from alignment perspective. Also,
it was observed that when medium frequency
ranges, such as 30-50, are used the best result,
in terms of precision, is around 20-28% for the
three corpora. However, since the corpus size is
small, here too the available and proposed aligned
word pairs are very few (below 25). Lower fre-
quency ranges (viz. 2-20 and its sub-ranges) re-
sult in the highest number of aligned pairs. We
noticd that these aligned word pairs are typically
verb, adjective, noun and adverb. But here too
the performance of the algorithm may be consid-
ered to be unsatisfactory. Although Somers has
recommended words in the frequency ranges 10-
30 to be considered for alignment, we have con-
sidered lower frequency words too in our experi-
ments. This is because the corpus size being small
we would otherwise have effectively overlooked
many small-frequency words (e.g. noun, verb,
adjective) that are significant from the alignment
point of view.
Somers has further observed that if the Best
Alignment Score Selection (BASS) filter is ap-
plied to yield the first few best results of alignment
the overall quality of the result improves. Figure
1 shows the results of the experiments done for
different alignment score cut-off without consid-
ering the Frequency Range constraint on the three
corpora. However, it was observed that the perfor-
mance of the algorithm reduced slightly on intro-
ducing this BASS filter.
The above experiments suggest that the perfor-
mance of the two algorithms is rather poor in the
context of English-Hindi parallel texts as com-
pared to other language pairs as shown by Fung
and Somers. In the following section we discuss
the reasons for the low recall and precision values.
</bodyText>
<subsectionHeader confidence="0.999297">
2.4 Why Recall and Precision are Low
</subsectionHeader>
<bodyText confidence="0.9999904">
We observed that the primary reason for the poor
performance of the above algorithms in English
- Hindi context is the presence of multiple Hindi
equivalents for the same English word. This can
happen primarily due to three reasons:
</bodyText>
<page confidence="0.996371">
651
</page>
<figureCaption confidence="0.99991">
Figure 1: Results of Somers’ Algorithm and Improved approach for different score cut-off
</figureCaption>
<bodyText confidence="0.998939428571428">
Declension of Adjective: Declensions of adjec-
tives are not present in English grammar. No mor-
phological variation in adjectives takes place along
with the number and gender of the noun. But,
in Hindi, adjectives may have such declensions.
For example, the Hindi for “black” is kaalaa when
the noun is masculine singular number (e.g. black
horse — kaalaa ghodaa). But the Hindi translation
of “black horses” is kaale ghode; whereas “black
mare” is translated as kaalii ghodii. Thus the same
English word “black” may have three Hindi equiv-
alents kaalaa, kaalii, and kale which are to be used
judiciously by considering the number and gender
of the noun concerned.
</bodyText>
<subsubsectionHeader confidence="0.551904">
Declensions of Pronouns and Nouns: Nouns or
</subsubsectionHeader>
<bodyText confidence="0.99959927027027">
pronouns may also have different declensions de-
pending upon the case endings and/or the gender
and number of the object. For example, the same
English word “my” may have different forms (e.g.
meraa, merii, mere) when translated in Hindi.
For illustration, while “my book” is translated as
— merii kitaab, the translation of “my name” is
meraa naam. This happens because naam is mas-
culine in Hindi, while kitaab is feminine. (Note
that in Hindi there is no concept of Neuter gen-
der). Similar declensions may be found with re-
spect to nouns too. For example, the Hindi equiv-
alent of the word “hour” is ghantaa. In plural
form it becomes ghante (e.g. “two hours” — do
ghante). But when used in a prepositional phrase,
it becomes ghanto. Thus the Hindi translation for
“in two hours” is do ghanto mein.
Verb Morphology: Morphology of verbs in
Hindi depends upon the gender, number and per-
son of the subject. There are 11 possible suffixes
(e.g taa, te, tii, egaa) in Hindi that may be at-
tached to the root Verb to render morphological
variations. For illustration,
I read. —* main padtaa hoon (Masculine) but
main padtii hoon (Feminine)
You read. —* tum padte ho (Masculine) or
tum padtii ho (Feminine)
He will read. —* wah padegaa.
Due to the presence of multiple Hindi equiva-
lents, the frequencies of word occurrences differ
significantly, and thereby jeopardize the calcula-
tions. As a consequence, many English words are
wrongly aligned.
In the following section we describe certain
measures that we propose for improving the effi-
ciency of the recency vector based algorithms for
word alignment in English - Hindi parallel texts.
</bodyText>
<sectionHeader confidence="0.995766" genericHeader="method">
3 Improvements in Word Alignment
</sectionHeader>
<bodyText confidence="0.999989894736842">
In order to take care of morphological variations,
we propose to use root words instead of various
declensions of the word. For the present work this
has been done manually for Hindi. However, al-
gorithms similar to Porter’s algorithm may be de-
veloped for Hindi too for cleaning a Hindi text of
morphological inflections (Ramanathan and Rao,
2003). The modified text, for both English and
Hindi, are then subjected to word alignment.
Table 4 gives the details about the root word
corpus used to improve the result of word align-
ment. Here the total number of words for the three
types of corpora is greater than the total number
of words in the original corpus (Table 1). This is
because of the presence of words like “I’ll” in the
English corpus which have been taken as “I shall”
in the root word corpus. Also words like Unkaa
have been taken as Un kaa in the Hindi root word
corpus, leading to an increase in the corpus size.
</bodyText>
<page confidence="0.997404">
652
</page>
<bodyText confidence="0.999991078431373">
Since we observed (see Section 2.2) that Eu-
clidean distance constraint does not add signifi-
cantly to the performance, we propose not to use
this constraint for English-Hindi word alignment.
However, we propose to impose both frequency
range constraint and length constraint (see Sec-
tion 2.1 and Section 2.3). Instead of the starting
point constraint, we have introduced a new con-
straint, viz. segment constraint, to localise the
search for the matching words. The starting point
constraint expresses range in terms of number of
words. However, it has been observed (see sec-
tion 2.2) that the optimum value of the range varies
with the nature of text. Hence no value for range
may be identified that applies uniformly on differ-
ent corpora. Also for noisy corpora the segment
constraint is expected to yield better results as the
search here is localised better. The proposed seg-
ment constraint expresses range in terms of seg-
ments. In order to impose this constraint, first the
parallel texts are aligned at sentence level. The
search for a target language word is then restricted
to few segments above and below the current one.
Use of sententially aligned corpora for word
alignment has already been recommended in
(Brown et al., 1993). However, the requirement
there is quite stringent – all the sentences are to
be correctly aligned. The segment constraint pro-
posed herein works well even if the text alignment
is not perfect. Use of roughly aligned corpora has
also been proposed in (Dagan and Gale, 1993) for
word alignment in bilingual corpora, where statis-
tical techniques have been used as the underlying
alignment scheme. In this work, the sentence level
alignment algorithm given in (Gale and Church,
1991) has been used for applying segment con-
straint. As shown in Table 5, the alignment ob-
tained using this algorithm is not very good (only
70% precision for Storybook corpus). The three
aligned root word corpora are then subjected to
segment constraint in our experiments.
Next important decision we need to take which
dynamic programming algorithm should be used.
Results shown in Section 2.2 and 2.3 demonstrate
that the performance of DK-vec algorithm and
Somers’ algorithm are almost at par. Hence keep-
ing in view the improved computational complex-
ity, we choose to use Levenshtein distance as used
in Somers’ algorithm for comparing recency vec-
tors. In the following subsection we discuss the
experimental results of the proposed approach.
</bodyText>
<subsectionHeader confidence="0.9884295">
3.1 Experimental Results and Comparison
with Existing algorithms
</subsectionHeader>
<bodyText confidence="0.999885591836735">
We have conducted experiments to determine the
number of segments above and below the current
segment that should be considered for searching
the match of a word for each corpus. In this re-
spect we define i-segment constraint in which the
search is restricted to the segments k − i to k + i
of the target language corpus when the word un-
der consideration is in the segment k of the source
language corpus.
Evidently, the value of i depends on the accu-
racy of sentence alignment. Table 5 suggests that
the quality of alignment is different for the three
corpora that we considered. Due to the very high
precision and recall for Sentence corpus we have
restricted our search to the kth segment only, i.e.
the value of i is 0. However, since the results are
not so good for the Storybook and Advertisement
corpora we found after experimenting that the best
results were obtained when i was 1. During the
experiments it was observed that as the number
of segments was lowered or increased from the
optimum segment the accuracy of alignment de-
creased continuously by around 10% for low fre-
quency ranges for the three corpora and remained
almost same for high frequency ranges.
Table 3 shows the results obtained when seg-
ment constraint is applied on the three corpora
at optimum segment range for various frequency
ranges. A comparison between the F-score given
by algorithm in (Somers, 1998) (the column F-
score old in the table) and the F-score obtained
by applying the improved scheme (the column F-
score new in the table) indicate that the results
have improved significantly for low frequency
ranges.
It is observed that the accuracy of alignment for
almost 95% of the available words has increased
significantly. This accounts for words within low
frequency range of 2–40 for Sentence corpus, 2–
30 for Storybook corpus, and 2–20 for Advertise-
ment corpus. Also, most of the correct word pairs
given by the modified approach are verbs, adjec-
tives or nouns. Also it was observed that as the
noise in the corpus increased the results became
poorer. This accounts for the lowest F-score val-
ues for advertisement corpus. The Sentence cor-
pus, however, has been found to be the least noisy,
and highest precision and recall values were ob-
tained with this corpus.
</bodyText>
<page confidence="0.998608">
653
</page>
<bodyText confidence="0.999920590909091">
Using Somers’ second filter on each corpus for
the optimum segment we found that the results at
low scores were better as shown in Figure 1. The
word pairs obtained after applying the modified
approach can be used as anchor points for further
alignment as well as for vocabulary extraction. In
case of the Sentence corpus, best result for anchor
points for further alignment lies at the score cut
off 1000 where precision and recall are 86.88%
and 80.35% respectively. Hence F-score is 0.835
which is very high as compared to 0.173 obtained
by Somers’ approach and indicates an improve-
ment of 382.65%. Also, here the number of cor-
rect word pairs is 198, whereas the algorithms in
(Fung and McKeown, 1994) and (Somers, 1998)
gave only 62 and 61 correct word pairs, respec-
tively. Hence the results are very useful for vo-
cabulary extraction as well. Similarly, Figure 2
and Figure 3 show significant improvements for
the other two corpora. At any score cut-off, the
modified approach gives better results than the al-
gorithms proposed in (Somers, 1998).
</bodyText>
<sectionHeader confidence="0.999297" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999987771428571">
This paper focuses on developing suitable word
alignment schemes in parallel texts where the size
of the corpus is not large. In languages, where
rich linguistic tools are yet to be developed, or
available freely, such an algorithm may prove to
be beneficial for various NLP activities, such as,
vocabulary extraction, alignment etc. This work
considers word alignment in English - Hindi par-
allel corpus, where the size of the corpus used is
about 18 thousand words for English and 20 thou-
sand words for Hindi.
The paucity of the resources suggests that sta-
tistical techniques are not suitable for the task.
On the other hand, Lexicon-based approaches are
highly resource-dependent. As a consequence,
they could not be considered as suitable schemes.
Recency vector based approaches provide a suit-
able alternative. Variations of this approach have
already been used for word alignment in parallel
texts involving European languages and Chinese,
Japanese. However, our initial experiments with
these algorithms on English-Hindi did not produce
good results. In order to improve their perfor-
mances certain measures have been taken. The
proposed algorithm improved the performance
manifold. This approach can be used for word
alignment in language pairs like English-Hindi.
Since the available corpus size is rather small
we could not compare the results obtained with
various other word alignment algorithms proposed
in the literature. In particular we like to compare
the proposed scheme with the famous IBM mod-
els. We hope that with a much larger corpus size
we shall be able to make the necessary compar-
isons in near future.
</bodyText>
<sectionHeader confidence="0.99853" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999019357142857">
L. Ahrenberg, M. Merkel, A. Sagvall Hein, and
J.Tiedemann. 2000. Evaluation of word alignment
systems. In Proc. 2nd International conference on
Linguistic resources and Evaluation (LREC-2000),
volume 3, pages 1255–1261, Athens, Greece.
P. Brown, S. A. Della Pietra, V. J. Della Pietra, , and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263–311.
K. W. Church Dagan, I. and W. A. Gale. 1993. Robust
bilingual word alignment for machine aided transla-
tion. In Proc. Workshop on Very Large Corpora:
Academic and Industrial Perspectives, pages 1–8,
Columbus, Ohio.
P. Fung and K. McKeown. 1994. Aligning noisy par-
allel corpora across language groups: Word pair fea-
ture matching by dynamic time warping. In Tech-
nology Partnerships for Crossing the Language Bar-
rier: Proc. First conference of the Association for
Machine Translation in the Americas, pages 81–88,
Columbia, Maryland.
W. A. Gale and K. W. Church. 1991. Identifying word
correspondences in parallel texts. In Proc. Fourth
DARPA Workshop on Speech and Natural Language,
pages 152–157. Morgan Kaufmann Publishers, Inc.
Jin-Xia Huang and Key-Sun Choi. 2000. Chinese ko-
rean word alignment based on linguistic compari-
son. In Proc. 38th annual meeting of the association
of computational linguistic, pages 392–399, Hong
Kong.
Ananthakrishnan Ramanathan and Durgesh D. Rao.
2003. A lightweight stemmer for hindi. In Proc.
Workshop of Computational Linguistics for South
Asian Languages -Expanding Synergies with Eu-
rope, EACL-2003, pages 42–48, Budapest, Hungary.
H Somers. 1998. Further experiments in bilingual text
alignment. International Journal of Corpus Linguis-
tics, 3:115–150.
J¨org Tiedemann. 2003. Combining clues word align-
ment. In Proc. 10th Conference of The European
Chapter of the Association for Computational Lin-
guistics, pages 339–346, Budapest, Hungary.
</reference>
<page confidence="0.997984">
654
</page>
<figure confidence="0.698688138888889">
Segment Constraint: 0-segment (Sentence Corpus)
Frequency a p c P% R% F-score F-score %
range (new) (old) increase
2-5 285 181 141 77.90 49.74 0.61 0.118 416.90
3-5 147 108 81 75.00 55.10 0.64 0.169 278.69
3-10 211 152 119 78.29 56.40 0.61 0.168 263.10
5-20 146 103 79 76.70 54.12 0.64 0.216 196.29
10-20 49 35 29 82.86 59.18 0.69 0.233 196.14
20-30 19 12 9 75.00 47.37 0.58 0.270 114.62
30-50 14 8 6 75.00 42.86 0.55 0.229 140.17
40-50 4 2 2 100.00 50.00 0.67 0.222 201.80
50-100 15 12 8 66.67 53.33 0.59 0.392 50.51
100-200 6 5 5 100.00 83.33 0.91 0.91 -
200-300 3 3 3 100.00 100.00 1.00 1.00 -
Segment Constraint: 1-segment (Story book Corpus)
2-5 281 184 89 48.37 31.67 0.38 0.039 874.35
3-5 143 108 52 48.15 36.36 0.41 0.042 876.19
5-10 125 89 35 39.39 28.00 0.33 0.090 266.67
10-20 75 50 25 50.00 33.33 0.40 0.115 247.83
10-30 117 76 39 51.32 33.33 0.41 0.114 259.65
20-30 32 23 11 47.83 34.38 0.37 0.041 802.43
30-40 14 8 2 25.00 14.29 0.18 0.100 80
40-50 7 7 2 28.57 28.57 0.29 0.200 45.00
50-100 11 10 2 20.00 18.18 0.19 0.110 72.72
100-200 5 5 2 40.00 40.00 0.40 0.444 -
Segment Constraint: 1-segment (Advertisement Corpus)
2-5 411 250 67 26.80 16.30 0.20 0.035 471.43
3-5 189 145 41 28.28 21.69 0.25 0.073 242.47
3-10 237 172 48 27.91 20.03 0.23 0.075 206.67
5-20 107 73 27 36.99 25.23 0.30 0.141 112.77
10-20 31 22 6 27.27 19.35 0.23 0.229 4.37
10-30 40 28 8 32.14 22.50 0.26 0.247 5.26
30-40 3 2 1 50.00 33.33 0.40 0.222 80.18
30-50 3 2 1 50.00 33.33 0.40 0.222 80.18
50-100 4 3 1 33.33 25.00 0.29 0.178 60.60
100-200 2 2 0 0 0 - 1.000 -
</figure>
<tableCaption confidence="0.978783">
Table 3: Comparison of experimental results with Segment Constraint on the three Engish-Hindi parallel
corpora
</tableCaption>
<table confidence="0.9335764">
Corpora English corpus Hindi corpus
Total words Distinct words Total words Distinct words
Storybook corpus 6609 895 7606 1100
Advertisement corpus 3795 1213 4057 1198
Sentence corpus 8540 1012 9159 1152
</table>
<tableCaption confidence="0.999512">
Table 4: Experimental root word parallel corpora of English -Hindi
</tableCaption>
<page confidence="0.983212">
655
</page>
<table confidence="0.8877838">
Different Corpora Actual alignment Alignment given Correct alignment R% P%
in text by system given by system
Advertisement corpus 323 358 253 78.32 70.68
Storybook corpus 609 546 476 78.16 87.18
Sentence corpus 4548 4548 4458 98.02 98.02
</table>
<tableCaption confidence="0.951738">
Table 5: Results of Church and Gale Algorithm for Sentence level Alignment
</tableCaption>
<figureCaption confidence="0.999814">
Figure 2: Alignment Results for Sentence Corpus
Figure 3: Alignment Results for Story Book Corpus
</figureCaption>
<page confidence="0.997036">
656
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.166112">
<title confidence="0.9995645">Word Alignment in English-Hindi Parallel Corpus Using Recency-Vector Approach: Some Studies</title>
<author confidence="0.999306">Niladri Chatterjee</author>
<affiliation confidence="0.857306666666667">Department of Mathematics Indian Institute of Technology Delhi Hauz Khas, New Delhi</affiliation>
<address confidence="0.937106">INDIA - 110016</address>
<email confidence="0.948924">niladriiitd@yahoo.com</email>
<author confidence="0.997515">Saumya Agrawal</author>
<affiliation confidence="0.999952">Department of Mathematics Indian Institute of Technology</affiliation>
<address confidence="0.9583995">Kharagpur, West Bengal INDIA - 721302</address>
<email confidence="0.357562">saumyaagrawal2000@yahoo.co.in</email>
<abstract confidence="0.997682565217391">Word alignment using recency-vector based approach has recently become popular. One major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small. This makes these algorithms worth-studying for languages where resources are scarce. In this work we studied the performance of two very popular recency-vector based approaches, proposed in (Fung and McKeown, 1994) and (Somers, 1998), respectively, for word alignment in English-Hindi parallel corpus. But performance of the above algorithms was not found to be satisfactory. However, subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique significantly for the said corpus. The present paper discusses the new version of the algorithm and its performance in detail.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Ahrenberg</author>
<author>M Merkel</author>
<author>A Sagvall Hein</author>
<author>J Tiedemann</author>
</authors>
<title>Evaluation of word alignment systems.</title>
<date>2000</date>
<booktitle>In Proc. 2nd International conference on Linguistic resources and Evaluation (LREC-2000),</booktitle>
<volume>3</volume>
<pages>1255--1261</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="1450" citStr="Ahrenberg et al., 2000" startWordPosition="208" endWordPosition="212">ers, 1998), respectively, for word alignment in English-Hindi parallel corpus. But performance of the above algorithms was not found to be satisfactory. However, subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique significantly for the said corpus. The present paper discusses the new version of the algorithm and its performance in detail. 1 Introduction Several approaches including statistical techniques (Gale and Church, 1991; Brown et al., 1993), lexical techniques (Huang and Choi, 2000; Tiedemann, 2003) and hybrid techniques (Ahrenberg et al., 2000), have been pursued to design schemes for word alignment which aims at establishing links between words of a source language and a target language in a parallel corpus. All these schemes rely heavily on rich linguistic resources, either in the form of huge data of parallel texts or various language/grammar related tools, such as parser, tagger, morphological analyser etc. Recency vector based approach has been proposed as an alternative strategy for word alignment. Approaches based on recency vectors typically consider the positions of the word in the corresponding texts rather than sentence b</context>
</contexts>
<marker>Ahrenberg, Merkel, Hein, Tiedemann, 2000</marker>
<rawString>L. Ahrenberg, M. Merkel, A. Sagvall Hein, and J.Tiedemann. 2000. Evaluation of word alignment systems. In Proc. 2nd International conference on Linguistic resources and Evaluation (LREC-2000), volume 3, pages 1255–1261, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1343" citStr="Brown et al., 1993" startWordPosition="193" endWordPosition="196">ance of two very popular recency-vector based approaches, proposed in (Fung and McKeown, 1994) and (Somers, 1998), respectively, for word alignment in English-Hindi parallel corpus. But performance of the above algorithms was not found to be satisfactory. However, subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique significantly for the said corpus. The present paper discusses the new version of the algorithm and its performance in detail. 1 Introduction Several approaches including statistical techniques (Gale and Church, 1991; Brown et al., 1993), lexical techniques (Huang and Choi, 2000; Tiedemann, 2003) and hybrid techniques (Ahrenberg et al., 2000), have been pursued to design schemes for word alignment which aims at establishing links between words of a source language and a target language in a parallel corpus. All these schemes rely heavily on rich linguistic resources, either in the form of huge data of parallel texts or various language/grammar related tools, such as parser, tagger, morphological analyser etc. Recency vector based approach has been proposed as an alternative strategy for word alignment. Approaches based on rec</context>
<context position="15952" citStr="Brown et al., 1993" startWordPosition="2672" endWordPosition="2675">e varies with the nature of text. Hence no value for range may be identified that applies uniformly on different corpora. Also for noisy corpora the segment constraint is expected to yield better results as the search here is localised better. The proposed segment constraint expresses range in terms of segments. In order to impose this constraint, first the parallel texts are aligned at sentence level. The search for a target language word is then restricted to few segments above and below the current one. Use of sententially aligned corpora for word alignment has already been recommended in (Brown et al., 1993). However, the requirement there is quite stringent – all the sentences are to be correctly aligned. The segment constraint proposed herein works well even if the text alignment is not perfect. Use of roughly aligned corpora has also been proposed in (Dagan and Gale, 1993) for word alignment in bilingual corpora, where statistical techniques have been used as the underlying alignment scheme. In this work, the sentence level alignment algorithm given in (Gale and Church, 1991) has been used for applying segment constraint. As shown in Table 5, the alignment obtained using this algorithm is not </context>
</contexts>
<marker>Brown, Pietra, Pietra, 1993</marker>
<rawString>P. Brown, S. A. Della Pietra, V. J. Della Pietra, , and R. L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church Dagan</author>
<author>I</author>
<author>W A Gale</author>
</authors>
<title>Robust bilingual word alignment for machine aided translation.</title>
<date>1993</date>
<booktitle>In Proc. Workshop on Very Large Corpora: Academic and Industrial Perspectives,</booktitle>
<pages>1--8</pages>
<location>Columbus, Ohio.</location>
<marker>Dagan, I, Gale, 1993</marker>
<rawString>K. W. Church Dagan, I. and W. A. Gale. 1993. Robust bilingual word alignment for machine aided translation. In Proc. Workshop on Very Large Corpora: Academic and Industrial Perspectives, pages 1–8, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>K McKeown</author>
</authors>
<title>Aligning noisy parallel corpora across language groups: Word pair feature matching by dynamic time warping.</title>
<date>1994</date>
<booktitle>In Technology Partnerships for Crossing the Language Barrier: Proc. First conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>81--88</pages>
<location>Columbia, Maryland.</location>
<contexts>
<context position="818" citStr="Fung and McKeown, 1994" startWordPosition="112" endWordPosition="115">INDIA - 110016 niladri iitd@yahoo.com Saumya Agrawal Department of Mathematics Indian Institute of Technology Kharagpur, West Bengal INDIA - 721302 saumya agrawal2000@yahoo.co.in Abstract Word alignment using recency-vector based approach has recently become popular. One major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small. This makes these algorithms worth-studying for languages where resources are scarce. In this work we studied the performance of two very popular recency-vector based approaches, proposed in (Fung and McKeown, 1994) and (Somers, 1998), respectively, for word alignment in English-Hindi parallel corpus. But performance of the above algorithms was not found to be satisfactory. However, subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique significantly for the said corpus. The present paper discusses the new version of the algorithm and its performance in detail. 1 Introduction Several approaches including statistical techniques (Gale and Church, 1991; Brown et al., 1993), lexical techniques (Huang and Choi, 2000; Tiedemann, 2003) and hybrid tec</context>
<context position="2129" citStr="Fung and McKeown, 1994" startWordPosition="321" endWordPosition="324"> which aims at establishing links between words of a source language and a target language in a parallel corpus. All these schemes rely heavily on rich linguistic resources, either in the form of huge data of parallel texts or various language/grammar related tools, such as parser, tagger, morphological analyser etc. Recency vector based approach has been proposed as an alternative strategy for word alignment. Approaches based on recency vectors typically consider the positions of the word in the corresponding texts rather than sentence boundaries. Two algorithms of this type can be found in (Fung and McKeown, 1994) and (Somers, 1998). The algorithms first compute the position vector V„ for the word w in the text. Typically, V� is of the form (p1p2 ... pk), where the pis indicate the positions of the word w in a text T. A new vector R,,,, called the recency vector, is computed using the position vector V�, and is defined as (p2 −p1, p3 −p2, . . . , pk −pk−1). In order to compute the alignment of a given word in the source language text, the recency vector of the word is compared with the recency vector of each target language word and the similarity between them is measured by computing a matching cost a</context>
<context position="4047" citStr="Fung and McKeown, 1994" startWordPosition="638" endWordPosition="641">texts. The corpus used for this work is described in Table 1. It has been made manually from three different sources: children’s storybooks, English to Hindi translation book material, and advertisements. We shall call 649 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 649–656, Sydney, July 2006. c�2006 Association for Computational Linguistics the three corpora as Storybook corpus, Sentence corpus and Advertisement corpus, respectively. 2 Word Alignment Algorithm: Recency Vector Based Approach DK-vec algorithm given in (Fung and McKeown, 1994) uses the following dynamic programming based approach to compute the matching cost C(n, m) of two vectors v1 and v2 of lengths n and m, respectively. The cost is calculated recursively using the following formula, C(i,j) = |(v1(i) − v2(j) |+ min{C(i − 1,j), C(i − 1,j − 1),C(i,j − 1)} where i and j have values from 2 to n and 2 to m respectively, n and m being the number of distinct words in source and target language corpus respectively. Note that vl(k) denotes the kth entry of the vector vl, for l = 1 and 2. The costs are initialised as follows. C(1, 1) = |v1(1) − v2(1)|; C(i, 1) = |v1(i) − </context>
<context position="5596" citStr="Fung and McKeown, 1994" startWordPosition="914" endWordPosition="917">e above scheme has been proposed in (Somers, 1998) which has a much lower computational complexity O(min(m, n)). In this new scheme, a distance called Levenshtein distance(S) is successively measured using: S = S + min{|v1(i + 1) − v2(j)|, |v1(i+1)−v2(j+1)|, |v1(i)−v2(j+1)|} The word in the target text having the minimum value of S (Levenshtein difference) is considered to be the translation of the word in the source text. 2.1 Constraints Used in the Dynamic Programming Algorithms In order to reduce the complexity of the dynamic programming algorithm certain constraints have been proposed in (Fung and McKeown, 1994). 1. Starting Point Constraint: The constraint imposed is: |first-occurrence of source language word (w1) - first-occurrence of target language word w2 |&lt; 12∗(length of the text). 2. Euclidean distance constraint: The constraint imposed is: �/(m1 − m2)2 + (s1 − s2)2 &lt; T, where mj and sj are the mean and standard deviation, respectively, of the recency vector of wj, j = 1 or 2. Here, T is some predefined threshold: 3. Length Constraint: The constraint imposed is: 21 ∗ f2 &lt; f1 &lt; 2 ∗ f2, where f1 and f2 are the frequencies of occurrence of w1 and w2, in their respective texts. 2.2 Experiments wit</context>
<context position="20202" citStr="Fung and McKeown, 1994" startWordPosition="3391" endWordPosition="3394">the results at low scores were better as shown in Figure 1. The word pairs obtained after applying the modified approach can be used as anchor points for further alignment as well as for vocabulary extraction. In case of the Sentence corpus, best result for anchor points for further alignment lies at the score cut off 1000 where precision and recall are 86.88% and 80.35% respectively. Hence F-score is 0.835 which is very high as compared to 0.173 obtained by Somers’ approach and indicates an improvement of 382.65%. Also, here the number of correct word pairs is 198, whereas the algorithms in (Fung and McKeown, 1994) and (Somers, 1998) gave only 62 and 61 correct word pairs, respectively. Hence the results are very useful for vocabulary extraction as well. Similarly, Figure 2 and Figure 3 show significant improvements for the other two corpora. At any score cut-off, the modified approach gives better results than the algorithms proposed in (Somers, 1998). 4 Conclusion This paper focuses on developing suitable word alignment schemes in parallel texts where the size of the corpus is not large. In languages, where rich linguistic tools are yet to be developed, or available freely, such an algorithm may prove</context>
</contexts>
<marker>Fung, McKeown, 1994</marker>
<rawString>P. Fung and K. McKeown. 1994. Aligning noisy parallel corpora across language groups: Word pair feature matching by dynamic time warping. In Technology Partnerships for Crossing the Language Barrier: Proc. First conference of the Association for Machine Translation in the Americas, pages 81–88, Columbia, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
</authors>
<title>Identifying word correspondences in parallel texts.</title>
<date>1991</date>
<booktitle>In Proc. Fourth DARPA Workshop on Speech and Natural Language,</booktitle>
<pages>152--157</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<contexts>
<context position="1322" citStr="Gale and Church, 1991" startWordPosition="189" endWordPosition="192"> we studied the performance of two very popular recency-vector based approaches, proposed in (Fung and McKeown, 1994) and (Somers, 1998), respectively, for word alignment in English-Hindi parallel corpus. But performance of the above algorithms was not found to be satisfactory. However, subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique significantly for the said corpus. The present paper discusses the new version of the algorithm and its performance in detail. 1 Introduction Several approaches including statistical techniques (Gale and Church, 1991; Brown et al., 1993), lexical techniques (Huang and Choi, 2000; Tiedemann, 2003) and hybrid techniques (Ahrenberg et al., 2000), have been pursued to design schemes for word alignment which aims at establishing links between words of a source language and a target language in a parallel corpus. All these schemes rely heavily on rich linguistic resources, either in the form of huge data of parallel texts or various language/grammar related tools, such as parser, tagger, morphological analyser etc. Recency vector based approach has been proposed as an alternative strategy for word alignment. Ap</context>
<context position="16432" citStr="Gale and Church, 1991" startWordPosition="2750" endWordPosition="2753">s above and below the current one. Use of sententially aligned corpora for word alignment has already been recommended in (Brown et al., 1993). However, the requirement there is quite stringent – all the sentences are to be correctly aligned. The segment constraint proposed herein works well even if the text alignment is not perfect. Use of roughly aligned corpora has also been proposed in (Dagan and Gale, 1993) for word alignment in bilingual corpora, where statistical techniques have been used as the underlying alignment scheme. In this work, the sentence level alignment algorithm given in (Gale and Church, 1991) has been used for applying segment constraint. As shown in Table 5, the alignment obtained using this algorithm is not very good (only 70% precision for Storybook corpus). The three aligned root word corpora are then subjected to segment constraint in our experiments. Next important decision we need to take which dynamic programming algorithm should be used. Results shown in Section 2.2 and 2.3 demonstrate that the performance of DK-vec algorithm and Somers’ algorithm are almost at par. Hence keeping in view the improved computational complexity, we choose to use Levenshtein distance as used </context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>W. A. Gale and K. W. Church. 1991. Identifying word correspondences in parallel texts. In Proc. Fourth DARPA Workshop on Speech and Natural Language, pages 152–157. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Xia Huang</author>
<author>Key-Sun Choi</author>
</authors>
<title>Chinese korean word alignment based on linguistic comparison.</title>
<date>2000</date>
<booktitle>In Proc. 38th annual meeting of the association of computational linguistic,</booktitle>
<pages>392--399</pages>
<location>Hong Kong.</location>
<contexts>
<context position="1385" citStr="Huang and Choi, 2000" startWordPosition="199" endWordPosition="202">ased approaches, proposed in (Fung and McKeown, 1994) and (Somers, 1998), respectively, for word alignment in English-Hindi parallel corpus. But performance of the above algorithms was not found to be satisfactory. However, subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique significantly for the said corpus. The present paper discusses the new version of the algorithm and its performance in detail. 1 Introduction Several approaches including statistical techniques (Gale and Church, 1991; Brown et al., 1993), lexical techniques (Huang and Choi, 2000; Tiedemann, 2003) and hybrid techniques (Ahrenberg et al., 2000), have been pursued to design schemes for word alignment which aims at establishing links between words of a source language and a target language in a parallel corpus. All these schemes rely heavily on rich linguistic resources, either in the form of huge data of parallel texts or various language/grammar related tools, such as parser, tagger, morphological analyser etc. Recency vector based approach has been proposed as an alternative strategy for word alignment. Approaches based on recency vectors typically consider the positi</context>
</contexts>
<marker>Huang, Choi, 2000</marker>
<rawString>Jin-Xia Huang and Key-Sun Choi. 2000. Chinese korean word alignment based on linguistic comparison. In Proc. 38th annual meeting of the association of computational linguistic, pages 392–399, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananthakrishnan Ramanathan</author>
<author>Durgesh D Rao</author>
</authors>
<title>A lightweight stemmer for hindi.</title>
<date>2003</date>
<booktitle>In Proc. Workshop of Computational Linguistics for South Asian Languages -Expanding Synergies with Europe, EACL-2003,</booktitle>
<pages>42--48</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="14131" citStr="Ramanathan and Rao, 2003" startWordPosition="2357" endWordPosition="2360"> consequence, many English words are wrongly aligned. In the following section we describe certain measures that we propose for improving the efficiency of the recency vector based algorithms for word alignment in English - Hindi parallel texts. 3 Improvements in Word Alignment In order to take care of morphological variations, we propose to use root words instead of various declensions of the word. For the present work this has been done manually for Hindi. However, algorithms similar to Porter’s algorithm may be developed for Hindi too for cleaning a Hindi text of morphological inflections (Ramanathan and Rao, 2003). The modified text, for both English and Hindi, are then subjected to word alignment. Table 4 gives the details about the root word corpus used to improve the result of word alignment. Here the total number of words for the three types of corpora is greater than the total number of words in the original corpus (Table 1). This is because of the presence of words like “I’ll” in the English corpus which have been taken as “I shall” in the root word corpus. Also words like Unkaa have been taken as Un kaa in the Hindi root word corpus, leading to an increase in the corpus size. 652 Since we observ</context>
</contexts>
<marker>Ramanathan, Rao, 2003</marker>
<rawString>Ananthakrishnan Ramanathan and Durgesh D. Rao. 2003. A lightweight stemmer for hindi. In Proc. Workshop of Computational Linguistics for South Asian Languages -Expanding Synergies with Europe, EACL-2003, pages 42–48, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Somers</author>
</authors>
<title>Further experiments in bilingual text alignment.</title>
<date>1998</date>
<journal>International Journal of Corpus Linguistics,</journal>
<pages>3--115</pages>
<contexts>
<context position="837" citStr="Somers, 1998" startWordPosition="117" endWordPosition="118">ahoo.com Saumya Agrawal Department of Mathematics Indian Institute of Technology Kharagpur, West Bengal INDIA - 721302 saumya agrawal2000@yahoo.co.in Abstract Word alignment using recency-vector based approach has recently become popular. One major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small. This makes these algorithms worth-studying for languages where resources are scarce. In this work we studied the performance of two very popular recency-vector based approaches, proposed in (Fung and McKeown, 1994) and (Somers, 1998), respectively, for word alignment in English-Hindi parallel corpus. But performance of the above algorithms was not found to be satisfactory. However, subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique significantly for the said corpus. The present paper discusses the new version of the algorithm and its performance in detail. 1 Introduction Several approaches including statistical techniques (Gale and Church, 1991; Brown et al., 1993), lexical techniques (Huang and Choi, 2000; Tiedemann, 2003) and hybrid techniques (Ahrenberg </context>
<context position="2148" citStr="Somers, 1998" startWordPosition="326" endWordPosition="327">inks between words of a source language and a target language in a parallel corpus. All these schemes rely heavily on rich linguistic resources, either in the form of huge data of parallel texts or various language/grammar related tools, such as parser, tagger, morphological analyser etc. Recency vector based approach has been proposed as an alternative strategy for word alignment. Approaches based on recency vectors typically consider the positions of the word in the corresponding texts rather than sentence boundaries. Two algorithms of this type can be found in (Fung and McKeown, 1994) and (Somers, 1998). The algorithms first compute the position vector V„ for the word w in the text. Typically, V� is of the form (p1p2 ... pk), where the pis indicate the positions of the word w in a text T. A new vector R,,,, called the recency vector, is computed using the position vector V�, and is defined as (p2 −p1, p3 −p2, . . . , pk −pk−1). In order to compute the alignment of a given word in the source language text, the recency vector of the word is compared with the recency vector of each target language word and the similarity between them is measured by computing a matching cost associated with the </context>
<context position="5023" citStr="Somers, 1998" startWordPosition="826" endWordPosition="827">the number of distinct words in source and target language corpus respectively. Note that vl(k) denotes the kth entry of the vector vl, for l = 1 and 2. The costs are initialised as follows. C(1, 1) = |v1(1) − v2(1)|; C(i, 1) = |v1(i) − v2(1) |+ C(i − 1,1); C(1,j) = |v1(1) − v2(j) |+ C(1,j − 1); The word in the target language that has the minimum normalized cost (C(n, m)/(n + m)) is taken as the translation of the word considered in the source text. One major shortcoming of the above scheme is its high computational complexity i.e. O(mn). A variation of the above scheme has been proposed in (Somers, 1998) which has a much lower computational complexity O(min(m, n)). In this new scheme, a distance called Levenshtein distance(S) is successively measured using: S = S + min{|v1(i + 1) − v2(j)|, |v1(i+1)−v2(j+1)|, |v1(i)−v2(j+1)|} The word in the target text having the minimum value of S (Levenshtein difference) is considered to be the translation of the word in the source text. 2.1 Constraints Used in the Dynamic Programming Algorithms In order to reduce the complexity of the dynamic programming algorithm certain constraints have been proposed in (Fung and McKeown, 1994). 1. Starting Point Constra</context>
<context position="6572" citStr="Somers, 1998" startWordPosition="1087" endWordPosition="1088"> wj, j = 1 or 2. Here, T is some predefined threshold: 3. Length Constraint: The constraint imposed is: 21 ∗ f2 &lt; f1 &lt; 2 ∗ f2, where f1 and f2 are the frequencies of occurrence of w1 and w2, in their respective texts. 2.2 Experiments with DK-vec Algorithm The results of the application of this algorithm have been very poor when applied on the three English to Hindi parallel corpora mentioned above without imposing any constraints. We then experimented by varying the values of the parameters in the constraints in order to observe their effects on the accuracy of alignment. As was suggested in (Somers, 1998), we also observed that the Euclidean distance constraint is not very beneficial when the corpus size is small. So this constraint has not been considered in our subsequent experiments. Starting point constraint imposes a range within which the search for the matching word is restricted. Although Fung and McKeown suggested the range to be half of the length of the text, we felt that the optimum value of this range will vary from text to text depending on the type of corpus, length ratio of the two texts etc. Table 2 shows the results obtained on applying the DK vec algorithm on Sentence corpus</context>
<context position="18626" citStr="Somers, 1998" startWordPosition="3122" endWordPosition="3123"> Storybook and Advertisement corpora we found after experimenting that the best results were obtained when i was 1. During the experiments it was observed that as the number of segments was lowered or increased from the optimum segment the accuracy of alignment decreased continuously by around 10% for low frequency ranges for the three corpora and remained almost same for high frequency ranges. Table 3 shows the results obtained when segment constraint is applied on the three corpora at optimum segment range for various frequency ranges. A comparison between the F-score given by algorithm in (Somers, 1998) (the column Fscore old in the table) and the F-score obtained by applying the improved scheme (the column Fscore new in the table) indicate that the results have improved significantly for low frequency ranges. It is observed that the accuracy of alignment for almost 95% of the available words has increased significantly. This accounts for words within low frequency range of 2–40 for Sentence corpus, 2– 30 for Storybook corpus, and 2–20 for Advertisement corpus. Also, most of the correct word pairs given by the modified approach are verbs, adjectives or nouns. Also it was observed that as the</context>
<context position="20221" citStr="Somers, 1998" startWordPosition="3396" endWordPosition="3397">e better as shown in Figure 1. The word pairs obtained after applying the modified approach can be used as anchor points for further alignment as well as for vocabulary extraction. In case of the Sentence corpus, best result for anchor points for further alignment lies at the score cut off 1000 where precision and recall are 86.88% and 80.35% respectively. Hence F-score is 0.835 which is very high as compared to 0.173 obtained by Somers’ approach and indicates an improvement of 382.65%. Also, here the number of correct word pairs is 198, whereas the algorithms in (Fung and McKeown, 1994) and (Somers, 1998) gave only 62 and 61 correct word pairs, respectively. Hence the results are very useful for vocabulary extraction as well. Similarly, Figure 2 and Figure 3 show significant improvements for the other two corpora. At any score cut-off, the modified approach gives better results than the algorithms proposed in (Somers, 1998). 4 Conclusion This paper focuses on developing suitable word alignment schemes in parallel texts where the size of the corpus is not large. In languages, where rich linguistic tools are yet to be developed, or available freely, such an algorithm may prove to be beneficial f</context>
</contexts>
<marker>Somers, 1998</marker>
<rawString>H Somers. 1998. Further experiments in bilingual text alignment. International Journal of Corpus Linguistics, 3:115–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Combining clues word alignment.</title>
<date>2003</date>
<booktitle>In Proc. 10th Conference of The European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>339--346</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="1403" citStr="Tiedemann, 2003" startWordPosition="203" endWordPosition="204">sed in (Fung and McKeown, 1994) and (Somers, 1998), respectively, for word alignment in English-Hindi parallel corpus. But performance of the above algorithms was not found to be satisfactory. However, subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique significantly for the said corpus. The present paper discusses the new version of the algorithm and its performance in detail. 1 Introduction Several approaches including statistical techniques (Gale and Church, 1991; Brown et al., 1993), lexical techniques (Huang and Choi, 2000; Tiedemann, 2003) and hybrid techniques (Ahrenberg et al., 2000), have been pursued to design schemes for word alignment which aims at establishing links between words of a source language and a target language in a parallel corpus. All these schemes rely heavily on rich linguistic resources, either in the form of huge data of parallel texts or various language/grammar related tools, such as parser, tagger, morphological analyser etc. Recency vector based approach has been proposed as an alternative strategy for word alignment. Approaches based on recency vectors typically consider the positions of the word in</context>
</contexts>
<marker>Tiedemann, 2003</marker>
<rawString>J¨org Tiedemann. 2003. Combining clues word alignment. In Proc. 10th Conference of The European Chapter of the Association for Computational Linguistics, pages 339–346, Budapest, Hungary.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>