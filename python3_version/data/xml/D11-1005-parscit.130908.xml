<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.999204">
Unsupervised Structure Prediction
with Non-Parallel Multilingual Guidance
</title>
<author confidence="0.988516">
Shay B. Cohen Dipanjan Das Noah A. Smith
</author>
<affiliation confidence="0.9160175">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998882">
{scohen,dipanjan,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999801769230769">
We describe a method for prediction of lin-
guistic structure in a language for which only
unlabeled data is available, using annotated
data from a set of one or more helper lan-
guages. Our approach is based on a model
that locally mixes between supervised mod-
els from the helper languages. Parallel data
is not used, allowing the technique to be ap-
plied even in domains where human-translated
texts are unavailable. We obtain state-of-the-
art performance for two tasks of structure pre-
diction: unsupervised part-of-speech tagging
and unsupervised dependency parsing.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995377306122449">
A major focus of recent NLP research has involved
unsupervised learning of structure such as POS
tag sequences and parse trees (Klein and Manning,
2004; Johnson et al., 2007; Berg-Kirkpatrick et al.,
2010; Cohen and Smith, 2010, inter alia). In its
purest form, such research has improved our un-
derstanding of unsupervised learning practically and
formally, and has led to a wide range of new algo-
rithmic ideas. Another strain of research has sought
to exploit resources and tools in some languages (es-
pecially English) to construct similar resources and
tools for other languages, through heuristic “projec-
tion” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005)
or constraints in learning (Burkett and Klein, 2008;
Smith and Eisner, 2009; Das and Petrov, 2011; Mc-
Donald et al., 2011) or inference (Smith and Smith,
2004). Joint unsupervised learning (Snyder and
Barzilay, 2008; Naseem et al., 2009; Snyder et al.,
50
2009) is yet another research direction that seeks to
learn models for many languages at once, exploiting
linguistic universals and language similarity. The
driving force behind all of this work has been the
hope of building NLP tools for languages that lack
annotated resources.1
In this paper, we present an approach to using
annotated data from one or more languages (helper
languages) to learn models for another language that
lacks annotated data (the target language). Unlike
the previous work mentioned above, our framework
does not rely on parallel data in any form. This is
advantageous because parallel text exists only in a
few text domains (e.g., religious texts, parliamentary
proceedings, and news).
We focus on generative probabilistic models pa-
rameterized by multinomial distributions. We be-
gin with supervised maximum likelihood estimates
for models of the helper languages. In the second
stage, we learn a model for the target language using
unannotated data, maximizing likelihood over inter-
polations of the helper language models’ distribu-
tions. The tying is performed at the parameter level,
through coarse, nearly-universal syntactic categories
(POS tags). The resulting model is then used to ini-
tialize learning of the target language’s model using
standard unsupervised parameter estimation.
Some previous multilingual research, such as
Bayesian parameter tying across languages (Co-
hen and Smith, 2009) or models of parameter
</bodyText>
<footnote confidence="0.9944885">
1Although the stated objective is often to build systems for
resource-poor languages and domains, for evaluation purposes,
annotated treebank test data figure prominently in this research
(including in this paper).
</footnote>
<note confidence="0.960809">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50–61,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999456363636364">
drift down phylogenetic trees (Berg-Kirkpatrick and
Klein, 2010) is comparable, but the practical as-
sumption of supervised helper languages is new to
this work. Naseem et al. (2010) used universal
syntactic categories and rules to improve grammar
induction, but their model required expert hand-
written rules as constraints.
Herein, we specifically focus on two problems
in linguistic structure prediction: unsupervised POS
tagging and unsupervised dependency grammar in-
duction. Our experiments demonstrate that the pre-
sented method outperforms strong state-of-the-art
unsupervised baselines for both tasks. Our approach
can be applied to other problems in which a sub-
set of the model parameters can be linked across
languages. We also experiment with unsupervised
learning of dependency structures from words, by
combining our tagger and parser. Our results show
that combining our tagger and parser with joint
inference outperforms pipeline inference, and, in
several cases, even outperforms models built using
gold-standard part-of-speech tags.
</bodyText>
<sectionHeader confidence="0.998315" genericHeader="introduction">
2 Overview
</sectionHeader>
<bodyText confidence="0.99955075">
For each language E, we assume the presence of a
set of fine-grained POS tags F`, used to annotate the
language’s treebank. Furthermore, we assume that
there is a set of universal, coarse-grained POS tags
C such that, for every language E, there is a determin-
istic mapping from fine-grained to coarse-grained
tags, A` : F` → C. Our approach can be summa-
rized using the following steps for a given task:
</bodyText>
<listItem confidence="0.95976625">
1. Select a set of L helper languages for which there
exists annotated data (D1, ... , DL). Here, we use
treebanks in these languages.
2. For all E E {1, ... , L}, convert the examples in
</listItem>
<bodyText confidence="0.997754777777778">
D` by applying A` to every POS tag in the data,
resulting in ˜D`. Estimate the parameters of a
probabilistic model using ˜D`. In this work, such
models are generative probabilistic models based
on multinomial distributions,2 including an HMM
and the dependency model with valence (DMV)
of Klein and Manning (2004). Denote the subset
of parameters that are unlexicalized by 0(`). (Lex-
icalized parameters will be denoted 77(`).)
</bodyText>
<footnote confidence="0.825873">
2In §4 we also consider a feature-based parametrization.
</footnote>
<listItem confidence="0.987357">
3. For the target language, define the set of valid un-
lexicalized parameters
</listItem>
<equation confidence="0.979067">
( )
L L
O = 0 ~~0k =XO`,k0V) O`,k = 1,0 ≥ 0 ,
`=1 `=1
(1)
</equation>
<bodyText confidence="0.986896428571429">
for each group of parameters k, and maximize
likelihood over that set, using the target-language
unannotated data U. Because the syntactic cate-
gories referenced by each 0(`) and all models in O
are in C, the models will be in the same parametric
family. (Figure 1 gives a graphical interpretation
of O.) Let the resulting model be 0.
</bodyText>
<listItem confidence="0.983828888888889">
4. Transform 0 by expanding the coarse-grained
syntactic categories into the target language’s
fine-grained categories. Use the resulting model
to initialize parameter estimation, this time over
fine-grained tags, again using the unannotated
target-language data U. Initialize lexicalized pa-
rameters 77 for the target language using standard
methods (e.g., uniform initialization with random
symmetry breaking).
</listItem>
<bodyText confidence="0.999974916666667">
The main idea in the approach is to estimate a
certain model family for one language, while using
supervised models from other languages. The link
between the languages is achieved through coarse-
grained categories, which are now now common-
place (and arguably central to any theory of natural
language syntax). A key novel contribution is the
use of helper languages for initialization, and of un-
supervised learning to learn the contribution of each
helper language to that initialization (step 3). Addi-
tional treatment is required in expanding the coarse-
grained model to the fine-grained one (step 4).
</bodyText>
<sectionHeader confidence="0.961848" genericHeader="method">
3 Interpolated Multilingual Probabilistic
Context-Free Grammars
</sectionHeader>
<bodyText confidence="0.999794714285714">
Our focus in this paper is on models that consist
of multinomial distributions that have relationships
between them through a generative process such as
a probabilistic context-free grammar (PCFG). More
specifically, we assume that we have a model defin-
ing a probability distribution over observed surface
forms x and derivations y parametrized by 0:
</bodyText>
<page confidence="0.993196">
51
</page>
<figure confidence="0.999347666666667">
(0,1,0)
Italian
German
English
Czech
(0,0,1) (1,0,0)
</figure>
<figureCaption confidence="0.9789416">
Figure 1: A simple case of interpolation within the 3-
event probability simplex. The shaded area corresponds
to a convex hull inside the probability simplex, indicating
a mixture of the parameters of the four languages shown
in the figure.
</figureCaption>
<equation confidence="0.9998128">
p(�, y  |9) = K YNk k z0ki(x,Y)
Y i=1 θ0
k=1 (2)
= exp XK XNk fk,i(�, y) log θk,i (3)
k=1 i=1
</equation>
<bodyText confidence="0.94023">
where fk,i is a function that “counts” the number
of times the kth distribution’s ith event occurs in
the derivation. The parameters 9 are a collection
of K multinomials h91, ... , 9Ki, the kth of which
includes Nk events. Letting 9k = hθk,1, . . . , θk,Nki,
each θk,i is a probability, such that ∀k, ∀i, θk,i ≥ 0
and ∀k, PNk
</bodyText>
<equation confidence="0.804747">
i=1 θk,i = 1.
</equation>
<subsectionHeader confidence="0.944883">
3.1 Multilingual Interpolation
</subsectionHeader>
<bodyText confidence="0.996980555555555">
Our framework places additional, temporary con-
straints on the parameters 9. More specifically, we
assume that we have L existing, parameter estimates
for the multinomial families from Eq. 3. Each such
estimate 9(`), for 1 ≤ ` ≤ L, corresponds to a the
maximum likelihood estimate based on annotated
data for the `th helper language. Then, to create a
model for new language, we define a new set of pa-
rameters 9 as:
</bodyText>
<equation confidence="0.99133">
θk,i = XL β`,kθ(`)
`=1 k,i, (4)
</equation>
<bodyText confidence="0.99991775">
where ,3 is the set of coefficients that we will now
be interested in estimating (instead of directly esti-
mating 9). Note that for each k, PL`=1 β`,k = 1 and
β`,k ≥ 0.
</bodyText>
<subsectionHeader confidence="0.99925">
3.2 Grammatical Interpretation
</subsectionHeader>
<bodyText confidence="0.999613434782609">
We now give an interpretation of our approach relat-
ing it to PCFGs. We assume familiarity with PCFGs.
For a PCFG hG, 9i we denote the set of nontermi-
nal symbols by N, the set of terminal symbols by
E, and the set of rewrite rules for each nonterminal
A ∈ N by R(A). Each r ∈ R(A) has the form
A → α where α ∈ (N ∪ E)∗. In addition, there is
a probability attached to each rule θA→α such that
∀A ∈ N, Pα:(A→α)∈R(A) θA→α = 1. A PCFG can
be framed as a model using Eq. 3, where 9 corre-
spond to K = |N |multinomial distributions, where
each distribution attaches probabilities to rules with
a specific left hand symbol.
We assume that the model we are trying to
estimate (over coarse part-of-speech tags) can be
framed as a PCFG hG, 9i. This is indeed the case
for part-of-speech tagging and dependency grammar
induction we experiment with in §6. In that case,
our approach can be framed for PCFGs as follow-
ing. We assume that there exists L set of parameters
for this PCFG 9(1), ... , 9(L), each corresponding to
a helper language. We then create a new PCFG G0
with parameters 90 and ,3 as follows:
</bodyText>
<listItem confidence="0.995130777777778">
1. G0 contains all nonterminal and terminal symbols
in G, and none of the rules in G.
2. For each nonterminal A in G, we create a new
nonterminal aA,` for ` ∈ {1, ... , L}.
3. For each nonterminal A in G, we create rules
A → aA,` for ` ∈ {1, ... , L} which have proba-
bilities βA→aA,`.
4. For each rule A → α in G, we add to G0 the rule
aA,` → α with
</listItem>
<equation confidence="0.9856705">
θ0 aA,`→α = θ(`)
A→α. (5)
</equation>
<bodyText confidence="0.99888125">
where θ(`)A→α is the probability associated with
rule A → α in the `th helper language.
At each point, the derivational process of this
PCFG uses the nonterminal’s specific ,3 coefficients
</bodyText>
<page confidence="0.990798">
52
</page>
<bodyText confidence="0.999986242424242">
to choose one of the helper languages. It then se-
lects a rule according to the multinomial from that
language. This step is repeated until a whole deriva-
tion is generated.
This PCFG representation of the approach in §3
points to a possible generalization. Instead of using
an identical CFG backbone for each language, we
can use a set of PCFGs, (G(�), θ(�)) with an iden-
tical nonterminal set and alphabet, and repeat the
same construction as above, replacing step 4 with
the addition of rules of the form aA,t -+ α for each
rule A -+ α in G(�). Such a construction allows
more syntactic variability in the language we are try-
ing to estimate, originating in the syntax of the var-
ious helper languages. In this paper, we do not use
this generalization, and always use the same PCFG
backbone for all languages.
Note that the interpolated model can still be un-
derstood in terms of the exponential model of Eq. 3.
For a given collection of multinomials and base
models of the form of Eq. 3, we can analogously
define a new log-linear model over a set of ex-
tended derivations. These derivations will now in-
clude L x K features of the form gt,k(x, y), cor-
responding to a count of the event of choosing the
Eth mixture component for multinomial k. In addi-
tion, the feature set fk,i(x, y) will be extended to
a feature set of the form ft,k,i(x, y), analogous to
step 4 in constructed PCFG above. The model pa-
rameterized according to Eq. 4 can be recovered by
marginalizing out the “g” features. We will refer to
the model with these new set of features as “the ex-
tended model.”
</bodyText>
<sectionHeader confidence="0.997379" genericHeader="method">
4 Inference and Parameter Estimation
</sectionHeader>
<bodyText confidence="0.999728642857143">
The main building block commonly required for un-
supervised learning in NLP is that of computing fea-
ture expectations for a given model. These feature
expectations can be used with an algorithm such as
expectation-maximization (where the expectations
are normalized to obtain a new set of multinomial
weights) or with other gradient based log-likelihood
optimization algorithms such as L-BFGS (Liu and
Nocedal, 1989) for feature-rich models.
Estimating Multinomial Distributions Given a
surface form x, a multinomial k and an event i in the
multinomial, “feature expectation” refers to the cal-
culation of the following quantities (in the extended
model):
</bodyText>
<equation confidence="0.9927865">
E[f�,k,i(x, y)] = Ey p(x, y  |θ)f�,k,i(x, y) (6)
E[gt,k(x, y)] = Ey p(x, y  |θ)gt,k(x, y) (7)
</equation>
<bodyText confidence="0.984765068965517">
These feature expectations can usually be computed
using algorithms such as the forward-backward al-
gorithm for hidden Markov models, or more gener-
ally, the inside-outside algorithm for PCFGs. In this
paper, however, the task of estimation is different
than the traditional task. As mentioned in §2, we are
interested in estimating β from Eq. 4, while fixing
θ(�). Therefore, we are only interested in computing
expectations of the form of Eq. 7.
As explained in §3.2, any model interpolating
with the β parameters can be reduced to a new log-
linear model with additional features representing
the mixture coefficients of β. We can then use the
inside-outside algorithm to obtain the necessary fea-
ture expectations for features of the form gt,k(x, y),
expectations which assist in the estimation of the β
parameters.
These feature expectations can readily be used
in estimation algorithms such as expectation-
maximization (EM). With EM, the update at itera-
tion t would be:
where the expectations are taken with respect to
β(t−1) and the fixed θ(l) for E = 1, ... , L.
Estimating Feature-Rich Directed Models Re-
cently Berg-Kirkpatrick et al. (2010) found that
replacing traditional multinomial parameterizations
with locally normalized, feature-based log-linear
models was advantageous. This can be understood
as parameterizing θ:
</bodyText>
<equation confidence="0.902319666666667">
expψTh(k, i)
Bk,i = (9)
exp ψTh(k, i&apos;)
</equation>
<bodyText confidence="0.922280166666667">
if
where h(k, i) are a set of features looking at event i
in context k. For such a feature-rich model, our mul-
tilingual modeling framework still substitutes θ with
a mixture of supervised multinomials for L helper
languages as in Eq. 4. However, for computational
</bodyText>
<equation confidence="0.989775">
β(t) �,k = E[gt,k(x, y)]
E (8)
� E[g�,k(x, y)]
,
</equation>
<page confidence="0.967065">
53
</page>
<bodyText confidence="0.957486">
convenience, we also reparametrize the mixture co-
efficients β:
</bodyText>
<equation confidence="0.910661">
exp γ`,k
L (10)
,`1=1 exp γ`1,k
</equation>
<bodyText confidence="0.998677294117647">
Here, each γ`,k is an unconstrained parameter, and
the above “softmax” transformation ensures that β
lies within the probability simplex for context k.
This is done so that a gradient-based optimization
method like L-BFGS (Liu and Nocedal, 1989) can
be used to estimate -y without having to worry about
additional simplex constraints. For optimization,
derivatives of the data log-likelihood with respect to
-y need to be computed. We calculate the derivatives
following Berg-Kirkpatrick et al. (2010, §3.1), mak-
ing use of feature expectations, calculated exactly as
before.
In addition to these estimation techniques, which
are based on the optimization of the log-likelihood,
we also consider a trivially simple technique for es-
timating β: setting βl,k to the uniform weight L−1,
where L is the number of helper languages.
</bodyText>
<sectionHeader confidence="0.9656" genericHeader="method">
5 Coarse-to-Fine Multinomial Expansion
</sectionHeader>
<bodyText confidence="0.9990755">
To expand these multinomials involving coarse-
grained categories into multinomials over fine-
grained categories specific to the target language t,
we do the following:
</bodyText>
<listItem confidence="0.970530714285714">
• Whenever a multinomial conditions on a coarse
category c E C, we make copies of it for each fine-
grained category in λ−1
t (c) ⊂ Ft.3 If the multino-
mial does not condition on coarse categories, it is
simply copied.
• Whenever a probability Oi within a multinomial
</listItem>
<bodyText confidence="0.6807494">
distribution involves a coarse-grained category c
as an event (i.e., it is on the left side of the condi-
tional bar), we expand the event into |λ−1
t (c) |new
events, one per corresponding fine-grained cate-
</bodyText>
<equation confidence="0.474101">
gory, each assigned the value θz 4
|λt (c)|.
</equation>
<footnote confidence="0.90037925">
3We note that in the models we experiment with, we always
condition on at most one fine-grained category.
4During this expansion process for a coarse event, we tried
adding random noise to λt Oi (c) |and renormalizing, to break
</footnote>
<bodyText confidence="0.901948333333333">
symmetry between the fine events, but that was found to be
harmful in preliminary experiments.
The result of this expansion is a model in the
desired family; we use it to initialize conventional
unsupervised parameter estimation. Lexical param-
eters, if any, do not undergo this expansion pro-
cess, and they are estimated anew in the fine grained
model during unsupervised learning, and are initial-
ized using standard methods.
</bodyText>
<sectionHeader confidence="0.998806" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9997515">
In this section, we describe the experiments under-
taken and the results achieved. We first note the
characteristics of the datasets and the universal POS
tags used in multilingual modeling.
</bodyText>
<subsectionHeader confidence="0.980943">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.999970766666667">
For our experiments, we fixed a set of four helper
languages with relatively large amounts of data,
displaying nontrivial linguistic diversity: Czech
(Slavic), English (West-Germanic), German (West-
Germanic), and Italian (Romance). The datasets are
the CoNLL-X shared task data for Czech and Ger-
man (Buchholz and Marsi, 2006),5 the Penn Tree-
bank for English (Marcus et al., 1993), and the
CoNLL 2007 shared task data for Italian (Monte-
magni et al., 2003). This was the only set of helper
languages we tested; improvements are likely pos-
sible. We leave an exploration of helper language
choice (a subset selection problem) to future re-
search, instead demonstrating that the concept has
merit.
We considered ten target languages: Bulgarian
(Bg), Danish (Da), Dutch (Nl), Greek (El), Japanese
(Jp), Portuguese (Pt), Slovene (Sl), Spanish (Es),
Swedish (Sv), and Turkish (Tr). The data come
from the CoNLL-X and CoNLL 2007 shared tasks
(Buchholz and Marsi, 2006; Nivre et al., 2007). For
all the experiments conducted, we trained models
on the training section of a language’s treebank and
tested on the test set. Table 1 shows the number of
sentences in the treebanks and the size of fine POS
tagsets for each language.
Following standard practice, in unsupervised
grammar induction experiments we remove punctu-
ation and then eliminate sentences from the data of
length greater than 10.
</bodyText>
<footnote confidence="0.880151">
5These are based on the Prague Dependency Treebank
(Hajiˇc, 1998) and the Tiger treebank (Brants et al., 2002) re-
spectively.
</footnote>
<page confidence="0.973803">
54
</page>
<table confidence="0.999594">
Pt Tr Bg Jp El Sv Es Sl Nl Da
Training sentences 9,071 4,997 12,823 17,044 2,705 11,042 3,306 1,534 13,349 5,190
Test sentences 288 623 398 709 197 389 206 402 386 322
Size of POS tagset 22 31 54 80 38 41 47 29 12 25
</table>
<tableCaption confidence="0.9655035">
Table 1: The first two rows show the sizes of the training and test datasets for each language. The third row shows the
number of fine POS tags in each language including punctuations.
</tableCaption>
<subsectionHeader confidence="0.997835">
6.2 Universal POS Tags
</subsectionHeader>
<bodyText confidence="0.9999855">
Our coarse-grained, universal POS tag set consists
of the following 12 tags: NOUN, VERB, ADJ
(adjective), ADV (adverb), PRON (pronoun), DET
(determiner), ADP (preposition or postposition),
NUM (numeral), CONJ (conjunction), PRT (parti-
cle), PUNC (punctuation mark) and X (a catch-all
for other categories such as abbreviations or foreign
words). These follow recent work by Das and Petrov
(2011) on unsupervised POS tagging in a multilin-
gual setting with parallel data, and have been de-
scribed in detail by Petrov et al. (2011).
While there might be some controversy about
what an appropriate universal tag set should include,
these 12 categories (or a subset) cover the most fre-
quent parts of speech and exist in one form or an-
other in all of the languages that we studied. For
each language in our data, a mapping from the
fine-grained treebank POS tags to these universal
POS tags was constructed manually by Petrov et al.
(2011).
</bodyText>
<subsectionHeader confidence="0.997345">
6.3 Part-of-Speech Tagging
</subsectionHeader>
<bodyText confidence="0.999966333333333">
Our first experimental task is POS tagging, and here
we describe the specific details of the model, train-
ing and inference and the results attained.
</bodyText>
<subsectionHeader confidence="0.89991">
6.3.1 Model
</subsectionHeader>
<bodyText confidence="0.999735">
The model is a hidden Markov model (HMM),
which has been popular for unsupervised tagging
tasks (Merialdo, 1994; Elworthy, 1994; Smith and
Eisner, 2005; Berg-Kirkpatrick et al., 2010).6 We
use a bigram model and a locally normalized log-
linear parameterization, like Berg-Kirkpatrick et al.
(2010). These locally normalized log-linear mod-
els can look at various aspects of the observation x
given a tag y, or the pair of tags in a transition, in-
corporating overlapping features. In basic monolin-
</bodyText>
<subsectionHeader confidence="0.800354">
6HMMs can be understood as a special case of PCFGs.
</subsectionHeader>
<bodyText confidence="0.99998275">
gual experiments, we used the same set of features
as Berg-Kirkpatrick et al. (2010). For the transi-
tion log-linear model, Berg-Kirkpatrick et al. (2010)
used only a single indicator feature of a tag pair, es-
sentially equating to a traditional multinomial dis-
tribution. For the emission log-linear model, sev-
eral features were used: an indicator feature con-
joining the state y and the word x, a feature checking
whether x contains a digit conjoined with the state y,
another feature indicating whether x contains a hy-
phen conjoined with y, whether the first letter of x is
upper case along with the state y, and finally indica-
tor features corresponding to suffixes up to length 3
present in x conjoined with the state y.
Since only the unlexicalized transition distribu-
tions are common across multiple languages, assum-
ing that they all use a set of universal POS tags, akin
to Eq. 4, we can have a multilingual version of the
transition distributions, by incorporating supervised
helper transition probabilities. Thus, we can write:
</bodyText>
<equation confidence="0.883687">
β`,yB(`) (11)
y→y
</equation>
<bodyText confidence="0.999992">
We use the above expression to replace the transi-
tion distributions, obtaining a multilingual mixture
version of the model. Here, the transition probabili-
</bodyText>
<subsectionHeader confidence="0.588839">
ties B(`)
</subsectionHeader>
<bodyText confidence="0.997564">
y→y for the Eth helper language are fixed after
being estimated using maximum likelihood estima-
tion on the helper language’s treebank.
</bodyText>
<subsectionHeader confidence="0.946491">
6.3.2 Training and Inference
</subsectionHeader>
<bodyText confidence="0.999950333333333">
We trained both the basic feature-based HMM
model as well as the multilingual mixture model by
optimizing the following objective function:7
</bodyText>
<equation confidence="0.76783">
p(x(i),y  |ψ) − C11ψ1122
</equation>
<footnote confidence="0.483725">
7Note that in the objective function, for brevity, we abuse
notation by using ψ for both models – monolingual and multi-
lingual; the latter model is also parameterized by γ.
</footnote>
<equation confidence="0.980237166666667">
L
By→y =
`=1
L(ψ) = N �
i=1 log
Y
</equation>
<page confidence="0.993694">
55
</page>
<table confidence="0.999268625">
Method Pt Tr Bg Jp El Sv Es Sl Nl Da Avg
Uniform+DG 45.7 43.6 38.0 60.4 36.7 37.7 31.8 35.9 43.7 36.2 41.0
Mixture+DG 51.5 38.6 35.8 61.7 38.9 39.9 40.5 36.0 50.2 39.9 43.3
DG (B-K et al., 2010) 53.5 27.9 34.7 52.3 35.3 34.4 40.0 33.4 45.4 48.8 40.6
Method Pt Tr Bg Jp El Sv Es Sl Nl Da Avg
Uniform+DG 83.8 50.4 81.3 77.9 80.3 69.0 82.3 82.8 79.3 82.0 76.9
Mixture+DG 84.7 50.0 82.6 79.9 80.3 67.0 83.3 82.8 80.0 82.0 77.3
DG (B-K et al., 2010) 75.4 50.4 80.7 83.4 88.0 61.5 82.3 75.6 79.2 82.3 75.9
</table>
<tableCaption confidence="0.98401">
Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con-
</tableCaption>
<bodyText confidence="0.9341115">
structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient
method of Berg-Kirkpatrick et al. (2010) using a monolingual feature-based HMM. “Mixture+DG” is the model where
multilingual mixture coefficients ,3 of helper languages are estimated using coarse tags (§4), followed by expansion
(§5), and then initializing DG with the expanded transition parameters. “Uniform+DG” is the case where ,3 are set
to 1/4, transitions of helper languages are mixed, expanded, and then DG is initialized with the result. For (a), eval-
uation is performed using one-to-one mapping accuracy. In case of (b), the tag dictionary solves the problem of tag
identification and performance is measured using per word POS accuracy. “Avg” denotes macro-average across the
ten languages.
Note that this involves marginalizing out all possible
state configurations y for a sentence x, resulting in
a non-convex objective. As described in §4, we opti-
mized this function using L-BFGS. For the mono-
lingual model, derivatives of the feature weights
took the exact same form as Berg-Kirkpatrick et al.
(2010), while for the mixture case, we computed
gradients with respect to -y, the unconstrained pa-
rameters used to express the mixture coefficients ,3
(see Eq. 10). The regularization constant C was set
to 1.0 for all experiments, and L-BFGS was run till
convergence.
During training, for the basic monolingual
feature-based HMM model, we initialized all param-
eters using small random real values, sampled from
N(0, 0.01). For estimation of the mixture parame-
ters -y for our multilingual model (step 3 in §2), we
similarly sampled real values from N(0, 0.01) as an
initialization point. Moreover, during this stage, the
emission parameters also go through parameter es-
timation, but they are monolingual, and are initial-
ized with real values sampled from N(0, 0.01); as
explained in §2, coarse universal tags are used both
in the transitions and emissions during multilingual
estimation.
After the mixture parameters -y are estimated, we
compute the mixture probabilities ,3 using Eq. 10.
Next, for each tag pair y, y&apos;, we compute Oy→y&amp;quot;
which are the coarse transition probabilities inter-
polated using ,3, given the helper languages. We
then expand these transition probabilities (see §5) to
result in transition probabilities based on fine tags.
Finally, we train a feature-HMM by initializing its
transition parameters with natural logarithms of the
expanded 0 parameters, and the emission parame-
ters using small random real values sampled from
N(0, 0.01). This implies that the lexicalized emis-
sion parameters rl that were previously estimated in
the coarse multilingual model are thrown away and
not used for initialization; instead standard initial-
ization is used.
For inference at the testing stage, we use min-
imum Bayes-risk decoding (or “posterior decod-
ing”), by choosing the most probable tag for each
word position, given the entire observation x. We
chose this strategy because it usually performs
slightly better than Viterbi decoding (Cohen and
Smith, 2009; Ganchev et al., 2010).
</bodyText>
<subsectionHeader confidence="0.907641">
6.3.3 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9999375">
For experiments, we considered three configura-
tions, and for each, we implemented two variants of
POS induction, one without any kind of supervision,
and the other with a tag dictionary. Our baseline is
</bodyText>
<page confidence="0.992451">
56
</page>
<bodyText confidence="0.999943825">
the direct gradient approach of Berg-Kirkpatrick et
al. (2010), which is the current state of the art for this
task, outperforming classical HMMs. Because this
model achieves strong performance using straight-
forward MLE, it also serves as the core model within
our approach. This model has also been applied in
a multilingual setting with parallel data (Das and
Petrov, 2011). In this baseline, we set the number
of HMM states to the number of fine-grained tree-
bank tags for the given language.
We test two versions of our model. The first ini-
tializes training of the target language’s POS model
using a uniform mixture of the helper language mod-
els (i.e., each βt,y = 1L = 14), and expansion from
coarse-grained to fine-grained POS tags as described
in §5. We call this model “Uniform+DG.”
The second version estimates the mixture coeffi-
cients to maximize likelihood, then expands the POS
tags (§5), using the result to initialize training of the
final model. We call this model “Mixture+DG.”
No Tag Dictionary For each of the above configura-
tions, we ran purely unsupervised training without a
tag dictionary, and evaluated using one-to-one map-
ping accuracy constraining at most one HMM state
to map to a unique treebank tag in the test data, us-
ing maximum bipartite matching. This is a variant of
the greedy one-to-one mapping scheme of Haghighi
and Klein (2006).8
With a Tag Dictionary We also ran a second ver-
sion of each experimental configuration, where we
used a tag dictionary to restrict the possible path se-
quences of the HMM during both learning and infer-
ence. This tag dictionary was constructed only from
the training section of a given language’s treebank.
It is widely known that such knowledge improves
the quality of the model, though it is an open debate
whether such knowledge is realistic to assume. For
this experiment we removed punctuation from the
training and test data, enabling direct use within the
dependency grammar induction experiments.
</bodyText>
<footnote confidence="0.996330166666667">
8We also evaluated our approach using the greedy version of
this evaluation metric, and results followed the same trends with
only minor differences. We did not choose the other variant,
many-to-one mapping accuracy, because quite often the metric
mapped several HMM states to one treebank tag, leaving many
treebank tags unaccounted for.
</footnote>
<sectionHeader confidence="0.674627" genericHeader="evaluation">
6.3.4 Results
</sectionHeader>
<bodyText confidence="0.999429090909091">
All results for POS induction are shown in Ta-
ble 2. Without a tag dictionary, in eight out of ten
cases, either Uniform+DG or Mixture+DG outper-
forms the monolingual baseline (Table 2a). For six
of these eight languages, the latter model where the
mixture coefficients are learned automatically fares
better than uniform weighting. With a tag dictionary,
the multilingual variants outperform the baseline in
seven out of ten cases, and the learned mixture out-
performs or matches the uniform mixture in five of
those seven (Table 2b).
</bodyText>
<subsectionHeader confidence="0.997176">
6.4 Dependency Grammar Induction
</subsectionHeader>
<bodyText confidence="0.999931">
We next describe experiments for dependency gram-
mar induction. As the basic grammatical model,
we adopt the dependency model with valence (Klein
and Manning, 2004), which forms the basis for state-
of-the-art results for dependency grammar induc-
tion in various settings (Cohen and Smith, 2009;
Spitkovsky et al., 2010; Gillenwater et al., 2010;
Berg-Kirkpatrick and Klein, 2010). As shown in Ta-
ble 3, DMV obtains much higher accuracy in the su-
pervised setting than the unsupervised setting, sug-
gesting that more can be achieved with this model
family.9 For this reason, and because DMV is eas-
ily interpreted as a PCFG, it is our starting point and
baseline.
We consider four conditions. The independent
variables are (1) whether we use uniform β (all set to
4) or estimate them using EM (as described in §4),
</bodyText>
<equation confidence="0.532923">
1
</equation>
<bodyText confidence="0.9369676875">
and (2) whether we simply use the mixture model to
decode the test data, or to initialize EM for the DMV.
The four settings are denoted “Uniform,” “Mixture,”
“Uniform+EM,” and “Mixture+EM.”
The results are given in Table 3. In general, the
use of data from other languages improves perfor-
mance considerably; all of our methods outperform
the Klein and Manning (2004) initializer, and we
achieve state-of-the-art performance for eight out of
ten languages. Uniform and Mixture behave simi-
larly, with a slight advantage to the trained mixture
setting. Using EM to train the mixture coefficients
more often hurts than helps (six languages out of
ten). It is well known that likelihood does not cor-
9Its supervised performance is still far from the supervised
state of the art in dependency parsing.
</bodyText>
<page confidence="0.99351">
57
</page>
<table confidence="0.999399777777778">
Method Pt Tr Bg Jp El Sv Es Sl Nl Da Avg
Uniform 78.6 45.0 75.6 56.3 57.0 74.0 73.2 46.1 50.7 59.2 61.6
Mixture 76.8 45.3 75.5 58.3 59.5 73.2 75.9 46.0 51.1 59.9 62.2
Uniform+EM 78.7 43.9 74.7 59.8 73.0 70.5 75.5 41.3 45.9 51.3 61.5
Mixture+EM 79.8 44.1 72.8 63.9 72.3 68.7 76.7 41.0 46.0 55.2 62.1
EM (K &amp; M, 2004) 42.5 36.3 54.3 43.0 41.0 42.3 38.1 37.0 38.6 41.4 41.4
PR (G et al., ’10) 47.8 53.4 54.0 60.2 - 42.2 62.4 50.3 37.9 44.0 -
Phylo. (B-K &amp; K, ’10) 63.1 - - - - 58.3 63.8 49.6 45.1 41.6 -
Supervised (MLE) 81.7 75.7 83.0 89.2 81.8 83.2 79.0 74.5 64.8 80.8 79.3
</table>
<tableCaption confidence="0.8383395">
Table 3: Results for dependency grammar induction given gold-standard POS tags, reported as attachment accuracy
(fraction of parents which are correct). The three existing methods are: our replication of EM with the initializer from
Klein and Manning (2004), denoted “EM”; reported results from Gillenwater et al. (2010) for posterior regularization
(“PR”); and reported results from Berg-Kirkpatrick and Klein (2010), denoted “Phylo.” “Supervised (MLE)” are oracle
results of estimating parameters from gold-standard annotated data using maximum likelihood estimation. “Avg”
denotes macro-average across the ten languages.
</tableCaption>
<figureCaption confidence="0.987927">
Figure 2: Projection of the learned mixture coefficients
through PCA. In green, Japanese. In red, Dutch, Danish
and Swedish. In blue, Bulgarian and Slovene. In ma-
genta, Portuguese and Spanish. In black, Greek. In cyan,
Turkish.
</figureCaption>
<bodyText confidence="0.99992759375">
relate with the true accuracy measurement, and so
it is unsurprising that this holds in the constrained
mixture family as well. In future work, a different
parametrization of the mixture coefficients, through
features, or perhaps a Bayesian prior on the weights,
might lead to an objective that better simulates ac-
curacy.
Table 3 shows that even uniform mixture coef-
ficients are sufficient to obtain accuracy which su-
percedes most unsupervised baselines. We were in-
terested in testing whether the coefficients which are
learned actually reflect similarities between the lan-
guages. To do that, we projected the learned vectors
β for each tested language using principal compo-
nent analysis and plotted the result in Figure 2. It
is interesting to note that languages which are closer
phylogenetically tend to appear closer to each other
in the plot.
Our experiments also show that multilingual
learning performs better for dependency grammar
induction than part-of-speech tagging. We believe
that this happens because of the nature of the mod-
els and data we use. The transition matrix in part-
of-speech tagging largely depends on word order in
the various helper languages, which differs greatly.
This means that a mixture of transition matrices will
not necessarily yield a meaningful transition matrix.
However, for dependency grammar, there are certain
universal dependencies which appear in all helper
languages, and therefore, a mixture between multi-
nomials for these dependencies still yields a useful
multinomial.
</bodyText>
<subsectionHeader confidence="0.994296">
6.5 Inducing Dependencies from Words
</subsectionHeader>
<bodyText confidence="0.999785">
Finally, we combine the models for POS tagging and
grammar induction to perform grammar induction
directly from words, instead of gold-standard POS
tags. Our approach is as follows:
</bodyText>
<footnote confidence="0.7177615">
1. With a tag dictionary, learn a fine-grained POS
tagging model unsupervised, using either DG or
Mixture+DG as described in §6.3 and shown in
Table 2b.
</footnote>
<page confidence="0.976933">
58
</page>
<table confidence="0.995472">
Method Tags Pt Tr Bg Jp El Sv Es Sl Nl Da Avg
Joint DG 68.4 52.4 62.4 61.4 63.5 58.2 67.7 47.2 48.3 50.4 57.9
Joint Mixture+DG 62.2 47.4 67.0 69.5 52.2 49.1 69.3 36.8 52.2 50.1 55.6
Pipeline DG 60.0 50.8 57.7 64.2 68.2 57.9 65.8 45.8 49.9 48.9 56.9
Pipeline Mixture+DG 59.8 47.1 62.9 68.6 50.0 47.6 68.1 36.4 51.2 48.3 54.0
Gold-standard tags 79.8 45.3 75.6 63.9 73.0 74.0 76.7 46.1 50.7 59.9 64.5
</table>
<tableCaption confidence="0.9999716">
Table 4: Results for dependency grammar induction over words. “Joint”/“Pipeline” refers to joint/pipeline decoding
of tags and dependencies as described in the text. See §6.3 for a description of DG and Mixture+DG. For the induction
of dependencies we use the Mixture+EM setting as described in §6.4. All tag induction uses a dictionary as specified
in §6.3. The last row in this table indicates the best results using multilingual guidance taken from our methods in
Table 3. “Avg” denotes macro-average across the ten languages.
</tableCaption>
<bodyText confidence="0.9639031">
2. Apply the fine-grained tagger to the words in the
training data for the dependency parser. We con-
sider two variants: the most probable assignment
of tags to words (denoted “Pipeline”), and the pos-
terior distribution over tags for each word, repre-
sented as a weighted “sausage” lattice (denoted
“Joint”). This idea was explored for joint infer-
ence by Cohen and Smith (2007).
3. We apply the Mixture+EM unsupervised parser
learning method from §6.4 to the automatically
tagged sentences, or the lattices.
4. Given the two models, we infer POS tags on the
test data using DG or Mixture+DG to get a lattice
(Joint) or a sequence (Pipeline) and then parse us-
ing the model from the previous step.10 The re-
sulting dependency trees are evaluated against the
gold standard.
Results are reported in Table 4. In almost all cases,
joint decoding of tags and trees performs better than
the pipeline. Even though our part-of-speech tagger
with multilingual guidance outperforms the com-
pletely unsupervised baseline, there is not always an
advantage of using this multilingually guided part-
of-speech tagger for dependency grammar induc-
tion. For Turkish, Japanese, Slovene and Dutch, our
unsupervised learner from words outperforms unsu-
pervised parsing using gold-standard part-of-speech
tags.
We note that some recent work gives a treatment
to unsupervised parsing (but not of dependencies)
10The decoding method on test data (Joint or Pipeline) was
matched to the training method, though they are orthogonal in
principle.
directly from words (Seginer, 2007). Earlier work
that induced part-of-speech tags and then performed
unsupervised parsing in a pipeline includes Klein
and Manning (2004) and Smith (2006). Headden
et al. (2009) described the use of a lexicalized vari-
ant of the DMV model, with the use of gold part-of-
speech tags.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999926">
We presented an approach to exploiting annotated
data in helper languages to infer part-of-speech tag-
ging and dependency parsing models in a different,
target language, without parallel data. Our approach
performs well in many cases. We also described a
way to do joint decoding of part-of-speech tags and
dependencies which performs better than a pipeline.
Future work might consider exploiting a larger num-
ber of treebanks, and more powerful techniques for
combining models than simple local mixtures.
</bodyText>
<sectionHeader confidence="0.998138" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996014">
We thank Ryan McDonald and Slav Petrov for help-
ful comments on an early draft of the paper. This re-
search has been funded by NSF grants IIS-0844507 and
IIS-0915187 and by U.S. Army Research Office grant
W911NF-10-1-0533.
</bodyText>
<sectionHeader confidence="0.999318" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9979392">
T. Berg-Kirkpatrick and D. Klein. 2010. Phylogenetic
grammar induction. In Proceedings ofACL.
T. Berg-Kirkpatrick, A. B. Cˆot´e, J. DeNero, and D. Klein.
2010. Painless unsupervised learning with features. In
Proceedings of NAACL-HLT.
</reference>
<page confidence="0.982918">
59
</page>
<reference confidence="0.999932377358491">
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In Proceedings of the
Workshop on Treebanks and Linguistic Theories.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proceedings
of CoNLL.
D. Burkett and D. Klein. 2008. Two languages are bet-
ter than one (for syntactic parsing). In Proceedings of
EMNLP.
S. B. Cohen and N. A. Smith. 2007. Joint morpholog-
ical and syntactic disambiguation. In Proceedings of
EMNLP-CoNLL.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In Proceedings of HLT-
NAACL.
S. B. Cohen and N. A. Smith. 2010. Covariance in unsu-
pervised learning of probabilistic grammars. Journal
of Machine Learning Research, 11:3017–3051.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In Proceedings of ACL-HLT.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Proceedings of ACL.
K. Ganchev, J. Grac¸a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search, 11:2001–2049.
J. Gillenwater, K. Ganchev, J. Grac¸a, F. Pereira, and
B. Taskar. 2010. Sparsity in dependency grammar
induction. In Proceedings of ACL.
A. Haghighi and D. Klein. 2006. Prototype driven learn-
ing for sequence models. In Proceedings of HLT-
NAACL.
J. Hajiˇc. 1998. Building a syntactically annotated
corpus: The Prague Dependency Treebank. In Is-
sues of Valency and Meaning. Studies in Honor of
Jarmila Panevov´a. Prague Karolinum, Charles Univer-
sity Press.
W. P. Headden, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proceedings of
NAACL-HLT.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proceedings of NAACL.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proceedings of ACL.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory BFGS method for large scale optimization. Math.
Programming, 45:503–528.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the Penn treebank. Computational Linguistics, 19.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-source
transfer of delexicalized dependency parsers. In Pro-
ceedings of EMNLP.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Compulational Lingustics, 20(2):155–
72.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Zampolli, F. Fanciulli, M. Massetani,
R. Raffaelli, R. Basili, M. T. Pazienza, D. Saracino,
F. Zanzotto, N. Mana, F. Pianesi, and R. Delmonte.
2003. Building the Italian Syntactic-Semantic Tree-
bank. In Building and using Parsed Corpora, Lan-
guage and Speech Series. Kluwer, Dordrecht.
T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.
2009. Multilingual part-of-speech tagging: Two un-
supervised approaches. JAIR, 36.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010.
Using universal linguistic knowledge to guide gram-
mar induction. In Proceedings of EMNLP.
J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceedings
of CoNLL.
S. Petrov, D. Das, and R. McDonald. 2011. A universal
part-of-speech tagset. ArXiv:1104.2086.
Y. Seginer. 2007. Fast unsupervised incremental parsing.
In Proceedings of ACL.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Pro-
ceedings of ACL.
D. A. Smith and J. Eisner. 2009. Parser adaptation and
projection with quasi-synchronous grammar features.
In Proceedings of EMNLP.
D. A. Smith and N. A. Smith. 2004. Bilingual parsing
with factored estimation: Using English to parse Ko-
rean. In Proceedings of EMNLP.
N. A. Smith. 2006. Novel Estimation Methods for Unsu-
pervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
B. Snyder and R. Barzilay. 2008. Unsupervised multi-
lingual learning for morphological segmentation. In
Proceedings of ACL.
B. Snyder, T. Naseem, and R. Barzilay. 2009. Unsuper-
vised multilingual grammar induction. In Proceedings
of ACL-IJCNLP.
V. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010. From
baby steps to leapfrog: How “less is more” in unsuper-
vised dependency parsing. In Proceedings of NAACL.
C. Xi and R. Hwa. 2005. A backoff model for bootstrap-
ping resources for non-English languages. In Proceed-
ings of HLT-EMNLP.
</reference>
<page confidence="0.970507">
60
</page>
<reference confidence="0.881408333333333">
D. Yarowsky and G. Ngai. 2001. Inducing multilingual
POS taggers and NP bracketers via robust projection
across aligned corpora. In Proceedings of NAACL.
</reference>
<page confidence="0.999232">
61
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.727653">
<title confidence="0.988134">Unsupervised Structure with Non-Parallel Multilingual Guidance</title>
<author confidence="0.997644">Shay B Cohen Dipanjan Das Noah A</author>
<affiliation confidence="0.918008333333333">Language Technologies School of Computer Carnegie Mellon</affiliation>
<address confidence="0.993025">Pittsburgh, PA 15213,</address>
<abstract confidence="0.999537857142857">We describe a method for prediction of linguistic structure in a language for which only unlabeled data is available, using annotated data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Berg-Kirkpatrick</author>
<author>D Klein</author>
</authors>
<title>Phylogenetic grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="3686" citStr="Berg-Kirkpatrick and Klein, 2010" startWordPosition="549" endWordPosition="552">ised parameter estimation. Some previous multilingual research, such as Bayesian parameter tying across languages (Cohen and Smith, 2009) or models of parameter 1Although the stated objective is often to build systems for resource-poor languages and domains, for evaluation purposes, annotated treebank test data figure prominently in this research (including in this paper). Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50–61, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics drift down phylogenetic trees (Berg-Kirkpatrick and Klein, 2010) is comparable, but the practical assumption of supervised helper languages is new to this work. Naseem et al. (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert handwritten rules as constraints. Herein, we specifically focus on two problems in linguistic structure prediction: unsupervised POS tagging and unsupervised dependency grammar induction. Our experiments demonstrate that the presented method outperforms strong state-of-the-art unsupervised baselines for both tasks. Our approach can be applied to other problems in which a </context>
<context position="29863" citStr="Berg-Kirkpatrick and Klein, 2010" startWordPosition="4933" endWordPosition="4936">an uniform weighting. With a tag dictionary, the multilingual variants outperform the baseline in seven out of ten cases, and the learned mixture outperforms or matches the uniform mixture in five of those seven (Table 2b). 6.4 Dependency Grammar Induction We next describe experiments for dependency grammar induction. As the basic grammatical model, we adopt the dependency model with valence (Klein and Manning, 2004), which forms the basis for stateof-the-art results for dependency grammar induction in various settings (Cohen and Smith, 2009; Spitkovsky et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010). As shown in Table 3, DMV obtains much higher accuracy in the supervised setting than the unsupervised setting, suggesting that more can be achieved with this model family.9 For this reason, and because DMV is easily interpreted as a PCFG, it is our starting point and baseline. We consider four conditions. The independent variables are (1) whether we use uniform β (all set to 4) or estimate them using EM (as described in §4), 1 and (2) whether we simply use the mixture model to decode the test data, or to initialize EM for the DMV. The four settings are denoted “Uniform,” “Mixture,” “Uniform+</context>
<context position="32084" citStr="Berg-Kirkpatrick and Klein (2010)" startWordPosition="5323" endWordPosition="5326">8.6 41.4 41.4 PR (G et al., ’10) 47.8 53.4 54.0 60.2 - 42.2 62.4 50.3 37.9 44.0 - Phylo. (B-K &amp; K, ’10) 63.1 - - - - 58.3 63.8 49.6 45.1 41.6 - Supervised (MLE) 81.7 75.7 83.0 89.2 81.8 83.2 79.0 74.5 64.8 80.8 79.3 Table 3: Results for dependency grammar induction given gold-standard POS tags, reported as attachment accuracy (fraction of parents which are correct). The three existing methods are: our replication of EM with the initializer from Klein and Manning (2004), denoted “EM”; reported results from Gillenwater et al. (2010) for posterior regularization (“PR”); and reported results from Berg-Kirkpatrick and Klein (2010), denoted “Phylo.” “Supervised (MLE)” are oracle results of estimating parameters from gold-standard annotated data using maximum likelihood estimation. “Avg” denotes macro-average across the ten languages. Figure 2: Projection of the learned mixture coefficients through PCA. In green, Japanese. In red, Dutch, Danish and Swedish. In blue, Bulgarian and Slovene. In magenta, Portuguese and Spanish. In black, Greek. In cyan, Turkish. relate with the true accuracy measurement, and so it is unsurprising that this holds in the constrained mixture family as well. In future work, a different parametri</context>
</contexts>
<marker>Berg-Kirkpatrick, Klein, 2010</marker>
<rawString>T. Berg-Kirkpatrick and D. Klein. 2010. Phylogenetic grammar induction. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Berg-Kirkpatrick</author>
<author>A B Cˆot´e</author>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<marker>Berg-Kirkpatrick, Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>T. Berg-Kirkpatrick, A. B. Cˆot´e, J. DeNero, and D. Klein. 2010. Painless unsupervised learning with features. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brants</author>
<author>S Dipper</author>
<author>S Hansen</author>
<author>W Lezius</author>
<author>G Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Treebanks and Linguistic Theories.</booktitle>
<contexts>
<context position="18769" citStr="Brants et al., 2002" startWordPosition="3102" endWordPosition="3105">he data come from the CoNLL-X and CoNLL 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). For all the experiments conducted, we trained models on the training section of a language’s treebank and tested on the test set. Table 1 shows the number of sentences in the treebanks and the size of fine POS tagsets for each language. Following standard practice, in unsupervised grammar induction experiments we remove punctuation and then eliminate sentences from the data of length greater than 10. 5These are based on the Prague Dependency Treebank (Hajiˇc, 1998) and the Tiger treebank (Brants et al., 2002) respectively. 54 Pt Tr Bg Jp El Sv Es Sl Nl Da Training sentences 9,071 4,997 12,823 17,044 2,705 11,042 3,306 1,534 13,349 5,190 Test sentences 288 623 398 709 197 389 206 402 386 322 Size of POS tagset 22 31 54 80 38 41 47 29 12 25 Table 1: The first two rows show the sizes of the training and test datasets for each language. The third row shows the number of fine POS tags in each language including punctuations. 6.2 Universal POS Tags Our coarse-grained, universal POS tag set consists of the following 12 tags: NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner), ADP</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith. 2002. The TIGER treebank. In Proceedings of the Workshop on Treebanks and Linguistic Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="17600" citStr="Buchholz and Marsi, 2006" startWordPosition="2911" endWordPosition="2914">l during unsupervised learning, and are initialized using standard methods. 6 Experiments and Results In this section, we describe the experiments undertaken and the results achieved. We first note the characteristics of the datasets and the universal POS tags used in multilingual modeling. 6.1 Data For our experiments, we fixed a set of four helper languages with relatively large amounts of data, displaying nontrivial linguistic diversity: Czech (Slavic), English (West-Germanic), German (WestGermanic), and Italian (Romance). The datasets are the CoNLL-X shared task data for Czech and German (Buchholz and Marsi, 2006),5 the Penn Treebank for English (Marcus et al., 1993), and the CoNLL 2007 shared task data for Italian (Montemagni et al., 2003). This was the only set of helper languages we tested; improvements are likely possible. We leave an exploration of helper language choice (a subset selection problem) to future research, instead demonstrating that the concept has merit. We considered ten target languages: Bulgarian (Bg), Danish (Da), Dutch (Nl), Greek (El), Japanese (Jp), Portuguese (Pt), Slovene (Sl), Spanish (Es), Swedish (Sv), and Turkish (Tr). The data come from the CoNLL-X and CoNLL 2007 shared</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Burkett</author>
<author>D Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1560" citStr="Burkett and Klein, 2008" startWordPosition="233" endWordPosition="236">such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to lear</context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>D. Burkett and D. Klein. 2008. Two languages are better than one (for syntactic parsing). In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Joint morphological and syntactic disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="35715" citStr="Cohen and Smith (2007)" startWordPosition="5903" endWordPosition="5906">escribed in §6.4. All tag induction uses a dictionary as specified in §6.3. The last row in this table indicates the best results using multilingual guidance taken from our methods in Table 3. “Avg” denotes macro-average across the ten languages. 2. Apply the fine-grained tagger to the words in the training data for the dependency parser. We consider two variants: the most probable assignment of tags to words (denoted “Pipeline”), and the posterior distribution over tags for each word, represented as a weighted “sausage” lattice (denoted “Joint”). This idea was explored for joint inference by Cohen and Smith (2007). 3. We apply the Mixture+EM unsupervised parser learning method from §6.4 to the automatically tagged sentences, or the lattices. 4. Given the two models, we infer POS tags on the test data using DG or Mixture+DG to get a lattice (Joint) or a sequence (Pipeline) and then parse using the model from the previous step.10 The resulting dependency trees are evaluated against the gold standard. Results are reported in Table 4. In almost all cases, joint decoding of tags and trees performs better than the pipeline. Even though our part-of-speech tagger with multilingual guidance outperforms the comp</context>
</contexts>
<marker>Cohen, Smith, 2007</marker>
<rawString>S. B. Cohen and N. A. Smith. 2007. Joint morphological and syntactic disambiguation. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="3190" citStr="Cohen and Smith, 2009" startWordPosition="481" endWordPosition="485"> begin with supervised maximum likelihood estimates for models of the helper languages. In the second stage, we learn a model for the target language using unannotated data, maximizing likelihood over interpolations of the helper language models’ distributions. The tying is performed at the parameter level, through coarse, nearly-universal syntactic categories (POS tags). The resulting model is then used to initialize learning of the target language’s model using standard unsupervised parameter estimation. Some previous multilingual research, such as Bayesian parameter tying across languages (Cohen and Smith, 2009) or models of parameter 1Although the stated objective is often to build systems for resource-poor languages and domains, for evaluation purposes, annotated treebank test data figure prominently in this research (including in this paper). Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50–61, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics drift down phylogenetic trees (Berg-Kirkpatrick and Klein, 2010) is comparable, but the practical assumption of supervised helper languages is new to this work. Naseem </context>
<context position="26342" citStr="Cohen and Smith, 2009" startWordPosition="4362" endWordPosition="4365">the expanded 0 parameters, and the emission parameters using small random real values sampled from N(0, 0.01). This implies that the lexicalized emission parameters rl that were previously estimated in the coarse multilingual model are thrown away and not used for initialization; instead standard initialization is used. For inference at the testing stage, we use minimum Bayes-risk decoding (or “posterior decoding”), by choosing the most probable tag for each word position, given the entire observation x. We chose this strategy because it usually performs slightly better than Viterbi decoding (Cohen and Smith, 2009; Ganchev et al., 2010). 6.3.3 Experimental Setup For experiments, we considered three configurations, and for each, we implemented two variants of POS induction, one without any kind of supervision, and the other with a tag dictionary. Our baseline is 56 the direct gradient approach of Berg-Kirkpatrick et al. (2010), which is the current state of the art for this task, outperforming classical HMMs. Because this model achieves strong performance using straightforward MLE, it also serves as the core model within our approach. This model has also been applied in a multilingual setting with paral</context>
<context position="29777" citStr="Cohen and Smith, 2009" startWordPosition="4921" endWordPosition="4924">l where the mixture coefficients are learned automatically fares better than uniform weighting. With a tag dictionary, the multilingual variants outperform the baseline in seven out of ten cases, and the learned mixture outperforms or matches the uniform mixture in five of those seven (Table 2b). 6.4 Dependency Grammar Induction We next describe experiments for dependency grammar induction. As the basic grammatical model, we adopt the dependency model with valence (Klein and Manning, 2004), which forms the basis for stateof-the-art results for dependency grammar induction in various settings (Cohen and Smith, 2009; Spitkovsky et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010). As shown in Table 3, DMV obtains much higher accuracy in the supervised setting than the unsupervised setting, suggesting that more can be achieved with this model family.9 For this reason, and because DMV is easily interpreted as a PCFG, it is our starting point and baseline. We consider four conditions. The independent variables are (1) whether we use uniform β (all set to 4) or estimate them using EM (as described in §4), 1 and (2) whether we simply use the mixture model to decode the test data, or to in</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>S. B. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Covariance in unsupervised learning of probabilistic grammars.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--3017</pages>
<contexts>
<context position="1079" citStr="Cohen and Smith, 2010" startWordPosition="156" endWordPosition="159">r approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing. 1 Introduction A major focus of recent NLP research has involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsu</context>
</contexts>
<marker>Cohen, Smith, 2010</marker>
<rawString>S. B. Cohen and N. A. Smith. 2010. Covariance in unsupervised learning of probabilistic grammars. Journal of Machine Learning Research, 11:3017–3051.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>S Petrov</author>
</authors>
<title>Unsupervised part-ofspeech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="1606" citStr="Das and Petrov, 2011" startWordPosition="241" endWordPosition="244">and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annot</context>
<context position="19606" citStr="Das and Petrov (2011)" startWordPosition="3248" endWordPosition="3251"> 38 41 47 29 12 25 Table 1: The first two rows show the sizes of the training and test datasets for each language. The third row shows the number of fine POS tags in each language including punctuations. 6.2 Universal POS Tags Our coarse-grained, universal POS tag set consists of the following 12 tags: NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner), ADP (preposition or postposition), NUM (numeral), CONJ (conjunction), PRT (particle), PUNC (punctuation mark) and X (a catch-all for other categories such as abbreviations or foreign words). These follow recent work by Das and Petrov (2011) on unsupervised POS tagging in a multilingual setting with parallel data, and have been described in detail by Petrov et al. (2011). While there might be some controversy about what an appropriate universal tag set should include, these 12 categories (or a subset) cover the most frequent parts of speech and exist in one form or another in all of the languages that we studied. For each language in our data, a mapping from the fine-grained treebank POS tags to these universal POS tags was constructed manually by Petrov et al. (2011). 6.3 Part-of-Speech Tagging Our first experimental task is POS</context>
<context position="26973" citStr="Das and Petrov, 2011" startWordPosition="4463" endWordPosition="4466">et al., 2010). 6.3.3 Experimental Setup For experiments, we considered three configurations, and for each, we implemented two variants of POS induction, one without any kind of supervision, and the other with a tag dictionary. Our baseline is 56 the direct gradient approach of Berg-Kirkpatrick et al. (2010), which is the current state of the art for this task, outperforming classical HMMs. Because this model achieves strong performance using straightforward MLE, it also serves as the core model within our approach. This model has also been applied in a multilingual setting with parallel data (Das and Petrov, 2011). In this baseline, we set the number of HMM states to the number of fine-grained treebank tags for the given language. We test two versions of our model. The first initializes training of the target language’s POS model using a uniform mixture of the helper language models (i.e., each βt,y = 1L = 14), and expansion from coarse-grained to fine-grained POS tags as described in §5. We call this model “Uniform+DG.” The second version estimates the mixture coefficients to maximize likelihood, then expands the POS tags (§5), using the result to initialize training of the final model. We call this m</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>D. Das and S. Petrov. 2011. Unsupervised part-ofspeech tagging with bilingual graph-based projections. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Does Baum-Welch re-estimation help taggers?</title>
<date>1994</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="20460" citStr="Elworthy, 1994" startWordPosition="3396" endWordPosition="3397"> categories (or a subset) cover the most frequent parts of speech and exist in one form or another in all of the languages that we studied. For each language in our data, a mapping from the fine-grained treebank POS tags to these universal POS tags was constructed manually by Petrov et al. (2011). 6.3 Part-of-Speech Tagging Our first experimental task is POS tagging, and here we describe the specific details of the model, training and inference and the results attained. 6.3.1 Model The model is a hidden Markov model (HMM), which has been popular for unsupervised tagging tasks (Merialdo, 1994; Elworthy, 1994; Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010).6 We use a bigram model and a locally normalized loglinear parameterization, like Berg-Kirkpatrick et al. (2010). These locally normalized log-linear models can look at various aspects of the observation x given a tag y, or the pair of tags in a transition, incorporating overlapping features. In basic monolin6HMMs can be understood as a special case of PCFGs. gual experiments, we used the same set of features as Berg-Kirkpatrick et al. (2010). For the transition log-linear model, Berg-Kirkpatrick et al. (2010) used only a single indicato</context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>D. Elworthy. 1994. Does Baum-Welch re-estimation help taggers? In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ganchev</author>
<author>J Grac¸a</author>
<author>J Gillenwater</author>
<author>B Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--2001</pages>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>K. Ganchev, J. Grac¸a, J. Gillenwater, and B. Taskar. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11:2001–2049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gillenwater</author>
<author>K Ganchev</author>
<author>J Grac¸a</author>
<author>F Pereira</author>
<author>B Taskar</author>
</authors>
<title>Sparsity in dependency grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Gillenwater, Ganchev, Grac¸a, Pereira, Taskar, 2010</marker>
<rawString>J. Gillenwater, K. Ganchev, J. Grac¸a, F. Pereira, and B. Taskar. 2010. Sparsity in dependency grammar induction. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Prototype driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="27963" citStr="Haghighi and Klein (2006)" startWordPosition="4632" endWordPosition="4635"> described in §5. We call this model “Uniform+DG.” The second version estimates the mixture coefficients to maximize likelihood, then expands the POS tags (§5), using the result to initialize training of the final model. We call this model “Mixture+DG.” No Tag Dictionary For each of the above configurations, we ran purely unsupervised training without a tag dictionary, and evaluated using one-to-one mapping accuracy constraining at most one HMM state to map to a unique treebank tag in the test data, using maximum bipartite matching. This is a variant of the greedy one-to-one mapping scheme of Haghighi and Klein (2006).8 With a Tag Dictionary We also ran a second version of each experimental configuration, where we used a tag dictionary to restrict the possible path sequences of the HMM during both learning and inference. This tag dictionary was constructed only from the training section of a given language’s treebank. It is widely known that such knowledge improves the quality of the model, though it is an open debate whether such knowledge is realistic to assume. For this experiment we removed punctuation from the training and test data, enabling direct use within the dependency grammar induction experime</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>A. Haghighi and D. Klein. 2006. Prototype driven learning for sequence models. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
</authors>
<title>Building a syntactically annotated corpus: The Prague Dependency Treebank.</title>
<date>1998</date>
<booktitle>In Issues of Valency and Meaning. Studies in Honor of Jarmila Panevov´a. Prague Karolinum,</booktitle>
<publisher>Charles University Press.</publisher>
<marker>Hajiˇc, 1998</marker>
<rawString>J. Hajiˇc. 1998. Building a syntactically annotated corpus: The Prague Dependency Treebank. In Issues of Valency and Meaning. Studies in Honor of Jarmila Panevov´a. Prague Karolinum, Charles University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W P Headden</author>
<author>M Johnson</author>
<author>D McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="37061" citStr="Headden et al. (2009)" startWordPosition="6111" endWordPosition="6114">ependency grammar induction. For Turkish, Japanese, Slovene and Dutch, our unsupervised learner from words outperforms unsupervised parsing using gold-standard part-of-speech tags. We note that some recent work gives a treatment to unsupervised parsing (but not of dependencies) 10The decoding method on test data (Joint or Pipeline) was matched to the training method, though they are orthogonal in principle. directly from words (Seginer, 2007). Earlier work that induced part-of-speech tags and then performed unsupervised parsing in a pipeline includes Klein and Manning (2004) and Smith (2006). Headden et al. (2009) described the use of a lexicalized variant of the DMV model, with the use of gold part-ofspeech tags. 7 Conclusion We presented an approach to exploiting annotated data in helper languages to infer part-of-speech tagging and dependency parsing models in a different, target language, without parallel data. Our approach performs well in many cases. We also described a way to do joint decoding of part-of-speech tags and dependencies which performs better than a pipeline. Future work might consider exploiting a larger number of treebanks, and more powerful techniques for combining models than sim</context>
</contexts>
<marker>Headden, Johnson, McClosky, 2009</marker>
<rawString>W. P. Headden, M. Johnson, and D. McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>T L Griffiths</author>
<author>S Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="1025" citStr="Johnson et al., 2007" startWordPosition="148" endWordPosition="151">d data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing. 1 Introduction A major focus of recent NLP research has involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., </context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>M. Johnson, T. L. Griffiths, and S. Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1003" citStr="Klein and Manning, 2004" startWordPosition="144" endWordPosition="147">available, using annotated data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing. 1 Introduction A major focus of recent NLP research has involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2</context>
<context position="5584" citStr="Klein and Manning (2004)" startWordPosition="854" endWordPosition="857">ne-grained to coarse-grained tags, A` : F` → C. Our approach can be summarized using the following steps for a given task: 1. Select a set of L helper languages for which there exists annotated data (D1, ... , DL). Here, we use treebanks in these languages. 2. For all E E {1, ... , L}, convert the examples in D` by applying A` to every POS tag in the data, resulting in ˜D`. Estimate the parameters of a probabilistic model using ˜D`. In this work, such models are generative probabilistic models based on multinomial distributions,2 including an HMM and the dependency model with valence (DMV) of Klein and Manning (2004). Denote the subset of parameters that are unlexicalized by 0(`). (Lexicalized parameters will be denoted 77(`).) 2In §4 we also consider a feature-based parametrization. 3. For the target language, define the set of valid unlexicalized parameters ( ) L L O = 0 ~~0k =XO`,k0V) O`,k = 1,0 ≥ 0 , `=1 `=1 (1) for each group of parameters k, and maximize likelihood over that set, using the target-language unannotated data U. Because the syntactic categories referenced by each 0(`) and all models in O are in C, the models will be in the same parametric family. (Figure 1 gives a graphical interpretati</context>
<context position="29650" citStr="Klein and Manning, 2004" startWordPosition="4901" endWordPosition="4904">either Uniform+DG or Mixture+DG outperforms the monolingual baseline (Table 2a). For six of these eight languages, the latter model where the mixture coefficients are learned automatically fares better than uniform weighting. With a tag dictionary, the multilingual variants outperform the baseline in seven out of ten cases, and the learned mixture outperforms or matches the uniform mixture in five of those seven (Table 2b). 6.4 Dependency Grammar Induction We next describe experiments for dependency grammar induction. As the basic grammatical model, we adopt the dependency model with valence (Klein and Manning, 2004), which forms the basis for stateof-the-art results for dependency grammar induction in various settings (Cohen and Smith, 2009; Spitkovsky et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010). As shown in Table 3, DMV obtains much higher accuracy in the supervised setting than the unsupervised setting, suggesting that more can be achieved with this model family.9 For this reason, and because DMV is easily interpreted as a PCFG, it is our starting point and baseline. We consider four conditions. The independent variables are (1) whether we use uniform β (all set to 4) or e</context>
<context position="31924" citStr="Klein and Manning (2004)" startWordPosition="5302" endWordPosition="5305">5 75.5 41.3 45.9 51.3 61.5 Mixture+EM 79.8 44.1 72.8 63.9 72.3 68.7 76.7 41.0 46.0 55.2 62.1 EM (K &amp; M, 2004) 42.5 36.3 54.3 43.0 41.0 42.3 38.1 37.0 38.6 41.4 41.4 PR (G et al., ’10) 47.8 53.4 54.0 60.2 - 42.2 62.4 50.3 37.9 44.0 - Phylo. (B-K &amp; K, ’10) 63.1 - - - - 58.3 63.8 49.6 45.1 41.6 - Supervised (MLE) 81.7 75.7 83.0 89.2 81.8 83.2 79.0 74.5 64.8 80.8 79.3 Table 3: Results for dependency grammar induction given gold-standard POS tags, reported as attachment accuracy (fraction of parents which are correct). The three existing methods are: our replication of EM with the initializer from Klein and Manning (2004), denoted “EM”; reported results from Gillenwater et al. (2010) for posterior regularization (“PR”); and reported results from Berg-Kirkpatrick and Klein (2010), denoted “Phylo.” “Supervised (MLE)” are oracle results of estimating parameters from gold-standard annotated data using maximum likelihood estimation. “Avg” denotes macro-average across the ten languages. Figure 2: Projection of the learned mixture coefficients through PCA. In green, Japanese. In red, Dutch, Danish and Swedish. In blue, Bulgarian and Slovene. In magenta, Portuguese and Spanish. In black, Greek. In cyan, Turkish. relat</context>
<context position="37021" citStr="Klein and Manning (2004)" startWordPosition="6104" endWordPosition="6107">lingually guided partof-speech tagger for dependency grammar induction. For Turkish, Japanese, Slovene and Dutch, our unsupervised learner from words outperforms unsupervised parsing using gold-standard part-of-speech tags. We note that some recent work gives a treatment to unsupervised parsing (but not of dependencies) 10The decoding method on test data (Joint or Pipeline) was matched to the training method, though they are orthogonal in principle. directly from words (Seginer, 2007). Earlier work that induced part-of-speech tags and then performed unsupervised parsing in a pipeline includes Klein and Manning (2004) and Smith (2006). Headden et al. (2009) described the use of a lexicalized variant of the DMV model, with the use of gold part-ofspeech tags. 7 Conclusion We presented an approach to exploiting annotated data in helper languages to infer part-of-speech tagging and dependency parsing models in a different, target language, without parallel data. Our approach performs well in many cases. We also described a way to do joint decoding of part-of-speech tags and dependencies which performs better than a pipeline. Future work might consider exploiting a larger number of treebanks, and more powerful </context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<journal>Math. Programming,</journal>
<pages>45--503</pages>
<contexts>
<context position="12745" citStr="Liu and Nocedal, 1989" startWordPosition="2125" endWordPosition="2128">rameterized according to Eq. 4 can be recovered by marginalizing out the “g” features. We will refer to the model with these new set of features as “the extended model.” 4 Inference and Parameter Estimation The main building block commonly required for unsupervised learning in NLP is that of computing feature expectations for a given model. These feature expectations can be used with an algorithm such as expectation-maximization (where the expectations are normalized to obtain a new set of multinomial weights) or with other gradient based log-likelihood optimization algorithms such as L-BFGS (Liu and Nocedal, 1989) for feature-rich models. Estimating Multinomial Distributions Given a surface form x, a multinomial k and an event i in the multinomial, “feature expectation” refers to the calculation of the following quantities (in the extended model): E[f�,k,i(x, y)] = Ey p(x, y |θ)f�,k,i(x, y) (6) E[gt,k(x, y)] = Ey p(x, y |θ)gt,k(x, y) (7) These feature expectations can usually be computed using algorithms such as the forward-backward algorithm for hidden Markov models, or more generally, the inside-outside algorithm for PCFGs. In this paper, however, the task of estimation is different than the traditio</context>
<context position="15098" citStr="Liu and Nocedal, 1989" startWordPosition="2507" endWordPosition="2510">ures looking at event i in context k. For such a feature-rich model, our multilingual modeling framework still substitutes θ with a mixture of supervised multinomials for L helper languages as in Eq. 4. However, for computational β(t) �,k = E[gt,k(x, y)] E (8) � E[g�,k(x, y)] , 53 convenience, we also reparametrize the mixture coefficients β: exp γ`,k L (10) ,`1=1 exp γ`1,k Here, each γ`,k is an unconstrained parameter, and the above “softmax” transformation ensures that β lies within the probability simplex for context k. This is done so that a gradient-based optimization method like L-BFGS (Liu and Nocedal, 1989) can be used to estimate -y without having to worry about additional simplex constraints. For optimization, derivatives of the data log-likelihood with respect to -y need to be computed. We calculate the derivatives following Berg-Kirkpatrick et al. (2010, §3.1), making use of feature expectations, calculated exactly as before. In addition to these estimation techniques, which are based on the optimization of the log-likelihood, we also consider a trivially simple technique for estimating β: setting βl,k to the uniform weight L−1, where L is the number of helper languages. 5 Coarse-to-Fine Mul</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D. C. Liu and J. Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Math. Programming, 45:503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="17654" citStr="Marcus et al., 1993" startWordPosition="2921" endWordPosition="2924">tandard methods. 6 Experiments and Results In this section, we describe the experiments undertaken and the results achieved. We first note the characteristics of the datasets and the universal POS tags used in multilingual modeling. 6.1 Data For our experiments, we fixed a set of four helper languages with relatively large amounts of data, displaying nontrivial linguistic diversity: Czech (Slavic), English (West-Germanic), German (WestGermanic), and Italian (Romance). The datasets are the CoNLL-X shared task data for Czech and German (Buchholz and Marsi, 2006),5 the Penn Treebank for English (Marcus et al., 1993), and the CoNLL 2007 shared task data for Italian (Montemagni et al., 2003). This was the only set of helper languages we tested; improvements are likely possible. We leave an exploration of helper language choice (a subset selection problem) to future research, instead demonstrating that the concept has merit. We considered ten target languages: Bulgarian (Bg), Danish (Da), Dutch (Nl), Greek (El), Japanese (Jp), Portuguese (Pt), Slovene (Sl), Spanish (Es), Swedish (Sv), and Turkish (Tr). The data come from the CoNLL-X and CoNLL 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007).</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>S Petrov</author>
<author>K Hall</author>
</authors>
<title>Multi-source transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1630" citStr="McDonald et al., 2011" startWordPosition="245" endWordPosition="249">nson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annotated data (the target la</context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>R. McDonald, S. Petrov, and K. Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Compulational Lingustics,</journal>
<volume>20</volume>
<issue>2</issue>
<pages>72</pages>
<contexts>
<context position="20444" citStr="Merialdo, 1994" startWordPosition="3394" endWordPosition="3395">nclude, these 12 categories (or a subset) cover the most frequent parts of speech and exist in one form or another in all of the languages that we studied. For each language in our data, a mapping from the fine-grained treebank POS tags to these universal POS tags was constructed manually by Petrov et al. (2011). 6.3 Part-of-Speech Tagging Our first experimental task is POS tagging, and here we describe the specific details of the model, training and inference and the results attained. 6.3.1 Model The model is a hidden Markov model (HMM), which has been popular for unsupervised tagging tasks (Merialdo, 1994; Elworthy, 1994; Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010).6 We use a bigram model and a locally normalized loglinear parameterization, like Berg-Kirkpatrick et al. (2010). These locally normalized log-linear models can look at various aspects of the observation x given a tag y, or the pair of tags in a transition, incorporating overlapping features. In basic monolin6HMMs can be understood as a special case of PCFGs. gual experiments, we used the same set of features as Berg-Kirkpatrick et al. (2010). For the transition log-linear model, Berg-Kirkpatrick et al. (2010) used only a</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>B. Merialdo. 1994. Tagging English text with a probabilistic model. Compulational Lingustics, 20(2):155– 72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Montemagni</author>
<author>F Barsotti</author>
<author>M Battista</author>
<author>N Calzolari</author>
<author>O Corazzari</author>
<author>A Zampolli</author>
<author>F Fanciulli</author>
<author>M Massetani</author>
<author>R Raffaelli</author>
<author>R Basili</author>
<author>M T Pazienza</author>
<author>D Saracino</author>
<author>F Zanzotto</author>
<author>N Mana</author>
<author>F Pianesi</author>
<author>R Delmonte</author>
</authors>
<title>Building the Italian Syntactic-Semantic Treebank. In Building and using Parsed Corpora, Language and Speech Series.</title>
<date>2003</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="17729" citStr="Montemagni et al., 2003" startWordPosition="2934" endWordPosition="2938"> the experiments undertaken and the results achieved. We first note the characteristics of the datasets and the universal POS tags used in multilingual modeling. 6.1 Data For our experiments, we fixed a set of four helper languages with relatively large amounts of data, displaying nontrivial linguistic diversity: Czech (Slavic), English (West-Germanic), German (WestGermanic), and Italian (Romance). The datasets are the CoNLL-X shared task data for Czech and German (Buchholz and Marsi, 2006),5 the Penn Treebank for English (Marcus et al., 1993), and the CoNLL 2007 shared task data for Italian (Montemagni et al., 2003). This was the only set of helper languages we tested; improvements are likely possible. We leave an exploration of helper language choice (a subset selection problem) to future research, instead demonstrating that the concept has merit. We considered ten target languages: Bulgarian (Bg), Danish (Da), Dutch (Nl), Greek (El), Japanese (Jp), Portuguese (Pt), Slovene (Sl), Spanish (Es), Swedish (Sv), and Turkish (Tr). The data come from the CoNLL-X and CoNLL 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). For all the experiments conducted, we trained models on the training secti</context>
</contexts>
<marker>Montemagni, Barsotti, Battista, Calzolari, Corazzari, Zampolli, Fanciulli, Massetani, Raffaelli, Basili, Pazienza, Saracino, Zanzotto, Mana, Pianesi, Delmonte, 2003</marker>
<rawString>S. Montemagni, F. Barsotti, M. Battista, N. Calzolari, O. Corazzari, A. Zampolli, F. Fanciulli, M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza, D. Saracino, F. Zanzotto, N. Mana, F. Pianesi, and R. Delmonte. 2003. Building the Italian Syntactic-Semantic Treebank. In Building and using Parsed Corpora, Language and Speech Series. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Naseem</author>
<author>B Snyder</author>
<author>J Eisenstein</author>
<author>R Barzilay</author>
</authors>
<title>Multilingual part-of-speech tagging: Two unsupervised approaches.</title>
<date>2009</date>
<journal>JAIR,</journal>
<volume>36</volume>
<contexts>
<context position="1744" citStr="Naseem et al., 2009" startWordPosition="263" endWordPosition="266">ch has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annotated data (the target language). Unlike the previous work mentioned above, our framework does not rely on parallel data in any form. This </context>
</contexts>
<marker>Naseem, Snyder, Eisenstein, Barzilay, 2009</marker>
<rawString>T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay. 2009. Multilingual part-of-speech tagging: Two unsupervised approaches. JAIR, 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Naseem</author>
<author>H Chen</author>
<author>R Barzilay</author>
<author>M Johnson</author>
</authors>
<title>Using universal linguistic knowledge to guide grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3803" citStr="Naseem et al. (2010)" startWordPosition="569" endWordPosition="572">, 2009) or models of parameter 1Although the stated objective is often to build systems for resource-poor languages and domains, for evaluation purposes, annotated treebank test data figure prominently in this research (including in this paper). Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50–61, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics drift down phylogenetic trees (Berg-Kirkpatrick and Klein, 2010) is comparable, but the practical assumption of supervised helper languages is new to this work. Naseem et al. (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert handwritten rules as constraints. Herein, we specifically focus on two problems in linguistic structure prediction: unsupervised POS tagging and unsupervised dependency grammar induction. Our experiments demonstrate that the presented method outperforms strong state-of-the-art unsupervised baselines for both tasks. Our approach can be applied to other problems in which a subset of the model parameters can be linked across languages. We also experiment with unsupervised learning of depen</context>
</contexts>
<marker>Naseem, Chen, Barzilay, Johnson, 2010</marker>
<rawString>T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010. Using universal linguistic knowledge to guide grammar induction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S K¨ubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
</authors>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, </marker>
<rawString>J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson,</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<marker>Riedel, Yuret, 2007</marker>
<rawString>S. Riedel, and D. Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Das</author>
<author>R McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2011</date>
<journal>ArXiv:1104.2086.</journal>
<contexts>
<context position="19738" citStr="Petrov et al. (2011)" startWordPosition="3272" endWordPosition="3275">s the number of fine POS tags in each language including punctuations. 6.2 Universal POS Tags Our coarse-grained, universal POS tag set consists of the following 12 tags: NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner), ADP (preposition or postposition), NUM (numeral), CONJ (conjunction), PRT (particle), PUNC (punctuation mark) and X (a catch-all for other categories such as abbreviations or foreign words). These follow recent work by Das and Petrov (2011) on unsupervised POS tagging in a multilingual setting with parallel data, and have been described in detail by Petrov et al. (2011). While there might be some controversy about what an appropriate universal tag set should include, these 12 categories (or a subset) cover the most frequent parts of speech and exist in one form or another in all of the languages that we studied. For each language in our data, a mapping from the fine-grained treebank POS tags to these universal POS tags was constructed manually by Petrov et al. (2011). 6.3 Part-of-Speech Tagging Our first experimental task is POS tagging, and here we describe the specific details of the model, training and inference and the results attained. 6.3.1 Model The m</context>
</contexts>
<marker>Petrov, Das, McDonald, 2011</marker>
<rawString>S. Petrov, D. Das, and R. McDonald. 2011. A universal part-of-speech tagset. ArXiv:1104.2086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Seginer</author>
</authors>
<title>Fast unsupervised incremental parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="36886" citStr="Seginer, 2007" startWordPosition="6087" endWordPosition="6088"> multilingual guidance outperforms the completely unsupervised baseline, there is not always an advantage of using this multilingually guided partof-speech tagger for dependency grammar induction. For Turkish, Japanese, Slovene and Dutch, our unsupervised learner from words outperforms unsupervised parsing using gold-standard part-of-speech tags. We note that some recent work gives a treatment to unsupervised parsing (but not of dependencies) 10The decoding method on test data (Joint or Pipeline) was matched to the training method, though they are orthogonal in principle. directly from words (Seginer, 2007). Earlier work that induced part-of-speech tags and then performed unsupervised parsing in a pipeline includes Klein and Manning (2004) and Smith (2006). Headden et al. (2009) described the use of a lexicalized variant of the DMV model, with the use of gold part-ofspeech tags. 7 Conclusion We presented an approach to exploiting annotated data in helper languages to infer part-of-speech tagging and dependency parsing models in a different, target language, without parallel data. Our approach performs well in many cases. We also described a way to do joint decoding of part-of-speech tags and dep</context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Y. Seginer. 2007. Fast unsupervised incremental parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="20484" citStr="Smith and Eisner, 2005" startWordPosition="3398" endWordPosition="3401">a subset) cover the most frequent parts of speech and exist in one form or another in all of the languages that we studied. For each language in our data, a mapping from the fine-grained treebank POS tags to these universal POS tags was constructed manually by Petrov et al. (2011). 6.3 Part-of-Speech Tagging Our first experimental task is POS tagging, and here we describe the specific details of the model, training and inference and the results attained. 6.3.1 Model The model is a hidden Markov model (HMM), which has been popular for unsupervised tagging tasks (Merialdo, 1994; Elworthy, 1994; Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010).6 We use a bigram model and a locally normalized loglinear parameterization, like Berg-Kirkpatrick et al. (2010). These locally normalized log-linear models can look at various aspects of the observation x given a tag y, or the pair of tags in a transition, incorporating overlapping features. In basic monolin6HMMs can be understood as a special case of PCFGs. gual experiments, we used the same set of features as Berg-Kirkpatrick et al. (2010). For the transition log-linear model, Berg-Kirkpatrick et al. (2010) used only a single indicator feature of a tag pair,</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N. A. Smith and J. Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous grammar features.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1584" citStr="Smith and Eisner, 2009" startWordPosition="237" endWordPosition="240"> and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another lan</context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>D. A. Smith and J. Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar features. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>N A Smith</author>
</authors>
<title>Bilingual parsing with factored estimation: Using English to parse Korean.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1667" citStr="Smith and Smith, 2004" startWordPosition="252" endWordPosition="255">t al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annotated data (the target language). Unlike the previous work men</context>
</contexts>
<marker>Smith, Smith, 2004</marker>
<rawString>D. A. Smith and N. A. Smith. 2004. Bilingual parsing with factored estimation: Using English to parse Korean. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
</authors>
<title>Novel Estimation Methods for Unsupervised Discovery of Latent Structure in Natural Language Text.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="37038" citStr="Smith (2006)" startWordPosition="6109" endWordPosition="6110">h tagger for dependency grammar induction. For Turkish, Japanese, Slovene and Dutch, our unsupervised learner from words outperforms unsupervised parsing using gold-standard part-of-speech tags. We note that some recent work gives a treatment to unsupervised parsing (but not of dependencies) 10The decoding method on test data (Joint or Pipeline) was matched to the training method, though they are orthogonal in principle. directly from words (Seginer, 2007). Earlier work that induced part-of-speech tags and then performed unsupervised parsing in a pipeline includes Klein and Manning (2004) and Smith (2006). Headden et al. (2009) described the use of a lexicalized variant of the DMV model, with the use of gold part-ofspeech tags. 7 Conclusion We presented an approach to exploiting annotated data in helper languages to infer part-of-speech tagging and dependency parsing models in a different, target language, without parallel data. Our approach performs well in many cases. We also described a way to do joint decoding of part-of-speech tags and dependencies which performs better than a pipeline. Future work might consider exploiting a larger number of treebanks, and more powerful techniques for co</context>
</contexts>
<marker>Smith, 2006</marker>
<rawString>N. A. Smith. 2006. Novel Estimation Methods for Unsupervised Discovery of Latent Structure in Natural Language Text. Ph.D. thesis, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>R Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for morphological segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1723" citStr="Snyder and Barzilay, 2008" startWordPosition="259" endWordPosition="262">ts purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annotated data (the target language). Unlike the previous work mentioned above, our framework does not rely on parallel da</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>B. Snyder and R. Barzilay. 2008. Unsupervised multilingual learning for morphological segmentation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>T Naseem</author>
<author>R Barzilay</author>
</authors>
<title>Unsupervised multilingual grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP.</booktitle>
<marker>Snyder, Naseem, Barzilay, 2009</marker>
<rawString>B. Snyder, T. Naseem, and R. Barzilay. 2009. Unsupervised multilingual grammar induction. In Proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>From baby steps to leapfrog: How “less is more” in unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="29802" citStr="Spitkovsky et al., 2010" startWordPosition="4925" endWordPosition="4928">fficients are learned automatically fares better than uniform weighting. With a tag dictionary, the multilingual variants outperform the baseline in seven out of ten cases, and the learned mixture outperforms or matches the uniform mixture in five of those seven (Table 2b). 6.4 Dependency Grammar Induction We next describe experiments for dependency grammar induction. As the basic grammatical model, we adopt the dependency model with valence (Klein and Manning, 2004), which forms the basis for stateof-the-art results for dependency grammar induction in various settings (Cohen and Smith, 2009; Spitkovsky et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010). As shown in Table 3, DMV obtains much higher accuracy in the supervised setting than the unsupervised setting, suggesting that more can be achieved with this model family.9 For this reason, and because DMV is easily interpreted as a PCFG, it is our starting point and baseline. We consider four conditions. The independent variables are (1) whether we use uniform β (all set to 4) or estimate them using EM (as described in §4), 1 and (2) whether we simply use the mixture model to decode the test data, or to initialize EM for the DMV. </context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2010</marker>
<rawString>V. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010. From baby steps to leapfrog: How “less is more” in unsupervised dependency parsing. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Xi</author>
<author>R Hwa</author>
</authors>
<title>A backoff model for bootstrapping resources for non-English languages.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<contexts>
<context position="1508" citStr="Xi and Hwa, 2005" startWordPosition="225" endWordPosition="228">s involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data f</context>
</contexts>
<marker>Xi, Hwa, 2005</marker>
<rawString>C. Xi and R. Hwa. 2005. A backoff model for bootstrapping resources for non-English languages. In Proceedings of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>G Ngai</author>
</authors>
<title>Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In</title>
<date>2001</date>
<booktitle>Proceedings of NAACL.</booktitle>
<contexts>
<context position="1489" citStr="Yarowsky and Ngai, 2001" startWordPosition="221" endWordPosition="224">of recent NLP research has involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to usi</context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>D. Yarowsky and G. Ngai. 2001. Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In Proceedings of NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>