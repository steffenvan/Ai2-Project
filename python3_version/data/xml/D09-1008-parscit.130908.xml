<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011374">
<title confidence="0.9915465">
Effective Use of Linguistic and Contextual Information
for Statistical Machine Translation
</title>
<note confidence="0.549476">
Libin Shen and Jinxi Xu and Bing Zhang and
Spyros Matsoukas and Ralph Weischedel
BBN Technologies
</note>
<address confidence="0.897403">
Cambridge, MA 02138, USA
</address>
<email confidence="0.998514">
{lshen,jxu,bzhang,smatsouk,weisched}@bbn.com
</email>
<sectionHeader confidence="0.99479" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999238318181818">
Current methods of using lexical features
in machine translation have difficulty in
scaling up to realistic MT tasks due to
a prohibitively large number of parame-
ters involved. In this paper, we propose
methods of using new linguistic and con-
textual features that do not suffer from
this problem and apply them in a state-of-
the-art hierarchical MT system. The fea-
tures used in this work are non-terminal
labels, non-terminal length distribution,
source string context and source depen-
dency LM scores. The effectiveness of
our techniques is demonstrated by signif-
icant improvements over a strong base-
line. On Arabic-to-English translation,
improvements in lower-cased BLEU are
2.0 on NIST MT06 and 1.7 on MT08
newswire data on decoding output. On
Chinese-to-English translation, the im-
provements are 1.0 on MT06 and 0.8 on
MT08 newswire data.
</bodyText>
<sectionHeader confidence="0.998891" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976461538462">
Linguistic and context features, especially sparse
lexical features, have been widely used in re-
cent machine translation (MT) research. Unfor-
tunately, existing methods of using such features
are not ideal for large-scale, practical translation
tasks.
In this paper, we will propose several prob-
abilistic models to effectively exploit linguistic
and contextual information for MT decoding, and
these new features do not suffer from the scalabil-
ity problem. Our new models are tested on NIST
MT06 and MT08 data, and they provide signifi-
cant improvement over a strong baseline system.
</bodyText>
<subsectionHeader confidence="0.961194">
1.1 Previous Work
</subsectionHeader>
<bodyText confidence="0.999929846153846">
The ideas of using labels, length preference and
source side context in MT decoding were explored
previously. Broadly speaking, two approaches
were commonly used in existing work.
One is to use a stochastic gradient descent
(SGD) or Perceptron like online learning algo-
rithm to optimize the weights of these features
directly for MT (Shen et al., 2004; Liang et al.,
2006; Tillmann and Zhang, 2006). This method is
very attractive, since it opens the door to rich lex-
ical features. However, in order to robustly opti-
mize the feature weights, one has to use a substan-
tially large development set, which results in sig-
nificantly slower tuning. Alternatively, one needs
to carefully select a development set that simulates
the test set to reduce the risk of over-fitting, which
however is not always realistic for practical use.
A remedy is to aggressively limit the feature
space, e.g. to syntactic labels or a small fraction
of the bi-lingual features available, as in (Chiang
et al., 2008; Chiang et al., 2009), but that reduces
the benefit of lexical features. A possible generic
solution is to cluster the lexical features in some
way. However, how to make it work on such a
large space of bi-lingual features is still an open
question.
The other approach is to estimate a single score
or likelihood of a translation with rich features,
for example, with the maximum entropy (Max-
Ent) method as in (Carpuat and Wu, 2007; Itty-
cheriah and Roukos, 2007; He et al., 2008). This
method avoids the over-fitting problem, at the ex-
pense of losing the benefit of discriminative train-
ing of rich features directly for MT. However, the
feature space problem still exists in these pub-
lished models.
He et al. (2008) extended the WSD-like ap-
proached proposed in (Carpuat and Wu, 2007) to
hierarchical decoders. In (He et al., 2008), lexical
</bodyText>
<page confidence="0.981018">
72
</page>
<note confidence="0.9966115">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 72–80,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999826714285714">
features were limited on each single side due to the
feature space problem. In order to further reduce
the complexity of MaxEnt training, they “trained
a MaxEnt model for each ambiguous hierarchical
LHS” (left-hand side or source side) of translation
rules. Different target sides were treated as possi-
ble labels. Therefore, the sample sets of each indi-
vidual MaxEnt model were very small, while the
number of features could easily exceed the number
of samples. Furthermore, optimizing individual
MaxEnt models in this way does not lead to global
maximum. In addition, MaxEnt models trained on
small sets are unstable.
The MaxEnt model in (Ittycheriah and Roukos,
2007) was optimized globally, so that it could bet-
ter employ the distribution of the training data.
However, one has to filter the training data ac-
cording to the test data to get competitive perfor-
mance with this model 1. In addition, the filtering
method causes some practical issues. First, such
methods are not suitable for real MT tasks, espe-
cially for applications with streamed input, since
the model has to be retrained with each new input
sentence or document and training is slow. Fur-
thermore, the model is ill-posed. The translation
of a source sentence depends on other source sen-
tences in the same batch with which the MaxEnt
model is trained. If we add one more sentence to
the batch, translations of other sentences may be-
come different due to the change of the MaxEnt
model.
To sum up, the existing models of employing
rich bi-lingual lexical information in MT are im-
perfect. Many of them are not ideal for practical
translation tasks.
</bodyText>
<subsectionHeader confidence="0.996568">
1.2 Our Approach
</subsectionHeader>
<bodyText confidence="0.9999023">
As for our approach, we mainly use simple proba-
bilistic models, i.e. Gaussian and n-gram models,
which are more robust and suitable for large-scale
training of real data, as manifested in state-of-the-
art systems of speech recognition. The unique
contribution of our work is to design effective and
efficient statistical models to capture useful lin-
guistic and context information for MT decoding.
Feature functions defined in this way are robust
and ideal for practical translation tasks.
</bodyText>
<footnote confidence="0.837969">
1According to footnote 2 of (Ittycheriah and Roukos,
2007), test set adaptation by test set sampling of the train-
ing corpus showed an advantage of more than 2 BLEU points
over a general system trained on all data.
</footnote>
<sectionHeader confidence="0.455615" genericHeader="introduction">
1.2.1 Features
</sectionHeader>
<bodyText confidence="0.999968361702128">
In this paper, we will introduce four new linguistic
and contextual feature functions. Here, we first
provide a high-level description of these features.
Details of the features are discussed in Section 2.
The first feature is based on non-terminal labels,
i.e. POS tags of the head words of target non-
terminals in transfer rules. This feature reduces
the ambiguity of translation rules. The other bene-
fit is that POS tags help to weed out bad target side
tree structures, as an enhancement to the target de-
pendency language model.
The second feature is based on the length dis-
tribution of non-terminals. In English as well as
in other languages, the same deep structure can
be represented in different syntactic structures de-
pending on the complexity of its constituents. We
model such preferences by associating each non-
terminal of a transfer rule with a probability distri-
bution over its length. Similar ideas were explored
in (He et al., 2008). However their length features
only provided insignificant improvement of 0.1
BLEU point. A crucial difference of our approach
is how the length preference is modeled. We ap-
proximate the length distribution of non-terminals
with a smoothed Gaussian, which is more robust
and gives rise to much larger improvement consis-
tently.
The third feature utilizes source side context in-
formation, i.e. the neighboring words of an input
span, to influence the selection of the target trans-
lation for a span. While the use of context infor-
mation has been explored in MT, e.g. (Carpuat
and Wu, 2007) and (He et al., 2008), the specific
technique we used by means of a context language
model is rather different. Our model is trained on
the whole training data, and it is not limited by the
constraint of MaxEnt training.
The fourth feature exploits structural informa-
tion on the source side. Specifically, the decoder
simultaneously generates both the source and tar-
get side dependency trees, and employs two de-
pendency LMs, one for the source and the other
for the target, for scoring translation hypotheses.
Our intuition is that the likelihood of source struc-
tures provides another piece of evidence about the
plausibility of a translation hypothesis and as such
would help weed out bad ones.
</bodyText>
<page confidence="0.997745">
73
</page>
<subsectionHeader confidence="0.9564105">
1.2.2 Baseline System and Experimental
Setup
</subsectionHeader>
<bodyText confidence="0.999931333333333">
We take BBN’s HierDec, a string-to-dependency
decoder as described in (Shen et al., 2008), as our
baseline for the following two reasons:
</bodyText>
<listItem confidence="0.9508677">
• It provides a strong baseline, which ensures
the validity of the improvement we would ob-
tain. The baseline model used in this paper
showed state-of-the-art performance at NIST
2008 MT evaluation.
• The baseline algorithm can be easily ex-
tended to incorporate the features proposed
in this paper. The use of source dependency
structures is a natural extension of the string-
to-tree model to a tree-to-tree model.
</listItem>
<bodyText confidence="0.9997521">
To ensure the generality of our results, we tested
the features on two rather different language pairs,
Arabic-to-English and Chinese-to-English, using
two metrics, IBM BLEU (Papineni et al., 2001)
and TER (Snover et al., 2006). Our experiments
show that each of the first three features: non-
terminal labels, length distribution and source side
context, improves MT performance. Surprisingly,
the source dependency feature does not produce
an improvement.
</bodyText>
<sectionHeader confidence="0.935846" genericHeader="method">
2 Linguistic and Context Features
</sectionHeader>
<subsectionHeader confidence="0.857545">
2.1 Non-terminal Labels
</subsectionHeader>
<bodyText confidence="0.992925076923077">
In the original string-to-dependency model (Shen
et al., 2008), a translation rule is composed of a
string of words and non-terminals on the source
side and a well-formed dependency structure on
the target side. A well-formed dependency struc-
ture could be either a single-rooted dependency
tree or a set of sibling trees. As in the Hiero system
(Chiang, 2007), there is only one non-terminal X
in the string-to-dependency model. Any sub de-
pendency structure can be used to replace a non-
terminal in a rule.
For example, we have a source sentence in Chi-
nese as follows.
</bodyText>
<listItem confidence="0.9771812">
• jiantao zhuyao baohan liang fangmian
The literal translation for individual words is
• ’review’ ’mainly’ ’to consist of’ ’two’ ’part’
The reference translation is
• the review mainly consists of two parts
</listItem>
<bodyText confidence="0.999925">
A single source word can be translated into
many English words. For example, jiantao can
be translated into a review, the review, reviews,
the reviews, reviewing, reviewed, etc. Suppose
we have source-string-to-target-dependency trans-
lation rules as shown in Figure 1. Since there is
no constraint on substitution, any translation for
jiantao could replace the X-1 slot.
One way to alleviate this problem is to limit the
search space by using a label system. We could
assign a label to each non-terminal on the target
side of the rules. Furthermore, we could assign a
label to the whole target dependency structure, as
shown in Figure 2. In decoding, each target de-
pendency sub-structure would be associated with
a label. Whenever substitution happens, we would
check whether the label of the sub-structure and
the label of the slot are the same. Substitutions
with unmatched labels would be prohibited.
In practice, we use a soft constraint by penaliz-
ing substitutions with unmatched labels. We intro-
duce a new feature: the number of times substitu-
tions with unmatched labels appear in the deriva-
tion of a translation hypothesis.
Obviously, to implement this feature we need to
associate a label with each non-terminal in the tar-
get side of a translation rule. The labels are gen-
erated during rule extraction. When we create a
rule from a training example, we replace a sub-
tree or dependency structure with a non-terminal
and associate it with the POS tag of the head word
if the non-terminal corresponds to a single-rooted
tree on the target side. Otherwise, it is assigned
the generic label X. (In decoding, all substitutions
of X are considered unmatched ones and incur a
penalty.)
</bodyText>
<subsectionHeader confidence="0.999036">
2.2 Length Distribution
</subsectionHeader>
<bodyText confidence="0.999967538461538">
In English, the length of a phrase may determine
the syntactic structure of a sentence. For example,
possessive relations can be represented either as
“A’s B” or “B of A”. The former is preferred if A
is a short phrase (e.g. “the boy’s mother”) while
the latter is preferred if A is a complex structure
(e.g. “the mother of the boy who is sick”).
Our solution is to build a model of length prefer-
ence for each non-terminal in each translation rule.
To address data sparseness, we assume the length
distribution of each non-terminal in a transfer rule
is a Gaussian, whose mean and variance can be
estimated from the training data. In rule extrac-
</bodyText>
<page confidence="0.982368">
74
</page>
<figure confidence="0.998453">
X
X X−1 zhuyao baohan
X−1
mainly of
consists
X−2
X−2
review
X
the
X
jiantao
reviews
X
the
X
jiantao
</figure>
<figureCaption confidence="0.999568">
Figure 1: Translation rules with one label X
Figure 2: Translation rules with multiple labels
</figureCaption>
<figure confidence="0.998350538461539">
consists
NN−1 mainly of NNS−2
X X−1 zhuyao baohan
X−2
VBZ
review
NN
the
X jiantao
reviews
NNS
the
X jiantao
</figure>
<bodyText confidence="0.999509333333333">
tion, each time a translation rule is generated from
a training example, we can record the length of the
source span corresponding to a non-terminal. In
the end, we have a frequency histogram for each
non-terminal in each translation rule. From the
histogram, a Gaussian distribution can be easily
computed.
In practice, we do not need to collect the fre-
quency histogram. Since all we need to know are
the mean and the variance, it is sufficient to col-
lect the sum of the length and the sum of squared
length.
Let r be a translation rule that occurs Nr times
in training. Let x be a specific non-terminal in that
rule. Let l(r, x, i) denote the length of the source
span corresponding to non-terminal x in the i-th
occurrence of rule r in training. Then, we can
compute the following quantities.
</bodyText>
<equation confidence="0.9998395">
l(r,x,i) (1)
l(r, x, i)2, (2)
</equation>
<bodyText confidence="0.987990666666667">
which can be subsequently used to estimate the
mean µr,x and variance σ2r,x of x’s length distri-
bution in rule r as follows.
</bodyText>
<equation confidence="0.840229">
µr,x = mr,x
2 = 2
σr,x sr,x − mr x
</equation>
<bodyText confidence="0.9858108">
Since many of the translation rules have few oc-
currences in training, smoothing of the above esti-
mates is necessary. A common smoothing method
is based on maximum a posteriori (MAP) estima-
tion as in (Gauvain and Lee, 1994).
</bodyText>
<equation confidence="0.994591166666667">
Nr
mr,x +
Nr + τ
Nr
sr,x +
Nr + τ
</equation>
<bodyText confidence="0.99990055">
where ˆ stands for an MAP distribution and ˜ rep-
resents a prior distribution. ˜mr,x and ˜sr,x can
be obtained from a prior Gaussian distribution
N(˜µr,x, ˜σr,x) via equations (3) and (4), and τ is
a weight of smoothing.
There are many ways to approximate the prior
distribution. For example, we can have one prior
for all the non-terminals or one for individual non-
terminal type. In practice, we assume ˜µr,x = µr,x,
and approximate ˜σr,x as (σ2r,x + sr,x)12 .
In this way, we do not change the mean, but
relax the variance with sr,x. We tried differ-
ent smoothing methods, but the performance did
not change much, therefore we kept this simplest
setup. We also tried the Poisson distribution, and
the performance is similar to Gaussian distribu-
tion, which is about 0.1 point lower in BLEU.
When a rule r is applied during decoding, we
compute a penalty for each non-terminal x in r
according to
</bodyText>
<equation confidence="0.883414666666667">
1
P(l  |r,x) = e
σr,x√2π
</equation>
<bodyText confidence="0.98088">
where l is length of source span corresponding to
x.
Our method to address the problem of length
bias in rule selection is very different from the
maximum entropy method used in existing stud-
ies, e.g. (He et al., 2008).
</bodyText>
<equation confidence="0.997660076923077">
1 Nr
mr,x = Nr Z=1
1 Nr
sr,x = Nr Z=1
ˆmr,x =
ˆsr,x =
τ
˜sr,x,
Nr + τ
τ
Nr + τ ˜mr,x
(l−µr,x)2
− 2σ2r,x ,
</equation>
<page confidence="0.98349">
75
</page>
<subsectionHeader confidence="0.984788">
2.3 Context Language Model
</subsectionHeader>
<bodyText confidence="0.99957425">
In the baseline string-to-dependency system, the
probability a translation rule is selected in decod-
ing does not depend on the sentence context. In
reality, translation is highly context dependent. To
address this defect, we introduce a new feature,
called context language model. The motivation of
this feature is to exploit surrounding words to in-
fluence the selection of the desired transfer rule for
a given input span.
To illustrate the problem, we use the same ex-
ample mentioned in Section 2.1. Suppose the
source span for rule selection is zhuyao baohan,
whose literal translation is mainly and to consist
of. There are many candidate translations for this
phrase, for example, mainly consist of, mainly
consists of, mainly including, mainly includes, etc.
The surrounding words can help to decide which
translation is more appropriate for zhuyao bao-
han. We compare the following two context-based
probabilities:
</bodyText>
<listItem confidence="0.999896">
• P( jiantao  |mainly consist )
• P( jiantao  |mainly consists )
</listItem>
<bodyText confidence="0.999755538461538">
Here, jiantao is the source word preceding the
source span zhuyao baohan.
In the training data, jiantao is usually trans-
lated into the review, third-person singular, then
the probability P( jiantao  |mainly consists ) will
be higher than P( jiantao  |mainly consist ), since
we have seen more context events like the former
in the training data.
Now we introduce context LM formally. Let the
source words be f1f2..fi..fj..fn. Suppose source
sub-string fi..fj is translated into ep..eq. We can
define tri-gram probabilities on the left and right
sides of the source span:
</bodyText>
<listItem confidence="0.9999285">
• left : PL(fi−1|ep, ep+1)
• right: PR(fj+1|eq, eq−1)
</listItem>
<bodyText confidence="0.99996932">
In our implementation, the left and right context
LMs are estimated from the training data as part
of the rule extraction procedure. When we exact a
rule, we collect two 3-gram events, one for the left
side and the other for the right side.
In decoding, whenever a partial hypothesis is
generated, we calculate the context LM scores
based on the leftmost two words and the rightmost
two words of the hypothesis as well as the source
context. The product of the left and right context
LM scores is used as a new feature in the scoring
function.
Please note that our approach is very different
from other approaches to context dependent rule
selection such as (Ittycheriah and Roukos, 2007)
and (He et al., 2008). Instead of using a large num-
ber of fine grained features with weights optimized
using the maximum entropy method, we treat con-
text dependency as an ngram LM problem, and it
is smoothed with Witten-Bell discounting. The es-
timation of the context LMs is very efficient and
robust.
The benefit is two fold. The estimation of the
context LMs is very efficient. It adds only one new
weight to the scoring function.
</bodyText>
<subsectionHeader confidence="0.996027">
2.4 Source Dependency Language Model
</subsectionHeader>
<bodyText confidence="0.999956269230769">
The context LM proposed in the previous sec-
tion only employs source words immediately be-
fore and after the current source span in decod-
ing. To exploit more source context, we use a
source side dependency language model as an-
other feature. The motivation is to take advantage
of the long distance dependency relations between
source words in scoring a translation theory.
We extended string-to-dependency rules in
the baseline system to dependency-to-dependency
rules. In each dependency-to-dependency rule, we
keep record of the source string as well as the
source dependency structure. Figure 3 shows ex-
amples of dependency-to-dependency rules.
We extended the string-to-dependency decod-
ing algorithm in the baseline to accommodate
dependency-to-dependency theories. In decoding,
we build both the source and the target depen-
dency structures simultaneously in chart parsing
over the source string. Thus, we can compute the
source dependency LM score in the same way we
compute the target side score, using a procedure
described in (Shen et al., 2008).
We introduce two new features for the source
side dependency LM as follows, in a way similar
to the target side.
</bodyText>
<listItem confidence="0.986479">
• Source dependency LM score
• Discount on ill-formed source dependency
structures
</listItem>
<bodyText confidence="0.976083">
The source dependency LM is trained on the
source side of the bi-lingual training data with
Witten-Bell smoothing. The source dependency
LM score represents the likelihood of the source
</bodyText>
<page confidence="0.90147">
76
</page>
<figure confidence="0.9971665">
X X−1
X
X−1 zhuyao X−2
mainly of
consists
baohan
X−2
review
X
X jiantao
the
reviews
X
the
X
jiantao
</figure>
<figureCaption confidence="0.999949">
Figure 3: Dependency-to-dependency translation rules
</figureCaption>
<bodyText confidence="0.999808777777778">
dependency tree generated by the decoder. The
source dependency tree with the highest score is
the one that is most likely to be generated by the
dependency model that created the source side of
the training data.
Source dependency trees are composed of frag-
ments embedded in the translation rules. There-
fore, a source dependency LM score can be
viewed as a measure whether the translation rules
are put together in a way similar to the training
data. Therefore, a source dependency LM score
serves as a feature to represent structural con-
text information that is capable of modeling long-
distance relations.
However, unlike source context LMs, the struc-
tural context information is used only when two
partial dependency structures are combined, while
source context LMs work as a look-ahead feature.
</bodyText>
<sectionHeader confidence="0.999578" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.988639333333333">
We designed our experiments to show the impact
of each feature separately as well as their cumula-
tive impact:
</bodyText>
<listItem confidence="0.999833222222222">
• BASE: baseline string-to-dependency system
• SLM: baseline + source dependency LM
• CLM: baseline + context LM
• LEN: baseline + length distribution
• LBL: baseline + syntactic labels
• LBL+LEN: baseline + syntactic labels +
length distribution
• LBL+LEN+CLM: baseline + syntactic labels
+ length distribution + context LM
</listItem>
<bodyText confidence="0.999798133333333">
All the models were optimized on lower-cased
IBM BLEU with Powell’s method (Powell, 1964;
Brent, 1973) on n-best translations (Ostendorf et
al., 1991), but evaluated on both IBM BLEU and
TER. The motivation is to detect if an improve-
ment is artificial, i.e., specific to the tuning met-
ric. For both Arabic-to-English and Chinese-to-
English MT, we tuned on NIST MT02-05 and
tested on MT06 and MT08 newswire sets.
The training data are different from what was
usd at MT06 or MT08. Our Arabic-to-English
data contain 29M Arabic words and 38M En-
glish words from 11 corpora: LDC2004T17,
LDC2004T18, LDC2005E46, LDC2006E25,
LDC2006G05, LDC2005E85, LDC2006E36,
LDC2006E82, LDC2006E95, Sakhr-A2E and
Sakhr-E2A. The Chinese-to-English data contain
107M Chinese words and 132M English words
from eight corpora: LDC2002E18, LDC2005T06,
LDC2005T10, LDC2006E26, LDC2006G05,
LDC2002L27, LDC2005T34 and LDC2003E07.
They are available under the DARPA GALE
program. Traditional 3-gram and 5-gram string
LMs were trained on the English side of the
parallel data plus the English Gigaword corpus
V3.0 in a way described in (Bulyko et al., 2007).
The target dependency LMs were trained on the
English side of the parallel training data. For that
purpose, we parsed the English side of the parallel
data. Two separate models were trained: one for
Arabic from the Arabic training data and the other
for Chinese from the Chinese training data.
To compute the source dependency LM for
Chinese-to-English MT, we parsed the Chinese
side of the Chinese-to-English parallel data. Due
to the lack of a good Arabic parser compatible
with the Sakhr tokenization that we used on the
source side, we did not test the source dependency
LM for Arabic-to-English MT.
When extracting rules with source dependency
structures, we applied the same well-formedness
constraint on the source side as we did on the tar-
get side, using a procedure described by (Shen
et al., 2008). Some candidate rules were thrown
away due to the source side constraint. On the
</bodyText>
<page confidence="0.997613">
77
</page>
<table confidence="0.999695647058824">
MT06 MT08
Model BLEU TER BLEU TER
lower mixed lower mixed lower mixed lower mixed
Decoding (3-gram LM)
BASE 48.75 46.74 43.43 45.79 49.58 47.46 42.80 45.08
CLM 49.44 47.36 42.96 45.22 49.73 47.53 42.64 44.92
LEN 49.37 47.28 43.01 45.35 50.29 48.19 42.32 44.45
LBL 49.33 47.07 43.09 45.53 50.46 48.19 42.27 44.57
LBL+LEN 49.91 47.70 42.59 45.17 51.10 48.85 41.88 44.16
LBL+LEN+CLM 50.75 48.51 42.13 44.50 51.24 49.10 41.63 43.80
Rescoring (5-gram LM)
BASE 51.24 49.23 42.08 44.42 51.23 49.11 42.01 44.15
CLM 51.57 49.54 41.74 43.88 51.44 49.37 41.63 43.74
LEN 52.05 50.01 41.50 43.72 51.88 49.89 41.51 43.47
LBL 51.80 49.69 41.54 43.76 51.93 49.86 41.27 43.33
LBL+LEN 51.90 49.76 41.41 43.70 52.42 50.29 40.93 43.00
LBL+LEN+CLM 52.61 50.51 40.77 43.03 52.60 50.56 40.69 42.81
</table>
<tableCaption confidence="0.999811">
Table 1: BLEU and TER percentage scores on MT06 and MT08 Arabic-to-English newswire sets.
</tableCaption>
<bodyText confidence="0.99990395">
other hand, one string-to-dependency rule may
split into several dependency-to-dependency rules
due to different source dependency structures. The
size of the dependency-to-dependency rule set is
slightly smaller than the size of the string-to-
dependency rule set.
Tables 1 and 2 show the BLEU and TER per-
centage scores on MT06 and MT08 for Arabic-
to-English and Chinese-to-English translation re-
spectively. The context LM feature, the length
feature and the syntax label feature all produce
a small improvement for most of the conditions.
When we combined the three features, we ob-
served significant improvements over the baseline.
For Arabic-to-English MT, the LBL+LEN+CLM
system improved lower-cased BLEU by 2.0 on
MT06 and 1.7 on MT08 on decoding output.
For Chinese-to-English MT, the improvements in
lower-cased BLEU were 1.0 on MT06 and 0.8 on
MT08. After re-scoring, the improvements be-
came smaller, but still noticeable, ranging from 0.7
to 1.4. TER scores were also improved noticeably
for all conditions, suggesting there was no metric
specific over-tuning.
Surprisingly, source dependency LM did not
provide any improvement over the baseline. There
are two possible reasons for this. One is that
the source and target parse trees were generated
by two stand-alone parsers, which may cause in-
compatible structures on the source and target
sides. By applying the well-formed constraints
on both sides, a lot of useful transfer rules are
discarded. A bi-lingual parser, trained on paral-
lel treebanks recently made available to the NLP
community, may overcome this problem. The
other is that the search space of dependency-to-
dependency decoding is much larger, since we
need to add source dependency information into
the chart parsing states. We will explore tech-
niques to address these problems in the future.
</bodyText>
<sectionHeader confidence="0.999771" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.9998139">
Linguistic information has been widely used in
SMT. For example, in (Wang et al., 2007), syntac-
tic structures were employed to reorder the source
language as a pre-processing step for phrase-based
decoding. In (Koehn and Hoang, 2007), shallow
syntactic analysis such as POS tagging and mor-
phological analysis were incorporated in a phrasal
decoder.
In ISI’s syntax-based system (Galley et al.,
2006) and CMU’s Hiero extension (Venugopal et
al., 2007), non-terminals in translation rules have
labels, which must be respected by substitutions
during decoding. In (Post and Gildea, 2008; Shen
et al., 2008), target trees were employed to im-
prove the scoring of translation theories. Mar-
ton and Resnik (2008) introduced features defined
on constituent labels to improve the Hiero system
(Chiang, 2005). However, due to the limitation of
MER training, only part of the feature space could
used in the system. This problem was fixed by
</bodyText>
<page confidence="0.995584">
78
</page>
<table confidence="0.999678578947368">
MT06 MT08
Model BLEU TER BLEU TER
lower mixed lower mixed lower mixed lower mixed
Decoding (3-gram LM)
BASE 37.44 35.62 54.64 56.47 33.05 31.26 56.79 58.69
SLM 37.30 35.48 54.24 55.90 33.03 31.00 56.59 58.46
CLM 37.66 35.81 53.45 55.19 32.97 31.01 55.99 57.77
LEN 38.09 36.26 53.98 55.81 33.23 31.34 56.51 58.41
LBL 38.37 36.53 54.14 55.99 33.25 31.34 56.60 58.49
LBL+LEN 38.36 36.59 53.95 55.60 33.72 31.83 56.79 58.65
LBL+LEN+CLM 38.41 36.57 53.83 55.70 33.83 31.79 56.55 58.51
Rescoring (5-gram LM)
BASE 38.91 37.04 53.65 55.45 34.34 32.32 55.60 57.60
SLM 38.27 36.38 53.64 55.29 34.25 32.28 55.35 57.21
CLM 38.79 36.88 53.09 54.80 35.01 32.98 55.39 57.28
LEN 39.22 37.30 53.34 55.06 34.65 32.70 55.61 57.51
LBL 39.11 37.30 53.61 55.29 35.02 33.00 55.39 57.48
LBL+LEN 38.91 37.17 53.56 55.27 35.03 33.08 55.47 57.46
LBL+LEN+CLM 39.58 37.62 53.21 54.94 35.72 33.63 54.88 56.98
</table>
<tableCaption confidence="0.998789">
Table 2: BLEU and TER percentage scores on MT06 and MT08 Chinese-to-English newswire sets.
</tableCaption>
<bodyText confidence="0.998959">
Chiang et al. (2008), which used an online learn-
ing method (Crammer and Singer, 2003) to handle
a large set of features.
Most SMT systems assume that translation
rules can be applied without paying attention to
the sentence context. A few studies (Carpuat and
Wu, 2007; Ittycheriah and Roukos, 2007; He et
al., 2008; Hasan et al., 2008) addressed this de-
fect by selecting the appropriate translation rules
for an input span based on its context in the in-
put sentence. The direct translation model in (It-
tycheriah and Roukos, 2007) employed syntactic
(POS tags) and context information (neighboring
words) within a maximum entropy model to pre-
dict the correct transfer rules. A similar technique
was applied by He et al. (2008) to improve the Hi-
ero system.
Our model differs from previous work on the
way in which linguistic and contextual informa-
tion is used.
</bodyText>
<sectionHeader confidence="0.998397" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999950294117647">
In this paper, we proposed four new linguistic
and contextual features for hierarchical decoding.
The use of non-terminal labels, length distribution
and context LM features gave rise to significant
improvement on Arabic-to-English and Chinese-
to-English translation on NIST MT06 and MT08
newswire data over a state-of-the-art string-to-
dependency baseline. Unlike previous work, we
employed robust probabilistic models to capture
useful linguistic and contextual information. Our
methods are more suitable for practical translation
tasks.
In future, we will continue this work in two
directions. We will employ a Gaussian model
to unify various linguistic and contextual fea-
tures. We will also improve the dependency-to-
dependency method with a better bi-lingual parser.
</bodyText>
<sectionHeader confidence="0.997561" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999638333333333">
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
</bodyText>
<sectionHeader confidence="0.997955" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992057545454546">
R. P. Brent. 1973. Algorithms for Minimization With-
out Derivatives. Prentice-Hall.
I. Bulyko, S. Matsoukas, R. Schwartz, L. Nguyen, and
J. Makhoul. 2007. Language model adaptation in
machine translation from speech. In Proceedings of
the 32nd IEEE International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP).
M. Carpuat and D. Wu. 2007. Context-dependent
phrasal translation lexicons for statistical machine
translation. In Proceedings of Machine Translation
Summit XI.
</reference>
<page confidence="0.990863">
79
</page>
<reference confidence="0.999907556603773">
D. Chiang, Y. Marton, and P. Resnik. 2008. On-
line large-margin training of syntactic and structural
translation features. In Proceedings of the 2008
Conference of Empirical Methods in Natural Lan-
guage Processing.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
Proceedings of the 2009 Human Language Technol-
ogy Conference of the North American Chapter of
the Association for Computational Linguistics.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings
of the 43th Annual Meeting of the Association for
Computational Linguistics (ACL).
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal
ofMachine Learning Research, 3:951–991.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable infer-
ence and training of context-rich syntactic models.
In COLING-ACL ’06: Proceedings of 44th Annual
Meeting of the Association for Computational Lin-
guistics and 21st Int. Conf. on Computational Lin-
guistics.
J.-L. Gauvain and Chin-Hui Lee. 1994. Maximum a
posteriori estimation for multivariate gaussian mix-
tureobservations of markov chains. IEEE Transac-
tions on Speech and Audio Processing, 2(2).
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andr´es-Ferrer.
2008. Triplet lexicon models for statistical machine
translation. In Proceedings of the 2008 Conference
ofEmpirical Methods in Natural Language Process-
ing.
Z. He, Q. Liu, and S. Lin. 2008. Improving statistical
machine translation using lexicalized rule selection.
In Proceedings of COLING ’08: The 22nd Int. Conf.
on Computational Linguistics.
A. Ittycheriah and S. Roukos. 2007. Direct translation
model 2. In Proceedings of the 2007 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics.
P. Koehn and H. Hoang. 2007. Factored translation
models. In Proceedings of the 2007 Conference of
Empirical Methods in Natural Language Process-
ing.
P. Liang, A. Bouchard-Cˆot´e, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In COLING-ACL ’06: Proceed-
ings of 44th Annual Meeting of the Association for
Computational Linguistics and 21st Int. Conf. on
Computational Linguistics.
Y. Marton and P. Resnik. 2008. Soft syntactic con-
straints for hierarchical phrased-based translation.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL).
M. Ostendorf, A. Kannan, S. Austin, O. Kimball,
R. Schwartz, and J. R. Rohlicek. 1991. Integra-
tion of diverse recognition methodologies through
reevaluation of nbest sentence hypotheses. In Pro-
ceedings of the DARPA Workshop on Speech and
Natural Language.
K. Papineni, S. Roukos, and T. Ward. 2001. Bleu: a
method for automatic evaluation of machine transla-
tion. IBM Research Report, RC22176.
M. Post and D. Gildea. 2008. Parsers as language
models for statistical machine translation. In The
Eighth Conference of the Association for Machine
Translation in the Americas.
M. J. D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables with-
out calculating derivatives. The Computer Journal,
7(2).
L. Shen, A. Sarkar, and F. J. Och. 2004. Discriminative
reranking for machine translation. In Proceedings of
the 2004 Human Language Technology Conference
of the North American Chapter of the Association
for Computational Linguistics.
L. Shen, J. Xu, and R. Weischedel. 2008. A New
String-to-Dependency Machine Translation Algo-
rithm with a Target Dependency Language Model.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL).
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
Association for Machine Translation in the Ameri-
cas.
C. Tillmann and T. Zhang. 2006. A discrimina-
tive global training algorithm for statistical mt. In
COLING-ACL ’06: Proceedings of 44th Annual
Meeting of the Association for Computational Lin-
guistics and 21st Int. Conf. on Computational Lin-
guistics.
A. Venugopal, A. Zollmann, and S. Vogel. 2007.
An efficient two-pass approach to synchronous-cfg
driven statistical mt. In Proceedings of the 2007 Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese
syntactic reordering for statistical machine transla-
tion. In Proceedings of the 2007 Conference of Em-
pirical Methods in Natural Language Processing.
</reference>
<page confidence="0.998246">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.713302">
<title confidence="0.9994015">Effective Use of Linguistic and Contextual for Statistical Machine Translation</title>
<author confidence="0.8761365">Shen Xu Zhang Matsoukas</author>
<affiliation confidence="0.936575">BBN</affiliation>
<address confidence="0.999605">Cambridge, MA 02138,</address>
<abstract confidence="0.998240086956522">Current methods of using lexical features in machine translation have difficulty in scaling up to realistic MT tasks due to a prohibitively large number of parameters involved. In this paper, we propose methods of using new linguistic and contextual features that do not suffer from this problem and apply them in a state-ofthe-art hierarchical MT system. The features used in this work are non-terminal labels, non-terminal length distribution, source string context and source dependency LM scores. The effectiveness of our techniques is demonstrated by significant improvements over a strong baseline. On Arabic-to-English translation, improvements in lower-cased BLEU are 2.0 on NIST MT06 and 1.7 on MT08 newswire data on decoding output. On Chinese-to-English translation, the improvements are 1.0 on MT06 and 0.8 on MT08 newswire data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R P Brent</author>
</authors>
<title>Algorithms for Minimization Without Derivatives.</title>
<date>1973</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="21062" citStr="Brent, 1973" startWordPosition="3511" endWordPosition="3512">ined, while source context LMs work as a look-ahead feature. 3 Experiments We designed our experiments to show the impact of each feature separately as well as their cumulative impact: • BASE: baseline string-to-dependency system • SLM: baseline + source dependency LM • CLM: baseline + context LM • LEN: baseline + length distribution • LBL: baseline + syntactic labels • LBL+LEN: baseline + syntactic labels + length distribution • LBL+LEN+CLM: baseline + syntactic labels + length distribution + context LM All the models were optimized on lower-cased IBM BLEU with Powell’s method (Powell, 1964; Brent, 1973) on n-best translations (Ostendorf et al., 1991), but evaluated on both IBM BLEU and TER. The motivation is to detect if an improvement is artificial, i.e., specific to the tuning metric. For both Arabic-to-English and Chinese-toEnglish MT, we tuned on NIST MT02-05 and tested on MT06 and MT08 newswire sets. The training data are different from what was usd at MT06 or MT08. Our Arabic-to-English data contain 29M Arabic words and 38M English words from 11 corpora: LDC2004T17, LDC2004T18, LDC2005E46, LDC2006E25, LDC2006G05, LDC2005E85, LDC2006E36, LDC2006E82, LDC2006E95, Sakhr-A2E and Sakhr-E2A. </context>
</contexts>
<marker>Brent, 1973</marker>
<rawString>R. P. Brent. 1973. Algorithms for Minimization Without Derivatives. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Bulyko</author>
<author>S Matsoukas</author>
<author>R Schwartz</author>
<author>L Nguyen</author>
<author>J Makhoul</author>
</authors>
<title>Language model adaptation in machine translation from speech.</title>
<date>2007</date>
<booktitle>In Proceedings of the 32nd IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="22084" citStr="Bulyko et al., 2007" startWordPosition="3664" endWordPosition="3667"> contain 29M Arabic words and 38M English words from 11 corpora: LDC2004T17, LDC2004T18, LDC2005E46, LDC2006E25, LDC2006G05, LDC2005E85, LDC2006E36, LDC2006E82, LDC2006E95, Sakhr-A2E and Sakhr-E2A. The Chinese-to-English data contain 107M Chinese words and 132M English words from eight corpora: LDC2002E18, LDC2005T06, LDC2005T10, LDC2006E26, LDC2006G05, LDC2002L27, LDC2005T34 and LDC2003E07. They are available under the DARPA GALE program. Traditional 3-gram and 5-gram string LMs were trained on the English side of the parallel data plus the English Gigaword corpus V3.0 in a way described in (Bulyko et al., 2007). The target dependency LMs were trained on the English side of the parallel training data. For that purpose, we parsed the English side of the parallel data. Two separate models were trained: one for Arabic from the Arabic training data and the other for Chinese from the Chinese training data. To compute the source dependency LM for Chinese-to-English MT, we parsed the Chinese side of the Chinese-to-English parallel data. Due to the lack of a good Arabic parser compatible with the Sakhr tokenization that we used on the source side, we did not test the source dependency LM for Arabic-to-Englis</context>
</contexts>
<marker>Bulyko, Matsoukas, Schwartz, Nguyen, Makhoul, 2007</marker>
<rawString>I. Bulyko, S. Matsoukas, R. Schwartz, L. Nguyen, and J. Makhoul. 2007. Language model adaptation in machine translation from speech. In Proceedings of the 32nd IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>D Wu</author>
</authors>
<title>Context-dependent phrasal translation lexicons for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of Machine Translation</booktitle>
<location>Summit XI.</location>
<contexts>
<context position="3145" citStr="Carpuat and Wu, 2007" startWordPosition="497" endWordPosition="500">listic for practical use. A remedy is to aggressively limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features available, as in (Chiang et al., 2008; Chiang et al., 2009), but that reduces the benefit of lexical features. A possible generic solution is to cluster the lexical features in some way. However, how to make it work on such a large space of bi-lingual features is still an open question. The other approach is to estimate a single score or likelihood of a translation with rich features, for example, with the maximum entropy (MaxEnt) method as in (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008). This method avoids the over-fitting problem, at the expense of losing the benefit of discriminative training of rich features directly for MT. However, the feature space problem still exists in these published models. He et al. (2008) extended the WSD-like approached proposed in (Carpuat and Wu, 2007) to hierarchical decoders. In (He et al., 2008), lexical 72 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 72–80, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP features were limited on each single s</context>
<context position="7591" citStr="Carpuat and Wu, 2007" startWordPosition="1233" endWordPosition="1236">were explored in (He et al., 2008). However their length features only provided insignificant improvement of 0.1 BLEU point. A crucial difference of our approach is how the length preference is modeled. We approximate the length distribution of non-terminals with a smoothed Gaussian, which is more robust and gives rise to much larger improvement consistently. The third feature utilizes source side context information, i.e. the neighboring words of an input span, to influence the selection of the target translation for a span. While the use of context information has been explored in MT, e.g. (Carpuat and Wu, 2007) and (He et al., 2008), the specific technique we used by means of a context language model is rather different. Our model is trained on the whole training data, and it is not limited by the constraint of MaxEnt training. The fourth feature exploits structural information on the source side. Specifically, the decoder simultaneously generates both the source and target side dependency trees, and employs two dependency LMs, one for the source and the other for the target, for scoring translation hypotheses. Our intuition is that the likelihood of source structures provides another piece of evide</context>
<context position="27844" citStr="Carpuat and Wu, 2007" startWordPosition="4597" endWordPosition="4600">6.88 53.09 54.80 35.01 32.98 55.39 57.28 LEN 39.22 37.30 53.34 55.06 34.65 32.70 55.61 57.51 LBL 39.11 37.30 53.61 55.29 35.02 33.00 55.39 57.48 LBL+LEN 38.91 37.17 53.56 55.27 35.03 33.08 55.47 57.46 LBL+LEN+CLM 39.58 37.62 53.21 54.94 35.72 33.63 54.88 56.98 Table 2: BLEU and TER percentage scores on MT06 and MT08 Chinese-to-English newswire sets. Chiang et al. (2008), which used an online learning method (Crammer and Singer, 2003) to handle a large set of features. Most SMT systems assume that translation rules can be applied without paying attention to the sentence context. A few studies (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008; Hasan et al., 2008) addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence. The direct translation model in (Ittycheriah and Roukos, 2007) employed syntactic (POS tags) and context information (neighboring words) within a maximum entropy model to predict the correct transfer rules. A similar technique was applied by He et al. (2008) to improve the Hiero system. Our model differs from previous work on the way in which linguistic and contextual information is used. 5 Conclu</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>M. Carpuat and D. Wu. 2007. Context-dependent phrasal translation lexicons for statistical machine translation. In Proceedings of Machine Translation Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>Y Marton</author>
<author>P Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2713" citStr="Chiang et al., 2008" startWordPosition="421" endWordPosition="424">06; Tillmann and Zhang, 2006). This method is very attractive, since it opens the door to rich lexical features. However, in order to robustly optimize the feature weights, one has to use a substantially large development set, which results in significantly slower tuning. Alternatively, one needs to carefully select a development set that simulates the test set to reduce the risk of over-fitting, which however is not always realistic for practical use. A remedy is to aggressively limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features available, as in (Chiang et al., 2008; Chiang et al., 2009), but that reduces the benefit of lexical features. A possible generic solution is to cluster the lexical features in some way. However, how to make it work on such a large space of bi-lingual features is still an open question. The other approach is to estimate a single score or likelihood of a translation with rich features, for example, with the maximum entropy (MaxEnt) method as in (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008). This method avoids the over-fitting problem, at the expense of losing the benefit of discriminative training of rich f</context>
<context position="27596" citStr="Chiang et al. (2008)" startWordPosition="4555" endWordPosition="4558">N 38.36 36.59 53.95 55.60 33.72 31.83 56.79 58.65 LBL+LEN+CLM 38.41 36.57 53.83 55.70 33.83 31.79 56.55 58.51 Rescoring (5-gram LM) BASE 38.91 37.04 53.65 55.45 34.34 32.32 55.60 57.60 SLM 38.27 36.38 53.64 55.29 34.25 32.28 55.35 57.21 CLM 38.79 36.88 53.09 54.80 35.01 32.98 55.39 57.28 LEN 39.22 37.30 53.34 55.06 34.65 32.70 55.61 57.51 LBL 39.11 37.30 53.61 55.29 35.02 33.00 55.39 57.48 LBL+LEN 38.91 37.17 53.56 55.27 35.03 33.08 55.47 57.46 LBL+LEN+CLM 39.58 37.62 53.21 54.94 35.72 33.63 54.88 56.98 Table 2: BLEU and TER percentage scores on MT06 and MT08 Chinese-to-English newswire sets. Chiang et al. (2008), which used an online learning method (Crammer and Singer, 2003) to handle a large set of features. Most SMT systems assume that translation rules can be applied without paying attention to the sentence context. A few studies (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008; Hasan et al., 2008) addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence. The direct translation model in (Ittycheriah and Roukos, 2007) employed syntactic (POS tags) and context information (neighboring words) within a maximu</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of the 2008 Conference of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>K Knight</author>
<author>W Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2735" citStr="Chiang et al., 2009" startWordPosition="425" endWordPosition="428">g, 2006). This method is very attractive, since it opens the door to rich lexical features. However, in order to robustly optimize the feature weights, one has to use a substantially large development set, which results in significantly slower tuning. Alternatively, one needs to carefully select a development set that simulates the test set to reduce the risk of over-fitting, which however is not always realistic for practical use. A remedy is to aggressively limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features available, as in (Chiang et al., 2008; Chiang et al., 2009), but that reduces the benefit of lexical features. A possible generic solution is to cluster the lexical features in some way. However, how to make it work on such a large space of bi-lingual features is still an open question. The other approach is to estimate a single score or likelihood of a translation with rich features, for example, with the maximum entropy (MaxEnt) method as in (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008). This method avoids the over-fitting problem, at the expense of losing the benefit of discriminative training of rich features directly for M</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new features for statistical machine translation. In Proceedings of the 2009 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="26470" citStr="Chiang, 2005" startWordPosition="4367" endWordPosition="4368">phrase-based decoding. In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder. In ISI’s syntax-based system (Galley et al., 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding. In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. Marton and Resnik (2008) introduced features defined on constituent labels to improve the Hiero system (Chiang, 2005). However, due to the limitation of MER training, only part of the feature space could used in the system. This problem was fixed by 78 MT06 MT08 Model BLEU TER BLEU TER lower mixed lower mixed lower mixed lower mixed Decoding (3-gram LM) BASE 37.44 35.62 54.64 56.47 33.05 31.26 56.79 58.69 SLM 37.30 35.48 54.24 55.90 33.03 31.00 56.59 58.46 CLM 37.66 35.81 53.45 55.19 32.97 31.01 55.99 57.77 LEN 38.09 36.26 53.98 55.81 33.23 31.34 56.51 58.41 LBL 38.37 36.53 54.14 55.99 33.25 31.34 56.60 58.49 LBL+LEN 38.36 36.59 53.95 55.60 33.72 31.83 56.79 58.65 LBL+LEN+CLM 38.41 36.57 53.83 55.70 33.83 31</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="9760" citStr="Chiang, 2007" startWordPosition="1582" endWordPosition="1583">of the first three features: nonterminal labels, length distribution and source side context, improves MT performance. Surprisingly, the source dependency feature does not produce an improvement. 2 Linguistic and Context Features 2.1 Non-terminal Labels In the original string-to-dependency model (Shen et al., 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. A well-formed dependency structure could be either a single-rooted dependency tree or a set of sibling trees. As in the Hiero system (Chiang, 2007), there is only one non-terminal X in the string-to-dependency model. Any sub dependency structure can be used to replace a nonterminal in a rule. For example, we have a source sentence in Chinese as follows. • jiantao zhuyao baohan liang fangmian The literal translation for individual words is • ’review’ ’mainly’ ’to consist of’ ’two’ ’part’ The reference translation is • the review mainly consists of two parts A single source word can be translated into many English words. For example, jiantao can be translated into a review, the review, reviews, the reviews, reviewing, reviewed, etc. Suppos</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>3--951</pages>
<contexts>
<context position="27661" citStr="Crammer and Singer, 2003" startWordPosition="4566" endWordPosition="4569">M 38.41 36.57 53.83 55.70 33.83 31.79 56.55 58.51 Rescoring (5-gram LM) BASE 38.91 37.04 53.65 55.45 34.34 32.32 55.60 57.60 SLM 38.27 36.38 53.64 55.29 34.25 32.28 55.35 57.21 CLM 38.79 36.88 53.09 54.80 35.01 32.98 55.39 57.28 LEN 39.22 37.30 53.34 55.06 34.65 32.70 55.61 57.51 LBL 39.11 37.30 53.61 55.29 35.02 33.00 55.39 57.48 LBL+LEN 38.91 37.17 53.56 55.27 35.03 33.08 55.47 57.46 LBL+LEN+CLM 39.58 37.62 53.21 54.94 35.72 33.63 54.88 56.98 Table 2: BLEU and TER percentage scores on MT06 and MT08 Chinese-to-English newswire sets. Chiang et al. (2008), which used an online learning method (Crammer and Singer, 2003) to handle a large set of features. Most SMT systems assume that translation rules can be applied without paying attention to the sentence context. A few studies (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008; Hasan et al., 2008) addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence. The direct translation model in (Ittycheriah and Roukos, 2007) employed syntactic (POS tags) and context information (neighboring words) within a maximum entropy model to predict the correct transfer rules. A similar </context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Y. Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal ofMachine Learning Research, 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>J Graehl</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>S DeNeefe</author>
<author>W Wang</author>
<author>I Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic models.</title>
<date>2006</date>
<booktitle>In COLING-ACL ’06: Proceedings of 44th Annual Meeting of the Association for Computational Linguistics and 21st Int. Conf. on Computational Linguistics.</booktitle>
<contexts>
<context position="26072" citStr="Galley et al., 2006" startWordPosition="4304" endWordPosition="4307">space of dependency-todependency decoding is much larger, since we need to add source dependency information into the chart parsing states. We will explore techniques to address these problems in the future. 4 Discussion Linguistic information has been widely used in SMT. For example, in (Wang et al., 2007), syntactic structures were employed to reorder the source language as a pre-processing step for phrase-based decoding. In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder. In ISI’s syntax-based system (Galley et al., 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding. In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. Marton and Resnik (2008) introduced features defined on constituent labels to improve the Hiero system (Chiang, 2005). However, due to the limitation of MER training, only part of the feature space could used in the system. This problem was fixed by 78 MT06 MT08 Model BLEU TER BLEU TER lower mixed lower mixed lower mi</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe, W. Wang, and I. Thayer. 2006. Scalable inference and training of context-rich syntactic models. In COLING-ACL ’06: Proceedings of 44th Annual Meeting of the Association for Computational Linguistics and 21st Int. Conf. on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-L Gauvain</author>
<author>Chin-Hui Lee</author>
</authors>
<title>Maximum a posteriori estimation for multivariate gaussian mixtureobservations of markov chains.</title>
<date>1994</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="14044" citStr="Gauvain and Lee, 1994" startWordPosition="2329" endWordPosition="2332">cific non-terminal in that rule. Let l(r, x, i) denote the length of the source span corresponding to non-terminal x in the i-th occurrence of rule r in training. Then, we can compute the following quantities. l(r,x,i) (1) l(r, x, i)2, (2) which can be subsequently used to estimate the mean µr,x and variance σ2r,x of x’s length distribution in rule r as follows. µr,x = mr,x 2 = 2 σr,x sr,x − mr x Since many of the translation rules have few occurrences in training, smoothing of the above estimates is necessary. A common smoothing method is based on maximum a posteriori (MAP) estimation as in (Gauvain and Lee, 1994). Nr mr,x + Nr + τ Nr sr,x + Nr + τ where ˆ stands for an MAP distribution and ˜ represents a prior distribution. ˜mr,x and ˜sr,x can be obtained from a prior Gaussian distribution N(˜µr,x, ˜σr,x) via equations (3) and (4), and τ is a weight of smoothing. There are many ways to approximate the prior distribution. For example, we can have one prior for all the non-terminals or one for individual nonterminal type. In practice, we assume ˜µr,x = µr,x, and approximate ˜σr,x as (σ2r,x + sr,x)12 . In this way, we do not change the mean, but relax the variance with sr,x. We tried different smoothing </context>
</contexts>
<marker>Gauvain, Lee, 1994</marker>
<rawString>J.-L. Gauvain and Chin-Hui Lee. 1994. Maximum a posteriori estimation for multivariate gaussian mixtureobservations of markov chains. IEEE Transactions on Speech and Audio Processing, 2(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hasan</author>
<author>J Ganitkevitch</author>
<author>H Ney</author>
<author>J Andr´es-Ferrer</author>
</authors>
<title>Triplet lexicon models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference ofEmpirical Methods in Natural Language Processing.</booktitle>
<marker>Hasan, Ganitkevitch, Ney, Andr´es-Ferrer, 2008</marker>
<rawString>S. Hasan, J. Ganitkevitch, H. Ney, and J. Andr´es-Ferrer. 2008. Triplet lexicon models for statistical machine translation. In Proceedings of the 2008 Conference ofEmpirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z He</author>
<author>Q Liu</author>
<author>S Lin</author>
</authors>
<title>Improving statistical machine translation using lexicalized rule selection.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING ’08: The 22nd Int. Conf. on Computational Linguistics.</booktitle>
<contexts>
<context position="3193" citStr="He et al., 2008" startWordPosition="506" endWordPosition="509">y limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features available, as in (Chiang et al., 2008; Chiang et al., 2009), but that reduces the benefit of lexical features. A possible generic solution is to cluster the lexical features in some way. However, how to make it work on such a large space of bi-lingual features is still an open question. The other approach is to estimate a single score or likelihood of a translation with rich features, for example, with the maximum entropy (MaxEnt) method as in (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008). This method avoids the over-fitting problem, at the expense of losing the benefit of discriminative training of rich features directly for MT. However, the feature space problem still exists in these published models. He et al. (2008) extended the WSD-like approached proposed in (Carpuat and Wu, 2007) to hierarchical decoders. In (He et al., 2008), lexical 72 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 72–80, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP features were limited on each single side due to the feature space problem. In order t</context>
<context position="7004" citStr="He et al., 2008" startWordPosition="1137" endWordPosition="1140">This feature reduces the ambiguity of translation rules. The other benefit is that POS tags help to weed out bad target side tree structures, as an enhancement to the target dependency language model. The second feature is based on the length distribution of non-terminals. In English as well as in other languages, the same deep structure can be represented in different syntactic structures depending on the complexity of its constituents. We model such preferences by associating each nonterminal of a transfer rule with a probability distribution over its length. Similar ideas were explored in (He et al., 2008). However their length features only provided insignificant improvement of 0.1 BLEU point. A crucial difference of our approach is how the length preference is modeled. We approximate the length distribution of non-terminals with a smoothed Gaussian, which is more robust and gives rise to much larger improvement consistently. The third feature utilizes source side context information, i.e. the neighboring words of an input span, to influence the selection of the target translation for a span. While the use of context information has been explored in MT, e.g. (Carpuat and Wu, 2007) and (He et a</context>
<context position="15217" citStr="He et al., 2008" startWordPosition="2545" endWordPosition="2548">ce with sr,x. We tried different smoothing methods, but the performance did not change much, therefore we kept this simplest setup. We also tried the Poisson distribution, and the performance is similar to Gaussian distribution, which is about 0.1 point lower in BLEU. When a rule r is applied during decoding, we compute a penalty for each non-terminal x in r according to 1 P(l |r,x) = e σr,x√2π where l is length of source span corresponding to x. Our method to address the problem of length bias in rule selection is very different from the maximum entropy method used in existing studies, e.g. (He et al., 2008). 1 Nr mr,x = Nr Z=1 1 Nr sr,x = Nr Z=1 ˆmr,x = ˆsr,x = τ ˜sr,x, Nr + τ τ Nr + τ ˜mr,x (l−µr,x)2 − 2σ2r,x , 75 2.3 Context Language Model In the baseline string-to-dependency system, the probability a translation rule is selected in decoding does not depend on the sentence context. In reality, translation is highly context dependent. To address this defect, we introduce a new feature, called context language model. The motivation of this feature is to exploit surrounding words to influence the selection of the desired transfer rule for a given input span. To illustrate the problem, we use the </context>
<context position="17671" citStr="He et al., 2008" startWordPosition="2962" endWordPosition="2965">part of the rule extraction procedure. When we exact a rule, we collect two 3-gram events, one for the left side and the other for the right side. In decoding, whenever a partial hypothesis is generated, we calculate the context LM scores based on the leftmost two words and the rightmost two words of the hypothesis as well as the source context. The product of the left and right context LM scores is used as a new feature in the scoring function. Please note that our approach is very different from other approaches to context dependent rule selection such as (Ittycheriah and Roukos, 2007) and (He et al., 2008). Instead of using a large number of fine grained features with weights optimized using the maximum entropy method, we treat context dependency as an ngram LM problem, and it is smoothed with Witten-Bell discounting. The estimation of the context LMs is very efficient and robust. The benefit is two fold. The estimation of the context LMs is very efficient. It adds only one new weight to the scoring function. 2.4 Source Dependency Language Model The context LM proposed in the previous section only employs source words immediately before and after the current source span in decoding. To exploit </context>
<context position="27891" citStr="He et al., 2008" startWordPosition="4605" endWordPosition="4608">7.30 53.34 55.06 34.65 32.70 55.61 57.51 LBL 39.11 37.30 53.61 55.29 35.02 33.00 55.39 57.48 LBL+LEN 38.91 37.17 53.56 55.27 35.03 33.08 55.47 57.46 LBL+LEN+CLM 39.58 37.62 53.21 54.94 35.72 33.63 54.88 56.98 Table 2: BLEU and TER percentage scores on MT06 and MT08 Chinese-to-English newswire sets. Chiang et al. (2008), which used an online learning method (Crammer and Singer, 2003) to handle a large set of features. Most SMT systems assume that translation rules can be applied without paying attention to the sentence context. A few studies (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008; Hasan et al., 2008) addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence. The direct translation model in (Ittycheriah and Roukos, 2007) employed syntactic (POS tags) and context information (neighboring words) within a maximum entropy model to predict the correct transfer rules. A similar technique was applied by He et al. (2008) to improve the Hiero system. Our model differs from previous work on the way in which linguistic and contextual information is used. 5 Conclusions and Future Work In this paper, we propose</context>
</contexts>
<marker>He, Liu, Lin, 2008</marker>
<rawString>Z. He, Q. Liu, and S. Lin. 2008. Improving statistical machine translation using lexicalized rule selection. In Proceedings of COLING ’08: The 22nd Int. Conf. on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>S Roukos</author>
</authors>
<title>Direct translation model 2.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3175" citStr="Ittycheriah and Roukos, 2007" startWordPosition="501" endWordPosition="505">se. A remedy is to aggressively limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features available, as in (Chiang et al., 2008; Chiang et al., 2009), but that reduces the benefit of lexical features. A possible generic solution is to cluster the lexical features in some way. However, how to make it work on such a large space of bi-lingual features is still an open question. The other approach is to estimate a single score or likelihood of a translation with rich features, for example, with the maximum entropy (MaxEnt) method as in (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008). This method avoids the over-fitting problem, at the expense of losing the benefit of discriminative training of rich features directly for MT. However, the feature space problem still exists in these published models. He et al. (2008) extended the WSD-like approached proposed in (Carpuat and Wu, 2007) to hierarchical decoders. In (He et al., 2008), lexical 72 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 72–80, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP features were limited on each single side due to the feature space p</context>
<context position="5886" citStr="Ittycheriah and Roukos, 2007" startWordPosition="948" endWordPosition="951">ect. Many of them are not ideal for practical translation tasks. 1.2 Our Approach As for our approach, we mainly use simple probabilistic models, i.e. Gaussian and n-gram models, which are more robust and suitable for large-scale training of real data, as manifested in state-of-theart systems of speech recognition. The unique contribution of our work is to design effective and efficient statistical models to capture useful linguistic and context information for MT decoding. Feature functions defined in this way are robust and ideal for practical translation tasks. 1According to footnote 2 of (Ittycheriah and Roukos, 2007), test set adaptation by test set sampling of the training corpus showed an advantage of more than 2 BLEU points over a general system trained on all data. 1.2.1 Features In this paper, we will introduce four new linguistic and contextual feature functions. Here, we first provide a high-level description of these features. Details of the features are discussed in Section 2. The first feature is based on non-terminal labels, i.e. POS tags of the head words of target nonterminals in transfer rules. This feature reduces the ambiguity of translation rules. The other benefit is that POS tags help t</context>
<context position="17649" citStr="Ittycheriah and Roukos, 2007" startWordPosition="2957" endWordPosition="2960">stimated from the training data as part of the rule extraction procedure. When we exact a rule, we collect two 3-gram events, one for the left side and the other for the right side. In decoding, whenever a partial hypothesis is generated, we calculate the context LM scores based on the leftmost two words and the rightmost two words of the hypothesis as well as the source context. The product of the left and right context LM scores is used as a new feature in the scoring function. Please note that our approach is very different from other approaches to context dependent rule selection such as (Ittycheriah and Roukos, 2007) and (He et al., 2008). Instead of using a large number of fine grained features with weights optimized using the maximum entropy method, we treat context dependency as an ngram LM problem, and it is smoothed with Witten-Bell discounting. The estimation of the context LMs is very efficient and robust. The benefit is two fold. The estimation of the context LMs is very efficient. It adds only one new weight to the scoring function. 2.4 Source Dependency Language Model The context LM proposed in the previous section only employs source words immediately before and after the current source span in</context>
<context position="27874" citStr="Ittycheriah and Roukos, 2007" startWordPosition="4601" endWordPosition="4604"> 32.98 55.39 57.28 LEN 39.22 37.30 53.34 55.06 34.65 32.70 55.61 57.51 LBL 39.11 37.30 53.61 55.29 35.02 33.00 55.39 57.48 LBL+LEN 38.91 37.17 53.56 55.27 35.03 33.08 55.47 57.46 LBL+LEN+CLM 39.58 37.62 53.21 54.94 35.72 33.63 54.88 56.98 Table 2: BLEU and TER percentage scores on MT06 and MT08 Chinese-to-English newswire sets. Chiang et al. (2008), which used an online learning method (Crammer and Singer, 2003) to handle a large set of features. Most SMT systems assume that translation rules can be applied without paying attention to the sentence context. A few studies (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008; Hasan et al., 2008) addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence. The direct translation model in (Ittycheriah and Roukos, 2007) employed syntactic (POS tags) and context information (neighboring words) within a maximum entropy model to predict the correct transfer rules. A similar technique was applied by He et al. (2008) to improve the Hiero system. Our model differs from previous work on the way in which linguistic and contextual information is used. 5 Conclusions and Future Work In this </context>
</contexts>
<marker>Ittycheriah, Roukos, 2007</marker>
<rawString>A. Ittycheriah and S. Roukos. 2007. Direct translation model 2. In Proceedings of the 2007 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Conference of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="25906" citStr="Koehn and Hoang, 2007" startWordPosition="4279" endWordPosition="4282">are discarded. A bi-lingual parser, trained on parallel treebanks recently made available to the NLP community, may overcome this problem. The other is that the search space of dependency-todependency decoding is much larger, since we need to add source dependency information into the chart parsing states. We will explore techniques to address these problems in the future. 4 Discussion Linguistic information has been widely used in SMT. For example, in (Wang et al., 2007), syntactic structures were employed to reorder the source language as a pre-processing step for phrase-based decoding. In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder. In ISI’s syntax-based system (Galley et al., 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding. In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. Marton and Resnik (2008) introduced features defined on constituent labels to improve the Hiero system (Chiang, 2005). However, due to the limitation of </context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>P. Koehn and H. Hoang. 2007. Factored translation models. In Proceedings of the 2007 Conference of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>A Bouchard-Cˆot´e</author>
<author>D Klein</author>
<author>B Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In COLING-ACL ’06: Proceedings of 44th Annual Meeting of the Association for Computational Linguistics and 21st Int. Conf. on Computational Linguistics.</booktitle>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>P. Liang, A. Bouchard-Cˆot´e, D. Klein, and B. Taskar. 2006. An end-to-end discriminative approach to machine translation. In COLING-ACL ’06: Proceedings of 44th Annual Meeting of the Association for Computational Linguistics and 21st Int. Conf. on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Marton</author>
<author>P Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="26377" citStr="Marton and Resnik (2008)" startWordPosition="4351" endWordPosition="4355">, 2007), syntactic structures were employed to reorder the source language as a pre-processing step for phrase-based decoding. In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder. In ISI’s syntax-based system (Galley et al., 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding. In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. Marton and Resnik (2008) introduced features defined on constituent labels to improve the Hiero system (Chiang, 2005). However, due to the limitation of MER training, only part of the feature space could used in the system. This problem was fixed by 78 MT06 MT08 Model BLEU TER BLEU TER lower mixed lower mixed lower mixed lower mixed Decoding (3-gram LM) BASE 37.44 35.62 54.64 56.47 33.05 31.26 56.79 58.69 SLM 37.30 35.48 54.24 55.90 33.03 31.00 56.59 58.46 CLM 37.66 35.81 53.45 55.19 32.97 31.01 55.99 57.77 LEN 38.09 36.26 53.98 55.81 33.23 31.34 56.51 58.41 LBL 38.37 36.53 54.14 55.99 33.25 31.34 56.60 58.49 LBL+LEN</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Y. Marton and P. Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ostendorf</author>
<author>A Kannan</author>
<author>S Austin</author>
<author>O Kimball</author>
<author>R Schwartz</author>
<author>J R Rohlicek</author>
</authors>
<title>Integration of diverse recognition methodologies through reevaluation of nbest sentence hypotheses.</title>
<date>1991</date>
<booktitle>In Proceedings of the DARPA Workshop on Speech and Natural Language.</booktitle>
<contexts>
<context position="21110" citStr="Ostendorf et al., 1991" startWordPosition="3516" endWordPosition="3519">s a look-ahead feature. 3 Experiments We designed our experiments to show the impact of each feature separately as well as their cumulative impact: • BASE: baseline string-to-dependency system • SLM: baseline + source dependency LM • CLM: baseline + context LM • LEN: baseline + length distribution • LBL: baseline + syntactic labels • LBL+LEN: baseline + syntactic labels + length distribution • LBL+LEN+CLM: baseline + syntactic labels + length distribution + context LM All the models were optimized on lower-cased IBM BLEU with Powell’s method (Powell, 1964; Brent, 1973) on n-best translations (Ostendorf et al., 1991), but evaluated on both IBM BLEU and TER. The motivation is to detect if an improvement is artificial, i.e., specific to the tuning metric. For both Arabic-to-English and Chinese-toEnglish MT, we tuned on NIST MT02-05 and tested on MT06 and MT08 newswire sets. The training data are different from what was usd at MT06 or MT08. Our Arabic-to-English data contain 29M Arabic words and 38M English words from 11 corpora: LDC2004T17, LDC2004T18, LDC2005E46, LDC2006E25, LDC2006G05, LDC2005E85, LDC2006E36, LDC2006E82, LDC2006E95, Sakhr-A2E and Sakhr-E2A. The Chinese-to-English data contain 107M Chinese</context>
</contexts>
<marker>Ostendorf, Kannan, Austin, Kimball, Schwartz, Rohlicek, 1991</marker>
<rawString>M. Ostendorf, A. Kannan, S. Austin, O. Kimball, R. Schwartz, and J. R. Rohlicek. 1991. Integration of diverse recognition methodologies through reevaluation of nbest sentence hypotheses. In Proceedings of the DARPA Workshop on Speech and Natural Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Research</journal>
<tech>Report, RC22176.</tech>
<contexts>
<context position="9084" citStr="Papineni et al., 2001" startWordPosition="1475" endWordPosition="1478">wo reasons: • It provides a strong baseline, which ensures the validity of the improvement we would obtain. The baseline model used in this paper showed state-of-the-art performance at NIST 2008 MT evaluation. • The baseline algorithm can be easily extended to incorporate the features proposed in this paper. The use of source dependency structures is a natural extension of the stringto-tree model to a tree-to-tree model. To ensure the generality of our results, we tested the features on two rather different language pairs, Arabic-to-English and Chinese-to-English, using two metrics, IBM BLEU (Papineni et al., 2001) and TER (Snover et al., 2006). Our experiments show that each of the first three features: nonterminal labels, length distribution and source side context, improves MT performance. Surprisingly, the source dependency feature does not produce an improvement. 2 Linguistic and Context Features 2.1 Non-terminal Labels In the original string-to-dependency model (Shen et al., 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. A well-formed dependency structure could be either a single-rooted depe</context>
</contexts>
<marker>Papineni, Roukos, Ward, 2001</marker>
<rawString>K. Papineni, S. Roukos, and T. Ward. 2001. Bleu: a method for automatic evaluation of machine translation. IBM Research Report, RC22176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Post</author>
<author>D Gildea</author>
</authors>
<title>Parsers as language models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In The Eighth Conference of the Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="26256" citStr="Post and Gildea, 2008" startWordPosition="4331" endWordPosition="4334"> problems in the future. 4 Discussion Linguistic information has been widely used in SMT. For example, in (Wang et al., 2007), syntactic structures were employed to reorder the source language as a pre-processing step for phrase-based decoding. In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder. In ISI’s syntax-based system (Galley et al., 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding. In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. Marton and Resnik (2008) introduced features defined on constituent labels to improve the Hiero system (Chiang, 2005). However, due to the limitation of MER training, only part of the feature space could used in the system. This problem was fixed by 78 MT06 MT08 Model BLEU TER BLEU TER lower mixed lower mixed lower mixed lower mixed Decoding (3-gram LM) BASE 37.44 35.62 54.64 56.47 33.05 31.26 56.79 58.69 SLM 37.30 35.48 54.24 55.90 33.03 31.00 56.59 58.46 CLM 37.66 35.81 53.45 55.19 32.97 31.01 55</context>
</contexts>
<marker>Post, Gildea, 2008</marker>
<rawString>M. Post and D. Gildea. 2008. Parsers as language models for statistical machine translation. In The Eighth Conference of the Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J D Powell</author>
</authors>
<title>An efficient method for finding the minimum of a function of several variables without calculating derivatives.</title>
<date>1964</date>
<journal>The Computer Journal,</journal>
<volume>7</volume>
<issue>2</issue>
<contexts>
<context position="21048" citStr="Powell, 1964" startWordPosition="3509" endWordPosition="3510">tures are combined, while source context LMs work as a look-ahead feature. 3 Experiments We designed our experiments to show the impact of each feature separately as well as their cumulative impact: • BASE: baseline string-to-dependency system • SLM: baseline + source dependency LM • CLM: baseline + context LM • LEN: baseline + length distribution • LBL: baseline + syntactic labels • LBL+LEN: baseline + syntactic labels + length distribution • LBL+LEN+CLM: baseline + syntactic labels + length distribution + context LM All the models were optimized on lower-cased IBM BLEU with Powell’s method (Powell, 1964; Brent, 1973) on n-best translations (Ostendorf et al., 1991), but evaluated on both IBM BLEU and TER. The motivation is to detect if an improvement is artificial, i.e., specific to the tuning metric. For both Arabic-to-English and Chinese-toEnglish MT, we tuned on NIST MT02-05 and tested on MT06 and MT08 newswire sets. The training data are different from what was usd at MT06 or MT08. Our Arabic-to-English data contain 29M Arabic words and 38M English words from 11 corpora: LDC2004T17, LDC2004T18, LDC2005E46, LDC2006E25, LDC2006G05, LDC2005E85, LDC2006E36, LDC2006E82, LDC2006E95, Sakhr-A2E a</context>
</contexts>
<marker>Powell, 1964</marker>
<rawString>M. J. D. Powell. 1964. An efficient method for finding the minimum of a function of several variables without calculating derivatives. The Computer Journal, 7(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>A Sarkar</author>
<author>F J Och</author>
</authors>
<title>Discriminative reranking for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2076" citStr="Shen et al., 2004" startWordPosition="314" endWordPosition="317">linguistic and contextual information for MT decoding, and these new features do not suffer from the scalability problem. Our new models are tested on NIST MT06 and MT08 data, and they provide significant improvement over a strong baseline system. 1.1 Previous Work The ideas of using labels, length preference and source side context in MT decoding were explored previously. Broadly speaking, two approaches were commonly used in existing work. One is to use a stochastic gradient descent (SGD) or Perceptron like online learning algorithm to optimize the weights of these features directly for MT (Shen et al., 2004; Liang et al., 2006; Tillmann and Zhang, 2006). This method is very attractive, since it opens the door to rich lexical features. However, in order to robustly optimize the feature weights, one has to use a substantially large development set, which results in significantly slower tuning. Alternatively, one needs to carefully select a development set that simulates the test set to reduce the risk of over-fitting, which however is not always realistic for practical use. A remedy is to aggressively limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features </context>
</contexts>
<marker>Shen, Sarkar, Och, 2004</marker>
<rawString>L. Shen, A. Sarkar, and F. J. Och. 2004. Discriminative reranking for machine translation. In Proceedings of the 2004 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>J Xu</author>
<author>R Weischedel</author>
</authors>
<title>A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="8425" citStr="Shen et al., 2008" startWordPosition="1370" endWordPosition="1373">ning. The fourth feature exploits structural information on the source side. Specifically, the decoder simultaneously generates both the source and target side dependency trees, and employs two dependency LMs, one for the source and the other for the target, for scoring translation hypotheses. Our intuition is that the likelihood of source structures provides another piece of evidence about the plausibility of a translation hypothesis and as such would help weed out bad ones. 73 1.2.2 Baseline System and Experimental Setup We take BBN’s HierDec, a string-to-dependency decoder as described in (Shen et al., 2008), as our baseline for the following two reasons: • It provides a strong baseline, which ensures the validity of the improvement we would obtain. The baseline model used in this paper showed state-of-the-art performance at NIST 2008 MT evaluation. • The baseline algorithm can be easily extended to incorporate the features proposed in this paper. The use of source dependency structures is a natural extension of the stringto-tree model to a tree-to-tree model. To ensure the generality of our results, we tested the features on two rather different language pairs, Arabic-to-English and Chinese-to-E</context>
<context position="19171" citStr="Shen et al., 2008" startWordPosition="3201" endWordPosition="3204">to dependency-to-dependency rules. In each dependency-to-dependency rule, we keep record of the source string as well as the source dependency structure. Figure 3 shows examples of dependency-to-dependency rules. We extended the string-to-dependency decoding algorithm in the baseline to accommodate dependency-to-dependency theories. In decoding, we build both the source and the target dependency structures simultaneously in chart parsing over the source string. Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al., 2008). We introduce two new features for the source side dependency LM as follows, in a way similar to the target side. • Source dependency LM score • Discount on ill-formed source dependency structures The source dependency LM is trained on the source side of the bi-lingual training data with Witten-Bell smoothing. The source dependency LM score represents the likelihood of the source 76 X X−1 X X−1 zhuyao X−2 mainly of consists baohan X−2 review X X jiantao the reviews X the X jiantao Figure 3: Dependency-to-dependency translation rules dependency tree generated by the decoder. The source depende</context>
<context position="22893" citStr="Shen et al., 2008" startWordPosition="3799" endWordPosition="3802">: one for Arabic from the Arabic training data and the other for Chinese from the Chinese training data. To compute the source dependency LM for Chinese-to-English MT, we parsed the Chinese side of the Chinese-to-English parallel data. Due to the lack of a good Arabic parser compatible with the Sakhr tokenization that we used on the source side, we did not test the source dependency LM for Arabic-to-English MT. When extracting rules with source dependency structures, we applied the same well-formedness constraint on the source side as we did on the target side, using a procedure described by (Shen et al., 2008). Some candidate rules were thrown away due to the source side constraint. On the 77 MT06 MT08 Model BLEU TER BLEU TER lower mixed lower mixed lower mixed lower mixed Decoding (3-gram LM) BASE 48.75 46.74 43.43 45.79 49.58 47.46 42.80 45.08 CLM 49.44 47.36 42.96 45.22 49.73 47.53 42.64 44.92 LEN 49.37 47.28 43.01 45.35 50.29 48.19 42.32 44.45 LBL 49.33 47.07 43.09 45.53 50.46 48.19 42.27 44.57 LBL+LEN 49.91 47.70 42.59 45.17 51.10 48.85 41.88 44.16 LBL+LEN+CLM 50.75 48.51 42.13 44.50 51.24 49.10 41.63 43.80 Rescoring (5-gram LM) BASE 51.24 49.23 42.08 44.42 51.23 49.11 42.01 44.15 CLM 51.57 49</context>
<context position="26276" citStr="Shen et al., 2008" startWordPosition="4335" endWordPosition="4338">. 4 Discussion Linguistic information has been widely used in SMT. For example, in (Wang et al., 2007), syntactic structures were employed to reorder the source language as a pre-processing step for phrase-based decoding. In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder. In ISI’s syntax-based system (Galley et al., 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding. In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. Marton and Resnik (2008) introduced features defined on constituent labels to improve the Hiero system (Chiang, 2005). However, due to the limitation of MER training, only part of the feature space could used in the system. This problem was fixed by 78 MT06 MT08 Model BLEU TER BLEU TER lower mixed lower mixed lower mixed lower mixed Decoding (3-gram LM) BASE 37.44 35.62 54.64 56.47 33.05 31.26 56.79 58.69 SLM 37.30 35.48 54.24 55.90 33.03 31.00 56.59 58.46 CLM 37.66 35.81 53.45 55.19 32.97 31.01 55.99 57.77 LEN 38.09 </context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>L. Shen, J. Xu, and R. Weischedel. 2008. A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="9114" citStr="Snover et al., 2006" startWordPosition="1481" endWordPosition="1484">ng baseline, which ensures the validity of the improvement we would obtain. The baseline model used in this paper showed state-of-the-art performance at NIST 2008 MT evaluation. • The baseline algorithm can be easily extended to incorporate the features proposed in this paper. The use of source dependency structures is a natural extension of the stringto-tree model to a tree-to-tree model. To ensure the generality of our results, we tested the features on two rather different language pairs, Arabic-to-English and Chinese-to-English, using two metrics, IBM BLEU (Papineni et al., 2001) and TER (Snover et al., 2006). Our experiments show that each of the first three features: nonterminal labels, length distribution and source side context, improves MT performance. Surprisingly, the source dependency feature does not produce an improvement. 2 Linguistic and Context Features 2.1 Non-terminal Labels In the original string-to-dependency model (Shen et al., 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. A well-formed dependency structure could be either a single-rooted dependency tree or a set of siblin</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>T Zhang</author>
</authors>
<title>A discriminative global training algorithm for statistical mt.</title>
<date>2006</date>
<booktitle>In COLING-ACL ’06: Proceedings of 44th Annual Meeting of the Association for Computational Linguistics and 21st Int. Conf. on Computational Linguistics.</booktitle>
<contexts>
<context position="2123" citStr="Tillmann and Zhang, 2006" startWordPosition="322" endWordPosition="325">or MT decoding, and these new features do not suffer from the scalability problem. Our new models are tested on NIST MT06 and MT08 data, and they provide significant improvement over a strong baseline system. 1.1 Previous Work The ideas of using labels, length preference and source side context in MT decoding were explored previously. Broadly speaking, two approaches were commonly used in existing work. One is to use a stochastic gradient descent (SGD) or Perceptron like online learning algorithm to optimize the weights of these features directly for MT (Shen et al., 2004; Liang et al., 2006; Tillmann and Zhang, 2006). This method is very attractive, since it opens the door to rich lexical features. However, in order to robustly optimize the feature weights, one has to use a substantially large development set, which results in significantly slower tuning. Alternatively, one needs to carefully select a development set that simulates the test set to reduce the risk of over-fitting, which however is not always realistic for practical use. A remedy is to aggressively limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features available, as in (Chiang et al., 2008; Chiang e</context>
</contexts>
<marker>Tillmann, Zhang, 2006</marker>
<rawString>C. Tillmann and T. Zhang. 2006. A discriminative global training algorithm for statistical mt. In COLING-ACL ’06: Proceedings of 44th Annual Meeting of the Association for Computational Linguistics and 21st Int. Conf. on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Venugopal</author>
<author>A Zollmann</author>
<author>S Vogel</author>
</authors>
<title>An efficient two-pass approach to synchronous-cfg driven statistical mt.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26123" citStr="Venugopal et al., 2007" startWordPosition="4312" endWordPosition="4315">h larger, since we need to add source dependency information into the chart parsing states. We will explore techniques to address these problems in the future. 4 Discussion Linguistic information has been widely used in SMT. For example, in (Wang et al., 2007), syntactic structures were employed to reorder the source language as a pre-processing step for phrase-based decoding. In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder. In ISI’s syntax-based system (Galley et al., 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding. In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. Marton and Resnik (2008) introduced features defined on constituent labels to improve the Hiero system (Chiang, 2005). However, due to the limitation of MER training, only part of the feature space could used in the system. This problem was fixed by 78 MT06 MT08 Model BLEU TER BLEU TER lower mixed lower mixed lower mixed lower mixed Decoding (3-gram LM) BASE 37.44 35.</context>
</contexts>
<marker>Venugopal, Zollmann, Vogel, 2007</marker>
<rawString>A. Venugopal, A. Zollmann, and S. Vogel. 2007. An efficient two-pass approach to synchronous-cfg driven statistical mt. In Proceedings of the 2007 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>M Collins</author>
<author>P Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Conference of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="25760" citStr="Wang et al., 2007" startWordPosition="4257" endWordPosition="4260">incompatible structures on the source and target sides. By applying the well-formed constraints on both sides, a lot of useful transfer rules are discarded. A bi-lingual parser, trained on parallel treebanks recently made available to the NLP community, may overcome this problem. The other is that the search space of dependency-todependency decoding is much larger, since we need to add source dependency information into the chart parsing states. We will explore techniques to address these problems in the future. 4 Discussion Linguistic information has been widely used in SMT. For example, in (Wang et al., 2007), syntactic structures were employed to reorder the source language as a pre-processing step for phrase-based decoding. In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder. In ISI’s syntax-based system (Galley et al., 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding. In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. Marton </context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>C. Wang, M. Collins, and P. Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proceedings of the 2007 Conference of Empirical Methods in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>