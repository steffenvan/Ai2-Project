<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.768375">
Some Empirical Evidence for Annotation Noise in a Benchmarked Dataset
Beata Beigman Klebanov Eyal Beigman
</title>
<author confidence="0.656502">
Kellogg School of Management Washington University in St. Louis
</author>
<affiliation confidence="0.721215">
Northwestern University beigman@wustl.edu
</affiliation>
<email confidence="0.997705">
beata@northwestern.edu
</email>
<sectionHeader confidence="0.997356" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999314333333333">
A number of recent articles in computational
linguistics venues called for a closer exami-
nation of the type of noise present in anno-
tated datasets used for benchmarking (Rei-
dsma and Carletta, 2008; Beigman Klebanov
and Beigman, 2009). In particular, Beigman
Klebanov and Beigman articulated a type of
noise they call annotation noise and showed
that in worst case such noise can severely
degrade the generalization ability of a linear
classifier (Beigman and Beigman Klebanov,
2009). In this paper, we provide quantita-
tive empirical evidence for the existence of
this type of noise in a recently benchmarked
dataset. The proposed methodology can be
used to zero in on unreliable instances, facili-
tating generation of cleaner gold standards for
benchmarking.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977333333333">
Traditionally, studies in computational linguistics
use few trained annotators. Lately this might be
changing, as inexpensive annotators are available in
large numbers through projects like Amazon Me-
chanical Turk or through online games where an-
notations are produced as a by-product (Poesio et
al., 2008; von Ahn, 2006), and, at least for certain
tasks, the quality of multiple non-expert annotations
is close to that of a small number of experts (Snow
et al., 2008; Callison-Burch, 2009).
Apart from the reduced costs, mass annotation is
a promising way to get detailed information about
the dataset, such as the level of difficulty of the dif-
ference instances. Such information is important
both from the linguistic and from the machine learn-
ing perspective, as the existence of a group of in-
stances difficult enough to look like they have been
labeled by random guesses can in the worst case
induce the machine learner training on the dataset
to misclassify a constant proportion of easy, non-
controversial instances, as well as produce incor-
rect comparative results in a benchmarking setting
(Beigman Klebanov and Beigman, 2009; Beigman
and Beigman Klebanov, 2009) .
In this article, we employ annotation generation
models to estimate the types of instances in a multi-
ply annotated dataset for a binary classification task.
We provide the first quantitative empirical demon-
stration, to our knowledge, of the existence of what
Beigman Klebanov and Beigman (2009) call “anno-
tation noise” in a benchmarked dataset, that is, for
a case where instances cannot be plausibly assigned
to just two classes, and where instances in the third
class can be plausibly described as having been an-
notated by flips of a nearly fair coin. The ability to
identify such instances helps improve the gold stan-
dard by eliminating them, and allows further empiri-
cal investigation of their impact on machine learning
for the task in question.
</bodyText>
<sectionHeader confidence="0.997892" genericHeader="method">
2 Generative models of annotation
</sectionHeader>
<bodyText confidence="0.999956888888889">
We present a graphical model for the generation of
annotations. The basic idea is that there are different
types of instances that induce different responses
from annotators. Each instance may have a true la-
bel of “0” or “1”, however, the researcher’s access
to it is mediated by annotators who are guessing the
true label by flipping a coin, where the bias of the
coin depends on the type of the instance. The bias
of the coin essentially models the difficulty of label-
</bodyText>
<page confidence="0.979819">
438
</page>
<note confidence="0.7529995">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 438–446,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99975575862069">
ing the instance; coins biased close to 0 and 1 cor-
respond to instances that are easy to classify; a fair
coin represents instances that are very difficult if not
impossible to classify correctly with the given pool
of annotators. The model presented in Beigman Kle-
banov and Beigman (2009) is a special case with 3
types (A,B,C) where pA=0, pC=1 (easy cases), and
0&lt;pB&lt;1 represents the hard cases, the harder the
closer pB is to 0.5. Models used here are a type of la-
tent class models (McCutcheon, 1987) widely used
in the Biometrics community (Espeland and Handel-
man, 1989; Yang and Becker, 1997; Albert et al.,
2001; Albert and Dodd, 2004).
The goal of modeling is to determine whether
more than two types of instances need to be postu-
lated, to estimate how difficult each type is, and to
identify the troublemaking instances.
The graphical model is presented in figure 1. We
assume the dataset of size N is a mixture of k dif-
ferent types of instances. The proportion of types is
given by θ = (θ1, ... , θk), and coin biases for each
type are given by p = (p1, ... , pk). Each instance is
annotated by n i.i.d coinflips, and random variable
x E 10, ... , n} counts the number of “1”s in the n
annotations given to an instance. Each instance be-
longs to a type t E 11, ..., k}, characterized by a coin
with the probability pt of annotating with the label
“1”. Conditioned on t, the number of “1”s in n an-
notations has a binomial distribution with parameter
</bodyText>
<equation confidence="0.5493255">
pt: Pr(x = j|t) = (n )pj t(1 − pt)n−j.
j
</equation>
<figureCaption confidence="0.99884">
Figure 1: A graphical model of annotation generation.
</figureCaption>
<bodyText confidence="0.9803695">
The probability of observing j “1”s out of n an-
notations for an instance given θ and p is therefore
</bodyText>
<equation confidence="0.56361325">
Pr(x = j|θ,p) = Ekt=1 Pr(t|θ) - Pr(x = j|t) =
= (�) �t_i Btpt(1 − pt)n−j. The annotations arex
thus generated by a superposition of k binomials.
N
</equation>
<sectionHeader confidence="0.99845" genericHeader="method">
3 Data
</sectionHeader>
<subsectionHeader confidence="0.99997">
3.1 Recognizing Textual Entailment -1
</subsectionHeader>
<bodyText confidence="0.999917977777778">
For the experiments reported here we use the 800
item test data of the first Recognizing Textual Entail-
ment benchmark (RTE-1) from Dagan et al. (2006).
This task drew a lot of attention in the community,
with a series of benchmarks in 2005-2007.
The task is defined as follows: “... textual entail-
ment is defined as a directional relationship between
pairs of text expressions, denoted by T - the entail-
ing “Text”, and H - the entailed “Hypothesis”. We
say that T entails H if the meaning of H can be in-
ferred from the meaning of T, as would typically be
interpreted by people. This somewhat informal defi-
nition is based on (and assumes) common human
understanding of language as well as common back-
ground knowledge” (Dagan et al., 2006). Further
guidelines included an instruction to disregard tense
differences, to accept cases where the inference is
“very probable (but not completely certain)” and to
avoid cases where the inference “has some positive
probability that is not clearly very high.” An exam-
ple of a true entailment is the pair T-H: (T) Cavern
Club sessions paid the Beatles £15 evenings and £5
lunchtime. (H) The Beatles perform at Cavern.
Although annotated by a small number of experts
for the benchmark, the RTE-1 dataset has been later
transferred to a mass annotation framework by Snow
et al. (2008), who submitted simplified guidelines
to the Amazon Mechanical Turk workplace (hence-
forth, AMT), collected 10 annotations per item from
the total of 164 annotators, and showed that major-
ity vote by Turkers agreed with expert annotation in
89.7% of the cases. We call the Snow et al. (2008)
Turker annotations SRTE dataset, and use it in sec-
tion 6. The instructions, followed by two examples,
read: “Please state whether the second sentence (the
Hypothesis) is implied by the information in the first
sentence (the Text), i.e., please state whether the Hy-
pothesis can be determined to be true given that the
Text is true. Assume that you do not know anything
about the situation except what the Text itself says.
Also, note that every part of the Hypothesis must be
implied by the Text in order for it to be true.” The
guidelines for Turkers are somewhat different from
the original, not mentioning the issue of highly prob-
able though not certain inference or a special treat-
</bodyText>
<equation confidence="0.993057666666667">
p
t x
N
</equation>
<page confidence="0.994715">
439
</page>
<bodyText confidence="0.999714">
ment of tense mismatch between H and T, as well as
discouraging reliance on background knowledge.
Using Snow et al. (2008) instructions, we col-
lected 20 annotations for each of the 800 items
through AMT from the total of 441 annotators. Each
annotator did the minimum of 2 items, and was
paid $0.01 for 2 items, for the total annotator cost
of $80. We used only annotators with prior AMT
approval rate of at least 95%, that is, only people
whose performance in previous tasks on AMT was
almost always approved by the requester of the task.
Our design is thus somewhat different from Snow et
al. (2008), as we paid more and selected annotators
with a stake in their AMT reputation.
</bodyText>
<subsectionHeader confidence="0.999954">
3.2 Preparing the data for model fitting
</subsectionHeader>
<bodyText confidence="0.999836461538462">
We collected the annotations in two separate batches
of 10 annotations per item, using the same set of in-
structions, incentives, and examples. We hypothe-
sized that controlling for these elements, we would
get two random samples from the same distribution
of Turkers, and hence will have two samples to make
sure a model fitted on one sample generalized to
the other. It turned out, however, that a 3-Binomial
model with a good fit on one of the samples was re-
jected with high probability for the other.1 Thus, on
the one hand, the variations between annotators in
each sample were not as high as to preclude a model
that captures only instance variability from fitting
well; on the other hand, evidently, the two samples
did not come from the same annotator distribution,
but differed systematically due to factors we did not
control for.2 In order for our models not to inherit a
systematic bias of any of the two samples, we mixed
the two samples, and constructed two sets, BRTEa
and BRTEb, each with 10 annotations per item, by
randomly splitting the 20 answers per item into two
groups, allowing the same annotator to contribute
to different groups on different instances. Indeed,
after the randomization, a model fitted for BRTEa
produced excellent generalization on BRTEb, as we
will see in section 4.2.
</bodyText>
<footnote confidence="0.966988">
1For details of the model fitting procedure, see section 4.
2Such factors could be the hour and day of assignment, as
the composition of AMT’s global 24/7 workforce could differ
systematically by day and hour.
</footnote>
<sectionHeader confidence="0.808115" genericHeader="method">
4 Fitting a model to BRTE data
</sectionHeader>
<bodyText confidence="0.999880909090909">
Using the model template presented in section 2, we
successively attempt to fit a model with k = 2, 3, .. .
until a model with a good fit is found or no degrees
of freedom are left. For a given k, we fit the pa-
rameters 0 and p using non-linear least squares trust-
region method as implemented in the default version
of MATLAB’s lsqnonlin function. We then use x2 to
measure goodness of fit; a model that cannot be re-
jected with 95% confidence (p&gt;0.05) would be con-
sidered a good fit. In all cases N=800, n=10, as we
use 10 annotations for each instance.
</bodyText>
<subsectionHeader confidence="0.989599">
4.1 Mixture of 2 Binomials
</subsectionHeader>
<bodyText confidence="0.9933122">
Suppose k=2, with types t0 and t1. The best fit yields
p0=0.237, p1=0.867, θ0=431
800, 01=1-00. The model
(shown in figure 2) is a poor fit, with x2=73.66 well
above the critical value of 14.07 for df=7, p=0.05.3
</bodyText>
<figure confidence="0.9529325">
0 1 2 3 4 5 6 7 8 9 10
Number of label &amp;quot;1&amp;quot; annotations
</figure>
<figureCaption confidence="0.759114">
Figure 2: Fitting the model B1+B2 to BRTEa data. B1—
13(10,0.237) on 431 instances, B2— 13(10,0.867) on 369
instances. The point (x,y) means that there are y in-
stances given label “1” in exactly x out of 10 annotations.
</figureCaption>
<subsectionHeader confidence="0.9968">
4.2 Model M: Mixture of 3 Binomials
</subsectionHeader>
<bodyText confidence="0.920259875">
Suppose now k=3. The best fitting model
M=B1+B2+B3 is specified in figure 3; M fits the
data very well. Assuming B1 and B3 reflect items
3For degrees of freedom, we take the number of datapoints
being fitted (11), take one degree of freedom off for knowing in
advance the total number of instances, and take off additional 3
degrees of freedom for estimating po, pi, and Bo from the data.
We are therefore left with 7 degrees of freedom in this case.
</bodyText>
<figure confidence="0.998505666666667">
Number of instances
160
140
120
100
40
80
60
20
0
Observed
Predicted
</figure>
<page confidence="0.992596">
440
</page>
<bodyText confidence="0.9800834">
with uncontroversial labels “0” and “1”, respec-
tively, the model suggests that detecting “0” (no tex-
tual entailment) is somewhat more difficult for non-
experts than detecting “1” (there is textual entail-
ment) in this dataset, with the rate of incorrect pre-
dictions of about 20% and 10%, respectively.4 The
model also predicts that 159
800 Pz� 20% of the data are
difficult cases, with annotators flipping a close-to-a-
fair coin (p=0.5487).
</bodyText>
<figure confidence="0.80157">
0 1 2 3 4 5 6 7 8 9 10
</figure>
<figureCaption confidence="0.9909405">
Figure 3: Fitting the model M=B1+B2+B3 to BRTEa
data. B1— 13(10,0.1978) on 343 instances, B2—
13(10,0.5487) on 159 instances, B3— 13(10,0.8942) on
298 instances. The binomials are shown in grey lines.
The model M fits with x2=5.091; for df=5, this corre-
sponds to p=0.4.
</figureCaption>
<bodyText confidence="0.95766069047619">
We use the dataset BRTEb to test the model de-
veloped on BRTEa. The model fits with x2=13.13,
which, for df=10,5 corresponds to p=0.2154.
We therefore conclude that, after eliminating sys-
tematic differences between annotators, we were un-
able to fit a model with two types of instances,
whereas a model with three types of instances pro-
vides a good fit both for the dataset on which it is
estimated and for a new dataset. This constitutes
empirical evidence for the existence of a group of
instances with near-random labels in this recently
4We note that any conclusions from the model hold for the
particular 800 item dataset in question, and not for the task of
recognizing textual entailment in general, as the dataset is not
necessarily a representative sample. In fact, we know from Da-
gan et al. (2006) that these 800 items are not a random sam-
ple, but rather what remained after some 400 instances were re-
moved due to disagreements between expert annotators or due
to the judgment of one of organizers of the RTE-1 challenge.
5No parameters are fitted using the BRTEb data.
benchmarked dataset, at least for our pool of more
than 400 non-expert annotators.
5 Could annotator heterogeneity provide
an alternative explanation?
In the previous section, we established that instance
heterogeneity can explain the observations. We
might however ask whether a different model could
provide a similarly fitting explanation. Specifically,
heterogeneity among annotators has been seen as a
major source of noise in the aggregate data and there
are several works attempting to separate high qual-
ity annotators from low quality ones (Raykar et al.,
2009; Donmez et al., 2009; Sheng et al., 2008; Car-
penter, 2008). Could we explain the observed beha-
vior with a model with only two types of instances
that allows for annotator heterogeneity?
In this section we construct such a model. We
p
show that this model entails an instance distribu-
tion that is a superposition of two normal distribu-
tions. We subsequently show that the best fitting
two-Gaussian model does not provide a good fit.
</bodyText>
<equation confidence="0.428613">
0 t x
</equation>
<bodyText confidence="0.66115875">
We use a generation model similar to those in
(Raykar et al., 2009; Carpenter, 2008) but with
weaker parametric assumptions. The graphical
model is given in figure 4.
</bodyText>
<figureCaption confidence="0.997062">
Figure 4: Annotation generation model with annotator
</figureCaption>
<bodyText confidence="0.834435857142857">
p
heterogeneity.
We assume there are two types of instances t E
10, 11 with the proportions 0 = (Bo,x01). The 2n
probabilities p = (pt1, ... , ptn) for t = 0,1 cor-
respond to coins drawn independently from some
distribution with parameter α = (α1, . . . , αn). We
</bodyText>
<subsectionHeader confidence="0.403359">
� a
</subsectionHeader>
<bodyText confidence="0.999764666666667">
make no assumption on the functional form apart
from a positive probability to draw a value between
0 and 1, this in particular is true for the beta distribu-
tion used in (Raykar et al., 2009; Carpenter, 2008).
As before, the number of “1”s attributed to an in-
stance of type t is a random variable x, determined
</bodyText>
<figure confidence="0.983103823529412">
140
120
100
80
60
40
20
0
Observed
B1
B2
B3
Predicted
p
t
x
N
</figure>
<page confidence="0.696817">
441
</page>
<table confidence="0.5917139">
by independent flips of the n coins that correspond 120
to the value of t. The marginal distribution of x is: 100
Pr(x = jJθ, α) = 80
X= ZPr(tJθ) [0,1]n Pr(ptJα)·Pr(x = jJpt,t,α)dpt 60
t=0,1 40
20
⎛ ⎞
Z ⎝X Y Y
θt [0,1]n Pr(ptJα) pti (1 — pti) ⎠dpt
|S|=j iES iOS
</table>
<figureCaption confidence="0.992655">
Figure 5: Model G’s fit to BRTEa data, G= N1+N2, a
mixture of two Gaussians.
</figureCaption>
<figure confidence="0.912735166666667">
0
Observed
Predicted
0 1 2 3 4 5 6 7 8 9 10
X=
t=0,1
</figure>
<bodyText confidence="0.99996424">
Let x1, ... , xN be the random variables correspond-
ing to the number of “1”s attributed to instances
1, . . . ,N. W.l.g we assume instances 1, ... , N&apos; are
all of type t0 (N&apos; = θ0 · N) and the rest of type t1.
Since 0 &lt; xj &lt; n it follows that E(xj), Var(xj) &lt;
oc for j = 1, ... , N. If for each instance the coin-
flips are independent, we can think of this as a two
step process where we first draw the coins and then
flip them. Thus, x1, ... , xN, are i.i.d and the cen-
tral limit theorem implies that the average number
of “1”s on t0 instances, namely the random variable
y0 = N1 P� 1 x j has an approximately normal dis-
tribution.6 Making the same argument for the distri-
bution of y1 for instances of type t1, it follows that
the number of “1”s attributed to an instance of any
type y = y0 + y1 would have a distribution that is a
superposition of two Gaussians.
The best least-squares fit of all two-Gaussian
models to BRTEa data is produced by G=N1+N2,
N1— jV(2.22, 1.73) on 418 instances, N2—
jV(9.07,1.41) on 382 instances; G is shown in
figure 5. G fits with χ2=36.77, much above the crit-
ical value χ2=11.07 for df=5, p=0.05. We can thus
rule out annotator heterogeneity as the only expla-
nation of the observed pattern of responses.
</bodyText>
<sectionHeader confidence="0.940482" genericHeader="method">
6 Testing M on SRTE data
</sectionHeader>
<bodyText confidence="0.972892725">
We further test M on the annotations collected by
Snow et al. (2008) for the same 800 item dataset.
While the instructions and the task were identical in
BRTEa, BRTEb, and BRTE datasets, and in all cases
6It can be shown that y0 — N(µ, v) for µ = n · EDist(α)(p)
and v = ,1VarDist(α)(p) · n, using the expectation and variance
of the coin parameter for type t0 instances. For example, for a
beta distribution with parameters α and ,Q these would be µ =
α+β n and v = «+β n.
each item was given 10 annotations, the incentive
design was different (see section 3).
Figure 6 shows that model M=B1+B2+B3 does
not fit well, as SRTE dataset exhibits a rather diffe-
rent distribution from both BRTE datasets. In par-
ticular, it is clear that had a model been fitted on
SRTE data, the coin flipping probabilities for the
clear types, B1 and B3, would have to be moved
towards 0.5; that is, an average annotator in SRTE
dataset had worse ability to detect clear 0s and clear
1s than an average BRTE annotator. We note that
BRTEa and BRTEb agreed with expert annotation
in 92.5% and 90.8% of the instances, respectively,
both better than 89.7% in SRTE.7 Since we offered
somewhat better incentives in BRTE, it is tempting
to attribute the observed better quality of BRTE an-
notations to the improved incentives, although it is
possible that some other uncontrolled AMT-related
factor is responsible for the difference between the
datasets, just as we found for our original two col-
lected samples (see section 3.2).
Supposing the main source of misfit is difference
in incentives, we conjecture that the difference be-
tween the 441 BRTE annotators and the 164 SRTE
ones is due to the existence in SRTE of unmotivated,
or “lazy” annotators, that is, people who flipped the
same coin on every instance, no matter what type.
Our hypothesis is that once an annotator is diligent
(and motivated) enough to pay attention to the data,
her annotations can be described by model M, but
some annotators are not sufficiently diligent.
</bodyText>
<footnote confidence="0.900642">
7Turker annotations were aggregated using majority vote, as
in Snow et al. (2008) section 4.3.
</footnote>
<page confidence="0.997184">
442
</page>
<figure confidence="0.9773725">
0 1 2 3 4 5 6 7 8 9 10
t
</figure>
<figureCaption confidence="0.999475">
Figure 6: Model M’s fit to SRTE data. BRTEa and
BRTEb are shown in grey lines.
</figureCaption>
<bodyText confidence="0.940646666666667">
In this model we assume there are three types
of instances as before, and two types of annotators
a E {D, L}, for Diligent and Lazy, with their pro-
</bodyText>
<figure confidence="0.9189874">
p
portions in the population ξ = (ξD, ξL). The corre-
x
sponding graphical model is shown in figure 7.
B t N
</figure>
<figureCaption confidence="0.99892">
Figure 7: Annotation generation with diligent and lazy
annotators.
</figureCaption>
<bodyText confidence="0.9998512">
We assume that diligent annotators flip coins cor-
responding to the types of instances, whereas lazy
annotators always flip the same coin pL.
Let nD and nL=n − nD be the number of diligent
and lazy annotations given to a certain instance, thus
</bodyText>
<equation confidence="0.86538075">
Pr(nD=r|ξ)=�n �ξr Dξn−r
L , and the probability of ob-
r
serving j label “1” annotations for an instance of
type t is given by:
X Cn − r) 1,3.2
(1 − pL)n−r−j2J J
j2 /I L
</equation>
<bodyText confidence="0.998222777777778">
where S={(j1, j2):j1+j2=j; j1&lt;r;j2&lt;n-r}. Finally,
Pr(x=j|θ, ξ, p)=Pkt=1 θt Pr(x=j|t, ξ, p).
We assume that model M provides the values for
θ and p for all diligent annotators, and estimate ξ
and pL, the proportion of the lazy annotators and
the coin they flip. The best fitting model yields
ξ=(0.79,0.21), and pL=0.74, predicting that about
one-fifth of SRTE annotators are lazy.8 This model
fits with χ2=14.63, which is below the critical level
of χ2=15.51 for df=8,p=0.05, hence a hypothesis
that model M behavior for the diligent annotators
and flipping a coin with bias 0.74 for the lazy ones
generated the SRTE data cannot be rejected with
high confidence. We note that Carpenter (2008) ar-
rived at a similar conclusion – that there are quite
a few annotators making random guesses in SRTE
dataset – by means of jointly estimating annotator
accuracies.
</bodyText>
<sectionHeader confidence="0.999306" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.919624285714286">
To summarize our findings: With systematic dif-
ferences between annotators smoothed out, there
is evidence that non-expert annotators performing
x
RTE task on RTE-1 test data tend to flip a close-
to-fair coin on about 20% of instances, according
to the best fitting model.9 This constitutes, to our
</bodyText>
<subsectionHeader confidence="0.784502">
B t N
</subsectionHeader>
<bodyText confidence="0.999968">
knowledge, the first empirical evidence for the ex-
istence of the kind of noise termed annotation noise
in Beigman Klebanov and Beigman (2009). Given
Beigman Klebanov and Beigman (2009) warning
against annotation noise in test data and their find-
ing in Beigman and Beigman Klebanov (2009) that
annotation noise in training data can potentially dev-
astate a linear classifier learning from the data, the
immediate usefulness of our result is that instances
of this difficult type can be identified, removed from
the dataset before further benchmarking, and pos-
</bodyText>
<footnote confidence="0.980746444444445">
8A more precise statement is that there are about one-fifth
lazy potential annotators in the SRTE pool for any given item.
It is possible that the length of stay of an annotator in the pool is
not independent of her diligence; for example, Callison-Burch
(2009) found in his AMT experiments with tasks related to ma-
chine translation that lazy annotators tended to stay longer and
do more annotations.
9Beigman Klebanov and Beigman (2009) discuss the con-
nection between noise models and inter-annotator agreement.
</footnote>
<figure confidence="0.855003521739131">
140
120
100
40
80
60
20
0
BRTEa
BRTEb
Observed (SRTE)
Predicted by M
p
a
x
t
N
X [(n)ξrDξLn−r rX
r=1
n
Pr(x = j|t, ξ, p) =
~ X X,j s 31) pjt 1(1 − pt)r−j1 X
(j12)E
</figure>
<page confidence="0.996736">
443
</page>
<bodyText confidence="0.999930619047619">
sibly used in a controlled fashion for subsequent
studies of the impact of annotation noise on specific
learning algorithms and feature spaces for this task.
The current literature on generating benchmark-
ing data from AMT annotations overwhelmingly
considers annotator heterogeneity as the source of
observed discrepancies, with instances falling into
two classes only. Our results suggest that, at least in
RTE data, instance heterogeneity cannot be ignored.
It also transpired that small variations in incen-
tives (as between SRTE and BRTE), and even un-
known factors possibly related to differences in the
composition of AMT’s workforce can lead to sys-
tematic differences in the resulting annotator pools,
which results in annotations that are described by
models with somewhat different parameter values.
This can potentially limit the usefulness of our main
finding, because it is not clear how reliable the iden-
tification of hard cases is using any particular group
of Turkers. While this is a valid concern in general,
we show in section 7.1 that many items consistently
found to be hard by different groups of Turkers war-
rant at least an additional examination, as they often
represent borderline cases of highly or not-so-highly
probable inferences, corruption of meaning by un-
grammaticality, or difficulties related to the treat-
ment of time references and background knowledge.
Finally, our findings seem to be at odds with the
fact that the 800 items analyzed here were left af-
ter all items on which two experts disagreed and all
items that looked controversial to the arbiter were
removed (see section 3). One potential explanation
is that things that are hard for Turkers are not nec-
essarily hard for experts. Yet it is possible that two
or three annotators, graduate students or faculty in
computational linguistics, are an especially homoge-
nous and small pool of people to base gold standard
annotations of the way things are “typically inter-
preted by people” upon. Furthermore, there is some
evidence from additional expert re-annotations of
this dataset that some controversies remain; we dis-
cuss relation to expert annotations in section 7.2.
</bodyText>
<subsectionHeader confidence="0.999312">
7.1 Hard cases
</subsectionHeader>
<bodyText confidence="0.9834544">
We examine some of the instances that in all likeli-
hood belong to the difficult type, according to Turk-
ers. We focus on items that received between 4 and
7 class “1” annotations in SRTE and in each of our
two datasets (before randomization).
</bodyText>
<listItem confidence="0.940963761904762">
(1) T: Saudi Arabia, the biggest oil producer in
the world, was once a supporter of Osama bin
Laden and his associates who led attacks against
the United States. H: Saudi Arabia is the
world’s biggest oil exporter.
(2) T: Seiler was reported missing March 27 and
was found four days later in a marsh near her
campus apartment. H: Abducted Audrey Seiler
found four days after missing.
(3) T: The spokesman for the rescue authorities,
Linart Ohlin, said that the accident took place
between 01:00 and dawn today, Friday (00:00
GMT) in a disco behind the theatre, where “hun-
dreds” of young people were present. H: The
fire happened in the early hours of Friday morn-
ing, and hundreds of young people were present.
(4) T: William Leonard Jennings sobbed loudly as
was charged with killing his 3-year-old son,
Stephen, who was last seen alive on Dec.12,
1962. H: William Leonard Jennings killed his
3-year-old son, Stephen.
</listItem>
<bodyText confidence="0.999818857142857">
Labeling of examples 1-4 seems to hinge on the
assessment of the likelihood of an alternative expla-
nation. Thus, it is possible that the biggest producer
of oil is not the biggest exporter, because, for ex-
ample, its internal consumption is much higher than
in the second-biggest producer. In 2, abduction is
a possible cause for being missing, but how rela-
tively probable is it? Similarly, fire is a kind of ac-
cident, but can we infer that there was fire from a
report about an accident? In 4, could the man have
sobbed because on top of loosing his son he was
also being falsely accused of having killed him? Ex-
perts marked all five as true entailments, while many
Turkers had reservations.
</bodyText>
<listItem confidence="0.90488575">
(5) T: Bush returned to the White House late Satur-
day while his running mate was off campaigning
in the West. H: Bush left the White House.
(6) T: De la Cruz’s family said he had gone to Saudi
Arabia a year ago to work as a driver after a long
period of unemployment. H: De la Cruz was
unemployed.
(7) T: Measurements by ground-based instruments
</listItem>
<bodyText confidence="0.811631">
around the world have shown a decrease of up
to 10 percent in sunlight from the late 1950s to
the early 1990s. H: The world is about 10 per
cent darker than half a century ago.
</bodyText>
<page confidence="0.99713">
444
</page>
<bodyText confidence="0.957424111111111">
In examples 5-7 time seems to be an issue. If Bush
returned to White House, he must have left it before-
hand, but does this count as entailment, or is the hy-
pothesis referencing a time concurrent with the text,
in which case T and H are in contradiction? In 6,
can H be seen as referring to some time more than a
year ago? In 7, if the hypothesis is taken to be stated
in mid- or late-2000s, the time of annotation, half
a century ago would reach to late 1950s, but it is
possible that further substantial reduction occurred
between early 1990s mentioned in the text and mid
2000s, amounting to much more than 10%. Experts
labeled example 5 as false, 6 and 7 as true.
(8) T: On 2 February 1990, at the opening of Parlia-
ment, he declared that apartheid had failed and
that the bans on political parties, including the
ANC, were to be lifted. H: Apartheid in South
Africa was abolished in 1990.
</bodyText>
<listItem confidence="0.87877275">
(9) T: Kennedy had just won California’s Demo-
cratic presidential primary when Sirhan shot
him in Los Angeles on June 5, 1968. H: Sirhan
killed Kennedy.
</listItem>
<bodyText confidence="0.8931104">
Labeling examples 8 and 9 (both true according to
the experts) requires knowledge about South African
and American politics, respectively. Was the ban on
ANC the only or the most important manifestation
of apartheid? Was abolishing apartheid merely an
issue of declaring that it failed? In 9, killing is a po-
tential but not necessary outcome of shooting, so de-
tails of Robert Kennedy’s case need to be known to
the annotator to render the case-specific judgment.
(10) T: The version for the PC has essentially the
same packaging as those for the big game con-
soles, but players have been complaining that
it offers significantly less versatility when it
comes to swinging through New York. H: Play-
ers have been complaining that it sells signifi-
</bodyText>
<reference confidence="0.888411086956522">
cantly less versatility when it comes to swinging
through New York.
(11) T: During his trip to the Middle East that took
three days, Clinton made the first visit by an
American president to the Palestinian Territories
and participated in a three-way meeting with Is-
raeli Prime Minister Benjamin Netanyahu and
Palestinian President Yasser Arafat. H: During
his trip to the east of the Middle which lasted
three days, the Clinton to first visit to Ameri-
can President to the occupied Palestinian terri-
tories and participated in meeting tripartite co-
operation with Israeli Prime Minister Benjamin
Netanyahu and Palestinian President, Yasser
Arafat.
(12) T: The ISM non-manufacturing index rose to
64.8 in July from 59.9 in June. H: The non-
manufacturing index of the ISM raised 64.8 in
July from 59.9 in June.
(13) T: Henryk Wieniawski, a Polish-born musician,
was known for his special preference for resur-
recting neglected or lost works for the violin. H:
Henryk Wieniawski was born in Polish.
</reference>
<bodyText confidence="0.99953325">
Examples 10-13 were labeled as false by experts,
possibly betraying over-sensitivity to the failings of
language technology. Sells is not an ideal substitu-
tion for offers, but in a certain sense versatility is
sold as part of a product. In 11-13, some Turkers
felt the hypothesis is not too bad a rendition of the
text or of its part, while experts seemed to hold MT
to a higher standard.
</bodyText>
<subsectionHeader confidence="0.994553">
7.2 Turkers vs experts
</subsectionHeader>
<bodyText confidence="0.999885863636364">
Model M puts 159 items in the difficult type B2.
While M is the best fitting model, it is possible to
find a model that still fits with p&gt;0.05 but places
a smaller number of items in B2, in order to ob-
tain a conservative estimate on the number of dif-
ficult cases. The model with B1∼ B(10, 0.21) on
373 items, B2∼ B(10,0.563) on 110 items, B3∼
B(10,0.89) on 327 items still produces a fit with
p&gt;0.05, but going down to 100 instances in B2
makes it impossible to find a good fit with a 3 type
model. There are therefore about 110 difficult cases
by a conservative estimate. Assuming there remain
110 hard cases in the 800 item dataset for which
even experts flip a fair coin, we expect about 55
disagreements between the 800 item gold standard
from RTE-1 and a replication by a new expert, or
an agreement of 745
800=93% on average. This estimate
is consistent with reports of 91% to 96% replication
accuracy for the expert annotations on various sub-
sets of the data by different groups of experts (see
section 2.3 in Dagan et al. (2006)).
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99968975">
We would like to thank the anonymous reviewers of
this and the previous draft for helping us improve the
paper significantly. We also thank Amar Cheema for
his advice on AMT.
</bodyText>
<page confidence="0.998941">
445
</page>
<sectionHeader confidence="0.998346" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999940573333333">
Paul Albert and Lori Dodd. 2004. A Cautionary Note on
the Robustness of Latent Class Models for Estimating
Diagnostic Error without a Gold Standard. Biometrics,
60(2):427–435.
Paul Albert, Lisa McShane, Joanna Shih, and The U.S.
National Cancer Institute Bladder Tumor Marker Net-
work. 2001. Latent Class Modeling Approaches for
Assessing Diagnostic Error without a Gold Standard:
With Applications to p53 Immunohistochemical As-
says in Bladder Tumors. Biometrics, 57(2):610–619.
Eyal Beigman and Beata Beigman Klebanov. 2009.
Learning with Annotation Noise. In Proceedings of
the 47th Annual Meeting of the Association for Com-
putational Linguistics, pages 280–287, Singapore.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From Annotator Agreement to Noise Models. Ac-
cepted to Computational Linguistics.
Chris Callison-Burch. 2009. Fast, Cheap, and Cre-
ative: Evaluating Translation Quality Using Amazon’s
Mechanical Turk. In Proceedings of the Empirical
Methods in Natural Language Processing Conference,
pages 286–295, Singapore.
Bob Carpenter. 2008. Multilevel Bayesian Mod-
els of Categorical Data Annotation. Unpub-
lished manuscript, last accessed 28 July 2009
at lingpipe.files.wordpress.com/2009/01/anno-bayes-
entities-09.pdf.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. In J. Qui˜nonero Candela, I. Dagan,
B. Magnini, and F. d’Alch´e-Buc, editors, Machine
Learning Challenges, pages 177–190. Springer.
Pinar Donmez, Jaime Carbonell, and Jeff Schneider.
2009. Efficiently Learning and Accuracy of Labeling
Sources for Selective Sampling. In Proceedings of the
15th International Conference on Knowledge Discov-
ery and Data Mining, pages 259–268, Paris, France.
Mark Espeland and Stanley Handelman. 1989. Using
Class Models to Characterize and Assess Relative Er-
ror in Discrete Measurements. Biometrics, 45(2):587–
599.
Allan McCutcheon. 1987. Latent Class Analysis. New-
bury Park, CA, USA: Sage.
Massimo Poesio, Udo Kruschwitz, and Jon Chamberlain.
2008. ANAWIKI: Creating Anaphorically Annotated
Resources through Web Cooperation. In Proceedings
of the 6th International Conference on Language Re-
sources and Evaluation, Marrakech, Morocco.
Vikas Raykar, Shipeng Yu, Linda Zhao, Anna Jerebko,
Charles Florin, Gerardo Hermosillo Valadez, Luca Bo-
goni, and Linda Moy. 2009. Supervised Learning
from Multiple Experts: Whom to Trust when Every-
one Lies a Bit. In Proceedings of the 26th Annual In-
ternational Conference on Machine Learning, pages
889–896, Montreal, Canada.
Dennis Reidsma and Jean Carletta. 2008. Reliability
Measurement without Limits. Computational Linguis-
tics, 34(3):319–326.
Victor Sheng, Foster Provost, and Panagiotis Ipeirotis.
2008. Get Another Label? Improving Data Quality
and Data Mining Using Multiple, Noisy Labelers. In
Proceedings of the 14th International Conference on
Knowledge Discovery and Data Mining, pages 614–
622, Las Vegas, Nevada, USA.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and Fast - But is it Good?
Evaluating Non-Expert Annotations for Natural Lan-
guage Tasks. In Proceedings of the Empirical Methods
in Natural Language Processing Conference, pages
254–263, Honolulu, Hawaii.
Luis von Ahn. 2006. Games with a Purpose. Computer,
39(6):92–94.
Ilsoon Yang and Mark Becker. 1997. Latent Vari-
able Modeling of Diagnostic Accuracy. Biometrics,
53(3):948–958.
</reference>
<page confidence="0.999111">
446
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.647222">
<title confidence="0.999403">Some Empirical Evidence for Annotation Noise in a Benchmarked Dataset</title>
<author confidence="0.851212">Louis</author>
<affiliation confidence="0.999491">University</affiliation>
<email confidence="0.999324">beata@northwestern.edu</email>
<abstract confidence="0.995100315789474">A number of recent articles in computational linguistics venues called for a closer examination of the type of noise present in annotated datasets used for benchmarking (Reidsma and Carletta, 2008; Beigman Klebanov and Beigman, 2009). In particular, Beigman Klebanov and Beigman articulated a type of they call noise showed that in worst case such noise can severely degrade the generalization ability of a linear classifier (Beigman and Beigman Klebanov, 2009). In this paper, we provide quantitative empirical evidence for the existence of this type of noise in a recently benchmarked dataset. The proposed methodology can be used to zero in on unreliable instances, facilitating generation of cleaner gold standards for benchmarking.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>cantly less versatility when it comes to swinging through</title>
<location>New York.</location>
<marker></marker>
<rawString>cantly less versatility when it comes to swinging through New York.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T</author>
</authors>
<title>During his trip to the Middle East that took three days, Clinton made the first visit by an American president to the Palestinian Territories and participated in a three-way meeting with Israeli Prime Minister Benjamin Netanyahu and Palestinian President Yasser Arafat. H: During his trip to the east of the Middle which lasted three days, the Clinton to first visit to American President to the occupied Palestinian territories and participated in meeting tripartite cooperation with Israeli Prime Minister Benjamin Netanyahu and Palestinian President, Yasser Arafat.</title>
<marker>T, </marker>
<rawString>(11) T: During his trip to the Middle East that took three days, Clinton made the first visit by an American president to the Palestinian Territories and participated in a three-way meeting with Israeli Prime Minister Benjamin Netanyahu and Palestinian President Yasser Arafat. H: During his trip to the east of the Middle which lasted three days, the Clinton to first visit to American President to the occupied Palestinian territories and participated in meeting tripartite cooperation with Israeli Prime Minister Benjamin Netanyahu and Palestinian President, Yasser Arafat.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T</author>
</authors>
<title>The ISM non-manufacturing index rose to 64.8</title>
<date></date>
<booktitle>in July from 59.9 in June. H: The nonmanufacturing index of the ISM raised 64.8 in July from 59.9 in</booktitle>
<marker>T, </marker>
<rawString>(12) T: The ISM non-manufacturing index rose to 64.8 in July from 59.9 in June. H: The nonmanufacturing index of the ISM raised 64.8 in July from 59.9 in June.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T</author>
</authors>
<title>Henryk Wieniawski, a Polish-born musician, was known for his special preference for resurrecting neglected or lost works for the violin. H: Henryk Wieniawski was born in Polish.</title>
<marker>T, </marker>
<rawString>(13) T: Henryk Wieniawski, a Polish-born musician, was known for his special preference for resurrecting neglected or lost works for the violin. H: Henryk Wieniawski was born in Polish.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Albert</author>
<author>Lori Dodd</author>
</authors>
<title>A Cautionary Note on the Robustness of Latent Class Models for Estimating Diagnostic Error without a Gold Standard.</title>
<date>2004</date>
<journal>Biometrics,</journal>
<volume>60</volume>
<issue>2</issue>
<contexts>
<context position="4286" citStr="Albert and Dodd, 2004" startWordPosition="686" endWordPosition="689">oins biased close to 0 and 1 correspond to instances that are easy to classify; a fair coin represents instances that are very difficult if not impossible to classify correctly with the given pool of annotators. The model presented in Beigman Klebanov and Beigman (2009) is a special case with 3 types (A,B,C) where pA=0, pC=1 (easy cases), and 0&lt;pB&lt;1 represents the hard cases, the harder the closer pB is to 0.5. Models used here are a type of latent class models (McCutcheon, 1987) widely used in the Biometrics community (Espeland and Handelman, 1989; Yang and Becker, 1997; Albert et al., 2001; Albert and Dodd, 2004). The goal of modeling is to determine whether more than two types of instances need to be postulated, to estimate how difficult each type is, and to identify the troublemaking instances. The graphical model is presented in figure 1. We assume the dataset of size N is a mixture of k different types of instances. The proportion of types is given by θ = (θ1, ... , θk), and coin biases for each type are given by p = (p1, ... , pk). Each instance is annotated by n i.i.d coinflips, and random variable x E 10, ... , n} counts the number of “1”s in the n annotations given to an instance. Each instanc</context>
</contexts>
<marker>Albert, Dodd, 2004</marker>
<rawString>Paul Albert and Lori Dodd. 2004. A Cautionary Note on the Robustness of Latent Class Models for Estimating Diagnostic Error without a Gold Standard. Biometrics, 60(2):427–435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Albert</author>
<author>Lisa McShane</author>
<author>Joanna Shih</author>
<author>U S The</author>
</authors>
<title>National Cancer Institute Bladder Tumor Marker Network.</title>
<date>2001</date>
<journal>Biometrics,</journal>
<volume>57</volume>
<issue>2</issue>
<contexts>
<context position="4262" citStr="Albert et al., 2001" startWordPosition="682" endWordPosition="685">s ing the instance; coins biased close to 0 and 1 correspond to instances that are easy to classify; a fair coin represents instances that are very difficult if not impossible to classify correctly with the given pool of annotators. The model presented in Beigman Klebanov and Beigman (2009) is a special case with 3 types (A,B,C) where pA=0, pC=1 (easy cases), and 0&lt;pB&lt;1 represents the hard cases, the harder the closer pB is to 0.5. Models used here are a type of latent class models (McCutcheon, 1987) widely used in the Biometrics community (Espeland and Handelman, 1989; Yang and Becker, 1997; Albert et al., 2001; Albert and Dodd, 2004). The goal of modeling is to determine whether more than two types of instances need to be postulated, to estimate how difficult each type is, and to identify the troublemaking instances. The graphical model is presented in figure 1. We assume the dataset of size N is a mixture of k different types of instances. The proportion of types is given by θ = (θ1, ... , θk), and coin biases for each type are given by p = (p1, ... , pk). Each instance is annotated by n i.i.d coinflips, and random variable x E 10, ... , n} counts the number of “1”s in the n annotations given to a</context>
</contexts>
<marker>Albert, McShane, Shih, The, 2001</marker>
<rawString>Paul Albert, Lisa McShane, Joanna Shih, and The U.S. National Cancer Institute Bladder Tumor Marker Network. 2001. Latent Class Modeling Approaches for Assessing Diagnostic Error without a Gold Standard: With Applications to p53 Immunohistochemical Assays in Bladder Tumors. Biometrics, 57(2):610–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Beigman</author>
<author>Beata Beigman Klebanov</author>
</authors>
<title>Learning with Annotation Noise.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>280--287</pages>
<marker>Beigman, Klebanov, 2009</marker>
<rawString>Eyal Beigman and Beata Beigman Klebanov. 2009. Learning with Annotation Noise. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics, pages 280–287, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Beigman Klebanov</author>
<author>Eyal Beigman</author>
</authors>
<title>From Annotator Agreement to Noise Models. Accepted to Computational Linguistics.</title>
<date>2009</date>
<contexts>
<context position="2148" citStr="Klebanov and Beigman, 2009" startWordPosition="327" endWordPosition="330"> reduced costs, mass annotation is a promising way to get detailed information about the dataset, such as the level of difficulty of the difference instances. Such information is important both from the linguistic and from the machine learning perspective, as the existence of a group of instances difficult enough to look like they have been labeled by random guesses can in the worst case induce the machine learner training on the dataset to misclassify a constant proportion of easy, noncontroversial instances, as well as produce incorrect comparative results in a benchmarking setting (Beigman Klebanov and Beigman, 2009; Beigman and Beigman Klebanov, 2009) . In this article, we employ annotation generation models to estimate the types of instances in a multiply annotated dataset for a binary classification task. We provide the first quantitative empirical demonstration, to our knowledge, of the existence of what Beigman Klebanov and Beigman (2009) call “annotation noise” in a benchmarked dataset, that is, for a case where instances cannot be plausibly assigned to just two classes, and where instances in the third class can be plausibly described as having been annotated by flips of a nearly fair coin. The ab</context>
<context position="3934" citStr="Klebanov and Beigman (2009)" startWordPosition="622" endWordPosition="626"> a coin, where the bias of the coin depends on the type of the instance. The bias of the coin essentially models the difficulty of label438 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 438–446, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics ing the instance; coins biased close to 0 and 1 correspond to instances that are easy to classify; a fair coin represents instances that are very difficult if not impossible to classify correctly with the given pool of annotators. The model presented in Beigman Klebanov and Beigman (2009) is a special case with 3 types (A,B,C) where pA=0, pC=1 (easy cases), and 0&lt;pB&lt;1 represents the hard cases, the harder the closer pB is to 0.5. Models used here are a type of latent class models (McCutcheon, 1987) widely used in the Biometrics community (Espeland and Handelman, 1989; Yang and Becker, 1997; Albert et al., 2001; Albert and Dodd, 2004). The goal of modeling is to determine whether more than two types of instances need to be postulated, to estimate how difficult each type is, and to identify the troublemaking instances. The graphical model is presented in figure 1. We assume the </context>
<context position="21289" citStr="Klebanov and Beigman (2009)" startWordPosition="3759" endWordPosition="3762">008) arrived at a similar conclusion – that there are quite a few annotators making random guesses in SRTE dataset – by means of jointly estimating annotator accuracies. 7 Discussion To summarize our findings: With systematic differences between annotators smoothed out, there is evidence that non-expert annotators performing x RTE task on RTE-1 test data tend to flip a closeto-fair coin on about 20% of instances, according to the best fitting model.9 This constitutes, to our B t N knowledge, the first empirical evidence for the existence of the kind of noise termed annotation noise in Beigman Klebanov and Beigman (2009). Given Beigman Klebanov and Beigman (2009) warning against annotation noise in test data and their finding in Beigman and Beigman Klebanov (2009) that annotation noise in training data can potentially devastate a linear classifier learning from the data, the immediate usefulness of our result is that instances of this difficult type can be identified, removed from the dataset before further benchmarking, and pos8A more precise statement is that there are about one-fifth lazy potential annotators in the SRTE pool for any given item. It is possible that the length of stay of an annotator in the</context>
</contexts>
<marker>Klebanov, Beigman, 2009</marker>
<rawString>Beata Beigman Klebanov and Eyal Beigman. 2009. From Annotator Agreement to Noise Models. Accepted to Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon’s Mechanical Turk.</title>
<date>2009</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference,</booktitle>
<pages>286--295</pages>
<contexts>
<context position="1506" citStr="Callison-Burch, 2009" startWordPosition="224" endWordPosition="225"> can be used to zero in on unreliable instances, facilitating generation of cleaner gold standards for benchmarking. 1 Introduction Traditionally, studies in computational linguistics use few trained annotators. Lately this might be changing, as inexpensive annotators are available in large numbers through projects like Amazon Mechanical Turk or through online games where annotations are produced as a by-product (Poesio et al., 2008; von Ahn, 2006), and, at least for certain tasks, the quality of multiple non-expert annotations is close to that of a small number of experts (Snow et al., 2008; Callison-Burch, 2009). Apart from the reduced costs, mass annotation is a promising way to get detailed information about the dataset, such as the level of difficulty of the difference instances. Such information is important both from the linguistic and from the machine learning perspective, as the existence of a group of instances difficult enough to look like they have been labeled by random guesses can in the worst case induce the machine learner training on the dataset to misclassify a constant proportion of easy, noncontroversial instances, as well as produce incorrect comparative results in a benchmarking s</context>
<context position="21966" citStr="Callison-Burch (2009)" startWordPosition="3871" endWordPosition="3872">nst annotation noise in test data and their finding in Beigman and Beigman Klebanov (2009) that annotation noise in training data can potentially devastate a linear classifier learning from the data, the immediate usefulness of our result is that instances of this difficult type can be identified, removed from the dataset before further benchmarking, and pos8A more precise statement is that there are about one-fifth lazy potential annotators in the SRTE pool for any given item. It is possible that the length of stay of an annotator in the pool is not independent of her diligence; for example, Callison-Burch (2009) found in his AMT experiments with tasks related to machine translation that lazy annotators tended to stay longer and do more annotations. 9Beigman Klebanov and Beigman (2009) discuss the connection between noise models and inter-annotator agreement. 140 120 100 40 80 60 20 0 BRTEa BRTEb Observed (SRTE) Predicted by M p a x t N X [(n)ξrDξLn−r rX r=1 n Pr(x = j|t, ξ, p) = ~ X X,j s 31) pjt 1(1 − pt)r−j1 X (j12)E 443 sibly used in a controlled fashion for subsequent studies of the impact of annotation noise on specific learning algorithms and feature spaces for this task. The current literature</context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon’s Mechanical Turk. In Proceedings of the Empirical Methods in Natural Language Processing Conference, pages 286–295, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>Multilevel Bayesian Models of Categorical Data Annotation.</title>
<date>2008</date>
<note>Unpublished manuscript, last accessed 28</note>
<contexts>
<context position="14133" citStr="Carpenter, 2008" startWordPosition="2433" endWordPosition="2435"> dataset, at least for our pool of more than 400 non-expert annotators. 5 Could annotator heterogeneity provide an alternative explanation? In the previous section, we established that instance heterogeneity can explain the observations. We might however ask whether a different model could provide a similarly fitting explanation. Specifically, heterogeneity among annotators has been seen as a major source of noise in the aggregate data and there are several works attempting to separate high quality annotators from low quality ones (Raykar et al., 2009; Donmez et al., 2009; Sheng et al., 2008; Carpenter, 2008). Could we explain the observed behavior with a model with only two types of instances that allows for annotator heterogeneity? In this section we construct such a model. We p show that this model entails an instance distribution that is a superposition of two normal distributions. We subsequently show that the best fitting two-Gaussian model does not provide a good fit. 0 t x We use a generation model similar to those in (Raykar et al., 2009; Carpenter, 2008) but with weaker parametric assumptions. The graphical model is given in figure 4. Figure 4: Annotation generation model with annotator </context>
<context position="20666" citStr="Carpenter (2008)" startWordPosition="3657" endWordPosition="3658">Pr(x=j|t, ξ, p). We assume that model M provides the values for θ and p for all diligent annotators, and estimate ξ and pL, the proportion of the lazy annotators and the coin they flip. The best fitting model yields ξ=(0.79,0.21), and pL=0.74, predicting that about one-fifth of SRTE annotators are lazy.8 This model fits with χ2=14.63, which is below the critical level of χ2=15.51 for df=8,p=0.05, hence a hypothesis that model M behavior for the diligent annotators and flipping a coin with bias 0.74 for the lazy ones generated the SRTE data cannot be rejected with high confidence. We note that Carpenter (2008) arrived at a similar conclusion – that there are quite a few annotators making random guesses in SRTE dataset – by means of jointly estimating annotator accuracies. 7 Discussion To summarize our findings: With systematic differences between annotators smoothed out, there is evidence that non-expert annotators performing x RTE task on RTE-1 test data tend to flip a closeto-fair coin on about 20% of instances, according to the best fitting model.9 This constitutes, to our B t N knowledge, the first empirical evidence for the existence of the kind of noise termed annotation noise in Beigman Kleb</context>
</contexts>
<marker>Carpenter, 2008</marker>
<rawString>Bob Carpenter. 2008. Multilevel Bayesian Models of Categorical Data Annotation. Unpublished manuscript, last accessed 28 July 2009 at lingpipe.files.wordpress.com/2009/01/anno-bayesentities-09.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge. In</title>
<date>2006</date>
<booktitle>Machine Learning Challenges,</booktitle>
<pages>177--190</pages>
<editor>J. Qui˜nonero Candela, I. Dagan, B. Magnini, and F. d’Alch´e-Buc, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="5639" citStr="Dagan et al. (2006)" startWordPosition="945" endWordPosition="948">t, the number of “1”s in n annotations has a binomial distribution with parameter pt: Pr(x = j|t) = (n )pj t(1 − pt)n−j. j Figure 1: A graphical model of annotation generation. The probability of observing j “1”s out of n annotations for an instance given θ and p is therefore Pr(x = j|θ,p) = Ekt=1 Pr(t|θ) - Pr(x = j|t) = = (�) �t_i Btpt(1 − pt)n−j. The annotations arex thus generated by a superposition of k binomials. N 3 Data 3.1 Recognizing Textual Entailment -1 For the experiments reported here we use the 800 item test data of the first Recognizing Textual Entailment benchmark (RTE-1) from Dagan et al. (2006). This task drew a lot of attention in the community, with a series of benchmarks in 2005-2007. The task is defined as follows: “... textual entailment is defined as a directional relationship between pairs of text expressions, denoted by T - the entailing “Text”, and H - the entailed “Hypothesis”. We say that T entails H if the meaning of H can be inferred from the meaning of T, as would typically be interpreted by people. This somewhat informal definition is based on (and assumes) common human understanding of language as well as common background knowledge” (Dagan et al., 2006). Further gui</context>
<context position="13233" citStr="Dagan et al. (2006)" startWordPosition="2287" endWordPosition="2291">tematic differences between annotators, we were unable to fit a model with two types of instances, whereas a model with three types of instances provides a good fit both for the dataset on which it is estimated and for a new dataset. This constitutes empirical evidence for the existence of a group of instances with near-random labels in this recently 4We note that any conclusions from the model hold for the particular 800 item dataset in question, and not for the task of recognizing textual entailment in general, as the dataset is not necessarily a representative sample. In fact, we know from Dagan et al. (2006) that these 800 items are not a random sample, but rather what remained after some 400 instances were removed due to disagreements between expert annotators or due to the judgment of one of organizers of the RTE-1 challenge. 5No parameters are fitted using the BRTEb data. benchmarked dataset, at least for our pool of more than 400 non-expert annotators. 5 Could annotator heterogeneity provide an alternative explanation? In the previous section, we established that instance heterogeneity can explain the observations. We might however ask whether a different model could provide a similarly fitti</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge. In J. Qui˜nonero Candela, I. Dagan, B. Magnini, and F. d’Alch´e-Buc, editors, Machine Learning Challenges, pages 177–190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pinar Donmez</author>
<author>Jaime Carbonell</author>
<author>Jeff Schneider</author>
</authors>
<title>Efficiently Learning and Accuracy of Labeling Sources for Selective Sampling.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>259--268</pages>
<location>Paris, France.</location>
<contexts>
<context position="14095" citStr="Donmez et al., 2009" startWordPosition="2425" endWordPosition="2428"> fitted using the BRTEb data. benchmarked dataset, at least for our pool of more than 400 non-expert annotators. 5 Could annotator heterogeneity provide an alternative explanation? In the previous section, we established that instance heterogeneity can explain the observations. We might however ask whether a different model could provide a similarly fitting explanation. Specifically, heterogeneity among annotators has been seen as a major source of noise in the aggregate data and there are several works attempting to separate high quality annotators from low quality ones (Raykar et al., 2009; Donmez et al., 2009; Sheng et al., 2008; Carpenter, 2008). Could we explain the observed behavior with a model with only two types of instances that allows for annotator heterogeneity? In this section we construct such a model. We p show that this model entails an instance distribution that is a superposition of two normal distributions. We subsequently show that the best fitting two-Gaussian model does not provide a good fit. 0 t x We use a generation model similar to those in (Raykar et al., 2009; Carpenter, 2008) but with weaker parametric assumptions. The graphical model is given in figure 4. Figure 4: Annot</context>
</contexts>
<marker>Donmez, Carbonell, Schneider, 2009</marker>
<rawString>Pinar Donmez, Jaime Carbonell, and Jeff Schneider. 2009. Efficiently Learning and Accuracy of Labeling Sources for Selective Sampling. In Proceedings of the 15th International Conference on Knowledge Discovery and Data Mining, pages 259–268, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Espeland</author>
<author>Stanley Handelman</author>
</authors>
<title>Using Class Models to Characterize and Assess Relative Error in Discrete Measurements.</title>
<date>1989</date>
<journal>Biometrics,</journal>
<volume>45</volume>
<issue>2</issue>
<pages>599</pages>
<contexts>
<context position="4218" citStr="Espeland and Handelman, 1989" startWordPosition="673" endWordPosition="677">2010. c�2010 Association for Computational Linguistics ing the instance; coins biased close to 0 and 1 correspond to instances that are easy to classify; a fair coin represents instances that are very difficult if not impossible to classify correctly with the given pool of annotators. The model presented in Beigman Klebanov and Beigman (2009) is a special case with 3 types (A,B,C) where pA=0, pC=1 (easy cases), and 0&lt;pB&lt;1 represents the hard cases, the harder the closer pB is to 0.5. Models used here are a type of latent class models (McCutcheon, 1987) widely used in the Biometrics community (Espeland and Handelman, 1989; Yang and Becker, 1997; Albert et al., 2001; Albert and Dodd, 2004). The goal of modeling is to determine whether more than two types of instances need to be postulated, to estimate how difficult each type is, and to identify the troublemaking instances. The graphical model is presented in figure 1. We assume the dataset of size N is a mixture of k different types of instances. The proportion of types is given by θ = (θ1, ... , θk), and coin biases for each type are given by p = (p1, ... , pk). Each instance is annotated by n i.i.d coinflips, and random variable x E 10, ... , n} counts the nu</context>
</contexts>
<marker>Espeland, Handelman, 1989</marker>
<rawString>Mark Espeland and Stanley Handelman. 1989. Using Class Models to Characterize and Assess Relative Error in Discrete Measurements. Biometrics, 45(2):587– 599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allan McCutcheon</author>
</authors>
<title>Latent Class Analysis.</title>
<date>1987</date>
<publisher>Sage.</publisher>
<location>Newbury Park, CA, USA:</location>
<contexts>
<context position="4148" citStr="McCutcheon, 1987" startWordPosition="665" endWordPosition="666">r of the ACL, pages 438–446, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics ing the instance; coins biased close to 0 and 1 correspond to instances that are easy to classify; a fair coin represents instances that are very difficult if not impossible to classify correctly with the given pool of annotators. The model presented in Beigman Klebanov and Beigman (2009) is a special case with 3 types (A,B,C) where pA=0, pC=1 (easy cases), and 0&lt;pB&lt;1 represents the hard cases, the harder the closer pB is to 0.5. Models used here are a type of latent class models (McCutcheon, 1987) widely used in the Biometrics community (Espeland and Handelman, 1989; Yang and Becker, 1997; Albert et al., 2001; Albert and Dodd, 2004). The goal of modeling is to determine whether more than two types of instances need to be postulated, to estimate how difficult each type is, and to identify the troublemaking instances. The graphical model is presented in figure 1. We assume the dataset of size N is a mixture of k different types of instances. The proportion of types is given by θ = (θ1, ... , θk), and coin biases for each type are given by p = (p1, ... , pk). Each instance is annotated by</context>
</contexts>
<marker>McCutcheon, 1987</marker>
<rawString>Allan McCutcheon. 1987. Latent Class Analysis. Newbury Park, CA, USA: Sage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Udo Kruschwitz</author>
<author>Jon Chamberlain</author>
</authors>
<title>ANAWIKI: Creating Anaphorically Annotated Resources through Web Cooperation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation,</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="1321" citStr="Poesio et al., 2008" startWordPosition="191" endWordPosition="194"> Beigman Klebanov, 2009). In this paper, we provide quantitative empirical evidence for the existence of this type of noise in a recently benchmarked dataset. The proposed methodology can be used to zero in on unreliable instances, facilitating generation of cleaner gold standards for benchmarking. 1 Introduction Traditionally, studies in computational linguistics use few trained annotators. Lately this might be changing, as inexpensive annotators are available in large numbers through projects like Amazon Mechanical Turk or through online games where annotations are produced as a by-product (Poesio et al., 2008; von Ahn, 2006), and, at least for certain tasks, the quality of multiple non-expert annotations is close to that of a small number of experts (Snow et al., 2008; Callison-Burch, 2009). Apart from the reduced costs, mass annotation is a promising way to get detailed information about the dataset, such as the level of difficulty of the difference instances. Such information is important both from the linguistic and from the machine learning perspective, as the existence of a group of instances difficult enough to look like they have been labeled by random guesses can in the worst case induce t</context>
</contexts>
<marker>Poesio, Kruschwitz, Chamberlain, 2008</marker>
<rawString>Massimo Poesio, Udo Kruschwitz, and Jon Chamberlain. 2008. ANAWIKI: Creating Anaphorically Annotated Resources through Web Cooperation. In Proceedings of the 6th International Conference on Language Resources and Evaluation, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Raykar</author>
<author>Shipeng Yu</author>
<author>Linda Zhao</author>
<author>Anna Jerebko</author>
<author>Charles Florin</author>
<author>Gerardo Hermosillo Valadez</author>
<author>Luca Bogoni</author>
<author>Linda Moy</author>
</authors>
<title>Supervised Learning from Multiple Experts: Whom to Trust when Everyone Lies a Bit.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<pages>889--896</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="14074" citStr="Raykar et al., 2009" startWordPosition="2421" endWordPosition="2424">e. 5No parameters are fitted using the BRTEb data. benchmarked dataset, at least for our pool of more than 400 non-expert annotators. 5 Could annotator heterogeneity provide an alternative explanation? In the previous section, we established that instance heterogeneity can explain the observations. We might however ask whether a different model could provide a similarly fitting explanation. Specifically, heterogeneity among annotators has been seen as a major source of noise in the aggregate data and there are several works attempting to separate high quality annotators from low quality ones (Raykar et al., 2009; Donmez et al., 2009; Sheng et al., 2008; Carpenter, 2008). Could we explain the observed behavior with a model with only two types of instances that allows for annotator heterogeneity? In this section we construct such a model. We p show that this model entails an instance distribution that is a superposition of two normal distributions. We subsequently show that the best fitting two-Gaussian model does not provide a good fit. 0 t x We use a generation model similar to those in (Raykar et al., 2009; Carpenter, 2008) but with weaker parametric assumptions. The graphical model is given in figu</context>
</contexts>
<marker>Raykar, Yu, Zhao, Jerebko, Florin, Valadez, Bogoni, Moy, 2009</marker>
<rawString>Vikas Raykar, Shipeng Yu, Linda Zhao, Anna Jerebko, Charles Florin, Gerardo Hermosillo Valadez, Luca Bogoni, and Linda Moy. 2009. Supervised Learning from Multiple Experts: Whom to Trust when Everyone Lies a Bit. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 889–896, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Reidsma</author>
<author>Jean Carletta</author>
</authors>
<title>Reliability Measurement without Limits.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<marker>Reidsma, Carletta, 2008</marker>
<rawString>Dennis Reidsma and Jean Carletta. 2008. Reliability Measurement without Limits. Computational Linguistics, 34(3):319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Sheng</author>
<author>Foster Provost</author>
<author>Panagiotis Ipeirotis</author>
</authors>
<title>Get Another Label? Improving Data Quality and Data Mining Using Multiple, Noisy Labelers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>614--622</pages>
<location>Las Vegas, Nevada, USA.</location>
<contexts>
<context position="14115" citStr="Sheng et al., 2008" startWordPosition="2429" endWordPosition="2432">Eb data. benchmarked dataset, at least for our pool of more than 400 non-expert annotators. 5 Could annotator heterogeneity provide an alternative explanation? In the previous section, we established that instance heterogeneity can explain the observations. We might however ask whether a different model could provide a similarly fitting explanation. Specifically, heterogeneity among annotators has been seen as a major source of noise in the aggregate data and there are several works attempting to separate high quality annotators from low quality ones (Raykar et al., 2009; Donmez et al., 2009; Sheng et al., 2008; Carpenter, 2008). Could we explain the observed behavior with a model with only two types of instances that allows for annotator heterogeneity? In this section we construct such a model. We p show that this model entails an instance distribution that is a superposition of two normal distributions. We subsequently show that the best fitting two-Gaussian model does not provide a good fit. 0 t x We use a generation model similar to those in (Raykar et al., 2009; Carpenter, 2008) but with weaker parametric assumptions. The graphical model is given in figure 4. Figure 4: Annotation generation mod</context>
</contexts>
<marker>Sheng, Provost, Ipeirotis, 2008</marker>
<rawString>Victor Sheng, Foster Provost, and Panagiotis Ipeirotis. 2008. Get Another Label? Improving Data Quality and Data Mining Using Multiple, Noisy Labelers. In Proceedings of the 14th International Conference on Knowledge Discovery and Data Mining, pages 614– 622, Las Vegas, Nevada, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Cheap and Fast - But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference,</booktitle>
<pages>254--263</pages>
<location>Honolulu, Hawaii.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and Fast - But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks. In Proceedings of the Empirical Methods in Natural Language Processing Conference, pages 254–263, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
</authors>
<date>2006</date>
<journal>Games with a Purpose. Computer,</journal>
<volume>39</volume>
<issue>6</issue>
<marker>von Ahn, 2006</marker>
<rawString>Luis von Ahn. 2006. Games with a Purpose. Computer, 39(6):92–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilsoon Yang</author>
<author>Mark Becker</author>
</authors>
<title>Latent Variable Modeling of Diagnostic Accuracy.</title>
<date>1997</date>
<journal>Biometrics,</journal>
<volume>53</volume>
<issue>3</issue>
<contexts>
<context position="4241" citStr="Yang and Becker, 1997" startWordPosition="678" endWordPosition="681">omputational Linguistics ing the instance; coins biased close to 0 and 1 correspond to instances that are easy to classify; a fair coin represents instances that are very difficult if not impossible to classify correctly with the given pool of annotators. The model presented in Beigman Klebanov and Beigman (2009) is a special case with 3 types (A,B,C) where pA=0, pC=1 (easy cases), and 0&lt;pB&lt;1 represents the hard cases, the harder the closer pB is to 0.5. Models used here are a type of latent class models (McCutcheon, 1987) widely used in the Biometrics community (Espeland and Handelman, 1989; Yang and Becker, 1997; Albert et al., 2001; Albert and Dodd, 2004). The goal of modeling is to determine whether more than two types of instances need to be postulated, to estimate how difficult each type is, and to identify the troublemaking instances. The graphical model is presented in figure 1. We assume the dataset of size N is a mixture of k different types of instances. The proportion of types is given by θ = (θ1, ... , θk), and coin biases for each type are given by p = (p1, ... , pk). Each instance is annotated by n i.i.d coinflips, and random variable x E 10, ... , n} counts the number of “1”s in the n a</context>
</contexts>
<marker>Yang, Becker, 1997</marker>
<rawString>Ilsoon Yang and Mark Becker. 1997. Latent Variable Modeling of Diagnostic Accuracy. Biometrics, 53(3):948–958.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>