<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000059">
<title confidence="0.8678315">
Simple Learning and Compositional Application of Perceptually
Grounded Word Meanings for Incremental Reference Resolution
</title>
<author confidence="0.985179">
Casey Kennington
</author>
<affiliation confidence="0.97757">
CITEC, Bielefeld University
</affiliation>
<address confidence="0.579512666666667">
Universit¨atsstraf3e 25
33615 Bielefeld, Germany
ckennington@cit-ec.
</address>
<email confidence="0.589627">
uni-bielefeld.de
</email>
<sectionHeader confidence="0.990125" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999647863636364">
An elementary way of using language is
to refer to objects. Often, these objects
are physically present in the shared envi-
ronment and reference is done via men-
tion of perceivable properties of the ob-
jects. This is a type of language use that is
modelled well neither by logical semantics
nor by distributional semantics, the former
focusing on inferential relations between
expressed propositions, the latter on simi-
larity relations between words or phrases.
We present an account of word and phrase
meaning that is perceptually grounded,
trainable, compositional, and ‘dialogue-
plausible’ in that it computes meanings
word-by-word. We show that the approach
performs well (with an accuracy of 65%
on a 1-out-of-32 reference resolution task)
on direct descriptions and target/landmark
descriptions, even when trained with less
than 800 training examples and automati-
cally transcribed utterances.
</bodyText>
<sectionHeader confidence="0.999473" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.915925214285714">
The most basic, fundamental site of language use
is co-located dialogue (Fillmore, 1975; Clark,
1996) and referring to objects, as in Example (1),
is a common occurrence in such a co-located set-
ting.
(1) The green book on the left next to the mug.
Logical semantics (Montague, 1973; Gamut,
1991; Partee et al., 1993) has little to say about
this process – its focus is on the construction of
syntactically manipulable objects that model infer-
ential relations; here, e.g. the inference that there
are (at least) two objects. Vector space approaches
to distributional semantics (Turney and Pantel,
2010) similarly focuses on something else, namely
</bodyText>
<note confidence="0.9094925">
David Schlangen
CITEC, Bielefeld University
Universit¨atsstraf3e 25
33615 Bielefeld, Germany
</note>
<bodyText confidence="0.963861090909091">
david.schlangen@
uni-bielefeld.de
semantic similarity relations between words or
phrases (e.g. finding closeness for “coloured tome
on the right of the cup”). Neither approach by it-
self says anything about processing; typically, the
assumption in applications is that fully presented
phrases are being processed.
Lacking in these approaches is a notion of
grounding of symbols in features of the world
(Harnad, 1990).1 In this paper, we present an ac-
count of word and phrase meaning that is (a) per-
ceptually grounded in that it provides a link be-
tween words and (computer) vision features of real
images, (b) trainable, as that link is learned from
examples of language use, (c) compositional in
that the meaning of phrases is a function of that
of its parts and composition is driven by structural
analysis, and (d) ‘dialogue-plausible’ in that it
computes meanings incrementally, word-by-word
and can work with noisy input from an automatic
speech recogniser (ASR). We show that the ap-
proach performs well (with an accuracy of 65%
on a reference resolution task out of 32 objects) on
direct descriptions as well as target/landmark de-
scriptions, even when trained with little data (less
than 800 training examples).
In the following section we will give a back-
ground on reference resolution, followed by a de-
scription of our model. We will then describe the
data we used and explain our evaluations. We fin-
ish by giving results, providing some additional
analysis, and discussion.
</bodyText>
<sectionHeader confidence="0.985866" genericHeader="method">
2 Background: Reference Resolution
</sectionHeader>
<bodyText confidence="0.9997865">
Reference resolution (RR) is the task of resolving
referring expressions (REs; as in Example (1)) to
a referent, the entity to which they are intended to
refer. Following Kennington et al. (2015a), this
can be formalised as a function frr that, given a
representation U of the RE and a representation W
</bodyText>
<footnote confidence="0.9491375">
1But see discussion below of recent extensions of these
approaches taking this into account.
</footnote>
<page confidence="0.881484">
292
</page>
<note confidence="0.974825">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 292–301,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999453444444444">
of the (relevant aspects of the) world, returns I*,
the identifier of one the objects in the world that is
the referent of the RE. A number of recent papers
have used stochastic models for frr where, given
W and U, a distribution over a specified set of can-
didate entities in W is obtained and the probabil-
ity assigned to each entity represents the strength
of belief that it is the referent. The referent is then
the argmax:
</bodyText>
<equation confidence="0.9947">
I* = argmax P(IIU,W) (1)
I
</equation>
<bodyText confidence="0.999819448275862">
Recently, generative approaches, including our
own, have been presented (Funakoshi et al., 2012;
Kennington et al., 2013; Kennington et al., 2014;
Kennington et al., 2015b; Engonopoulos et al.,
2013) which model U as words or ngrams and
the world W as a set of objects in a virtual game
board, represented as a set properties or concepts
(in some cases, extra-linguistic or discourse as-
pects were also modelled in W, such as deixis).
In Matuszek et al. (2014), W was represented as a
distribution over properties of tangible objects and
U was a Combinatory Categorical Grammar parse.
In all of these approaches, the objects are distinct
and represented via symbolically specified prop-
erties, such as colour and shape. The set of prop-
erties is either read directly from the world if it
is virtual, or computed (i.e., discretised) from the
real world objects.
In this paper, we learn a mapping from W to
U directly, without mediating symbolic properties;
such a mapping is a kind of perceptual ground-
ing of meaning between W and U. Situated RR
is a convenient setting for learning perceptually-
grounded meaning, as objects that are referred to
are physically present, are described by the RE,
and have visual features that can be computation-
ally extracted and represented.
Further comparison to related work will be dis-
cussed in Section 5.
</bodyText>
<sectionHeader confidence="0.922863" genericHeader="method">
3 Modelling Reference to Visible Objects
</sectionHeader>
<bodyText confidence="0.998840086956522">
Overview As a representative of the kind of
model explained above with formula (1), we want
our model to compute a probability distribution
over candidate objects, given a RE (or rather, pos-
sibly just a prefix of it). We break this task down
into components: The basis of our model is a
model of word meaning as a function from per-
ceptual features of a given object to a judgement
about how well a word and that object “fit to-
gether”. (See Section 5 for discussion of prior uses
of this “words as classifiers”-approach.) This can
(loosely) be seen as corresponding to the inten-
sion of a word, which for example in Montague’s
approach is similarly modelled as a function, but
from possible worlds to extensions (Gamut, 1991).
We model two different types of words / word
meanings: those picking out properties of single
objects (e.g., “green” in “the green book”), follow-
ing Kennington et al. (2015a), and those picking
out relations of two objects (e.g., “next to” in (1)),
going beyond Kennington et al. (2015a). These
word meanings are learned from instances of lan-
guage use.
The second component then is the application
of these word meanings in the context of an actual
reference and within a phrase. This application
gives the desired result of a probability distribu-
tion over candidate objects, where the probability
expresses the strength of belief in the object falling
in the extension of the expression. Here we model
two different types of composition, of what we call
simple references and relational references. These
applications are strictly compositional in the sense
that the meanings of the more complex construc-
tions are a function of those of their parts.
Word Meanings The first type of word (or
rather, word meaning) we model picks out a sin-
gle object via its visual properties. (At least, this
is what we use here; any type of feature could be
used.) To model this, we train for each word w
from our corpus of REs a binary logistic regression
classifier that takes a representation of a candidate
object via visual features (x) and returns a proba-
bility pw for it being a good fit to the word (where
w is the weight vector that is learned and Q is the
logistic function):
</bodyText>
<equation confidence="0.999664">
pw(x) = Q(wTx + b) (2)
</equation>
<bodyText confidence="0.99909575">
Formalising the correspondence mentioned
above, the intension of a word can in this approach
then be seen as the classifier itself, a function from
a representation of an object to a probability:
</bodyText>
<equation confidence="0.994771">
Qw]obj = Ax.pw(x) (3)
</equation>
<bodyText confidence="0.99692175">
(Where Qw] denotes the meaning of w, and x is
of the type of feature given by fobj, the function
computing a feature representation for a given ob-
ject.)
</bodyText>
<page confidence="0.997253">
293
</page>
<bodyText confidence="0.999803488372093">
We train these classifiers using a corpus of REs
(further described in Section 4), coupled with rep-
resentations of the scenes in which they were used
and an annotation of the referent of that scene. The
setting was restricted to reference to single ob-
jects. To get positive training examples, we pair
each word of a RE with the features of the refer-
ent. To get negative training examples, we pair the
word with features of (randomly picked) other ob-
jects present in the same scene, but not referred to
by it. This selection of negative examples makes
the assumption that the words from the RE apply
only to the referent. This is wrong as a strict rule,
as other objects could have similar visual features
as the referent; for this to work, however, this has
to be the case only more often than it is not.
The second type of word that we model ex-
presses a relation between objects. Its meaning is
trained in a similar fashion, except that it is pre-
sented a vector of features of a pair of objects,
such as their euclidean distance, vertical and hor-
izontal differences, and binary features denoting
higher than/lower than and left/right relationships.
Application and Composition The model just
described gives us a prediction for a pair of word
and object (or pair of objects). What we wanted,
however, is a distribution over all candidate ob-
jects in a given utterance situation, and not only for
individual words, but for (incrementally growing)
REs. Again as mentioned above, we model two
types of application and composition. First, what
we call ‘simple references’—which roughly cor-
responds to simple NPs—that refer only by men-
tioning properties of the referent (e.g. “the red
cross on the left”). To get a distribution for a sin-
gle word, we apply the word classifier (the inten-
sion) to all candidate objects and normalise; this
can then be seen as the extension of the word in a
given (here, visual) discourse universe W, which
provides the candidate objects (xi is the feature
vector for object i, normalize() vectorized nor-
malisation, and I a random variable ranging over
the candidates):
</bodyText>
<equation confidence="0.998109666666667">
[w]Wobj =
normalize(([w]obj(x1),..., [w]obj(xk))) =
normalize((pw(x1),..., pw(xk))) = P(I|w) (4)
</equation>
<bodyText confidence="0.999934909090909">
In effect, this combines the individual classifiers
into something like a multi-class logistic regres-
sion / maximum entropy model—but, nota bene,
only for application. The training regime did not
need to make any assumptions about the number
of objects present, as it trained classifiers for a 2-
class problem (how well does this given object fit
to the word?). The multi-class nature is also indi-
cated in Figure 1, which shows multiple applica-
tions of the logistic regression network for a word,
and a normalisation layer on top.
</bodyText>
<figureCaption confidence="0.997897">
Figure 1: Representation as network with normalisation
layer.
</figureCaption>
<bodyText confidence="0.999975272727273">
To compose the evidence from individual words
wi, ... , wk into a prediction for a ‘simple’ RE
[srwl, ... , wk] (where the bracketing indicates the
structural assumption that the words belong to
one, possibly incomplete, ‘simple reference’), we
average the contributions of its constituent words.
The averaging function avg() over distributions
then is the contribution of the construction ‘sim-
ple reference (phrase)’, sr, and the meaning of the
whole phrase is the application of the meaning of
the construction to the meaning of the words:
</bodyText>
<equation confidence="0.733278">
[[srw1,..., wk]]W = [sr]W[w1,..., wk]W =
avg([w1]W , ... , [wk]W) (5)
</equation>
<bodyText confidence="0.747684">
where avg() is defined as
</bodyText>
<equation confidence="0.998913">
avg([w1]W, [w2]W) = Pavg(I|w1, w2)
with Pavg(I = i|w1, w2) =
12(P(I = i|w1) + P(I = i|w2)) for i ∈ I (6)
</equation>
<bodyText confidence="0.996055666666667">
The averaging function is inherently incre-
mental, in the sense that avg(a, b, c) =
avg(avg(a, b), c) and hence it can be extended “on
the right”. This represents an incremental model
where new information from the current increment
is added to what is already known, resulting in an
intersective way of composing the meaning of the
phrase. This cannot account for all constructions
(such as negation or generally quantification), of
course; we leave exploring other constructions that
could occur even in our ‘simple references’ to fu-
ture work.
</bodyText>
<equation confidence="0.984462">
o(w|x, + b) o(w|x2 + b) o(w|x3 + b)
X1 X2 X3
</equation>
<page confidence="0.987647">
294
</page>
<bodyText confidence="0.999767894736842">
Relational references such as in Exam-
ple (1) from the introduction have a more
complex structure, being a relation between a
(simple) reference to a landmark and a (sim-
ple) reference to a target. This structure is
indicated abstractly in the following ‘parse’:
[rel[srw1, ... , wk][rr1, ... , rn][srw&apos;1, ... , w&apos;m]],
where the w are the target words, r the relational
expression words, and w&apos; the landmark words.
As mentioned above, the relational expression
similarly is treated as a classifier (in fact, techni-
cally we contract expressions such as “to the left
of” into a single token and learn one classifier for
it), but expressing a judgement for pairs of objects.
It can be applied to a specific scene with a set of
candidate objects (and hence, candidate pairs) in a
similar way by applying the classifier to all pairs
and normalising, resulting in a distribution over
pairs:
</bodyText>
<equation confidence="0.997063">
[r]W = P(R1, R2|r) (7)
</equation>
<bodyText confidence="0.99989125">
We expect the meaning of the phrase to be a
function of the meaning of the constituent parts
(the simple references, the relation expression, and
the construction), that is:
</bodyText>
<equation confidence="0.986154">
[[rel[srw1, ... , wk][rr][srw01, ... , w0m]]] =
[rel]([sr][w1 ... wk], [r], [sr][w01 ... w0m]) (8)
</equation>
<bodyText confidence="0.985758529411764">
(dropping the indicator for concrete application,
W on Q ], for reasons of space and readability).
What is the contribution of the relational con-
struction, [rel]? Intuitively, what we want to
express here is that the belief in an object be-
ing the intended referent should combine the ev-
idence from the simple reference to the land-
mark object (e.g., “the mug” in (1)), from the
simple (but presumably deficient) reference to
the target object (“the green book on the left”),
and that for the relation between them (“next
to”). Instead of averaging (that is, combining
additively), as for sr, we combine this evidence
multiplicatively here: If the target constituent
contributes P(It|w1, ... , wk), the landmark con-
stituent P(Il|w&apos;1, ... , w&apos;m), and the relation ex-
pression P(R1, R2|r), with Il, It, R1 and R2 all
having the same domain, the set of all candidate
objects, then the combination is
P(R1|w1, ... ,wk, r, w01, ... , w0m) =
The last two factors force identity on the elements
of the pair and target and landmark, respectively
(they are not learnt, but rather set to be 0 unless
the values of R and I are equal), and so effectively
reduce the summations so that all pairs need to be
evaluated only once. The contribution of the con-
struction then is this multiplication of the contri-
butions of the parts, together with the factors en-
forcing that the pairs being evaluated by the rela-
tion expression consist of the objects evaluated by
target and landmark expression, respectively.
In the following section, we will explain the
data we collected and used to evaluate our model,
the evaluation procedure, and the results.
</bodyText>
<sectionHeader confidence="0.993921" genericHeader="method">
4 Experiments
</sectionHeader>
<figureCaption confidence="0.999316666666667">
Figure 2: Example episode for phase-2 where the target is
outlined in green (solid arrow added here for presentation),
the landmark outlined in blue (dashed arrow).
</figureCaption>
<bodyText confidence="0.98901035">
Data We evaluated our model using data we col-
lected in a Wizard-of-Oz setting (that is, a hu-
man/computer interaction setting where parts of
the functionality of the computer system were pro-
vided by a human experimentor). Participants
were seated in front of a table with 36 Pen-
tomino puzzle pieces that were randomly placed
with some space between them, as shown in
Figure 2. Above the table was a camera that
recorded a video feed of the objects, processed
using OpenCV (Pulli et al., 2012) to segment the
objects (see below for details); of those, one (or
one pair) was chosen randomly by the experiment
software. The video image was presented to the
participant on a display placed behind the table,
but with the randomly selected piece (or pair of
pieces) indicated by an overlay).
The task of the participant was to refer to that
object using only speech, as if identifying it for a
friend sitting next to the participant. The wizard
</bodyText>
<equation confidence="0.789663875">
P(R1, R2|r) ∗ P(Il|w01, ... , w0m)∗
P(It|w1, ... , wk) ∗ P(R1|It) ∗ P(R2|Il) (9)
E
R2
E
It
�
Il
</equation>
<page confidence="0.992954">
295
</page>
<bodyText confidence="0.999989156250001">
(experimentor) had an identical screen depicting
the scene but not the selected object. The wiz-
ard listened to the participant’s RE and clicked on
the object she thought was being referred on her
screen. If it was the target object, a tone sounded
and a new object was randomly chosen. This con-
stituted a single episode. If a wrong object was
clicked, a different tone sounded, the episode was
flagged, and a new episode began. At varied in-
tervals, the participant was instructed to “shuffle”
the board between episodes by moving around the
pieces.
The first half of the allotted time constituted
phase-1. After phase-1 was complete, instructions
for phase-2 were explained: the screen showed the
target and also a landmark object, outlined in blue,
near the target (again, see Figure 2). The partici-
pant was to refer to the target using the landmark.
(In the instructions, the concepts of landmark and
target were explained in general terms.) All other
instructions remained the same as phase-1. The
target’s identifier, which was always known be-
forehand, was always recorded. For phase-2, the
landmark’s identifier was also recorded.
Nine participants (6 female, 3 male; avg. age
of 22) took part in the study; the language of
the study was German. Phase-1 for one partici-
pant and phase-2 for another participant were not
used due to misunderstanding and a technical diffi-
culty. This produced a corpus of 870 non-flagged
episodes in total. Even though each episode had
36 objects in the scene, all objects were not always
recognised by the computer vision processing. On
average, 32 objects were recognized.
To obtain transcriptions, we used Google Web
Speech (with a word error rate of 0.65, as deter-
mined by comparing to a hand transcribed sample)
This resulted in 1587 distinct words, with 15.53
words on average per episode. The objects were
not manipulated in any way during an episode, so
the episode was guaranteed to remain static during
a RE and a single image is sufficient to represent
the layout of one episode’s scene. Each scene was
processed using computer vision techniques to ob-
tain low-level features for each (detected) object in
the scene which were used for the word classifiers.
We annotated each episode’s RE with a simple
tagging scheme that segmented the RE into words
that directly referred to the target, words that di-
rectly referred to the landmark (or multiple land-
marks, in some cases) and the relation words. For
certain word types, additional information about
the word was included in the tag if it described
colour, shape, or spatial placement (denoted con-
tributing REs in the evaluations below). The direc-
tion of certain relation words was normalised (e.g.,
left-of should always denote a landmark-target re-
lation). This represents a minimal amount of “syn-
tactic” information needed for the application of
the classifiers and the composition of the phrase
meanings. We leave applying a syntactic parser to
future work. An example RE in the original Ger-
man (as recognised by the ASR), English gloss,
and tags for each word is given in (2).
</bodyText>
<listItem confidence="0.750781333333333">
(2) a. grauer stein ¨uber dem gr¨unen m unten links
b. gray block above the green m bottom left
c. tc ts r l lc ls tf tf
</listItem>
<bodyText confidence="0.99946785">
To obtain visual features of each object, we used
the same simple computer-vision pipeline of ob-
ject segmentation and contour reconstruction as
used by Kennington et al. (2015a), providing us
with RGB representations for the colour and fea-
tures such as skewness, number of edges etc. for
the shapes.
Procedure We break down our data as follows:
episodes where the target was referred directly
via a ‘simple reference’ construction (DD; 410
episodes) and episodes where a target was referred
via a landmark relation (RD; 460 episodes). We
also test with either knowledge about structure
(simple or relational reference) provided (ST) or
not (WO, for “words-only”). All results shown are
from 10-fold cross validations averaged over 10
runs; where for evaluations labelled RD the train-
ing data always includes all of DD plus 9 folds of
RD, testing on RD. The sets address the following
questions:
</bodyText>
<listItem confidence="0.994546272727273">
• how well does the sr model work on its own
with just words? – DD.WO
• how well does the sr model work when it
knows about REs? – DD.ST
• how well does the sr model work when it
knows about REs, but not about relations? –
RD.ST (sr)
• how well does the model learn relation words
after it has learned about sr? RD.ST (r)
• how well does the rr model work (together
with the sr)? RD.ST with DD.ST (rr)
</listItem>
<bodyText confidence="0.8201955">
Words were stemmed using the NLTK (Loper
and Bird, 2002) Snowball Stemmer, reducing the
</bodyText>
<page confidence="0.99615">
296
</page>
<bodyText confidence="0.99997290625">
vocabulary size to 1306. Due to sparsity, for rela-
tion words with a token count of less than 4 (found
by ranging over values in a held-out set) relational
features were piped into an UNK relation, which
was used for unseen relations during evaluation
(we assume the UNK relation would learn a gen-
eral notion of ‘nearness’). For the individual word
classifiers, we always paired one negative example
with one positive example.
For this evaluation, word classifiers for sr were
given the following features: RGB values, HSV
values, x and y coordinates of the centroids, eu-
clidean distance of centroid from the center, and
number of edges. The relation classifiers received
information relating two objects, namely the eu-
clidean distance between them, the vertical and
horizontal distances, and two binary features that
denoted if the landmark was higher than/lower
than or left/right of the target.
Metrics for Evaluation To give a picture of the
overall performance of the model, we report accu-
racy (how often was the argmax the gold target)
and mean reciprocal rank (MRR) of the gold tar-
get in the distribution over all the objects (like ac-
curacy, higher MRR values are better; values range
between 0 and 1). The use of MRR is motivated by
the assumption that in general, a good rank for the
correct object is desirable, even if it doesn’t reach
the first position, as when integrated in a dialogue
system this information might still be useful to for-
mulate clarification questions.
Results Figure 3 shows the results. (Random
baseline of 1/32 or 3% not shown in plot.) DD.WO
shows how well the sr model performs using the
whole utterances and not just the REs. (Note that
all evaluations are on noisy ASR transcriptions.)
DD.ST adds structure by only considering words
that are part of the actual RE, improving the re-
sults further. The remaining sets evaluate the con-
tributions of the rr model. RD.ST (sr) does this
indirectly, by including the target and landmark
simple references, but not the model for the rela-
tions; the task here is to resolve target and land-
mark SRs as they are. This provides the baseline
for the next two evaluations, which include the re-
lation model. In RD.ST (sr+r), the model learns
SRs from DD data and only relations from RD. The
performance is substantially better than the base-
line without the relation model. Performance is
best finally for RD.ST (rr), where the landmark
and target SRs in the training portion of RD also
contribute to the word models.
The mean reciprocal rank scores follow a sim-
ilar pattern and show that even though the target
object was not the argmax of the distribution, on
average it was high in the distribution. For all eval-
uations, the average standard deviation across the
10 runs was very small (0.01), meaning the model
was fairly stable, despite the possibility of one run
having randomly chosen more discriminating neg-
ative examples. Our conclusion from these exper-
iments is that despite the small amount of training
data and noise from ASR as well as the scene, the
model is robust and yields respectable results.
</bodyText>
<figure confidence="0.9828">
25
20
15
10
5
0
5
0 2 4 6 8 10 12 14
</figure>
<figureCaption confidence="0.987287">
Figure 5: Incremental results: average rank improves over
time
</figureCaption>
<bodyText confidence="0.9991585">
Incremental Results Figure 5 shows how our
rr model processes incrementally, by giving the
average rank of the (gold) target at each increment
for the REs with the most common length in our
data (13 words, of which there were 64 examples).
A system that works incrementally would have a
monotonically decreasing average rank as the ut-
terance unfolds. The overall trend as shown in that
</bodyText>
<figure confidence="0.988418416666667">
50 %
40.9 %
70 %
65.3 %
60 %
40 %
30 %
20 %
10 %
0 %
DD. DD. RD. RD. RD. DD. DD. RD. RD. RD.
WO ST ST(sr) ST(sr+r) ST(rr) WO ST ST(sr) ST(sr+r) ST(rr)
</figure>
<figureCaption confidence="0.98156">
Figure 3: Results of our evaluation.
</figureCaption>
<figure confidence="0.995142727272727">
accuracy
mean reciprocal rank
54 %
55 %
42 %
0.8
0.7
0.68
0.759
0.608
0.6
0.5
0.4
0.3
0.2
0.1
0
0.56
0.68
297
1.0
0.8
0.6
0.4
0.2
0.00 50 100 150 200 250
0.9
0.8
400
0.7
0.6
300
0.5
0.4
200
0.3
0.2
100
0.1
100 200 300 400 500 600
100 200 300 400 500 600
0.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
400
300
200
100
</figure>
<figureCaption confidence="0.94783325">
Figure 4: Each plot represents how well selected words fit assumptions about their lexical semantics: the leftmost plot ecke
(corner) yields higher probabilities as objects are closer to the corner; the middle plot gr¨un (green) yields higher probabilities
when the colour spectrum values are nearer to green; the rightmost plot ¨uber (above) yields higher probabilities when targets
are nearer to a landmark set in the middle.
</figureCaption>
<bodyText confidence="0.6592966">
Figure is as expected. There is a slight increase
between 6-7, though very small (a difference of
0.09). Overall, these results seem to show that our
model indeed works intersectively and “zooms in”
on the intended referent.
</bodyText>
<subsectionHeader confidence="0.994846">
4.1 Further Analysis
</subsectionHeader>
<bodyText confidence="0.999759616666667">
Analysis of Selected Words We analysed sev-
eral individual word classifiers to determine how
well their predictions match assumptions about
their lexical semantics. For example, for the spa-
tial word Ecke (corner), we would expect its clas-
sifier to return high probabilities if features related
to an object’s position (e.g., x and y coordinates,
distance from the center) are near corners of the
scene. The leftmost plot in Figure 4 shows that
this is indeed the case; by holding all non-position
features constant and ranging over all points on
the screen, we can see that the classifier gives high
probabilities around the edges, particularly in the
four corners, and very low probabilities in the mid-
dle region. Similarly for the colour word gr¨un,
the centre plot in Figure 4 (overlaid with a colour
spectrum) shows high probabilities are given when
presented with the colour green, as expected. Sim-
ilarly, for the relational word ¨uber (above), by
treating the center point as the landmark and rang-
ing over all other points on the plot for the target,
the ¨uber classifier gives high probabilities when
directly above the center point, with linear nega-
tive growth as the distance from the landmark in-
creases.
Note that we selected the type of feature to vary
here for presentation; all classifiers get the full fea-
ture set and learn automatically to “ignore” the ir-
relevant features (e.g., that for gr¨un does not re-
spond to variations in positional features). They
do this wuite well, but we noticed some ‘blurring’,
due to not all combinations of colours and shape
being represented in the objects in the training set.
Analysis of Incremental Processing Figure 6
finally shows the interpretation of the RE in Ex-
ample (2) in the scene from Figure 2. The top
row depicts the distribution over objects (true tar-
get shown in red) after the relation word unten
(bottom) is uttered; the second row that for land-
mark objects, after the landmark description be-
gins (dem gr¨unen m / the green m). The third row
(target objects), ceases to change after the rela-
tional word is uttered, but continues again as ad-
ditional target words are uttered (unten links / bot-
tom left). While the true target is ranked highly
already on the basis of the target SR alone, it is
only when the relational information is added (top
row) that it becomes argmax.
Discussion We did not explore how well our
model could handle generalised quantifiers, such
as all (e.g., all the red objects) or a specific num-
ber of objects (e.g., the two green Ts). We specu-
late that one could see as the contribution of words
such as all or two a change to how the distribution
is evaluated (“return the n top candidates”). Our
model also doesn’t yet directly handle more de-
scriptive REs like the cross in the top-right corner
on the left, as left is learned as a global term, or
negation (the cross that’s not red). We leave ex-
ploring such constructions to future work.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9998629">
Kelleher et al. (2005) approached RR us-
ing perceptually-grounded models, focusing on
saliency and discourse context. In Gorniak and
Roy (2004), descriptions of objects were used to
learn a perceptually-grounded meaning with focus
on spatial terms such as on the left. Steels and
Belpaeme (2005) used neural networks to connect
language with colour terms by interacting with hu-
mans. Larsson (2013) is closest in spirit to what
we are attempting here; he provides a detailed
</bodyText>
<page confidence="0.991148">
298
</page>
<bodyText confidence="0.717532">
grauer stein über dem grünen m unten links
</bodyText>
<figureCaption confidence="0.997172">
Figure 6: A depiction of the model working incrementally for the RE in Example (2): the distribution over objects for relation
is row 1, landmark is row 2, target is row 3.
</figureCaption>
<bodyText confidence="0.999984578947369">
formal semantics for similarly descriptive terms,
where parts of the semantics are modelled by a
perceptual classifier. These approaches had lim-
ited lexicons (where we attempt to model all words
in our corpus), and do not process incrementally,
which we do here.
Recent efforts in multimodal distributional se-
mantics have also looked at modelling word mean-
ing based on visual context. Originally, vector
space distributional semantics focused words in
the context of other words (Turney and Pantel,
2010); recent multimodal approaches also con-
sider low-level features from images. Bruni et
al. (2012) and Bruni et al. (2014) for example
model word meaning by word and visual con-
text; each modality is represented by a vector,
fused by concatenation. Socher et al. (2014)
and Kiros et al. (2014) present approaches where
words/phrases and images are mapped into the
same high-dimensional space. While these ap-
proaches similarly provide a link between words
and images, they are typically tailored towards
a different setting (the words being descriptions
of the whole image, and not utterance intended
to perform a function within a visual situation).
We leave more detailed exploration of similarities
and differences to future work and only note for
now that our approach, relying on much simpler
classifiers (log-linear, basically), works with much
smaller data sets and additionally seem to pro-
vide an easier interface to more traditional ways
of composition (see Section 3 above).
The issue of semantic compositionality is also
actively discussed in the distributional semantics
literature (see, e.g., (Mitchell and Lapata, 2010;
Erk, 2013; Lewis and Steedman, 2013; Paperno
et al., 2014)), investigating how to combine vec-
tors. This could be seen as composition on the
level of intensions (if one sees distributional rep-
resentations as intensions, as is variously hinted
at, e.g. Erk (2013)). In our approach, composition
is done on the extensional level (by interpolating
distributions over candidate objects).
We do not see our approach as being in op-
position to these attempts. Rather, we envision
a system of semantics that combines traditional
symbolic expressions (on which inferences can
be modelled via syntactic calculi) with distributed
representations (which model conceptual knowl-
edge / semantic networks, as well as encyclopedic
knowledge) and with our action-based (namely,
identification in the environment via perceptual
information) semantics. This line of approach
is connected to a number of recent works (e.g.,
(Erk, 2013; Lewis and Steedman, 2013; Larsson,
2013)); for now, exploring its ramifications is left
for future work.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999930538461539">
In this paper, we presented a model of reference
resolution that learns a perceptually-grounded
meaning of words, including relational words. The
model is simple, compositional, and robust despite
low amounts of training data and noisy modalities.
Our model is not without limitations; it so far only
handles definite descriptions, yet there are other
ways to refer to real-world objects, such as via pro-
nouns and deixis. A unified model that can handle
all of these, similar in spirit perhaps to Funakoshi
et al. (2012), but with perceptual groundings, is
left for future work. Our approach could also ben-
efit from improved object segmentation and repre-
</bodyText>
<page confidence="0.993413">
299
</page>
<bodyText confidence="0.9987066875">
sentation.
Our next steps with this model is to handle com-
positional structures without relying on our closed
tag set (e.g., using a syntactic parser). We also
plan to test our model in a natural, interactive dia-
logue system.
Acknowledgements We want to thank the anonymous
reviewers for their comments. We also want to thank Spy-
ros Kousidis for helping with data collection, Livia Dia for
help with the computer vision processing, and Julian Hough
for fruitful discussions on semantics, though we can’t blame
them for any problems of the work that may remain. This re-
search/work was supported by the Cluster of Excellence Cog-
nitive Interaction Technology ’CITEC’ (EXC 277) at Biele-
feld University, which is funded by the German Research
Foundation (DFG).
</bodyText>
<sectionHeader confidence="0.9994" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999920423529412">
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics, volume 1, pages 136–145.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1–47.
Herbert H Clark. 1996. Using Language, volume 23.
Cambridge University Press.
Nikos Engonopoulos, Martin Villalba, Ivan Titov, and
Alexander Koller. 2013. Predicting the resolu-
tion of referring expressions from user behavior. In
Proceedings of EMLNP, pages 1354–1359, Seattle,
Washington, USA. Association for Computational
Linguistics.
Katrin Erk. 2013. Towards a semantics for distri-
butional representations. In Proceedings of IWCS,
pages 1–11, Potsdam, Germany.
Charles J Fillmore. 1975. Pragmatics and the descrip-
tion of discourse. Radical pragmatics, pages 143–
166.
Kotaro Funakoshi, Mikio Nakano, Takenobu Toku-
naga, and Ryu Iida. 2012. A Unified Probabilis-
tic Approach to Referring Expressions. In Proceed-
ings of SIGDial, pages 237–246, Seoul, South Ko-
rea, July. Association for Computational Linguistics.
L T F Gamut. 1991. Logic, Language and Meaning:
Intensional Logic and Logical Grammar, volume 2.
Chicago University Press, Chicago.
Peter Gorniak and Deb Roy. 2004. Grounded semantic
composition for visual scenes. Journal of Artificial
Intelligence Research, 21:429–470.
Stevan Harnad. 1990. The Symbol Grounding Prob-
lem. Physica D, 42:335–346.
John Kelleher, Fintan Costello, and Jofsef Van Gen-
abith. 2005. Dynamically structuring, updating
and interrelating representations of visual and lin-
guistic discourse context. Artificial Intelligence,
167(1–2):62–102.
Casey Kennington, Spyros Kousidis, and David
Schlangen. 2013. Interpreting Situated Dialogue
Utterances: an Update Model that Uses Speech,
Gaze, and Gesture Information. In Proceedings of
SIGdial.
Casey Kennington, Spyros Kousidis, and David
Schlangen. 2014. Situated Incremental Natu-
ral Language Understanding using a Multimodal,
Linguistically-driven Update Model. In Proceed-
ings of CoLing.
Casey Kennington, Livia Dia, and David Schlangen.
2015a. A Discriminative Model for Perceptually-
Grounded Incremental Reference Resolution. In
Proceedings of IWCS. Association for Computa-
tional Linguistics.
Casey Kennington, Ryu Iida, Takenobu Tokunaga, and
David Schlangen. 2015b. Incrementally Track-
ing Reference in Human/Human Dialogue Using
Linguistic and Extra-Linguistic Information. In
NAACL, Denver, U.S.A. Association for Computa-
tional Linguistics.
Ryan Kiros, Ruslan Salakhutdinov, and Richard S
Zemel. 2014. Unifying Visual-Semantic Embed-
dings with Multimodal Neural Language Models.
In Proceedings of NIPS 2014 Deep Learning Work-
shop, pages 1–13.
Staffan Larsson. 2013. Formal semantics for percep-
tual classification. Journal of Logic and Computa-
tion.
Mike Lewis and Mark Steedman. 2013. Combined
Distributional and Logical Semantics. Transactions
of the ACL, 1:179–192.
Edward Loper and Steven Bird. 2002. NLTK: The nat-
ural language toolkit. In Proceedings of the ACL-02
Workshop on Effective tools and methodologies for
teaching natural language processing and computa-
tional linguistics-Volume 1, pages 63–70. Associa-
tion for Computational Linguistics.
Cynthia Matuszek, Liefeng Bo, Luke Zettlemoyer, and
Dieter Fox. 2014. Learning from Unscripted Deic-
tic Gesture and Language for Human-Robot Interac-
tions. In AAAI. AAAI Press.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive sci-
ence, 34(8):1388–1429, November.
</reference>
<page confidence="0.970758">
300
</page>
<reference confidence="0.9999157">
Richard Montague. 1973. The Proper Treatment of
Quantifikation in Ordinary English. In J Hintikka,
J Moravcsik, and P Suppes, editors, Approaches to
Natural Language: Proceedings of the 1970 Stan-
ford Workshop on Grammar and Semantics, pages
221–242, Dordrecht. Reidel.
Denis Paperno, Nghia The Pham, and Marco Baroni.
2014. A practical and linguistically-motivated ap-
proach to compositional distributional semantics. In
Proceedings ofACL, pages 90–99.
Barbara H Partee, Alice ter Meuelen, and Robert E
Wall. 1993. Mathematical Methods in Linguistics.
Kluwer Academic Publishers, Dordrecht.
Kari Pulli, Anatoly Baksheev, Kirill Kornyakov, and
Victor Eruhimov. 2012. Real-time computer vi-
sion with OpenCV. Communications of the ACM,
55(6):61–69.
Richard Socher, Andrej Karpathy, Quoc V Le, Christo-
pher D Manning, and Andrew Y Ng. 2014.
Grounded Compositional Semantics for Finding and
Describing Images with Sentences. Transactions
of the Association for Computational Linguistics
(TACL), 2:207–218.
Luc Steels and Tony Belpaeme. 2005. Coordinating
perceptually grounded categories through language:
a case study for colour. The Behavioral and brain
sciences, 28(4):469–489; discussion 489–529.
Peter D Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Seman-
tics. Artificial Intelligence, 37(1):141–188.
</reference>
<page confidence="0.998919">
301
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.835995">
<title confidence="0.9969065">Simple Learning and Compositional Application of Perceptually Grounded Word Meanings for Incremental Reference Resolution</title>
<author confidence="0.996489">Casey</author>
<affiliation confidence="0.9268725">CITEC, Bielefeld Universit¨atsstraf3e</affiliation>
<address confidence="0.993279">33615 Bielefeld,</address>
<email confidence="0.999102">uni-bielefeld.de</email>
<abstract confidence="0.999360956521739">An elementary way of using language is objects. Often, these objects are physically present in the shared environment and reference is done via mention of perceivable properties of the objects. This is a type of language use that is modelled well neither by logical semantics nor by distributional semantics, the former focusing on inferential relations between expressed propositions, the latter on similarity relations between words or phrases. We present an account of word and phrase meaning that is perceptually grounded, trainable, compositional, and ‘dialogueplausible’ in that it computes meanings word-by-word. We show that the approach performs well (with an accuracy of 65% on a 1-out-of-32 reference resolution task) on direct descriptions and target/landmark descriptions, even when trained with less than 800 training examples and automatically transcribed utterances.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>NamKhanh Tran</author>
</authors>
<title>Distributional semantics in technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>136--145</pages>
<contexts>
<context position="30223" citStr="Bruni et al. (2012)" startWordPosition="5125" endWordPosition="5128">, target is row 3. formal semantics for similarly descriptive terms, where parts of the semantics are modelled by a perceptual classifier. These approaches had limited lexicons (where we attempt to model all words in our corpus), and do not process incrementally, which we do here. Recent efforts in multimodal distributional semantics have also looked at modelling word meaning based on visual context. Originally, vector space distributional semantics focused words in the context of other words (Turney and Pantel, 2010); recent multimodal approaches also consider low-level features from images. Bruni et al. (2012) and Bruni et al. (2014) for example model word meaning by word and visual context; each modality is represented by a vector, fused by concatenation. Socher et al. (2014) and Kiros et al. (2014) present approaches where words/phrases and images are mapped into the same high-dimensional space. While these approaches similarly provide a link between words and images, they are typically tailored towards a different setting (the words being descriptions of the whole image, and not utterance intended to perform a function within a visual situation). We leave more detailed exploration of similaritie</context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and NamKhanh Tran. 2012. Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, volume 1, pages 136–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>49--1</pages>
<contexts>
<context position="30247" citStr="Bruni et al. (2014)" startWordPosition="5130" endWordPosition="5133">l semantics for similarly descriptive terms, where parts of the semantics are modelled by a perceptual classifier. These approaches had limited lexicons (where we attempt to model all words in our corpus), and do not process incrementally, which we do here. Recent efforts in multimodal distributional semantics have also looked at modelling word meaning based on visual context. Originally, vector space distributional semantics focused words in the context of other words (Turney and Pantel, 2010); recent multimodal approaches also consider low-level features from images. Bruni et al. (2012) and Bruni et al. (2014) for example model word meaning by word and visual context; each modality is represented by a vector, fused by concatenation. Socher et al. (2014) and Kiros et al. (2014) present approaches where words/phrases and images are mapped into the same high-dimensional space. While these approaches similarly provide a link between words and images, they are typically tailored towards a different setting (the words being descriptions of the whole image, and not utterance intended to perform a function within a visual situation). We leave more detailed exploration of similarities and differences to fut</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artificial Intelligence Research, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
</authors>
<title>Using Language, volume 23.</title>
<date>1996</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1273" citStr="Clark, 1996" startWordPosition="178" endWordPosition="179">he latter on similarity relations between words or phrases. We present an account of word and phrase meaning that is perceptually grounded, trainable, compositional, and ‘dialogueplausible’ in that it computes meanings word-by-word. We show that the approach performs well (with an accuracy of 65% on a 1-out-of-32 reference resolution task) on direct descriptions and target/landmark descriptions, even when trained with less than 800 training examples and automatically transcribed utterances. 1 Introduction The most basic, fundamental site of language use is co-located dialogue (Fillmore, 1975; Clark, 1996) and referring to objects, as in Example (1), is a common occurrence in such a co-located setting. (1) The green book on the left next to the mug. Logical semantics (Montague, 1973; Gamut, 1991; Partee et al., 1993) has little to say about this process – its focus is on the construction of syntactically manipulable objects that model inferential relations; here, e.g. the inference that there are (at least) two objects. Vector space approaches to distributional semantics (Turney and Pantel, 2010) similarly focuses on something else, namely David Schlangen CITEC, Bielefeld University Universit¨a</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>Herbert H Clark. 1996. Using Language, volume 23. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikos Engonopoulos</author>
<author>Martin Villalba</author>
<author>Ivan Titov</author>
<author>Alexander Koller</author>
</authors>
<title>Predicting the resolution of referring expressions from user behavior.</title>
<date>2013</date>
<booktitle>In Proceedings of EMLNP,</booktitle>
<pages>1354--1359</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="4741" citStr="Engonopoulos et al., 2013" startWordPosition="734" endWordPosition="737">f the) world, returns I*, the identifier of one the objects in the world that is the referent of the RE. A number of recent papers have used stochastic models for frr where, given W and U, a distribution over a specified set of candidate entities in W is obtained and the probability assigned to each entity represents the strength of belief that it is the referent. The referent is then the argmax: I* = argmax P(IIU,W) (1) I Recently, generative approaches, including our own, have been presented (Funakoshi et al., 2012; Kennington et al., 2013; Kennington et al., 2014; Kennington et al., 2015b; Engonopoulos et al., 2013) which model U as words or ngrams and the world W as a set of objects in a virtual game board, represented as a set properties or concepts (in some cases, extra-linguistic or discourse aspects were also modelled in W, such as deixis). In Matuszek et al. (2014), W was represented as a distribution over properties of tangible objects and U was a Combinatory Categorical Grammar parse. In all of these approaches, the objects are distinct and represented via symbolically specified properties, such as colour and shape. The set of properties is either read directly from the world if it is virtual, or</context>
</contexts>
<marker>Engonopoulos, Villalba, Titov, Koller, 2013</marker>
<rawString>Nikos Engonopoulos, Martin Villalba, Ivan Titov, and Alexander Koller. 2013. Predicting the resolution of referring expressions from user behavior. In Proceedings of EMLNP, pages 1354–1359, Seattle, Washington, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Towards a semantics for distributional representations.</title>
<date>2013</date>
<booktitle>In Proceedings of IWCS,</booktitle>
<pages>1--11</pages>
<location>Potsdam, Germany.</location>
<contexts>
<context position="31266" citStr="Erk, 2013" startWordPosition="5289" endWordPosition="5290"> being descriptions of the whole image, and not utterance intended to perform a function within a visual situation). We leave more detailed exploration of similarities and differences to future work and only note for now that our approach, relying on much simpler classifiers (log-linear, basically), works with much smaller data sets and additionally seem to provide an easier interface to more traditional ways of composition (see Section 3 above). The issue of semantic compositionality is also actively discussed in the distributional semantics literature (see, e.g., (Mitchell and Lapata, 2010; Erk, 2013; Lewis and Steedman, 2013; Paperno et al., 2014)), investigating how to combine vectors. This could be seen as composition on the level of intensions (if one sees distributional representations as intensions, as is variously hinted at, e.g. Erk (2013)). In our approach, composition is done on the extensional level (by interpolating distributions over candidate objects). We do not see our approach as being in opposition to these attempts. Rather, we envision a system of semantics that combines traditional symbolic expressions (on which inferences can be modelled via syntactic calculi) with dis</context>
</contexts>
<marker>Erk, 2013</marker>
<rawString>Katrin Erk. 2013. Towards a semantics for distributional representations. In Proceedings of IWCS, pages 1–11, Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Pragmatics and the description of discourse. Radical pragmatics,</title>
<date>1975</date>
<pages>143--166</pages>
<contexts>
<context position="1259" citStr="Fillmore, 1975" startWordPosition="176" endWordPosition="177"> propositions, the latter on similarity relations between words or phrases. We present an account of word and phrase meaning that is perceptually grounded, trainable, compositional, and ‘dialogueplausible’ in that it computes meanings word-by-word. We show that the approach performs well (with an accuracy of 65% on a 1-out-of-32 reference resolution task) on direct descriptions and target/landmark descriptions, even when trained with less than 800 training examples and automatically transcribed utterances. 1 Introduction The most basic, fundamental site of language use is co-located dialogue (Fillmore, 1975; Clark, 1996) and referring to objects, as in Example (1), is a common occurrence in such a co-located setting. (1) The green book on the left next to the mug. Logical semantics (Montague, 1973; Gamut, 1991; Partee et al., 1993) has little to say about this process – its focus is on the construction of syntactically manipulable objects that model inferential relations; here, e.g. the inference that there are (at least) two objects. Vector space approaches to distributional semantics (Turney and Pantel, 2010) similarly focuses on something else, namely David Schlangen CITEC, Bielefeld Universi</context>
</contexts>
<marker>Fillmore, 1975</marker>
<rawString>Charles J Fillmore. 1975. Pragmatics and the description of discourse. Radical pragmatics, pages 143– 166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kotaro Funakoshi</author>
<author>Mikio Nakano</author>
<author>Takenobu Tokunaga</author>
<author>Ryu Iida</author>
</authors>
<title>A Unified Probabilistic Approach to Referring Expressions.</title>
<date>2012</date>
<booktitle>In Proceedings of SIGDial,</booktitle>
<pages>237--246</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seoul, South</location>
<contexts>
<context position="4637" citStr="Funakoshi et al., 2012" startWordPosition="718" endWordPosition="721">China, July 26-31, 2015. c�2015 Association for Computational Linguistics of the (relevant aspects of the) world, returns I*, the identifier of one the objects in the world that is the referent of the RE. A number of recent papers have used stochastic models for frr where, given W and U, a distribution over a specified set of candidate entities in W is obtained and the probability assigned to each entity represents the strength of belief that it is the referent. The referent is then the argmax: I* = argmax P(IIU,W) (1) I Recently, generative approaches, including our own, have been presented (Funakoshi et al., 2012; Kennington et al., 2013; Kennington et al., 2014; Kennington et al., 2015b; Engonopoulos et al., 2013) which model U as words or ngrams and the world W as a set of objects in a virtual game board, represented as a set properties or concepts (in some cases, extra-linguistic or discourse aspects were also modelled in W, such as deixis). In Matuszek et al. (2014), W was represented as a distribution over properties of tangible objects and U was a Combinatory Categorical Grammar parse. In all of these approaches, the objects are distinct and represented via symbolically specified properties, suc</context>
<context position="32809" citStr="Funakoshi et al. (2012)" startWordPosition="5525" endWordPosition="5528">man, 2013; Larsson, 2013)); for now, exploring its ramifications is left for future work. 6 Conclusion In this paper, we presented a model of reference resolution that learns a perceptually-grounded meaning of words, including relational words. The model is simple, compositional, and robust despite low amounts of training data and noisy modalities. Our model is not without limitations; it so far only handles definite descriptions, yet there are other ways to refer to real-world objects, such as via pronouns and deixis. A unified model that can handle all of these, similar in spirit perhaps to Funakoshi et al. (2012), but with perceptual groundings, is left for future work. Our approach could also benefit from improved object segmentation and repre299 sentation. Our next steps with this model is to handle compositional structures without relying on our closed tag set (e.g., using a syntactic parser). We also plan to test our model in a natural, interactive dialogue system. Acknowledgements We want to thank the anonymous reviewers for their comments. We also want to thank Spyros Kousidis for helping with data collection, Livia Dia for help with the computer vision processing, and Julian Hough for fruitful </context>
</contexts>
<marker>Funakoshi, Nakano, Tokunaga, Iida, 2012</marker>
<rawString>Kotaro Funakoshi, Mikio Nakano, Takenobu Tokunaga, and Ryu Iida. 2012. A Unified Probabilistic Approach to Referring Expressions. In Proceedings of SIGDial, pages 237–246, Seoul, South Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L T F Gamut</author>
</authors>
<title>Logic, Language and Meaning: Intensional Logic and Logical Grammar,</title>
<date>1991</date>
<volume>2</volume>
<publisher>Chicago University Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="1466" citStr="Gamut, 1991" startWordPosition="213" endWordPosition="214">hat it computes meanings word-by-word. We show that the approach performs well (with an accuracy of 65% on a 1-out-of-32 reference resolution task) on direct descriptions and target/landmark descriptions, even when trained with less than 800 training examples and automatically transcribed utterances. 1 Introduction The most basic, fundamental site of language use is co-located dialogue (Fillmore, 1975; Clark, 1996) and referring to objects, as in Example (1), is a common occurrence in such a co-located setting. (1) The green book on the left next to the mug. Logical semantics (Montague, 1973; Gamut, 1991; Partee et al., 1993) has little to say about this process – its focus is on the construction of syntactically manipulable objects that model inferential relations; here, e.g. the inference that there are (at least) two objects. Vector space approaches to distributional semantics (Turney and Pantel, 2010) similarly focuses on something else, namely David Schlangen CITEC, Bielefeld University Universit¨atsstraf3e 25 33615 Bielefeld, Germany david.schlangen@ uni-bielefeld.de semantic similarity relations between words or phrases (e.g. finding closeness for “coloured tome on the right of the cup</context>
<context position="6643" citStr="Gamut, 1991" startWordPosition="1065" endWordPosition="1066">e a probability distribution over candidate objects, given a RE (or rather, possibly just a prefix of it). We break this task down into components: The basis of our model is a model of word meaning as a function from perceptual features of a given object to a judgement about how well a word and that object “fit together”. (See Section 5 for discussion of prior uses of this “words as classifiers”-approach.) This can (loosely) be seen as corresponding to the intension of a word, which for example in Montague’s approach is similarly modelled as a function, but from possible worlds to extensions (Gamut, 1991). We model two different types of words / word meanings: those picking out properties of single objects (e.g., “green” in “the green book”), following Kennington et al. (2015a), and those picking out relations of two objects (e.g., “next to” in (1)), going beyond Kennington et al. (2015a). These word meanings are learned from instances of language use. The second component then is the application of these word meanings in the context of an actual reference and within a phrase. This application gives the desired result of a probability distribution over candidate objects, where the probability </context>
</contexts>
<marker>Gamut, 1991</marker>
<rawString>L T F Gamut. 1991. Logic, Language and Meaning: Intensional Logic and Logical Grammar, volume 2. Chicago University Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Gorniak</author>
<author>Deb Roy</author>
</authors>
<title>Grounded semantic composition for visual scenes.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>21--429</pages>
<contexts>
<context position="29072" citStr="Gorniak and Roy (2004)" startWordPosition="4938" endWordPosition="4941"> or a specific number of objects (e.g., the two green Ts). We speculate that one could see as the contribution of words such as all or two a change to how the distribution is evaluated (“return the n top candidates”). Our model also doesn’t yet directly handle more descriptive REs like the cross in the top-right corner on the left, as left is learned as a global term, or negation (the cross that’s not red). We leave exploring such constructions to future work. 5 Related Work Kelleher et al. (2005) approached RR using perceptually-grounded models, focusing on saliency and discourse context. In Gorniak and Roy (2004), descriptions of objects were used to learn a perceptually-grounded meaning with focus on spatial terms such as on the left. Steels and Belpaeme (2005) used neural networks to connect language with colour terms by interacting with humans. Larsson (2013) is closest in spirit to what we are attempting here; he provides a detailed 298 grauer stein über dem grünen m unten links Figure 6: A depiction of the model working incrementally for the RE in Example (2): the distribution over objects for relation is row 1, landmark is row 2, target is row 3. formal semantics for similarly descriptive terms,</context>
</contexts>
<marker>Gorniak, Roy, 2004</marker>
<rawString>Peter Gorniak and Deb Roy. 2004. Grounded semantic composition for visual scenes. Journal of Artificial Intelligence Research, 21:429–470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stevan Harnad</author>
</authors>
<date>1990</date>
<booktitle>The Symbol Grounding Problem. Physica D,</booktitle>
<pages>42--335</pages>
<contexts>
<context position="2327" citStr="Harnad, 1990" startWordPosition="338" endWordPosition="339">aches to distributional semantics (Turney and Pantel, 2010) similarly focuses on something else, namely David Schlangen CITEC, Bielefeld University Universit¨atsstraf3e 25 33615 Bielefeld, Germany david.schlangen@ uni-bielefeld.de semantic similarity relations between words or phrases (e.g. finding closeness for “coloured tome on the right of the cup”). Neither approach by itself says anything about processing; typically, the assumption in applications is that fully presented phrases are being processed. Lacking in these approaches is a notion of grounding of symbols in features of the world (Harnad, 1990).1 In this paper, we present an account of word and phrase meaning that is (a) perceptually grounded in that it provides a link between words and (computer) vision features of real images, (b) trainable, as that link is learned from examples of language use, (c) compositional in that the meaning of phrases is a function of that of its parts and composition is driven by structural analysis, and (d) ‘dialogue-plausible’ in that it computes meanings incrementally, word-by-word and can work with noisy input from an automatic speech recogniser (ASR). We show that the approach performs well (with an</context>
</contexts>
<marker>Harnad, 1990</marker>
<rawString>Stevan Harnad. 1990. The Symbol Grounding Problem. Physica D, 42:335–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Kelleher</author>
<author>Fintan Costello</author>
<author>Jofsef Van Genabith</author>
</authors>
<title>Dynamically structuring, updating and interrelating representations of visual and linguistic discourse context.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<pages>167--1</pages>
<marker>Kelleher, Costello, Van Genabith, 2005</marker>
<rawString>John Kelleher, Fintan Costello, and Jofsef Van Genabith. 2005. Dynamically structuring, updating and interrelating representations of visual and linguistic discourse context. Artificial Intelligence, 167(1–2):62–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Kennington</author>
<author>Spyros Kousidis</author>
<author>David Schlangen</author>
</authors>
<title>Interpreting Situated Dialogue Utterances: an Update Model that Uses Speech, Gaze, and Gesture Information.</title>
<date>2013</date>
<booktitle>In Proceedings of SIGdial.</booktitle>
<contexts>
<context position="4662" citStr="Kennington et al., 2013" startWordPosition="722" endWordPosition="725"> c�2015 Association for Computational Linguistics of the (relevant aspects of the) world, returns I*, the identifier of one the objects in the world that is the referent of the RE. A number of recent papers have used stochastic models for frr where, given W and U, a distribution over a specified set of candidate entities in W is obtained and the probability assigned to each entity represents the strength of belief that it is the referent. The referent is then the argmax: I* = argmax P(IIU,W) (1) I Recently, generative approaches, including our own, have been presented (Funakoshi et al., 2012; Kennington et al., 2013; Kennington et al., 2014; Kennington et al., 2015b; Engonopoulos et al., 2013) which model U as words or ngrams and the world W as a set of objects in a virtual game board, represented as a set properties or concepts (in some cases, extra-linguistic or discourse aspects were also modelled in W, such as deixis). In Matuszek et al. (2014), W was represented as a distribution over properties of tangible objects and U was a Combinatory Categorical Grammar parse. In all of these approaches, the objects are distinct and represented via symbolically specified properties, such as colour and shape. Th</context>
</contexts>
<marker>Kennington, Kousidis, Schlangen, 2013</marker>
<rawString>Casey Kennington, Spyros Kousidis, and David Schlangen. 2013. Interpreting Situated Dialogue Utterances: an Update Model that Uses Speech, Gaze, and Gesture Information. In Proceedings of SIGdial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Kennington</author>
<author>Spyros Kousidis</author>
<author>David Schlangen</author>
</authors>
<title>Situated Incremental Natural Language Understanding using a Multimodal, Linguistically-driven Update Model.</title>
<date>2014</date>
<booktitle>In Proceedings of CoLing.</booktitle>
<contexts>
<context position="4687" citStr="Kennington et al., 2014" startWordPosition="726" endWordPosition="729">omputational Linguistics of the (relevant aspects of the) world, returns I*, the identifier of one the objects in the world that is the referent of the RE. A number of recent papers have used stochastic models for frr where, given W and U, a distribution over a specified set of candidate entities in W is obtained and the probability assigned to each entity represents the strength of belief that it is the referent. The referent is then the argmax: I* = argmax P(IIU,W) (1) I Recently, generative approaches, including our own, have been presented (Funakoshi et al., 2012; Kennington et al., 2013; Kennington et al., 2014; Kennington et al., 2015b; Engonopoulos et al., 2013) which model U as words or ngrams and the world W as a set of objects in a virtual game board, represented as a set properties or concepts (in some cases, extra-linguistic or discourse aspects were also modelled in W, such as deixis). In Matuszek et al. (2014), W was represented as a distribution over properties of tangible objects and U was a Combinatory Categorical Grammar parse. In all of these approaches, the objects are distinct and represented via symbolically specified properties, such as colour and shape. The set of properties is ei</context>
</contexts>
<marker>Kennington, Kousidis, Schlangen, 2014</marker>
<rawString>Casey Kennington, Spyros Kousidis, and David Schlangen. 2014. Situated Incremental Natural Language Understanding using a Multimodal, Linguistically-driven Update Model. In Proceedings of CoLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Kennington</author>
<author>Livia Dia</author>
<author>David Schlangen</author>
</authors>
<title>A Discriminative Model for PerceptuallyGrounded Incremental Reference Resolution.</title>
<date>2015</date>
<booktitle>In Proceedings of IWCS. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3622" citStr="Kennington et al. (2015" startWordPosition="551" endWordPosition="554">rect descriptions as well as target/landmark descriptions, even when trained with little data (less than 800 training examples). In the following section we will give a background on reference resolution, followed by a description of our model. We will then describe the data we used and explain our evaluations. We finish by giving results, providing some additional analysis, and discussion. 2 Background: Reference Resolution Reference resolution (RR) is the task of resolving referring expressions (REs; as in Example (1)) to a referent, the entity to which they are intended to refer. Following Kennington et al. (2015a), this can be formalised as a function frr that, given a representation U of the RE and a representation W 1But see discussion below of recent extensions of these approaches taking this into account. 292 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 292–301, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics of the (relevant aspects of the) world, returns I*, the identifier of one the objects in the world that is the referent of the RE. A </context>
<context position="6817" citStr="Kennington et al. (2015" startWordPosition="1092" endWordPosition="1095">r model is a model of word meaning as a function from perceptual features of a given object to a judgement about how well a word and that object “fit together”. (See Section 5 for discussion of prior uses of this “words as classifiers”-approach.) This can (loosely) be seen as corresponding to the intension of a word, which for example in Montague’s approach is similarly modelled as a function, but from possible worlds to extensions (Gamut, 1991). We model two different types of words / word meanings: those picking out properties of single objects (e.g., “green” in “the green book”), following Kennington et al. (2015a), and those picking out relations of two objects (e.g., “next to” in (1)), going beyond Kennington et al. (2015a). These word meanings are learned from instances of language use. The second component then is the application of these word meanings in the context of an actual reference and within a phrase. This application gives the desired result of a probability distribution over candidate objects, where the probability expresses the strength of belief in the object falling in the extension of the expression. Here we model two different types of composition, of what we call simple references</context>
<context position="20032" citStr="Kennington et al. (2015" startWordPosition="3348" endWordPosition="3351">n). This represents a minimal amount of “syntactic” information needed for the application of the classifiers and the composition of the phrase meanings. We leave applying a syntactic parser to future work. An example RE in the original German (as recognised by the ASR), English gloss, and tags for each word is given in (2). (2) a. grauer stein ¨uber dem gr¨unen m unten links b. gray block above the green m bottom left c. tc ts r l lc ls tf tf To obtain visual features of each object, we used the same simple computer-vision pipeline of object segmentation and contour reconstruction as used by Kennington et al. (2015a), providing us with RGB representations for the colour and features such as skewness, number of edges etc. for the shapes. Procedure We break down our data as follows: episodes where the target was referred directly via a ‘simple reference’ construction (DD; 410 episodes) and episodes where a target was referred via a landmark relation (RD; 460 episodes). We also test with either knowledge about structure (simple or relational reference) provided (ST) or not (WO, for “words-only”). All results shown are from 10-fold cross validations averaged over 10 runs; where for evaluations labelled RD t</context>
</contexts>
<marker>Kennington, Dia, Schlangen, 2015</marker>
<rawString>Casey Kennington, Livia Dia, and David Schlangen. 2015a. A Discriminative Model for PerceptuallyGrounded Incremental Reference Resolution. In Proceedings of IWCS. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Kennington</author>
<author>Ryu Iida</author>
<author>Takenobu Tokunaga</author>
<author>David Schlangen</author>
</authors>
<title>Incrementally Tracking Reference in Human/Human Dialogue Using Linguistic and Extra-Linguistic Information.</title>
<date>2015</date>
<booktitle>In NAACL,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, U.S.A.</location>
<contexts>
<context position="3622" citStr="Kennington et al. (2015" startWordPosition="551" endWordPosition="554">rect descriptions as well as target/landmark descriptions, even when trained with little data (less than 800 training examples). In the following section we will give a background on reference resolution, followed by a description of our model. We will then describe the data we used and explain our evaluations. We finish by giving results, providing some additional analysis, and discussion. 2 Background: Reference Resolution Reference resolution (RR) is the task of resolving referring expressions (REs; as in Example (1)) to a referent, the entity to which they are intended to refer. Following Kennington et al. (2015a), this can be formalised as a function frr that, given a representation U of the RE and a representation W 1But see discussion below of recent extensions of these approaches taking this into account. 292 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 292–301, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics of the (relevant aspects of the) world, returns I*, the identifier of one the objects in the world that is the referent of the RE. A </context>
<context position="6817" citStr="Kennington et al. (2015" startWordPosition="1092" endWordPosition="1095">r model is a model of word meaning as a function from perceptual features of a given object to a judgement about how well a word and that object “fit together”. (See Section 5 for discussion of prior uses of this “words as classifiers”-approach.) This can (loosely) be seen as corresponding to the intension of a word, which for example in Montague’s approach is similarly modelled as a function, but from possible worlds to extensions (Gamut, 1991). We model two different types of words / word meanings: those picking out properties of single objects (e.g., “green” in “the green book”), following Kennington et al. (2015a), and those picking out relations of two objects (e.g., “next to” in (1)), going beyond Kennington et al. (2015a). These word meanings are learned from instances of language use. The second component then is the application of these word meanings in the context of an actual reference and within a phrase. This application gives the desired result of a probability distribution over candidate objects, where the probability expresses the strength of belief in the object falling in the extension of the expression. Here we model two different types of composition, of what we call simple references</context>
<context position="20032" citStr="Kennington et al. (2015" startWordPosition="3348" endWordPosition="3351">n). This represents a minimal amount of “syntactic” information needed for the application of the classifiers and the composition of the phrase meanings. We leave applying a syntactic parser to future work. An example RE in the original German (as recognised by the ASR), English gloss, and tags for each word is given in (2). (2) a. grauer stein ¨uber dem gr¨unen m unten links b. gray block above the green m bottom left c. tc ts r l lc ls tf tf To obtain visual features of each object, we used the same simple computer-vision pipeline of object segmentation and contour reconstruction as used by Kennington et al. (2015a), providing us with RGB representations for the colour and features such as skewness, number of edges etc. for the shapes. Procedure We break down our data as follows: episodes where the target was referred directly via a ‘simple reference’ construction (DD; 410 episodes) and episodes where a target was referred via a landmark relation (RD; 460 episodes). We also test with either knowledge about structure (simple or relational reference) provided (ST) or not (WO, for “words-only”). All results shown are from 10-fold cross validations averaged over 10 runs; where for evaluations labelled RD t</context>
</contexts>
<marker>Kennington, Iida, Tokunaga, Schlangen, 2015</marker>
<rawString>Casey Kennington, Ryu Iida, Takenobu Tokunaga, and David Schlangen. 2015b. Incrementally Tracking Reference in Human/Human Dialogue Using Linguistic and Extra-Linguistic Information. In NAACL, Denver, U.S.A. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Kiros</author>
<author>Ruslan Salakhutdinov</author>
<author>Richard S Zemel</author>
</authors>
<title>Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models.</title>
<date>2014</date>
<booktitle>In Proceedings of NIPS 2014 Deep Learning Workshop,</booktitle>
<pages>1--13</pages>
<contexts>
<context position="30417" citStr="Kiros et al. (2014)" startWordPosition="5160" endWordPosition="5163"> to model all words in our corpus), and do not process incrementally, which we do here. Recent efforts in multimodal distributional semantics have also looked at modelling word meaning based on visual context. Originally, vector space distributional semantics focused words in the context of other words (Turney and Pantel, 2010); recent multimodal approaches also consider low-level features from images. Bruni et al. (2012) and Bruni et al. (2014) for example model word meaning by word and visual context; each modality is represented by a vector, fused by concatenation. Socher et al. (2014) and Kiros et al. (2014) present approaches where words/phrases and images are mapped into the same high-dimensional space. While these approaches similarly provide a link between words and images, they are typically tailored towards a different setting (the words being descriptions of the whole image, and not utterance intended to perform a function within a visual situation). We leave more detailed exploration of similarities and differences to future work and only note for now that our approach, relying on much simpler classifiers (log-linear, basically), works with much smaller data sets and additionally seem to </context>
</contexts>
<marker>Kiros, Salakhutdinov, Zemel, 2014</marker>
<rawString>Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. 2014. Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models. In Proceedings of NIPS 2014 Deep Learning Workshop, pages 1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Staffan Larsson</author>
</authors>
<title>Formal semantics for perceptual classification.</title>
<date>2013</date>
<journal>Journal of Logic and Computation.</journal>
<contexts>
<context position="29326" citStr="Larsson (2013)" startWordPosition="4980" endWordPosition="4981">ore descriptive REs like the cross in the top-right corner on the left, as left is learned as a global term, or negation (the cross that’s not red). We leave exploring such constructions to future work. 5 Related Work Kelleher et al. (2005) approached RR using perceptually-grounded models, focusing on saliency and discourse context. In Gorniak and Roy (2004), descriptions of objects were used to learn a perceptually-grounded meaning with focus on spatial terms such as on the left. Steels and Belpaeme (2005) used neural networks to connect language with colour terms by interacting with humans. Larsson (2013) is closest in spirit to what we are attempting here; he provides a detailed 298 grauer stein über dem grünen m unten links Figure 6: A depiction of the model working incrementally for the RE in Example (2): the distribution over objects for relation is row 1, landmark is row 2, target is row 3. formal semantics for similarly descriptive terms, where parts of the semantics are modelled by a perceptual classifier. These approaches had limited lexicons (where we attempt to model all words in our corpus), and do not process incrementally, which we do here. Recent efforts in multimodal distributio</context>
<context position="32211" citStr="Larsson, 2013" startWordPosition="5431" endWordPosition="5432">ing distributions over candidate objects). We do not see our approach as being in opposition to these attempts. Rather, we envision a system of semantics that combines traditional symbolic expressions (on which inferences can be modelled via syntactic calculi) with distributed representations (which model conceptual knowledge / semantic networks, as well as encyclopedic knowledge) and with our action-based (namely, identification in the environment via perceptual information) semantics. This line of approach is connected to a number of recent works (e.g., (Erk, 2013; Lewis and Steedman, 2013; Larsson, 2013)); for now, exploring its ramifications is left for future work. 6 Conclusion In this paper, we presented a model of reference resolution that learns a perceptually-grounded meaning of words, including relational words. The model is simple, compositional, and robust despite low amounts of training data and noisy modalities. Our model is not without limitations; it so far only handles definite descriptions, yet there are other ways to refer to real-world objects, such as via pronouns and deixis. A unified model that can handle all of these, similar in spirit perhaps to Funakoshi et al. (2012), </context>
</contexts>
<marker>Larsson, 2013</marker>
<rawString>Staffan Larsson. 2013. Formal semantics for perceptual classification. Journal of Logic and Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<date>2013</date>
<booktitle>Combined Distributional and Logical Semantics. Transactions of the ACL,</booktitle>
<pages>1--179</pages>
<contexts>
<context position="31292" citStr="Lewis and Steedman, 2013" startWordPosition="5291" endWordPosition="5294">riptions of the whole image, and not utterance intended to perform a function within a visual situation). We leave more detailed exploration of similarities and differences to future work and only note for now that our approach, relying on much simpler classifiers (log-linear, basically), works with much smaller data sets and additionally seem to provide an easier interface to more traditional ways of composition (see Section 3 above). The issue of semantic compositionality is also actively discussed in the distributional semantics literature (see, e.g., (Mitchell and Lapata, 2010; Erk, 2013; Lewis and Steedman, 2013; Paperno et al., 2014)), investigating how to combine vectors. This could be seen as composition on the level of intensions (if one sees distributional representations as intensions, as is variously hinted at, e.g. Erk (2013)). In our approach, composition is done on the extensional level (by interpolating distributions over candidate objects). We do not see our approach as being in opposition to these attempts. Rather, we envision a system of semantics that combines traditional symbolic expressions (on which inferences can be modelled via syntactic calculi) with distributed representations (</context>
</contexts>
<marker>Lewis, Steedman, 2013</marker>
<rawString>Mike Lewis and Mark Steedman. 2013. Combined Distributional and Logical Semantics. Transactions of the ACL, 1:179–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>NLTK: The natural language toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics-Volume 1,</booktitle>
<pages>63--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21210" citStr="Loper and Bird, 2002" startWordPosition="3559" endWordPosition="3562">0 runs; where for evaluations labelled RD the training data always includes all of DD plus 9 folds of RD, testing on RD. The sets address the following questions: • how well does the sr model work on its own with just words? – DD.WO • how well does the sr model work when it knows about REs? – DD.ST • how well does the sr model work when it knows about REs, but not about relations? – RD.ST (sr) • how well does the model learn relation words after it has learned about sr? RD.ST (r) • how well does the rr model work (together with the sr)? RD.ST with DD.ST (rr) Words were stemmed using the NLTK (Loper and Bird, 2002) Snowball Stemmer, reducing the 296 vocabulary size to 1306. Due to sparsity, for relation words with a token count of less than 4 (found by ranging over values in a held-out set) relational features were piped into an UNK relation, which was used for unseen relations during evaluation (we assume the UNK relation would learn a general notion of ‘nearness’). For the individual word classifiers, we always paired one negative example with one positive example. For this evaluation, word classifiers for sr were given the following features: RGB values, HSV values, x and y coordinates of the centroi</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. NLTK: The natural language toolkit. In Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics-Volume 1, pages 63–70. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Liefeng Bo</author>
<author>Luke Zettlemoyer</author>
<author>Dieter Fox</author>
</authors>
<title>Learning from Unscripted Deictic Gesture and Language for Human-Robot Interactions. In AAAI.</title>
<date>2014</date>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="5001" citStr="Matuszek et al. (2014)" startWordPosition="783" endWordPosition="786">and the probability assigned to each entity represents the strength of belief that it is the referent. The referent is then the argmax: I* = argmax P(IIU,W) (1) I Recently, generative approaches, including our own, have been presented (Funakoshi et al., 2012; Kennington et al., 2013; Kennington et al., 2014; Kennington et al., 2015b; Engonopoulos et al., 2013) which model U as words or ngrams and the world W as a set of objects in a virtual game board, represented as a set properties or concepts (in some cases, extra-linguistic or discourse aspects were also modelled in W, such as deixis). In Matuszek et al. (2014), W was represented as a distribution over properties of tangible objects and U was a Combinatory Categorical Grammar parse. In all of these approaches, the objects are distinct and represented via symbolically specified properties, such as colour and shape. The set of properties is either read directly from the world if it is virtual, or computed (i.e., discretised) from the real world objects. In this paper, we learn a mapping from W to U directly, without mediating symbolic properties; such a mapping is a kind of perceptual grounding of meaning between W and U. Situated RR is a convenient s</context>
</contexts>
<marker>Matuszek, Bo, Zettlemoyer, Fox, 2014</marker>
<rawString>Cynthia Matuszek, Liefeng Bo, Luke Zettlemoyer, and Dieter Fox. 2014. Learning from Unscripted Deictic Gesture and Language for Human-Robot Interactions. In AAAI. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="31255" citStr="Mitchell and Lapata, 2010" startWordPosition="5285" endWordPosition="5288">ifferent setting (the words being descriptions of the whole image, and not utterance intended to perform a function within a visual situation). We leave more detailed exploration of similarities and differences to future work and only note for now that our approach, relying on much simpler classifiers (log-linear, basically), works with much smaller data sets and additionally seem to provide an easier interface to more traditional ways of composition (see Section 3 above). The issue of semantic compositionality is also actively discussed in the distributional semantics literature (see, e.g., (Mitchell and Lapata, 2010; Erk, 2013; Lewis and Steedman, 2013; Paperno et al., 2014)), investigating how to combine vectors. This could be seen as composition on the level of intensions (if one sees distributional representations as intensions, as is variously hinted at, e.g. Erk (2013)). In our approach, composition is done on the extensional level (by interpolating distributions over candidate objects). We do not see our approach as being in opposition to these attempts. Rather, we envision a system of semantics that combines traditional symbolic expressions (on which inferences can be modelled via syntactic calcul</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive science, 34(8):1388–1429, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>The Proper Treatment of Quantifikation in Ordinary English. In</title>
<date>1973</date>
<booktitle>Approaches to Natural Language: Proceedings of the 1970 Stanford Workshop on Grammar and Semantics,</booktitle>
<pages>221--242</pages>
<editor>J Hintikka, J Moravcsik, and P Suppes, editors,</editor>
<location>Dordrecht. Reidel.</location>
<contexts>
<context position="1453" citStr="Montague, 1973" startWordPosition="211" endWordPosition="212">eplausible’ in that it computes meanings word-by-word. We show that the approach performs well (with an accuracy of 65% on a 1-out-of-32 reference resolution task) on direct descriptions and target/landmark descriptions, even when trained with less than 800 training examples and automatically transcribed utterances. 1 Introduction The most basic, fundamental site of language use is co-located dialogue (Fillmore, 1975; Clark, 1996) and referring to objects, as in Example (1), is a common occurrence in such a co-located setting. (1) The green book on the left next to the mug. Logical semantics (Montague, 1973; Gamut, 1991; Partee et al., 1993) has little to say about this process – its focus is on the construction of syntactically manipulable objects that model inferential relations; here, e.g. the inference that there are (at least) two objects. Vector space approaches to distributional semantics (Turney and Pantel, 2010) similarly focuses on something else, namely David Schlangen CITEC, Bielefeld University Universit¨atsstraf3e 25 33615 Bielefeld, Germany david.schlangen@ uni-bielefeld.de semantic similarity relations between words or phrases (e.g. finding closeness for “coloured tome on the rig</context>
</contexts>
<marker>Montague, 1973</marker>
<rawString>Richard Montague. 1973. The Proper Treatment of Quantifikation in Ordinary English. In J Hintikka, J Moravcsik, and P Suppes, editors, Approaches to Natural Language: Proceedings of the 1970 Stanford Workshop on Grammar and Semantics, pages 221–242, Dordrecht. Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Paperno</author>
</authors>
<title>Nghia The Pham, and</title>
<date>2014</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>90--99</pages>
<marker>Paperno, 2014</marker>
<rawString>Denis Paperno, Nghia The Pham, and Marco Baroni. 2014. A practical and linguistically-motivated approach to compositional distributional semantics. In Proceedings ofACL, pages 90–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara H Partee</author>
<author>Alice ter Meuelen</author>
<author>Robert E Wall</author>
</authors>
<date>1993</date>
<booktitle>Mathematical Methods in Linguistics.</booktitle>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="1488" citStr="Partee et al., 1993" startWordPosition="215" endWordPosition="218">es meanings word-by-word. We show that the approach performs well (with an accuracy of 65% on a 1-out-of-32 reference resolution task) on direct descriptions and target/landmark descriptions, even when trained with less than 800 training examples and automatically transcribed utterances. 1 Introduction The most basic, fundamental site of language use is co-located dialogue (Fillmore, 1975; Clark, 1996) and referring to objects, as in Example (1), is a common occurrence in such a co-located setting. (1) The green book on the left next to the mug. Logical semantics (Montague, 1973; Gamut, 1991; Partee et al., 1993) has little to say about this process – its focus is on the construction of syntactically manipulable objects that model inferential relations; here, e.g. the inference that there are (at least) two objects. Vector space approaches to distributional semantics (Turney and Pantel, 2010) similarly focuses on something else, namely David Schlangen CITEC, Bielefeld University Universit¨atsstraf3e 25 33615 Bielefeld, Germany david.schlangen@ uni-bielefeld.de semantic similarity relations between words or phrases (e.g. finding closeness for “coloured tome on the right of the cup”). Neither approach b</context>
</contexts>
<marker>Partee, Meuelen, Wall, 1993</marker>
<rawString>Barbara H Partee, Alice ter Meuelen, and Robert E Wall. 1993. Mathematical Methods in Linguistics. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kari Pulli</author>
<author>Anatoly Baksheev</author>
<author>Kirill Kornyakov</author>
<author>Victor Eruhimov</author>
</authors>
<title>Real-time computer vision with OpenCV.</title>
<date>2012</date>
<journal>Communications of the ACM,</journal>
<volume>55</volume>
<issue>6</issue>
<contexts>
<context position="16112" citStr="Pulli et al., 2012" startWordPosition="2679" endWordPosition="2682"> where the target is outlined in green (solid arrow added here for presentation), the landmark outlined in blue (dashed arrow). Data We evaluated our model using data we collected in a Wizard-of-Oz setting (that is, a human/computer interaction setting where parts of the functionality of the computer system were provided by a human experimentor). Participants were seated in front of a table with 36 Pentomino puzzle pieces that were randomly placed with some space between them, as shown in Figure 2. Above the table was a camera that recorded a video feed of the objects, processed using OpenCV (Pulli et al., 2012) to segment the objects (see below for details); of those, one (or one pair) was chosen randomly by the experiment software. The video image was presented to the participant on a display placed behind the table, but with the randomly selected piece (or pair of pieces) indicated by an overlay). The task of the participant was to refer to that object using only speech, as if identifying it for a friend sitting next to the participant. The wizard P(R1, R2|r) ∗ P(Il|w01, ... , w0m)∗ P(It|w1, ... , wk) ∗ P(R1|It) ∗ P(R2|Il) (9) E R2 E It � Il 295 (experimentor) had an identical screen depicting the</context>
</contexts>
<marker>Pulli, Baksheev, Kornyakov, Eruhimov, 2012</marker>
<rawString>Kari Pulli, Anatoly Baksheev, Kirill Kornyakov, and Victor Eruhimov. 2012. Real-time computer vision with OpenCV. Communications of the ACM, 55(6):61–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded Compositional Semantics for Finding and Describing Images with Sentences. Transactions of the Association for Computational Linguistics (TACL),</title>
<date>2014</date>
<pages>2--207</pages>
<contexts>
<context position="30393" citStr="Socher et al. (2014)" startWordPosition="5155" endWordPosition="5158">exicons (where we attempt to model all words in our corpus), and do not process incrementally, which we do here. Recent efforts in multimodal distributional semantics have also looked at modelling word meaning based on visual context. Originally, vector space distributional semantics focused words in the context of other words (Turney and Pantel, 2010); recent multimodal approaches also consider low-level features from images. Bruni et al. (2012) and Bruni et al. (2014) for example model word meaning by word and visual context; each modality is represented by a vector, fused by concatenation. Socher et al. (2014) and Kiros et al. (2014) present approaches where words/phrases and images are mapped into the same high-dimensional space. While these approaches similarly provide a link between words and images, they are typically tailored towards a different setting (the words being descriptions of the whole image, and not utterance intended to perform a function within a visual situation). We leave more detailed exploration of similarities and differences to future work and only note for now that our approach, relying on much simpler classifiers (log-linear, basically), works with much smaller data sets a</context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D Manning, and Andrew Y Ng. 2014. Grounded Compositional Semantics for Finding and Describing Images with Sentences. Transactions of the Association for Computational Linguistics (TACL), 2:207–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luc Steels</author>
<author>Tony Belpaeme</author>
</authors>
<title>Coordinating perceptually grounded categories through language: a case study for colour. The Behavioral and brain sciences, 28(4):469–489; discussion 489–529.</title>
<date>2005</date>
<contexts>
<context position="29224" citStr="Steels and Belpaeme (2005)" startWordPosition="4962" endWordPosition="4965">to how the distribution is evaluated (“return the n top candidates”). Our model also doesn’t yet directly handle more descriptive REs like the cross in the top-right corner on the left, as left is learned as a global term, or negation (the cross that’s not red). We leave exploring such constructions to future work. 5 Related Work Kelleher et al. (2005) approached RR using perceptually-grounded models, focusing on saliency and discourse context. In Gorniak and Roy (2004), descriptions of objects were used to learn a perceptually-grounded meaning with focus on spatial terms such as on the left. Steels and Belpaeme (2005) used neural networks to connect language with colour terms by interacting with humans. Larsson (2013) is closest in spirit to what we are attempting here; he provides a detailed 298 grauer stein über dem grünen m unten links Figure 6: A depiction of the model working incrementally for the RE in Example (2): the distribution over objects for relation is row 1, landmark is row 2, target is row 3. formal semantics for similarly descriptive terms, where parts of the semantics are modelled by a perceptual classifier. These approaches had limited lexicons (where we attempt to model all words in our</context>
</contexts>
<marker>Steels, Belpaeme, 2005</marker>
<rawString>Luc Steels and Tony Belpaeme. 2005. Coordinating perceptually grounded categories through language: a case study for colour. The Behavioral and brain sciences, 28(4):469–489; discussion 489–529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From Frequency to Meaning: Vector Space Models of Semantics.</title>
<date>2010</date>
<journal>Artificial Intelligence,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="1773" citStr="Turney and Pantel, 2010" startWordPosition="259" endWordPosition="262">rances. 1 Introduction The most basic, fundamental site of language use is co-located dialogue (Fillmore, 1975; Clark, 1996) and referring to objects, as in Example (1), is a common occurrence in such a co-located setting. (1) The green book on the left next to the mug. Logical semantics (Montague, 1973; Gamut, 1991; Partee et al., 1993) has little to say about this process – its focus is on the construction of syntactically manipulable objects that model inferential relations; here, e.g. the inference that there are (at least) two objects. Vector space approaches to distributional semantics (Turney and Pantel, 2010) similarly focuses on something else, namely David Schlangen CITEC, Bielefeld University Universit¨atsstraf3e 25 33615 Bielefeld, Germany david.schlangen@ uni-bielefeld.de semantic similarity relations between words or phrases (e.g. finding closeness for “coloured tome on the right of the cup”). Neither approach by itself says anything about processing; typically, the assumption in applications is that fully presented phrases are being processed. Lacking in these approaches is a notion of grounding of symbols in features of the world (Harnad, 1990).1 In this paper, we present an account of wor</context>
<context position="30127" citStr="Turney and Pantel, 2010" startWordPosition="5111" endWordPosition="5114">lly for the RE in Example (2): the distribution over objects for relation is row 1, landmark is row 2, target is row 3. formal semantics for similarly descriptive terms, where parts of the semantics are modelled by a perceptual classifier. These approaches had limited lexicons (where we attempt to model all words in our corpus), and do not process incrementally, which we do here. Recent efforts in multimodal distributional semantics have also looked at modelling word meaning based on visual context. Originally, vector space distributional semantics focused words in the context of other words (Turney and Pantel, 2010); recent multimodal approaches also consider low-level features from images. Bruni et al. (2012) and Bruni et al. (2014) for example model word meaning by word and visual context; each modality is represented by a vector, fused by concatenation. Socher et al. (2014) and Kiros et al. (2014) present approaches where words/phrases and images are mapped into the same high-dimensional space. While these approaches similarly provide a link between words and images, they are typically tailored towards a different setting (the words being descriptions of the whole image, and not utterance intended to </context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D Turney and Patrick Pantel. 2010. From Frequency to Meaning: Vector Space Models of Semantics. Artificial Intelligence, 37(1):141–188.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>