<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998026">
An Iterative Reinforcement Approach for Fine-Grained
Opinion Mining
</title>
<author confidence="0.995104">
Weifu Du
</author>
<affiliation confidence="0.725687">
Haerbin Institute of Technology
Haerbin, China
</affiliation>
<email confidence="0.993468">
duweifu@software.ict.ac.cn
</email>
<sectionHeader confidence="0.993799" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922230769231">
With the in-depth study of sentiment analysis
research, finer-grained opinion mining, which
aims to detect opinions on different review fea-
tures as opposed to the whole review level, has
been receiving more and more attention in the
sentiment analysis research community re-
cently. Most of existing approaches rely mainly
on the template extraction to identify the ex-
plicit relatedness between product feature and
opinion terms, which is insufficient to detect
the implicit review features and mine the hid-
den sentiment association in reviews, which
satisfies (1) the review features are not appear
explicit in the review sentences; (2) it can be
deduced by the opinion words in its context.
From an information theoretic point of view,
this paper proposed an iterative reinforcement
framework based on the improved information
bottleneck algorithm to address such problem.
More specifically, the approach clusters prod-
uct features and opinion words simultaneously
and iteratively by fusing both their semantic in-
formation and co-occurrence information. The
experimental results demonstrate that our ap-
proach outperforms the template extraction
based approaches.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999175">
In the Web2.0 era, the Internet turns from a static
information media into a platform for dynamic
information exchanging, on which people can ex-
press their views and show their selfhood. More
and more people are willing to record their feel-
ings (blog), give voice to public affairs (news re-
view), express their likes and dislikes on products
(product review), and so on. In the face of the vol-
ume of sentimental information available on the
Internet continues to increase, there is growing
interest in helping people better find, filter, and
manage these resources.
</bodyText>
<author confidence="0.548158">
Songbo Tan
</author>
<affiliation confidence="0.706479">
Institute of Computing Technology
Beijing, China
</affiliation>
<email confidence="0.948565">
tansongbo@software.ict.ac.cn
</email>
<bodyText confidence="0.999702243902439">
Automatic opinion mining (Turney et al., 2003;
Ku et al., 2006; Devitt et al., 2007) can play an
important role in a wide variety of more flexible
and dynamic information management tasks. For
example, with the help of sentiment analysis sys-
tem, in the field of public administration, the ad-
ministrators can receive the feedbacks on one pol-
icy in a timelier manner; in the field of business,
manufacturers can perform more targeted updates
on products to improve the consumer experience.
The research of opinion mining began in 1997,
the early research results mainly focused on the
polarity of opinion words (Hatzivassiloglou et al.,
1997) and treated the text-level opinion mining as
a classification of either positive or negative on the
number of positive or negative opinion words in
one text (Turney et al., 2003; Pang et al., 2002;
Zagibalov et al., 2008;). With the in-depth study of
opinion mining, researchers committed their ef-
forts for more accurate results: the research of sen-
timent summarization (Philip et al., 2004; Hu et al.,
KDD 2004), domain transfer problem of the sen-
timent analysis (Kanayama et al., 2006; Tan et al.,
2007; Blitzer et al., 2007; Tan et al., 2008; An-
dreevskaia et al., 2008; Tan et al., 2009) and fine-
grained opinion mining (Hatzivassiloglou et al.,
2000; Takamura et al., 2007; Bloom et al., 2007;
Wang et al., 2008; Titov et al., 2008) are the main
branches of the research of opinion mining. In this
paper, we focus on the fine-grained (feature-level)
opinion mining.
For many applications (e.g. the task of public
affairs review analysis and the products review
analysis), simply judging the sentiment orientation
of a review unit is not sufficient. Researchers (Ku-
shal, 2003; Hu et al., KDD 2004; Hu et al., AAAI
2004; Popescu et al., 2005) began to work on
finer-grained opinion mining which predicts the
sentiment orientation related to different review
features. The task is known as feature-level opin-
ion mining.
</bodyText>
<page confidence="0.985185">
486
</page>
<note confidence="0.890545">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 486–493,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999858158536586">
In feature-level opinion mining, most of the ex-
isting researches associate product features and
opinion words by their explicit co-occurrence.
Template extraction based method (Popescu et al.,
2005) and association rule mining based method
(Hu et al., AAAI 2004) are the representative ones.
These approaches did good jobs for identifying
the review features that appear explicitly in re-
views, however, real reviews from customers are
usually complicated. In some cases, the review
features are implicit in the review sentences, but
can be deduced by the opinion words in its context.
The detection of such hidden sentiment association
is a big challenge in feature-level opinion mining
on Chinese reviews due to the nature of Chinese
language (Qi et al., 2008). Obviously, neither the
template extraction based method nor the associa-
tion rule mining based method is effective for such
cases. Moreover, in some cases, even if the review
features appear explicitly in the review sentences,
the co-occurrence information between review
features and opinion words is too quantitatively
sparse to be utilized. So we consider whether it is a
more sensible way to construct or cluster review
feature groups and opinion words groups to mine
the implicit or hidden sentiment association in the
reviews.
The general approach will cluster the two types
of objects separately, which neglects the highly
interrelationship. To address this problem, in this
paper, we propose an iterative reinforcement
framework, under which we cluster product fea-
tures and opinion words simultaneously and itera-
tively by fusing both their semantic information
and sentiment link information. We take improved
information bottleneck algorithm (Tishby, 1999)
as the kernel of the proposed framework.
The information bottleneck approach was pre-
sented by Tishby (1999). The basic idea of the ap-
proach is that it treats the clustering problems from
the information compressing point of view, and
takes this problem as a case of much more funda-
mental problem: what are the features of the vari-
able X that are relevant for the prediction of an-
other, relevance, variable Y? Based on the infor-
mation theory, the problem can be formulated as:
find a compressed representation of the variable X,
denoted C, such that the mutual information be-
tween C and Y is as high as possible, under a con-
straint on the mutual information between X and C.
For our case, take the hotel reviews as example, X
is one type of objects of review features (e.g. fa-
cilities, service, surrounding environment, etc) or
opinion words (e.g. perfect, circumspect, quiet,
etc), and Y is another one. Given some review fea-
tures (or opinion words) gained from review cor-
pus, we want to assemble them into categories,
conserving the information about opinion words
(or review features) as high as possible.
The information bottleneck algorithm has some
benefits, mainly including (1) it treats the trade-off
of precision versus complexity of clustering model
through the rate distortion theory, which is a sub-
field of information theory; (2) it defines the “dis-
tance” or “similarity” in a well-defined way based
on the mutual information. The efficiency of in-
formation bottleneck algorithm (Slonim and
Tishby, 2000) motivates us to take it as the kernel
of our framework. As far as we know, this ap-
proach has not been employed in opinion mining
yet.
In traditional information bottleneck approach,
the distance between two data objects is measured
by the Jensen-Shannon divergence (Lin, 1991),
which aims to measure the divergence between
two probability distributions. We alter this meas-
ure to integrate more semantic information, which
will be illustrated in detail in the following sec-
tions, and the experimental result shows the
effectiveness of the alteration.
It would be worthwhile to highlight several as-
pects of our work here:
</bodyText>
<listItem confidence="0.99106025">
• We propose an iterative reinforcement
framework, and under this framework, review
feature words and opinion words are organized
into categories in a simultaneous and iterative
manner.
• In the process of clustering, the semantic in-
formation and the co-occurrence information
are integrated.
• The experimental results on real Chinese
web reviews demonstrate that proposed
method outperforms the template extraction
based algorithm.
</listItem>
<sectionHeader confidence="0.947186" genericHeader="method">
2 Proposed Algorithm
</sectionHeader>
<subsectionHeader confidence="0.964344">
2.1 The Problem
</subsectionHeader>
<bodyText confidence="0.99334">
In product reviews, opinion words are used to ex-
press opinion, sentiment or attitude of reviewers.
Although some review units may express general
opinions toward a product, most review units are
</bodyText>
<page confidence="0.99746">
487
</page>
<bodyText confidence="0.999896125">
regarding to specific features of the product.
A product is always reviewed under a certain
feature set F. Suppose we have got a lexical list O
which includes all the opinion expressions and
their sentiment polarities. For the feature-level
opinion mining, identifying the sentiment associa-
tion between F and O is essential. The key points
in the whole process are as follows:
</bodyText>
<listItem confidence="0.999919666666667">
• get opinion word set O (with polarity labels)
• get product feature set F
• identify relationships between F and O
</listItem>
<bodyText confidence="0.9999728">
The focus of the paper is on the latter two steps,
especially for the case of hidden sentiment asso-
ciation that the review features are implicit in the
review sentences, but can be deduced by the opin-
ion words in its context. In contrast to existing ex-
plicit adjacency approaches, the proposed
approach detects the sentiment association
between F and O based on review feature
categories and opinion word groups gained from
the review corpus.
To this end, we first consider two sets of asso-
ciation objects: the set of product feature words F
= {f1,f2,...,fm} and the set of opinion words O =
{o1,o2,...on}. A weighted bipartite graph from F
and O can be built, denoted by G = {F, O, R}.
Here R = [rij] is the m*n link weight matrix con-
taining all the pair-wise weights between set F and
O. The weight can be calculated with different
weighting schemes, in this paper, we set rij by the
co-appearance frequency of fi and oj in clause level.
We take F and O as two random variables, and
the question of constructing or clustering the ob-
ject groups can be defined as finding compressed
representation of each variable that reserves the
information about another variable as high as pos-
sible. Take F as an example, we want to find its
compression, denoted as C, such that the mutual
information between C and O is as high as possi-
ble, under a constraint on the mutual information
between F and C.
We propose an iterative reinforcement frame-
work to deal with the tasks. An improved informa-
tion bottleneck algorithm is employed in this
framework, which will be illustrated in detail in
the following sections.
</bodyText>
<subsectionHeader confidence="0.995412">
2.2 Information Bottleneck Algorithm
</subsectionHeader>
<bodyText confidence="0.9966535">
The information bottleneck method (IB) was pre-
sented by Tishby et al. (1999). According to Shan-
non‘s information theory (Cover and Thomas,
1991), for the two random variables X, Y, the mu-
tual information I(X;Y) between the random vari-
ables X, Y is given by the symmetric functional:
</bodyText>
<equation confidence="0.974935666666667">
I(X;Y) = E P(x)P(Y I x)log p(y  |x) (1)
xE X,yEY
p
x)] = I(C,
Y) (2)
This solution is given in terms of the three dis-
</equation>
<bodyText confidence="0.797024714285714">
tributions that characterize every cluster
C, the
prior probability for this cluster, p(c), its member-
ship probabilities
and its distribution over
the relevance variable
In general, the mem-
</bodyText>
<equation confidence="0.768707">
bership probabilities,
is
i.e. every
x E X can be assigned to every
</equation>
<bodyText confidence="0.8363024">
C in some
(normalized) probability. The information bottle-
neck principle determines the distortion measure
between the points x and c to be
the
</bodyText>
<equation confidence="0.989676136363636">
[
(
x)
(
c) ]
L[p(c|
X) −,6I(C,
c E
p(c|x),
p(y|c).
p(c|x)
“soft”,
c E
KL
p
y|
||p
y|
=
P(Y  |x) log PV&apos;  |x) , the
y p y c
(  |)
</equation>
<bodyText confidence="0.98886203125">
and the mutual information between them meas-
ures the relative entropy between their joint distri-
bution p(x, y) and the product of respective mar-
ginal distributions p(x)p(y), which is the only con-
sistent statistical measure of the information that
variable X contains about variable Y (and vice
versa). Roughly speaking, some of the mutual in-
formation will be lost in the process of compres-
sion, e.g. I(C,Y) &lt;_ I(X,Y) (C is a compressed rep-
resentation
This representation is defined through a (possi-
bly stochastic) mapping between each value
x E X to each representative value c E C. Formally,
this mapping can be characterized by a conditional
distri
of X).
bution p(c|x), inducing a soft partitioning of X
values, Specifically, each value of X _is associated
with all the codebook elements (C values), with
some normalized probability.
The IB method is based on the following simple
idea. Given the empirical joint distribution of two
variables, one variable is compressed so that the
mutual information about the other variable is pre-
served as much as possible. The method can be
considered as finding a minimal sufficient partition
or efficient relevant coding of one variable with
respect to the other one. This problem can be
solved by introducing a Lagrange multiplier ,6 ,
and then minimizing the functional:
Kullback-Leibler divergence (Cover and Thomas,
1991) between the conditional distri
</bodyText>
<equation confidence="0.6848855">
butions p(y|x)
y
(
)
</equation>
<page confidence="0.97717">
488
</page>
<bodyText confidence="0.999751666666667">
and p(y|c). Specifically, the formal optimal solu-
tion is given by the following equations which
must be solved together.
</bodyText>
<equation confidence="0.982162052631579">
x)
⎧⎪ 1,
c*|
=⎨⎪⎩0,
x∈cior x∈cj
otherwise
(c  |x) = p(c) exp( −βDKL[p(y  |x)  ||p(y  |c)])
Z(β, x)
1
p y c
(  |) = ∑ p c x p x p y x
(  |) ( ) (  |)
x
p c
( )
p c
( ) = ∑xp(c  |x)p(x)
(3)
Where Z(β, x) is a normalization factor, and the
</equation>
<bodyText confidence="0.996867235294118">
single positive (Lagrange) parameter β determines
the “softness” of the classification. Intuitively, in
this procedure the information contained in X
about Y ‘squeezed’ through a compact ‘bottleneck’
of clusters C, that is forced to represent the ‘rele-
vant’ part in X w.r.t to Y.
An important special case is the “hard” cluster-
ing case where C is a deterministic function of X.
That is, p(c|x) can only take values of zero or one,
This restriction, which corresponds to the
limit β → ∞ in Eqs 3 meaning every x ∈ X is as-
signed to exactly one cluster c ∈ C with a prob-
ability of one and to all the others with a probabil-
ity of zero. This yields a natural distance measure
between distributions which can be easily imple-
mented in an agglomerative hierarchical clustering
procedure (Slonim and Tishby, 1999).
</bodyText>
<equation confidence="0.993729466666667">
⎧ 1,
(  |)
c x = ⎨ ⎩ 0,
1
(  |)
y c = ∑ p x y
( , )
p c
( ) x c
∈
( )
c = ∑p x
( )
x c
∈
</equation>
<bodyText confidence="0.98341775">
The algorithm starts with a trivial partitioning
into |X |singleton clusters, where each cluster con-
tains exactly one element of X. At each step we
merge two components of the current partition into
a single new component in a way that locally
minimizes the loss of mutual information about the
categories. Every merger, (ci, cj) ⇒ c* , is formally
defined by the following equation:
</bodyText>
<table confidence="0.997980705882353">
p c (5)
( ) p c
( )
i j
(  |)
y c = p y c
(  |) + p y c
(  |)
* i j
p c
( ) p c
( )
* *
( ) ( ) ( )
c p c p c
= +
* i j
</table>
<bodyText confidence="0.942176">
The decrease in the mutual information I(C, Y) due
to this merger is defined by
δI(ci , cj) ≡ I (Cbefore,Y) − I(Cafter,Y) (6)
When I(Cbefore,Y) and I(Cafter,Y) are the informa-
tion values before and after the merger, respec-
tively. After a little algebra, one can see
</bodyText>
<equation confidence="0.9983475">
δI(ci,cj) (p(ci) p(cj)) DJS p(y |ci),p(y |cj)
≡ + ⋅ ⎡ ⎣ ⎤ ⎦ (7)
</equation>
<bodyText confidence="0.9814695">
Where the functional DJS is the Jensen-Shannon
divergence (Lin, 1991), defined as
</bodyText>
<equation confidence="0.544571">
D
</equation>
<bodyText confidence="0.998663">
where in our case
</bodyText>
<equation confidence="0.999440888888889">
⎧{ } {
p p
, ≡ p y c p y c
(  |), (  |)}
⎪ i j i j
⎪ ⎨ p c p c
( ) ( )
{ } * i j
≡ ⎨ ⎧ ⎫ ⎬
π π
, ,
i j
⎪ ⎩ p c p c
( ) ( )
* ⎭
(  |) (  |)
y c + π p y c
i j j
</equation>
<bodyText confidence="0.995923">
By introducing the information optimization cri-
terion the resulting similarity measure directly
emerges from the analysis. The algorithm is now
very simple. At each step we perform “the best
possible merger”, i.e. merge the clusters {ci , cj }
which minimize δI (ci , cj) .
</bodyText>
<subsectionHeader confidence="0.82272">
2.3 Improved Information Bottleneck Algo-
</subsectionHeader>
<bodyText confidence="0.395111">
rithm for Semantic Information
</bodyText>
<equation confidence="0.453657375">
⎧
⎪ ⎪p
⎪ ⎨p
⎪
⎪p
⎪
⎪⎩
(
</equation>
<bodyText confidence="0.990708272727273">
In traditional information bottleneck approach, the
distance between two data objects is measured by
the difference of information values before and
after the merger, which is measured by the Jensen-
Shannon divergence. This divergence aims to
measure the divergence between two probability
distributions. For our case, the divergence is based
on the co-occurrence information between the two
variables F and O.
While the co-occurrence in corpus is usually
quantitatively sparse; additionally, Statistics based
</bodyText>
<figure confidence="0.993186419354839">
p
⎪⎨
⎪
⎪
⎪⎩
⎨
⎪
⎪
⎪
⎪⎩
⎧
⎪ ⎪p
⎪ p
p
if x c
∈
otherwise
(4)
^ ^
JS pi, pj π iDKL pi  ||p πjDKL pj  ||p
⎡ ⎤ ⎡ ⎤
⎡ ⎤ = +
⎣ ⎦ ⎢ ⎥ ⎢ ⎥
⎣ ⎦ (8)
⎣ ⎦
(9)
⎪
⎪
^
⎩ p p
= π i
</figure>
<page confidence="0.998464">
489
</page>
<bodyText confidence="0.9999394">
on word-occurrence loses semantic related infor-
mation. To avoid such reversed effects, in the pro-
posed framework we combine the co-occurrence
information and semantic information as the final
distance between the two types of objects.
</bodyText>
<equation confidence="0.9949464">
D X X
( , ) = αD ( , )
X X
i j semantic i j
+(1−α)CSI(Xi,Xj)
</equation>
<bodyText confidence="0.733676">
where {XiE F∧XjE F}v{XiE O∧XjE O}
</bodyText>
<equation confidence="0.780916">
(10)
</equation>
<bodyText confidence="0.818709222222222">
In equation 10, the distance between two data
objects Xi and Xj is denoted as a linear combination
of semantic distance and information value differ-
ence. The parameter α reflects the contribution of
different distances to the final distance.
Input: Joint probability distribution p(f,o)
Output: A partition of F into m clusters, ∀m∈
{1,...,|F|}, and a partition of O into n clusters ∀
n∈{1,...,|O|}
</bodyText>
<figure confidence="0.6013146">
1. t←0
2. Repeat
a. Construct CFt←Ft
b. ∀i, j=1,...,|CFt|, i&lt;j, calculate
dijt +-αDsemantic(cf,t,cfj)+(1−α)CSI(cfit,cfj)
</figure>
<listItem confidence="0.786725166666666">
c. for m←|CFt|-1 to 1
1) find the indices {i, j}, for which dijt is
minimized
t
2) merge {cfi t, cfj t}into cf*
3) update CFt+- {CFt -{cfit, cfjt}} U {cf*t}
</listItem>
<equation confidence="0.6696222">
t
4) update dij t costs w.r.t cf*
d. Construct COt←Ot
e. ∀i, j=1,...,|COt|, i&lt;j,calculate
dijt +- αD emantic (coi , cotj) + (1 − α )CSI (coi , cotj )
</equation>
<bodyText confidence="0.621307">
f. for n←|COt|-1 to 1
</bodyText>
<listItem confidence="0.561661666666667">
1) find the indices {i, j}, for which dijt is
minimized
t
2) merge {coit,cojt}into co*
3) update COt +- {COt -{coit,cojt}} U {co*t}
t
4) update dij t costs w.r.t co*
g. t←t+1
3. until (CFt = CFt-1 and COt =COt-1)
</listItem>
<figureCaption confidence="0.991759">
Figure 1: Pseudo-code of semantic information bot-
tleneck in iterative reinforcement framework
</figureCaption>
<bodyText confidence="0.99993855">
The semantic distance can be got by the usage
of lexicon, such as WordNet (Budanitsky and Hirst,
2006). In this paper, we use the Chinese lexicon
HowNet1.
The basic idea of the iterative reinforcement
principle is to propagate the clustered results be-
tween different type data objects by updating their
inter-relationship spaces. The clustering process
can begin from an arbitrary type of data object.
The clustering results of one data object type up-
date the interrelationship thus reinforce the data
object categorization of another type. The process
is iterative until clustering results of both object
types converge. Suppose we begin the clustering
process from data objects in set F, and then the
steps can be expressed as Figure 1. After the itera-
tion, we can get the strongest n links between
product feature categories and opinion word
groups. That constitutes our set of sentiment asso-
ciation.
</bodyText>
<sectionHeader confidence="0.997686" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9992405">
In this section we describe our experiments and the
data used in these experiments.
</bodyText>
<subsectionHeader confidence="0.997656">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.99985825">
Our experiments take hotel reviews (in Chinese)
as example. The corpus used in the experiments is
composed by 4000 editor reviews on hotel, includ-
ing 857,692 Chinese characters. They are extracted
from www.ctrip.com. Each review contains a
user’s rating represented by “stars”, the number of
the star denotes the user’s satisfaction. The de-
tailed information is illustrated in Table 1,
</bodyText>
<tableCaption confidence="0.999888">
Table 1: The detail information of corpus
</tableCaption>
<table confidence="0.8836758">
User’s rating Number
1 star 555
2 star 1375
3 star 70
4 star 2000
</table>
<bodyText confidence="0.997799">
Then we use ICTCLAS2, a Chinese word seg-
mentation software to extract candidate review
features and opinion words.
Usually, adjectives are normally used to express
opinions in reviews. Therefore, most of the exist-
ing researches take adjectives as opinion words. In
the research of Hu et al. (2004), they proposed that
</bodyText>
<footnote confidence="0.9993955">
1 http://www.keenage.com/
2 www.searchforum.org.cn
</footnote>
<page confidence="0.99722">
490
</page>
<bodyText confidence="0.999951">
other components of a sentence are unlikely to be
product features except for nouns and noun
phrases. Some researchers (Fujii and Ishikawa,
2006) targeted nouns, noun phrases and verb
phrases. The adding of verb phrases caused the
identification of more possible product features,
while brought lots of noises. So in this paper, we
follow the points of Hu’s, extracting nouns and
noun phrases as candidate product feature words.
Take the whole set of nouns and noun phrases
as candidate features will bring some noise. In or-
der to reduce such adverse effects, we use the
function of Named Entity Recognition (NER) in
ICTCLAS to filter out named entities, including:
person, location, organization. Since the NEs have
small probability of being product features, we
prune the candidate nouns or noun phrases which
have the above NE taggers.
</bodyText>
<tableCaption confidence="0.9960715">
Table 2: The number of candidate review features
and opinion words in our corpus
</tableCaption>
<table confidence="0.9992308">
Extracted In- Total Non-
stance Repeated
Candidate re- 86,623 15,249
view feature
Opinion word 26,721 1,231
</table>
<bodyText confidence="0.9996985">
By pruning candidate product feature words, we
get the set of product feature words F. And the set
of opinion words O is composed by all the adjec-
tives in reviews. The number of candidate product
feature words and opinion words extracted from
the corpus are shown as Table 2:
</bodyText>
<subsectionHeader confidence="0.998581">
3.2 Experimental Procedure
</subsectionHeader>
<bodyText confidence="0.999904">
We evaluate our approach from two perspectives:
</bodyText>
<listItem confidence="0.997587">
1) Effectiveness of product feature category
construction by mutual reinforcement based clus-
tering;
2) Precision of sentiment association between
product feature categories and opinion word
groups;
</listItem>
<sectionHeader confidence="0.984022" genericHeader="method">
4 Experimental Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.9935815">
4.1 Evaluation of Review Feature Category
Construction
</subsectionHeader>
<bodyText confidence="0.999815714285714">
To calculate agreement between the review feature
category construction results and the correct labels,
we make use of the Rand index (Rand, 1971). This
allows for a measure of agreement between two
partitions, P, and P2, of the same data set D. Each
partition is viewed as a collection of n*(n-1)/2 pair
wise decisions, where n is the size of D. For each
pair of points di and dj in D, Pi either assigns them
to the same cluster or to different clusters. Let a be
the number of decisions where di is in the same
cluster as dj in P, and in P2. Let b be the number of
decisions where the two instances are placed in
different clusters in both partitions. Total agree-
ment can then be calculated using
</bodyText>
<equation confidence="0.8444615">
a b
+
Rand P P
( 1 , 2) =
n (n 1) / 2
−
</equation>
<bodyText confidence="0.999929117647059">
In our case, the parts of product feature words in
the pre-constructed evaluation set are used to rep-
resent the data set D; a and b represent the parti-
tion agreements between the pairs of any two
words in the parts and in the clustering results re-
spectively.
In equation 10, the parameter α reflects the re-
spective contribution of semantic information and
co-occurrence information to the final distance.
When α = 0 or α =1 , the co-occurrence informa-
tion or the semantic information will be utilized
alone.
In order to get the optimal combination of the
two type of distance, we adjust the parameter
α from 0 to 1(stepped by 0.2), and the accuracy of
feature category construction with different α are
shown in Figure 2:
</bodyText>
<figureCaption confidence="0.9961645">
Figure 2: The accuracy of review feature category
construction with the variation of the parameter α
</figureCaption>
<bodyText confidence="0.999938571428571">
From this figure, we can find that the semantic
information (α =1) contributes much more to the
accuracy of review feature category construction
than the co-occurrence information ( α =0), and
when α =0, the approach is equivalent to the tradi-
tional information bottleneck approach. We con-
sider this is due partly to the sparseness of the cor-
</bodyText>
<equation confidence="0.48164">
(11)
</equation>
<page confidence="0.991563">
491
</page>
<bodyText confidence="0.9999273">
pus, by enlarging the scale of the corpus or using
the search engine (e.g. google etc), we can get
more accurate results.
Additionally, by a sensible adjust on the pa-
rameter α (in this experiment, we set α as 0.6),
we can get higher accuracy than the two baselines
( α =0 and α =1), which indicates the necessity
and effectiveness of the integration of semantic
information and co-occurrence information in the
proposed approach.
</bodyText>
<subsectionHeader confidence="0.994474">
4.2 Evaluation of Sentiment Association
</subsectionHeader>
<bodyText confidence="0.999928142857143">
We use precision to evaluate the performance of
sentiment association. An evaluation set is con-
structed manually first, in which there are not only
the categories that every review feature word be-
long to, but also the relationship between each
category and opinion word. Then we define preci-
sion as:
</bodyText>
<equation confidence="0.451877">
Precision = number of correctly associated pairs
number of detected pairs
(12)
</equation>
<bodyText confidence="0.999637181818182">
A comparative result is got by the means of
template-extraction based approach on the same
test set. By the usage of regular expression, the
nouns (phrase) and gerund (phrase) are extracted
as the review features, and the nearest adjectives
are extracted as the related opinion words. Because
the modifiers of adjectives in reviews also contain
rich sentiment information and express the view of
customs, we extract adjectives and their modifiers
simultaneously, and take them as the opinion
words.
</bodyText>
<tableCaption confidence="0.9260405">
Table 3: Performance comparison in sentiment asso-
ciation
</tableCaption>
<table confidence="0.998172">
Approach Pairs Precision
Template extraction 27,683 65.89%
Proposed approach 141,826 78.90%
</table>
<bodyText confidence="0.997653166666667">
Table 3 shows the advantage of our approach
over the extraction by explicit adjacency. Using
the same product feature categorization, our sen-
timent association approach get a more accurate
pair set than the direct extraction based on explicit
adjacency. The precision we obtained by the itera-
tive reinforcement approach is 78.90%, almost 13
points higher than the adjacency approach. This
indicates that there are a large number of hidden
sentiment associations in the real custom reviews,
which underlines the importance and value of our
work.
</bodyText>
<sectionHeader confidence="0.991012" genericHeader="conclusions">
5 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.918576310344828">
In this paper, we propose a novel iterative rein-
forcement framework based on improved informa-
tion bottleneck approach to deal with the feature-
level product opinion-mining problem. We alter
traditional information bottleneck method by inte-
gration with semantic information, and the ex-
perimental result demonstrates the effectiveness of
the alteration. The main contribution of our work
mainly including:
• We propose an iterative reinforcement in-
formation bottleneck framework, and in this
framework, review feature words and opinion
words are organized into categories in a simul-
taneous and iterative manner.
• In the process of clustering, the semantic in-
formation and the co-occurrence information
are integrated.
• The experimental results based on real Chi-
nese web reviews demonstrate that our method
outperforms the template extraction based al-
gorithm.
Although our methods for candidate product
feature extraction and filtering (see in 3.1) can
partly identify real product features, it may lose
some data and remain some noises. We’ll conduct
deeper research in this area in future work. Addi-
tionally, we plan to exploit more information, such
as background knowledge, to improve the per-
formance.
</bodyText>
<sectionHeader confidence="0.999594" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.998520333333333">
This work was mainly supported by two funds, i.e.,
0704021000 and 60803085, and one another pro-
ject, i.e., 2004CB318109.
</bodyText>
<sectionHeader confidence="0.99945" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999808166666667">
A. Andreevskaia, S. Bergler. When Specialists and
Generalists Work Together: Overcoming Domain
Dependence in Sentiment Tagging. ACL 2008.
A. Budanitsky and G. Hirst. Evaluating wordnetbased
measures of lexical semantic relatedness. Computa-
tional Linguistics, 32(1):13–47, 2006.
</reference>
<page confidence="0.981034">
492
</page>
<reference confidence="0.999970793478261">
A. Devitt, K. Ahmad. Sentiment Polarity Identification
in Financial News: A Cohesion-based Approach.
ACL 2007.
A. Fujii and T. Ishikawa. A system for summarizing
and visualizing arguments in subjective documents:
Toward supporting decision making. The Workshop
on Sentiment and Subjectivity in Text ACL2006.
2006.
A. Popescu and O. Etzioni. Extracting product features
and opinions from reviews. HLT-EMNLP 2005.
B. Liu, M. Hu, and J. Cheng. Opinion observer: analyz-
ing and comparing opinions on the web. WWW 2005.
B. Pang and L. Lee. Seeing stars: Exploiting class rela-
tionships for sentiment categorization with respect to
rating scales. ACL 2005.
B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up?
Sentiment classification using machine learning
techniques. EMNLP 2002.
B. Philip, T. Hastie, C. Manning, and S. Vaithyanathan.
Exploring sentiment summarization. In AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications (AAAI tech report SS-04-
07). 2004.
B. Wang, H. Wang. Bootstrapping Both Product Fea-
tures and Opinion Words from Chinese Customer
Reviews with Cross-Inducing. IJCNLP 2008.
D. Kushal, S. Lawrence, and D. Pennock. Mining the
peanut gallery: Opinion extraction and semantic clas-
sification of product reviews. WWW 2003.
H. Kanayama, T. Nasukawa. Fully Automatic Lexicon
Expansion for Domain-oriented Sentiment Analysis.
EMNLP 2006
H. Takamura, T. Inui. Extracting Semantic Orientations
of Phrases from Dictionary. NAACL-HLT 2007.
I. Titov, R. McDonald. Modeling online reviews with
multi-grain topic models. WWW 2008.
L. Ku, Y. Liang and H. Chen. Opinion Extraction,
Summarization and Tracking in News and Blog Cor-
pora. AAAI-CAAW 2006.
J. Blitzer, M. Dredze, F. Pereira. Biographies, Bolly-
wood, Boom-boxes and Blenders: Domain Adapta-
tion for Sentiment Classification. ACL 2007.
J. Lin. Divergence Measures Based on the Shannon
Entropy. IEEE Transactions on Information theory,
37(1):145–151, 1991.
K. Bloom and N. Garg and S. Argamon. Extracting
Appraisal Expressions. NAACL-HLT 2007.
M. Hu and B. Liu. Mining and summarizing customer
reviews. KDD 2004.
M. Hu and B. Liu. Mining opinion features in customer
reviews. AAAI 2004.
N. Slonim, N. Tishby. Agglomerative information bot-
tleneck. NIPS 1999.
N. Slonim and N. Tishby. Document Clustering Using
word Clusters via the Information Bottleneck
Method. SIGIR 2000.
N. Slonim and N. Tishby. The power of word clusters
for text classification. ECIR 2001.
N. Tishby, F. Pereira, W. Bialek. The information bot-
tleneck method. 1999, arXiv: physics/0004057v1
P. Turney. Thumbs up or thumbs down? Semantic ori-
entation applied to unsupervised classification of re-
views. ACL 2002.
P. Turney and M. Littman. Measuring Praise and Criti-
cism: Inference of Semantic Orientation from Asso-
ciation. ACM Transactions on Information Systems,
2003,21(4): 315-346.
Q. Su, X. Xu, H. Guo, Z. Guo, X. Wu, X. Zhang, B.
Swen and Z. Su. Hidden sentiment association in
Chinese web opinion mining. WWW 2008.
S. Tan, G. Wu, H. Tang and X. Cheng. A novel scheme
for domain-transfer problem in the context of senti-
ment analysis. CIKM 2007.
S. Tan, Y. Wang, G. Wu and X. Cheng. Using unla-
beled data to handle domain-transfer problem of
semantic detection. SAC 2008.
S. Tan, X. Cheng, Y. Wang and H. Xu. Adapting Naive
Bayes to Domain Adaptation for Sentiment Analysis.
ECIR 2009.
T. Cover and J. Thomas. Elements of Information The-
ory. John Wiley &amp; Sons, New York, 1991.
T. Zagibalov, J. Carroll. Automatic Seed Word Selec-
tion for Unsupervised Sentiment Classification of
Chinese Text. Coling 2008.
V. Hatzivassiloglou and K. McKeown. Predicting the
semantic orientation of adjectives. ACL 1997.
V. Hatzivassiloglou and J. Wiebe. Effects of adjective
orientation and gradability on sentence subjectivity.
Coling 2000.
W. Rand. Objective criteria for the evaluation of clus-
tering methods. Journal of the American Statistical
Association, 66, 846-850. 1971
</reference>
<page confidence="0.999375">
493
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951869">
<title confidence="0.9976755">An Iterative Reinforcement Approach for Opinion Mining</title>
<author confidence="0.986645">Weifu Du</author>
<affiliation confidence="0.99993">Haerbin Institute of Technology</affiliation>
<address confidence="0.994246">Haerbin, China</address>
<email confidence="0.9915">duweifu@software.ict.ac.cn</email>
<abstract confidence="0.99933537037037">With the in-depth study of sentiment analysis research, finer-grained opinion mining, which aims to detect opinions on different review features as opposed to the whole review level, has been receiving more and more attention in the sentiment analysis research community recently. Most of existing approaches rely mainly on the template extraction to identify the explicit relatedness between product feature and opinion terms, which is insufficient to detect the implicit review features and mine the hidden sentiment association in reviews, which satisfies (1) the review features are not appear explicit in the review sentences; (2) it can be deduced by the opinion words in its context. From an information theoretic point of view, this paper proposed an iterative reinforcement framework based on the improved information bottleneck algorithm to address such problem. More specifically, the approach clusters product features and opinion words simultaneously and iteratively by fusing both their semantic information and co-occurrence information. The experimental results demonstrate that our approach outperforms the template extraction based approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Andreevskaia</author>
<author>S Bergler</author>
</authors>
<title>When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging.</title>
<date>2008</date>
<publisher>ACL</publisher>
<marker>Andreevskaia, Bergler, 2008</marker>
<rawString>A. Andreevskaia, S. Bergler. When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging. ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating wordnetbased measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="18139" citStr="Budanitsky and Hirst, 2006" startWordPosition="3160" endWordPosition="3163"> 3) update CFt+- {CFt -{cfit, cfjt}} U {cf*t} t 4) update dij t costs w.r.t cf* d. Construct COt←Ot e. ∀i, j=1,...,|COt|, i&lt;j,calculate dijt +- αD emantic (coi , cotj) + (1 − α )CSI (coi , cotj ) f. for n←|COt|-1 to 1 1) find the indices {i, j}, for which dijt is minimized t 2) merge {coit,cojt}into co* 3) update COt +- {COt -{coit,cojt}} U {co*t} t 4) update dij t costs w.r.t co* g. t←t+1 3. until (CFt = CFt-1 and COt =COt-1) Figure 1: Pseudo-code of semantic information bottleneck in iterative reinforcement framework The semantic distance can be got by the usage of lexicon, such as WordNet (Budanitsky and Hirst, 2006). In this paper, we use the Chinese lexicon HowNet1. The basic idea of the iterative reinforcement principle is to propagate the clustered results between different type data objects by updating their inter-relationship spaces. The clustering process can begin from an arbitrary type of data object. The clustering results of one data object type update the interrelationship thus reinforce the data object categorization of another type. The process is iterative until clustering results of both object types converge. Suppose we begin the clustering process from data objects in set F, and then the</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>A. Budanitsky and G. Hirst. Evaluating wordnetbased measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Devitt</author>
<author>K Ahmad</author>
</authors>
<title>Sentiment Polarity Identification in Financial News: A Cohesion-based Approach.</title>
<date>2007</date>
<publisher>ACL</publisher>
<marker>Devitt, Ahmad, 2007</marker>
<rawString>A. Devitt, K. Ahmad. Sentiment Polarity Identification in Financial News: A Cohesion-based Approach. ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fujii</author>
<author>T Ishikawa</author>
</authors>
<title>A system for summarizing and visualizing arguments in subjective documents: Toward supporting decision making.</title>
<date>2006</date>
<booktitle>The Workshop on Sentiment and Subjectivity in Text ACL2006.</booktitle>
<contexts>
<context position="20067" citStr="Fujii and Ishikawa, 2006" startWordPosition="3468" endWordPosition="3471">able 1: The detail information of corpus User’s rating Number 1 star 555 2 star 1375 3 star 70 4 star 2000 Then we use ICTCLAS2, a Chinese word segmentation software to extract candidate review features and opinion words. Usually, adjectives are normally used to express opinions in reviews. Therefore, most of the existing researches take adjectives as opinion words. In the research of Hu et al. (2004), they proposed that 1 http://www.keenage.com/ 2 www.searchforum.org.cn 490 other components of a sentence are unlikely to be product features except for nouns and noun phrases. Some researchers (Fujii and Ishikawa, 2006) targeted nouns, noun phrases and verb phrases. The adding of verb phrases caused the identification of more possible product features, while brought lots of noises. So in this paper, we follow the points of Hu’s, extracting nouns and noun phrases as candidate product feature words. Take the whole set of nouns and noun phrases as candidate features will bring some noise. In order to reduce such adverse effects, we use the function of Named Entity Recognition (NER) in ICTCLAS to filter out named entities, including: person, location, organization. Since the NEs have small probability of being p</context>
</contexts>
<marker>Fujii, Ishikawa, 2006</marker>
<rawString>A. Fujii and T. Ishikawa. A system for summarizing and visualizing arguments in subjective documents: Toward supporting decision making. The Workshop on Sentiment and Subjectivity in Text ACL2006. 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Popescu</author>
<author>O Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews. HLT-EMNLP</title>
<date>2005</date>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>A. Popescu and O. Etzioni. Extracting product features and opinions from reviews. HLT-EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
<author>M Hu</author>
<author>J Cheng</author>
</authors>
<title>Opinion observer: analyzing and comparing opinions on the web. WWW</title>
<date>2005</date>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>B. Liu, M. Hu, and J. Cheng. Opinion observer: analyzing and comparing opinions on the web. WWW 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<journal>ACL</journal>
<marker>Pang, Lee, 2005</marker>
<rawString>B. Pang and L. Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<journal>EMNLP</journal>
<contexts>
<context position="2829" citStr="Pang et al., 2002" startWordPosition="430" endWordPosition="433">entiment analysis system, in the field of public administration, the administrators can receive the feedbacks on one policy in a timelier manner; in the field of business, manufacturers can perform more targeted updates on products to improve the consumer experience. The research of opinion mining began in 1997, the early research results mainly focused on the polarity of opinion words (Hatzivassiloglou et al., 1997) and treated the text-level opinion mining as a classification of either positive or negative on the number of positive or negative opinion words in one text (Turney et al., 2003; Pang et al., 2002; Zagibalov et al., 2008;). With the in-depth study of opinion mining, researchers committed their efforts for more accurate results: the research of sentiment summarization (Philip et al., 2004; Hu et al., KDD 2004), domain transfer problem of the sentiment analysis (Kanayama et al., 2006; Tan et al., 2007; Blitzer et al., 2007; Tan et al., 2008; Andreevskaia et al., 2008; Tan et al., 2009) and finegrained opinion mining (Hatzivassiloglou et al., 2000; Takamura et al., 2007; Bloom et al., 2007; Wang et al., 2008; Titov et al., 2008) are the main branches of the research of opinion mining. In </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? Sentiment classification using machine learning techniques. EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Philip</author>
<author>T Hastie</author>
<author>C Manning</author>
<author>S Vaithyanathan</author>
</authors>
<title>Exploring sentiment summarization.</title>
<date>2004</date>
<booktitle>In AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications (AAAI tech report</booktitle>
<pages>04--07</pages>
<contexts>
<context position="3023" citStr="Philip et al., 2004" startWordPosition="460" endWordPosition="463">rform more targeted updates on products to improve the consumer experience. The research of opinion mining began in 1997, the early research results mainly focused on the polarity of opinion words (Hatzivassiloglou et al., 1997) and treated the text-level opinion mining as a classification of either positive or negative on the number of positive or negative opinion words in one text (Turney et al., 2003; Pang et al., 2002; Zagibalov et al., 2008;). With the in-depth study of opinion mining, researchers committed their efforts for more accurate results: the research of sentiment summarization (Philip et al., 2004; Hu et al., KDD 2004), domain transfer problem of the sentiment analysis (Kanayama et al., 2006; Tan et al., 2007; Blitzer et al., 2007; Tan et al., 2008; Andreevskaia et al., 2008; Tan et al., 2009) and finegrained opinion mining (Hatzivassiloglou et al., 2000; Takamura et al., 2007; Bloom et al., 2007; Wang et al., 2008; Titov et al., 2008) are the main branches of the research of opinion mining. In this paper, we focus on the fine-grained (feature-level) opinion mining. For many applications (e.g. the task of public affairs review analysis and the products review analysis), simply judging </context>
</contexts>
<marker>Philip, Hastie, Manning, Vaithyanathan, 2004</marker>
<rawString>B. Philip, T. Hastie, C. Manning, and S. Vaithyanathan. Exploring sentiment summarization. In AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications (AAAI tech report SS-04-07). 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wang</author>
<author>H Wang</author>
</authors>
<title>Bootstrapping Both Product Features and Opinion Words from Chinese Customer Reviews with Cross-Inducing. IJCNLP</title>
<date>2008</date>
<marker>Wang, Wang, 2008</marker>
<rawString>B. Wang, H. Wang. Bootstrapping Both Product Features and Opinion Words from Chinese Customer Reviews with Cross-Inducing. IJCNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kushal</author>
<author>S Lawrence</author>
<author>D Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<publisher>WWW</publisher>
<marker>Kushal, Lawrence, Pennock, 2003</marker>
<rawString>D. Kushal, S. Lawrence, and D. Pennock. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. WWW 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kanayama</author>
<author>T Nasukawa</author>
</authors>
<title>Fully Automatic Lexicon Expansion for Domain-oriented Sentiment Analysis. EMNLP</title>
<date>2006</date>
<marker>Kanayama, Nasukawa, 2006</marker>
<rawString>H. Kanayama, T. Nasukawa. Fully Automatic Lexicon Expansion for Domain-oriented Sentiment Analysis. EMNLP 2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Takamura</author>
<author>T Inui</author>
</authors>
<title>Extracting Semantic Orientations of Phrases from Dictionary. NAACL-HLT</title>
<date>2007</date>
<marker>Takamura, Inui, 2007</marker>
<rawString>H. Takamura, T. Inui. Extracting Semantic Orientations of Phrases from Dictionary. NAACL-HLT 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>R McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models. WWW</title>
<date>2008</date>
<marker>Titov, McDonald, 2008</marker>
<rawString>I. Titov, R. McDonald. Modeling online reviews with multi-grain topic models. WWW 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ku</author>
<author>Y Liang</author>
<author>H Chen</author>
</authors>
<title>Opinion Extraction, Summarization and Tracking</title>
<date>2006</date>
<booktitle>in News and Blog Corpora. AAAI-CAAW</booktitle>
<contexts>
<context position="2054" citStr="Ku et al., 2006" startWordPosition="302" endWordPosition="305">rmation exchanging, on which people can express their views and show their selfhood. More and more people are willing to record their feelings (blog), give voice to public affairs (news review), express their likes and dislikes on products (product review), and so on. In the face of the volume of sentimental information available on the Internet continues to increase, there is growing interest in helping people better find, filter, and manage these resources. Songbo Tan Institute of Computing Technology Beijing, China tansongbo@software.ict.ac.cn Automatic opinion mining (Turney et al., 2003; Ku et al., 2006; Devitt et al., 2007) can play an important role in a wide variety of more flexible and dynamic information management tasks. For example, with the help of sentiment analysis system, in the field of public administration, the administrators can receive the feedbacks on one policy in a timelier manner; in the field of business, manufacturers can perform more targeted updates on products to improve the consumer experience. The research of opinion mining began in 1997, the early research results mainly focused on the polarity of opinion words (Hatzivassiloglou et al., 1997) and treated the text-</context>
</contexts>
<marker>Ku, Liang, Chen, 2006</marker>
<rawString>L. Ku, Y. Liang and H. Chen. Opinion Extraction, Summarization and Tracking in News and Blog Corpora. AAAI-CAAW 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Biographies</author>
</authors>
<title>Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification.</title>
<date>2007</date>
<publisher>ACL</publisher>
<marker>Biographies, 2007</marker>
<rawString>J. Blitzer, M. Dredze, F. Pereira. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
</authors>
<title>Divergence Measures Based on the Shannon Entropy.</title>
<date>1991</date>
<journal>IEEE Transactions on Information theory,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="7670" citStr="Lin, 1991" startWordPosition="1208" endWordPosition="1209">) it treats the trade-off of precision versus complexity of clustering model through the rate distortion theory, which is a subfield of information theory; (2) it defines the “distance” or “similarity” in a well-defined way based on the mutual information. The efficiency of information bottleneck algorithm (Slonim and Tishby, 2000) motivates us to take it as the kernel of our framework. As far as we know, this approach has not been employed in opinion mining yet. In traditional information bottleneck approach, the distance between two data objects is measured by the Jensen-Shannon divergence (Lin, 1991), which aims to measure the divergence between two probability distributions. We alter this measure to integrate more semantic information, which will be illustrated in detail in the following sections, and the experimental result shows the effectiveness of the alteration. It would be worthwhile to highlight several aspects of our work here: • We propose an iterative reinforcement framework, and under this framework, review feature words and opinion words are organized into categories in a simultaneous and iterative manner. • In the process of clustering, the semantic information and the co-oc</context>
<context position="15314" citStr="Lin, 1991" startWordPosition="2600" endWordPosition="2601"> the categories. Every merger, (ci, cj) ⇒ c* , is formally defined by the following equation: p c (5) ( ) p c ( ) i j ( |) y c = p y c ( |) + p y c ( |) * i j p c ( ) p c ( ) * * ( ) ( ) ( ) c p c p c = + * i j The decrease in the mutual information I(C, Y) due to this merger is defined by δI(ci , cj) ≡ I (Cbefore,Y) − I(Cafter,Y) (6) When I(Cbefore,Y) and I(Cafter,Y) are the information values before and after the merger, respectively. After a little algebra, one can see δI(ci,cj) (p(ci) p(cj)) DJS p(y |ci),p(y |cj) ≡ + ⋅ ⎡ ⎣ ⎤ ⎦ (7) Where the functional DJS is the Jensen-Shannon divergence (Lin, 1991), defined as D where in our case ⎧{ } { p p , ≡ p y c p y c ( |), ( |)} ⎪ i j i j ⎪ ⎨ p c p c ( ) ( ) { } * i j ≡ ⎨ ⎧ ⎫ ⎬ π π , , i j ⎪ ⎩ p c p c ( ) ( ) * ⎭ ( |) ( |) y c + π p y c i j j By introducing the information optimization criterion the resulting similarity measure directly emerges from the analysis. The algorithm is now very simple. At each step we perform “the best possible merger”, i.e. merge the clusters {ci , cj } which minimize δI (ci , cj) . 2.3 Improved Information Bottleneck Algorithm for Semantic Information ⎧ ⎪ ⎪p ⎪ ⎨p ⎪ ⎪p ⎪ ⎪⎩ ( In traditional information bottleneck appro</context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>J. Lin. Divergence Measures Based on the Shannon Entropy. IEEE Transactions on Information theory, 37(1):145–151, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bloom</author>
<author>N Garg</author>
<author>S Argamon</author>
</authors>
<title>Extracting Appraisal Expressions. NAACL-HLT</title>
<date>2007</date>
<contexts>
<context position="3328" citStr="Bloom et al., 2007" startWordPosition="515" endWordPosition="518">ive or negative on the number of positive or negative opinion words in one text (Turney et al., 2003; Pang et al., 2002; Zagibalov et al., 2008;). With the in-depth study of opinion mining, researchers committed their efforts for more accurate results: the research of sentiment summarization (Philip et al., 2004; Hu et al., KDD 2004), domain transfer problem of the sentiment analysis (Kanayama et al., 2006; Tan et al., 2007; Blitzer et al., 2007; Tan et al., 2008; Andreevskaia et al., 2008; Tan et al., 2009) and finegrained opinion mining (Hatzivassiloglou et al., 2000; Takamura et al., 2007; Bloom et al., 2007; Wang et al., 2008; Titov et al., 2008) are the main branches of the research of opinion mining. In this paper, we focus on the fine-grained (feature-level) opinion mining. For many applications (e.g. the task of public affairs review analysis and the products review analysis), simply judging the sentiment orientation of a review unit is not sufficient. Researchers (Kushal, 2003; Hu et al., KDD 2004; Hu et al., AAAI 2004; Popescu et al., 2005) began to work on finer-grained opinion mining which predicts the sentiment orientation related to different review features. The task is known as featu</context>
</contexts>
<marker>Bloom, Garg, Argamon, 2007</marker>
<rawString>K. Bloom and N. Garg and S. Argamon. Extracting Appraisal Expressions. NAACL-HLT 2007. M. Hu and B. Liu. Mining and summarizing customer reviews. KDD 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining opinion features in customer reviews.</title>
<date>2004</date>
<publisher>AAAI</publisher>
<marker>Hu, Liu, 2004</marker>
<rawString>M. Hu and B. Liu. Mining opinion features in customer reviews. AAAI 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Slonim</author>
<author>N Tishby</author>
</authors>
<title>Agglomerative information bottleneck.</title>
<date>1999</date>
<publisher>NIPS</publisher>
<contexts>
<context position="14325" citStr="Slonim and Tishby, 1999" startWordPosition="2364" endWordPosition="2367">ompact ‘bottleneck’ of clusters C, that is forced to represent the ‘relevant’ part in X w.r.t to Y. An important special case is the “hard” clustering case where C is a deterministic function of X. That is, p(c|x) can only take values of zero or one, This restriction, which corresponds to the limit β → ∞ in Eqs 3 meaning every x ∈ X is assigned to exactly one cluster c ∈ C with a probability of one and to all the others with a probability of zero. This yields a natural distance measure between distributions which can be easily implemented in an agglomerative hierarchical clustering procedure (Slonim and Tishby, 1999). ⎧ 1, ( |) c x = ⎨ ⎩ 0, 1 ( |) y c = ∑ p x y ( , ) p c ( ) x c ∈ ( ) c = ∑p x ( ) x c ∈ The algorithm starts with a trivial partitioning into |X |singleton clusters, where each cluster contains exactly one element of X. At each step we merge two components of the current partition into a single new component in a way that locally minimizes the loss of mutual information about the categories. Every merger, (ci, cj) ⇒ c* , is formally defined by the following equation: p c (5) ( ) p c ( ) i j ( |) y c = p y c ( |) + p y c ( |) * i j p c ( ) p c ( ) * * ( ) ( ) ( ) c p c p c = + * i j The decrea</context>
</contexts>
<marker>Slonim, Tishby, 1999</marker>
<rawString>N. Slonim, N. Tishby. Agglomerative information bottleneck. NIPS 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Slonim</author>
<author>N Tishby</author>
</authors>
<title>Document Clustering Using word Clusters via the Information Bottleneck Method.</title>
<date>2000</date>
<publisher>SIGIR</publisher>
<contexts>
<context position="7393" citStr="Slonim and Tishby, 2000" startWordPosition="1160" endWordPosition="1163">nother one. Given some review features (or opinion words) gained from review corpus, we want to assemble them into categories, conserving the information about opinion words (or review features) as high as possible. The information bottleneck algorithm has some benefits, mainly including (1) it treats the trade-off of precision versus complexity of clustering model through the rate distortion theory, which is a subfield of information theory; (2) it defines the “distance” or “similarity” in a well-defined way based on the mutual information. The efficiency of information bottleneck algorithm (Slonim and Tishby, 2000) motivates us to take it as the kernel of our framework. As far as we know, this approach has not been employed in opinion mining yet. In traditional information bottleneck approach, the distance between two data objects is measured by the Jensen-Shannon divergence (Lin, 1991), which aims to measure the divergence between two probability distributions. We alter this measure to integrate more semantic information, which will be illustrated in detail in the following sections, and the experimental result shows the effectiveness of the alteration. It would be worthwhile to highlight several aspec</context>
</contexts>
<marker>Slonim, Tishby, 2000</marker>
<rawString>N. Slonim and N. Tishby. Document Clustering Using word Clusters via the Information Bottleneck Method. SIGIR 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Slonim</author>
<author>N Tishby</author>
</authors>
<title>The power of word clusters for text classification. ECIR</title>
<date>2001</date>
<marker>Slonim, Tishby, 2001</marker>
<rawString>N. Slonim and N. Tishby. The power of word clusters for text classification. ECIR 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Tishby</author>
<author>F Pereira</author>
<author>W Bialek</author>
</authors>
<title>The information bottleneck method.</title>
<date>1999</date>
<pages>0004057--1</pages>
<contexts>
<context position="10895" citStr="Tishby et al. (1999)" startWordPosition="1746" endWordPosition="1749"> of each variable that reserves the information about another variable as high as possible. Take F as an example, we want to find its compression, denoted as C, such that the mutual information between C and O is as high as possible, under a constraint on the mutual information between F and C. We propose an iterative reinforcement framework to deal with the tasks. An improved information bottleneck algorithm is employed in this framework, which will be illustrated in detail in the following sections. 2.2 Information Bottleneck Algorithm The information bottleneck method (IB) was presented by Tishby et al. (1999). According to Shannon‘s information theory (Cover and Thomas, 1991), for the two random variables X, Y, the mutual information I(X;Y) between the random variables X, Y is given by the symmetric functional: I(X;Y) = E P(x)P(Y I x)log p(y |x) (1) xE X,yEY p x)] = I(C, Y) (2) This solution is given in terms of the three distributions that characterize every cluster C, the prior probability for this cluster, p(c), its membership probabilities and its distribution over the relevance variable In general, the membership probabilities, is i.e. every x E X can be assigned to every C in some (normalize</context>
</contexts>
<marker>Tishby, Pereira, Bialek, 1999</marker>
<rawString>N. Tishby, F. Pereira, W. Bialek. The information bottleneck method. 1999, arXiv: physics/0004057v1</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<journal>ACL</journal>
<marker>Turney, 2002</marker>
<rawString>P. Turney. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. ACL 2002.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Turney</author>
<author>M Littman</author>
</authors>
<title>Measuring Praise and Criticism: Inference of Semantic Orientation from Association.</title>
<journal>ACM Transactions on Information Systems,</journal>
<volume>21</volume>
<issue>4</issue>
<pages>315--346</pages>
<marker>Turney, Littman, </marker>
<rawString>P. Turney and M. Littman. Measuring Praise and Criticism: Inference of Semantic Orientation from Association. ACM Transactions on Information Systems, 2003,21(4): 315-346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Su</author>
<author>X Xu</author>
<author>H Guo</author>
<author>Z Guo</author>
<author>X Wu</author>
<author>X Zhang</author>
<author>B Swen</author>
<author>Z Su</author>
</authors>
<title>Hidden sentiment association in Chinese web opinion mining. WWW</title>
<date>2008</date>
<marker>Su, Xu, Guo, Guo, Wu, Zhang, Swen, Su, 2008</marker>
<rawString>Q. Su, X. Xu, H. Guo, Z. Guo, X. Wu, X. Zhang, B. Swen and Z. Su. Hidden sentiment association in Chinese web opinion mining. WWW 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tan</author>
<author>G Wu</author>
<author>H Tang</author>
<author>X Cheng</author>
</authors>
<title>A novel scheme for domain-transfer problem in the context of sentiment analysis.</title>
<date>2007</date>
<journal>CIKM</journal>
<contexts>
<context position="3137" citStr="Tan et al., 2007" startWordPosition="481" endWordPosition="484">997, the early research results mainly focused on the polarity of opinion words (Hatzivassiloglou et al., 1997) and treated the text-level opinion mining as a classification of either positive or negative on the number of positive or negative opinion words in one text (Turney et al., 2003; Pang et al., 2002; Zagibalov et al., 2008;). With the in-depth study of opinion mining, researchers committed their efforts for more accurate results: the research of sentiment summarization (Philip et al., 2004; Hu et al., KDD 2004), domain transfer problem of the sentiment analysis (Kanayama et al., 2006; Tan et al., 2007; Blitzer et al., 2007; Tan et al., 2008; Andreevskaia et al., 2008; Tan et al., 2009) and finegrained opinion mining (Hatzivassiloglou et al., 2000; Takamura et al., 2007; Bloom et al., 2007; Wang et al., 2008; Titov et al., 2008) are the main branches of the research of opinion mining. In this paper, we focus on the fine-grained (feature-level) opinion mining. For many applications (e.g. the task of public affairs review analysis and the products review analysis), simply judging the sentiment orientation of a review unit is not sufficient. Researchers (Kushal, 2003; Hu et al., KDD 2004; Hu e</context>
</contexts>
<marker>Tan, Wu, Tang, Cheng, 2007</marker>
<rawString>S. Tan, G. Wu, H. Tang and X. Cheng. A novel scheme for domain-transfer problem in the context of sentiment analysis. CIKM 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tan</author>
<author>Y Wang</author>
<author>G Wu</author>
<author>X Cheng</author>
</authors>
<title>Using unlabeled data to handle domain-transfer problem of semantic detection.</title>
<date>2008</date>
<publisher>SAC</publisher>
<contexts>
<context position="3177" citStr="Tan et al., 2008" startWordPosition="489" endWordPosition="492">ocused on the polarity of opinion words (Hatzivassiloglou et al., 1997) and treated the text-level opinion mining as a classification of either positive or negative on the number of positive or negative opinion words in one text (Turney et al., 2003; Pang et al., 2002; Zagibalov et al., 2008;). With the in-depth study of opinion mining, researchers committed their efforts for more accurate results: the research of sentiment summarization (Philip et al., 2004; Hu et al., KDD 2004), domain transfer problem of the sentiment analysis (Kanayama et al., 2006; Tan et al., 2007; Blitzer et al., 2007; Tan et al., 2008; Andreevskaia et al., 2008; Tan et al., 2009) and finegrained opinion mining (Hatzivassiloglou et al., 2000; Takamura et al., 2007; Bloom et al., 2007; Wang et al., 2008; Titov et al., 2008) are the main branches of the research of opinion mining. In this paper, we focus on the fine-grained (feature-level) opinion mining. For many applications (e.g. the task of public affairs review analysis and the products review analysis), simply judging the sentiment orientation of a review unit is not sufficient. Researchers (Kushal, 2003; Hu et al., KDD 2004; Hu et al., AAAI 2004; Popescu et al., 2005) </context>
</contexts>
<marker>Tan, Wang, Wu, Cheng, 2008</marker>
<rawString>S. Tan, Y. Wang, G. Wu and X. Cheng. Using unlabeled data to handle domain-transfer problem of semantic detection. SAC 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tan</author>
<author>X Cheng</author>
<author>Y Wang</author>
<author>H Xu</author>
</authors>
<title>Adapting Naive Bayes to Domain Adaptation for Sentiment Analysis. ECIR</title>
<date>2009</date>
<contexts>
<context position="3223" citStr="Tan et al., 2009" startWordPosition="498" endWordPosition="501">ivassiloglou et al., 1997) and treated the text-level opinion mining as a classification of either positive or negative on the number of positive or negative opinion words in one text (Turney et al., 2003; Pang et al., 2002; Zagibalov et al., 2008;). With the in-depth study of opinion mining, researchers committed their efforts for more accurate results: the research of sentiment summarization (Philip et al., 2004; Hu et al., KDD 2004), domain transfer problem of the sentiment analysis (Kanayama et al., 2006; Tan et al., 2007; Blitzer et al., 2007; Tan et al., 2008; Andreevskaia et al., 2008; Tan et al., 2009) and finegrained opinion mining (Hatzivassiloglou et al., 2000; Takamura et al., 2007; Bloom et al., 2007; Wang et al., 2008; Titov et al., 2008) are the main branches of the research of opinion mining. In this paper, we focus on the fine-grained (feature-level) opinion mining. For many applications (e.g. the task of public affairs review analysis and the products review analysis), simply judging the sentiment orientation of a review unit is not sufficient. Researchers (Kushal, 2003; Hu et al., KDD 2004; Hu et al., AAAI 2004; Popescu et al., 2005) began to work on finer-grained opinion mining </context>
</contexts>
<marker>Tan, Cheng, Wang, Xu, 2009</marker>
<rawString>S. Tan, X. Cheng, Y. Wang and H. Xu. Adapting Naive Bayes to Domain Adaptation for Sentiment Analysis. ECIR 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cover</author>
<author>J Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons,</publisher>
<location>New York,</location>
<contexts>
<context position="10963" citStr="Cover and Thomas, 1991" startWordPosition="1756" endWordPosition="1759">iable as high as possible. Take F as an example, we want to find its compression, denoted as C, such that the mutual information between C and O is as high as possible, under a constraint on the mutual information between F and C. We propose an iterative reinforcement framework to deal with the tasks. An improved information bottleneck algorithm is employed in this framework, which will be illustrated in detail in the following sections. 2.2 Information Bottleneck Algorithm The information bottleneck method (IB) was presented by Tishby et al. (1999). According to Shannon‘s information theory (Cover and Thomas, 1991), for the two random variables X, Y, the mutual information I(X;Y) between the random variables X, Y is given by the symmetric functional: I(X;Y) = E P(x)P(Y I x)log p(y |x) (1) xE X,yEY p x)] = I(C, Y) (2) This solution is given in terms of the three distributions that characterize every cluster C, the prior probability for this cluster, p(c), its membership probabilities and its distribution over the relevance variable In general, the membership probabilities, is i.e. every x E X can be assigned to every C in some (normalized) probability. The information bottleneck principle determines the </context>
<context position="13114" citStr="Cover and Thomas, 1991" startWordPosition="2130" endWordPosition="2133">_is associated with all the codebook elements (C values), with some normalized probability. The IB method is based on the following simple idea. Given the empirical joint distribution of two variables, one variable is compressed so that the mutual information about the other variable is preserved as much as possible. The method can be considered as finding a minimal sufficient partition or efficient relevant coding of one variable with respect to the other one. This problem can be solved by introducing a Lagrange multiplier ,6 , and then minimizing the functional: Kullback-Leibler divergence (Cover and Thomas, 1991) between the conditional distri butions p(y|x) y ( ) 488 and p(y|c). Specifically, the formal optimal solution is given by the following equations which must be solved together. x) ⎧⎪ 1, c*| =⎨⎪⎩0, x∈cior x∈cj otherwise (c |x) = p(c) exp( −βDKL[p(y |x) ||p(y |c)]) Z(β, x) 1 p y c ( |) = ∑ p c x p x p y x ( |) ( ) ( |) x p c ( ) p c ( ) = ∑xp(c |x)p(x) (3) Where Z(β, x) is a normalization factor, and the single positive (Lagrange) parameter β determines the “softness” of the classification. Intuitively, in this procedure the information contained in X about Y ‘squeezed’ through a compact ‘bottl</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>T. Cover and J. Thomas. Elements of Information Theory. John Wiley &amp; Sons, New York, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zagibalov</author>
<author>J Carroll</author>
</authors>
<title>Automatic Seed Word Selection for Unsupervised Sentiment Classification of Chinese Text. Coling</title>
<date>2008</date>
<marker>Zagibalov, Carroll, 2008</marker>
<rawString>T. Zagibalov, J. Carroll. Automatic Seed Word Selection for Unsupervised Sentiment Classification of Chinese Text. Coling 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>K McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<publisher>ACL</publisher>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>V. Hatzivassiloglou and K. McKeown. Predicting the semantic orientation of adjectives. ACL 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>J Wiebe</author>
</authors>
<title>Effects of adjective orientation and gradability on sentence subjectivity. Coling</title>
<date>2000</date>
<marker>Hatzivassiloglou, Wiebe, 2000</marker>
<rawString>V. Hatzivassiloglou and J. Wiebe. Effects of adjective orientation and gradability on sentence subjectivity. Coling 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Rand</author>
</authors>
<title>Objective criteria for the evaluation of clustering methods.</title>
<date>1971</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>66</volume>
<pages>846--850</pages>
<contexts>
<context position="21737" citStr="Rand, 1971" startWordPosition="3734" endWordPosition="3735"> candidate product feature words and opinion words extracted from the corpus are shown as Table 2: 3.2 Experimental Procedure We evaluate our approach from two perspectives: 1) Effectiveness of product feature category construction by mutual reinforcement based clustering; 2) Precision of sentiment association between product feature categories and opinion word groups; 4 Experimental Results and Discussion 4.1 Evaluation of Review Feature Category Construction To calculate agreement between the review feature category construction results and the correct labels, we make use of the Rand index (Rand, 1971). This allows for a measure of agreement between two partitions, P, and P2, of the same data set D. Each partition is viewed as a collection of n*(n-1)/2 pair wise decisions, where n is the size of D. For each pair of points di and dj in D, Pi either assigns them to the same cluster or to different clusters. Let a be the number of decisions where di is in the same cluster as dj in P, and in P2. Let b be the number of decisions where the two instances are placed in different clusters in both partitions. Total agreement can then be calculated using a b + Rand P P ( 1 , 2) = n (n 1) / 2 − In our </context>
</contexts>
<marker>Rand, 1971</marker>
<rawString>W. Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, 66, 846-850. 1971</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>