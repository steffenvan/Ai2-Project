<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997526">
Improving Nominal SRL in Chinese Language with Verbal SRL In-
formation and Automatic Predicate Recognition
</title>
<author confidence="0.996423">
Junhui Li† Guodong Zhou†∗ Hai Zhao†* Qiaoming Zhu† Peide Qian†† Jiangsu Provincial Key Lab for Computer Information Processing Technologies
</author>
<affiliation confidence="0.925909">
School of Computer Science and Technology
Soochow University, Suzhou, China 215006
‡ Department of Chinese, Translation and Linguistics
City University of HongKong, China
</affiliation>
<email confidence="0.997762">
Email: {lijunhui,gdzhou,hzhao,qmzhu,pdqian}@suda.edu.cn
</email>
<sectionHeader confidence="0.982974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997925625">
This paper explores Chinese semantic role la-
beling (SRL) for nominal predicates. Besides
those widely used features in verbal SRL,
various nominal SRL-specific features are
first included. Then, we improve the perform-
ance of nominal SRL by integrating useful
features derived from a state-of-the-art verbal
SRL system. Finally, we address the issue of
automatic predicate recognition, which is es-
sential for a nominal SRL system. Evaluation
on Chinese NomBank shows that our research
in integrating various features derived from
verbal SRL significantly improves the per-
formance. It also shows that our nominal SRL
system much outperforms the state-of-the-art
ones.
</bodyText>
<sectionHeader confidence="0.995478" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999965444444444">
Semantic parsing maps a natural language sen-
tence into a formal representation of its meaning.
Due to the difficulty in deep semantic parsing,
most of previous work focuses on shallow se-
mantic parsing, which assigns a simple structure
(such as WHO did WHAT to WHOM, WHEN,
WHERE, WHY, HOW) to each predicate in a
sentence. In particular, the well-defined seman-
tic role labeling (SRL) task has been drawing
more and more attention in recent years due to
its importance in deep NLP applications, such as
question answering (Narayanan and Harabagiu,
2004), information extraction (Surdeanu et al.,
2003), and co-reference resolution (Ponzetto and
Strube, 2006). Given a sentence and a predicate
(either a verb or a noun) in it, SRL recognizes
and maps all the constituents in the sentence into
their corresponding semantic arguments (roles)
</bodyText>
<note confidence="0.339598">
∗ Corresponding author
</note>
<bodyText confidence="0.98277205">
of the predicate. According to the predicate
types, SRL could be divided into SRL for verbal
predicates (verbal SRL, in short) and SRL for
nominal predicates (nominal SRL, in short).
During the past few years, verbal SRL has
dominated the research on SRL with the avail-
ability of FrameNet (Baker et al., 1998), Prop-
Bank (Palmer et al., 2005), and the consecutive
CoNLL shared tasks (Carreras and Màrquez,
2004 &amp; 2005) in English language. As a com-
plement to PropBank on verbal predicates,
NomBank (Meyers et al., 2004) annotates nomi-
nal predicates and their corresponding semantic
roles using similar semantic framework as
PropBank. As a representative, Jiang and Ng
(2006) pioneered the exploration of various
nominal SRL-specific features besides the tradi-
tional verbal SRL-related features on NomBank.
They achieved the performance of 72.73 and
69.14 in F1-measure on golden and automatic
syntactic parse trees, respectively, given golden
nominal predicates.
For SRL in Chinese, Sun and Jurafsky (2004)
and Pradhan et al. (2004) pioneered the research
on Chinese verbal and nominal SRLs, respec-
tively, on small private datasets. Taking the ad-
vantage of recent release of Chinese PropBank
(Xue and Palmer, 2003) and Chinese NomBank
(Xue, 2006a), Xue and his colleagues (Xue and
Palmer 2005; Xue 2006b; Xue, 2008) pioneered
the exploration of Chinese verbal and nominal
SRLs, given golden predicates. Among them,
Xue and Palmer (2005) studied Chinese verbal
SRL using Chinese PropBank and achieved the
performance of 91.3 and 61.3 in F1-measure on
golden and automatic syntactic parse trees, re-
spectively. Xue (2006b) extended their study on
Chinese nominal SRL and attempted to improve
the performance of nominal SRL by simply in-
1280
</bodyText>
<note confidence="0.9990545">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1280–1288,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999700115384615">
cluding the Chinese PropBank training instances
into the training data for nominal SRL on Chi-
nese NomBank. However, such integration was
empirically proven unsuccessful due to the dif-
ferent nature of certain features for verbal and
nominal SRLs. Xue (2008) further improved the
performance on both verbal and nominal SRLs
with a better syntactic parser and new features.
Ding and Chang (2008) focused on argument
classification for Chinese verbal predicates with
hierarchical feature selection strategy. They
achieved the classification precision of 94.68%
on golden parse trees on Chinese PropBank.
This paper focuses on Chinese nominal SRL.
This is done by adopting a traditional verbal
SRL architecture to handle Chinese nominal
predicates with additional nominal SRL-specific
features. Moreover, we significantly enhance the
performance of nominal SRL by properly inte-
grating various features derived from verbal
SRL. Finally, this paper investigates the effect of
automatic nominal predicate recognition on the
performance of Chinese nominal SRL. Although
previous research (e.g. CoNLL’2008) in English
nominal SRL reveals the importance of auto-
matic predicate recognition, there has no re-
</bodyText>
<note confidence="0.352952">
IP
</note>
<bodyText confidence="0.999042461538461">
ported research on automatic predicate
recognition in Chinese nominal SRL.
The rest of this paper is organized as follows:
Section 2 introduces Chinese NomBank while
the baseline nominal SRL system is described in
Section 3 with traditional and nominal SRL-
specific features. Then, the baseline nominal
SRL system is improved by integrating useful
features derived from verbal SRL (Section 4)
and extended with automatic recognition of
nominal predicates (Section 5). Section 6 gives
experimental results and discussion. Finally,
Section 7 concludes the paper.
</bodyText>
<sectionHeader confidence="0.959098" genericHeader="introduction">
2. Chinese NomBank
</sectionHeader>
<bodyText confidence="0.9798194">
Chinese NomBank (Xue, 2006a) adopts similar
semantic framework as NomBank, and focuses
on Chinese nominal predicates with their argu-
ments in Chinese TreeBank. The semantic ar-
guments include:
</bodyText>
<listItem confidence="0.999637571428571">
1) Core arguments: Arg0 to Arg5. Generally,
Arg0 and Arg1 denotes the agent and the
patient, respectively, while arguments from
Arg2 to Arg5 are predicate-specific.
2) Adjunct arguments, which are universal to
all predicates, e.g. ArgM-LOC for locative,
and ArgM-TMP for temporal.
</listItem>
<table confidence="0.874903545454545">
Arg1/Rel2 VP PU
NP Arg0/Rel2 VP 。
.
NN NN PP Sup/Rel2 NP
中国 银行 P NP VV QP NP
Bank of China
向 Arg0/Rel1 Rel1 Arg1/Rel1 提供 CD ArgM-MNR/Rel2 Rel2
to NN NN NN provide 四十亿 NN NN
外商 投资 银行 4 billion 人民币 贷款
Foreign Investment Bank RMB loan
Bank of China provides 4 billion RMB loan to Foreign Investment Bank.
</table>
<figureCaption confidence="0.996079">
Figure 1: Two nominal predicates and their arguments in the style of NomBank.
</figureCaption>
<bodyText confidence="0.9993073">
All the arguments are annotated on parse tree
nodes with their boundaries aligning with the
spans of tree nodes. Figure 1 gives an example
with two nominal predicates and their respective
arguments, while the nominal predicate “投资
/investment” has two core arguments, “NN(外商
/foreign businessman)” as Arg0 and “NN(银行
/bank)” as Arg1, and the other nominal predicate
“ 贷 款 /loan” also has two core arguments,
“NP(中国银行/Bank of China)” as Arg1 and
</bodyText>
<page confidence="0.439025">
1281
</page>
<bodyText confidence="0.995201266666667">
“PP( 向 外 商 投 资 银 行 /to Foreign Investment
Bank)” as Arg0, and 1 adjunct argument,
“NN(人民币/RMB)” as ArgM-MNR, denoting
the manner of loan. It is worth noticing that
there is a (Chinese) NomBank-specific label in
Figure 1, Sup (support verb) (Xue, 2006a), in
helping introduce the arguments, which occur
outside the nominal predicate-headed noun
phrase. This is illustrated by the nominal predi-
cate “贷款/loan”, whose Arg0 and Arg1 are both
realized outside the nominal predicate-headed
noun phrase, NP(四十亿人民币贷款/4 billion
RMB loan). Normally, a verb is marked as a
support verb only when it shares some argu-
ments with the nominal predicate.
</bodyText>
<sectionHeader confidence="0.805855" genericHeader="method">
3. Baseline: Chinese Nominal SRL
</sectionHeader>
<bodyText confidence="0.996945">
Popular SRL systems usually formulate SRL as
a classification problem, which annotates each
constituent in a parse tree with a semantic role
label or with the non-argument label NULL. Be-
sides, we divide the system into three consecu-
tive phases so as to overcome the imbalance
between the training instances of the NULL
class and those of any other argument classes.
Argument pruning. Here, several heuristic
rules are adopted to filter out constituents, which
are most likely non-arguments. According to the
argument structures of nominal predicates, we
categorize arguments into two types: arguments
inside NP (called inside arguments) and argu-
ments introduced via a support verb (called out-
side arguments), and handle them separately.
For the inside arguments, the following three
heuristic rules are applied to find inside argu-
ment candidates:
</bodyText>
<listItem confidence="0.9998628">
• All the sisters of the predicate are candi-
dates.
• If a CP or DNP node is a candidate, its chil-
dren are candidates too.
• For any node X, if its parent is an ancestral
</listItem>
<bodyText confidence="0.998388509803922">
node of the predicate, and the internal
nodes along the path between X and the
predicate are all NPs, then X is a candidate.
For outside arguments, we look for the sup-
port verb of the focus nominal predicate, and
then adopt the rules as proposed in Xue and
Palmer (2005) to find the candidates for the sup-
port verb, since outside argument candidates are
introduced via this support verb. That to say, the
argument candidates of the support verb are re-
garded as outside argument candidates of the
nominal predicate. However, as support verbs
are not annotated explicitly in the testing phase,
we identify intervening verbs as alternatives to
support verbs in both training and testing phases
with the path between the nominal predicate and
intervening verb in the form of
“VV&lt;VP&gt;[NP&gt;]+NN”, where “[NP&gt;]+” denotes
one or more NPs. Our statistics on Chinese
NomBank shows that 51.96% of nominal predi-
cates have no intervening verb while 48.04% of
nominal predicates have only one intervening
verb.
Taken the nominal predicate “贷款/loan” in
Figure 1 as an example, NN(人民币/RMB) and
QP(四十亿/4 billion) are identified as inside
argument candidates, while PP(向外商投资银
行/to Foreign Investment Bank) and NP(中国银
行/Bank of China) are identified as outside ar-
gument candidates via the support verb VV(提
供/provide).
Argument identification. A binary classifier
is applied to determine the candidates as either
valid arguments or non-arguments. It is worth
pointing out that we only mark those candidates
that are most likely to be NULL (with probabil-
ity &gt; 0.90) as non-arguments. Our empirical
study shows that this little trick much benefits
nominal SRL, since argument identification for
nominal predicates is much more difficult than
that for verbal predicates and thus many argu-
ments would have been falsely marked as non-
arguments if the threshold is set as 0.5.
Argument classification. A multi-class classi-
fier is employed to label identified arguments
with specific argument labels (including the
NULL class for non-argument).
In the following, we first adapt some tradi-
tional features, which have been proven effec-
tive in verbal SRL, to nominal SRL, and then
introduce several nominal SRL-specific features.
</bodyText>
<subsectionHeader confidence="0.989802">
3.1. Traditional Features
</subsectionHeader>
<bodyText confidence="0.9764714">
Using the feature naming convention as adopted
in Jiang and Ng (2006), Table 1 lists the tradi-
tional features, where “I” and “C” indicate the
features for argument identification and classifi-
cation, respectively. Among them, the predicate
class (b2) feature was first introduced in Xue
and Palmer (2005) to overcome the imbalance of
the predicate distribution in that some predicates
can be only found in the training data while
some predicates in the testing data are absent
from the training data. In particular, the verb
class is classified along three dimensions: the
number of arguments, the number of framesets
and selected syntactic alternations. For example,
1282
the verb class of “C1C2a” means that it has two
framesets, with the first frameset having one
argument and the second having two arguments.
The symbol “a” in the second frameset repre-
sents a type of syntactic alternation.
</bodyText>
<figure confidence="0.914277416666667">
Feature Remarks: b1-b5(C, I), b6-b7(C)
b1 Predicate: the nominal predicate itself. (Rr
/loan)
b2 Predicate class: the verb class that the predi-
cate belongs to. (C4a)
b3 Head word (b3H) and its POS (b3P). (VT7
/bank, NN)
b4 Phrase type: the syntactic category of the
constituent. (NP)
b5 Path: the path from the constituent to the
nominal predicate.
(NP&lt;IP&gt;VP&gt;VP&gt;NP&gt;NP&gt;NN)
</figure>
<figureCaption confidence="0.425408666666667">
b6 Position: the positional relationship of the
constituent with the predicate. “left” or
“right”. (left)
</figureCaption>
<table confidence="0.5504432">
b7 First word (b7F) and last word (b7L) of the
focus constituent. (rP[M/China, VT7/bank)
Combined features: b11-b14(C, I), b15(C)
b11: b1&amp;b4; b12: b1&amp;b3H; b13: b2&amp;b4;
b14: b2&amp;b3H; b15: b5&amp;b6
</table>
<tableCaption confidence="0.842801">
Table 1: Traditional features and their instantiations
</tableCaption>
<bodyText confidence="0.81453375">
for argument identification and classification, with
NP(rP[MVT7/Bank of China) as the focus constitu-
ent and NN(Rr/loan) as the nominal predicate, re-
garding Figure 1.
</bodyText>
<subsectionHeader confidence="0.986181">
3.2. Nominal SRL-specific Features
</subsectionHeader>
<bodyText confidence="0.9216227">
To capture more useful information in the predi-
cate-argument structure, we also study addi-
tional features which provide extra information.
Statistics on Chinese NomBank show that about
40% of pruned inside candidates are arguments.
Since inside arguments usually locate near to the
nominal predicate, its surroundings are expected
to be helpful in SRL. Table 2 shows the features
in better capturing the details between inside
arguments and nominal predicates. Specially,
features ai6 and ai7 are sister-related features,
inspired by the features related with the
neighboring arguments in Jiang and Ng (2006).
Statistics on NomBank and Chinese Nom-
Bank show that about 20% and 22% of argu-
ments are introduced via a support verb,
respectively. Since a support verb pivots outside
arguments and the nominal predicate on its two
sides, support verbs play an important role in
labeling these arguments. Here, we also identify
intervening verbs as alternatives to support verbs
since support verbs are not explicitly in the test-
ing phase. Table 3 lists the intervening verb-
related features (ao1-ao4, ao11-ao14) employed
in this paper.
Feature Remarks
ai1 Whether the focus constituent is adjacent to
the predicate. Yes or No. (Yes)
ai2 The headword (ai2H) and pos (ai2P) of the
predicate’s nearest right sister. (VT7/bank,
NN)
ai3 Whether the predicate has right sisters. Yes
or No. (Yes)
ai4 Compressed path of b5: compressing se-
quences of identical labels into one.
(NN&lt;NP&gt;NN)
ai5 Whether the predicate has sisters. Yes or
No. (Yes)
ai6 For each sister of the focus constituent,
combine b3H&amp;b4&amp;b5&amp;b6. ( V T7
</bodyText>
<equation confidence="0.452235">
/bank&amp;NN &amp; NN&lt;NP&gt;NN&amp;right)
ai7 Coarse version of ai6, b4&amp;b6. (NN&amp;right)
</equation>
<bodyText confidence="0.6262568">
Table 2: Additional features and their instantiations
for inside argument candidates, with “NN( * -0
/foreign businessman)” as the focus constituent and
“NN( &amp; 5f /investment)” as the nominal predicate,
regarding Figure1.
</bodyText>
<table confidence="0.79007225">
Feature Remarks
ao1 Intervening verb itself. (19#/provide)
ao2 The verb class that the intervening verb
belongs to. (C3b)
ao3 The path from the focus constituent to the
intervening verb. (NP&lt;IP&gt;VP&gt;VP&gt;VV)
ao4 The compressed path of ao3: compressing
sequences of identical labels into one.
(NP&lt;IP&gt;VP&gt;VV)
Combined features: ao11-ao14
ao11: ao1&amp;ao3; ao12: ao1&amp;ao4;
ao13: ao2&amp;ao3; ao14: ao2&amp;ao4.
</table>
<tableCaption confidence="0.958824">
Table 3: Additional features and their instantiations
</tableCaption>
<bodyText confidence="0.971720333333333">
for outside argument candidates, with “NP(rP[MVT7
/Bank of China)” as the focus constituent and “Rr
/loan” as the nominal predicate, regarding Figure1.
Feature selection. Some Features proposed
above may not be effective in tasks of identifica-
tion and classification. We adopt the greedy fea-
ture selection algorithm as described in Jiang
and Ng (2006) to pick up positive features em-
pirically and incrementally according to their
contributions on the development data. The al-
gorithm repeatedly selects one feature each time
which contributes most, and stops when adding
any of the remaining features fails to improve
the performance. As far as the SRL task con-
cerned, the whole feature selection process could
be done as follows: 1). Feature selection for ar-
gument identification: run the selection algo-
1283
rithm with the basic set of features (b1-b5, b11-
b14) to pick up effective features from (ai1-ai7,
ao1-ao4, ao11-ao14); 2). Feature selection for
argument classification: fix the output returned
in step1 as the feature set of argument identifica-
tion, and run the selection algorithm with the
basic set of features (b1-b7, b11-b15) to select
positive features from (ai1-ai7, ao1-ao4, ao11-
ao14) for argument classification.
</bodyText>
<sectionHeader confidence="0.902005" genericHeader="method">
4. Integrating Features derived from
Verbal SRL
</sectionHeader>
<bodyText confidence="0.999399">
Since Chinese PropBank and NomBank are an-
notated on the same data set with the same lexi-
cal guidelines (e.g. frame files), it may be
interesting to investigate the contribution of
Chinese verbal SRL on the performance of Chi-
nese nominal SRL. In the frame files, argument
labels are defined with regard to their semantic
roles to the predicate, either a verbal or nominal
predicate. For example, in the frame file of
predicate “贷款/loan”, the borrower is always
labeled with Arg0 and the lender labeled with
Arg1. This can be demonstrated by the follow-
ing two sentences: “贷款/loan” is annotated as a
nominal and a verbal predicate in S1 and S2,
respectively.
</bodyText>
<table confidence="0.9661245">
S1 [Arg1 中国银行/Bank of China] [Arg0 向外商
投资银行/to Foreign Investment Bank] 提供
/provide [Rel 贷款/loan]
S2 [Arg0 中国银行/Bank of China] [Arg1 向外商
投资银行/from Foreign Investment Bank] [Rel
贷款/loan]
</table>
<bodyText confidence="0.989633018181818">
Therefore, it is straightforward to augment
nominal training instances with verbal ones.
However, Xue (2006b) found that simply adding
the training instances for verbal SRL to the
training data for nominal SRL and indiscrimi-
nately extracting the same features in both ver-
bal and nominal SRLs hurt the performance.
This may be due to that certain features (e.g. the
path feature) are much different for verbal and
nominal SRLs. This can be illustrated in sen-
tences S1 and S2: the verbal instances in S2 are
negative for semantic role labeling of the nomi-
nal predicate “贷款/loan” in S1, since “中国银
行/Bank of China” takes opposite roles in S1
and S2. So does “向外商投资银行/(from/to)
Foreign Investment Bank”.
Although several support verb-related features
(ao1-ao4, ao11-ao14) have been proposed, one
may still ask how large the role support verbs
can play in nominal SRL. It is interesting to note
that outside arguments and the highest NP
phrase headed by the nominal predicate are also
annotated as arguments of the support verb in
Chinese PropBank. For example, Chinese Prop-
Bank marks “中国银行/Bank of China” as Arg0
and “四十亿人民币贷款/4 billion RMB loan”
as Arg1 for verb “提供/provide” in Figure1. Let
OA be the outside argument, VV be the support
verb, and NP be the highest NP phrase headed
by the nominal predicate NN, then there exists a
pattern “OA VV NN” in the sentence, where the
support verb VV plays a certain role in trans-
ferring roles between OA and NN. For example,
if OA is the agent of VV, then OA is also the
agent of phrase VP(VV NN). Like the example
in Figure1, supposing a NP is the agent of sup-
port verb “提供/provide” as well as VP phrase
(“提供四十 亿人民币 贷款/provide 4 billion
RMB loan”), we can infer that the NP is the
lender of the nominal predicate “贷款/loan” in-
dependently on any other information, such as
the NP content and the path from the NP to the
nominal predicate “贷款/loan”.
Let C be the focus constituent, V be the inter-
vening verb, and NP be the highest NP headed
by the nominal predicate. Table 4 shows the fea-
tures (ao5-ao8, p1-p7) derived from verbal SRL.
In this paper, we develop a state-of-the-art Chi-
nese verbal SRL system, similar to the one as
shown in Xue (2008), to achieve the goal. Based
on golden parse trees on Chinese PropBank, our
Chinese verbal SRL system achieves the per-
formance of 92.38 in F1-measure, comparable to
Xue (2008) which achieved the performance of
92.0 in F1-measure.
</bodyText>
<figure confidence="0.839704">
Feature Remarks
ao5 Whether C is an argument for V. Yes or No
ao6 The semantic role of C for V.
ao7 Whether NP is an argument for V. Yes or No
ao8 The semantic role of NP for V.
Combined features: p1-p7
</figure>
<tableCaption confidence="0.870319">
p1: ao1&amp;ao5; p2: ao1&amp;ao6; p3: ao1&amp;ao5&amp;b1;
p4: ao1&amp;ao6&amp;b1; p5: ao1&amp;apo7; p6: ao1&amp;ao8;
p7: ao5&amp;ao7.
Table 4: Features derived from verbal SRL.
</tableCaption>
<sectionHeader confidence="0.942815" genericHeader="method">
5. Automatic Predicate Recognition
</sectionHeader>
<bodyText confidence="0.9996155">
Unlike Chinese PropBank where almost all the
verbs are annotated as predicates, Chinese Nom-
Bank only marks those nouns having arguments
as predicates. Statistics on Chinese NomBank
show that only 17.5% of nouns are marked as
predicates. It is possible that a noun is a predi-
</bodyText>
<page confidence="0.548265">
1284
</page>
<bodyText confidence="0.999982315789474">
cate in some cases but not in others. Previous
Chinese nominal SRL systems (Xue, 2006b;
Xue, 2008) assume that nominal predicates have
already been manually annotated and thus are
available. To our best knowledge, there is no
report on addressing automatic recognition of
nominal predicates on Chinese nominal SRL.
Automatic recognition of nominal predicates
can be cast as a binary classification (e.g., Predi-
cate vs. Non-Predicate) problem. This paper
employs the convolution tree kernel, as proposed
in Collins and Duffy (2001), on automatic rec-
ognition of nominal predicates.
Given the convolution tree kernel, the key
problem is how to extract a parse tree structure
from the parse tree for a nominal predicate can-
didate. In this paper, the parse tree structure is
constructed as follows: 1) starting from the
predicate candidate’s POS node, collect all of its
sister nodes (with their headwords); 2). recur-
sively move one level up and collect all of its
sister nodes (with their headwords) till reaching
a non-NP node. Specially, in order to explicitly
mark the positional relation between a node and
the predicate candidate, all nodes on the left side
of the candidate are augmented with tags 1 and 2
for nodes on the right side. Figure 2 shows an
example of the parse tree structure with regard
to the predicate candidate “XJIA/loan” as shown
in Figure 1.
In our extra experiments we found global sta-
tistic features (e.g. g1-g5) about the predicate
candidate are helpful in a feature vector-based
method for predicate recognition. Figure 2
makes an attempt to utilize those features in ker-
nel-based method. We have explored other ways
to include those global features. However, the
way in Figure 2 works best.
</bodyText>
<equation confidence="0.835317">
VP
g2 Whether
</equation>
<bodyText confidence="0.851232">
is ever annotated as a nominal
predicate in the training data? Yes or No.
g3 The most likely label for
when it occurs
</bodyText>
<equation confidence="0.4027068">
together with w
1 an
0
0
-
</equation>
<bodyText confidence="0.7975474">
d w1.
g4 The most likely label for w0 when it occurs
together with w-1.
g5 The most likely label for w0 when it occurs
together with w1.
</bodyText>
<sectionHeader confidence="0.78854" genericHeader="method">
6. Experiment Results and Discussion
</sectionHeader>
<bodyText confidence="0.839069666666667">
We have evaluated our Chinese nominal SRL
system on Chinese NomBank with Chinese
PropBank 2.0 as its counterpart.
</bodyText>
<figure confidence="0.971202105263158">
VV1
NP
QP1 NP
NA 1
provide
VQ+IL 1
4 billion
NN1 NN
g1 .... g5
measure on golden an
d automatic word segmen-
tation, respectively2.
AMfi 1 XJIA In addition, SVMLight with the tree kernel
gives a performance of 82.5 and 85.5 in F1-
RMB loan
is selected as our
classifier. In order to han
2004)3
dle multi-classification
</figure>
<figureCaption confidence="0.983988">
Figure 2: Semantic sub-tree for nominal pre
</figureCaption>
<bodyText confidence="0.9286392">
dicate
Let the predicate candidate be
and its left
and right neighbor words be
an
</bodyText>
<equation confidence="0.9214255">
w0,
w-1
</equation>
<bodyText confidence="0.961026916666667">
d w1, respec-
tively. The five global features are defined as
follows.
g1 Whether w0 is ever tagged as a verb in the
training data? Yes or No.
Berkeley Parser. http://code.google.com/p/berkeleyparser/
POSs are not counted in evaluating the performance of
word-based syntactic parser, but they are counted in evalu-
ating the performance of character-based parser. Therefore
the F1-measure for the later is higher than that for the for-
mer.
SVM-LIGHT-TK.
</bodyText>
<figure confidence="0.55484525">
1
2
3
http://dit.unitn.it/~moschitt/
</figure>
<subsectionHeader confidence="0.977381">
6.1. Experimental Settings
</subsectionHeader>
<bodyText confidence="0.976910307692308">
This version of Chinese NomBank consists of
standoff annotations on the files (chtb 001 to
1151.fid) of Chinese Penn TreeBank 5.1. Fol-
lowing the experimental setting in Xue (2008),
648 files (chtb _081 to 899.fid) are selected as
the training data, 72 files (chtb 001 to 040.fid
and chtb _900 to 931.fid) are held out as the test
data, and 40 files (chtb 041 to 080.fid) as the
development data, with 8642, 1124, and 731
propositions, respectively.
As Chinese words are not naturally segmented
in raw sentences, two Chinese automatic parsers
are constructed: word-based parser (assuming
golden word segmentation) and character-based
parser (with automatic word segmentation).
Here, Berkeley parser (Petrov and Klein, 2007)1
is chosen as the Chinese automatic parser. With
regard to character-based parsing, we employ a
Chinese word segmenter, similar to Ng and Low
(2004), to obtain the best automatic segmenta-
tion result for a given sentence, which is then
fed into Berkeley parser for further syntactic
parsing. Both the word segmenter and Berkeley
parser are developed with the same training and
development datasets as our SRL experiments.
The word segmenter achieves the performance
of 96.1 in F1-measure while the Berkeley parser
function (Moschitti,
1285
problem in argument classification, we apply the
one vs. others strategy, which builds K classifi-
ers so as to separate one class from all others.
For argument identification and classification,
we adopt the linear kernel and the training pa-
rameter C is fine-tuned to 0.220. For automatic
recognition of nominal predicates, the training
parameter C and the decay factor λ in the con-
volution tree kernel are fine-tuned to 2.0 and 0.2,
respectively.
</bodyText>
<subsectionHeader confidence="0.633843">
6.2. Results with Golden Parse Trees and
</subsectionHeader>
<table confidence="0.936485833333333">
Golden Nominal Predicates
Effect of nominal SRL-specific features
Rec.(%) Pre.(%) F1
traditional features 62.83 73.58 67.78
+nominal SRL-specific 69.90 75.11 72.55
features
</table>
<tableCaption confidence="0.984309666666667">
Table 5: The performance of nominal SRL on the
development data with golden parse trees and golden
nominal predicates
</tableCaption>
<bodyText confidence="0.9658805">
After performing the greedy feature selection
algorithm on the development data, features
{ao1, ai6, ai2P, ai5, ao2, ao12, ao14}, as pro-
posed in Section 3.2, are selected consecutively
for argument identification, while features {ai7,
ao1, ai1, ao2, ai5, ao4} are selected for argument
classification. Table 5 presents the SRL results
on the development data. It shows that nominal
SRL-specific features significantly improve the
performance from 67.78 to 72.55 ( X 2 p &lt;
; 0. 05)
in F1-measure.
</bodyText>
<table confidence="0.990485142857143">
Effect of features derived from verbal SRL
Features Rec.(%) Pre.(%) F1
baseline 67.86 73.63 70.63
+ao5 68.15 73.60 70.77 (+0.14)
+ao6 67.66 72.80 70.14 (-0.49)
+ao7 68.20 75.41 71.62 (+0.99)
+ao8 68.30 75.39 71.67 (+1.04)
+p1 67.91 74.40 71.00 (+0.37)
+p2 67.76 74.20 70.83 (+0.20)
+p3 67.96 74.69 71.16 (+0.53)
+p4 68.01 74.18 70.96 (+0.33)
+p5 68.01 75.01 71.39 (+0.76)
+p6 68.20 75.12 71.49 (+0.86)
+p7 68.40 75.70 71.87 (+1.24)
</table>
<tableCaption confidence="0.8781888">
Table 6: Effect of features derived from verbal SRL
on the performance of nominal SRL on the test data
with golden parse trees and golden nominal predi-
cates. The first row presents the performance using
traditional and nominal SRL-specific features.
</tableCaption>
<table confidence="0.9998822">
Rec.(%) Pre.(%) F1
baseline 67.86 73.63 70.63
+features derived 68.40 77.51 72.67
from verbal SRL
Xue (2008) 66.1 73.4 69.6
</table>
<tableCaption confidence="0.883691333333333">
Table 7: The performance of nominal SRL on the test
data with golden parse trees and golden nominal
predicates
</tableCaption>
<bodyText confidence="0.991973083333333">
Table 6 shows the effect of features derived
from verbal SRL in an incremental way. It
shows that only the feature ao6 has negative ef-
fect due to its strong relevance with intervening
verbs and thus not included thereafter. Table 7
shows the performance on the test data with or
without using the features derived from the ver-
bal SRL system. It shows these features signifi-
cantly improve the performance ( X 2 p &lt;
; 0.05)
on nominal SRL. Table 7 also shows our system
outperforms Xue (2008) by 3.1 in F1-measure.
</bodyText>
<subsectionHeader confidence="0.992117">
6.3. Results with Automatic Parse Trees
and Golden Nominal Predicates
</subsectionHeader>
<bodyText confidence="0.9999005">
In previous section we have assumed the avail-
ability of golden parse trees during the testing
process. Here we conduct experiments on auto-
matic parse trees, using the Berkeley parser.
Since arguments come from constituents in
parse trees, those arguments, which do not align
with any syntactic constituents, are simply dis-
carded. Moreover, for any nominal predicate
segmented incorrectly by the word segmenter,
all its arguments are unable to be labeled neither.
Table 8 presents the SRL performance on the
test data by using automatic parse trees. It shows
that the performance drops from 72.67 to 60.87
in F1-measure when replacing golden parse trees
with word-based automatic ones, partly due to
the absence of 6.9% arguments in automatic
trees, and wrong POS tagging of nominal predi-
cates. Table 8 also compares our system with
Xue (2008). It shows that our system also out-
performs Xue (2008) on Chinese NomBank.
</bodyText>
<equation confidence="0.515502">
Rec. (%) Pre. (%) F1
</equation>
<bodyText confidence="0.798071181818182">
This paper 56.95(53.55) 66.74(66.69) 60.87(59.40)
Xue (2008) 53.1 (52.9) 62.9 (62.3) 57.6 (57.3)
Table 8: The performance of nominal SRL on the test
data with automatic parse trees and golden predicates.
Here, the numbers outside the parentheses indicate
the performance using a word-based parser, while the
numbers inside indicate the performance using a
character-based parser4.
4 About 1.6% nominal predicates are mistakenly segmented
by the character-based parser, thus their arguments are
missed directly.
</bodyText>
<page confidence="0.461259">
1286
</page>
<sectionHeader confidence="0.6661635" genericHeader="method">
6.4. Results with Automatic Nominal Predi-
cates
</sectionHeader>
<bodyText confidence="0.999586125">
So far nominal predicates are assumed to be
manually annotated and available. Here we turn
to a more realistic scenario in which both the
parse tree and nominal predicates are automati-
cally obtained. In the following, we first report
the results of automatic nominal predicate rec-
ognition and then the results of nominal SRL on
automatic recognition of nominal predicates.
</bodyText>
<table confidence="0.9449135">
Results of nominal predicate recognition
Parses g1-g5 Rec.(%) Pre.(%) F1
golden no 91.46 88.93 90.18
yes 92.62 89.36 90.96
word-based yes 86.39 81.80 84.03
character-based yes 84.79 81.94 83.34
</table>
<tableCaption confidence="0.9530745">
Table 9: The performance of automatic nominal
predicate recognition on the test data
</tableCaption>
<bodyText confidence="0.976371666666667">
Table 9 lists the predicate recognition results,
using the parse tree structure, as shown in Sec-
tion 5, and the convolution tree kernel, as pro-
posed in Collins and Duffy (2001). The second
column (g1-g5) indicates whether the global fea-
tures (g1-g5) are included in the parse tree struc-
ture. We have also defined a simple rule that
treats a noun which is ever a verb or a nominal
predicate in the training data as a nominal predi-
cate. Based on golden parse trees, the rule re-
ceives the performance of 81.40 in F1-measure.
This suggests that our method significantly out-
performs the simple rule-based one. Table 9 also
shows that:
• As a complement to local structural informa-
tion, global features improve the performance
of automatic nominal predicate recognition
by 0.78 in F1-measure.
• The word-based syntactic parser decreases
the F1-measure from 90.96 to 84.03, mostly
due to the POSTagging errors between NN
and VV, while the character-based syntactic
parser further drops the F1-measure by 0.69,
due to automatic word segmentation.
</bodyText>
<table confidence="0.926137">
Results with automatic predicates
Parses Predicates Rec.(%) Pre.(%) F1
golden golden 68.40 77.51 72.67
automatic 65.07 74.65 69.53
word- golden 55.95 66.74 60.87
based automatic 52.67 59.56 55.90
character- golden 53.55 66.69 59.40
based automatic 50.66 59.60 54.77
</table>
<tableCaption confidence="0.888271333333333">
Table 10: The performance of nominal SRL on the
test data with the choices of golden/automatic parse
trees and golden/automatic predicates
</tableCaption>
<bodyText confidence="0.99985875">
In order to have a clear performance comparison
among nominal SRL on golden/automatic parse
trees and golden/automatic predicates, Table 10
lists all the results in those scenarios.
</bodyText>
<subsectionHeader confidence="0.942974">
6.5. Comparison
</subsectionHeader>
<bodyText confidence="0.993302023255814">
Chinese nominal SRL vs. Chinese verbal SRL
Comparison with Xue (2008) shows that the per-
formance of Chinese nominal SRL is about 20
lower (e.g. 72.67 vs. 92.38 in F1-measure) than
that of Chinese verbal SRL, partly due to the
smaller amount of annotated data (about 1/5) in
Chinese NomBank than that in Chinese Prop-
Bank. Moreover, according to Chinese Nom-
Bank annotation criteria (Xue 2006a), even
when a noun is a true deverbal noun, not all of
its modifiers are legitimate arguments or ad-
juncts of this predicate. Only arguments that can
co-occur with both the nominal and verbal forms
of the predicate are considered in the NomBank
annotation. This means that the judgment of ar-
guments is semantic rather than syntactic. These
facts may also partly explain the lower nominal
SRL performance, especially the performance of
argument identification. This can be illustrated
by the statistics on the development data that
96% (40%) of verbal (nominal) predicates’ sis-
ters are annotated as arguments. Finally, the
predicate-argument structure of nominal predi-
cates is more flexible and complicated than that
of verbal predicates as illustrated in Xue (2006a).
Chinese nominal SRL vs. English nominal
SRL
Liu and Ng (2007) reported the performance of
77.04 and 72.83 in F1-measure on English Nom-
Bank when golden and automatic parse trees are
used, respectively. Taking into account that Chi-
nese verbal SRL achieves comparable perform-
ance with English verbal SRL on golden parse
trees, the performance gap between Chinese and
English nominal SRL (e.g. 72.67 vs. 77.04 in
F1-measure) presents great challenge for Chi-
nese nominal SRL. Moreover, while automatic
parse trees only decrease the performance of
English nominal SRL by about 4.2 in F1-
measure, automatic parse trees significantly de-
crease the performance of Chinese nominal SRL
by more than 12 in F1-measure due to the much
lower performance of Chinese syntactic parsing.
</bodyText>
<sectionHeader confidence="0.945711" genericHeader="conclusions">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.884216315789473">
In this paper we investigate nominal SRL in
Chinese language. In particular, some nominal
SRL-specific features are included to improve
1287
the performance. Moreover, various features
derived from verbal SRL are properly integrated
into nominal SRL. Finally, a convolution tree
kernel is adopted to address the issue of auto-
matic nominal predicates recognition, which is
essential in a nominal SRL system.
To our best knowledge, this is the first re-
search on
1) Exploring Chinese nominal SRL on auto-
matic parse trees with automatic predicate
recognition;
2) Successfully integrating features derived
from Chinese verbal SRL into Chinese nomi-
nal SRL with much performance improve-
ment.
</bodyText>
<sectionHeader confidence="0.965093" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9997736">
This research was supported by Project
60673041 and 60873150 under the National
Natural Science Foundation of China, Project
2006AA01Z147 under the “863” National
High-Tech Research and Development of China,
and Project BK2008160 under the Natural Sci-
ence Foundation of the Jiangsu province of
China. We also want to thank Dr. Nianwen Xue
for share of the verb class file. We also want to
thank the reviewers for insightful comments.
</bodyText>
<sectionHeader confidence="0.996461" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999887974025974">
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet Project. In
Proceedings of COLING-ACL 1998.
Xavier Carreras and Lluis Màrquez. 2004. Introduc-
tion to the CoNLL-2004 Shared Task: Semantic
Role Labeling. In Proceedings of CoNLL 2004.
Xavier Carreras and Lluis Màrquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic
Role Labeling. In Proceedings of CoNLL 2005.
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Proceedings of
NIPS 2001.
Weiwei Ding and Baobao Chang. 2008. Improving
Chinese Semantic Role Classification with Hierar-
chical Feature Selection Strategy. In Proceedings
of EMNLP 2008.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic
role Labeling of NomBank: a Maximum Entropy
Approach. In Proceedings of EMNLP 2006.
Chang Liu and Hwee Tou Ng. 2007. Learning Predic-
tive Structures for Semantic Role Labeling of
NomBank. In Proceedings of ACL 2007.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V.
Zielinska, B. Yong, and R. Grishman. 2004. Anno-
tating Noun Argument Structure for NomBank. In
Proceedings of LREC 2004.
Alessandro Moschitti. 2004. A Study on Convolution
Kernels for Shallow Semantic Parsing. In Pro-
ceedings of ACL 2004.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion Answering based on Semantic Structures. In
Proceedings of COLING 2004.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-
of-speech Tagging: One-at-a-time or All-at-once?
Word-based or Character-based? In Proceedings
of EMNLP 2004.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics.
Slav Petrov. and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proceesings of
NAACL 2007.
Simone Paolo Ponzetto and Michael Strube. 2006.
Semantic Role Labeling for Coreference Resolu-
tion. In Proceedings of EACL 2006.
Sameer Pradhan, Honglin Sun, Wayne Ward, James
H. Martin, and Dan Jurafsky. 2004. Parsing Ar-
guments of Nominalizations in English and Chi-
nese. In Proceedings of NAACL-HLT 2004.
Honglin Sun and Daniel Jurafsky. 2004. Shallow
Semantic Parsing of Chinese. In Proceedings of
NAACL 2004.
Mihai Surdeanu, Sanda Harabagiu, John Williams
and Paul Aarseth. 2003. Using Predicate-argument
Structures for Information Extraction. In Proceed-
ings of ACL 2003.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluis Màrquez, and Joakim Nivre. 2008. The
CoNLL-2008 Shared Task on Joint Parsing of
Syntactic and Semantic Dependencies. In Pro-
ceedings of CoNLL 2008.
Nianwen Xue and Martha Palmer. 2003. Annotating
the Propositions in the Penn Chinese TreeBank. In
Proceedings of 2nd SIGHAN Workshop on Chinese
Language Processing.
Nianwen Xue and Martha Palmer. 2005. Automatic
Semantic Role Labeling for Chinese verbs. In
Proceedings of IJCAI 2005.
Nianwen Xue. 2006a. Annotating the Predicate-
Argument Structure of Chinese Nominalizations.
In Proceedings of the LREC 2006.
Nianwen Xue. 2006b. Semantic Role Labeling of
Nominalized Predicates in Chinese. In Proceed-
ings of HLT-NAACL 2006.
Nianwen Xue. 2008. Labeling Chinese Predicates
with Semantic Roles. Computational Linguistics,
34(2):225-255.
</reference>
<page confidence="0.722172">
1288
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.670634">
<title confidence="0.9979015">Nominal SRL in Chinese Language with Verbal SRL formation and Automatic Predicate Recognition</title>
<author confidence="0.779122">Guodong Hai Qiaoming Peide Provincial Key Lab for Computer Information Processing</author>
<affiliation confidence="0.98220775">School of Computer Science and Soochow University, Suzhou, China of Chinese, Translation and City University of HongKong,</affiliation>
<email confidence="0.992113">lijunhui,gdzhou,hzhao,qmzhu,pdqian}@suda.edu.cn</email>
<abstract confidence="0.995620823529412">This paper explores Chinese semantic role labeling (SRL) for nominal predicates. Besides those widely used features in verbal SRL, various nominal SRL-specific features are first included. Then, we improve the performance of nominal SRL by integrating useful features derived from a state-of-the-art verbal SRL system. Finally, we address the issue of automatic predicate recognition, which is essential for a nominal SRL system. Evaluation on Chinese NomBank shows that our research in integrating various features derived from verbal SRL significantly improves the performance. It also shows that our nominal SRL system much outperforms the state-of-the-art ones.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<contexts>
<context position="2333" citStr="Baker et al., 1998" startWordPosition="347" endWordPosition="350">), information extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ Corresponding author of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 &amp; 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse t</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of COLING-ACL 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis Màrquez</author>
</authors>
<title>Introduction to the CoNLL-2004 Shared Task: Semantic Role Labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<contexts>
<context position="2433" citStr="Carreras and Màrquez, 2004" startWordPosition="363" endWordPosition="366">Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ Corresponding author of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 &amp; 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and</context>
</contexts>
<marker>Carreras, Màrquez, 2004</marker>
<rawString>Xavier Carreras and Lluis Màrquez. 2004. Introduction to the CoNLL-2004 Shared Task: Semantic Role Labeling. In Proceedings of CoNLL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis Màrquez</author>
</authors>
<title>Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<marker>Carreras, Màrquez, 2005</marker>
<rawString>Xavier Carreras and Lluis Màrquez. 2005. Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling. In Proceedings of CoNLL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<booktitle>In Proceedings of NIPS</booktitle>
<contexts>
<context position="20794" citStr="Collins and Duffy (2001)" startWordPosition="3297" endWordPosition="3300"> that only 17.5% of nouns are marked as predicates. It is possible that a noun is a predi1284 cate in some cases but not in others. Previous Chinese nominal SRL systems (Xue, 2006b; Xue, 2008) assume that nominal predicates have already been manually annotated and thus are available. To our best knowledge, there is no report on addressing automatic recognition of nominal predicates on Chinese nominal SRL. Automatic recognition of nominal predicates can be cast as a binary classification (e.g., Predicate vs. Non-Predicate) problem. This paper employs the convolution tree kernel, as proposed in Collins and Duffy (2001), on automatic recognition of nominal predicates. Given the convolution tree kernel, the key problem is how to extract a parse tree structure from the parse tree for a nominal predicate candidate. In this paper, the parse tree structure is constructed as follows: 1) starting from the predicate candidate’s POS node, collect all of its sister nodes (with their headwords); 2). recursively move one level up and collect all of its sister nodes (with their headwords) till reaching a non-NP node. Specially, in order to explicitly mark the positional relation between a node and the predicate candidate</context>
<context position="29759" citStr="Collins and Duffy (2001)" startWordPosition="4758" endWordPosition="4761">he following, we first report the results of automatic nominal predicate recognition and then the results of nominal SRL on automatic recognition of nominal predicates. Results of nominal predicate recognition Parses g1-g5 Rec.(%) Pre.(%) F1 golden no 91.46 88.93 90.18 yes 92.62 89.36 90.96 word-based yes 86.39 81.80 84.03 character-based yes 84.79 81.94 83.34 Table 9: The performance of automatic nominal predicate recognition on the test data Table 9 lists the predicate recognition results, using the parse tree structure, as shown in Section 5, and the convolution tree kernel, as proposed in Collins and Duffy (2001). The second column (g1-g5) indicates whether the global features (g1-g5) are included in the parse tree structure. We have also defined a simple rule that treats a noun which is ever a verb or a nominal predicate in the training data as a nominal predicate. Based on golden parse trees, the rule receives the performance of 81.40 in F1-measure. This suggests that our method significantly outperforms the simple rule-based one. Table 9 also shows that: • As a complement to local structural information, global features improve the performance of automatic nominal predicate recognition by 0.78 in F</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution Kernels for Natural Language. In Proceedings of NIPS 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Ding</author>
<author>Baobao Chang</author>
</authors>
<title>Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="4306" citStr="Ding and Chang (2008)" startWordPosition="652" endWordPosition="655">L and attempted to improve the performance of nominal SRL by simply in1280 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1280–1288, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP cluding the Chinese PropBank training instances into the training data for nominal SRL on Chinese NomBank. However, such integration was empirically proven unsuccessful due to the different nature of certain features for verbal and nominal SRLs. Xue (2008) further improved the performance on both verbal and nominal SRLs with a better syntactic parser and new features. Ding and Chang (2008) focused on argument classification for Chinese verbal predicates with hierarchical feature selection strategy. They achieved the classification precision of 94.68% on golden parse trees on Chinese PropBank. This paper focuses on Chinese nominal SRL. This is done by adopting a traditional verbal SRL architecture to handle Chinese nominal predicates with additional nominal SRL-specific features. Moreover, we significantly enhance the performance of nominal SRL by properly integrating various features derived from verbal SRL. Finally, this paper investigates the effect of automatic nominal predi</context>
</contexts>
<marker>Ding, Chang, 2008</marker>
<rawString>Weiwei Ding and Baobao Chang. 2008. Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy. In Proceedings of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Ping Jiang</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Semantic role Labeling of NomBank: a Maximum Entropy Approach.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="2697" citStr="Jiang and Ng (2006)" startWordPosition="404" endWordPosition="407">L could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 &amp; 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (X</context>
<context position="10972" citStr="Jiang and Ng (2006)" startWordPosition="1713" endWordPosition="1716">minal predicates is much more difficult than that for verbal predicates and thus many arguments would have been falsely marked as nonarguments if the threshold is set as 0.5. Argument classification. A multi-class classifier is employed to label identified arguments with specific argument labels (including the NULL class for non-argument). In the following, we first adapt some traditional features, which have been proven effective in verbal SRL, to nominal SRL, and then introduce several nominal SRL-specific features. 3.1. Traditional Features Using the feature naming convention as adopted in Jiang and Ng (2006), Table 1 lists the traditional features, where “I” and “C” indicate the features for argument identification and classification, respectively. Among them, the predicate class (b2) feature was first introduced in Xue and Palmer (2005) to overcome the imbalance of the predicate distribution in that some predicates can be only found in the training data while some predicates in the testing data are absent from the training data. In particular, the verb class is classified along three dimensions: the number of arguments, the number of framesets and selected syntactic alternations. For example, 12</context>
<context position="13330" citStr="Jiang and Ng (2006)" startWordPosition="2074" endWordPosition="2077">c Features To capture more useful information in the predicate-argument structure, we also study additional features which provide extra information. Statistics on Chinese NomBank show that about 40% of pruned inside candidates are arguments. Since inside arguments usually locate near to the nominal predicate, its surroundings are expected to be helpful in SRL. Table 2 shows the features in better capturing the details between inside arguments and nominal predicates. Specially, features ai6 and ai7 are sister-related features, inspired by the features related with the neighboring arguments in Jiang and Ng (2006). Statistics on NomBank and Chinese NomBank show that about 20% and 22% of arguments are introduced via a support verb, respectively. Since a support verb pivots outside arguments and the nominal predicate on its two sides, support verbs play an important role in labeling these arguments. Here, we also identify intervening verbs as alternatives to support verbs since support verbs are not explicitly in the testing phase. Table 3 lists the intervening verbrelated features (ao1-ao4, ao11-ao14) employed in this paper. Feature Remarks ai1 Whether the focus constituent is adjacent to the predicate.</context>
<context position="15416" citStr="Jiang and Ng (2006)" startWordPosition="2395" endWordPosition="2398">ing verb. (NP&lt;IP&gt;VP&gt;VP&gt;VV) ao4 The compressed path of ao3: compressing sequences of identical labels into one. (NP&lt;IP&gt;VP&gt;VV) Combined features: ao11-ao14 ao11: ao1&amp;ao3; ao12: ao1&amp;ao4; ao13: ao2&amp;ao3; ao14: ao2&amp;ao4. Table 3: Additional features and their instantiations for outside argument candidates, with “NP(rP[MVT7 /Bank of China)” as the focus constituent and “Rr /loan” as the nominal predicate, regarding Figure1. Feature selection. Some Features proposed above may not be effective in tasks of identification and classification. We adopt the greedy feature selection algorithm as described in Jiang and Ng (2006) to pick up positive features empirically and incrementally according to their contributions on the development data. The algorithm repeatedly selects one feature each time which contributes most, and stops when adding any of the remaining features fails to improve the performance. As far as the SRL task concerned, the whole feature selection process could be done as follows: 1). Feature selection for argument identification: run the selection algo1283 rithm with the basic set of features (b1-b5, b11- b14) to pick up effective features from (ai1-ai7, ao1-ao4, ao11-ao14); 2). Feature selection </context>
</contexts>
<marker>Jiang, Ng, 2006</marker>
<rawString>Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic role Labeling of NomBank: a Maximum Entropy Approach. In Proceedings of EMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Liu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Learning Predictive Structures for Semantic Role Labeling of NomBank.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="32444" citStr="Liu and Ng (2007)" startWordPosition="5185" endWordPosition="5188">te are considered in the NomBank annotation. This means that the judgment of arguments is semantic rather than syntactic. These facts may also partly explain the lower nominal SRL performance, especially the performance of argument identification. This can be illustrated by the statistics on the development data that 96% (40%) of verbal (nominal) predicates’ sisters are annotated as arguments. Finally, the predicate-argument structure of nominal predicates is more flexible and complicated than that of verbal predicates as illustrated in Xue (2006a). Chinese nominal SRL vs. English nominal SRL Liu and Ng (2007) reported the performance of 77.04 and 72.83 in F1-measure on English NomBank when golden and automatic parse trees are used, respectively. Taking into account that Chinese verbal SRL achieves comparable performance with English verbal SRL on golden parse trees, the performance gap between Chinese and English nominal SRL (e.g. 72.67 vs. 77.04 in F1-measure) presents great challenge for Chinese nominal SRL. Moreover, while automatic parse trees only decrease the performance of English nominal SRL by about 4.2 in F1- measure, automatic parse trees significantly decrease the performance of Chines</context>
</contexts>
<marker>Liu, Ng, 2007</marker>
<rawString>Chang Liu and Hwee Tou Ng. 2007. Learning Predictive Structures for Semantic Role Labeling of NomBank. In Proceedings of ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekely</author>
<author>V Zielinska</author>
<author>B Yong</author>
<author>R Grishman</author>
</authors>
<title>Annotating Noun Argument Structure for NomBank.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="2542" citStr="Meyers et al., 2004" startWordPosition="382" endWordPosition="385">nstituents in the sentence into their corresponding semantic arguments (roles) ∗ Corresponding author of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 &amp; 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small priv</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Yong, Grishman, 2004</marker>
<rawString>A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Yong, and R. Grishman. 2004. Annotating Noun Argument Structure for NomBank. In Proceedings of LREC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A Study on Convolution Kernels for Shallow Semantic Parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A Study on Convolution Kernels for Shallow Semantic Parsing. In Proceedings of ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Narayanan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Question Answering based on Semantic Structures.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="1715" citStr="Narayanan and Harabagiu, 2004" startWordPosition="248" endWordPosition="251">nominal SRL system much outperforms the state-of-the-art ones. 1. Introduction Semantic parsing maps a natural language sentence into a formal representation of its meaning. Due to the difficulty in deep semantic parsing, most of previous work focuses on shallow semantic parsing, which assigns a simple structure (such as WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate in a sentence. In particular, the well-defined semantic role labeling (SRL) task has been drawing more and more attention in recent years due to its importance in deep NLP applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ Corresponding author of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (B</context>
</contexts>
<marker>Narayanan, Harabagiu, 2004</marker>
<rawString>Srini Narayanan and Sanda Harabagiu. 2004. Question Answering based on Semantic Structures. In Proceedings of COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese Partof-speech Tagging: One-at-a-time or All-at-once? Word-based or Character-based?</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="24264" citStr="Ng and Low (2004)" startWordPosition="3879" endWordPosition="3882">les (chtb 001 to 040.fid and chtb _900 to 931.fid) are held out as the test data, and 40 files (chtb 041 to 080.fid) as the development data, with 8642, 1124, and 731 propositions, respectively. As Chinese words are not naturally segmented in raw sentences, two Chinese automatic parsers are constructed: word-based parser (assuming golden word segmentation) and character-based parser (with automatic word segmentation). Here, Berkeley parser (Petrov and Klein, 2007)1 is chosen as the Chinese automatic parser. With regard to character-based parsing, we employ a Chinese word segmenter, similar to Ng and Low (2004), to obtain the best automatic segmentation result for a given sentence, which is then fed into Berkeley parser for further syntactic parsing. Both the word segmenter and Berkeley parser are developed with the same training and development datasets as our SRL experiments. The word segmenter achieves the performance of 96.1 in F1-measure while the Berkeley parser function (Moschitti, 1285 problem in argument classification, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others. For argument identification and classification, we adopt the li</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Partof-speech Tagging: One-at-a-time or All-at-once? Word-based or Character-based? In Proceedings of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics.</title>
<date>2005</date>
<contexts>
<context position="2365" citStr="Palmer et al., 2005" startWordPosition="353" endWordPosition="356">eanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ Corresponding author of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 &amp; 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing.</title>
<date>2007</date>
<booktitle>In Proceesings of NAACL</booktitle>
<contexts>
<context position="24115" citStr="Klein, 2007" startWordPosition="3857" endWordPosition="3858">e Penn TreeBank 5.1. Following the experimental setting in Xue (2008), 648 files (chtb _081 to 899.fid) are selected as the training data, 72 files (chtb 001 to 040.fid and chtb _900 to 931.fid) are held out as the test data, and 40 files (chtb 041 to 080.fid) as the development data, with 8642, 1124, and 731 propositions, respectively. As Chinese words are not naturally segmented in raw sentences, two Chinese automatic parsers are constructed: word-based parser (assuming golden word segmentation) and character-based parser (with automatic word segmentation). Here, Berkeley parser (Petrov and Klein, 2007)1 is chosen as the Chinese automatic parser. With regard to character-based parsing, we employ a Chinese word segmenter, similar to Ng and Low (2004), to obtain the best automatic segmentation result for a given sentence, which is then fed into Berkeley parser for further syntactic parsing. Both the word segmenter and Berkeley parser are developed with the same training and development datasets as our SRL experiments. The word segmenter achieves the performance of 96.1 in F1-measure while the Berkeley parser function (Moschitti, 1285 problem in argument classification, we apply the one vs. oth</context>
</contexts>
<marker>Klein, 2007</marker>
<rawString>Slav Petrov. and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In Proceesings of NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Semantic Role Labeling for Coreference Resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL</booktitle>
<contexts>
<context position="1820" citStr="Ponzetto and Strube, 2006" startWordPosition="261" endWordPosition="264">l language sentence into a formal representation of its meaning. Due to the difficulty in deep semantic parsing, most of previous work focuses on shallow semantic parsing, which assigns a simple structure (such as WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate in a sentence. In particular, the well-defined semantic role labeling (SRL) task has been drawing more and more attention in recent years due to its importance in deep NLP applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ Corresponding author of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and </context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2006. Semantic Role Labeling for Coreference Resolution. In Proceedings of EACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Honglin Sun</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Parsing Arguments of Nominalizations in English and Chinese.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<contexts>
<context position="3055" citStr="Pradhan et al. (2004)" startWordPosition="456" endWordPosition="459">&amp; 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended the</context>
</contexts>
<marker>Pradhan, Sun, Ward, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Honglin Sun, Wayne Ward, James H. Martin, and Dan Jurafsky. 2004. Parsing Arguments of Nominalizations in English and Chinese. In Proceedings of NAACL-HLT 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglin Sun</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Shallow Semantic Parsing of Chinese.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="3029" citStr="Sun and Jurafsky (2004)" startWordPosition="451" endWordPosition="454">(Carreras and Màrquez, 2004 &amp; 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively</context>
</contexts>
<marker>Sun, Jurafsky, 2004</marker>
<rawString>Honglin Sun and Daniel Jurafsky. 2004. Shallow Semantic Parsing of Chinese. In Proceedings of NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda Harabagiu</author>
<author>John Williams</author>
<author>Paul Aarseth</author>
</authors>
<title>Using Predicate-argument Structures for Information Extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1763" citStr="Surdeanu et al., 2003" startWordPosition="254" endWordPosition="257"> ones. 1. Introduction Semantic parsing maps a natural language sentence into a formal representation of its meaning. Due to the difficulty in deep semantic parsing, most of previous work focuses on shallow semantic parsing, which assigns a simple structure (such as WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate in a sentence. In particular, the well-defined semantic role labeling (SRL) task has been drawing more and more attention in recent years due to its importance in deep NLP applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ Corresponding author of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 200</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Mihai Surdeanu, Sanda Harabagiu, John Williams and Paul Aarseth. 2003. Using Predicate-argument Structures for Information Extraction. In Proceedings of ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluis Màrquez</author>
<author>Joakim Nivre</author>
</authors>
<date>2008</date>
<booktitle>The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of CoNLL</booktitle>
<marker>Surdeanu, Johansson, Meyers, Màrquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluis Màrquez, and Joakim Nivre. 2008. The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of CoNLL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Annotating the Propositions in the Penn Chinese TreeBank.</title>
<date>2003</date>
<booktitle>In Proceedings of 2nd SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="3237" citStr="Xue and Palmer, 2003" startWordPosition="485" endWordPosition="488"> similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply in1280 Proceedings of the 2009 Conference on Empirical Methods in Natural Language P</context>
</contexts>
<marker>Xue, Palmer, 2003</marker>
<rawString>Nianwen Xue and Martha Palmer. 2003. Annotating the Propositions in the Penn Chinese TreeBank. In Proceedings of 2nd SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Automatic Semantic Role Labeling for Chinese verbs.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI</booktitle>
<contexts>
<context position="3315" citStr="Xue and Palmer 2005" startWordPosition="498" endWordPosition="501">) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply in1280 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1280–1288, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP c</context>
<context position="8949" citStr="Xue and Palmer (2005)" startWordPosition="1398" endWordPosition="1401">a support verb (called outside arguments), and handle them separately. For the inside arguments, the following three heuristic rules are applied to find inside argument candidates: • All the sisters of the predicate are candidates. • If a CP or DNP node is a candidate, its children are candidates too. • For any node X, if its parent is an ancestral node of the predicate, and the internal nodes along the path between X and the predicate are all NPs, then X is a candidate. For outside arguments, we look for the support verb of the focus nominal predicate, and then adopt the rules as proposed in Xue and Palmer (2005) to find the candidates for the support verb, since outside argument candidates are introduced via this support verb. That to say, the argument candidates of the support verb are regarded as outside argument candidates of the nominal predicate. However, as support verbs are not annotated explicitly in the testing phase, we identify intervening verbs as alternatives to support verbs in both training and testing phases with the path between the nominal predicate and intervening verb in the form of “VV&lt;VP&gt;[NP&gt;]+NN”, where “[NP&gt;]+” denotes one or more NPs. Our statistics on Chinese NomBank shows t</context>
<context position="11206" citStr="Xue and Palmer (2005)" startWordPosition="1749" endWordPosition="1752">oyed to label identified arguments with specific argument labels (including the NULL class for non-argument). In the following, we first adapt some traditional features, which have been proven effective in verbal SRL, to nominal SRL, and then introduce several nominal SRL-specific features. 3.1. Traditional Features Using the feature naming convention as adopted in Jiang and Ng (2006), Table 1 lists the traditional features, where “I” and “C” indicate the features for argument identification and classification, respectively. Among them, the predicate class (b2) feature was first introduced in Xue and Palmer (2005) to overcome the imbalance of the predicate distribution in that some predicates can be only found in the training data while some predicates in the testing data are absent from the training data. In particular, the verb class is classified along three dimensions: the number of arguments, the number of framesets and selected syntactic alternations. For example, 1282 the verb class of “C1C2a” means that it has two framesets, with the first frameset having one argument and the second having two arguments. The symbol “a” in the second frameset represents a type of syntactic alternation. Feature R</context>
</contexts>
<marker>Xue, Palmer, 2005</marker>
<rawString>Nianwen Xue and Martha Palmer. 2005. Automatic Semantic Role Labeling for Chinese verbs. In Proceedings of IJCAI 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Annotating the PredicateArgument Structure of Chinese Nominalizations.</title>
<date>2006</date>
<booktitle>In Proceedings of the LREC</booktitle>
<contexts>
<context position="3268" citStr="Xue, 2006" startWordPosition="492" endWordPosition="493"> a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply in1280 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1280–1288, Sin</context>
<context position="5716" citStr="Xue, 2006" startWordPosition="857" endWordPosition="858">eIP ported research on automatic predicate recognition in Chinese nominal SRL. The rest of this paper is organized as follows: Section 2 introduces Chinese NomBank while the baseline nominal SRL system is described in Section 3 with traditional and nominal SRLspecific features. Then, the baseline nominal SRL system is improved by integrating useful features derived from verbal SRL (Section 4) and extended with automatic recognition of nominal predicates (Section 5). Section 6 gives experimental results and discussion. Finally, Section 7 concludes the paper. 2. Chinese NomBank Chinese NomBank (Xue, 2006a) adopts similar semantic framework as NomBank, and focuses on Chinese nominal predicates with their arguments in Chinese TreeBank. The semantic arguments include: 1) Core arguments: Arg0 to Arg5. Generally, Arg0 and Arg1 denotes the agent and the patient, respectively, while arguments from Arg2 to Arg5 are predicate-specific. 2) Adjunct arguments, which are universal to all predicates, e.g. ArgM-LOC for locative, and ArgM-TMP for temporal. Arg1/Rel2 VP PU NP Arg0/Rel2 VP 。 . NN NN PP Sup/Rel2 NP 中国 银行 P NP VV QP NP Bank of China 向 Arg0/Rel1 Rel1 Arg1/Rel1 提供 CD ArgM-MNR/Rel2 Rel2 to NN NN NN</context>
<context position="7242" citStr="Xue, 2006" startWordPosition="1117" endWordPosition="1118"> spans of tree nodes. Figure 1 gives an example with two nominal predicates and their respective arguments, while the nominal predicate “投资 /investment” has two core arguments, “NN(外商 /foreign businessman)” as Arg0 and “NN(银行 /bank)” as Arg1, and the other nominal predicate “ 贷 款 /loan” also has two core arguments, “NP(中国银行/Bank of China)” as Arg1 and 1281 “PP( 向 外 商 投 资 银 行 /to Foreign Investment Bank)” as Arg0, and 1 adjunct argument, “NN(人民币/RMB)” as ArgM-MNR, denoting the manner of loan. It is worth noticing that there is a (Chinese) NomBank-specific label in Figure 1, Sup (support verb) (Xue, 2006a), in helping introduce the arguments, which occur outside the nominal predicate-headed noun phrase. This is illustrated by the nominal predicate “贷款/loan”, whose Arg0 and Arg1 are both realized outside the nominal predicate-headed noun phrase, NP(四十亿人民币贷款/4 billion RMB loan). Normally, a verb is marked as a support verb only when it shares some arguments with the nominal predicate. 3. Baseline: Chinese Nominal SRL Popular SRL systems usually formulate SRL as a classification problem, which annotates each constituent in a parse tree with a semantic role label or with the non-argument label NU</context>
<context position="17295" citStr="Xue (2006" startWordPosition="2696" endWordPosition="2697">te, either a verbal or nominal predicate. For example, in the frame file of predicate “贷款/loan”, the borrower is always labeled with Arg0 and the lender labeled with Arg1. This can be demonstrated by the following two sentences: “贷款/loan” is annotated as a nominal and a verbal predicate in S1 and S2, respectively. S1 [Arg1 中国银行/Bank of China] [Arg0 向外商 投资银行/to Foreign Investment Bank] 提供 /provide [Rel 贷款/loan] S2 [Arg0 中国银行/Bank of China] [Arg1 向外商 投资银行/from Foreign Investment Bank] [Rel 贷款/loan] Therefore, it is straightforward to augment nominal training instances with verbal ones. However, Xue (2006b) found that simply adding the training instances for verbal SRL to the training data for nominal SRL and indiscriminately extracting the same features in both verbal and nominal SRLs hurt the performance. This may be due to that certain features (e.g. the path feature) are much different for verbal and nominal SRLs. This can be illustrated in sentences S1 and S2: the verbal instances in S2 are negative for semantic role labeling of the nominal predicate “贷款/loan” in S1, since “中国银 行/Bank of China” takes opposite roles in S1 and S2. So does “向外商投资银行/(from/to) Foreign Investment Bank”. Althoug</context>
<context position="20349" citStr="Xue, 2006" startWordPosition="3233" endWordPosition="3234">es or No ao8 The semantic role of NP for V. Combined features: p1-p7 p1: ao1&amp;ao5; p2: ao1&amp;ao6; p3: ao1&amp;ao5&amp;b1; p4: ao1&amp;ao6&amp;b1; p5: ao1&amp;apo7; p6: ao1&amp;ao8; p7: ao5&amp;ao7. Table 4: Features derived from verbal SRL. 5. Automatic Predicate Recognition Unlike Chinese PropBank where almost all the verbs are annotated as predicates, Chinese NomBank only marks those nouns having arguments as predicates. Statistics on Chinese NomBank show that only 17.5% of nouns are marked as predicates. It is possible that a noun is a predi1284 cate in some cases but not in others. Previous Chinese nominal SRL systems (Xue, 2006b; Xue, 2008) assume that nominal predicates have already been manually annotated and thus are available. To our best knowledge, there is no report on addressing automatic recognition of nominal predicates on Chinese nominal SRL. Automatic recognition of nominal predicates can be cast as a binary classification (e.g., Predicate vs. Non-Predicate) problem. This paper employs the convolution tree kernel, as proposed in Collins and Duffy (2001), on automatic recognition of nominal predicates. Given the convolution tree kernel, the key problem is how to extract a parse tree structure from the pars</context>
<context position="31614" citStr="Xue 2006" startWordPosition="5055" endWordPosition="5056">automatic predicates In order to have a clear performance comparison among nominal SRL on golden/automatic parse trees and golden/automatic predicates, Table 10 lists all the results in those scenarios. 6.5. Comparison Chinese nominal SRL vs. Chinese verbal SRL Comparison with Xue (2008) shows that the performance of Chinese nominal SRL is about 20 lower (e.g. 72.67 vs. 92.38 in F1-measure) than that of Chinese verbal SRL, partly due to the smaller amount of annotated data (about 1/5) in Chinese NomBank than that in Chinese PropBank. Moreover, according to Chinese NomBank annotation criteria (Xue 2006a), even when a noun is a true deverbal noun, not all of its modifiers are legitimate arguments or adjuncts of this predicate. Only arguments that can co-occur with both the nominal and verbal forms of the predicate are considered in the NomBank annotation. This means that the judgment of arguments is semantic rather than syntactic. These facts may also partly explain the lower nominal SRL performance, especially the performance of argument identification. This can be illustrated by the statistics on the development data that 96% (40%) of verbal (nominal) predicates’ sisters are annotated as a</context>
</contexts>
<marker>Xue, 2006</marker>
<rawString>Nianwen Xue. 2006a. Annotating the PredicateArgument Structure of Chinese Nominalizations. In Proceedings of the LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Semantic Role Labeling of Nominalized Predicates in Chinese.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<contexts>
<context position="3268" citStr="Xue, 2006" startWordPosition="492" endWordPosition="493"> a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply in1280 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1280–1288, Sin</context>
<context position="5716" citStr="Xue, 2006" startWordPosition="857" endWordPosition="858">eIP ported research on automatic predicate recognition in Chinese nominal SRL. The rest of this paper is organized as follows: Section 2 introduces Chinese NomBank while the baseline nominal SRL system is described in Section 3 with traditional and nominal SRLspecific features. Then, the baseline nominal SRL system is improved by integrating useful features derived from verbal SRL (Section 4) and extended with automatic recognition of nominal predicates (Section 5). Section 6 gives experimental results and discussion. Finally, Section 7 concludes the paper. 2. Chinese NomBank Chinese NomBank (Xue, 2006a) adopts similar semantic framework as NomBank, and focuses on Chinese nominal predicates with their arguments in Chinese TreeBank. The semantic arguments include: 1) Core arguments: Arg0 to Arg5. Generally, Arg0 and Arg1 denotes the agent and the patient, respectively, while arguments from Arg2 to Arg5 are predicate-specific. 2) Adjunct arguments, which are universal to all predicates, e.g. ArgM-LOC for locative, and ArgM-TMP for temporal. Arg1/Rel2 VP PU NP Arg0/Rel2 VP 。 . NN NN PP Sup/Rel2 NP 中国 银行 P NP VV QP NP Bank of China 向 Arg0/Rel1 Rel1 Arg1/Rel1 提供 CD ArgM-MNR/Rel2 Rel2 to NN NN NN</context>
<context position="7242" citStr="Xue, 2006" startWordPosition="1117" endWordPosition="1118"> spans of tree nodes. Figure 1 gives an example with two nominal predicates and their respective arguments, while the nominal predicate “投资 /investment” has two core arguments, “NN(外商 /foreign businessman)” as Arg0 and “NN(银行 /bank)” as Arg1, and the other nominal predicate “ 贷 款 /loan” also has two core arguments, “NP(中国银行/Bank of China)” as Arg1 and 1281 “PP( 向 外 商 投 资 银 行 /to Foreign Investment Bank)” as Arg0, and 1 adjunct argument, “NN(人民币/RMB)” as ArgM-MNR, denoting the manner of loan. It is worth noticing that there is a (Chinese) NomBank-specific label in Figure 1, Sup (support verb) (Xue, 2006a), in helping introduce the arguments, which occur outside the nominal predicate-headed noun phrase. This is illustrated by the nominal predicate “贷款/loan”, whose Arg0 and Arg1 are both realized outside the nominal predicate-headed noun phrase, NP(四十亿人民币贷款/4 billion RMB loan). Normally, a verb is marked as a support verb only when it shares some arguments with the nominal predicate. 3. Baseline: Chinese Nominal SRL Popular SRL systems usually formulate SRL as a classification problem, which annotates each constituent in a parse tree with a semantic role label or with the non-argument label NU</context>
<context position="17295" citStr="Xue (2006" startWordPosition="2696" endWordPosition="2697">te, either a verbal or nominal predicate. For example, in the frame file of predicate “贷款/loan”, the borrower is always labeled with Arg0 and the lender labeled with Arg1. This can be demonstrated by the following two sentences: “贷款/loan” is annotated as a nominal and a verbal predicate in S1 and S2, respectively. S1 [Arg1 中国银行/Bank of China] [Arg0 向外商 投资银行/to Foreign Investment Bank] 提供 /provide [Rel 贷款/loan] S2 [Arg0 中国银行/Bank of China] [Arg1 向外商 投资银行/from Foreign Investment Bank] [Rel 贷款/loan] Therefore, it is straightforward to augment nominal training instances with verbal ones. However, Xue (2006b) found that simply adding the training instances for verbal SRL to the training data for nominal SRL and indiscriminately extracting the same features in both verbal and nominal SRLs hurt the performance. This may be due to that certain features (e.g. the path feature) are much different for verbal and nominal SRLs. This can be illustrated in sentences S1 and S2: the verbal instances in S2 are negative for semantic role labeling of the nominal predicate “贷款/loan” in S1, since “中国银 行/Bank of China” takes opposite roles in S1 and S2. So does “向外商投资银行/(from/to) Foreign Investment Bank”. Althoug</context>
<context position="20349" citStr="Xue, 2006" startWordPosition="3233" endWordPosition="3234">es or No ao8 The semantic role of NP for V. Combined features: p1-p7 p1: ao1&amp;ao5; p2: ao1&amp;ao6; p3: ao1&amp;ao5&amp;b1; p4: ao1&amp;ao6&amp;b1; p5: ao1&amp;apo7; p6: ao1&amp;ao8; p7: ao5&amp;ao7. Table 4: Features derived from verbal SRL. 5. Automatic Predicate Recognition Unlike Chinese PropBank where almost all the verbs are annotated as predicates, Chinese NomBank only marks those nouns having arguments as predicates. Statistics on Chinese NomBank show that only 17.5% of nouns are marked as predicates. It is possible that a noun is a predi1284 cate in some cases but not in others. Previous Chinese nominal SRL systems (Xue, 2006b; Xue, 2008) assume that nominal predicates have already been manually annotated and thus are available. To our best knowledge, there is no report on addressing automatic recognition of nominal predicates on Chinese nominal SRL. Automatic recognition of nominal predicates can be cast as a binary classification (e.g., Predicate vs. Non-Predicate) problem. This paper employs the convolution tree kernel, as proposed in Collins and Duffy (2001), on automatic recognition of nominal predicates. Given the convolution tree kernel, the key problem is how to extract a parse tree structure from the pars</context>
<context position="31614" citStr="Xue 2006" startWordPosition="5055" endWordPosition="5056">automatic predicates In order to have a clear performance comparison among nominal SRL on golden/automatic parse trees and golden/automatic predicates, Table 10 lists all the results in those scenarios. 6.5. Comparison Chinese nominal SRL vs. Chinese verbal SRL Comparison with Xue (2008) shows that the performance of Chinese nominal SRL is about 20 lower (e.g. 72.67 vs. 92.38 in F1-measure) than that of Chinese verbal SRL, partly due to the smaller amount of annotated data (about 1/5) in Chinese NomBank than that in Chinese PropBank. Moreover, according to Chinese NomBank annotation criteria (Xue 2006a), even when a noun is a true deverbal noun, not all of its modifiers are legitimate arguments or adjuncts of this predicate. Only arguments that can co-occur with both the nominal and verbal forms of the predicate are considered in the NomBank annotation. This means that the judgment of arguments is semantic rather than syntactic. These facts may also partly explain the lower nominal SRL performance, especially the performance of argument identification. This can be illustrated by the statistics on the development data that 96% (40%) of verbal (nominal) predicates’ sisters are annotated as a</context>
</contexts>
<marker>Xue, 2006</marker>
<rawString>Nianwen Xue. 2006b. Semantic Role Labeling of Nominalized Predicates in Chinese. In Proceedings of HLT-NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Labeling Chinese Predicates with Semantic Roles.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<pages>34--2</pages>
<contexts>
<context position="3338" citStr="Xue, 2008" startWordPosition="504" endWordPosition="505">arious nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply in1280 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1280–1288, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP cluding the Chinese Prop</context>
<context position="19376" citStr="Xue (2008)" startWordPosition="3066" endWordPosition="3067">a NP is the agent of support verb “提供/provide” as well as VP phrase (“提供四十 亿人民币 贷款/provide 4 billion RMB loan”), we can infer that the NP is the lender of the nominal predicate “贷款/loan” independently on any other information, such as the NP content and the path from the NP to the nominal predicate “贷款/loan”. Let C be the focus constituent, V be the intervening verb, and NP be the highest NP headed by the nominal predicate. Table 4 shows the features (ao5-ao8, p1-p7) derived from verbal SRL. In this paper, we develop a state-of-the-art Chinese verbal SRL system, similar to the one as shown in Xue (2008), to achieve the goal. Based on golden parse trees on Chinese PropBank, our Chinese verbal SRL system achieves the performance of 92.38 in F1-measure, comparable to Xue (2008) which achieved the performance of 92.0 in F1-measure. Feature Remarks ao5 Whether C is an argument for V. Yes or No ao6 The semantic role of C for V. ao7 Whether NP is an argument for V. Yes or No ao8 The semantic role of NP for V. Combined features: p1-p7 p1: ao1&amp;ao5; p2: ao1&amp;ao6; p3: ao1&amp;ao5&amp;b1; p4: ao1&amp;ao6&amp;b1; p5: ao1&amp;apo7; p6: ao1&amp;ao8; p7: ao5&amp;ao7. Table 4: Features derived from verbal SRL. 5. Automatic Predicate Rec</context>
<context position="23572" citStr="Xue (2008)" startWordPosition="3773" endWordPosition="3774"> is ever tagged as a verb in the training data? Yes or No. Berkeley Parser. http://code.google.com/p/berkeleyparser/ POSs are not counted in evaluating the performance of word-based syntactic parser, but they are counted in evaluating the performance of character-based parser. Therefore the F1-measure for the later is higher than that for the former. SVM-LIGHT-TK. 1 2 3 http://dit.unitn.it/~moschitt/ 6.1. Experimental Settings This version of Chinese NomBank consists of standoff annotations on the files (chtb 001 to 1151.fid) of Chinese Penn TreeBank 5.1. Following the experimental setting in Xue (2008), 648 files (chtb _081 to 899.fid) are selected as the training data, 72 files (chtb 001 to 040.fid and chtb _900 to 931.fid) are held out as the test data, and 40 files (chtb 041 to 080.fid) as the development data, with 8642, 1124, and 731 propositions, respectively. As Chinese words are not naturally segmented in raw sentences, two Chinese automatic parsers are constructed: word-based parser (assuming golden word segmentation) and character-based parser (with automatic word segmentation). Here, Berkeley parser (Petrov and Klein, 2007)1 is chosen as the Chinese automatic parser. With regard </context>
<context position="26720" citStr="Xue (2008)" startWordPosition="4267" endWordPosition="4268">62 (+0.99) +ao8 68.30 75.39 71.67 (+1.04) +p1 67.91 74.40 71.00 (+0.37) +p2 67.76 74.20 70.83 (+0.20) +p3 67.96 74.69 71.16 (+0.53) +p4 68.01 74.18 70.96 (+0.33) +p5 68.01 75.01 71.39 (+0.76) +p6 68.20 75.12 71.49 (+0.86) +p7 68.40 75.70 71.87 (+1.24) Table 6: Effect of features derived from verbal SRL on the performance of nominal SRL on the test data with golden parse trees and golden nominal predicates. The first row presents the performance using traditional and nominal SRL-specific features. Rec.(%) Pre.(%) F1 baseline 67.86 73.63 70.63 +features derived 68.40 77.51 72.67 from verbal SRL Xue (2008) 66.1 73.4 69.6 Table 7: The performance of nominal SRL on the test data with golden parse trees and golden nominal predicates Table 6 shows the effect of features derived from verbal SRL in an incremental way. It shows that only the feature ao6 has negative effect due to its strong relevance with intervening verbs and thus not included thereafter. Table 7 shows the performance on the test data with or without using the features derived from the verbal SRL system. It shows these features significantly improve the performance ( X 2 p &lt; ; 0.05) on nominal SRL. Table 7 also shows our system outpe</context>
<context position="28272" citStr="Xue (2008)" startWordPosition="4528" endWordPosition="4529">trees, those arguments, which do not align with any syntactic constituents, are simply discarded. Moreover, for any nominal predicate segmented incorrectly by the word segmenter, all its arguments are unable to be labeled neither. Table 8 presents the SRL performance on the test data by using automatic parse trees. It shows that the performance drops from 72.67 to 60.87 in F1-measure when replacing golden parse trees with word-based automatic ones, partly due to the absence of 6.9% arguments in automatic trees, and wrong POS tagging of nominal predicates. Table 8 also compares our system with Xue (2008). It shows that our system also outperforms Xue (2008) on Chinese NomBank. Rec. (%) Pre. (%) F1 This paper 56.95(53.55) 66.74(66.69) 60.87(59.40) Xue (2008) 53.1 (52.9) 62.9 (62.3) 57.6 (57.3) Table 8: The performance of nominal SRL on the test data with automatic parse trees and golden predicates. Here, the numbers outside the parentheses indicate the performance using a word-based parser, while the numbers inside indicate the performance using a character-based parser4. 4 About 1.6% nominal predicates are mistakenly segmented by the character-based parser, thus their arguments are missed dir</context>
<context position="31294" citStr="Xue (2008)" startWordPosition="5000" endWordPosition="5001">e.(%) F1 golden golden 68.40 77.51 72.67 automatic 65.07 74.65 69.53 word- golden 55.95 66.74 60.87 based automatic 52.67 59.56 55.90 character- golden 53.55 66.69 59.40 based automatic 50.66 59.60 54.77 Table 10: The performance of nominal SRL on the test data with the choices of golden/automatic parse trees and golden/automatic predicates In order to have a clear performance comparison among nominal SRL on golden/automatic parse trees and golden/automatic predicates, Table 10 lists all the results in those scenarios. 6.5. Comparison Chinese nominal SRL vs. Chinese verbal SRL Comparison with Xue (2008) shows that the performance of Chinese nominal SRL is about 20 lower (e.g. 72.67 vs. 92.38 in F1-measure) than that of Chinese verbal SRL, partly due to the smaller amount of annotated data (about 1/5) in Chinese NomBank than that in Chinese PropBank. Moreover, according to Chinese NomBank annotation criteria (Xue 2006a), even when a noun is a true deverbal noun, not all of its modifiers are legitimate arguments or adjuncts of this predicate. Only arguments that can co-occur with both the nominal and verbal forms of the predicate are considered in the NomBank annotation. This means that the ju</context>
</contexts>
<marker>Xue, 2008</marker>
<rawString>Nianwen Xue. 2008. Labeling Chinese Predicates with Semantic Roles. Computational Linguistics, 34(2):225-255.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>