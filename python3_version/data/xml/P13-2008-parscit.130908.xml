<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000227">
<title confidence="0.99437">
Paraphrasing Adaptation for Web Search Ranking
</title>
<author confidence="0.993378">
Chenguang Wang∗
</author>
<affiliation confidence="0.9826265">
School of EECS
Peking University
</affiliation>
<email confidence="0.977182">
wangchenguang@pku.edu.cn
</email>
<author confidence="0.982693">
Ming Zhou
</author>
<affiliation confidence="0.978968">
Microsoft Research Asia
</affiliation>
<email confidence="0.997117">
mingzhou@microsoft.com
</email>
<sectionHeader confidence="0.993857" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999365">
Mismatch between queries and documents
is a key issue for the web search task. In
order to narrow down such mismatch, in
this paper, we present an in-depth inves-
tigation on adapting a paraphrasing tech-
nique to web search from three aspect-
s: a search-oriented paraphrasing mod-
el; an NDCG-based parameter optimiza-
tion algorithm; an enhanced ranking mod-
el leveraging augmented features comput-
ed on paraphrases of original queries. Ex-
periments performed on the large scale
query-document data set show that, the
search performance can be significantly
improved, with +3.28% and +1.14% ND-
CG gains on dev and test sets respectively.
</bodyText>
<sectionHeader confidence="0.998981" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999736631578947">
Paraphrasing is an NLP technique that generates
alternative expressions to convey the same mean-
ing of the input text in different ways. Researcher-
s have made great efforts to improve paraphrasing
from different perspectives, such as paraphrase ex-
traction (Zhao et al., 2007), paraphrase generation
(Quirk et al., 2004), model optimization (Zhao et
al., 2009) and etc. But as far as we know, none of
previous work has explored the impact of using a
well designed paraphrasing engine for web search
ranking task specifically.
In web search, mismatches between queries and
their relevant documents are usually caused by ex-
pressing the same meaning in different natural lan-
guage ways. E.g., X is the author of Y and Y was
written by X have identical meaning in most cas-
es, but they are quite different in literal sense. The
capability of paraphrasing is just right to alleviate
such issues. Motivated by this, this paper presents
</bodyText>
<footnote confidence="0.940326">
∗ This work has been done while the author was visiting
Microsoft Research Asia.
</footnote>
<author confidence="0.89118">
Nan Duan
</author>
<affiliation confidence="0.882675">
Microsoft Research Asia
</affiliation>
<email confidence="0.92361">
nanduan@microsoft.com
</email>
<author confidence="0.980768">
Ming Zhang
</author>
<affiliation confidence="0.9946115">
School of EECS
Peking University
</affiliation>
<email confidence="0.981391">
mzhang@net.pku.edu.cn
</email>
<bodyText confidence="0.99991">
an in-depth study on adapting paraphrasing to web
search. First, we propose a search-oriented para-
phrasing model, which includes specifically de-
signed features for web queries that can enable a
paraphrasing engine to learn preferences on dif-
ferent paraphrasing strategies. Second, we opti-
mize the parameters of the paraphrasing model ac-
cording to the Normalized Discounted Cumulative
Gain (NDCG) score, by leveraging the minimum
error rate training (MERT) algorithm (Och, 2003).
Third, we propose an enhanced ranking model by
using augmented features computed on paraphras-
es of original queries.
Many query reformulation approaches have
been proposed to tackle the query-document mis-
match issue, which can be generally summarized
as query expansion and query substitution. Query
expansion (Baeza-Yates, 1992; Jing and Croft,
1994; Lavrenko and Croft, 2001; Cui et al., 2002;
Yu et al., 2003; Zhang and Yu, 2006; Craswell and
Szummer, 2007; Elsas et al., 2008; Xu et al., 2009)
adds new terms extracted from different sources to
the original query directly; while query substitu-
tion (Brill and Moore, 2000; Jones et al., 2006;
Guo et al., 2008; Wang and Zhai, 2008; Dang
and Croft, 2010) uses probabilistic models, such
as graphical models, to predict the sequence of
rewritten query words to form a new query. Com-
paring to these works, our paraphrasing engine al-
ters queries in a similar way to statistical machine
translation, with systematic tuning and decoding
components. Zhao et al. (2009) proposes an uni-
fied paraphrasing framework that can be adapted
to different applications using different usability
models. Our work can be seen as an extension a-
long this line of research, by carrying out in-depth
study on adapting paraphrasing to web search.
Experiments performed on the large scale data
set show that, by leveraging additional matching
features computed on query paraphrases, signif-
icant NDCG gains can be achieved on both dev
</bodyText>
<page confidence="0.992145">
41
</page>
<bodyText confidence="0.234581666666667">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 41–46,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
(+3.28%) and test (+1.14%) sets.
</bodyText>
<sectionHeader confidence="0.902127" genericHeader="method">
2 Paraphrasing for Web Search
</sectionHeader>
<bodyText confidence="0.999733333333333">
In this section, we first summarize our paraphrase
extraction approaches, and then describe our para-
phrasing engine for the web search task from three
aspects, including: 1) a search-oriented paraphras-
ing model; 2) an NDCG-based parameter opti-
mization algorithm; 3) an enhanced ranking model
with augmented features that are computed based
on the extra knowledge provided by the paraphrase
candidates of the original queries.
</bodyText>
<subsectionHeader confidence="0.998251">
2.1 Paraphrase Extraction
</subsectionHeader>
<bodyText confidence="0.99952175">
Paraphrases can be mined from various resources.
Given a bilingual corpus, we use Bannard and
Callison-Burch (2005)’s pivot-based approach to
extract paraphrases. Given a monolingual cor-
pus, Lin and Pantel (2001)’s method is used to ex-
tract paraphrases based on distributional hypoth-
esis. Additionally, human annotated data can al-
so be used as high-quality paraphrases. We use
Miller (1995)’s approach to extract paraphrases
from the synonym dictionary of WordNet. Word
alignments within each paraphrase pair are gener-
ated using GIZA++ (Och and Ney, 2000).
</bodyText>
<subsectionHeader confidence="0.998833">
2.2 Search-Oriented Paraphrasing Model
</subsectionHeader>
<bodyText confidence="0.99972075">
Similar to statistical machine translation (SMT),
given an input query Q, our paraphrasing engine
generates paraphrase candidates1 based on a linear
model.
</bodyText>
<equation confidence="0.99834025">
Qˆ = arg max
Q&apos;E?i(Q)
= arg max
Q&apos;E?i(Q)
</equation>
<bodyText confidence="0.999902222222222">
W(Q) is the hypothesis space containing all para-
phrase candidates of Q, hm is the mth feature
function with weight am, Q&apos; denotes one candi-
date. In order to enable our paraphrasing model
to learn the preferences on different paraphrasing
strategies according to the characteristics of web
queries, we design search-oriented features2 based
on word alignments within Q and Q&apos;, which can
be described as follows:
</bodyText>
<footnote confidence="0.984939">
1We apply CYK algorithm (Chappelier and Rajman,
1998), which is most commonly used in SMT (Chiang,
2005), to generating paraphrase candidates.
2Similar features have been demonstrated effective in
(Jones et al., 2006). But we use SMT-like model to gener-
ate query reformulations.
</footnote>
<listItem confidence="0.992453956521739">
• Word Addition feature hWADD(Q, Q&apos;),
which is defined as the number of words in
the paraphrase candidate Q&apos; without being
aligned to any word in the original query Q.
• Word Deletion feature hWDEL(Q, Q&apos;),
which is defined as the number of words in
the original query Q without being aligned
to any word in the paraphrase candidate Q&apos;.
• Word Overlap feature hWO(Q, Q&apos;), which is
defined as the number of word pairs that align
identical words between Q and Q&apos;.
• Word Alteration feature hWA(Q, Q&apos;), which
is defined as the number of word pairs that
align different words between Q and Q&apos;.
• Word Reorder feature hWR(Q, Q&apos;), which is
modeled by a relative distortion probability
distribution, similar to the distortion model in
(Koehn et al., 2003).
• Length Difference feature hLD(Q, Q&apos;),
which is defined as |Q&apos; |− |Q|.
• Edit Distance feature hED(Q, Q&apos;), which is
defined as the character-level edit distance
between Q and Q&apos;.
</listItem>
<bodyText confidence="0.9992732">
Besides, a set of traditional SMT features
(Koehn et al., 2003) are also used in our paraphras-
ing model, including translation probability, lex-
ical weight, word count, paraphrase rule count3,
and language model feature.
</bodyText>
<subsectionHeader confidence="0.995336">
2.3 NDCG-based Parameter Optimization
</subsectionHeader>
<bodyText confidence="0.996637642857143">
We utilize minimum error rate training (MERT)
(Och, 2003) to optimize feature weights of the
paraphrasing model according to NDCG. We de-
fine D as the entire document set. R is a rank-
ing model4 that can rank documents in D based
on each input query. {Qi, DLabel
i }Si=1 is a human-
labeled development set. Qi is the ith query and
DLabel C D is a subset of documents, in which
i
the relevance between Qi and each document is
labeled by human annotators.
MERT is used to optimize feature weights
of our linear-formed paraphrasing model. For
</bodyText>
<footnote confidence="0.97606125">
3Paraphrase rule count is the number of rules that are used
to generate paraphrase candidates.
4The ranking model R (Liu et al., 2007) uses matching
features computed based on original queries and documents.
</footnote>
<equation confidence="0.98944425">
P(Q&apos;|Q)
M
amhm(Q, Q&apos;)
m=1
</equation>
<page confidence="0.953009">
42
</page>
<bodyText confidence="0.984608379310345">
each query Qi in {Qi}Si=1, we first generate N-
best paraphrase candidates {Qji }N j=1, and com-
pute NDCG score for each paraphrase based on
documents ranked by the ranker R and labeled
documents DLabel
i . We then optimize the feature
weights according to the following criterion:
Err(DLabel
i , ˆQi; λM1 , R)}
The objective of MERT is to find the optimal fea-
ture weight vector ˆλM1 that minimizes the error cri-
terion Err according to the NDCG scores of top-1
paraphrase candidates.
The error function Err is defined as:
Err(DLabel
i , ˆQi; λM1 , R) = 1 − N(DLabel i, ˆQi, R)
where ˆQi is the best paraphrase candidate accord-
ing to the paraphrasing model based on the weight
vector λM1 , N(DLabel
i ,ˆQi, R) is the NDCG score
of ˆQi computed on the documents ranked by R of
ˆQi and labeled document set DLabel of Qi. The
i
relevance rating labeled by human annotators can
be represented by five levels: “Perfect”, “Excel-
lent”, “Good”, “Fair”, and “Bad”. When comput-
ing NDCG scores, these five levels are commonly
mapped to the numerical scores 31, 15, 7, 3, 0 re-
spectively.
</bodyText>
<subsectionHeader confidence="0.899492">
2.4 Enhanced Ranking Model
</subsectionHeader>
<bodyText confidence="0.999618875">
In web search, the key objective of the ranking
model is to rank the retrieved documents based on
their relevance to a given query.
Given a query Q and its retrieved document set
D = {DQ}, for each DQ ∈ D, we use the fol-
lowing ranking model to compute their relevance,
which is formulated as a weighted combination of
matching features:
</bodyText>
<equation confidence="0.9624855">
K
R(Q, DQ) = λkFk(Q, DQ)
k=1
F = {F1,..., FK} denotes a set of matching fea-
</equation>
<bodyText confidence="0.8454125">
tures that measure the matching degrees between
Q and DQ, Fk(Q, DQ) ∈ F is the kth matching
feature, λk is its corresponding feature weight.
How to learn the weight vector {λk}Kk=1 is a s-
tandard learning-to-rank task. The goal of learning
ˆλk}Kk=1, such
that for any two documents DiQ ∈ D and DjQ ∈ D,
the following condition holds:
</bodyText>
<equation confidence="0.276593">
R(Q, DiQ) &gt; R(Q, DjQ) ⇔ rDiQ &gt; rDj
Q
</equation>
<bodyText confidence="0.99999415">
where rDQ denotes a numerical relevance rating
labeled by human annotators denoting the rele-
vance between Q and DQ.
As the ultimate goal of improving paraphrasing
is to help the search task, we present a straight-
forward but effective method to enhance the rank-
ing model R described above, by leveraging para-
phrase candidates of the original query as the extra
knowledge to compute matching features.
Formally, given a query Q and its N-best para-
phrase candidates {Q&apos;1, ..., Q&apos;N}, we enrich the o-
riginal feature vector F to {F, F1, ..., FN} for Q
and DQ, where all features in Fn have the same
meanings as they are in F, however, their feature
values are computed based on Q&apos; n and DQ, instead
of Q and DQ. In this way, the paraphrase candi-
dates act as hidden variables and expanded match-
ing features between queries and documents, mak-
ing our ranking model more tunable and flexible
for web search.
</bodyText>
<sectionHeader confidence="0.999759" genericHeader="method">
3 Experiment
</sectionHeader>
<subsectionHeader confidence="0.999708">
3.1 Data and Metric
</subsectionHeader>
<bodyText confidence="0.999991538461538">
Paraphrase pairs are extracted as we described in
Section 2.1. The bilingual corpus includes 5.1M
sentence pairs from the NIST 2008 constrained
track of Chinese-to-English machine translation
task. The monolingual corpus includes 16.7M
queries from the log of a commercial search en-
gine. Human annotated data contains 0.3M syn-
onym pairs from WordNet dictionary. Word align-
ments of each paraphrase pair are trained by
GIZA++. The language model is trained based
on a portion of queries, in which the frequency of
each query is higher than a predefined threshold,
5. The number of paraphrase pairs is 58M. The
minimum length of paraphrase rule is 1, while the
maximum length of paraphrase rule is 5.
We randomly select 2, 838 queries from the log
of a commercial search engine, each of which at-
tached with a set of documents that are annotat-
ed with relevance ratings described in Section 2.3.
We use the first 1, 419 queries together with their
annotated documents as the development set to
tune paraphrasing parameters (as we discussed in
Section 2.3), and use the rest as the test set. The
ranking model is trained based on the develop-
ment set. NDCG is used as the evaluation metric
of the web search task.
</bodyText>
<equation confidence="0.915567166666667">
ˆλM1 = arg min
λM
1
S
{
i=1
</equation>
<bodyText confidence="0.56067">
is to find an optimal weight vector {
</bodyText>
<page confidence="0.999255">
43
</page>
<subsectionHeader confidence="0.997281">
3.2 Baseline Systems
</subsectionHeader>
<bodyText confidence="0.999974761904762">
The baselines of the paraphrasing and the ranking
model are described as follows:
The paraphrasing baseline is denoted as BL-
Para, which only uses traditional SMT features
described at the end of Section 2.2. Weights are
optimized by MERT using BLEU (Papineni et al.,
2002) as the error criterion. Development data are
generated based on the English references of NIST
2008 constrained track of Chinese-to-English ma-
chine translation task. We use the first reference
as the source, and the rest as its paraphrases.
The ranking model baseline (Liu et al., 2007) is
denoted as BL-Rank, which only uses matching
features computed based on original queries and
different meta-streams of web pages, including
URL, page title, page body, meta-keywords, meta-
description and anchor texts. The feature function-
s we use include unigram/bigram/trigram BM25
and original/normalized Perfect-Match. The rank-
ing model is learned based on SV Mrank toolkit
(Joachims, 2006) with default parameter setting.
</bodyText>
<subsectionHeader confidence="0.999321">
3.3 Impacts of Search-Oriented Features
</subsectionHeader>
<bodyText confidence="0.999924384615385">
We first evaluate the effectiveness of the search-
oriented features. To do so, we add these features
into the paraphrasing model baseline, and denote it
as BL-Para+SF, whose weights are optimized in
the same way with BL-Para. The ranking model
baseline BL-Rank is used to rank the documents.
We then compare the NDCG@1 scores of the best
documents retrieved using either original query, or
query paraphrases generated by BL-Para and BL-
Para+SF respectively, and list comparison results
in Table 1, where Cand@1 denotes the best para-
phrase candidate generated by each paraphrasing
model.
</bodyText>
<table confidence="0.99893725">
Test Set
BL-Para BL-Para+SF
Original Query Cand@1 Cand@1
27.28% 26.44% 26.53%
</table>
<tableCaption confidence="0.999773">
Table 1: Impacts of search-oriented features.
</tableCaption>
<bodyText confidence="0.9998082">
From Table 1, we can see, even using the best
query paraphrase, its corresponding NDCG score
is still lower than the NDCG score of the original
query. This performance dropping makes sense,
as changing user queries brings the risks of query
drift. When adding search-oriented features in-
to the baseline, the performance changes little, as
these two models are optimized based on BLEU
score only, without considering characteristics of
mismatches in search.
</bodyText>
<subsectionHeader confidence="0.998216">
3.4 Impacts of Optimization Algorithm
</subsectionHeader>
<bodyText confidence="0.9669816">
We then evaluate the impact of our NDCG-based
optimization method. We add the optimization al-
gorithm described in Section 2.3 into BL-Para+SF,
and get a paraphrasing model BL-Para+SF+Opt.
The ranking model baseline BL-Rank is used.
Similar to the experiment in Table 1, we compare
the NDCG@1 scores of the best documents re-
trieved using query paraphrases generated by BL-
Para+SF and BL-Para+SF+Opt respectively, with
results shown in Table 2.
</bodyText>
<table confidence="0.99888175">
Test Set
BL-Para+SF BL-Para+SF+Opt
Original Query Cand@1 Cand@1
27.28% 26.53% 27.06%(+0.53%)
</table>
<tableCaption confidence="0.997679">
Table 2: Impacts of NDCG-based optimization.
</tableCaption>
<bodyText confidence="0.947097888888889">
Table 2 indicates that, by leveraging NDCG as
the error criterion for MERT, search-oriented fea-
tures benefit more (+0.53% NDCG) in selecting
the best query paraphrase from the whole para-
phrasing search space. The improvement is statis-
tically significant (p &lt; 0.001) by t-test (Smucker
et al., 2007). The quality of the top-1 paraphrase
generated by BL-Para+SF+Opt is very close to the
original query.
</bodyText>
<subsectionHeader confidence="0.995239">
3.5 Impacts of Enhanced Ranking Model
</subsectionHeader>
<bodyText confidence="0.996968222222222">
We last evaluate the effectiveness of the en-
hanced ranking model. The ranking model base-
line BL-Rank only uses original queries to com-
pute matching features between queries and docu-
ments; while the enhanced ranking model, denot-
ed as BL-Rank+Para, uses not only the original
query but also its top-1 paraphrase candidate gen-
erated by BL-Para+SF+Opt to compute augment-
ed matching features described in Section 2.4.
</bodyText>
<table confidence="0.99915075">
Dev Set
NDCG@1 NDCG@5
BL-Rank 25.31% 33.76%
BL-Rank+Para 28.59%(+3.28%) 34.25%(+0.49%)
Test Set
NDCG@1 NDCG@5
BL-Rank 27.28% 34.79%
BL-Rank+Para 28.42%(+1.14%) 35.68%(+0.89%)
</table>
<tableCaption confidence="0.999739">
Table 3: Impacts of enhanced ranking model.
</tableCaption>
<bodyText confidence="0.636816666666667">
From Table 3, we can see that NDCG@k (k =
1, 5) scores of BL-Rank+Para outperforms BL-
Rank on both dev and test sets. T-test shows that
</bodyText>
<page confidence="0.99784">
44
</page>
<bodyText confidence="0.999387166666667">
the improvement is statistically significant (p &lt;
0.001). Such end-to-end NDCG improvements
come from the extra knowledge provided by the
hidden paraphrases of original queries. This nar-
rows down the query-document mismatch issue to
a certain extent.
</bodyText>
<sectionHeader confidence="0.987634" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999783555555556">
In this paper, we present an in-depth study on us-
ing paraphrasing for web search, which pays close
attention to various aspects of the application in-
cluding choice of model and optimization tech-
nique. In the future, we will compare and com-
bine paraphrasing with other query reformulation
techniques, e.g., pseudo-relevance feedback (Yu et
al., 2003) and a conditional random field-based ap-
proach (Guo et al., 2008).
</bodyText>
<sectionHeader confidence="0.998561" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996618">
This work is supported by the National Natu-
ral Science Foundation of China (NSFC Grant
No. 61272343) as well as the Doctoral Program
of Higher Education of China (FSSP Grant No.
20120001110112).
</bodyText>
<sectionHeader confidence="0.997935" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9991095">
Ricardo A Baeza-Yates. 1992. Introduction to data
structures and algorithms related to information re-
trieval.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of ACL, pages 597–604.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of ACL, pages 286–293.
Jean-C´edric Chappelier and Martin Rajman. 1998. A
generalized cyk algorithm for parsing stochastic cfg.
In Workshop on Tabulation in Parsing and Deduc-
tion, pages 133–137.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, pages 263–270.
Nick Craswell and Martin Szummer. 2007. Random
walks on the click graph. In Proceedings of SIGIR,
SIGIR ’07, pages 239–246.
Hang Cui, Ji-Rong Wen, Jian-Yun Nie, and Wei-Ying
Ma. 2002. Probabilistic query expansion using
query logs. In Proceedings of WWW, pages 325–
332.
Van Dang and Bruce W. Croft. 2010. Query reformu-
lation using anchor text. In Proceedings of WSDM,
pages 41–50.
Jonathan L. Elsas, Jaime Arguello, Jamie Callan, and
Jaime G. Carbonell. 2008. Retrieval and feedback
models for blog feed search. In Proceedings of SI-
GIR, pages 347–354.
Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng. 2008.
A unified and discriminative model for query refine-
ment. In Proceedings of SIGIR, SIGIR ’08, pages
379–386.
Yufeng Jing and W. Bruce Croft. 1994. An association
thesaurus for information retrieval. In In RIAO 94
Conference Proceedings, pages 146–160.
Thorsten Joachims. 2006. Training linear svms in lin-
ear time. In Proceedings of KDD, pages 217–226.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proceedings of WWW, pages 387–396.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL, pages 48–54.
Victor Lavrenko and W. Bruce Croft. 2001. Relevance
based language models. In Proceedings of SIGIR,
pages 120–127.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question-answering. Natural Lan-
guage Engineering, pages 343–360.
Tie-Yan Liu, Jun Xu, Tao Qin, Wenying Xiong, and
Hang Li. 2007. Letor: Benchmark dataset for re-
search on learning to rank for information retrieval.
In Proceedings of SIGIR workshop, pages 3–10.
George A Miller. 1995. Wordnet: a lexical database
for english. Communications of the ACM, pages 39–
41.
Franz Josef Och and Hermann Ney. 2000. Improved s-
tatistical alignment models. In Proceedings of ACL,
pages 440–447.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic e-
valuation of machine translation. In Proceedings of
ACL, pages 311–318.
Chris Quirk, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proceedings of EMNLP, pages
142–149.
Mark D Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests
for information retrieval evaluation. In Proceedings
of CIKM, pages 623–632.
</reference>
<page confidence="0.984168">
45
</page>
<reference confidence="0.999721">
Xuanhui Wang and ChengXiang Zhai. 2008. Mining
term association patterns from search logs for ef-
fective query reformulation. In Proceedings of the
17th ACM conference on Information and knowl-
edge management, Proceedings of CIKM, pages
479–488.
Yang Xu, Gareth J.F. Jones, and Bin Wang. 2009.
Query dependent pseudo-relevance feedback based
on wikipedia. In Proceedings of SIGIR, pages 59–
66.
Shipeng Yu, Deng Cai, Ji-Rong Wen, and Wei-Ying
Ma. 2003. Improving pseudo-relevance feedback in
web information retrieval using web page segmenta-
tion. In Proceedings of WWW, pages 11–18.
Wei Zhang and Clement Yu. 2006. Uic at trec 2006
blog track. In Proceedings of TREC.
Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learning
question paraphrases for qa from encarta logs. In
Proceedings of IJCAI, pages 1795–1800.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of ACL, pages 834–842.
</reference>
<page confidence="0.999612">
46
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.723383">
<title confidence="0.984135">Paraphrasing Adaptation for Web Search Ranking</title>
<affiliation confidence="0.998048">School of EECS Peking University</affiliation>
<email confidence="0.818974">wangchenguang@pku.edu.cn</email>
<author confidence="0.998789">Ming Zhou</author>
<affiliation confidence="0.999667">Microsoft Research Asia</affiliation>
<email confidence="0.999728">mingzhou@microsoft.com</email>
<abstract confidence="0.993385117647059">Mismatch between queries and documents is a key issue for the web search task. In order to narrow down such mismatch, in this paper, we present an in-depth investigation on adapting a paraphrasing technique to web search from three aspects: a search-oriented paraphrasing model; an NDCG-based parameter optimization algorithm; an enhanced ranking model leveraging augmented features computed on paraphrases of original queries. Experiments performed on the large scale query-document data set show that, the search performance can be significantly with and ND- CG gains on dev and test sets respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ricardo A Baeza-Yates</author>
</authors>
<title>Introduction to data structures and algorithms related to information retrieval.</title>
<date>1992</date>
<contexts>
<context position="2764" citStr="Baeza-Yates, 1992" startWordPosition="422" endWordPosition="423">e a paraphrasing engine to learn preferences on different paraphrasing strategies. Second, we optimize the parameters of the paraphrasing model according to the Normalized Discounted Cumulative Gain (NDCG) score, by leveraging the minimum error rate training (MERT) algorithm (Och, 2003). Third, we propose an enhanced ranking model by using augmented features computed on paraphrases of original queries. Many query reformulation approaches have been proposed to tackle the query-document mismatch issue, which can be generally summarized as query expansion and query substitution. Query expansion (Baeza-Yates, 1992; Jing and Croft, 1994; Lavrenko and Croft, 2001; Cui et al., 2002; Yu et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitution (Brill and Moore, 2000; Jones et al., 2006; Guo et al., 2008; Wang and Zhai, 2008; Dang and Croft, 2010) uses probabilistic models, such as graphical models, to predict the sequence of rewritten query words to form a new query. Comparing to these works, our paraphrasing engine alters queries in a similar way to statistical </context>
</contexts>
<marker>Baeza-Yates, 1992</marker>
<rawString>Ricardo A Baeza-Yates. 1992. Introduction to data structures and algorithms related to information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>597--604</pages>
<contexts>
<context position="4716" citStr="Bannard and Callison-Burch (2005)" startWordPosition="727" endWordPosition="730">cs (+3.28%) and test (+1.14%) sets. 2 Paraphrasing for Web Search In this section, we first summarize our paraphrase extraction approaches, and then describe our paraphrasing engine for the web search task from three aspects, including: 1) a search-oriented paraphrasing model; 2) an NDCG-based parameter optimization algorithm; 3) an enhanced ranking model with augmented features that are computed based on the extra knowledge provided by the paraphrase candidates of the original queries. 2.1 Paraphrase Extraction Paraphrases can be mined from various resources. Given a bilingual corpus, we use Bannard and Callison-Burch (2005)’s pivot-based approach to extract paraphrases. Given a monolingual corpus, Lin and Pantel (2001)’s method is used to extract paraphrases based on distributional hypothesis. Additionally, human annotated data can also be used as high-quality paraphrases. We use Miller (1995)’s approach to extract paraphrases from the synonym dictionary of WordNet. Word alignments within each paraphrase pair are generated using GIZA++ (Och and Ney, 2000). 2.2 Search-Oriented Paraphrasing Model Similar to statistical machine translation (SMT), given an input query Q, our paraphrasing engine generates paraphrase </context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of ACL, pages 597–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling correction.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>286--293</pages>
<contexts>
<context position="3061" citStr="Brill and Moore, 2000" startWordPosition="472" endWordPosition="475">d, we propose an enhanced ranking model by using augmented features computed on paraphrases of original queries. Many query reformulation approaches have been proposed to tackle the query-document mismatch issue, which can be generally summarized as query expansion and query substitution. Query expansion (Baeza-Yates, 1992; Jing and Croft, 1994; Lavrenko and Croft, 2001; Cui et al., 2002; Yu et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitution (Brill and Moore, 2000; Jones et al., 2006; Guo et al., 2008; Wang and Zhai, 2008; Dang and Croft, 2010) uses probabilistic models, such as graphical models, to predict the sequence of rewritten query words to form a new query. Comparing to these works, our paraphrasing engine alters queries in a similar way to statistical machine translation, with systematic tuning and decoding components. Zhao et al. (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models. Our work can be seen as an extension along this line of research, by carrying out in-d</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Eric Brill and Robert C. Moore. 2000. An improved error model for noisy channel spelling correction. In Proceedings of ACL, pages 286–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-C´edric Chappelier</author>
<author>Martin Rajman</author>
</authors>
<title>A generalized cyk algorithm for parsing stochastic cfg.</title>
<date>1998</date>
<booktitle>In Workshop on Tabulation in Parsing and Deduction,</booktitle>
<pages>133--137</pages>
<contexts>
<context position="5858" citStr="Chappelier and Rajman, 1998" startWordPosition="903" endWordPosition="906">anslation (SMT), given an input query Q, our paraphrasing engine generates paraphrase candidates1 based on a linear model. Qˆ = arg max Q&apos;E?i(Q) = arg max Q&apos;E?i(Q) W(Q) is the hypothesis space containing all paraphrase candidates of Q, hm is the mth feature function with weight am, Q&apos; denotes one candidate. In order to enable our paraphrasing model to learn the preferences on different paraphrasing strategies according to the characteristics of web queries, we design search-oriented features2 based on word alignments within Q and Q&apos;, which can be described as follows: 1We apply CYK algorithm (Chappelier and Rajman, 1998), which is most commonly used in SMT (Chiang, 2005), to generating paraphrase candidates. 2Similar features have been demonstrated effective in (Jones et al., 2006). But we use SMT-like model to generate query reformulations. • Word Addition feature hWADD(Q, Q&apos;), which is defined as the number of words in the paraphrase candidate Q&apos; without being aligned to any word in the original query Q. • Word Deletion feature hWDEL(Q, Q&apos;), which is defined as the number of words in the original query Q without being aligned to any word in the paraphrase candidate Q&apos;. • Word Overlap feature hWO(Q, Q&apos;), whi</context>
</contexts>
<marker>Chappelier, Rajman, 1998</marker>
<rawString>Jean-C´edric Chappelier and Martin Rajman. 1998. A generalized cyk algorithm for parsing stochastic cfg. In Workshop on Tabulation in Parsing and Deduction, pages 133–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="5909" citStr="Chiang, 2005" startWordPosition="914" endWordPosition="915">enerates paraphrase candidates1 based on a linear model. Qˆ = arg max Q&apos;E?i(Q) = arg max Q&apos;E?i(Q) W(Q) is the hypothesis space containing all paraphrase candidates of Q, hm is the mth feature function with weight am, Q&apos; denotes one candidate. In order to enable our paraphrasing model to learn the preferences on different paraphrasing strategies according to the characteristics of web queries, we design search-oriented features2 based on word alignments within Q and Q&apos;, which can be described as follows: 1We apply CYK algorithm (Chappelier and Rajman, 1998), which is most commonly used in SMT (Chiang, 2005), to generating paraphrase candidates. 2Similar features have been demonstrated effective in (Jones et al., 2006). But we use SMT-like model to generate query reformulations. • Word Addition feature hWADD(Q, Q&apos;), which is defined as the number of words in the paraphrase candidate Q&apos; without being aligned to any word in the original query Q. • Word Deletion feature hWDEL(Q, Q&apos;), which is defined as the number of words in the original query Q without being aligned to any word in the paraphrase candidate Q&apos;. • Word Overlap feature hWO(Q, Q&apos;), which is defined as the number of word pairs that alig</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Craswell</author>
<author>Martin Szummer</author>
</authors>
<title>Random walks on the click graph.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR, SIGIR ’07,</booktitle>
<pages>239--246</pages>
<contexts>
<context position="2895" citStr="Craswell and Szummer, 2007" startWordPosition="444" endWordPosition="447">he paraphrasing model according to the Normalized Discounted Cumulative Gain (NDCG) score, by leveraging the minimum error rate training (MERT) algorithm (Och, 2003). Third, we propose an enhanced ranking model by using augmented features computed on paraphrases of original queries. Many query reformulation approaches have been proposed to tackle the query-document mismatch issue, which can be generally summarized as query expansion and query substitution. Query expansion (Baeza-Yates, 1992; Jing and Croft, 1994; Lavrenko and Croft, 2001; Cui et al., 2002; Yu et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitution (Brill and Moore, 2000; Jones et al., 2006; Guo et al., 2008; Wang and Zhai, 2008; Dang and Croft, 2010) uses probabilistic models, such as graphical models, to predict the sequence of rewritten query words to form a new query. Comparing to these works, our paraphrasing engine alters queries in a similar way to statistical machine translation, with systematic tuning and decoding components. Zhao et al. (2009) proposes an unified paraphrasing framework </context>
</contexts>
<marker>Craswell, Szummer, 2007</marker>
<rawString>Nick Craswell and Martin Szummer. 2007. Random walks on the click graph. In Proceedings of SIGIR, SIGIR ’07, pages 239–246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Ji-Rong Wen</author>
<author>Jian-Yun Nie</author>
<author>Wei-Ying Ma</author>
</authors>
<title>Probabilistic query expansion using query logs.</title>
<date>2002</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>325--332</pages>
<contexts>
<context position="2830" citStr="Cui et al., 2002" startWordPosition="432" endWordPosition="435">ing strategies. Second, we optimize the parameters of the paraphrasing model according to the Normalized Discounted Cumulative Gain (NDCG) score, by leveraging the minimum error rate training (MERT) algorithm (Och, 2003). Third, we propose an enhanced ranking model by using augmented features computed on paraphrases of original queries. Many query reformulation approaches have been proposed to tackle the query-document mismatch issue, which can be generally summarized as query expansion and query substitution. Query expansion (Baeza-Yates, 1992; Jing and Croft, 1994; Lavrenko and Croft, 2001; Cui et al., 2002; Yu et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitution (Brill and Moore, 2000; Jones et al., 2006; Guo et al., 2008; Wang and Zhai, 2008; Dang and Croft, 2010) uses probabilistic models, such as graphical models, to predict the sequence of rewritten query words to form a new query. Comparing to these works, our paraphrasing engine alters queries in a similar way to statistical machine translation, with systematic tuning and decoding component</context>
</contexts>
<marker>Cui, Wen, Nie, Ma, 2002</marker>
<rawString>Hang Cui, Ji-Rong Wen, Jian-Yun Nie, and Wei-Ying Ma. 2002. Probabilistic query expansion using query logs. In Proceedings of WWW, pages 325– 332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Van Dang</author>
<author>Bruce W Croft</author>
</authors>
<title>Query reformulation using anchor text.</title>
<date>2010</date>
<booktitle>In Proceedings of WSDM,</booktitle>
<pages>41--50</pages>
<marker>Van Dang, Croft, 2010</marker>
<rawString>Van Dang and Bruce W. Croft. 2010. Query reformulation using anchor text. In Proceedings of WSDM, pages 41–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan L Elsas</author>
<author>Jaime Arguello</author>
<author>Jamie Callan</author>
<author>Jaime G Carbonell</author>
</authors>
<title>Retrieval and feedback models for blog feed search.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="2915" citStr="Elsas et al., 2008" startWordPosition="448" endWordPosition="451">ing to the Normalized Discounted Cumulative Gain (NDCG) score, by leveraging the minimum error rate training (MERT) algorithm (Och, 2003). Third, we propose an enhanced ranking model by using augmented features computed on paraphrases of original queries. Many query reformulation approaches have been proposed to tackle the query-document mismatch issue, which can be generally summarized as query expansion and query substitution. Query expansion (Baeza-Yates, 1992; Jing and Croft, 1994; Lavrenko and Croft, 2001; Cui et al., 2002; Yu et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitution (Brill and Moore, 2000; Jones et al., 2006; Guo et al., 2008; Wang and Zhai, 2008; Dang and Croft, 2010) uses probabilistic models, such as graphical models, to predict the sequence of rewritten query words to form a new query. Comparing to these works, our paraphrasing engine alters queries in a similar way to statistical machine translation, with systematic tuning and decoding components. Zhao et al. (2009) proposes an unified paraphrasing framework that can be adapted </context>
</contexts>
<marker>Elsas, Arguello, Callan, Carbonell, 2008</marker>
<rawString>Jonathan L. Elsas, Jaime Arguello, Jamie Callan, and Jaime G. Carbonell. 2008. Retrieval and feedback models for blog feed search. In Proceedings of SIGIR, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiafeng Guo</author>
<author>Gu Xu</author>
<author>Hang Li</author>
<author>Xueqi Cheng</author>
</authors>
<title>A unified and discriminative model for query refinement.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR, SIGIR ’08,</booktitle>
<pages>379--386</pages>
<contexts>
<context position="3099" citStr="Guo et al., 2008" startWordPosition="480" endWordPosition="483">using augmented features computed on paraphrases of original queries. Many query reformulation approaches have been proposed to tackle the query-document mismatch issue, which can be generally summarized as query expansion and query substitution. Query expansion (Baeza-Yates, 1992; Jing and Croft, 1994; Lavrenko and Croft, 2001; Cui et al., 2002; Yu et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitution (Brill and Moore, 2000; Jones et al., 2006; Guo et al., 2008; Wang and Zhai, 2008; Dang and Croft, 2010) uses probabilistic models, such as graphical models, to predict the sequence of rewritten query words to form a new query. Comparing to these works, our paraphrasing engine alters queries in a similar way to statistical machine translation, with systematic tuning and decoding components. Zhao et al. (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models. Our work can be seen as an extension along this line of research, by carrying out in-depth study on adapting paraphrasing to</context>
</contexts>
<marker>Guo, Xu, Li, Cheng, 2008</marker>
<rawString>Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng. 2008. A unified and discriminative model for query refinement. In Proceedings of SIGIR, SIGIR ’08, pages 379–386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yufeng Jing</author>
<author>W Bruce Croft</author>
</authors>
<title>An association thesaurus for information retrieval.</title>
<date>1994</date>
<booktitle>In In RIAO 94 Conference Proceedings,</booktitle>
<pages>146--160</pages>
<contexts>
<context position="2786" citStr="Jing and Croft, 1994" startWordPosition="424" endWordPosition="427">gine to learn preferences on different paraphrasing strategies. Second, we optimize the parameters of the paraphrasing model according to the Normalized Discounted Cumulative Gain (NDCG) score, by leveraging the minimum error rate training (MERT) algorithm (Och, 2003). Third, we propose an enhanced ranking model by using augmented features computed on paraphrases of original queries. Many query reformulation approaches have been proposed to tackle the query-document mismatch issue, which can be generally summarized as query expansion and query substitution. Query expansion (Baeza-Yates, 1992; Jing and Croft, 1994; Lavrenko and Croft, 2001; Cui et al., 2002; Yu et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitution (Brill and Moore, 2000; Jones et al., 2006; Guo et al., 2008; Wang and Zhai, 2008; Dang and Croft, 2010) uses probabilistic models, such as graphical models, to predict the sequence of rewritten query words to form a new query. Comparing to these works, our paraphrasing engine alters queries in a similar way to statistical machine translation, w</context>
</contexts>
<marker>Jing, Croft, 1994</marker>
<rawString>Yufeng Jing and W. Bruce Croft. 1994. An association thesaurus for information retrieval. In In RIAO 94 Conference Proceedings, pages 146–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear svms in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of KDD,</booktitle>
<pages>217--226</pages>
<contexts>
<context position="13109" citStr="Joachims, 2006" startWordPosition="2165" endWordPosition="2166"> English references of NIST 2008 constrained track of Chinese-to-English machine translation task. We use the first reference as the source, and the rest as its paraphrases. The ranking model baseline (Liu et al., 2007) is denoted as BL-Rank, which only uses matching features computed based on original queries and different meta-streams of web pages, including URL, page title, page body, meta-keywords, metadescription and anchor texts. The feature functions we use include unigram/bigram/trigram BM25 and original/normalized Perfect-Match. The ranking model is learned based on SV Mrank toolkit (Joachims, 2006) with default parameter setting. 3.3 Impacts of Search-Oriented Features We first evaluate the effectiveness of the searchoriented features. To do so, we add these features into the paraphrasing model baseline, and denote it as BL-Para+SF, whose weights are optimized in the same way with BL-Para. The ranking model baseline BL-Rank is used to rank the documents. We then compare the NDCG@1 scores of the best documents retrieved using either original query, or query paraphrases generated by BL-Para and BLPara+SF respectively, and list comparison results in Table 1, where Cand@1 denotes the best p</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training linear svms in linear time. In Proceedings of KDD, pages 217–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
<author>Benjamin Rey</author>
<author>Omid Madani</author>
<author>Wiley Greiner</author>
</authors>
<title>Generating query substitutions.</title>
<date>2006</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>387--396</pages>
<contexts>
<context position="3081" citStr="Jones et al., 2006" startWordPosition="476" endWordPosition="479">ed ranking model by using augmented features computed on paraphrases of original queries. Many query reformulation approaches have been proposed to tackle the query-document mismatch issue, which can be generally summarized as query expansion and query substitution. Query expansion (Baeza-Yates, 1992; Jing and Croft, 1994; Lavrenko and Croft, 2001; Cui et al., 2002; Yu et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitution (Brill and Moore, 2000; Jones et al., 2006; Guo et al., 2008; Wang and Zhai, 2008; Dang and Croft, 2010) uses probabilistic models, such as graphical models, to predict the sequence of rewritten query words to form a new query. Comparing to these works, our paraphrasing engine alters queries in a similar way to statistical machine translation, with systematic tuning and decoding components. Zhao et al. (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models. Our work can be seen as an extension along this line of research, by carrying out in-depth study on adapti</context>
<context position="6022" citStr="Jones et al., 2006" startWordPosition="927" endWordPosition="930">he hypothesis space containing all paraphrase candidates of Q, hm is the mth feature function with weight am, Q&apos; denotes one candidate. In order to enable our paraphrasing model to learn the preferences on different paraphrasing strategies according to the characteristics of web queries, we design search-oriented features2 based on word alignments within Q and Q&apos;, which can be described as follows: 1We apply CYK algorithm (Chappelier and Rajman, 1998), which is most commonly used in SMT (Chiang, 2005), to generating paraphrase candidates. 2Similar features have been demonstrated effective in (Jones et al., 2006). But we use SMT-like model to generate query reformulations. • Word Addition feature hWADD(Q, Q&apos;), which is defined as the number of words in the paraphrase candidate Q&apos; without being aligned to any word in the original query Q. • Word Deletion feature hWDEL(Q, Q&apos;), which is defined as the number of words in the original query Q without being aligned to any word in the paraphrase candidate Q&apos;. • Word Overlap feature hWO(Q, Q&apos;), which is defined as the number of word pairs that align identical words between Q and Q&apos;. • Word Alteration feature hWA(Q, Q&apos;), which is defined as the number of word </context>
</contexts>
<marker>Jones, Rey, Madani, Greiner, 2006</marker>
<rawString>Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006. Generating query substitutions. In Proceedings of WWW, pages 387–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="6831" citStr="Koehn et al., 2003" startWordPosition="1068" endWordPosition="1071">ed to any word in the original query Q. • Word Deletion feature hWDEL(Q, Q&apos;), which is defined as the number of words in the original query Q without being aligned to any word in the paraphrase candidate Q&apos;. • Word Overlap feature hWO(Q, Q&apos;), which is defined as the number of word pairs that align identical words between Q and Q&apos;. • Word Alteration feature hWA(Q, Q&apos;), which is defined as the number of word pairs that align different words between Q and Q&apos;. • Word Reorder feature hWR(Q, Q&apos;), which is modeled by a relative distortion probability distribution, similar to the distortion model in (Koehn et al., 2003). • Length Difference feature hLD(Q, Q&apos;), which is defined as |Q&apos; |− |Q|. • Edit Distance feature hED(Q, Q&apos;), which is defined as the character-level edit distance between Q and Q&apos;. Besides, a set of traditional SMT features (Koehn et al., 2003) are also used in our paraphrasing model, including translation probability, lexical weight, word count, paraphrase rule count3, and language model feature. 2.3 NDCG-based Parameter Optimization We utilize minimum error rate training (MERT) (Och, 2003) to optimize feature weights of the paraphrasing model according to NDCG. We define D as the entire doc</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>W Bruce Croft</author>
</authors>
<title>Relevance based language models.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="2812" citStr="Lavrenko and Croft, 2001" startWordPosition="428" endWordPosition="431">ces on different paraphrasing strategies. Second, we optimize the parameters of the paraphrasing model according to the Normalized Discounted Cumulative Gain (NDCG) score, by leveraging the minimum error rate training (MERT) algorithm (Och, 2003). Third, we propose an enhanced ranking model by using augmented features computed on paraphrases of original queries. Many query reformulation approaches have been proposed to tackle the query-document mismatch issue, which can be generally summarized as query expansion and query substitution. Query expansion (Baeza-Yates, 1992; Jing and Croft, 1994; Lavrenko and Croft, 2001; Cui et al., 2002; Yu et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitution (Brill and Moore, 2000; Jones et al., 2006; Guo et al., 2008; Wang and Zhai, 2008; Dang and Croft, 2010) uses probabilistic models, such as graphical models, to predict the sequence of rewritten query words to form a new query. Comparing to these works, our paraphrasing engine alters queries in a similar way to statistical machine translation, with systematic tuning and </context>
</contexts>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>Victor Lavrenko and W. Bruce Croft. 2001. Relevance based language models. In Proceedings of SIGIR, pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question-answering. Natural Language Engineering,</title>
<date>2001</date>
<pages>343--360</pages>
<contexts>
<context position="4813" citStr="Lin and Pantel (2001)" startWordPosition="741" endWordPosition="744">aphrase extraction approaches, and then describe our paraphrasing engine for the web search task from three aspects, including: 1) a search-oriented paraphrasing model; 2) an NDCG-based parameter optimization algorithm; 3) an enhanced ranking model with augmented features that are computed based on the extra knowledge provided by the paraphrase candidates of the original queries. 2.1 Paraphrase Extraction Paraphrases can be mined from various resources. Given a bilingual corpus, we use Bannard and Callison-Burch (2005)’s pivot-based approach to extract paraphrases. Given a monolingual corpus, Lin and Pantel (2001)’s method is used to extract paraphrases based on distributional hypothesis. Additionally, human annotated data can also be used as high-quality paraphrases. We use Miller (1995)’s approach to extract paraphrases from the synonym dictionary of WordNet. Word alignments within each paraphrase pair are generated using GIZA++ (Och and Ney, 2000). 2.2 Search-Oriented Paraphrasing Model Similar to statistical machine translation (SMT), given an input query Q, our paraphrasing engine generates paraphrase candidates1 based on a linear model. Qˆ = arg max Q&apos;E?i(Q) = arg max Q&apos;E?i(Q) W(Q) is the hypothe</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question-answering. Natural Language Engineering, pages 343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tie-Yan Liu</author>
<author>Jun Xu</author>
<author>Tao Qin</author>
<author>Wenying Xiong</author>
<author>Hang Li</author>
</authors>
<title>Letor: Benchmark dataset for research on learning to rank for information retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR workshop,</booktitle>
<pages>3--10</pages>
<contexts>
<context position="7942" citStr="Liu et al., 2007" startWordPosition="1258" endWordPosition="1261">03) to optimize feature weights of the paraphrasing model according to NDCG. We define D as the entire document set. R is a ranking model4 that can rank documents in D based on each input query. {Qi, DLabel i }Si=1 is a humanlabeled development set. Qi is the ith query and DLabel C D is a subset of documents, in which i the relevance between Qi and each document is labeled by human annotators. MERT is used to optimize feature weights of our linear-formed paraphrasing model. For 3Paraphrase rule count is the number of rules that are used to generate paraphrase candidates. 4The ranking model R (Liu et al., 2007) uses matching features computed based on original queries and documents. P(Q&apos;|Q) M amhm(Q, Q&apos;) m=1 42 each query Qi in {Qi}Si=1, we first generate Nbest paraphrase candidates {Qji }N j=1, and compute NDCG score for each paraphrase based on documents ranked by the ranker R and labeled documents DLabel i . We then optimize the feature weights according to the following criterion: Err(DLabel i , ˆQi; λM1 , R)} The objective of MERT is to find the optimal feature weight vector ˆλM1 that minimizes the error criterion Err according to the NDCG scores of top-1 paraphrase candidates. The error functi</context>
<context position="12713" citStr="Liu et al., 2007" startWordPosition="2106" endWordPosition="2109">o find an optimal weight vector { 43 3.2 Baseline Systems The baselines of the paraphrasing and the ranking model are described as follows: The paraphrasing baseline is denoted as BLPara, which only uses traditional SMT features described at the end of Section 2.2. Weights are optimized by MERT using BLEU (Papineni et al., 2002) as the error criterion. Development data are generated based on the English references of NIST 2008 constrained track of Chinese-to-English machine translation task. We use the first reference as the source, and the rest as its paraphrases. The ranking model baseline (Liu et al., 2007) is denoted as BL-Rank, which only uses matching features computed based on original queries and different meta-streams of web pages, including URL, page title, page body, meta-keywords, metadescription and anchor texts. The feature functions we use include unigram/bigram/trigram BM25 and original/normalized Perfect-Match. The ranking model is learned based on SV Mrank toolkit (Joachims, 2006) with default parameter setting. 3.3 Impacts of Search-Oriented Features We first evaluate the effectiveness of the searchoriented features. To do so, we add these features into the paraphrasing model bas</context>
</contexts>
<marker>Liu, Xu, Qin, Xiong, Li, 2007</marker>
<rawString>Tie-Yan Liu, Jun Xu, Tao Qin, Wenying Xiong, and Hang Li. 2007. Letor: Benchmark dataset for research on learning to rank for information retrieval. In Proceedings of SIGIR workshop, pages 3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<pages>39--41</pages>
<contexts>
<context position="4991" citStr="Miller (1995)" startWordPosition="771" endWordPosition="772">parameter optimization algorithm; 3) an enhanced ranking model with augmented features that are computed based on the extra knowledge provided by the paraphrase candidates of the original queries. 2.1 Paraphrase Extraction Paraphrases can be mined from various resources. Given a bilingual corpus, we use Bannard and Callison-Burch (2005)’s pivot-based approach to extract paraphrases. Given a monolingual corpus, Lin and Pantel (2001)’s method is used to extract paraphrases based on distributional hypothesis. Additionally, human annotated data can also be used as high-quality paraphrases. We use Miller (1995)’s approach to extract paraphrases from the synonym dictionary of WordNet. Word alignments within each paraphrase pair are generated using GIZA++ (Och and Ney, 2000). 2.2 Search-Oriented Paraphrasing Model Similar to statistical machine translation (SMT), given an input query Q, our paraphrasing engine generates paraphrase candidates1 based on a linear model. Qˆ = arg max Q&apos;E?i(Q) = arg max Q&apos;E?i(Q) W(Q) is the hypothesis space containing all paraphrase candidates of Q, hm is the mth feature function with weight am, Q&apos; denotes one candidate. In order to enable our paraphrasing model to learn t</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, pages 39– 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="5156" citStr="Och and Ney, 2000" startWordPosition="794" endWordPosition="797"> candidates of the original queries. 2.1 Paraphrase Extraction Paraphrases can be mined from various resources. Given a bilingual corpus, we use Bannard and Callison-Burch (2005)’s pivot-based approach to extract paraphrases. Given a monolingual corpus, Lin and Pantel (2001)’s method is used to extract paraphrases based on distributional hypothesis. Additionally, human annotated data can also be used as high-quality paraphrases. We use Miller (1995)’s approach to extract paraphrases from the synonym dictionary of WordNet. Word alignments within each paraphrase pair are generated using GIZA++ (Och and Ney, 2000). 2.2 Search-Oriented Paraphrasing Model Similar to statistical machine translation (SMT), given an input query Q, our paraphrasing engine generates paraphrase candidates1 based on a linear model. Qˆ = arg max Q&apos;E?i(Q) = arg max Q&apos;E?i(Q) W(Q) is the hypothesis space containing all paraphrase candidates of Q, hm is the mth feature function with weight am, Q&apos; denotes one candidate. In order to enable our paraphrasing model to learn the preferences on different paraphrasing strategies according to the characteristics of web queries, we design search-oriented features2 based on word alignments wit</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of ACL, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="2434" citStr="Och, 2003" startWordPosition="375" endWordPosition="376">earch Asia. Nan Duan Microsoft Research Asia nanduan@microsoft.com Ming Zhang School of EECS Peking University mzhang@net.pku.edu.cn an in-depth study on adapting paraphrasing to web search. First, we propose a search-oriented paraphrasing model, which includes specifically designed features for web queries that can enable a paraphrasing engine to learn preferences on different paraphrasing strategies. Second, we optimize the parameters of the paraphrasing model according to the Normalized Discounted Cumulative Gain (NDCG) score, by leveraging the minimum error rate training (MERT) algorithm (Och, 2003). Third, we propose an enhanced ranking model by using augmented features computed on paraphrases of original queries. Many query reformulation approaches have been proposed to tackle the query-document mismatch issue, which can be generally summarized as query expansion and query substitution. Query expansion (Baeza-Yates, 1992; Jing and Croft, 1994; Lavrenko and Croft, 2001; Cui et al., 2002; Yu et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitu</context>
<context position="7328" citStr="Och, 2003" startWordPosition="1148" endWordPosition="1149">odeled by a relative distortion probability distribution, similar to the distortion model in (Koehn et al., 2003). • Length Difference feature hLD(Q, Q&apos;), which is defined as |Q&apos; |− |Q|. • Edit Distance feature hED(Q, Q&apos;), which is defined as the character-level edit distance between Q and Q&apos;. Besides, a set of traditional SMT features (Koehn et al., 2003) are also used in our paraphrasing model, including translation probability, lexical weight, word count, paraphrase rule count3, and language model feature. 2.3 NDCG-based Parameter Optimization We utilize minimum error rate training (MERT) (Och, 2003) to optimize feature weights of the paraphrasing model according to NDCG. We define D as the entire document set. R is a ranking model4 that can rank documents in D based on each input query. {Qi, DLabel i }Si=1 is a humanlabeled development set. Qi is the ith query and DLabel C D is a subset of documents, in which i the relevance between Qi and each document is labeled by human annotators. MERT is used to optimize feature weights of our linear-formed paraphrasing model. For 3Paraphrase rule count is the number of rules that are used to generate paraphrase candidates. 4The ranking model R (Liu</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="12426" citStr="Papineni et al., 2002" startWordPosition="2060" endWordPosition="2063">tated documents as the development set to tune paraphrasing parameters (as we discussed in Section 2.3), and use the rest as the test set. The ranking model is trained based on the development set. NDCG is used as the evaluation metric of the web search task. ˆλM1 = arg min λM 1 S { i=1 is to find an optimal weight vector { 43 3.2 Baseline Systems The baselines of the paraphrasing and the ranking model are described as follows: The paraphrasing baseline is denoted as BLPara, which only uses traditional SMT features described at the end of Section 2.2. Weights are optimized by MERT using BLEU (Papineni et al., 2002) as the error criterion. Development data are generated based on the English references of NIST 2008 constrained track of Chinese-to-English machine translation task. We use the first reference as the source, and the rest as its paraphrases. The ranking model baseline (Liu et al., 2007) is denoted as BL-Rank, which only uses matching features computed based on original queries and different meta-streams of web pages, including URL, page title, page body, meta-keywords, metadescription and anchor texts. The feature functions we use include unigram/bigram/trigram BM25 and original/normalized Per</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>142--149</pages>
<contexts>
<context position="1147" citStr="Quirk et al., 2004" startWordPosition="169" endWordPosition="172">nking model leveraging augmented features computed on paraphrases of original queries. Experiments performed on the large scale query-document data set show that, the search performance can be significantly improved, with +3.28% and +1.14% NDCG gains on dev and test sets respectively. 1 Introduction Paraphrasing is an NLP technique that generates alternative expressions to convey the same meaning of the input text in different ways. Researchers have made great efforts to improve paraphrasing from different perspectives, such as paraphrase extraction (Zhao et al., 2007), paraphrase generation (Quirk et al., 2004), model optimization (Zhao et al., 2009) and etc. But as far as we know, none of previous work has explored the impact of using a well designed paraphrasing engine for web search ranking task specifically. In web search, mismatches between queries and their relevant documents are usually caused by expressing the same meaning in different natural language ways. E.g., X is the author of Y and Y was written by X have identical meaning in most cases, but they are quite different in literal sense. The capability of paraphrasing is just right to alleviate such issues. Motivated by this, this paper p</context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and William Dolan. 2004. Monolingual machine translation for paraphrase generation. In Proceedings of EMNLP, pages 142–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark D Smucker</author>
<author>James Allan</author>
<author>Ben Carterette</author>
</authors>
<title>A comparison of statistical significance tests for information retrieval evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>623--632</pages>
<contexts>
<context position="15264" citStr="Smucker et al., 2007" startWordPosition="2493" endWordPosition="2496"> in Table 1, we compare the NDCG@1 scores of the best documents retrieved using query paraphrases generated by BLPara+SF and BL-Para+SF+Opt respectively, with results shown in Table 2. Test Set BL-Para+SF BL-Para+SF+Opt Original Query Cand@1 Cand@1 27.28% 26.53% 27.06%(+0.53%) Table 2: Impacts of NDCG-based optimization. Table 2 indicates that, by leveraging NDCG as the error criterion for MERT, search-oriented features benefit more (+0.53% NDCG) in selecting the best query paraphrase from the whole paraphrasing search space. The improvement is statistically significant (p &lt; 0.001) by t-test (Smucker et al., 2007). The quality of the top-1 paraphrase generated by BL-Para+SF+Opt is very close to the original query. 3.5 Impacts of Enhanced Ranking Model We last evaluate the effectiveness of the enhanced ranking model. The ranking model baseline BL-Rank only uses original queries to compute matching features between queries and documents; while the enhanced ranking model, denoted as BL-Rank+Para, uses not only the original query but also its top-1 paraphrase candidate generated by BL-Para+SF+Opt to compute augmented matching features described in Section 2.4. Dev Set NDCG@1 NDCG@5 BL-Rank 25.31% 33.76% BL</context>
</contexts>
<marker>Smucker, Allan, Carterette, 2007</marker>
<rawString>Mark D Smucker, James Allan, and Ben Carterette. 2007. A comparison of statistical significance tests for information retrieval evaluation. In Proceedings of CIKM, pages 623–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuanhui Wang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Mining term association patterns from search logs for effective query reformulation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th ACM conference on Information and knowledge management, Proceedings of CIKM,</booktitle>
<pages>479--488</pages>
<contexts>
<context position="3120" citStr="Wang and Zhai, 2008" startWordPosition="484" endWordPosition="487">atures computed on paraphrases of original queries. Many query reformulation approaches have been proposed to tackle the query-document mismatch issue, which can be generally summarized as query expansion and query substitution. Query expansion (Baeza-Yates, 1992; Jing and Croft, 1994; Lavrenko and Croft, 2001; Cui et al., 2002; Yu et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitution (Brill and Moore, 2000; Jones et al., 2006; Guo et al., 2008; Wang and Zhai, 2008; Dang and Croft, 2010) uses probabilistic models, such as graphical models, to predict the sequence of rewritten query words to form a new query. Comparing to these works, our paraphrasing engine alters queries in a similar way to statistical machine translation, with systematic tuning and decoding components. Zhao et al. (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models. Our work can be seen as an extension along this line of research, by carrying out in-depth study on adapting paraphrasing to web search. Experime</context>
</contexts>
<marker>Wang, Zhai, 2008</marker>
<rawString>Xuanhui Wang and ChengXiang Zhai. 2008. Mining term association patterns from search logs for effective query reformulation. In Proceedings of the 17th ACM conference on Information and knowledge management, Proceedings of CIKM, pages 479–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Xu</author>
<author>Gareth J F Jones</author>
<author>Bin Wang</author>
</authors>
<title>Query dependent pseudo-relevance feedback based on wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>59--66</pages>
<contexts>
<context position="2933" citStr="Xu et al., 2009" startWordPosition="452" endWordPosition="455">d Discounted Cumulative Gain (NDCG) score, by leveraging the minimum error rate training (MERT) algorithm (Och, 2003). Third, we propose an enhanced ranking model by using augmented features computed on paraphrases of original queries. Many query reformulation approaches have been proposed to tackle the query-document mismatch issue, which can be generally summarized as query expansion and query substitution. Query expansion (Baeza-Yates, 1992; Jing and Croft, 1994; Lavrenko and Croft, 2001; Cui et al., 2002; Yu et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitution (Brill and Moore, 2000; Jones et al., 2006; Guo et al., 2008; Wang and Zhai, 2008; Dang and Croft, 2010) uses probabilistic models, such as graphical models, to predict the sequence of rewritten query words to form a new query. Comparing to these works, our paraphrasing engine alters queries in a similar way to statistical machine translation, with systematic tuning and decoding components. Zhao et al. (2009) proposes an unified paraphrasing framework that can be adapted to different appli</context>
</contexts>
<marker>Xu, Jones, Wang, 2009</marker>
<rawString>Yang Xu, Gareth J.F. Jones, and Bin Wang. 2009. Query dependent pseudo-relevance feedback based on wikipedia. In Proceedings of SIGIR, pages 59– 66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shipeng Yu</author>
<author>Deng Cai</author>
<author>Ji-Rong Wen</author>
<author>Wei-Ying Ma</author>
</authors>
<title>Improving pseudo-relevance feedback in web information retrieval using web page segmentation.</title>
<date>2003</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>11--18</pages>
<contexts>
<context position="2847" citStr="Yu et al., 2003" startWordPosition="436" endWordPosition="439">cond, we optimize the parameters of the paraphrasing model according to the Normalized Discounted Cumulative Gain (NDCG) score, by leveraging the minimum error rate training (MERT) algorithm (Och, 2003). Third, we propose an enhanced ranking model by using augmented features computed on paraphrases of original queries. Many query reformulation approaches have been proposed to tackle the query-document mismatch issue, which can be generally summarized as query expansion and query substitution. Query expansion (Baeza-Yates, 1992; Jing and Croft, 1994; Lavrenko and Croft, 2001; Cui et al., 2002; Yu et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitution (Brill and Moore, 2000; Jones et al., 2006; Guo et al., 2008; Wang and Zhai, 2008; Dang and Croft, 2010) uses probabilistic models, such as graphical models, to predict the sequence of rewritten query words to form a new query. Comparing to these works, our paraphrasing engine alters queries in a similar way to statistical machine translation, with systematic tuning and decoding components. Zhao et al. (2</context>
</contexts>
<marker>Yu, Cai, Wen, Ma, 2003</marker>
<rawString>Shipeng Yu, Deng Cai, Ji-Rong Wen, and Wei-Ying Ma. 2003. Improving pseudo-relevance feedback in web information retrieval using web page segmentation. In Proceedings of WWW, pages 11–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Zhang</author>
<author>Clement Yu</author>
</authors>
<title>Uic at trec</title>
<date>2006</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="2867" citStr="Zhang and Yu, 2006" startWordPosition="440" endWordPosition="443"> the parameters of the paraphrasing model according to the Normalized Discounted Cumulative Gain (NDCG) score, by leveraging the minimum error rate training (MERT) algorithm (Och, 2003). Third, we propose an enhanced ranking model by using augmented features computed on paraphrases of original queries. Many query reformulation approaches have been proposed to tackle the query-document mismatch issue, which can be generally summarized as query expansion and query substitution. Query expansion (Baeza-Yates, 1992; Jing and Croft, 1994; Lavrenko and Croft, 2001; Cui et al., 2002; Yu et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitution (Brill and Moore, 2000; Jones et al., 2006; Guo et al., 2008; Wang and Zhai, 2008; Dang and Croft, 2010) uses probabilistic models, such as graphical models, to predict the sequence of rewritten query words to form a new query. Comparing to these works, our paraphrasing engine alters queries in a similar way to statistical machine translation, with systematic tuning and decoding components. Zhao et al. (2009) proposes an uni</context>
</contexts>
<marker>Zhang, Yu, 2006</marker>
<rawString>Wei Zhang and Clement Yu. 2006. Uic at trec 2006 blog track. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
</authors>
<title>Learning question paraphrases for qa from encarta logs.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1795--1800</pages>
<contexts>
<context position="1103" citStr="Zhao et al., 2007" startWordPosition="163" endWordPosition="166">eter optimization algorithm; an enhanced ranking model leveraging augmented features computed on paraphrases of original queries. Experiments performed on the large scale query-document data set show that, the search performance can be significantly improved, with +3.28% and +1.14% NDCG gains on dev and test sets respectively. 1 Introduction Paraphrasing is an NLP technique that generates alternative expressions to convey the same meaning of the input text in different ways. Researchers have made great efforts to improve paraphrasing from different perspectives, such as paraphrase extraction (Zhao et al., 2007), paraphrase generation (Quirk et al., 2004), model optimization (Zhao et al., 2009) and etc. But as far as we know, none of previous work has explored the impact of using a well designed paraphrasing engine for web search ranking task specifically. In web search, mismatches between queries and their relevant documents are usually caused by expressing the same meaning in different natural language ways. E.g., X is the author of Y and Y was written by X have identical meaning in most cases, but they are quite different in literal sense. The capability of paraphrasing is just right to alleviate </context>
</contexts>
<marker>Zhao, Zhou, Liu, 2007</marker>
<rawString>Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learning question paraphrases for qa from encarta logs. In Proceedings of IJCAI, pages 1795–1800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Xiang Lan</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Application-driven statistical paraphrase generation.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>834--842</pages>
<contexts>
<context position="1187" citStr="Zhao et al., 2009" startWordPosition="175" endWordPosition="178"> computed on paraphrases of original queries. Experiments performed on the large scale query-document data set show that, the search performance can be significantly improved, with +3.28% and +1.14% NDCG gains on dev and test sets respectively. 1 Introduction Paraphrasing is an NLP technique that generates alternative expressions to convey the same meaning of the input text in different ways. Researchers have made great efforts to improve paraphrasing from different perspectives, such as paraphrase extraction (Zhao et al., 2007), paraphrase generation (Quirk et al., 2004), model optimization (Zhao et al., 2009) and etc. But as far as we know, none of previous work has explored the impact of using a well designed paraphrasing engine for web search ranking task specifically. In web search, mismatches between queries and their relevant documents are usually caused by expressing the same meaning in different natural language ways. E.g., X is the author of Y and Y was written by X have identical meaning in most cases, but they are quite different in literal sense. The capability of paraphrasing is just right to alleviate such issues. Motivated by this, this paper presents ∗ This work has been done while </context>
<context position="3451" citStr="Zhao et al. (2009)" startWordPosition="537" endWordPosition="540">u et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitution (Brill and Moore, 2000; Jones et al., 2006; Guo et al., 2008; Wang and Zhai, 2008; Dang and Croft, 2010) uses probabilistic models, such as graphical models, to predict the sequence of rewritten query words to form a new query. Comparing to these works, our paraphrasing engine alters queries in a similar way to statistical machine translation, with systematic tuning and decoding components. Zhao et al. (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models. Our work can be seen as an extension along this line of research, by carrying out in-depth study on adapting paraphrasing to web search. Experiments performed on the large scale data set show that, by leveraging additional matching features computed on query paraphrases, significant NDCG gains can be achieved on both dev 41 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 41–46, Sofia, Bulgaria, August 4-9 2013. c�2013 Associa</context>
</contexts>
<marker>Zhao, Lan, Liu, Li, 2009</marker>
<rawString>Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009. Application-driven statistical paraphrase generation. In Proceedings of ACL, pages 834–842.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>