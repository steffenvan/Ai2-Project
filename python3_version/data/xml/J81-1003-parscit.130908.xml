<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.847160833333333">
Operating Statistics for
The Transformational Question Answering System
Fred J. Damerau
Mathematical Sciences Department
IBM Thomas J. Watson Research Center
Post Office Box 218
</title>
<author confidence="0.728362">
Yorktown Heights, New York 10598
</author>
<bodyText confidence="0.998100857142857">
This paper presents a statistical summary of the use of the Transformational Question
Answering (TQA) system by the City of White Plains Planning Department during the year
1978. A complete record of the 788 questions submitted to the system that year is
included, as are separate listings of some of the problem inputs. Tables summarizing the
performance of the system are also included and discussed. In general, performance of the
system was sufficiently good that we believe that the approach being followed is a viable
one, and are continuing to develop and extend the system.
</bodyText>
<sectionHeader confidence="0.812538" genericHeader="abstract">
Introduction
</sectionHeader>
<bodyText confidence="0.999951176470588">
Natural language question answering systems have
been the subject of much research over approximately
20 years. In only very few cases have such systems
been exposed to real users trying to solve real prob-
lems, another example perhaps being Krause (1979).
In an attempt to see if a useful natural language query
system could be built for an application which existed
independently of the research program, an approach
was made to the Planning Department of the City of
White Plains asking them to take part in such an expe-
riment. Their incentive was the free use of an interac-
tive query facility which would allow them to explore
their data base more freely than the batch computer
facility run by the city could do. The remainder of the
paper describes the user environment, describes the
operation of the TQA system, and discusses the oper-
ating results in the first year of operation, 1978.
</bodyText>
<subsectionHeader confidence="0.960808">
The User Environment
</subsectionHeader>
<bodyText confidence="0.975029">
When the experiment was first being discussed, the
Planning Department had five professionals plus the
services of a consultant more or less constantly availa-
ble. The main files used for planning were the parcel
file and the geobase file, which had been converted to
machine readable form under a grant from the Depart-
ment of Housing and Urban Development. Depart-
ment members were very familiar with these files, in
many instances knowing the correspondence between
codes in the file and their English equivalents.
The parcel file contained a record for each parcel
of land in the city, approximately 10,000 of them,
which had a taxable existence. Each record contained
the account number, the block, the owner, the address,
land use information, number of dwelling units, area,
taxes, and the like. The geobase file contained a re-
cord for each city block, telling what census tract it
was in, what traffic zone, what neighborhood associa-
tion, etc. Selected summaries of these files had been
prepared by the City&apos;s computing center, and the files
themselves had been printed in a couple of large vol-
umes. Ad hoc queries required special programs to be
written by the computing center, with sufficient delay
that this was seldom done. Thus, we were told that
during the 1974 gasoline shortage, the parcel file was
searched by hand on the land use code field to find
the locations of all the gas stations so that police could
be routed there to direct traffic. Other uses of the
files are apparent from inspection of the questions
asked of the system, a sample of which are given in
Figure 1.
Although the terminal was located in an open area
available to all members of the department, it turned
out that most of the questions were asked by one of
the department members, who had been designated as
our liaison. The reason for this was never clear, but
may have simply been normal reluctance to experiment
with radically new technology on the part of the older
members of the department. Other personnel did
sometimes use the system, but under his sign-on code,
Copyright 1981 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted
provided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included on
the first page. To copy otherwise, or to republish, requires a fee and/or specific permission.
</bodyText>
<page confidence="0.41507">
0362-613X/81/010030-13$01.00
</page>
<note confidence="0.814733">
30 American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981
Fred J. Damerau Operating Statistics for the Transformational Question Answering System
</note>
<bodyText confidence="0.9339878">
Where are the parcels with a LUC of 641 ?
What single family houses in Fisher Hill have
exemptions greater than $0 ?
How many two family houses are there in the
Oak Ridge Residents Assn. ?
Who owns the parcels in subplanning area 7.60 ?
Where are the churches on North Street ?
What parcels on Stevens St. have a LUC of 116 ?
What parcel in ward 1 does city own ?
Print the parcel area of the LUC&apos;S 300 - 399 !
What is the assessment of the parcels in the
Battle Hill Assn. having more than 3 dwelling units ?
Where is Martin &apos;s parcel ?
Where are the apartment dwellings which have more than 50
units which are more than 6 stories high on Lake St. ?
Where is the Calvary Baptist Church parcels ?
Where is parcel 50550006401 ?
What properties does Longo own ?
Where are the apartment buildings having less
than 8401 sq ft ground floor area ?
</bodyText>
<figureCaption confidence="0.994633">
Figure 1. Example inputs to TQA.
</figureCaption>
<bodyText confidence="0.938155">
so that the actual user for any question cannot be
determined from the log in most cases. During the
course of the experiment, a new planning director was
appointed, who changed the mission of the department
somewhat. The result was that members of the plan-
ning department did fewer of the conventional plan-
ning activities than formerly, with results that are ap-
parent in the statistics that follow.
The TQA System
The TQA system, originally named REQUEST, has
been under development for some time at the IBM
Thomas J. Watson Research Center. An experimental
application using business statistics had been quite
satisfying. For the White Plains application, major
additions were made to the lexicon, new grammar rules
extending coverage were written, a new component for
interfacing the grammar and a data base was devel-
oped, and an interface was built to an existing data
base management system, the RSS. The system was
installed in White Plains late in 1977 for final debug-
ging, and turned over to the planners at the beginning
of 1978. It was disconnected at the end of 1979 part-
ly for legal reasons and partly because we felt little
new could be learned by leaving it there.
A generalized flow diagram of the TQA system is
given in Figure 2, and an example of processing in
Figures 3a-g. The structures printed in Figure 3a are
a bracketed terminal string representation of structures
which are stored and manipulated as trees by the proc-
essing programs. The trees, together with their associ-
ated complex features, for the example are shown in
Figures 3b-g. These structures are the outputs at simi-
</bodyText>
<figure confidence="0.998469315789474">
Input
IPreprocessorl &lt; Lexicon
I List of lexical trees
ITransformational parserl &lt;----String transformations
I List of trees
&apos;Context free parserl &lt; Context free phrase
structure rules
I List of surface trees
ITransformational parserl &lt;----Inverse transformational
grammar
I Underlying structure(s)
ITransformational parserl &lt;----Data base specific
transformational rules
I Query structure(s)
ISemantic interpreterl &lt; Semantic rules
&lt;--I
I Logical form(s)
lEvaluatorl &lt; Data base
Answer
</figure>
<figureCaption confidence="0.999916">
Figure 2. Flow diagram of TQA.
</figureCaption>
<bodyText confidence="0.999468333333334">
larly named points of the flow diagram in Figure 2.
Input from an IBM 3275 display station is fed to
the preprocessor, which segments the input character
string into words and performs lexical lookup. The
process of lookup is complicated somewhat by a provi-
sion for synonym and phrase replacement. Words like
&amp;quot;car&amp;quot; and &amp;quot;automobile&amp;quot; are changed to &amp;quot;auto&amp;quot;, and
strings like &amp;quot;gas station&amp;quot; are frozen into single lexical
units. The output from the lexical lookup is a list of
trees, each tree containing part-of-speech information,
syntactic features and semantic features, as required.
A description of the lexical component, now obsolete
</bodyText>
<note confidence="0.697728">
American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 31
Fred J. Damerau Operating Statistics for the Transformational Question Answering System
</note>
<bodyText confidence="0.575316">
TYPE NEXT QUESTION.
Where are the gas stations in traffic zone 579 ?
</bodyText>
<equation confidence="0.979875391304348">
STRING TRANSFORMATIONS:
((AT ((WH SOME) (PLACE X11))) BE THE
((GAS_STATION =S17) X4) IN (TRAFFIC_ZONE 579) ?)
SURFACE STRUCTURES:
1. ((AT ((WH SOME) (PLACE X11))) BE (THE
(((GAS_STATION =S17) X4) (IN (TRAFFIC_ZONE 579)))) ?)
2. ((AT ((WH SOME) (PLACE X11))) BE (THE
((GAS_STATION =S17) X4)) (IN (TRAFFIC_ZONE 579)) ?)
UNDERLYING STRUCTURES:
1. (BD LOCATED (THE (((GAS_STATION =S17) X4)
(* BD LOCATED X4 (TRAFFIC_ZONE 579) BD *)))
((WH SOME) (PLACE X11)) BD)
QUERY STRUCTURES:
1. (BD ADDRESS ((WH SOME) (THING X11))
(THE (((GAS_STATION 553) X4)
(* BD TRAFFIC_ZONE 579 X4 BD *))) BD)
LOGICAL FORM:
(setx &apos;Xll
&apos;(foratleast @1 &apos;X63
(setx &apos;X4
&apos;(and
(testfct @579
&apos;(TRAFZ X4 &apos;@1976)
</equation>
<bodyText confidence="0.486399153846154">
(testfct 0553
TUC X4 &apos;@1976)
&apos;(testfct X11
(ADDRESS X63 &apos;@1976)
) ) )
ANSWER:
ADDRESS
1976
2 06300 02000 122 S LEXINGTON AV
2 06600 00100 101 W POST RD
2 05500 09300 102 W POST RD
2 07100 03300 109 W POST RD
2 07100 02900 115 W POST RD
</bodyText>
<figureCaption confidence="0.931628">
Figure 3a. Short trace of example question showing major interme-
diate structures.
</figureCaption>
<bodyText confidence="0.99985">
in its detail but still valid in main outline, is given in
Robinson (1973). Without going into great detail, one
can see in Figure 3b that &amp;quot;gas station&amp;quot; and &amp;quot;traffic
zone&amp;quot; have been made into single units. The node
=S17 in the entry for gas station is a macro standing
for a bundle of semantic features. Many of the fea-
ture names should be obvious, but, e.g., PL stands for
&amp;quot;place&amp;quot;, PAG for parcel aggregate, i.e., an aggregate
of separate parcels, and CINS for &amp;quot;cardinal insertion&apos;,
i.e., can be followed by a cardinal number.
</bodyText>
<equation confidence="0.99859934375">
( (where
( ((RP ((+ WH) (+ LOC)))
((NOM)
((NOUN ((+ RED (- HU) (+ PL)))
((INDEX ((- CONST)))
((X0)) ) ) ) ) ) )
(are
( ((BAUX ((- PAST) (- SG)))
((BE)) ) ) )
(the
(((DET) ((THE)))) )
(gas_station
( ((NOM)
((NOUN ((- HU) (- SG) (+ (+ SV)))
((V)
((V) ((GAS_STATION)))
((=S17)) )
((INDEX ((- CONST)))) ) ) ) )
(in
(((PREP) ((IN)))) )
(traffic_zone
( ((NOM)
((NOUN ((- HU) (+ SG) (+ (+ PAG) (+ GEO)))
((V ((+ OGEN) (+ POBJ)))
((TRAFFIC_ZONE)) )
((INDEX ((- CONST) (+ CINS)))) ) ) ) )
(579
( ((VADJ ((+ ADJ) (+ CARD) (+ D3)))
((579)) ) ) )
(?
( ((PUNCT ((+ QUES)))
((?)) ) ) ) )
</equation>
<figureCaption confidence="0.975674">
Figure 3b. List of lexical trees.
</figureCaption>
<bodyText confidence="0.999032428571428">
The list of lexical trees is input to a set of string
transformations, described in Plath (1974). These
transformations operate on adjacent lexical items to
deal with patterns of classifiers, ordinal numbers,
stranded prepositions, and the like. The effect of this
phase is to reduce the number of surface parses and
the amount of work done in the transformational cy-
cle. Referring to Figure 3c, note that &amp;quot;579&amp;quot; has been
incorporated with &amp;quot;traffic zone&amp;quot; under a single node,
PROPNOM.
The resulting list of trees is input to a context free
parser, which produces a set of surface trees. In the
example, two surface trees are produced, shown in
Figures 3d and 3e. The trees differ in the point of
attachment of the phrase &amp;quot;in traffic zone 579&amp;quot;. In
structure 1, it is attached to the NP &amp;quot;the gas station&amp;quot;,
and in structure 2 it is directly under the S node.
The recognizer attempts to find an underlying
structure for each surface tree (Plath 1973, 1976).
Typically, only one of a set of surface trees will result
in an underlying structure. In the example, the struc-
</bodyText>
<page confidence="0.893378">
32 American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981
</page>
<note confidence="0.750021">
Fred J. Damerau Operating Statistics for the Transformational Question Answering System
</note>
<equation confidence="0.997194171428571">
((AT ((WH SOME) (PLACE X11))) BE THE
((GAS_STATION =S17) X4) IN (TRAFFIC_ZONE 579) ?)
((ST)
((PP)
((NSPREP) ((AT)))
UNP)
((NOM)
((V ((+ ADJ) (+ QUANT)))
((WH))
((SOME)) )
((NOM)
((NOUN ((+ SG) (- HU) (+ P1)))
((V) ((PLACE)))
((INDEX ((- CONST)))
((X11)) ) ) ) ) ) )
((BAUx ((- PAST) (- SG)))
((BE)) )
((DET) ((THE)))
((NOM)
((NOUN ((- HU) (- SG) (+ P1) (+ SV)))
((V)
((V) ((GAS_STATION)))
((=S17)) )
((INDEX ((- CONST)))
((X4)) ) ) )
((NSPREP) ((IN)))
((PROPNOM)
((NOUN ((- HU) (+ SG) (+ PL) (+ PAG) (+ GEO)))
((V ((+ OGEN) (+ POBJ)))
((TRAFFIC_ZONE)) )
((INDEX ((+ CONST)))
((VADJ ((+ ADJ) (+ CARD) (+ D3)))
((579)) ) ) ) )
((PUNCT ((+ QUES)))
((?)) ) )
</equation>
<figureCaption confidence="0.954445">
Figure 3c. List of trees after string transformations.
</figureCaption>
<bodyText confidence="0.999970666666667">
ture in which the prepositional phrase was attached to
&amp;quot;gas station&amp;quot; survives. The underlying structures are
similar to those proposed under the variant of trans-
formational grammar called generative semantics. This
is not the place to defend that particular theory or its
use on the TQA system; suffice it to say that a consid-
erable body of grammatical work on English has been
done in a style compatible with this theory. To simpli-
fy, every S in the underlying structure has a predicate
and some number of noun phrase arguments. The
noun phrases may dominate imbedded S&apos;s. In the
example, the top level predicate is LOCATED with
arguments of &amp;quot;gas station&amp;quot; and &amp;quot;some place&amp;quot;. The
NP for &amp;quot;gas station&amp;quot; dominates an S which also has a
main predicate of LOCATED and arguments of &amp;quot;X4&amp;quot;,
i.e., the same index as that for &amp;quot;gas station&amp;quot;, and
&amp;quot;traffic zone 579&amp;quot;, i.e., the gas station located in traf-
fic zone 579. Notice that the parser has supplied both
</bodyText>
<equation confidence="0.985793044444444">
1. ((AT ((WH SOME) (PLACE X11))) BE (THE
(((GAS_STATION =S17) x4) (IN (TRAFFIC_ZONE 579)))) ?)
((Si)
((PP)
((NSPREP) ((AT)))
((NP)
((NOM)
((V ((+ ADJ) (+ QUANT)))
((WH))
((SOME)) )
((NOM)
((NOUN ((+ SG) (- HU) (+ PL)))
((V) ((PLACE)))
((INDEX ((- CONST)))
((X11)) ) ) ) ) ) )
((BAUx ((- PAST) (- SG)))
((BE)) )
((NP)
((DETX)
((DET) ((THE))) )
((NOMX)
((NOMZ)
((NOM)
((NOUN ((- HU) (- SG) (+ PL) (+ SV)))
((V)
((V) ((GAS_STATION)))
((=S17)) )
((INDEX ((- CONST)))
((X4)) ) ) )
((Z1)
UPp3X)
((PP)
((NSPREP) ((IN)))
((RPX)
((NP)
((PROPNOM)
((NOUN ((- HU) (+ SG) (+ P1)
(+ PAG) (+ GEO)))
((V ((+ OGEN) (+ POBJ)))
((TRAFFIC_ZONE)) )
((INDEX ((+ CONST)))
((vADJ ((+ ADJ) (+ CARD) (+ D3)))
((579)) ) ) ) ) ) ) ) ) ) ) ) )
((PUNCT ((+ QUES)))
((?)) ) )
</equation>
<figureCaption confidence="0.982948">
Figure 3d. Surface structure tree 1.
</figureCaption>
<bodyText confidence="0.999844875">
instances of &amp;quot;located&amp;quot;. A fuller explanation is given
in Plath (1973, 1976).
The data base has no predicate, i.e., column head-
ing, named &amp;quot;located&amp;quot;. Even if it did, the two LO-
CATEDs in the underlying structures are different,
one for address and one for traffic zone. The underly-
ing structure is designed to reflect the linguistic struc-
ture of the query, but, as in this case, that structure
</bodyText>
<note confidence="0.41456">
American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 33
Fred J. Damerau Operating Statistics for the Transformational Question Answering System
</note>
<equation confidence="0.891001604651163">
2. ((AT ((WH SOME) (PLACE X11))) BE (THE
((GAS_STATION =SI7) X4)) (IN (TRAFFIC_ZONE 579)) ?)
((Si)
((PP)
((NSPREP) ((AT)))
((NP)
((NOM)
((V ((+ ADJ) (+ QUANT)))
((WH))
((SOME)) )
((NOM)
((NOUN ((+ SG) (- HU) (+ PL)))
((V) ((PLACE)))
((INDEX ((- CONST)))
((X11)) ) ) ) ) ) )
((BAUX ((- PAST) (- SG)))
((BE)) )
((NP)
((DETX)
((DET) ((THE))) )
((NOMX)
((NOM)
((NOUN ((- HU) (- SG) (+ PL) (+ SV)))
((V)
((V) ((GAS_STATION)))
((=S17)) )
((INDEX ((- CONST)))
((X4)) ) ) ) ) )
((PP4X)
((PP)
((NSPREP) ((IN)))
((RPX)
((NP)
((PROPNOM)
((NOUN ((- HU) (+ SG) (+ PL) (+ PAG) (+ GEO)))
((V ((+ OGEN) (+ POBJ)))
((TRAFFIC_ZONE)) )
((INDEX ((+ CONST)))
((VADJ ((+ ADJ) (+ CARD) (+ D3)))
((579)) ) ) ) ) ) ) ) )
((S2PCX)
((PUNCT ((+ QUES)))
((?)) ) ) )
</equation>
<figureCaption confidence="0.982724">
Figure 3e. Surface structure tree 2.
</figureCaption>
<bodyText confidence="0.999974">
may not match a particular data base organization.
There are many approaches one could take to make
this match. We have chosen to implement the match-
ing function as a separate transformational component
in the grammar (Damerau 1977). The underlying
structure itself is therefore input to the transforma-
tional recognizer, using a (small) set of grammar rules
tailored to a specific data base and produces a query
structure. Query structures are similar to underlying
structures in form, but reflect the particular meaning
</bodyText>
<equation confidence="0.925710551020408">
I. (BD LOCATED (THE (((GAS_STATION =S17) X4)
(* BD LOCATED X4 (TRAFFIC_ZONE 579) BD *)))
((WH SOME) (PLACE X11)) BD)
((Si ((- PAST) (+ WH) (+ QUES) (+ TOP)))
((BD))
((V ((+ ADJ) (+ EN) (+ LOC) (+ TEMP)
(+ PSUBJ) (+ POBJ)))
((LOCATED)) )
((NP)
((DET) ((THE)))
((NOM)
((NOM)
((NOUN ((- HU) (- SG) (+ PL) (+ SV)))
((V)
((V) ((GAS_STATION)))
((=S17)) )
((INDEX ((- CONST)))
((X4)) ) ) )
((Si)
((BD))
((V ((+ ADJ) (+ EN) (+ LOC) (+ TEMP)
(+ pSUBJ) (+ POBJ)))
((LOCATED)) )
((NP)
((NOM)
((NOUN ((- HU) (- SG) (+ PL) (+ SV)))
((INDEX ((- CONST)))
((X4)) ) ) ) )
((NP ((+ IN) (+ LOC)))
((NOM)
((NOUN ((- HU) (+ SG) (+ PL)
(+ PAG) (+ GEO)))
((V ((+ OGEN) (+ POBJ)))
((TRAFFIC_ZONE)) )
((INDEX ((+ CONST)))
((V ((+ ADJ) (+ CARD) (+ D3)))
((579)) ) ) ) ) )
((BD)) ) ) )
((NP ((+ AT) (+ LOC)))
((NOM)
((V ((+ ADJ) (+ QUANT)))
((WH))
((SOME)) )
((NOM)
((NOUN ((+ SG) (- HU) (+ PL)))
((V) ((PLACE)))
((INDEX ((- CONST)))
((X11)) ) ) ) ) )
((BD)) )
</equation>
<figureCaption confidence="0.992343">
Figure 3f. Underlying structure.
</figureCaption>
<bodyText confidence="0.99137">
constraints resulting from the format and content of a
given data base. In Figure 3g, one can see that the
first instance of LOCATED has been changed to the
</bodyText>
<note confidence="0.6319575">
34 American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981
Fred J. Damerau Operating Statistics for the Transformational Question Answering System
</note>
<equation confidence="0.933465957446808">
I. (BD ADDRESS ((WH SOME) (THING X11))
(THE (((GAS_STATION 553) X4)
(* BD TRAFFIC_ZONE 579 X4 BD *))) BD)
((51 ((- PAST) (+ WH) (+ QUES) (+ TOP)))
((BD))
((V ((+ ADJ) (+ EN) (+ LOC) (+ TEMP)
(+ PSUBJ) (+ POBJ)))
((ADDRESS)) )
((NP ((+ AT) (+ LOC)))
((NOM)
((V ((+ ADJ) (+ QUANT)))
((WH))
((SOME)) )
((NOM)
((NOUN ((+ SG) (- HU) (+ PL)))
((V) ((THING)))
((INDEX ((- CONST)))
((X11)) ) ) ) ) )
((NP)
((DET) ((THE)))
((NOM)
((NOM)
((NOUN ((- HU) (- SG) (+ PL) (+ SV)))
((V)
((V) ((GAS_STATION)))
((LUC) ((553))) )
((INDEX ((- CONST)))
((X4)) ) ) )
((Si)
((BD))
((V ((+ ADJ) (+ EN) (+ LOC) (+ TEMP)
(+ PSUBJ) (+ POBJ)))
((TRAFFIC_ZONE)) )
((NP ((+ IN) (+ LOC)))
((NOM)
((NOUN ((- HU) (+ SG) (+ PL)
(+ PAG) (+ GEO)))
((INDEX ((+ CONST)))
((V ((+ ADJ) (+ CARD) (+ 03)))
((579)) ) ) ) ) )
((NP)
((NOM)
((NOUN ((- HU) (- SG) (+ PL) (+ SV)))
((INDEX ((- CONST)))
((X4)) ) ) ) )
((BD)) ) ) )
((BD)) )
</equation>
<figureCaption confidence="0.961766">
Figure 3g. Query structure.
</figureCaption>
<bodyText confidence="0.997183795454546">
predicate ADDRESS and the second instance to the
predicate TRAFFIC_ZONE.
The query structure tree is processed by a Knuth-
style semantic interpreter (Petrick 1977), producing a
logical form. A logical form can best be thought of, in
our context, as a retrieval expression which is to be
evaluated, producing an answer to the English input
query. Referring to Figure 3a, the logical form can be
read as:
Find the set of things X11 such that for at
least 1 of the things X63 in the set of things
X4 where the traffic zone of X4 is 579 and
the LUC (land use code) is 0553, (viz., the
set of account numbers having both these
properties), the address of X63 is X11.
The process of answer extraction from the data
base is accomplished by a combination of LISP and
PL/I programs (Damerau 1978), and an experimental
relational data base management system called Rela-
tional Storage System (RSS) (Astrahan, et al., 1976).
The RSS provides the capability to generate a data
base of n-ary relations, with indexes on any field of
the relation, and low-level access commands like
OPEN, NEXT, CLOSE, with appropriate parameters,
to retrieve information from such a data base. This
particular data base had just one relation of 40 col-
umns. The LISP programs examine the logical form to
establish relationships between variables and to gener-
ate requests to the data base component to find items
with specified properties. In the example, one retriev-
al request would find the qualifying account numbers,
i.e., the X4s, and a second request would find the
addresses of those account numbers.
Notice that the logical form simply specifies a set
of addresses as the answer. This is clearly unsatisfac-
tory, and the data base interface program supplies the
account number for each address as part of the an-
swer. The long numbers in the answer are the parcel
identifiers, (ward-block-lot) referred to above as ac-
count numbers and sometimes called lot numbers.
All the processing modules are under the control of
a driver module which maintains communication with
the user, calls the processors in the correct sequence,
and tests for errors.
</bodyText>
<subsectionHeader confidence="0.994524">
Usage Statistics
</subsectionHeader>
<bodyText confidence="0.9999748">
The statistics presented below are not based on a
constant system. When a problem was discovered in
the course of operation, an attempt was usually made
to change the system so as to make the problem query
run successfully. This was not always possible, but a
great many changes were incorporated into the system
during the course of the year. An attempt to compen-
sate in part for the effect of this situation on the sta-
tistics was made by rerunning all the sentences which
failed during the year with the system in use in May,
1979. The results of this run are incorporated into the
appendices. An additional source of contamination
resulted when a user needed an answer to a question
and none of his attempts was successful. He might
then telephone one of the system developers and ask
</bodyText>
<note confidence="0.7943905">
American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 35
Fred J. Damerau Operating Statistics for the Transformational Question Answering System
</note>
<tableCaption confidence="0.81117">
Table I. Number of events. JAN FEB MAR APR MAY JUN JUL AUG SEP OCT NOV DEC TOTAL
</tableCaption>
<table confidence="0.99757468">
QUERIES 45 161 180 52 45 114 87 31 32 20 13 8 788
COMPLETED 18 92 127 44 28 66 68 22 17 17 9 5 513
LOOKUP FAILURES 3 19 17 5 10 19 31 8 1 2 3 1 119
PARSING FAILURES 14 26 38 4 4 29 11 7 11 0 3 0 147
NOTHING IN DATA BASE 12 8 12 3 1 2 13 5 1 1 3 0 61
ABORTED 3 9 14 2 5 9 4 2 1 2 1 1 53
USER COMMENTS 9 10 16 7 3 10 21 11 3 1 4 1 96
USER MESSAGES 0 3 0 0 0 3 4 1 0 0 0 0 11
OPERATOR MESSAGES 14 6 10 2 2 0 2 3 2 0 4 0 45
PROGRAM ERRORS 1 12 9 1 6 5 0 0 1 1 0 3 39
USER CANCELLED 1 5 1 1 2 5 4 0 2 0 0 0 21
LEXICAL CHOICE 6 32 11 4 10 31 6 3 1 11 3 1 119
Table II. Percentage of events to number of queries. APR MAY JUN JUL AUG SEP OCT NOV DEC TOTAL
JAN FEB MAR
COMPLETED 40.0 57.1 70.6 84.6 62.2 57.9 78.2 71.0 53.1 85.0 69.2 62.5 65.1
LOOKUP FAILURES 6.7 11.8 9.4 9.6 22.2 16.7 35.6 25.8 3.1 10.0 23.1 12.5 15.1
PARSING FAILURES 31.1 16.1 21.1 7.7 8.9 25.4 12.6 22.6 34.4 0.0 23.1 0.0 18.7
NOTHING IN DATA BASE 26.7 5.0 6.7 5.8 2.2 1.8 14.9 16.1 3.1 5.0 23.1 0.0 7.7
ABORTED 6.7 5.6 7.8 3.8 11.1 7.9 4.6 6.5 3.1 10.0 7.7 12.5 6.7
USER COMMENTS 20.0 6.2 8.9 13.5 6.7 8.8 24.1 35.5 9.4 5.0 30.8 12.5 12.2
USER MESSAGES 0.0 1.9 0.0 0.0 0.0 2.6 4.6 3.2 0.0 0.0 0.0 0.0 1.4
OPERATOR MESSAGES 31.1 3.7 5.6 3.8 4.4 0.0 2.3 9.7 6.3 0.0 30.8 0.0 5.7
PROGRAM ERRORS 2.2 7.5 5.0 1.9 13.3 4.4 0.0 0.0 3.1 5.0 0.0 37.5 4.9
USER CANCELLED 2.2 3.1 0.6 1.9 4.4 4.4 4.6 0.0 6.3 0.0 0.0 0.0 2.7
LEXICAL CHOICE 13.3 19.9 6.1 7.7 22.2 27.2 6.9 9.7 3.1 55.0 23.1 12.5 15.1
</table>
<bodyText confidence="0.999735">
for a suggestion. If that suggestion worked, this would
inflate the percentage of success somewhat.
The TQA system incorporates two logging facili-
ties. One of them is a verbatim record of all output
that appeared on the user terminal. The other is a
much more comprehensive trace of the system flow
while it processed each question. The primary use of
the second trace was to allow us to isolate the nature
of the problems which arose with a view to correction.
This file, however, contains much interesting detail
about the amount of computer time, (as opposed to
elapsed time), used in each processing step, the num-
ber of intermediate structures created, indications as
to exactly which step caused a failure, and the like.
Unfortunately, the second file was not originally
meant to be amenable to machine processing, and its
format was changed a few times during the course of
the year. In addition, when the computing system
failed, this second file was sometimes lost, although
the basic log file seldom was. As a result, the statis-
tics in the tables may be in error by a percentage point
or two. In any case, the error is not sufficiently large
to affect any of the major conclusions, which are qual-
itative at best.
Table I lists the raw numbers, by month and to-
taled for the year, for a number of the events which
occur in system operation. Percentages for each event
relative to the number of queries are given in Table II.
QUERIES refers to user inputs terminated by a
question mark or exclamation point. COMPLETED is
the number of queries which resulted in access to the
data base and a resulting response to the user.
LOOKUP FAILURES is the number of times the user
typed a word which was not in the lexicon and was
not capitalized. Such words were displayed back, with
the option of changing the word entered or entering an
entire new question. PARSING FAILURE means that
the system did not reach the point of producing a
query structure. NOTHING IN DATA BASE means
that a logical form was generated and a query issued
to System/R, but nothing satisfied the query. This
could happen for a variety of reasons, including an
erroneous parse, a wrong logical form, a mistake in the
program which generated the search request, or a real
case of missing data. Detailed analysis of each case
would be required to be sure what proportion fell into
each category.
The other categories are more or less self-
explanatory. ABORTED is an indication that query
</bodyText>
<page confidence="0.788211">
36 American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981
</page>
<note confidence="0.92204">
Fred J. Damerau Operating Statistics for the Transformational Question Answering System
</note>
<bodyText confidence="0.999988980952382">
processing did not reach a normal termination, usually
because of a machine failure, i.e., hardware or system
software, but sometimes because of a problem in the
TQA code. The USER COMMENTS line comes from
a feature we had included in the system in an attempt
to collect on-going user reaction to system behavior.
At the end of each question, before producing the
message &amp;quot;TYPE NEXT QUESTION&amp;quot;, we displayed a
request for comment on the preceding question. It
turned out that users found this something of a nui-
sance, since mostly they were satisfied with the an-
swer, so we made an early change to the terminal read
program such that a reply to the prompting message
for comments which ended in &amp;quot;?&amp;quot; would be treated as
a null comment and another question, in effect making
the comment optional. Student users took the prompt
somewhat more seriously than the regular employees
and made more comments. The two MESSAGE cate-
gories refer to a system facility enabling a user to send
messages to the TQA operator and the reverse. One
can see that it is usually used by the TQA operator as
a somewhat more convenient means than the tele-
phone for advising a user about the nature of a prob-
lem or for warning that the system was to be stopped,
and the like. PROGRAM ERROR is an error in the
TQA program which was detected and caused a query
to abort but left the system able to accept another
input. There is also a facility, tabulated under USER
CANCELLED, allowing a user to cancel a query at
any time by pressing a special key on the terminal
keyboard. This is used when the user realizes he made
a mistake or when processing seems to be going on too
long. The category LEXICAL CHOICE indicates
how often the system asked the user to clarify the
meaning of a word. For example, &amp;quot;area&amp;quot; can be
&amp;quot;parcel area&amp;quot; or &amp;quot;floor area&amp;quot; and if the system cannot
disambiguate by the context, the user is shown the two
choices and asked to &amp;quot;Type A or B&amp;quot;. The most fre-
quent ambiguities have to do with duplication of
names for streets, neighborhoods and schools.
As can be seen in line 1 of Table I, there was a
drastic fall-off in usage in the latter part of the year.
This has two primary causes. In the first place, the
planning office was being rebuilt during the period and
the work space was often disrupted. The major cause,
as mentioned above, was a re-orientation of the plan-
ning department activities by a newly appointed direc-
tor. The department members now spend the major
part of their time on administrative activities; planning
activities, like land analyses and drafting of new zon-
ing districts, are now carried out by off-premises con-
sultants who do not have ready access to the terminal.
From the latter part of 1978 through 1979, the system
was used only intermittently by the planners, who
occasionally needed some of the basic land record
data, and by others, like the fire department, who
wanted to have a list of tall buildings.
There is an obvious rise in USER COMMENTS,
LOOKUP FAILURES, and PARSING FAILURES
during June, July, and August. During this period, the
planning department had a number of student interns
who were using the system, sometimes in a play mode.
It is easy to understand that new users exploring the
system would have more than an ordinary number of
parsing failures. The increase in lookup failures is a
little more puzzling, although part of the increase has
the same cause, i.e., some of the questions were out-
side the domain, and consequently the words used
were not in the lexicon.
The large percentage for the NOTHING IN DATA
BASE category in January comes in large part from
two queries which had four underlying structures, all
of which resulted in the answer &amp;quot;NOTHING IN THE
DATA BASE.&amp;quot; This was caused by a problem in the
grammar which has since been corrected. Some of the
other instances of this message during the year result-
ed from an inadvertently, and unfortunately, successful
example of subtle user training or conditioning. We
discovered that users tended to respond to the system
by echoing, sometimes exactly and sometimes only
partially, what the system had printed to them, no
matter if their initial phrasing had been accepted or
not. Some input word sequences are treated as phras-
es by the strategy of scanning an input query against a
phrase lexicon first before looking in the regular lexi-
con. An entry like &amp;quot;gas station&amp;quot; came out of the
phrase lookup represented as &amp;quot;GAS_STATION&amp;quot; for
purposes of lexical lookup. The users would some-
times input that, or some variant like &amp;quot;Gas Station&amp;quot;,
which would be taken as a proper name. The result
would then be interpreted as a query having to do with
an owner named &amp;quot;Gas Station&amp;quot;. We now echo what
the user has typed, or some variant which will be ac-
ceptable if typed. Most of the instances of the catego-
ry NOTHING IN DATA BASE really are the response
to a request for information which is not in the data
base. Many of these are requests for information
about people who are not owners of property, or
about addresses which are not legitimate.
Apart from the drastic fall in usage at the end of
the year discussed above, there appears not to be any
trend in any of the other measures of system opera-
tion. The sequence of peaks and valleys may be char-
acteristic of systems like this, or may simply result
from insufficient time for the system to settle down.
</bodyText>
<subsectionHeader confidence="0.911462">
Operating Characteristics
</subsectionHeader>
<footnote confidence="0.625264">
Cumulative statistics for the system operating char-
acteristics are found in Tables III and IV and Figures
4 through 8 for each system component. The histo-
grams have a Poisson-like shape, with a single peak
</footnote>
<note confidence="0.958829">
American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 37
Fred J. Damerau Operating Statistics for the Transformational Question Answering System
</note>
<tableCaption confidence="0.992214">
Table III. Query/logical form time - No. of structures.
</tableCaption>
<table confidence="0.999976222222222">
1 2 3 4 5 &gt;
TOTAL YEAR
QUERY STRUCTURE TIME (SEC) 497 53 22 16 3
LOGICAL FORM TIME (SEC) 536 24 4 2 12
NO. OF SURFACE STRUCTURES 470 146 38 9 3
NO. OF UNDERLYING STRUCTURES 581 7 0 2 0
NO. OF QUERY STRUCTURES 581 7 0 2 0
NO. OF LOGICAL FORMS 571 5 0 2 0
NO. OF ANSWERS 513 0 0 0 0
</table>
<tableCaption confidence="0.963218">
Table IV. Query/logical form time - No. of structures by month.
</tableCaption>
<table confidence="0.9998266">
1 2 3 4 5&gt;
MAY
QUERY STRUCTURE TIME (SEC) 26 3 7 0 0
LOGICAL FORM TIME (SEC) 29 5 2 0 0
NO. OF SURFACE STRUCTURES 6 26 5 3 0
NO. OF UNDERLYING STRUCTURES 36 0 0 0 0
NO. OF QUERY STRUCTURES 36 0 0 0 0
NO. OF LOGICAL FORMS 36 0 0 0 0
NO. OF ANSWERS 28 0 0 0 0
JUNE
QUERY STRUCTURE TIME (SEC) 35 18 10 7 2
LOGICAL FORM TIME (SEC) 64 4 2 2 0
NO. OF SURFACE STRUCTURES 34 31 21 3 0
NO. OF UNDERLYING STRUCTURES 72 0 0 0 0
NO. OF QUERY STRUCTURES 72 0 0 0 0
NO. OF LOGICAL FORMS 72 0 0 0 0
NO. OF ANSWERS 66 0 0 0 0
JULY
QUERY STRUCTURE TIME (SEC) 58 10 1 1 0
LOGICAL FORM TIME (SEC) 69 0 0 0 0
NO. OF SURFACE STRUCTURES 62 12 1 0 0
NO. OF UNDERLYING STRUCTURES 70 0 0 0 0
NO. OF QUERY STRUCTURES 70 0 0 0 0
NO. OF LOGICAL FORMS 69 0 0 0 0
NO. OF ANSWERS 68 0 0 0 0
</table>
<bodyText confidence="0.996635235294118">
and a long tail. (The apparent spike at the right-hand
end of the &amp;quot;Number of lines in answer&amp;quot; is not real;
that point is really &amp;quot;answers of 50 lines or more&amp;quot;.
The right-most point of all the histograms should be
read in a similar way, i.e., as including the total of all
values in the remainder of the tail.)
Tables III and IV contain two kinds of information.
The first two lines under &amp;quot;Total Year&amp;quot; in Table III and
under each month in Table IV show the number of
queries whose machine processing time required the
number of seconds shown by the column head. Thus,
in Table III, 4 queries took 3 seconds of processing
time to generate a logical form. A table form was
used for this information because a histogram for
&amp;quot;time to produce a query structure&amp;quot; and &amp;quot;time to
produce a logical form&amp;quot; would have been simply a
spike. The other lines in the tables are counts, so that,
</bodyText>
<figure confidence="0.99330228">
QUERIES
1
1
2001
X
XX
XX
1501 XX
XX
XXX
XXX
XXX
1001 XXX
XXX
XXX
XXX
XXXX
501 XXXXX
1XXXXXX
IXXXXXXX
IXXXXXXXXXX
IXXXXXXXXXXXXXXXXXX XXXXX XXX X
51 101 151 201 251 301
&gt;
MINUTES
</figure>
<figureCaption confidence="0.997272">
Figure 4. Elapsed time - Total.
</figureCaption>
<figure confidence="0.596806666666667">
QUERIES
2501
1
IX
IX
IX
50IX
IX X
IX X x
IXXXX XXX X
IXXXX XXXXXXXXXXXXXXXXXXXXXXXX X X XX XX X XX
51 101 151 201 251 301 351 401 451 501
</figure>
<figureCaption confidence="0.996281">
Figure 5. Number of lines in answer - Total.
</figureCaption>
<bodyText confidence="0.9961308">
again in Table III, 7 queries had 2 underlying struc-
tures.
From the first two lines of Table III, it can be seen
that neither the query structure nor the logical form
account for very much of the processing time, mostly
</bodyText>
<page confidence="0.772409">
38 American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981
</page>
<note confidence="0.683148">
Fred J. Damerau Operating Statistics for the Transformational Question Answering System
</note>
<figure confidence="0.979997666666667">
QUERIES QUERIES
2001 1
4001
51 101 151 201 251 301
&gt;
SECONDS
</figure>
<figureCaption confidence="0.99811">
Figure 6. Generation of surface structures - Total.
</figureCaption>
<figure confidence="0.976865555555555">
QUERIES
1
1501
X
X
1001 X
XXXX
XXXXX
XXXXX
XXXXX
501 XXXXX
XXXXXXX
XXXXXXX XX
IXXXXXXXXXXX
1XXXXXXXXXXXXXXXXX XXXXX XXXXXX XX X
51 101 151 201 251 301 351 401 451 501
&gt;
SECONDS
</figure>
<figureCaption confidence="0.999936">
Figure 7. Transformational parsing - Total.
</figureCaption>
<bodyText confidence="0.98072325">
less than 1 second, and Figure 8 shows that answer
processing, i.e., data base lookup and printing, is also
not a major time user. From Figures 6 and 7 it can be
seen that surface structure parsing, which includes the
</bodyText>
<figure confidence="0.994282833333333">
Ix
Ix
350 IX
IX
IX
501X
IX
I XX
1 XXXX
I XXXXXXXXXXXXX XXX X XXXX X
51 101 151 201 251 301
SECONDS
</figure>
<figureCaption confidence="0.999985">
Figure 8. Answer processing - Total.
</figureCaption>
<bodyText confidence="0.999571827586207">
time for string transformations, and transformational
processing both typically consume around 4 seconds.
Therefore, total machine time for a typical query is
around 10 seconds, although extreme cases can take
much longer.
From Figure 4, it can be seen that elapsed time for
a query is around 3 minutes, although there is again a
long tail. Elapsed time depends primarily on machine
load and user behavior at the terminal. The computer
on which the system operated was an IBM 370/168
with an attached processor, 8 megabytes of memory
and extensive peripheral storage, operating under the
VM/370 time-sharing system. During business hours,
there were usually in excess of 200 users logged on to
the system, so any one user received only a small
share of the resources. Besides queuing for the CPU
and memory, this system developed queues on an IBM
3850 Mass Storage System, on which the TQA data
base was stored. With all delays accounted for, it
could easily take several minutes of elapsed time to
accumulate 10 or 15 seconds of CPU time.
User-caused delays tend to occur when a reply is
needed to correct a lookup failure or to resolve an
ambiguity, and when the answer to a query requires
more than one screen for display. In the latter case,
the display has a built in delay of up to one minute, or
can be held indefinitely by depressing a key. The hold
feature is often used for the long displays, because the
user is writing down information from the display.
</bodyText>
<note confidence="0.909194">
American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 39
Fred J. Damerau Operating Statistics for the Transformational Question Answering System
</note>
<bodyText confidence="0.999957652173913">
The user is, of course, not concerned with his own
delays, but only with the system delay. In order to
keep him informed of progress through the query, and
give him assurance that the computer is still operating,
a CPU time clock is displayed in the lower right corner
of the screen, along with an indication of the process-
ing phase being carried out at that time. In general,
these users did not become concerned with response
time, possibly because they had no experience with
other interactive computer systems. However, if the
system was slow, a user might well choose to look up a
single piece of information in the printed listing of the
data base rather than ask through the computer sys-
tem.
Figure 5 shows that most queries have a one line
answer, but that the tail is very long. Generally
speaking, users were interested in totals or averages of
data fields like &amp;quot;dwelling units&amp;quot; or &amp;quot;parcel area&amp;quot; for
aggregates of parcels specified by land use, neighbor-
hood, census tract, and the like, often in combination.
Questions of this kind are not readily answered from
the printed copies of the parcel file, and specific pat-
terns do not occur often enough to make it worthwhile
to ask for a batch program to be written by the City
computer staff.
The yearly summary statistics discussed above con-
ceal the considerable variation which can be encoun-
tered over shorter time periods. Table IV shows the
monthly statistics, corresponding to Table III for May,
June and the more typical July. The percentage of
queries with more than one surface structure is clearly
very high in May and June. This results in longer
average times for query structure processing and logi-
cal form processing, as seen also in Table IV. The
effect is even more obvious in the histograms for sur-
face structure processing, Figures 9-11, and underlying
structure processing, Figures 12-14. Inspection of the
logs for May and June shows that most of this effect
results from only a few questions, e.g., &amp;quot;What is the
area of the x family houses in the y zone?&amp;quot;, repeated
for a number of combinations of x and y. It happens
that one of the planners was checking figures to be
included in a table in a large report during this period.
Sequences of questions like this are, of course, of little
help in system development, but do serve to build
confidence in the utility of the system.
</bodyText>
<subsectionHeader confidence="0.92367">
The Input Queries
</subsectionHeader>
<bodyText confidence="0.999787875">
Figure 1 given earlier shows a small selection of the
queries put to the system during 1978, to give the
reader an idea of the kinds of questions asked by the
users. Additionally, there are four lengthy appendices
which appear only in the microfiche supplement to this
issue of the Journal. Appendix A presents the entire
set of queries submitted to the system during 1978,
(but note the caveat at the beginning regarding possi-
</bodyText>
<figure confidence="0.993573222222222">
QUERIES
15
X
X
X
101 X
X
X
X
XX
51 XXX
XXX X
XXX X
XXX X X
XXXXXX XXXX X X X XX
51 101 151 201 251 301
&gt;
SECONDS
</figure>
<figureCaption confidence="0.999684">
Figure 9. Generation of surface structures - May.
</figureCaption>
<figure confidence="0.987648538461539">
QUERIES
10.
XXX
XXXX X
XXXXX X
SI XXXXXX X
XXXXXX XX
XXXXXXXXXX XXX XX
XXXXXXXXXXX XXX X XX
XXXXXXXXXXXXXXXXX XXX XXX
51 101 151 201 251 301
&gt;
SECONDS
</figure>
<figureCaption confidence="0.999901">
Figure 10. Generation of surface structures - June.
</figureCaption>
<bodyText confidence="0.999695428571429">
ble missing data because of lost logs). Sentences pre-
ceded by a C in Appendix A were completed success-
fully, but one must remember that &amp;quot;NOTHING IN
THE DATA BASE&amp;quot; was counted as a successful an-
swer by the data reduction program even though there
may have been some earlier problem in the query.
Appendix B is a list of sentences which did not parse
</bodyText>
<page confidence="0.675714">
40 American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981
</page>
<note confidence="0.728534">
Fred J. Damerau Operating Statistics for the Transformational Question Answering System
</note>
<figure confidence="0.851368161290323">
QUERIES QUERIES
1
1
1
1 15
301
1 X IX
X
X X
101 X X
151 X I X X
X I XX X X
X I XXX X X
X I XXX X X
XX 51 XXX X X
101 XXX I XXX X X X
XXXX I XXX XX XX X X
XXXX I XXXXXXXXXX X X XX X
XXXX 1XXXXXXXXXXXX XXXX X X XXXXXX XX X
XXXX
SI XXXXX 51 101 151 201 251 301 351 401 451 501
XXXXXX
&gt;
XXXXXXX SECONDS
X X X X X X X X Figure 13. Transformational parsing - June.
X X X X X X XXX x
51 101 151 201 251 301
&gt;
SECONDS
QUERIES
201
</figure>
<figureCaption confidence="0.999919">
Figure 11. Generation of surface structures - July.
</figureCaption>
<table confidence="0.90819465">
QUERIES X
1
251 151 X
1 I XX
1 I XX
1 I XXX
1 X IXXX
201 X 101 XXX
XXX X
XXXXX
XXXXX
1 X I XXXXX
51 X 51 XXXXX
1 X I XXXXX X
1 X X I XXXXXX
/
1 X X X X X X I XXXXXXXXXX X
1 X XX XXX X X XX XX I XXXXXXXXXXXX
51 101 151 201 251 301 351 401 451 501 51 101 151 201 251 301 351 401 451 501
&gt; &gt;
</table>
<sectionHeader confidence="0.435601" genericHeader="categories and subject descriptors">
SECONDS SECONDS
</sectionHeader>
<figureCaption confidence="0.587151">
Figure 12. Transformational parsing - May. Figure 14. Transformational parsing - July.
American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 41
Fred J. Damerau Operating Statistics for the Transformational Question Answering System
</figureCaption>
<bodyText confidence="0.999951321428572">
at their first submission, and Appendix C is a list
which failed for some reason other than parsing at
their first submission. Appendix D is a list of sen-
tences for which the answer was &amp;quot;NOTHING IN THE
DATA BASE.&amp;quot; In most cases where this answer was
not correct, the problem was in dealing with searches
on persons&apos; names. The treatment used presently is
still not completely satisfactory, but has been im-
proved since the initial version. Names in the data
base occur in a great variety of patterns, more than we
felt worth devising procedures to handle. In the three
latter appendices, those sentences which are satisfacto-
rily answered by the system of May 14, 1979 are pre-
ceded by an X. (Any necessary spelling corrections or
ambiguity resolution were supplied.) A little over 60
percent of the failing sentences are processed correctly
by this version. As can be seen, many of the remain-
ing queries are so flawed that no system should be
expected to answer them.
The set of successful queries is not really indicative
of the full coverage of the system. There are a num-
ber of permissible constructions which the planners
simply never used, so the subset is somewhat richer
than shown. On the other hand there are large num-
bers of constructions involving personal pronouns,
three-argument comparatives, quantification with
&amp;quot;each&amp;quot;, etc., which the planners knew the system did
not handle, and which they consequently did not use.
</bodyText>
<subsectionHeader confidence="0.505077">
Conclusions
</subsectionHeader>
<bodyText confidence="0.999992852941177">
The motivation for publishing this paper with its
long appendices was to make it possible for a reader to
come to some conclusion of his own regarding the
utility of the TQA system or something like it in a real
environment. For reasons mentioned above, none of
us thinks this experiment provides a definitive test for
our system or even that very strong claims about per-
cent of success can be made. For such purposes, a
controlled experiment using a fixed system is clearly
necessary, and we hope to make such a test in the near
future. However, those of us working on the project
are encouraged by the results summarized here, and
even more by our conversations with the users of the
system, who have been very positive. Within the limi-
tations established by the subset of English that the
system can recognize, it appears that non-data proc-
essing professionals can extract useful information
from their files with almost no training. A number of
the gaps in our system can certainly be plugged if we
can find the resources to work on them.
Our next step will be to make the TQA system a
front end to an existing formal query language system,
probably SQL-based System R, developed at the IBM
San Jose Research Laboratory, a version of which has
been announced as an IBM Program Product for use
under the DOS/VS operating system. Such a step will
permit us to devote all of our resources to the prob-
lems of language and knowledge representation, in-
stead of spending part of that effort on data manage-
ment. Beyond that, we will consider seriously the
question of moving to different environments, both to
new situations in the city planning domain and to com-
pletely different domains, without having to rewrite all
or even a major portion of our base system.
</bodyText>
<sectionHeader confidence="0.981826" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996406375">
Besides Petrick, Plath and the author, the TQA
project currently includes Mark Pivovonsky, who has
done the systems programming for the project. Since
this document summarizes one year&apos;s activity, it neces-
sarily reflects the content of many discussions with
these individuals, and some of their opinions and in-
sights on the nature of these problems are incorporat-
ed herein. The wrong ones naturally are mine.
</bodyText>
<sectionHeader confidence="0.994909" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999652421052632">
Astrahan, M.M.; Blasgen, M.W.; Chamberlin, D.D.; Eswaran, K.P.;
Gray, J.N.; Griffiths, P.P.; King, W.F.; Lone, R.A.; McJones,
J.; Mehl, J.W.; Putzolu, G.R.; Traiger, IL.; Wade, B.W.; Wat-
son, V. (1976). System R: Relational Approach to Database
Management. ACM Transactions on Database Systems, 1 (21),
pp. 97-137.
Damerau, Fred J. (1977). Advantages of a Transformational
Grammar for Question Answering. Proceedings of the Fifth
International Joint Conference on Artificial Intelligence, vol 1, p.
192.
Damerau, Fred J. (1978). The Derivation of Answers from Logical
Forms in a Question Answering System. American Journal of
Computational Linguistics, Microfiche 75, pp. 3-42.
Krause, Juergen. (1979). Results of a User Study with the &apos;User
Specialty Languages&apos; System and Consequences for the Archi-
tecture of Natural Language Interfaces. Technical Report
79.04.003, IBM Heidelberg Scientific Center, May 1979.
Petrick, Stanley R. (1977). Semantic Interpretation in the Request
System. In Computational and Mathematical Linguistics, Pro-
ceedings of the International Conference on Computational Linguis-
tics, Pisa, pp. 585-610.
Plath, Warren J. (1973). Transformational Grammar and Transfor-
mational Parsing in the REQUEST System. IBM Research
Report RC 4396, Thomas J. Watson Research Center, Yorktown
Heights, N.Y.
Plath, Warren J. (1974). String Transformations in the REQUEST
System. American Journal of Computational Linguistics, Micro-
fiche 8.
Plath, Warren J. (1976). REQUEST: A Natural Language
Question-Answering System. IBM Journal of Research and
Development, 20:4, (July 1976), pp. 326-335.
Robinson, Jane J. (1973). An Inverse Transformational Lexicon.
In Natural Language Processing, Randall Rustin, ed., Algorithm-
ics Press, Inc., New York, pp. 43-60.
Fred J. Damerau is a Research Staff Member at the
IBM Thomas J. Watson Research Center in Yorktown
Heights, New York. He received the Ph.D. degree in
linguistics from Yale University in 1966.
</reference>
<page confidence="0.94035">
42 American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.273341">
<title confidence="0.9999065">Operating Statistics for The Transformational Question Answering System</title>
<author confidence="0.999998">Fred J Damerau</author>
<affiliation confidence="0.937764">Mathematical Sciences IBM Thomas J. Watson Research Post Office Box</affiliation>
<address confidence="0.866564">Yorktown Heights, New York 10598</address>
<note confidence="0.605448">This paper presents a statistical summary of the use of the Transformational Question Answering (TQA) system by the City of White Plains Planning Department during the year</note>
<abstract confidence="0.9553046">1978. A complete record of the 788 questions submitted to the system that year is included, as are separate listings of some of the problem inputs. Tables summarizing the performance of the system are also included and discussed. In general, performance of the system was sufficiently good that we believe that the approach being followed is a viable one, and are continuing to develop and extend the system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M M Astrahan</author>
<author>M W Blasgen</author>
<author>D D Chamberlin</author>
<author>K P Eswaran</author>
<author>J N Gray</author>
<author>P P Griffiths</author>
<author>W F King</author>
<author>R A Lone</author>
<author>J McJones</author>
<author>J W Mehl</author>
<author>G R Putzolu</author>
<author>IL Traiger</author>
<author>B W Wade</author>
<author>V Watson</author>
</authors>
<title>System R: Relational Approach to Database Management.</title>
<date>1976</date>
<journal>ACM Transactions on Database Systems,</journal>
<volume>1</volume>
<issue>21</issue>
<pages>97--137</pages>
<contexts>
<context position="19035" citStr="Astrahan, et al., 1976" startWordPosition="3299" endWordPosition="3302">valuated, producing an answer to the English input query. Referring to Figure 3a, the logical form can be read as: Find the set of things X11 such that for at least 1 of the things X63 in the set of things X4 where the traffic zone of X4 is 579 and the LUC (land use code) is 0553, (viz., the set of account numbers having both these properties), the address of X63 is X11. The process of answer extraction from the data base is accomplished by a combination of LISP and PL/I programs (Damerau 1978), and an experimental relational data base management system called Relational Storage System (RSS) (Astrahan, et al., 1976). The RSS provides the capability to generate a data base of n-ary relations, with indexes on any field of the relation, and low-level access commands like OPEN, NEXT, CLOSE, with appropriate parameters, to retrieve information from such a data base. This particular data base had just one relation of 40 columns. The LISP programs examine the logical form to establish relationships between variables and to generate requests to the data base component to find items with specified properties. In the example, one retrieval request would find the qualifying account numbers, i.e., the X4s, and a sec</context>
</contexts>
<marker>Astrahan, Blasgen, Chamberlin, Eswaran, Gray, Griffiths, King, Lone, McJones, Mehl, Putzolu, Traiger, Wade, Watson, 1976</marker>
<rawString>Astrahan, M.M.; Blasgen, M.W.; Chamberlin, D.D.; Eswaran, K.P.; Gray, J.N.; Griffiths, P.P.; King, W.F.; Lone, R.A.; McJones, J.; Mehl, J.W.; Putzolu, G.R.; Traiger, IL.; Wade, B.W.; Watson, V. (1976). System R: Relational Approach to Database Management. ACM Transactions on Database Systems, 1 (21), pp. 97-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred J Damerau</author>
</authors>
<title>Advantages of a Transformational Grammar for Question Answering.</title>
<date>1977</date>
<booktitle>Proceedings of the Fifth International Joint Conference on Artificial Intelligence,</booktitle>
<volume>1</volume>
<pages>192</pages>
<contexts>
<context position="15616" citStr="Damerau 1977" startWordPosition="2672" endWordPosition="2673"> ((V) ((V) ((GAS_STATION))) ((=S17)) ) ((INDEX ((- CONST))) ((X4)) ) ) ) ) ) ((PP4X) ((PP) ((NSPREP) ((IN))) ((RPX) ((NP) ((PROPNOM) ((NOUN ((- HU) (+ SG) (+ PL) (+ PAG) (+ GEO))) ((V ((+ OGEN) (+ POBJ))) ((TRAFFIC_ZONE)) ) ((INDEX ((+ CONST))) ((VADJ ((+ ADJ) (+ CARD) (+ D3))) ((579)) ) ) ) ) ) ) ) ) ((S2PCX) ((PUNCT ((+ QUES))) ((?)) ) ) ) Figure 3e. Surface structure tree 2. may not match a particular data base organization. There are many approaches one could take to make this match. We have chosen to implement the matching function as a separate transformational component in the grammar (Damerau 1977). The underlying structure itself is therefore input to the transformational recognizer, using a (small) set of grammar rules tailored to a specific data base and produces a query structure. Query structures are similar to underlying structures in form, but reflect the particular meaning I. (BD LOCATED (THE (((GAS_STATION =S17) X4) (* BD LOCATED X4 (TRAFFIC_ZONE 579) BD *))) ((WH SOME) (PLACE X11)) BD) ((Si ((- PAST) (+ WH) (+ QUES) (+ TOP))) ((BD)) ((V ((+ ADJ) (+ EN) (+ LOC) (+ TEMP) (+ PSUBJ) (+ POBJ))) ((LOCATED)) ) ((NP) ((DET) ((THE))) ((NOM) ((NOM) ((NOUN ((- HU) (- SG) (+ PL) (+ SV))) </context>
</contexts>
<marker>Damerau, 1977</marker>
<rawString>Damerau, Fred J. (1977). Advantages of a Transformational Grammar for Question Answering. Proceedings of the Fifth International Joint Conference on Artificial Intelligence, vol 1, p. 192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred J Damerau</author>
</authors>
<title>The Derivation of Answers from Logical Forms in a Question Answering System.</title>
<date>1978</date>
<journal>American Journal of Computational Linguistics, Microfiche</journal>
<volume>75</volume>
<pages>3--42</pages>
<contexts>
<context position="18911" citStr="Damerau 1978" startWordPosition="3283" endWordPosition="3284"> a logical form. A logical form can best be thought of, in our context, as a retrieval expression which is to be evaluated, producing an answer to the English input query. Referring to Figure 3a, the logical form can be read as: Find the set of things X11 such that for at least 1 of the things X63 in the set of things X4 where the traffic zone of X4 is 579 and the LUC (land use code) is 0553, (viz., the set of account numbers having both these properties), the address of X63 is X11. The process of answer extraction from the data base is accomplished by a combination of LISP and PL/I programs (Damerau 1978), and an experimental relational data base management system called Relational Storage System (RSS) (Astrahan, et al., 1976). The RSS provides the capability to generate a data base of n-ary relations, with indexes on any field of the relation, and low-level access commands like OPEN, NEXT, CLOSE, with appropriate parameters, to retrieve information from such a data base. This particular data base had just one relation of 40 columns. The LISP programs examine the logical form to establish relationships between variables and to generate requests to the data base component to find items with spe</context>
</contexts>
<marker>Damerau, 1978</marker>
<rawString>Damerau, Fred J. (1978). The Derivation of Answers from Logical Forms in a Question Answering System. American Journal of Computational Linguistics, Microfiche 75, pp. 3-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juergen Krause</author>
</authors>
<title>Results of a User Study with the &apos;User Specialty Languages&apos; System and Consequences for the Architecture of Natural Language Interfaces.</title>
<date>1979</date>
<tech>Technical Report 79.04.003,</tech>
<institution>IBM Heidelberg Scientific Center,</institution>
<contexts>
<context position="1060" citStr="Krause (1979)" startWordPosition="164" endWordPosition="165">ystem that year is included, as are separate listings of some of the problem inputs. Tables summarizing the performance of the system are also included and discussed. In general, performance of the system was sufficiently good that we believe that the approach being followed is a viable one, and are continuing to develop and extend the system. Introduction Natural language question answering systems have been the subject of much research over approximately 20 years. In only very few cases have such systems been exposed to real users trying to solve real problems, another example perhaps being Krause (1979). In an attempt to see if a useful natural language query system could be built for an application which existed independently of the research program, an approach was made to the Planning Department of the City of White Plains asking them to take part in such an experiment. Their incentive was the free use of an interactive query facility which would allow them to explore their data base more freely than the batch computer facility run by the city could do. The remainder of the paper describes the user environment, describes the operation of the TQA system, and discusses the operating results</context>
</contexts>
<marker>Krause, 1979</marker>
<rawString>Krause, Juergen. (1979). Results of a User Study with the &apos;User Specialty Languages&apos; System and Consequences for the Architecture of Natural Language Interfaces. Technical Report 79.04.003, IBM Heidelberg Scientific Center, May 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley R Petrick</author>
</authors>
<title>Semantic Interpretation in the Request System. In</title>
<date>1977</date>
<booktitle>Computational and Mathematical Linguistics, Proceedings of the International Conference on Computational Linguistics,</booktitle>
<pages>585--610</pages>
<location>Pisa,</location>
<contexts>
<context position="18287" citStr="Petrick 1977" startWordPosition="3165" endWordPosition="3166">V) ((V) ((GAS_STATION))) ((LUC) ((553))) ) ((INDEX ((- CONST))) ((X4)) ) ) ) ((Si) ((BD)) ((V ((+ ADJ) (+ EN) (+ LOC) (+ TEMP) (+ PSUBJ) (+ POBJ))) ((TRAFFIC_ZONE)) ) ((NP ((+ IN) (+ LOC))) ((NOM) ((NOUN ((- HU) (+ SG) (+ PL) (+ PAG) (+ GEO))) ((INDEX ((+ CONST))) ((V ((+ ADJ) (+ CARD) (+ 03))) ((579)) ) ) ) ) ) ((NP) ((NOM) ((NOUN ((- HU) (- SG) (+ PL) (+ SV))) ((INDEX ((- CONST))) ((X4)) ) ) ) ) ((BD)) ) ) ) ((BD)) ) Figure 3g. Query structure. predicate ADDRESS and the second instance to the predicate TRAFFIC_ZONE. The query structure tree is processed by a Knuthstyle semantic interpreter (Petrick 1977), producing a logical form. A logical form can best be thought of, in our context, as a retrieval expression which is to be evaluated, producing an answer to the English input query. Referring to Figure 3a, the logical form can be read as: Find the set of things X11 such that for at least 1 of the things X63 in the set of things X4 where the traffic zone of X4 is 579 and the LUC (land use code) is 0553, (viz., the set of account numbers having both these properties), the address of X63 is X11. The process of answer extraction from the data base is accomplished by a combination of LISP and PL/I</context>
</contexts>
<marker>Petrick, 1977</marker>
<rawString>Petrick, Stanley R. (1977). Semantic Interpretation in the Request System. In Computational and Mathematical Linguistics, Proceedings of the International Conference on Computational Linguistics, Pisa, pp. 585-610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Warren J Plath</author>
</authors>
<title>Transformational Grammar and Transformational Parsing in the REQUEST System.</title>
<date>1973</date>
<journal>IBM Research Report RC 4396, Thomas J. Watson Research</journal>
<location>Center, Yorktown Heights, N.Y.</location>
<contexts>
<context position="11308" citStr="Plath 1973" startWordPosition="1916" endWordPosition="1917">k done in the transformational cycle. Referring to Figure 3c, note that &amp;quot;579&amp;quot; has been incorporated with &amp;quot;traffic zone&amp;quot; under a single node, PROPNOM. The resulting list of trees is input to a context free parser, which produces a set of surface trees. In the example, two surface trees are produced, shown in Figures 3d and 3e. The trees differ in the point of attachment of the phrase &amp;quot;in traffic zone 579&amp;quot;. In structure 1, it is attached to the NP &amp;quot;the gas station&amp;quot;, and in structure 2 it is directly under the S node. The recognizer attempts to find an underlying structure for each surface tree (Plath 1973, 1976). Typically, only one of a set of surface trees will result in an underlying structure. In the example, the struc32 American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 Fred J. Damerau Operating Statistics for the Transformational Question Answering System ((AT ((WH SOME) (PLACE X11))) BE THE ((GAS_STATION =S17) X4) IN (TRAFFIC_ZONE 579) ?) ((ST) ((PP) ((NSPREP) ((AT))) UNP) ((NOM) ((V ((+ ADJ) (+ QUANT))) ((WH)) ((SOME)) ) ((NOM) ((NOUN ((+ SG) (- HU) (+ P1))) ((V) ((PLACE))) ((INDEX ((- CONST))) ((X11)) ) ) ) ) ) ) ((BAUx ((- PAST) (- SG))) ((BE)) ) ((</context>
<context position="14110" citStr="Plath (1973" startWordPosition="2410" endWordPosition="2411">DEX ((- CONST))) ((X11)) ) ) ) ) ) ) ((BAUx ((- PAST) (- SG))) ((BE)) ) ((NP) ((DETX) ((DET) ((THE))) ) ((NOMX) ((NOMZ) ((NOM) ((NOUN ((- HU) (- SG) (+ PL) (+ SV))) ((V) ((V) ((GAS_STATION))) ((=S17)) ) ((INDEX ((- CONST))) ((X4)) ) ) ) ((Z1) UPp3X) ((PP) ((NSPREP) ((IN))) ((RPX) ((NP) ((PROPNOM) ((NOUN ((- HU) (+ SG) (+ P1) (+ PAG) (+ GEO))) ((V ((+ OGEN) (+ POBJ))) ((TRAFFIC_ZONE)) ) ((INDEX ((+ CONST))) ((vADJ ((+ ADJ) (+ CARD) (+ D3))) ((579)) ) ) ) ) ) ) ) ) ) ) ) ) ((PUNCT ((+ QUES))) ((?)) ) ) Figure 3d. Surface structure tree 1. instances of &amp;quot;located&amp;quot;. A fuller explanation is given in Plath (1973, 1976). The data base has no predicate, i.e., column heading, named &amp;quot;located&amp;quot;. Even if it did, the two LOCATEDs in the underlying structures are different, one for address and one for traffic zone. The underlying structure is designed to reflect the linguistic structure of the query, but, as in this case, that structure American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 33 Fred J. Damerau Operating Statistics for the Transformational Question Answering System 2. ((AT ((WH SOME) (PLACE X11))) BE (THE ((GAS_STATION =SI7) X4)) (IN (TRAFFIC_ZONE 579)) ?) ((Si) (</context>
</contexts>
<marker>Plath, 1973</marker>
<rawString>Plath, Warren J. (1973). Transformational Grammar and Transformational Parsing in the REQUEST System. IBM Research Report RC 4396, Thomas J. Watson Research Center, Yorktown Heights, N.Y.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Warren J Plath</author>
</authors>
<title>String Transformations in the REQUEST System.</title>
<date>1974</date>
<journal>American Journal of Computational Linguistics, Microfiche</journal>
<volume>8</volume>
<contexts>
<context position="10460" citStr="Plath (1974)" startWordPosition="1769" endWordPosition="1770"> ((X0)) ) ) ) ) ) ) (are ( ((BAUX ((- PAST) (- SG))) ((BE)) ) ) ) (the (((DET) ((THE)))) ) (gas_station ( ((NOM) ((NOUN ((- HU) (- SG) (+ (+ SV))) ((V) ((V) ((GAS_STATION))) ((=S17)) ) ((INDEX ((- CONST)))) ) ) ) ) (in (((PREP) ((IN)))) ) (traffic_zone ( ((NOM) ((NOUN ((- HU) (+ SG) (+ (+ PAG) (+ GEO))) ((V ((+ OGEN) (+ POBJ))) ((TRAFFIC_ZONE)) ) ((INDEX ((- CONST) (+ CINS)))) ) ) ) ) (579 ( ((VADJ ((+ ADJ) (+ CARD) (+ D3))) ((579)) ) ) ) (? ( ((PUNCT ((+ QUES))) ((?)) ) ) ) ) Figure 3b. List of lexical trees. The list of lexical trees is input to a set of string transformations, described in Plath (1974). These transformations operate on adjacent lexical items to deal with patterns of classifiers, ordinal numbers, stranded prepositions, and the like. The effect of this phase is to reduce the number of surface parses and the amount of work done in the transformational cycle. Referring to Figure 3c, note that &amp;quot;579&amp;quot; has been incorporated with &amp;quot;traffic zone&amp;quot; under a single node, PROPNOM. The resulting list of trees is input to a context free parser, which produces a set of surface trees. In the example, two surface trees are produced, shown in Figures 3d and 3e. The trees differ in the point of a</context>
</contexts>
<marker>Plath, 1974</marker>
<rawString>Plath, Warren J. (1974). String Transformations in the REQUEST System. American Journal of Computational Linguistics, Microfiche 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Warren J Plath</author>
</authors>
<title>REQUEST: A Natural Language Question-Answering System.</title>
<date>1976</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>20</volume>
<pages>326--335</pages>
<marker>Plath, 1976</marker>
<rawString>Plath, Warren J. (1976). REQUEST: A Natural Language Question-Answering System. IBM Journal of Research and Development, 20:4, (July 1976), pp. 326-335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane J Robinson</author>
</authors>
<title>An Inverse Transformational Lexicon.</title>
<date>1973</date>
<booktitle>In Natural Language Processing,</booktitle>
<pages>43--60</pages>
<editor>Randall Rustin, ed.,</editor>
<publisher>Algorithmics Press, Inc.,</publisher>
<location>New York,</location>
<contexts>
<context position="9302" citStr="Robinson (1973)" startWordPosition="1546" endWordPosition="1547">BD) QUERY STRUCTURES: 1. (BD ADDRESS ((WH SOME) (THING X11)) (THE (((GAS_STATION 553) X4) (* BD TRAFFIC_ZONE 579 X4 BD *))) BD) LOGICAL FORM: (setx &apos;Xll &apos;(foratleast @1 &apos;X63 (setx &apos;X4 &apos;(and (testfct @579 &apos;(TRAFZ X4 &apos;@1976) (testfct 0553 TUC X4 &apos;@1976) &apos;(testfct X11 (ADDRESS X63 &apos;@1976) ) ) ) ANSWER: ADDRESS 1976 2 06300 02000 122 S LEXINGTON AV 2 06600 00100 101 W POST RD 2 05500 09300 102 W POST RD 2 07100 03300 109 W POST RD 2 07100 02900 115 W POST RD Figure 3a. Short trace of example question showing major intermediate structures. in its detail but still valid in main outline, is given in Robinson (1973). Without going into great detail, one can see in Figure 3b that &amp;quot;gas station&amp;quot; and &amp;quot;traffic zone&amp;quot; have been made into single units. The node =S17 in the entry for gas station is a macro standing for a bundle of semantic features. Many of the feature names should be obvious, but, e.g., PL stands for &amp;quot;place&amp;quot;, PAG for parcel aggregate, i.e., an aggregate of separate parcels, and CINS for &amp;quot;cardinal insertion&apos;, i.e., can be followed by a cardinal number. ( (where ( ((RP ((+ WH) (+ LOC))) ((NOM) ((NOUN ((+ RED (- HU) (+ PL))) ((INDEX ((- CONST))) ((X0)) ) ) ) ) ) ) (are ( ((BAUX ((- PAST) (- SG))) (</context>
</contexts>
<marker>Robinson, 1973</marker>
<rawString>Robinson, Jane J. (1973). An Inverse Transformational Lexicon. In Natural Language Processing, Randall Rustin, ed., Algorithmics Press, Inc., New York, pp. 43-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fred</author>
</authors>
<title>Damerau is a Research Staff Member at the IBM Thomas J. Watson Research Center in Yorktown Heights,</title>
<date>1966</date>
<location>New York.</location>
<marker>Fred, 1966</marker>
<rawString>Fred J. Damerau is a Research Staff Member at the IBM Thomas J. Watson Research Center in Yorktown Heights, New York. He received the Ph.D. degree in linguistics from Yale University in 1966.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>