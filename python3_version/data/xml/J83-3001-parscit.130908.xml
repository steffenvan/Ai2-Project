<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.523066666666667">
Recovery Strategies for
Parsing Extragrammatical Languagel
Jaime G. Carbonell and Philip J. Hayes
Computer Science Department
Carnegie-Mellon University
Pittsburgh, PA 15213
</title>
<bodyText confidence="0.975819428571429">
Practical natural language interfaces must exhibit robust behaviour in the presence of
extragrammatical user input. This paper classifies different types of grammatical deviations
and related phenomena at the lexical, sentential and dialogue levels and presents recovery
strategies tailored to specific phenomena in the classification. Such strategies constitute a
tool chest of computationally tractable methods for coping with extragrammaticality in
restricted domain natural language. Some of the strategies have been tested and proven
viable in existing parsers.
</bodyText>
<sectionHeader confidence="0.990181" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.979604580645161">
Any robust natural language interface must be capable
of processing input utterances that deviate from its
grammatical and semantic expectations. Many re-
searchers have made this observation and have taken
initial steps towards coverage of certain classes of
extragrammatical constructions. Since robust parsers
must deal primarily with input that does meet their
expectations, the various efforts at coping with extra-
grammaticality have been generally structured as ex-
tensions to existing parsing methods. Probably the
most popular approach has been to extend
syntactically-oriented parsing techniques employing
Augmented Transition Networks (ATNs) (Kwasny and
Sondheimer 1981, Weischedel and Sondheimer 1984,
Weischedel and Black 1980, Woods et al. 1976). Oth-
er researchers have attempted to deal with ungrammat-
ical input through network-based semantic grammar
techniques (Hendrix 1977), through extensions to
pattern matching parsing in which partial pattern
matching is allowed (Hayes and Mouradian 1981),
through conceptual case frame instantiation (Dejong
1979, Schank, Lebowitz, and Birnbaum 1980), and
through approaches involving multiple cooperating
parsing strategies (Carbonell and Hayes 1984, Carbo-
nell et al. 1983, Hayes and Carbonell 1981).
This research was sponsored in part by the Air Force Office
of Scientific Research under Contract AFOSR-82-0219 and in part
by Digital Equipment Corporation as part of the XCALIBUR
project.
Given the background of existing work, this paper
focuses on three major objectives:
</bodyText>
<listItem confidence="0.849283">
1. to create a taxonomy of possible grammatical devi-
ations covering a broad range of extragrammaticali-
ties, including some lexical and discourse phenom-
ena (for example, novel words and dialogue level
ellipsis) that can be handled by the same mecha-
nisms that detect and process true grammatical
errors;
2. to outline strategies for processing many of these
</listItem>
<bodyText confidence="0.983405">
deviations — some of these strategies have been
presented in our earlier work, some are similar to
strategies proposed by other researchers, and some
have never been analyzed before;
</bodyText>
<listItem confidence="0.92610825">
3. to assess how easily these strategies can be em-
ployed in conjunction with several of the existing
approaches to parsing ungrammatical input, and to
examine why mismatches arise.
</listItem>
<bodyText confidence="0.921594866666667">
The overall result should be a synthesis of different
parse-recovery strategies organized by the grammatical
phenomena they address (or violate), an evaluation of
how well the strategies integrate with existing ap-
proaches to parsing extragrammatical input, and a set
of characteristics desirable in any parsing process deal-
ing with extragrammatical input. We hope this will aid
researchers designing robust natural language interfac-
es in two ways:
1. by providing a tool chest of computationally ef-
fective approaches to cope with extragrammatical-
ity;
Copyright 1984 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted
provided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included on
the first page. To copy otherwise, or to republish, requires a fee and/or specific permission.
</bodyText>
<page confidence="0.403951">
0362-613X/83/030123-24$03.00
</page>
<note confidence="0.935948">
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 123
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.998476301587302">
2. by assisting in the selection of a basic parsing
methodology in which to embed these recovery
techniques.
In assessing the degree of compatibility between
recovery techniques and various approaches to parsing,
we avoid the issue of whether a given recovery tech-
nique can be used with a specific approach to parsing.
The answer to such a question is almost always affirm-
ative. Instead, we are concerned with how naturally
the recovery strategies fit with the various parsing
approaches. In particular, we consider the computa-
tional tractability of the recovery strategies and how
easily they can obtain the information they need to
operate in the context of different parsing approaches.
The need for robust parsing is greatest for interac-
tive natural language interfaces that have to cope with
language produced spontaneously by their users. Such
interfaces typically operate in the context of a well-
defined, but restricted, domain in which strong seman-
tic constraints are available. In contrast, text process-
ing often operates in domains that are semantically
much more open-ended. However, the need to deal
with extragrammaticality is much less pronounced in
text processing, since texts are normally carefully pre-
pared and edited, eliminating most grammatical errors
and suppressing many dialogue phenomena that pro-
duce fragmentary utterances. Consequently, we shall
emphasize recovery techniques that exploit and depend
on strong semantic constraints. In some cases, it is
unclear whether the techniques we discuss will scale
up properly to unrestricted text or discourse, but even
where they may not, we anticipate that their use in the
restricted situation will provide insights into the more
general problem.
Before proceeding with our discussion, the term
extragrammaticality requires clarification. Extra-
grammaticalities include patently ungrammatical con-
structions, which may nevertheless be semantically
comprehensible, as well as lexical difficulties (for ex-
ample, misspellings), violations of semantic con-
straints, utterances that may be grammatically accept-
able but are beyond the syntactic coverage of the sys-
tem, ellipsed fragments and other dialogue phenomena,
and any other difficulties that may arise in parsing
individual utterances. An extragrammaticality is thus
defined with respect to the capabilities of a particular
system, rather than with respect to an absolute exter-
nal competence model of the ideal speaker.
Extragrammaticality may arise at various levels:
lexical, sentential, and dialogue. The following sec-
tions examine each of these levels in turn, classifying
the extragrammaticalities that can occur, and discuss-
ing recovery strategies. At the end of each section, we
consider how well the various recovery strategies
would fit with or be supported by various approaches
to parsing. A final section discusses some experimen-
tal robust parsers that we have implemented. Our
experience with these parsers forms the basis for many
of the observations we offer throughout the paper.
We also discuss some more recent work on integrating
many of the recovery strategies considered earlier into
a single robust multi-strategy parser for restricted do-
main natural language interpretation.
</bodyText>
<sectionHeader confidence="0.906441" genericHeader="method">
2. Lexical Level Extragrammaticalities
</sectionHeader>
<bodyText confidence="0.9995958">
One of the most frequent parsing problems is finding
an unrecognizable word in the input stream. The fol-
lowing subsections discuss the underlying reasons for
the presence of unrecognizable words and develop
applicable recovery strategies.
</bodyText>
<subsectionHeader confidence="0.989232">
2.1. The unknown word problem
</subsectionHeader>
<bodyText confidence="0.998640068965517">
The word is a legitimate lexeme but is not in the
system&apos;s dictionary. There are three reasons for this:
■ The word is outside the intended coverage of the
interface (For example, there is no reason why a
natural language interface to an electronic mail
system should know words like &amp;quot;chair&amp;quot; or &amp;quot;sky&amp;quot;,
which cannot be defined in terms of concepts in its
semantic domain).
■ The word refers to a legitimate domain concept or
combination of domain concepts, but was not in-
cluded in the dictionary. (For example, a word like
&amp;quot;forward&amp;quot; [a message] can be defined as a com-
mand verb, its action can be clearly specified, and
the objects upon which it operates — an old mes-
sage and a new recipient — are already well-formed
domain concepts.)
■ The word is a proper name or a unique identifier,
such as a catalogue part name/number, not hereto-
fore encountered by the system, but recognizable
by a combination of contextual expectations and
morphological or orthographic features (for exam-
ple, capitalization).
In the first situation, there is no meaningful re-
covery strategy other than focused interaction (Hayes
1981) to inform the user of the precise difficulty. In
the third, little action is required beyond recognizing
the proper name and recording it appropriately for
future reference. The second situation is more compli-
cated; three basic recovery strategies are possible:
</bodyText>
<listItem confidence="0.993283769230769">
1. Follow the KLAUS (Haas and Hendrix 1983) ap-
proach, where the system temporarily wrests initia-
tive from the user and plays a well designed
&amp;quot;twenty questions&amp;quot; game, classifying the unknown
term syntactically, and relating it semantically to
existing concepts encoded in an inheritance hier-
archy. This method has proven successful for verbs,
nouns and adjectives, but only when they turn out
to be instances of predefined general classes of
objects and actions in the domain model.
2. Apply the project and integrate method (Carbonell
1979) to infer the meaning and syntactic category
of the word from context. This method has proven
</listItem>
<page confidence="0.588055">
124 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
</page>
<note confidence="0.958592">
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.997981588235294">
useful for nouns and adjectives whose meaning can
be viewed as a recombination of features present
elsewhere in the input. Unlike the KLAUS method,
it operates in the background, placing no major
run-time burden on the user. However, it remains
highly experimental and may not prove practical
without user confirmation.
3. Interact with the user in a focused manner to pro-
vide a paraphrase of the segment of input contain-
ing the unknown word. If this paraphrase results in
the desired action, it is stored and becomes the
meaning of the new word in the immediate context
in which it appeared. The LIFER system (Hendrix
1977) had a rudimentary capacity for defining syn-
onymous phrases. A more general method would
generalize synonyms to classify the new word or
phrase in different semantic contexts.
</bodyText>
<subsectionHeader confidence="0.687407">
2.2.2 Misspellings
</subsectionHeader>
<bodyText confidence="0.999559538461539">
Misspellings arise when an otherwise recognizable
lexeme has letters omitted, substituted, transposed, or
spuriously inserted. Misspellings are the most common
form of extragrammaticality encountered by natural
language interfaces. Usually, a word is misspelt into
an unrecognizable character string. But, occasionally a
word is misspelt into another word in the dictionary
that violates semantic or syntactic expectations. For
instance:
&amp;quot;Copy the flies from the accounts directory to
my directory&amp;quot;
Although &amp;quot;flies&amp;quot; may be a legitimate word in the do-
main of a particular interface (for example, the files
could consist of statistics on med-fly infestation in
California), it is obvious to the human reader that
there is a misspelling in the sentence above.
There are well-known algorithms for matching a
misspelt word against a set of possible corrections
(Durham, Lamb, and Saxe 1983), and the simplest
recovery strategy is to match unknown words against
the set of all words in an interface&apos;s dictionary. How-
ever, this obviously produces incorrect results when a
word is misspelt into a word already in the dictionary,
and can produce unnecessary ambiguities in other cas-
es.
Superior results are obtained by making the spelling
correction sensitive to the parser&apos;s syntactic and se-
mantic expectations. In the following example:
Add two fixed haed dual prot disks to the order
&amp;quot;haed&amp;quot; can be corrected to: &amp;quot;had&amp;quot;, &amp;quot;head&amp;quot;, &amp;quot;hand&amp;quot;,
&amp;quot;heed&amp;quot;, and &amp;quot;hated&amp;quot;. Syntactic expectations rule two
of these out, and domain semantics rule out two oth-
ers, leaving &amp;quot;fixed head disk&amp;quot; as the appropriate cor-
rection. Computationally, there are two ways to or-
ganize this. One can either match parser expectations
against all possible corrections in the parser&apos;s current
vocabulary, and rule out spurious corrections, or one
can use the parse expectations to generate a set of
possible words that can be recognized at the present
point and use this as input to the spelling correction
algorithm. The latter, when it can be done, is clearly
the preferable choice on efficiency criteria. Generating
all possible corrections with a 10,000 word dictionary,
only to rule out all but one or two, is a
computationally-intensive process, whereas exploiting
fully-indexed parser expectations is far more con-
strained and less likely to generate ambiguity. For the
example above, &amp;quot;prot&amp;quot; has 16 possible corrections in a
small on-line dictionary. However, domain semantics
allow only one word in the same position as &amp;quot;prot&amp;quot;, so
correction is most effective if the list of possible words
is generated first.
</bodyText>
<subsectionHeader confidence="0.970462">
2.3. Interaction of morphology and misspelling
</subsectionHeader>
<bodyText confidence="0.999953">
Troublesome side-effects of spelling correction can
arise with parsers that have an initial morphological
analysis phase to reduce words to their root form. For
instance, a parser might just store the root form of
&apos;directory&apos; and reduce &apos;directories&apos; to &apos;directory&apos; plus a
plural marker as part of its initial morphological phase.
This process is normally driven by first failing to rec-
ognize the inflected form as a word that is present in
the dictionary, and then applying standard morphologi-
cal rules (for example, —ies =&gt; +y) to derive a root
from the inflected form. If any root thus derived is in
the dictionary, the input word is assumed to be the
appropriate inflected form.
There are several ways in which this procedure can
interact with spelling correction:
</bodyText>
<listItem confidence="0.654808363636364">
1. The same test, viz, not finding the word in the
dictionary, is used to trigger both morphological
analysis and spelling correction, so there is a ques-
tion of which to do first.
2. The root of the word may be misspelt (e.g. dircto-
ries), even though the inflexion is correct, so that
after the inflexion is removed, there is still no
matching dictionary entry.
3. The inflexion itself may be misspelt (e.g. director-
ise), so that the standard morphological transfor-
mations do not apply.
</listItem>
<bodyText confidence="0.999635769230769">
The first kind of interaction is not usually a major
problem. On the assumption that inflexion is more
common than misspelling, the most straightforward
and probably best strategy is to try inflexion first on
unknown words and then if that does not produce a
word in the dictionary, try spelling correction. Match-
ing only against contextually appropriate words should
avoid cases in which a misspelling produces an inflect-
ed form of a different word.
If the root of an inflected word is misspelt, it will
be necessary to spelling correct all of the (possibly
several) uninflected forms, which might be inefficient.
Again, contextual sensitivity can help.
</bodyText>
<note confidence="0.933046">
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 125
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.999957652173913">
The third kind of interaction is most troublesome.
Most inflexions are too short for spelling correction to
be effective — letter substitution or omission on two
letter sequences is hard to identify. Moreover, inflex-
ion processing does not normally use an explicit list of
inflexions, but instead is organized as a discrimination
net, containing the inflexions implicitly. One solution
may be to have a list of all misspellings of inflected
forms, but even utilizing hash coding schemes, search-
ing this set would be inefficient.
A simpler solution to the entire problem of interac-
tion between spelling correction and morphological
analysis is to eliminate the morphological analysis, and
just store all inflected forms in the dictionary. This
has the disadvantages of being unaesthetic and being
unable to deal with novel inflexions, but neither of
these are major problems for restricted domain natural
language interfaces. There is also a second order
problem in that more than one inflected form of the
same word could be found as candidate corrections
through spelling correction, but this can be overcome
by explicitly grouping the various inflexions of a given
root together in the lexicon.
</bodyText>
<subsectionHeader confidence="0.997121">
2.4. Incorrect seg mentation
</subsectionHeader>
<bodyText confidence="0.9983414">
Input typed to a natural language interface is segment-
ed into words by spaces and punctuation marks. Both
kinds of segmenting markers, especially the second,
can be omitted or inserted speciously. Incorrect seg-
mentation at the lexical level results in two or more
words being run together, as in &amp;quot;runtogether&amp;quot;, or a
single word being split up into two or more segments,
as in &amp;quot;tog ether&amp;quot; or (inconveniently) &amp;quot;to get her&amp;quot;, or
combinations of these effects as in &amp;quot;runto geth er&amp;quot;.
In all these cases, it is possible to deal with such
errors by extending the spelling correction mechanism
to be able to recognize target words as initial segments
of unknown words, and vice-versa. For instance, by
spelling correcting &amp;quot;portdisks&amp;quot; against what is accepta-
ble in the position it occupies in:
Add two dual portdisks to the order
it should be possible to recognize the initial segment
&amp;quot;port&amp;quot; as the intended word, with &amp;quot;disks&amp;quot; as a left
over segment to be inserted into the input string after
the corrected word for further parsing, resulting in this
case in the correct parse. Again, in:
Add two dual port disks to the ord er
an unrecognized (and uncorrectable) word &amp;quot;er&amp;quot; fol-
lowing a word &amp;quot;ord&amp;quot; which has been recognized as an
initial segment abbreviation should trigger an attempt
to attach the unknown word to the end of the abbrevi-
ation to see if it completes it. Correction of
Add two du alport disks to the order
would be somewhat harder. After failing in the above
recovery methods, one letter at a time would be strip-
ped off the beginning of the second unrecognizable
word (&amp;quot;alport&amp;quot;) and added at the end of the first un-
recognizable word (&amp;quot;du&amp;quot;). This process succeeds only
if at some step both words are recognizable and enable
the parse to continue. Migrating the delimiter (the
space) backwards as well as forwards should also be
attempted between a pair of unknown words, stopping
if both words become recognizable. Of course, the
compounding of multiple lexical deviations (for exam-
ple, misspellings, run-on words and split words in the
same segment) requires combinatorially inefficient
recovery strategies. Strong parser expectations amelio-
rate this problem partially, but trade-offs must be
made between resilience and efficiency for compound
error recovery.
</bodyText>
<subsectionHeader confidence="0.9806545">
2.5. Support for recovery strategies by various
parsing approaches
</subsectionHeader>
<bodyText confidence="0.999956666666667">
In general, lexical level recovery strategies operate in a
sufficiently localized manner that the variations in
global behaviour of different approaches to parsing do
not come into play. However, most of the strategies
are capable of using contextual restrictions on what
incorrect lexical item might be, and therefore are most
effective when the constraints on the unknown word
are strongest. This suggests that they will be most
successful when used with an approach to parsing in
which it is easy to bring semantic constraints to bear.
So, for instance, such techniques are likely to be more
effective using a semantic grammar (Hendrix 1977,
Brown and Burton 1975) or case frame instantiation
(Dejong 1979, Hayes and Carbonell 1981) approach,
than in an approach using a syntactic ATN (Woods,
Kaplan and Nash-Webber 1972), where the expecta-
tions are never more specific than membership in one
or more general syntactic categories.
</bodyText>
<sectionHeader confidence="0.666205" genericHeader="method">
3. Sentential Level Extragrammaticalities
</sectionHeader>
<bodyText confidence="0.999946230769231">
Recovery from extragrammaticality at the sentential
level is much more dependent on the particular kind of
parsing techniques that are employed. Some tech-
niques lend themselves to straightforward recovery
methods, while others make recovery difficult. An
initial examination of the requirements for recovery
from various kinds of sentential level ungrammaticality
will allow us to draw some general conclusions about
the most suitable basic parsing approaches to build on.
We examine ungrammaticalities in five categories:
missing words, spurious words or phrases, out of order
constituents, agreement violations, and semantic con-
straint violations.
</bodyText>
<subsectionHeader confidence="0.999708">
3.1. Missing constituents
</subsectionHeader>
<bodyText confidence="0.915573">
It is not uncommon for the user of a natural language
interface to omit words from his input, either by mis-
</bodyText>
<page confidence="0.850649">
126 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
</page>
<note confidence="0.685162">
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.999766934065934">
take or in an attempt to be cryptic. The degree of
recovery possible from such ungrammaticalities is, of
course, dependent on which words were left out. For
instance in:
Add two fixed head dual ported disks to my
order
omitting &amp;quot;dual&amp;quot; would be unrecoverable since all disks
are ported and the discriminating information about
the number of ports would not be there. On the other
hand, if &amp;quot;ported&amp;quot; is omitted, all vital information is
still there (the only thing dual about disks is the num-
ber of ports) and it should be possible to recover.
Also the omission of function words like prepositions
or determiners is usually (though not always) recover-
able. In practice, most omissions are of words whose
contribution to the sentence is redundant, and are
done consciously in an attempt to be cryptic or
&amp;quot;computer-like&amp;quot; (as in &amp;quot;Copy new files my
directory&amp;quot;). This suggests that techniques that fill in
the gaps on semantic grounds are more likely to be
successful than strategies that do not facilitate the
application of domain semantics.
In general, coping with missing words requires a
parsing process to determine the parse structure that
would have been obtained if those words had been
there. If the information provided by the missing
words is not redundant (as in the case of &amp;quot;dual&amp;quot;
above), then this structure will have gaps, but the
structure will convey the broad sense of the user&apos;s
intention, and the gaps can be filled in by inference or
(more practically and safely) by interaction with the
user, focusing on the precise gaps in the context of the
global parse structure (see Section 4.2 for further dis-
cussion of focused interaction techniques.)
A parsing process postulates a missing word error
when its expectations (syntactic or semantic) of what
should go at a certain place in the input utterance are
violated. To discover that the problem is in fact a
missing word, and to find the parse structure corre-
sponding to the user&apos;s intention, the parsing process
must &amp;quot;step back&amp;quot; and examine the context of the
parse as a whole. It needs to ignore temporarily the
unfulfilled expectations and their contribution to the
overall structure while it tries to fulfil some of its oth-
er expectations through parsing other parts of the in-
put and integrating them with already parsed constitu-
ents. In terms of a left-to-right parse of the above
example (minus &amp;quot;dual&amp;quot;), this would mean that when
the parser encountered &amp;quot;ported&amp;quot;, it should note that
even though it was expecting the start of a modifier
suitable for a computer component (assuming its ex-
pectations are semantic), it had in fact found the latter
part of a modifier for a disk and so could proceed as
though the whole of the modifier was there. A parser
with greater directional freedom might find &amp;quot;disk&amp;quot;
first and then look more specifically for qualifiers suit-
able for disks. Again, the existence of a complete disk
qualifier in the user&apos;s intended utterance could be as-
sumed from finding part of the qualifier in a place
where a whole one should go.
Another way of looking at this is as an attempt to
delimit the gap in the input utterance, correlate it with
a gap in the parse structure (filling in that gap if it is
uniquely determined), and realign the parsing mecha-
nism as though the gap did not exist. Such a realign-
ment can be done top-down by hypothesizing the oth-
er expected constituents from the parse structure al-
ready obtained and attempting to find them in the
input stream. Alternatively, realignment can be done
bottom-up by recognizing as yet unparsed elements of
the input, and either fitting them into an existing parse
structure, or finding a larger structure to subsume both
them and the existing structure. This latter approach
is essential when the structuring words are missing or
garbled.
Whether a top-down or a bottom-up method is best
in any given instance will depend on how much struc-
ture the parser can recognize before having to deal
with the missing word. If the parser is left-to-right
and the gap appears early in the input, there is likely
to be little structure already built up, so a bottom-up
approach will probably produce better results. Similar-
ly, if the missing word itself provides the highest level
of structure (for example, &amp;quot;add&amp;quot; in the example
above), a bottom-up approach is essential. On the
other hand, if the missing word corresponds to a spot
low-down in the parse structure, and the gap is late in
the utterance, or the parser is not bound to a strict
left-to-right directionality, a top-down approach is
likely to be much more efficient. In general, both
methods should be available.
</bodyText>
<subsectionHeader confidence="0.99982">
3.2. Spurious constituents
</subsectionHeader>
<bodyText confidence="0.987886">
Words in an input utterance that are spurious to a
parse can arise from a variety of sources:
</bodyText>
<listItem confidence="0.530649833333333">
■ legitimate phrases that the parser cannot deal with: It
is not uncommon for the user of a restricted do-
main interface to say things that the interface can-
not understand because of either conceptual or
grammatical limitations. Sometimes, spurious ver-
bosity or politeness is involved:
</listItem>
<bodyText confidence="0.951846019607843">
Add if you would be so kind two fixed head
and if possible dual ported disks to my order.
Or the user may offer irrelevant (to the system)
explanations or justifications, as observed in pre-
paratory experiments for the GUS system (Bobrow
et al. 1977), for example,
I think I need more storage capacity, so add
two fixed head dual ported disks to my or-
der.
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 127
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
Some common phrases of politeness can be recog-
nized explicitly, but in most cases, the only reason-
able response is to ignore the unknown phrases,
realign the parse on the recognizable input, and if a
semantically and syntactically complete structure
results, postulate that the ignored segment was in-
deed redundant. In most such cases, the user
should be informed that part of the input was ig-
nored.
■ broken-off and restarted utterances: These occur
when people start to say one thing, change their
mind, and say another:
Add I mean remove a disk from my order
Utterances in this form are more likely to occur in
spoken input, but a similar effect can arise in typed
input when a user forgets to hit the erase line or
erase character key:
Add remove a disk from my order
Add a single ported dual ported disk from
my order
Again the best tactic is to discard the broken-off
fragment, but identifying and delineating the super-
seded fragment requires strategies such as the one
discussed below.
■ unknown words filling a known grammatical role:
Sometimes the user will generate an incomprehensi-
ble phrase synonymous with a constituent the sys-
tem is perfectly capable of understanding:
Add a dual ported rotating mass storage de-
vice to my order
Here the system might not know that &amp;quot;rotating
mass storage device&amp;quot; is synonymous with &amp;quot;disk&amp;quot;.
This phenomenon will result in missing words as
well as spurious words. If the system has a unique
expectation for what should go in the gap, it should
(with appropriate confirmation from the user) re-
cord the unknown words as synonymous with what
it expected. If the system has a limited set of ex-
pectations for what might go in the gap, it could
ask the user which one (if any) he meant and again
record the synonym for future reference. In cases
where there are no strong expectations, the system
would ask for a paraphrase of the incomprehensible
fragment. If this proved comprehensible, it would
then postulate the synonymy relation, ask the user
for confirmation, and again store the results for
future reference.
The kind of recovery strategies required here are
surprisingly similar to those required for missing
words. Essentially, the parser must recognize that the
input contains recognizable segments as well as unex-
pected and unrecognizable words and phrases inter-
spersed among them. The way that a parser (at least a
left-to-right parser) would encounter the problem is
identical to the way that missing words are manifested,
viz, the next word in sequence does not fulfil the
parser&apos;s expectations. Overcoming this problem in-
volves the same notion of &amp;quot;stepping back&amp;quot; and seeing
how subsequent elements of the input fit with the
parsing structure built up so far. A major difference is
that the word that violated the expectations, and pos-
sibly other subsequent words, may not be incorporated
into the resulting structure. Moreover, in the case of
purely spurious phrases, that structure may not have
any gaps. For a parser with more directional freedom,
the process of finding spurious phrases may be simpler
in that it could parse all the words that fit into the
structure before concluding that the unrecognizable
words and phrases were indeed spurious. When gaps
in the parse structure remain after parsing all the rec-
ognizable input, the unrecognizable segment may not
be spurious after all. It can be aligned with the gap in
the parse and the possible synonymy relations dis-
cussed above can be presented to the user for approv-
al.
In the case of broken-off utterances, there are
some more specific methods that allow the spurious
part of the input to be detected:
■ If a sequence of two constituents of identical syn-
tactic and semantic type is found where only one is
permissible, simply ignore the first constituent.
Two main command verbs in sequence (for exam-
ple, as in &amp;quot;Add remove ...&amp;quot; above), instantiate the
identical sentential case header role in a case frame
parser, enabling the former to be ignored. Similar-
ly, two instantiations of the same prenominal case
for the &amp;quot;disk&amp;quot; case frame would be recognized as
mutually incompatible and the former again ig-
nored. Other parsing strategies can be extended to
recognize equivalent constituent repetition, but case
frame instantiation seems uniquely well suited to it.
</bodyText>
<listItem confidence="0.698903222222222">
■ Recognize explicit corrective phrases and if the
constituent to the right is of equivalent syntactic
and semantic type as the constituent to the left,
substitute the right constituent for the left constitu-
ent and continue the parse. This strategy recovers
from utterances such as &amp;quot;Add I mean remove ...&amp;quot;,
if &amp;quot;I mean&amp;quot; is recognized as a corrective phrase.
■ Select the minimal constituent for all substitutions.
For instance in
</listItem>
<bodyText confidence="0.9943599">
Add a high speed tape drive, that&apos;s disk
drive, to the order
one desires &amp;quot;disk drive&amp;quot; to substitute for &amp;quot;tape
drive&amp;quot;, not for the larger phrase &amp;quot;high speed tape
drive&amp;quot;, which also forms a legitimate constituent of
like semantic and syntactic type. This preference is
based solely on pragmatic grounds and empirical
evidence.
In addition to identifying and ignoring spurious
input, a robust interface must tell the user what it has
</bodyText>
<page confidence="0.611084">
128 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
</page>
<note confidence="0.540758">
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.999955266666667">
ignored and should paraphrase the part of the input
that it did recognize. The unrecognized input may
express vital information, and if that information is not
captured by the paraphrase, the user may wish to try
again. Exceptions to this rule arise when the spurious
input can be recognized explicitly as such. Expres-
sions of politeness, for instance, might be treated this
way. The ability to recognize such &amp;quot;noise&amp;quot; phrases
makes them in some sense part of the expectations of
the parser, and thus not truly spurious. However,
isolating them in the same way as spurious input pro-
vides the advantage that they can then be recognized
at any point in the input without having to clutter the
parser&apos;s normal processing with expectations about
where they might occur.
</bodyText>
<subsectionHeader confidence="0.983519">
3.3. Out of order constituents and fragmentary
input
</subsectionHeader>
<bodyText confidence="0.999006783333334">
Sometimes, a user will use non-standard word order.
There are a variety of reasons why users violate ex-
pected constituent ordering relations, including unwill-
ingness to change what has already been typed, espe-
cially when extensive retyping would be required.
Two fixed head dual ported disk drives add to
the order
or a belief that a computer will understand a clipped
pseudo-military style more easily than standard usage:
two disk drives fixed head dual ported to my
order add
Similar myths about what computers understand best
can lead to a very fragmented and cryptic style in
which all function words are eliminated:
Add disk drive order
instead of &amp;quot;add a disk drive to my order&amp;quot;.
These two phenomena, out of order constituents
and fragmentary input, are grouped together because
they are similar from the parsing point of view. The
parser&apos;s problem in each case is to put together a
group of recognizable sentence fragments without the
normal syntactic glue of function words or position
cues to indicate how the fragments should be com-
bined. Since this syntactic information is not present,
semantic considerations have to shoulder the burden
alone. Hence, parsers that make it easy for semantic
information to be brought to bear are at a considera-
ble advantage.
Both bottom-up and top-down recovery strategies
are possible for detecting and recovering from missing
and spurious constituents. In the bottom-up approach,
all the fragments are recognized independently, and
purely semantic constraints are used to assemble them
into a single framework meaningful in terms of the
domain of discourse. When the domain is restricted
enough, the semantic constraints can be such that they
always produce a unique result. This characteristic
was exploited to good effect in the PLANES system
(Waltz 1978) in which an input utterance was recog-
nized as a sequence of fragments which were then
assembled into a meaningful whole on the basis of
semantic considerations alone. A top-down approach
to fragment recognition requires that the top-level or
organizing concept in the utterance (&amp;quot;add&amp;quot; in the
above examples) be located first and the predictions
obtainable from it about what else might appear in the
utterance be used to guide and constrain the recogni-
tion of the other fragments.
As a final point, note that in the case of out of
order constituents, a parser relying on a strict left-to-
right scan will have much greater difficulty than one
with more directional freedom. In out of order input,
there may be no meaningful set of left-to-right expec-
tations, even allowing for gaps or extra constituents,
that will fit the input. For instance, a case frame
parser that scans for the head of a case frame, and
subsequently attempts to instantiate the individual
cases from surrounding input, is far more amenable to
this type of recovery than one dependent upon rigid
word order constraints.
</bodyText>
<subsectionHeader confidence="0.99563">
3.4. Syntactic and semantic constraint
violations
</subsectionHeader>
<bodyText confidence="0.990295082191781">
Input to a natural language system can violate both
syntactic and semantic constraints. The most common
form of syntactic constraint violation is agreement
failure between subject and verb or determiner and
head noun:
Do the order include a disk drives?
Semantic constraint violations can occur because the
user has conceptual problems:
Add a floating head tape drive to the order
or because he is imprecise in his language, using a
related object in place of the object he really means.
For instance, if he is trying to decide on the amount of
memory to include in an order he might say
Can you connect a video disk drive to the two
megabytes.
when what he really means is &amp;quot;... to the computer with
two megabytes of memory&amp;quot;.
These different kinds of constraint violation require
quite different kinds of treatment. In general, the
syntactic agreement violations can be ignored; cases in
which agreement or lack of it distinguishes between
two otherwise valid readings of an input are rare.
However, one problem that sometimes arises is know-
ing whether a noun phrase is singular or plural when
the determiner or quantifier disagrees with the head
noun. It is typically best to let quantifiers dominate
when they are used; for example, &amp;quot;two disk&amp;quot; really
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 129
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
means &amp;quot;two disks&amp;quot;. And with determiner disagree-
ment, it is often unimportant which reading is taken.
In the example of disagreement above, it does not
matter whether the user meant &amp;quot;a disk drive&amp;quot; or &amp;quot;any
disk drives&amp;quot;. The answer will be the same in either
case, viz, a listing of all the disk drives that the order
contains. In cases where the action of the system
would be different depending on whether the noun
phrase was singular or plural (e.g. &amp;quot;delete a disks from
the order&amp;quot;), the system should interact with the user in
a focused way to determine what he really meant.
Semantic constraint violations due to a user&apos;s con-
ceptual problems are harder to deal with. Once de-
tected, the only solution is to inform the user of his
misconception and let him take it from there. The
actual detection of the problem, however, can cause
some difficulty for a parser relying heavily on semantic
constraints to guide its parse. The constraint violation
might cause it to assume there was some other prob-
lem such as out of order or spurious constituents, and
look for (and perhaps even find) some alternative and
unintended way of putting all the pieces together.
This is one case where syntactic considerations should
come to the fore.
Semantic constraint violations based on the men-
tion of a related object instead of the entity actually
intended by the user will manifest themselves in the
same way as the semantic constraint violations based
on misconceptions, but their processing needs to be
quite different. The violation can be resolved if the
system can look at objects related to the one the user
mentioned and find one that satisfies the constraints.
In the example above, this means going from the mem-
ory size to the machine that has that amount of memo-
ry. Clearly, the distance of the relationship over
which this kind of substitution is allowed needs to be
controlled fairly carefully — in a restricted domain
everything is eventually related to everything else.
But there may well be rules that control the kind of
substitutions that are allowed. In the above example,
it suffices to allow a part to substitute for a whole
(metonymy), especially if, as we assumed, it had been
used earlier in the dialogue to distinguish between
different instances of the whole.
</bodyText>
<subsectionHeader confidence="0.982813">
3.5. Support for recovery strategies by various
parsing approaches
</subsectionHeader>
<bodyText confidence="0.99966275">
We now turn the question of incorporating the senten-
tial level recovery strategies we have been discussing
into the various approaches to parsing mentioned in
the introduction. As we shall see, there are considera-
ble differences in the underlying suitability of the vari-
ous approaches as bases for the recovery strategies.
To address this issue, we classify parsing approaches
into three general groups: transition network ap-
proaches (including syntactic ATNs and network-
based semantic grammars), pattern matching ap-
proaches, and approaches based on case frame instan-
tiation.
</bodyText>
<subsectionHeader confidence="0.7058305">
3.5.1. Recovery strategies using a transition
network approach
</subsectionHeader>
<bodyText confidence="0.989368343137255">
Although attempts have been made to incorporate
sentential level recovery strategies into network-based
parsers including both syntactically-based ATNs
(Kwasny and Sondheimer 1981, Weischedel and Son-
dheimer 1984, Weischedel and Black 1980, Woods et
al. 1976) and semantic grammar networks (Hendrix
1977), the network paradigm itself is not well suited
to the kinds of recovery strategies discussed in the
preceding sections. These strategies generally require
an interpretive ability to &amp;quot;step back&amp;quot; and take a broad
view of the situation when a parser&apos;s expectations are
violated, and this is very hard to do when using net-
works. The underlying problem is that a significant
amount of state information during the parse is implic-
itly encoded by the position in the network; in the
case of ATNs, other aspects of the state are contained
in the settings of scattered registers. As demonstrated
by the meta-rule approach to diagnosing parse failures
described by Weischedel and Sondheimer (1983) else-
where in this journal issue, these and other difficulties
elaborated below do not preclude recovery from extra-
grammatical input. However, they do make it difficult
and often impractical, since much of the procedurally
encoded state must be made declarative and explicit to
the recovery strategies.
Often an ATN parse will continue beyond the point
where the grammatical deviation, say an omitted word,
occurred, and reach a node in the network from which
it can make no further progress (that is, no arcs can be
traversed). At this point, the parser cannot ascertain
the source of the error by examining its internal state
even if the state is accessible — the parser may have
popped from embedded subnets, or followed a totally
spurious sequence of arcs before realizing it was get-
ting in trouble. If these problems can be overcome
and the source of the error determined precisely, a
major problem remains: in order to recover, and parse
input that does not accord with the grammar, while
remaining true to the network formalism, the parser
must modify the network dynamically and temporarily,
using the modified network to proceed through the
present difficulties. Needless to say, this is at best a
very complex process, one whose computational tract-
ability is open to question. It is perhaps not surprising
that in one of the most effective recovery mechanisms
developed for network-based parsing, the LIFER
system&apos;s ellipsis handling routine (Hendrix 1977), the
key step operates completely outside the network for-
malism.
As we have seen, semantic constraints are very
important in recovering from many types of ungram-
matical input, and these are by definition unavailable
130 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
in a purely syntactic ATN parser. However, semantic
information can be brought to bear on network based
parsing, either through the semantic grammar approach
in which joint semantic and syntactic categories are
used directly in the ATN, or by allowing the tests on
ATN arcs to depend on semantic criteria (Bobrow
1978, Bobrow and Webber 1980). In the former tech-
nique, the appropriate semantic information for re-
covery can be applied only if the correct network node
can be located — a sometimes difficult task as we have
seen. In the latter technique, sometimes known as
cascaded ATNs (Woods 1980), the syntactic and se-
mantic parts of the grammar are kept separate, thus
giving the potential for a higher degree of interpretive-
ness in using the semantic information. However, the
natural way to use this technique is to employ the
semantic information only to confirm or disconfirm
parses arrived at on syntactic grounds. So the rigidity
of the network formalism makes it very difficult to
bring the available semantic information to bear effec-
tively on extragrammatical input.
A further disadvantage of the network approach for
implementing flexible recovery strategies is that net-
works naturally operate in a top-down left-to-right
mode. As we have seen, a bottom-up capability is
essential for many recovery strategies, and directional
flexibility often enables easier and more efficient oper-
ation of the strategies. Of course, the top-down left-
to-right mode of operation is a characteristic of the
network interpreter, not of the network formalism
itself, and an attempt (Woods et al. 1976) has been
made to operate an ATN in an &amp;quot;island&amp;quot; mode, that is,
bottom-up, center-out. This experiment was done in
the context of a speech parser where the low-level
recognition of many of the input words was uncertain,
though the input as a whole was assumed to be gram-
matical. In that situation, there were clear advantages
to starting with islands of relative lexical certainty, and
working out from there. Problems, however, arise
during leftward expansion from an island when it is
necessary to run the network backwards. The admissi-
bility of ATN transitions can depend on tests that ac-
cess the values of registers which would have been set
earlier when traversing the network forwards, but
which cannot have been set when traversing back-
wards. This leads at best to an increase in non-
determinism, and at worse to blocking the traversal
completely.
</bodyText>
<subsectionHeader confidence="0.98348">
3.5.2. Recovery strategies using a pattern
matching approach
</subsectionHeader>
<bodyText confidence="0.996600810344828">
A pattern matching approach to parsing provides a
better framework to recover from some sentential-
level deviations than a network-based approach. In
particular, the definition of what constitutes a pattern
match can be relaxed to allow for missing or spurious
constituents. For missing constituents, patterns which
match some, but not all, of their components can be
counted temporarily as complete matches, and spurious
constituents can be ignored so long as they are embed-
ded in a pattern whose other components do match.
In these cases, the patterns taken as a whole provide a
basis on which to perform the kind of &amp;quot;stepping back&amp;quot;
discussed above as being vital for flexible recovery. In
addition, when pattern elements are defined semanti-
cally instead of lexically, as with Wilks&apos;s (1975) ma-
chine translation system, semantic constraints can
easily be brought to bear on the recognition. Howev-
er, dealing with out of order constituents is not so easy
for a pattern-based approach since constituent order is
built into a pattern in a rigid way, similarly to a net-
work. It is possible to accept any permutation of ele-
ments of a pattern as a match, but this provides so
much flexibility that many spurious recognitions are
likely to be obtained as well as the correct ones (see
Hayes and Mouradian 1981).
An underlying problem here is that there is no nat-
ural way to make the distinctions about the relative
importance or difference in role between one word
and another. For instance, parsing many of the exam-
ples we have used might involve use of a pattern like:
(&lt;determiner&gt; &lt;disk-drive-attribute&gt;* &lt;disk-drive&gt;)
which specifies a pattern of a determiner, followed by
zero or more attributes of a disk drive, followed by a
phrase synonymous with &amp;quot;disk drive&amp;quot;. So this pattern
would recognize phrases like &amp;quot;a dual ported disk&amp;quot; or
&amp;quot;the disk drive&amp;quot;. Using the method of dealing with
missing constituents mentioned above, &amp;quot;the&amp;quot; would
constitute just as good a partial match for this pattern
as &amp;quot;disk drive&amp;quot;, a clearly undesirable result. The
problem is that there is no way to tell the flexible
matcher which components of the pattern are discrimi-
nating from the point of view of recognition and which
are not. Another manifestation of the same problem is
that different words and constituents may be easier or
harder to recognize (for example, prepositions are
easier to recognize than the noun phrases they intro-
duce), and thus may be more or less worthwhile to
look for in an attempt to recover from a grammatical
deviation.
The underlying problem then is the uniformity of
the grammar representation and the method of apply-
ing it to the input. Any uniformly represented gram-
mar, whether based on patterns or networks, will have
trouble representing and using the kinds of distinctions
just outlined, and thus will be less well equipped to
deal with many grammatical deviations in an efficient
and discriminating manner. See Hayes and Carbonell
(1981) for a fuller discussion of this point.
</bodyText>
<footnote confidence="0.957344333333333">
3.5.3. Recovery strategies in a case frame
paradigm
Recursive case frame instantiation appears to provide
</footnote>
<note confidence="0.8661915">
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 131
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.834341333333333">
a better framework for recovery from missing words
than approaches based on either network traversal or
pattern matching. There are several reasons:
</bodyText>
<listItem confidence="0.89162305">
■ Case frame instantiation is inherently a highly inter-
pretive process. Case frames provide a high-level
set of syntactic and semantic expectations that can
be applied to the input in a variety of ways. They
also provide an overall framework that can be used
to realize the notion of &amp;quot;stepping back&amp;quot; to obtain a
broad view of a parser&apos;s expectations. As we have
emphasised, this ability to &amp;quot;step back&amp;quot; is important
when input deviates from the standard expecta-
tions.
■ Case frame instantiation is a good vehicle for bring-
ing semantic and pragmatic information to bear in
order to help determine the appropriate parse in the
absence of expected syntactic constituents. If a
preposition is omitted (as commonly happens when
dealing with cryptic input from hunt-and-peck typ-
ists), the resulting sentence is syntactically anoma-
lous. However, semantic case constraints can be
sufficiently strong to attach each noun phrase to
the correct structure. Consider, for instance, the
</listItem>
<bodyText confidence="0.98982528">
following sentence typed to an electronic mail sys-
tem natural language interface:
&amp;quot;Send message John Smith&amp;quot;
The missing determiner presents few problems, but
the missing preposition can be more serious. Do we
mean to send a message &amp;quot;to John Smith&amp;quot;, &amp;quot;about
John Smith&amp;quot;, &amp;quot;with John Smith&amp;quot;, &amp;quot;for John
Smith&amp;quot;, &amp;quot;from John Smith&amp;quot;, &amp;quot;in John Smith&amp;quot;, &amp;quot;of
John Smith&amp;quot;, etc.? The domain semantics of the
case frame rule out the latter three possibilities and
others like them as nonsensical. However, prag-
matic knowledge is required to select &amp;quot;to John
Smith&amp;quot; as the preferred reading (possibly subject to
user confirmation) — the destination case of the
verb is required for the command to be effective,
whereas the other cases, if present, are optional.
This knowledge of the underlying action must be
brought to bear at parse time to disambiguate the
cryptic command. In the XCALIBUR system case
frame encoding (Carbonell, Boggs, Mauldin, and
Anick 1983), we apply precisely such pragmatic
knowledge represented as preference constraints
(cf. Wilks 1975) on case fillers at parse time.
Thus, problems created by the absence of expected
case markers can be overcome by the application of
domain knowledge.
■ The propagation of semantic knowledge through a
case frame (via attached procedures such as those
of KRL (Bobrow and Winograd 1977) or SRL
(Wright and Fox 1983)) can fill in parser defaults
and allow the internal completion of phrases such
as &amp;quot;dual disks&amp;quot; to mean &amp;quot;dual ported disks&amp;quot;. This
process is also responsible for noticing when infor-
mation is either missing or ambiguously determined,
thereby initiating a focused clarificational dialogue
(Hayes 1981).
■ The representation of case frames is inherently
non-uniform. Case fillers, case markers, and case
headers are all represented separately, and this dis-
tinction can be used by the parser interpretively
instantiating the case frame. For instance, if a case
frame accounts for the non-spurious part of an
input containing spurious constituents, a recovery
strategy can skip over the unrecognizable words by
scanning for case markers as opposed to case fillers
which typically are much harder to find and parse.
This ability to exploit non-uniformity goes a long
way to overcoming the problems with uniform pars-
ing methods outlined in the previous section on
pattern matching.
</bodyText>
<sectionHeader confidence="0.969029" genericHeader="method">
4. Dialogue Level Extragrammaticality
</sectionHeader>
<bodyText confidence="0.999988620689655">
The underlying causes of many extragrammaticalities
detected at the sentential level are rooted in dialogue
phenomena. For instance, ellipses and other fragmen-
tary inputs are patently ungrammatical at the senten-
tial level, but can be understood in the context of a
dialogue. Viewed at this more global level, ellipsis is
not an &amp;quot;ungrammaticality&amp;quot;. Nevertheless, the same
computational mechanisms required to recover from
lexical and (especially) sentential problems are neces-
sary to detect ellipsis and parse the fragments correct-
ly for incorporation into a larger structure. In the
same way, many dialogue phenomena are classified
pragmatically as extragrammaticalities.
In addition to addressing dialogue level extragram-
maticalities, any robust parsing system must engage
the user in dialogue for cooperative resolution of pars-
ing problems too difficult for automatic recovery. In-
teraction with the user is also necessary for a coopera-
tive parser to confirm any assumptions it makes in
interpreting extragrammatical input and to resolve any
ambiguities it cannot overcome on its own. We have
referred several times in our discussions to the princi-
ple of focused interaction, and stated that practical
recovery dialogues should be focused as tightly as
possible on the specific problem at hand. Section 4.2
discusses some considerations for structuring focused
interaction dialogues — in particular, why they need to
be so tightly focused, and what mechanisms are need-
ed to achieve tight focusing in a natural manner.
</bodyText>
<subsectionHeader confidence="0.98394">
4.1. Ellipsis
</subsectionHeader>
<bodyText confidence="0.999988">
Ellipsis is a many-faceted phenomenon. Its manifesta-
tions are varied and wide ranging, and recovery strate-
gies for many types of ellipsis remain to be discovered.
Nevertheless, it is also a very common phenomenon
and must be addressed by any interface intended for
serious use by real users. Empirical observations have
shown that users of natural language interfaces employ
</bodyText>
<note confidence="0.7264925">
132 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.997892575757576">
ellipsis and other abbreviating devices (for example,
anaphora, short definite noun phrases, cryptic lan-
guage omitting semantically superfluous words, and
lexical abbreviations) with alarming frequency
(Carbonell 1983). The results of our empirical obser-
vations can be summarized as follows:
Terseness principle: Users of natural language
interfaces insist on being as terse as possible,
independent of task, communication media, typ-
ing ability, or instructions to the contrary, with-
out sacrificing the flexibility of expression inher-
ent in natural language communication.
Broadly speaking, one can classify ellipsis into in-
trasentential and intersentential ellipsis, with the latter
category being far more prevalent in practical natural
language interfaces. Intrasentential ellipsis occurs
most frequently in coordinate clauses such as:
John likes oranges and Mary apples.
Often, this type of ellipsis is detectable only on se-
mantic grounds (there is no meaningful noun-noun
unit called &amp;quot;Mary apples&amp;quot;). The following sentence
with identical syntax has a preferred reading that con-
tains no ellipsis:
John likes oranges and MacIntosh apples.
We know of no proven general strategies for interpret-
ing this class of intrasentential ellipsis. An interesting,
but untried, approach might be an application of the
strategies described below with each coordinate clause
in an intrasentential ellipsis being considered as a sep-
arate utterance and with extensions to exploit the syn-
tactic and semantic parallelism between corresponding
constituents of coordinate clauses.
There are several forms of intersentential ellipsis:
</bodyText>
<listItem confidence="0.941827111111111">
■ Elaboration — An ellipsed fragment by either speak-
er can be an elaboration of a previous utterance.
Either speaker can make the elaboration, but the
second speaker usually does so, as in the following
example:
User: Give me a large capacity disk.
System: With dual ports?
User: Yes, and a universal frequency adap-
ter.
■ Echo — A fragment of the first speaker&apos;s utterance is
echoed by the second speaker. As described more
fully in Hayes and Reddy (1983), this allows the
second speaker to confirm his understanding of the
first speaker&apos;s utterance without requiring an ex-
plicit confirmation.
User: Add a dual disk to the order.
System: A dual ported disk. What storage ca-
pacity?
</listItem>
<bodyText confidence="0.988791823529412">
If, on the other hand, the system had explicitly
asked &amp;quot;Do you mean a dual ported disk?&amp;quot;, the user
would have been conversationally obliged to reply.
However, in either case, the user is free to correct
any misapprehension the system displays. Some-
times, as in the example in the next bullet below,
an echo may also be an expression of bewilder-
ment. In general, this form of ellipsis is far more
prevalent in spoken interactions than in typed com-
munication, but the need for a robust parsing sys-
tem to confirm assumptions it is making without
being too disruptive of the flow of conversation
makes it very useful for natural language interfaces
in general (see Section 4.2).
■ Correction — An ellipsed fragment substitutes for a
portion of an earlier utterance that was in error.
The correction occurs in three typical modes:
</bodyText>
<listItem confidence="0.96143525">
• The first speaker can correct himself immediate-
ly (much like the repeated segment problem dis-
cussed in Section 3.2).
• The second speaker can offer a correction
(marked as such, or simply an ellipsed fragment
in the interrogative).
• Or, the first speaker can correct himself in re-
sponse to a clarificational query from the second
</listItem>
<bodyText confidence="0.8311658">
speaker. The form of the clarificational query
can be a direct question, a statement of confu-
sion, or echoing the troublesome fragment of
the input, thereby combining two forms of ellip-
sis as illustrated below.
</bodyText>
<listItem confidence="0.8726219">
User: Give me a dual port tape drive.
System: A dual port tape drive?
User: Sorry, a dual port disk drive.
■ Reformulation — Part of an old utterance is reformu-
lated and meant to be interpreted in place of the
corresponding old constituent. This is perhaps the
most common form of ellipsis and the only one for
which tractable computational strategies have been
implemented. All the examples below are of this
type.
</listItem>
<bodyText confidence="0.999904133333333">
The LIFER/LADDER system (Hendrix 1977, Sacer-
doti 1977) handled a restricted form of reformulation
ellipsis. LIFER&apos;s ellipsis algorithm accepted a frag-
mentary input if it matched a partial parse tree derived
from the previous complete parse tree by (a) selecting
a subtree that accounted for a contiguous segment of
the previous input, and (b) possibly pruning back one
or more of its branches. If a fragmentary input
matched such a partial parse tree, it was assumed to be
a reformulation ellipsis and the missing parts of the
partial parse tree were filled out from the previous
complete parse tree. In particular, if a single grammar
category accounted for the entire fragment, and this
category was present in the last query parsed by the
system, the ellipsis algorithm substituted the fragment
</bodyText>
<note confidence="0.484939">
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 133
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.941151484848485">
directly for whatever filled the category in the last
parse. An example of this is:
User: What is the length of the Kennedy?
System: 200 meters
User: The fastest aircraft carrier?
Since both &amp;quot;the Kennedy&amp;quot; and the &amp;quot;the fastest aircraft
carrier&amp;quot; match the semantic category &lt;ship&gt;, the lat-
ter phrase is allowed to substitute for the former. Note
that a purely syntactic parse would not be sufficiently
selective to make the proper substitution. &amp;quot;The fastest
aircraft carrier&amp;quot; is a noun phrase, and there are three
noun phrases in the original sentence: &amp;quot;the length&amp;quot;,
&amp;quot;the length of the Kennedy&amp;quot; and &amp;quot;the Kennedy&amp;quot;.
However, the rigid structure of semantic grammars
proves insufficient to handle some common forms of
reformulation ellipsis. The semantic grammar formal-
ism is too restrictive for a simple substitution strategy
to apply effectively if there is more than one fragment,
if there is a bridging fragment (such as &amp;quot;the smallest
with two ports&amp;quot; in the example below that bridges
over &amp;quot;disk&amp;quot;), or if the fragment does not preserve
linear ordering. In contrast, case frame substitution
provides the freedom to handle such ellipsed frag-
ments.
The following examples are illustrative of the kind
of sentence fragments the case frame method handles.
We assume that each sentence fragment occurs imme-
diately following the initial query below. Note also
that we are using case frame here to refer to nominal
as well as sentential case frames — the case frame be-
ing instantiated in these examples is the one for a disk
with cases such as storage capacity, number of ports,
etc..
</bodyText>
<sectionHeader confidence="0.63896" genericHeader="method">
INITIAL QUERY:
</sectionHeader>
<bodyText confidence="0.6033965">
&amp;quot;What is the price of the three largest single
port fixed media disks?&amp;quot;
</bodyText>
<equation confidence="0.385718666666667">
SUBSEQUENT QUERIES:
&amp;quot;Speed?&amp;quot;
&amp;quot;Two smallest?&amp;quot;
</equation>
<bodyText confidence="0.988427745454545">
&amp;quot;How about the price of the two smallest?&amp;quot;
&amp;quot;Also the smallest with dual ports?&amp;quot;
&amp;quot;Speed with two ports?&amp;quot;
&amp;quot;Disk with two ports.?&amp;quot;
In these representative examples, punctuation is of no
help, and pure syntax is of very limited utility. For
instance, the last three phrases are syntactically similar
(indeed, the last two are indistinguishable), but each
requires that a different substitution be made on the
preceding query.
The DYPAR-II system (discussed in Section 5.2)
handles ellipsis at the case frame level. Here we pre-
sent the basic case frame ellipsis resolution method it
employs. Its coverage appears to be a superset of the
LIFER/LADDER system (Hendrix 1977, Sacerdoti
1977) and the PLANES ellipsis module (Waltz and
Goodman 1977). Although it handles most of the
reformulation ellipsis we encountered, it is not meant
to be a general linguistic solution to the ellipsis phe-
nomenon.
Consider the following example:
&gt;What is the size of the 3 largest single port fixed
media disks?
&gt;disks with two ports?
Note that it is impossible to resolve this kind of ellipsis
in a general manner if the previous query is stored
verbatim or as a semantic grammar parse tree. &amp;quot;Disks
with two ports&amp;quot; would at best correspond to some
&lt;disk-descriptor&gt; non-terminal, and hence, according
to the LIFER algorithm, would replace the entire
phrase &amp;quot;single port fixed media disks&amp;quot; that corre-
sponded to &lt;disk-descriptor&gt; in the parse of the origi-
nal query. However, an informal poll of potential
users suggests that the preferred interpretation of the
ellipsis retains the MEDIA specifier of the original
query. The ellipsis resolution process, therefore, re-
quires a finer grain substitution method than simply
inserting the highest level non-terminals in the ellipsed
input in place of the matching non-terminals in the
parse tree of the previous utterance.
Taking advantage of the fact that a case frame
analysis of a sentence or object description captures
the relevant semantic relations among its constituents
in a canonical manner, a partially instantiated nominal
case frame can be merged with the previous case
frame as follows:
• If a case is instantiated both in the original query
and in the ellipsis, use the filler from the ellipsis.
For instance &amp;quot;with two ports&amp;quot; overrides &amp;quot;single
port&amp;quot; in our example, as both entail different val-
ues of the same case descriptor, regardless of their
different syntactic roles. (&amp;quot;Single port&amp;quot; in the
original query is an adjectival construction, whereas
&amp;quot;with two ports&amp;quot; is a post-nominal modifier in the
ellipsed fragment.)
</bodyText>
<listItem confidence="0.992226692307692">
• Retain any cases in the original parse that are not
explicitly contradicted by new information in the
ellipsed fragment. For instance, &amp;quot;fixed media&amp;quot; is
retained as part of the disk description, as are all
the sentential-level cases in the original query, such
as the quantity specifier and the projection attri-
bute of the query (&amp;quot;size&amp;quot;).
• If a case is specified in the ellipsed fragment, but
not in the original query, use the filler from the
ellipsis. For instance, the &amp;quot;fixed head&amp;quot; descriptor is
added as the media case of the disk nominal case
frame in resolving the ellipsed fragment in the fol-
lowing example:
</listItem>
<bodyText confidence="0.49007">
&gt;Which disks are configurable on a VAX
11-780?
</bodyText>
<page confidence="0.787183">
134 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
</page>
<note confidence="0.805637">
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.993736433962265">
&gt;Any configurable fixed head disks?
■ In the event that a new case frame is mentioned in
the ellipsed fragment, wholesale substitution occurs,
much as in the semantic grammar approach. For
instance, if after the last example one were to ask
&amp;quot;How about tape drives?&amp;quot;, the substitution would
replace &amp;quot;fixed head disks&amp;quot; with &amp;quot;tape drives&amp;quot;, rath-
er than replacing only &amp;quot;disks&amp;quot; and producing the
phrase &amp;quot;fixed head tape drives&amp;quot;, which is semanti-
cally anomalous. In these instances, the semantic
relations captured in a case frame representation
and not in a semantic grammar parse tree prove
critical.
The key advantage case frame instantiation pro-
vides for ellipsis resolution is the ability to match cor-
responding cases, rather than surface strings, syntactic
structures, or non-canonical representations. Imple-
menting an ellipsis resolution mechanism of equal
power for a semantic grammar approach would, there-
fore, be very difficult. The essential problem is that
semantic grammars inextricably combine syntax with
semantics in a manner that requires multiple represen-
tations for the same semantic entity. For instance, the
ordering of marked cases in the input does not reflect
any difference in meaning,2 while the surface ordering
of unmarked cases does. With a semantic grammar,
the parse trees produced by different marked case
orderings can differ, so the knowledge that surface
positioning of unmarked cases is meaningful, but posi-
tioning of marked ones is not, must be contained with-
in the ellipsis resolution process. This is a very unnat-
ural repository for such basic information. Moreover,
in order to attain the functionality described above for
case frames, an ellipsis resolution based on semantic
grammar parse trees would also have to keep track of
semantically equivalent adjectival and post nominal
forms (corresponding to different non-terminals and
different relative positions in the parse trees). This is
necessary to allow ellipsed structures such as &amp;quot;a disk
with 1 port&amp;quot; to replace the &amp;quot;dual-port&amp;quot; part of the
phrase &amp;quot;...dual-port fixed-media disk ...&amp;quot; in an earlier
utterance. One way to achieve this effect would be to
collect together specific nonterminals that can substi-
tute for each other in certain contexts, in essence
grouping non-canonical representations into context-
sensitive semantic equivalence classes. However, this
process would require hand-crafting large associative
tables or similar data structures, a high price to pay for
each domain-specific semantic grammar. In brief, the
encoding of domain semantics and canonical structure
for multiple surface manifestations makes case frame
instantiation a much better basis for robust ellipsis
resolution than semantic grammars.
</bodyText>
<footnote confidence="0.624149">
2 leaving aside the differential emphasis and other pragmatic
considerations reflected in surface ordering
</footnote>
<subsectionHeader confidence="0.865041">
4.2. Focused interaction
</subsectionHeader>
<bodyText confidence="0.994648775">
In addition to dealing with ellipsis and other extra-
grammatical phenomena that arise naturally for an
interactive interface, a truly robust parsing system
must initiate subdialogues of its own. Such dialogues
are needed
■ when a robust parser makes assumptions that may
not be justified and needs confirmation from the
user that it has guessed correctly;
■ when a parser comes up against ambiguities that it
cannot resolve on its own, either because of extra-
grammaticality on the part of the user or because
of some essential ambiguity in perfectly grammati-
cal input;
■ when the more automated strategies may prove too
costly or uncertain (e.g., when recovering from
compound lexical errors);
■ or when the required information is simply not pres-
ent.
When an interactive system moves from the passive
role of answering questions or awaiting individual user
commands to a more active information-seeking role in
clarificational dialogues, it must address the question
of how to organize its communication so that it will
behave in a way that fits with the conversational ex-
pectations and conventions of its human user. Issues
of when explicit replies are required, how to convey
information in such a way as to require the minimal
response from the user, how to keep the conversation
within the domain of discourse of the system, etc.,
must all be addressed by a natural language interface
capable of mixed-initiative dialogue. Examining all
these topics here would take us too far afield from the
issue of robust parsing, so we will confine ourselves to
issues specific to the kind of recovery interaction de-
scribed above. See Carbonell (1982) and Hayes and
Reddy (1983) for a fuller discussion of the issues in-
volved in organizing the dialogue of an interactive
natural language system.
We offer four guidelines for organizing recovery
dialogues:
</bodyText>
<listItem confidence="0.8588675">
■ the interaction should be as focused as possible;
■ the required user response should be as terse as
possible;
■ the interaction should be in terms of the system&apos;s
domain of discourse rather than the linguistic con-
cepts it uses internally;
</listItem>
<bodyText confidence="0.861339">
■ there should be as few such interactions as possible.
To see the need for focused interaction, consider
the input:
Add two fixed head ported disks to my order
The problem is that the user has omitted &amp;quot;dual&amp;quot; be-
tween &amp;quot;head&amp;quot; and &amp;quot;ported&amp;quot;. Assuming that disks can
only be single or dual ported, and using the sentential
level recovery strategies described earlier, a parser
</bodyText>
<note confidence="0.574235">
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 135
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.999522125">
should be able to come up with an interpretation of
the input that is two ways ambiguous. Interaction
with the user is required to resolve this ambiguity, but
the degree to which the system&apos;s initial question is
focused on the problem can make a big difference in
how easy it is for the user to respond, and how much
work is required of the system to interpret the user
response. An unfocused way of asking the question is:
</bodyText>
<subsectionHeader confidence="0.509604">
Do you mean:
</subsectionHeader>
<bodyText confidence="0.9064745">
Add two fixed head single ported disks to my or-
der, or
</bodyText>
<subsectionHeader confidence="0.559148">
Add two fixed head dual ported disks to my order
</subsectionHeader>
<bodyText confidence="0.995303489795918">
Here the user is forced to compare two very similar
looking possibilities to ascertain the system&apos;s interpre-
tation problem. Comparisons of this kind to isolate
possible interpretation problems place an unnecessary
cognitive load on the user. Furthermore, it is unclear
how the user should reply. Other than saying &amp;quot;the
second one&amp;quot;, he has little option but to repeat the
whole input. Since the system&apos;s query is not focused
on the source of the ambiguity, it is conversationally
awkward for the user to give the single word reply,
&amp;quot;dual&amp;quot;. This response is highly elliptical, but from the
point of view of required information, it is complete.
It also satisfies our second guideline that the required
response be as terse as possible.
A much better way of asking the user to resolve the
ambiguity is:
Do you mean &apos;single&apos; or &apos;dual&apos; ported disks?
This question focuses precisely on the ambiguity, and
therefore requires no effort from the user besides that
of giving the information the system desires. Moreo-
ver, it invites the highly desirable reply &amp;quot;dual&amp;quot;. Since
the system is focused on the precise ambiguity, it can
also generate a discourse expectation for this and oth-
er appropriate elliptical fragments in the user&apos;s re-
sponse, and thereby recognize them with little difficul-
ty.
The ability to generate focused queries to resolve
ambiguities places certain requirements on how a par-
ser represents the ambiguous structure internally. Un-
less the ambiguity is represented as locally as possible,
it will be very hard to generate focused queries. If a
parser finds the ambiguity in the above example by
discovering it has two independent parse structures at
the end of the parsing process, then generating a fo-
cused query involves a computationally taxing intracta-
ble comparison process. However, if the ambiguity is
represented as locally as possible, for instance as two
alternative fillers for a single instantiation of a disk
frame nested within the &amp;quot;add to order&amp;quot; frame, then
generating the focused query is easy — just output a
paraphrase of the case frame (the one for disk) at the
level immediately above the ambiguity with a disjunc-
tion taking the place of the single filler of the ambigu-
ous case (the portedness case). Moreover, such a
representation forms an excellent basis for interpreting
the natural elliptical reply. As Hayes and Carbonell
(1981) show, parsers based on case frame instantia-
tion are particularly well suited to generating ambigui-
ty representations of this kind.
Another tactic related to focused interaction that
parsing systems can employ to smooth recovery dia-
logues is to couch their questions in terms that make it
more likely that the user&apos;s reply will be something they
can understand. Thus in:
Please add two 300 megabyte rotating mass storage
devices to my order.
if &amp;quot;rotating mass storage device&amp;quot; is not in the system&apos;s
vocabulary, it is unwise for it to reply &amp;quot;what is a rotat-
ing mass storage device?&amp;quot;, since the terms the user
chooses to clarify his input may be equally unintelligi-
ble to the system. It is much better to give the user a
choice between the things that the system could recog-
nize in the place where the unrecognizable phrase
occurred. In this example, this would mean giving the
user a choice between all the computer components
that can admit 300 megabytes as a possible case filler.
If this list was unmanageably long, the system should
at least confirm explicitly that the unknown phrase
refers to a computer component by something like:
By &apos;rotating mass storage device&apos; are you referring
to a computer component?
This at least establishes whether the user is trying to
do something that the system can help him with or
whether the user has misconceptions about the abilities
of the system.
Upon confirmation that the user meant &apos;disk&apos;, the
system could add the new phrase as a synonym for
disk, perhaps after engaging the user in further clarifi-
cational dialogue to ascertain that &apos;disk&apos; is not merely
one kind of &apos;rotating mass storage device&apos;, or vice
versa. If it were the case that one was more general
than the other, the new entry could be placed in a
semantic hierarchy and used in future recognition
(perhaps after determining on what key features the
two differ).
Our third guideline stated that the interaction
should be in terms of the domain of discourse rather
than the internal linguistic conventions of the system.
Breaking this rule might involve requiring the user to,
for instance, compare the parse trees representing two
ambiguous interpretations of his input or telling him
the name of the internal state where the parse failed in
an ATN parser. Such interaction requires a linguisti-
cally and computationally sophisticated user. Moreo-
ver, it is highly non-focused from the user&apos;s point of
view since it requires him to translate the parser&apos;s view
of the problem into one that has meaning within the
task domain, thereby switching contexts from perform-
</bodyText>
<page confidence="0.791819">
136 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
</page>
<note confidence="0.829678">
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.997421794871795">
ance of the task to linguistic issues. This enforced
digression places an undue cognitive load on the user
and should be avoided.
The final guideline is to minimize the amount of
corrective interactions that occur. It is very tedious
for a user to be confronted with questions about what
he meant after almost every input, or as Codd (1974)
has suggested, to approve a paraphrase of each input
before the system does anything. Clearly, there are
situations when the user must be asked a direct ques-
tion, to wit, when information is missing or in the
presence of real ambiguity. However, a technique not
requiring a reply is preferable when the system makes
assumptions that are very likely to be correct, or when
there are strong preferences for one alternative among
several in ambiguity, anaphora, or ellipsis resolution.
The echoing technique mentioned in Section 4.1 is
very useful in keeping required user replies to a mini-
mum while still allowing the user to overrule any un-
warranted assumptions on the part of the system. The
trick is for the system to incorporate any assumptions
it makes into its next output, so the user can see what
it has understood, correct it if it is wrong, and ignore
it if it is correct:
User: Add two dual ported rotating mass storage
devices to my order
System: What storage capacity should the two dual
ported disks have?
Here the system informs the user of its assumption
about the meaning of &amp;quot;rotating mass storage device&amp;quot;
(possible because only disks have dual ports) without
asking him directly if he means &amp;quot;disk&amp;quot;.
This section has given a brief glimpse of some of
the dialogue issues that arise in a robust parsing sys-
tem. The overriding point here is that robust parsing
techniques do not stop at the single sentence level.
Instead, they must be integrated with dialogue tech-
niques that allow for active user cooperation as a re-
covery strategy of last resort.
</bodyText>
<sectionHeader confidence="0.978842" genericHeader="evaluation">
5. Experiments in Robust Parsing
</sectionHeader>
<bodyText confidence="0.99984413559322">
Having examined various kinds of extragrammaticality
and the kinds of recovery strategies required to handle
them, we turn finally to a series of experiments we
have conducted or plan to conduct in robust parsing.
Before describing some of the parsers involved in
these experiments, we summarize some of the broad
lessons that can be drawn from our earlier discussion.
These observations have had a major role in guiding
the design of our experimental systems.
o• The parsing process should be as interpretive as
possible. We have seen several times the need for
a parsing process to &amp;quot;stand back&amp;quot; and look at a
broad picture of the set of expectations (or gram-
mar) it is applying to the input when an ungram-
maticality arises. The more interpretive a parser is,
the better able it is to do this. A highly interpre-
tive parser is also better able to apply its expecta-
tions to the input in more than one way, which may
be crucial if the standard way does not work in the
face of an ungrammaticality.
■ The parsing process should make it easy to apply
semantic information. As we have seen, semantic
information is often very important in resolving
ungrammaticality.
■ The parsing process should be able to take advan-
tage of non-uniformity in language like that identi-
fied in Section 3.5.2. As we have seen, recovery
can be much more efficient and reliable if a parser
is able to make use of variations in ease of recogni-
tion or discriminating power between different con-
stituents. This kind of &amp;quot;opportunism&amp;quot; can be built
into recovery strategies.
■ The parsing process should be capable of operating
top-down as well as bottom-up. We have seen
examples where both of these modes are essential.
Our earliest experiments in robust parsing were
conducted through the FlexP parsing system (Hayes
and Mouradian 1981). This system was based on
partial pattern matching, and while it had the first and
last of the characteristics listed above, it did not meas-
ure up well to the other two. Indeed, many of our
ideas on the importance of those characteristics were
developed though observation of FlexP&apos;s shortcomings
as described in 3.5.2, and more fully in Hayes and
Carbonell (1981). With these lessons in mind, we
constructed two additional experimental parsers:
CASPAR to explore the utility of case frame instantia-
tion in robust parsing, and DYPAR to explore the no-
tion of combining several different parsing strategies
in a single parser. Both experiments proved fruitful,
as the next two sections show, and DYPAR has now
been developed into a complete parsing system, the
DYPAR-II parser, as part of the XCALIBUR expert
system interface (Carbonell et al. 1983). After that,
we describe an approach to parsing we are currently
developing that we believe to be based on the best
features of both systems. A final section discusses
other methods and approaches that we consider prom-
ising avenues for future research.
</bodyText>
<subsectionHeader confidence="0.983279">
5.1. The CASPAR parser
</subsectionHeader>
<bodyText confidence="0.9430557">
As our earlier discussion on sentential-level ungram-
maticality pointed out, case frame instantiation ap-
pears to have many advantages as a framework for
robust parsing. Our initial experiments in realizing
these advantages were conducted through the CASPAR
parser (Hayes and Carbonell 1981). CASPAR was
restricted in coverage, but could deal with simple im-
perative verb phrases (that is, imperative verbs fol-
lowed by a sequence of noun phrases possibly marked
by prepositions) in a very robust way.
</bodyText>
<note confidence="0.781241">
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 137
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.978518384615385">
Examples of grammatical input for CASPAR (drawn
from an interface to a data base keeping track of
course registration at a university) include:
Cancel math 247
Enrol Jim Campbell in English 324
Transfer student 5518 from Economics 101 to
Business Administration 111
Such constructions are classic examples of case con-
structions; the verb or command is the central con-
cept, and the noun phrases or arguments are its cases.
Considered as surface cases, the command arguments
are either marked by prepositions, or unmarked and
identified by position, such as the position of direct
object in the examples above.
The types of grammatical deviation that CASPAR
could deal with include:
■ Unexpected and unrecognizable (to the system)
interjections as in:
+S+Q+ S3 Enrol if you don&apos;t mind student
2476 I think in Economics 247.
■ missing case markers:
Enrol Jim Campbell Economics 247.
■ out of order cases:
In Economics 247 Jim Campbell enrol.
■ ambiguous cases:
Transfer Jim Campbell Economics 247 English
332.
Combinations of these ungrammaticalities could also
be dealt with.
CASPAR used a parsing strategy specifically de-
signed to exploit the recognition characteristics of
imperative case frames, viz, that the prepositions used
to mark cases are much easier to recognize than their
corresponding case fillers. Below the clause level,
CASPAR used linear pattern matching to recognize
lower level constituents, which were defined in seman-
tic terms appropriate to the restricted domain in which
CASPAR was used. The algorithm used by CASPAR
was as follows:
</bodyText>
<listItem confidence="0.9072005">
1. Starting from the left of the input string, apply
the linear pattern matcher in scanning mode4 us-
ing all the patterns which correspond to impera-
tive verbs (commands). If this succeeds, the
</listItem>
<bodyText confidence="0.793455">
3 The reason for including these particular extraneous charac-
ters will be easily guessed by users of certain computers.
4 The linear pattern matcher may be operated in anchored
mode, where it tries to match one of a number of linear patterns
starting at a fixed word in the input, or in scanning mode, where it
tries to match the patterns it is given at successive points in the
input string until one of the patterns matches, or it reaches the end
of the string.
command corresponding to the pattern that
matched becomes the current command, and the
remainder of the input string is parsed relative to
its domain-specific case frame. If it fails,
CASPAR cannot parse the input.
</bodyText>
<listItem confidence="0.874187">
2. lithe current command has an unmarked direct
object case, apply the linear pattern matcher in
anchored mode at the next5 word using the set of
patterns appropriate to the type of object that
should fill the case. If this succeeds, record the
filler thus obtained as the filler for the case.
3. Starting from the next word, apply the pattern
matcher in scanning mode using the patterns cor-
responding to the surface markers of all the mark-
ed cases that have not yet been filled. If this
fails, terminate.
4. If the last step succeeds, CASPAR selects a mark-
ed case — the one from which the successful pat-
tern came. Apply the matcher in anchored mode
at the next word using the set of patterns appro-
priate to the type of object that should fill the
case selected. If this succeeds record the filler
thus obtained as the filler for the case.
5. Go to step 3.
</listItem>
<bodyText confidence="0.99837028">
Unless the input turns out to be completely unparsa-
ble, this algorithm will produce a command and a
(possibly incomplete) set of arguments. It is also in-
sensitive to spurious input immediately preceding a
case marker. However, it is not able to deal with any
of the other ungrammaticalities mentioned above.
Dealing with them involves going back over any parts
of the input that were skipped by the pattern matcher
in scanning mode. If, after the above algorithm has
terminated, there are any such skipped substrings, and
there are also arguments to the command that have
not been filled, the pattern matcher is applied in scan-
ning mode to each of the skipped substrings using the
patterns corresponding to the filler types of the un-
filled arguments. This will pick up any arguments
which were misplaced, or had garbled or missing case
markers.
This algorithm would deal with, for instance, the
convoluted example:
To Economics 247 Jim Campbell transfer please
from Mathematics 121
as follows:
i■ The initial scan for a command verb would find
&amp;quot;transfer&amp;quot;, and thus cause all further parsing to be
in terms of the case frame for that command.
</bodyText>
<footnote confidence="0.81923175">
5 The word after the last one the pattern matcher matched
the last time it was applied. If some input was skipped in finding
the verb in step I, this is tacked onto the end of the sequence used
by the next operation.
</footnote>
<page confidence="0.57293">
138 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
</page>
<note confidence="0.905162">
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.981101871428572">
■ The direct object required by &amp;quot;transfer&amp;quot; would not
be found its expected place, after the verb, so
CASPAR would skip to look for a case marker.
■ The case marker &amp;quot;from&amp;quot; would be found, and
CASPAR would subsequently recognize the case
marked by &amp;quot;from&amp;quot; and put it in the source course
slot of the transfer case frame.
■ The end of the input is then reached, but some cas-
es remain unfilled, so CASPAR goes into skipping
mode looking for case markers on the missed initial
segment and finds the destination course case.
■ Now only the &apos;Jim Campbell&apos; and &apos;please&apos; segments
are left and the student case is left unfilled, so
CASPAR can fill the student case correctly, and has
&apos;please&apos; left over as spurious input.
While limited in its scope of coverage, CASPAR
provides a practical demonstration of how well case
frame instantiation fulfills our list of desiderata for
robust parsing.
■ CASPAR uses its case frames in a highly interpretive
manner. It can, for instance, search directly after
the verb for the filler of a case which is expected to
be the direct object, but if that does not work, it is
prepared to recognize the same case elsewhere in
the input. Also, when it deals with out of order
input, it &amp;quot;steps back&amp;quot; and takes a broad view by
only considering unparsed input segments as poten-
tial fillers of cases that have not yet been filled.
■ The case frame representation makes it easy to
bring semantic information to bear, e.g. restrictions
on what can fill each case, considerations of which
cases are optional or mandatory, and whether any
cases can have fillers that impose pragmatic const-
raints.
■ CASPAR also shows the ability of case frame instan-
tiation to exploit variations in importance and ease
of recognition among different constituents. The
power of exploiting such variations is shown both
by the range of grammatical deviations CASPAR
can handle, and by the efficiency it displays in
straightforward parsing of grammatical input. This
efficiency is derived from the limited number of
patterns that the pattern matcher has to deal with
at any one time. On its first application, the
matcher only deals with command patterns; on sub-
sequent applications, it alternates between the pat-
terns for the markers of the unfilled cases of the
current command, and the patterns for a specific
object type. Also, except in post-processing of
skipped input, only case marker and command pat-
terns are employed when the pattern matcher is in
its less efficient scanning mode. The constituents
that are more difficult to recognize (e.g., object
descriptions) are processed in the more efficient
anchored mode.
Only in its predominance of top-down versus
bottom-up processing does CASPAR fail to meet
our desiderata. The only bottom-up component to
CASPAR is the initial verb recognition phrase. If
the verb were not there, it would be completely
unable to parse. An extension to CASPAR to ame-
liorate this problem would be to start parsing case
fillers bottom-up, and hypothesize the existence of
the verb whose case frame most closely matched
the set of case fillers found (or ask the user if there
was no clear choice). This is obviously a much less
efficient mode of operation than the one presented
above, but it illustrates a way in which the basic
case frame information could be interpreted to deal
with the lack of a recognizable case header.
</bodyText>
<subsectionHeader confidence="0.994793">
5.2. The DYPAR parser
</subsectionHeader>
<bodyText confidence="0.998363178571429">
DYPAR originated as an experimental vehicle to test
the feasibility and potential benefits of combining
multiple parsing strategies into a uniform framework.
Initially, three parsing strategies (pattern matching,
semantic grammar interpretation, and syntactic trans-
formations) were combined. Transformations were
used to reduce variant sentential structures to canoni-
cal form. In addition to a large set of operators,6 the
patterns could contain recursive non-terminal sub-
constituents corresponding to semantic grammar cate-
gories or other subconstituents. Each grammar non-
terminal could expand to a full pattern containing
additional non-terminals.
The experiment proved successful in that DYPAR
allowed one to write grammars at least an order of
magnitude more concise than pure semantic grammars
of equivalent coverage. This version of the system is
called DYPAR-I (Boggs, Carbonell, and Monarch
1983) and has been made available for general use.
Subsequently, case frame instantiation was introduced
as the new dominant strategy, and the new system,
DYPAR-II, is currently used as the experimental parser
for XCALIBUR, a natural language interface to expert
systems (Carbonell et al. 1983).
The multi-strategy approach to parsing grammatical
input in DYPAR-II facilitated the introduction of sever-
al additional strategies to recover from different kinds
of extragrammaticality:
</bodyText>
<listItem confidence="0.969157">
■ Spelling correction combined with morphological
decomposition of inflected words.
■ Bridging garbled or spurious phrases in otherwise
comprehensible input.
■ Recognizing constituents when they occur in unex-
pected order in the input.
■ Generalized case frame ellipsis resolution, exploit-
ing strong domain semantics.
</listItem>
<footnote confidence="0.8561885">
6 Operators in DYPAR-I include: matching arbitrary subcon-
stituent repetition, optional constituents, free permutation matches,
register assignment and reference, forbidden constituents, and
anchored and scanning modes.
</footnote>
<note confidence="0.9178665">
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 139
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.999831538461539">
The two sentential-level recovery strategies (second
and third on the list above) were inspired by, and
largely patterned after, the corresponding strategies in
CASPAR, therefore little additional commentary is
required. However, an additional complication in
DYPAR-II is that the case frame instantiation process
recognizes recursively embedded case frames, and in
the presence of ill-formed input must deal with multi-
ple levels of expectations. Were it not for strong do-
main semantics, this additional source of complexity
would have introduced some ambiguity in the correc-
tion process requiring additional interaction with the
user.
</bodyText>
<subsectionHeader confidence="0.9690865">
5.2.1. Spelling correction and morphology in
DYPAR
</subsectionHeader>
<bodyText confidence="0.98213387654321">
DYPAR combines expectation-based spelling correction
and morphological decomposition of inflected words.
Since the DYPAR grammars are compiled into a cross-
referenced form that indexes dictionary entries from
patterns, it proved simple to generate lists of expected
words when encountering an unrecognizable term.
Although often the lists were short (highly constrained
by expectations), on occasion a substantial fraction of
the dictionary was generated.
Since spelling correction interacts with morphologi-
cal decomposition, the two were combined into a sin-
gle recovery algorithm. Here we present a somewhat
simplified form of the algorithm in which the only
morphological operations allowed are on word endings
(e.g., singularization and other suffix stripping opera-
tions).
1. Define the reduced dictionary to be the set of ex-
pected words at the point in the parse where the
unrecognized word was found. This set may con-
tain expected or allowed morphological inflexions
and variants, as well as root forms of words.
2. Morphological decomposition phase — If the word
(plus any accompanying morphological informa-
tion) is a member of the reduced dictionary, return
it and exit.
3. Attempt to perform a one level morphological oper-
ation on the current word (e.g., stripping a legal
suffix)
a. If successful, set the word to the decomposed
form (e.g. root and suffix), save the potential
decomposition on a list, and go to step 2.
b. If no morphological operation is possible, go to
the spelling correction phase (step 4). Only legal
sequences of suffixes are allowed.
4. Spelling correction phase — For each element in the
list of possible decompositions (starting with the
original unrecognized word), apply the spelling
correction algorithm to the root word using the
reduced dictionary as the candidate correction set.
a. If successful, return the corrected word (along
with any morphological information) and exit.
b. If no spelling correction is possible, go on to the
next proposed decomposition.
5. If no proposed morphological decomposition yields
a recognizable root, either by direct match or spell-
ing correction, exit the algorithm with a failure
condition.
Clearly this strategy incorporates a best-match or
minimal-correction criterion, rather than generating
the set of all possible corrections. Moreover, words
are only looked up in the reduced dictionary. This
means that misspellings into words that are in the full
dictionary but violate expectations (and are therefore
not members of the reduced dictionary) are handled in
the same manner as ordinary misspellings.
Let us trace this correction strategy on the word
&amp;quot;intrestingness&amp;quot;. Since that word is not recognized, we
enter the algorithm above and generate a reduced dic-
tionary. Assume that the reduced dictionary contains
the word &amp;quot;interest&amp;quot;, but none of its morphological
variants. First we strip the &amp;quot;ness&amp;quot; suffix, but the re-
sulting character string remains unrecognizable. Then
we strip the &amp;quot;ing&amp;quot; suffix with similar results. Finally
we strip off the coincidental &amp;quot;est&amp;quot; as a suffix and still
find no recognizable root. At this point, morphology
can do no more and the algorithm enters the spelling
correction phase with the following candidate
((root: (intrestingness) suffixes: 0)
(root: (intresting) suffixes: (ness))
(root: (intrest) suffixes: (ing ness))
(root: (intr) suffixes: (est ing ness))
Next, we attempt to spelling correct &amp;quot;intrestingness&amp;quot;
using the reduced dictionary and fail. We also fail with
&amp;quot;intresting&amp;quot;, but succeed with &amp;quot;intrest&amp;quot; and exit the
algorithm with the value
(root: (interest) suffixes: (ing ness))
and without considering the spurious &amp;quot;est&amp;quot; stripping.
Had the word been correctly spelt, or had any of the
compound morphological forms been inserted into the
dictionary explicitly, the algorithm would have suc-
ceeded and exited sooner.
</bodyText>
<subsectionHeader confidence="0.782962">
5.2.2. Ellipsis resolution
</subsectionHeader>
<bodyText confidence="0.978348636363636">
DYPAR-II utilizes a variant of the case frame ellipsis
resolution method discussed in Section 4.1. In addi-
tion to the general algorithm, it incorporates a method
for dealing with ellipsis when another component of
the XCALIBUR system has generated strong discourse
expectations. The ellipsed fragment is parsed in the
context of these expectations, as illustrated by the
recovery strategy below:
Exemplary discourse expectation rule:
IF: The system generated a query for confirmation
or disconfirmation of a proposed value of a
</bodyText>
<page confidence="0.604007">
140 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
</page>
<note confidence="0.823326">
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.9910665">
filler of a case in a case frame in focus,
THEN: EXPECT one or more of the following:
</bodyText>
<listItem confidence="0.842018">
1) A confirmation or disconfirmation pattern
appropriate to the query in focus.
</listItem>
<bodyText confidence="0.948086196078431">
2) A different but semantically permissible
filler of the case frame in question
(optionally naming the attribute or provid-
ing the case marker).
3) A comparative or evaluative pattern appro-
priate to the proposed value of the case in
focus.
4) A query about possible fillers or constraints
on possible fillers of the case in question.
[If this expectation is confirmed, a sub-
dialogue is entered, where previously fo-
cused entities remain in focus.]
The following dialogue fragment illustrates how
these expectations come into play in a focused dia-
logue:
&gt;Add a line printer with graphics capabilities.
Is 150 lines per minute acceptable?
&gt;No, 320 is better Expectations 1, 2 &amp; 3
(or) other options for the speed? Expectation 4
(or) Too slow, try 300 or faster Expectations 2 &amp; 3
The utterance &amp;quot;try 300 or faster&amp;quot; is syntactically a
complete sentence, but semantically it is just as frag-
mentary as the previous utterances. The strong dis-
course expectations suggest that it be processed in the
same manner as syntactically incomplete utterances,
since it satisfies the dialogue expectations listed above.
Thus, the terseness principle operates at all levels:
syntactic, semantic and pragmatic.
Additionally, DYPAR-II contains rules to ensure
semantic completeness of utterances even in the ab-
sence of specific discourse expectations. As we have
just seen, not all sentence fragments are fragmentary
in the syntactic sense. But not all such purely seman-
tic ellipsis can be predicted through dialogue generated
expectations.
&gt;Which fixed media disks are configurable on a
VAX780?
The RP07-aa, the RP07-ab,
&gt;Add the largest
In this example, there is no good basis for predicting
what the user will do in response to the information in
the answer to his question. His response turns out to
be semantically elliptical — we need to answer the
question &amp;quot;largest what?&amp;quot; before proceeding. One can
call this problem a special case of definite noun phrase
resolution, rather than semantic ellipsis, but terminol-
ogy is immaterial. Such phrases occur with regularity
in our corpus of examples and must be resolved by a
fairly general process. The following rule answers the
question from context, regardless of the syntactic com-
pleteness of the new utterance.
</bodyText>
<subsectionHeader confidence="0.931806">
Contextual substitution rule
</subsectionHeader>
<bodyText confidence="0.93267516">
IF: A command or query case frame lacks one or
more required case fillers, and the last case
frame in focus has an instantiated case that
meets all the semantic tests for the case miss-
ing the filler,
THEN: 1) Copy the filler onto the new case frame,
and
2) Attempt to copy uninstantiated case fillers
as well (if they meet semantic tests).
3) Echo the action being performed for im-
plicit confirmation by the user.
For the example above, the case frame with a missing
component is the selection case frame introduced by
&amp;quot;largest&amp;quot; that requires a set of components from
which to select. The previous (and therefore still fo-
cused) input has a set of disks in its only case slot and
this meets the semantic criteria for the selection slot;
hence it is copied over and used.
Rules such as the one above are fairly general in
coverage, and the statement of the rule is independent
of any specific case grammar or domain semantics.
The rules, however, rely on the presence of the same
specific case frames and the semantic constraints as
used in the normal parsing of isolated grammatical
constructions.
</bodyText>
<subsectionHeader confidence="0.992348">
5.3. Multi-strategy parsing
</subsectionHeader>
<bodyText confidence="0.99888528">
In addition to underscoring the importance of our four
desiderata for robust parsers listed at the beginning of
this section, our experiments with CASPAR and
DYPAR demonstrated that robustness can be achieved
by the use of several different parsing strategies on the
same input. These strategies operate both on gram-
matical input and as a means of recovery from un-
grammatical input. The notion of multiple strategies
fits very well with the four desiderata. In particular:
■ The required high degree of interpretiveness can be
obtained by having several different strategies ap-
ply the same grammatical information to the input
in several different ways.
■ Different strategies can be written to take advan-
tage of different aspects of non-uniformity for dif-
ferent construction types.
■ Some strategies can operate top-down and others
bottom up.
Nor, as we have seen in DYPAR, is a multiple strat-
egy approach inconsistent with our previous emphasis
on case frame instantiation as a suitable vehicle for
robust parsing. Indeed, many of the strategies re-
quired by a robust parser will be based on case frame
instantiation with all the flexibility that that entails.
However, case frame instantiation cannot carry the
</bodyText>
<note confidence="0.7013565">
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 141
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.999526333333333">
entire burden of robustness alone, and so must be
supplemented by other strategies such as the ones
present in DYPAR. In fact, even the method of case
frame instantiation presented for CASPAR can be seen
as two strategies: one an initial pass using standard
expectations, and the other a recovery strategy for
when the first fails. The bottom-up strategy discussed
at the end of the section on CASPAR would make a
third.
</bodyText>
<subsectionHeader confidence="0.991974">
5.3.1. Coordinating multiple strategies through
an entity-oriented approach
</subsectionHeader>
<bodyText confidence="0.999966444444444">
A major problem that arises in using multiple parsing
strategies is coordination between the strategies.
Questions of interaction and order of application are
involved. In CASPAR and DYPAR, the problem was
solved simply by &amp;quot;hard-wiring&amp;quot; the interactions, but
this is not satisfactory in general, especially if we wish
to extend the set of strategies available in a smooth
way. One alternative we have begun to explore in-
volves the idea of entity-oriented parsing (Hayes
1984).
The central notion behind entity-oriented parsing is
that the primary task of a natural language interface is
to recognize entities — objects, actions, states, com-
mands, etc. — from the domain of discourse of the
interface. This recognition may be recursive in the
sense that descriptions of entities may contain descrip-
tions of subsidiary entities (for example, commands
refer to objects).
In entity-oriented parsing, all the entities that a
particular interface system needs to recognize are de-
fined separately. These definitions contain informa-
tion both about the way the entities will be manifested
in the natural language input (this information can also
be used to generate output), and about the internal
semantic structure of the entities. This arrangement
has the following advantages for parsing robustness:
■ The individual entity definitions form an ideal
framework around which to organize multiple pars-
ing strategies. In particular, each definition can
specify which strategies are applicable to recogniz-
ing it. Of course, this only provides a framework
for robust recognition, the robustness achieved will
still depend on the quality of the various recogni-
tion strategies used.
■ The individual definition of all recognizable domain
entities allows them to be recognized independent-
ly. Assuming there is appropriate indexing of enti-
ties through lexical items that might appear in a
surface description of them, this recognition can be
done bottom-up, thus allowing for recognition of
elliptical, fragmentary, or partially incomprehensi-
ble input. The same definitions can also be used in
a more efficient top-down manner when the input
conforms to the system&apos;s expectations.
■ This style of organization is particularly well suited
to case frame instantiation. The appropriate case
frames can be associated with each entity definition
for use by case-oriented strategies. Of course, this
does not prevent other strategies from being used
to recognize the entity, so long as suitable informa-
tion for the other strategies to interpret is provided
in the entity definition.
These arguments can be made more concrete by exam-
ple.
</bodyText>
<subsectionHeader confidence="0.682414">
5.3.2. Example entity definitions
</subsectionHeader>
<bodyText confidence="0.997700714285714">
First we examine some example entity and language
definitions suitable for use in entity-oriented parsing.
The examples are drawn from the domain of an inter-
face to a data base of college courses. Here is the
(partial) definition of a course. Square brackets de-
note attribute/value lists, and round brackets ordinary
lists.
</bodyText>
<figure confidence="0.990496826086957">
EntityName: CollegeCourse
Type: Structured
Components: (
[ComponentName: CourseNumber
Type: Integer
GreaterThan: 99
LessThan: 1000
[ComponentName: CourseDepartment
Type: CollegeDepartment
[ComponentName: CourseClass
Type: CollegeClass
[ComponentName: CourseInstructor
Type: CollegeProfessor
Surf aceRepresentation: (
[SyntaxType: Pattern
Pattern: ($CourseDepartment $CourseNumber)
[SyntaxType: NounPhrase
Head: (course I seminar I ...)
AdjectivalComponents: (CourseDepartment ...)
Adjectives: (
[AdjectivalPhrase: (new I most recent)
Component: CourseSemester
Value: CurrentSemester
</figure>
<footnote confidence="0.551087">
PostNominalCases: (
[Case-marker: (?intended for I directed to I .••)
Component: CourseClass
</footnote>
<note confidence="0.5600985">
142 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
[Case-marker: (?taught by I .••) [ComponentName: EnrolIn
Component: CourseInstructor Type: CollegeCourse
</note>
<bodyText confidence="0.993904555555555">
Precise details of this language are not relevant here.
Important features to note include the definition of a
course as a structured object with components: num-
ber, department, instructor, etc.. This definition is
separate from the surface representation of a course
which is defined to take one of two forms: a simple
word pattern of the course department followed by the
course number (dollar signs refer back to the compo-
nents), or a full noun phrase with adjectives, post-
nominal cases, etc. Since we are assuming a multi-
strategy approach to parsing, the two quite different
kinds of surface language definition do not cause any
problem — they can both be applied to the input inde-
pendently by different construction-specific strategies,
and the one which accounts for the input best will be
used.
Subsidiary objects like CollegeDepartment are de-
fined in similar fashion.
</bodyText>
<figure confidence="0.9532178">
EntityName: CollegeDepartment
Type: Enumeration
EnumeratedValues: (
ComputerScienceDepartment
MathematicsDepartment
HistoryDepartment
SurfaceRepresentation: (
[SyntaxType: Pattern
Pattern: (CS I Computer Science I Comp Sci
Value: ComputerScienceDepartment
</figure>
<bodyText confidence="0.99773925">
CollegeCourse itself will be a subsidiary entity in
other higher-level entities of our restricted domain,
such as a command to the data base system to enrol a
student in a course.
</bodyText>
<figure confidence="0.617972166666667">
EntityName: EnrolCommand
Type: Structured
Components: (
[ComponentName: Enrollee
Type: CollegeStudent
SurfaceRepresentation: (
[SyntaxType: ImperativeCaseFrame
Head: (enrol I register I include I •••)
DirectObject: ($Enrollee)
Cases: (
[Case-marker: (in I into I ...)
Component: EnrolIn
</figure>
<subsectionHeader confidence="0.889245">
5.3.3. Parsing with an entity-oriented approach
</subsectionHeader>
<bodyText confidence="0.98358566">
Now we turn to the question of how language defini-
tions like those in the examples just given can be used
to drive a parser. Let us examine first how a simple
data base command like
Enrol Susan Smith in CS 101
might be parsed using the above language definitions.
The first job is to recognize that we are parsing an
EnrolCommand. In a purely top-down system, we
would establish this by having a list of all the entities
that we are prepared to recognize as complete inputs
and trying each one of these to see if they could be
recognized, a rather inefficient process. A more natu-
ral strategy in an entity-oriented approach is to try to
index bottom-up from words in the input to those
entities that they might appear in. In this case, the
best indexer for EnrolCommand is the first word,
&apos;enrol&apos;. In general, the best indexer need not be the
first word of the input and we need to consider all
words, thus raising the potential of indexing more than
one entity. Hence we might also index CollegeStu-
dent, CollegeCourse, and CollegeDepartment. A sim-
ple method of cutting down the number of index-
generated possibilities to investigate top-down is to
eliminate all those that are subsidiary to others that
have been indexed. For our example, this would elim-
inate everything except EnrolCommand, the desired
result. One final point about indexing: it is clearly
undesirable to index from every word that could ap-
pear in the surface representation of an entity; only
highly discriminating words like &apos;enrol&apos; or &apos;CS&apos; should
be used. Whether a word is sufficiently discriminating
can be determined either manually, which is unreliable,
or automatically by keeping a count of the number of
entities indexed by a given word and removing it from
the index if it indexes more than a certain threshold
number.
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 143
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
Once EnrolCommand has been established as the
entity to recognize in the above example, the remain-
der of the recognition can be accomplished straightfor-
wardly in a top-down manner. The definition of the
surface representation of EnrolCommand is an impera-
tive case frame with a CollegeStudent as direct object
and with a CollegeCourse as a second case indicated
by &apos;in&apos;. This information can be used directly by a
simple case frame recognition strategy of the type used
in CASPAR. No translation into a structurally differ-
ent representation is necessary. The most natural way
to represent the resulting parse would be:
</bodyText>
<reference confidence="0.966116285714286">
[InstanceOf: EnrolCommand
Enrollee: [InstanceOf: CollegeStudent
FirstNames: (Susan)
Surname: Smith
EnrolIn: [InstanceOf: CollegeCourse
CourseDepartment: ComputerScienceDepartment
CourseNumber: 101
</reference>
<bodyText confidence="0.999865127906977">
Note how this parse result is expressed in terms of the
underlying structural representation used in the entity
definitions without the need for a separate semantic
interpretation step.
To see the possibilities for robustness with the
entity-oriented approach, consider the input:
Place Susan Smith in computer science for fresh-
men
There are two problems here: we assume that the user
intended &apos;place&apos; as a synonym for &apos;enrol&apos;, but that it
happens not to be in the system&apos;s vocabulary; the user
has also shortened the grammatically acceptable
phrase, &apos;the computer science course for freshmen&apos;, to
an equivalent phrase not covered by the surface repre-
sentation for CollegeCourse as defined above. Since
&apos;place&apos; is not a synonym for &apos;enrol&apos; in the language as
presently defined, we cannot index EnrolCommand
from it and hence cannot get the same kind of top-
down recognition as before. So we are forced to rec-
ognize smaller fragments bottom-up. Let&apos;s assume we
have a complete listing of students and so can recog-
nize &apos;Susan Smith&apos; as a student. That leaves &apos;computer
science for freshmen&apos;. We can recognize &apos;computer
science&apos; as a CollegeDepartment and &apos;freshmen&apos; as a
CollegeClass, so since they are both components of
CollegeCourse, we can attempt to unify our currently
fragmentary recognition by trying to recognize a
course description from the segment of the input that
they span, viz. &apos;computer science for freshmen&apos;.
There are two possible surface representations giv-
en for CollegeCourse. The first, a pattern, is partially
matched by &apos;computer science&apos;, but does not unify the
two fragments. The second, a noun phrase accounts
for both of the fragments (one is adjectival, the other
part of a post-nominal case), but would not normally
match them because the head noun is missing. In
fragment recognition mode, however, this kind of gap
is acceptable, and the phrase can be accepted as a
description of a CollegeCourse with ComputerScien-
ceDepartment as CourseDepartment, and Freshman-
Class as CourseClass.
The input still consists of two fragments, however,
a CollegeStudent and a CollegeCourse, and since we
do not have any information about the word &apos;place&apos;,
we are forced to consider all the entities that have
those two sub-entities as components. We will sup-
pose there are three: EnrolCommand, WithdrawCom-
mand, and TransferCommand (with the obvious inter-
pretations). Trying to recognize each of these, we can
rule out TransferCommand in favour of the first two
because it requires two courses and we only have one.
Also, EnrolCommand is preferred to WithdrawCom-
mand since the preposition &apos;in&apos; indicates the EnrolIn
case of EnrolCommand, but does not indicate With-
drawFrom, the course-containing case of Withdraw-
Command. Thus we can conclude that the user in-
tended an EnrolCommand.
In following this bottom-up fragment combination
procedure, we have ignored other combination possi-
bilities that did not lead to the correct answer — for
instance, taking &apos;Computer Science&apos; as the StudentDe-
partment case of the CollegeStudent, &apos;Susan Smith&apos;.
In practice, an algorithm for bottom-up fragment com-
bination would have to consider all such possibilities.
However, if, as in this case, the combination did not
turn out to fit into a higher-level combination that
accounted for all of the input, it could be discarded in
favour of combinations that did lead to a complete
parse. More than one complete parse would be han-
dled, just like any other ambiguity, through focused
interaction.
Even assuming that the above example had a uni-
que result, since it involved several significant assump-
tions, we would need to use focused interaction tech-
niques (Hayes 1981) to present a paraphrase of our
interpretation to the user for approval before acting on
it. Note that if the user does approve it, we should be
able (perhaps with further approval) to add &apos;place&apos; to
the vocabulary as a synonym for &apos;enrol&apos; since &apos;place&apos;
was an unrecognized word in the surface position
where &apos;enrol&apos; should have been.
A pilot implementation of a parser constructed
according to the entity-oriented principles outlined
above has been completed and preliminary evaluation
is promising. We are hoping to build a more complete
parser along these lines.
</bodyText>
<sectionHeader confidence="0.94634" genericHeader="conclusions">
6. Concluding Remarks
</sectionHeader>
<bodyText confidence="0.9991325">
Any practical natural language interface must be capa-
ble of dealing with a wide range of extragrammatical
</bodyText>
<page confidence="0.732217">
144 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
</page>
<note confidence="0.630206">
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
</note>
<bodyText confidence="0.999944815789474">
input. This paper has proposed a taxonomy of the
prevalent forms of extragrammaticality in real lan-
guage use and presented recovery strategies for many
of them. We also discussed how well various ap-
proaches to parsing could support the recovery strate-
gies, and concluded that case frame instantiation pro-
vided the best framework among the commonly used
parsing methodologies.
At a more general level, we argued that the superi-
ority of case frame instantiation over other parsing
methodologies for robust parsing is due to how well it
satisfies four parsing characteristics that are important
for many of the recovery strategies that we described:
■ The parsing process should be as interpretive as
possible.
■ The parsing process should make it easy to apply
semantic information.
■ The parsing process should be able to take advan-
tage of non-uniformity in language.
■ The parsing process should be capable of operating
top-down as well as bottom-up.
We claimed that while case frame instantiation satis-
fies these desiderata better than any other commonly
used parsing methodology, it was possible to do even
better by using a multi-strategy approach in which
case frame instantiation was just one member (albeit a
very important one) of a whole array of parsing and
recovery strategies. We described some experiments
that led us to this view and outlined a parsing metho-
dology, entity-oriented parsing, that we believe will
support a multi-strategy approach.
It is our hope that by pursuing lines of research
leading to parsers that maximize the characteristics
listed above, we can approach, in semantically limited
domains, the extraordinary degree of robustness in
language recognition exhibited by human beings, and
gain some insights into how robustness might be
achieved in more general language settings.
</bodyText>
<sectionHeader confidence="0.973038" genericHeader="references">
7. References
</sectionHeader>
<reference confidence="0.999811050420168">
Bobrow, D.G. and Winograd, T. 1977 An Overview of KRL, a
Knowledge Representation Language. Cognitive Science 1(1):
3-46.
Bobrow, R.J. 1978 The RUS System. BBN Report 3878. Bolt
Beranek and Newman, Cambridge, Massachuseets.
Bobrow, R.J. and Webber, B. 1980 Knowledge Representation for
Syntactic/Semantic Processing. Proc. National Conference of the
American Association for Artificial Intelligence. Stanford Univer-
sity, Stanford, California (August).
Bobrow, D.G.; Kaplan, R.M.; Kay, M.; Norman D.A.; Thompson,
H.; and Winograd, T. 1977 GUS: a Frame-Driven Dialogue
System. Artificial Intelligence 8: 155-173.
Boggs, W.M. and Carbonell, J.G. and Monarch, I. 1983 The
DYPAR-I Tutorial and Reference Manual. Technical report.
Computer Science Department, Carnegie-Mellon University,
Pittsburgh, Pennsylvania.
Brown, J.S. and Burton, R.R. 1975 Multiple Representations of
Knowledge for Tutorial Reasoning. In Bobrow, D.G. and Col-
lins, A., eds. Representation and Understanding. Academic Press,
New York, New York: 311-349.
Carbonell, J.G. 1979 Towards a Self-Extending Parser. Proceed-
ings of the 17th Meeting of the Association for Computational
Linguistics: 3-7.
Carbonell, J.G. and Hayes, P.J. 1984 Robust Parsing Using Multi-
ple Construction-Specific Strategies. In Bole, L., ed., Natural
Language Parsing Systems. Springer-Verlag, New York, New
York.
Carbonell, J.G.; Boggs, W.M.; Mauldin, M.L.; and Anick, P.G.
1983 The XCALIBUR Project, A Natural Language Interface
to Expert Systems. Proceedings of the Eighth International Joint
Conference on Artificial Intelligence.
Carbonell, J.G.; Boggs, W.M.; Mauldin, M.L.; and Anick, P.G.
1983 XCALIBUR Progress Report #1: First Steps Towards an
Integrated Natural Language Interface. Technical report.
Computer Science Department, Carnegie-Mellon University,
Pittsburgh, Pennsylvania.
Carbonell, J.G. 1982 Meta-Language Utterances in Purposive
Discourse. Technical report. Computer Science Department,
Carnegie-Mellon University, Pittsburgh, Pennsylvania.
Carbonell, J.G. 1983 Discourse Pragmatics in Task-Oriented
Natural Language Interfaces. Proceedings of the 21st Annual
Meeting of the Association for Computational Linguistics.
Codd, E.F. 1974 Seven Steps to RENDEZVOUS with the Casual
User In Klimbie, J.W. and Koffeman, K.L., eds., Proceedings of
the IFIP TC-2 Working Conference on Data Base Management
Systems. North Holland, Amsterdam: 179-200.
Dejong, G. 1979 Skimming Stories in Real-Time. Ph.D. disserta-
tion. Computer Science Department, Yale University, New
Haven, Connecticut.
Durham, I.; Lamb, D.D.; and Saxe, J.B. 1983 Spelling Correction
in User Interfaces. Communications of the ACM 26.
Haas, N. and Hendrix, G.G. 1983 Learning by Being Told: Ac-
quiring Knowledge for Information Management. In Michalski,
R.S.; Carbonell, J.G.; and Mitchell, T.M., eds., Machine Learn-
ing, An Artificial Intelligence Approach. Tioga Press, Palo Alto,
California.
Hayes, P.J. 1981 A Construction Specific Approach to Focused
Interaction in Flexible Parsing. Proceedings of 19th Annual
Meeting of the Association for Computational Linguistics (June):
149-152.
Hayes, P.J. 1984 Entity-Oriented Parsing. COLING84, Stanford
University, Stanford, California (July).
Hayes, P.J. and Mouradian, G.V. 1981 Flexible Parsing. Ameri-
can Journal of Computational Linguistics 7(4); 232-241.
Hayes, P.J. and Carbonell, J.G. 1981 Multi-strategy Construction-
Specific Parsing for Flexible Data Base Query and Update.
Proceedings of the Seventh International Joint Conference on Arti-
ficial Intelligence. Vancouver (August): 432-439.
Hayes, P.J. and Reddy, D.R. 1983 Steps Toward Graceful Interac-
tion. Spoken and Written Man-Machine Communication, Interna-
tional Journal of Man-Machine Studies. 19(3): 211-284.
Hayes, P.J. and Carbonell, J.G. 1981 Multi-Strategy Parsing and
its Role in Robust Man-Machine Communication. Technical
report CMU-CS-81-1 18. Computer Science Department,
Carnegie-Mellon University, Pittsburgh, Pennsylvania (May).
Hendrix, G.G. 1977 Human Engineering for Applied Natural
Language Processing. Proceedings of the Fifth International Joint
Conference on Artificial Intelligence: 183-191.
Kwasny, S.C. and Sondheimer, N.K. 1981 Relaxation Techniques
for Parsing Grammatically Ill-Formed Input in Natural Lan-
guage Understanding Systems. American Journal of Computa-
tional Linguistics 7(2): 99-108.
Sacerdoti, E.D. 1977 Language Access to Distributed Data with
Error Recovery. Proceedings of the Fifth International Joint
Conference on Artificial Intelligence: 196-202.
Schank, R.C.; Lebowitz, M.; and Birnbaum, L. 1980 An Integrat-
ed Understander. American Journal of Computational Linguistics
6(1): 13-30.
Waltz, D.L. 1978 An English Language Question Answering
System for a Large Relational Data Base, Communications of the
ACM 21(7): 526-539.
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 145
Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language
Waltz, D.L. and Goodman, A.B. 1977 Writing a Natural Language
Data Base System. Proceedings of the Fifth International Joint
Conference on Artificial Intelligence: 144-150.
Weischedel, R.M. and Sondheimer, N.K. 1983 Meta-Rules as a
Basis for Processing Ill-formed Input. American Journal of
Computational Linguistics 9(3-4): 161-177.
Weischedel, R.M. and Black, J. 1980 Responding to Potentially
Unparseable Sentences. American Journal of Computational
Linguistics 6: 97-109.
Wilks, Y. A. 1975 Preference Semantics. In Keenan, ed., Formal
Semantics of Natural Lanbuage. Cambridge University Press,
Cambridge, England.
Woods, W. A. 1980 Cascaded ATN Grammars. American Journal
of Computational Linguistics 6: 1-12.
Woods, W.A.; Kaplan, R.M.; and Nash-Webber, B. 1972 The
Lunar Sciences Language System: Final Report. Technical
report 2378. Bolt Beranek and Newman, Cambridge, Massa-
chusetts.
Woods, W.A.; Bates, M.; Brown, G.; Bruce, B.; Cook, C.; Klovst-
ad, J.; Mak houl, J.; Nash-Webber, B.; Schwartz, R.; Wolf, J.;
and Zue, V. 1976 Speech Understanding Systems — Final
Technical Report. Technical report 3438. Bolt Beranek and
Newman, Cambridge, Massachusetts.
Wright, K. and Fox, M 1983 The SRL Users Manual. Technical
report. Robotics Institute, Carnegie-Mellon University, Pitts-
burgh, Pennsylvania.
</reference>
<page confidence="0.924842">
146 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.862328">
<title confidence="0.9986195">Recovery Strategies Parsing Extragrammatical Languagel</title>
<author confidence="0.998726">G Carbonell</author>
<author confidence="0.998726">J Philip</author>
<affiliation confidence="0.987802">Computer Science Carnegie-Mellon</affiliation>
<address confidence="0.997767">Pittsburgh, PA 15213</address>
<abstract confidence="0.983503">Practical natural language interfaces must exhibit robust behaviour in the presence of extragrammatical user input. This paper classifies different types of grammatical deviations and related phenomena at the lexical, sentential and dialogue levels and presents recovery strategies tailored to specific phenomena in the classification. Such strategies constitute a tool chest of computationally tractable methods for coping with extragrammaticality in restricted domain natural language. Some of the strategies have been tested and proven viable in existing parsers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Enrollee: [InstanceOf: CollegeStudent FirstNames: (Susan) Surname: Smith EnrolIn: [InstanceOf: CollegeCourse CourseDepartment: ComputerScienceDepartment CourseNumber:</title>
<pages>101</pages>
<marker></marker>
<rawString>[InstanceOf: EnrolCommand Enrollee: [InstanceOf: CollegeStudent FirstNames: (Susan) Surname: Smith EnrolIn: [InstanceOf: CollegeCourse CourseDepartment: ComputerScienceDepartment CourseNumber: 101</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Bobrow</author>
<author>T Winograd</author>
</authors>
<title>An Overview of KRL, a Knowledge Representation Language.</title>
<date>1977</date>
<journal>Cognitive Science</journal>
<volume>1</volume>
<issue>1</issue>
<pages>3--46</pages>
<contexts>
<context position="51065" citStr="Bobrow and Winograd 1977" startWordPosition="8186" endWordPosition="8189"> other cases, if present, are optional. This knowledge of the underlying action must be brought to bear at parse time to disambiguate the cryptic command. In the XCALIBUR system case frame encoding (Carbonell, Boggs, Mauldin, and Anick 1983), we apply precisely such pragmatic knowledge represented as preference constraints (cf. Wilks 1975) on case fillers at parse time. Thus, problems created by the absence of expected case markers can be overcome by the application of domain knowledge. ■ The propagation of semantic knowledge through a case frame (via attached procedures such as those of KRL (Bobrow and Winograd 1977) or SRL (Wright and Fox 1983)) can fill in parser defaults and allow the internal completion of phrases such as &amp;quot;dual disks&amp;quot; to mean &amp;quot;dual ported disks&amp;quot;. This process is also responsible for noticing when information is either missing or ambiguously determined, thereby initiating a focused clarificational dialogue (Hayes 1981). ■ The representation of case frames is inherently non-uniform. Case fillers, case markers, and case headers are all represented separately, and this distinction can be used by the parser interpretively instantiating the case frame. For instance, if a case frame accounts</context>
</contexts>
<marker>Bobrow, Winograd, 1977</marker>
<rawString>Bobrow, D.G. and Winograd, T. 1977 An Overview of KRL, a Knowledge Representation Language. Cognitive Science 1(1): 3-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Bobrow</author>
</authors>
<title>The RUS System.</title>
<date>1978</date>
<tech>BBN Report 3878.</tech>
<institution>Bolt Beranek and Newman,</institution>
<location>Cambridge, Massachuseets.</location>
<contexts>
<context position="43136" citStr="Bobrow 1978" startWordPosition="6905" endWordPosition="6906">ry important in recovering from many types of ungrammatical input, and these are by definition unavailable 130 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language in a purely syntactic ATN parser. However, semantic information can be brought to bear on network based parsing, either through the semantic grammar approach in which joint semantic and syntactic categories are used directly in the ATN, or by allowing the tests on ATN arcs to depend on semantic criteria (Bobrow 1978, Bobrow and Webber 1980). In the former technique, the appropriate semantic information for recovery can be applied only if the correct network node can be located — a sometimes difficult task as we have seen. In the latter technique, sometimes known as cascaded ATNs (Woods 1980), the syntactic and semantic parts of the grammar are kept separate, thus giving the potential for a higher degree of interpretiveness in using the semantic information. However, the natural way to use this technique is to employ the semantic information only to confirm or disconfirm parses arrived at on syntactic gro</context>
</contexts>
<marker>Bobrow, 1978</marker>
<rawString>Bobrow, R.J. 1978 The RUS System. BBN Report 3878. Bolt Beranek and Newman, Cambridge, Massachuseets.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Bobrow</author>
<author>B Webber</author>
</authors>
<title>Knowledge Representation for Syntactic/Semantic Processing.</title>
<date>1980</date>
<booktitle>Proc. National Conference of the American Association for Artificial Intelligence.</booktitle>
<location>Stanford University, Stanford, California</location>
<contexts>
<context position="43161" citStr="Bobrow and Webber 1980" startWordPosition="6907" endWordPosition="6910">in recovering from many types of ungrammatical input, and these are by definition unavailable 130 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language in a purely syntactic ATN parser. However, semantic information can be brought to bear on network based parsing, either through the semantic grammar approach in which joint semantic and syntactic categories are used directly in the ATN, or by allowing the tests on ATN arcs to depend on semantic criteria (Bobrow 1978, Bobrow and Webber 1980). In the former technique, the appropriate semantic information for recovery can be applied only if the correct network node can be located — a sometimes difficult task as we have seen. In the latter technique, sometimes known as cascaded ATNs (Woods 1980), the syntactic and semantic parts of the grammar are kept separate, thus giving the potential for a higher degree of interpretiveness in using the semantic information. However, the natural way to use this technique is to employ the semantic information only to confirm or disconfirm parses arrived at on syntactic grounds. So the rigidity of </context>
</contexts>
<marker>Bobrow, Webber, 1980</marker>
<rawString>Bobrow, R.J. and Webber, B. 1980 Knowledge Representation for Syntactic/Semantic Processing. Proc. National Conference of the American Association for Artificial Intelligence. Stanford University, Stanford, California (August).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Bobrow</author>
<author>R M Kaplan</author>
<author>M Kay</author>
<author>D A Norman</author>
<author>H Thompson</author>
<author>T Winograd</author>
</authors>
<title>GUS: a Frame-Driven Dialogue System.</title>
<date>1977</date>
<journal>Artificial Intelligence</journal>
<volume>8</volume>
<pages>155--173</pages>
<contexts>
<context position="26162" citStr="Bobrow et al. 1977" startWordPosition="4128" endWordPosition="4131">an input utterance that are spurious to a parse can arise from a variety of sources: ■ legitimate phrases that the parser cannot deal with: It is not uncommon for the user of a restricted domain interface to say things that the interface cannot understand because of either conceptual or grammatical limitations. Sometimes, spurious verbosity or politeness is involved: Add if you would be so kind two fixed head and if possible dual ported disks to my order. Or the user may offer irrelevant (to the system) explanations or justifications, as observed in preparatory experiments for the GUS system (Bobrow et al. 1977), for example, I think I need more storage capacity, so add two fixed head dual ported disks to my order. American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 127 Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language Some common phrases of politeness can be recognized explicitly, but in most cases, the only reasonable response is to ignore the unknown phrases, realign the parse on the recognizable input, and if a semantically and syntactically complete structure results, postulate that the ignored segment was indeed re</context>
</contexts>
<marker>Bobrow, Kaplan, Kay, Norman, Thompson, Winograd, 1977</marker>
<rawString>Bobrow, D.G.; Kaplan, R.M.; Kay, M.; Norman D.A.; Thompson, H.; and Winograd, T. 1977 GUS: a Frame-Driven Dialogue System. Artificial Intelligence 8: 155-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Boggs</author>
<author>J G Carbonell</author>
<author>I Monarch</author>
</authors>
<title>The DYPAR-I Tutorial and Reference Manual.</title>
<date>1983</date>
<tech>Technical report.</tech>
<institution>Computer Science Department, Carnegie-Mellon University,</institution>
<location>Pittsburgh, Pennsylvania.</location>
<marker>Boggs, Carbonell, Monarch, 1983</marker>
<rawString>Boggs, W.M. and Carbonell, J.G. and Monarch, I. 1983 The DYPAR-I Tutorial and Reference Manual. Technical report. Computer Science Department, Carnegie-Mellon University, Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Brown</author>
<author>R R Burton</author>
</authors>
<title>Multiple Representations of Knowledge for Tutorial Reasoning.</title>
<date>1975</date>
<pages>311--349</pages>
<editor>In Bobrow, D.G. and Collins, A., eds. Representation and Understanding.</editor>
<publisher>Academic Press,</publisher>
<location>New York, New York:</location>
<contexts>
<context position="19623" citStr="Brown and Burton 1975" startWordPosition="3045" endWordPosition="3048"> operate in a sufficiently localized manner that the variations in global behaviour of different approaches to parsing do not come into play. However, most of the strategies are capable of using contextual restrictions on what incorrect lexical item might be, and therefore are most effective when the constraints on the unknown word are strongest. This suggests that they will be most successful when used with an approach to parsing in which it is easy to bring semantic constraints to bear. So, for instance, such techniques are likely to be more effective using a semantic grammar (Hendrix 1977, Brown and Burton 1975) or case frame instantiation (Dejong 1979, Hayes and Carbonell 1981) approach, than in an approach using a syntactic ATN (Woods, Kaplan and Nash-Webber 1972), where the expectations are never more specific than membership in one or more general syntactic categories. 3. Sentential Level Extragrammaticalities Recovery from extragrammaticality at the sentential level is much more dependent on the particular kind of parsing techniques that are employed. Some techniques lend themselves to straightforward recovery methods, while others make recovery difficult. An initial examination of the requireme</context>
</contexts>
<marker>Brown, Burton, 1975</marker>
<rawString>Brown, J.S. and Burton, R.R. 1975 Multiple Representations of Knowledge for Tutorial Reasoning. In Bobrow, D.G. and Collins, A., eds. Representation and Understanding. Academic Press, New York, New York: 311-349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Carbonell</author>
</authors>
<title>Towards a Self-Extending Parser.</title>
<date>1979</date>
<booktitle>Proceedings of the 17th Meeting of the Association for Computational Linguistics:</booktitle>
<pages>3--7</pages>
<contexts>
<context position="9583" citStr="Carbonell 1979" startWordPosition="1435" endWordPosition="1436">tuation is more complicated; three basic recovery strategies are possible: 1. Follow the KLAUS (Haas and Hendrix 1983) approach, where the system temporarily wrests initiative from the user and plays a well designed &amp;quot;twenty questions&amp;quot; game, classifying the unknown term syntactically, and relating it semantically to existing concepts encoded in an inheritance hierarchy. This method has proven successful for verbs, nouns and adjectives, but only when they turn out to be instances of predefined general classes of objects and actions in the domain model. 2. Apply the project and integrate method (Carbonell 1979) to infer the meaning and syntactic category of the word from context. This method has proven 124 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language useful for nouns and adjectives whose meaning can be viewed as a recombination of features present elsewhere in the input. Unlike the KLAUS method, it operates in the background, placing no major run-time burden on the user. However, it remains highly experimental and may not prove practical without user confirmation.</context>
</contexts>
<marker>Carbonell, 1979</marker>
<rawString>Carbonell, J.G. 1979 Towards a Self-Extending Parser. Proceedings of the 17th Meeting of the Association for Computational Linguistics: 3-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Carbonell</author>
<author>P J Hayes</author>
</authors>
<title>Robust Parsing Using Multiple Construction-Specific Strategies.</title>
<date>1984</date>
<editor>In Bole, L., ed.,</editor>
<publisher>Springer-Verlag,</publisher>
<location>New York, New York.</location>
<contexts>
<context position="1960" citStr="Carbonell and Hayes 1984" startWordPosition="259" endWordPosition="262">ted parsing techniques employing Augmented Transition Networks (ATNs) (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976). Other researchers have attempted to deal with ungrammatical input through network-based semantic grammar techniques (Hendrix 1977), through extensions to pattern matching parsing in which partial pattern matching is allowed (Hayes and Mouradian 1981), through conceptual case frame instantiation (Dejong 1979, Schank, Lebowitz, and Birnbaum 1980), and through approaches involving multiple cooperating parsing strategies (Carbonell and Hayes 1984, Carbonell et al. 1983, Hayes and Carbonell 1981). This research was sponsored in part by the Air Force Office of Scientific Research under Contract AFOSR-82-0219 and in part by Digital Equipment Corporation as part of the XCALIBUR project. Given the background of existing work, this paper focuses on three major objectives: 1. to create a taxonomy of possible grammatical deviations covering a broad range of extragrammaticalities, including some lexical and discourse phenomena (for example, novel words and dialogue level ellipsis) that can be handled by the same mechanisms that detect and proc</context>
</contexts>
<marker>Carbonell, Hayes, 1984</marker>
<rawString>Carbonell, J.G. and Hayes, P.J. 1984 Robust Parsing Using Multiple Construction-Specific Strategies. In Bole, L., ed., Natural Language Parsing Systems. Springer-Verlag, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Carbonell</author>
<author>W M Boggs</author>
<author>M L Mauldin</author>
<author>P G Anick</author>
</authors>
<title>The XCALIBUR Project, A Natural Language Interface to Expert Systems.</title>
<date>1983</date>
<booktitle>Proceedings of the Eighth International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1983" citStr="Carbonell et al. 1983" startWordPosition="263" endWordPosition="267">loying Augmented Transition Networks (ATNs) (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976). Other researchers have attempted to deal with ungrammatical input through network-based semantic grammar techniques (Hendrix 1977), through extensions to pattern matching parsing in which partial pattern matching is allowed (Hayes and Mouradian 1981), through conceptual case frame instantiation (Dejong 1979, Schank, Lebowitz, and Birnbaum 1980), and through approaches involving multiple cooperating parsing strategies (Carbonell and Hayes 1984, Carbonell et al. 1983, Hayes and Carbonell 1981). This research was sponsored in part by the Air Force Office of Scientific Research under Contract AFOSR-82-0219 and in part by Digital Equipment Corporation as part of the XCALIBUR project. Given the background of existing work, this paper focuses on three major objectives: 1. to create a taxonomy of possible grammatical deviations covering a broad range of extragrammaticalities, including some lexical and discourse phenomena (for example, novel words and dialogue level ellipsis) that can be handled by the same mechanisms that detect and process true grammatical er</context>
<context position="80137" citStr="Carbonell et al. 1983" startWordPosition="12890" endWordPosition="12893">aracteristics were developed though observation of FlexP&apos;s shortcomings as described in 3.5.2, and more fully in Hayes and Carbonell (1981). With these lessons in mind, we constructed two additional experimental parsers: CASPAR to explore the utility of case frame instantiation in robust parsing, and DYPAR to explore the notion of combining several different parsing strategies in a single parser. Both experiments proved fruitful, as the next two sections show, and DYPAR has now been developed into a complete parsing system, the DYPAR-II parser, as part of the XCALIBUR expert system interface (Carbonell et al. 1983). After that, we describe an approach to parsing we are currently developing that we believe to be based on the best features of both systems. A final section discusses other methods and approaches that we consider promising avenues for future research. 5.1. The CASPAR parser As our earlier discussion on sentential-level ungrammaticality pointed out, case frame instantiation appears to have many advantages as a framework for robust parsing. Our initial experiments in realizing these advantages were conducted through the CASPAR parser (Hayes and Carbonell 1981). CASPAR was restricted in coverag</context>
<context position="90500" citStr="Carbonell et al. 1983" startWordPosition="14602" endWordPosition="14605">onterminal could expand to a full pattern containing additional non-terminals. The experiment proved successful in that DYPAR allowed one to write grammars at least an order of magnitude more concise than pure semantic grammars of equivalent coverage. This version of the system is called DYPAR-I (Boggs, Carbonell, and Monarch 1983) and has been made available for general use. Subsequently, case frame instantiation was introduced as the new dominant strategy, and the new system, DYPAR-II, is currently used as the experimental parser for XCALIBUR, a natural language interface to expert systems (Carbonell et al. 1983). The multi-strategy approach to parsing grammatical input in DYPAR-II facilitated the introduction of several additional strategies to recover from different kinds of extragrammaticality: ■ Spelling correction combined with morphological decomposition of inflected words. ■ Bridging garbled or spurious phrases in otherwise comprehensible input. ■ Recognizing constituents when they occur in unexpected order in the input. ■ Generalized case frame ellipsis resolution, exploiting strong domain semantics. 6 Operators in DYPAR-I include: matching arbitrary subconstituent repetition, optional constit</context>
</contexts>
<marker>Carbonell, Boggs, Mauldin, Anick, 1983</marker>
<rawString>Carbonell, J.G.; Boggs, W.M.; Mauldin, M.L.; and Anick, P.G. 1983 The XCALIBUR Project, A Natural Language Interface to Expert Systems. Proceedings of the Eighth International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Carbonell</author>
<author>W M Boggs</author>
<author>M L Mauldin</author>
<author>P G Anick</author>
</authors>
<title>XCALIBUR Progress Report #1: First Steps Towards an Integrated Natural Language Interface.</title>
<date>1983</date>
<tech>Technical report.</tech>
<institution>Computer Science Department, Carnegie-Mellon University,</institution>
<location>Pittsburgh, Pennsylvania.</location>
<contexts>
<context position="1983" citStr="Carbonell et al. 1983" startWordPosition="263" endWordPosition="267">loying Augmented Transition Networks (ATNs) (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976). Other researchers have attempted to deal with ungrammatical input through network-based semantic grammar techniques (Hendrix 1977), through extensions to pattern matching parsing in which partial pattern matching is allowed (Hayes and Mouradian 1981), through conceptual case frame instantiation (Dejong 1979, Schank, Lebowitz, and Birnbaum 1980), and through approaches involving multiple cooperating parsing strategies (Carbonell and Hayes 1984, Carbonell et al. 1983, Hayes and Carbonell 1981). This research was sponsored in part by the Air Force Office of Scientific Research under Contract AFOSR-82-0219 and in part by Digital Equipment Corporation as part of the XCALIBUR project. Given the background of existing work, this paper focuses on three major objectives: 1. to create a taxonomy of possible grammatical deviations covering a broad range of extragrammaticalities, including some lexical and discourse phenomena (for example, novel words and dialogue level ellipsis) that can be handled by the same mechanisms that detect and process true grammatical er</context>
<context position="80137" citStr="Carbonell et al. 1983" startWordPosition="12890" endWordPosition="12893">aracteristics were developed though observation of FlexP&apos;s shortcomings as described in 3.5.2, and more fully in Hayes and Carbonell (1981). With these lessons in mind, we constructed two additional experimental parsers: CASPAR to explore the utility of case frame instantiation in robust parsing, and DYPAR to explore the notion of combining several different parsing strategies in a single parser. Both experiments proved fruitful, as the next two sections show, and DYPAR has now been developed into a complete parsing system, the DYPAR-II parser, as part of the XCALIBUR expert system interface (Carbonell et al. 1983). After that, we describe an approach to parsing we are currently developing that we believe to be based on the best features of both systems. A final section discusses other methods and approaches that we consider promising avenues for future research. 5.1. The CASPAR parser As our earlier discussion on sentential-level ungrammaticality pointed out, case frame instantiation appears to have many advantages as a framework for robust parsing. Our initial experiments in realizing these advantages were conducted through the CASPAR parser (Hayes and Carbonell 1981). CASPAR was restricted in coverag</context>
<context position="90500" citStr="Carbonell et al. 1983" startWordPosition="14602" endWordPosition="14605">onterminal could expand to a full pattern containing additional non-terminals. The experiment proved successful in that DYPAR allowed one to write grammars at least an order of magnitude more concise than pure semantic grammars of equivalent coverage. This version of the system is called DYPAR-I (Boggs, Carbonell, and Monarch 1983) and has been made available for general use. Subsequently, case frame instantiation was introduced as the new dominant strategy, and the new system, DYPAR-II, is currently used as the experimental parser for XCALIBUR, a natural language interface to expert systems (Carbonell et al. 1983). The multi-strategy approach to parsing grammatical input in DYPAR-II facilitated the introduction of several additional strategies to recover from different kinds of extragrammaticality: ■ Spelling correction combined with morphological decomposition of inflected words. ■ Bridging garbled or spurious phrases in otherwise comprehensible input. ■ Recognizing constituents when they occur in unexpected order in the input. ■ Generalized case frame ellipsis resolution, exploiting strong domain semantics. 6 Operators in DYPAR-I include: matching arbitrary subconstituent repetition, optional constit</context>
</contexts>
<marker>Carbonell, Boggs, Mauldin, Anick, 1983</marker>
<rawString>Carbonell, J.G.; Boggs, W.M.; Mauldin, M.L.; and Anick, P.G. 1983 XCALIBUR Progress Report #1: First Steps Towards an Integrated Natural Language Interface. Technical report. Computer Science Department, Carnegie-Mellon University, Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Carbonell</author>
</authors>
<title>Meta-Language Utterances in Purposive Discourse.</title>
<date>1982</date>
<tech>Technical report.</tech>
<institution>Computer Science Department, Carnegie-Mellon University,</institution>
<location>Pittsburgh, Pennsylvania.</location>
<contexts>
<context position="68876" citStr="Carbonell (1982)" startWordPosition="11000" endWordPosition="11001">in a way that fits with the conversational expectations and conventions of its human user. Issues of when explicit replies are required, how to convey information in such a way as to require the minimal response from the user, how to keep the conversation within the domain of discourse of the system, etc., must all be addressed by a natural language interface capable of mixed-initiative dialogue. Examining all these topics here would take us too far afield from the issue of robust parsing, so we will confine ourselves to issues specific to the kind of recovery interaction described above. See Carbonell (1982) and Hayes and Reddy (1983) for a fuller discussion of the issues involved in organizing the dialogue of an interactive natural language system. We offer four guidelines for organizing recovery dialogues: ■ the interaction should be as focused as possible; ■ the required user response should be as terse as possible; ■ the interaction should be in terms of the system&apos;s domain of discourse rather than the linguistic concepts it uses internally; ■ there should be as few such interactions as possible. To see the need for focused interaction, consider the input: Add two fixed head ported disks to m</context>
</contexts>
<marker>Carbonell, 1982</marker>
<rawString>Carbonell, J.G. 1982 Meta-Language Utterances in Purposive Discourse. Technical report. Computer Science Department, Carnegie-Mellon University, Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Carbonell</author>
</authors>
<title>Discourse Pragmatics in Task-Oriented Natural Language Interfaces.</title>
<date>1983</date>
<booktitle>Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="54420" citStr="Carbonell 1983" startWordPosition="8692" endWordPosition="8693">vertheless, it is also a very common phenomenon and must be addressed by any interface intended for serious use by real users. Empirical observations have shown that users of natural language interfaces employ 132 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language ellipsis and other abbreviating devices (for example, anaphora, short definite noun phrases, cryptic language omitting semantically superfluous words, and lexical abbreviations) with alarming frequency (Carbonell 1983). The results of our empirical observations can be summarized as follows: Terseness principle: Users of natural language interfaces insist on being as terse as possible, independent of task, communication media, typing ability, or instructions to the contrary, without sacrificing the flexibility of expression inherent in natural language communication. Broadly speaking, one can classify ellipsis into intrasentential and intersentential ellipsis, with the latter category being far more prevalent in practical natural language interfaces. Intrasentential ellipsis occurs most frequently in coordin</context>
</contexts>
<marker>Carbonell, 1983</marker>
<rawString>Carbonell, J.G. 1983 Discourse Pragmatics in Task-Oriented Natural Language Interfaces. Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Codd</author>
</authors>
<title>Seven Steps to RENDEZVOUS with the Casual User</title>
<date>1974</date>
<booktitle>Proceedings of the IFIP TC-2 Working Conference on Data Base Management Systems. North</booktitle>
<pages>179--200</pages>
<editor>In Klimbie, J.W. and Koffeman, K.L., eds.,</editor>
<location>Holland, Amsterdam:</location>
<contexts>
<context position="75886" citStr="Codd (1974)" startWordPosition="12170" endWordPosition="12171">one that has meaning within the task domain, thereby switching contexts from perform136 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language ance of the task to linguistic issues. This enforced digression places an undue cognitive load on the user and should be avoided. The final guideline is to minimize the amount of corrective interactions that occur. It is very tedious for a user to be confronted with questions about what he meant after almost every input, or as Codd (1974) has suggested, to approve a paraphrase of each input before the system does anything. Clearly, there are situations when the user must be asked a direct question, to wit, when information is missing or in the presence of real ambiguity. However, a technique not requiring a reply is preferable when the system makes assumptions that are very likely to be correct, or when there are strong preferences for one alternative among several in ambiguity, anaphora, or ellipsis resolution. The echoing technique mentioned in Section 4.1 is very useful in keeping required user replies to a minimum while st</context>
</contexts>
<marker>Codd, 1974</marker>
<rawString>Codd, E.F. 1974 Seven Steps to RENDEZVOUS with the Casual User In Klimbie, J.W. and Koffeman, K.L., eds., Proceedings of the IFIP TC-2 Working Conference on Data Base Management Systems. North Holland, Amsterdam: 179-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Dejong</author>
</authors>
<title>Skimming Stories in Real-Time.</title>
<date>1979</date>
<tech>Ph.D. dissertation.</tech>
<institution>Computer Science Department, Yale University,</institution>
<location>New Haven, Connecticut.</location>
<contexts>
<context position="1822" citStr="Dejong 1979" startWordPosition="244" endWordPosition="245">ructured as extensions to existing parsing methods. Probably the most popular approach has been to extend syntactically-oriented parsing techniques employing Augmented Transition Networks (ATNs) (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976). Other researchers have attempted to deal with ungrammatical input through network-based semantic grammar techniques (Hendrix 1977), through extensions to pattern matching parsing in which partial pattern matching is allowed (Hayes and Mouradian 1981), through conceptual case frame instantiation (Dejong 1979, Schank, Lebowitz, and Birnbaum 1980), and through approaches involving multiple cooperating parsing strategies (Carbonell and Hayes 1984, Carbonell et al. 1983, Hayes and Carbonell 1981). This research was sponsored in part by the Air Force Office of Scientific Research under Contract AFOSR-82-0219 and in part by Digital Equipment Corporation as part of the XCALIBUR project. Given the background of existing work, this paper focuses on three major objectives: 1. to create a taxonomy of possible grammatical deviations covering a broad range of extragrammaticalities, including some lexical and </context>
<context position="19664" citStr="Dejong 1979" startWordPosition="3053" endWordPosition="3054"> variations in global behaviour of different approaches to parsing do not come into play. However, most of the strategies are capable of using contextual restrictions on what incorrect lexical item might be, and therefore are most effective when the constraints on the unknown word are strongest. This suggests that they will be most successful when used with an approach to parsing in which it is easy to bring semantic constraints to bear. So, for instance, such techniques are likely to be more effective using a semantic grammar (Hendrix 1977, Brown and Burton 1975) or case frame instantiation (Dejong 1979, Hayes and Carbonell 1981) approach, than in an approach using a syntactic ATN (Woods, Kaplan and Nash-Webber 1972), where the expectations are never more specific than membership in one or more general syntactic categories. 3. Sentential Level Extragrammaticalities Recovery from extragrammaticality at the sentential level is much more dependent on the particular kind of parsing techniques that are employed. Some techniques lend themselves to straightforward recovery methods, while others make recovery difficult. An initial examination of the requirements for recovery from various kinds of se</context>
</contexts>
<marker>Dejong, 1979</marker>
<rawString>Dejong, G. 1979 Skimming Stories in Real-Time. Ph.D. dissertation. Computer Science Department, Yale University, New Haven, Connecticut.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Durham</author>
<author>D D Lamb</author>
<author>J B Saxe</author>
</authors>
<title>Spelling Correction in User Interfaces.</title>
<date>1983</date>
<journal>Communications of the ACM</journal>
<volume>26</volume>
<marker>Durham, Lamb, Saxe, 1983</marker>
<rawString>Durham, I.; Lamb, D.D.; and Saxe, J.B. 1983 Spelling Correction in User Interfaces. Communications of the ACM 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Haas</author>
<author>G G Hendrix</author>
</authors>
<title>Learning by Being Told: Acquiring Knowledge for Information Management.</title>
<date>1983</date>
<booktitle>Machine Learning, An Artificial Intelligence Approach.</booktitle>
<editor>In Michalski, R.S.; Carbonell, J.G.; and Mitchell, T.M., eds.,</editor>
<publisher>Tioga Press,</publisher>
<location>Palo Alto, California.</location>
<contexts>
<context position="9086" citStr="Haas and Hendrix 1983" startWordPosition="1355" endWordPosition="1358"> a catalogue part name/number, not heretofore encountered by the system, but recognizable by a combination of contextual expectations and morphological or orthographic features (for example, capitalization). In the first situation, there is no meaningful recovery strategy other than focused interaction (Hayes 1981) to inform the user of the precise difficulty. In the third, little action is required beyond recognizing the proper name and recording it appropriately for future reference. The second situation is more complicated; three basic recovery strategies are possible: 1. Follow the KLAUS (Haas and Hendrix 1983) approach, where the system temporarily wrests initiative from the user and plays a well designed &amp;quot;twenty questions&amp;quot; game, classifying the unknown term syntactically, and relating it semantically to existing concepts encoded in an inheritance hierarchy. This method has proven successful for verbs, nouns and adjectives, but only when they turn out to be instances of predefined general classes of objects and actions in the domain model. 2. Apply the project and integrate method (Carbonell 1979) to infer the meaning and syntactic category of the word from context. This method has proven 124 Ameri</context>
</contexts>
<marker>Haas, Hendrix, 1983</marker>
<rawString>Haas, N. and Hendrix, G.G. 1983 Learning by Being Told: Acquiring Knowledge for Information Management. In Michalski, R.S.; Carbonell, J.G.; and Mitchell, T.M., eds., Machine Learning, An Artificial Intelligence Approach. Tioga Press, Palo Alto, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Hayes</author>
</authors>
<title>A Construction Specific Approach to Focused Interaction in Flexible Parsing.</title>
<date>1981</date>
<booktitle>Proceedings of 19th Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>149--152</pages>
<contexts>
<context position="8780" citStr="Hayes 1981" startWordPosition="1309" endWordPosition="1310">r example, a word like &amp;quot;forward&amp;quot; [a message] can be defined as a command verb, its action can be clearly specified, and the objects upon which it operates — an old message and a new recipient — are already well-formed domain concepts.) ■ The word is a proper name or a unique identifier, such as a catalogue part name/number, not heretofore encountered by the system, but recognizable by a combination of contextual expectations and morphological or orthographic features (for example, capitalization). In the first situation, there is no meaningful recovery strategy other than focused interaction (Hayes 1981) to inform the user of the precise difficulty. In the third, little action is required beyond recognizing the proper name and recording it appropriately for future reference. The second situation is more complicated; three basic recovery strategies are possible: 1. Follow the KLAUS (Haas and Hendrix 1983) approach, where the system temporarily wrests initiative from the user and plays a well designed &amp;quot;twenty questions&amp;quot; game, classifying the unknown term syntactically, and relating it semantically to existing concepts encoded in an inheritance hierarchy. This method has proven successful for ve</context>
<context position="51393" citStr="Hayes 1981" startWordPosition="8239" endWordPosition="8240">on case fillers at parse time. Thus, problems created by the absence of expected case markers can be overcome by the application of domain knowledge. ■ The propagation of semantic knowledge through a case frame (via attached procedures such as those of KRL (Bobrow and Winograd 1977) or SRL (Wright and Fox 1983)) can fill in parser defaults and allow the internal completion of phrases such as &amp;quot;dual disks&amp;quot; to mean &amp;quot;dual ported disks&amp;quot;. This process is also responsible for noticing when information is either missing or ambiguously determined, thereby initiating a focused clarificational dialogue (Hayes 1981). ■ The representation of case frames is inherently non-uniform. Case fillers, case markers, and case headers are all represented separately, and this distinction can be used by the parser interpretively instantiating the case frame. For instance, if a case frame accounts for the non-spurious part of an input containing spurious constituents, a recovery strategy can skip over the unrecognizable words by scanning for case markers as opposed to case fillers which typically are much harder to find and parse. This ability to exploit non-uniformity goes a long way to overcoming the problems with un</context>
</contexts>
<marker>Hayes, 1981</marker>
<rawString>Hayes, P.J. 1981 A Construction Specific Approach to Focused Interaction in Flexible Parsing. Proceedings of 19th Annual Meeting of the Association for Computational Linguistics (June): 149-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Hayes</author>
</authors>
<title>Entity-Oriented Parsing. COLING84,</title>
<date>1984</date>
<location>Stanford University, Stanford, California</location>
<contexts>
<context position="1960" citStr="Hayes 1984" startWordPosition="261" endWordPosition="262">chniques employing Augmented Transition Networks (ATNs) (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976). Other researchers have attempted to deal with ungrammatical input through network-based semantic grammar techniques (Hendrix 1977), through extensions to pattern matching parsing in which partial pattern matching is allowed (Hayes and Mouradian 1981), through conceptual case frame instantiation (Dejong 1979, Schank, Lebowitz, and Birnbaum 1980), and through approaches involving multiple cooperating parsing strategies (Carbonell and Hayes 1984, Carbonell et al. 1983, Hayes and Carbonell 1981). This research was sponsored in part by the Air Force Office of Scientific Research under Contract AFOSR-82-0219 and in part by Digital Equipment Corporation as part of the XCALIBUR project. Given the background of existing work, this paper focuses on three major objectives: 1. to create a taxonomy of possible grammatical deviations covering a broad range of extragrammaticalities, including some lexical and discourse phenomena (for example, novel words and dialogue level ellipsis) that can be handled by the same mechanisms that detect and proc</context>
<context position="102616" citStr="Hayes 1984" startWordPosition="16483" endWordPosition="16484">sed at the end of the section on CASPAR would make a third. 5.3.1. Coordinating multiple strategies through an entity-oriented approach A major problem that arises in using multiple parsing strategies is coordination between the strategies. Questions of interaction and order of application are involved. In CASPAR and DYPAR, the problem was solved simply by &amp;quot;hard-wiring&amp;quot; the interactions, but this is not satisfactory in general, especially if we wish to extend the set of strategies available in a smooth way. One alternative we have begun to explore involves the idea of entity-oriented parsing (Hayes 1984). The central notion behind entity-oriented parsing is that the primary task of a natural language interface is to recognize entities — objects, actions, states, commands, etc. — from the domain of discourse of the interface. This recognition may be recursive in the sense that descriptions of entities may contain descriptions of subsidiary entities (for example, commands refer to objects). In entity-oriented parsing, all the entities that a particular interface system needs to recognize are defined separately. These definitions contain information both about the way the entities will be manife</context>
</contexts>
<marker>Hayes, 1984</marker>
<rawString>Hayes, P.J. 1984 Entity-Oriented Parsing. COLING84, Stanford University, Stanford, California (July).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Hayes</author>
<author>G V Mouradian</author>
</authors>
<title>Flexible Parsing.</title>
<date>1981</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>7</volume>
<issue>4</issue>
<pages>232--241</pages>
<contexts>
<context position="1764" citStr="Hayes and Mouradian 1981" startWordPosition="235" endWordPosition="238">arious efforts at coping with extragrammaticality have been generally structured as extensions to existing parsing methods. Probably the most popular approach has been to extend syntactically-oriented parsing techniques employing Augmented Transition Networks (ATNs) (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976). Other researchers have attempted to deal with ungrammatical input through network-based semantic grammar techniques (Hendrix 1977), through extensions to pattern matching parsing in which partial pattern matching is allowed (Hayes and Mouradian 1981), through conceptual case frame instantiation (Dejong 1979, Schank, Lebowitz, and Birnbaum 1980), and through approaches involving multiple cooperating parsing strategies (Carbonell and Hayes 1984, Carbonell et al. 1983, Hayes and Carbonell 1981). This research was sponsored in part by the Air Force Office of Scientific Research under Contract AFOSR-82-0219 and in part by Digital Equipment Corporation as part of the XCALIBUR project. Given the background of existing work, this paper focuses on three major objectives: 1. to create a taxonomy of possible grammatical deviations covering a broad r</context>
<context position="46632" citStr="Hayes and Mouradian 1981" startWordPosition="7476" endWordPosition="7479">ecovery. In addition, when pattern elements are defined semantically instead of lexically, as with Wilks&apos;s (1975) machine translation system, semantic constraints can easily be brought to bear on the recognition. However, dealing with out of order constituents is not so easy for a pattern-based approach since constituent order is built into a pattern in a rigid way, similarly to a network. It is possible to accept any permutation of elements of a pattern as a match, but this provides so much flexibility that many spurious recognitions are likely to be obtained as well as the correct ones (see Hayes and Mouradian 1981). An underlying problem here is that there is no natural way to make the distinctions about the relative importance or difference in role between one word and another. For instance, parsing many of the examples we have used might involve use of a pattern like: (&lt;determiner&gt; &lt;disk-drive-attribute&gt;* &lt;disk-drive&gt;) which specifies a pattern of a determiner, followed by zero or more attributes of a disk drive, followed by a phrase synonymous with &amp;quot;disk drive&amp;quot;. So this pattern would recognize phrases like &amp;quot;a dual ported disk&amp;quot; or &amp;quot;the disk drive&amp;quot;. Using the method of dealing with missing constituents</context>
<context position="79289" citStr="Hayes and Mouradian 1981" startWordPosition="12751" endWordPosition="12754">s should be able to take advantage of non-uniformity in language like that identified in Section 3.5.2. As we have seen, recovery can be much more efficient and reliable if a parser is able to make use of variations in ease of recognition or discriminating power between different constituents. This kind of &amp;quot;opportunism&amp;quot; can be built into recovery strategies. ■ The parsing process should be capable of operating top-down as well as bottom-up. We have seen examples where both of these modes are essential. Our earliest experiments in robust parsing were conducted through the FlexP parsing system (Hayes and Mouradian 1981). This system was based on partial pattern matching, and while it had the first and last of the characteristics listed above, it did not measure up well to the other two. Indeed, many of our ideas on the importance of those characteristics were developed though observation of FlexP&apos;s shortcomings as described in 3.5.2, and more fully in Hayes and Carbonell (1981). With these lessons in mind, we constructed two additional experimental parsers: CASPAR to explore the utility of case frame instantiation in robust parsing, and DYPAR to explore the notion of combining several different parsing strat</context>
</contexts>
<marker>Hayes, Mouradian, 1981</marker>
<rawString>Hayes, P.J. and Mouradian, G.V. 1981 Flexible Parsing. American Journal of Computational Linguistics 7(4); 232-241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Hayes</author>
<author>J G Carbonell</author>
</authors>
<title>Multi-strategy ConstructionSpecific Parsing for Flexible Data Base Query and Update.</title>
<date>1981</date>
<booktitle>Proceedings of the Seventh International Joint Conference on Artificial Intelligence.</booktitle>
<pages>432--439</pages>
<location>Vancouver</location>
<contexts>
<context position="2010" citStr="Hayes and Carbonell 1981" startWordPosition="268" endWordPosition="271">tion Networks (ATNs) (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976). Other researchers have attempted to deal with ungrammatical input through network-based semantic grammar techniques (Hendrix 1977), through extensions to pattern matching parsing in which partial pattern matching is allowed (Hayes and Mouradian 1981), through conceptual case frame instantiation (Dejong 1979, Schank, Lebowitz, and Birnbaum 1980), and through approaches involving multiple cooperating parsing strategies (Carbonell and Hayes 1984, Carbonell et al. 1983, Hayes and Carbonell 1981). This research was sponsored in part by the Air Force Office of Scientific Research under Contract AFOSR-82-0219 and in part by Digital Equipment Corporation as part of the XCALIBUR project. Given the background of existing work, this paper focuses on three major objectives: 1. to create a taxonomy of possible grammatical deviations covering a broad range of extragrammaticalities, including some lexical and discourse phenomena (for example, novel words and dialogue level ellipsis) that can be handled by the same mechanisms that detect and process true grammatical errors; 2. to outline strateg</context>
<context position="19691" citStr="Hayes and Carbonell 1981" startWordPosition="3055" endWordPosition="3058">n global behaviour of different approaches to parsing do not come into play. However, most of the strategies are capable of using contextual restrictions on what incorrect lexical item might be, and therefore are most effective when the constraints on the unknown word are strongest. This suggests that they will be most successful when used with an approach to parsing in which it is easy to bring semantic constraints to bear. So, for instance, such techniques are likely to be more effective using a semantic grammar (Hendrix 1977, Brown and Burton 1975) or case frame instantiation (Dejong 1979, Hayes and Carbonell 1981) approach, than in an approach using a syntactic ATN (Woods, Kaplan and Nash-Webber 1972), where the expectations are never more specific than membership in one or more general syntactic categories. 3. Sentential Level Extragrammaticalities Recovery from extragrammaticality at the sentential level is much more dependent on the particular kind of parsing techniques that are employed. Some techniques lend themselves to straightforward recovery methods, while others make recovery difficult. An initial examination of the requirements for recovery from various kinds of sentential level ungrammatica</context>
<context position="48282" citStr="Hayes and Carbonell (1981)" startWordPosition="7748" endWordPosition="7751">ze (for example, prepositions are easier to recognize than the noun phrases they introduce), and thus may be more or less worthwhile to look for in an attempt to recover from a grammatical deviation. The underlying problem then is the uniformity of the grammar representation and the method of applying it to the input. Any uniformly represented grammar, whether based on patterns or networks, will have trouble representing and using the kinds of distinctions just outlined, and thus will be less well equipped to deal with many grammatical deviations in an efficient and discriminating manner. See Hayes and Carbonell (1981) for a fuller discussion of this point. 3.5.3. Recovery strategies in a case frame paradigm Recursive case frame instantiation appears to provide American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 131 Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language a better framework for recovery from missing words than approaches based on either network traversal or pattern matching. There are several reasons: ■ Case frame instantiation is inherently a highly interpretive process. Case frames provide a high-level set of syntac</context>
<context position="72785" citStr="Hayes and Carbonell (1981)" startWordPosition="11651" endWordPosition="11654">putationally taxing intractable comparison process. However, if the ambiguity is represented as locally as possible, for instance as two alternative fillers for a single instantiation of a disk frame nested within the &amp;quot;add to order&amp;quot; frame, then generating the focused query is easy — just output a paraphrase of the case frame (the one for disk) at the level immediately above the ambiguity with a disjunction taking the place of the single filler of the ambiguous case (the portedness case). Moreover, such a representation forms an excellent basis for interpreting the natural elliptical reply. As Hayes and Carbonell (1981) show, parsers based on case frame instantiation are particularly well suited to generating ambiguity representations of this kind. Another tactic related to focused interaction that parsing systems can employ to smooth recovery dialogues is to couch their questions in terms that make it more likely that the user&apos;s reply will be something they can understand. Thus in: Please add two 300 megabyte rotating mass storage devices to my order. if &amp;quot;rotating mass storage device&amp;quot; is not in the system&apos;s vocabulary, it is unwise for it to reply &amp;quot;what is a rotating mass storage device?&amp;quot;, since the terms t</context>
<context position="79654" citStr="Hayes and Carbonell (1981)" startWordPosition="12813" endWordPosition="12816">he parsing process should be capable of operating top-down as well as bottom-up. We have seen examples where both of these modes are essential. Our earliest experiments in robust parsing were conducted through the FlexP parsing system (Hayes and Mouradian 1981). This system was based on partial pattern matching, and while it had the first and last of the characteristics listed above, it did not measure up well to the other two. Indeed, many of our ideas on the importance of those characteristics were developed though observation of FlexP&apos;s shortcomings as described in 3.5.2, and more fully in Hayes and Carbonell (1981). With these lessons in mind, we constructed two additional experimental parsers: CASPAR to explore the utility of case frame instantiation in robust parsing, and DYPAR to explore the notion of combining several different parsing strategies in a single parser. Both experiments proved fruitful, as the next two sections show, and DYPAR has now been developed into a complete parsing system, the DYPAR-II parser, as part of the XCALIBUR expert system interface (Carbonell et al. 1983). After that, we describe an approach to parsing we are currently developing that we believe to be based on the best </context>
</contexts>
<marker>Hayes, Carbonell, 1981</marker>
<rawString>Hayes, P.J. and Carbonell, J.G. 1981 Multi-strategy ConstructionSpecific Parsing for Flexible Data Base Query and Update. Proceedings of the Seventh International Joint Conference on Artificial Intelligence. Vancouver (August): 432-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Hayes</author>
<author>D R Reddy</author>
</authors>
<title>Steps Toward Graceful Interaction. Spoken and Written Man-Machine Communication,</title>
<date>1983</date>
<journal>International Journal of Man-Machine Studies.</journal>
<volume>19</volume>
<issue>3</issue>
<pages>211--284</pages>
<contexts>
<context position="56273" citStr="Hayes and Reddy (1983)" startWordPosition="8973" endWordPosition="8976">sions to exploit the syntactic and semantic parallelism between corresponding constituents of coordinate clauses. There are several forms of intersentential ellipsis: ■ Elaboration — An ellipsed fragment by either speaker can be an elaboration of a previous utterance. Either speaker can make the elaboration, but the second speaker usually does so, as in the following example: User: Give me a large capacity disk. System: With dual ports? User: Yes, and a universal frequency adapter. ■ Echo — A fragment of the first speaker&apos;s utterance is echoed by the second speaker. As described more fully in Hayes and Reddy (1983), this allows the second speaker to confirm his understanding of the first speaker&apos;s utterance without requiring an explicit confirmation. User: Add a dual disk to the order. System: A dual ported disk. What storage capacity? If, on the other hand, the system had explicitly asked &amp;quot;Do you mean a dual ported disk?&amp;quot;, the user would have been conversationally obliged to reply. However, in either case, the user is free to correct any misapprehension the system displays. Sometimes, as in the example in the next bullet below, an echo may also be an expression of bewilderment. In general, this form of</context>
<context position="68903" citStr="Hayes and Reddy (1983)" startWordPosition="11003" endWordPosition="11006">th the conversational expectations and conventions of its human user. Issues of when explicit replies are required, how to convey information in such a way as to require the minimal response from the user, how to keep the conversation within the domain of discourse of the system, etc., must all be addressed by a natural language interface capable of mixed-initiative dialogue. Examining all these topics here would take us too far afield from the issue of robust parsing, so we will confine ourselves to issues specific to the kind of recovery interaction described above. See Carbonell (1982) and Hayes and Reddy (1983) for a fuller discussion of the issues involved in organizing the dialogue of an interactive natural language system. We offer four guidelines for organizing recovery dialogues: ■ the interaction should be as focused as possible; ■ the required user response should be as terse as possible; ■ the interaction should be in terms of the system&apos;s domain of discourse rather than the linguistic concepts it uses internally; ■ there should be as few such interactions as possible. To see the need for focused interaction, consider the input: Add two fixed head ported disks to my order The problem is that</context>
</contexts>
<marker>Hayes, Reddy, 1983</marker>
<rawString>Hayes, P.J. and Reddy, D.R. 1983 Steps Toward Graceful Interaction. Spoken and Written Man-Machine Communication, International Journal of Man-Machine Studies. 19(3): 211-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Hayes</author>
<author>J G Carbonell</author>
</authors>
<title>Multi-Strategy Parsing and its Role in Robust Man-Machine Communication.</title>
<date>1981</date>
<tech>Technical report CMU-CS-81-1 18.</tech>
<institution>Computer Science Department, Carnegie-Mellon University,</institution>
<location>Pittsburgh, Pennsylvania</location>
<contexts>
<context position="2010" citStr="Hayes and Carbonell 1981" startWordPosition="268" endWordPosition="271">tion Networks (ATNs) (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976). Other researchers have attempted to deal with ungrammatical input through network-based semantic grammar techniques (Hendrix 1977), through extensions to pattern matching parsing in which partial pattern matching is allowed (Hayes and Mouradian 1981), through conceptual case frame instantiation (Dejong 1979, Schank, Lebowitz, and Birnbaum 1980), and through approaches involving multiple cooperating parsing strategies (Carbonell and Hayes 1984, Carbonell et al. 1983, Hayes and Carbonell 1981). This research was sponsored in part by the Air Force Office of Scientific Research under Contract AFOSR-82-0219 and in part by Digital Equipment Corporation as part of the XCALIBUR project. Given the background of existing work, this paper focuses on three major objectives: 1. to create a taxonomy of possible grammatical deviations covering a broad range of extragrammaticalities, including some lexical and discourse phenomena (for example, novel words and dialogue level ellipsis) that can be handled by the same mechanisms that detect and process true grammatical errors; 2. to outline strateg</context>
<context position="19691" citStr="Hayes and Carbonell 1981" startWordPosition="3055" endWordPosition="3058">n global behaviour of different approaches to parsing do not come into play. However, most of the strategies are capable of using contextual restrictions on what incorrect lexical item might be, and therefore are most effective when the constraints on the unknown word are strongest. This suggests that they will be most successful when used with an approach to parsing in which it is easy to bring semantic constraints to bear. So, for instance, such techniques are likely to be more effective using a semantic grammar (Hendrix 1977, Brown and Burton 1975) or case frame instantiation (Dejong 1979, Hayes and Carbonell 1981) approach, than in an approach using a syntactic ATN (Woods, Kaplan and Nash-Webber 1972), where the expectations are never more specific than membership in one or more general syntactic categories. 3. Sentential Level Extragrammaticalities Recovery from extragrammaticality at the sentential level is much more dependent on the particular kind of parsing techniques that are employed. Some techniques lend themselves to straightforward recovery methods, while others make recovery difficult. An initial examination of the requirements for recovery from various kinds of sentential level ungrammatica</context>
<context position="48282" citStr="Hayes and Carbonell (1981)" startWordPosition="7748" endWordPosition="7751">ze (for example, prepositions are easier to recognize than the noun phrases they introduce), and thus may be more or less worthwhile to look for in an attempt to recover from a grammatical deviation. The underlying problem then is the uniformity of the grammar representation and the method of applying it to the input. Any uniformly represented grammar, whether based on patterns or networks, will have trouble representing and using the kinds of distinctions just outlined, and thus will be less well equipped to deal with many grammatical deviations in an efficient and discriminating manner. See Hayes and Carbonell (1981) for a fuller discussion of this point. 3.5.3. Recovery strategies in a case frame paradigm Recursive case frame instantiation appears to provide American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 131 Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language a better framework for recovery from missing words than approaches based on either network traversal or pattern matching. There are several reasons: ■ Case frame instantiation is inherently a highly interpretive process. Case frames provide a high-level set of syntac</context>
<context position="72785" citStr="Hayes and Carbonell (1981)" startWordPosition="11651" endWordPosition="11654">putationally taxing intractable comparison process. However, if the ambiguity is represented as locally as possible, for instance as two alternative fillers for a single instantiation of a disk frame nested within the &amp;quot;add to order&amp;quot; frame, then generating the focused query is easy — just output a paraphrase of the case frame (the one for disk) at the level immediately above the ambiguity with a disjunction taking the place of the single filler of the ambiguous case (the portedness case). Moreover, such a representation forms an excellent basis for interpreting the natural elliptical reply. As Hayes and Carbonell (1981) show, parsers based on case frame instantiation are particularly well suited to generating ambiguity representations of this kind. Another tactic related to focused interaction that parsing systems can employ to smooth recovery dialogues is to couch their questions in terms that make it more likely that the user&apos;s reply will be something they can understand. Thus in: Please add two 300 megabyte rotating mass storage devices to my order. if &amp;quot;rotating mass storage device&amp;quot; is not in the system&apos;s vocabulary, it is unwise for it to reply &amp;quot;what is a rotating mass storage device?&amp;quot;, since the terms t</context>
<context position="79654" citStr="Hayes and Carbonell (1981)" startWordPosition="12813" endWordPosition="12816">he parsing process should be capable of operating top-down as well as bottom-up. We have seen examples where both of these modes are essential. Our earliest experiments in robust parsing were conducted through the FlexP parsing system (Hayes and Mouradian 1981). This system was based on partial pattern matching, and while it had the first and last of the characteristics listed above, it did not measure up well to the other two. Indeed, many of our ideas on the importance of those characteristics were developed though observation of FlexP&apos;s shortcomings as described in 3.5.2, and more fully in Hayes and Carbonell (1981). With these lessons in mind, we constructed two additional experimental parsers: CASPAR to explore the utility of case frame instantiation in robust parsing, and DYPAR to explore the notion of combining several different parsing strategies in a single parser. Both experiments proved fruitful, as the next two sections show, and DYPAR has now been developed into a complete parsing system, the DYPAR-II parser, as part of the XCALIBUR expert system interface (Carbonell et al. 1983). After that, we describe an approach to parsing we are currently developing that we believe to be based on the best </context>
</contexts>
<marker>Hayes, Carbonell, 1981</marker>
<rawString>Hayes, P.J. and Carbonell, J.G. 1981 Multi-Strategy Parsing and its Role in Robust Man-Machine Communication. Technical report CMU-CS-81-1 18. Computer Science Department, Carnegie-Mellon University, Pittsburgh, Pennsylvania (May).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G G Hendrix</author>
</authors>
<title>Human Engineering for Applied Natural Language Processing.</title>
<date>1977</date>
<booktitle>Proceedings of the Fifth International Joint Conference on Artificial Intelligence:</booktitle>
<pages>183--191</pages>
<contexts>
<context position="1644" citStr="Hendrix 1977" startWordPosition="220" endWordPosition="221"> constructions. Since robust parsers must deal primarily with input that does meet their expectations, the various efforts at coping with extragrammaticality have been generally structured as extensions to existing parsing methods. Probably the most popular approach has been to extend syntactically-oriented parsing techniques employing Augmented Transition Networks (ATNs) (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976). Other researchers have attempted to deal with ungrammatical input through network-based semantic grammar techniques (Hendrix 1977), through extensions to pattern matching parsing in which partial pattern matching is allowed (Hayes and Mouradian 1981), through conceptual case frame instantiation (Dejong 1979, Schank, Lebowitz, and Birnbaum 1980), and through approaches involving multiple cooperating parsing strategies (Carbonell and Hayes 1984, Carbonell et al. 1983, Hayes and Carbonell 1981). This research was sponsored in part by the Air Force Office of Scientific Research under Contract AFOSR-82-0219 and in part by Digital Equipment Corporation as part of the XCALIBUR project. Given the background of existing work, thi</context>
<context position="10488" citStr="Hendrix 1977" startWordPosition="1582" endWordPosition="1583">nouns and adjectives whose meaning can be viewed as a recombination of features present elsewhere in the input. Unlike the KLAUS method, it operates in the background, placing no major run-time burden on the user. However, it remains highly experimental and may not prove practical without user confirmation. 3. Interact with the user in a focused manner to provide a paraphrase of the segment of input containing the unknown word. If this paraphrase results in the desired action, it is stored and becomes the meaning of the new word in the immediate context in which it appeared. The LIFER system (Hendrix 1977) had a rudimentary capacity for defining synonymous phrases. A more general method would generalize synonyms to classify the new word or phrase in different semantic contexts. 2.2.2 Misspellings Misspellings arise when an otherwise recognizable lexeme has letters omitted, substituted, transposed, or spuriously inserted. Misspellings are the most common form of extragrammaticality encountered by natural language interfaces. Usually, a word is misspelt into an unrecognizable character string. But, occasionally a word is misspelt into another word in the dictionary that violates semantic or synta</context>
<context position="19599" citStr="Hendrix 1977" startWordPosition="3043" endWordPosition="3044">ery strategies operate in a sufficiently localized manner that the variations in global behaviour of different approaches to parsing do not come into play. However, most of the strategies are capable of using contextual restrictions on what incorrect lexical item might be, and therefore are most effective when the constraints on the unknown word are strongest. This suggests that they will be most successful when used with an approach to parsing in which it is easy to bring semantic constraints to bear. So, for instance, such techniques are likely to be more effective using a semantic grammar (Hendrix 1977, Brown and Burton 1975) or case frame instantiation (Dejong 1979, Hayes and Carbonell 1981) approach, than in an approach using a syntactic ATN (Woods, Kaplan and Nash-Webber 1972), where the expectations are never more specific than membership in one or more general syntactic categories. 3. Sentential Level Extragrammaticalities Recovery from extragrammaticality at the sentential level is much more dependent on the particular kind of parsing techniques that are employed. Some techniques lend themselves to straightforward recovery methods, while others make recovery difficult. An initial exam</context>
<context position="40260" citStr="Hendrix 1977" startWordPosition="6444" endWordPosition="6445">ress this issue, we classify parsing approaches into three general groups: transition network approaches (including syntactic ATNs and networkbased semantic grammars), pattern matching approaches, and approaches based on case frame instantiation. 3.5.1. Recovery strategies using a transition network approach Although attempts have been made to incorporate sentential level recovery strategies into network-based parsers including both syntactically-based ATNs (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976) and semantic grammar networks (Hendrix 1977), the network paradigm itself is not well suited to the kinds of recovery strategies discussed in the preceding sections. These strategies generally require an interpretive ability to &amp;quot;step back&amp;quot; and take a broad view of the situation when a parser&apos;s expectations are violated, and this is very hard to do when using networks. The underlying problem is that a significant amount of state information during the parse is implicitly encoded by the position in the network; in the case of ATNs, other aspects of the state are contained in the settings of scattered registers. As demonstrated by the meta</context>
<context position="42415" citStr="Hendrix 1977" startWordPosition="6794" endWordPosition="6795">of the error determined precisely, a major problem remains: in order to recover, and parse input that does not accord with the grammar, while remaining true to the network formalism, the parser must modify the network dynamically and temporarily, using the modified network to proceed through the present difficulties. Needless to say, this is at best a very complex process, one whose computational tractability is open to question. It is perhaps not surprising that in one of the most effective recovery mechanisms developed for network-based parsing, the LIFER system&apos;s ellipsis handling routine (Hendrix 1977), the key step operates completely outside the network formalism. As we have seen, semantic constraints are very important in recovering from many types of ungrammatical input, and these are by definition unavailable 130 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language in a purely syntactic ATN parser. However, semantic information can be brought to bear on network based parsing, either through the semantic grammar approach in which joint semantic and syntactic </context>
<context position="58322" citStr="Hendrix 1977" startWordPosition="9324" endWordPosition="9325">e a direct question, a statement of confusion, or echoing the troublesome fragment of the input, thereby combining two forms of ellipsis as illustrated below. User: Give me a dual port tape drive. System: A dual port tape drive? User: Sorry, a dual port disk drive. ■ Reformulation — Part of an old utterance is reformulated and meant to be interpreted in place of the corresponding old constituent. This is perhaps the most common form of ellipsis and the only one for which tractable computational strategies have been implemented. All the examples below are of this type. The LIFER/LADDER system (Hendrix 1977, Sacerdoti 1977) handled a restricted form of reformulation ellipsis. LIFER&apos;s ellipsis algorithm accepted a fragmentary input if it matched a partial parse tree derived from the previous complete parse tree by (a) selecting a subtree that accounted for a contiguous segment of the previous input, and (b) possibly pruning back one or more of its branches. If a fragmentary input matched such a partial parse tree, it was assumed to be a reformulation ellipsis and the missing parts of the partial parse tree were filled out from the previous complete parse tree. In particular, if a single grammar c</context>
<context position="61643" citStr="Hendrix 1977" startWordPosition="9861" endWordPosition="9862">smallest with dual ports?&amp;quot; &amp;quot;Speed with two ports?&amp;quot; &amp;quot;Disk with two ports.?&amp;quot; In these representative examples, punctuation is of no help, and pure syntax is of very limited utility. For instance, the last three phrases are syntactically similar (indeed, the last two are indistinguishable), but each requires that a different substitution be made on the preceding query. The DYPAR-II system (discussed in Section 5.2) handles ellipsis at the case frame level. Here we present the basic case frame ellipsis resolution method it employs. Its coverage appears to be a superset of the LIFER/LADDER system (Hendrix 1977, Sacerdoti 1977) and the PLANES ellipsis module (Waltz and Goodman 1977). Although it handles most of the reformulation ellipsis we encountered, it is not meant to be a general linguistic solution to the ellipsis phenomenon. Consider the following example: &gt;What is the size of the 3 largest single port fixed media disks? &gt;disks with two ports? Note that it is impossible to resolve this kind of ellipsis in a general manner if the previous query is stored verbatim or as a semantic grammar parse tree. &amp;quot;Disks with two ports&amp;quot; would at best correspond to some &lt;disk-descriptor&gt; non-terminal, and hen</context>
</contexts>
<marker>Hendrix, 1977</marker>
<rawString>Hendrix, G.G. 1977 Human Engineering for Applied Natural Language Processing. Proceedings of the Fifth International Joint Conference on Artificial Intelligence: 183-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Kwasny</author>
<author>N K Sondheimer</author>
</authors>
<title>Relaxation Techniques for Parsing Grammatically Ill-Formed Input in Natural Language Understanding Systems.</title>
<date>1981</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>7</volume>
<issue>2</issue>
<pages>99--108</pages>
<contexts>
<context position="1433" citStr="Kwasny and Sondheimer 1981" startWordPosition="188" endWordPosition="191">ble of processing input utterances that deviate from its grammatical and semantic expectations. Many researchers have made this observation and have taken initial steps towards coverage of certain classes of extragrammatical constructions. Since robust parsers must deal primarily with input that does meet their expectations, the various efforts at coping with extragrammaticality have been generally structured as extensions to existing parsing methods. Probably the most popular approach has been to extend syntactically-oriented parsing techniques employing Augmented Transition Networks (ATNs) (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976). Other researchers have attempted to deal with ungrammatical input through network-based semantic grammar techniques (Hendrix 1977), through extensions to pattern matching parsing in which partial pattern matching is allowed (Hayes and Mouradian 1981), through conceptual case frame instantiation (Dejong 1979, Schank, Lebowitz, and Birnbaum 1980), and through approaches involving multiple cooperating parsing strategies (Carbonell and Hayes 1984, Carbonell et al. 1983, Hayes and Carbonell 1981). This research was spo</context>
<context position="40136" citStr="Kwasny and Sondheimer 1981" startWordPosition="6423" endWordPosition="6426">, there are considerable differences in the underlying suitability of the various approaches as bases for the recovery strategies. To address this issue, we classify parsing approaches into three general groups: transition network approaches (including syntactic ATNs and networkbased semantic grammars), pattern matching approaches, and approaches based on case frame instantiation. 3.5.1. Recovery strategies using a transition network approach Although attempts have been made to incorporate sentential level recovery strategies into network-based parsers including both syntactically-based ATNs (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976) and semantic grammar networks (Hendrix 1977), the network paradigm itself is not well suited to the kinds of recovery strategies discussed in the preceding sections. These strategies generally require an interpretive ability to &amp;quot;step back&amp;quot; and take a broad view of the situation when a parser&apos;s expectations are violated, and this is very hard to do when using networks. The underlying problem is that a significant amount of state information during the parse is implicitly encoded by the position in the network; in th</context>
</contexts>
<marker>Kwasny, Sondheimer, 1981</marker>
<rawString>Kwasny, S.C. and Sondheimer, N.K. 1981 Relaxation Techniques for Parsing Grammatically Ill-Formed Input in Natural Language Understanding Systems. American Journal of Computational Linguistics 7(2): 99-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E D Sacerdoti</author>
</authors>
<title>Language Access to Distributed Data with Error Recovery.</title>
<date>1977</date>
<booktitle>Proceedings of the Fifth International Joint Conference on Artificial Intelligence:</booktitle>
<pages>196--202</pages>
<contexts>
<context position="58339" citStr="Sacerdoti 1977" startWordPosition="9326" endWordPosition="9328">stion, a statement of confusion, or echoing the troublesome fragment of the input, thereby combining two forms of ellipsis as illustrated below. User: Give me a dual port tape drive. System: A dual port tape drive? User: Sorry, a dual port disk drive. ■ Reformulation — Part of an old utterance is reformulated and meant to be interpreted in place of the corresponding old constituent. This is perhaps the most common form of ellipsis and the only one for which tractable computational strategies have been implemented. All the examples below are of this type. The LIFER/LADDER system (Hendrix 1977, Sacerdoti 1977) handled a restricted form of reformulation ellipsis. LIFER&apos;s ellipsis algorithm accepted a fragmentary input if it matched a partial parse tree derived from the previous complete parse tree by (a) selecting a subtree that accounted for a contiguous segment of the previous input, and (b) possibly pruning back one or more of its branches. If a fragmentary input matched such a partial parse tree, it was assumed to be a reformulation ellipsis and the missing parts of the partial parse tree were filled out from the previous complete parse tree. In particular, if a single grammar category accounted</context>
<context position="61660" citStr="Sacerdoti 1977" startWordPosition="9863" endWordPosition="9864">dual ports?&amp;quot; &amp;quot;Speed with two ports?&amp;quot; &amp;quot;Disk with two ports.?&amp;quot; In these representative examples, punctuation is of no help, and pure syntax is of very limited utility. For instance, the last three phrases are syntactically similar (indeed, the last two are indistinguishable), but each requires that a different substitution be made on the preceding query. The DYPAR-II system (discussed in Section 5.2) handles ellipsis at the case frame level. Here we present the basic case frame ellipsis resolution method it employs. Its coverage appears to be a superset of the LIFER/LADDER system (Hendrix 1977, Sacerdoti 1977) and the PLANES ellipsis module (Waltz and Goodman 1977). Although it handles most of the reformulation ellipsis we encountered, it is not meant to be a general linguistic solution to the ellipsis phenomenon. Consider the following example: &gt;What is the size of the 3 largest single port fixed media disks? &gt;disks with two ports? Note that it is impossible to resolve this kind of ellipsis in a general manner if the previous query is stored verbatim or as a semantic grammar parse tree. &amp;quot;Disks with two ports&amp;quot; would at best correspond to some &lt;disk-descriptor&gt; non-terminal, and hence, according to </context>
</contexts>
<marker>Sacerdoti, 1977</marker>
<rawString>Sacerdoti, E.D. 1977 Language Access to Distributed Data with Error Recovery. Proceedings of the Fifth International Joint Conference on Artificial Intelligence: 196-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
<author>M Lebowitz</author>
<author>L Birnbaum</author>
</authors>
<title>An Integrated Understander.</title>
<date>1980</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>6</volume>
<issue>1</issue>
<pages>13--30</pages>
<marker>Schank, Lebowitz, Birnbaum, 1980</marker>
<rawString>Schank, R.C.; Lebowitz, M.; and Birnbaum, L. 1980 An Integrated Understander. American Journal of Computational Linguistics 6(1): 13-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Waltz</author>
</authors>
<title>An English Language Question Answering System for a Large Relational Data Base,</title>
<date>1978</date>
<journal>Communications of the ACM</journal>
<volume>21</volume>
<issue>7</issue>
<pages>526--539</pages>
<contexts>
<context position="34400" citStr="Waltz 1978" startWordPosition="5484" endWordPosition="5485"> semantic information to be brought to bear are at a considerable advantage. Both bottom-up and top-down recovery strategies are possible for detecting and recovering from missing and spurious constituents. In the bottom-up approach, all the fragments are recognized independently, and purely semantic constraints are used to assemble them into a single framework meaningful in terms of the domain of discourse. When the domain is restricted enough, the semantic constraints can be such that they always produce a unique result. This characteristic was exploited to good effect in the PLANES system (Waltz 1978) in which an input utterance was recognized as a sequence of fragments which were then assembled into a meaningful whole on the basis of semantic considerations alone. A top-down approach to fragment recognition requires that the top-level or organizing concept in the utterance (&amp;quot;add&amp;quot; in the above examples) be located first and the predictions obtainable from it about what else might appear in the utterance be used to guide and constrain the recognition of the other fragments. As a final point, note that in the case of out of order constituents, a parser relying on a strict left-toright scan w</context>
</contexts>
<marker>Waltz, 1978</marker>
<rawString>Waltz, D.L. 1978 An English Language Question Answering System for a Large Relational Data Base, Communications of the ACM 21(7): 526-539.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime G Carbonell</author>
<author>J Philip</author>
</authors>
<title>Hayes Recovery Strategies for Parsing Extrammatical Language</title>
<date>1983</date>
<journal>American Journal of Computational Linguistics, Volume 9, Numbers</journal>
<pages>3--4</pages>
<marker>Carbonell, Philip, 1983</marker>
<rawString>American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 145 Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Waltz</author>
<author>A B Goodman</author>
</authors>
<title>Writing a Natural Language Data Base System.</title>
<date>1977</date>
<booktitle>Proceedings of the Fifth International Joint Conference on Artificial Intelligence:</booktitle>
<pages>144--150</pages>
<contexts>
<context position="61716" citStr="Waltz and Goodman 1977" startWordPosition="9870" endWordPosition="9873">two ports.?&amp;quot; In these representative examples, punctuation is of no help, and pure syntax is of very limited utility. For instance, the last three phrases are syntactically similar (indeed, the last two are indistinguishable), but each requires that a different substitution be made on the preceding query. The DYPAR-II system (discussed in Section 5.2) handles ellipsis at the case frame level. Here we present the basic case frame ellipsis resolution method it employs. Its coverage appears to be a superset of the LIFER/LADDER system (Hendrix 1977, Sacerdoti 1977) and the PLANES ellipsis module (Waltz and Goodman 1977). Although it handles most of the reformulation ellipsis we encountered, it is not meant to be a general linguistic solution to the ellipsis phenomenon. Consider the following example: &gt;What is the size of the 3 largest single port fixed media disks? &gt;disks with two ports? Note that it is impossible to resolve this kind of ellipsis in a general manner if the previous query is stored verbatim or as a semantic grammar parse tree. &amp;quot;Disks with two ports&amp;quot; would at best correspond to some &lt;disk-descriptor&gt; non-terminal, and hence, according to the LIFER algorithm, would replace the entire phrase &amp;quot;si</context>
</contexts>
<marker>Waltz, Goodman, 1977</marker>
<rawString>Waltz, D.L. and Goodman, A.B. 1977 Writing a Natural Language Data Base System. Proceedings of the Fifth International Joint Conference on Artificial Intelligence: 144-150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Weischedel</author>
<author>N K Sondheimer</author>
</authors>
<title>Meta-Rules as a Basis for Processing Ill-formed Input.</title>
<date>1983</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>9</volume>
<issue>3</issue>
<pages>161--177</pages>
<contexts>
<context position="40949" citStr="Weischedel and Sondheimer (1983)" startWordPosition="6554" endWordPosition="6557">inds of recovery strategies discussed in the preceding sections. These strategies generally require an interpretive ability to &amp;quot;step back&amp;quot; and take a broad view of the situation when a parser&apos;s expectations are violated, and this is very hard to do when using networks. The underlying problem is that a significant amount of state information during the parse is implicitly encoded by the position in the network; in the case of ATNs, other aspects of the state are contained in the settings of scattered registers. As demonstrated by the meta-rule approach to diagnosing parse failures described by Weischedel and Sondheimer (1983) elsewhere in this journal issue, these and other difficulties elaborated below do not preclude recovery from extragrammatical input. However, they do make it difficult and often impractical, since much of the procedurally encoded state must be made declarative and explicit to the recovery strategies. Often an ATN parse will continue beyond the point where the grammatical deviation, say an omitted word, occurred, and reach a node in the network from which it can make no further progress (that is, no arcs can be traversed). At this point, the parser cannot ascertain the source of the error by e</context>
</contexts>
<marker>Weischedel, Sondheimer, 1983</marker>
<rawString>Weischedel, R.M. and Sondheimer, N.K. 1983 Meta-Rules as a Basis for Processing Ill-formed Input. American Journal of Computational Linguistics 9(3-4): 161-177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Weischedel</author>
<author>J Black</author>
</authors>
<title>Responding to Potentially Unparseable Sentences.</title>
<date>1980</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>6</volume>
<pages>97--109</pages>
<contexts>
<context position="1492" citStr="Weischedel and Black 1980" startWordPosition="196" endWordPosition="199">mmatical and semantic expectations. Many researchers have made this observation and have taken initial steps towards coverage of certain classes of extragrammatical constructions. Since robust parsers must deal primarily with input that does meet their expectations, the various efforts at coping with extragrammaticality have been generally structured as extensions to existing parsing methods. Probably the most popular approach has been to extend syntactically-oriented parsing techniques employing Augmented Transition Networks (ATNs) (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976). Other researchers have attempted to deal with ungrammatical input through network-based semantic grammar techniques (Hendrix 1977), through extensions to pattern matching parsing in which partial pattern matching is allowed (Hayes and Mouradian 1981), through conceptual case frame instantiation (Dejong 1979, Schank, Lebowitz, and Birnbaum 1980), and through approaches involving multiple cooperating parsing strategies (Carbonell and Hayes 1984, Carbonell et al. 1983, Hayes and Carbonell 1981). This research was sponsored in part by the Air Force Office of Scientific Resear</context>
<context position="40195" citStr="Weischedel and Black 1980" startWordPosition="6432" endWordPosition="6435">bility of the various approaches as bases for the recovery strategies. To address this issue, we classify parsing approaches into three general groups: transition network approaches (including syntactic ATNs and networkbased semantic grammars), pattern matching approaches, and approaches based on case frame instantiation. 3.5.1. Recovery strategies using a transition network approach Although attempts have been made to incorporate sentential level recovery strategies into network-based parsers including both syntactically-based ATNs (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976) and semantic grammar networks (Hendrix 1977), the network paradigm itself is not well suited to the kinds of recovery strategies discussed in the preceding sections. These strategies generally require an interpretive ability to &amp;quot;step back&amp;quot; and take a broad view of the situation when a parser&apos;s expectations are violated, and this is very hard to do when using networks. The underlying problem is that a significant amount of state information during the parse is implicitly encoded by the position in the network; in the case of ATNs, other aspects of the state are contained in</context>
</contexts>
<marker>Weischedel, Black, 1980</marker>
<rawString>Weischedel, R.M. and Black, J. 1980 Responding to Potentially Unparseable Sentences. American Journal of Computational Linguistics 6: 97-109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y A Wilks</author>
</authors>
<title>Preference Semantics.</title>
<date>1975</date>
<booktitle>Formal Semantics of Natural Lanbuage.</booktitle>
<editor>In Keenan, ed.,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="50781" citStr="Wilks 1975" startWordPosition="8141" endWordPosition="8142">ibilities and others like them as nonsensical. However, pragmatic knowledge is required to select &amp;quot;to John Smith&amp;quot; as the preferred reading (possibly subject to user confirmation) — the destination case of the verb is required for the command to be effective, whereas the other cases, if present, are optional. This knowledge of the underlying action must be brought to bear at parse time to disambiguate the cryptic command. In the XCALIBUR system case frame encoding (Carbonell, Boggs, Mauldin, and Anick 1983), we apply precisely such pragmatic knowledge represented as preference constraints (cf. Wilks 1975) on case fillers at parse time. Thus, problems created by the absence of expected case markers can be overcome by the application of domain knowledge. ■ The propagation of semantic knowledge through a case frame (via attached procedures such as those of KRL (Bobrow and Winograd 1977) or SRL (Wright and Fox 1983)) can fill in parser defaults and allow the internal completion of phrases such as &amp;quot;dual disks&amp;quot; to mean &amp;quot;dual ported disks&amp;quot;. This process is also responsible for noticing when information is either missing or ambiguously determined, thereby initiating a focused clarificational dialogue </context>
</contexts>
<marker>Wilks, 1975</marker>
<rawString>Wilks, Y. A. 1975 Preference Semantics. In Keenan, ed., Formal Semantics of Natural Lanbuage. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>Cascaded ATN Grammars.</title>
<date>1980</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>6</volume>
<pages>1--12</pages>
<contexts>
<context position="43417" citStr="Woods 1980" startWordPosition="6953" endWordPosition="6954">cal Language in a purely syntactic ATN parser. However, semantic information can be brought to bear on network based parsing, either through the semantic grammar approach in which joint semantic and syntactic categories are used directly in the ATN, or by allowing the tests on ATN arcs to depend on semantic criteria (Bobrow 1978, Bobrow and Webber 1980). In the former technique, the appropriate semantic information for recovery can be applied only if the correct network node can be located — a sometimes difficult task as we have seen. In the latter technique, sometimes known as cascaded ATNs (Woods 1980), the syntactic and semantic parts of the grammar are kept separate, thus giving the potential for a higher degree of interpretiveness in using the semantic information. However, the natural way to use this technique is to employ the semantic information only to confirm or disconfirm parses arrived at on syntactic grounds. So the rigidity of the network formalism makes it very difficult to bring the available semantic information to bear effectively on extragrammatical input. A further disadvantage of the network approach for implementing flexible recovery strategies is that networks naturally</context>
</contexts>
<marker>Woods, 1980</marker>
<rawString>Woods, W. A. 1980 Cascaded ATN Grammars. American Journal of Computational Linguistics 6: 1-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
<author>R M Kaplan</author>
<author>B Nash-Webber</author>
</authors>
<title>The Lunar Sciences Language System: Final Report. Technical report 2378. Bolt Beranek and Newman,</title>
<date>1972</date>
<location>Cambridge, Massachusetts.</location>
<marker>Woods, Kaplan, Nash-Webber, 1972</marker>
<rawString>Woods, W.A.; Kaplan, R.M.; and Nash-Webber, B. 1972 The Lunar Sciences Language System: Final Report. Technical report 2378. Bolt Beranek and Newman, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
<author>M Bates</author>
<author>G Brown</author>
<author>B Bruce</author>
<author>C Cook</author>
<author>J Klovstad</author>
<author>Mak houl</author>
<author>J Nash-Webber</author>
<author>B Schwartz</author>
<author>R Wolf</author>
<author>J</author>
<author>V Zue</author>
</authors>
<title>Speech Understanding Systems —</title>
<date>1976</date>
<tech>Final Technical Report. Technical report 3438.</tech>
<institution>Bolt Beranek and Newman,</institution>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1512" citStr="Woods et al. 1976" startWordPosition="200" endWordPosition="203">tations. Many researchers have made this observation and have taken initial steps towards coverage of certain classes of extragrammatical constructions. Since robust parsers must deal primarily with input that does meet their expectations, the various efforts at coping with extragrammaticality have been generally structured as extensions to existing parsing methods. Probably the most popular approach has been to extend syntactically-oriented parsing techniques employing Augmented Transition Networks (ATNs) (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976). Other researchers have attempted to deal with ungrammatical input through network-based semantic grammar techniques (Hendrix 1977), through extensions to pattern matching parsing in which partial pattern matching is allowed (Hayes and Mouradian 1981), through conceptual case frame instantiation (Dejong 1979, Schank, Lebowitz, and Birnbaum 1980), and through approaches involving multiple cooperating parsing strategies (Carbonell and Hayes 1984, Carbonell et al. 1983, Hayes and Carbonell 1981). This research was sponsored in part by the Air Force Office of Scientific Research under Contract AF</context>
<context position="40215" citStr="Woods et al. 1976" startWordPosition="6436" endWordPosition="6439">aches as bases for the recovery strategies. To address this issue, we classify parsing approaches into three general groups: transition network approaches (including syntactic ATNs and networkbased semantic grammars), pattern matching approaches, and approaches based on case frame instantiation. 3.5.1. Recovery strategies using a transition network approach Although attempts have been made to incorporate sentential level recovery strategies into network-based parsers including both syntactically-based ATNs (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976) and semantic grammar networks (Hendrix 1977), the network paradigm itself is not well suited to the kinds of recovery strategies discussed in the preceding sections. These strategies generally require an interpretive ability to &amp;quot;step back&amp;quot; and take a broad view of the situation when a parser&apos;s expectations are violated, and this is very hard to do when using networks. The underlying problem is that a significant amount of state information during the parse is implicitly encoded by the position in the network; in the case of ATNs, other aspects of the state are contained in the settings of sca</context>
<context position="44414" citStr="Woods et al. 1976" startWordPosition="7107" endWordPosition="7110"> difficult to bring the available semantic information to bear effectively on extragrammatical input. A further disadvantage of the network approach for implementing flexible recovery strategies is that networks naturally operate in a top-down left-to-right mode. As we have seen, a bottom-up capability is essential for many recovery strategies, and directional flexibility often enables easier and more efficient operation of the strategies. Of course, the top-down leftto-right mode of operation is a characteristic of the network interpreter, not of the network formalism itself, and an attempt (Woods et al. 1976) has been made to operate an ATN in an &amp;quot;island&amp;quot; mode, that is, bottom-up, center-out. This experiment was done in the context of a speech parser where the low-level recognition of many of the input words was uncertain, though the input as a whole was assumed to be grammatical. In that situation, there were clear advantages to starting with islands of relative lexical certainty, and working out from there. Problems, however, arise during leftward expansion from an island when it is necessary to run the network backwards. The admissibility of ATN transitions can depend on tests that access the v</context>
</contexts>
<marker>Woods, Bates, Brown, Bruce, Cook, Klovstad, houl, Nash-Webber, Schwartz, Wolf, J, Zue, 1976</marker>
<rawString>Woods, W.A.; Bates, M.; Brown, G.; Bruce, B.; Cook, C.; Klovstad, J.; Mak houl, J.; Nash-Webber, B.; Schwartz, R.; Wolf, J.; and Zue, V. 1976 Speech Understanding Systems — Final Technical Report. Technical report 3438. Bolt Beranek and Newman, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wright</author>
<author>M Fox</author>
</authors>
<title>The SRL Users Manual.</title>
<date>1983</date>
<tech>Technical report.</tech>
<institution>Robotics Institute, Carnegie-Mellon University,</institution>
<location>Pittsburgh, Pennsylvania.</location>
<contexts>
<context position="51094" citStr="Wright and Fox 1983" startWordPosition="8192" endWordPosition="8195">onal. This knowledge of the underlying action must be brought to bear at parse time to disambiguate the cryptic command. In the XCALIBUR system case frame encoding (Carbonell, Boggs, Mauldin, and Anick 1983), we apply precisely such pragmatic knowledge represented as preference constraints (cf. Wilks 1975) on case fillers at parse time. Thus, problems created by the absence of expected case markers can be overcome by the application of domain knowledge. ■ The propagation of semantic knowledge through a case frame (via attached procedures such as those of KRL (Bobrow and Winograd 1977) or SRL (Wright and Fox 1983)) can fill in parser defaults and allow the internal completion of phrases such as &amp;quot;dual disks&amp;quot; to mean &amp;quot;dual ported disks&amp;quot;. This process is also responsible for noticing when information is either missing or ambiguously determined, thereby initiating a focused clarificational dialogue (Hayes 1981). ■ The representation of case frames is inherently non-uniform. Case fillers, case markers, and case headers are all represented separately, and this distinction can be used by the parser interpretively instantiating the case frame. For instance, if a case frame accounts for the non-spurious part of</context>
</contexts>
<marker>Wright, Fox, 1983</marker>
<rawString>Wright, K. and Fox, M 1983 The SRL Users Manual. Technical report. Robotics Institute, Carnegie-Mellon University, Pittsburgh, Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>