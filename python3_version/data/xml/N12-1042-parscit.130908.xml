<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017909">
<title confidence="0.883412">
Towards Using EEG to Improve ASR Accuracy
</title>
<author confidence="0.940202">
Yun-Nung Chen, Kai-Min Chang, and Jack Mostow
</author>
<affiliation confidence="0.9026075">
Project LISTEN (http://www.cs.cmu.edu/—listen)
School of Computer Science, Carnegie Mellon University
</affiliation>
<address confidence="0.51142">
5000 Forbes Avenue, Pittsburgh, PA 15213-3891, USA
</address>
<email confidence="0.999118">
{yvchen,kkchang,mostow}@cs.cmu.edu
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999483153846154">
We report on a pilot experiment to improve the per-
formance of an automatic speech recognizer (ASR)
by using a single-channel EEG signal to classify the
speaker’s mental state as reading easy or hard text.
We use a previously published method (Mostow et
al., 2011) to train the EEG classifier. We use its prob-
abilistic output to control weighted interpolation of
separate language models for easy and difficult read-
ing. The EEG-adapted ASR achieves higher accu-
racy than two baselines. We analyze how its perfor-
mance depends on EEG classification accuracy. This
pilot result is a step towards improving ASR more
generally by using EEG to distinguish mental states.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980423076923">
Humans use speech to communicate what’s on their
mind. However, until now, automatic speech recogniz-
ers (ASR) and dialogue systems have had no direct way
to take into account what is going on in a speaker’s
mind. Some work has attempted to infer cognitive
states from volume and speaking rate to adapt language
modeling (Ward and Vega, 2009) or from query click
logs (Hakkani-T¨ur et al., 2011) to detect domains. A
new way to address this limitation is to infer mental states
from electroencephalogram (EEG) signals.
EEG is a voltage signal that can be measured on the
surface of the scalp, arising from large areas of coordi-
nated neural activity. This neural activity varies as a func-
tion of development, mental state, and cognitive activity,
and EEG can measurably detect such variation.
Recently, a few companies have scaled back medical
grade EEG technology to create portable EEG headsets
that are commercially available and simple to use. The
NeuroSky MindSetTM (2009), for example, is an audio
headset equipped with a single-channel EEG sensor. It
measures the voltage between an electrode that rests on
the forehead and electrodes in contact with the ear. Un-
like the multi-channel electrode nets worn in labs, the
sensor requires no gel or saline for recording, and re-
quires no expertise to wear. Even with the limitations
of recording from only a single sensor and working with
untrained users, Furthermore, Mostow et al.(2011) used
its output signal to distinguish easy from difficult reading,
achieving above-chance accuracy. Here we build on that
work by using the output of such classifiers to adapt lan-
guage models for ASR and thereby improve recognition
accuracy.
The most similar work is Jou and Schultz’s (2008) use
of electromyographic (EMG) signals generated by human
articulatory muscles in producing speech. They showed
that augmenting acoustic features with these EMG fea-
tures can achieve rudimentary silent speech detection.
Pasley et al. (2012) used electrocorticographic (ECoG)
recordings from nonprimary auditory cortex in the human
superior temporal gyrus to reconstruct acoustic informa-
tion in speech sounds. Our work differs from these efforts
in that we use a consumer-grade single-channel EEG sen-
sor measuring frontal lobe activities, and that we use the
detected mental state just to help improve ASR perfor-
mance rather than to dictate or reconstruct speech, which
are much harder tasks.
Section 2 describes how to use machine learning to dis-
tinguish mental states associated with easy and difficult
readings. Section 3 describes how we use EEG classifier
output to adapt ASR language models. Section 4 uses an
oracle simulation to show how increasing EEG classifier
accuracy will affect ASR accuracy. Section 5 concludes.
</bodyText>
<sectionHeader confidence="0.956555" genericHeader="method">
2 Mental State Classification Using EEG
</sectionHeader>
<bodyText confidence="0.9992686">
We use training and testing data from Mostow et al.’s
(2011) experiment, which presented text passages, one
sentence at a time, to 10 adults and 11 nine- to ten-year-
olds wearing a Neurosky MindsetTM (2009). They read
three easy and three difficult texts aloud, in alternating
</bodyText>
<page confidence="0.907032333333333">
382
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 382–385,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999633153846154">
order. The “easy” passages were from texts classified by
the Common Core Standards1 at the K-1 level. The “diffi-
cult” passages were from practice materials for the Grad-
uate Record Exam2 and the ACE GED test3. Across the
reading conditions, passages ranged from 62 to 83 words
long. Although instructed to read the text aloud, the read-
ers (especially children) did not always read correctly or
follow the displayed sentences.
Following Mostow et al. (2011), we trained binary lo-
gistic regression classifiers to estimate the probability that
an EEG signal is associated with reading an easy (or diffi-
cult) sentence. As features for logistic regression we used
the streams of values logged by the MindSet:
</bodyText>
<listItem confidence="0.999007444444444">
1. The raw EEG signal, sampled at 512 Hz
2. A filtered version of the raw signal, also sampled at
512 Hz, which is raw signal smoothed over a win-
dow of 2 seconds
3. Proprietary “attention” and “meditation” measures,
reported at 1 Hz
4. A power spectrum of 1Hz bands from 1-256 Hz, re-
ported at 8 Hz
5. An indicator of signal quality, reported at 1 Hz
</listItem>
<bodyText confidence="0.999784">
Head movement or system instability led to missing or
poor-quality EEG data for some utterances, which we ex-
cluded in order to focus on utterances with clear acous-
tic and EEG signals. The features for each utterance
consisted of measures 1-4, averaged over the utterance,
excluding the 15% of observations where measure 5 re-
ported poor signals. After filtering, the data includes 269
utterances from adults and 243 utterances from children,
where 327 utterances are for the easy passages and 185
utterances are for the difficult passages. To balance the
classes, we used the undersampling method for training.
We trained a reader-specific classifier on each reader’s
data from all but one text passage, tested it on each
sentence in the held-out passage, performed this proce-
dure for each passage, and averaged the results to cross-
validate accuracy within readers. We computed classifi-
cation accuracy as the percentage of utterances classified
correctly. Classification accuracy for adults’, children’s,
and total oral reading was 71.49%, 58.74%, and 65.45%
respectively. A one-tailed t-test, with classification accu-
racy on an utterance as the random variable, showed that
EEG classification was significantly better than chance.
</bodyText>
<sectionHeader confidence="0.997693" genericHeader="method">
3 Language Model Adaptation for ASR
</sectionHeader>
<bodyText confidence="0.999868">
Traditional ASR decodes a word sequence W* from the
acoustic model and language model as below:
</bodyText>
<footnote confidence="0.9495685">
1http://www.corestandards.org
2http://majortests.com/gre/reading comprehension.php
3http://college.cengage.com:80/devenglish/resources/reading
ace/students
</footnote>
<equation confidence="0.92617925">
W* = argmaxW P(W  |A) (1)
P(A  |W) - P(W)
P(A)
To incorporate EEG, we include mental state N as an ad-
ditional observation in the decoding procedure:
W* = argmaxW P(W  |A, N) (2)
P(A  |W) - P(W  |N)
P(A)
</equation>
<bodyText confidence="0.999964461538462">
The six passages use a vocabulary of 430 distinct
words. To evaluate the impact on ASR accuracy of us-
ing EEG to adapt language models, we needed acoustic
models appropriate for the speakers. For adult speech, we
used the US English HUB4 Acoustic Model from CMU
Sphinx. For children’s speech, we used Project LISTEN’s
acoustic models trained on children’s oral reading.
We used separate trigram language models (with bi-
gram and unigram backoff) for easy and difficult text –
EasyLM, trained on the three easy passages, and Diffi-
cultLM, trained on the three difficult passages. Both lan-
guage models used the same lexicon, consisting of the
430 words in all six target passages. All experiments used
the same ASR parameter values.
As a gold standard, all utterances were manually tran-
scribed by a native English speaker. To measure ASR per-
formance, we computed Word Accuracy (WACC) as the
number of words recognized correctly minus insertions
divided by number of words in the reference transcripts
for each reader, and averaged them.
Then we can adapt the language model to estimate
P(W  |N) using mental state information. Using the
EEG classifier described in Section 2, we adapted the lan-
guage model separately for each utterance, using three
types of language model adaptation: hard selection, soft
selection, and combination with ASR output.
</bodyText>
<subsectionHeader confidence="0.998752">
3.1 Hard Selection of Language Models
</subsectionHeader>
<bodyText confidence="0.999923">
Given the probabilistic estimate that a given utterance
was easy or difficult (SEasy(N) and SDifficult(N)), hard se-
lection simply picks EasyLM if the utterance was likelier
to be easy, or DifficultLM otherwise:
</bodyText>
<equation confidence="0.977977">
PHard(W  |N) = IC(N) - PEasy(W) (3)
+ (1 − IC(N)) - PDiff(W).
Here IC(N) = 1 if SEasy(N) &gt; SDifficult(N), and
</equation>
<listItem confidence="0.924496">
PEasy(W) and PDiff(W) are the probability of word W in
EasyLM and DifficultLM, respectively. For comparison,
the Random Pick baseline randomly picks either EasyLM
or DifficultLM:
</listItem>
<bodyText confidence="0.5502115">
= argmaxW
= argmaxW
</bodyText>
<page confidence="0.956677">
383
</page>
<table confidence="0.999709">
WACC Adult Child
Easy Difficult All Easy Difficult All
Baseline 1: Random Pick 54.5 51.2 53.8 32.8 14.7 30.6
EEG-based: Hard Selection 57.6 49.4 52.7 36.4 17.0 32.8
Baseline 2: Equal Weight 63.2 59.9 56.5 37.3 19.5 33.4
EEG-based: Soft Selection w/o smoothing 57.2 48.8 52.4 35.8 17.2 32.5
EEG-based: Soft Selection w/ smoothing 66.0 62.3 64.2 39.8 22.7 36.2
Baseline 3: Weight from ASR (α = 0) 63.8 60.6 61.5 39.2 20.0 35.0
Weight from ASR and EEG (α = 0.5) 64.5 63.4 63.5 39.2 21.9 36.0
</table>
<tableCaption confidence="0.999887">
Table 1: ASR performance of proposed approaches using EEG-based classification of mental states.
</tableCaption>
<equation confidence="0.999769">
PRandom(W) = IR · PEasy(W) (4)
+ (1 − IRandom) · PDiff(W).
</equation>
<bodyText confidence="0.538948">
Here IR is randomly set to 0 or 1.
</bodyText>
<subsectionHeader confidence="0.999098">
3.2 Soft Selection of Language Models
</subsectionHeader>
<bodyText confidence="0.999681">
Mental state classification based on EEG is imperfect,
and using only the corresponding language model (Ea-
syLM or DifficultLM) to decode the target utterance is li-
able to perform worse when the classifier is wrong. Thus,
we use the classifier’s probabilistic estimate that the ut-
terance is easy (or difficult) as interpolation weights to
linearly combine EasyLM and DifficultLM:
</bodyText>
<equation confidence="0.95936525">
PSoft(W  |N) = wEasy(N) · PEasy(W) (5)
+ wDiff(N) · PDiff(W).
Here wEasy(N) and wDiff(N) are from classifier’s output.
wEasy(N) = SEasy(N),wDiff(N) = SDiff(N) (6)
</equation>
<bodyText confidence="0.999468">
Additionally, we can adjust the range of weights by
smoothing the probability outputted by the EEG classi-
fier:
</bodyText>
<equation confidence="0.958588">
+ SEasy (N)
δwEasy (N) = 2δ + 1 , (7)
δ + SDiff(N)
wDiff(N) = 2δ + 1
</equation>
<bodyText confidence="0.998783833333333">
Here SEasy(N) (or SDiff(N)) is the classifier’s probabilis-
tic estimate that the sentence is easy (or difficult) and
δ is the smoothing weight, which we set to 0.5. Af-
ter smoothing the probabilities, wEasy(N) and wDiff(N)
each lie within the interval [0.25, 0.75], and wEasy(N) +
wDiff(N) = 1. That is, Soft Selection with smoothing in-
terpolates the two language models, but assigns a weight
of at least 0.25 to each one to reduce the impact of EEG
classifier errors. Notice that δ = 0 is equivalent to EEG
Soft Selection without smoothing.
For comparison, the Equal Weight baseline interpo-
lates EasyLM and DifficultLM with equal weights:
</bodyText>
<equation confidence="0.990165">
PEqual(W) = 0.5 · PEasy(W) + 0.5 · PDiff(W) (8)
</equation>
<subsectionHeader confidence="0.999242">
3.3 Combination with ASR Output
</subsectionHeader>
<bodyText confidence="0.987903">
Given the ASR results from the Equal Weight baseline,
we can derive SLsy(N) as:
</bodyText>
<equation confidence="0.99997875">
SLsy(N) = α · SEasy(N) (9)
PEasy(W0)
+ (1 − α) ·
PEasy(Wo) + PDiff(W0)
</equation>
<bodyText confidence="0.9997847">
Here we can estimate SEasy(N) based on the classifier’s
output and the probability of the recognized words Wo in
EasyLM. We can derive SDiff(N) in the same way. Then
we can use (5) and (7) to re-decode the utterances by us-
ing SEasy(N) and SDiff(N). Here α is a linear interpola-
tion weight, where we set to 0.5 to give equal weights to
ASR output and EEG. For comparison, the ASR baseline
uses weights from only the ASR results, where α = 0.
Notice that the case of α = 1 is equivalent to EEG Soft
Selection with smoothing.
</bodyText>
<subsectionHeader confidence="0.990166">
3.4 Results of Proposed Approaches
</subsectionHeader>
<bodyText confidence="0.990510636363636">
Table 1 shows the performance of our proposed ap-
proaches and the corresponding baselines as measured by
WACC. According to one-tailed t-tests with word accu-
racy of an utterance as the random variable, the results
in boldface are significantly better tgan their respective
baselines (p &lt; 0.05).
Hard Selection (row b) outperforms the Random Pick
baseline (row a). Soft Selection without smoothing (row
d) has similar performance as Hard Selection because the
classifier often outputs probability estimates that are ei-
ther 1 or 0. However, Soft Selection with smoothing (row
e) outperforms the Equal Weight baseline (row c). The
Weight from ASR baseline (row f) is better than the other
baselines. Weight from ASR and EEG (row g) can fur-
ther improve performance, but it’s not better than Soft
Selection with smoothing (row e) - evidence that EEG
gives good estimation for choosing language models. In
short, Table 1 shows that using EEG to choose between
EasyLM and DifficultLM achieves higher ASR accuracy
than the baselines that do not use EEG.
Comparing the first two baselines, the Equal Weight
baseline (row c) outperforms the Random Pick baseline
</bodyText>
<page confidence="0.995806">
384
</page>
<figure confidence="0.998926033333334">
Predicted WACC (%)
0 10 20 30 40 50 60 70 80 90 100
Simulated Accuracy of Classification (%)
(a) Adult
Predicted WACC (%)
0 10 20 30 40 50 60 70 80 90 100
Simulated Accuracy of Classification (%)
(b) Child
40
20
70
60
50
30
10
0
Easy utt.
Difficult utt.
All utt.
40
20
70
60
50
30
10
0
Easy utt.
Difficult utt.
All utt.
</figure>
<figureCaption confidence="0.99952">
Figure 1: The simulated accuracy graphs plot the predicted ASR word accuracy against the level of EEG classification accuracy
simulated by an oracle.
</figureCaption>
<bodyText confidence="0.999816083333333">
(row a) in every column, because the loss in ASR accu-
racy from picking the wrong language model outweighs
the improvement from picking the right one. Similarly,
EEG-based Soft Selection with smoothing (row e) out-
performs EEG-based Hard Selection (row b) in every col-
umn because the interpolated language model is more
robust to EEG classification error. The third base-line,
Weight from ASR (row f) depends solely on ASR results
to estimate weights; it performs better than other base-
lines, but not as well as EEG-based Soft Selection with
smoothing (row e). That is, using EEG alone can weight
the two language models better than ASR alone.
</bodyText>
<sectionHeader confidence="0.998526" genericHeader="method">
4 Oracle Simulation
</sectionHeader>
<bodyText confidence="0.999867869565217">
To explore the relationship between EEG classifier ac-
curacy and the effect of EEG-based adaptation on ASR
accuracy, we simulate different classification accuracies
and used Hard Selection to predict the resulting ASR ac-
curacy by selecting between the ASR output from Ea-
syLM and DifficultLM according to the simulated clas-
sifier accuracy. We use the resulting Word Accuracy to
predict ASR performance at that level of EEG classifier
accuracy.
Figure 1 plots predicted ASR WACC against simulated
EEG classification accuracy. As expected, the predicted
ASR accuracy increases as EEG classification accuracy
increases, for both groups (adults and children) and both
levels of difficulty (easy and difficult). However, Figure
1a and 1b shows that WACC was much lower for children
than for adults, especially on difficult utterances, where
even 100% simulated EEG classifier accuracy achieves
barely 20% WACC. One explanation is that on difficult
sentences, children produced reading mistakes and/ or
off-task speech. In contrast, adults read better and stayed
on task. Not only is predicted ASR accuracy higher on
adults’ reading, it improves substantially as simulated
EEG classifier accuracy increases.
</bodyText>
<sectionHeader confidence="0.99973" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985923076923">
This paper shows that classifying EEG signals from an in-
expensive single-channel device can help adapt language
models to significantly improve ASR performance. An
interpolated language model smoothed to compensate for
classification errors yielded the best performance. ASR
performance depended on the accuracy of mental state
classification. Future work includes improving EEG clas-
sification accuracy, detecting other relevant mental states,
such as emotion, and improving ASR by using word-level
EEG classification. A neurologically-informed ASR may
better capture what people intend to communicate, and
augment acoustic input with non-verbal cues to ASR or
dialogue systems.
</bodyText>
<sectionHeader confidence="0.998196" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999683">
This work was supported by the Institute of Education
Sciences, U.S. Department of Education, through Grant
R305A080628 to Carnegie Mellon University. Any opin-
ions, findings, and conclusions or recommendations ex-
pressed in this publication are those of the authors and do
not necessarily reflect the views or official policies, either
expressed or implied of the Institute or the U.S. Depart-
ment of Education. We thank the students, educators, and
LISTENers who helped create our data, and the reviewers
for their helpful comments.
</bodyText>
<sectionHeader confidence="0.999454" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999329875">
Hakkani-T¨nr, D., Tur, G., Heck, L., and Shriberg, E. 2011.
Bootstrapping domain detection using query click logs for
new domains Proceedings of InterSpeech, 709-712.
Jou, S.-C. S. and Schultz, T.. 2008. Ears: Electromyograpical
Automatic Recognition of Speech. Proceedings of Biosig-
nals, 3-12.
Mostow, J., Chang, K.-M., and Nelson, J. 2011. Toward Ex-
ploiting EEG Input in a Reading Tutor. Proceedings of the
15th International Conference on Artificial Intelligence in
Education, 230-237.
NeuroSky 2009. NeuroSky’s SenseTM Meters and Detection of
Mental State: Neurisky, Inc.
Pasley, B. N. and et al. 2012. Reconstructing speech from au-
ditory cortex. PLos Biology, 10(1), 1-13.
Ward, N. G. and Vega, A. 2009. Towards the use of cognitive
states in language modeling. Proceedings ofASRU, 323-326.
</reference>
<page confidence="0.999088">
385
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.921481">
<title confidence="0.997435">Towards Using EEG to Improve ASR Accuracy</title>
<author confidence="0.997139">Yun-Nung Chen</author>
<author confidence="0.997139">Kai-Min Chang</author>
<author confidence="0.997139">Jack</author>
<affiliation confidence="0.9656805">LISTEN School of Computer Science, Carnegie Mellon</affiliation>
<address confidence="0.99965">5000 Forbes Avenue, Pittsburgh, PA 15213-3891,</address>
<abstract confidence="0.9995175">We report on a pilot experiment to improve the performance of an automatic speech recognizer (ASR) by using a single-channel EEG signal to classify the speaker’s mental state as reading easy or hard text. We use a previously published method (Mostow et al., 2011) to train the EEG classifier. We use its probabilistic output to control weighted interpolation of separate language models for easy and difficult reading. The EEG-adapted ASR achieves higher accuracy than two baselines. We analyze how its performance depends on EEG classification accuracy. This pilot result is a step towards improving ASR more generally by using EEG to distinguish mental states.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Hakkani-T¨nr</author>
<author>G Tur</author>
<author>L Heck</author>
<author>E Shriberg</author>
</authors>
<title>Bootstrapping domain detection using query click logs for new domains</title>
<date>2011</date>
<booktitle>Proceedings of InterSpeech,</booktitle>
<pages>709--712</pages>
<marker>Hakkani-T¨nr, Tur, Heck, Shriberg, 2011</marker>
<rawString>Hakkani-T¨nr, D., Tur, G., Heck, L., and Shriberg, E. 2011. Bootstrapping domain detection using query click logs for new domains Proceedings of InterSpeech, 709-712.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-C S Jou</author>
<author>T Schultz</author>
</authors>
<title>Ears: Electromyograpical Automatic Recognition of Speech.</title>
<date>2008</date>
<booktitle>Proceedings of Biosignals,</booktitle>
<pages>3--12</pages>
<marker>Jou, Schultz, 2008</marker>
<rawString>Jou, S.-C. S. and Schultz, T.. 2008. Ears: Electromyograpical Automatic Recognition of Speech. Proceedings of Biosignals, 3-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mostow</author>
<author>K-M Chang</author>
<author>J Nelson</author>
</authors>
<title>Toward Exploiting EEG Input in a Reading Tutor.</title>
<date>2011</date>
<booktitle>Proceedings of the 15th International Conference on Artificial Intelligence in Education,</booktitle>
<pages>230--237</pages>
<contexts>
<context position="4718" citStr="Mostow et al. (2011)" startWordPosition="735" endWordPosition="738">ation for Computational Linguistics: Human Language Technologies, pages 382–385, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics order. The “easy” passages were from texts classified by the Common Core Standards1 at the K-1 level. The “difficult” passages were from practice materials for the Graduate Record Exam2 and the ACE GED test3. Across the reading conditions, passages ranged from 62 to 83 words long. Although instructed to read the text aloud, the readers (especially children) did not always read correctly or follow the displayed sentences. Following Mostow et al. (2011), we trained binary logistic regression classifiers to estimate the probability that an EEG signal is associated with reading an easy (or difficult) sentence. As features for logistic regression we used the streams of values logged by the MindSet: 1. The raw EEG signal, sampled at 512 Hz 2. A filtered version of the raw signal, also sampled at 512 Hz, which is raw signal smoothed over a window of 2 seconds 3. Proprietary “attention” and “meditation” measures, reported at 1 Hz 4. A power spectrum of 1Hz bands from 1-256 Hz, reported at 8 Hz 5. An indicator of signal quality, reported at 1 Hz He</context>
</contexts>
<marker>Mostow, Chang, Nelson, 2011</marker>
<rawString>Mostow, J., Chang, K.-M., and Nelson, J. 2011. Toward Exploiting EEG Input in a Reading Tutor. Proceedings of the 15th International Conference on Artificial Intelligence in Education, 230-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NeuroSky</author>
</authors>
<title>NeuroSky’s SenseTM Meters and Detection of Mental State:</title>
<date>2009</date>
<publisher>Neurisky, Inc.</publisher>
<marker>NeuroSky, 2009</marker>
<rawString>NeuroSky 2009. NeuroSky’s SenseTM Meters and Detection of Mental State: Neurisky, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B N Pasley</author>
</authors>
<title>Reconstructing speech from auditory cortex. PLos Biology,</title>
<date>2012</date>
<volume>10</volume>
<issue>1</issue>
<pages>1--13</pages>
<marker>Pasley, 2012</marker>
<rawString>Pasley, B. N. and et al. 2012. Reconstructing speech from auditory cortex. PLos Biology, 10(1), 1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N G Ward</author>
<author>A Vega</author>
</authors>
<title>Towards the use of cognitive states in language modeling.</title>
<date>2009</date>
<booktitle>Proceedings ofASRU,</booktitle>
<pages>323--326</pages>
<contexts>
<context position="1304" citStr="Ward and Vega, 2009" startWordPosition="198" endWordPosition="201"> difficult reading. The EEG-adapted ASR achieves higher accuracy than two baselines. We analyze how its performance depends on EEG classification accuracy. This pilot result is a step towards improving ASR more generally by using EEG to distinguish mental states. 1 Introduction Humans use speech to communicate what’s on their mind. However, until now, automatic speech recognizers (ASR) and dialogue systems have had no direct way to take into account what is going on in a speaker’s mind. Some work has attempted to infer cognitive states from volume and speaking rate to adapt language modeling (Ward and Vega, 2009) or from query click logs (Hakkani-T¨ur et al., 2011) to detect domains. A new way to address this limitation is to infer mental states from electroencephalogram (EEG) signals. EEG is a voltage signal that can be measured on the surface of the scalp, arising from large areas of coordinated neural activity. This neural activity varies as a function of development, mental state, and cognitive activity, and EEG can measurably detect such variation. Recently, a few companies have scaled back medical grade EEG technology to create portable EEG headsets that are commercially available and simple to </context>
</contexts>
<marker>Ward, Vega, 2009</marker>
<rawString>Ward, N. G. and Vega, A. 2009. Towards the use of cognitive states in language modeling. Proceedings ofASRU, 323-326.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>