<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011269">
<note confidence="0.994159">
Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
Philadelphia, July 2002, pp. 64-73. Association for Computational Linguistics.
</note>
<title confidence="0.988652">
Adaptive Dialogue Systems - Interaction with Interact
</title>
<author confidence="0.981965">
Kristiina Jokinen and Antti Kerminen and Mauri Kaipainen
</author>
<affiliation confidence="0.980063">
Media Lab, University of Art and Design Helsinki
</affiliation>
<address confidence="0.924543">
H¨ameentie 135 C, FIN-00560 Helsinki, Finland
</address>
<email confidence="0.999034">
{kjokinen|akermine|mkaipain}@uiah.fi
</email>
<author confidence="0.996156">
Tommi Jauhiainen and Graham Wilcock
</author>
<affiliation confidence="0.9990375">
Department of General Linguistics, University of Helsinki
FIN-00014 University of Helsinki, Finland
</affiliation>
<email confidence="0.997869">
{tsjauhia|gwilcock}@ling.helsinki.fi
</email>
<author confidence="0.987587">
Markku Turunen and Jaakko Hakulinen
</author>
<affiliation confidence="0.999069">
TAUCHI Unit, University of Tampere
FIN-33014 University of Tampere, Finland
</affiliation>
<email confidence="0.99354">
{mturunen|jh}@cs.uta.fi
</email>
<author confidence="0.985516">
Jukka Kuusisto and Krista Lagus
</author>
<affiliation confidence="0.994267">
Neural Networks Research Centre, Helsinki University of Technology
</affiliation>
<address confidence="0.96433">
P.O.9800 FIN-02015 HUT, Finland
</address>
<email confidence="0.998432">
{krista|jkuusist}@james.hut.fi
</email>
<sectionHeader confidence="0.994879" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999911315789474">
Technological development has made
computer interaction more common
and also commercially feasible, and
the number of interactive systems has
grown rapidly. At the same time, the
systems should be able to adapt to var-
ious situations and various users, so as
to provide the most efficient and help-
ful mode of interaction. The aim of
the Interact project is to explore nat-
ural human-computer interaction and
to develop dialogue models which will
allow users to interact with the com-
puter in a natural and robust way. The
paper describes the innovative goals of
the project and presents ways that the
Interact system supports adaptivity on
different system design and interaction
management levels.
</bodyText>
<sectionHeader confidence="0.997714" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998196977272728">
The need for flexible interaction is apparent not
only in everyday computer use, but also in vari-
ous situations and services where interactive sys-
tems can diminish routine work on the part of
the service provider, and also cater for the users
with fast and tailored access to digital infor-
mation (call centers, help systems, interactive
banking and booking facilities, routing systems,
information retrieval, etc.).
The innovative goal of the Finnish Interact
project is to enable natural language interac-
tion in a wider range of situations than has been
possible so far, and in situations where its use
has not been functional or robust enough. This
means that the systems should support rich in-
teraction and also be able to learn and adapt
their functionality to the changing situation. It
also implies that the needs of special groups will
be taken into account when designing more nat-
ural interactive systems. Within the current sys-
tem, such scenarios can e.g. include an intelli-
gent bus-stop which allows spoken and text in-
teraction concerning city transportation, with a
sign language help facility.
The project addresses especially the problem
of adaptivity: the users are situated in mo-
bile environments in which their needs, activities
and abilities vary. To allow the users to express
their wishes in a way characteristic to them and
the situation, interaction with the system should
take place in a robust and efficient manner, en-
abling rich and flexible communication. Natu-
ral language is thus the preferred mode of in-
teraction, compared to graphical interfaces for
example. Adaptivity also appears in the tech-
niques and methods used in the modelling of
the interaction and the system’s processing ca-
pabilities. An important aspect in this respect
is to combine machine learning techniques with
rule-based natural language processing, to in-
vestigate limitations and advantages of the two
approaches for language technology.
In this paper we focus on adaptivity which
manifests itself in various system properties:
</bodyText>
<listItem confidence="0.99979025">
• agent-based architecture
• natural language capability
• self-organising topic recognition
• conversational ability.
</listItem>
<bodyText confidence="0.999945428571429">
The paper is organized as follows. We first
introduce the dialogue system architecture. We
then explain how the modules function and
address the specific design decisions that con-
tribute to the system’s adaptivity. We conclude
by discussing the system’s capabilities and pro-
viding pointers for future work.
</bodyText>
<sectionHeader confidence="0.969691" genericHeader="method">
2 Agent-based architecture
</sectionHeader>
<bodyText confidence="0.999970833333333">
To allow system development with reusable
modules, flexible application building and easy
combination of different techniques, the frame-
work must itself be designed specifically to sup-
port adaptivity. We argue in favour of a sys-
tem architecture using highly specialized agents,
and use the Jaspis adaptive speech application
framework (Turunen and Hakulinen, 2000; Tu-
runen and Hakulinen, 2001a). Compared to e.g.
Galaxy (Seneff et al., 1998), the system supports
more flexible component communication. The
system is depicted in Figure 1.
</bodyText>
<subsectionHeader confidence="0.92034">
2.1 Information Storage
</subsectionHeader>
<bodyText confidence="0.999846588235294">
The Jaspis architecture contains several features
which support adaptive applications. First of
all, the information about the system state is
kept in a shared knowledge base called Informa-
tion Storage. This blackboard-type information
storage can be accessed by each system compo-
nent via the Information Manager, which allows
them to utilize all the information that the sys-
tem contains, such as dialogue history and user
profiles, directly. Since the important informa-
tion is kept in a shared place, system compo-
nents can be stateless, and the system can switch
between them dynamically. Information Stor-
age thus facilitates the system’s adaptation to
different internal situations, and it also enables
the most suitable component to be chosen to
handle each situation.
</bodyText>
<subsectionHeader confidence="0.992845">
2.2 Flexible Component Management
</subsectionHeader>
<bodyText confidence="0.999992192307692">
The system is organized into modules which
contain three kinds of components: managers,
agents and evaluators. Each module contains
one manager which co-ordinates component in-
teraction inside the module. The present archi-
tecture implements e.g. the Input/Output Man-
ager, the Dialogue Manager and the Presenta-
tion Manager, and they have different priorities
which allow them to react to the interaction flow
differently. The basic principle is that whenever
a manager stops processing, all managers can
react to the situation, and based on their prior-
ities, one of them is selected. There is also the
Interaction Manager which coordinates applica-
tions on the most general level.
The number and type of modules that can be
connected to the system is not limited. The In-
teraction Manager handles all the connections
between modules and the system can be dis-
tributed for multiple computers. In Interact
we have built a demonstration application on
bus-timetable information which runs on several
platforms using different operating systems and
programming languages. This makes the system
highly modular and allows experiments with dif-
ferent approaches from multiple disciplines.
</bodyText>
<subsectionHeader confidence="0.994837">
2.3 Interaction Agents and Evaluators
</subsectionHeader>
<bodyText confidence="0.99898225">
Inside the modules, there are several agents
which handle various interaction situations such
as speech output presentations and dialogue de-
cisions. These interaction agents can be very
</bodyText>
<figureCaption confidence="0.999817">
Figure 1: The system architecture.
</figureCaption>
<bodyText confidence="0.999987382352941">
specialized, e.g. they deal only with speech
recognition errors or outputs related to greet-
ings. They can also be used to model differ-
ent interaction strategies for the same task, e.g.
different dialogue agents can implement alterna-
tive dialogue strategies and control techniques.
Using specialized agents it is possible to con-
struct modular, reusable and extendable inter-
action components that are easy to implement
and maintain. For example, different error han-
dling methods can be included to the system by
constructing new agents which handle errors us-
ing alternative approaches. Similarly, we can
support multilingual outputs by constructing
presentation agents that incorporate language
specific features for each language, while imple-
menting general interaction techniques, such as
error correction methods, to take care of error
situations in speech applications in general (Tu-
runen and Hakulinen, 2001b).
The agents have different capabilities and the
appropriate agent to handle a particular situa-
tion at hand is selected dynamically based on
the context. The choice is done using evalua-
tors which determine applicability of the agents
to various interaction situations. Each evaluator
gives a score for every agent, using a scale be-
tween [0,1]. Zero means that an agent is not
suitable for the situation, one means that an
agent is perfectly suitable for the situation, val-
ues between zero and one indicate the level of
suitability. Scaling functions can be used to em-
phasize certain evaluators over the others The
scores are then multiplied, and the final score, a
suitability factor, is given for every agent. Since
scores are multiplied, an agent which receives
zero from one evaluator is useless for that situ-
ation. It is possible to use different approaches
in the evaluation of the agents, and for instance,
the dialogue evaluators are based on reinforce-
ment learning.
Simple examples of evaluators are for instance
presentation evaluators that select presentation
agents to generate suitable implicit or explicit
confirmations based on the dialogue history and
the system’s knowledge of the user. Another ex-
ample concerns dialogue strategies: the evalua-
tors may give better scores for system-initiative
agents if the dialogue is not proceeding well with
the user-initiative dialogue style, or the evalua-
tors may prefer presentation agents which give
more detailed and helpful information, if the
users seem to have problems in communicating
with the application.
Different evaluators evaluate different aspects
of interaction, and this makes the evaluation
process highly adaptive itself: there is no single
evaluator which makes the final decision. In-
stead, the choice of the appropriate interaction
agent is a combination of different evaluations.
Evaluators have access to all information in the
Information Storage, for example dialogue his-
tory and other contextual information, and it is
also possible to use different approaches in the
evaluation of the agents (such as rule-based and
statistical approaches). Evaluators are the key
concept when considering the whole system and
its adaptation to various interaction situations.
</bodyText>
<subsectionHeader confidence="0.994142">
2.4 Distributed Input and Output
</subsectionHeader>
<bodyText confidence="0.9999899375">
The input/output subsystem is also distributed
which makes it possible to use several input and
output devices for the same purposes. For ex-
ample, we can use several speech recognition
engines, each of which with different capabili-
ties, to adapt the system to the user’s way of
talking. The system architecture contains vir-
tual devices which abstract the actual devices,
such as speech recognizers and speech synthesiz-
ers. From the application developers viewpoint
this makes it easy to experiment with different
modalities, since special agents are used to add
and interpret modality specific features. It is
also used for multilingual inputs and outputs,
although the Interact project focuses on Finnish
speech applications.
</bodyText>
<sectionHeader confidence="0.991451" genericHeader="method">
3 Natural Language Capabilities
</sectionHeader>
<bodyText confidence="0.999983307692308">
The use of Finnish as an interaction language
brings special problems for the system’s nat-
ural language understanding component. The
extreme multiplicity of word forms prevents the
use of all-including dictionaries. For instance,
a Finnish noun can theoretically have around
2200, and a verb around 12000 different forms
(Karlsson, 1983). In spoken language these
numbers are further increased as all the differ-
ent ways to pronounce any given word come into
consideration (Jauhiainen, 2001). Our dialogue
system is designed to understand both written
and spoken input.
</bodyText>
<subsectionHeader confidence="0.990674">
3.1 Written and spoken input
</subsectionHeader>
<bodyText confidence="0.999980576923077">
The different word forms are analyzed using
Fintwol, the two-level morphological analyzer
for Finnish (Koskenniemi, 1983). The forms are
currently input to the syntactic parser CPARSE
(Carlson, 2001). However, the flexible sys-
tem architecture also allows us to experiment
with different morphosyntactic analyzers, such
as TextMorfo (Kielikone Oy 1999) and Conexor
FDG (Conexor Oy 1997-2000), and we plan
to run them in parallel as separate competing
agents to test and compare their applicability
as well as the Jaspis architecture in the given
task.
We use the Lingsoft Speech Recognizer for the
spoken language input. The current state of the
Finnish speech recognizer forces us to limit the
user’s spoken input to rather restricted vocab-
ulary and utterance structure, compared to the
unlimited written input. The system uses full
word lists which include all the morphological
forms that are to be recognized, and a strict
context-free grammar which dictates all the pos-
sible utterance structures. We are currently ex-
ploring possibilities for a HMM-based language
model, with the conditional probabilities deter-
mined by a trigram backoff model.
</bodyText>
<subsectionHeader confidence="0.999907">
3.2 Language analysis
</subsectionHeader>
<bodyText confidence="0.99995225">
The task of the parsing component is to map
the speaker utterances into task-relevant do-
main concepts which are to be processed by
the dialogue manager. The number of domain
concepts concerning the demonstration system’s
application domain, bus-timetables, is rather
small and contains e.g. bus, departure-time
and arrival-location. However, semantically
equivalent utterances can of course vary in the
lexical elements they contain, and in written and
especially in spoken Finnish the word order in
almost any given sentence can also be changed
without major changes on the semantic level un-
derstood by the system (the difference lies in the
information structure of the utterance). For in-
stance, the request How does one get to Malmi?
can be realised as given in Table 1.
There are two ways to approach the problem:
on one hand we can concentrate on finding the
keywords and their relevant word forms, on the
other hand we can use more specialized syntac-
tic analyzers. At the moment we use CPARSE
as the syntactic analyzer for text-based input.
The grammar has been adjusted for the demon-
</bodyText>
<table confidence="0.999777285714286">
Kuinka p¨a¨asee bussilla Malmille?
Miten p¨a¨asee Malmille bussilla?
Kuinka Malmille p¨a¨asee bussilla?
Malmille miten p¨a¨asee bussilla?
Mill¨a bussilla p¨a¨ase Malmille?
Malmille olisin bussia kysellyt.
P¨a¨aseek¨o bussilla Malmille?
</table>
<tableCaption confidence="0.828327666666667">
Table 1: Some alternative utterances for Kuinka
pddsee Malmille bussilla? ’How does-one-get to-
Malmi by bus?
</tableCaption>
<bodyText confidence="0.999857461538462">
stration system so that it especially looks for
phrases relevant to the task at hand. For in-
stance, if we can correctly identify the inflected
word form Malmille from the input string, we
can be quite certain of the user wishing to know
something about getting to Malmi.
The current speech input does not go through
any special morpho-syntactic analysis because
of the strict context-free grammar used by the
speech recognizer. The dictionary used by the
recognizer is tagged with the needed morpholog-
ical information and the context-free rules are
tagged with the needed syntactic information.
</bodyText>
<subsectionHeader confidence="0.994617">
3.3 Language generation
</subsectionHeader>
<bodyText confidence="0.999912326530612">
The language generation function is located in
the system’s Presentation Manager module. Un-
like language analysis, for which different ex-
isting Finnish morphosyntactic analyzers can
be used, there are no readily available general-
purpose Finnish language generators. We are
therefore developing specific generation compo-
nents for this project. The flexible system ar-
chitecture allows us to experiment with different
generators.
Unfortunately the existing Finnish syntactic
analyzers have been designed from the outset as
“parsing grammars”, which are difficult or im-
possible to use for generation. However, the two-
level morphology model (Koskenniemi, 1983) is
in principle bi-directional, and we are work-
ing towards its use in morphological generation.
Fortunately there is also an existing Finnish
speech synthesis project (Vainio, 2001), which
we can use together with the language genera-
tors.
Some of our language generation components
use the XML-based generation framework de-
scribed by Wilcock (2001), which has the ad-
vantage of integrating well with the XML-based
system architecture. The generator starts from
an agenda which is created by the dialogue man-
ager, and is available in the system’s Informa-
tion Storage in XML format. The agenda con-
tains a list of semantic concepts which the dia-
logue manager has tagged as Topic or NewInfo.
From the agenda the generator creates a re-
sponse plan, which passes through the genera-
tion pipeline stages for lexicalization, aggrega-
tion, referring expressions, syntactic and mor-
phological realization. At all stages the response
specification is XML-based, including the final
speech markup language which is passed to the
speech synthesizer.
The system architecture allows multiple gen-
erators to be used. In addition to the XML-
based pipeline components we have some pre-
generated outputs, such as greetings at the start
and end of the dialogue or meta-acts such as
wait-requests and thanking. We are also ex-
ploiting the agent-based architecture to increase
the system’s adaptivity in response generation,
using the level of communicative confidence as
described by Jokinen and Wilcock (2001).
</bodyText>
<sectionHeader confidence="0.917078" genericHeader="method">
4 Recognition of Discussion Topic
</sectionHeader>
<bodyText confidence="0.999923">
One of the important aspects of the system’s
adaptivity is that it can recognize the correct
topic that the user wants to talk about. By
’topic’ we refer to the general subject matter
that a dialogue is about, such as ’bus timetables’
and ’bus tickets’, realized by particular words in
the utterances. In this sense, individual doc-
uments or short conversations may be seen to
have one or a small number of topics, one at a
time.
</bodyText>
<subsectionHeader confidence="0.979191">
4.1 Topically ordered semantic space
</subsectionHeader>
<bodyText confidence="0.9999374">
Collections of short documents, such as news-
paper articles, scientific abstracts and the like,
can be automatically organized onto document
maps utilizing the Self-Organizing Map algo-
rithm (Kohonen, 1995). The document map
methodology has been developed in the WEB-
SOM project (Kohonen et al., 2000), where the
largest map organized consisted of nearly 7 mil-
lion patent abstracts.
We have applied the method to dialogue topic
recognition by carring out experiments on 57
Finnish dialogues, recorded from the customer
service phone line of Helsinki City Transport
and transcribed manually into text. The dia-
logues are first split into topically coherent seg-
ments (utterances or longer segments), and then
organized on a document map. On the ordered
map, each dialogue segment is found in a spe-
cific map location, and topically similar dialogue
segments are found near it. The document map
thus forms a kind of topically ordered semantic
space. A new dialogue segment, either an utter-
ance or a longer history, can likewise be auto-
matically positioned on the map. The coordi-
nates of the best-matching map unit may then
be considered as a latent topical representation
for the dialogue segment.
Furthermore, the map units can be labeled us-
ing named topic classes such as ’timetables’ and
’tickets’. One can then estimate the probability
of a named topic class for a new dialogue seg-
ment by construing a probability model defined
on top of the map. A detailed description of the
experiments as well as results can be found in
(Lagus and Kuusisto, 2002).
</bodyText>
<subsectionHeader confidence="0.959947">
4.2 Topic recognition module
</subsectionHeader>
<bodyText confidence="0.999981916666667">
The topical semantic representation, i.e. the
map coordinates, can be used as input for the
dialogue manager, as one of the values of the
current dialogue state. The system architecture
thus integrates a special topic recognition mod-
ule that outputs the utterance topic in the In-
formation Storage. For a given text segment,
say, the recognition result from the speech rec-
ognizer, the module returns the coordinates of
the best-matching dialogue map unit as well as
the most probable prior topic category (if prior
categorization was used in labeling the map).
</bodyText>
<sectionHeader confidence="0.99109" genericHeader="method">
5 Dialogue Management
</sectionHeader>
<bodyText confidence="0.999935666666667">
The main task of the dialogue manager com-
ponent is to decide on the appropriate way to
react to the user input. The reasoning includes
recognition of communicative intentions behind
the user’s utterances as well as planning of the
system’s next action, whether this is information
retrieval from a database or a question to clarify
an insufficiently specified request. Natural inter-
action with the user also means that the system
should not produce relevant responses only in
terms of correct database facts but also in terms
of rational and cooperative reactions. The sys-
tem could learn suitable interaction strategies
from its interaction with the user, showing adap-
tation to various user habits and situations.
</bodyText>
<subsectionHeader confidence="0.954537">
5.1 Constructive Dialogue Model
</subsectionHeader>
<bodyText confidence="0.999988028571429">
A uniform basis for dialogue management can
be found in the communicative principles re-
lated to human rational and coordinated inter-
action (Allwood et al., 2000; Jokinen, 1996).
The speakers are engaged in a particular activ-
ity, they have a certain role in that activity, and
their actions are constrained by communicative
obligations. They act by exchanging new in-
formation and constructing a shared context in
which to resolve the underlying task satisfacto-
rily.
The model consists of a set of dialogue states,
defined with the help of dialogue acts, obser-
vations of the context, and reinforcement val-
ues. Each action results in a new dialogue
state. The dialogue act, Dact, describes the act
that the speaker performs by a particular utter-
ance, while the topic Top and new information
NewInfo denote the semantic content of the ut-
terance and are related to the task domain. To-
gether these three create a useful first approx-
imation of the utterance meaning by abstract-
ing over possible linguistic realisations. Unfilled
task goals TGoals keep track of the activity re-
lated information still necessary to fulfil the un-
derlying task (a kind of plan), and the speaker
information is needed to link the state to pos-
sible speaker characteristics. The expectations,
Expect are related to communicative obligations,
and used to constrain possible interpretations of
the next act. Consequently, the system’s inter-
nal states can be reduced to a combination of
these categories, all of which form an indepen-
dent source of information for the system to de-
cide on the next move.
</bodyText>
<subsectionHeader confidence="0.999179">
5.2 Dialogue agents and evaluators
</subsectionHeader>
<bodyText confidence="0.999941375">
A dialogue state and all agents that contribute
to a dialogue state are shown in Figure 2. The
Dialogue Model is used to classify the current
utterance into one of the dialogue act categories
(Jokinen et al., 2001), and to predict the next
dialogue acts (Expect). The Topic Model rec-
ognizes the domain, or discussion topic, of the
user input as described above.
and anaphora resolution agents (which are In-
put Agents). Task related goals are produced by
Task Agents, located in a separate Task Man-
ager module. They also access the backend
database, the public transportation timetables
of Helsinki.
The Dialogue Manager (DM) consists of
agents corresponding to possible system actions
(Figure 3). There are also some agents for inter-
nal system interaction, illustrated in the figure
with a stack of agents labeled with Agent1. One
agent is selected at a time, and the architecture
permits us to experiment with various compet-
ing agents for the same subtask: the evaluators
are responsible for choosing the one that best
fits in the particular situation.
</bodyText>
<figureCaption confidence="0.496211">
Figure 2: Dialogue states for user’s utter-
ance and system action, together with dialogue
agents involved in producing various informa-
tion.
</figureCaption>
<bodyText confidence="0.9998471875">
All domains out of the system’s capabili-
ties are handled with the help of a special
OutOfDomain-agent which informs the user of
the relevant tasks and possible topics directly.
This allows the system to deal with error sit-
uations, such as irrelevant user utterances, ef-
ficiently and flexibly without invoking the Dia-
logue Manager to evaluate appropriate dialogue
strategies. The information about error situ-
ations and the selected system action is still
available for dialogue and task goal management
through the shared Information Storage.
The utterance Topic and New Information
(Topic, NewInfo) of the relevant user utter-
ances are given by the parsing unit, and sup-
plemented with discourse knowledge by ellipsis
</bodyText>
<figureCaption confidence="0.998467">
Figure 3: The Dialogue Manager component.
</figureCaption>
<bodyText confidence="0.999875571428571">
Two types of evaluators are responsible for
choosing the agent in DM, and thus implement-
ing the dialogue strategy. The QEstimate eval-
uator chooses the agent that has proven to be
most rewarding so far, according to a Q-learning
(Watkins and Dayan, 1992) algorithm with on-
line E-greedy policy (Sutton and Barto, 1998).
That agent is used in the normal case and the
decision is based on the dialogue state presented
in Figure 2. The underlying structure of the
QEstimate evaluator is illustrated in Figure 4.
The evaluator is based on a table of real val-
ues, indexed by dialogue states, and updated af-
ter each dialogue. The agent with the highest
</bodyText>
<figureCaption confidence="0.999277">
Figure 4: The QEstimate evaluator.
</figureCaption>
<bodyText confidence="0.9999365625">
value for the current dialogue state gets selected.
Adaptivity of the dialogue management comes
from the reinforcement learning algorithm of
this evaluator.
On the other hand, if one of the error evalu-
ators (labeled with Error1..N) detects that an
error has occurred, the QEstimate evaluator is
overridden and a predetermined agent is selected
to handle the error situation (Figure 5). In these
cases, only the the correct agent is given a non-
zero value, forcing the dialogue manager to se-
lect that agent. Examples of such errors include
situations when the user utterance is not recog-
nized by the speech recognizer, its topic is irrele-
vant to the current domain, or its interpretation
is inconsistent with the dialogue context.
</bodyText>
<figureCaption confidence="0.989638">
Figure 5: An error evaluator.
</figureCaption>
<bodyText confidence="0.999747777777778">
Because all possible system actions are
reusable agents, we can easily implement a dif-
ferent dialogue management strategy by adding
evaluators, or replacing the current QEstimate
evaluator. We are developing another strategy
based on recurrent self-organizing maps, that
learns to map dialogue states to correct actions
by fuzzy clustering, minimizing the amount of
human labor in designing the dialogue strategy.
</bodyText>
<sectionHeader confidence="0.808865" genericHeader="method">
6 Demo System and future Work
</sectionHeader>
<bodyText confidence="0.961705045454545">
The project deals with both speech and text
communication, interfacing with the user by
telephone and web-applications. An initial in-
tegrated system with basic functionality and in-
teraction capabilities concerning bus timetable
inquiries has been built. Sample interactions are
given in Figures 6 and 7. Various other appli-
cation types such as intelligent question-answer
systems, automatic call-centers, and other ser-
vices that would benefit from flexible natural
interaction can also be envisaged.
The current system is the first prototype and
intended to provide useful basis for further ex-
perimentation and system development. The
planned activities include further investigations
of the limitations and advantages of various ap-
proaches for language technology, as well as us-
ability studies concerning real users and users
with special needs.
(1) UT: mmill¨as bussilla m¨a p¨a¨asen malamille
’with which bus do I get to Malami’
UR: mill¨a bussilla p¨a¨asen malmille
’with which bus do I get to Malmi’
SU: mist¨a olittekaan l¨ahd¨oss¨a malmille
’where do you want to leave for Malmi’
UT: no hakaniemest¨a
’well from Hakaniemi’
UR: hakaniemest¨a
’from Hakaniemi’
SU: bussi 70 l¨ahtee sielt¨a malmille
’bus 70 leaves from there to Malmi’
UT: selv¨a kiitos hei
’ok thank you bye’
UR: bussi kiitos hei
’bus thank you bye’
SU: kiitos hei
’thank you bye’
Figure 6: Sample dialogue with low recognition
score for the destination Malmille ’to Malmi’
(UT = Utterance as actually made by the user,
UR = Utterance as recognized by the system,
SU = System utterance).
(2) UT: koskas mennee seuraava pussi arapi-
asta malmille
’when will the next bus go from Arapia to
Malmi’
UR: koska menee seuraava bussi arabiasta
malmille
’when will the next bus go from Arabia to
Malmi’
SU: kello 14 30
’at 14:30’
UT: mink¨a numeroinen se on
’which number is it’
UR: min¨a nelonen selv¨a
’I four okay’
SU: olen pahoillani osaan vastata vain
bussiaikatauluja koskeviin kysymyksiin
’sorry I can reply only to questions con-
cerning bus timetables’
UT: aha no joo jaa kiitos hei
’I see ok thank you bye’
UR: onko aijaa kiitos hei
’is I see thank you bye’
SU: kiitos hei
’thank you bye’
</bodyText>
<figureCaption confidence="0.5284936">
Figure 7: Sample dialogue with misrecognition
of user utterance mink¨a numeroinen se on
(UT = Utterance as actually made by the user,
UR = Utterance as recognized by the system,
SU = System utterance).
</figureCaption>
<sectionHeader confidence="0.998367" genericHeader="conclusions">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99993">
The research is carried out in the context of the
USIX Interact project, a collaboration project
between four Finnish universities, and funded
by the National Technology Agency, the leading
IT companies ICL Invia oyj, Sonera oyj, Ling-
soft oy, and Gurusoft oy, as well as the Finnish
Association for the Deaf and the Arla Institute.
</bodyText>
<sectionHeader confidence="0.998964" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999902323529412">
J. Allwood, D. Traum, and K. Jokinen. 2000. Coop-
eration, dialogue and ethics. International Jour-
nal of Human-Computer Studies, 53:871–914.
L. Carlson. 2001. CPARSE manual. http://www
.ling.helsinki.fi/ lcarlson/cparse09en.html.
T. Jauhiainen. 2001. Using existing written lan-
guage analyzers in understanding natural spoken
Finnish. In Proceedings of Nodalida ’01, Uppsala.
K. Jokinen and G. Wilcock. 2001. Confidence-based
adaptivity in response generation for a spoken di-
alogue system. In Proceedings of the 2nd SIGdial
Workshop on Discourse and Dialogue, pages 80–
89, Aarhus.
K. Jokinen, T. Hurtig, K. Hynn¨a, K. Kanto,
M. Kaipainen, and A. Kerminen. 2001. Self-
organizing dialogue management. In Proceedings
of the 2nd Workshop on Natural Language Pro-
cessing and Neural Networks, pages 77–84, Tokyo.
K. Jokinen. 1996. Goal formulation based on com-
municative principles. In Proceedings of the 16th
COLING, pages 598–603.
F. Karlsson. 1983. Suomen kielen ¨a¨anne- ja muoto-
rakenne. WSOY, Juva.
T. Kohonen, S. Kaski, K. Lagus, J. Saloj¨arvi,
V. Paatero, and A. Saarela. 2000. Organization
of a massive document collection. IEEE Transac-
tions on Neural Networks, Special Issue on Neural
Networks for Data Mining and Knowledge Discov-
ery, 11(3):574–585, May.
T. Kohonen. 1995. Self-Organizing Maps. Springer,
Berlin.
K. Koskenniemi. 1983. Two-level morphology: a
general computational model for word-form recog-
nition and production. University of Helsinki,
Helsinki.
K. Lagus and J. Kuusisto. 2002. Topic identifica-
tion in natural language dialogues using neural
networks. In Proceedings of the 3rd SIGdial Work-
shop on Discourse and Dialogue, Philadelphia.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. Galaxy-II: A reference architecture
for conversational system development. In Pro-
ceedings of ICSLP-98, Sydney.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing: An Introduction. MIT Press, Cambridge,
Massachusetts.
M. Turunen and J. Hakulinen. 2000. Jaspis - a
framework for multilingual adaptive speech appli-
cations. In Proceedings of the 6th International
Conference on Spoken Language Processing, Bei-
jing.
M. Turunen and J. Hakulinen. 2001a. Agent-based
adaptive interaction and dialogue management ar-
chitecture for speech applications. In Text, Speech
and Dialogue. Proceedings of the Fourth Interna-
tional Conference (TSD-2001), pages 357–364.
M. Turunen and J. Hakulinen. 2001b. Agent-based
error handling in spoken dialogue systems. In Pro-
ceedings of Eurospeech 2001, pages 2189–2192.
M. Vainio. 2001. Artificial Neural Network Based
Prosody Models for Finnish Text-to-Speech Syn-
thesis. Ph.D. thesis, University of Helsinki.
C. Watkins and P. Dayan. 1992. Technical note:
Q-learning. Machine Learning, 8:279–292.
G. Wilcock. 2001. Pipelines, templates and transfor-
mations: XML for natural language generation. In
Proceedings of the 1st NLP and XML Workshop,
pages 1–8, Tokyo.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.019170">
<note confidence="0.674585">Proceedings of the Third SIGdial Workshop on Discourse and Dialogue, Philadelphia, July 2002, pp. 64-73. Association for Computational Linguistics.</note>
<title confidence="0.940537">Adaptive Dialogue Systems - Interaction with Interact</title>
<author confidence="0.982375">Jokinen Kerminen</author>
<affiliation confidence="0.664644">Media Lab, University of Art and Design</affiliation>
<address confidence="0.614677">H¨ameentie 135 C, FIN-00560 Helsinki,</address>
<email confidence="0.621194">Jauhiainen</email>
<affiliation confidence="0.989241">Department of General Linguistics, University of FIN-00014 University of Helsinki,</affiliation>
<email confidence="0.348523">Turunen</email>
<affiliation confidence="0.9704235">TAUCHI Unit, University of FIN-33014 University of Tampere,</affiliation>
<email confidence="0.634661">Kuusisto</email>
<note confidence="0.4377755">Neural Networks Research Centre, Helsinki University of P.O.9800 FIN-02015 HUT,</note>
<abstract confidence="0.9990659">Technological development has made computer interaction more common and also commercially feasible, and the number of interactive systems has grown rapidly. At the same time, the systems should be able to adapt to various situations and various users, so as to provide the most efficient and helpful mode of interaction. The aim of the Interact project is to explore natural human-computer interaction and to develop dialogue models which will allow users to interact with the computer in a natural and robust way. The paper describes the innovative goals of the project and presents ways that the Interact system supports adaptivity on different system design and interaction management levels.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allwood</author>
<author>D Traum</author>
<author>K Jokinen</author>
</authors>
<title>Cooperation, dialogue and ethics.</title>
<date>2000</date>
<journal>International Journal of Human-Computer Studies,</journal>
<pages>53--871</pages>
<contexts>
<context position="20333" citStr="Allwood et al., 2000" startWordPosition="3124" endWordPosition="3127">eval from a database or a question to clarify an insufficiently specified request. Natural interaction with the user also means that the system should not produce relevant responses only in terms of correct database facts but also in terms of rational and cooperative reactions. The system could learn suitable interaction strategies from its interaction with the user, showing adaptation to various user habits and situations. 5.1 Constructive Dialogue Model A uniform basis for dialogue management can be found in the communicative principles related to human rational and coordinated interaction (Allwood et al., 2000; Jokinen, 1996). The speakers are engaged in a particular activity, they have a certain role in that activity, and their actions are constrained by communicative obligations. They act by exchanging new information and constructing a shared context in which to resolve the underlying task satisfactorily. The model consists of a set of dialogue states, defined with the help of dialogue acts, observations of the context, and reinforcement values. Each action results in a new dialogue state. The dialogue act, Dact, describes the act that the speaker performs by a particular utterance, while the to</context>
</contexts>
<marker>Allwood, Traum, Jokinen, 2000</marker>
<rawString>J. Allwood, D. Traum, and K. Jokinen. 2000. Cooperation, dialogue and ethics. International Journal of Human-Computer Studies, 53:871–914.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
</authors>
<date>2001</date>
<note>CPARSE manual. http://www .ling.helsinki.fi/ lcarlson/cparse09en.html.</note>
<contexts>
<context position="11606" citStr="Carlson, 2001" startWordPosition="1745" endWordPosition="1746">e of all-including dictionaries. For instance, a Finnish noun can theoretically have around 2200, and a verb around 12000 different forms (Karlsson, 1983). In spoken language these numbers are further increased as all the different ways to pronounce any given word come into consideration (Jauhiainen, 2001). Our dialogue system is designed to understand both written and spoken input. 3.1 Written and spoken input The different word forms are analyzed using Fintwol, the two-level morphological analyzer for Finnish (Koskenniemi, 1983). The forms are currently input to the syntactic parser CPARSE (Carlson, 2001). However, the flexible system architecture also allows us to experiment with different morphosyntactic analyzers, such as TextMorfo (Kielikone Oy 1999) and Conexor FDG (Conexor Oy 1997-2000), and we plan to run them in parallel as separate competing agents to test and compare their applicability as well as the Jaspis architecture in the given task. We use the Lingsoft Speech Recognizer for the spoken language input. The current state of the Finnish speech recognizer forces us to limit the user’s spoken input to rather restricted vocabulary and utterance structure, compared to the unlimited wr</context>
</contexts>
<marker>Carlson, 2001</marker>
<rawString>L. Carlson. 2001. CPARSE manual. http://www .ling.helsinki.fi/ lcarlson/cparse09en.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Jauhiainen</author>
</authors>
<title>Using existing written language analyzers in understanding natural spoken Finnish.</title>
<date>2001</date>
<booktitle>In Proceedings of Nodalida ’01,</booktitle>
<location>Uppsala.</location>
<contexts>
<context position="11299" citStr="Jauhiainen, 2001" startWordPosition="1700" endWordPosition="1701">gual inputs and outputs, although the Interact project focuses on Finnish speech applications. 3 Natural Language Capabilities The use of Finnish as an interaction language brings special problems for the system’s natural language understanding component. The extreme multiplicity of word forms prevents the use of all-including dictionaries. For instance, a Finnish noun can theoretically have around 2200, and a verb around 12000 different forms (Karlsson, 1983). In spoken language these numbers are further increased as all the different ways to pronounce any given word come into consideration (Jauhiainen, 2001). Our dialogue system is designed to understand both written and spoken input. 3.1 Written and spoken input The different word forms are analyzed using Fintwol, the two-level morphological analyzer for Finnish (Koskenniemi, 1983). The forms are currently input to the syntactic parser CPARSE (Carlson, 2001). However, the flexible system architecture also allows us to experiment with different morphosyntactic analyzers, such as TextMorfo (Kielikone Oy 1999) and Conexor FDG (Conexor Oy 1997-2000), and we plan to run them in parallel as separate competing agents to test and compare their applicabi</context>
</contexts>
<marker>Jauhiainen, 2001</marker>
<rawString>T. Jauhiainen. 2001. Using existing written language analyzers in understanding natural spoken Finnish. In Proceedings of Nodalida ’01, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jokinen</author>
<author>G Wilcock</author>
</authors>
<title>Confidence-based adaptivity in response generation for a spoken dialogue system.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>80--89</pages>
<location>Aarhus.</location>
<contexts>
<context position="16766" citStr="Jokinen and Wilcock (2001)" startWordPosition="2541" endWordPosition="2544">actic and morphological realization. At all stages the response specification is XML-based, including the final speech markup language which is passed to the speech synthesizer. The system architecture allows multiple generators to be used. In addition to the XMLbased pipeline components we have some pregenerated outputs, such as greetings at the start and end of the dialogue or meta-acts such as wait-requests and thanking. We are also exploiting the agent-based architecture to increase the system’s adaptivity in response generation, using the level of communicative confidence as described by Jokinen and Wilcock (2001). 4 Recognition of Discussion Topic One of the important aspects of the system’s adaptivity is that it can recognize the correct topic that the user wants to talk about. By ’topic’ we refer to the general subject matter that a dialogue is about, such as ’bus timetables’ and ’bus tickets’, realized by particular words in the utterances. In this sense, individual documents or short conversations may be seen to have one or a small number of topics, one at a time. 4.1 Topically ordered semantic space Collections of short documents, such as newspaper articles, scientific abstracts and the like, can</context>
</contexts>
<marker>Jokinen, Wilcock, 2001</marker>
<rawString>K. Jokinen and G. Wilcock. 2001. Confidence-based adaptivity in response generation for a spoken dialogue system. In Proceedings of the 2nd SIGdial Workshop on Discourse and Dialogue, pages 80– 89, Aarhus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jokinen</author>
<author>T Hurtig</author>
<author>K Hynn¨a</author>
<author>K Kanto</author>
<author>M Kaipainen</author>
<author>A Kerminen</author>
</authors>
<title>Selforganizing dialogue management.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Workshop on Natural Language Processing and Neural Networks,</booktitle>
<pages>77--84</pages>
<location>Tokyo.</location>
<marker>Jokinen, Hurtig, Hynn¨a, Kanto, Kaipainen, Kerminen, 2001</marker>
<rawString>K. Jokinen, T. Hurtig, K. Hynn¨a, K. Kanto, M. Kaipainen, and A. Kerminen. 2001. Selforganizing dialogue management. In Proceedings of the 2nd Workshop on Natural Language Processing and Neural Networks, pages 77–84, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jokinen</author>
</authors>
<title>Goal formulation based on communicative principles.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th COLING,</booktitle>
<pages>598--603</pages>
<contexts>
<context position="20349" citStr="Jokinen, 1996" startWordPosition="3128" endWordPosition="3129">r a question to clarify an insufficiently specified request. Natural interaction with the user also means that the system should not produce relevant responses only in terms of correct database facts but also in terms of rational and cooperative reactions. The system could learn suitable interaction strategies from its interaction with the user, showing adaptation to various user habits and situations. 5.1 Constructive Dialogue Model A uniform basis for dialogue management can be found in the communicative principles related to human rational and coordinated interaction (Allwood et al., 2000; Jokinen, 1996). The speakers are engaged in a particular activity, they have a certain role in that activity, and their actions are constrained by communicative obligations. They act by exchanging new information and constructing a shared context in which to resolve the underlying task satisfactorily. The model consists of a set of dialogue states, defined with the help of dialogue acts, observations of the context, and reinforcement values. Each action results in a new dialogue state. The dialogue act, Dact, describes the act that the speaker performs by a particular utterance, while the topic Top and new </context>
</contexts>
<marker>Jokinen, 1996</marker>
<rawString>K. Jokinen. 1996. Goal formulation based on communicative principles. In Proceedings of the 16th COLING, pages 598–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Karlsson</author>
</authors>
<title>Suomen kielen ¨a¨anne- ja muotorakenne.</title>
<date>1983</date>
<publisher>WSOY,</publisher>
<location>Juva.</location>
<contexts>
<context position="11146" citStr="Karlsson, 1983" startWordPosition="1676" endWordPosition="1677">sy to experiment with different modalities, since special agents are used to add and interpret modality specific features. It is also used for multilingual inputs and outputs, although the Interact project focuses on Finnish speech applications. 3 Natural Language Capabilities The use of Finnish as an interaction language brings special problems for the system’s natural language understanding component. The extreme multiplicity of word forms prevents the use of all-including dictionaries. For instance, a Finnish noun can theoretically have around 2200, and a verb around 12000 different forms (Karlsson, 1983). In spoken language these numbers are further increased as all the different ways to pronounce any given word come into consideration (Jauhiainen, 2001). Our dialogue system is designed to understand both written and spoken input. 3.1 Written and spoken input The different word forms are analyzed using Fintwol, the two-level morphological analyzer for Finnish (Koskenniemi, 1983). The forms are currently input to the syntactic parser CPARSE (Carlson, 2001). However, the flexible system architecture also allows us to experiment with different morphosyntactic analyzers, such as TextMorfo (Kielik</context>
</contexts>
<marker>Karlsson, 1983</marker>
<rawString>F. Karlsson. 1983. Suomen kielen ¨a¨anne- ja muotorakenne. WSOY, Juva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kohonen</author>
<author>S Kaski</author>
<author>K Lagus</author>
<author>J Saloj¨arvi</author>
<author>V Paatero</author>
<author>A Saarela</author>
</authors>
<title>Organization of a massive document collection.</title>
<date>2000</date>
<journal>IEEE Transactions on Neural Networks, Special Issue on Neural Networks for Data Mining and Knowledge Discovery,</journal>
<volume>11</volume>
<issue>3</issue>
<marker>Kohonen, Kaski, Lagus, Saloj¨arvi, Paatero, Saarela, 2000</marker>
<rawString>T. Kohonen, S. Kaski, K. Lagus, J. Saloj¨arvi, V. Paatero, and A. Saarela. 2000. Organization of a massive document collection. IEEE Transactions on Neural Networks, Special Issue on Neural Networks for Data Mining and Knowledge Discovery, 11(3):574–585, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kohonen</author>
</authors>
<title>Self-Organizing Maps.</title>
<date>1995</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="17472" citStr="Kohonen, 1995" startWordPosition="2659" endWordPosition="2660">is that it can recognize the correct topic that the user wants to talk about. By ’topic’ we refer to the general subject matter that a dialogue is about, such as ’bus timetables’ and ’bus tickets’, realized by particular words in the utterances. In this sense, individual documents or short conversations may be seen to have one or a small number of topics, one at a time. 4.1 Topically ordered semantic space Collections of short documents, such as newspaper articles, scientific abstracts and the like, can be automatically organized onto document maps utilizing the Self-Organizing Map algorithm (Kohonen, 1995). The document map methodology has been developed in the WEBSOM project (Kohonen et al., 2000), where the largest map organized consisted of nearly 7 million patent abstracts. We have applied the method to dialogue topic recognition by carring out experiments on 57 Finnish dialogues, recorded from the customer service phone line of Helsinki City Transport and transcribed manually into text. The dialogues are first split into topically coherent segments (utterances or longer segments), and then organized on a document map. On the ordered map, each dialogue segment is found in a specific map loc</context>
</contexts>
<marker>Kohonen, 1995</marker>
<rawString>T. Kohonen. 1995. Self-Organizing Maps. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Koskenniemi</author>
</authors>
<title>Two-level morphology: a general computational model for word-form recognition and production.</title>
<date>1983</date>
<institution>University of Helsinki,</institution>
<location>Helsinki.</location>
<contexts>
<context position="11528" citStr="Koskenniemi, 1983" startWordPosition="1733" endWordPosition="1734">ge understanding component. The extreme multiplicity of word forms prevents the use of all-including dictionaries. For instance, a Finnish noun can theoretically have around 2200, and a verb around 12000 different forms (Karlsson, 1983). In spoken language these numbers are further increased as all the different ways to pronounce any given word come into consideration (Jauhiainen, 2001). Our dialogue system is designed to understand both written and spoken input. 3.1 Written and spoken input The different word forms are analyzed using Fintwol, the two-level morphological analyzer for Finnish (Koskenniemi, 1983). The forms are currently input to the syntactic parser CPARSE (Carlson, 2001). However, the flexible system architecture also allows us to experiment with different morphosyntactic analyzers, such as TextMorfo (Kielikone Oy 1999) and Conexor FDG (Conexor Oy 1997-2000), and we plan to run them in parallel as separate competing agents to test and compare their applicability as well as the Jaspis architecture in the given task. We use the Lingsoft Speech Recognizer for the spoken language input. The current state of the Finnish speech recognizer forces us to limit the user’s spoken input to rath</context>
<context position="15285" citStr="Koskenniemi, 1983" startWordPosition="2310" endWordPosition="2311">ocated in the system’s Presentation Manager module. Unlike language analysis, for which different existing Finnish morphosyntactic analyzers can be used, there are no readily available generalpurpose Finnish language generators. We are therefore developing specific generation components for this project. The flexible system architecture allows us to experiment with different generators. Unfortunately the existing Finnish syntactic analyzers have been designed from the outset as “parsing grammars”, which are difficult or impossible to use for generation. However, the twolevel morphology model (Koskenniemi, 1983) is in principle bi-directional, and we are working towards its use in morphological generation. Fortunately there is also an existing Finnish speech synthesis project (Vainio, 2001), which we can use together with the language generators. Some of our language generation components use the XML-based generation framework described by Wilcock (2001), which has the advantage of integrating well with the XML-based system architecture. The generator starts from an agenda which is created by the dialogue manager, and is available in the system’s Information Storage in XML format. The agenda contains</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>K. Koskenniemi. 1983. Two-level morphology: a general computational model for word-form recognition and production. University of Helsinki, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lagus</author>
<author>J Kuusisto</author>
</authors>
<title>Topic identification in natural language dialogues using neural networks.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="18814" citStr="Lagus and Kuusisto, 2002" startWordPosition="2882" endWordPosition="2885">ered semantic space. A new dialogue segment, either an utterance or a longer history, can likewise be automatically positioned on the map. The coordinates of the best-matching map unit may then be considered as a latent topical representation for the dialogue segment. Furthermore, the map units can be labeled using named topic classes such as ’timetables’ and ’tickets’. One can then estimate the probability of a named topic class for a new dialogue segment by construing a probability model defined on top of the map. A detailed description of the experiments as well as results can be found in (Lagus and Kuusisto, 2002). 4.2 Topic recognition module The topical semantic representation, i.e. the map coordinates, can be used as input for the dialogue manager, as one of the values of the current dialogue state. The system architecture thus integrates a special topic recognition module that outputs the utterance topic in the Information Storage. For a given text segment, say, the recognition result from the speech recognizer, the module returns the coordinates of the best-matching dialogue map unit as well as the most probable prior topic category (if prior categorization was used in labeling the map). 5 Dialogu</context>
</contexts>
<marker>Lagus, Kuusisto, 2002</marker>
<rawString>K. Lagus and J. Kuusisto. 2002. Topic identification in natural language dialogues using neural networks. In Proceedings of the 3rd SIGdial Workshop on Discourse and Dialogue, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
<author>E Hurley</author>
<author>R Lau</author>
<author>C Pao</author>
<author>P Schmid</author>
<author>V Zue</author>
</authors>
<title>Galaxy-II: A reference architecture for conversational system development.</title>
<date>1998</date>
<booktitle>In Proceedings of ICSLP-98,</booktitle>
<location>Sydney.</location>
<contexts>
<context position="4502" citStr="Seneff et al., 1998" startWordPosition="665" endWordPosition="668">esign decisions that contribute to the system’s adaptivity. We conclude by discussing the system’s capabilities and providing pointers for future work. 2 Agent-based architecture To allow system development with reusable modules, flexible application building and easy combination of different techniques, the framework must itself be designed specifically to support adaptivity. We argue in favour of a system architecture using highly specialized agents, and use the Jaspis adaptive speech application framework (Turunen and Hakulinen, 2000; Turunen and Hakulinen, 2001a). Compared to e.g. Galaxy (Seneff et al., 1998), the system supports more flexible component communication. The system is depicted in Figure 1. 2.1 Information Storage The Jaspis architecture contains several features which support adaptive applications. First of all, the information about the system state is kept in a shared knowledge base called Information Storage. This blackboard-type information storage can be accessed by each system component via the Information Manager, which allows them to utilize all the information that the system contains, such as dialogue history and user profiles, directly. Since the important information is k</context>
</contexts>
<marker>Seneff, Hurley, Lau, Pao, Schmid, Zue, 1998</marker>
<rawString>S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and V. Zue. 1998. Galaxy-II: A reference architecture for conversational system development. In Proceedings of ICSLP-98, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sutton</author>
<author>A Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="24044" citStr="Sutton and Barto, 1998" startWordPosition="3728" endWordPosition="3731">ll available for dialogue and task goal management through the shared Information Storage. The utterance Topic and New Information (Topic, NewInfo) of the relevant user utterances are given by the parsing unit, and supplemented with discourse knowledge by ellipsis Figure 3: The Dialogue Manager component. Two types of evaluators are responsible for choosing the agent in DM, and thus implementing the dialogue strategy. The QEstimate evaluator chooses the agent that has proven to be most rewarding so far, according to a Q-learning (Watkins and Dayan, 1992) algorithm with online E-greedy policy (Sutton and Barto, 1998). That agent is used in the normal case and the decision is based on the dialogue state presented in Figure 2. The underlying structure of the QEstimate evaluator is illustrated in Figure 4. The evaluator is based on a table of real values, indexed by dialogue states, and updated after each dialogue. The agent with the highest Figure 4: The QEstimate evaluator. value for the current dialogue state gets selected. Adaptivity of the dialogue management comes from the reinforcement learning algorithm of this evaluator. On the other hand, if one of the error evaluators (labeled with Error1..N) dete</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>R. Sutton and A. Barto. 1998. Reinforcement Learning: An Introduction. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Turunen</author>
<author>J Hakulinen</author>
</authors>
<title>Jaspis - a framework for multilingual adaptive speech applications.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th International Conference on Spoken Language Processing,</booktitle>
<location>Beijing.</location>
<contexts>
<context position="4424" citStr="Turunen and Hakulinen, 2000" startWordPosition="652" endWordPosition="655">tem architecture. We then explain how the modules function and address the specific design decisions that contribute to the system’s adaptivity. We conclude by discussing the system’s capabilities and providing pointers for future work. 2 Agent-based architecture To allow system development with reusable modules, flexible application building and easy combination of different techniques, the framework must itself be designed specifically to support adaptivity. We argue in favour of a system architecture using highly specialized agents, and use the Jaspis adaptive speech application framework (Turunen and Hakulinen, 2000; Turunen and Hakulinen, 2001a). Compared to e.g. Galaxy (Seneff et al., 1998), the system supports more flexible component communication. The system is depicted in Figure 1. 2.1 Information Storage The Jaspis architecture contains several features which support adaptive applications. First of all, the information about the system state is kept in a shared knowledge base called Information Storage. This blackboard-type information storage can be accessed by each system component via the Information Manager, which allows them to utilize all the information that the system contains, such as dial</context>
</contexts>
<marker>Turunen, Hakulinen, 2000</marker>
<rawString>M. Turunen and J. Hakulinen. 2000. Jaspis - a framework for multilingual adaptive speech applications. In Proceedings of the 6th International Conference on Spoken Language Processing, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Turunen</author>
<author>J Hakulinen</author>
</authors>
<title>Agent-based adaptive interaction and dialogue management architecture for speech applications.</title>
<date>2001</date>
<booktitle>In Text, Speech and Dialogue. Proceedings of the Fourth International Conference (TSD-2001),</booktitle>
<pages>357--364</pages>
<contexts>
<context position="4453" citStr="Turunen and Hakulinen, 2001" startWordPosition="656" endWordPosition="660">lain how the modules function and address the specific design decisions that contribute to the system’s adaptivity. We conclude by discussing the system’s capabilities and providing pointers for future work. 2 Agent-based architecture To allow system development with reusable modules, flexible application building and easy combination of different techniques, the framework must itself be designed specifically to support adaptivity. We argue in favour of a system architecture using highly specialized agents, and use the Jaspis adaptive speech application framework (Turunen and Hakulinen, 2000; Turunen and Hakulinen, 2001a). Compared to e.g. Galaxy (Seneff et al., 1998), the system supports more flexible component communication. The system is depicted in Figure 1. 2.1 Information Storage The Jaspis architecture contains several features which support adaptive applications. First of all, the information about the system state is kept in a shared knowledge base called Information Storage. This blackboard-type information storage can be accessed by each system component via the Information Manager, which allows them to utilize all the information that the system contains, such as dialogue history and user profile</context>
<context position="7774" citStr="Turunen and Hakulinen, 2001" startWordPosition="1156" endWordPosition="1160">ing specialized agents it is possible to construct modular, reusable and extendable interaction components that are easy to implement and maintain. For example, different error handling methods can be included to the system by constructing new agents which handle errors using alternative approaches. Similarly, we can support multilingual outputs by constructing presentation agents that incorporate language specific features for each language, while implementing general interaction techniques, such as error correction methods, to take care of error situations in speech applications in general (Turunen and Hakulinen, 2001b). The agents have different capabilities and the appropriate agent to handle a particular situation at hand is selected dynamically based on the context. The choice is done using evaluators which determine applicability of the agents to various interaction situations. Each evaluator gives a score for every agent, using a scale between [0,1]. Zero means that an agent is not suitable for the situation, one means that an agent is perfectly suitable for the situation, values between zero and one indicate the level of suitability. Scaling functions can be used to emphasize certain evaluators over</context>
</contexts>
<marker>Turunen, Hakulinen, 2001</marker>
<rawString>M. Turunen and J. Hakulinen. 2001a. Agent-based adaptive interaction and dialogue management architecture for speech applications. In Text, Speech and Dialogue. Proceedings of the Fourth International Conference (TSD-2001), pages 357–364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Turunen</author>
<author>J Hakulinen</author>
</authors>
<title>Agent-based error handling in spoken dialogue systems.</title>
<date>2001</date>
<booktitle>In Proceedings of Eurospeech</booktitle>
<pages>2189--2192</pages>
<contexts>
<context position="4453" citStr="Turunen and Hakulinen, 2001" startWordPosition="656" endWordPosition="660">lain how the modules function and address the specific design decisions that contribute to the system’s adaptivity. We conclude by discussing the system’s capabilities and providing pointers for future work. 2 Agent-based architecture To allow system development with reusable modules, flexible application building and easy combination of different techniques, the framework must itself be designed specifically to support adaptivity. We argue in favour of a system architecture using highly specialized agents, and use the Jaspis adaptive speech application framework (Turunen and Hakulinen, 2000; Turunen and Hakulinen, 2001a). Compared to e.g. Galaxy (Seneff et al., 1998), the system supports more flexible component communication. The system is depicted in Figure 1. 2.1 Information Storage The Jaspis architecture contains several features which support adaptive applications. First of all, the information about the system state is kept in a shared knowledge base called Information Storage. This blackboard-type information storage can be accessed by each system component via the Information Manager, which allows them to utilize all the information that the system contains, such as dialogue history and user profile</context>
<context position="7774" citStr="Turunen and Hakulinen, 2001" startWordPosition="1156" endWordPosition="1160">ing specialized agents it is possible to construct modular, reusable and extendable interaction components that are easy to implement and maintain. For example, different error handling methods can be included to the system by constructing new agents which handle errors using alternative approaches. Similarly, we can support multilingual outputs by constructing presentation agents that incorporate language specific features for each language, while implementing general interaction techniques, such as error correction methods, to take care of error situations in speech applications in general (Turunen and Hakulinen, 2001b). The agents have different capabilities and the appropriate agent to handle a particular situation at hand is selected dynamically based on the context. The choice is done using evaluators which determine applicability of the agents to various interaction situations. Each evaluator gives a score for every agent, using a scale between [0,1]. Zero means that an agent is not suitable for the situation, one means that an agent is perfectly suitable for the situation, values between zero and one indicate the level of suitability. Scaling functions can be used to emphasize certain evaluators over</context>
</contexts>
<marker>Turunen, Hakulinen, 2001</marker>
<rawString>M. Turunen and J. Hakulinen. 2001b. Agent-based error handling in spoken dialogue systems. In Proceedings of Eurospeech 2001, pages 2189–2192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vainio</author>
</authors>
<title>Artificial Neural Network Based Prosody Models for Finnish Text-to-Speech Synthesis.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Helsinki.</institution>
<contexts>
<context position="15467" citStr="Vainio, 2001" startWordPosition="2337" endWordPosition="2338">neralpurpose Finnish language generators. We are therefore developing specific generation components for this project. The flexible system architecture allows us to experiment with different generators. Unfortunately the existing Finnish syntactic analyzers have been designed from the outset as “parsing grammars”, which are difficult or impossible to use for generation. However, the twolevel morphology model (Koskenniemi, 1983) is in principle bi-directional, and we are working towards its use in morphological generation. Fortunately there is also an existing Finnish speech synthesis project (Vainio, 2001), which we can use together with the language generators. Some of our language generation components use the XML-based generation framework described by Wilcock (2001), which has the advantage of integrating well with the XML-based system architecture. The generator starts from an agenda which is created by the dialogue manager, and is available in the system’s Information Storage in XML format. The agenda contains a list of semantic concepts which the dialogue manager has tagged as Topic or NewInfo. From the agenda the generator creates a response plan, which passes through the generation pip</context>
</contexts>
<marker>Vainio, 2001</marker>
<rawString>M. Vainio. 2001. Artificial Neural Network Based Prosody Models for Finnish Text-to-Speech Synthesis. Ph.D. thesis, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Watkins</author>
<author>P Dayan</author>
</authors>
<date>1992</date>
<booktitle>Technical note: Q-learning. Machine Learning,</booktitle>
<pages>8--279</pages>
<contexts>
<context position="23981" citStr="Watkins and Dayan, 1992" startWordPosition="3718" endWordPosition="3721">ion about error situations and the selected system action is still available for dialogue and task goal management through the shared Information Storage. The utterance Topic and New Information (Topic, NewInfo) of the relevant user utterances are given by the parsing unit, and supplemented with discourse knowledge by ellipsis Figure 3: The Dialogue Manager component. Two types of evaluators are responsible for choosing the agent in DM, and thus implementing the dialogue strategy. The QEstimate evaluator chooses the agent that has proven to be most rewarding so far, according to a Q-learning (Watkins and Dayan, 1992) algorithm with online E-greedy policy (Sutton and Barto, 1998). That agent is used in the normal case and the decision is based on the dialogue state presented in Figure 2. The underlying structure of the QEstimate evaluator is illustrated in Figure 4. The evaluator is based on a table of real values, indexed by dialogue states, and updated after each dialogue. The agent with the highest Figure 4: The QEstimate evaluator. value for the current dialogue state gets selected. Adaptivity of the dialogue management comes from the reinforcement learning algorithm of this evaluator. On the other han</context>
</contexts>
<marker>Watkins, Dayan, 1992</marker>
<rawString>C. Watkins and P. Dayan. 1992. Technical note: Q-learning. Machine Learning, 8:279–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Wilcock</author>
</authors>
<title>Pipelines, templates and transformations: XML for natural language generation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 1st NLP and XML Workshop,</booktitle>
<pages>1--8</pages>
<location>Tokyo.</location>
<contexts>
<context position="15634" citStr="Wilcock (2001)" startWordPosition="2363" endWordPosition="2364">xperiment with different generators. Unfortunately the existing Finnish syntactic analyzers have been designed from the outset as “parsing grammars”, which are difficult or impossible to use for generation. However, the twolevel morphology model (Koskenniemi, 1983) is in principle bi-directional, and we are working towards its use in morphological generation. Fortunately there is also an existing Finnish speech synthesis project (Vainio, 2001), which we can use together with the language generators. Some of our language generation components use the XML-based generation framework described by Wilcock (2001), which has the advantage of integrating well with the XML-based system architecture. The generator starts from an agenda which is created by the dialogue manager, and is available in the system’s Information Storage in XML format. The agenda contains a list of semantic concepts which the dialogue manager has tagged as Topic or NewInfo. From the agenda the generator creates a response plan, which passes through the generation pipeline stages for lexicalization, aggregation, referring expressions, syntactic and morphological realization. At all stages the response specification is XML-based, in</context>
</contexts>
<marker>Wilcock, 2001</marker>
<rawString>G. Wilcock. 2001. Pipelines, templates and transformations: XML for natural language generation. In Proceedings of the 1st NLP and XML Workshop, pages 1–8, Tokyo.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>