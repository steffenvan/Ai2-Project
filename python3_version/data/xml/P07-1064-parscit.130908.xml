<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000501">
<title confidence="0.9968845">
Making Sense of Sound:
Unsupervised Topic Segmentation over Acoustic Input
</title>
<author confidence="0.999816">
Igor Malioutov, Alex Park, Regina Barzilay, and James Glass
</author>
<affiliation confidence="0.999367">
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.998508">
{igorm,malex,regina,glass}@csail.mit.edu
</email>
<sectionHeader confidence="0.995633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999724">
We address the task of unsupervised topic
segmentation of speech data operating over
raw acoustic information. In contrast to ex-
isting algorithms for topic segmentation of
speech, our approach does not require in-
put transcripts. Our method predicts topic
changes by analyzing the distribution of re-
occurring acoustic patterns in the speech sig-
nal corresponding to a single speaker. The
algorithm robustly handles noise inherent in
acoustic matching by intelligently aggregat-
ing information about the similarity profile
from multiple local comparisons. Our ex-
periments show that audio-based segmen-
tation compares favorably with transcript-
based segmentation computed over noisy
transcripts. These results demonstrate the
desirability of our method for applications
where a speech recognizer is not available,
or its output has a high word error rate.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999916477272727">
An important practical application of topic segmen-
tation is the analysis of spoken data. Paragraph
breaks, section markers and other structural cues
common in written documents are entirely missing
in spoken data. Insertion of these structural markers
can benefit multiple speech processing applications,
including audio browsing, retrieval, and summariza-
tion.
Not surprisingly, a variety of methods for
topic segmentation have been developed in the
past (Beeferman et al., 1999; Galley et al., 2003;
Dielmann and Renals, 2005). These methods typi-
cally assume that a segmentation algorithm has ac-
cess not only to acoustic input, but also to its tran-
script. This assumption is natural for applications
where the transcript has to be computed as part of the
system output, or it is readily available from other
system components. However, for some domains
and languages, the transcripts may not be available,
or the recognition performance may not be adequate
to achieve reliable segmentation. In order to process
such data, we need a method for topic segmentation
that does not require transcribed input.
In this paper, we explore a method for topic seg-
mentation that operates directly on a raw acoustic
speech signal, without using any input transcripts.
This method predicts topic changes by analyzing the
distribution of reoccurring acoustic patterns in the
speech signal corresponding to a single speaker. In
the same way that unsupervised segmentation algo-
rithms predict boundaries based on changes in lexi-
cal distribution, our algorithm is driven by changes
in the distribution of acoustic patterns. The central
hypothesis here is that similar sounding acoustic se-
quences produced by the same speaker correspond
to similar lexicographic sequences. Thus, by ana-
lyzing the distribution of acoustic patterns we could
approximate a traditional content analysis based on
the lexical distribution of words in a transcript.
Analyzing high-level content structure based on
low-level acoustic features poses interesting compu-
tational and linguistic challenges. For instance, we
need to handle the noise inherent in matching based
on acoustic similarity, because of possible varia-
</bodyText>
<page confidence="0.971032">
504
</page>
<note confidence="0.925526">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 504–511,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999957638888889">
tions in speaking rate or pronunciation. Moreover,
in the absence of higher-level knowledge, informa-
tion about word boundaries is not always discernible
from the raw acoustic input. This causes problems
because we have no obvious unit of comparison. Fi-
nally, noise inherent in the acoustic matching pro-
cedure complicates the detection of distributional
changes in the comparison matrix.
The algorithm presented in this paper demon-
strates the feasibility of topic segmentation over raw
acoustic input corresponding to a single speaker. We
first apply a variant of the dynamic time warping al-
gorithm to find similar fragments in the speech input
through alignment. Next, we construct a compari-
son matrix that aggregates the output of the align-
ment stage. Since aligned utterances are separated
by gaps and differ in duration, this representation
gives rise to sparse and irregular input. To obtain ro-
bust similarity change detection, we invoke a series
of transformations to smooth and refine the compar-
ison matrix. Finally, we apply the minimum-cut seg-
mentation algorithm to the transformed comparison
matrix to detect topic boundaries.
We compare the performance of our method
against traditional transcript-based segmentation al-
gorithms. As expected, the performance of the lat-
ter depends on the accuracy of the input transcript.
When a manual transcription is available, the gap
between audio-based segmentation and transcript-
based segmentation is substantial. However, in
a more realistic scenario when the transcripts are
fraught with recognition errors, the two approaches
exhibit similar performance. These results demon-
strate that audio-based algorithms are an effective
and efficient solution for applications where tran-
scripts are unavailable or highly errorful.
</bodyText>
<sectionHeader confidence="0.999768" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998375946428572">
Speech-based Topic Segmentation A variety of
supervised and unsupervised methods have been
employed to segment speech input. Some of these
algorithms have been originally developed for pro-
cessing written text (Beeferman et al., 1999). Others
are specifically adapted for processing speech input
by adding relevant acoustic features such as pause
length and speaker change (Galley et al., 2003; Diel-
mann and Renals, 2005). In parallel, researchers ex-
tensively study the relationship between discourse
structure and intonational variation (Hirschberg and
Nakatani, 1996; Shriberg et al., 2000). However,
all of the existing segmentation methods require as
input a speech transcript of reasonable quality. In
contrast, the method presented in this paper does
not assume the availability of transcripts, which pre-
vents us from using segmentation algorithms devel-
oped for written text.
At the same time, our work is closely related to
unsupervised approaches for text segmentation. The
central assumption here is that sharp changes in lex-
ical distribution signal the presence of topic bound-
aries (Hearst, 1994; Choi et al., 2001). These ap-
proaches determine segment boundaries by identi-
fying homogeneous regions within a similarity ma-
trix that encodes pairwise similarity between textual
units, such as sentences. Our segmentation algo-
rithm operates over a distortion matrix, but the unit
of comparison is the speech signal over a time in-
terval. This change in representation gives rise to
multiple challenges related to the inherent noise of
acoustic matching, and requires the development of
new methods for signal discretization, interval com-
parison and matrix analysis.
Pattern Induction in Acoustic Data Our work
is related to research on unsupervised lexical acqui-
sition from continuous speech. These methods aim
to infer vocabulary from unsegmented audio streams
by analyzing regularities in pattern distribution (de
Marcken, 1996; Brent, 1999; Venkataraman, 2001).
Traditionally, the speech signal is first converted into
a string-like representation such as phonemes and
syllables using a phonetic recognizer.
Park and Glass (2006) have recently shown the
feasibility of an audio-based approach for word dis-
covery. They induce the vocabulary from the au-
dio stream directly, avoiding the need for phonetic
transcription. Their method can accurately discover
words which appear with high frequency in the au-
dio stream. While the results obtained by Park and
Glass inspire our approach, we cannot directly use
their output as proxies for words in topic segmen-
tation. Many of the content words occurring only
a few times in the text are pruned away by this
method. Our results show that this data that is too
sparse and noisy for robustly discerning changes in
</bodyText>
<page confidence="0.900489">
505
</page>
<bodyText confidence="0.998325854166667">
lexical distribution. alent to word boundary detection, as segmentation
3 Algorithm by silence detection alone only accounts for 20% of
The audio-based segmentation algorithm identifies word boundaries in our corpus.
topic boundaries by analyzing changes in the dis- Next, we convert each utterance into a time se-
tribution of acoustic patterns. The analysis is per- ries of vectors consisting of Mel-scale cepstral co-
formed in three steps. First, we identify recurring efficients (MFCCs). This compact low-dimensional
patterns in the audio stream and compute distortion representation is commonly used in speech process-
between them (Section 3.1). These acoustic patterns ing applications because it approximates human au-
correspond to high-frequency words and phrases, ditory models.
but they only cover a fraction of the words that ap- The process of extracting MFCCs from the speech
pear in the input. As a result, the distributional pro- signal can be summarized as follows. First, the 16
file obtained during this process is too sparse to de- kHz digitized audio waveform is normalized by re-
liver robust topic analysis. Second, we generate an moving the mean and scaling the peak amplitude.
acoustic comparison matrix that aggregates infor- Next, the short-time Fourier transform is taken at
mation from multiple pattern matches (Section 3.2). a frame interval of 10 ms using a 25.6 ms Ham-
Additional matrix transformations during this step ming window. The spectral energy from the Fourier
reduce the noise and irregularities inherent in acous- transform is then weighted by Mel-frequency fil-
tic matching. Third, we partition the matrix to iden- ters (Huang et al., 2001). Finally, the discrete cosine
tify segments with a homogeneous distribution of transform of the log of these Mel-frequency spec-
acoustic patterns (Section 3.3). tral coefficients is computed, yielding a series of 14-
3.1 Comparing Acoustic Patterns dimensional MFCC vectors. We take the additional
Given a raw acoustic waveform, we extract a set of step of whitening the feature vectors, which normal-
acoustic patterns that occur frequently in the speech izes the variance and decorrelates the dimensions of
document. Continuous speech includes many word the feature vectors (Bishop, 1995). This whitened
sequences that lack clear low-level acoustic cues to spectral representation enables us to use the stan-
denote word boundaries. Therefore, we cannot per- dard unweighted Euclidean distance metric. After
form this task through simple counting of speech this transformation, the distances in each dimension
segments separated by silence. Instead, we use a lo- will be uncorrelated and have equal variance.
cal alignment algorithm to search for similar speech Alignment Now, our goal is to identify acoustic
segments and quantify the amount of distortion be- patterns that occur multiple times in the audio wave-
tween them. In what follows, we first present a vec- form. The patterns may not be repeated exactly, but
tor representation used in this computation, and then will most likely reoccur in varied forms. We capture
specify the alignment algorithm that finds similar this information by extracting pairs of patterns with
segments. an associated distortion score. The computation is
MFCC Representation We start by transforming performed using a sequence alignment algorithm.
the acoustic signal into a vector representation that Table 1 shows examples of alignments automati-
facilitates the comparison of acoustic sequences. cally computed by our algorithm. The correspond-
First, we perform silence detection on the original ing phonetic transcriptions&apos; demonstrate that the
waveform by registering a pause if the energy falls matching procedure can robustly handle variations
below a certain threshold for a duration of 2s. This in pronunciations. For example, two instances of the
enables us to break up the acoustic stream into con- word “direction” are matched to one another despite
tinuous spoken utterances. different pronunciations, (“d ay” vs. “d ax” in the
This step is necessary as it eliminates spurious first syllable). At the same time, some aligned pairs
alignments between silent regions of the acoustic form erroneous matches, such as “my prediction”
waveform. Note that silence detection is not equiv- matching “y direction” due to their high acoustic
506
&apos;Phonetic transcriptions are not used by our algorithm and
are provided for illustrative purposes only.
</bodyText>
<table confidence="0.7582692">
Aligned Word(s) Phonetic Transcription
the x direction dh iy eh kcl k s dcl d ax r eh kcl sh ax n
D iy Ek^k s d^d @r Ek^S@n
the y direction dh ax w ay dcl d ay r eh kcl sh epi en
D @w ay d^ay r Ek^k S@n
of my prediction ax v m ay kcl k r iy l iy kcl k sh ax n
@v m ay k^k r iy l iy k^k S@n
acceleration eh kcl k s eh l ax r ey sh epi en
Ek^k s El @r Ey S- n&amp;quot;
acceleration ax kcl k s ah n ax r eh n epi sh epi en
@k^k s 2n @r En - S- n&amp;quot;
the derivation dcl d ih dx ih z dcl dh ey sh epi en
d^d IRIz d^D Ey S- n&amp;quot;
a demonstration uh dcl d eh m ax n epi s tcl t r ey sh en
Ud^d Em @n - s t^t r Ey Sn&amp;quot;
</table>
<tableCaption confidence="0.928983">
Table 1: Aligned Word Paths. Each group of rows
</tableCaption>
<bodyText confidence="0.95658425">
represents audio segments that were aligned to one
another, along with their corresponding phonetic
transcriptions using TIMIT conventions (Garofolo et
al., 1993) and their IPA equivalents.
similarity.
The alignment algorithm operates on the audio
waveform represented by a list of silence-free utter-
ances (u1, u2, ... , un). Each utterance u&apos; is a time
series of MFCC vectors (
two input utterances u&apos; and u&apos;&apos;, the algorithm out-
puts a set of alignments between the corresponding
MFCC vectors. The alignment distortion score is
computed by summing the Euclidean distances of
matching vectors.
To compute the optimal alignment we use a vari-
ant of the dynamic time warping algorithm (Huang
et al., 2001). For every possible starting alignment
point, we optimize the following dynamic program-
ming objective:
{ D(ik − 1,jk)
D(ik, jk − 1)
D(ik − 1, jk − 1)
In the equation above, ik and jk are alignment end-
points in the k-th subproblem of dynamic program-
ming.
This objective corresponds to a descent through
a dynamic programming trellis by choosing right,
down, or diagonal steps at each stage.
During the search process, we consider not only
the alignment distortion score, but also the shape of
the alignment path. To limit the amount of temporal
warping, we enforce the following constraint:
</bodyText>
<equation confidence="0.627007">
(ik − i1) − (jk − j1)I &lt; R, bk, (1)
</equation>
<bodyText confidence="0.978425107142857">
ik &lt; Nx and jk &lt; Ny,
where Nx and Ny are the number of MFCC samples
in each utterance. The value 2R + 1 is the width of
the diagonal band that controls the extent of tempo-
ral warping. The parameter R is tuned on a develop-
ment set.
This alignment procedure may produce paths with
high distortion subpaths. Therefore, we trim each
path to retain the subpath with lowest average dis-
tortion and length at least L. More formally, given
an alignment of length N, we seek to find m and n
such that:
d(ik,jk) n−m &gt; L
We accomplish this by computing the length con-
strained minimum average distortion subsequence
of the path sequence using an O(N log(L)) algo-
rithm proposed by Lin et al (2002). The length
parameter, L, allows us to avoid overtrimming and
control the length of alignments that are found. Af-
ter trimming, the distortion of each alignment path
is normalized by the path length.
Alignments with a distortion exceeding a prespec-
ified threshold are pruned away to ensure that the
aligned phrasal units are close acoustic matches.
This parameter is tuned on a development set.
In the next section, we describe how to aggregate
information from multiple noisy matches into a rep-
resentation that facilitates boundary detection.
</bodyText>
<subsectionHeader confidence="0.971027">
3.2 Construction of Acoustic Comparison
Matrix
</subsectionHeader>
<bodyText confidence="0.999812142857143">
The goal of this step is to construct an acoustic com-
parison matrix that will guide topic segmentation.
This matrix encodes variations in the distribution of
acoustic patterns for a given speech document. We
construct this matrix by first discretizing the acoustic
signal into constant-length blocks and then comput-
ing the distortion between pairs of blocks.
</bodyText>
<figure confidence="0.8903274">
x2,... ,
~x&apos;m). Given
~&apos;
~ &apos;
x 1,
D(ik,jk) = d(ik,jk) + min
arg min 1 n
1&lt;m&lt;n&lt;N k=m
n − m + 1
507
</figure>
<figureCaption confidence="0.999019">
Figure 1: a) Similarity matrix for a Physics lecture constructed using a manual transcript. b) Similarity
</figureCaption>
<bodyText confidence="0.969641813559322">
matrix for the same lecture constructed from acoustic data. The intensity of a pixel indicates the degree of
block similarity. c) Acoustic comparison matrix after 2000 iterations of anisotropic diffusion. Vertical lines
correspond to the reference segmentation.
Unfortunately, the paths and distortions generated
during the alignment step (Section 3.1) cannot be
mapped directly to an acoustic comparison matrix.
Since we compare only commonly repeated acous-
tic patterns, some portions of the signal correspond
to gaps between alignment paths. In fact, in our cor-
pus only 67% of the data is covered by alignment
paths found during the alignment stage. Moreover,
many of these paths are not disjoint. For instance,
our experiments show that 74% of them overlap with
at least one additional alignment path. Finally, these
alignments vary significantly in duration, ranging
from 0.350 ms to 2.7 ms in our corpus.
Discretization and Distortion Computation To
compensate for the irregular distribution of align-
ment paths, we quantize the data by splitting the in-
put signal into uniform contiguous time blocks. A
time block does not necessarily correspond to any
one discovered alignment path. It may contain sev-
eral complete paths and also portions of other paths.
We compute the aggregate distortion score D(x, y)
of two blocks x and y by summing the distortions of
all alignment paths that fall within x and y.
Matrix Smoothing Equipped with a block dis-
tortion measure, we can now construct an acoustic
comparison matrix. In principle, this matrix can be
processed employing standard methods developed
for text segmentation. However, as Figure 1 illus-
trates, the structure of the acoustic matrix is quite
different from the one obtained from text. In a tran-
script similarity matrix shown in Figure 1 a), refer-
ence boundaries delimit homogeneous regions with
high internal similarity. On the other hand, looking
at the acoustic similarity matrix2 shown in Figure 1
b), it is difficult to observe any block structure cor-
responding to the reference segmentation.
This deficiency can be attributed to the sparsity of
acoustic alignments. Consider, for example, the case
when a segment is interspersed with blocks that con-
tain very few or no complete paths. Even though the
rest of the blocks in the segment could be closely
related, these path-free blocks dilute segment homo-
geneity. This is problematic because it is not always
possible to tell whether a sudden shift in scores sig-
nifies a transition or if it is just an artifact of irreg-
ularities in acoustic matching. Without additional
matrix processing, these irregularities will lead the
system astray.
We further refine the acoustic comparison matrix
using anisotropic diffusion. This technique has been
developed for enhancing edge detection accuracy in
image processing (Perona and Malik, 1990), and has
been shown to be an effective smoothing method in
text segmentation (Ji and Zha, 2003). When ap-
plied to a comparison matrix, anisotropic diffusion
reduces score variability within homogeneous re-
</bodyText>
<footnote confidence="0.942670666666667">
2We converted the original comparison distortion matrix to
the similarity matrix by subtracting the component distortions
from the maximum alignment distortion score.
</footnote>
<page confidence="0.995883">
508
</page>
<bodyText confidence="0.99992275">
gions of the matrix and makes edges between these
regions more pronounced. Consequently, this trans-
formation facilitates boundary detection, potentially
increasing segmentation accuracy. In Figure 1 c), we
can observe that the boundary structure in the dif-
fused comparison matrix becomes more salient and
corresponds more closely to the reference segmen-
tation.
</bodyText>
<subsectionHeader confidence="0.998599">
3.3 Matrix Partitioning
</subsectionHeader>
<bodyText confidence="0.999943">
Given a target number of segments k, the goal of
the partitioning step is to divide a matrix into k
square submatrices along the diagonal. This pro-
cess is guided by an optimization function that max-
imizes the homogeneity within a segment or mini-
mizes the homogeneity across segments. This opti-
mization problem can be solved using one of many
unsupervised segmentation approaches (Choi et al.,
2001; Ji and Zha, 2003; Malioutov and Barzilay,
2006).
In our implementation, we employ the minimum-
cut segmentation algorithm (Shi and Malik, 2000;
Malioutov and Barzilay, 2006). In this graph-
theoretic framework, segmentation is cast as a prob-
lem of partitioning a weighted undirected graph
that minimizes the normalized-cut criterion. The
minimum-cut method achieves robust analysis by
jointly considering all possible partitionings of a
document, moving beyond localized decisions. This
allows us to aggregate comparisons from multiple
locations, thereby compensating for the noise of in-
dividual matches.
</bodyText>
<sectionHeader confidence="0.998682" genericHeader="method">
4 Evaluation Set-Up
</sectionHeader>
<bodyText confidence="0.999670090909091">
Data We use a publicly available3 corpus of intro-
ductory Physics lectures described in our previous
work (Malioutov and Barzilay, 2006). This mate-
rial is a particularly appealing application area for an
audio-based segmentation algorithm — many aca-
demic subjects lack transcribed data for training,
while a high ratio of in-domain technical terms lim-
its the use of out-of-domain transcripts. This corpus
is also challenging from the segmentation perspec-
tive because the lectures are long and transitions be-
tween topics are subtle.
</bodyText>
<footnote confidence="0.91816">
3See http://www.csail.mit.edu/˜igorm/
acl06.html
</footnote>
<bodyText confidence="0.999946170212766">
The corpus consists of 33 lectures, with an aver-
age length of 8500 words and an average duration
of 50 minutes. On average, a lecture was anno-
tated with six segments, and a typical segment cor-
responds to two pages of a transcript. Three lectures
from this set were used for development, and 30 lec-
tures were used for testing. The lectures were deliv-
ered by the same speaker.
To evaluate the performance of traditional
transcript-based segmentation algorithms on this
corpus, we also use several types of transcripts at
different levels of recognition accuracy. In addi-
tion to manual transcripts, our corpus contains two
types of automatic transcripts, one obtained using
speaker-dependent (SD) models and the other ob-
tained using speaker-independent (SI) models. The
speaker-independent model was trained on 85 hours
of out-of-domain general lecture material and con-
tained no speech from the speaker in the test set.
The speaker-dependent model was trained by us-
ing 38 hours of audio data from other lectures given
by the speaker. Both recognizers incorporated word
statistics from the accompanying class textbook into
the language model. The word error rates for the
speaker-independent and speaker-dependent models
are 44.9% and 19.4%, respectively.
Evaluation Metrics We use the Pk and WindowD-
iff measures to evaluate our system (Beeferman et
al., 1999; Pevzner and Hearst, 2002). The Pk mea-
sure estimates the probability that a randomly cho-
sen pair of words within a window of length k words
is inconsistently classified. The WindowDiff met-
ric is a variant of the Pk measure, which penalizes
false positives and near misses equally. For both of
these metrics, lower scores indicate better segmen-
tation accuracy.
Baseline We use the state-of-the-art mincut seg-
mentation system by Malioutov and Barzilay (2006)
as our point of comparison. This model is an appro-
priate baseline, because it has been shown to com-
pare favorably with other top-performing segmenta-
tion systems (Choi et al., 2001; Utiyama and Isa-
hara, 2001). We use the publicly available imple-
mentation of the system.
As additional points of comparison, we test the
uniform and random baselines. These correspond
to segmentations obtained by uniformly placing
</bodyText>
<page confidence="0.994235">
509
</page>
<table confidence="0.999550857142857">
Pk WindowDiff
MAN 0.298 0.311
SD 0.340 0.351
AUDIO 0.358 0.370
SI 0.378 0.390
RAND 0.472 0.497
UNI 0.476 0.484
</table>
<tableCaption confidence="0.995625">
Table 2: Segmentation accuracy for audio-based
</tableCaption>
<bodyText confidence="0.991249117647059">
segmentor (AUDIO), random (RAND), uniform
(UNI) and three transcript-based segmentation algo-
rithms that use manual (MAN), speaker-dependent
(SD) and speaker-independent (SI) transcripts. For
all of the algorithms, the target number of segments
is set to the reference number of segments.
boundaries along the span of the lecture and select-
ing random boundaries, respectively.
To control for segmentation granularity, we spec-
ify the number of segments in the reference segmen-
tation for both our system and the baselines.
Parameter Tuning We tuned the number of quan-
tized blocks, the edge cutoff parameter of the min-
imum cut algorithm, and the anisotropic diffusion
parameters on a heldout set of three development
lectures. We used the same development set for the
baseline segmentation systems.
</bodyText>
<sectionHeader confidence="0.999836" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999967166666667">
The goal of our evaluation experiments is two-fold.
First, we are interested in understanding the condi-
tions in which an audio-based segmentation is ad-
vantageous over a transcript-based one. Second, we
aim to analyze the impact of various design deci-
sions on the performance of our algorithm.
</bodyText>
<subsectionHeader confidence="0.58125">
Comparison with Transcript-Based Segmenta-
</subsectionHeader>
<bodyText confidence="0.989141367346939">
tion Table 2 shows the segmentation accuracy
of the audio-based segmentation algorithm and three
transcript-based segmentors on the set of 30 Physics
lectures. Our algorithm yields an average Pk mea-
sure of 0.358 and an average WindowDiff mea-
sure of 0.370. This result is markedly better than
the scores attained by uniform and random seg-
mentations. As expected, the best segmentation re-
sults are obtained using manual transcripts. How-
ever, the gap between audio-based segmentation
and transcript-based segmentation narrows when the
recognition accuracy decreases. In fact, perfor-
mance of the audio-based segmentation beats the
transcript-based segmentation baseline obtained us-
ing speaker-independent (SI) models (0.358 for AU-
DIO versus Pk measurements of 0.378 for SI).
Analysis of Audio-based Segmentation A cen-
tral challenge in audio-based segmentation is how to
overcome the noise inherent in acoustic matching.
We addressed this issue by using anisotropic diffu-
sion to refine the comparison matrix. We can quan-
tify the effects of this smoothing technique by gener-
ating segmentations directly from the similarity ma-
trix. We obtain similarities from the distortions in
the comparison matrix by subtracting the distortion
scores from the maximum distortion:
[D(si, sj)] − D(x, y)
Using this matrix with the min-cut algorithm, seg-
mentation accuracy drops to a Pk measure of 0.418
(0.450 WindowDiff). This difference in perfor-
mance shows that anisotropic diffusion compensates
for noise introduced during acoustic matching.
An alternative solution to the problem of irregu-
larities in audio-based matching is to compute clus-
ters of acoustically similar utterances. Each of the
derived clusters can be thought of as a unique word
type.4 We compute these clusters, employing a
method for unsupervised vocabulary induction de-
veloped by Park and Glass (2006). Using the out-
put of their algorithm, the continuous audio stream
is transformed into a sequence of word-like units,
which in turn can be segmented using any stan-
dard transcript-based segmentation algorithm, such
as the minimum-cut segmentor. On our corpus, this
method achieves disappointing results — a Pk mea-
sure of 0.423 (0.424 WindowDiff). The result can
be attributed to the sparsity of clusters5 generated by
this method, which focuses primarily on discovering
the frequently occurring content words.
</bodyText>
<sectionHeader confidence="0.997616" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9989735">
We presented an unsupervised algorithm for audio-
based topic segmentation. In contrast to existing
</bodyText>
<footnote confidence="0.998844">
4In practice, a cluster can correspond to a phrase, word, or
word fragment (See Table 1 for examples).
5We tuned the number of clusters on the development set.
</footnote>
<equation confidence="0.776383">
5(x, y) = max
si,sj
</equation>
<page confidence="0.969488">
510
</page>
<bodyText confidence="0.999915875">
algorithms for speech segmentation, our approach
does not require an input transcript. Thus, it can
be used in domains where a speech recognizer is
not available or its output is too noisy. Our ap-
proach approximates the distribution of cohesion
ties by considering the distribution of acoustic pat-
terns. Our experimental results demonstrate the util-
ity of this approach: audio-based segmentation com-
pares favorably with transcript-based segmentation
computed over noisy transcripts.
The segmentation algorithm presented in this pa-
per focuses on one source of linguistic information
for discourse analysis — lexical cohesion. Multiple
studies of discourse structure, however, have shown
that prosodic cues are highly predictive of changes
in topic structure (Hirschberg and Nakatani, 1996;
Shriberg et al., 2000). In a supervised framework,
we can further enhance audio-based segmentation
by combining features derived from pattern analy-
sis with prosodic information. We can also explore
an unsupervised fusion of these two sources of in-
formation; for instance, we can induce informative
prosodic cues by using distributional evidence.
Another interesting direction for future research
lies in combining the results of noisy recogni-
tion with information obtained from distribution of
acoustic patterns. We hypothesize that these two
sources provide complementary information about
the audio stream, and therefore can compensate for
each other’s mistakes. This combination can be par-
ticularly fruitful when processing speech documents
with multiple speakers or background noise.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999598818181818">
The authors acknowledge the support of the Microsoft Faculty
Fellowship and the National Science Foundation (CAREER
grant IIS-0448168, grant IIS-0415865, and the NSF Graduate
Fellowship). Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of the au-
thor(s) and do not necessarily reflect the views of the National
Science Foundation. We would like to thank T.J. Hazen for
his assistance with the speech recognizer and to acknowledge
Tara Sainath, Natasha Singh, Ben Snyder, Chao Wang, Luke
Zettlemoyer and the three anonymous reviewers for their valu-
able comments and suggestions.
</bodyText>
<sectionHeader confidence="0.999234" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989490786885246">
C. Bishop, 1995. Neural Networks for Pattern Recognition,
pg. 38. Oxford University Press, New York, 1995.
M. R. Brent. 1999. An efficient, probabilistically sound algo-
rithm for segmentation and word discovery. Machine Learn-
ing, 34(1-3):71–105.
F. Choi, P. Wiemer-Hastings, J. Moore. 2001. Latent semantic
analysis for text segmentation. In Proceedings of EMNLP,
109–117.
C. G. de Marcken. 1996. Unsupervised Language Acquisition.
Ph.D. thesis, Massachusetts Institute of Technology.
A. Dielmann, S. Renals. 2005. Multistream dynamic Bayesian
network for meeting segmentation. In Proceedings Mul-
timodal Interaction and Related Machine Learning Algo-
rithms Workshop (MLMI–04), 76–86.
M. Galley, K. McKeown, E. Fosler-Lussier, H. Jing. 2003.
Discourse segmentation of multi-party conversation. In Pro-
ceedings of the ACL, 562–569.
J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallet,
N. Dahlgren, V. Zue. 1993. TIMIT Acoustic-Phonetic Con-
tinuous Speech Corpus. Linguistic Data Consortium, 1993.
M. Hearst. 1994. Multi-paragraph segmentation of expository
text. In Proceedings of the ACL, 9–16.
J. Hirschberg, C. H. Nakatani. 1996. A prosodic analysis of
discourse segments in direction-giving monologues. In Pro-
ceedings of the ACL, 286–293.
X. Huang, A. Acero, H.-W. Hon. 2001. Spoken Language Pro-
cessing. Prentice Hall.
X. Ji, H. Zha. 2003. Domain-independent text segmentation
using anisotropic diffusion and dynamic programming. In
Proceedings of SIGIR, 322–329.
Y.-L. Lin, T. Jiang, K.-M. Chao. 2002. Efficient algorithms
for locating the length-constrained heaviest segments with
applications to biomolecular sequence analysis. J. Computer
and System Sciences, 65(3):570–586.
I. Malioutov, R. Barzilay. 2006. Minimum cut model for
spoken lecture segmentation. In Proceedings of the COL-
ING/ACL, 25–32.
A. Park, J. R. Glass. 2006. Unsupervised word acquisition from
speech using pattern discovery. In Proceedings ofICASSP.
P. Perona, J. Malik. 1990. Scale-space and edge detection using
anisotropic diffusion. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 12(7):629–639.
L. Pevzner, M. Hearst. 2002. A critique and improvement of
an evaluation metric for text segmentation. Computational
Linguistics, 28(1):19–36.
J. Shi, J. Malik. 2000. Normalized cuts and image segmenta-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 22(8):888–905.
E. Shriberg, A. Stolcke, D. Hakkani-Tur, G. Tur. 2000.
Prosody-based automatic segmentation of speech into sen-
tences and topics. Speech Communication, 32(1-2):127–
154.
M. Utiyama, H. Isahara. 2001. A statistical model for domain-
independent text segmentation. In Proceedings of the ACL,
499–506.
A. Venkataraman. 2001. A statistical model for word dis-
covery in transcribed speech. Computational Linguistics,
27(3):353–372.
D. Beeferman, A. Berger, J. D. Lafferty. 1999. Statistical mod-
els for text segmentation. Machine Learning, 34(1-3):177–
210.
</reference>
<page confidence="0.997124">
511
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.840483">
<title confidence="0.984827">Making Sense of Sound: Unsupervised Topic Segmentation over Acoustic Input</title>
<author confidence="0.875078">Alex Park Malioutov</author>
<author confidence="0.875078">Regina Barzilay</author>
<author confidence="0.875078">Glass</author>
<affiliation confidence="0.997288">Massachusetts Institute of Technology</affiliation>
<abstract confidence="0.998415952380952">We address the task of unsupervised topic segmentation of speech data operating over raw acoustic information. In contrast to existing algorithms for topic segmentation of speech, our approach does not require input transcripts. Our method predicts topic changes by analyzing the distribution of reoccurring acoustic patterns in the speech signal corresponding to a single speaker. The algorithm robustly handles noise inherent in acoustic matching by intelligently aggregating information about the similarity profile from multiple local comparisons. Our experiments show that audio-based segmentation compares favorably with transcriptbased segmentation computed over noisy transcripts. These results demonstrate the desirability of our method for applications where a speech recognizer is not available, or its output has a high word error rate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Bishop</author>
</authors>
<title>Neural Networks for Pattern Recognition,</title>
<date>1995</date>
<pages>38</pages>
<publisher>Oxford University Press,</publisher>
<location>New York,</location>
<contexts>
<context position="10267" citStr="Bishop, 1995" startWordPosition="1545" endWordPosition="1546">ters (Huang et al., 2001). Finally, the discrete cosine tify segments with a homogeneous distribution of transform of the log of these Mel-frequency specacoustic patterns (Section 3.3). tral coefficients is computed, yielding a series of 14- 3.1 Comparing Acoustic Patterns dimensional MFCC vectors. We take the additional Given a raw acoustic waveform, we extract a set of step of whitening the feature vectors, which normalacoustic patterns that occur frequently in the speech izes the variance and decorrelates the dimensions of document. Continuous speech includes many word the feature vectors (Bishop, 1995). This whitened sequences that lack clear low-level acoustic cues to spectral representation enables us to use the standenote word boundaries. Therefore, we cannot per- dard unweighted Euclidean distance metric. After form this task through simple counting of speech this transformation, the distances in each dimension segments separated by silence. Instead, we use a lo- will be uncorrelated and have equal variance. cal alignment algorithm to search for similar speech Alignment Now, our goal is to identify acoustic segments and quantify the amount of distortion be- patterns that occur multiple </context>
</contexts>
<marker>Bishop, 1995</marker>
<rawString>C. Bishop, 1995. Neural Networks for Pattern Recognition, pg. 38. Oxford University Press, New York, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="7185" citStr="Brent, 1999" startWordPosition="1064" endWordPosition="1065">m operates over a distortion matrix, but the unit of comparison is the speech signal over a time interval. This change in representation gives rise to multiple challenges related to the inherent noise of acoustic matching, and requires the development of new methods for signal discretization, interval comparison and matrix analysis. Pattern Induction in Acoustic Data Our work is related to research on unsupervised lexical acquisition from continuous speech. These methods aim to infer vocabulary from unsegmented audio streams by analyzing regularities in pattern distribution (de Marcken, 1996; Brent, 1999; Venkataraman, 2001). Traditionally, the speech signal is first converted into a string-like representation such as phonemes and syllables using a phonetic recognizer. Park and Glass (2006) have recently shown the feasibility of an audio-based approach for word discovery. They induce the vocabulary from the audio stream directly, avoiding the need for phonetic transcription. Their method can accurately discover words which appear with high frequency in the audio stream. While the results obtained by Park and Glass inspire our approach, we cannot directly use their output as proxies for words </context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>M. R. Brent. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34(1-3):71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Choi</author>
<author>P Wiemer-Hastings</author>
<author>J Moore</author>
</authors>
<title>Latent semantic analysis for text segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>109--117</pages>
<contexts>
<context position="6364" citStr="Choi et al., 2001" startWordPosition="940" endWordPosition="943">ure and intonational variation (Hirschberg and Nakatani, 1996; Shriberg et al., 2000). However, all of the existing segmentation methods require as input a speech transcript of reasonable quality. In contrast, the method presented in this paper does not assume the availability of transcripts, which prevents us from using segmentation algorithms developed for written text. At the same time, our work is closely related to unsupervised approaches for text segmentation. The central assumption here is that sharp changes in lexical distribution signal the presence of topic boundaries (Hearst, 1994; Choi et al., 2001). These approaches determine segment boundaries by identifying homogeneous regions within a similarity matrix that encodes pairwise similarity between textual units, such as sentences. Our segmentation algorithm operates over a distortion matrix, but the unit of comparison is the speech signal over a time interval. This change in representation gives rise to multiple challenges related to the inherent noise of acoustic matching, and requires the development of new methods for signal discretization, interval comparison and matrix analysis. Pattern Induction in Acoustic Data Our work is related </context>
<context position="20274" citStr="Choi et al., 2001" startWordPosition="3225" endWordPosition="3228">egmentation accuracy. In Figure 1 c), we can observe that the boundary structure in the diffused comparison matrix becomes more salient and corresponds more closely to the reference segmentation. 3.3 Matrix Partitioning Given a target number of segments k, the goal of the partitioning step is to divide a matrix into k square submatrices along the diagonal. This process is guided by an optimization function that maximizes the homogeneity within a segment or minimizes the homogeneity across segments. This optimization problem can be solved using one of many unsupervised segmentation approaches (Choi et al., 2001; Ji and Zha, 2003; Malioutov and Barzilay, 2006). In our implementation, we employ the minimumcut segmentation algorithm (Shi and Malik, 2000; Malioutov and Barzilay, 2006). In this graphtheoretic framework, segmentation is cast as a problem of partitioning a weighted undirected graph that minimizes the normalized-cut criterion. The minimum-cut method achieves robust analysis by jointly considering all possible partitionings of a document, moving beyond localized decisions. This allows us to aggregate comparisons from multiple locations, thereby compensating for the noise of individual matche</context>
<context position="23469" citStr="Choi et al., 2001" startWordPosition="3719" endWordPosition="3722">, 2002). The Pk measure estimates the probability that a randomly chosen pair of words within a window of length k words is inconsistently classified. The WindowDiff metric is a variant of the Pk measure, which penalizes false positives and near misses equally. For both of these metrics, lower scores indicate better segmentation accuracy. Baseline We use the state-of-the-art mincut segmentation system by Malioutov and Barzilay (2006) as our point of comparison. This model is an appropriate baseline, because it has been shown to compare favorably with other top-performing segmentation systems (Choi et al., 2001; Utiyama and Isahara, 2001). We use the publicly available implementation of the system. As additional points of comparison, we test the uniform and random baselines. These correspond to segmentations obtained by uniformly placing 509 Pk WindowDiff MAN 0.298 0.311 SD 0.340 0.351 AUDIO 0.358 0.370 SI 0.378 0.390 RAND 0.472 0.497 UNI 0.476 0.484 Table 2: Segmentation accuracy for audio-based segmentor (AUDIO), random (RAND), uniform (UNI) and three transcript-based segmentation algorithms that use manual (MAN), speaker-dependent (SD) and speaker-independent (SI) transcripts. For all of the algo</context>
</contexts>
<marker>Choi, Wiemer-Hastings, Moore, 2001</marker>
<rawString>F. Choi, P. Wiemer-Hastings, J. Moore. 2001. Latent semantic analysis for text segmentation. In Proceedings of EMNLP, 109–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C G de Marcken</author>
</authors>
<title>Unsupervised Language Acquisition.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<marker>de Marcken, 1996</marker>
<rawString>C. G. de Marcken. 1996. Unsupervised Language Acquisition. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dielmann</author>
<author>S Renals</author>
</authors>
<title>Multistream dynamic Bayesian network for meeting segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings Multimodal Interaction and Related Machine Learning Algorithms Workshop (MLMI–04),</booktitle>
<pages>76--86</pages>
<contexts>
<context position="1614" citStr="Dielmann and Renals, 2005" startWordPosition="225" endWordPosition="228">ere a speech recognizer is not available, or its output has a high word error rate. 1 Introduction An important practical application of topic segmentation is the analysis of spoken data. Paragraph breaks, section markers and other structural cues common in written documents are entirely missing in spoken data. Insertion of these structural markers can benefit multiple speech processing applications, including audio browsing, retrieval, and summarization. Not surprisingly, a variety of methods for topic segmentation have been developed in the past (Beeferman et al., 1999; Galley et al., 2003; Dielmann and Renals, 2005). These methods typically assume that a segmentation algorithm has access not only to acoustic input, but also to its transcript. This assumption is natural for applications where the transcript has to be computed as part of the system output, or it is readily available from other system components. However, for some domains and languages, the transcripts may not be available, or the recognition performance may not be adequate to achieve reliable segmentation. In order to process such data, we need a method for topic segmentation that does not require transcribed input. In this paper, we explo</context>
<context position="5660" citStr="Dielmann and Renals, 2005" startWordPosition="833" endWordPosition="837">ibit similar performance. These results demonstrate that audio-based algorithms are an effective and efficient solution for applications where transcripts are unavailable or highly errorful. 2 Related Work Speech-based Topic Segmentation A variety of supervised and unsupervised methods have been employed to segment speech input. Some of these algorithms have been originally developed for processing written text (Beeferman et al., 1999). Others are specifically adapted for processing speech input by adding relevant acoustic features such as pause length and speaker change (Galley et al., 2003; Dielmann and Renals, 2005). In parallel, researchers extensively study the relationship between discourse structure and intonational variation (Hirschberg and Nakatani, 1996; Shriberg et al., 2000). However, all of the existing segmentation methods require as input a speech transcript of reasonable quality. In contrast, the method presented in this paper does not assume the availability of transcripts, which prevents us from using segmentation algorithms developed for written text. At the same time, our work is closely related to unsupervised approaches for text segmentation. The central assumption here is that sharp c</context>
</contexts>
<marker>Dielmann, Renals, 2005</marker>
<rawString>A. Dielmann, S. Renals. 2005. Multistream dynamic Bayesian network for meeting segmentation. In Proceedings Multimodal Interaction and Related Machine Learning Algorithms Workshop (MLMI–04), 76–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
<author>E Fosler-Lussier</author>
<author>H Jing</author>
</authors>
<title>Discourse segmentation of multi-party conversation.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>562--569</pages>
<contexts>
<context position="1586" citStr="Galley et al., 2003" startWordPosition="221" endWordPosition="224">d for applications where a speech recognizer is not available, or its output has a high word error rate. 1 Introduction An important practical application of topic segmentation is the analysis of spoken data. Paragraph breaks, section markers and other structural cues common in written documents are entirely missing in spoken data. Insertion of these structural markers can benefit multiple speech processing applications, including audio browsing, retrieval, and summarization. Not surprisingly, a variety of methods for topic segmentation have been developed in the past (Beeferman et al., 1999; Galley et al., 2003; Dielmann and Renals, 2005). These methods typically assume that a segmentation algorithm has access not only to acoustic input, but also to its transcript. This assumption is natural for applications where the transcript has to be computed as part of the system output, or it is readily available from other system components. However, for some domains and languages, the transcripts may not be available, or the recognition performance may not be adequate to achieve reliable segmentation. In order to process such data, we need a method for topic segmentation that does not require transcribed in</context>
<context position="5632" citStr="Galley et al., 2003" startWordPosition="829" endWordPosition="832">he two approaches exhibit similar performance. These results demonstrate that audio-based algorithms are an effective and efficient solution for applications where transcripts are unavailable or highly errorful. 2 Related Work Speech-based Topic Segmentation A variety of supervised and unsupervised methods have been employed to segment speech input. Some of these algorithms have been originally developed for processing written text (Beeferman et al., 1999). Others are specifically adapted for processing speech input by adding relevant acoustic features such as pause length and speaker change (Galley et al., 2003; Dielmann and Renals, 2005). In parallel, researchers extensively study the relationship between discourse structure and intonational variation (Hirschberg and Nakatani, 1996; Shriberg et al., 2000). However, all of the existing segmentation methods require as input a speech transcript of reasonable quality. In contrast, the method presented in this paper does not assume the availability of transcripts, which prevents us from using segmentation algorithms developed for written text. At the same time, our work is closely related to unsupervised approaches for text segmentation. The central ass</context>
</contexts>
<marker>Galley, McKeown, Fosler-Lussier, Jing, 2003</marker>
<rawString>M. Galley, K. McKeown, E. Fosler-Lussier, H. Jing. 2003. Discourse segmentation of multi-party conversation. In Proceedings of the ACL, 562–569.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Garofolo</author>
<author>L Lamel</author>
<author>W Fisher</author>
<author>J Fiscus</author>
<author>D Pallet</author>
</authors>
<marker>Garofolo, Lamel, Fisher, Fiscus, Pallet, </marker>
<rawString>J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallet,</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Dahlgren</author>
<author>V Zue</author>
</authors>
<title>TIMIT Acoustic-Phonetic Continuous Speech Corpus. Linguistic Data Consortium,</title>
<date>1993</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>9--16</pages>
<marker>Dahlgren, Zue, 1993</marker>
<rawString>N. Dahlgren, V. Zue. 1993. TIMIT Acoustic-Phonetic Continuous Speech Corpus. Linguistic Data Consortium, 1993. M. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proceedings of the ACL, 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>C H Nakatani</author>
</authors>
<title>A prosodic analysis of discourse segments in direction-giving monologues.</title>
<date>1996</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>286--293</pages>
<contexts>
<context position="5807" citStr="Hirschberg and Nakatani, 1996" startWordPosition="852" endWordPosition="855">ranscripts are unavailable or highly errorful. 2 Related Work Speech-based Topic Segmentation A variety of supervised and unsupervised methods have been employed to segment speech input. Some of these algorithms have been originally developed for processing written text (Beeferman et al., 1999). Others are specifically adapted for processing speech input by adding relevant acoustic features such as pause length and speaker change (Galley et al., 2003; Dielmann and Renals, 2005). In parallel, researchers extensively study the relationship between discourse structure and intonational variation (Hirschberg and Nakatani, 1996; Shriberg et al., 2000). However, all of the existing segmentation methods require as input a speech transcript of reasonable quality. In contrast, the method presented in this paper does not assume the availability of transcripts, which prevents us from using segmentation algorithms developed for written text. At the same time, our work is closely related to unsupervised approaches for text segmentation. The central assumption here is that sharp changes in lexical distribution signal the presence of topic boundaries (Hearst, 1994; Choi et al., 2001). These approaches determine segment bounda</context>
<context position="28462" citStr="Hirschberg and Nakatani, 1996" startWordPosition="4482" endWordPosition="4485">ble or its output is too noisy. Our approach approximates the distribution of cohesion ties by considering the distribution of acoustic patterns. Our experimental results demonstrate the utility of this approach: audio-based segmentation compares favorably with transcript-based segmentation computed over noisy transcripts. The segmentation algorithm presented in this paper focuses on one source of linguistic information for discourse analysis — lexical cohesion. Multiple studies of discourse structure, however, have shown that prosodic cues are highly predictive of changes in topic structure (Hirschberg and Nakatani, 1996; Shriberg et al., 2000). In a supervised framework, we can further enhance audio-based segmentation by combining features derived from pattern analysis with prosodic information. We can also explore an unsupervised fusion of these two sources of information; for instance, we can induce informative prosodic cues by using distributional evidence. Another interesting direction for future research lies in combining the results of noisy recognition with information obtained from distribution of acoustic patterns. We hypothesize that these two sources provide complementary information about the aud</context>
</contexts>
<marker>Hirschberg, Nakatani, 1996</marker>
<rawString>J. Hirschberg, C. H. Nakatani. 1996. A prosodic analysis of discourse segments in direction-giving monologues. In Proceedings of the ACL, 286–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Huang</author>
<author>A Acero</author>
<author>H-W Hon</author>
</authors>
<title>Spoken Language Processing.</title>
<date>2001</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="9679" citStr="Huang et al., 2001" startWordPosition="1455" endWordPosition="1458">ized audio waveform is normalized by reliver robust topic analysis. Second, we generate an moving the mean and scaling the peak amplitude. acoustic comparison matrix that aggregates infor- Next, the short-time Fourier transform is taken at mation from multiple pattern matches (Section 3.2). a frame interval of 10 ms using a 25.6 ms HamAdditional matrix transformations during this step ming window. The spectral energy from the Fourier reduce the noise and irregularities inherent in acous- transform is then weighted by Mel-frequency filtic matching. Third, we partition the matrix to iden- ters (Huang et al., 2001). Finally, the discrete cosine tify segments with a homogeneous distribution of transform of the log of these Mel-frequency specacoustic patterns (Section 3.3). tral coefficients is computed, yielding a series of 14- 3.1 Comparing Acoustic Patterns dimensional MFCC vectors. We take the additional Given a raw acoustic waveform, we extract a set of step of whitening the feature vectors, which normalacoustic patterns that occur frequently in the speech izes the variance and decorrelates the dimensions of document. Continuous speech includes many word the feature vectors (Bishop, 1995). This white</context>
<context position="13807" citStr="Huang et al., 2001" startWordPosition="2167" endWordPosition="2170">sponding phonetic transcriptions using TIMIT conventions (Garofolo et al., 1993) and their IPA equivalents. similarity. The alignment algorithm operates on the audio waveform represented by a list of silence-free utterances (u1, u2, ... , un). Each utterance u&apos; is a time series of MFCC vectors ( two input utterances u&apos; and u&apos;&apos;, the algorithm outputs a set of alignments between the corresponding MFCC vectors. The alignment distortion score is computed by summing the Euclidean distances of matching vectors. To compute the optimal alignment we use a variant of the dynamic time warping algorithm (Huang et al., 2001). For every possible starting alignment point, we optimize the following dynamic programming objective: { D(ik − 1,jk) D(ik, jk − 1) D(ik − 1, jk − 1) In the equation above, ik and jk are alignment endpoints in the k-th subproblem of dynamic programming. This objective corresponds to a descent through a dynamic programming trellis by choosing right, down, or diagonal steps at each stage. During the search process, we consider not only the alignment distortion score, but also the shape of the alignment path. To limit the amount of temporal warping, we enforce the following constraint: (ik − i1)</context>
</contexts>
<marker>Huang, Acero, Hon, 2001</marker>
<rawString>X. Huang, A. Acero, H.-W. Hon. 2001. Spoken Language Processing. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Ji</author>
<author>H Zha</author>
</authors>
<title>Domain-independent text segmentation using anisotropic diffusion and dynamic programming.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>322--329</pages>
<contexts>
<context position="19213" citStr="Ji and Zha, 2003" startWordPosition="3065" endWordPosition="3068"> these path-free blocks dilute segment homogeneity. This is problematic because it is not always possible to tell whether a sudden shift in scores signifies a transition or if it is just an artifact of irregularities in acoustic matching. Without additional matrix processing, these irregularities will lead the system astray. We further refine the acoustic comparison matrix using anisotropic diffusion. This technique has been developed for enhancing edge detection accuracy in image processing (Perona and Malik, 1990), and has been shown to be an effective smoothing method in text segmentation (Ji and Zha, 2003). When applied to a comparison matrix, anisotropic diffusion reduces score variability within homogeneous re2We converted the original comparison distortion matrix to the similarity matrix by subtracting the component distortions from the maximum alignment distortion score. 508 gions of the matrix and makes edges between these regions more pronounced. Consequently, this transformation facilitates boundary detection, potentially increasing segmentation accuracy. In Figure 1 c), we can observe that the boundary structure in the diffused comparison matrix becomes more salient and corresponds more</context>
</contexts>
<marker>Ji, Zha, 2003</marker>
<rawString>X. Ji, H. Zha. 2003. Domain-independent text segmentation using anisotropic diffusion and dynamic programming. In Proceedings of SIGIR, 322–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-L Lin</author>
<author>T Jiang</author>
<author>K-M Chao</author>
</authors>
<title>Efficient algorithms for locating the length-constrained heaviest segments with applications to biomolecular sequence analysis.</title>
<date>2002</date>
<journal>J. Computer and System Sciences,</journal>
<volume>65</volume>
<issue>3</issue>
<contexts>
<context position="15117" citStr="Lin et al (2002)" startWordPosition="2407" endWordPosition="2410"> in each utterance. The value 2R + 1 is the width of the diagonal band that controls the extent of temporal warping. The parameter R is tuned on a development set. This alignment procedure may produce paths with high distortion subpaths. Therefore, we trim each path to retain the subpath with lowest average distortion and length at least L. More formally, given an alignment of length N, we seek to find m and n such that: d(ik,jk) n−m &gt; L We accomplish this by computing the length constrained minimum average distortion subsequence of the path sequence using an O(N log(L)) algorithm proposed by Lin et al (2002). The length parameter, L, allows us to avoid overtrimming and control the length of alignments that are found. After trimming, the distortion of each alignment path is normalized by the path length. Alignments with a distortion exceeding a prespecified threshold are pruned away to ensure that the aligned phrasal units are close acoustic matches. This parameter is tuned on a development set. In the next section, we describe how to aggregate information from multiple noisy matches into a representation that facilitates boundary detection. 3.2 Construction of Acoustic Comparison Matrix The goal </context>
</contexts>
<marker>Lin, Jiang, Chao, 2002</marker>
<rawString>Y.-L. Lin, T. Jiang, K.-M. Chao. 2002. Efficient algorithms for locating the length-constrained heaviest segments with applications to biomolecular sequence analysis. J. Computer and System Sciences, 65(3):570–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Malioutov</author>
<author>R Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="20323" citStr="Malioutov and Barzilay, 2006" startWordPosition="3233" endWordPosition="3236"> we can observe that the boundary structure in the diffused comparison matrix becomes more salient and corresponds more closely to the reference segmentation. 3.3 Matrix Partitioning Given a target number of segments k, the goal of the partitioning step is to divide a matrix into k square submatrices along the diagonal. This process is guided by an optimization function that maximizes the homogeneity within a segment or minimizes the homogeneity across segments. This optimization problem can be solved using one of many unsupervised segmentation approaches (Choi et al., 2001; Ji and Zha, 2003; Malioutov and Barzilay, 2006). In our implementation, we employ the minimumcut segmentation algorithm (Shi and Malik, 2000; Malioutov and Barzilay, 2006). In this graphtheoretic framework, segmentation is cast as a problem of partitioning a weighted undirected graph that minimizes the normalized-cut criterion. The minimum-cut method achieves robust analysis by jointly considering all possible partitionings of a document, moving beyond localized decisions. This allows us to aggregate comparisons from multiple locations, thereby compensating for the noise of individual matches. 4 Evaluation Set-Up Data We use a publicly ava</context>
<context position="23289" citStr="Malioutov and Barzilay (2006)" startWordPosition="3688" endWordPosition="3691">dent and speaker-dependent models are 44.9% and 19.4%, respectively. Evaluation Metrics We use the Pk and WindowDiff measures to evaluate our system (Beeferman et al., 1999; Pevzner and Hearst, 2002). The Pk measure estimates the probability that a randomly chosen pair of words within a window of length k words is inconsistently classified. The WindowDiff metric is a variant of the Pk measure, which penalizes false positives and near misses equally. For both of these metrics, lower scores indicate better segmentation accuracy. Baseline We use the state-of-the-art mincut segmentation system by Malioutov and Barzilay (2006) as our point of comparison. This model is an appropriate baseline, because it has been shown to compare favorably with other top-performing segmentation systems (Choi et al., 2001; Utiyama and Isahara, 2001). We use the publicly available implementation of the system. As additional points of comparison, we test the uniform and random baselines. These correspond to segmentations obtained by uniformly placing 509 Pk WindowDiff MAN 0.298 0.311 SD 0.340 0.351 AUDIO 0.358 0.370 SI 0.378 0.390 RAND 0.472 0.497 UNI 0.476 0.484 Table 2: Segmentation accuracy for audio-based segmentor (AUDIO), random </context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>I. Malioutov, R. Barzilay. 2006. Minimum cut model for spoken lecture segmentation. In Proceedings of the COLING/ACL, 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Park</author>
<author>J R Glass</author>
</authors>
<title>Unsupervised word acquisition from speech using pattern discovery.</title>
<date>2006</date>
<booktitle>In Proceedings ofICASSP.</booktitle>
<contexts>
<context position="7375" citStr="Park and Glass (2006)" startWordPosition="1088" endWordPosition="1091">o the inherent noise of acoustic matching, and requires the development of new methods for signal discretization, interval comparison and matrix analysis. Pattern Induction in Acoustic Data Our work is related to research on unsupervised lexical acquisition from continuous speech. These methods aim to infer vocabulary from unsegmented audio streams by analyzing regularities in pattern distribution (de Marcken, 1996; Brent, 1999; Venkataraman, 2001). Traditionally, the speech signal is first converted into a string-like representation such as phonemes and syllables using a phonetic recognizer. Park and Glass (2006) have recently shown the feasibility of an audio-based approach for word discovery. They induce the vocabulary from the audio stream directly, avoiding the need for phonetic transcription. Their method can accurately discover words which appear with high frequency in the audio stream. While the results obtained by Park and Glass inspire our approach, we cannot directly use their output as proxies for words in topic segmentation. Many of the content words occurring only a few times in the text are pruned away by this method. Our results show that this data that is too sparse and noisy for robus</context>
<context position="26855" citStr="Park and Glass (2006)" startWordPosition="4236" endWordPosition="4239">rom the maximum distortion: [D(si, sj)] − D(x, y) Using this matrix with the min-cut algorithm, segmentation accuracy drops to a Pk measure of 0.418 (0.450 WindowDiff). This difference in performance shows that anisotropic diffusion compensates for noise introduced during acoustic matching. An alternative solution to the problem of irregularities in audio-based matching is to compute clusters of acoustically similar utterances. Each of the derived clusters can be thought of as a unique word type.4 We compute these clusters, employing a method for unsupervised vocabulary induction developed by Park and Glass (2006). Using the output of their algorithm, the continuous audio stream is transformed into a sequence of word-like units, which in turn can be segmented using any standard transcript-based segmentation algorithm, such as the minimum-cut segmentor. On our corpus, this method achieves disappointing results — a Pk measure of 0.423 (0.424 WindowDiff). The result can be attributed to the sparsity of clusters5 generated by this method, which focuses primarily on discovering the frequently occurring content words. 6 Conclusion and Future Work We presented an unsupervised algorithm for audiobased topic se</context>
</contexts>
<marker>Park, Glass, 2006</marker>
<rawString>A. Park, J. R. Glass. 2006. Unsupervised word acquisition from speech using pattern discovery. In Proceedings ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Perona</author>
<author>J Malik</author>
</authors>
<title>Scale-space and edge detection using anisotropic diffusion.</title>
<date>1990</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>12</volume>
<issue>7</issue>
<contexts>
<context position="19117" citStr="Perona and Malik, 1990" startWordPosition="3048" endWordPosition="3051"> few or no complete paths. Even though the rest of the blocks in the segment could be closely related, these path-free blocks dilute segment homogeneity. This is problematic because it is not always possible to tell whether a sudden shift in scores signifies a transition or if it is just an artifact of irregularities in acoustic matching. Without additional matrix processing, these irregularities will lead the system astray. We further refine the acoustic comparison matrix using anisotropic diffusion. This technique has been developed for enhancing edge detection accuracy in image processing (Perona and Malik, 1990), and has been shown to be an effective smoothing method in text segmentation (Ji and Zha, 2003). When applied to a comparison matrix, anisotropic diffusion reduces score variability within homogeneous re2We converted the original comparison distortion matrix to the similarity matrix by subtracting the component distortions from the maximum alignment distortion score. 508 gions of the matrix and makes edges between these regions more pronounced. Consequently, this transformation facilitates boundary detection, potentially increasing segmentation accuracy. In Figure 1 c), we can observe that th</context>
</contexts>
<marker>Perona, Malik, 1990</marker>
<rawString>P. Perona, J. Malik. 1990. Scale-space and edge detection using anisotropic diffusion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(7):629–639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Pevzner</author>
<author>M Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="22859" citStr="Pevzner and Hearst, 2002" startWordPosition="3618" endWordPosition="3621">dels. The speaker-independent model was trained on 85 hours of out-of-domain general lecture material and contained no speech from the speaker in the test set. The speaker-dependent model was trained by using 38 hours of audio data from other lectures given by the speaker. Both recognizers incorporated word statistics from the accompanying class textbook into the language model. The word error rates for the speaker-independent and speaker-dependent models are 44.9% and 19.4%, respectively. Evaluation Metrics We use the Pk and WindowDiff measures to evaluate our system (Beeferman et al., 1999; Pevzner and Hearst, 2002). The Pk measure estimates the probability that a randomly chosen pair of words within a window of length k words is inconsistently classified. The WindowDiff metric is a variant of the Pk measure, which penalizes false positives and near misses equally. For both of these metrics, lower scores indicate better segmentation accuracy. Baseline We use the state-of-the-art mincut segmentation system by Malioutov and Barzilay (2006) as our point of comparison. This model is an appropriate baseline, because it has been shown to compare favorably with other top-performing segmentation systems (Choi et</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>L. Pevzner, M. Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28(1):19–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shi</author>
<author>J Malik</author>
</authors>
<title>Normalized cuts and image segmentation.</title>
<date>2000</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>22</volume>
<issue>8</issue>
<contexts>
<context position="20416" citStr="Shi and Malik, 2000" startWordPosition="3247" endWordPosition="3250">corresponds more closely to the reference segmentation. 3.3 Matrix Partitioning Given a target number of segments k, the goal of the partitioning step is to divide a matrix into k square submatrices along the diagonal. This process is guided by an optimization function that maximizes the homogeneity within a segment or minimizes the homogeneity across segments. This optimization problem can be solved using one of many unsupervised segmentation approaches (Choi et al., 2001; Ji and Zha, 2003; Malioutov and Barzilay, 2006). In our implementation, we employ the minimumcut segmentation algorithm (Shi and Malik, 2000; Malioutov and Barzilay, 2006). In this graphtheoretic framework, segmentation is cast as a problem of partitioning a weighted undirected graph that minimizes the normalized-cut criterion. The minimum-cut method achieves robust analysis by jointly considering all possible partitionings of a document, moving beyond localized decisions. This allows us to aggregate comparisons from multiple locations, thereby compensating for the noise of individual matches. 4 Evaluation Set-Up Data We use a publicly available3 corpus of introductory Physics lectures described in our previous work (Malioutov and</context>
</contexts>
<marker>Shi, Malik, 2000</marker>
<rawString>J. Shi, J. Malik. 2000. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888–905.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>D Hakkani-Tur</author>
<author>G Tur</author>
</authors>
<title>Prosody-based automatic segmentation of speech into sentences and topics. Speech Communication,</title>
<date>2000</date>
<pages>32--1</pages>
<contexts>
<context position="5831" citStr="Shriberg et al., 2000" startWordPosition="856" endWordPosition="859">ighly errorful. 2 Related Work Speech-based Topic Segmentation A variety of supervised and unsupervised methods have been employed to segment speech input. Some of these algorithms have been originally developed for processing written text (Beeferman et al., 1999). Others are specifically adapted for processing speech input by adding relevant acoustic features such as pause length and speaker change (Galley et al., 2003; Dielmann and Renals, 2005). In parallel, researchers extensively study the relationship between discourse structure and intonational variation (Hirschberg and Nakatani, 1996; Shriberg et al., 2000). However, all of the existing segmentation methods require as input a speech transcript of reasonable quality. In contrast, the method presented in this paper does not assume the availability of transcripts, which prevents us from using segmentation algorithms developed for written text. At the same time, our work is closely related to unsupervised approaches for text segmentation. The central assumption here is that sharp changes in lexical distribution signal the presence of topic boundaries (Hearst, 1994; Choi et al., 2001). These approaches determine segment boundaries by identifying homo</context>
<context position="28486" citStr="Shriberg et al., 2000" startWordPosition="4486" endWordPosition="4489"> Our approach approximates the distribution of cohesion ties by considering the distribution of acoustic patterns. Our experimental results demonstrate the utility of this approach: audio-based segmentation compares favorably with transcript-based segmentation computed over noisy transcripts. The segmentation algorithm presented in this paper focuses on one source of linguistic information for discourse analysis — lexical cohesion. Multiple studies of discourse structure, however, have shown that prosodic cues are highly predictive of changes in topic structure (Hirschberg and Nakatani, 1996; Shriberg et al., 2000). In a supervised framework, we can further enhance audio-based segmentation by combining features derived from pattern analysis with prosodic information. We can also explore an unsupervised fusion of these two sources of information; for instance, we can induce informative prosodic cues by using distributional evidence. Another interesting direction for future research lies in combining the results of noisy recognition with information obtained from distribution of acoustic patterns. We hypothesize that these two sources provide complementary information about the audio stream, and therefore</context>
</contexts>
<marker>Shriberg, Stolcke, Hakkani-Tur, Tur, 2000</marker>
<rawString>E. Shriberg, A. Stolcke, D. Hakkani-Tur, G. Tur. 2000. Prosody-based automatic segmentation of speech into sentences and topics. Speech Communication, 32(1-2):127– 154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Utiyama</author>
<author>H Isahara</author>
</authors>
<title>A statistical model for domainindependent text segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>499--506</pages>
<contexts>
<context position="23497" citStr="Utiyama and Isahara, 2001" startWordPosition="3723" endWordPosition="3727">sure estimates the probability that a randomly chosen pair of words within a window of length k words is inconsistently classified. The WindowDiff metric is a variant of the Pk measure, which penalizes false positives and near misses equally. For both of these metrics, lower scores indicate better segmentation accuracy. Baseline We use the state-of-the-art mincut segmentation system by Malioutov and Barzilay (2006) as our point of comparison. This model is an appropriate baseline, because it has been shown to compare favorably with other top-performing segmentation systems (Choi et al., 2001; Utiyama and Isahara, 2001). We use the publicly available implementation of the system. As additional points of comparison, we test the uniform and random baselines. These correspond to segmentations obtained by uniformly placing 509 Pk WindowDiff MAN 0.298 0.311 SD 0.340 0.351 AUDIO 0.358 0.370 SI 0.378 0.390 RAND 0.472 0.497 UNI 0.476 0.484 Table 2: Segmentation accuracy for audio-based segmentor (AUDIO), random (RAND), uniform (UNI) and three transcript-based segmentation algorithms that use manual (MAN), speaker-dependent (SD) and speaker-independent (SI) transcripts. For all of the algorithms, the target number of</context>
</contexts>
<marker>Utiyama, Isahara, 2001</marker>
<rawString>M. Utiyama, H. Isahara. 2001. A statistical model for domainindependent text segmentation. In Proceedings of the ACL, 499–506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Venkataraman</author>
</authors>
<title>A statistical model for word discovery in transcribed speech.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="7206" citStr="Venkataraman, 2001" startWordPosition="1066" endWordPosition="1067">er a distortion matrix, but the unit of comparison is the speech signal over a time interval. This change in representation gives rise to multiple challenges related to the inherent noise of acoustic matching, and requires the development of new methods for signal discretization, interval comparison and matrix analysis. Pattern Induction in Acoustic Data Our work is related to research on unsupervised lexical acquisition from continuous speech. These methods aim to infer vocabulary from unsegmented audio streams by analyzing regularities in pattern distribution (de Marcken, 1996; Brent, 1999; Venkataraman, 2001). Traditionally, the speech signal is first converted into a string-like representation such as phonemes and syllables using a phonetic recognizer. Park and Glass (2006) have recently shown the feasibility of an audio-based approach for word discovery. They induce the vocabulary from the audio stream directly, avoiding the need for phonetic transcription. Their method can accurately discover words which appear with high frequency in the audio stream. While the results obtained by Park and Glass inspire our approach, we cannot directly use their output as proxies for words in topic segmentation</context>
</contexts>
<marker>Venkataraman, 2001</marker>
<rawString>A. Venkataraman. 2001. A statistical model for word discovery in transcribed speech. Computational Linguistics, 27(3):353–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J D Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1565" citStr="Beeferman et al., 1999" startWordPosition="217" endWordPosition="220">esirability of our method for applications where a speech recognizer is not available, or its output has a high word error rate. 1 Introduction An important practical application of topic segmentation is the analysis of spoken data. Paragraph breaks, section markers and other structural cues common in written documents are entirely missing in spoken data. Insertion of these structural markers can benefit multiple speech processing applications, including audio browsing, retrieval, and summarization. Not surprisingly, a variety of methods for topic segmentation have been developed in the past (Beeferman et al., 1999; Galley et al., 2003; Dielmann and Renals, 2005). These methods typically assume that a segmentation algorithm has access not only to acoustic input, but also to its transcript. This assumption is natural for applications where the transcript has to be computed as part of the system output, or it is readily available from other system components. However, for some domains and languages, the transcripts may not be available, or the recognition performance may not be adequate to achieve reliable segmentation. In order to process such data, we need a method for topic segmentation that does not r</context>
<context position="5473" citStr="Beeferman et al., 1999" startWordPosition="805" endWordPosition="808">sed segmentation and transcriptbased segmentation is substantial. However, in a more realistic scenario when the transcripts are fraught with recognition errors, the two approaches exhibit similar performance. These results demonstrate that audio-based algorithms are an effective and efficient solution for applications where transcripts are unavailable or highly errorful. 2 Related Work Speech-based Topic Segmentation A variety of supervised and unsupervised methods have been employed to segment speech input. Some of these algorithms have been originally developed for processing written text (Beeferman et al., 1999). Others are specifically adapted for processing speech input by adding relevant acoustic features such as pause length and speaker change (Galley et al., 2003; Dielmann and Renals, 2005). In parallel, researchers extensively study the relationship between discourse structure and intonational variation (Hirschberg and Nakatani, 1996; Shriberg et al., 2000). However, all of the existing segmentation methods require as input a speech transcript of reasonable quality. In contrast, the method presented in this paper does not assume the availability of transcripts, which prevents us from using segm</context>
<context position="22832" citStr="Beeferman et al., 1999" startWordPosition="3614" endWordPosition="3617">aker-independent (SI) models. The speaker-independent model was trained on 85 hours of out-of-domain general lecture material and contained no speech from the speaker in the test set. The speaker-dependent model was trained by using 38 hours of audio data from other lectures given by the speaker. Both recognizers incorporated word statistics from the accompanying class textbook into the language model. The word error rates for the speaker-independent and speaker-dependent models are 44.9% and 19.4%, respectively. Evaluation Metrics We use the Pk and WindowDiff measures to evaluate our system (Beeferman et al., 1999; Pevzner and Hearst, 2002). The Pk measure estimates the probability that a randomly chosen pair of words within a window of length k words is inconsistently classified. The WindowDiff metric is a variant of the Pk measure, which penalizes false positives and near misses equally. For both of these metrics, lower scores indicate better segmentation accuracy. Baseline We use the state-of-the-art mincut segmentation system by Malioutov and Barzilay (2006) as our point of comparison. This model is an appropriate baseline, because it has been shown to compare favorably with other top-performing se</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>D. Beeferman, A. Berger, J. D. Lafferty. 1999. Statistical models for text segmentation. Machine Learning, 34(1-3):177– 210.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>