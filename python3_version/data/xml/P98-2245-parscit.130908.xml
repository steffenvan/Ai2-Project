<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.072605">
<title confidence="0.987639">
Bridging the Gap between Dictionary and Thesaurus
</title>
<author confidence="0.994178">
Oi Yee Kwong
</author>
<affiliation confidence="0.999256">
Computer Laboratory, University of Cambridge
</affiliation>
<address confidence="0.567757">
New Museums Site, Cambridge CB2 3QG, U.K.
</address>
<email confidence="0.981657">
oyk20@el.cam.ae.uk
</email>
<sectionHeader confidence="0.993494" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999585">
This paper presents an algorithm to integrate dif-
ferent lexical resources, through which we hope to
overcome the individual inadequacy of the resources,
and thus obtain some enriched lexical semantic in-
formation for applications such as word sense disam-
biguation. We used WordNet as a mediator between
a conventional dictionary and a thesaurus. Prelimi-
nary results support our hypothesised structural re-
lationship, which enables the integration, of the re-
sources. These results also suggest that we can com-
bine the resources to achieve an overall balanced de-
gree of sense discrimination.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906777777778">
It is generally accepted that applications such as
word sense disambiguation (WSD), machine trans-
lation (MT) and information retrieval (IR), require
a wide range of resources to supply the necessary
lexical semantic information. For instance, Cal-
zolari (1988) proposed a lexical database in Italian
which has the features of both a dictionary and a
thesaurus; and Klavans and Tzoukermann (1995)
tried to build a fuller bilingual lexicon by enhancing
machine-readable dictionaries with large corpora.
Among the attempts to enrich lexical information,
many have been directed to the analysis of dictio-
nary definitions and the transformation of the im-
plicit information to explicit knowledge bases for
computational purposes (Amsler, 1981; Calzolari,
1984; Chodorow et al., 1985; Markowitz et al.,
1986; Klavans et al., 1990; Vossen and Copestake,
1993). Nonetheless, dictionaries are also infamous
of their non-standardised sense granularity, and the
taxonomies obtained from definitions are inevitably
ad hoc. It would therefore be a good idea if we can
unify our lexical semantic knowledge by some exist-
ing, and widely exploited, classifications such as the
system in Roget&apos;s Thesaurus (Roget, 1852), which
has remained intact for years and has been used in
WSD (Yarowsky, 1992).
While the objective is to integrate different lex-
ical resources, the problem is: how do we recon-
cile the rich but variable information in dictionary
senses with the cruder but more stable taxonomies
like those in thesauri?
This work is intended to fill this gap. We use
WordNet as a mediator in the process. In the fol-
lowing, we will outline an algorithm to map word
senses in a dictionary to semantic classes in some
established classification scheme.
</bodyText>
<sectionHeader confidence="0.696659" genericHeader="method">
2 Inter-relatedness of the Resources
</sectionHeader>
<bodyText confidence="0.9998578">
The three lexical resources used in this work are the
1987 revision of Roget&apos;s Thesaurus (ROGET) (Kirk-
patrick, 1987), the Longman Dictionary of Contem-
porary English (LDOCE) (Procter, 1978) and Word-
Net 1.5 (WN) (Miller et al., 1993). Figure 1 shows
how word senses are organised in them. As we have
mentioned, instead of directly mapping an LDOCE
definition to a ROGET class, we bridge the gap with
WN, as indicated by the arrows in the figure. Such
a route is made feasible by linking the structures in
common among the resources.
Words are organised in alphabetical order in
LDOCE, as in other conventional dictionaries. The
senses are listed after each entry, in the form of text
definitions. WN groups words into sets of synonyms
(&amp;quot;synsets&amp;quot;), with an optional textual gloss. These
synsets form the nodes of a taxonomic hierarchy.
In ROGET, each semantic class comes with a num-
ber, under which words are first assorted by part of
speech and then grouped into paragraphs according
to the conveyed idea.
Let us refer to Figure 1 and start from word x2 in
WN synset X. Since words expressing every aspect
of an idea are grouped together in ROGET, we can
therefore expect to find not only words in synset X,
but also those in the coordinate WN synsets (i.e. M
and P, with words ml, m2, 731, p2, etc.) and the su-
perordinate WN synsets (i.e. C and A, with words
c1, c2, etc.) in the same ROGET paragraph. In
other words, the thesaurus class to which x2 belongs
should include roughly XUMUPUCU A. Mean-
while, the LDOCE definition corresponding to the
sense of synset X (denoted by Dx) is expected to be
similar to the textual gloss of synset X (denoted by
G/(X)). In addition, given that it is not unusual for
</bodyText>
<page confidence="0.943842">
1487
</page>
<figure confidence="0.551711066666667">
A
120. N. cl, c2.... (in C);
m1, m2, (in hi); pl. p2,
... (in P); xl, x2, ... (in X)
2
X 3
f p2, ..1. 01(P) Isl. x2... I, G1(X)
121. N.... x3
RT
(ROGET) (WN) (LDOCE)
EAF
(ml. m2.... LOW)
X2 definition (Dx) similiar to 01(X)
cl, c2, ). MC) or defined in terms of words in
X or C. etc.
</figure>
<figureCaption confidence="0.999901">
Figure 1: Organisation of word senses in different resources
</figureCaption>
<bodyText confidence="0.9795564">
dictionary definitions to be phrased with synonyms
or superordinate terms, we would also expect to find
words from X and C, or even A, in the LDOCE def-
inition. That means we believe D r Gl(X) and
Dr n (x u C u 0 0.
</bodyText>
<sectionHeader confidence="0.997508" genericHeader="method">
3 The Algorithm
</sectionHeader>
<bodyText confidence="0.989231325581395">
The possibility of using statistical methods to assign
ROGET category labels to dictionary definitions has
been suggested by Yarowsky (1992). Our algorithm
offers a systematic way of linking existing resources
by defining a mapping chain from LDOCE to RO-
GET through WN. It is based on shallow process-
ing within the resources themselves, exploiting their
inter-relatedness, and does not rely on extensive sta-
tistical data. It therefore has an advantage of being
immune to any change of sense discrimination with
time, since it only depends on the organisation but
not the individual entries of the resources. Given a
word with part of speech, W(p), the core steps are
as follows:
Step 1: From LDOCE, get the sense definitions
Di, ..., Dt under the entry W(p).
Step 2: From WN, find all the synsets
Sn {wi , w2, ...} such that W(p) E S. Also
collect the corresponding gloss definitions,
Gl(S), if any, the hypernym synsets Hyp(Sn),
and the coordinate synsets Co(Sn).
Step 3: Compute a similarity score matrix A for
the LDOCE senses and the WN synsets. A
similarity score A(i , j) is computed for the ith
LDOCE sense and the jth WN synset using
a weighted sum of the overlaps between the
LDOCE sense and the WN synset, hypernyms,
and gloss respectively, that is
A(i, j) = ai IDi n Sit + adDi n Hyp(s1)1
+ a3IDi n Gi(s;),
For our tests, we tried setting al = 3, a2 = 5
and a3 = 2 to reveal the relative significance of
finding a synonym, a hypernym, and any word
in the textual gloss respectively in the dictio-
nary definition.
Step 4: From ROGET, find all paragraphs
Prn {W1) W2, ...} such that W(p) E Pm.
Step 5: Compute a similarity score matrix B for the
WN synsets and the ROGET classes. A simi-
larity score B(j, k) is computed for the jth WN
synset (taking the synset itself, the hypernyms,
and the coordinate terms) and the kth ROGET
class, according to the following:
</bodyText>
<equation confidence="0.804634">
B(j, k) = bilSj n Pk&apos; + b2IHYP(S3) n Pki
b3ICO(Sj) n Pk1
</equation>
<bodyText confidence="0.999830904761905">
We have set b1 = b2 = b3 = 1. Since a ROGET
class contains words expressing every aspect of
the same idea, it should be equally likely to find
synonyms, hypernyms and coordinate terms in
common.
Step 6: For i = 1 to t (i.e. each LDOCE sense), find
max(A(i, j)) from matrix A. Then trace from
matrix B the jth row and find max(B(j, k)).
The ith LDOCE sense should finally be mapped
to the ROGET class to which Pk belongs.
We have made an operational assumption about
the analysis of definitions. We did not attempt to
parse definitions to identify genus terms but simply
approximated this by using the weights al, a2 and a3
in Step 3. Considering that words are often defined
in terms of superordinates and slightly less often by
synonyms, we assign numerical weights in the order
a2 &gt; al &gt; a3. We are also aware that definitions can
take other forms which may involve part-of relations,
membership, and so on, though we did not deal with
them in this study.
</bodyText>
<sectionHeader confidence="0.973024" genericHeader="method">
4 Testing and Results
</sectionHeader>
<bodyText confidence="0.999616444444445">
The algorithm was tested on 12 nouns, listed in Ta-
ble 1 with the number of senses in the various lexical
resources.
The various types of possible mapping errors are
summarised in Table 2. Incorrectly Mapped and
Unmapped-a are both &amp;quot;misses&amp;quot;, whereas Forced Er-
ror and Unmapped-b are both &amp;quot;false alarms&amp;quot;.
The performance of the three parts of mapping
is shown in Table 3. The &amp;quot;carry-over error&amp;quot; is only
</bodyText>
<page confidence="0.969964">
1488
</page>
<table confidence="0.998732142857143">
Word R W L Word R W L
Country 3 4 5 Matter 8 5 7
Water 9 8 8 System 6 8 5
School 3 6 7 Interest 14 8 6
Room 3 4 5 Voice 4 8 9
Money 1 3 2 State 7 5 6
Girl 4 5 5 Company 10 8 9
</table>
<tableCaption confidence="0.999137">
Table 1: The 12 nouns used in testing
</tableCaption>
<table confidence="0.99910125">
Target Exists Mapping Outcome
Wrong Match No Match
Yes Incorrectly Mapped Unmapped-a
No Forced Error Unmapped-b
</table>
<tableCaption confidence="0.9966">
Table 2: Different types of errors
</tableCaption>
<bodyText confidence="0.999947285714286">
ing only one single resource in any application, it also
suggests there is additional information we can use
to overcome the inadequacy of individual resources.
For example, we may take the senses from one re-
source and complement them with the unattached
senses from the other two, thus resulting in a more
complete but not redundant sense discrimination.
</bodyText>
<sectionHeader confidence="0.998426" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.993019636363637">
This study can be extended in at least two paths.
One is to focus on the generality of the algorithm by
testing it on a bigger variety of words, and the other
on its practical value by applying the resultant lexi-
cal information in some real applications and check-
ing the effect of using multiple resources. It is also
desirable to explore definition parsing to see if map-
ping results will be improved.
applicable to the last stage, L -&gt;R, and it refers to
cases where the final answer is wrong as a result of References
a faulty outcome from the first stage (L -&gt;W).
</bodyText>
<table confidence="0.999347857142857">
L-*W W-+R L-+R
Accurately Mapped 68.9% 75.0% 55.4%
Incorrectly Mapped 12.2% 1.4% 4.1%
Unmapped-a 2.7% 6.9% 13.5%
Unmapped-6 13.5% 5.6% 16.2%
Forced Error 2.7% 11.1% -
Carry-over Error - - 10.8%
</table>
<tableCaption confidence="0.999625">
Table 3: Performance of the algorithm
</tableCaption>
<sectionHeader confidence="0.999104" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9997986">
Overall, the Accurately Mapped figures support our
hypothesis that conventional dictionaries and the-
sauri can be related through WordNet. Looking at
the unsuccessful cases, we see that there are rela-
tively more &amp;quot;false alarms&amp;quot; than &amp;quot;misses&amp;quot;, showing
that errors mostly arise from the inadequacy of indi-
vidual resources because there are no targets rather
than from partial failures of the process. Moreover,
the number of &amp;quot;misses&amp;quot; can possibly be reduced if
more definition patterns are considered.
Clearly the successful mappings are influenced by
the fineness of the sense discrimination in the re-
sources. How finely they are distinguished can be
inferred from the similarity score matrices. Reading
the matrices row-wise shows how vaguely a certain
sense is defined, whereas reading them column-wise
reveals how polysemous a word is.
While the links resulting from the algorithm can
be right or wrong, there were some senses of the
test words which appeared in one resource but had
no counterpart in the others, i.e. they were not at-
tached to any links. Thus 18.9% of the LDOCE
senses, 11.1% of the WN synsets and 58.1% of
the ROGET classes were among these unattached
senses. Though this implies the insufficiency of us-
</bodyText>
<reference confidence="0.999795073170732">
R. Amsler. 1981. A taxonomy for English nouns and
verbs. In Proceedings of ACL &apos;81, pages 133-138.
N. Calzolari. 1984. Detecting patterns in a lexical data
base. In Proceedings of COLING-84, pages 170-173.
N. Calzolari. 1988. The dictionary and the thesaurus
can be combined. In M.W. Evens, editor, Relational
Models of the Lexicon: Representing Knowledge in Se-
mantic Networks. Cambridge University Press.
M.S. Chodorow, R.J. Byrd, and G.E. Heidorn. 1985.
Extracting semantic hierarchies from a large on-line
dictionary. In Proceedings of ACL &apos;85, pages 299-304.
B. Kirkpatrick. 1987. Roget&apos;s Thesaurus of English
Words and Phrases. Penguin Books.
J. Klavans and E. Tzoukermann. 1995. Combining cor-
pus and machine-readable dictionary data for building
bilingual lexicons. Machine Translation, 10:185-218.
J. Klavans, M. Chodorow, and N. Wacholder. 1990.
From dictionary to knowledge base via taxonomy. In
Proceedings of the Sixth Conference of the University
of Waterloo, Canada. Centre for the New Oxford En-
glish dictionary and Text Research: Electronic Text
Research.
J. Markowitz, T. Ahlswede, and M. Evens. 1986. Se-
mantically significant patterns in dictionary defini-
tions. In Proceedings of ACL &apos;86, pages 112-119.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1993. Introduction to WordNet: An on-
line lexical database. Five Papers on WordNet.
P. Procter. 1978. Longman Dictionary of Contemporary
English. Longman Group Ltd.
P.M. Roget. 1852. Roget&apos;s Thesaurus of English Words
and Phrases. Penguin Books.
P. Vossen and A. Copestake. 1993. Untangling def-
inition structure into knowledge representation. In
T. Briscoe, A. Copestake, and V. de Paiva, editors, In-
heritance, Defaults and the Lexicon. Cambridge Uni-
versity Press.
D. Yarowsky. 1992. Word-sense disambiguation using
statistical models of Roget&apos;s categories trained on
large corpora. In Proceedings of COLING-92, pages
454-460, Nantes, France.
</reference>
<page confidence="0.995976">
1489
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.751828">
<title confidence="0.999882">Bridging the Gap between Dictionary and Thesaurus</title>
<author confidence="0.989301">Oi Yee Kwong</author>
<affiliation confidence="0.997572">Computer Laboratory, University of Cambridge</affiliation>
<address confidence="0.800118">New Museums Site, Cambridge CB2 3QG, U.K.</address>
<email confidence="0.966565">oyk20@el.cam.ae.uk</email>
<abstract confidence="0.997500615384615">This paper presents an algorithm to integrate different lexical resources, through which we hope to overcome the individual inadequacy of the resources, and thus obtain some enriched lexical semantic information for applications such as word sense disambiguation. We used WordNet as a mediator between a conventional dictionary and a thesaurus. Preliminary results support our hypothesised structural relationship, which enables the integration, of the resources. These results also suggest that we can combine the resources to achieve an overall balanced degree of sense discrimination.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Amsler</author>
</authors>
<title>A taxonomy for English nouns and verbs.</title>
<date>1981</date>
<booktitle>In Proceedings of ACL &apos;81,</booktitle>
<pages>133--138</pages>
<contexts>
<context position="1516" citStr="Amsler, 1981" startWordPosition="224" endWordPosition="225">n retrieval (IR), require a wide range of resources to supply the necessary lexical semantic information. For instance, Calzolari (1988) proposed a lexical database in Italian which has the features of both a dictionary and a thesaurus; and Klavans and Tzoukermann (1995) tried to build a fuller bilingual lexicon by enhancing machine-readable dictionaries with large corpora. Among the attempts to enrich lexical information, many have been directed to the analysis of dictionary definitions and the transformation of the implicit information to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolari, 1984; Chodorow et al., 1985; Markowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonetheless, dictionaries are also infamous of their non-standardised sense granularity, and the taxonomies obtained from definitions are inevitably ad hoc. It would therefore be a good idea if we can unify our lexical semantic knowledge by some existing, and widely exploited, classifications such as the system in Roget&apos;s Thesaurus (Roget, 1852), which has remained intact for years and has been used in WSD (Yarowsky, 1992). While the objective is to integrate different lexical r</context>
</contexts>
<marker>Amsler, 1981</marker>
<rawString>R. Amsler. 1981. A taxonomy for English nouns and verbs. In Proceedings of ACL &apos;81, pages 133-138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Calzolari</author>
</authors>
<title>Detecting patterns in a lexical data base.</title>
<date>1984</date>
<booktitle>In Proceedings of COLING-84,</booktitle>
<pages>170--173</pages>
<contexts>
<context position="1533" citStr="Calzolari, 1984" startWordPosition="226" endWordPosition="227">R), require a wide range of resources to supply the necessary lexical semantic information. For instance, Calzolari (1988) proposed a lexical database in Italian which has the features of both a dictionary and a thesaurus; and Klavans and Tzoukermann (1995) tried to build a fuller bilingual lexicon by enhancing machine-readable dictionaries with large corpora. Among the attempts to enrich lexical information, many have been directed to the analysis of dictionary definitions and the transformation of the implicit information to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolari, 1984; Chodorow et al., 1985; Markowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonetheless, dictionaries are also infamous of their non-standardised sense granularity, and the taxonomies obtained from definitions are inevitably ad hoc. It would therefore be a good idea if we can unify our lexical semantic knowledge by some existing, and widely exploited, classifications such as the system in Roget&apos;s Thesaurus (Roget, 1852), which has remained intact for years and has been used in WSD (Yarowsky, 1992). While the objective is to integrate different lexical resources, the pro</context>
</contexts>
<marker>Calzolari, 1984</marker>
<rawString>N. Calzolari. 1984. Detecting patterns in a lexical data base. In Proceedings of COLING-84, pages 170-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Calzolari</author>
</authors>
<title>The dictionary and the thesaurus can be combined.</title>
<date>1988</date>
<booktitle>Relational Models of the Lexicon: Representing Knowledge in Semantic Networks.</booktitle>
<editor>In M.W. Evens, editor,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1040" citStr="Calzolari (1988)" startWordPosition="152" endWordPosition="154">e disambiguation. We used WordNet as a mediator between a conventional dictionary and a thesaurus. Preliminary results support our hypothesised structural relationship, which enables the integration, of the resources. These results also suggest that we can combine the resources to achieve an overall balanced degree of sense discrimination. 1 Introduction It is generally accepted that applications such as word sense disambiguation (WSD), machine translation (MT) and information retrieval (IR), require a wide range of resources to supply the necessary lexical semantic information. For instance, Calzolari (1988) proposed a lexical database in Italian which has the features of both a dictionary and a thesaurus; and Klavans and Tzoukermann (1995) tried to build a fuller bilingual lexicon by enhancing machine-readable dictionaries with large corpora. Among the attempts to enrich lexical information, many have been directed to the analysis of dictionary definitions and the transformation of the implicit information to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolari, 1984; Chodorow et al., 1985; Markowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonethe</context>
</contexts>
<marker>Calzolari, 1988</marker>
<rawString>N. Calzolari. 1988. The dictionary and the thesaurus can be combined. In M.W. Evens, editor, Relational Models of the Lexicon: Representing Knowledge in Semantic Networks. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Chodorow</author>
<author>R J Byrd</author>
<author>G E Heidorn</author>
</authors>
<title>Extracting semantic hierarchies from a large on-line dictionary.</title>
<date>1985</date>
<booktitle>In Proceedings of ACL &apos;85,</booktitle>
<pages>299--304</pages>
<contexts>
<context position="1556" citStr="Chodorow et al., 1985" startWordPosition="228" endWordPosition="231">e range of resources to supply the necessary lexical semantic information. For instance, Calzolari (1988) proposed a lexical database in Italian which has the features of both a dictionary and a thesaurus; and Klavans and Tzoukermann (1995) tried to build a fuller bilingual lexicon by enhancing machine-readable dictionaries with large corpora. Among the attempts to enrich lexical information, many have been directed to the analysis of dictionary definitions and the transformation of the implicit information to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolari, 1984; Chodorow et al., 1985; Markowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonetheless, dictionaries are also infamous of their non-standardised sense granularity, and the taxonomies obtained from definitions are inevitably ad hoc. It would therefore be a good idea if we can unify our lexical semantic knowledge by some existing, and widely exploited, classifications such as the system in Roget&apos;s Thesaurus (Roget, 1852), which has remained intact for years and has been used in WSD (Yarowsky, 1992). While the objective is to integrate different lexical resources, the problem is: how do we reco</context>
</contexts>
<marker>Chodorow, Byrd, Heidorn, 1985</marker>
<rawString>M.S. Chodorow, R.J. Byrd, and G.E. Heidorn. 1985. Extracting semantic hierarchies from a large on-line dictionary. In Proceedings of ACL &apos;85, pages 299-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kirkpatrick</author>
</authors>
<title>Roget&apos;s Thesaurus of English Words and Phrases.</title>
<date>1987</date>
<publisher>Penguin Books.</publisher>
<contexts>
<context position="2667" citStr="Kirkpatrick, 1987" startWordPosition="409" endWordPosition="411">sky, 1992). While the objective is to integrate different lexical resources, the problem is: how do we reconcile the rich but variable information in dictionary senses with the cruder but more stable taxonomies like those in thesauri? This work is intended to fill this gap. We use WordNet as a mediator in the process. In the following, we will outline an algorithm to map word senses in a dictionary to semantic classes in some established classification scheme. 2 Inter-relatedness of the Resources The three lexical resources used in this work are the 1987 revision of Roget&apos;s Thesaurus (ROGET) (Kirkpatrick, 1987), the Longman Dictionary of Contemporary English (LDOCE) (Procter, 1978) and WordNet 1.5 (WN) (Miller et al., 1993). Figure 1 shows how word senses are organised in them. As we have mentioned, instead of directly mapping an LDOCE definition to a ROGET class, we bridge the gap with WN, as indicated by the arrows in the figure. Such a route is made feasible by linking the structures in common among the resources. Words are organised in alphabetical order in LDOCE, as in other conventional dictionaries. The senses are listed after each entry, in the form of text definitions. WN groups words into </context>
</contexts>
<marker>Kirkpatrick, 1987</marker>
<rawString>B. Kirkpatrick. 1987. Roget&apos;s Thesaurus of English Words and Phrases. Penguin Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Klavans</author>
<author>E Tzoukermann</author>
</authors>
<title>Combining corpus and machine-readable dictionary data for building bilingual lexicons.</title>
<date>1995</date>
<journal>Machine Translation,</journal>
<pages>10--185</pages>
<contexts>
<context position="1175" citStr="Klavans and Tzoukermann (1995)" startWordPosition="173" endWordPosition="176">pport our hypothesised structural relationship, which enables the integration, of the resources. These results also suggest that we can combine the resources to achieve an overall balanced degree of sense discrimination. 1 Introduction It is generally accepted that applications such as word sense disambiguation (WSD), machine translation (MT) and information retrieval (IR), require a wide range of resources to supply the necessary lexical semantic information. For instance, Calzolari (1988) proposed a lexical database in Italian which has the features of both a dictionary and a thesaurus; and Klavans and Tzoukermann (1995) tried to build a fuller bilingual lexicon by enhancing machine-readable dictionaries with large corpora. Among the attempts to enrich lexical information, many have been directed to the analysis of dictionary definitions and the transformation of the implicit information to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolari, 1984; Chodorow et al., 1985; Markowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonetheless, dictionaries are also infamous of their non-standardised sense granularity, and the taxonomies obtained from definitions are inev</context>
</contexts>
<marker>Klavans, Tzoukermann, 1995</marker>
<rawString>J. Klavans and E. Tzoukermann. 1995. Combining corpus and machine-readable dictionary data for building bilingual lexicons. Machine Translation, 10:185-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Klavans</author>
<author>M Chodorow</author>
<author>N Wacholder</author>
</authors>
<title>From dictionary to knowledge base via taxonomy.</title>
<date>1990</date>
<booktitle>In Proceedings of the Sixth Conference of the</booktitle>
<institution>University of Waterloo, Canada. Centre for</institution>
<contexts>
<context position="1602" citStr="Klavans et al., 1990" startWordPosition="236" endWordPosition="239">xical semantic information. For instance, Calzolari (1988) proposed a lexical database in Italian which has the features of both a dictionary and a thesaurus; and Klavans and Tzoukermann (1995) tried to build a fuller bilingual lexicon by enhancing machine-readable dictionaries with large corpora. Among the attempts to enrich lexical information, many have been directed to the analysis of dictionary definitions and the transformation of the implicit information to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolari, 1984; Chodorow et al., 1985; Markowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonetheless, dictionaries are also infamous of their non-standardised sense granularity, and the taxonomies obtained from definitions are inevitably ad hoc. It would therefore be a good idea if we can unify our lexical semantic knowledge by some existing, and widely exploited, classifications such as the system in Roget&apos;s Thesaurus (Roget, 1852), which has remained intact for years and has been used in WSD (Yarowsky, 1992). While the objective is to integrate different lexical resources, the problem is: how do we reconcile the rich but variable information in dic</context>
</contexts>
<marker>Klavans, Chodorow, Wacholder, 1990</marker>
<rawString>J. Klavans, M. Chodorow, and N. Wacholder. 1990. From dictionary to knowledge base via taxonomy. In Proceedings of the Sixth Conference of the University of Waterloo, Canada. Centre for the New Oxford English dictionary and Text Research: Electronic Text Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Markowitz</author>
<author>T Ahlswede</author>
<author>M Evens</author>
</authors>
<title>Semantically significant patterns in dictionary definitions.</title>
<date>1986</date>
<booktitle>In Proceedings of ACL &apos;86,</booktitle>
<pages>112--119</pages>
<contexts>
<context position="1580" citStr="Markowitz et al., 1986" startWordPosition="232" endWordPosition="235"> supply the necessary lexical semantic information. For instance, Calzolari (1988) proposed a lexical database in Italian which has the features of both a dictionary and a thesaurus; and Klavans and Tzoukermann (1995) tried to build a fuller bilingual lexicon by enhancing machine-readable dictionaries with large corpora. Among the attempts to enrich lexical information, many have been directed to the analysis of dictionary definitions and the transformation of the implicit information to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolari, 1984; Chodorow et al., 1985; Markowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonetheless, dictionaries are also infamous of their non-standardised sense granularity, and the taxonomies obtained from definitions are inevitably ad hoc. It would therefore be a good idea if we can unify our lexical semantic knowledge by some existing, and widely exploited, classifications such as the system in Roget&apos;s Thesaurus (Roget, 1852), which has remained intact for years and has been used in WSD (Yarowsky, 1992). While the objective is to integrate different lexical resources, the problem is: how do we reconcile the rich but varia</context>
</contexts>
<marker>Markowitz, Ahlswede, Evens, 1986</marker>
<rawString>J. Markowitz, T. Ahlswede, and M. Evens. 1986. Semantically significant patterns in dictionary definitions. In Proceedings of ACL &apos;86, pages 112-119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Introduction to WordNet: An online lexical database. Five Papers on WordNet.</title>
<date>1993</date>
<contexts>
<context position="2782" citStr="Miller et al., 1993" startWordPosition="427" endWordPosition="430">the rich but variable information in dictionary senses with the cruder but more stable taxonomies like those in thesauri? This work is intended to fill this gap. We use WordNet as a mediator in the process. In the following, we will outline an algorithm to map word senses in a dictionary to semantic classes in some established classification scheme. 2 Inter-relatedness of the Resources The three lexical resources used in this work are the 1987 revision of Roget&apos;s Thesaurus (ROGET) (Kirkpatrick, 1987), the Longman Dictionary of Contemporary English (LDOCE) (Procter, 1978) and WordNet 1.5 (WN) (Miller et al., 1993). Figure 1 shows how word senses are organised in them. As we have mentioned, instead of directly mapping an LDOCE definition to a ROGET class, we bridge the gap with WN, as indicated by the arrows in the figure. Such a route is made feasible by linking the structures in common among the resources. Words are organised in alphabetical order in LDOCE, as in other conventional dictionaries. The senses are listed after each entry, in the form of text definitions. WN groups words into sets of synonyms (&amp;quot;synsets&amp;quot;), with an optional textual gloss. These synsets form the nodes of a taxonomic hierarchy</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1993</marker>
<rawString>G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1993. Introduction to WordNet: An online lexical database. Five Papers on WordNet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Procter</author>
</authors>
<title>Longman Dictionary of Contemporary English.</title>
<date>1978</date>
<publisher>Longman Group Ltd.</publisher>
<contexts>
<context position="2739" citStr="Procter, 1978" startWordPosition="420" endWordPosition="421"> the problem is: how do we reconcile the rich but variable information in dictionary senses with the cruder but more stable taxonomies like those in thesauri? This work is intended to fill this gap. We use WordNet as a mediator in the process. In the following, we will outline an algorithm to map word senses in a dictionary to semantic classes in some established classification scheme. 2 Inter-relatedness of the Resources The three lexical resources used in this work are the 1987 revision of Roget&apos;s Thesaurus (ROGET) (Kirkpatrick, 1987), the Longman Dictionary of Contemporary English (LDOCE) (Procter, 1978) and WordNet 1.5 (WN) (Miller et al., 1993). Figure 1 shows how word senses are organised in them. As we have mentioned, instead of directly mapping an LDOCE definition to a ROGET class, we bridge the gap with WN, as indicated by the arrows in the figure. Such a route is made feasible by linking the structures in common among the resources. Words are organised in alphabetical order in LDOCE, as in other conventional dictionaries. The senses are listed after each entry, in the form of text definitions. WN groups words into sets of synonyms (&amp;quot;synsets&amp;quot;), with an optional textual gloss. These syns</context>
</contexts>
<marker>Procter, 1978</marker>
<rawString>P. Procter. 1978. Longman Dictionary of Contemporary English. Longman Group Ltd.</rawString>
</citation>
<citation valid="false">
<title>Roget&apos;s Thesaurus of English Words and Phrases.</title>
<publisher>Penguin Books.</publisher>
<marker></marker>
<rawString>P.M. Roget. 1852. Roget&apos;s Thesaurus of English Words and Phrases. Penguin Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
<author>A Copestake</author>
</authors>
<title>Untangling definition structure into knowledge representation. In</title>
<date>1993</date>
<booktitle>Inheritance, Defaults and the Lexicon.</booktitle>
<editor>T. Briscoe, A. Copestake, and V. de Paiva, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1631" citStr="Vossen and Copestake, 1993" startWordPosition="240" endWordPosition="243">tion. For instance, Calzolari (1988) proposed a lexical database in Italian which has the features of both a dictionary and a thesaurus; and Klavans and Tzoukermann (1995) tried to build a fuller bilingual lexicon by enhancing machine-readable dictionaries with large corpora. Among the attempts to enrich lexical information, many have been directed to the analysis of dictionary definitions and the transformation of the implicit information to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolari, 1984; Chodorow et al., 1985; Markowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonetheless, dictionaries are also infamous of their non-standardised sense granularity, and the taxonomies obtained from definitions are inevitably ad hoc. It would therefore be a good idea if we can unify our lexical semantic knowledge by some existing, and widely exploited, classifications such as the system in Roget&apos;s Thesaurus (Roget, 1852), which has remained intact for years and has been used in WSD (Yarowsky, 1992). While the objective is to integrate different lexical resources, the problem is: how do we reconcile the rich but variable information in dictionary senses with the crude</context>
</contexts>
<marker>Vossen, Copestake, 1993</marker>
<rawString>P. Vossen and A. Copestake. 1993. Untangling definition structure into knowledge representation. In T. Briscoe, A. Copestake, and V. de Paiva, editors, Inheritance, Defaults and the Lexicon. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<pages>454--460</pages>
<location>Nantes, France.</location>
<contexts>
<context position="2059" citStr="Yarowsky, 1992" startWordPosition="308" endWordPosition="309">ion to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolari, 1984; Chodorow et al., 1985; Markowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonetheless, dictionaries are also infamous of their non-standardised sense granularity, and the taxonomies obtained from definitions are inevitably ad hoc. It would therefore be a good idea if we can unify our lexical semantic knowledge by some existing, and widely exploited, classifications such as the system in Roget&apos;s Thesaurus (Roget, 1852), which has remained intact for years and has been used in WSD (Yarowsky, 1992). While the objective is to integrate different lexical resources, the problem is: how do we reconcile the rich but variable information in dictionary senses with the cruder but more stable taxonomies like those in thesauri? This work is intended to fill this gap. We use WordNet as a mediator in the process. In the following, we will outline an algorithm to map word senses in a dictionary to semantic classes in some established classification scheme. 2 Inter-relatedness of the Resources The three lexical resources used in this work are the 1987 revision of Roget&apos;s Thesaurus (ROGET) (Kirkpatric</context>
<context position="4976" citStr="Yarowsky (1992)" startWordPosition="836" endWordPosition="837"> 01(P) Isl. x2... I, G1(X) 121. N.... x3 RT (ROGET) (WN) (LDOCE) EAF (ml. m2.... LOW) X2 definition (Dx) similiar to 01(X) cl, c2, ). MC) or defined in terms of words in X or C. etc. Figure 1: Organisation of word senses in different resources dictionary definitions to be phrased with synonyms or superordinate terms, we would also expect to find words from X and C, or even A, in the LDOCE definition. That means we believe D r Gl(X) and Dr n (x u C u 0 0. 3 The Algorithm The possibility of using statistical methods to assign ROGET category labels to dictionary definitions has been suggested by Yarowsky (1992). Our algorithm offers a systematic way of linking existing resources by defining a mapping chain from LDOCE to ROGET through WN. It is based on shallow processing within the resources themselves, exploiting their inter-relatedness, and does not rely on extensive statistical data. It therefore has an advantage of being immune to any change of sense discrimination with time, since it only depends on the organisation but not the individual entries of the resources. Given a word with part of speech, W(p), the core steps are as follows: Step 1: From LDOCE, get the sense definitions Di, ..., Dt und</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>D. Yarowsky. 1992. Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora. In Proceedings of COLING-92, pages 454-460, Nantes, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>