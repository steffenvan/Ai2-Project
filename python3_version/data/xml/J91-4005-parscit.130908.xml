<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005170">
<title confidence="0.988176">
Natural Language Generation in Artificial Intelligence and
Computational Linguistics
</title>
<author confidence="0.931261">
Cecile Paris, William R. Swartout, and William C. Mann (editors)
</author>
<affiliation confidence="0.991928666666667">
(Information Sciences Institute, University of Southern California)
Boston: Kluwer Academic Publishers
(The Kluwer International Series in
</affiliation>
<bodyText confidence="0.389702166666667">
Engineering and Computer Science:
Natural Language Processing and
Machine Translation), 1991,
xxii + 403 pp.
Hardbound, ISBN 0-7923-9098-9, $85.00,
£50.75, Dfl 180.00
</bodyText>
<figure confidence="0.94492">
Reviewed by
Robert Dale
</figure>
<affiliation confidence="0.518548">
University of Edinburgh
</affiliation>
<bodyText confidence="0.998668615384615">
A number of collections of papers from the field of natural language generation (NLG)
have been published over the last few years: Kempen (1987), Zock and Sabah (1988),
Dale, Mellish, and Zock (1990), and now the present volume. All have in common that
they are derived in one way or another from workshops on the subject, and should
therefore make available new and often exploratory research in a timely fashion. If
such a book is to be more than a conference proceedings, it has to do a little more too,
of course; it should present the research in more detail than a conference proceedings
would, there should be greater cohesion amongst the papers, and it should be pro-
duced to an appropriate standard. The present book, like its predecessors, succeeds
on some counts but fails on others. The papers in the book are organized into three
strands, described in turn below: text planning, lexical choice, and grammatical resources.
The balance between these is rather skewed, however: the first section contains eight
papers, and the second and third contain only three papers each.
</bodyText>
<sectionHeader confidence="0.709922" genericHeader="method">
1. Text Planning
</sectionHeader>
<bodyText confidence="0.999974866666667">
It is the first section of the book that exhibits the most coherence. Papers by Moore
and Swartout, Paris, and Hovy all describe experiments in text planning based on
Rhetorical Structure Theory (RsT). The first two of these papers describe work in the
Explainable Expert Systems framework, whose basic idea is that expert systems have
to be designed from the outset with explanation in mind, rather than having a sep-
arate generation module tacked on the end. Paris provides some good background
on the framework, describing the different kinds of knowledge that this requires such
systems to have, along with the concomitant problems this poses for generation; she
describes an earlier experiment using McKeown-like schemata (McKeown 1985) for
generation in this context, and uses this to motivate the switch to a top-down text
planner based on Rhetorical Structure Theory. Moore and Swartout describe in some
detail the Program Enhancement Advisor, a system that participates in explanatory di-
alog, and emphasize the need for such systems to explicitly represent and reason about
the design of the explanations they construct, so as to permit elaboration of previous
explanations, provision of clarifications, and the answering of follow-up questions.
</bodyText>
<page confidence="0.996879">
448
</page>
<subsectionHeader confidence="0.949152">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999951411764706">
Although also based on RST, Hovy&apos;s system is fundamentally different from the
two just described, primarily because it uses RST only to organize preselected content,
whereas Moore and Swartout&apos;s and Paris&apos;s systems also use RST to determine content.
Each of these three papers devotes some discussion to the others, which is good,
and provides the coherence alluded to earlier; however, there is a little redundancy of
background information across the papers, which could have been cut. Together, these
three papers give the reader a good idea of some of the issues in text planning; Hovy&apos;s
exposition in particular could serve as the basis for implementation of a simple text
structuring program.
Of the other papers in this section, my favorites are those by Bateman and Meteer.
Bateman describes what he calls the &amp;quot;grammar-as-filter&amp;quot; methodology, and provides
a good introduction to the basic tenets of systemic functional linguistics (SFL). The
basic points here will not be new to most people in the generation field, but, like
other papers in this volume, the background provided makes the paper potentially
more useful to newcomers to the field. Bateman elaborates on how, in SFL, the three
metafunctions of language — the ideational, the interpersonal, and the textual — are
treated equally, in contrast to other approaches where the ideational (roughly, the no-
tion of propositional content) usually takes precedence, with the other categories all
too often being assigned to the wastebasket of pragmatics. In SFL, the core idea is that
each variation in language carries a functional load, unless there are good reasons
to think otherwise. Bateman begins by demonstrating the relatively uncontroversial
idea that interpersonal meaning (the expression of social relationships and speaker
attitudes) is grammaticized in Japanese; then, with some fairly good linguistic argu-
ment, he focuses on the textual metafunction (roughly, those aspects of language use
concerned with connectivity and cohesion), and unfolds his story by examining the
use of particle assignment in Japanese. He shows how the topic particle wa carries
two types of textual meaning (topicality and contrast) and goes on to present a de-
tailed argument for the view that Japanese case particles play a role in information
structuring. The result of these analyses is a number of textual distinctions that a text
planner needs to be able to talk in terms of. Although this is really only a very small
exploration of the space of things that have to be considered, the paper is a valuable
exposition of the approach.
The basis of Meteer&apos;s paper is an analysis of texts revised by professional edi-
tors, comparing the changes with the linguistic structures used in the originals. After
presenting a fascinating collection of examples of each of the categories of changes
she posits, she looks at how these revisions might be explained, and in so doing in-
troduces a notion of text structure that abstracts away from specific realizations, and
talks in terms of more general notions of heads and arguments, and matrices and
adjuncts; revisions in texts can then be characterized as transformations at this level.
This level of text structure has a fundamental role to play in that old chestnut for
generation researchers of how you draw the line between what to say and how to say
it: the problem is that part of the motivation for the distinction is to keep conceptual
and linguistic knowledge separate, and yet, without any understanding of linguistic
resources, there is the danger that the what-to-say component may construct some
message specification that has no linguistic realization. The notion of text structure
provides a level of abstraction that can be used as the interface language between
the two components: in my view, the significance of Meteer&apos;s work here is very great
indeed.
Reithinger describes the POPEL system, which takes a different stance on the prob-
lem of integrating what to say and how to say it. The system he describes in some
detail consists of two components, POPEL-WHAT and POPEL-HOW, which together em-
</bodyText>
<page confidence="0.993274">
449
</page>
<note confidence="0.481295">
Computational Linguistics Volume 17, Number 4
</note>
<bodyText confidence="0.999726181818182">
body an incremental generation process. An important aspect of the system is the
bidirectional interaction between the two components, which allows each component
to post queries to the other. The paper also includes a couple of tantalizing tasters that
I would liked to have seen taken further: a brief pointer to the multi-modal capabili-
ties of the system, and some consolidatory work on integrating the discourse theories
of Reichman, Grosz and Sidner, and RST that is, alas, described in too little detail to
assess.
The other papers in this section are by McCoy and Cheng, and Sparck Jones.
McCoy and Cheng describe their notion of focus trees, an attempt at providing a
unified approach to focus phenomena that integrates a number of observations that
people have made over the years: there is definitely an intuition of importance here,
but it&apos;s clear that there&apos;s a lot more working-out to be done. Sparck Jones&apos;s paper
questions the general presumption that the output of a natural language generation
system should be tailored to the user. After a quick summary of some of the relevant
distinctions in user modeling — useful for those who are new to that field — she uses
a number of examples to press her point that systems should not be too quick to make
assumptions about their users. The limited sophistication of our systems means that
any hypotheses we may derive are generally poorly supported, so it&apos;s better to play
safe; that way, you are less likely to make mistakes or offend people. Furthermore, she
argues, input data and application information is enough to do the job, so, while in
principle you could get heavily into user modeling, why bother? This is an interesting
paper, if a little overlong.
</bodyText>
<sectionHeader confidence="0.924234" genericHeader="method">
2. Lexical Choice
</sectionHeader>
<bodyText confidence="0.999736576923077">
I found this section of the book the most thought-provoking, and at the same time the
most frustrating. It is here that integration across the papers is lacking most, and yet
would have been, at least for me, most interesting and useful.
McDonald takes the view that lexical variation at the surface level has its origins
very deep in the conceptual system, and that the selection of key lexical items is
the first step in the generation process; this is not the standard approach, and so,
like Sparck Jones&apos;s paper in the previous section, the general argument here is one
of questioning widely held assumptions. Much of the paper is driven by a detailed
analysis of the differences that motivate the choice between You can only stay until 4
and You have to leave by 4, leading to a closer scrutiny of the conceptual models that
must underlie generation.
Matthiessen&apos;s paper focuses on a different aspect of lexical choice: how should
lexical resources be organized? His view is that lexical choice should be viewed as
part of the unified problem of lexicogrammatical choice. Like Bateman&apos;s paper earlier
in the volume, he provides some background on systemic functional linguistics for
those who are new to the area; just as in the descriptions of RST in the first three
papers on text planning, this leads to some redundancy across the book as a whole.
Matthiessen describes an interesting model of lexis (not just the lexicon, which, as he
points out, makes us think too much of lexical data divorced of its related processing)
that is closer to a thesaurus than a dictionary.
The final paper in the section describes the use of Menuk&apos;s meaning-text theory
(mTT) in the context of a generation system that provides reports about operating
system usage. The focus here is on how the multiple levels of representation in NM
allow for the introduction of paraphrase in the process of mapping between the levels.
The number of paraphrases thus available is quite large, and has then to be further
constrained by notions of information structure; it would be interesting to see what
</bodyText>
<page confidence="0.992967">
450
</page>
<subsectionHeader confidence="0.849126">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.99921475">
Meteer, McDonald, and Bateman&apos;s approaches would say about the variation here.
All three papers in this section are interesting, but the interconnections are weak:
there are lots of links the reader is tempted to construct for herself, but can only ponder
because the authors don&apos;t generally situate themselves with respect to each other.
</bodyText>
<sectionHeader confidence="0.603173" genericHeader="method">
3. Grammatical Resources
</sectionHeader>
<bodyText confidence="0.999959809523809">
This is the least satisfying section of the book, since it has the feel of a grab-bag sec-
tion for papers that wouldn&apos;t fit in the other two. De Smedt and Kempen provide
a very good and detailed exposition of segment grammar as an incremental tactical
generation process; particularly valuable here is the comparison with other grammat-
ical formalisms. Comparison of this kind is also a virtue of the paper by McKeown
and Elhadad, who argue that unification-based formalisms such as their Functional
Unification Grammar (FUG) allow for a much more flexible ordering of decisions than
other formalisms; this makes it easier to integrate multiple constraints in the genera-
tion process, a point that they demonstrate with respect to the choice of connectives
between clauses. The comparison with other approaches is quite detailed, although
the inclusion of ATNs as one of these alternatives seems a little dated.
Most striking in this section is the lack of any papers that focus specifically on
reversible grammar (although, to be fair, the McKeown and Elhadad paper can be
seen in this light). In fact, that part of the generation community who started their
research lives in parsing seems quite under-represented overall. It&apos;s clear that there
is a fundamental divide between those who take the view that generation is simply
parsing in reverse, and those who feel that the deeper issues in text organization
and planning are where the real action is, but this divide will only be overcome if
opportunities are provided for members of each community to read the work of the
other side, and one way of doing this is to put together books that represent each
more equally.
</bodyText>
<sectionHeader confidence="0.944719" genericHeader="conclusions">
4. Conclusions
</sectionHeader>
<bodyText confidence="0.99993925">
I have a general ambivalence toward books of this kind. On the one hand, they are
valuable because they make available research that is otherwise hard to find out about,
typically only described in technical reports; on the other hand, I feel rather concerned
about the prevalent expectancy that, if you have a workshop, you must have a book
derived from the workshop. On balance, I think this volume is a Good Thing; as a
whole, it provides a decent snapshot of a reasonably recent span of work in NLG, warts
and all.
I don&apos;t know why it is that so little of this work is reported in, for example, the
ACL conferences: is it because the program committees are biased against generation,
because the research just isn&apos;t up to standard, or because we NLG people have decided
to stay with our own at our cozy workshops? There&apos;s probably an element of truth
in each explanation. Inevitably, the quality of papers in this volume is variable —
only a few approach the standard one would expect of a journal article — but there is
some genuinely thought-provoking work here, some of which has relevance for people
working in natural language understanding too. This is particularly true of those that
address more fundamental issues and matters of methodology, such as the papers by
Bateman, Meteer, McDonald, and Sparck Jones: if the issues they raise are important
for NLG, then they are surely also important for NLU.
The book does have its bad points too, most of which have already been alluded to.
Some of these are par for the course: the preliminary and exploratory nature of much
</bodyText>
<page confidence="0.993129">
451
</page>
<note confidence="0.560511">
Computational Linguistics Volume 17, Number 4
</note>
<bodyText confidence="0.99990332">
of the work described, and the lack of integration and cohesion across the papers, are
also faults of the other workshop-based books mentioned earlier. A major deficiency
is the lack of any papers that address directly either grammar reversibility or multi-
modal generation, other than in passing (Reithinger&apos;s paper contains a page on this
aspect of his system); these lacunae are particularly serious in a book that claims to
present &amp;quot;the most current research&amp;quot; in the field (this claim is also rather odd given the
rather long time lag between the 1988 workshop on which the book is based and the
appearance of the book itself).
By far the most annoying thing about the book, however, is the quality of produc-
tion. The text has not been properly proofread or copyedited, resulting in mistakes at
all levels, from spelling errors through formatting mistakes to at least one reference to
another paper in the volume that turns out not to be present. Such a state of affairs is,
I suppose, acceptable for fast-turnaround conference proceedings, but this book took
three years to appear. The overall impression is of a publisher not doing the job it
is there to do; this alone would be bad enough, but it is quite unacceptable when
combined with the outrageous price charged for this book.
Who should buy this book? At its extortionate price, I imagine that it will be mostly
libraries. As workers in the field, we have an obligation to ensure that books are made
available more cheaply than this. If it were cheaper, then I would say it should be
on every generation researcher&apos;s bookshelf. Its audience outside this community is,
I think, limited; as a subarea of computational linguistics, we have not yet gotten
around to describing our research in such a way as to make it more generally useful.
The papers that address more fundamental issues will be of interest outside the NLG
community, but I cannot see anyone other than generation researchers being interested
in those papers that are essentially system descriptions.
</bodyText>
<reference confidence="0.976913571428571">
References McKeown, Kathleen R. (1985). Text
Dale, Robert; Mellish, Chris; and Zock, Generation: Using Discourse Strategies and
Michael (1990). Current Research in Natural Focus Constraints to Generate Natural
Language Generation. London: Academic Language Text. Cambridge, England:
Press. Cambridge University Press.
Kempen, Gerard (1987). Natural Language Zock, Michael, and Sabah, Gerard (1988).
Generation: New Results in Artificial Advances in Natural Language Generation:
Intelligence, Psychology and Linguistics. An Interdisciplinary Perspective. London:
Dordrecht: Martinus Nijhoff Publishers. Pinter Publishers.
Robert Dale lectures in Computational Linguistics at the Centre for Cognitive Science and the
Department of Artificial Intelligence at the University of Edinburgh. He did his Ph.D. work on
the generation of referring expressions; a book derived from this work will be published in the
near future in the ACL—MIT Press series in natural language processing. He co-edited a book
of papers derived from another workshop on generation.
</reference>
<page confidence="0.998462">
452
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.267496">
<title confidence="0.87741">Natural Language Generation in Artificial Intelligence and Computational Linguistics</title>
<author confidence="0.83625">Cecile Paris</author>
<author confidence="0.83625">William R Swartout</author>
<author confidence="0.83625">William C Mann</author>
<affiliation confidence="0.8478126">(Information Sciences Institute, University of Southern California) Boston: Kluwer Academic Publishers (The Kluwer International Series in Engineering and Computer Science: Natural Language Processing and</affiliation>
<note confidence="0.9153772">Machine Translation), 1991, xxii + 403 pp. Hardbound, ISBN 0-7923-9098-9, $85.00, £50.75, Dfl 180.00 Reviewed by</note>
<author confidence="0.999967">Robert Dale</author>
<affiliation confidence="0.996174">University of Edinburgh</affiliation>
<abstract confidence="0.986497634146341">A number of collections of papers from the field of natural language generation (NLG) have been published over the last few years: Kempen (1987), Zock and Sabah (1988), Dale, Mellish, and Zock (1990), and now the present volume. All have in common that they are derived in one way or another from workshops on the subject, and should therefore make available new and often exploratory research in a timely fashion. If such a book is to be more than a conference proceedings, it has to do a little more too, of course; it should present the research in more detail than a conference proceedings would, there should be greater cohesion amongst the papers, and it should be produced to an appropriate standard. The present book, like its predecessors, succeeds on some counts but fails on others. The papers in the book are organized into three described in turn below: planning, lexical choice, resources. The balance between these is rather skewed, however: the first section contains eight papers, and the second and third contain only three papers each. 1. Text Planning It is the first section of the book that exhibits the most coherence. Papers by Moore and Swartout, Paris, and Hovy all describe experiments in text planning based on Rhetorical Structure Theory (RsT). The first two of these papers describe work in the Explainable Expert Systems framework, whose basic idea is that expert systems have to be designed from the outset with explanation in mind, rather than having a separate generation module tacked on the end. Paris provides some good background on the framework, describing the different kinds of knowledge that this requires such systems to have, along with the concomitant problems this poses for generation; she describes an earlier experiment using McKeown-like schemata (McKeown 1985) for generation in this context, and uses this to motivate the switch to a top-down text planner based on Rhetorical Structure Theory. Moore and Swartout describe in some detail the Program Enhancement Advisor, a system that participates in explanatory dialog, and emphasize the need for such systems to explicitly represent and reason about the design of the explanations they construct, so as to permit elaboration of previous explanations, provision of clarifications, and the answering of follow-up questions. 448 Book Reviews also based on system is fundamentally different from the just described, primarily because it uses to organize preselected content, Moore and Swartout&apos;s and Paris&apos;s systems also use to Each of these three papers devotes some discussion to the others, which is good, and provides the coherence alluded to earlier; however, there is a little redundancy of background information across the papers, which could have been cut. Together, these three papers give the reader a good idea of some of the issues in text planning; Hovy&apos;s exposition in particular could serve as the basis for implementation of a simple text structuring program. Of the other papers in this section, my favorites are those by Bateman and Meteer.</abstract>
<intro confidence="0.666273">Bateman describes what he calls the &amp;quot;grammar-as-filter&amp;quot; methodology, and provides</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>References Dale</author>
<author>Mellish Robert</author>
<author>Chris</author>
<author>Michael Zock</author>
</authors>
<title>Current Research in Natural Language Generation. London: Academic Press.</title>
<date>1990</date>
<publisher>Cambridge University Press.</publisher>
<location>Kempen, Gerard</location>
<marker>Dale, Robert, Chris, Zock, 1990</marker>
<rawString>References Dale, Robert; Mellish, Chris; and Zock, Michael (1990). Current Research in Natural Language Generation. London: Academic Press. Kempen, Gerard (1987). Natural Language Generation: New Results in Artificial Intelligence, Psychology and Linguistics. Dordrecht: Martinus Nijhoff Publishers. McKeown, Kathleen R. (1985). Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Cambridge, England: Cambridge University Press. Zock, Michael, and Sabah, Gerard (1988). Advances in Natural Language Generation: An Interdisciplinary Perspective. London: Pinter Publishers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>