<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.026473">
<title confidence="0.98905">
A Syntactic and Lexical-Based Discourse Segmenter
</title>
<author confidence="0.99311">
Milan ToÞloski
</author>
<affiliation confidence="0.825433666666667">
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada
</affiliation>
<email confidence="0.996647">
mta45@sfu.ca
</email>
<author confidence="0.993466">
Julian Brooke
</author>
<affiliation confidence="0.824973666666667">
Department of Linguistics
Simon Fraser University
Burnaby, BC, Canada
</affiliation>
<email confidence="0.996047">
jab18@sfu.ca
</email>
<author confidence="0.974894">
Maite Taboada
</author>
<affiliation confidence="0.823135333333333">
Department of Linguistics
Simon Fraser University
Burnaby, BC, Canada
</affiliation>
<email confidence="0.998376">
mtaboada@sfu.ca
</email>
<sectionHeader confidence="0.997384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999742666666667">
We present a syntactic and lexically based
discourse segmenter (SLSeg) that is de-
signed to avoid the common problem of
over-segmenting text. Segmentation is the
first step in a discourse parser, a system
that constructs discourse trees from el-
ementary discourse units. We compare
SLSeg to a probabilistic segmenter, show-
ing that a conservative approach increases
precision at the expense of recall, while re-
taining a high F-score across both formal
and informal texts.
</bodyText>
<sectionHeader confidence="0.994709" genericHeader="keywords">
1 Introduction*
</sectionHeader>
<bodyText confidence="0.999956772727273">
Discourse segmentation is the process of de-
composing discourse into elementary discourse
units (EDUs), which may be simple sentences or
clauses in a complex sentence, and from which
discourse trees are constructed. In this sense, we
are performing low-level discourse segmentation,
as opposed to segmenting text into chunks or top-
ics (e.g., Passonneau and Litman (1997)). Since
segmentation is the first stage of discourse parsing,
quality discourse segments are critical to build-
ing quality discourse representations (Soricut and
Marcu, 2003). Our objective is to construct a dis-
course segmenter that is robust in handling both
formal (newswire) and informal (online reviews)
texts, while minimizing the insertion of incorrect
discourse boundaries. Robustness is achieved by
constructing discourse segments in a principled
way using syntactic and lexical information.
Our approach employs a set of rules for insert-
ing segment boundaries based on the syntax of
each sentence. The segment boundaries are then
further refined by using lexical information that
</bodyText>
<footnote confidence="0.750755">
*This work was supported by an NSERC Discovery Grant
(261104-2008) to Maite Taboada. We thank Angela Cooper
and Morgan Mameni for their help with the reliability study.
</footnote>
<bodyText confidence="0.979272277777778">
takes into consideration lexical cues, including
multi-word expressions. We also identify clauses
that are parsed as discourse segments, but are not
in fact independent discourse units, and join them
to the matrix clause.
Most parsers can break down a sentence into
constituent clauses, approaching the type of out-
put that we need as input to a discourse parser.
The segments produced by a parser, however, are
too fine-grained for discourse purposes, breaking
off complement and other clauses that are not in a
discourse relation to any other segment. For this
reason, we have implemented our own segmenter,
utilizing the output of a standard parser. The pur-
pose of this paper is to describe our syntactic and
lexical-based segmenter (SLSeg), demonstrate its
performance against state-of-the-art systems, and
make it available to the wider community.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999917210526316">
Soricut and Marcu (2003) construct a statistical
discourse segmenter as part of their sentence-level
discourse parser (SPADE), the only implemen-
tation available for our comparison. SPADE is
trained on the RST Discourse Treebank (Carlson
et al., 2002). The probabilities for segment bound-
ary insertion are learned using lexical and syntac-
tic features. Subba and Di Eugenio (2007) use
neural networks trained on RST-DT for discourse
segmentation. They obtain an F-score of 84.41%
(86.07% using a perfect parse), whereas SPADE
achieved 83.1% and 84.7% respectively.
Thanh et al. (2004) construct a rule-based
segmenter, employing manually annotated parses
from the Penn Treebank. Our approach is con-
ceptually similar, but we are only concerned with
established discourse relations, i.e., we avoid po-
tential same-unit relations by preserving NP con-
stituency.
</bodyText>
<page confidence="0.993302">
77
</page>
<note confidence="0.9720645">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 77–80,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<sectionHeader confidence="0.991113" genericHeader="method">
3 Principles For Discourse Segmentation
</sectionHeader>
<bodyText confidence="0.999983675">
Our primary concern is to capture interesting dis-
course relations, rather than all possible relations,
i.e., capturing more specific relations such as Con-
dition, Evidence or Purpose, rather than more gen-
eral and less informative relations such as Elabo-
ration or Joint, as defined in Rhetorical Structure
Theory (Mann and Thompson, 1988). By having a
stricter definition of an elementary discourse unit
(EDU), this approach increases precision at the ex-
pense of recall.
Grammatical units that are candidates for dis-
course segments are clauses and sentences. Our
basic principles for discourse segmentation follow
the proposals in RST as to what a minimal unit
of text is. Many of our differences with Carl-
son and Marcu (2001), who defined EDUs for the
RST Discourse Treebank (Carlson et al., 2002),
are due to the fact that we adhere closer to the orig-
inal RST proposals (Mann and Thompson, 1988),
which defined as ‘spans’ adjunct clauses, rather
than complement (subject and object) clauses. In
particular, we propose that complements of at-
tributive and cognitive verbs (He said (that)..., I
think (that)...) are not EDUs. We preserve con-
sistency by not breaking at direct speech (“X,” he
said.). Reported and direct speech are certainly
important in discourse (Prasad et al., 2006); we do
not believe, however, that they enter discourse re-
lations of the type that RST attempts to capture.
In general, adjunct, but not complement clauses
are discourse units. We require all discourse seg-
ments to contain a verb. Whenever a discourse
boundary is inserted, the two newly created seg-
ments must each contain a verb. We segment coor-
dinated clauses (but not coordinated VPs), adjunct
clauses with either finite or non-finite verbs, and
non-restrictive relative clauses (marked by com-
mas). In all cases, the choice is motivated by
whether a discourse relation could hold between
the resulting segments.
</bodyText>
<sectionHeader confidence="0.998164" genericHeader="method">
4 Implementation
</sectionHeader>
<bodyText confidence="0.999984863636364">
The core of the implementation involves the con-
struction of 12 syntactically-based segmentation
rules, along with a few lexical rules involving a list
of stop phrases, discourse cue phrases and word-
level parts of speech (POS) tags. First, paragraph
boundaries and sentence boundaries using NIST’s
sentence segmenter1 are inserted. Second, a sta-
tistical parser applies POS tags and the sentence’s
syntactic tree is constructed. Our syntactic rules
are executed at this stage. Finally, lexical rules,
as well as rules that consider the parts-of-speech
for individual words, are applied. Segment bound-
aries are removed from phrases with a syntactic
structure resembling independent clauses that ac-
tually are used idiomatically, such as as it stands
or if you will. A list of phrasal discourse cues
(e.g., as soon as, in order to) are used to insert
boundaries not derivable from the parser’s output
(phrases that begin with in order to... are tagged as
PP rather than SBAR). Segmentation is also per-
formed within parentheticals (marked by paren-
theses or hyphens).
</bodyText>
<sectionHeader confidence="0.993682" genericHeader="method">
5 Data and Evaluation
</sectionHeader>
<subsectionHeader confidence="0.883434">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999982034482759">
The gold standard test set consists of 9 human-
annotated texts. The 9 documents include 3 texts
from the RST literature2, 3 online product reviews
from Epinions.com, and 3 Wall Street Journal ar-
ticles taken from the Penn Treebank. The texts av-
erage 21.2 sentences, with the longest text having
43 sentences and the shortest having 6 sentences,
for a total of 191 sentences and 340 discourse seg-
ments in the 9 gold-standard texts.
The texts were segmented by one of the au-
thors following guidelines that were established
from the project’s beginning and was used as the
gold standard. The annotator was not directly in-
volved in the coding of the segmenter. To ensure
the guidelines followed clear and sound principles,
a reliability study was performed. The guidelines
were given to two annotators, both graduate stu-
dents in Linguistics, that had no direct knowledge
of the project. They were asked to segment the 9
texts used in the evaluation.
Inter-annotator agreement across all three anno-
tators using Kappa was .85, showing a high level
of agreement. Using F-score, average agreement
of the two annotators against the gold standard was
also high at .86. The few disagreements were pri-
marily due to a lack of full understanding of the
guidelines (e.g., the guidelines specify to break ad-
junct clauses when they contain a verb, but one
of the annotators segmented prepositional phrases
</bodyText>
<footnote confidence="0.999675666666667">
1http://duc.nist.gov/duc2004/software/
duc2003.breakSent.tar.gz
2Available from the RST website http://www.sfu.ca/rst/
</footnote>
<page confidence="0.995565">
78
</page>
<table confidence="0.9995255">
Epinions Treebank Original RST Combined Total
System P R F P R F P R F P R F
Baseline .22 .70 .33 .27 .89 .41 .26 .90 .41 .25 .80 .38
SPADE (coarse) .59 .66 .63 .63 1.0 .77 .64 .76 .69 .61 .79 .69
SPADE (original) .36 .67 .46 .37 1.0 .54 .38 .76 .50 .37 .77 .50
Sundance .54 .56 .55 .53 .67 .59 .71 .47 .57 .56 .58 .57
SLSeg (Charniak) .97 .66 .79 .89 .86 .87 .94 .76 .84 .93 .74 .83
SLSeg (Stanford) .82 .74 .77 .82 .86 .84 .88 .71 .79 .83 .77 .80
</table>
<tableCaption confidence="0.999936">
Table 1: Comparison of segmenters
</tableCaption>
<bodyText confidence="0.9996722">
that had a similar function to a full clause). With
high inter-annotator agreement (and with any dis-
agreements and errors resolved), we proceeded to
use the co-author’s segmentations as the gold stan-
dard.
</bodyText>
<subsectionHeader confidence="0.988059">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.991197970588235">
The evaluation uses standard precision, recall and
F-score to compute correctly inserted segment
boundaries (we do not consider sentence bound-
aries since that would inflate the scores). Precision
is the number of boundaries in agreement with the
gold standard. Recall is the total number of bound-
aries correct in the system’s output divided by the
number of total boundaries in the gold standard.
We compare the output of SLSeg to SPADE.
Since SPADE is trained on RST-DT, it inserts seg-
ment boundaries that are different from what our
annotation guidelines prescribe. To provide a fair
comparison, we implement a coarse version of
SPADE where segment boundaries prescribed by
the RST-DT guidelines, but not part of our seg-
mentation guidelines, are manually removed. This
version leads to increased precision while main-
taining identical recall, thus improving F-score.
In addition to SPADE, we also used the Sun-
dance parser (Riloff and Phillips, 2004) in our
evaluation. Sundance is a shallow parser which
provides clause segmentation on top of a basic
word-tagging and phrase-chunking system. Since
Sundance clauses are also too fine-grained for our
purposes, we use a few simple rules to collapse
clauses that are unlikely to meet our definition of
EDU. The baseline segmenter in Table 1 inserts
segment boundaries before and after all instances
of S, SBAR, SQ, SINV, SBARQ from the syntac-
tic parse (text spans that represent full clauses able
to stand alone as sentential units). Finally, two
parsers are compared for their effect on segmenta-
tion quality: Charniak (Charniak, 2000) and Stan-
ford (Klein and Manning, 2003).
</bodyText>
<subsectionHeader confidence="0.998767">
5.3 Qualitative Comparison
</subsectionHeader>
<bodyText confidence="0.999984">
Comparing the outputs of SLSeg and SPADE on
the Epinions.com texts illustrates key differences
between the two approaches.
[Luckily we bought the extended pro-
tection plans from Lowe’s,] # [so we
are waiting] [for Whirlpool to decide]
[if they want to do the costly repair] [or
provide us with a new machine].
In this example, SLSeg inserts a single bound-
ary (#) before the word so, whereas SPADE in-
serts four boundaries (indicated by square brack-
ets). Our breaks err on the side of preserving se-
mantic coherence, e.g., the segment for Whirlpool
to decide depends crucially on the adjacent seg-
ments for its meaning. In our opinion, the rela-
tions between these segments are properly the do-
main of a semantic, but not a discourse, parser. A
clearer example that illustrates the pitfalls of fine-
grained discourse segmenting is shown in the fol-
lowing output from SPADE:
[The thing] [that caught my attention
was the fact] [that these fantasy novels
were marketed...]
Because the segments are a restrictive relative
clause and a complement clause, respectively,
SLSeg does not insert any segment boundaries.
</bodyText>
<sectionHeader confidence="0.999913" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999272666666667">
Results are shown in Table 1. The combined in-
formal and formal texts show SLSeg (using Char-
niak’s parser) with high precision; however, our
overall recall was lower than both SPADE and the
baseline. The performance of SLSeg on the in-
formal and formal texts is similar to our perfor-
</bodyText>
<page confidence="0.996574">
79
</page>
<bodyText confidence="0.999828">
mance overall: high precision, nearly identical re-
call. Our system outperforms all the other systems
in both precision and F-score, confirming our hy-
pothesis that adapting an existing system would
not provide the high-quality discourse segments
we require.
The results of using the Stanford parser as an
alternative to the Charniak parser show that the
performance of our system is parser-independent.
High F-score in the Treebank data can be at-
tributed to the parsers having been trained on Tree-
bank. Since SPADE also utilizes the Charniak
parser, the results are comparable.
Additionally, we compared SLSeg and SPADE
to the original RST segmentations of the three
RST texts taken from RST literature. Performance
was similar to that of our own annotations, with
SLSeg achieving an F-score of .79, and SPADE
attaining .38. This demonstrates that our approach
to segmentation is more consistent with the origi-
nal RST guidelines.
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999992703703704">
We have shown that SLSeg, a conservative rule-
based segmenter that inserts fewer discourse
boundaries, leads to higher precision compared to
a statistical segmenter. This higher precision does
not come at the expense of a significant loss in
recall, as evidenced by a higher F-score. Unlike
statistical parsers, our system requires no training
when porting to a new domain.
All software and data are available3. The
discourse-related data includes: a list of clause-
like phrases that are in fact discourse markers
(e.g., if you will, mind you); a list of verbs used
in to-infinitival and if complement clauses that
should not be treated as separate discourse seg-
ments (e.g., decide in I decided to leave the car
at home); a list of unambiguous lexical cues for
segment boundary insertion; and a list of attribu-
tive/cognitive verbs (e.g., think, said) used to pre-
vent segmentation of floating attributive clauses.
Future work involves studying the robustness of
our discourse segments on other corpora, such as
formal texts from the medical domain and other
informal texts. Also to be investigated is a quan-
titative study of the effects of high-precision/low-
recall vs. low-precision/high-recall segmenters on
the construction of discourse trees. Besides its use
in automatic discourse parsing, the system could
</bodyText>
<footnote confidence="0.625685">
3http://www.sfu.ca/÷mtaboada/research/SLSeg.html
</footnote>
<bodyText confidence="0.998297">
assist manual annotators by providing a set of dis-
course segments as starting point for manual an-
notation of discourse relations.
</bodyText>
<sectionHeader confidence="0.998452" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999939511111111">
Lynn Carlson and Daniel Marcu. 2001. Discourse
Tagging Reference Manual. ISI Technical Report
ISI-TR-545.
Lynn Carlson, Daniel Marcu and Mary E. Okurowski.
2002. RST Discourse Treebank. Philadelphia, PA:
Linguistic Data Consortium.
Eugene Charniak. 2000. A Maximum-Entropy In-
spired Parser. Proc. of NAACL, pp. 132–139. Seat-
tle, WA.
Barbara J. Grosz and Candace L. Sidner. 1986. At-
tention, Intentions, and the Structure of Discourse.
Computational Linguistics, 12:175–204.
Dan Klein and Christopher D. Manning. 2003. Fast
Exact Inference with a Factored Model for Natu-
ral Language Parsing. Advances in NIPS 15 (NIPS
2002), Cambridge, MA: MIT Press, pp. 3–10.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a Functional
Theory of Text Organization. Text, 8:243–281.
Daniel Marcu. 2000. The Theory and Practice of
Discourse Parsing and Summarization. MIT Press,
Cambridge, MA.
Rebecca J. Passonneau and Diane J. Litman. 1997.
Discourse Segmentation by Human and Automated
Means. Computational Linguistics, 23(1):103–139.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind
Joshi and Bonnie Webber. 2006. Attribution and its
Annotation in the Penn Discourse TreeBank. Traite-
ment Automatique des Langues, 47(2):43–63.
Ellen Riloff and William Phillips. 2004. An Introduc-
tion to the Sundance and AutoSlog Systems. Univer-
sity of Utah Technical Report #UUCS-04-015.
Radu Soricut and Daniel Marcu. 2003. Sentence Level
Discourse Parsing Using Syntactic and Lexical In-
formation. Proc. of HLT-NAACL, pp. 149–156. Ed-
monton, Canada.
Rajen Subba and Barbara Di Eugenio. 2007. Auto-
matic Discourse Segmentation Using Neural Net-
works. Proc. of the 11th Workshop on the Se-
mantics and Pragmatics of Dialogue, pp. 189–190.
Rovereto, Italy.
Huong Le Thanh, Geetha Abeysinghe, and Christian
Huyck. 2004. Automated Discourse Segmentation
by Syntactic Information and Cue Phrases. Proc. of
IASTED. Innsbruck, Austria.
</reference>
<page confidence="0.998249">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.016178">
<title confidence="0.961304">A Syntactic and Lexical-Based Discourse Segmenter</title>
<affiliation confidence="0.9906045">School of Computing Science Simon Fraser University</affiliation>
<address confidence="0.993109">Burnaby, BC, Canada</address>
<email confidence="0.992466">mta45@sfu.ca</email>
<author confidence="0.999966">Julian Brooke</author>
<affiliation confidence="0.997076">Department of Linguistics Simon Fraser University</affiliation>
<address confidence="0.998621">Burnaby, BC, Canada</address>
<email confidence="0.994086">jab18@sfu.ca</email>
<author confidence="0.985587">Maite Taboada</author>
<affiliation confidence="0.997097">Department of Linguistics Simon Fraser University</affiliation>
<address confidence="0.997013">Burnaby, BC, Canada</address>
<email confidence="0.998398">mtaboada@sfu.ca</email>
<abstract confidence="0.990660287539936">We present a syntactic and lexically based discourse segmenter (SLSeg) that is designed to avoid the common problem of over-segmenting text. Segmentation is the step in a discourse parser, a system that constructs discourse trees from elementary discourse units. We compare SLSeg to a probabilistic segmenter, showing that a conservative approach increases precision at the expense of recall, while retaining a high F-score across both formal and informal texts. Discourse segmentation is the process of decomposing discourse into elementary discourse units (EDUs), which may be simple sentences or clauses in a complex sentence, and from which discourse trees are constructed. In this sense, we are performing low-level discourse segmentation, as opposed to segmenting text into chunks or topics (e.g., Passonneau and Litman (1997)). Since is the stage of discourse parsing, quality discourse segments are critical to building quality discourse representations (Soricut and Marcu, 2003). Our objective is to construct a discourse segmenter that is robust in handling both formal (newswire) and informal (online reviews) texts, while minimizing the insertion of incorrect discourse boundaries. Robustness is achieved by constructing discourse segments in a principled way using syntactic and lexical information. Our approach employs a set of rules for inserting segment boundaries based on the syntax of each sentence. The segment boundaries are then by using lexical information that work was supported by an NSERC Discovery Grant (261104-2008) to Maite Taboada. We thank Angela Cooper and Morgan Mameni for their help with the reliability study. takes into consideration lexical cues, including multi-word expressions. We also identify clauses that are parsed as discourse segments, but are not in fact independent discourse units, and join them to the matrix clause. Most parsers can break down a sentence into constituent clauses, approaching the type of output that we need as input to a discourse parser. The segments produced by a parser, however, are for discourse purposes, breaking off complement and other clauses that are not in a discourse relation to any other segment. For this reason, we have implemented our own segmenter, utilizing the output of a standard parser. The purpose of this paper is to describe our syntactic and lexical-based segmenter (SLSeg), demonstrate its performance against state-of-the-art systems, and make it available to the wider community. 2 Related Work Soricut and Marcu (2003) construct a statistical discourse segmenter as part of their sentence-level discourse parser (SPADE), the only implementation available for our comparison. SPADE is trained on the RST Discourse Treebank (Carlson et al., 2002). The probabilities for segment boundary insertion are learned using lexical and syntactic features. Subba and Di Eugenio (2007) use neural networks trained on RST-DT for discourse segmentation. They obtain an F-score of 84.41% (86.07% using a perfect parse), whereas SPADE achieved 83.1% and 84.7% respectively. Thanh et al. (2004) construct a rule-based segmenter, employing manually annotated parses from the Penn Treebank. Our approach is conceptually similar, but we are only concerned with established discourse relations, i.e., we avoid poby preserving NP constituency. 77 of the ACL-IJCNLP 2009 Conference Short pages 77–80, Singapore, 4 August 2009. ACL and AFNLP 3 Principles For Discourse Segmentation Our primary concern is to capture interesting discourse relations, rather than all possible relations, capturing more relations such as Condition, Evidence or Purpose, rather than more general and less informative relations such as Elaboor Joint, as in Rhetorical Structure Theory (Mann and Thompson, 1988). By having a of an elementary discourse unit (EDU), this approach increases precision at the expense of recall. Grammatical units that are candidates for discourse segments are clauses and sentences. Our basic principles for discourse segmentation follow the proposals in RST as to what a minimal unit of text is. Many of our differences with Carland Marcu (2001), who EDUs for the RST Discourse Treebank (Carlson et al., 2002), are due to the fact that we adhere closer to the original RST proposals (Mann and Thompson, 1988), as ‘spans’ adjunct clauses, rather than complement (subject and object) clauses. In particular, we propose that complements of atand cognitive verbs said are not EDUs. We preserve conby not breaking at direct speech he Reported and direct speech are certainly important in discourse (Prasad et al., 2006); we do not believe, however, that they enter discourse relations of the type that RST attempts to capture. In general, adjunct, but not complement clauses are discourse units. We require all discourse segments to contain a verb. Whenever a discourse boundary is inserted, the two newly created segments must each contain a verb. We segment coordinated clauses (but not coordinated VPs), adjunct with either or verbs, and non-restrictive relative clauses (marked by commas). In all cases, the choice is motivated by whether a discourse relation could hold between the resulting segments. 4 Implementation The core of the implementation involves the construction of 12 syntactically-based segmentation rules, along with a few lexical rules involving a list of stop phrases, discourse cue phrases and wordlevel parts of speech (POS) tags. First, paragraph boundaries and sentence boundaries using NIST’s are inserted. Second, a statistical parser applies POS tags and the sentence’s syntactic tree is constructed. Our syntactic rules are executed at this stage. Finally, lexical rules, as well as rules that consider the parts-of-speech for individual words, are applied. Segment boundaries are removed from phrases with a syntactic structure resembling independent clauses that acare used idiomatically, such as it stands you A list of phrasal discourse cues soon as, in order are used to insert boundaries not derivable from the parser’s output that begin with order to... tagged as PP rather than SBAR). Segmentation is also performed within parentheticals (marked by parentheses or hyphens). 5 Data and Evaluation 5.1 Data The gold standard test set consists of 9 humanannotated texts. The 9 documents include 3 texts the RST 3 online product reviews from Epinions.com, and 3 Wall Street Journal articles taken from the Penn Treebank. The texts average 21.2 sentences, with the longest text having 43 sentences and the shortest having 6 sentences, for a total of 191 sentences and 340 discourse segments in the 9 gold-standard texts. The texts were segmented by one of the authors following guidelines that were established from the project’s beginning and was used as the gold standard. The annotator was not directly involved in the coding of the segmenter. To ensure the guidelines followed clear and sound principles, a reliability study was performed. The guidelines were given to two annotators, both graduate students in Linguistics, that had no direct knowledge of the project. They were asked to segment the 9 texts used in the evaluation. Inter-annotator agreement across all three annotators using Kappa was .85, showing a high level of agreement. Using F-score, average agreement of the two annotators against the gold standard was also high at .86. The few disagreements were primarily due to a lack of full understanding of the guidelines (e.g., the guidelines specify to break adjunct clauses when they contain a verb, but one of the annotators segmented prepositional phrases duc2003.breakSent.tar.gz from the RST website http://www.sfu.ca/rst/ 78 Epinions Treebank Original RST Combined Total System P R F P R F P R F P R F Baseline .22 .70 .33 .27 .89 .41 .26 .90 .41 .25 .80 .38 SPADE (coarse) .59 .66 .63 .63 1.0 .77 .64 .76 .69 .61 .79 .69 SPADE (original) .36 .67 .46 .37 1.0 .54 .38 .76 .50 .37 .77 .50 Sundance .54 .56 .55 .53 .67 .59 .71 .47 .57 .56 .58 .57 SLSeg (Charniak) .97 .66 .79 .89 .86 .87 .94 .76 .84 .93 .74 .83 SLSeg (Stanford) .82 .74 .77 .82 .86 .84 .88 .71 .79 .83 .77 .80 Table 1: Comparison of segmenters that had a similar function to a full clause). With high inter-annotator agreement (and with any disagreements and errors resolved), we proceeded to use the co-author’s segmentations as the gold standard. 5.2 Evaluation The evaluation uses standard precision, recall and F-score to compute correctly inserted segment boundaries (we do not consider sentence boundsince that would the scores). Precision is the number of boundaries in agreement with the gold standard. Recall is the total number of boundaries correct in the system’s output divided by the number of total boundaries in the gold standard. We compare the output of SLSeg to SPADE. Since SPADE is trained on RST-DT, it inserts segment boundaries that are different from what our annotation guidelines prescribe. To provide a fair comparison, we implement a coarse version of SPADE where segment boundaries prescribed by the RST-DT guidelines, but not part of our segmentation guidelines, are manually removed. This version leads to increased precision while maintaining identical recall, thus improving F-score. In addition to SPADE, we also used the Sundance parser (Riloff and Phillips, 2004) in our evaluation. Sundance is a shallow parser which provides clause segmentation on top of a basic word-tagging and phrase-chunking system. Since clauses are also too for our purposes, we use a few simple rules to collapse that are unlikely to meet our of EDU. The baseline segmenter in Table 1 inserts segment boundaries before and after all instances of S, SBAR, SQ, SINV, SBARQ from the syntactic parse (text spans that represent full clauses able to stand alone as sentential units). Finally, two parsers are compared for their effect on segmentation quality: Charniak (Charniak, 2000) and Stanford (Klein and Manning, 2003). 5.3 Qualitative Comparison Comparing the outputs of SLSeg and SPADE on the Epinions.com texts illustrates key differences between the two approaches. [Luckily we bought the extended protection plans from Lowe’s,] # [so we are waiting] [for Whirlpool to decide] [if they want to do the costly repair] [or provide us with a new machine]. In this example, SLSeg inserts a single bound- (#) before the word whereas SPADE inserts four boundaries (indicated by square brackets). Our breaks err on the side of preserving secoherence, e.g., the segment Whirlpool decide crucially on the adjacent segments for its meaning. In our opinion, the relations between these segments are properly the domain of a semantic, but not a discourse, parser. A example that illustrates the pitfalls of grained discourse segmenting is shown in the following output from SPADE: [The thing] [that caught my attention was the fact] [that these fantasy novels were marketed...] Because the segments are a restrictive relative clause and a complement clause, respectively, SLSeg does not insert any segment boundaries. 6 Results Results are shown in Table 1. The combined informal and formal texts show SLSeg (using Charniak’s parser) with high precision; however, our overall recall was lower than both SPADE and the baseline. The performance of SLSeg on the inand formal texts is similar to our perfor- 79 mance overall: high precision, nearly identical recall. Our system outperforms all the other systems both precision and F-score, our hypothesis that adapting an existing system would not provide the high-quality discourse segments we require. The results of using the Stanford parser as an alternative to the Charniak parser show that the performance of our system is parser-independent. High F-score in the Treebank data can be attributed to the parsers having been trained on Treebank. Since SPADE also utilizes the Charniak parser, the results are comparable. Additionally, we compared SLSeg and SPADE to the original RST segmentations of the three RST texts taken from RST literature. Performance was similar to that of our own annotations, with SLSeg achieving an F-score of .79, and SPADE attaining .38. This demonstrates that our approach to segmentation is more consistent with the original RST guidelines. 7 Discussion We have shown that SLSeg, a conservative rulebased segmenter that inserts fewer discourse boundaries, leads to higher precision compared to a statistical segmenter. This higher precision does come at the expense of a loss in recall, as evidenced by a higher F-score. Unlike statistical parsers, our system requires no training when porting to a new domain. software and data are The discourse-related data includes: a list of clauselike phrases that are in fact discourse markers you will, mind a list of verbs used and clauses that should not be treated as separate discourse seg- (e.g., decided to leave the car a list of unambiguous lexical cues for segment boundary insertion; and a list of attribuverbs (e.g., used to presegmentation of attributive clauses. Future work involves studying the robustness of our discourse segments on other corpora, such as formal texts from the medical domain and other informal texts. Also to be investigated is a quantitative study of the effects of high-precision/lowrecall vs. low-precision/high-recall segmenters on the construction of discourse trees. Besides its use in automatic discourse parsing, the system could assist manual annotators by providing a set of discourse segments as starting point for manual annotation of discourse relations.</abstract>
<note confidence="0.889524404255319">References Carlson and Daniel Marcu. 2001. Reference Manual. Technical Report ISI-TR-545. Lynn Carlson, Daniel Marcu and Mary E. Okurowski. Discourse Treebank. PA: Linguistic Data Consortium. Eugene Charniak. 2000. A Maximum-Entropy In- Parser. of pp. 132–139. Seattle, WA. Barbara J. Grosz and Candace L. Sidner. 1986. Attention, Intentions, and the Structure of Discourse. 12:175–204. Dan Klein and Christopher D. Manning. 2003. Fast Exact Inference with a Factored Model for Natu- Language Parsing. in NIPS 15 (NIPS Cambridge, MA: MIT Press, pp. 3–10. William C. Mann and Sandra A. Thompson. 1988. Rhetorical Structure Theory: Toward a Functional of Text Organization. 8:243–281. Marcu. 2000. Theory and Practice of Parsing and Summarization. Press, Cambridge, MA. Rebecca J. Passonneau and Diane J. Litman. 1997. Discourse Segmentation by Human and Automated 23(1):103–139. Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind Joshi and Bonnie Webber. 2006. Attribution and its in the Penn Discourse TreeBank. Traite- Automatique des 47(2):43–63. Riloff and William Phillips. 2004. Introducto the Sundance and AutoSlog Systems. University of Utah Technical Report #UUCS-04-015. Radu Soricut and Daniel Marcu. 2003. Sentence Level Discourse Parsing Using Syntactic and Lexical Inof pp. 149–156. Edmonton, Canada. Rajen Subba and Barbara Di Eugenio. 2007. Automatic Discourse Segmentation Using Neural Netof the 11th Workshop on the Seand Pragmatics of pp. 189–190. Rovereto, Italy. Huong Le Thanh, Geetha Abeysinghe, and Christian Huyck. 2004. Automated Discourse Segmentation Syntactic Information and Cue Phrases. of Innsbruck, Austria. 80</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
</authors>
<title>Discourse Tagging Reference Manual.</title>
<date>2001</date>
<tech>ISI Technical Report ISI-TR-545.</tech>
<contexts>
<context position="4685" citStr="Carlson and Marcu (2001)" startWordPosition="703" endWordPosition="707">s, i.e., capturing more specific relations such as Condition, Evidence or Purpose, rather than more general and less informative relations such as Elaboration or Joint, as defined in Rhetorical Structure Theory (Mann and Thompson, 1988). By having a stricter definition of an elementary discourse unit (EDU), this approach increases precision at the expense of recall. Grammatical units that are candidates for discourse segments are clauses and sentences. Our basic principles for discourse segmentation follow the proposals in RST as to what a minimal unit of text is. Many of our differences with Carlson and Marcu (2001), who defined EDUs for the RST Discourse Treebank (Carlson et al., 2002), are due to the fact that we adhere closer to the original RST proposals (Mann and Thompson, 1988), which defined as ‘spans’ adjunct clauses, rather than complement (subject and object) clauses. In particular, we propose that complements of attributive and cognitive verbs (He said (that)..., I think (that)...) are not EDUs. We preserve consistency by not breaking at direct speech (“X,” he said.). Reported and direct speech are certainly important in discourse (Prasad et al., 2006); we do not believe, however, that they en</context>
</contexts>
<marker>Carlson, Marcu, 2001</marker>
<rawString>Lynn Carlson and Daniel Marcu. 2001. Discourse Tagging Reference Manual. ISI Technical Report ISI-TR-545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary E Okurowski</author>
</authors>
<date>2002</date>
<booktitle>RST Discourse Treebank.</booktitle>
<institution>Linguistic Data Consortium.</institution>
<location>Philadelphia, PA:</location>
<contexts>
<context position="3185" citStr="Carlson et al., 2002" startWordPosition="473" endWordPosition="476">at are not in a discourse relation to any other segment. For this reason, we have implemented our own segmenter, utilizing the output of a standard parser. The purpose of this paper is to describe our syntactic and lexical-based segmenter (SLSeg), demonstrate its performance against state-of-the-art systems, and make it available to the wider community. 2 Related Work Soricut and Marcu (2003) construct a statistical discourse segmenter as part of their sentence-level discourse parser (SPADE), the only implementation available for our comparison. SPADE is trained on the RST Discourse Treebank (Carlson et al., 2002). The probabilities for segment boundary insertion are learned using lexical and syntactic features. Subba and Di Eugenio (2007) use neural networks trained on RST-DT for discourse segmentation. They obtain an F-score of 84.41% (86.07% using a perfect parse), whereas SPADE achieved 83.1% and 84.7% respectively. Thanh et al. (2004) construct a rule-based segmenter, employing manually annotated parses from the Penn Treebank. Our approach is conceptually similar, but we are only concerned with established discourse relations, i.e., we avoid potential same-unit relations by preserving NP constitue</context>
<context position="4757" citStr="Carlson et al., 2002" startWordPosition="716" endWordPosition="719">urpose, rather than more general and less informative relations such as Elaboration or Joint, as defined in Rhetorical Structure Theory (Mann and Thompson, 1988). By having a stricter definition of an elementary discourse unit (EDU), this approach increases precision at the expense of recall. Grammatical units that are candidates for discourse segments are clauses and sentences. Our basic principles for discourse segmentation follow the proposals in RST as to what a minimal unit of text is. Many of our differences with Carlson and Marcu (2001), who defined EDUs for the RST Discourse Treebank (Carlson et al., 2002), are due to the fact that we adhere closer to the original RST proposals (Mann and Thompson, 1988), which defined as ‘spans’ adjunct clauses, rather than complement (subject and object) clauses. In particular, we propose that complements of attributive and cognitive verbs (He said (that)..., I think (that)...) are not EDUs. We preserve consistency by not breaking at direct speech (“X,” he said.). Reported and direct speech are certainly important in discourse (Prasad et al., 2006); we do not believe, however, that they enter discourse relations of the type that RST attempts to capture. In gen</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2002</marker>
<rawString>Lynn Carlson, Daniel Marcu and Mary E. Okurowski. 2002. RST Discourse Treebank. Philadelphia, PA: Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A Maximum-Entropy Inspired Parser.</title>
<date>2000</date>
<booktitle>Proc. of NAACL,</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="10761" citStr="Charniak, 2000" startWordPosition="1707" endWordPosition="1708">. Sundance is a shallow parser which provides clause segmentation on top of a basic word-tagging and phrase-chunking system. Since Sundance clauses are also too fine-grained for our purposes, we use a few simple rules to collapse clauses that are unlikely to meet our definition of EDU. The baseline segmenter in Table 1 inserts segment boundaries before and after all instances of S, SBAR, SQ, SINV, SBARQ from the syntactic parse (text spans that represent full clauses able to stand alone as sentential units). Finally, two parsers are compared for their effect on segmentation quality: Charniak (Charniak, 2000) and Stanford (Klein and Manning, 2003). 5.3 Qualitative Comparison Comparing the outputs of SLSeg and SPADE on the Epinions.com texts illustrates key differences between the two approaches. [Luckily we bought the extended protection plans from Lowe’s,] # [so we are waiting] [for Whirlpool to decide] [if they want to do the costly repair] [or provide us with a new machine]. In this example, SLSeg inserts a single boundary (#) before the word so, whereas SPADE inserts four boundaries (indicated by square brackets). Our breaks err on the side of preserving semantic coherence, e.g., the segment f</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A Maximum-Entropy Inspired Parser. Proc. of NAACL, pp. 132–139. Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<date>1986</date>
<booktitle>Attention, Intentions, and the Structure of Discourse. Computational Linguistics,</booktitle>
<pages>12--175</pages>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, Intentions, and the Structure of Discourse. Computational Linguistics, 12:175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast Exact Inference with a Factored Model for Natural Language Parsing.</title>
<date>2003</date>
<booktitle>Advances in NIPS 15 (NIPS 2002),</booktitle>
<pages>3--10</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="10800" citStr="Klein and Manning, 2003" startWordPosition="1712" endWordPosition="1715"> which provides clause segmentation on top of a basic word-tagging and phrase-chunking system. Since Sundance clauses are also too fine-grained for our purposes, we use a few simple rules to collapse clauses that are unlikely to meet our definition of EDU. The baseline segmenter in Table 1 inserts segment boundaries before and after all instances of S, SBAR, SQ, SINV, SBARQ from the syntactic parse (text spans that represent full clauses able to stand alone as sentential units). Finally, two parsers are compared for their effect on segmentation quality: Charniak (Charniak, 2000) and Stanford (Klein and Manning, 2003). 5.3 Qualitative Comparison Comparing the outputs of SLSeg and SPADE on the Epinions.com texts illustrates key differences between the two approaches. [Luckily we bought the extended protection plans from Lowe’s,] # [so we are waiting] [for Whirlpool to decide] [if they want to do the costly repair] [or provide us with a new machine]. In this example, SLSeg inserts a single boundary (#) before the word so, whereas SPADE inserts four boundaries (indicated by square brackets). Our breaks err on the side of preserving semantic coherence, e.g., the segment for Whirlpool to decide depends cruciall</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Fast Exact Inference with a Factored Model for Natural Language Parsing. Advances in NIPS 15 (NIPS 2002), Cambridge, MA: MIT Press, pp. 3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Toward a Functional Theory of Text Organization.</title>
<date>1988</date>
<tech>Text, 8:243–281.</tech>
<contexts>
<context position="4297" citStr="Mann and Thompson, 1988" startWordPosition="640" endWordPosition="643">ed with established discourse relations, i.e., we avoid potential same-unit relations by preserving NP constituency. 77 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 77–80, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP 3 Principles For Discourse Segmentation Our primary concern is to capture interesting discourse relations, rather than all possible relations, i.e., capturing more specific relations such as Condition, Evidence or Purpose, rather than more general and less informative relations such as Elaboration or Joint, as defined in Rhetorical Structure Theory (Mann and Thompson, 1988). By having a stricter definition of an elementary discourse unit (EDU), this approach increases precision at the expense of recall. Grammatical units that are candidates for discourse segments are clauses and sentences. Our basic principles for discourse segmentation follow the proposals in RST as to what a minimal unit of text is. Many of our differences with Carlson and Marcu (2001), who defined EDUs for the RST Discourse Treebank (Carlson et al., 2002), are due to the fact that we adhere closer to the original RST proposals (Mann and Thompson, 1988), which defined as ‘spans’ adjunct clause</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical Structure Theory: Toward a Functional Theory of Text Organization. Text, 8:243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Diane J Litman</author>
</authors>
<title>Discourse Segmentation by Human and Automated Means.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="1210" citStr="Passonneau and Litman (1997)" startWordPosition="172" endWordPosition="175">rse trees from elementary discourse units. We compare SLSeg to a probabilistic segmenter, showing that a conservative approach increases precision at the expense of recall, while retaining a high F-score across both formal and informal texts. 1 Introduction* Discourse segmentation is the process of decomposing discourse into elementary discourse units (EDUs), which may be simple sentences or clauses in a complex sentence, and from which discourse trees are constructed. In this sense, we are performing low-level discourse segmentation, as opposed to segmenting text into chunks or topics (e.g., Passonneau and Litman (1997)). Since segmentation is the first stage of discourse parsing, quality discourse segments are critical to building quality discourse representations (Soricut and Marcu, 2003). Our objective is to construct a discourse segmenter that is robust in handling both formal (newswire) and informal (online reviews) texts, while minimizing the insertion of incorrect discourse boundaries. Robustness is achieved by constructing discourse segments in a principled way using syntactic and lexical information. Our approach employs a set of rules for inserting segment boundaries based on the syntax of each sen</context>
</contexts>
<marker>Passonneau, Litman, 1997</marker>
<rawString>Rebecca J. Passonneau and Diane J. Litman. 1997. Discourse Segmentation by Human and Automated Means. Computational Linguistics, 23(1):103–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<date>2006</date>
<booktitle>Attribution and its Annotation in the Penn Discourse TreeBank. Traitement Automatique des Langues,</booktitle>
<pages>47--2</pages>
<contexts>
<context position="5243" citStr="Prasad et al., 2006" startWordPosition="795" endWordPosition="798">text is. Many of our differences with Carlson and Marcu (2001), who defined EDUs for the RST Discourse Treebank (Carlson et al., 2002), are due to the fact that we adhere closer to the original RST proposals (Mann and Thompson, 1988), which defined as ‘spans’ adjunct clauses, rather than complement (subject and object) clauses. In particular, we propose that complements of attributive and cognitive verbs (He said (that)..., I think (that)...) are not EDUs. We preserve consistency by not breaking at direct speech (“X,” he said.). Reported and direct speech are certainly important in discourse (Prasad et al., 2006); we do not believe, however, that they enter discourse relations of the type that RST attempts to capture. In general, adjunct, but not complement clauses are discourse units. We require all discourse segments to contain a verb. Whenever a discourse boundary is inserted, the two newly created segments must each contain a verb. We segment coordinated clauses (but not coordinated VPs), adjunct clauses with either finite or non-finite verbs, and non-restrictive relative clauses (marked by commas). In all cases, the choice is motivated by whether a discourse relation could hold between the result</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Joshi, Webber, 2006</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind Joshi and Bonnie Webber. 2006. Attribution and its Annotation in the Penn Discourse TreeBank. Traitement Automatique des Langues, 47(2):43–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>William Phillips</author>
</authors>
<title>An Introduction to the Sundance and AutoSlog Systems.</title>
<date>2004</date>
<tech>Technical Report #UUCS-04-015.</tech>
<institution>University of Utah</institution>
<contexts>
<context position="10128" citStr="Riloff and Phillips, 2004" startWordPosition="1603" endWordPosition="1606">s output divided by the number of total boundaries in the gold standard. We compare the output of SLSeg to SPADE. Since SPADE is trained on RST-DT, it inserts segment boundaries that are different from what our annotation guidelines prescribe. To provide a fair comparison, we implement a coarse version of SPADE where segment boundaries prescribed by the RST-DT guidelines, but not part of our segmentation guidelines, are manually removed. This version leads to increased precision while maintaining identical recall, thus improving F-score. In addition to SPADE, we also used the Sundance parser (Riloff and Phillips, 2004) in our evaluation. Sundance is a shallow parser which provides clause segmentation on top of a basic word-tagging and phrase-chunking system. Since Sundance clauses are also too fine-grained for our purposes, we use a few simple rules to collapse clauses that are unlikely to meet our definition of EDU. The baseline segmenter in Table 1 inserts segment boundaries before and after all instances of S, SBAR, SQ, SINV, SBARQ from the syntactic parse (text spans that represent full clauses able to stand alone as sentential units). Finally, two parsers are compared for their effect on segmentation q</context>
</contexts>
<marker>Riloff, Phillips, 2004</marker>
<rawString>Ellen Riloff and William Phillips. 2004. An Introduction to the Sundance and AutoSlog Systems. University of Utah Technical Report #UUCS-04-015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Sentence Level Discourse Parsing Using Syntactic and Lexical Information.</title>
<date>2003</date>
<booktitle>Proc. of HLT-NAACL,</booktitle>
<pages>149--156</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1384" citStr="Soricut and Marcu, 2003" startWordPosition="196" endWordPosition="199"> retaining a high F-score across both formal and informal texts. 1 Introduction* Discourse segmentation is the process of decomposing discourse into elementary discourse units (EDUs), which may be simple sentences or clauses in a complex sentence, and from which discourse trees are constructed. In this sense, we are performing low-level discourse segmentation, as opposed to segmenting text into chunks or topics (e.g., Passonneau and Litman (1997)). Since segmentation is the first stage of discourse parsing, quality discourse segments are critical to building quality discourse representations (Soricut and Marcu, 2003). Our objective is to construct a discourse segmenter that is robust in handling both formal (newswire) and informal (online reviews) texts, while minimizing the insertion of incorrect discourse boundaries. Robustness is achieved by constructing discourse segments in a principled way using syntactic and lexical information. Our approach employs a set of rules for inserting segment boundaries based on the syntax of each sentence. The segment boundaries are then further refined by using lexical information that *This work was supported by an NSERC Discovery Grant (261104-2008) to Maite Taboada. </context>
<context position="2959" citStr="Soricut and Marcu (2003)" startWordPosition="440" endWordPosition="443">onstituent clauses, approaching the type of output that we need as input to a discourse parser. The segments produced by a parser, however, are too fine-grained for discourse purposes, breaking off complement and other clauses that are not in a discourse relation to any other segment. For this reason, we have implemented our own segmenter, utilizing the output of a standard parser. The purpose of this paper is to describe our syntactic and lexical-based segmenter (SLSeg), demonstrate its performance against state-of-the-art systems, and make it available to the wider community. 2 Related Work Soricut and Marcu (2003) construct a statistical discourse segmenter as part of their sentence-level discourse parser (SPADE), the only implementation available for our comparison. SPADE is trained on the RST Discourse Treebank (Carlson et al., 2002). The probabilities for segment boundary insertion are learned using lexical and syntactic features. Subba and Di Eugenio (2007) use neural networks trained on RST-DT for discourse segmentation. They obtain an F-score of 84.41% (86.07% using a perfect parse), whereas SPADE achieved 83.1% and 84.7% respectively. Thanh et al. (2004) construct a rule-based segmenter, employi</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>Radu Soricut and Daniel Marcu. 2003. Sentence Level Discourse Parsing Using Syntactic and Lexical Information. Proc. of HLT-NAACL, pp. 149–156. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajen Subba</author>
<author>Barbara Di Eugenio</author>
</authors>
<title>Automatic Discourse Segmentation Using Neural Networks.</title>
<date>2007</date>
<booktitle>Proc. of the 11th Workshop on the Semantics and Pragmatics of Dialogue,</booktitle>
<pages>189--190</pages>
<location>Rovereto, Italy.</location>
<marker>Subba, Di Eugenio, 2007</marker>
<rawString>Rajen Subba and Barbara Di Eugenio. 2007. Automatic Discourse Segmentation Using Neural Networks. Proc. of the 11th Workshop on the Semantics and Pragmatics of Dialogue, pp. 189–190. Rovereto, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huong Le Thanh</author>
<author>Geetha Abeysinghe</author>
<author>Christian Huyck</author>
</authors>
<title>Automated Discourse Segmentation by Syntactic Information and Cue Phrases.</title>
<date>2004</date>
<booktitle>Proc. of IASTED.</booktitle>
<location>Innsbruck, Austria.</location>
<marker>Le Thanh, Abeysinghe, Huyck, 2004</marker>
<rawString>Huong Le Thanh, Geetha Abeysinghe, and Christian Huyck. 2004. Automated Discourse Segmentation by Syntactic Information and Cue Phrases. Proc. of IASTED. Innsbruck, Austria.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>