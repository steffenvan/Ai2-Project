<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9946035">
Discriminative Word Alignment by
Linear Modeling
</title>
<author confidence="0.999848">
Yang Liu*
</author>
<affiliation confidence="0.9766995">
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<author confidence="0.99077">
Qun Liu*
</author>
<affiliation confidence="0.987868">
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<author confidence="0.98576">
Shouxun Lin*
</author>
<affiliation confidence="0.9861115">
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<bodyText confidence="0.967246">
Word alignment plays an important role in many NLP tasks as it indicates the correspondence
between words in a parallel text. Although widely used to align large bilingual corpora, gen-
erative models are hard to extend to incorporate arbitrary useful linguistic information. This
article presents a discriminative framework for word alignment based on a linear model. Within
this framework, all knowledge sources are treated as feature functions, which depend on a source
language sentence, a target language sentence, and the alignment between them. We describe a
number offeatures that could produce symmetric alignments. Our model is easy to extend and
can be optimized with respect to evaluation metrics directly. The model achieves state-of-the-art
alignment quality on three word alignment shared tasks for five language pairs with varying
divergence and richness of resources. We further show that our approach improves translation
performance for various statistical machine translation systems.
</bodyText>
<sectionHeader confidence="0.99636" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.7499245">
Word alignment, which can be defined as an object for indicating the corresponding
words in a parallel text, was first introduced as an intermediate result of statistical
machine translation (Brown et al. 1993).
Consider the following Chinese sentence and its English translation:
+Q VXJNk ng$ff)A _*fIFI VT K
Zhongguo jianzhuye duiwaikaifang chengxian xin geju
</bodyText>
<note confidence="0.670374222222222">
The opening of China’s construction industry to the outside presents a new structure
* Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese
Academy of Sciences, No. 6 Kexueyuan South Road, Haidian District, P.O. Box 2704, Beijing 100190,
China. E-mail: {yliu, liuqun, sxlin}@ict.ac.cn.
Submission received: 13 September 2007; revised submission received: 7 January 2010; accepted for
publication: 21 February 2010.
© 2010 Association for Computational Linguistics
Published under Creative Commons Attribution-NonCommercial 3.0 Unported license
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.999112555555556">
The Chinese word Zhongguo is aligned to the English word China because they are
translations of one another. Similarly, the Chinese word xin is aligned to the English
word new. These connections are not necessarily one-to-one. For example, one Chinese
word jianzhuye corresponds to two English words construction industry. In addition, the
English words (e.g., opening ... to the outside) connected to a Chinese word (e.g., dui-
waikaifang) could be discontinuous. Figure 1 shows an alignment for this sentence pair.
The Chinese and English words are listed horizontally and vertically, respectively. They
are numbered to facilitate identification. The dark points indicate the correspondence
between the words in two languages. The goal of word alignment is to identify such
correspondences in a parallel text.
Word alignment plays an important role in many NLP tasks. In statistical machine
translation, word-aligned corpora serve as an excellent source for translation-related
knowledge. The estimation of translation model parameters usually relies heavily on
word-aligned corpora, not only for phrase-based and hierarchical phrase-based models
(Koehn, Och, and Marcu 2003; Och and Ney 2004; Chiang 2005, 2007), but also for
syntax-based models (Quirk, Menezes, and Cherry 2005; Galley et al. 2006; Liu, Liu,
and Lin 2006; Marcu et al. 2006). Besides machine translation, many applications for
word-aligned corpora have been suggested, including machine-assisted translation,
</bodyText>
<figureCaption confidence="0.718096">
Figure 1
</figureCaption>
<bodyText confidence="0.79590225">
Example of a word alignment between a Chinese–English sentence pair. The Chinese and
English words are listed horizontally and vertically, respectively. They are numbered to facilitate
identification. The dark points indicate the correspondences between the words in the two
languages.
</bodyText>
<page confidence="0.994611">
304
</page>
<note confidence="0.982282">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<bodyText confidence="0.999905833333333">
translation assessment and critiquing tools, text generation, bilingual lexigraphy, and
word sense disambiguation.
Various methods have been proposed for finding word alignments between parallel
texts. Among them, generative alignment models (Brown et al. 1993; Vogel and Ney
1996) have been widely used to produce word alignments for large bilingual corpora.
Describing the relationship of a bilingual sentence pair, a generative model treats word
alignment as a hidden process and maximizes the likelihood of a training corpus using
the expectation maximization (EM) algorithm. After the maximization process is com-
plete, the unknown model parameters are determined and the word alignments are set
to the maximum posterior predictions of the model.
However, one drawback of generative models is that they are hard to extend. Gen-
erative models usually impose strong independence assumptions between sub-models,
making it very difficult to incorporate arbitrary features explicitly. For example, when
considering whether to align two words, generative models cannot include information
about lexical and syntactic features such as part of speech and orthographic similarity in
an easy way. Such features would allow for more effective use of sparse data and result
in a model that is more robust in the presence of unseen words. Extending a generative
model requires that the interdependence of information sources be modeled explicitly,
which often makes the resulting system quite complex.
In this article, we introduce a discriminative framework for word alignment based
on the linear modeling approach. Within this framework, we treat all knowledge
sources as feature functions that depend on a source sentence, a target sentence,
and the alignment between them. Each feature function is associated with a feature
weight. The linear combination of features gives an overall score to each candidate
alignment. The best alignment is the one with the highest overall score. A linear
model not only allows for easy integration of new features, but also admits optimiz-
ing feature weights directly with respect to evaluation metrics. Experimental results
show that our approach improves both alignment quality and translation performance
significantly.
This article is organized as follows. Section 2 gives a formal description of our
model. We show how to train feature weights by taking evaluation metrics into account
and how to find the most probable alignment in an exponential search space efficiently.
Section 3 describes a number of features used in our experiments, focusing on the
features that produce symmetric alignments. In Section 4, we evaluate our model in
both alignment and translation tasks. Section 5 reviews previous work related to our
approach and the article closes with a conclusion in Section 6.
</bodyText>
<sectionHeader confidence="0.988044" genericHeader="keywords">
2. Approach
2.1 The Model
</sectionHeader>
<bodyText confidence="0.99987425">
Given a source language sentence f = f1, ... , fj, ... ,fJ and a target language sentence e =
e1,..., ei, ... , eI, we define a link l = (j, i) to exist if fj and ei are translations (or part of a
translation) of one another. Then, an alignment is defined as a subset of the Cartesian
product of the word positions:
</bodyText>
<equation confidence="0.611776">
a ⊆ {(j,i) : j = 1,...,J;i = 1,...,I} (1)
</equation>
<page confidence="0.994153">
305
</page>
<note confidence="0.792021">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.921902">
We propose a linear alignment model:
</bodyText>
<equation confidence="0.988779666666667">
M
score(f, e, a) = E λmhm(f, e, a) (2)
m=1
</equation>
<bodyText confidence="0.999646">
where hm(f, e, a) is a feature function and λm is its associated feature weight. The linear
combination of features gives an overall score score(f, e, a) to each candidate alignment
a for a given sentence pair (f, e).
</bodyText>
<subsectionHeader confidence="0.99117">
2.2 Training
</subsectionHeader>
<bodyText confidence="0.999989666666667">
To achieve good alignment quality, it is essential to find a good set of feature weights
λM1 . Before discussing how to train λM1 , we first describe two evaluation metrics that
measure alignment quality, because we will optimize λM1 with respect to them directly.
</bodyText>
<subsubsectionHeader confidence="0.833746">
2.2.1 Evaluation Metrics. The first metric is alignment error rate (AER), proposed by
</subsubsectionHeader>
<bodyText confidence="0.998133714285714">
Och and Ney (2003). AER has been used as official evaluation criterion in most word
alignment shared tasks. Och and Ney define two kinds of links in hand-aligned align-
ments: sure links for alignments that are unambiguous and possible links for ambiguous
alignments. Sure links usually connect content words such as Zhongguo and China.
In contrast, possible links often align words within idiomatic expressions and free
translations.
An AER score is given by
</bodyText>
<equation confidence="0.9587585">
AER(S,P,A)=1_ |A n S |+ |A n P |(3)
|A |+ |S|
</equation>
<bodyText confidence="0.999928333333333">
where S is a set of sure links in a reference alignment that is hand-aligned by human
experts, P is a set of possible links in the reference alignment, and A is a candidate
alignment. Note that S is a subset of P: S C_ P. The lower the AER score is, the better the
alignment quality is.
Although widely used, AER has been criticized for correlating poorly with transla-
tion quality (Ayan and Dorr 2006a; Fraser and Marcu 2007b). In other words, lower AER
scores do not necessarily lead to better translation quality.1 Fraser and Marcu (2007b)
argue that reference alignments should consist of only sure links. They propose a new
measure called the balanced F-measure:
</bodyText>
<equation confidence="0.981870142857143">
precision(S,A) = |A nS |(4)
|S|
recall(S,A) = |A n S |(5)
|A|
1
F-measure(S,α,A) = − (6)
precis of n(S,A) + recall(S,A)
</equation>
<footnote confidence="0.983516333333333">
1 It has not yet been uniformly accepted that better word alignments yield better translations. Ayan and
Dorr (2006a) present a detailed discussion of the impact of word alignment on statistical machine
translation.
</footnote>
<page confidence="0.989039">
306
</page>
<note confidence="0.982306">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<bodyText confidence="0.993278892857143">
where α is a parameter that sets the trade-off between precision and recall. Higher
F-measure means better alignment quality. Obviously, α less than 0.5 weights recall
higher, whereas α greater than 0.5 weights precision higher.
We use both AER and F-measure in our experiments. AER is used in experiments
evaluating alignment quality (Section 4.1) and F-measure is used in experiments evalu-
ating translation performance (Section 4.2).
2.2.2 Minimum Error Rate Training. Suppose we have three candidate alignments: a1, a2,
and a3. Their AER scores are 0.21, 0.20, and 0.22, respectively. Therefore, a2 is the best
candidate alignment, a1 is the second best, and a3 is the third best. We use three features
to score each candidate. Table 1 lists the feature values for each candidate.
If the set of feature weights is {1.0,1.0,1.0}, the model scores (see Equation (2)) of
the three candidates are −71, −74, and −76, respectively. Whereas reference alignment
considers a2 as the best candidate, a1 has the maximal model score. This is undesirable
because the model fails to agree with the reference. If we change the feature weights
to {1.0, −2.0,2.0}, the model scores become −73, −71, and −83, respectively. Now, the
model chooses a2 as the best candidate correctly.
If a set of feature weights manages to make model predictions agree with refer-
ence alignments in training examples, we would expect the model to achieve good
alignment quality on unseen data as well. To do this, we adopt the minimum er-
ror rate training (MERT) algorithm proposed by Och (2003) to find feature weights
that minimize AER or maximize F-measure on a representative hand-aligned training
corpus.
Given a reference alignment r and a candidate alignment a, we use a loss func-
tion E(r, a) to measure alignment performance. Note that E(r, a) can be either AER or
1 − F-measure. Given a bilingual corpus (fS1, eS1) with a reference alignment rs and a set
of K different candidate alignments Cs = {as,1 . . . as,K} for each sentence pair (fs, es), our
goal is to find a set of feature weights ˆλM1 that minimizes the overall loss on the training
corpus:
</bodyText>
<equation confidence="0.465564142857143">
ˆλM1 = argmin S �E(rs, ˆa(fs, es; λM 1 )) (7)
λM �E
1 s=1
= argmin �j:j:E(rs,as,k)δ(ˆa(fs,es;λM1
λM S K
1 ),as,k)I (8)
s=1 k=1
</equation>
<tableCaption confidence="0.989872">
Table 1
</tableCaption>
<bodyText confidence="0.397414666666667">
Example feature values and alignment error rates.
feature values
candidate h1 h2 h3 AER
</bodyText>
<note confidence="0.962885666666667">
a1 –85 4 10 0.21
a2 –89 3 12 0.20
a3 –93 6 11 0.22
</note>
<page confidence="0.917894">
307
</page>
<note confidence="0.793359">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.97264">
where ˆa(fs, es; λM1 ) is the best candidate alignment produced by the linear model:
</bodyText>
<equation confidence="0.5755828">
ˆa(fs, es; λM1 ) = argmax 1
a M
E T
λmhm(fs, es, a) (9)
m=1
</equation>
<bodyText confidence="0.998842">
The basic idea of MERT is to optimize only one parameter (i.e., feature weight)
each time and keep all other parameters fixed. This process runs iteratively over M
parameters until the overall loss on the training corpus does not decrease.
Formally, suppose we tune a parameter and keep the other M − 1 parameters fixed;
each candidate alignment corresponds to a line in the plane with γ as the independent
variable:
</bodyText>
<equation confidence="0.682016">
γ · μ(f, e, a) + σ(f, e, a) (10)
</equation>
<bodyText confidence="0.9986725">
where γ denotes the parameter being tuned (i.e., λm) and μ(f, e, a) and σ(f, e, a) are
constants with respect to γ:
</bodyText>
<equation confidence="0.99660125">
μ(f,e,a) = hm(f,e,a) (11)
σ(f, e, a) = M λm,hm,(f, e, a) (12)
E
m,=1,m,#m
</equation>
<bodyText confidence="0.9999364">
The set of candidates in Cs defines a set of lines. For example, given the candidate
alignments in Table 1, suppose we only tune λ2 and keep λ1 and λ3 fixed with an initial
set of parameters {1.0,1.0,1.0}. According to Equation (10), a1 corresponds to a line
4γ − 75, a2 corresponds to a line 3γ − 77, and a3 corresponds to a line 6γ − 82.
The decision rule in Equation (9) states that aˆ is the line with the highest model
score for a given γ. The selection of γ for each sentence pair ultimately determines the
loss at γ. How do we find values of γ that could generate different loss values?
As the loss can only change if we move to a γ where the highest line is different
than before, Och (2003) suggests only evaluating the loss at values in between the
intersections that line the top surface of the cluster of lines. Figure 2 demonstrates eight
</bodyText>
<figureCaption confidence="0.526226">
Figure 2
</figureCaption>
<footnote confidence="0.99172675">
Candidate alignments in dimension γ and the critical intersections. Each candidate alignment is
represented as a line. γ1,γ2, and γ3 are critical intersections where the best candidate aˆ
(highlighted in bold) will change: aˆ is a1 in (−oo,γ1], a2 in (γ1,γ2], a7 in (γ2,γ3], and a5 in
(γ3, +oo).
</footnote>
<page confidence="0.99663">
308
</page>
<note confidence="0.931978">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<bodyText confidence="0.977707666666667">
candidate alignments. The sequence of the topmost line segments highlighted in bold
constitutes an upper envelope, which indicates the best candidate alignments the model
predicts with various values of -y. Instead of computing all possible K2 intersections
between the lines in Cs, we just need to find the critical intersections where the topmost
line changes. In Figure 2, -y1, -y2, and -y3 are critical intersections. In the interval (−oo,-y1],
a1 has the highest score. Similarly, the best candidates are a2 for (-y1,-y2], a7 for (-y2,-y3],
and a5 for (-y3,+oo), respectively. The optimal -yˆ can be found by collecting all critical
intersections on the training corpus and choosing one -y that results in the minimal loss
value. Please refer to Och (2003) for more details.
</bodyText>
<subsectionHeader confidence="0.999183">
2.3 Search
</subsectionHeader>
<bodyText confidence="0.999752">
Given a source language sentence f and a target language sentence e, we try to find the
best candidate alignment with the highest model score:
</bodyText>
<equation confidence="0.998164">
aˆ = argmax ( �
a score(f, e, a) (13)
= argmax ~ EM ~
a Amhm(f,e,a) (14)
m=1
</equation>
<bodyText confidence="0.9846145">
To do this, we begin with an empty alignment and keep adding new links until
the model score of the current alignment does not increase. Figure 3 illustrates this
search process. Given a source language sentence f1f2 and a target language sentence
e1e2, the initial alignment a1 is empty (i.e., all words are unaligned). Then, we obtain a
new alignment a2 by adding a link (1, 1) to a1. Similarly, the addition of (1, 2) to a1 leads
to a3. a2 and a3 can be further extended to produce more alignments.
Graphically speaking, the search space of a sentence pair can be organized as a
directed acyclic graph. Each node in the graph is a candidate alignment and each edge
corresponds to a link. We say that alignments that have the same number of links
constitute a level. There are 2J×I possible nodes and J x I + 1 levels in a graph. In
Figure 3, a2, a3, a4, and a5 belong to the same level because they all contain one link.
The maximum level width is given by ( L J zI ). In Figure 3, the maximal level width is
�4) = 6. Our goal is to find the node with the highest model score in a search graph.
2
As the search space of word alignment is exponential (although enumerable), it is
computationally prohibitive to explore all the graph. Instead, we can search efficiently
in a greedy way. In Figure 3, starting from a1, we add single links to a1 and obtain four
new alignments: a2, a3, a4, and a5. We retain the best new alignment that has a higher
score than a1, say a3, and discard the others. Then, we add single links to a3 and obtain
three new alignments: a7, a9, and a11. After choosing a9 as the current best alignment, the
next candidates are a12 and a14. Suppose the model scores of both a12 and a14 are lower
than that of a9. We terminate the search process and choose a9 as the best candidate
alignment.
During this search process, we expect that the addition of a single link l to the
current best alignment a will result in a new alignment a U 1l} with a higher score:
score(f, e, a U 1l}) &gt; score(f, e, a) (15)
</bodyText>
<page confidence="0.995827">
309
</page>
<figure confidence="0.856939">
Computational Linguistics Volume 36, Number 3
</figure>
<figureCaption confidence="0.922567">
Figure 3
</figureCaption>
<bodyText confidence="0.9873365">
Search space of a sentence pair: f1 f2 and e1e2. Each node in the directed graph is a candidate
alignment and each edge denotes a transition between two nodes by adding a link.
</bodyText>
<equation confidence="0.98084675">
that is
M
E λm(hm(f,e,a U 1l}) − hm(f,e,a)) &gt; 0 (16)
m=1
</equation>
<bodyText confidence="0.999064">
As a result, we can remove most of the computational overhead by calculating only
the difference of scores instead of the scores themselves. The difference of alignment
scores with the addition of a link, which we refer to as a link gain, is defined as
</bodyText>
<equation confidence="0.998593">
M
g(f,e,a,l) = E λmgm(f, e, a,l) (17)
m=1
</equation>
<bodyText confidence="0.988835">
where gm(f, e, a,l) is a feature gain, which is the incremental feature value after adding
a link l to the current alignment a:
</bodyText>
<equation confidence="0.986304">
gm(f,e,a,l) = hm(f,e,a U 1l}) − hm(f,e,a) (18)
</equation>
<bodyText confidence="0.995366">
In our experiments, we use a beam search algorithm that is more general than the
above greedy algorithm. In the greedy algorithm, we retain at most one candidate in
each level of the space graph while traversing top-down. In the beam search algorithm,
we retain at most b candidates at each level.
Algorithm 1 shows the beam search algorithm. The input is a source language
sentence f and a target language sentence e (line 1). The algorithm maintains a list of
</bodyText>
<page confidence="0.999026">
310
</page>
<note confidence="0.929466">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<table confidence="0.939910454545455">
Algorithm 1 A beam search algorithm for word alignment
1: procedure ALIGN(f, e)
open &lt;-- 0 &gt; a list of active alignments
N &lt;-- 0 &gt; n-best list
a &lt;-- 0 &gt; begin with an empty alignment
ADD(open, a, R, b) &gt; initialize the list
while open # 0 do
closed &lt;-- 0 &gt; a list of promising alignments
for all a E open do
for all l E J x I − a do &gt; enumerate all possible new links
a&apos; &lt;-- a U {l} &gt; produce a new alignment
g &lt;-- GAIN(f, e, a,l) &gt; compute the link gain
if g &gt; 0 then &gt; ensure that the score will increase
ADD(closed, a&apos;, R, b) &gt; update promising alignments
end if
ADD(N, a&apos;, 0, n) &gt; update n-best list
end for
end for
open &lt;-- closed &gt; update active alignments
end while
return N &gt; return n-best list
21: end procedure
</table>
<bodyText confidence="0.973762">
active alignments open (line 2) and an n-best list N (line 3). The aligning process begins
with an empty alignment a (line 4) and the procedure ADD(open, a, R, b) adds a to open.
The procedure prunes the search space by discarding any alignment that has a score
worse than:
</bodyText>
<listItem confidence="0.9938255">
1. R multiplied with the best score in the list, or
2. the score of b-th best alignment in the list.
</listItem>
<bodyText confidence="0.99647025">
For each iteration (line 6), we use a list closed to store promising alignments that
have higher scores than the current alignment. For every possible link l (line 9), we
produce a new alignment a&apos; (line 10) and calculate the link gain 9 by calling the
procedure GAIN(f, e, a, l). If a&apos; has a higher score (line 12), it is added to closed (line 13).
We also update N to keep the top n alignments explored during the search (line 15). The
n-best list will be used in training feature weights by MERT. This process iterates
until there are no promising alignments. The theoretical running time of this algorithm
is 0(bJ2I2).
</bodyText>
<sectionHeader confidence="0.969001" genericHeader="method">
3. Feature Functions
</sectionHeader>
<bodyText confidence="0.98972825">
The primary art in discriminative modeling is to define useful features that capture var-
ious characteristics of word alignments. Intuitively, we can include generative models
such as the IBM Models 1–5 (Brown et al. 1993) as features in a discriminative model.
A straightforward way is to use a generative model itself as a feature directly (Liu, Liu,
</bodyText>
<page confidence="0.995609">
311
</page>
<note confidence="0.295693">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.9997015">
and Lin 2005). Another way is to treat each sub-model of a generative model as a feature
(Fraser and Marcu 2006). In either case, a generative model can be regarded as a special
case of a discriminative model where all feature weights are one. A detailed discussion
of the treatment of the IBM models as features can be found in Appendix B.
One major drawback of the IBM models is asymmetry. They are restricted such that
each source word is assigned to exactly one target word. This is not the case for many
language pairs. For example, in our running example, one Chinese word jianzhuye cor-
responds to two English words construction industry. As a result, our linear model will
produce only one-to-one alignments if the IBM models in two translation directions (i.e.,
source-to-target and target-to-source) are both used. Although some authors would use
the one-to-one assumption to simplify the modeling problem (Melamed 2000; Taskar,
Lacoste-Julien, and Klein 2005), many translation phenomena cannot be handled and
the recall cannot reach 100% in principle.
A more general way is to model alignment as an arbitrary relation between source
and target language word positions. As our linear model is capable of including many
overlapping features regardless of their interdependencies, it is easy to add features
that characterize symmetric alignments. In the following subsections, we will introduce
a number of symmetric features used in our experiments.
</bodyText>
<subsectionHeader confidence="0.999264">
3.1 Translation Probability Product
</subsectionHeader>
<bodyText confidence="0.998572">
To determine the correspondence of words in two languages, word-to-word translation
probabilities are always the most important knowledge source. To model a symmetric
alignment, a straightforward way is to compute the product of the translation probabil-
ities of each link in two directions.
For example, suppose that there is an alignment {(1,2)} for a source language
sentence f1 f2 and a target language sentence e1e2; the translation probability prod-
uct is
</bodyText>
<equation confidence="0.928075">
t(e2|f1) x t( f1|e2)
</equation>
<bodyText confidence="0.966455833333333">
where t(e|f ) is the probability that f is translated to e and t( f |e) is the probability that e
is translated to f, respectively.
Unfortunately, the underlying model is biased: The more links added, the smaller
the product will be. For example, if we add a link (2,2) to the current alignment and
obtain a new alignment {(1,2), (2,2)}, the resulting product will decrease after being
multiplied with t(e2|f2) x t( f2|e2):
</bodyText>
<equation confidence="0.990191">
t(e2|f1) x t(f1|e2) x t(e2|f2) x t(f2|e2)
</equation>
<bodyText confidence="0.999916875">
The problem results from the absence of empty cepts. Following Brown et al. (1993),
a cept in an alignment is either a single source word or it is empty. They assign cepts
to positions in the source sentence and reserve position zero for the empty cept. All
unaligned target words are assumed to be “aligned” to the empty cept. For example,
in the current example alignment {(1,2)}, the unaligned target word e1 is said to be
“aligned” to the empty cept f0. As our model is symmetric, we use f0 to denote the
empty cept on the source side and use e0 to denote the empty cept on the target side,
respectively.
</bodyText>
<page confidence="0.9976">
312
</page>
<note confidence="0.920816">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<equation confidence="0.87236325">
If we take empty cepts into account, the product for {(1,2)} can be rewritten as
t(e2|f1) × t(f1|e2) × t(e1|f0) × t( f2|e0)
Similarly, the product for {(1, 2), (2,2)} now becomes
t(e2|f1) × t(f1|e2) × t(e2|f2) × t( f2|e2) × t(e1|f0)
</equation>
<bodyText confidence="0.999355666666667">
Note that after adding the link (2, 2), the new product still has more factors than the old
product. However, the new product is not necessarily always smaller than the old one.
In this case, the new product divided by the old product is
</bodyText>
<equation confidence="0.9995745">
t(e2|f2) × t(f2|e2)
t( f2|e0)
</equation>
<bodyText confidence="0.972911">
Whether a new product increases or not depends on actual translation probabilities.2
Depending on whether they are aligned or not, we divide the words in a sentence
pair into two categories: aligned and unaligned. For each aligned word, we use trans-
lation probabilities conditioned on its counterpart in two directions (i.e., t(ei|fj) and
t( fj|ei)). For each unaligned word, we use translation probabilities conditioned on empty
cepts on the other side in two directions (i.e., t(ei|f0) and t(fj|e0)).
Formally, the feature function for translation probability product is given by3
</bodyText>
<equation confidence="0.998978833333333">
htpp(f, e, a) = E (log(t(ei|fj)) + log(t( fj|ei))) +
(j,i)Ea
E J log(6(*j, 0) × t( fj|e0) + 1 − 6(*j, 0)) +
j=1
EI log(6(-�i,0) × t(ei|f0) + 1 − 6(-�i,0)) (19)
i=1
</equation>
<bodyText confidence="0.9603855">
where 6(x,y) is the Kronecker function, which is 1 if x = y and 0 otherwise. We define
the fertility of a source word fj as the number of aligned target words:
</bodyText>
<equation confidence="0.9596935">
*j = E 6(j1,j) (20)
(j�,i)Ea
</equation>
<bodyText confidence="0.654729666666667">
2 Even though we take empty cepts into account, the bias problem still exists because the product will
decrease by adding new links if there are no unaligned words. For example, the product will go down if
we further add a link (1, 1) to {(1, 2), (2,2)} as all source words are aligned. This might not be a bad bias
because reference alignments usually do not have all words aligned and contain too many links.
Although translation probability product is degenerate as a generative model, the bias problem can be
alleviated when this feature is combined with other features such as link count (see Section 3.8).
</bodyText>
<footnote confidence="0.765517">
3 We use the logarithmic form of translation probability product to avoid manipulating very small
numbers (e.g., 4.3 × e−100) just for practical reasons.
</footnote>
<page confidence="0.997074">
313
</page>
<table confidence="0.41552">
Computational Linguistics Volume 36, Number 3
</table>
<tableCaption confidence="0.967221">
Table 2
</tableCaption>
<bodyText confidence="0.93877">
Calculating feature values of translation probability product for a source sentence f1 f2 and a
target sentence e1e2.
alignment feature value
</bodyText>
<equation confidence="0.965584875">
log�t(e1|f0) &apos; t(e2|f0) &apos; t(f1|e0) &apos; t( f2|e0)�
{}
log�t(e1|f0) &apos; t(e2|f1) &apos; t(f1|e2) &apos; t( f2|e0)�
{(1,2)}
{(1,2), (2,2)} log t(e1|f0) &apos; t(e2|f1) &apos; t(e2|f2) &apos; t(f1|e2) &apos; t(f2|e2))
Similarly, the fertility of a target word ei is the number of aligned source words:
�φi = δ(i&apos;, i) (21)
(j,i&apos;)∈a
</equation>
<bodyText confidence="0.999478090909091">
For example, as only one English word China is aligned to the first Chinese word
Zhongguo in Figure 1, the fertility of Zhongguo is ψ1 = 1. Similarly, the fertility of the
third Chinese word duiwaikaifang is ψ3 = 4 because there are four aligned English
words. The fertility of the first English word The is φ1 = 0. Obviously, the words with
zero fertilities (e.g., The, ’s, and a in Figure 1) are unaligned.
In Equation (19), the first term calculates the product of aligned words, the second
term deals with unaligned source words, and the third term deals with unaligned target
words. Table 2 shows the feature values for some word alignments.
For efficiency, we need to calculate the difference of feature values instead of the
values themselves, which we call feature gain (see Equation (18)). The feature gain for
translation probability product is4
</bodyText>
<equation confidence="0.999994">
gtpp(f,e,a,j,i) = log(t(ei|fj)) + log(t(fj|ei)) −
log(δ(ψj,0) x t( fj|e0) + 1 − δ(ψj, 0)~ −
log(δ(φi,0) x t(ei|f0) + 1 − δ(φi,0)) (22)
</equation>
<bodyText confidence="0.999875666666667">
where ψj and φi are the fertilities before adding the link (j, i).
Although this feature is symmetric, we obtain the translation probabilities t( f |e) and
t(e|f ) by training the IBM models using GIZA++ (Och and Ney 2003).
</bodyText>
<subsectionHeader confidence="0.999942">
3.2 Exact Match
</subsectionHeader>
<bodyText confidence="0.9999685">
Motivated by the fact that proper names (e.g., IBM) or specialized terms (e.g., DNA) are
often the same in both languages, Taskar, Lacoste-Julien, and Klein (2005) use a feature
that sums up the number of words linked to identical words. We adopt this exact match
feature in our model:
</bodyText>
<equation confidence="0.703646">
�hem(f,e,a) = δ( fj, ei) (23)
(j,i)∈a
4 For clarity, we use gtpp(f, e, a, j, i) instead of gtpp(f, e, a,l) because j and i appear in the equation.
</equation>
<page confidence="0.986516">
314
</page>
<note confidence="0.683774">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<construct confidence="0.455735">
gem(f, e, a, j, i) = δ( fj,ei) (24)
</construct>
<subsectionHeader confidence="0.998979">
3.3 Cross Count
</subsectionHeader>
<bodyText confidence="0.998803416666667">
Due to the diversity of natural languages, word orders between two languages are usu-
ally different. For example, subject-verb-object (SVO) languages such as Chinese and
English often put an object after a verb while subject-object-verb (SOV) languages such
as Japanese and Turkish often put an object before a verb. Even between SVO languages
such as Chinese and English, word orders could be quite different too. In Figure 1,
while Zhongguo is the first Chinese word, its counterpart China is the fourth English
word. Meanwhile, the third Chinese word duiwaikaifang after Zhongguo is aligned to the
second English word opening before China. We say that there is a cross between the two
links (1, 4) and (3,2) because (1 − 3) x (4 − 2) &lt; 0. In Figure 1, there is only one cross.
As a result, we could use the number of crosses in alignments to capture the divergence
of word orders between two languages.
Formally, the cross count feature function is given by
</bodyText>
<equation confidence="0.96098975">
�h,,(f,e, a) = � Q(j − j�) x (i − i&apos;) &lt; 0� (25)
(j,i)∈a (jl,il)∈a
�g,,(f,e,a,j,i) = Q(j − j&apos;) x (i − i&apos;) &lt; 0� (26)
(jl,il)∈a
</equation>
<bodyText confidence="0.870275333333333">
where Qexpr� is an indicator function that takes a boolean expression expr as the
argument:
� 1 if expr is true
</bodyText>
<equation confidence="0.8940045">
�expr� = (27)
0 otherwise
</equation>
<subsectionHeader confidence="0.921412">
3.4 Neighbor Count
</subsectionHeader>
<bodyText confidence="0.931138">
Moore (2005) finds that word alignments between closely related languages tend to be
approximately monotonic. Even for distantly related languages, the number of crossing
links is far less than chance since phrases tend to be translated as contiguous chunks.
In Figure 1, the dark points are positioned approximately in parallel with the diagonal
line, indicating that the alignment is approximately monotonic.
To capture such monotonicity, we follow Lacoste-Julien et al. (2006) to encourage
strictly monotonic alignments by adding a bonus for any pair of links (j, i) and (j&apos;, i&apos;)
such that
j − j&apos; = 1 ∧ i − i&apos; = 1
In Figure 1, there is one such link pair: (3,10) and (4,11). We call these links
neighbors. Similarly, (5,13) and (6,14) are also neighbors.
Formally, the neighbor count feature function is given by
�hn,(f, e, a) = � V − j&apos; = 1 ∧ i − i&apos; = 1� (28)
(j,i)∈a (jl,il)∈a
</bodyText>
<page confidence="0.961631">
315
</page>
<note confidence="0.213282">
Computational Linguistics Volume 36, Number 3
</note>
<equation confidence="0.7350495">
Lgnc(f,e,a,j,i) = 1j − j� = 1 ∧ i − i&apos; = 1� (29)
(jl,il)∈a
</equation>
<subsectionHeader confidence="0.958522">
3.5 Fertility Probability Product
</subsectionHeader>
<bodyText confidence="0.998544384615385">
Casual inspection of some word alignments quickly establishes that some Chinese
words such as Zhongguo and chengxian are often aligned to one English word whereas
other Chinese words such as duiwaikaifang tend to be translated into multiple English
words. Brown et al. (1993) call the number of target words to which a source word f is
connected the fertility off. Recall that we have given the formal definition of fertility in
the symmetric scenario in Equation (20) and Equation (21).
Besides word association (Sections 3.1 and 3.2) and word distortion (Sections 3.3
and 3.4), fertility also proves to be very important in modeling alignment because
sophisticated generative models such as the IBM Models 3–5 parameterize fertilities
directly. As our goal is to produce symmetric alignments, we calculate the product of
fertility probabilities in two directions.
Given an alignment {(1,2)} for a source sentence f1 f2 and a target sentence e1e2, the
fertility probability product is
</bodyText>
<equation confidence="0.926117">
n(1|f0) x n(1|f1) x n(0|f2) x n(1|e0) x n(0|e1) x n(1|e2)
</equation>
<bodyText confidence="0.987774125">
where n(*j|fj) is the probability that fj has a fertility of *j and n(φi|ei) is the probability
that ei has a fertility of φi, respectively.5 For example, n(1|f0) denotes the probability
that one target word is “aligned” to the source empty cept f0 and n(1|e2) denotes the
probability that one source word is aligned to e2.
If we add a link (2,2) to the current alignment and obtain a new alignment
{(1, 2), (2,2)}, the resulting product will be
n(1|f0) x n(1|f1) x n(1|f2) x n(0|e0) x n(0|e1) x n(2|e2)
The new product divided by the old product is
</bodyText>
<equation confidence="0.923744">
n(1|f2) x n(0|e0) x n(2|e2)
n(0|f2) x n(1|e0) x n(1|e2)
Formally, the feature function for fertility probability product is given by
hfpp(f,e,a) = J log(n(*j|fj)) + LI log(n(φi|ei)) (30)
L i=0
j=0
</equation>
<bodyText confidence="0.6921415">
5 Brown et al. (1993) treat the empty cept in a different way. They assume that at most half of the source
words in an alignment are not aligned (i.e., φ0 :5 J/2) and define a binomial distribution relying on an
auxiliary parameter p0. Here, we use n(φ0|e0) instead of the original form n0(φ0 |EIi=1 φi) just for
simplicity. See Appendix B for more details.
</bodyText>
<page confidence="0.996525">
316
</page>
<note confidence="0.701418">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<bodyText confidence="0.942518">
The corresponding feature gain is
</bodyText>
<equation confidence="0.99624575">
gfpp(f, e, a, j, i) = log(n(ψ0 − δ(φi, 0)|f0)) − log(n(ψ0|f0)) +
log(n(ψj + 1|fj) − log(n(ψj|fj)) +
log(n(φ0 − δ(ψj,0)|e0)) − log(n(φ0|e0)) +
log(n(φi + 1|ei)) − log(n(φi|ei)) (31)
</equation>
<bodyText confidence="0.99975225">
where ψj and φi are the fertilities before adding the link (j, i).
Table 3 gives the feature values for some word alignments. In practice, we also
obtain all fertility probabilities n(ψj|fj) and n(φi|ei) by using the output of GIZA++
directly.
</bodyText>
<subsectionHeader confidence="0.976313">
3.6 Linked Word Count
</subsectionHeader>
<bodyText confidence="0.999991714285714">
We observe that there should not be too many unaligned words in good alignments.
For example, there are only three unaligned words on the target side in Figure 1: The,
’s, and a. Unaligned words are usually function words that have little lexical meaning
but instead serve to express grammatical relationships with other words or specify the
attitude or mood of the speaker. To control the number of unaligned words, we follow
Moore, Yih, and Bode (2006) to introduce a linked word count feature that simply counts
the number of aligned words:
</bodyText>
<equation confidence="0.956935333333333">
hlwc(f,e,a) = � J Qψj &gt; 0� + �I Qφi &gt; 0� (32)
j=1 i=1
glwc(f, e, a, j, i) = δ(ψj, 0) + δ(φi, 0) (33)
</equation>
<bodyText confidence="0.963804">
In Equation (33), ψj and φi are the fertilities before adding l.
</bodyText>
<subsectionHeader confidence="0.995427">
3.7 Sibling Distance
</subsectionHeader>
<bodyText confidence="0.999881">
In word alignments, there are usually several words connected to the same word on the
other side. For example, in Figure 1, two English words construction and industry are
aligned to one Chinese word jianzhuye. We call the words aligned to the same word on
the other side siblings. In Figure 1, opening, to, the, and outside are also siblings because
they are aligned to duiwaikaifang. A word (e.g., jianzhuye) often tends to produce a series
of words in another language that belong together, whereas others (e.g., duiwaikaifang)
</bodyText>
<tableCaption confidence="0.921806">
Table 3
</tableCaption>
<table confidence="0.547638333333333">
Calculating feature values of fertility probability product for a source sentence f1 f2 and a target
sentence e1e2.
alignment feature value
{} (2,2)} log(n(2|f0) &apos; n(0|f1) &apos; n(0|f2) &apos; n(2|e0) &apos; n(0|e1) &apos; n(0|e2))
{(1,2)} log(n(1|f0) &apos; n(1|f1) &apos; n(0|f2) &apos; n(1|e0) &apos; n(0|e1) &apos; n(1|e2))
{(1,2), log(n(1|f0) &apos; n(1|f1) &apos; n(1|f2) &apos; n(0|e0) &apos; n(0|e1) &apos; n(2|e2))
</table>
<page confidence="0.902514">
317
</page>
<note confidence="0.253866">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.985385166666667">
tend to produce a series of words that should be separate. To model this tendency, we
introduce a feature that sums up the distances between siblings.
Formally, we use ωj,k to denote the position of the k-th target word aligned to a
source word fj and use πi,k to denote the position of the k-th source word aligned to a
target word ei. For example, jianzhuye is the second source word (i.e., f2) in Figure 1.
As the first target word aligned to f2 is construction (i.e., e6), therefore we say that
ω2,1 = 6. Similarly, ω2,2 = 7 because industry (i.e., e7) is the second target word aligned
to jianzhuye. Obviously, ωj,k+1 is always greater than ωj,k by definition.
As construction and industry are siblings, we define the distance between them
as ω2,2 − ω2,1 − 1 = 0. Note that we give no penalty to siblings that belong closely
together. In Figure 1, there are four siblings opening, to, the, and outside aligned to the
source word duiwaikaifang. The sum of distances between them is calculated as
</bodyText>
<equation confidence="0.976747">
ω3,2 − ω3,1 − 1 + ω3,3 − ω3,2 − 1 + ω3,4 − ω3,3 − 1
= ω3,4 − ω3,1 − 3
= 10 − 2 − 3
= 5
Therefore, the distance sum of fj can be efficiently calculated as
� _ ωj,ψj − ωj,1 − ψj + 1 if ψj &gt; 1
Δ(j,ψj) (34)
0 otherwise
Accordingly, the distance sum of ei is
� ( 4) ni,φ 35
i− πo-4)i+ 1 if φi&gt;1 ( )
W, i — 0 otherwise
Formally, the feature function for sibling distance is given by
hsd(f,e,a) = � J Δ(j,ψj) + �I ∇(i, φi) (36)
j=1 i=1
The corresponding feature gain is
gsd(f,e,a,j,i) = Δ(j,ψj + 1) − Δ(j,ψj) +
∇(i, φi + 1) − ∇(i, φi) (37)
</equation>
<bodyText confidence="0.996281">
where ψj and φi are the fertilities before adding the link (j, i).
</bodyText>
<subsectionHeader confidence="0.996389">
3.8 Link Count
</subsectionHeader>
<bodyText confidence="0.97656425">
Given a source sentence with J words and a target sentence with I words, there are
J × I possible links. However, the actual number of links in a reference alignment is
usually far less. For example, there are only 10 links in Figure 1 although the maximum
is 6 × 14 = 84. The number of links has an important effect on alignment quality because
</bodyText>
<page confidence="0.996919">
318
</page>
<note confidence="0.920162">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<bodyText confidence="0.997007">
more links result in higher recall while fewer links result in higher precision. A good
trade-off between recall and precision usually results from a reasonable number of links.
Using the number of links as a feature could also alleviate the bias problem posed by
the translation probability product feature (see Section 3.1). A negative weight of the
link count feature often leads to fewer links while a positive weight favors more links.
Formally, the feature function for link count is
</bodyText>
<equation confidence="0.999712">
hl,(f,e,a) = |a |(38)
gl,(f,e,a,l) = 1 (39)
</equation>
<bodyText confidence="0.809926">
where |a |is the cardinality of a (i.e., the number of links in a).
</bodyText>
<subsectionHeader confidence="0.978024">
3.9 Link Type Count
</subsectionHeader>
<bodyText confidence="0.966862444444444">
Due to the different fertilities of words, there are different types of links. For instance,
one-to-one links indicate that one source word (e.g., Zhongguo) is translated into ex-
actly one target word (e.g., China) while many-to-many links exist for phrase-to-phrase
translation. The distribution of link types differs for different language pairs. For ex-
ample, one-to-one links occur more frequently in closely related language pairs (e.g.,
French–English) and one-to-many links are more common in distantly related language
pairs (e.g., Chinese–English). To capture the distribution of link types independent of
languages, we use features to count different types of links.
Following Moore (2005), we divide links in an alignment into four categories:
</bodyText>
<listItem confidence="0.99874825">
1. one-to-one links, in which neither the source nor the target word
participates in other links;
2. one-to-many links, in which only the source word participates in other
links;
3. many-to-one links, in which only the target word participates in other
links;
4. many-to-many links, in which both the source and target words
participate in other links.
</listItem>
<bodyText confidence="0.902530666666667">
In Figure 1, (1, 4), (4,11), (5,13), and (6,14) are one-to-one links and the others are
one-to-many links.
As a result, we introduce four features:
</bodyText>
<equation confidence="0.863942142857143">
�ho2o(f, e, a) = J*j = 1 ∧ 4)i = 1�
(j,i)Ea
�ho2m(f, e, a) = Q*j &gt; 1 ∧ 4)i = 1�
(j,i)Ea
�hm2o(f,e,a) = Q*j = 1 ∧ 4)i &gt; 1�
(j,i)Ea
�hm2m(f,e,a) = Q*j &gt; 1 ∧ 4)i &gt; 1�
</equation>
<table confidence="0.513702">
(j,i)Ea
319
Computational Linguistics Volume 36, Number 3
</table>
<bodyText confidence="0.9988471">
Their feature gains cannot be calculated in a straightforward way because the
addition of a link might change the link types of its siblings on both the source and
target sides. For example, if we align the Chinese word chengxian and the English word
industry, the newly added link (4,7) is a many-to-many link. Its source sibling (2, 7),
which was a one-to-many link, now becomes a many-to-many link. Meanwhile, its
target sibling (4,11), which was a one-to-one link, now becomes a one-to-many link.
Algorithm 2 shows how to calculate the four feature gains. After initialization
(line 2), we first decide the type of l (lines 3–11). Then, we consider the siblings of l
on the target side (lines 12–24) and those on the source side (lines 25–38), respectively.
Note that the feature gains of siblings will not change if ψi =� 1 or φj =� 1.
</bodyText>
<sectionHeader confidence="0.482368" genericHeader="method">
3.10 Bilingual Dictionary
</sectionHeader>
<bodyText confidence="0.9999676">
A conventional bilingual dictionary can be considered an additional knowledge source.
The intuition is that a dictionary is expected to be more reliable than an automatically
trained lexicon. For example, if Zhongguo and China appear in an entry of a dictionary,
they should be more likely to be aligned. Thus, we use a single indicator feature to
encourage linking word pairs that occur in a dictionary D:
</bodyText>
<equation confidence="0.973616">
hbd(f,e,a,D) = � Q( fj, ei) E D� (44)
(j,i)Ea
gbd(f, e, a, D, j, i) = Q( fj, ei) E D� (45)
</equation>
<subsectionHeader confidence="0.670702">
3.11 Link Co-Occurrence Count
</subsectionHeader>
<bodyText confidence="0.9996355">
The system combination technique that integrates predictions from multiple systems
proves to be effective in machine translation (Rosti, Matsoukas, and Schwartz 2007;
He et al. 2008). In word alignment, a link should be aligned if it appears in most
system predictions. Taskar, Lacoste-Julien, and Klein (2005) include the IBM Model 4
predictions as features and obtain substantial improvements.
To enable system combination, we design a feature to favor links voted by most
systems. Given an alignment a&apos; produced by another system, we use the number of
links of the intersection of a and a&apos; as a feature:
</bodyText>
<equation confidence="0.8810805">
hlcc(f, e, a, a&apos;) = ja n a&apos;j (46)
glcc(f, e, a, a&apos;, j, i) = Ql E a n a&apos;� (47)
</equation>
<sectionHeader confidence="0.987903" genericHeader="method">
4. Experiments
</sectionHeader>
<bodyText confidence="0.980173">
In this section, we try to answer two questions:
</bodyText>
<listItem confidence="0.9985475">
1. Does the proposed approach achieve higher alignment quality than
generative alignment models?
2. Do statistical machine translation systems produce better translations if
we replace generative alignment models with the proposed approach?
</listItem>
<bodyText confidence="0.9709875">
In Section 4.1, we evaluate our approach on three word alignment shared tasks for
five language pairs with varying divergence and richness of resources. Experimental
</bodyText>
<page confidence="0.994096">
320
</page>
<note confidence="0.971573">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<bodyText confidence="0.411084">
Algorithm 2 Calculating gains for the link type count features
</bodyText>
<listItem confidence="0.987851948717949">
1: procedure GAINLINKTYPECOUNT(f, e, a, j, i)
2: {go2o,go2m,gm2o,gm2m} — {0,0,0,0} &gt; initialize the feature gains
3: if *j = 0 ∧ φi = 0 then &gt; consider (j, i) first
4: go2o — go2o + 1
5: else if *j &gt; 0 ∧ φi = 0 then
6: go2m — go2m + 1
7: else if *j = 0 ∧ φi &gt; 0 then
8: gm2o gm2o + 1
9: else
10: gm2m gm2m + 1
11: end if
12: if *j = 1 then &gt; consider the siblings of (j, i) on the target side
13: for i&apos; = 1... Ido
14: if (j, i&apos;) E a ∧ i&apos; =� i then &gt; (j, i&apos;) is a sibling of (j, i) on the target side
15: if φi, = 1 then &gt; (j, i&apos;) was a one-to-one link
16: go2o — go2o — 1
17: go2m go2m + 1 &gt; (j, i&apos;) now becomes a one-to-many link
18: else &gt; (j, i&apos;) was a many-to-one link
19: gm2o gm2o — 1
20: gm2m gm2m + 1 &gt; (j, i&apos;) now becomes a many-to-many link
21: end if
22: end if
23: end for
24: end if
25: if φi = 1 then &gt; consider the siblings of (j, i) on the source side
26: for j~ = 1... J do
27: if (j&apos;, i) E a ∧ j&apos; =� j then &gt; (j&apos;, i) is a sibling of (j, i) on the source side
28: if *j, = 1 then &gt; (j&apos;, i) was a one-to-one link
29: go2o — go2o — 1
30: gm2o gm2o + 1 &gt; (j&apos;, i) now becomes a many-to-one link
31: else &gt; (j&apos;, i) was a one-to-many link
32: go2m go2m — 1
33: gm2m gm2m + 1 &gt; (j&apos;, i) now becomes a many-to-many link
34: end if
35: end if
36: end for
37: end if
38: return {go2o, go2m, gm2o, gm2m} &gt; return the four feature gains
39: end procedure
</listItem>
<bodyText confidence="0.978316714285714">
results show that our system outperforms systems participating in the three shared
tasks significantly and achieves comparable results with other state-of-the-art discrimi-
native alignment models.
In Section 4.2, we investigate the effect of our model on translation quality. By
training feature weights with respect to F-measure instead of AER, our model results
in superior translation quality over generative methods for phrase-based, hierarchical
phrase-based, and tree-to-string SMT systems.
</bodyText>
<page confidence="0.98668">
321
</page>
<note confidence="0.440012">
Computational Linguistics Volume 36, Number 3
</note>
<subsectionHeader confidence="0.988736">
4.1 Evaluation of Alignment Quality
</subsectionHeader>
<bodyText confidence="0.995828">
In this section, we present results of experiments on three word alignment shared tasks:
</bodyText>
<listItem confidence="0.9990945">
1. HLT/NAACL 2003 shared task (Mihalcea and Pedersen 2003). As part of
the HLT/NAACL 2003 workshop on “Building and Using Parallel Texts:
Data Driven Machine Translation and Beyond,” this shared task includes
two language pairs: English–French and Romanian–English. Participants
can use both limited and unlimited resources.
2. ACL 2005 shared task (Martin, Mihalcea, and Pedersen 2005). As part of
the ACL 2005 workshop on “Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond,” this shared task includes three
language pairs to cover different language and data characteristics:
English–Inuktitut, Romanian–English, and English–Hindi. Participants
can use both limited and unlimited resources.
3. HTRDP 2005 shared task. As part of the 2005 HTRDP (National High
Technology Research and Development Program of China, also called
“863” Program) Evaluation on Chinese Information Processing and
</listItem>
<bodyText confidence="0.959130384615385">
Intelligent Human-Machine Interface Technology, this shared task
included only one language pair: Chinese–English. Participants can use
unlimited resources.
Among these, we choose two tasks, English–French and Chinese–English, to report
detailed experimental results. Results for the other tasks can also be found in Table 11.
Corpus statistics for the English–French and Chinese–English tasks are shown in
Tables 4 and 5. The English–French data from the HLT/NAACL 2003 shared task consist
of a training corpus of 1,130,104 sentence pairs, a development corpus of 37 sentence
pairs, and a test corpus of 447 sentence pairs. The development and test sets are manu-
ally aligned and marked with both sure and possible labels. Although the Canadian
Hansard bilingual corpus is widely used in the community, direct comparisons are
difficult due to the differences in splitting of training data, development data, and test
data. To make our results more comparable to previous work, we followed Lacoste-
</bodyText>
<tableCaption confidence="0.8249675">
Table 4
Corpus characteristics of the English–French task.
</tableCaption>
<table confidence="0.9680984">
English French
Training corpus Sentences 1,130,104
Words 20.01M 23.61M
Vocabulary 68,019 86,591
Development corpus Sentences 37
Words 661 721
Vocabulary 322 344
Test corpus Sentences 447
Words 7,020 7,761
Vocabulary 1,732 1,943
</table>
<page confidence="0.979199">
322
</page>
<note confidence="0.976368">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<tableCaption confidence="0.7412885">
Table 5
Corpus characteristics of the Chinese–English task.
</tableCaption>
<table confidence="0.9856929">
Chinese English
Training corpus Sentences 837,594
Words 10.32M 10.71M
Vocabulary 93,532 134,143
Development corpus Sentences 502
Words 9,338 9,364
Vocabulary 2,608 2,587
Test corpus Sentences 505
Words 9,088 10,224
Vocabulary 2,319 2,651
</table>
<bodyText confidence="0.99005675">
Julien et al. (2006) splitting the original test set into two parts: the first 200 sentences
as the development set and the remaining 247 sentences as the test set. To compare
with systems participating in the 2003 NAACL shared task, we also used the small
development set of 37 sentences to optimize feature weights, and ran our system on the
original test set of 447 sentences. The results are shown in Table 11.
The Chinese–English data from the HTRDP 2005 shared task contains a develop-
ment corpus of 502 sentence pairs and a test corpus of 505 sentence pairs. We use
a training corpus of 837,594 sentence pairs available from Chinese Linguistic Data
Consortium and a bilingual dictionary containing 415,753 entries.
4.1.1 Comparison of the Search Algorithm with GIZA++. We develop a word alignment
system named Vigne based on the linear modeling approach. As we mentioned before,
our model can include the IBM models as features (see Appendix B). To investigate the
effectiveness of our search algorithm, we compare Vigne with GIZA++ by using the
same models.
Table 6 shows the alignment error rate percentages for various IBM models in
GIZA++ and Vigne. To make the results comparable, we ensured that Vigne shared
</bodyText>
<tableCaption confidence="0.644927">
Table 6
Comparison of AER scores for various IBM models in GIZA++ and Vigne. These models are
trained only on development and test sets. The pruning setting for Vigne is R = 0 and b = 1. All
differences are not statistically significant.
</tableCaption>
<table confidence="0.999777">
English–French Chinese–English
Model Training Scheme Direction GIZA++ Vigne GIZA++ Vigne
Model 1 15 S → T 50.6 50.6 58.0 58.0
T → S 46.2 46.2 56.1 56.1
Model 2 1525 S → T 47.8 47.8 59.3 59.3
T → S 43.6 43.6 57.4 57.4
Model 3 15H533 S → T 31.6 31.4 45.0 44.5
T → S 27.9 27.9 47.4 46.5
Model 4 15H53343 S → T 34.5 34.2 44.9 44.6
T → S 30.8 30.6 46.7 46.4
</table>
<page confidence="0.957628">
323
</page>
<note confidence="0.470337">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.996866454545455">
the same parameters with GIZA++.6 Table 6 also gives the training schemes used for
GIZA++. For example, the training scheme for Model 4 is 15H53343. This notation
indicates that five iterations of Model 1, five iterations of HMM, three iterations of
Model 3, and three iterations of Model 4 are performed. As the two systems use the same
model parameters, the amount of training data will have no effect on the comparison.
Therefore, we trained the IBM Models only on the development and test sets. As a re-
sult, the AER scores in Table 6 look quite high.
In GIZA++, there exist simple polynomial algorithms to find the Viterbi alignments
for Models 1 and 2. We observe that the greedy search algorithm (β = 0 and b = 1)
used by Vigne can also find the optimal alignments. Note that the two systems achieve
identical AER scores because there are no search errors.
For Models 3 and 4, maximization over all alignments cannot be efficiently carried
out as the corresponding search problem is NP-complete. To alleviate the problem,
GIZA++ resorts to a greedy search algorithm. The basic idea is to compute a Viterbi
alignment of a simple model such as Model 2 or HMM. This alignment (an intermediate
node in the search space) is then iteratively improved with respect to the alignment
probability of the refined model by moving or swapping links. In contrast, our search
algorithm starts from an empty alignment and has only one operation: adding a link.
In addition, we treat the fertility probability of an empty cept in a different way (see
Equation B.7). Interestingly, Vigne achieves slightly better results than GIZA++ for both
models. All differences are not statistically significant.
4.1.2 Comparison to Generative Models Using Asymmetric Features. Table 7 compares the
AER scores achieved by GIZA++, Cross-EM (Liang, Taskar, and Klein 2006), and Vigne.
On both tasks, we lowercased all English words in the training, development, and
test sets as a preprocessing step. For GIZA++, we used the default training scheme
of 15H53545. We used the three symmetrization heuristics proposed by Och and Ney
(2003): intersection, union, and refined method. For Cross-EM, we also used the default
configuration and jointly trained Model 1 and HMM for five iterations. For Vigne, we
used a greedy search strategy by setting β = 0 and b = 1. Note that both GIZA++ and
Cross-EM are unsupervised alignment methods.
On the English–French task, the refined combination of Model 4 alignments pro-
duced by GIZA++ in both translation directions yields an AER of 5.9%. Cross-EM
outperforms GIZA++ significantly by achieving 5.1%. For Vigne, we use Model 4 as
the primary feature. The linear combination of Model 4 in both directions achieves
a lower AER than either one separately. The link count feature controls the number
of links in the resulting alignments and leads to an absolute improvement of 0.1%.
With the addition of cross count and neighbor count features, the AER score drops to
5.4%. We attribute this to the fact that the two features are capable of capturing the
locality and monotonicity properties of natural languages, especially for closely related
language pairs such as English–French. After adding the linked word count feature, our
model achieves an AER of 5.2%. Finally, Vigne achieves an AER of 4.0% by combining
predictions from refined Model 4 and jointly trained HMM.
On the Chinese–English task, one-to-many and many-to-one relationships occur
more frequently in the reference alignments than the English–French task. As Cross-EM
</bodyText>
<footnote confidence="0.98685">
6 In GIZA++ training, the final parameters are estimated on the final alignments, which are computed
using the parameters obtained in the previous iteration. As a result, Vigne made use of the parameters
generated by the iteration before the final iteration. In other experiments, Vigne used the final parameters.
</footnote>
<page confidence="0.996169">
324
</page>
<note confidence="0.989836">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<tableCaption confidence="0.772205166666667">
Table 7
Comparison of GIZA++, Cross-EM, and Vigne on both tasks. Note that Vigne yields only
one-to-one alignments if both “Model 4 s2t” and “Model 4 t2s” features are used. The pruning
setting for Vigne is R = 0 and b = 1. While the final results of our system are better than the best
baseline generative models significantly at p &lt; 0.01, adding a single feature will not always
produce a significant improvement, especially for English–French.
</tableCaption>
<table confidence="0.998431764705882">
System Setting English–French Chinese–English
Model 4 s2t 7.7 20.9
Model 4 t2s 9.2 30.3
GIZA++ Intersection 6.8 21.8
Union 9.6 28.1
Refined method 5.9 18.4
Cross-EM FEMM, joint 5.1 18.9
Model 4 s2t 7.8 20.5
+Model 4 t2s 5.6 18.3
+link count 5.5 17.7
+cross count 5.4 17.6
Vigne +neighbor count 5.2 17.4
+exact match 5.3 -
+linked word count 5.2 17.3
+bilingual dictionary - 17.1
+link co-occurrence count (GIZA++) 5.1 16.3
+link co-occurrence count (Cross-EM) 4.0 15.7
</table>
<bodyText confidence="0.999448045454545">
is prone to produce one-to-one alignments by encouraging agreement, symmetrizing
Model 4 by refined method yields better results than Cross-EM. We observe that the ad-
vantages of adding features such as link count, cross count, neighbor count, and linked
word count to our linear model continue to hold, resulting in a much lower AER than
both GIZA++ and Cross-EM. The addition of the bilingual dictionary is beneficial and
yields an AER of 17.1%. Further improvements were obtained by including predictions
from GIZA++ and Cross-EM.
As the IBM models do not allow a source word to be aligned with more than one
target word, the activation of the IBM models in both directions always yields one-
to-one alignments and thus has a loss in recall. To alleviate this problem, we use a
heuristic postprocessing step to produce many-to-one or one-to-many alignments. First,
we collect links that have higher translation probabilities than corresponding null links
in both directions. Then, these candidate links are sorted according to their translation
probabilities. Finally, they are added to the alignments under structural constraints
similar to those of Och and Ney (2003).
On the English–French task, this symmetrization method achieves relatively small
but very consistent improvements ranging from 0.1% to 0.2%. On the Chinese–English
task, the improvements are more significant, ranging from 0.1% to 0.8%. This differ-
ence also results from the fact that the reference alignments of the Chinese–English
task contain more one-to-many and many-to-one relationships than the English–French
task. After symmetrization, the final AER scores for the two tasks are 3.8% and 15.1%,
respectively.
</bodyText>
<footnote confidence="0.8548855">
4.1.3 Resulting Feature Weights. Table 8 shows the resulting feature weights of minimum
error rate training. We observe that adding new features has an effect on the weights
</footnote>
<page confidence="0.995962">
325
</page>
<table confidence="0.412334">
Computational Linguistics Volume 36, Number 3
</table>
<tableCaption confidence="0.8000356">
Table 8
Resulting feature weights of minimum error rate training on the Chinese–English task (M4ST:
Model 4 s2t; M4TS: Model 4 t2s; LC: link count; CC: cross count; NC: neighbor count; LWC:
linked word count; BD: bilingual dictionary; LCCG: link co-occurrence count (GIZA++); LCCC:
link co-occurrence count (Cross-EM)).
</tableCaption>
<table confidence="0.9985848">
M4ST M4TS LC CC NC LWC BD LCCG LCCC
M4ST 1.00 - - - - - - - -
+M4TS 0.63 0.37 - - - - - - -
+LC 0.18 0.07 −0.75 - - - - - -
+CC 0.19 0.07 −0.56 −0.18 - - - - -
+NC 0.12 0.06 −0.55 −0.08 0.17 - - - -
+LWC 0.14 0.08 −0.22 −0.08 0.25 −0.26 - - -
+BD 0.07 0.02 −0.35 −0.05 0.16 0.01 0.34 - -
+LCCG 0.03 0.04 −0.13 −0.05 0.20 −0.16 0.28 0.11 -
+LCCC 0.02 0.02 0.14 −0.03 0.10 −0.26 0.30 0.04 0.09
</table>
<bodyText confidence="0.990171545454546">
of other features. The weights of the cross count feature are consistently negative,
suggesting that crossing links are always discouraged for Chinese–English. Also, the
positive weights of the neighbor count feature indicate that monotonic alignments
are encouraged. When the bilingual dictionary was included, the weights of Model 4
features in both directions dramatically decreased.
4.1.4 Results of the Symmetric Alignment Model. As we mentioned before, the linear model
can model many-to-many alignments directly without any postprocessing symmetriza-
tion heuristics.
Table 9 demonstrates the results of the symmetric alignment model on both tasks.
As the activation of translation and fertility probability products allows for arbitrary
relationships, the addition of the link count feature excludes most loosely related links
</bodyText>
<tableCaption confidence="0.9477554">
Table 9
AER scores achieved by the symmetric alignment model on both tasks. The pruning setting for
Vigne is R = 0 and b = 1. Although the final model obviously outperforms the initial model
significantly at p &lt; 0.01, adding a single feature will not always result in a significant
improvement, especially for English–French.
</tableCaption>
<table confidence="0.885635923076923">
Features English–French Chinese–English
translation probability product 17.3 23.6
+fertility probability product 14.6 22.6
+link count 14.5 21.6
+cross count 5.8 18.5
+neighbor count 5.2 17.2
+exact match 5.1 -
+linked word count 5.2 17.0
+link types 5.0 16.9
+sibling distance 4.9 16.2
+bilingual dictionary - 15.9
+link co-occurrence count (GIZA++) 4.5 15.1
+link co-occurrence count (Cross-EM) 3.7 14.5
</table>
<page confidence="0.998065">
326
</page>
<note confidence="0.967947">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<bodyText confidence="0.995772958333334">
and results in more significant improvements than for asymmetric IBM models. One
interesting finding is that the cross count feature is very useful, leading to dramatic
absolute reduction of 8.7% on the English–French task and 3.1% on the Chinese–English
task, respectively. We find that the advantages of adding neighbor count and linked
word count still hold. By further including predictions from GIZA++ and Cross-EM,
our linear model achieves the best result: 3.7% on the English–French task and 14.5%
on the Chinese–English task.
We find that the symmetric linear model outperforms the asymmetric one, espe-
cially on the Chinese–English task. This suggests that although the asymmetric model
can produce symmetric alignments via symmetrization heuristics, the “genuine” sym-
metric model produces many-to-many alignment in a more natural way.
4.1.5 Effect of Beam Search. Table 10 shows the effect of varying beam widths. The aligning
speed (words per second) decreases almost linearly with the increase of beam width b.
For simple alignment models such as using only the translation probability product
feature, enlarging the beam size fails to bring improvements due to modeling errors.
When more features are added, the model becomes more expressive. Therefore, our
system benefits from larger beam size consistently, although some benefits are not
significant statistically. When we set b = 10, the final AER scores for the English–French
and Chinese–English tasks are 3.6% and 14.3%, respectively.
4.1.6 Effect of Training Corpus Size. One disadvantage of our approach is that we need
a hand-aligned training corpus for training feature weights. However, compared with
building a treebank, manual alignment is far less expensive because one annotator only
needs to answer yes–no questions: Should this pair of words be aligned or not? If well
trained, even a non-linguist who is familiar with both source and target languages could
</bodyText>
<tableCaption confidence="0.978659">
Table 10
</tableCaption>
<table confidence="0.893958111111111">
Comparison of aligning speed (words per second) and AER score with varying beam widths for
the Chinese–English task. We fix R = 0.01. Bold numbers refer to the results that are better than
the baseline but not significantly so. We use “+” to denote the results that outperform the best
baseline (b = 1) and are statistically significant at p &lt; 0.05. Similarly, we use “++” to denote
significantly better than baseline at p &lt; 0.01.
b=1 b=5 b=10
Features w/sec AER w/sec AER w/sec AER
translation probability product 3,941 23.6 843 23.6 426 23.7
+fertility probability product 1,418 22.6 300 22.7 150 22.9
+link count 1,557 21.6 330 21.7 166 21.9
+cross count 1,696 18.5 359 18.6 180 18.6
+neighbor count 1,648 17.2 355 16.8+ 178 16.7+
+linked word count 1,627 17.0 351 16.4+ 176 16.5+
+sibling distance 1,531 16.9 326 16.5+ 165 16.4+
+link types 899 16.2 187 15.6+ 96 15.5++
+bilingual dictionary 890 15.9 187 15.6 94 15.5+
+link co-occurrence count (GIZA++) 877 15.1 182 15.0 92 14.9
+link co-occurrence count (Cross-EM) 867 14.5 183 14.4 92 14.3
</table>
<page confidence="0.899329">
327
</page>
<note confidence="0.437647">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.997195090909091">
produce high-quality alignments. We estimate that aligning a sentence pair usually
takes only two minutes on average.
An interesting question is: How many training examples are needed to train a
good discriminative model? Figure 4 shows the learning curves with different numbers
of features on the Chinese–English task. We choose four feature groups with varying
numbers of features: 3, 6, 10, and 14. There are eight fractions of the training corpus: 10,
20, 50, 100, 200, 300, 400, and 502. Generally, the more features a model uses, the more
training examples are needed to train feature weights. Surprisingly, even when we use
14 features, 50 sentences seem to be good enough for minimum error rate training. This
finding suggests that our approach could work well even with a quite small training
corpus.
4.1.7 Summary of Results. Table 11 summarizes the results on all three shared tasks.
Vigne used the same configuration for all tasks. We used the symmetric linear model
and activated all features. The pruning setting is β = 0.01 and b = 10. Our system
outperforms the systems participating in all the three shared tasks significantly.
Note that for the English–French task we used the small development set of 37
sentences to optimize feature weights, and ran our system on the original test set of
447 sentences. For the Romanian–English language pair, we follow Fraser and Marcu
(2006) in reducing the vocabulary by stemming Romanian and English words down to
their first four characters. For the other language pairs, English–Inuktitut and English–
Hindi, the symmetric linear model maintains its superiority over the asymmetric linear
model and yields better results than the other participants.
</bodyText>
<figureCaption confidence="0.640155">
Figure 4
</figureCaption>
<bodyText confidence="0.928598666666667">
Effect of training corpus size on the Chinese–English task. We choose four feature groups with
varying numbers of features: 3, 6, 10, and 14. There are eight training corpora with varying
numbers of sentence pairs: 10, 20, 50, 100, 200, 300, 400, and 502.
</bodyText>
<page confidence="0.997598">
328
</page>
<note confidence="0.992744">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<tableCaption confidence="0.7664672">
Table 11
Comparison with the systems participating in the three shared tasks. “non-null” denotes that the
reference alignments have no null links, “null” denotes that the reference alignments have null
links, “limited” denotes only limited resources can be used, and “unlimited” denotes that there
are no restrictions on resources used.
</tableCaption>
<table confidence="0.999602777777778">
Shared Task Task Participants Vigne
Romanian–English, non-null, limited 28.9–52.7 23.5
HLT-NAACL 2003 Romanian–English, null, limited 37.4–59.8 26.9
English–French, non-null, limited 8.5–29.4 4.0
English–French, null, limited 18.5–51.7 4.6
English–Inuktitut, limited 9.5–71.3 8.9
ACL 2005 Romanian–English, limited 26.6–44.5 24.7
English–Hindi, limited 51.4 44.8
HTRDP 2005 Chinese–English, unlimited 23.5–49.2 14.3
</table>
<subsubsectionHeader confidence="0.88958">
4.1.8 Comparison to Other Work. In the word alignment literature, the Canadian Hansard
</subsubsectionHeader>
<bodyText confidence="0.9903436">
bilingual corpus is the most widely used data set. Table 12 lists alignment error rates
achieved by previous work and our system. Note that direct comparisons are problem-
atic due to the different configurations of training data, development data, and test data.
Our result matches the state-of-the-art performance on the Hansard data (Lacoste-Julien
et al. 2006; Moore, Yih, and Bode 2006).
</bodyText>
<subsectionHeader confidence="0.999338">
4.2 Evaluation of Translation Quality
</subsectionHeader>
<bodyText confidence="0.994801666666667">
In this section, we report on experiments with Chinese-to-English translation. To inves-
tigate the effect of our discriminative model on translation performance, we used three
translation systems:
</bodyText>
<listItem confidence="0.9976718">
1. Moses (Koehn and Hoang 2007), a state-of-the-art phrase-based SMT
system;
2. Hiero (Chiang 2007), a state-of-the-art hierarchical phrase-based system;
3. Lynx (Liu, Liu, and Lin 2006), a linguistically syntax-based system that
makes use of tree-to-string rules.
</listItem>
<tableCaption confidence="0.998004">
Table 12
</tableCaption>
<table confidence="0.9600532">
Comparison of some word alignment systems on the Canadian Hansard data.
System Training Test AER
Och and Ney (2003) 1.5M 500 5.2
Moore (2005) 500K 223 7.5
Taskar, Lacoste-Julien, and Klein (2005) 1.1M 347 5.4
Liang, Taskar, and Klein (2006) 1.1M 347 4.9
Lacoste-Julien et al. (2006) 1.1M 247 3.8
Blunsom and Cohn (2006) 1.1M 347 5.2
Moore, Yih, and Bode (2006) 1.1M 223 3.7
This work 1.1M 247 3.6
</table>
<page confidence="0.922813">
329
</page>
<note confidence="0.545773">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.999468344827586">
For all three systems we trained the translation models on the FBIS corpus
(7.2M+9.2M words). For the language model, we used the SRI Language Modeling
Toolkit (Stolcke 2002) to train a trigram model with modified Kneser-Ney smoothing
on the Xinhua portion of the Gigaword corpus. We used the 2002 NIST MT evaluation
test set as the development set for training feature weights of translation systems, the
2005 test set as the devtest set for choosing optimal values of α for different translation
systems, and the 2008 test set as the final test set. Our evaluation metric is case-sensitive
BLEU-4, as defined by NIST, that is, using the shortest (as opposed to closest) reference
length for brevity penalty.
We annotated the first 200 sentences of the FBIS corpus using the Blinker guidelines
(Melamed 1998). All links are sure ones. These hand-aligned sentences served as the
training corpus for Vigne. To train the feature weights in our discriminative model
using minimum-error-rate training (Och 2003), we adopt balanced F-measure (Fraser
and Marcu 2007b) as the optimization criterion.
The pipeline begins by running GIZA++ and Cross-EM on the FBIS corpus. We
used seven generative alignment methods based on IBM Model 4 and HMM as baseline
systems: (1) CSE, (2) ESC, (3) intersection, (4) union, (5) refined method (Och and
Ney 2003), (6) grow-diag-final (Koehn, Och, and Marcu 2003), and (7) Cross-EM (Liang,
Taskar, and Klein 2006). Instead of exploring the entire search space, our linear model
only searches within the union of baseline predictions, which enables our system to
align large bilingual corpus at a very fast speed of 3, 000 words per second. In other
words, our system is able to annotate the FBIS corpus in about 1.5 hours. Then, we train
the feature weights of the linear model on the training corpus with respect to F-measure
under different settings of α. After that, our system runs on the FBIS corpus to produce
word alignments using the optimized weights. Finally, the three SMT systems train their
models on the word-aligned FBIS corpus.
Can our approach achieve higher F-measure scores than generative methods with
different values of α (the weighting factor in F-measure)? Table 13 shows the results of
all the systems on the development set. To estimate the loss from restricting the search
</bodyText>
<tableCaption confidence="0.639412">
Table 13
</tableCaption>
<bodyText confidence="0.965519">
Maximization of F-measure with different settings of α (the weighting factor in the balanced
F-measure). We use IBM Model 4 and HMM as baseline systems. Our system restricts the search
space by exploring only the union of baseline predictions. We compute the “oracle” alignments
by intersecting the union with reference alignments. We use “+” to denote the result that
outperforms the best baseline result with statistical significance at p &lt; 0.05. Similarly, we use
“++” to denote significantly better than baseline at p &lt; 0.01.
</bodyText>
<table confidence="0.996104">
α = 0.1 α = 0.3 α = 0.5 α = 0.7 α = 0.9
IBM Model 4 CSE 82.6 81.3 80.1 79.0 77.8
IBM Model 4 ESC 68.2 70.5 73.0 75.7 78.5
IBM Model 4 intersection 63.6 68.9 75.2 82.8 92.1
IBM Model 4 union 86.6 82.0 77.9 74.2 70.8
IBM Model 4 refined method 75.4 78.5 81.8 85.4 89.4
IBM Model 4 grow-diag-final 82.4 82.1 81.7 81.4 81.1
Cross-EM HMM 70.4 73.7 77.3 81.2 85.5
oracle 91.9 93.6 95.3 97.1 99.0
Vigne 87.8++ 85.8++ 86.4++ 88.6++ 93.3++
</table>
<page confidence="0.997323">
330
</page>
<note confidence="0.978215">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<bodyText confidence="0.999955363636364">
space, we compute oracle alignments by intersecting the union of baseline predictions
with reference alignments. The F-measures achieved by oracle alignments range from
91.9 to 99.0, indicating that the union of baseline predictions is good enough to approx-
imate the true search space. We observe that C→E, union, and grow-diag-final weight
recall higher because F-measure decreases when α increases. On the other hand, E→C,
intersection, refined method, and Cross-EM weight precision higher. In particular, α
has a weak effect on grow-diag-final as its F-measure always keeps above 0.8 when α
is varied. For each α, we trained a set of feature weights to maximize the F-measure on
the development set. We find that our discriminative model outperforms the baseline
systems significantly at all values of α.
Table 14 shows the BLEU scores of the three systems on the devtest set. For Moses
and Hiero, we used the default setting. For Lynx, we used the phrase pairs learned by
Moses to improve rule coverage (Liu, Liu, and Lin 2006). The best generative alignment
method is grow-diag-final, which is widely used in SMT. For all the three SMT systems,
our system outperforms the baseline systems statistically significantly. For Moses, the
best value of α is 0.5. For Hiero and Lynx, the best α is 0.3, suggesting that recall-
oriented alignments yield better translation performance.
Table 15 gives the BLEU scores of the three systems on the final test set. We used the
parameters optimized on the dev and devtest sets. More specifically, Moses used grow-
diag-final and α = 0.5, Hiero used grow-diag-final and α = 0.3, and Lynx used union
and α = 0.3. We find that our discriminative alignment model improves the three
systems significantly.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="method">
5. Related Work
</sectionHeader>
<bodyText confidence="0.99964875">
The first generative alignment models were the IBM Models 1–5 proposed by Brown
et al. (1993). Vogel and Ney (1996) propose a first-order Hidden Markov model (HMM)
for word alignment. They show that it is beneficial to make the alignment probabilities
dependent on differences in position rather than on the absolute positions. Och and
</bodyText>
<tableCaption confidence="0.801554">
Table 14
</tableCaption>
<bodyText confidence="0.614731666666667">
BLEU scores on the devtest set. We use “+” to denote the result that outperforms the best
baseline result (highlighted in bold) statistically significantly at p &lt; 0.05. Similarly, we use “++”
to denote significantly better than baseline at p &lt; 0.01.
</bodyText>
<table confidence="0.98955415">
Moses Hiero Lynx
24.7 25.7 24.8
20.6 23.5 21.6
20.1 23.2 21.2
24.3 24.1 25.1
24.2 24.0 24.2
25.0 25.8 24.3
23.6 24.9 24.8
23.9 25.3 26.0++
24.9 26.8++ 26.1++
25.7+ 26.6++ 24.3
23.7 25.4 24.7
21.9 24.7 23.9
IBM Model 4 C→E
IBM Model 4 E→C
IBM Model 4 intersection
IBM Model 4 union
IBM Model 4 refined method
IBM Model 4 grow-diag-final
Cross-EM HMM
</table>
<bodyText confidence="0.7720774">
α = 0.1 tuned
α = 0.3 tuned
Vigne α = 0.5 tuned
α = 0.7 tuned
α = 0.9 tuned
</bodyText>
<page confidence="0.965417">
331
</page>
<note confidence="0.442067">
Computational Linguistics Volume 36, Number 3
</note>
<tableCaption confidence="0.989748">
Table 15
</tableCaption>
<bodyText confidence="0.987677">
BLEU scores on the final test set. We use the parameters optimized on the dev and devtest sets.
We use “+” to denote the result that outperforms the best baseline result (indicated in bold)
statistically significantly at p &lt; 0.05. Similarly, we use “++” to denote significantly better than
baseline at p &lt; 0.01.
</bodyText>
<subsectionHeader confidence="0.873289">
Moses Hiero Lynx
</subsectionHeader>
<bodyText confidence="0.993994146341463">
generative 20.1 20.7 19.9
discriminative 20.8+ 21.6+ 21.0++
Ney (2003) re-implement the IBM models and the HMM model and compare them with
heuristic approaches systematically. The resulting toolkit GIZA++ developed by Franz
J. Och is the most popular alignment system nowadays. Liang, Taskar, and Klein (2006)
present an unsupervised way to produce symmetric alignments by training two simple
asymmetric models (e.g., IBM Model 1 and the HMM model) jointly to maximize a
combination of data likelihood and agreement between the models. Fraser and Marcu
(2007a) introduce a new generative model called LEAF that directly models many-
to-many non-consecutive word alignments. Their model can be trained using both
unsupervised and semi-supervised training methods.
Recent years have witnessed the rapid development of discriminative alignment
methods. As a first attempt, Och and Ney (2003) proposed the Model 6, which is a
log-linear combination of the IBM models and the HMM model. Cherry and Lin (2003)
develop a statistical model to find word alignments, which allows for easy integration
of context-specific features. Liu, Liu, and Lin (2005) apply the log-linear model used
in SMT (Och and Ney 2002) to word alignment and report significant improvements
over the IBM models. Moore (2005) presents a discriminative framework for word
alignment and uses averaged perceptron for parameter optimization. Taskar, Lacoste-
Julien, and Klein (2005) treat the alignment prediction task as a maximum weight
bipartite matching problem and use the large-margin method to train feature weights.
Neural networks and transformation-based learning have also been introduced to word
alignment (Ayan, Dorr, and Monz 2005a, 2005b). Blunsom and Cohn (2006) propose a
new discriminative model based on conditional random fields (CRF). Fraser and Marcu
(2006) use sub-models of IBM Model 4 as features and train feature weights using a
semi-supervised algorithm. Ayan and Dorr (2006b) use a maximum entropy model to
combine word alignments. Cherry and Lin (2006) show that introducing soft syntactic
constraints through discriminative training can improve alignment quality. Lacoste-
Julien et al. (2006) extend the bipartite matching model of Taskar, Lacoste-Julien, and
Klein (2005) by including fertility and first-order interactions. Recently, max-product
belief propagation has been successfully applied to discriminative word alignment
(Niehues and Vogel 2008; Cromier`es and Kurohashi 2009). Haghighi et al. (2009) investi-
gate supervised word alignment methods that exploit inversion transduction grammar
(ITG) constraints.
Our work can be seen as an application of the linear model (Och 2003) in word
alignment. While aiming at producing symmetric word alignments in a discriminative
way, our approach uses asymmetric generative models (Brown et al. 1993) as the major
information sources. Our linear model is similar to that of Moore, Yih, and Bode (2006).
They train two linear models called stage 1 and stage 2. The feature values are extracted
from word-aligned sentence pairs. After the stage 1 model aligns the entire training
corpus automatically, the stage 2 model uses features based not only on the parallel
</bodyText>
<page confidence="0.990612">
332
</page>
<note confidence="0.97524">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<bodyText confidence="0.999881166666667">
sentences themselves but also on statistics of the alignments produced by the stage 1
model. They use average perceptron and support vector machine (SVM) to train feature
weights and use a beam search algorithm to find the most probable alignments. Table 12
shows that the two methods achieve comparable results on the Hansard data, confirm-
ing Moore, Yih, and Bode’s (2006) claim that model structure and feature selection are
more important than discriminative training method.
</bodyText>
<sectionHeader confidence="0.995861" genericHeader="method">
6. Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999990066666667">
We have presented a discriminative framework for word alignment based on the linear
modeling approach. This framework is easy to extend by including features that char-
acterize the aligning process. In addition, our approach supports symmetric alignment
modeling that allows for an arbitrary relationship between source and target language
positions. As the linear model offers excellent flexibility in using a large variety of
features and in combining information from various sources, it is able to produce good
predictions on language pairs that are either closely related (e.g., English–French) or dis-
tantly related (e.g., English–Inuktitut), either with rich resources (e.g., Chinese–English)
or with scarce resources (e.g., English–Hindi). We further show that our approach can
benefit different types of SMT systems: phrase-based, hierarchical phrase-based, and
syntax-based.
The real benefit of our model does not stem from the use of the linear model, but
rather from the discriminative training that optimizes feature weights with respect to
evaluation metrics on the gold-standard word alignments. One disadvantage of our
approach is the need for annotated training data. Although we have shown that a
very small number of training examples would be enough for parameter estimation
(Section 4.1.6), it is difficult to select such a representative training corpus to ensure that
the model will work well on unseen data, especially when the bilingual corpus to be
aligned consists of parallel texts from different domains.
Another problem is that it is hard to find an evaluation metric for word alignment
that correlates well with translation quality because the relationship between alignment
and translation is still not quite clear. Without a good loss function, discriminative
models cannot outperform generative models in large-scale applications. Therefore, it is
important to investigate how to select training examples and how to choose optimiza-
tion criterion.
The design of feature functions is most important for a discriminative alignment
model. Often, we need to try various feature groups manually on the development set
to determine the optimal feature group. Furthermore, a feature group optimized for one
language pair may not have the same effect on another one. In the future, we plan to
investigate an algorithm for automatic feature selection.
</bodyText>
<sectionHeader confidence="0.963596" genericHeader="method">
Appendix A: Table of Notation
</sectionHeader>
<bodyText confidence="0.965548">
f source sentence
fS sequence of source sentences: f1, ... , fs, ... , fS
</bodyText>
<page confidence="0.303933">
1
</page>
<bodyText confidence="0.9087268">
f source word
J length of f
j position in f, j = 1, 2, ... , J
fj the j-th word in f
f0 empty cept on the source side
</bodyText>
<page confidence="0.996717">
333
</page>
<figure confidence="0.7959158">
Computational Linguistics Volume 36, Number 3
e target sentence
eS sequence of target sentences: e1,. . . , es, ... , eS
1
e target word
I length of e
i position in e, i = 1, 2,.. . , I
ei the i-th word in e
e0 empty cept on the target side
a alignment
</figure>
<bodyText confidence="0.734673714285714">
l a link (j, i) in a
ψj number of positions of e connected to position j of f
φi number of positions of f connected to position i of e
ωj,k position of the k-th target word aligned to fj
πi,k position of the k-th source word aligned to ei
Δ(j, ψj) sum of sibling distances for fj
0(i, φi) sum of sibling distances for ei
</bodyText>
<construct confidence="0.514497888888889">
score(f, e, a) a score that indicates how well a is the alignment between f and e
aˆ the best candidate alignment
λ feature weight
γ the feature weight being optimized
h(f, e, a) feature function
9(f, e, a,l) link gain after adding l to a
g(f, e, a,l) feature gain after adding l to a
μ(f, e, a) value of the feature being optimized
σ(f, e, a) dot-product of fixed features
</construct>
<bodyText confidence="0.960972307692308">
t(e|f ) the probability that f is translated to e
t( f |e) the probability that e is translated to f
n(ψ|f ) the probability that f has a fertility of ψ
n(φ|e) the probability that e has a fertility of φ
r reference alignment
Cs set of candidate alignments for the s-th training example
as,k the k-th candidate alignment for the s-th training example
E(r, a) loss function that measures alignment quality
α the precision/recall weighting factor in balanced F-measure
β pruning threshold in the beam search algorithm
b beam size in the beam search algorithm
δ(x, y) the Kronecker function, which is 1 if x = y and 0 otherwise
Qexpr� an indicator function taking a boolean expression expr as the argument
</bodyText>
<sectionHeader confidence="0.85988" genericHeader="conclusions">
Appendix B: Using the IBM Models as Feature Functions
</sectionHeader>
<bodyText confidence="0.992065">
In this article, we use IBM Models 1–4 as feature functions by taking the logarithm of the
models themselves rather than the sub-models just for simplicity. It is easy to separate
each sub-model as a feature as suggested by Fraser and Marcu (2006). We distinguish
</bodyText>
<page confidence="0.996716">
334
</page>
<note confidence="0.975362">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<bodyText confidence="0.99695525">
between two translation directions (i.e., source-to-target and target-to-source) to use the
IBM models as feature functions. All model parameters are estimated by GIZA++ (Och
and Ney 2003).
The feature function for the IBM Model 1 is
</bodyText>
<equation confidence="0.955513666666667">
C J
hm1 (f, e, a) = log E(J|I)rlt(fj|eaj) (B.1) + 1)J
j=1
</equation>
<bodyText confidence="0.974814230769231">
where c(J|I) predicts the length of the source sentence conditioned on that of the target
sentence, (I + 1)−J defines a uniform distribution of the alignment between source and
target words, and t( fj|ei) is a translation sub-model. Note that aj = i, which means that
fj is connected to ei.
The corresponding feature gain is
gm1(f, e, a,l) = log(t(fj|ei)) − log(t( fj|e0)) (B.2)
where fj and ei are linked by l and e0 is the empty cept to which all unaligned source
words are “aligned.”
Based on a similar generative story to Model 1, Model 2 replaces the uniform
alignment probability distribution with an alignment sub-model a(i|j, I, J). This sub-
model assumes that the position of ei depends on the position of its translation fj and
sentence lengths I and J.
The feature function for Model 2 is
</bodyText>
<equation confidence="0.939606">
J
hm2 (f, e, a) = log (c (J|I) rl t(fj  |eaj)a(aj |j, I,J)) (B.3)
j=1
</equation>
<bodyText confidence="0.691366">
The corresponding feature gain is
</bodyText>
<equation confidence="0.9913095">
gm2(f, e, a, l) = log(t( fj|ei)) − log(t(fj|e0)) +
log(a(i|j,I,J)) − log(a(0|j, I,J)) (B.4)
</equation>
<bodyText confidence="0.9995875">
where fj and ei are linked by l and 0 is the index of the empty cept e0.
Model 3 is a fertility-based model that parameterizes fertility of words. Unlike
Model 2, Model 3 uses a fertility sub-model n(φi|ei) and a distortion sub-model d(j|i, I,J).
Formally, the feature function of Model 3 is given by
</bodyText>
<equation confidence="0.920446833333333">
� I I J
hm3 (a, f, e) = log
n0 (φ0  |E φi) rl n(φi  |ei)φi! rl t( fj|eaj)
i=1 i=1 j=1
rlX �d(j|i, I, J) (B.5)
j:ajo0
</equation>
<bodyText confidence="0.9685615">
Brown et al. (1993) treat n0(φ0 |EIi=1 φi), the fertility probability of e0, in a differ-
ent way. They assume that at most half of the source words in an alignment are not
</bodyText>
<page confidence="0.993025">
335
</page>
<figure confidence="0.3636616">
Computational Linguistics Volume 36, Number 3
aligned (i.e., 4)0 &lt; J2) and define a binomial distribution relying on an auxiliary parame-
ter p0:
n0 1 �0 4)i J = ( �J φo 0)pJ0 2φ0 (1 − p0)φ0if 4)0 &lt; 2 (B.6)
\ 111 0 otherwise
</figure>
<bodyText confidence="0.963038714285714">
Note that we follow Brown et al. (1993) in replacing EIi=1 4)i with J − 4)0 for simplicity.
The original form should be (YIi=1 φi �p�I i=1 φi−φ0(1 − p0)φ0.
φ0 0
However, this assumption results in a problem for our search algorithm that begins
with an empty alignment (see Algorithm 1), for which 4)0 is J and the feature value
hm3(f, e, a) is negative infinity. To alleviate this problem, we modify Equation B.6 slightly
by adding a smoothing parameter pn E (0, 1):
</bodyText>
<equation confidence="0.9812347">
I (J−φ0)pJ0 2φ0(1 − p0)φ0pn if 4)0 &lt; i
n0 1 4)o 4)i J = 1−pn otherwise (B.7)
\ � J
i=1 2 J
Therefore, the feature gain for Model 3 is
gm3(f,e,a,l) = log(gn0(J, 4)0)) +
log(n(4)i + 1|ei)) − log(n(4)i|ei)) +
log(4)i + 1) +
log(t( fj|ei)) − log(t(fj|e0)) +
log(d(j|i, I, J)) (B.8)
</equation>
<bodyText confidence="0.997352">
where fj and ei are linked by l, 4)i is the fertility before adding l, and gn0(J, 4)0) is the gain
for n0(4)0 |EIi=1 4)i):
</bodyText>
<figure confidence="0.949782">
i =1
φ0×(J−φ0+1) if 4)0 &lt; J
(J−2φ0+1)×(J−2φ0+2) 2
1−pn
(J−φ0+1
φ0−1 )×pJ−2φ0+2 ×(1−p0)φ0−1×pn×� 2 J �
0
1 otherwise
(B.9)
J2 &lt; 4)0 &lt; J2 + 1
⎧
⎨⎪⎪
⎪⎪⎩
gn0(J,4)0) =
</figure>
<bodyText confidence="0.825917">
Model 4 defines a new distortion sub-model D(a) that relies on word classes A and
13 to capture movement of phrases. The feature function for Model 4 is
</bodyText>
<equation confidence="0.909605">
I I J
hm4(a,f,e) =log(n0(4)0 |�4)i)�n(4)i|ei)�t(fj |eaj)14)0!D(a) I (B.10)
</equation>
<bodyText confidence="0.729246">
where
</bodyText>
<equation confidence="0.9968405">
i i j
=1 =1 =1
D(a) = I �φi pik(7tik) (B.11)
i=1 k=1
</equation>
<page confidence="0.998963">
336
</page>
<note confidence="0.7831715">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
J d1(j − cρi  |A(eρi), B(τi1)) if k = 1
</note>
<bodyText confidence="0.9921074">
pik (j) = l (B.12)
d&gt;1(j − πi,k−1|B(τik)) otherwise
Brown et al. (1993) propose two distortion models for Model 4: d1(·) for the first
word of a tablet τ and d&gt;1(·) for the other words of the tablet. In Equation B.12, ρi is
the first position to the left of i for which φi &gt; 0, cρi is the ceiling of the average position
of the words in τρ, τik denotes the k-th French word aligned to ek, πi,k−1 denotes the
position of the k − 1-th French word aligned to ei, and A(·) and B(·) are word classes
for the source and target languages, respectively. Please refer to Brown et al. (1993) for
more details.
The corresponding feature gain is
</bodyText>
<equation confidence="0.9987524">
gm4(f,e,a,l) = log(gn0(J, φ0)) +
log(n(φi + 1|ei)) − log(n(φi|ei)) +
log(t(fj|ei)) − log(t(fj|e0)) +
log(φ0) +
log(D(a ∪ {l})) − log(D(a)) (B.13)
</equation>
<bodyText confidence="0.99293775">
where fj and ei are linked by l and φi is the fertility before adding l.
In Model 4, the addition of a single link might change the distortion probabilities
pik(j) of other links. As a result, we have to compute the overall distortion probabilities
D(a) every time.
</bodyText>
<sectionHeader confidence="0.997313" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9620105">
This work was supported by National
Natural Science Foundation of China,
Contract No. 60603095 and 60573188.
Thanks to the three anonymous reviewers
for their insightful and constructive
comments and suggestions. We are
grateful to Rada Mihalcea for giving us
the Romanian–English training data and
David Chiang for allowing us to use Hiero.
Stephan Vogel, Vamshi Ambati, and
Kelly Widmaier offered valuable feedback
on an earlier version of this article.
</bodyText>
<sectionHeader confidence="0.995518" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999481095238095">
Ayan, Necip Fazil and Bonnie J. Dorr. 2006a.
Going beyond AER: An extensive analysis
of word alignments and their impact on
MT. In Proceedings of COLING-ACL 2006,
pages 9–16, Sydney.
Ayan, Necip Fazil and Bonnie J. Dorr. 2006b.
A maximum entropy approach to
combining word alignments. In Proceedings
of HLT-NAACL 2006, pages 96–103, New
York, NY.
Ayan, Necip Fazil, Bonnie J. Dorr, and
Christof Monz. 2005a. Alignment link
projection using transformation-based
learning. In Proceedings of HLT-EMNLP
2005, pages 185–192, Vancouver.
Ayan, Necip Fazil, Bonnie J. Dorr, and
Christof Monz. 2005b. Neuralign:
Combining word alignments using neural
networks. In Proceedings of HLT-EMNLP
2005, pages 65–72, Vancouver.
Blunsom, Phil and Trevor Cohn. 2006.
Discriminative word alignment with
conditional random fields. In Proceedings
of COLING-ACL 2006, pages 65–72,
Sydney.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263–311.
Cherry, Colin and Dekang Lin. 2003. A
probability model to improve word
alignment. In Proceedings of ACL 2003,
pages 88–95, Sapporo.
Cherry, Colin and Dekang Lin. 2006. Soft
syntactic constraints for word alignment
through discriminative training. In
Proceedings of COLING-ACL 2006 (poster),
pages 105–112, Sydney.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
</reference>
<page confidence="0.953813">
337
</page>
<reference confidence="0.994919235294118">
Computational Linguistics Volume 36, Number 3
translation. In Proceedings of ACL 2005,
pages 263–270, Ann Arbor, MI.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201–228.
Cromier`es, Fabien and Sadao Kurohashi.
2009. An alignment algorithm using belief
propagation and a structure-based
distortion model. In Proceedings of EACL
2009, pages 166–174, Athens.
Fraser, Alexander and Daniel Marcu. 2006.
Semi-supervised training for statistical
word alignment. In Proceedings of
COLING-ACL 2006, pages 769–776,
Sydney.
Fraser, Alexander and Daniel Marcu. 2007a.
Getting the structure right for word
alignment: LEAF. In Proceedings of
EMNLP-CoNLL 2007, pages 51–60, Prague.
Fraser, Alexander and Daniel Marcu. 2007b.
Measuring word alignment quality for
statistical machine translation.
Computational Linguistics, 33(3):293–303.
Galley, Michel, Jonathan Graehl, Kevin
Knight, Daniel Marcu, Steve DeNeefe, Wei
Wang, and Ignacio Thayer. 2006. Scalable
inference and training of context-rich
syntactic translation models. In Proceedings
of COLING-ACL 2006, pages 961–968,
Sydney.
Haghighi, Aria, John Blitzer, John DeNero,
and Dan Klein. 2009. Better word
alignments with supervised ITG models.
In Proceedings of ACL-IJCNLP 2009,
pages 923–931, Suntec.
He, Xiaodong, Mei Yang, Jianfeng Gao,
Patrick Nguyen, and Robert Moore. 2008.
Indirect-HMM-based hypothesis
alignment for combining outputs from
machine translation systems. In
Proceedings of EMNLP 2008, pages 98–107,
Honolulu, HI.
Koehn, Philipp and Hieu Hoang. 2007.
Factored translation models. In Proceedings
of EMNLP-CoNLL 2007, pages 868–876,
Prague.
Koehn, Philipp, Franz J. Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of HLT-NAACL
2003, pages 127–133, Edmonton.
Lacoste-Julien, Simon, Ben Taskar, Dan Klein,
and Michael I. Jordan. 2006. Word
alignment via quadratic assignment.
In Proceedings of HLT-NAACL 2007,
pages 112–119, New York, NY.
Liang, Percy, Ben Taskar, and Dan Klein.
2006. Alignment by agreement. In
Proceedings of HLT-NAACL 2006,
pages 104–111, New York, NY.
Liu, Yang, Qun Liu, and Shouxun Lin. 2005.
Log-linear models for word alignment. In
Proceedings of ACL 2005, pages 459–466,
Ann Arbor, MI.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006.
Tree-to-string alignment template for
statistical machine translation. In
Proceedings of COLING-ACL 2006,
pages 609–616, Sydney.
Marcu, Daniel, Wei Wang, Abdessamad
Echihabi, and Kevin Knight. 2006. SPMT:
Statistical machine translation with
syntactified target language phrases. In
Proceedings of EMNLP 2006, pages 44–52,
Sydney.
Martin, Joel, Rada Mihalcea, and Ted
Pedersen. 2005. Word alignment for
languages with scarce resources. In
Proceedings of the ACL 2005 Workshop on
Building and Using Parallel Texts,
pages 65–74, Ann Arbor, MI.
Melamed, I. Dan. 1998. Annotation style
guide for the blinker project. Technical
report No. 98-06, University of
Pennsylvania, Philadelphia.
Melamed, I. Dan. 2000. Models for
translational equivalence among words.
Computational Linguistics, 26(2):221–249.
Mihalcea, Rada and Ted Pedersen. 2003.
An evaluation exercise for word
alignment. In Proceedings of HLT-NAACL
2003 Workshop on Building and Using
Parallel Texts, pages 1–10, Edmonton.
Moore, Robert C. 2005. A discriminative
framework for bilingual word alignment.
In Proceedings of HLT-EMNLP 2005,
pages 81–88, Vancouver.
Moore, Robert C., Wen-tau Yih, and Andreas
Bode. 2006. Improved discriminative
bilingual word alignment. In Proceedings
of COLING-ACL 2006, pages 513–520,
Sydney.
Niehues, Jan and Stephan Vogel. 2008.
Discriminative word alignment via
alignment matrix modeling. In
Proceedings of the Third Workshop on
Statistical Machine Translation, pages 18–25,
Columbus, OH.
Och, Franz J. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of ACL 2003, pages 160–167,
Sapporo.
Och, Franz J. and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of ACL 2002,
pages 295–302, Philadephia, PA.
Och, Franz J. and Hermann Ney. 2003. A
systematic comparison of various
</reference>
<page confidence="0.992196">
338
</page>
<note confidence="0.890378">
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
</note>
<reference confidence="0.999871433333334">
statistical alignment models.
Computational Linguistics, 29(1):19–51.
Och, Franz J. and Hermann Ney. 2004.
The alignment template approach
to statistical machine translation.
Computational Linguistics,
30(4):417–449.
Quirk, Chris, Arul Menezes, and Colin
Cherry. 2005. Dependency treelet
translation: Syntactically informed
phrasal SMT. In Proceedings of ACL 2005,
pages 271–279, Ann Arbor, MI.
Rosti, Antti-Veikko, Spyros Matsoukas, and
Richard Schwartz. 2007. Improved
word-level system combination for
machine translation. In Proceedings of ACL
2007, pages 312–319, Prague.
Stolcke, Andreas. 2002. SRILM—an
extensible language modeling toolkit. In
Proceedings of ICSLP 2002, pages 901–904,
Denver, CO.
Taskar, Ben, Simon Lacoste-Julien, and Dan
Klein. 2005. A discriminative matching
approach to word alignment. In
Proceedings of HLT-EMNLP 2005,
pages 73–80, Vancouver.
Vogel, Stephan and Hermann Ney. 1996.
HMM-based word alignment in statistical
translation. In Proceedings of COLING 1996,
pages 836–841, Copenhagen.
</reference>
<page confidence="0.999217">
339
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.710583">
<title confidence="0.971998">Discriminative Word Alignment by</title>
<author confidence="0.877055">Linear Modeling</author>
<affiliation confidence="0.981815333333333">Institute of Computing Technology Chinese Academy of Sciences Institute of Computing Technology Chinese Academy of Sciences Institute of Computing Technology Chinese Academy of Sciences</affiliation>
<abstract confidence="0.993342454545455">Word alignment plays an important role in many NLP tasks as it indicates the correspondence between words in a parallel text. Although widely used to align large bilingual corpora, generative models are hard to extend to incorporate arbitrary useful linguistic information. This article presents a discriminative framework for word alignment based on a linear model. Within this framework, all knowledge sources are treated as feature functions, which depend on a source language sentence, a target language sentence, and the alignment between them. We describe a number offeatures that could produce symmetric alignments. Our model is easy to extend and can be optimized with respect to evaluation metrics directly. The model achieves state-of-the-art alignment quality on three word alignment shared tasks for five language pairs with varying divergence and richness of resources. We further show that our approach improves translation performance for various statistical machine translation systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Going beyond AER: An extensive analysis of word alignments and their impact on MT.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>9--16</pages>
<location>Sydney.</location>
<contexts>
<context position="8947" citStr="Ayan and Dorr 2006" startWordPosition="1377" endWordPosition="1380">onnect content words such as Zhongguo and China. In contrast, possible links often align words within idiomatic expressions and free translations. An AER score is given by AER(S,P,A)=1_ |A n S |+ |A n P |(3) |A |+ |S| where S is a set of sure links in a reference alignment that is hand-aligned by human experts, P is a set of possible links in the reference alignment, and A is a candidate alignment. Note that S is a subset of P: S C_ P. The lower the AER score is, the better the alignment quality is. Although widely used, AER has been criticized for correlating poorly with translation quality (Ayan and Dorr 2006a; Fraser and Marcu 2007b). In other words, lower AER scores do not necessarily lead to better translation quality.1 Fraser and Marcu (2007b) argue that reference alignments should consist of only sure links. They propose a new measure called the balanced F-measure: precision(S,A) = |A nS |(4) |S| recall(S,A) = |A n S |(5) |A| 1 F-measure(S,α,A) = − (6) precis of n(S,A) + recall(S,A) 1 It has not yet been uniformly accepted that better word alignments yield better translations. Ayan and Dorr (2006a) present a detailed discussion of the impact of word alignment on statistical machine translatio</context>
<context position="72962" citStr="Ayan and Dorr (2006" startWordPosition="12257" endWordPosition="12260">nt and uses averaged perceptron for parameter optimization. Taskar, LacosteJulien, and Klein (2005) treat the alignment prediction task as a maximum weight bipartite matching problem and use the large-margin method to train feature weights. Neural networks and transformation-based learning have also been introduced to word alignment (Ayan, Dorr, and Monz 2005a, 2005b). Blunsom and Cohn (2006) propose a new discriminative model based on conditional random fields (CRF). Fraser and Marcu (2006) use sub-models of IBM Model 4 as features and train feature weights using a semi-supervised algorithm. Ayan and Dorr (2006b) use a maximum entropy model to combine word alignments. Cherry and Lin (2006) show that introducing soft syntactic constraints through discriminative training can improve alignment quality. LacosteJulien et al. (2006) extend the bipartite matching model of Taskar, Lacoste-Julien, and Klein (2005) by including fertility and first-order interactions. Recently, max-product belief propagation has been successfully applied to discriminative word alignment (Niehues and Vogel 2008; Cromier`es and Kurohashi 2009). Haghighi et al. (2009) investigate supervised word alignment methods that exploit inv</context>
</contexts>
<marker>Ayan, Dorr, 2006</marker>
<rawString>Ayan, Necip Fazil and Bonnie J. Dorr. 2006a. Going beyond AER: An extensive analysis of word alignments and their impact on MT. In Proceedings of COLING-ACL 2006, pages 9–16, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
</authors>
<title>A maximum entropy approach to combining word alignments.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL 2006,</booktitle>
<pages>96--103</pages>
<location>New York, NY.</location>
<contexts>
<context position="8947" citStr="Ayan and Dorr 2006" startWordPosition="1377" endWordPosition="1380">onnect content words such as Zhongguo and China. In contrast, possible links often align words within idiomatic expressions and free translations. An AER score is given by AER(S,P,A)=1_ |A n S |+ |A n P |(3) |A |+ |S| where S is a set of sure links in a reference alignment that is hand-aligned by human experts, P is a set of possible links in the reference alignment, and A is a candidate alignment. Note that S is a subset of P: S C_ P. The lower the AER score is, the better the alignment quality is. Although widely used, AER has been criticized for correlating poorly with translation quality (Ayan and Dorr 2006a; Fraser and Marcu 2007b). In other words, lower AER scores do not necessarily lead to better translation quality.1 Fraser and Marcu (2007b) argue that reference alignments should consist of only sure links. They propose a new measure called the balanced F-measure: precision(S,A) = |A nS |(4) |S| recall(S,A) = |A n S |(5) |A| 1 F-measure(S,α,A) = − (6) precis of n(S,A) + recall(S,A) 1 It has not yet been uniformly accepted that better word alignments yield better translations. Ayan and Dorr (2006a) present a detailed discussion of the impact of word alignment on statistical machine translatio</context>
<context position="72962" citStr="Ayan and Dorr (2006" startWordPosition="12257" endWordPosition="12260">nt and uses averaged perceptron for parameter optimization. Taskar, LacosteJulien, and Klein (2005) treat the alignment prediction task as a maximum weight bipartite matching problem and use the large-margin method to train feature weights. Neural networks and transformation-based learning have also been introduced to word alignment (Ayan, Dorr, and Monz 2005a, 2005b). Blunsom and Cohn (2006) propose a new discriminative model based on conditional random fields (CRF). Fraser and Marcu (2006) use sub-models of IBM Model 4 as features and train feature weights using a semi-supervised algorithm. Ayan and Dorr (2006b) use a maximum entropy model to combine word alignments. Cherry and Lin (2006) show that introducing soft syntactic constraints through discriminative training can improve alignment quality. LacosteJulien et al. (2006) extend the bipartite matching model of Taskar, Lacoste-Julien, and Klein (2005) by including fertility and first-order interactions. Recently, max-product belief propagation has been successfully applied to discriminative word alignment (Niehues and Vogel 2008; Cromier`es and Kurohashi 2009). Haghighi et al. (2009) investigate supervised word alignment methods that exploit inv</context>
</contexts>
<marker>Ayan, Dorr, 2006</marker>
<rawString>Ayan, Necip Fazil and Bonnie J. Dorr. 2006b. A maximum entropy approach to combining word alignments. In Proceedings of HLT-NAACL 2006, pages 96–103, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
<author>Christof Monz</author>
</authors>
<title>Alignment link projection using transformation-based learning.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP</booktitle>
<pages>185--192</pages>
<location>Vancouver.</location>
<marker>Ayan, Dorr, Monz, 2005</marker>
<rawString>Ayan, Necip Fazil, Bonnie J. Dorr, and Christof Monz. 2005a. Alignment link projection using transformation-based learning. In Proceedings of HLT-EMNLP 2005, pages 185–192, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
<author>Christof Monz</author>
</authors>
<title>Neuralign: Combining word alignments using neural networks.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP</booktitle>
<pages>65--72</pages>
<location>Vancouver.</location>
<marker>Ayan, Dorr, Monz, 2005</marker>
<rawString>Ayan, Necip Fazil, Bonnie J. Dorr, and Christof Monz. 2005b. Neuralign: Combining word alignments using neural networks. In Proceedings of HLT-EMNLP 2005, pages 65–72, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Discriminative word alignment with conditional random fields.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>65--72</pages>
<location>Sydney.</location>
<contexts>
<context position="64341" citStr="Blunsom and Cohn (2006)" startWordPosition="10833" endWordPosition="10836">ed three translation systems: 1. Moses (Koehn and Hoang 2007), a state-of-the-art phrase-based SMT system; 2. Hiero (Chiang 2007), a state-of-the-art hierarchical phrase-based system; 3. Lynx (Liu, Liu, and Lin 2006), a linguistically syntax-based system that makes use of tree-to-string rules. Table 12 Comparison of some word alignment systems on the Canadian Hansard data. System Training Test AER Och and Ney (2003) 1.5M 500 5.2 Moore (2005) 500K 223 7.5 Taskar, Lacoste-Julien, and Klein (2005) 1.1M 347 5.4 Liang, Taskar, and Klein (2006) 1.1M 347 4.9 Lacoste-Julien et al. (2006) 1.1M 247 3.8 Blunsom and Cohn (2006) 1.1M 347 5.2 Moore, Yih, and Bode (2006) 1.1M 223 3.7 This work 1.1M 247 3.6 329 Computational Linguistics Volume 36, Number 3 For all three systems we trained the translation models on the FBIS corpus (7.2M+9.2M words). For the language model, we used the SRI Language Modeling Toolkit (Stolcke 2002) to train a trigram model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We used the 2002 NIST MT evaluation test set as the development set for training feature weights of translation systems, the 2005 test set as the devtest set for choosing optimal values of α </context>
<context position="72738" citStr="Blunsom and Cohn (2006)" startWordPosition="12222" endWordPosition="12225">res. Liu, Liu, and Lin (2005) apply the log-linear model used in SMT (Och and Ney 2002) to word alignment and report significant improvements over the IBM models. Moore (2005) presents a discriminative framework for word alignment and uses averaged perceptron for parameter optimization. Taskar, LacosteJulien, and Klein (2005) treat the alignment prediction task as a maximum weight bipartite matching problem and use the large-margin method to train feature weights. Neural networks and transformation-based learning have also been introduced to word alignment (Ayan, Dorr, and Monz 2005a, 2005b). Blunsom and Cohn (2006) propose a new discriminative model based on conditional random fields (CRF). Fraser and Marcu (2006) use sub-models of IBM Model 4 as features and train feature weights using a semi-supervised algorithm. Ayan and Dorr (2006b) use a maximum entropy model to combine word alignments. Cherry and Lin (2006) show that introducing soft syntactic constraints through discriminative training can improve alignment quality. LacosteJulien et al. (2006) extend the bipartite matching model of Taskar, Lacoste-Julien, and Klein (2005) by including fertility and first-order interactions. Recently, max-product </context>
</contexts>
<marker>Blunsom, Cohn, 2006</marker>
<rawString>Blunsom, Phil and Trevor Cohn. 2006. Discriminative word alignment with conditional random fields. In Proceedings of COLING-ACL 2006, pages 65–72, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1496" citStr="Brown et al. 1993" startWordPosition="214" endWordPosition="217">alignments. Our model is easy to extend and can be optimized with respect to evaluation metrics directly. The model achieves state-of-the-art alignment quality on three word alignment shared tasks for five language pairs with varying divergence and richness of resources. We further show that our approach improves translation performance for various statistical machine translation systems. 1. Introduction Word alignment, which can be defined as an object for indicating the corresponding words in a parallel text, was first introduced as an intermediate result of statistical machine translation (Brown et al. 1993). Consider the following Chinese sentence and its English translation: +Q VXJNk ng$ff)A _*fIFI VT K Zhongguo jianzhuye duiwaikaifang chengxian xin geju The opening of China’s construction industry to the outside presents a new structure * Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, No. 6 Kexueyuan South Road, Haidian District, P.O. Box 2704, Beijing 100190, China. E-mail: {yliu, liuqun, sxlin}@ict.ac.cn. Submission received: 13 September 2007; revised submission received: 7 January 2010; accepted for publication: 21 Febr</context>
<context position="4386" citStr="Brown et al. 1993" startWordPosition="620" endWordPosition="623">Example of a word alignment between a Chinese–English sentence pair. The Chinese and English words are listed horizontally and vertically, respectively. They are numbered to facilitate identification. The dark points indicate the correspondences between the words in the two languages. 304 Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling translation assessment and critiquing tools, text generation, bilingual lexigraphy, and word sense disambiguation. Various methods have been proposed for finding word alignments between parallel texts. Among them, generative alignment models (Brown et al. 1993; Vogel and Ney 1996) have been widely used to produce word alignments for large bilingual corpora. Describing the relationship of a bilingual sentence pair, a generative model treats word alignment as a hidden process and maximizes the likelihood of a training corpus using the expectation maximization (EM) algorithm. After the maximization process is complete, the unknown model parameters are determined and the word alignments are set to the maximum posterior predictions of the model. However, one drawback of generative models is that they are hard to extend. Generative models usually impose </context>
<context position="20376" citStr="Brown et al. 1993" startWordPosition="3441" endWordPosition="3444">g the procedure GAIN(f, e, a, l). If a&apos; has a higher score (line 12), it is added to closed (line 13). We also update N to keep the top n alignments explored during the search (line 15). The n-best list will be used in training feature weights by MERT. This process iterates until there are no promising alignments. The theoretical running time of this algorithm is 0(bJ2I2). 3. Feature Functions The primary art in discriminative modeling is to define useful features that capture various characteristics of word alignments. Intuitively, we can include generative models such as the IBM Models 1–5 (Brown et al. 1993) as features in a discriminative model. A straightforward way is to use a generative model itself as a feature directly (Liu, Liu, 311 Computational Linguistics Volume 36, Number 3 and Lin 2005). Another way is to treat each sub-model of a generative model as a feature (Fraser and Marcu 2006). In either case, a generative model can be regarded as a special case of a discriminative model where all feature weights are one. A detailed discussion of the treatment of the IBM models as features can be found in Appendix B. One major drawback of the IBM models is asymmetry. They are restricted such th</context>
<context position="23082" citStr="Brown et al. (1993)" startWordPosition="3877" endWordPosition="3880">ence e1e2; the translation probability product is t(e2|f1) x t( f1|e2) where t(e|f ) is the probability that f is translated to e and t( f |e) is the probability that e is translated to f, respectively. Unfortunately, the underlying model is biased: The more links added, the smaller the product will be. For example, if we add a link (2,2) to the current alignment and obtain a new alignment {(1,2), (2,2)}, the resulting product will decrease after being multiplied with t(e2|f2) x t( f2|e2): t(e2|f1) x t(f1|e2) x t(e2|f2) x t(f2|e2) The problem results from the absence of empty cepts. Following Brown et al. (1993), a cept in an alignment is either a single source word or it is empty. They assign cepts to positions in the source sentence and reserve position zero for the empty cept. All unaligned target words are assumed to be “aligned” to the empty cept. For example, in the current example alignment {(1,2)}, the unaligned target word e1 is said to be “aligned” to the empty cept f0. As our model is symmetric, we use f0 to denote the empty cept on the source side and use e0 to denote the empty cept on the target side, respectively. 312 Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling If</context>
<context position="30705" citStr="Brown et al. (1993)" startWordPosition="5213" endWordPosition="5216">call these links neighbors. Similarly, (5,13) and (6,14) are also neighbors. Formally, the neighbor count feature function is given by �hn,(f, e, a) = � V − j&apos; = 1 ∧ i − i&apos; = 1� (28) (j,i)∈a (jl,il)∈a 315 Computational Linguistics Volume 36, Number 3 Lgnc(f,e,a,j,i) = 1j − j� = 1 ∧ i − i&apos; = 1� (29) (jl,il)∈a 3.5 Fertility Probability Product Casual inspection of some word alignments quickly establishes that some Chinese words such as Zhongguo and chengxian are often aligned to one English word whereas other Chinese words such as duiwaikaifang tend to be translated into multiple English words. Brown et al. (1993) call the number of target words to which a source word f is connected the fertility off. Recall that we have given the formal definition of fertility in the symmetric scenario in Equation (20) and Equation (21). Besides word association (Sections 3.1 and 3.2) and word distortion (Sections 3.3 and 3.4), fertility also proves to be very important in modeling alignment because sophisticated generative models such as the IBM Models 3–5 parameterize fertilities directly. As our goal is to produce symmetric alignments, we calculate the product of fertility probabilities in two directions. Given an </context>
<context position="32242" citStr="Brown et al. (1993)" startWordPosition="5472" endWordPosition="5475">mple, n(1|f0) denotes the probability that one target word is “aligned” to the source empty cept f0 and n(1|e2) denotes the probability that one source word is aligned to e2. If we add a link (2,2) to the current alignment and obtain a new alignment {(1, 2), (2,2)}, the resulting product will be n(1|f0) x n(1|f1) x n(1|f2) x n(0|e0) x n(0|e1) x n(2|e2) The new product divided by the old product is n(1|f2) x n(0|e0) x n(2|e2) n(0|f2) x n(1|e0) x n(1|e2) Formally, the feature function for fertility probability product is given by hfpp(f,e,a) = J log(n(*j|fj)) + LI log(n(φi|ei)) (30) L i=0 j=0 5 Brown et al. (1993) treat the empty cept in a different way. They assume that at most half of the source words in an alignment are not aligned (i.e., φ0 :5 J/2) and define a binomial distribution relying on an auxiliary parameter p0. Here, we use n(φ0|e0) instead of the original form n0(φ0 |EIi=1 φi) just for simplicity. See Appendix B for more details. 316 Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling The corresponding feature gain is gfpp(f, e, a, j, i) = log(n(ψ0 − δ(φi, 0)|f0)) − log(n(ψ0|f0)) + log(n(ψj + 1|fj) − log(n(ψj|fj)) + log(n(φ0 − δ(ψj,0)|e0)) − log(n(φ0|e0)) + log(n(φi + 1|ei)</context>
<context position="69686" citStr="Brown et al. (1993)" startWordPosition="11727" endWordPosition="11730">ses, the best value of α is 0.5. For Hiero and Lynx, the best α is 0.3, suggesting that recalloriented alignments yield better translation performance. Table 15 gives the BLEU scores of the three systems on the final test set. We used the parameters optimized on the dev and devtest sets. More specifically, Moses used growdiag-final and α = 0.5, Hiero used grow-diag-final and α = 0.3, and Lynx used union and α = 0.3. We find that our discriminative alignment model improves the three systems significantly. 5. Related Work The first generative alignment models were the IBM Models 1–5 proposed by Brown et al. (1993). Vogel and Ney (1996) propose a first-order Hidden Markov model (HMM) for word alignment. They show that it is beneficial to make the alignment probabilities dependent on differences in position rather than on the absolute positions. Och and Table 14 BLEU scores on the devtest set. We use “+” to denote the result that outperforms the best baseline result (highlighted in bold) statistically significantly at p &lt; 0.05. Similarly, we use “++” to denote significantly better than baseline at p &lt; 0.01. Moses Hiero Lynx 24.7 25.7 24.8 20.6 23.5 21.6 20.1 23.2 21.2 24.3 24.1 25.1 24.2 24.0 24.2 25.0 2</context>
<context position="73841" citStr="Brown et al. 1993" startWordPosition="12380" endWordPosition="12383">askar, Lacoste-Julien, and Klein (2005) by including fertility and first-order interactions. Recently, max-product belief propagation has been successfully applied to discriminative word alignment (Niehues and Vogel 2008; Cromier`es and Kurohashi 2009). Haghighi et al. (2009) investigate supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. Our work can be seen as an application of the linear model (Och 2003) in word alignment. While aiming at producing symmetric word alignments in a discriminative way, our approach uses asymmetric generative models (Brown et al. 1993) as the major information sources. Our linear model is similar to that of Moore, Yih, and Bode (2006). They train two linear models called stage 1 and stage 2. The feature values are extracted from word-aligned sentence pairs. After the stage 1 model aligns the entire training corpus automatically, the stage 2 model uses features based not only on the parallel 332 Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling sentences themselves but also on statistics of the alignments produced by the stage 1 model. They use average perceptron and support vector machine (SVM) to train fea</context>
<context position="81148" citStr="Brown et al. (1993)" startWordPosition="13639" endWordPosition="13642">, a) = log (c (J|I) rl t(fj |eaj)a(aj |j, I,J)) (B.3) j=1 The corresponding feature gain is gm2(f, e, a, l) = log(t( fj|ei)) − log(t(fj|e0)) + log(a(i|j,I,J)) − log(a(0|j, I,J)) (B.4) where fj and ei are linked by l and 0 is the index of the empty cept e0. Model 3 is a fertility-based model that parameterizes fertility of words. Unlike Model 2, Model 3 uses a fertility sub-model n(φi|ei) and a distortion sub-model d(j|i, I,J). Formally, the feature function of Model 3 is given by � I I J hm3 (a, f, e) = log n0 (φ0 |E φi) rl n(φi |ei)φi! rl t( fj|eaj) i=1 i=1 j=1 rlX �d(j|i, I, J) (B.5) j:ajo0 Brown et al. (1993) treat n0(φ0 |EIi=1 φi), the fertility probability of e0, in a different way. They assume that at most half of the source words in an alignment are not 335 Computational Linguistics Volume 36, Number 3 aligned (i.e., 4)0 &lt; J2) and define a binomial distribution relying on an auxiliary parameter p0: n0 1 �0 4)i J = ( �J φo 0)pJ0 2φ0 (1 − p0)φ0if 4)0 &lt; 2 (B.6) \ 111 0 otherwise Note that we follow Brown et al. (1993) in replacing EIi=1 4)i with J − 4)0 for simplicity. The original form should be (YIi=1 φi �p�I i=1 φi−φ0(1 − p0)φ0. φ0 0 However, this assumption results in a problem for our search</context>
<context position="83026" citStr="Brown et al. (1993)" startWordPosition="13999" endWordPosition="14002">0+1) if 4)0 &lt; J (J−2φ0+1)×(J−2φ0+2) 2 1−pn (J−φ0+1 φ0−1 )×pJ−2φ0+2 ×(1−p0)φ0−1×pn×� 2 J � 0 1 otherwise (B.9) J2 &lt; 4)0 &lt; J2 + 1 ⎧ ⎨⎪⎪ ⎪⎪⎩ gn0(J,4)0) = Model 4 defines a new distortion sub-model D(a) that relies on word classes A and 13 to capture movement of phrases. The feature function for Model 4 is I I J hm4(a,f,e) =log(n0(4)0 |�4)i)�n(4)i|ei)�t(fj |eaj)14)0!D(a) I (B.10) where i i j =1 =1 =1 D(a) = I �φi pik(7tik) (B.11) i=1 k=1 336 Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling J d1(j − cρi |A(eρi), B(τi1)) if k = 1 pik (j) = l (B.12) d&gt;1(j − πi,k−1|B(τik)) otherwise Brown et al. (1993) propose two distortion models for Model 4: d1(·) for the first word of a tablet τ and d&gt;1(·) for the other words of the tablet. In Equation B.12, ρi is the first position to the left of i for which φi &gt; 0, cρi is the ceiling of the average position of the words in τρ, τik denotes the k-th French word aligned to ek, πi,k−1 denotes the position of the k − 1-th French word aligned to ei, and A(·) and B(·) are word classes for the source and target languages, respectively. Please refer to Brown et al. (1993) for more details. The corresponding feature gain is gm4(f,e,a,l) = log(gn0(J, φ0)) + log(</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>A probability model to improve word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL 2003,</booktitle>
<pages>88--95</pages>
<location>Sapporo.</location>
<marker>Cherry, Lin, 2003</marker>
<rawString>Cherry, Colin and Dekang Lin. 2003. A probability model to improve word alignment. In Proceedings of ACL 2003, pages 88–95, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Soft syntactic constraints for word alignment through discriminative training.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>105--112</pages>
<location>Sydney.</location>
<contexts>
<context position="73042" citStr="Cherry and Lin (2006)" startWordPosition="12270" endWordPosition="12273">ien, and Klein (2005) treat the alignment prediction task as a maximum weight bipartite matching problem and use the large-margin method to train feature weights. Neural networks and transformation-based learning have also been introduced to word alignment (Ayan, Dorr, and Monz 2005a, 2005b). Blunsom and Cohn (2006) propose a new discriminative model based on conditional random fields (CRF). Fraser and Marcu (2006) use sub-models of IBM Model 4 as features and train feature weights using a semi-supervised algorithm. Ayan and Dorr (2006b) use a maximum entropy model to combine word alignments. Cherry and Lin (2006) show that introducing soft syntactic constraints through discriminative training can improve alignment quality. LacosteJulien et al. (2006) extend the bipartite matching model of Taskar, Lacoste-Julien, and Klein (2005) by including fertility and first-order interactions. Recently, max-product belief propagation has been successfully applied to discriminative word alignment (Niehues and Vogel 2008; Cromier`es and Kurohashi 2009). Haghighi et al. (2009) investigate supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. Our work can be seen as an applic</context>
</contexts>
<marker>Cherry, Lin, 2006</marker>
<rawString>Cherry, Colin and Dekang Lin. 2006. Soft syntactic constraints for word alignment through discriminative training. In Proceedings of COLING-ACL 2006 (poster), pages 105–112, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine Computational Linguistics Volume 36,</title>
<date>2005</date>
<journal>Number</journal>
<volume>3</volume>
<contexts>
<context position="3487" citStr="Chiang 2005" startWordPosition="497" endWordPosition="498">ely. They are numbered to facilitate identification. The dark points indicate the correspondence between the words in two languages. The goal of word alignment is to identify such correspondences in a parallel text. Word alignment plays an important role in many NLP tasks. In statistical machine translation, word-aligned corpora serve as an excellent source for translation-related knowledge. The estimation of translation model parameters usually relies heavily on word-aligned corpora, not only for phrase-based and hierarchical phrase-based models (Koehn, Och, and Marcu 2003; Och and Ney 2004; Chiang 2005, 2007), but also for syntax-based models (Quirk, Menezes, and Cherry 2005; Galley et al. 2006; Liu, Liu, and Lin 2006; Marcu et al. 2006). Besides machine translation, many applications for word-aligned corpora have been suggested, including machine-assisted translation, Figure 1 Example of a word alignment between a Chinese–English sentence pair. The Chinese and English words are listed horizontally and vertically, respectively. They are numbered to facilitate identification. The dark points indicate the correspondences between the words in the two languages. 304 Liu, Liu, and Lin Discrimina</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>Chiang, David. 2005. A hierarchical phrase-based model for statistical machine Computational Linguistics Volume 36, Number 3</rawString>
</citation>
<citation valid="true">
<authors>
<author>translation</author>
</authors>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI.</location>
<marker>translation, 2005</marker>
<rawString>translation. In Proceedings of ACL 2005, pages 263–270, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="63847" citStr="Chiang 2007" startWordPosition="10757" endWordPosition="10758">k and our system. Note that direct comparisons are problematic due to the different configurations of training data, development data, and test data. Our result matches the state-of-the-art performance on the Hansard data (Lacoste-Julien et al. 2006; Moore, Yih, and Bode 2006). 4.2 Evaluation of Translation Quality In this section, we report on experiments with Chinese-to-English translation. To investigate the effect of our discriminative model on translation performance, we used three translation systems: 1. Moses (Koehn and Hoang 2007), a state-of-the-art phrase-based SMT system; 2. Hiero (Chiang 2007), a state-of-the-art hierarchical phrase-based system; 3. Lynx (Liu, Liu, and Lin 2006), a linguistically syntax-based system that makes use of tree-to-string rules. Table 12 Comparison of some word alignment systems on the Canadian Hansard data. System Training Test AER Och and Ney (2003) 1.5M 500 5.2 Moore (2005) 500K 223 7.5 Taskar, Lacoste-Julien, and Klein (2005) 1.1M 347 5.4 Liang, Taskar, and Klein (2006) 1.1M 347 4.9 Lacoste-Julien et al. (2006) 1.1M 247 3.8 Blunsom and Cohn (2006) 1.1M 347 5.2 Moore, Yih, and Bode (2006) 1.1M 223 3.7 This work 1.1M 247 3.6 329 Computational Linguistic</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>Chiang, David. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabien Cromier`es</author>
<author>Sadao Kurohashi</author>
</authors>
<title>An alignment algorithm using belief propagation and a structure-based distortion model.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL 2009,</booktitle>
<pages>166--174</pages>
<location>Athens.</location>
<marker>Cromier`es, Kurohashi, 2009</marker>
<rawString>Cromier`es, Fabien and Sadao Kurohashi. 2009. An alignment algorithm using belief propagation and a structure-based distortion model. In Proceedings of EACL 2009, pages 166–174, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Semi-supervised training for statistical word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>769--776</pages>
<location>Sydney.</location>
<contexts>
<context position="20669" citStr="Fraser and Marcu 2006" startWordPosition="3491" endWordPosition="3494">e no promising alignments. The theoretical running time of this algorithm is 0(bJ2I2). 3. Feature Functions The primary art in discriminative modeling is to define useful features that capture various characteristics of word alignments. Intuitively, we can include generative models such as the IBM Models 1–5 (Brown et al. 1993) as features in a discriminative model. A straightforward way is to use a generative model itself as a feature directly (Liu, Liu, 311 Computational Linguistics Volume 36, Number 3 and Lin 2005). Another way is to treat each sub-model of a generative model as a feature (Fraser and Marcu 2006). In either case, a generative model can be regarded as a special case of a discriminative model where all feature weights are one. A detailed discussion of the treatment of the IBM models as features can be found in Appendix B. One major drawback of the IBM models is asymmetry. They are restricted such that each source word is assigned to exactly one target word. This is not the case for many language pairs. For example, in our running example, one Chinese word jianzhuye corresponds to two English words construction industry. As a result, our linear model will produce only one-to-one alignmen</context>
<context position="61634" citStr="Fraser and Marcu (2006)" startWordPosition="10435" endWordPosition="10438">with a quite small training corpus. 4.1.7 Summary of Results. Table 11 summarizes the results on all three shared tasks. Vigne used the same configuration for all tasks. We used the symmetric linear model and activated all features. The pruning setting is β = 0.01 and b = 10. Our system outperforms the systems participating in all the three shared tasks significantly. Note that for the English–French task we used the small development set of 37 sentences to optimize feature weights, and ran our system on the original test set of 447 sentences. For the Romanian–English language pair, we follow Fraser and Marcu (2006) in reducing the vocabulary by stemming Romanian and English words down to their first four characters. For the other language pairs, English–Inuktitut and English– Hindi, the symmetric linear model maintains its superiority over the asymmetric linear model and yields better results than the other participants. Figure 4 Effect of training corpus size on the Chinese–English task. We choose four feature groups with varying numbers of features: 3, 6, 10, and 14. There are eight training corpora with varying numbers of sentence pairs: 10, 20, 50, 100, 200, 300, 400, and 502. 328 Liu, Liu, and Lin </context>
<context position="72839" citStr="Fraser and Marcu (2006)" startWordPosition="12237" endWordPosition="12240">ment and report significant improvements over the IBM models. Moore (2005) presents a discriminative framework for word alignment and uses averaged perceptron for parameter optimization. Taskar, LacosteJulien, and Klein (2005) treat the alignment prediction task as a maximum weight bipartite matching problem and use the large-margin method to train feature weights. Neural networks and transformation-based learning have also been introduced to word alignment (Ayan, Dorr, and Monz 2005a, 2005b). Blunsom and Cohn (2006) propose a new discriminative model based on conditional random fields (CRF). Fraser and Marcu (2006) use sub-models of IBM Model 4 as features and train feature weights using a semi-supervised algorithm. Ayan and Dorr (2006b) use a maximum entropy model to combine word alignments. Cherry and Lin (2006) show that introducing soft syntactic constraints through discriminative training can improve alignment quality. LacosteJulien et al. (2006) extend the bipartite matching model of Taskar, Lacoste-Julien, and Klein (2005) by including fertility and first-order interactions. Recently, max-product belief propagation has been successfully applied to discriminative word alignment (Niehues and Vogel </context>
<context position="79339" citStr="Fraser and Marcu (2006)" startWordPosition="13312" endWordPosition="13315">sures alignment quality α the precision/recall weighting factor in balanced F-measure β pruning threshold in the beam search algorithm b beam size in the beam search algorithm δ(x, y) the Kronecker function, which is 1 if x = y and 0 otherwise Qexpr� an indicator function taking a boolean expression expr as the argument Appendix B: Using the IBM Models as Feature Functions In this article, we use IBM Models 1–4 as feature functions by taking the logarithm of the models themselves rather than the sub-models just for simplicity. It is easy to separate each sub-model as a feature as suggested by Fraser and Marcu (2006). We distinguish 334 Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling between two translation directions (i.e., source-to-target and target-to-source) to use the IBM models as feature functions. All model parameters are estimated by GIZA++ (Och and Ney 2003). The feature function for the IBM Model 1 is C J hm1 (f, e, a) = log E(J|I)rlt(fj|eaj) (B.1) + 1)J j=1 where c(J|I) predicts the length of the source sentence conditioned on that of the target sentence, (I + 1)−J defines a uniform distribution of the alignment between source and target words, and t( fj|ei) is a translatio</context>
</contexts>
<marker>Fraser, Marcu, 2006</marker>
<rawString>Fraser, Alexander and Daniel Marcu. 2006. Semi-supervised training for statistical word alignment. In Proceedings of COLING-ACL 2006, pages 769–776, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Getting the structure right for word alignment: LEAF.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL 2007,</booktitle>
<pages>51--60</pages>
<location>Prague.</location>
<contexts>
<context position="8971" citStr="Fraser and Marcu 2007" startWordPosition="1381" endWordPosition="1384">such as Zhongguo and China. In contrast, possible links often align words within idiomatic expressions and free translations. An AER score is given by AER(S,P,A)=1_ |A n S |+ |A n P |(3) |A |+ |S| where S is a set of sure links in a reference alignment that is hand-aligned by human experts, P is a set of possible links in the reference alignment, and A is a candidate alignment. Note that S is a subset of P: S C_ P. The lower the AER score is, the better the alignment quality is. Although widely used, AER has been criticized for correlating poorly with translation quality (Ayan and Dorr 2006a; Fraser and Marcu 2007b). In other words, lower AER scores do not necessarily lead to better translation quality.1 Fraser and Marcu (2007b) argue that reference alignments should consist of only sure links. They propose a new measure called the balanced F-measure: precision(S,A) = |A nS |(4) |S| recall(S,A) = |A n S |(5) |A| 1 F-measure(S,α,A) = − (6) precis of n(S,A) + recall(S,A) 1 It has not yet been uniformly accepted that better word alignments yield better translations. Ayan and Dorr (2006a) present a detailed discussion of the impact of word alignment on statistical machine translation. 306 Liu, Liu, and Lin</context>
<context position="65528" citStr="Fraser and Marcu 2007" startWordPosition="11026" endWordPosition="11029"> for choosing optimal values of α for different translation systems, and the 2008 test set as the final test set. Our evaluation metric is case-sensitive BLEU-4, as defined by NIST, that is, using the shortest (as opposed to closest) reference length for brevity penalty. We annotated the first 200 sentences of the FBIS corpus using the Blinker guidelines (Melamed 1998). All links are sure ones. These hand-aligned sentences served as the training corpus for Vigne. To train the feature weights in our discriminative model using minimum-error-rate training (Och 2003), we adopt balanced F-measure (Fraser and Marcu 2007b) as the optimization criterion. The pipeline begins by running GIZA++ and Cross-EM on the FBIS corpus. We used seven generative alignment methods based on IBM Model 4 and HMM as baseline systems: (1) CSE, (2) ESC, (3) intersection, (4) union, (5) refined method (Och and Ney 2003), (6) grow-diag-final (Koehn, Och, and Marcu 2003), and (7) Cross-EM (Liang, Taskar, and Klein 2006). Instead of exploring the entire search space, our linear model only searches within the union of baseline predictions, which enables our system to align large bilingual corpus at a very fast speed of 3, 000 words per</context>
<context position="71559" citStr="Fraser and Marcu (2007" startWordPosition="12047" endWordPosition="12050">ignificantly better than baseline at p &lt; 0.01. Moses Hiero Lynx generative 20.1 20.7 19.9 discriminative 20.8+ 21.6+ 21.0++ Ney (2003) re-implement the IBM models and the HMM model and compare them with heuristic approaches systematically. The resulting toolkit GIZA++ developed by Franz J. Och is the most popular alignment system nowadays. Liang, Taskar, and Klein (2006) present an unsupervised way to produce symmetric alignments by training two simple asymmetric models (e.g., IBM Model 1 and the HMM model) jointly to maximize a combination of data likelihood and agreement between the models. Fraser and Marcu (2007a) introduce a new generative model called LEAF that directly models manyto-many non-consecutive word alignments. Their model can be trained using both unsupervised and semi-supervised training methods. Recent years have witnessed the rapid development of discriminative alignment methods. As a first attempt, Och and Ney (2003) proposed the Model 6, which is a log-linear combination of the IBM models and the HMM model. Cherry and Lin (2003) develop a statistical model to find word alignments, which allows for easy integration of context-specific features. Liu, Liu, and Lin (2005) apply the log-</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Fraser, Alexander and Daniel Marcu. 2007a. Getting the structure right for word alignment: LEAF. In Proceedings of EMNLP-CoNLL 2007, pages 51–60, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Measuring word alignment quality for statistical machine translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="8971" citStr="Fraser and Marcu 2007" startWordPosition="1381" endWordPosition="1384">such as Zhongguo and China. In contrast, possible links often align words within idiomatic expressions and free translations. An AER score is given by AER(S,P,A)=1_ |A n S |+ |A n P |(3) |A |+ |S| where S is a set of sure links in a reference alignment that is hand-aligned by human experts, P is a set of possible links in the reference alignment, and A is a candidate alignment. Note that S is a subset of P: S C_ P. The lower the AER score is, the better the alignment quality is. Although widely used, AER has been criticized for correlating poorly with translation quality (Ayan and Dorr 2006a; Fraser and Marcu 2007b). In other words, lower AER scores do not necessarily lead to better translation quality.1 Fraser and Marcu (2007b) argue that reference alignments should consist of only sure links. They propose a new measure called the balanced F-measure: precision(S,A) = |A nS |(4) |S| recall(S,A) = |A n S |(5) |A| 1 F-measure(S,α,A) = − (6) precis of n(S,A) + recall(S,A) 1 It has not yet been uniformly accepted that better word alignments yield better translations. Ayan and Dorr (2006a) present a detailed discussion of the impact of word alignment on statistical machine translation. 306 Liu, Liu, and Lin</context>
<context position="65528" citStr="Fraser and Marcu 2007" startWordPosition="11026" endWordPosition="11029"> for choosing optimal values of α for different translation systems, and the 2008 test set as the final test set. Our evaluation metric is case-sensitive BLEU-4, as defined by NIST, that is, using the shortest (as opposed to closest) reference length for brevity penalty. We annotated the first 200 sentences of the FBIS corpus using the Blinker guidelines (Melamed 1998). All links are sure ones. These hand-aligned sentences served as the training corpus for Vigne. To train the feature weights in our discriminative model using minimum-error-rate training (Och 2003), we adopt balanced F-measure (Fraser and Marcu 2007b) as the optimization criterion. The pipeline begins by running GIZA++ and Cross-EM on the FBIS corpus. We used seven generative alignment methods based on IBM Model 4 and HMM as baseline systems: (1) CSE, (2) ESC, (3) intersection, (4) union, (5) refined method (Och and Ney 2003), (6) grow-diag-final (Koehn, Och, and Marcu 2003), and (7) Cross-EM (Liang, Taskar, and Klein 2006). Instead of exploring the entire search space, our linear model only searches within the union of baseline predictions, which enables our system to align large bilingual corpus at a very fast speed of 3, 000 words per</context>
<context position="71559" citStr="Fraser and Marcu (2007" startWordPosition="12047" endWordPosition="12050">ignificantly better than baseline at p &lt; 0.01. Moses Hiero Lynx generative 20.1 20.7 19.9 discriminative 20.8+ 21.6+ 21.0++ Ney (2003) re-implement the IBM models and the HMM model and compare them with heuristic approaches systematically. The resulting toolkit GIZA++ developed by Franz J. Och is the most popular alignment system nowadays. Liang, Taskar, and Klein (2006) present an unsupervised way to produce symmetric alignments by training two simple asymmetric models (e.g., IBM Model 1 and the HMM model) jointly to maximize a combination of data likelihood and agreement between the models. Fraser and Marcu (2007a) introduce a new generative model called LEAF that directly models manyto-many non-consecutive word alignments. Their model can be trained using both unsupervised and semi-supervised training methods. Recent years have witnessed the rapid development of discriminative alignment methods. As a first attempt, Och and Ney (2003) proposed the Model 6, which is a log-linear combination of the IBM models and the HMM model. Cherry and Lin (2003) develop a statistical model to find word alignments, which allows for easy integration of context-specific features. Liu, Liu, and Lin (2005) apply the log-</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Fraser, Alexander and Daniel Marcu. 2007b. Measuring word alignment quality for statistical machine translation. Computational Linguistics, 33(3):293–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>961--968</pages>
<location>Sydney.</location>
<contexts>
<context position="3581" citStr="Galley et al. 2006" startWordPosition="510" endWordPosition="513">spondence between the words in two languages. The goal of word alignment is to identify such correspondences in a parallel text. Word alignment plays an important role in many NLP tasks. In statistical machine translation, word-aligned corpora serve as an excellent source for translation-related knowledge. The estimation of translation model parameters usually relies heavily on word-aligned corpora, not only for phrase-based and hierarchical phrase-based models (Koehn, Och, and Marcu 2003; Och and Ney 2004; Chiang 2005, 2007), but also for syntax-based models (Quirk, Menezes, and Cherry 2005; Galley et al. 2006; Liu, Liu, and Lin 2006; Marcu et al. 2006). Besides machine translation, many applications for word-aligned corpora have been suggested, including machine-assisted translation, Figure 1 Example of a word alignment between a Chinese–English sentence pair. The Chinese and English words are listed horizontally and vertically, respectively. They are numbered to facilitate identification. The dark points indicate the correspondences between the words in the two languages. 304 Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling translation assessment and critiquing tools, text gener</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING-ACL 2006, pages 961–968, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Better word alignments with supervised ITG models.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP 2009,</booktitle>
<pages>923--931</pages>
<contexts>
<context position="73499" citStr="Haghighi et al. (2009)" startWordPosition="12329" endWordPosition="12332">ures and train feature weights using a semi-supervised algorithm. Ayan and Dorr (2006b) use a maximum entropy model to combine word alignments. Cherry and Lin (2006) show that introducing soft syntactic constraints through discriminative training can improve alignment quality. LacosteJulien et al. (2006) extend the bipartite matching model of Taskar, Lacoste-Julien, and Klein (2005) by including fertility and first-order interactions. Recently, max-product belief propagation has been successfully applied to discriminative word alignment (Niehues and Vogel 2008; Cromier`es and Kurohashi 2009). Haghighi et al. (2009) investigate supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. Our work can be seen as an application of the linear model (Och 2003) in word alignment. While aiming at producing symmetric word alignments in a discriminative way, our approach uses asymmetric generative models (Brown et al. 1993) as the major information sources. Our linear model is similar to that of Moore, Yih, and Bode (2006). They train two linear models called stage 1 and stage 2. The feature values are extracted from word-aligned sentence pairs. After the stage 1 model aligns </context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>Haghighi, Aria, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised ITG models. In Proceedings of ACL-IJCNLP 2009, pages 923–931, Suntec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect-HMM-based hypothesis alignment for combining outputs from machine translation systems.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>98--107</pages>
<location>Honolulu, HI.</location>
<contexts>
<context position="40513" citStr="He et al. 2008" startWordPosition="6930" endWordPosition="6933">tion is that a dictionary is expected to be more reliable than an automatically trained lexicon. For example, if Zhongguo and China appear in an entry of a dictionary, they should be more likely to be aligned. Thus, we use a single indicator feature to encourage linking word pairs that occur in a dictionary D: hbd(f,e,a,D) = � Q( fj, ei) E D� (44) (j,i)Ea gbd(f, e, a, D, j, i) = Q( fj, ei) E D� (45) 3.11 Link Co-Occurrence Count The system combination technique that integrates predictions from multiple systems proves to be effective in machine translation (Rosti, Matsoukas, and Schwartz 2007; He et al. 2008). In word alignment, a link should be aligned if it appears in most system predictions. Taskar, Lacoste-Julien, and Klein (2005) include the IBM Model 4 predictions as features and obtain substantial improvements. To enable system combination, we design a feature to favor links voted by most systems. Given an alignment a&apos; produced by another system, we use the number of links of the intersection of a and a&apos; as a feature: hlcc(f, e, a, a&apos;) = ja n a&apos;j (46) glcc(f, e, a, a&apos;, j, i) = Ql E a n a&apos;� (47) 4. Experiments In this section, we try to answer two questions: 1. Does the proposed approach ach</context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>He, Xiaodong, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore. 2008. Indirect-HMM-based hypothesis alignment for combining outputs from machine translation systems. In Proceedings of EMNLP 2008, pages 98–107, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL 2007,</booktitle>
<pages>868--876</pages>
<location>Prague.</location>
<contexts>
<context position="63779" citStr="Koehn and Hoang 2007" startWordPosition="10746" endWordPosition="10749"> used data set. Table 12 lists alignment error rates achieved by previous work and our system. Note that direct comparisons are problematic due to the different configurations of training data, development data, and test data. Our result matches the state-of-the-art performance on the Hansard data (Lacoste-Julien et al. 2006; Moore, Yih, and Bode 2006). 4.2 Evaluation of Translation Quality In this section, we report on experiments with Chinese-to-English translation. To investigate the effect of our discriminative model on translation performance, we used three translation systems: 1. Moses (Koehn and Hoang 2007), a state-of-the-art phrase-based SMT system; 2. Hiero (Chiang 2007), a state-of-the-art hierarchical phrase-based system; 3. Lynx (Liu, Liu, and Lin 2006), a linguistically syntax-based system that makes use of tree-to-string rules. Table 12 Comparison of some word alignment systems on the Canadian Hansard data. System Training Test AER Och and Ney (2003) 1.5M 500 5.2 Moore (2005) 500K 223 7.5 Taskar, Lacoste-Julien, and Klein (2005) 1.1M 347 5.4 Liang, Taskar, and Klein (2006) 1.1M 347 4.9 Lacoste-Julien et al. (2006) 1.1M 247 3.8 Blunsom and Cohn (2006) 1.1M 347 5.2 Moore, Yih, and Bode (20</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Koehn, Philipp and Hieu Hoang. 2007. Factored translation models. In Proceedings of EMNLP-CoNLL 2007, pages 868–876, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>127--133</pages>
<location>Edmonton.</location>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, Philipp, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, pages 127–133, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Lacoste-Julien</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Michael I Jordan</author>
</authors>
<title>Word alignment via quadratic assignment.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL 2007,</booktitle>
<pages>112--119</pages>
<location>New York, NY.</location>
<contexts>
<context position="29884" citStr="Lacoste-Julien et al. (2006)" startWordPosition="5060" endWordPosition="5063">xpr� is an indicator function that takes a boolean expression expr as the argument: � 1 if expr is true �expr� = (27) 0 otherwise 3.4 Neighbor Count Moore (2005) finds that word alignments between closely related languages tend to be approximately monotonic. Even for distantly related languages, the number of crossing links is far less than chance since phrases tend to be translated as contiguous chunks. In Figure 1, the dark points are positioned approximately in parallel with the diagonal line, indicating that the alignment is approximately monotonic. To capture such monotonicity, we follow Lacoste-Julien et al. (2006) to encourage strictly monotonic alignments by adding a bonus for any pair of links (j, i) and (j&apos;, i&apos;) such that j − j&apos; = 1 ∧ i − i&apos; = 1 In Figure 1, there is one such link pair: (3,10) and (4,11). We call these links neighbors. Similarly, (5,13) and (6,14) are also neighbors. Formally, the neighbor count feature function is given by �hn,(f, e, a) = � V − j&apos; = 1 ∧ i − i&apos; = 1� (28) (j,i)∈a (jl,il)∈a 315 Computational Linguistics Volume 36, Number 3 Lgnc(f,e,a,j,i) = 1j − j� = 1 ∧ i − i&apos; = 1� (29) (jl,il)∈a 3.5 Fertility Probability Product Casual inspection of some word alignments quickly esta</context>
<context position="63484" citStr="Lacoste-Julien et al. 2006" startWordPosition="10703" endWordPosition="10706">6 English–Inuktitut, limited 9.5–71.3 8.9 ACL 2005 Romanian–English, limited 26.6–44.5 24.7 English–Hindi, limited 51.4 44.8 HTRDP 2005 Chinese–English, unlimited 23.5–49.2 14.3 4.1.8 Comparison to Other Work. In the word alignment literature, the Canadian Hansard bilingual corpus is the most widely used data set. Table 12 lists alignment error rates achieved by previous work and our system. Note that direct comparisons are problematic due to the different configurations of training data, development data, and test data. Our result matches the state-of-the-art performance on the Hansard data (Lacoste-Julien et al. 2006; Moore, Yih, and Bode 2006). 4.2 Evaluation of Translation Quality In this section, we report on experiments with Chinese-to-English translation. To investigate the effect of our discriminative model on translation performance, we used three translation systems: 1. Moses (Koehn and Hoang 2007), a state-of-the-art phrase-based SMT system; 2. Hiero (Chiang 2007), a state-of-the-art hierarchical phrase-based system; 3. Lynx (Liu, Liu, and Lin 2006), a linguistically syntax-based system that makes use of tree-to-string rules. Table 12 Comparison of some word alignment systems on the Canadian Hans</context>
</contexts>
<marker>Lacoste-Julien, Taskar, Klein, Jordan, 2006</marker>
<rawString>Lacoste-Julien, Simon, Ben Taskar, Dan Klein, and Michael I. Jordan. 2006. Word alignment via quadratic assignment. In Proceedings of HLT-NAACL 2007, pages 112–119, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL 2006,</booktitle>
<pages>104--111</pages>
<location>New York, NY.</location>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Liang, Percy, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of HLT-NAACL 2006, pages 104–111, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Log-linear models for word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL 2005,</booktitle>
<pages>459--466</pages>
<location>Ann Arbor, MI.</location>
<marker>Liu, Liu, Lin, 2005</marker>
<rawString>Liu, Yang, Qun Liu, and Shouxun Lin. 2005. Log-linear models for word alignment. In Proceedings of ACL 2005, pages 459–466, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-to-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>609--616</pages>
<location>Sydney.</location>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation. In Proceedings of COLING-ACL 2006, pages 609–616, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>44--52</pages>
<location>Sydney.</location>
<contexts>
<context position="3625" citStr="Marcu et al. 2006" startWordPosition="519" endWordPosition="522">. The goal of word alignment is to identify such correspondences in a parallel text. Word alignment plays an important role in many NLP tasks. In statistical machine translation, word-aligned corpora serve as an excellent source for translation-related knowledge. The estimation of translation model parameters usually relies heavily on word-aligned corpora, not only for phrase-based and hierarchical phrase-based models (Koehn, Och, and Marcu 2003; Och and Ney 2004; Chiang 2005, 2007), but also for syntax-based models (Quirk, Menezes, and Cherry 2005; Galley et al. 2006; Liu, Liu, and Lin 2006; Marcu et al. 2006). Besides machine translation, many applications for word-aligned corpora have been suggested, including machine-assisted translation, Figure 1 Example of a word alignment between a Chinese–English sentence pair. The Chinese and English words are listed horizontally and vertically, respectively. They are numbered to facilitate identification. The dark points indicate the correspondences between the words in the two languages. 304 Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling translation assessment and critiquing tools, text generation, bilingual lexigraphy, and word sense </context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Marcu, Daniel, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proceedings of EMNLP 2006, pages 44–52, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Martin</author>
<author>Rada Mihalcea</author>
<author>Ted Pedersen</author>
</authors>
<title>Word alignment for languages with scarce resources.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL 2005 Workshop on Building and Using Parallel Texts,</booktitle>
<pages>65--74</pages>
<location>Ann Arbor, MI.</location>
<marker>Martin, Mihalcea, Pedersen, 2005</marker>
<rawString>Martin, Joel, Rada Mihalcea, and Ted Pedersen. 2005. Word alignment for languages with scarce resources. In Proceedings of the ACL 2005 Workshop on Building and Using Parallel Texts, pages 65–74, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Annotation style guide for the blinker project.</title>
<date>1998</date>
<tech>Technical report No. 98-06,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="65278" citStr="Melamed 1998" startWordPosition="10991" endWordPosition="10992">del with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We used the 2002 NIST MT evaluation test set as the development set for training feature weights of translation systems, the 2005 test set as the devtest set for choosing optimal values of α for different translation systems, and the 2008 test set as the final test set. Our evaluation metric is case-sensitive BLEU-4, as defined by NIST, that is, using the shortest (as opposed to closest) reference length for brevity penalty. We annotated the first 200 sentences of the FBIS corpus using the Blinker guidelines (Melamed 1998). All links are sure ones. These hand-aligned sentences served as the training corpus for Vigne. To train the feature weights in our discriminative model using minimum-error-rate training (Och 2003), we adopt balanced F-measure (Fraser and Marcu 2007b) as the optimization criterion. The pipeline begins by running GIZA++ and Cross-EM on the FBIS corpus. We used seven generative alignment methods based on IBM Model 4 and HMM as baseline systems: (1) CSE, (2) ESC, (3) intersection, (4) union, (5) refined method (Och and Ney 2003), (6) grow-diag-final (Koehn, Och, and Marcu 2003), and (7) Cross-EM</context>
</contexts>
<marker>Melamed, 1998</marker>
<rawString>Melamed, I. Dan. 1998. Annotation style guide for the blinker project. Technical report No. 98-06, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Models for translational equivalence among words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="21485" citStr="Melamed 2000" startWordPosition="3626" endWordPosition="3627"> be found in Appendix B. One major drawback of the IBM models is asymmetry. They are restricted such that each source word is assigned to exactly one target word. This is not the case for many language pairs. For example, in our running example, one Chinese word jianzhuye corresponds to two English words construction industry. As a result, our linear model will produce only one-to-one alignments if the IBM models in two translation directions (i.e., source-to-target and target-to-source) are both used. Although some authors would use the one-to-one assumption to simplify the modeling problem (Melamed 2000; Taskar, Lacoste-Julien, and Klein 2005), many translation phenomena cannot be handled and the recall cannot reach 100% in principle. A more general way is to model alignment as an arbitrary relation between source and target language word positions. As our linear model is capable of including many overlapping features regardless of their interdependencies, it is easy to add features that characterize symmetric alignments. In the following subsections, we will introduce a number of symmetric features used in our experiments. 3.1 Translation Probability Product To determine the correspondence </context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>Melamed, I. Dan. 2000. Models for translational equivalence among words. Computational Linguistics, 26(2):221–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ted Pedersen</author>
</authors>
<title>An evaluation exercise for word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003 Workshop on Building and Using Parallel Texts,</booktitle>
<pages>1--10</pages>
<location>Edmonton.</location>
<contexts>
<context position="43710" citStr="Mihalcea and Pedersen 2003" startWordPosition="7546" endWordPosition="7549"> and achieves comparable results with other state-of-the-art discriminative alignment models. In Section 4.2, we investigate the effect of our model on translation quality. By training feature weights with respect to F-measure instead of AER, our model results in superior translation quality over generative methods for phrase-based, hierarchical phrase-based, and tree-to-string SMT systems. 321 Computational Linguistics Volume 36, Number 3 4.1 Evaluation of Alignment Quality In this section, we present results of experiments on three word alignment shared tasks: 1. HLT/NAACL 2003 shared task (Mihalcea and Pedersen 2003). As part of the HLT/NAACL 2003 workshop on “Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,” this shared task includes two language pairs: English–French and Romanian–English. Participants can use both limited and unlimited resources. 2. ACL 2005 shared task (Martin, Mihalcea, and Pedersen 2005). As part of the ACL 2005 workshop on “Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,” this shared task includes three language pairs to cover different language and data characteristics: English–Inuktitut, Romanian–English, and English–Hin</context>
</contexts>
<marker>Mihalcea, Pedersen, 2003</marker>
<rawString>Mihalcea, Rada and Ted Pedersen. 2003. An evaluation exercise for word alignment. In Proceedings of HLT-NAACL 2003 Workshop on Building and Using Parallel Texts, pages 1–10, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>A discriminative framework for bilingual word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP</booktitle>
<pages>81--88</pages>
<location>Vancouver.</location>
<contexts>
<context position="29417" citStr="Moore (2005)" startWordPosition="4993" endWordPosition="4994">We say that there is a cross between the two links (1, 4) and (3,2) because (1 − 3) x (4 − 2) &lt; 0. In Figure 1, there is only one cross. As a result, we could use the number of crosses in alignments to capture the divergence of word orders between two languages. Formally, the cross count feature function is given by �h,,(f,e, a) = � Q(j − j�) x (i − i&apos;) &lt; 0� (25) (j,i)∈a (jl,il)∈a �g,,(f,e,a,j,i) = Q(j − j&apos;) x (i − i&apos;) &lt; 0� (26) (jl,il)∈a where Qexpr� is an indicator function that takes a boolean expression expr as the argument: � 1 if expr is true �expr� = (27) 0 otherwise 3.4 Neighbor Count Moore (2005) finds that word alignments between closely related languages tend to be approximately monotonic. Even for distantly related languages, the number of crossing links is far less than chance since phrases tend to be translated as contiguous chunks. In Figure 1, the dark points are positioned approximately in parallel with the diagonal line, indicating that the alignment is approximately monotonic. To capture such monotonicity, we follow Lacoste-Julien et al. (2006) to encourage strictly monotonic alignments by adding a bonus for any pair of links (j, i) and (j&apos;, i&apos;) such that j − j&apos; = 1 ∧ i − i&apos;</context>
<context position="38157" citStr="Moore (2005)" startWordPosition="6520" endWordPosition="6521">stance, one-to-one links indicate that one source word (e.g., Zhongguo) is translated into exactly one target word (e.g., China) while many-to-many links exist for phrase-to-phrase translation. The distribution of link types differs for different language pairs. For example, one-to-one links occur more frequently in closely related language pairs (e.g., French–English) and one-to-many links are more common in distantly related language pairs (e.g., Chinese–English). To capture the distribution of link types independent of languages, we use features to count different types of links. Following Moore (2005), we divide links in an alignment into four categories: 1. one-to-one links, in which neither the source nor the target word participates in other links; 2. one-to-many links, in which only the source word participates in other links; 3. many-to-one links, in which only the target word participates in other links; 4. many-to-many links, in which both the source and target words participate in other links. In Figure 1, (1, 4), (4,11), (5,13), and (6,14) are one-to-one links and the others are one-to-many links. As a result, we introduce four features: �ho2o(f, e, a) = J*j = 1 ∧ 4)i = 1� (j,i)Ea</context>
<context position="64163" citStr="Moore (2005)" startWordPosition="10805" endWordPosition="10806"> In this section, we report on experiments with Chinese-to-English translation. To investigate the effect of our discriminative model on translation performance, we used three translation systems: 1. Moses (Koehn and Hoang 2007), a state-of-the-art phrase-based SMT system; 2. Hiero (Chiang 2007), a state-of-the-art hierarchical phrase-based system; 3. Lynx (Liu, Liu, and Lin 2006), a linguistically syntax-based system that makes use of tree-to-string rules. Table 12 Comparison of some word alignment systems on the Canadian Hansard data. System Training Test AER Och and Ney (2003) 1.5M 500 5.2 Moore (2005) 500K 223 7.5 Taskar, Lacoste-Julien, and Klein (2005) 1.1M 347 5.4 Liang, Taskar, and Klein (2006) 1.1M 347 4.9 Lacoste-Julien et al. (2006) 1.1M 247 3.8 Blunsom and Cohn (2006) 1.1M 347 5.2 Moore, Yih, and Bode (2006) 1.1M 223 3.7 This work 1.1M 247 3.6 329 Computational Linguistics Volume 36, Number 3 For all three systems we trained the translation models on the FBIS corpus (7.2M+9.2M words). For the language model, we used the SRI Language Modeling Toolkit (Stolcke 2002) to train a trigram model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We used the 2</context>
<context position="72290" citStr="Moore (2005)" startWordPosition="12161" endWordPosition="12162"> model can be trained using both unsupervised and semi-supervised training methods. Recent years have witnessed the rapid development of discriminative alignment methods. As a first attempt, Och and Ney (2003) proposed the Model 6, which is a log-linear combination of the IBM models and the HMM model. Cherry and Lin (2003) develop a statistical model to find word alignments, which allows for easy integration of context-specific features. Liu, Liu, and Lin (2005) apply the log-linear model used in SMT (Och and Ney 2002) to word alignment and report significant improvements over the IBM models. Moore (2005) presents a discriminative framework for word alignment and uses averaged perceptron for parameter optimization. Taskar, LacosteJulien, and Klein (2005) treat the alignment prediction task as a maximum weight bipartite matching problem and use the large-margin method to train feature weights. Neural networks and transformation-based learning have also been introduced to word alignment (Ayan, Dorr, and Monz 2005a, 2005b). Blunsom and Cohn (2006) propose a new discriminative model based on conditional random fields (CRF). Fraser and Marcu (2006) use sub-models of IBM Model 4 as features and trai</context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>Moore, Robert C. 2005. A discriminative framework for bilingual word alignment. In Proceedings of HLT-EMNLP 2005, pages 81–88, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Wen-tau Yih</author>
<author>Andreas Bode</author>
</authors>
<title>Improved discriminative bilingual word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>513--520</pages>
<location>Sydney.</location>
<marker>Moore, Yih, Bode, 2006</marker>
<rawString>Moore, Robert C., Wen-tau Yih, and Andreas Bode. 2006. Improved discriminative bilingual word alignment. In Proceedings of COLING-ACL 2006, pages 513–520, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Stephan Vogel</author>
</authors>
<title>Discriminative word alignment via alignment matrix modeling.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>18--25</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="73443" citStr="Niehues and Vogel 2008" startWordPosition="12321" endWordPosition="12324">r and Marcu (2006) use sub-models of IBM Model 4 as features and train feature weights using a semi-supervised algorithm. Ayan and Dorr (2006b) use a maximum entropy model to combine word alignments. Cherry and Lin (2006) show that introducing soft syntactic constraints through discriminative training can improve alignment quality. LacosteJulien et al. (2006) extend the bipartite matching model of Taskar, Lacoste-Julien, and Klein (2005) by including fertility and first-order interactions. Recently, max-product belief propagation has been successfully applied to discriminative word alignment (Niehues and Vogel 2008; Cromier`es and Kurohashi 2009). Haghighi et al. (2009) investigate supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. Our work can be seen as an application of the linear model (Och 2003) in word alignment. While aiming at producing symmetric word alignments in a discriminative way, our approach uses asymmetric generative models (Brown et al. 1993) as the major information sources. Our linear model is similar to that of Moore, Yih, and Bode (2006). They train two linear models called stage 1 and stage 2. The feature values are extracted from word</context>
</contexts>
<marker>Niehues, Vogel, 2008</marker>
<rawString>Niehues, Jan and Stephan Vogel. 2008. Discriminative word alignment via alignment matrix modeling. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 18–25, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL 2003,</booktitle>
<pages>160--167</pages>
<location>Sapporo.</location>
<contexts>
<context position="11180" citStr="Och (2003)" startWordPosition="1742" endWordPosition="1743">ignment considers a2 as the best candidate, a1 has the maximal model score. This is undesirable because the model fails to agree with the reference. If we change the feature weights to {1.0, −2.0,2.0}, the model scores become −73, −71, and −83, respectively. Now, the model chooses a2 as the best candidate correctly. If a set of feature weights manages to make model predictions agree with reference alignments in training examples, we would expect the model to achieve good alignment quality on unseen data as well. To do this, we adopt the minimum error rate training (MERT) algorithm proposed by Och (2003) to find feature weights that minimize AER or maximize F-measure on a representative hand-aligned training corpus. Given a reference alignment r and a candidate alignment a, we use a loss function E(r, a) to measure alignment performance. Note that E(r, a) can be either AER or 1 − F-measure. Given a bilingual corpus (fS1, eS1) with a reference alignment rs and a set of K different candidate alignments Cs = {as,1 . . . as,K} for each sentence pair (fs, es), our goal is to find a set of feature weights ˆλM1 that minimizes the overall loss on the training corpus: ˆλM1 = argmin S �E(rs, ˆa(fs, es;</context>
<context position="13555" citStr="Och (2003)" startWordPosition="2195" endWordPosition="2196">nments in Table 1, suppose we only tune λ2 and keep λ1 and λ3 fixed with an initial set of parameters {1.0,1.0,1.0}. According to Equation (10), a1 corresponds to a line 4γ − 75, a2 corresponds to a line 3γ − 77, and a3 corresponds to a line 6γ − 82. The decision rule in Equation (9) states that aˆ is the line with the highest model score for a given γ. The selection of γ for each sentence pair ultimately determines the loss at γ. How do we find values of γ that could generate different loss values? As the loss can only change if we move to a γ where the highest line is different than before, Och (2003) suggests only evaluating the loss at values in between the intersections that line the top surface of the cluster of lines. Figure 2 demonstrates eight Figure 2 Candidate alignments in dimension γ and the critical intersections. Each candidate alignment is represented as a line. γ1,γ2, and γ3 are critical intersections where the best candidate aˆ (highlighted in bold) will change: aˆ is a1 in (−oo,γ1], a2 in (γ1,γ2], a7 in (γ2,γ3], and a5 in (γ3, +oo). 308 Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling candidate alignments. The sequence of the topmost line segments highlig</context>
<context position="14845" citStr="Och (2003)" startWordPosition="2403" endWordPosition="2404">lignments the model predicts with various values of -y. Instead of computing all possible K2 intersections between the lines in Cs, we just need to find the critical intersections where the topmost line changes. In Figure 2, -y1, -y2, and -y3 are critical intersections. In the interval (−oo,-y1], a1 has the highest score. Similarly, the best candidates are a2 for (-y1,-y2], a7 for (-y2,-y3], and a5 for (-y3,+oo), respectively. The optimal -yˆ can be found by collecting all critical intersections on the training corpus and choosing one -y that results in the minimal loss value. Please refer to Och (2003) for more details. 2.3 Search Given a source language sentence f and a target language sentence e, we try to find the best candidate alignment with the highest model score: aˆ = argmax ( � a score(f, e, a) (13) = argmax ~ EM ~ a Amhm(f,e,a) (14) m=1 To do this, we begin with an empty alignment and keep adding new links until the model score of the current alignment does not increase. Figure 3 illustrates this search process. Given a source language sentence f1f2 and a target language sentence e1e2, the initial alignment a1 is empty (i.e., all words are unaligned). Then, we obtain a new alignme</context>
<context position="65476" citStr="Och 2003" startWordPosition="11020" endWordPosition="11021">ms, the 2005 test set as the devtest set for choosing optimal values of α for different translation systems, and the 2008 test set as the final test set. Our evaluation metric is case-sensitive BLEU-4, as defined by NIST, that is, using the shortest (as opposed to closest) reference length for brevity penalty. We annotated the first 200 sentences of the FBIS corpus using the Blinker guidelines (Melamed 1998). All links are sure ones. These hand-aligned sentences served as the training corpus for Vigne. To train the feature weights in our discriminative model using minimum-error-rate training (Och 2003), we adopt balanced F-measure (Fraser and Marcu 2007b) as the optimization criterion. The pipeline begins by running GIZA++ and Cross-EM on the FBIS corpus. We used seven generative alignment methods based on IBM Model 4 and HMM as baseline systems: (1) CSE, (2) ESC, (3) intersection, (4) union, (5) refined method (Och and Ney 2003), (6) grow-diag-final (Koehn, Och, and Marcu 2003), and (7) Cross-EM (Liang, Taskar, and Klein 2006). Instead of exploring the entire search space, our linear model only searches within the union of baseline predictions, which enables our system to align large bilin</context>
<context position="73678" citStr="Och 2003" startWordPosition="12358" endWordPosition="12359"> syntactic constraints through discriminative training can improve alignment quality. LacosteJulien et al. (2006) extend the bipartite matching model of Taskar, Lacoste-Julien, and Klein (2005) by including fertility and first-order interactions. Recently, max-product belief propagation has been successfully applied to discriminative word alignment (Niehues and Vogel 2008; Cromier`es and Kurohashi 2009). Haghighi et al. (2009) investigate supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. Our work can be seen as an application of the linear model (Och 2003) in word alignment. While aiming at producing symmetric word alignments in a discriminative way, our approach uses asymmetric generative models (Brown et al. 1993) as the major information sources. Our linear model is similar to that of Moore, Yih, and Bode (2006). They train two linear models called stage 1 and stage 2. The feature values are extracted from word-aligned sentence pairs. After the stage 1 model aligns the entire training corpus automatically, the stage 2 model uses features based not only on the parallel 332 Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling sen</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, Franz J. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL 2003, pages 160–167, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL 2002,</booktitle>
<pages>295--302</pages>
<location>Philadephia, PA.</location>
<contexts>
<context position="72202" citStr="Och and Ney 2002" startWordPosition="12146" endWordPosition="12149">ive model called LEAF that directly models manyto-many non-consecutive word alignments. Their model can be trained using both unsupervised and semi-supervised training methods. Recent years have witnessed the rapid development of discriminative alignment methods. As a first attempt, Och and Ney (2003) proposed the Model 6, which is a log-linear combination of the IBM models and the HMM model. Cherry and Lin (2003) develop a statistical model to find word alignments, which allows for easy integration of context-specific features. Liu, Liu, and Lin (2005) apply the log-linear model used in SMT (Och and Ney 2002) to word alignment and report significant improvements over the IBM models. Moore (2005) presents a discriminative framework for word alignment and uses averaged perceptron for parameter optimization. Taskar, LacosteJulien, and Klein (2005) treat the alignment prediction task as a maximum weight bipartite matching problem and use the large-margin method to train feature weights. Neural networks and transformation-based learning have also been introduced to word alignment (Ayan, Dorr, and Monz 2005a, 2005b). Blunsom and Cohn (2006) propose a new discriminative model based on conditional random </context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Och, Franz J. and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of ACL 2002, pages 295–302, Philadephia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="8061" citStr="Och and Ney (2003)" startWordPosition="1220" endWordPosition="1223">(f, e, a) (2) m=1 where hm(f, e, a) is a feature function and λm is its associated feature weight. The linear combination of features gives an overall score score(f, e, a) to each candidate alignment a for a given sentence pair (f, e). 2.2 Training To achieve good alignment quality, it is essential to find a good set of feature weights λM1 . Before discussing how to train λM1 , we first describe two evaluation metrics that measure alignment quality, because we will optimize λM1 with respect to them directly. 2.2.1 Evaluation Metrics. The first metric is alignment error rate (AER), proposed by Och and Ney (2003). AER has been used as official evaluation criterion in most word alignment shared tasks. Och and Ney define two kinds of links in hand-aligned alignments: sure links for alignments that are unambiguous and possible links for ambiguous alignments. Sure links usually connect content words such as Zhongguo and China. In contrast, possible links often align words within idiomatic expressions and free translations. An AER score is given by AER(S,P,A)=1_ |A n S |+ |A n P |(3) |A |+ |S| where S is a set of sure links in a reference alignment that is hand-aligned by human experts, P is a set of possi</context>
<context position="27589" citStr="Och and Ney 2003" startWordPosition="4661" endWordPosition="4664">shows the feature values for some word alignments. For efficiency, we need to calculate the difference of feature values instead of the values themselves, which we call feature gain (see Equation (18)). The feature gain for translation probability product is4 gtpp(f,e,a,j,i) = log(t(ei|fj)) + log(t(fj|ei)) − log(δ(ψj,0) x t( fj|e0) + 1 − δ(ψj, 0)~ − log(δ(φi,0) x t(ei|f0) + 1 − δ(φi,0)) (22) where ψj and φi are the fertilities before adding the link (j, i). Although this feature is symmetric, we obtain the translation probabilities t( f |e) and t(e|f ) by training the IBM models using GIZA++ (Och and Ney 2003). 3.2 Exact Match Motivated by the fact that proper names (e.g., IBM) or specialized terms (e.g., DNA) are often the same in both languages, Taskar, Lacoste-Julien, and Klein (2005) use a feature that sums up the number of words linked to identical words. We adopt this exact match feature in our model: �hem(f,e,a) = δ( fj, ei) (23) (j,i)∈a 4 For clarity, we use gtpp(f, e, a, j, i) instead of gtpp(f, e, a,l) because j and i appear in the equation. 314 Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling gem(f, e, a, j, i) = δ( fj,ei) (24) 3.3 Cross Count Due to the diversity of na</context>
<context position="50195" citStr="Och and Ney (2003)" startWordPosition="8592" endWordPosition="8595">bility of an empty cept in a different way (see Equation B.7). Interestingly, Vigne achieves slightly better results than GIZA++ for both models. All differences are not statistically significant. 4.1.2 Comparison to Generative Models Using Asymmetric Features. Table 7 compares the AER scores achieved by GIZA++, Cross-EM (Liang, Taskar, and Klein 2006), and Vigne. On both tasks, we lowercased all English words in the training, development, and test sets as a preprocessing step. For GIZA++, we used the default training scheme of 15H53545. We used the three symmetrization heuristics proposed by Och and Ney (2003): intersection, union, and refined method. For Cross-EM, we also used the default configuration and jointly trained Model 1 and HMM for five iterations. For Vigne, we used a greedy search strategy by setting β = 0 and b = 1. Note that both GIZA++ and Cross-EM are unsupervised alignment methods. On the English–French task, the refined combination of Model 4 alignments produced by GIZA++ in both translation directions yields an AER of 5.9%. Cross-EM outperforms GIZA++ significantly by achieving 5.1%. For Vigne, we use Model 4 as the primary feature. The linear combination of Model 4 in both dire</context>
<context position="54100" citStr="Och and Ney (2003)" startWordPosition="9221" endWordPosition="9224">do not allow a source word to be aligned with more than one target word, the activation of the IBM models in both directions always yields oneto-one alignments and thus has a loss in recall. To alleviate this problem, we use a heuristic postprocessing step to produce many-to-one or one-to-many alignments. First, we collect links that have higher translation probabilities than corresponding null links in both directions. Then, these candidate links are sorted according to their translation probabilities. Finally, they are added to the alignments under structural constraints similar to those of Och and Ney (2003). On the English–French task, this symmetrization method achieves relatively small but very consistent improvements ranging from 0.1% to 0.2%. On the Chinese–English task, the improvements are more significant, ranging from 0.1% to 0.8%. This difference also results from the fact that the reference alignments of the Chinese–English task contain more one-to-many and many-to-one relationships than the English–French task. After symmetrization, the final AER scores for the two tasks are 3.8% and 15.1%, respectively. 4.1.3 Resulting Feature Weights. Table 8 shows the resulting feature weights of m</context>
<context position="64137" citStr="Och and Ney (2003)" startWordPosition="10798" endWordPosition="10801">valuation of Translation Quality In this section, we report on experiments with Chinese-to-English translation. To investigate the effect of our discriminative model on translation performance, we used three translation systems: 1. Moses (Koehn and Hoang 2007), a state-of-the-art phrase-based SMT system; 2. Hiero (Chiang 2007), a state-of-the-art hierarchical phrase-based system; 3. Lynx (Liu, Liu, and Lin 2006), a linguistically syntax-based system that makes use of tree-to-string rules. Table 12 Comparison of some word alignment systems on the Canadian Hansard data. System Training Test AER Och and Ney (2003) 1.5M 500 5.2 Moore (2005) 500K 223 7.5 Taskar, Lacoste-Julien, and Klein (2005) 1.1M 347 5.4 Liang, Taskar, and Klein (2006) 1.1M 347 4.9 Lacoste-Julien et al. (2006) 1.1M 247 3.8 Blunsom and Cohn (2006) 1.1M 347 5.2 Moore, Yih, and Bode (2006) 1.1M 223 3.7 This work 1.1M 247 3.6 329 Computational Linguistics Volume 36, Number 3 For all three systems we trained the translation models on the FBIS corpus (7.2M+9.2M words). For the language model, we used the SRI Language Modeling Toolkit (Stolcke 2002) to train a trigram model with modified Kneser-Ney smoothing on the Xinhua portion of the Giga</context>
<context position="65810" citStr="Och and Ney 2003" startWordPosition="11073" endWordPosition="11076">he first 200 sentences of the FBIS corpus using the Blinker guidelines (Melamed 1998). All links are sure ones. These hand-aligned sentences served as the training corpus for Vigne. To train the feature weights in our discriminative model using minimum-error-rate training (Och 2003), we adopt balanced F-measure (Fraser and Marcu 2007b) as the optimization criterion. The pipeline begins by running GIZA++ and Cross-EM on the FBIS corpus. We used seven generative alignment methods based on IBM Model 4 and HMM as baseline systems: (1) CSE, (2) ESC, (3) intersection, (4) union, (5) refined method (Och and Ney 2003), (6) grow-diag-final (Koehn, Och, and Marcu 2003), and (7) Cross-EM (Liang, Taskar, and Klein 2006). Instead of exploring the entire search space, our linear model only searches within the union of baseline predictions, which enables our system to align large bilingual corpus at a very fast speed of 3, 000 words per second. In other words, our system is able to annotate the FBIS corpus in about 1.5 hours. Then, we train the feature weights of the linear model on the training corpus with respect to F-measure under different settings of α. After that, our system runs on the FBIS corpus to produ</context>
<context position="71887" citStr="Och and Ney (2003)" startWordPosition="12093" endWordPosition="12096">nowadays. Liang, Taskar, and Klein (2006) present an unsupervised way to produce symmetric alignments by training two simple asymmetric models (e.g., IBM Model 1 and the HMM model) jointly to maximize a combination of data likelihood and agreement between the models. Fraser and Marcu (2007a) introduce a new generative model called LEAF that directly models manyto-many non-consecutive word alignments. Their model can be trained using both unsupervised and semi-supervised training methods. Recent years have witnessed the rapid development of discriminative alignment methods. As a first attempt, Och and Ney (2003) proposed the Model 6, which is a log-linear combination of the IBM models and the HMM model. Cherry and Lin (2003) develop a statistical model to find word alignments, which allows for easy integration of context-specific features. Liu, Liu, and Lin (2005) apply the log-linear model used in SMT (Och and Ney 2002) to word alignment and report significant improvements over the IBM models. Moore (2005) presents a discriminative framework for word alignment and uses averaged perceptron for parameter optimization. Taskar, LacosteJulien, and Klein (2005) treat the alignment prediction task as a max</context>
<context position="79615" citStr="Och and Ney 2003" startWordPosition="13352" endWordPosition="13355">boolean expression expr as the argument Appendix B: Using the IBM Models as Feature Functions In this article, we use IBM Models 1–4 as feature functions by taking the logarithm of the models themselves rather than the sub-models just for simplicity. It is easy to separate each sub-model as a feature as suggested by Fraser and Marcu (2006). We distinguish 334 Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling between two translation directions (i.e., source-to-target and target-to-source) to use the IBM models as feature functions. All model parameters are estimated by GIZA++ (Och and Ney 2003). The feature function for the IBM Model 1 is C J hm1 (f, e, a) = log E(J|I)rlt(fj|eaj) (B.1) + 1)J j=1 where c(J|I) predicts the length of the source sentence conditioned on that of the target sentence, (I + 1)−J defines a uniform distribution of the alignment between source and target words, and t( fj|ei) is a translation sub-model. Note that aj = i, which means that fj is connected to ei. The corresponding feature gain is gm1(f, e, a,l) = log(t(fj|ei)) − log(t( fj|e0)) (B.2) where fj and ei are linked by l and e0 is the empty cept to which all unaligned source words are “aligned.” Based on </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, Franz J. and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="3474" citStr="Och and Ney 2004" startWordPosition="493" endWordPosition="496">tically, respectively. They are numbered to facilitate identification. The dark points indicate the correspondence between the words in two languages. The goal of word alignment is to identify such correspondences in a parallel text. Word alignment plays an important role in many NLP tasks. In statistical machine translation, word-aligned corpora serve as an excellent source for translation-related knowledge. The estimation of translation model parameters usually relies heavily on word-aligned corpora, not only for phrase-based and hierarchical phrase-based models (Koehn, Och, and Marcu 2003; Och and Ney 2004; Chiang 2005, 2007), but also for syntax-based models (Quirk, Menezes, and Cherry 2005; Galley et al. 2006; Liu, Liu, and Lin 2006; Marcu et al. 2006). Besides machine translation, many applications for word-aligned corpora have been suggested, including machine-assisted translation, Figure 1 Example of a word alignment between a Chinese–English sentence pair. The Chinese and English words are listed horizontally and vertically, respectively. They are numbered to facilitate identification. The dark points indicate the correspondences between the words in the two languages. 304 Liu, Liu, and L</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Och, Franz J. and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL 2005,</booktitle>
<pages>271--279</pages>
<location>Ann Arbor, MI.</location>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Quirk, Chris, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings of ACL 2005, pages 271–279, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved word-level system combination for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>312--319</pages>
<location>Prague.</location>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Rosti, Antti-Veikko, Spyros Matsoukas, and Richard Schwartz. 2007. Improved word-level system combination for machine translation. In Proceedings of ACL 2007, pages 312–319, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM—an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP 2002,</booktitle>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="64643" citStr="Stolcke 2002" startWordPosition="10886" endWordPosition="10887">on of some word alignment systems on the Canadian Hansard data. System Training Test AER Och and Ney (2003) 1.5M 500 5.2 Moore (2005) 500K 223 7.5 Taskar, Lacoste-Julien, and Klein (2005) 1.1M 347 5.4 Liang, Taskar, and Klein (2006) 1.1M 347 4.9 Lacoste-Julien et al. (2006) 1.1M 247 3.8 Blunsom and Cohn (2006) 1.1M 347 5.2 Moore, Yih, and Bode (2006) 1.1M 223 3.7 This work 1.1M 247 3.6 329 Computational Linguistics Volume 36, Number 3 For all three systems we trained the translation models on the FBIS corpus (7.2M+9.2M words). For the language model, we used the SRI Language Modeling Toolkit (Stolcke 2002) to train a trigram model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We used the 2002 NIST MT evaluation test set as the development set for training feature weights of translation systems, the 2005 test set as the devtest set for choosing optimal values of α for different translation systems, and the 2008 test set as the final test set. Our evaluation metric is case-sensitive BLEU-4, as defined by NIST, that is, using the shortest (as opposed to closest) reference length for brevity penalty. We annotated the first 200 sentences of the FBIS corpus using th</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, Andreas. 2002. SRILM—an extensible language modeling toolkit. In Proceedings of ICSLP 2002, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simon Lacoste-Julien</author>
<author>Dan Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP</booktitle>
<pages>73--80</pages>
<location>Vancouver.</location>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>Taskar, Ben, Simon Lacoste-Julien, and Dan Klein. 2005. A discriminative matching approach to word alignment. In Proceedings of HLT-EMNLP 2005, pages 73–80, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>836--841</pages>
<location>Copenhagen.</location>
<contexts>
<context position="4407" citStr="Vogel and Ney 1996" startWordPosition="624" endWordPosition="627">lignment between a Chinese–English sentence pair. The Chinese and English words are listed horizontally and vertically, respectively. They are numbered to facilitate identification. The dark points indicate the correspondences between the words in the two languages. 304 Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling translation assessment and critiquing tools, text generation, bilingual lexigraphy, and word sense disambiguation. Various methods have been proposed for finding word alignments between parallel texts. Among them, generative alignment models (Brown et al. 1993; Vogel and Ney 1996) have been widely used to produce word alignments for large bilingual corpora. Describing the relationship of a bilingual sentence pair, a generative model treats word alignment as a hidden process and maximizes the likelihood of a training corpus using the expectation maximization (EM) algorithm. After the maximization process is complete, the unknown model parameters are determined and the word alignments are set to the maximum posterior predictions of the model. However, one drawback of generative models is that they are hard to extend. Generative models usually impose strong independence a</context>
<context position="69708" citStr="Vogel and Ney (1996)" startWordPosition="11731" endWordPosition="11734">f α is 0.5. For Hiero and Lynx, the best α is 0.3, suggesting that recalloriented alignments yield better translation performance. Table 15 gives the BLEU scores of the three systems on the final test set. We used the parameters optimized on the dev and devtest sets. More specifically, Moses used growdiag-final and α = 0.5, Hiero used grow-diag-final and α = 0.3, and Lynx used union and α = 0.3. We find that our discriminative alignment model improves the three systems significantly. 5. Related Work The first generative alignment models were the IBM Models 1–5 proposed by Brown et al. (1993). Vogel and Ney (1996) propose a first-order Hidden Markov model (HMM) for word alignment. They show that it is beneficial to make the alignment probabilities dependent on differences in position rather than on the absolute positions. Och and Table 14 BLEU scores on the devtest set. We use “+” to denote the result that outperforms the best baseline result (highlighted in bold) statistically significantly at p &lt; 0.05. Similarly, we use “++” to denote significantly better than baseline at p &lt; 0.01. Moses Hiero Lynx 24.7 25.7 24.8 20.6 23.5 21.6 20.1 23.2 21.2 24.3 24.1 25.1 24.2 24.0 24.2 25.0 25.8 24.3 23.6 24.9 24.</context>
</contexts>
<marker>Vogel, Ney, 1996</marker>
<rawString>Vogel, Stephan and Hermann Ney. 1996. HMM-based word alignment in statistical translation. In Proceedings of COLING 1996, pages 836–841, Copenhagen.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>