<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9981345">
SimLex-999: Evaluating Semantic Models
With (Genuine) Similarity Estimation
</title>
<author confidence="0.999259">
Felix Hill*
</author>
<affiliation confidence="0.995872">
University of Cambridge
</affiliation>
<author confidence="0.994799">
Roi Reichart**
</author>
<affiliation confidence="0.992464">
Technion, Israel Institute of Technology
</affiliation>
<author confidence="0.986016">
Anna Korhonen*
</author>
<affiliation confidence="0.990609">
University of Cambridge
</affiliation>
<bodyText confidence="0.995969266666667">
We present SimLex-999, a gold standard resource for evaluating distributional semantic mod-
els that improves on existing resources in several important ways. First, in contrast to gold
standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than
association or relatedness so that pairs of entities that are associated but not actually similar
(Freud, psychology) have a low rating. We show that, via this focus on similarity, SimLex-999
incentivizes the development of models with a different, and arguably wider, range of applications
than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete
and abstract adjective, noun, and verb pairs, together with an independent rating of concreteness
and (free) association strength for each pair. This diversity enables fine-grained analyses of the
performance of models on concepts of different types, and consequently greater insight into how
architectures can be improved. Further, unlike existing gold standard evaluations, for which
automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-
the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope
for SimLex-999 to quantify future improvements to distributional semantic models, guiding the
development of the next generation of representation-learning architectures.
</bodyText>
<sectionHeader confidence="0.996903" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999689">
There is very little similar about coffee and cups. Coffee refers to a plant, which is a
living organism or a hot brown (liquid) drink. In contrast, a cup is a man-made solid of
broadly well-defined shape and size with a specific function relating to the consumption
of liquids. Perhaps the only clear trait these concepts have in common is that they are
concrete entities. Nevertheless, in what is currently the most popular evaluation gold
standard for semantic similarity, WordSim(WS)-353 (Finkelstein et al. 2001), coffee and
</bodyText>
<affiliation confidence="0.526767666666667">
* Computer Laboratory University of Cambridge, UK. E-mail: Ifelix.hill, anna.korhonen}®
cl.cam.ac.uk.
** Technion, Israel Institute of Technology, Haifa, Israel. E-mail: roiri®ie.technion.ac.il.
</affiliation>
<note confidence="0.8715804">
Submission received: 25 July 2014; revised submission received: 10 June 2015; accepted for publication:
31 August 2015.
doi:10.1162/COLI a 00237
© 2015 Association for Computational Linguistics
Computational Linguistics Volume 41, Number 4
</note>
<bodyText confidence="0.999595466666667">
cup are rated as more “similar” than pairs such as car and train, which share numerous
common properties (function, material, dynamic behavior, wheels, windows, etc.). Such
anomalies also exist in other gold standards such as the MEN data set (Bruni et al.
2012a). As a consequence, these evaluations effectively penalize models for learning the
evident truth that coffee and cup are dissimilar.
Although clearly different, coffee and cup are very much related. The psychological
literature refers to the conceptual relationship between these concepts as association, al-
though it has been given a range of names including relatedness (Budanitsky and Hirst
2006; Agirre et al. 2009), topical similarity (Hatzivassiloglou et al. 2001), and domain
similarity (Turney 2012). Association contrasts with similarity, the relation connecting
cup and mug (Tversky 1977). At its strongest, the similarity relation is exemplified by
pairs of synonyms; words with identical referents.
Computational models that effectively capture similarity as distinct from associ-
ation have numerous applications. Such models are used for the automatic genera-
tion of dictionaries, thesauri, ontologies, and language correction tools (Biemann 2005;
Cimiano, Hotho, and Staab 2005; Li et al. 2006). Machine translation systems, which
aim to define mappings between fragments of different languages whose meaning is
similar, but not necessarily associated, are another established application (He et al.
2008; Marton, Callison-Burch, and Resnik 2009). Moreover, since, as we establish, sim-
ilarity is a cognitively complex operation that can require rich, structured concep-
tual knowledge to compute accurately, similarity estimation constitutes an effective
proxy evaluation for general-purpose representation-learning models whose ultimate
application is variable or unknown (Collobert and Weston 2008; Baroni and Lenci
2010).
As we show in Section 2, the predominant gold standards for semantic evaluation in
NLP do not measure the ability of models to reflect similarity. In particular, in both WS-
353 and MEN, pairs of words with associated meaning, such as coffee and cup (rating =
6.810), telephone and communication (7.510), or movie and theater (7.710), receive a high
rating regardless of whether or not their constituents are similar. Thus, the utility of
such resources to the development and application of similarity models is limited, a
problem exacerbated by the fact that many researchers appear unaware of what their
evaluation resources actually measure.1
Although certain smaller gold standards—those of Rubenstein and Goodenough
(1965) (RG) and Agirre et al. (2009) (WS-Sim)—do focus clearly on similarity, these
resources suffer from other important limitations. For instance, as we show, and as is
also the case for WS-353 and MEN, state-of-the-art models have reached the average
performance of a human annotator on these evaluations. It is common practice in NLP
to define the upper limit for automated performance on an evaluation as the average hu-
man performance or inter-annotator agreement (Yong and Foo 1999; Cunningham 2005;
Resnik and Lin 2010). Based on this established principle and the current evaluations, it
would therefore be reasonable to conclude that the problem of representation learning,
at least for similarity modeling, is approaching resolution. However, circumstantial
evidence suggests that distributional models are far from perfect. For instance, we are
some way from automatically generated dictionaries, thesauri, or ontologies that can be
used with the same confidence as their manually created equivalents.
</bodyText>
<footnote confidence="0.62824175">
1 For instance, Huang et al. (2012, pages 1, 4, 10) and Reisinger and Mooney (2010b, page 4) refer to MEN
and/or WS-353 as “similarity data sets.” Others evaluate on both these association-based and genuine
similarity-based gold standards with no reference to the fact that they measure different things
(Medelyan et al. 2009; Li et al. 2014).
</footnote>
<page confidence="0.98959">
666
</page>
<note confidence="0.949991">
Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models
</note>
<bodyText confidence="0.999841869565217">
Motivated by these observations, in Section 3 we present SimLex-999, a gold stan-
dard resource for evaluating the ability of models to reflect similarity. SimLex-999 was
produced by 500 paid native English speakers, recruited via Amazon Mechanical Turk,2
who were asked to rate the similarity, as opposed to association, of concepts via a
simple visual interface. The choice of evaluation pairs in SimLex-999 was motivated
by empirical evidence that humans represent concepts of distinct part-of-speech (POS)
(Gentner 1978) and conceptual concreteness (Hill, Korhonen, and Bentz 2014) differ-
ently. Whereas existing gold standards contain only concrete noun concepts (MEN) or
cover only some of these distinctions via a random selection of items (WS-353, RG),
SimLex-999 contains a principled selection of adjective, verb, and noun concept pairs
covering the full concreteness spectrum. This design enables more nuanced analyses of
how computational models overcome the distinct challenges of representing concepts
of these types.
In Section 4 we present quantitative and qualitative analyses of the SimLex-999
ratings, which indicate that participants found it unproblematic to quantify consistently
the similarity of the full range of concepts and to distinguish it from association. Unlike
existing data sets, SimLex-999 therefore contains a significant number of pairs, such as
[movie, theater], which are strongly associated but receive low similarity scores.
The second main contribution of this paper, presented in Section 5, is the evaluation
of state-of-the-art distributional semantic models using SimLex-999. These include the
well-known neural language models (NLMs) of Huang et al. (2012), Collobert and
Weston (2008), and Mikolov et al. (2013a), which we compare with traditional vector-
space co-occurrence models (VSMs) (Turney and Pantel 2010) with and without dimen-
sionality reduction (SVD) (Landauer and Dumais 1997). Our analyses demonstrate how
SimLex-999 can be applied to uncover substantial differences in the ability of models
to represent concepts of different types.
Despite these differences, the models we consider each share the characteristic of
being better able to capture association than similarity. We show that the difficulty of
estimating similarity is driven primarily by those strongly associated pairs with a high
(association) rating in gold standards such as WS-353 and MEN, but a low similarity
rating in SimLex-999. As a result of including these challenging cases, together with
a wider diversity of lexical concepts in general, current models achieve notably lower
scores on SimLex-999 than on existing gold standard evaluations, and well below the
SimLex-999 inter-human agreement ceiling.
Finally, we explore ways in which distributional models might improve on this
performance in similarity modeling. To do so, we evaluate the models on the SimLex-
999 subsets of adjectives, nouns, and verbs, as well as on abstract and concrete subsets
and subsets of more and less strongly associated pairs (Sections 5.2.2–5.2.4). As part
of these analyses, we confirm the hypothesis (Agirre et al. 2009; Levy and Goldberg
2014) that models learning from input informed by dependency parsing, rather than
simple running-text input, yield improved similarity estimation and, specifically, clearer
distinction between similarity and association. In contrast, we find no evidence for a
related hypothesis (Agirre et al. 2009; Kiela and Clark 2014) that smaller context win-
dows improve the ability of models to capture similarity. We do, however, observe clear
differences in model performance on the distinct concept types included in SimLex-999.
Taken together, these experiments demonstrate the benefit of the diversity of concepts
</bodyText>
<footnote confidence="0.786875">
2 www.mturk.com/.
</footnote>
<page confidence="0.981304">
667
</page>
<note confidence="0.839445">
Computational Linguistics Volume 41, Number 4
</note>
<bodyText confidence="0.999378142857143">
included in SimLex-999; it would not have been possible to derive similar insights by
evaluating based on existing gold standards.
We conclude by discussing how observations such as these can guide future re-
search into distributional semantic models. By facilitating better-defined evaluations
and finer-grained analyses, we hope that SimLex-999 will ultimately contribute to the
development of models that accurately reflect human intuitions of similarity for the full
range of concepts in language.
</bodyText>
<sectionHeader confidence="0.997901" genericHeader="method">
2. Design Motivation
</sectionHeader>
<bodyText confidence="0.999937">
In this section, we motivate the design decisions made in developing SimLex-999. We
begin (2.1) by examining the distinction between similarity and association. We then
show that for a meaningful treatment of similarity it is also important to take a princi-
pled approach to both POS and conceptual concreteness (2.2). We finish by reviewing
existing gold standards, and show that none enables a satisfactory evaluation of the
capability of models to capture similarity (2.3).
</bodyText>
<subsectionHeader confidence="0.971676">
2.1 Similarity and Association
</subsectionHeader>
<bodyText confidence="0.999950838709678">
The difference between association and similarity is exemplified by the concept pairs
[car, bike] and [car, petrol]. Car is said to be (semantically) similar to bike and associated
with (but not similar to) petrol. Intuitively, car and bike can be understood as similar
because of their common physical features (e.g., wheels), their common function (trans-
port), or because they fall within a clearly definable category (modes of transport). In
contrast, car and petrol are associated because they frequently occur together in space
and language, in this case as a result of a clear functional relationship (Plaut 1995;
McRae, Khalkhali, and Hare 2012).
Association and similarity are neither mutually exclusive nor independent. Bike
and car, for instance, are related to some degree by both relations. Because it is com-
mon in both the physical world and in language for distinct entities to interact, it is
relatively easy to conceive of concept pairs, such as car and petrol, that are strongly
associated but not similar. Identifying pairs of concepts for which the converse is true
is comparatively more difficult. One exception is common concepts paired with low
frequency synonyms, such as camel and dromedary. Because the essence of association is
co-occurrence (linguistic or otherwise [McRae, Khalkhali, and Hare 2012]), such pairs
can seem, at least intuitively, to be similar but not strongly associated.
To explore the interaction between the two cognitive phenomena quantitatively, we
exploited perhaps the only two existing large-scale means of quantifying similarity and
association. To estimate similarity, we considered proximity in the WordNet taxonomy
(Fellbaum 1998). Specifically, we applied the measure of Wu and Palmer (1994) (hence-
forth WupSim), which approximates similarity on a [0,1] scale reflecting the minimum
distance between any two synsets of two given concepts in WordNet. WupSim has
been shown to correlate well with human judgments on the similarity-focused RG data
set (Wu and Palmer 1994). To estimate association, we extracted ratings directly from
the University of South Florida Free Association Database (USF) (Nelson, McEvoy, and
Schreiber 2004). These data were generated by presenting human subjects with one of
5,000 cue concepts and asking them to write the first word that comes into their head that
is associated with or meaningfully related to that concept. Each cue concept c was normed in
this way by over 10 participants, resulting in a set of associates for each cue, and a total
of over 72,000 (c, a) pairs. Moreover, for each such pair, the proportion of participants
</bodyText>
<page confidence="0.995927">
668
</page>
<note confidence="0.979787">
Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models
</note>
<tableCaption confidence="0.689734333333333">
Table 1
Top: Concept pairs with the lowest WupSim scores in the USF data set overall. Bottom: Pairs
with the largest discrepancy in rank between association strength (high) and WupSim (low).
</tableCaption>
<table confidence="0.530737777777778">
Concept 1 Concept 2 USF WupSim
hatchet murder 0.013 0.091
robbery jail 0.020 0.100
lung disease 0.014 0.105
burglar robbery 0.020 0.105
sheriff police 0.333 0.133
colonel army 0.303 0.111
quart milk 0.462 0.235
refrigerator food 0.424 0.235
</table>
<bodyText confidence="0.998705133333334">
who produced associate a when presented with cue c can be used as a proxy for the
strength of association between the two concepts.
By measuring WupSim between all pairs in the USF data set, we observed, as
expected, a high correlation between similarity and association strength across all USF
pairs (Spearman p = 0.65,p &lt; 0.001). However, in line with the intuitive ubiquity of
pairs such as car and petrol, of the USF pairs (all of which are associated to a greater or
lesser degree) over 10% had a WupSim score of less than 0.25. These include pairs of
ontologically different entities with a clear functional relationship in the world [refrig-
erator, food], which may be of differing concreteness [lung, disease]; pairs in which one
concept is a small concrete part of a larger abstract category [sheriff, police]; pairs in a
relationship of modification or subcategorization [gravy, boat]; and even those whose
principal connection is phonetic [wiggle, giggle]. As we show in Section 2.2, these are
precisely the sort of pairs that are not contained in existing evaluation gold standards.
Table 1 lists the USF noun pairs with the lowest similarity scores overall, and also those
with the largest additive discrepancy between association strength and similarity.
2.1.1 Association and Similarity in NLP. As noted in the Introduction, the similar-
ity/association distinction is not only of interest to researchers in psychology or linguis-
tics. Models of similarity are particularly applicable to various NLP tasks, such as lexical
resource building, semantic parsing, and machine translation (Haghighi et al. 2008; He
et al. 2008; Marton, Callison-Burch, and Resnik 2009; Beltagy, Erk, and Mooney 2014).
Models of association, on the other hand, may be better suited to tasks such as word-
sense disambiguation (Navigli 2009), and applications such as text classification (Phan,
Nguyen, and Horiguchi 2008) in which the target classes correspond to topical domains
such as agriculture or sport (Rose, Stevenson, and Whitehead 2002).
Much recent research in distributional semantics does not distinguish between asso-
ciation and similarity in a principled way (see, e.g., Reisinger and Mooney 2010b; Huang
et al. 2012; Luong, Socher, and Manning 2013).3 One exception is Turney (2012), who
constructs two distributional models with different features and parameter settings,
explicitly designed to capture either similarity or association. Using the output of these
two models as input to a logistic regression classifier, Turney predicts whether two
</bodyText>
<footnote confidence="0.446975">
3 Several papers that take a knowledge-based or symbolic approach to meaning do address the
similarity/association issue (Budanitsky and Hirst 2006).
</footnote>
<page confidence="0.993044">
669
</page>
<note confidence="0.839895">
Computational Linguistics Volume 41, Number 4
</note>
<bodyText confidence="0.9778085">
concepts are associated, similar, or both, with 61% accuracy. However, in the absence
of a gold standard covering the full range of similarity ratings (rather than a list of pairs
identified as being similar or not) Turney cannot confirm directly that the similarity-
focused model does indeed effectively quantify similarity.
Agirre et al. (2009) explicitly examine the distinction between association and simi-
larity in relation to distributional semantic models. Their study is based on the partition
of WS-353 into a subset focused on similarity, which we refer to as WS-Sim, and a subset
focused on association, which we term WS-Rel. More precisely, WS-Sim is the union of
the pairs in WS-353 judged by three annotators to be similar and the set U of entirely
unrelated pairs, and WS-Rel is the union of U and pairs judged to be associated but not
similar. Agirre et al. confirm the importance of the association/similarity distinction by
showing that certain models perform relatively well on WS-Rel, whereas others perform
comparatively better on WS-Sim. However, as shown in the following section, a model
need not be an exemplary model of similarity in order to perform well on WS-Sim,
because an important class of concept pair (associated but not similar entities) is not
represented in this data set. Therefore the insights that can be drawn from the results
of the Agirre et al. study are limited.
Several other authors touch on the similarity/association distinction in inspecting
the output of distributional models (Andrews, Vigliocco, and Vinson 2009; Kiela and
Clark 2014; Levy and Goldberg 2014). Although the strength of the conclusions that
can be drawn from such qualitative analyses is clearly limited, there appear to be two
broad areas of consensus concerning similarity and distributional models:
</bodyText>
<listItem confidence="0.999503714285714">
• Models that learn from input annotated for syntactic or dependency
relations better reflect similarity, whereas approaches that learn from
running-text or bag-of-words input better model association (Agirre et al.
2009; Levy and Goldberg 2014).
• Models with larger context windows may learn representations that better
capture association, whereas models with narrower windows better reflect
similarity (Agirre et al. 2009; Kiela and Clark 2014).
</listItem>
<subsectionHeader confidence="0.999071">
2.2 Concepts, Part-of-Speech, and Concreteness
</subsectionHeader>
<bodyText confidence="0.9991388">
Empirical studies have shown that the performance of both humans and distributional
models depends on the POS category of the concepts learned. Gentner (2006) showed
that children find verb concepts harder to learn than noun concepts, and Markman and
Wisniewski (1997) present evidence that different cognitive operations are used when
comparing two nouns or two verbs. Hill, Reichart, and Korhonen (2014) demonstrate
differences in the ability of distributional models to acquire noun and verb semantics.
Further, they show that these differences are greater for models that learn from both text
and perceptual input (as with humans).
In addition to POS category, differences in human and computational concept learn-
ing and representation have been attributed to the effects of concreteness, the extent
to which a concept has a directly perceptible physical referent. On the cognitive side,
these “concreteness effects” are well established, even if the causes are still debated
(Paivio 1991; Hill, Korhonen, and Bentz 2014). Concreteness has also been associated
with differential performance in computational text-based (Hill, Kiela, and Korhonen
2013) and multi-modal semantic models (Kiela et al. 2014).
</bodyText>
<page confidence="0.997922">
670
</page>
<note confidence="0.87436">
Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models
</note>
<subsectionHeader confidence="0.998632">
2.3 Existing Gold Standards and Evaluation Resources
</subsectionHeader>
<bodyText confidence="0.999543677419355">
For brevity, we do not exhaustively review all methods that have been used to evaluate
semantic models, but instead focus on the similarity or association-based gold standards
that are most commonly applied in recent work in NLP. In each case, we consider how
well the data set satisfies one of the three following criteria:
Representative. The resource should cover the full range of concepts that occur in nat-
ural language. In particular, it should include cases representing the different ways in
which humans represent or process concepts, and cases that are both challenging and
straightforward for computational models.
Clearly defined. In order for a gold standard to be diagnostic of how well a model
can be applied to downstream applications, a clear understanding is needed of what
exactly the gold standard measures. In particular, it must clearly distinguish between
dissociable semantic relations such as association and similarity.
Consistent and reliable. Untrained native speakers must be able to quantify the target
property consistently, without requiring lengthy or detailed instructions. This ensures
that the data reflect a meaningful cognitive or semantic phenomenon, and also enables
the data set to be scaled up or transferred to other languages at minimal cost and effort.
We begin our review of existing evaluation with the gold standard most commonly
applied in current NLP research.
WordSim-353. WS-353 (Finkelstein et al. 2001) is perhaps the most commonly used
evaluation gold standard for semantic models. Despite its name, and the fact that it
is often referred to as a “similarity gold standard,”4 in fact, the instructions given to
annotators when producing WS-353 were ambiguous with respect to similarity and
association. Subjects were asked to:
Assign a numerical similarity score between 0 and 10 (0 = words totally unrelated,10 =
words VERY closely related) ... when estimating similarity of antonyms, consider them
“similar” (i.e., belonging to the same domain or representing features of the same
concept), not “dissimilar”.
As we confirm analytically in Section 5.2, these instructions result in pairs being
rated according to association rather than similarity.5 WS-353 consequently suffers two
important limitations as an evaluation of similarity (which also afflict other resources
to a greater or lesser degree):
</bodyText>
<listItem confidence="0.997258">
1. Many dissimilar word pairs receive a high rating.
2. No associated but dissimilar concepts receive low ratings.
</listItem>
<bodyText confidence="0.8719705">
As noted in the Introduction, an arguably more serious third limitation of WS-353
is low inter-annotator agreement, and the fact that state-of-the-art models such as those
</bodyText>
<footnote confidence="0.996348666666667">
4 See, e.g., Huang et al. 2012 and Bansal, Gimpel, and Livescu 2014.
5 This fact is also noted by the data set authors. See www.cs.technion.ac.il/~gabr/resources/
data/wordsim353/.
</footnote>
<page confidence="0.986575">
671
</page>
<note confidence="0.353176">
Computational Linguistics Volume 41, Number 4
</note>
<bodyText confidence="0.996566613636364">
of Collobert and Weston (2008) and Huang et al. (2012) reach, or even surpass, the
inter-annotator agreement ceiling in estimating the WS-353 scores. Huang et al. report
a Spearman correlation of p = 0.713 between their model output and WS-353. This is 10
percentage points higher than inter-annotator agreement (p = 0.611) when defined as
the average pairwise correlation between two annotators, as is common in NLP work
(Pad´o, Pad´o, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014). It
could be argued that a different comparison is more appropriate: Because the model
is compared to the gold-standard average across all annotators, we should compare
a single annotator with the (almost) gold-standard average over all other annotators.
Based on this metric the average performance of an annotator on WS-353 is p = 0.756,
which is still only marginally better than the best automatic method.6
Thus, at least according to the established wisdom in NLP evaluation (Yong and
Foo 1999; Cunningham 2005; Resnik and Lin 2010), the strength of the conclusions that
can be inferred from improvements on WS-353 is limited. At the same time, however,
state-of-the-art distributional models are clearly not perfect representation-learning or
even similarity estimation engines, as evidenced by the fact they cannot yet be applied,
for instance, to generate flawless lexical resources (Alfonseca and Manandhar 2002).
WS-Sim. WS-Sim is the set of pairs in WS-353 identified by Agirre et al. (2009) as
either containing similar or unrelated (neither similar nor associated) concepts. The
ratings in WS-Sim are mapped directly from WS-353, so that all concept pairs in WS-
Sim that receive a high rating are associated and all pairs that receive a low rating
are unassociated. Consequently, any model that simply reflects association would score
highly on WS-Sim, irrespective of how well it captures similarity.
Such a possibility could be excluded by requiring models to perform well on WS-
Sim and poorly on WS-Rel, the subset of WS-353 identified by Agirre et al. (2009) as
containing no pairs of similar concepts. However, although this would exclude models
of pure association, it would not test the ability of models to quantify the similarity of
the pairs in WS-Sim. Put another way, the WS-Sim/WS-Rel partition could in theory
resolve limitation (1) of WS-353 but it would not resolve limitation (2): Models are not
tested on their ability to attribute low scores to associated but dissimilar pairs.
In fact, there are more fundamental limitations of WS-Sim as a similarity-based
evaluation resource. It does not, strictly speaking, reflect similarity at all, since the
ratings of its constituent pairs were assigned by the WS-353 annotators, who were asked
to estimate association, not similarity. Moreover, it inherits the limitation of low inter-
annotator agreement from WS-353. The average pairwise correlation between annota-
tors on WS-Sim is p = 0.667, and the average correlation of a single annotator with
the gold standard is only p = 0.651, both below the performance of automatic methods
(Agirre et al. 2009). Finally, the small size of WS-Sim renders it poorly representative of
the full range of concepts that semantic models may be required to learn.
Rubenstein &amp; Goodenough. Prior to WS-353, the smaller RG data set, consisting of 65
pairs, was often used to evaluate semantic models. The 15 raters employed in the data
collection were asked to rate the “similarity of meaning” of each concept pair. Thus RG
does appear to reflect similarity rather than association. However, although limitation
(1) of WS-353 is therefore avoided, RG still suffers from limitation (2): By inspection,
</bodyText>
<footnote confidence="0.9959745">
6 Individual annotator responses for WS-353 were downloaded from www.cs.technion.ac.il/~gabr/
resources/data/wordsim353.
</footnote>
<page confidence="0.994994">
672
</page>
<note confidence="0.823997">
Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models
</note>
<bodyText confidence="0.993856655172414">
it is clear that the low similarity pairs in RG are not associated. A further limitation is
that distributional models now achieve better performance on RG (correlations of up to
Pearson r = 0.86 [Hassan and Mihalcea 2011]) than the reported inter-annotator agree-
ment of r = 0.85 (Rubenstein and Goodenough 1965). Finally, the size of RG renders it
an even less comprehensive evaluation than WS-Sim.
The MEN Test Collection. A larger data set, MEN (Bruni et al. 2012a), is used in a
handful of recent studies (Bruni et al. 2012b; Bernardi et al. 2013). As with WS-353, both
terms similarity and relatedness are used by the authors when describing MEN, although
the annotators were expressly asked to rate pairs according to relatedness.7
The construction of MEN differed from RG and WS-353 in that each pair was only
considered by one rater, who ranked it for relatedness relative to 50 other pairs in the
data set. An overall score out of 50 was then attributed to each pair corresponding to
how many times it was ranked as more related than an alternative. However, because
these rankings are based on relatedness, with respect to evaluating similarity MEN
necessarily suffers from both of the limitations (1) and (2) that apply to WS-353. Further,
there is a strong bias towards concrete concepts in MEN because the concepts were
originally selected from those identified in an image-bank (Bruni et al. 2012a).
Synonym Detection Sets. Multiple-choice synonym detection tasks, such as the TOEFL
test questions (Landauer and Dumais 1997), are an alternative means of evaluating
distributional models. A question in the TOEFL task consists of a cue word and four
possible answer words, only one of which is a true synonym. Models are scored
on the number of true synonyms identified out of 80 questions. The questions were
designed by linguists to evaluate synonymy, so, unlike the evaluations considered
thus far, TOEFL-style tests effectively discriminate between similarity and association.
However, because they require a zero-one classification of pairs as synonymous or not,
they do not test how well models discern pairs of medium or low similarity. More
generally, in opposition to the fuzzy, statistical approaches to meaning predominant in
both cognitive psychology (Griffiths, Steyvers, and Tenenbaum 2007) and NLP (Turney
and Pantel 2010), they do not require similarity to be measured on a continuous scale.
</bodyText>
<sectionHeader confidence="0.926078" genericHeader="method">
3. The SimLex-999 Data Set
</sectionHeader>
<bodyText confidence="0.9997675">
Having considered the limitations of existing gold standards, in this section we describe
the design of SimLex-999 in detail.
</bodyText>
<subsectionHeader confidence="0.999965">
3.1 Choice of Concepts
</subsectionHeader>
<bodyText confidence="0.9994725">
Separating similarity from association. To create a test of the ability of models to capture
similarity as opposed to association, we started with the ≈ 72,000 pairs of concepts in
the USF data set. As the output of a free-association experiment, each of these pairs
is associated to a greater or lesser extent. Importantly, inspecting the pairs revealed
that a good range of similarity values are represented. In particular, there were many
examples of hypernym/hyponym pairs [body, abdomen], cohyponym pairs [cat, dog],
synonyms or near synonyms [deodorant, antiperspirant], and antonym pairs [good, evil].
From this cohort, we excluded pairs containing a multiple-word item [hot dog, mustard],
</bodyText>
<page confidence="0.7664535">
7 http://clic.cimec.unitn.it/~elia.bruni/MEN.html.
673
</page>
<note confidence="0.356489">
Computational Linguistics Volume 41, Number 4
</note>
<bodyText confidence="0.998614590909091">
and pairs containing a capital letter [Mexico, sun]. We ultimately sampled 900 of the
SimLex-999 pairs from the resulting cohort of pairs, according to the stratification
procedures outlined in the following sections.
To complement this cohort with entirely unassociated pairs, we paired up the con-
cepts from the 900 associated pairs at random. From these random parings, we excluded
those that coincidentally occurred elsewhere in USF (and therefore had a degree of
association). From the remaining pairs, we accepted only those in which both concepts
had been subject to the USF norming procedure, ensuring that these non-USF pairs
were indeed unassociated rather than simply not normed. We sampled the remaining
99 SimLex-999 pairs from this resulting cohort of unassociated pairs.
POS category. In light of the conceptual differences outlined in Section 2.2, SimLex-
999 includes subsets of pairs from the three principle meaning-bearing POS categories:
nouns, verbs, and adjectives. To classify potential pairs according to POS, we counted
the frequency with which the items in each pair occurred with the three possible tags in
the POS-tagged British National Corpus (Leech, Garside, and Bryant 1994). To minimize
POS ambiguity, which could lead to inconsistent ratings, we excluded pairs containing
a concept with lower than 75% tendency towards one particular POS. This yielded three
sets of potential pairs: [A,A] pairs (of two concepts whose majority tag was Adjective),
[N,N] pairs, and [V,V] pairs.
Given the likelihood that different cognitive operations are used in estimating the
similarity between items of different POS-category (Section 2.2), concept pairs were
presented to raters in batches defined according to POS. Unlike both WS-353 and MEN,
pairs of concepts of mixed POS ([white, rabbit], [run,marathon]) were excluded. POS
categories are generally considered to reflect very broad ontological classes (Fellbaum
1998). We thus felt it would be very difficult, or even counter-intuitive, for annotators to
quantify the similarity of mixed POS pairs according to our instructions.
Concreteness. Although a clear majority of pairs in gold standards such as MEN and RG
contain concrete items, perhaps surprisingly, the vast majority of adjective, noun, and
verb concepts in everyday language are in fact abstract (Hill, Reichart, and Korhonen
2014; Kiela et al. 2014).8 To facilitate the evaluation of models for both concrete and
abstract concept meaning, and in light of the cognitive and computational modeling
differences between abstract and concrete concepts noted in Section 2.2, we aimed to
include both concept types in SimLex-999.
Unlike the POS distinction, concreteness is generally considered to be a gradual
phenomenon. One benefit of sampling pairs for SimLex-999 from the USF data set is
that most items have been rated according to concreteness on a scale of 1–7 by at least
10 human subjects. As Figure 1 demonstrates, concreteness (as the average over these
ratings) interacts with POS on these concepts: Nouns are on average more concrete than
verbs, which are more concrete than adjectives. However, there is also clear variation in
concreteness within each POS category. We therefore aimed to select pairs for SimLex-
999 that spanned the full abstract–concrete continuum within each POS category.
After excluding any pairs that contained an item with no concreteness rating, for
each potential SimLex-999 pair we considered both the concreteness of the first item
and the additive difference in concreteness between the two items. This enabled us
</bodyText>
<footnote confidence="0.7724265">
8 According to the USF concreteness ratings, 72% of noun or verb types in the British National Corpus are
more abstract than the concept war, a concept many would already consider quite abstract.
</footnote>
<page confidence="0.997235">
674
</page>
<note confidence="0.527183">
Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models
</note>
<figure confidence="0.9986625">
belief
Adjectives
Nouns
Verbs
liberal
seem
failure
happy
make
scare
christmas
loud
property
dark
athlete
cough
●
tree
2 3 4 5 6 7
Concreteness Rating
</figure>
<figureCaption confidence="0.992898">
Figure 1
</figureCaption>
<bodyText confidence="0.995179153846154">
Boxplots showing the interaction between concreteness and POS for concepts in USF. The white
boxes range from the first to third quartiles and the central vertical line indicates the median.
to stratify our sampling equally across four classes: (C1) concrete first item (rating
&gt; 4) with below-median concreteness difference; (C2) concrete first item (rating&gt; 4),
second item of lower concreteness and the difference being greater than the median;
(C3) abstract first item (rating &lt; 4) with below-median concreteness difference; and (C4)
abstract first item (rating &lt; 4) with the second item of greater concreteness and the
difference being greater than the median.
Final sampling. From the associated (USF) cohort of potential pairs we selected 600 noun
pairs, 200 verb pairs, and 100 adjective pairs, and from the unassociated (non-USF)
cohort, we sampled 66 nouns pairs, 22 verb pairs, and 11 adjective pairs. In both cases,
the sampling was stratified such that, in each POS subset, each of the four concreteness
classes C1−C4 was equally represented.
</bodyText>
<subsectionHeader confidence="0.999129">
3.2 Question Design
</subsectionHeader>
<bodyText confidence="0.999988722222222">
The annotator instructions for SimLex-999 are shown in Figure 2. We did not attempt
to formalize the notion of similarity, but rather introduce it via the well-understood
idea of synonymy, and in contrast to association. Even if a formal characterization of
similarity existed, the evidence in Section 2 suggests that the instructions would need
separate cases to cover different concept types, increasing the difficulty of the rating
task. Therefore, we preferred to appeal to intuition on similarity, and to verify post hoc
that subjects were able to interpret and apply the informal characterization consistently
for each concept type.
Immediately following the instructions in Figure 2, participants were presented
with two “checkpoint” questions, one with abstract examples and one with concrete
examples. In each case the participant was required to identify the most similar pair from
a set of three options, all of which were associated, but only one of which was clearly
similar (e.g. [bread, butter] [bread, toast] [stale, bread]). After this, the participants began
rating pairs in groups of six or seven pairs by moving a slider, as shown in Figure 3.
This group size was chosen because the (relative) rating of a set of pairs implicitly
requires pairwise comparisons between all pairs in that set. Therefore, larger groups
would have significantly increased the cognitive load on the annotators. Another
advantage of grouping was the clear break (submitting a set of ratings and moving to
</bodyText>
<page confidence="0.987888">
675
</page>
<figure confidence="0.860011">
Computational Linguistics Volume 41, Number 4
</figure>
<figureCaption confidence="0.97623">
Figure 2
</figureCaption>
<bodyText confidence="0.902760666666667">
Instructions for SimLex-999 annotators.
the next page) between the tasks of rating adjective, noun, and verb pairs. For better
inter-group calibration, from the second group onwards the last pair of the previous
group became the first pair of the present group, and participants were asked to
re-assign the rating previously attributed to the first pair before rating the remaining
new items.
</bodyText>
<subsectionHeader confidence="0.998247">
3.3 Context-Free Rating
</subsectionHeader>
<bodyText confidence="0.999911692307692">
As with MEN, WS-353, and RG, SimLex-999 consists of pairs of concept words together
with a numerical rating. Thus, unlike in the small evaluation constructed by Huang et al.
(2012), words are not rated in a phrasal or sentential context. Such meaning-in-context
evaluations are motivated by a desire to disambiguate words that otherwise might be
considered to have multiple senses.
We did not attempt to construct an evaluation based on meaning-in-context for
several reasons. First, determining the set of senses for a given word, and then the set
of contexts that represent those senses, introduces a high degree of subjectivity into the
design process. Second, ensuring that a model has learned a high quality representation
of a given concept would have required evaluating that concept in each of its given
contexts, necessitating many more cases and a far greater annotation effort. Third, in
the (infrequent) case that some concept c1 in an evaluation pair (c1,c2) is genuinely
(etymologically) polysemous, c2 can provide sufficient context to disambiguate c1.9
</bodyText>
<footnote confidence="0.991136666666667">
9 This is supported by the fact that the WordNet-based methods that perform best at modeling human
ratings model the similarity between concepts c1 and c2 as the minimum of all pairwise distances
between the senses of c1 and the senses of c2 (Resnik 1995; Pedersen, Patwardhan, and Michelizzi 2004).
</footnote>
<page confidence="0.99622">
676
</page>
<note confidence="0.913413">
Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models
</note>
<figureCaption confidence="0.928131">
Figure 3
</figureCaption>
<bodyText confidence="0.990199">
A group of noun pairs to be rated by moving the sliders. The rating slider was initially at
position 0, and it was possible to attribute a rating of 0, although it was necessary to have
actively moved the slider to that position to proceed to the next page.
Finally, the POS grouping of pairs in the survey can also serve to disambiguate in
the case that the conflicting senses of the polysemous concept are of differing POS
categories.
</bodyText>
<subsectionHeader confidence="0.998894">
3.4 Questionnaire Structure
</subsectionHeader>
<bodyText confidence="0.9999675">
Each participant was asked to rate 20 groups of pairs on a 0–6 scale of integers (non-
integral ratings were not possible). Checkpoint multiple-choice questions were inserted
at points between the 20 groups in order to ensure the participant had retained the
correct notion of similarity. In addition to the checkpoint of three noun pairs presented
before the first group (which contained noun pairs), checkpoint questions containing
adjective pairs were inserted before the first adjective group and checkpoints of three
verb pairs were inserted before the first verb group.
From the 999 evaluation pairs, 14 noun pairs, 4 verb pairs, and 2 adjective pairs
were selected as a consistency set. The data set of pairs was then partitioned into 10
tranches, each consisting of 119 pairs, of which 20 were from the consistency set and the
remaining 99 unique to that tranche. To reduce workload, each annotator was asked to
rate the pairs in a single tranche only. The tranche itself was divided into 20 groups, with
</bodyText>
<page confidence="0.976904">
677
</page>
<note confidence="0.336984">
Computational Linguistics Volume 41, Number 4
</note>
<bodyText confidence="0.999658333333333">
each group consisting of 7 pairs (with the exception of the last group of the 20, which
had 6). Of these seven pairs, the first pair was the last pair from the previous group, and
the second pair was taken from the consistency set. The remaining pairs were unique
to that particular group and tranche. The design enabled control for possible systematic
differences between annotators and tranches, which could be detected by variation on
the consistency set.
</bodyText>
<subsectionHeader confidence="0.868704">
3.5 Participants
</subsectionHeader>
<bodyText confidence="0.999952375">
Five hundred residents of the United States were recruited from Amazon Mechanical
Turk, each with at least 95% approval rate for work on the Web service. Each participant
was required to check a box confirming that he or she was a native speaker of English
and warned that work would be rejected if the pattern of responses indicated otherwise.
The participants were distributed evenly to rate pairs in one of the ten question tranches,
so that each pair was rated by approximately 50 subjects. Participants took between 8
and 21 minutes to rate the 119 pairs across the 20 groups, together with the checkpoint
questions.
</bodyText>
<subsectionHeader confidence="0.999424">
3.6 Post-Processing
</subsectionHeader>
<bodyText confidence="0.976252321428572">
In order to correct for systematic differences in the overall calibration of the rating scale
between respondents, we measured the average (mean) response of each rater on the
consistency set. For 32 respondents, the absolute difference between this average and
the mean of all such averages was greater than 1 (though never greater than 2); that is,
32 respondents demonstrated a clear tendency to rate pairs as either more or less similar
than the overall rater population. To correct for this bias, we increased (or decreased) the
rating of such respondents for each pair by one, except in cases where they had given
the maximum rating, 6 (or minimum rating, 0). This adjustment, which ensured that the
average response of each participant was within one of the mean of all respondents on
the consistency set, resulted in a small increase to the inter-rater agreement on the data
set as a whole.
After controlling for systematic calibration differences, we imposed three conditions
for the responses of a rater to be included in the final data collation. First, the average
pairwise Spearman correlation of responses with all other responses for a participant
could not be more than one standard deviation below the mean of all such averages.
Second, the increase in inter-rater agreement when a rater was excluded from the
analysis needed to be smaller than at least 50 other raters (i.e., 10% of raters were
excluded on this criterion). Third, we excluded the six participants who got one or more
of the checkpoint questions wrong. A total of 99 participants were excluded based on
one or more of these conditions, but no more than 16 from any one tranche (so that each
pair in the final data set was rated by a minimum of 36 raters). Finally, we computed
average (mean) scores for each pair, and transformed all scores linearly from the interval
[0, 6] to the interval [0,10].
4. Analysis of the Data Set
In this section we analyze the responses of the SimLex-999 annotators and the resulting
ratings. First, by considering inter-annotator agreement, we examine the consistency
with which annotators were able to apply the characterization of similarity, outlined in
the instructions for the range of concept types in SimLex-999. Second, we verify that a
</bodyText>
<page confidence="0.979062">
678
</page>
<figure confidence="0.991372833333333">
Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models
Inter−Annotator Agreement (average pairwise ρ)
0.8
0.7
0.6
0.5
0.673
0.614
0.703
0.792
0.612
0.717
Response Consistency (1 σ)
0.9
0.8
0.7
0.6
0.751 0.752
0.877
0.892
0.781
0.758
SimLex−999
subset
All
Concrete
Abstract
Adjective
Noun
Verb
</figure>
<figureCaption confidence="0.996704">
Figure 4
</figureCaption>
<bodyText confidence="0.9846598">
Left: Inter-annotator agreement, measured by average pairwise Spearman p correlation, for
ratings of concept types in SimLex-999. Right: Response consistency, reflecting the standard
deviation of annotator ratings for each pair, averaged over all pairs in the concept category.
valid notion of similarity was understood by the annotators, in that they were able to
accurately separate similarity from association.
</bodyText>
<subsectionHeader confidence="0.846762">
4.1 Inter-Annotator Agreement
</subsectionHeader>
<bodyText confidence="0.999888857142857">
As in previous annotation or data collection for computational semantics (Pad´o, Pad´o,
and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014) we computed the
inter-rater agreement as the average of pairwise Spearman p correlations between the
ratings of all respondents. Overall agreement was p = 0.67. This compares favorably
with the agreement on WS-353 (p = 0.61 using the same method). The design of the
MEN rating system precludes a conventional calculation of inter-rater agreement (Bruni
et al. 2012b). However, two of the creators of MEN who independently rated the data
set achieved an agreement of p = 0.68.10
The SimLex-999 inter-rater agreement suggests that participants were able to un-
derstand the (single) characterization of similarity presented in the instructions and to
apply it to concepts of various types consistently. This conclusion was supported by
inspection of the brief feedback offered by the majority of annotators in a final text field
in the questionnaire: 78% expressed sentiment that the test was clear, easy to complete,
or some similar sentiment.
Interestingly, as shown in Figure 4 (left), agreement was not uniform across the
concept types. Contrary to what might be expected given established concreteness
effects (Paivio 1991), we observed not only higher inter-rater agreement but also less
per-pair variability for abstract rather than concrete concepts.11
Strikingly, the highest inter-rater consistency and lowest per-pair variation (defined
as the inverse of the standard deviation of all ratings for that pair) was observed on
adjective pairs. Although we are unsure exactly what drives this effect, a possible cause
</bodyText>
<footnote confidence="0.5800206">
10 Reported at http://clic.cimec.unitn.it/~elia.bruni/MEN. Itis reasonable to assume that actual
agreement on MEN may be somewhat lower than 0.68, given the small sample size and the expertise of
the raters.
11 Per-pair variability was measured by calculating the standard deviation of responses for each pair, and
averaging these scores across the pairs of each concept type.
</footnote>
<page confidence="0.991473">
679
</page>
<note confidence="0.342163">
Computational Linguistics Volume 41, Number 4
</note>
<bodyText confidence="0.999862285714286">
is that many pairs of adjectives in SimLex-999 cohabit a single salient, one-dimensional
scale ( freezing &gt; cold &gt; warm &gt; hot). This may be a consequence of the fact that many
pairs in SimLex-999 were selected (from USF) to have a degree of association. On
inspection, pairs of nouns and verbs in SimLex-999 do not appear to occupy scales in
the same way, possibly because concepts of these POS categories come to be associated
via a more diverse range of relations. It seems plausible that humans are able to estimate
the similarity of scale-based concepts more consistently than pairs of concepts related
in a less uni-dimensional fashion.
Regardless of cause, however, the high agreement on adjectives is a satisfactory
property of SimLex-999. Adjectives exhibit various aspects of lexical semantics that have
proved challenging for computational models, including antonymy, polarity (Williams
and Anand 2009), and sentiment (Wiebe 2000). To approach the high level of human
confidence on the adjective pairs in SimLex-999, it may be necessary to focus particu-
larly on developing automatic ways to capture these phenomena.
</bodyText>
<subsectionHeader confidence="0.966171">
4.2 Response Validity: Similarity not Association
</subsectionHeader>
<bodyText confidence="0.999980071428571">
Inspection of the SimLex-999 ratings indicated that pairs were indeed evaluated accord-
ing to similarity rather than association. Table 2 includes examples that demonstrate a
clear dissociation between the two semantic relations.
To verify this effect quantitatively, we recruited 100 additional participants to rate
the WS-353 pairs, but following the SimLex-999 instructions and question format. As
shown in Fig 5(a), there were clear differences between these new ratings and the
original WS-353 ratings. In particular, a high proportion of pairs was given a lower
rating by subjects following the SimLex-999 instructions than those following the
WS-353 guidelines: The mean SimLex rating was 4.07 compared with 5.91 for WS-353.
This was consistent with our expectations that pairs of associated but dissimilar
concepts would receive lower ratings based on the SimLex-999 than on the WS-353
instructions, whereas pairs that were both associated and similar would receive sim-
ilar ratings in both cases. To confirm this, we compared the WS-353 and SimLex-999-
based ratings on the subsets WS-Rel and WS-Sim, which were hand-sorted by Agirre
</bodyText>
<tableCaption confidence="0.9612215">
Table 2
Top: Similarity aligns with association. Pairs with a small difference in rank between USF
(association) and SimLex-999 (similarity) scores for each POS category. Bottom: Similarity
contrasts with association. Pairs with a high difference in rank for each POS category. *Note that
the distribution of USF association scores on the interval [0,10] is highly skewed towards the
lower bound in both SimLex-999 and the USF data set as a whole.
</tableCaption>
<table confidence="0.9991">
C1 C2 POS USF* USF rank (of 999) SimLex SimLex rank (of 999)
dirty narrow A 0.00 999 0.30 996
student pupil N 6.80 12 9.40 12
win dominate V 0.41 364 5.68 361
smart dumb A 2.10 92 0.60 947
attention awareness N 0.10 895 8.73 58
leave enter V 2.16 89 1.38 841
</table>
<page confidence="0.964096">
680
</page>
<figure confidence="0.935864215753425">
Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models
12.5
10.0
Rating (0−10)
7.5
5.0
2.5
0.0
● ● ●
● ● ●
● ● ●
● ● ● ●
● ● ● ●
● ● ●●
● ● ● ● ● ●
● ●
● ● ● ●
● ● ● ●
●
● ● ●
● ● ● ● ●
● ● ● ● ● ● ●
● ● ● ● ●
● ●
● ● ● ●
● ● ● ● ● ● ● ● ●
● ● ● ●
● ● ● ● ● ● ● ● ● ●
● ● ● ● ● ● ●
● ●
● ● ● ●
● ● ● ● ● ● ● ● ● ●
● ●
●
● ● ● ● ● ● ● ●
● ● ● ● ● Spearman correlation = 0.76
● ● ● ●
●
●
●
Survey_Instructions
●
●●
● SimLex−999
● WS−353
●
●
●
(a) WS−353
●
●
●
●
●
●
●●
●
●
●●●
●
● ● ●
●
●
● ●
●●●
●
●
● ●
● ● ● ●
● ●
●
● ● ●
●
●
●
● ●
● ● ●
● ● ● ●
● ●
●
●
● ● ● ● ● ●
● ● ●
● ●
● ●
●
●
●
● ● ●
● ● ●
● ● ● ●
●
● ● ●
●
●
●
●
●
● ● ● ● ● ● ●
● ● ● ●
●
●
●
●
● ●
●●●●
●
●
●
●
● ●
●
●●
●
●● ● ●
●
● ●
● ● ●
●
●
●
● ●
●
●
●
●
●
●
● ● ● ●
●
● ●
● ●
●
●
●
● ●
●
●
●
● ●●
●● ●
●
●
●
●
●
Spearman correlation = 0.73
●
●
●
(b) WS−Sim only
●
●
●●
●
●●
●
●
●
●
●●
● ●
●
●
●
●
●●●
● ● ●
●
●
●
●●
●
●●
●
●
●●●●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
● ●
● ●●
●
● ●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
Spearman correlation = 0.37
●
●
● ● ●
●
(c) WS−Rel on
●
●
●
●
●●
●
●
●
● ●
● ● ●●
●
●
●●
●
●
● ●
● ● ● ●
●
●
●
● ●
●
● ●
● ●
●
● ● ●
●
●
●
●
●
●
● ●
● ● ● ●
●
●
●
● ● ● ● ●
●
●
●
●
●
●●●
● ● ●
●●
●
ly
● ●
● ●
● ● ● ●
●
●
●
●●
●
●
●●●
●
● ●
●
● ●
●
●
●
●
●
●
●
●
●
0 100 200 300 0 25 50 75 100 0 50 100
Rank of WS rating Rank of WS rating Rank of WS rating
</figure>
<figureCaption confidence="0.848959">
Figure 5
</figureCaption>
<bodyText confidence="0.918987">
(a) Pairs rated by WS-353 annotators (blue points, ranked by rating) and the corresponding
rating of annotators following the SimLex-999 instructions (red points). (b-c) The same analysis,
restricted to pairs in the WS-Sim or WS-Rel subsets of WS-353.
et al. (2009) to include pairs connected by association (and not similarity) and those
connected by similarity (but possibly also association), respectively.
As shown in Figure 5(b–c), the correlation between the SimLex-999-based and WS-
353 ratings was notably higher (p = 0.73) on the WS-Sim subset than the WS-Rel subset
(p = 0.38). Specifically, the tendency of subjects following the SimLex-999 instructions
to assign lower ratings than those following the WS-353 instructions was far more
pronounced for pairs in WS-Sim (Figure 5(b)) than for those in WS-Rel (Figure 5(c)).
This observation suggests that the associated but dissimilar pairs in WS-353 were an
important driver of the overall lower mean for SimLex-999-based ratings, and thus
provide strong evidence that the SimLex-999 instructions do indeed enable subjects to
distinguish similarity from association effectively.
</bodyText>
<subsectionHeader confidence="0.998903">
4.3 Finer-Grained Semantic Relations
</subsectionHeader>
<bodyText confidence="0.999931571428571">
We have established the validity of similarity as a notion understood by human raters
and distinct from association. However, much theoretical semantics focuses on relations
between words or concepts that are finer-grained than similarity and association. These
include meronymy (a part to its whole, e.g., blade–knife), hypernymy (a category concept
to a member of that category, e.g., animal–dog), and cohyponymy (two members of the
same implicit category, e.g., the pair of animals dog–cat) (Cruse 1986). Beyond theoretical
interest, these relations can have practical relevance. For instance, hypernymy can form
the basis of semantic entailment and therefore textual inference: The proposition a cat
is on the table entails that an animal is on the table precisely because of the hypernymy
relation from animal to cat.
We chose not to make these finer-grained relations the basis of our evaluation for
several reasons. At present, detecting relations such as hypernymy using distributional
methods is challenging, even when supported by supervised classifiers with access
to labeled pairs (Levy et al. 2015). Such a designation can seem to require specific
</bodyText>
<page confidence="0.980415">
681
</page>
<bodyText confidence="0.959622096774193">
Computational Linguistics Volume 41, Number 4
world-knowledge (is a snale a reptile?), can be gradual, as evidenced by typicality effects
(Rosch, Simpson, and Miller 1976), or simply highly subjective. Moreover, a fine-grained
relation R will only be attested (to any degree) between a small subset of all possible
word pairs, whereas similarity can in theory be quantified for any two words chosen
at random. We thus considered a focus on fine-grained semantic relations to be less
appropriate for a general-purpose evaluation of representation quality.
Nevertheless, post hoc analysis of the SimLex annotator responses and fine-grained
relation classes, as defined by lexicographers, yields further interesting insights into the
nature of both similarity and association. Of the 999 word pairs in SimLex, 382 are also
connected by one of the common finer-grained semantic relations in WordNet. For each
of these relations, Figure 6 shows the average similarity rating and average USF free
association score for all pairs that exhibit that relation.
In cases where a relationship of hypernymy/hyponymy exists between the words
in a pair (not necessarily immediate : 1 hypernym, 2 hypernym, etc.) similarity and associ-
ation coincide. Hyper/hyponym pairs that are separated by fewer levels in the WordNet
hierarchy are both more strongly associated and rated as more similar. However, there
are also interesting discrepancies between similarity and association. Unsurprisingly,
pairs that are classed as synonyms in WordNet (i.e., having at least one sense in
some common synset) are rated as more similar than pairs of any other relation type
by SimLex annotators. In contrast, antonyms are the most strongly associated word
pairs among these finer-grained relations. Further, pairs consisting of a meronym and
holonym (part and whole) are comparatively strongly associated but not judged to be
similar.
The analysis also highlights a case that can be particularly problematic when rating
similarity: cohyponyms, or members of the same salient category (such as knife and
fork). We gave no specific guidelines for how to rate such pairs in the SimLex annotator
instructions, and whether they are considered similar or not seems to be a matter of
perspective. On one hand, their membership of a common category could make them
appear similar, particularly if the category is relatively specific. On the other hand, in
synonym 1_hypernym 2_hypernym 3_hypernym 4_hypernym 5_hypernym cohypernym meronym antonym
</bodyText>
<figureCaption confidence="0.818066428571429">
Figure 6
Average SimLex and USF free association scores across pairs representing different fine-grained
semantic relations. All relations were extracted from WordNet. n hypernym refers to a direct
hypernymy path of length n. Note that the average SimLex rating across all 999 word pairs
(dashed red line) is much higher than the average USF rating (dashed golden line) because of
differences in the rating procedure. The more interesting differences concern the relative
strength of similarity vs. association across the different relation types.
</figureCaption>
<figure confidence="0.973883785714285">
Rating or Score (0-10)
4
0
8 7.70
6
3
7
5
2
1
1.57
6.62
1.06
6.19
0.67
5.73 5.77
0.48
0.19
3.76
0.14
4.94
0.82
3.93
0.89
SimLex-999 (similarity)
USF (free association)
1.72
2.99
</figure>
<page confidence="0.993566">
682
</page>
<note confidence="0.911197">
Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models
</note>
<bodyText confidence="0.9931535">
the case of knife and fork, for instance, the underlying category cultery might provide a
backdrop against which the differences of distinct members become particularly salient.
</bodyText>
<sectionHeader confidence="0.874041" genericHeader="method">
5. Evaluating Models with SimLex-999
</sectionHeader>
<bodyText confidence="0.999967666666667">
In this section, we demonstrate the applicability of SimLex-999 by analyzing the per-
formance of various distributional semantic models in estimating the new ratings. The
models were selected to cover the main classes of representation learning architectures
(Baroni, Dinu, and Kruszewski 2014): Vector space co-occurrence (counting) models
and NLMs (Bengio et al. 2003). We first show that SimLex-999 is notably more difficult
for state-of-the-art models to estimate than existing gold standards. We then conduct
more focused analyses on the various concept subsets defined in SimLex-999, exploring
possible causes for the comparatively low performance of current models and, in turn,
demonstrating how SimLex-999 can be applied to investigate such questions.
</bodyText>
<subsectionHeader confidence="0.94855">
5.1 Semantic Models
</subsectionHeader>
<bodyText confidence="0.9999297">
Collobert &amp; Weston. Collobert and Weston (2008) apply the architecture of an NLM
to learn a word representations vw for each word w in some corpus vocabulary V.
Each sentence s in the input text is represented by a matrix containing the vector
representations of the words in s in order. The model then computes output scores f (s)
and f (sw), where sw denotes an “incorrect” sentence created from s by replacing its last
word with some other word w from V. Training involves updating the parameters of
the function f and the entries of the vector representations vw such that f (s) is larger
than f (sw) for any w in V, other than the correct final word of s. This corresponds to
minimizing the sum of the following sentence objectives Cs over all sentences in the
input corpus, which is achieved via (mini-batch) stochastic gradient descent:
</bodyText>
<equation confidence="0.993131">
�Cs = max(0,1 − f (s) + f (sw))
w∈V
</equation>
<bodyText confidence="0.999971785714286">
The relatively low-dimension, dense (vector) representations learned by this model
and the other NLMs introduced in this section are sometimes referred to as embeddings
(Turian, Ratinov, and Bengio 2010). Collobert and Weston (2008) train their models on
852 million words of text from a 2007 dump of Wikipedia and the RCV1 Corpus (Lewis
et al. 2004) and use their embeddings to achieve state-of-the-art results on a variety of
NLP tasks. We downloaded the embeddings directly from the authors’ Web page.12
Huang et al. Huang et al. (2012) present a NLM that learns word embeddings to
maximize the likelihood of predicting the last word in a sentence s based on (i) the
previous words in that sentence (local context, as with Collobert and Weston [2008]) and
(ii) the document d in which that word occurs (global context). As with Collobert and
Weston (2008), the model represents input sentences as a matrix of word embeddings.
In addition, it represents documents in the input corpus as single-vector averages
over all word embeddings in that document. It can then compute scores g(s, d) and
g(sw, d), whereas before sw is a sentence with an “incorrect” randomly selected last word.
</bodyText>
<footnote confidence="0.547115">
12 http://ml.nec-labs.com/senna/.
</footnote>
<page confidence="0.996141">
683
</page>
<note confidence="0.559337">
Computational Linguistics Volume 41, Number 4
</note>
<bodyText confidence="0.9971645">
Training is again by stochastic gradient descent, and corresponds to minimizing the sum
of the sentence objectives Cs,d over all of the sentences in the corpus:
</bodyText>
<equation confidence="0.996445">
�Cs,d = max(0,1 − g(s, d) + g(sw, d))
wEV
</equation>
<bodyText confidence="0.999888764705882">
The combination of local and global contexts in the objective encourages the final
word embeddings to reflect aspects of both the meaning of nearby words and of the
documents in which those words appear. When learning from 990M words of Wikipedia
text, Huang et al. report a Spearman correlation of p = 71.3 between the cosine similar-
ity of their model embeddings and the WS-353 scores, which constitutes state-of-the-art
performance for a NLM model on that data set. We downloaded these embeddings
from the authors’ Web page.13
Mikolov et al. Mikolov et al. (2013a) present an architecture that learns word em-
beddings similar to those of standard NLMs but with no nonlinear hidden layer (re-
sulting in a simpler scoring function). This enables faster representation learning for
large vocabularies. Despite this simplification, the embeddings achieve state-of-the-
art performance on several semantic tasks including sentence completion and analogy
modeling (Mikolov et al. 2013a, 2013b).
For each word type w in the vocabulary V, the model learns both a “target-
embedding” rw E Rd and a “context-embedding” ˆrw E Rd such that, given a target word,
its ability to predict nearby context words is maximized. The probability of seeing
context word c given target w is defined as:
</bodyText>
<equation confidence="0.88558775">
/ eˆrc&apos;rw
pIC |w) =
Eeˆrv&apos;rw
vEV
</equation>
<bodyText confidence="0.999923181818182">
The model learns from a set of (target-word, context-word) pairs, extracted from
a corpus of sentences as follows. In a given sentence s (of length N), for each position
n &lt; N, each word wn is treated in turn as a target word. An integer t(n) is then sampled
from a uniform distribution on {1,... k}, where k &gt; 0 is a predefined maximum context-
window parameter. The pair tokens {(wn,wn+j) : −t(n) &lt; j &lt; t(n),wi E s} are then ap-
pended to the training data. Thus, target/context training pairs are such that (i) only
words within a k-window of the target are selected as context words for that target, and
(ii) words closer to the target are more likely to be selected than those further away.
The training objective is then to maximize the log probability T, defined here, across
all such examples from s, and then across all sentences in the corpus. This is achieved
by stochastic gradient descent.
</bodyText>
<equation confidence="0.8777">
T = 1 N E log(p(wn+j|wn))
N E −t(n)&lt;j&lt;t(n),j#0
n=1
</equation>
<bodyText confidence="0.991354">
As with other NLMs, Mikolov et al.’s model captures conceptual semantics by
exploiting the fact that words appearing in similar linguistic contexts are likely to
</bodyText>
<footnote confidence="0.9238245">
13 www.socher.org/index.php/Main/ImprovingWordRepresentationsViaGlobalContextAndMultiple
WordPrototypes.
</footnote>
<page confidence="0.993981">
684
</page>
<note confidence="0.942477">
Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models
</note>
<bodyText confidence="0.99984888">
have similar meanings. Informally, the model adjusts its embeddings to increase the
probability of observing the training corpus. Because this probability increases with
p(c|w), and p(c|w) increases with the dot product ˆrc · rw, the updates have the effect of
moving each target-embedding incrementally “closer” to the context-embeddings of its
collocates. In the target-embedding space, this results in embeddings of concept words
that regularly occur in similar contexts moving closer together.
We use the author’s Word2vec software in order to train their model and use the
target embeddings in our evaluations. We experimented with embeddings of dimension
100, 200, 300, 400, and 500 and found that 200 gave the best performance on both WS-353
and SimLex-999.
Vector Space Model (VSM). As an alternative to NLMs, we constructed a vector space
model following the guidelines for optimal performance outlined by Kiela and Clark
(2014). After extracting the 2,000 most frequent word tokens in the corpus that are not
in a common list of stopwords14 as features, we populated a matrix of co-occurrence
counts with a row for each of the concepts in some pair in our evaluation sets, and
a column for each of the features. Co-occurrence was counted within a specified win-
dow size, although never across a sentence boundary. This resulting matrix was then
weighted according to Pointwise Mutual Information (PMI) (Recchia and Jones 2009).
The rows of the resulting matrix constitute the vector representations of the concepts.
SVD. As proposed initially in Landauer and Dumais (1997), we also experimented with
models in which SVD (Golub and Reinsch 1970) is applied to the PMI-weighted VSM
matrix, reducing the dimension of each concept representation to 300 (which yielded
best results after experimenting, as before, with 100–500 dimension vectors).
For each model described in this section, we calculate similarity as the cosine similarity
between the (vector) representations learned by that model.
</bodyText>
<subsectionHeader confidence="0.893515">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999930615384615">
In experimenting with different models on SimLex-999, we aimed to answer the follow-
ing questions: (i) How well do the established models perform on SimLex-999 versus
on existing gold standards? (ii) Are any observed differences caused by the potential
of different models to measure similarity vs. association? (iii) Are there interesting
differences in ability of models to capture similarity between adjectives vs. nouns vs.
verbs? (iv) In this case, are the observed differences driven by concreteness, and its
interaction with POS, or are other factors also relevant?
Overall Performance on SimLex-999. Figure 7 shows the performance of the NLMs on
SimLex-999 versus on comparable data sets, measured by Spearman’s ρ correlation. All
models estimate the ratings of MEN and WS-353 more accurately than SimLex-999. The
Huang et al. (2012) model performs well on WS-353,15 but is not very robust to changes
in evaluation gold standard, and performs worst of all the models on SimLex-999. Given
the focus of the WS-353 ratings, it is tempting to explain this by concluding that the
</bodyText>
<footnote confidence="0.486070666666667">
14 Taken from the Python Natural Language Toolkit (Bird 2006).
15 This score, based on embeddings downloaded from the authors’ webpage, is notably lower than the score
reported in Huang et al. (2012), mentioned in Section 5.1.
</footnote>
<page confidence="0.996124">
685
</page>
<figure confidence="0.987445052631579">
Computational Linguistics Volume 41, Number 4
Correlation ρ
0.75
0.50
0.25
0.00
0.623
0.3
Evaluation Gold Standard
RN353
imLex−999
0.098
0.494
0.575
0.268
0.655
0.699
0.414
Huang et al. Collobert &amp; Weston Mikolov et al.
</figure>
<figureCaption confidence="0.999726">
Figure 7
</figureCaption>
<bodyText confidence="0.999628766666667">
Performance of NLMs on WS-353, MEN, and SimLex-999. All models are trained on Wikipedia;
note that as Wikipedia is constantly growing, the Mikolov et al. (2013a) model exploited slightly
more training data (:t1000M tokens) than the Huang et al. (2012) model (:t990M), which in turn
exploited more than the Collobert and Weston (2008) model (:t852M). Dashed horizontal lines
indicate the level of inter-annotator agreement for the three data sets.
global context objective leads the Huang et al. (2012) model to focus on association
rather than similarity. However, the true explanation may be less simple, since the
Huang et al. (2012) model performs weakly on the association-based MEN data set.
The Collobert and Weston (2008) model is more robust across WS-353 and MEN, but
still does not match the performance of the Mikolov et al. (2013a) model on SimLex-999.
Figure 8 compares the best performing NLM model (Mikolov et al. 2013a) with
the VSM and SVD models.16 In contrast to recent results that emphasize the superior-
ity of NLMs over alternatives (Baroni, Dinu, and Kruszewski 2014), we observed no
clear advantage for the NLM over the VSM or SVD when considering the association-
based gold standards WS-353 and MEN together. While the NLM is the strongest per-
former on WS-353, SVD is the strongest performer on MEN. However, the NLM model
performs notably better than the alternatives at modeling similarity, as measured by
SimLex-999.
Comparing all models in Figures 7 and 8 suggests that SimLex-999 is notably more
challenging to model than the alternative data sets, with correlation scores ranging from
0.098 to 0.414. Thus, even when state-of-the-art models are trained for several days
on massive text corpora,17 their performance on SimLex-999 is well below the inter-
annotator agreement (Figure 7). This suggests that there is ample scope for SimLex-999
to guide the development of improved models.
Modeling Similarity vs. Association. The comparatively low performance of NLM,
VSM, and SVD models on SimLex-999 compared with MEN and WS-353 is consistent
with our hypothesis that modeling similarity is more difficult than modeling associa-
tion. Indeed, given that many strongly associated but dissimilar pairs, such as [coffee,
cup], are likely to have high co-occurrence in the training data, and that all models infer
connections between concepts from linguistic co-occurrence in some form or another,
</bodyText>
<footnote confidence="0.98680875">
16 We conduct this comparison on the smaller RCV1 Corpus (Lewis et al. 2004) because training the VSM
and SVD models is comparatively slow.
17 Training times reported by Huang et al. (2012) and for Collobert and Weston (2008) at
http://ronan.collobert.com/senna/.
</footnote>
<page confidence="0.993244">
686
</page>
<note confidence="0.975634">
Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models
</note>
<figureCaption confidence="0.979441">
Figure 8
</figureCaption>
<bodyText confidence="0.979424117647059">
Comparison between the leading NLM, Mikolov et al., the vector space model, VSM, and the
SVD model. All models were trained on the ≈150m word RCV1 Corpus (Lewis et al. 2004).
it seems plausible that models may overestimate the similarity of such pairs because
they are “distracted” by association.
To test this hypothesis more precisely, we compared the performance of models on
the whole of SimLex-999 versus its 333 most associated pairs (according to the USF free
association scores). Importantly, pairs in this strongly associated subset still span the full
range of possible similarity scores (min similarity = 0.23 [shrink, grow], max similarity =
9.80 [vanish, disappear]).
As shown in Figure 9, all models performed worse when the evaluation was re-
stricted to pairs of strongly associated concepts, which was consistent with our hy-
pothesis. The Collobert and Weston (2008) model was better than the Huang et al.
(2012) model at estimating similarity in the face of high association. This is not en-
tirely surprising given the global-context objective in the latter model, which may have
encouraged more association-based connections between concepts. The Mikolov et al.
model, however, performed notably better than both other NLMs. Moreover, this supe-
riority is proportionally greater when evaluating on the most associated pairs only (as
</bodyText>
<note confidence="0.770766">
Huang et al. Collobert &amp; Weston Mikolov et al. Levy &amp; Goldberg
</note>
<figureCaption confidence="0.756693">
Figure 9
</figureCaption>
<footnote confidence="0.60349425">
The ability of NLMs to model the similarity of highly associated concepts versus concepts in
general. The two models on the right-hand side also demonstrate the effect of training an NLM
(the Mikolov et al. [2013a] model) on running-text (Mikolov et al.) vs. on dependency-based
input (Levy &amp; Goldberg).
</footnote>
<figure confidence="0.9949078125">
Correlation ρ
0.4
0.2
0.0
Evaluation Gold Standard
0.098
SimLex−999 − all
Most associated 333
−0.037
0.27
0.07
0.414
effect of adding dependency information
0.26
0.446
0.347
</figure>
<page confidence="0.655856">
687
</page>
<note confidence="0.434317">
Computational Linguistics Volume 41, Number 4
</note>
<bodyText confidence="0.99958584375">
indicated by the difference between the red and gray bars), suggesting that the improve-
ment is driven at least in part by an increased ability to “distinguish” similarity from
association.
To understand better how the architecture of models captures information pertinent
to similarity modeling, we performed two additional experiments using SimLex-999.
These comparisons were also motivated by the hypotheses, made in previous studies
and outlined in Section 2.1.2, that both dependency-informed input and smaller context
windows encourage models to capture similarity rather than association.
We tested the first hypothesis using the embeddings of Levy and Goldberg (2014),
whose model extends the Mikolov et al. (2013a) model so that target-context training
instances are extracted based on dependency-parsed rather than simple running text.
As illustrated in Figure 9, the dependency-based embeddings outperform the original
(running text) embeddings trained on the same corpus. Moreover, the comparatively
large increase in the red bar compared to the gray bar suggests that an important part
of the improvement of the dependency-based model derives from a greater ability to
discern similarity from association.
Our comparisons provided less support for the second (window size) hypothesis.
As shown in Figure 10, there is a negligible improvement in the performance of the
model when the window size is reduced from 10 to 2. However, for the SVD model we
observed the converse. The SVD model with window size 10 slightly outperforms the
SVD model with window 2, and this improvement is quite pronounced on the most
associated pairs in SimLex-999.
Learning Concepts of Different POS. Given the theoretical likelihood of variation
in model performance across POS categories noted in Section 2.2, we evaluated the
Mikolov et al. (2013a), VSM, and SVD models on the subsets of SimLex-999 containing
adjective, noun, and verb concept pairs.
The analyses yield two notable conclusions, as shown in Figure 11. First, perhaps
contrary to intuition, all models estimate the similarity of adjectives better than other
concept categories. This aligns with the (also unexpected) observation that humans rate
the similarity of adjectives more consistently and with more agreement than other parts
of speech (see the dashed lines). However, the parallels between human raters and the
models do not extend to verbs and nouns; verb similarity is rated more consistently
</bodyText>
<subsectionHeader confidence="0.258415">
Figure 10
</subsectionHeader>
<bodyText confidence="0.677894">
The effect of different window sizes (indicated in square brackets [ ]) on NLM and SVD models.
</bodyText>
<page confidence="0.996908">
688
</page>
<note confidence="0.975362">
Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models
</note>
<figureCaption confidence="0.958487">
Figure 11
</figureCaption>
<bodyText confidence="0.949863235294118">
Performance of models on POS-based subsets of SimLex-999. The window size for each model is
indicated in parentheses. Inter-annotator agreement for each POS is indicated by the dashed
horizontal line.
than noun similarity by humans, but models estimate these ratings more accurately for
nouns than for verbs.
To better understand the linguistic information exploited by models when acquir-
ing concepts of different POS, we also computed performance on the POS subsets
of SimLex-999 of the dependency-based model of Levy and Goldberg (2014) and the
standard skipgram model, in which linguistic contexts are encoded as simple bags-
of-words (BOW) (Mikolov et al. (2013a) [trained on the same Wikipedia text]). As
shown in Figure 12, dependency-aware contexts yield the largest improvements for
capturing verb similarity. This aligns with the cognitive theory of verbs as relational
concepts (Markman and Wisniewski 1997) whose meanings rely on their interaction
with (or dependency on) other words or concepts. It is also consistent with research on
the automatic acquisition of verb semantics, in which syntactic features have proven
particularly important (Sun, Korhonen, and Krymolowski 2008). Although a deeper
exploration of these effects is beyond the scope of this work, this preliminary analysis
</bodyText>
<footnote confidence="0.274893">
adjectives verbs nouns
</footnote>
<figureCaption confidence="0.5540565">
Figure 12
The importance of dependency-focused contexts (in the Levy &amp; Goldberg model) for capturing
concepts of different POS, when compared to a standard Skipgram (BOW) model trained on the
same Wikipedia corpus.
</figureCaption>
<figure confidence="0.979022611111111">
Spearman correlation
0.4
0.6
0.5
0.3
0.2
0.1
0.0
0.50
0.48
0.38
0.27
0.47
0.38
Levy &amp; Goldberg
Mikolov (BOW)
689
Computational Linguistics Volume 41, Number 4
</figure>
<figureCaption confidence="0.988953">
Figure 13
</figureCaption>
<bodyText confidence="0.991383291666667">
Performance of models on concreteness-based subsets of SimLex-999. Window size is indicated
in parentheses. Horizontal dashed lines indicate inter-annotator agreement between SimLex-999
annotators on the two subsets.
again highlights how the word classes integrated into SimLex-999 are pertinent to a
range of questions concerning lexical semantics.
Learning Concrete and Abstract Concepts. Given the strong interdependence between
POS and conceptual concreteness (Figure 1), we aimed to explore whether the variation
in model performance on different POS categories was in fact driven by an underly-
ing effect of concreteness. To do so, we ranked each pair in the SimLex-999 data set
according to the sum of the concreteness of the two words, and compared performance
of models on the most concrete and least concrete quartiles according to this ranking
(Figure 13).
Interestingly, the performance of models on the most abstract and most concrete
pairs suggests that the distinction characterized by concreteness is at least partially inde-
pendent of POS. Specifically, while the Mikolov et al. model was the highest performer
of all POS categories, its performance was worse than both the simple VSM and SVD
models (of window size 10) on the most concrete concept pairs.
This finding supports the growing evidence for systematic differences in represen-
tation and/or similarity operations between abstract and concrete concepts (Hill, Kiela,
and Korhonen 2013), and suggests that at least part of these concreteness effects are
independent of POS. In particular, it appears that models built from underlying vectors
of co-occurrence counts, such as VSMs and SVD, are better equipped to capture the
semantics of concrete entities, whereas the embeddings learned by NLMs can better
capture abstract semantics.
</bodyText>
<sectionHeader confidence="0.998919" genericHeader="conclusions">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.999953833333333">
Although the ultimate test of semantic models should be their utility in downstream ap-
plications, the research community can undoubtedly benefit from ways to evaluate the
general quality of the representations learned by such models, prior to their integration
in any particular system. We have presented SimLex-999, a gold standard resource for
the evaluation of semantic representations containing similarity ratings of word pairs
of different POS categories and concreteness levels.
</bodyText>
<page confidence="0.99511">
690
</page>
<note confidence="0.932248">
Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models
</note>
<bodyText confidence="0.99991394">
The development of SimLex-999 was principally motivated by two factors. First,
as we demonstrated, several existing gold standards measure the ability of models
to capture association rather than similarity, and others do not adequately test their
ability to discriminate similarity from association. This is despite the many potential
applications for accurate similarity-focused representation learning models. Analysis
of the ratings of the 500 SimLex-999 annotators showed that subjects can consistently
quantify similarity, as distinct from association, and apply it to various concept types,
based on minimal intuitive instructions.
Second, as we showed, state-of-the-art models trained solely on running-text cor-
pora have now reached or surpassed the human agreement ceiling on WordSim-353
and MEN, the most popular existing gold standards, as well as on RG and WS-Sim.
These evaluations may therefore have limited use in guiding or moderating future im-
provements to distributional semantic models. Nevertheless, there is clearly still room
for improvement in terms of the use of distributional models in functional applications.
We therefore consider the comparatively low performance of state-of-the-art models on
SimLex-999 to be one of its principal strengths. There is clear room under the inter-rating
ceiling to guide the development of the next generation of distributional models.
We conducted a brief exploration of how models might improve on this perfor-
mance, and verified the hypotheses that models trained on dependency-based input
capture similarity more effectively than those trained on running-text input. The evi-
dence that smaller context windows are also beneficial for similarity models was mixed,
however. Indeed, we showed that the optimal window size depends on both the general
model architecture and the part-of-speech and concreteness of the target concepts.
Our analysis of these hypotheses illustrates how the design of SimLex-999—
covering a principled set of concept categories and including meta-information on
concreteness and free-association strength—enables fine-grained analyses of the per-
formance and parameterization of semantic models. However, these experiments only
scratch the surface in terms of the possible analyses. We hope that researchers will adopt
the resource as a robust means of answering a diverse range of questions pertinent to
similarity modeling, distributional semantics, and representation learning in general.
In particular, for models to learn high-quality representations for all linguistic con-
cepts, we believe that future work must uncover ways to explicitly or implicitly infer
“deeper,” more general, conceptual properties such as intentionality, polarity, subjectiv-
ity, or concreteness (Gershman and Dyer 2014). However, although improving corpus-
based models in this direction is certainly realistic, models that learn exclusively via the
linguistic modality may never reach human-level performance on evaluations such as
SimLex-999. This is because much conceptual knowledge, and particularly that which
underlines similarity computations for concrete concepts, appears to be grounded in the
perceptual modalities as much as in language (Barsalou et al. 2003).
Whatever the means by which the improvements are achieved, accurate concept-
level representation is likely to constitute a necessary first step towards learning infor-
mative, language-neutral phrasal and sentential representations. Such representations
would be hugely valuable for fundamental NLP applications such as language under-
standing tools and machine translation.
Distributional semantics aims to infer the meaning of words based on the company
they keep (Firth 1957). However, although words that occur together in text often have
associated meanings, these meanings may be very similar or indeed very different.
Thus, possibly excepting the population of Argentina, most people would agree that,
strictly speaking, Maradona is not synonymous with football (despite their high rating
of 8.62 in WordSim-353). The challenge for the next generation of distributional models
</bodyText>
<page confidence="0.991879">
691
</page>
<note confidence="0.629412">
Computational Linguistics Volume 41, Number 4
</note>
<bodyText confidence="0.999563333333333">
may therefore be to infer what is useful from the co-occurrence signal and to overlook
what is not. Perhaps only then will models capture most, or even all, of what humans
know when they know how to use a language.
</bodyText>
<sectionHeader confidence="0.998764" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999047700934579">
Agirre, Eneko, Enrique Alfonseca, Keith
Hall, Jana Kravalova, Marius Pas¸ca, and
Aitor Soroa. 2009. A study on similarity
and relatedness using distributional and
Wordnet-based approaches. In Proceedings
of NAACL, Boulder, CO.
Alfonseca, Enrique and Suresh Manandhar.
2002. Extending a lexical ontology by a
combination of distributional semantics
signatures. In G. Schrieber et al.,
Knowledge Engineering and Knowledge
Management: Ontologies and the
Semantic Web. Springer,
pages 1–7.
Andrews, Mark, Gabriella Vigliocco,
and David Vinson. 2009. Integrating
experiential and distributional data
to learn semantic representations.
Psychological Review, 116(3):463.
Bansal, Mohit, Kevin Gimpel, and Karen
Livescu. 2014. Tailoring continuous word
representations for dependency parsing. In
Proceedings of ACL, Baltimore, MD.
Baroni, Marco, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! A
systematic comparison of context-counting
vs. context-predicting semantic vectors. In
Proceedings of ACL, Baltimore, MD.
Baroni, Marco and Alessandro Lenci. 2010.
Distributional memory: A general
framework for corpus-based semantics.
Computational Linguistics, 36(4):673–721.
Barsalou, Lawrence W., W. Kyle Simmons,
Aron K. Barbey, and Christine D. Wilson.
2003. Grounding conceptual knowledge in
modality-specific systems. Trends in
Cognitive Sciences, 7(2):84–91.
Beltagy, Islam, Katrin Erk, and Raymond
Mooney. 2014. Semantic parsing using
distributional semantics and probabilistic
logic. In ACL 2014 Workshop on Semantic
Parsing.
Bengio, Yoshua, R´ejean Ducharme, Pascal
Vincent, and Christian Jauvin. 2003. A
neural probabilistic language model. The
Journal of Machine Learning Research,
3:1137–1155.
Bernardi, Raffaella, Georgiana Dinu,
Marco Marelli, and Marco Baroni. 2013.
A relatedness benchmark to test the
role of determiners in compositional
distributional semantics. In Proceedings of
ACL, Sofia.
Biemann, Chris. 2005. Ontology learning
from text: A survey of methods. LDV
Forum, 20(2):75–93.
Bird, Steven. 2006. Nltk: the natural language
toolkit. In Proceedings of the COLING/ACL
on Interactive Presentation sessions,
pages 69–72, Sydney.
Bruni, Elia, Gemma Boleda, Marco
Baroni, and Nam-Khanh Tran. 2012a.
Distributional semantics in technicolor.
In Proceedings of ACL, Jeju Island.
Bruni, Elia, Jasper Uijlings, Marco Baroni,
and Nicu Sebe. 2012b. Distributional
semantics with eyes: Using image analysis
to improve computational representations
of word meaning. In Proceedings of the 20th
ACM International Conference on Multimedia,
Nara.
Budanitsky, Alexander and Graeme
Hirst. 2006. Evaluating Wordnet-based
measures of lexical semantic relatedness.
Computational Linguistics, 32(1):13–47.
Cimiano, Philipp, Andreas Hotho, and
Steffen Staab. 2005. Learning concept
hierarchies from text corpora using
formal concept analysis. J. Artif. Intell.
Res. (JAIR), 24:305–339.
Collobert, R. and J. Weston. 2008. A unified
architecture for natural language process-
ing: Deep neural networks with multitask
learning. In International Conference
on Machine Learning, ICML, Helsinki.
Cruse, D. Alan. 1986. Lexical semantics.
Cambridge University Press.
Cunningham, Hamish. 2005. Information
extraction, automatic. Encyclopedia
of language and linguistics, pages 665–677.
Fellbaum, Christiane 1998. WordNet.
Wiley Online Library.
Finkelstein, Lev, Evgeniy Gabrilovich,
Yossi Matias, Ehud Rivlin, Zach Solan,
Gadi Wolfman, and Eytan Ruppin.
2001. Placing search in context: The
concept revisited. In Proceedings of the
10th International Conference on World
Wide Web, pages 406–414, Hong Kong.
Firth, J. R. 1957. Papers in Linguistics
1934–1951. Oxford University Press.
Gentner, Dedre. 1978. On relational
meaning: The acquisition of verb meaning.
Child Development, pages 988–998.
Gentner, Dedre. 2006. Why verbs are hard
to learn. Action meets word: How Children
Learn Verbs, pages 544–564.
</reference>
<page confidence="0.99452">
692
</page>
<note confidence="0.922714">
Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models
</note>
<reference confidence="0.99918686440678">
Gershman, Anatole, Yulia Tsvetkov, Leonid
Boytsov, Eric Nyberg, and Chris Dyer. 2014.
Metaphor detection with cross-lingual
model transfer. In Proceedings of ACL,
Baltimore, MD.
Golub, Gene H. and Christian Reinsch.
1970. Singular value decomposition
and least squares solutions. Numerische
Mathematik, 14(5):403–420.
Griffiths, Thomas L., Mark Steyvers, and
Joshua B. Tenenbaum. 2007. Topics
in semantic representation. Psychological
Review, 114(2):211.
Haghighi, Aria, Percy Liang, Taylor
Berg-Kirkpatrick, and Dan Klein.
2008. Learning bilingual lexicons from
monolingual corpora. In Proceedings of
ACL 2008, Columbus, OH.
Hassan, Samer and Rada Mihalcea. 2011.
Semantic relatedness using salient
semantic analysis. In AAAI,
San Francisco, CA.
Hatzivassiloglou, Vasileios, Judith L.
Klavans, Melissa L. Holcombe, Regina
Barzilay, Min-Yen Kan, and Kathleen
McKeown. 2001. Simfinder: A flexible
clustering tool for summarization. In
NAACL Workshop on Automatic
Summarization, Pittsburgh,
PA.
He, Xiaodong, Mei Yang, Jianfeng
Gao, Patrick Nguyen, and Robert Moore.
2008. Indirect-HMM-based hypothesis
alignment for combining outputs from
machine translation systems. In Proceedings
of EMNLP, pages 98–107, Edinburgh.
Hill, Felix, Douwe Kiela, and Anna
Korhonen. 2013. Concreteness and
corpora: A theoretical and practical
analysis. CMCL 2013, page 75, Sofia.
Hill, Felix, Anna Korhonen, and Christian
Bentz. 2014. A quantitative empirical
analysis of the abstract/concrete
distinction. Cognitive Science, 38(1):162–177.
Hill, Felix, Roi Reichart, and Anna Korhonen.
2014. Multi-modal models for concrete
and abstract concept meaning. Transactions
of the Association for Computational
Linguistics (TACL), 2:285–296.
Huang, Eric H., Richard Socher, Christopher
D. Manning, and Andrew Y. Ng. 2012.
Improving word representations via global
context and multiple word prototypes.
In Proceedings of ACL, pages 873–882,
Jeju Island.
Kiela, Douwe and Stephen Clark. 2014.
A systematic study of semantic vector
space model parameters. In Proceedings
of the 2nd Workshop on Continuous Vector
Space Models and their Compositionality
(CVSC)@ EACL, pages 21–30, Gothenburg.
Kiela, Douwe, Felix Hill, Anna Korhonen,
and Stephen Clark. 2014. Improving
multi-modal representations using image
dispersion: Why less is sometimes more.
In Proceedings of ACL, Baltimore, MD.
Landauer, Thomas K. and Susan T. Dumais.
1997. A solution to Plato’s problem:
The latent semantic analysis theory of
acquisition, induction, and representation
of knowledge. Psychological Review,
104(2):211.
Leech, Geoffrey, Roger Garside, and Michael
Bryant. 1994. Claws4: The tagging of
the British National Corpus. In Proceedings
of COLING, pages 622–628, Kyoto.
Levy, Omer and Yoav Goldberg. 2014.
Dependency-based word embeddings.
In Proceedings of ACL, volume 2.
Levy, Omer, Steffen Remus, Chris Biemann,
and Idol Dagan. 2015. Do supervised
distributional methods really learn
lexical inference relations? Proceedings
of NAACL, Denver, CO.
Lewis, David D., Yiming Yang, Tony G. Rose,
and Fan Li. 2004. Rcv1: A new benchmark
collection for text categorization research.
The Journal of Machine Learning Research,
5:361–397.
Li, Changliang, Bo Xu, Gaowei Wu, Xiuying
Wang, Wendong Ge, and Yan Li. 2014.
Obtaining better word representations via
language transfer. In A. Gelbukh, editor,
Computational Linguistics and Intelligent
Text Processing. Springer, pages 128–137.
Li, Mu, Yang Zhang, Muhua Zhu, and
Ming Zhou. 2006. Exploring distributional
similarity based models for query
spelling correction. In Proceedings of ALC,
pages 1025–1032.
Luong, Minh-Thang, Richard Socher,
and Christopher D. Manning. 2013. Better
word representations with recursive neural
networks for morphology. CoNLL-2013,
page 104, Sofia.
Markman, Arthur B. and Edward J.
Wisniewski. 1997. Similar and different:
The differentiation of basic-level categories.
Journal of Experimental Psychology:
Learning, Memory, and Cognition, 23(1):54.
Marton, Yuval, Chris Callison-Burch, and
Philip Resnik. 2009. Improved statistical
machine translation using monolingually-
derived paraphrases. In Proceedings
of EMNLP, pages 381–390, Edinburgh.
McRae, Ken, Saman Khalkhali, and
Mary Hare. 2012. Semantic and associative
relations in adolescents and young
</reference>
<page confidence="0.974139">
693
</page>
<reference confidence="0.994488210084034">
Computational Linguistics Volume 41, Number 4
adults: Examining a tenuous dichotomy.
In Valerie F. Reyna, Sandra B. Chapman,
Michael R. Dougherty, and Jere Ed
Confrey, editors, The Adolescent Brain:
Learning, Reasoning, and Decision Making.
American Psychological Association,
pages 39–66.
Medelyan, Olena, David Milne, Catherine
Legg, and Ian H. Witten. 2009. Mining
meaning from Wikipedia. International
Journal of Human-Computer Studies,
67(9):716–754.
Mikolov, Tomas, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efficient
estimation of word representations in
vector space. In Proceedings of International
Conference of Learning Representations,
Scottsdale, AZ.
Mikolov, Tomas, Ilya Sutskever, Kai Chen,
Greg S. Corrado, and Jeff Dean. 2013b.
Distributed representations of words
and phrases and their compositionality.
In Advances in Neural Information
Processing Systems, pages 3111–3119,
Lake Tahoe, NV.
Navigli, Roberto. 2009. Word sense
disambiguation: A survey. ACM
Computing Surveys (CSUR), 41(2):10.
Nelson, Douglas L., Cathy L. McEvoy, and
Thomas A. Schreiber. 2004. The University
of South Florida free association, rhyme,
and word fragment norms. Behavior
Research Methods, Instruments, &amp; Computers,
36(3):402–407.
Pad´o, Sebastian, Ulrike Pad´o, and Katrin Erk.
2007. Flexible, corpus-based modelling
of human plausibility judgements. In
Proceedings of EMNLP-CoNLL,
pages 400–409, Prague.
Paivio, Allan. 1991. Dual coding theory:
Retrospect and current status. Canadian
Journal of Psychology/Revue canadienne
de psychologie, 45(3):255.
Pedersen, Ted, Siddharth Patwardhan,
and Jason Michelizzi. 2004. Wordnet::
Similarity: Measuring the relatedness
of concepts. In Demonstration Papers at
HLT-NAACL 2004, pages 38–41, New York,
NY.
Phan, Xuan-Hieu, Le-Minh Nguyen,
and Susumu Horiguchi. 2008. Learning
to classify short and sparse text &amp; Web
with hidden topics from large-scale data
collections. In Proceedings of the 17th
International Conference on World Wide
Web, pages 91–100, Beijing.
Plaut, David C. 1995. Semantic and
associative priming in a distributed
attractor network. In Proceedings
of CogSci, volume 17, pages 37–42,
Pittsburgh, PA.
Recchia, Gabriel and Michael N. Jones. 2009.
More data trumps smarter algorithms:
Comparing pointwise mutual information
with latent semantic analysis. Behavior
Research Methods, 41(3):647–656.
Reisinger, Joseph and Raymond Mooney.
2010a. A mixture model with sharing for
lexical semantics. In Proceedings of EMNLP,
pages 1173–1182, Cambridge, MA.
Reisinger, Joseph and Raymond J. Mooney.
2010b. Multi-prototype vector-space
models of word meaning. In Human
Language Technologies: The 2010 Annual
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 109–117, Los Angeles, CA.
Resnik, Philip. 1995. Using information
content to evaluate semantic similarity
in a taxonomy. In Proceedings of IJCAI.
Resnik, Philip and Jimmy Lin. 2010. 11
evaluations of NLP systems. The handbook of
computational linguistics and natural language
processing, 57:271.
Rosch, Eleanor, Carol Simpson, and R. Scott
Miller. 1976. Structural bases of typicality
effects. Journal of Experimental Psychology:
Human Perception and Performance,
2(4):491.
Rose, Tony, Mark Stevenson,
and Miles Whitehead. 2002. The Reuters
corpus volume 1—from yesterday’s
news to tomorrow’s language resources.
In LREC, volume 2, pages 827–832,
Las Palmas.
Rubenstein, Herbert and John B.
Goodenough. 1965. Contextual
correlates of synonymy. Communications
of the ACM, 8(10):627–633.
Silberer, Carina and Mirella Lapata.
2014. Learning grounded meaning
representations with autoencoders. In
Proceedings of ACL, Sofia.
Sun, Lin, Anna Korhonen, and Yuval
Krymolowski. 2008. Verb class discovery
from rich syntactic data. In A. Gelbukh,
editor, Computational Linguistics and
Intelligent Text processing. Springer,
pages 16–27.
Turian, Joseph, Lev Ratinov, and
Yoshua Bengio. 2010. Word
representations: A simple and general
method for semi-supervised learning. In
Proceedings of ACL, pages 384–394,
Uppsala.
Turney, Peter D. 2012. Domain and
function: A dual-space model of semantic
relations and compositions. Journal
</reference>
<page confidence="0.979037">
694
</page>
<note confidence="0.675532">
Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models
</note>
<reference confidence="0.999189923076923">
of Artificial Intelligence Research (JAIR),
179(44):533–585.
Turney, Peter D. and Patrick Pantel. 2010.
From frequency to meaning: Vector space
models of semantics. Journal of Artificial
Intelligence Research, 37(1):141–188.
Tversky, Amos. 1977. Features of similarity.
Psychological Review, 84(4):327.
Wiebe, Janyce. 2000. Learning subjective
adjectives from corpora. In AAAI/IAAI,
pages 735–740, Austin, TX.
Williams, Gbolahan K. and Sarabjot Singh
Anand. 2009. Predicting the polarity
strength of adjectives using Wordnet.
In ICWSM, San Jose, CA.
Wu, Zhibiao and Martha Palmer. 1994.
Verbs, semantics and lexical selection.
In Proceedings of ACL, pages 133–138,
Las Cruces, NM.
Yong, Chung and Shou King Foo. 1999.
A case study on inter-annotator
agreement for word sense
disambiguation. In Proceedings of the
ACL SIGLEX Workshop on Standardizing
Lexical Resources (SIGLEX99), College
Park, MD.
</reference>
<page confidence="0.998765">
695
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.705708">
<title confidence="0.966467">SimLex-999: Evaluating Semantic Models</title>
<author confidence="0.723581">With Similarity Estimation</author>
<affiliation confidence="0.996927333333333">University of Cambridge Technion, Israel Institute of Technology University of Cambridge</affiliation>
<abstract confidence="0.999303133333333">We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold such as WordSim-353 and MEN, it explicitly quantifies than that pairs of entities that are associated but not actually similar have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider, range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun, and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-ofthe-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jana Kravalova Hall</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and Wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Boulder, CO.</location>
<marker>Hall, Pas¸ca, Soroa, 2009</marker>
<rawString>Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and Wordnet-based approaches. In Proceedings of NAACL, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Alfonseca</author>
<author>Suresh Manandhar</author>
</authors>
<title>Extending a lexical ontology by a combination of distributional semantics signatures.</title>
<date>2002</date>
<booktitle>In G. Schrieber et al., Knowledge Engineering and Knowledge Management: Ontologies and the Semantic Web.</booktitle>
<pages>1--7</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="25331" citStr="Alfonseca and Manandhar 2002" startWordPosition="3797" endWordPosition="3800">ormance of an annotator on WS-353 is p = 0.756, which is still only marginally better than the best automatic method.6 Thus, at least according to the established wisdom in NLP evaluation (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010), the strength of the conclusions that can be inferred from improvements on WS-353 is limited. At the same time, however, state-of-the-art distributional models are clearly not perfect representation-learning or even similarity estimation engines, as evidenced by the fact they cannot yet be applied, for instance, to generate flawless lexical resources (Alfonseca and Manandhar 2002). WS-Sim. WS-Sim is the set of pairs in WS-353 identified by Agirre et al. (2009) as either containing similar or unrelated (neither similar nor associated) concepts. The ratings in WS-Sim are mapped directly from WS-353, so that all concept pairs in WSSim that receive a high rating are associated and all pairs that receive a low rating are unassociated. Consequently, any model that simply reflects association would score highly on WS-Sim, irrespective of how well it captures similarity. Such a possibility could be excluded by requiring models to perform well on WSSim and poorly on WS-Rel, the</context>
</contexts>
<marker>Alfonseca, Manandhar, 2002</marker>
<rawString>Alfonseca, Enrique and Suresh Manandhar. 2002. Extending a lexical ontology by a combination of distributional semantics signatures. In G. Schrieber et al., Knowledge Engineering and Knowledge Management: Ontologies and the Semantic Web. Springer, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Andrews</author>
<author>Gabriella Vigliocco</author>
<author>David Vinson</author>
</authors>
<title>Integrating experiential and distributional data to learn semantic representations.</title>
<date>2009</date>
<journal>Psychological Review,</journal>
<volume>116</volume>
<issue>3</issue>
<marker>Andrews, Vigliocco, Vinson, 2009</marker>
<rawString>Andrews, Mark, Gabriella Vigliocco, and David Vinson. 2009. Integrating experiential and distributional data to learn semantic representations. Psychological Review, 116(3):463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Baltimore, MD.</location>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Bansal, Mohit, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of ACL, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Baltimore, MD.</location>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Baroni, Marco, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of ACL, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="4490" citStr="Baroni and Lenci 2010" startWordPosition="627" endWordPosition="630">hine translation systems, which aim to define mappings between fragments of different languages whose meaning is similar, but not necessarily associated, are another established application (He et al. 2008; Marton, Callison-Burch, and Resnik 2009). Moreover, since, as we establish, similarity is a cognitively complex operation that can require rich, structured conceptual knowledge to compute accurately, similarity estimation constitutes an effective proxy evaluation for general-purpose representation-learning models whose ultimate application is variable or unknown (Collobert and Weston 2008; Baroni and Lenci 2010). As we show in Section 2, the predominant gold standards for semantic evaluation in NLP do not measure the ability of models to reflect similarity. In particular, in both WS353 and MEN, pairs of words with associated meaning, such as coffee and cup (rating = 6.810), telephone and communication (7.510), or movie and theater (7.710), receive a high rating regardless of whether or not their constituents are similar. Thus, the utility of such resources to the development and application of similarity models is limited, a problem exacerbated by the fact that many researchers appear unaware of what</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Baroni, Marco and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence W Barsalou</author>
<author>W Kyle Simmons</author>
<author>Aron K Barbey</author>
<author>Christine D Wilson</author>
</authors>
<title>Grounding conceptual knowledge in modality-specific systems.</title>
<date>2003</date>
<journal>Trends in Cognitive Sciences,</journal>
<volume>7</volume>
<issue>2</issue>
<contexts>
<context position="81702" citStr="Barsalou et al. 2003" startWordPosition="12907" endWordPosition="12910">ys to explicitly or implicitly infer “deeper,” more general, conceptual properties such as intentionality, polarity, subjectivity, or concreteness (Gershman and Dyer 2014). However, although improving corpusbased models in this direction is certainly realistic, models that learn exclusively via the linguistic modality may never reach human-level performance on evaluations such as SimLex-999. This is because much conceptual knowledge, and particularly that which underlines similarity computations for concrete concepts, appears to be grounded in the perceptual modalities as much as in language (Barsalou et al. 2003). Whatever the means by which the improvements are achieved, accurate conceptlevel representation is likely to constitute a necessary first step towards learning informative, language-neutral phrasal and sentential representations. Such representations would be hugely valuable for fundamental NLP applications such as language understanding tools and machine translation. Distributional semantics aims to infer the meaning of words based on the company they keep (Firth 1957). However, although words that occur together in text often have associated meanings, these meanings may be very similar or </context>
</contexts>
<marker>Barsalou, Simmons, Barbey, Wilson, 2003</marker>
<rawString>Barsalou, Lawrence W., W. Kyle Simmons, Aron K. Barbey, and Christine D. Wilson. 2003. Grounding conceptual knowledge in modality-specific systems. Trends in Cognitive Sciences, 7(2):84–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Semantic parsing using distributional semantics and probabilistic logic.</title>
<date>2014</date>
<booktitle>In ACL 2014 Workshop on Semantic Parsing.</booktitle>
<marker>Beltagy, Erk, Mooney, 2014</marker>
<rawString>Beltagy, Islam, Katrin Erk, and Raymond Mooney. 2014. Semantic parsing using distributional semantics and probabilistic logic. In ACL 2014 Workshop on Semantic Parsing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="58107" citStr="Bengio et al. 2003" startWordPosition="9252" endWordPosition="9255">ex-999: Evaluating Semantic Models the case of knife and fork, for instance, the underlying category cultery might provide a backdrop against which the differences of distinct members become particularly salient. 5. Evaluating Models with SimLex-999 In this section, we demonstrate the applicability of SimLex-999 by analyzing the performance of various distributional semantic models in estimating the new ratings. The models were selected to cover the main classes of representation learning architectures (Baroni, Dinu, and Kruszewski 2014): Vector space co-occurrence (counting) models and NLMs (Bengio et al. 2003). We first show that SimLex-999 is notably more difficult for state-of-the-art models to estimate than existing gold standards. We then conduct more focused analyses on the various concept subsets defined in SimLex-999, exploring possible causes for the comparatively low performance of current models and, in turn, demonstrating how SimLex-999 can be applied to investigate such questions. 5.1 Semantic Models Collobert &amp; Weston. Collobert and Weston (2008) apply the architecture of an NLM to learn a word representations vw for each word w in some corpus vocabulary V. Each sentence s in the input</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Bengio, Yoshua, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raffaella Bernardi</author>
<author>Georgiana Dinu</author>
<author>Marco Marelli</author>
<author>Marco Baroni</author>
</authors>
<title>A relatedness benchmark to test the role of determiners in compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Sofia.</location>
<contexts>
<context position="28360" citStr="Bernardi et al. 2013" startWordPosition="4280" endWordPosition="4283">2 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models it is clear that the low similarity pairs in RG are not associated. A further limitation is that distributional models now achieve better performance on RG (correlations of up to Pearson r = 0.86 [Hassan and Mihalcea 2011]) than the reported inter-annotator agreement of r = 0.85 (Rubenstein and Goodenough 1965). Finally, the size of RG renders it an even less comprehensive evaluation than WS-Sim. The MEN Test Collection. A larger data set, MEN (Bruni et al. 2012a), is used in a handful of recent studies (Bruni et al. 2012b; Bernardi et al. 2013). As with WS-353, both terms similarity and relatedness are used by the authors when describing MEN, although the annotators were expressly asked to rate pairs according to relatedness.7 The construction of MEN differed from RG and WS-353 in that each pair was only considered by one rater, who ranked it for relatedness relative to 50 other pairs in the data set. An overall score out of 50 was then attributed to each pair corresponding to how many times it was ranked as more related than an alternative. However, because these rankings are based on relatedness, with respect to evaluating similar</context>
</contexts>
<marker>Bernardi, Dinu, Marelli, Baroni, 2013</marker>
<rawString>Bernardi, Raffaella, Georgiana Dinu, Marco Marelli, and Marco Baroni. 2013. A relatedness benchmark to test the role of determiners in compositional distributional semantics. In Proceedings of ACL, Sofia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Ontology learning from text: A survey of methods.</title>
<date>2005</date>
<journal>LDV Forum,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="3814" citStr="Biemann 2005" startWordPosition="537" endWordPosition="538">ames including relatedness (Budanitsky and Hirst 2006; Agirre et al. 2009), topical similarity (Hatzivassiloglou et al. 2001), and domain similarity (Turney 2012). Association contrasts with similarity, the relation connecting cup and mug (Tversky 1977). At its strongest, the similarity relation is exemplified by pairs of synonyms; words with identical referents. Computational models that effectively capture similarity as distinct from association have numerous applications. Such models are used for the automatic generation of dictionaries, thesauri, ontologies, and language correction tools (Biemann 2005; Cimiano, Hotho, and Staab 2005; Li et al. 2006). Machine translation systems, which aim to define mappings between fragments of different languages whose meaning is similar, but not necessarily associated, are another established application (He et al. 2008; Marton, Callison-Burch, and Resnik 2009). Moreover, since, as we establish, similarity is a cognitively complex operation that can require rich, structured conceptual knowledge to compute accurately, similarity estimation constitutes an effective proxy evaluation for general-purpose representation-learning models whose ultimate applicati</context>
</contexts>
<marker>Biemann, 2005</marker>
<rawString>Biemann, Chris. 2005. Ontology learning from text: A survey of methods. LDV Forum, 20(2):75–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
</authors>
<title>Nltk: the natural language toolkit.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Interactive Presentation sessions,</booktitle>
<pages>69--72</pages>
<location>Sydney.</location>
<contexts>
<context position="66624" citStr="Bird 2006" startWordPosition="10627" endWordPosition="10628">S, or are other factors also relevant? Overall Performance on SimLex-999. Figure 7 shows the performance of the NLMs on SimLex-999 versus on comparable data sets, measured by Spearman’s ρ correlation. All models estimate the ratings of MEN and WS-353 more accurately than SimLex-999. The Huang et al. (2012) model performs well on WS-353,15 but is not very robust to changes in evaluation gold standard, and performs worst of all the models on SimLex-999. Given the focus of the WS-353 ratings, it is tempting to explain this by concluding that the 14 Taken from the Python Natural Language Toolkit (Bird 2006). 15 This score, based on embeddings downloaded from the authors’ webpage, is notably lower than the score reported in Huang et al. (2012), mentioned in Section 5.1. 685 Computational Linguistics Volume 41, Number 4 Correlation ρ 0.75 0.50 0.25 0.00 0.623 0.3 Evaluation Gold Standard RN353 imLex−999 0.098 0.494 0.575 0.268 0.655 0.699 0.414 Huang et al. Collobert &amp; Weston Mikolov et al. Figure 7 Performance of NLMs on WS-353, MEN, and SimLex-999. All models are trained on Wikipedia; note that as Wikipedia is constantly growing, the Mikolov et al. (2013a) model exploited slightly more training </context>
</contexts>
<marker>Bird, 2006</marker>
<rawString>Bird, Steven. 2006. Nltk: the natural language toolkit. In Proceedings of the COLING/ACL on Interactive Presentation sessions, pages 69–72, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>Nam-Khanh Tran</author>
</authors>
<title>Distributional semantics in technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL, Jeju Island.</booktitle>
<contexts>
<context position="2856" citStr="Bruni et al. 2012" startWordPosition="401" endWordPosition="404">rhonen}® cl.cam.ac.uk. ** Technion, Israel Institute of Technology, Haifa, Israel. E-mail: roiri®ie.technion.ac.il. Submission received: 25 July 2014; revised submission received: 10 June 2015; accepted for publication: 31 August 2015. doi:10.1162/COLI a 00237 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 4 cup are rated as more “similar” than pairs such as car and train, which share numerous common properties (function, material, dynamic behavior, wheels, windows, etc.). Such anomalies also exist in other gold standards such as the MEN data set (Bruni et al. 2012a). As a consequence, these evaluations effectively penalize models for learning the evident truth that coffee and cup are dissimilar. Although clearly different, coffee and cup are very much related. The psychological literature refers to the conceptual relationship between these concepts as association, although it has been given a range of names including relatedness (Budanitsky and Hirst 2006; Agirre et al. 2009), topical similarity (Hatzivassiloglou et al. 2001), and domain similarity (Turney 2012). Association contrasts with similarity, the relation connecting cup and mug (Tversky 1977).</context>
<context position="28275" citStr="Bruni et al. 2012" startWordPosition="4264" endWordPosition="4267">3 were downloaded from www.cs.technion.ac.il/~gabr/ resources/data/wordsim353. 672 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models it is clear that the low similarity pairs in RG are not associated. A further limitation is that distributional models now achieve better performance on RG (correlations of up to Pearson r = 0.86 [Hassan and Mihalcea 2011]) than the reported inter-annotator agreement of r = 0.85 (Rubenstein and Goodenough 1965). Finally, the size of RG renders it an even less comprehensive evaluation than WS-Sim. The MEN Test Collection. A larger data set, MEN (Bruni et al. 2012a), is used in a handful of recent studies (Bruni et al. 2012b; Bernardi et al. 2013). As with WS-353, both terms similarity and relatedness are used by the authors when describing MEN, although the annotators were expressly asked to rate pairs according to relatedness.7 The construction of MEN differed from RG and WS-353 in that each pair was only considered by one rater, who ranked it for relatedness relative to 50 other pairs in the data set. An overall score out of 50 was then attributed to each pair corresponding to how many times it was ranked as more related than an alternative. However</context>
<context position="45910" citStr="Bruni et al. 2012" startWordPosition="7051" endWordPosition="7054">they were able to accurately separate similarity from association. 4.1 Inter-Annotator Agreement As in previous annotation or data collection for computational semantics (Pad´o, Pad´o, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014) we computed the inter-rater agreement as the average of pairwise Spearman p correlations between the ratings of all respondents. Overall agreement was p = 0.67. This compares favorably with the agreement on WS-353 (p = 0.61 using the same method). The design of the MEN rating system precludes a conventional calculation of inter-rater agreement (Bruni et al. 2012b). However, two of the creators of MEN who independently rated the data set achieved an agreement of p = 0.68.10 The SimLex-999 inter-rater agreement suggests that participants were able to understand the (single) characterization of similarity presented in the instructions and to apply it to concepts of various types consistently. This conclusion was supported by inspection of the brief feedback offered by the majority of annotators in a final text field in the questionnaire: 78% expressed sentiment that the test was clear, easy to complete, or some similar sentiment. Interestingly, as shown</context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Bruni, Elia, Gemma Boleda, Marco Baroni, and Nam-Khanh Tran. 2012a. Distributional semantics in technicolor. In Proceedings of ACL, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Jasper Uijlings</author>
<author>Marco Baroni</author>
<author>Nicu Sebe</author>
</authors>
<title>Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning.</title>
<date>2012</date>
<booktitle>In Proceedings of the 20th ACM International Conference on Multimedia,</booktitle>
<location>Nara.</location>
<contexts>
<context position="2856" citStr="Bruni et al. 2012" startWordPosition="401" endWordPosition="404">rhonen}® cl.cam.ac.uk. ** Technion, Israel Institute of Technology, Haifa, Israel. E-mail: roiri®ie.technion.ac.il. Submission received: 25 July 2014; revised submission received: 10 June 2015; accepted for publication: 31 August 2015. doi:10.1162/COLI a 00237 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 4 cup are rated as more “similar” than pairs such as car and train, which share numerous common properties (function, material, dynamic behavior, wheels, windows, etc.). Such anomalies also exist in other gold standards such as the MEN data set (Bruni et al. 2012a). As a consequence, these evaluations effectively penalize models for learning the evident truth that coffee and cup are dissimilar. Although clearly different, coffee and cup are very much related. The psychological literature refers to the conceptual relationship between these concepts as association, although it has been given a range of names including relatedness (Budanitsky and Hirst 2006; Agirre et al. 2009), topical similarity (Hatzivassiloglou et al. 2001), and domain similarity (Turney 2012). Association contrasts with similarity, the relation connecting cup and mug (Tversky 1977).</context>
<context position="28275" citStr="Bruni et al. 2012" startWordPosition="4264" endWordPosition="4267">3 were downloaded from www.cs.technion.ac.il/~gabr/ resources/data/wordsim353. 672 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models it is clear that the low similarity pairs in RG are not associated. A further limitation is that distributional models now achieve better performance on RG (correlations of up to Pearson r = 0.86 [Hassan and Mihalcea 2011]) than the reported inter-annotator agreement of r = 0.85 (Rubenstein and Goodenough 1965). Finally, the size of RG renders it an even less comprehensive evaluation than WS-Sim. The MEN Test Collection. A larger data set, MEN (Bruni et al. 2012a), is used in a handful of recent studies (Bruni et al. 2012b; Bernardi et al. 2013). As with WS-353, both terms similarity and relatedness are used by the authors when describing MEN, although the annotators were expressly asked to rate pairs according to relatedness.7 The construction of MEN differed from RG and WS-353 in that each pair was only considered by one rater, who ranked it for relatedness relative to 50 other pairs in the data set. An overall score out of 50 was then attributed to each pair corresponding to how many times it was ranked as more related than an alternative. However</context>
<context position="45910" citStr="Bruni et al. 2012" startWordPosition="7051" endWordPosition="7054">they were able to accurately separate similarity from association. 4.1 Inter-Annotator Agreement As in previous annotation or data collection for computational semantics (Pad´o, Pad´o, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014) we computed the inter-rater agreement as the average of pairwise Spearman p correlations between the ratings of all respondents. Overall agreement was p = 0.67. This compares favorably with the agreement on WS-353 (p = 0.61 using the same method). The design of the MEN rating system precludes a conventional calculation of inter-rater agreement (Bruni et al. 2012b). However, two of the creators of MEN who independently rated the data set achieved an agreement of p = 0.68.10 The SimLex-999 inter-rater agreement suggests that participants were able to understand the (single) characterization of similarity presented in the instructions and to apply it to concepts of various types consistently. This conclusion was supported by inspection of the brief feedback offered by the majority of annotators in a final text field in the questionnaire: 78% expressed sentiment that the test was clear, easy to complete, or some similar sentiment. Interestingly, as shown</context>
</contexts>
<marker>Bruni, Uijlings, Baroni, Sebe, 2012</marker>
<rawString>Bruni, Elia, Jasper Uijlings, Marco Baroni, and Nicu Sebe. 2012b. Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning. In Proceedings of the 20th ACM International Conference on Multimedia, Nara.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating Wordnet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="3255" citStr="Budanitsky and Hirst 2006" startWordPosition="459" endWordPosition="462">n pairs such as car and train, which share numerous common properties (function, material, dynamic behavior, wheels, windows, etc.). Such anomalies also exist in other gold standards such as the MEN data set (Bruni et al. 2012a). As a consequence, these evaluations effectively penalize models for learning the evident truth that coffee and cup are dissimilar. Although clearly different, coffee and cup are very much related. The psychological literature refers to the conceptual relationship between these concepts as association, although it has been given a range of names including relatedness (Budanitsky and Hirst 2006; Agirre et al. 2009), topical similarity (Hatzivassiloglou et al. 2001), and domain similarity (Turney 2012). Association contrasts with similarity, the relation connecting cup and mug (Tversky 1977). At its strongest, the similarity relation is exemplified by pairs of synonyms; words with identical referents. Computational models that effectively capture similarity as distinct from association have numerous applications. Such models are used for the automatic generation of dictionaries, thesauri, ontologies, and language correction tools (Biemann 2005; Cimiano, Hotho, and Staab 2005; Li et a</context>
<context position="17325" citStr="Budanitsky and Hirst 2006" startWordPosition="2578" endWordPosition="2581"> semantics does not distinguish between association and similarity in a principled way (see, e.g., Reisinger and Mooney 2010b; Huang et al. 2012; Luong, Socher, and Manning 2013).3 One exception is Turney (2012), who constructs two distributional models with different features and parameter settings, explicitly designed to capture either similarity or association. Using the output of these two models as input to a logistic regression classifier, Turney predicts whether two 3 Several papers that take a knowledge-based or symbolic approach to meaning do address the similarity/association issue (Budanitsky and Hirst 2006). 669 Computational Linguistics Volume 41, Number 4 concepts are associated, similar, or both, with 61% accuracy. However, in the absence of a gold standard covering the full range of similarity ratings (rather than a list of pairs identified as being similar or not) Turney cannot confirm directly that the similarityfocused model does indeed effectively quantify similarity. Agirre et al. (2009) explicitly examine the distinction between association and similarity in relation to distributional semantic models. Their study is based on the partition of WS-353 into a subset focused on similarity, </context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Budanitsky, Alexander and Graeme Hirst. 2006. Evaluating Wordnet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Andreas Hotho</author>
<author>Steffen Staab</author>
</authors>
<title>Learning concept hierarchies from text corpora using formal concept analysis.</title>
<date>2005</date>
<journal>J. Artif. Intell. Res. (JAIR),</journal>
<pages>24--305</pages>
<marker>Cimiano, Hotho, Staab, 2005</marker>
<rawString>Cimiano, Philipp, Andreas Hotho, and Steffen Staab. 2005. Learning concept hierarchies from text corpora using formal concept analysis. J. Artif. Intell. Res. (JAIR), 24:305–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning, ICML,</booktitle>
<location>Helsinki.</location>
<contexts>
<context position="4466" citStr="Collobert and Weston 2008" startWordPosition="623" endWordPosition="626"> 2005; Li et al. 2006). Machine translation systems, which aim to define mappings between fragments of different languages whose meaning is similar, but not necessarily associated, are another established application (He et al. 2008; Marton, Callison-Burch, and Resnik 2009). Moreover, since, as we establish, similarity is a cognitively complex operation that can require rich, structured conceptual knowledge to compute accurately, similarity estimation constitutes an effective proxy evaluation for general-purpose representation-learning models whose ultimate application is variable or unknown (Collobert and Weston 2008; Baroni and Lenci 2010). As we show in Section 2, the predominant gold standards for semantic evaluation in NLP do not measure the ability of models to reflect similarity. In particular, in both WS353 and MEN, pairs of words with associated meaning, such as coffee and cup (rating = 6.810), telephone and communication (7.510), or movie and theater (7.710), receive a high rating regardless of whether or not their constituents are similar. Thus, the utility of such resources to the development and application of similarity models is limited, a problem exacerbated by the fact that many researcher</context>
<context position="8369" citStr="Collobert and Weston (2008)" startWordPosition="1206" endWordPosition="1209">Lex-999 ratings, which indicate that participants found it unproblematic to quantify consistently the similarity of the full range of concepts and to distinguish it from association. Unlike existing data sets, SimLex-999 therefore contains a significant number of pairs, such as [movie, theater], which are strongly associated but receive low similarity scores. The second main contribution of this paper, presented in Section 5, is the evaluation of state-of-the-art distributional semantic models using SimLex-999. These include the well-known neural language models (NLMs) of Huang et al. (2012), Collobert and Weston (2008), and Mikolov et al. (2013a), which we compare with traditional vectorspace co-occurrence models (VSMs) (Turney and Pantel 2010) with and without dimensionality reduction (SVD) (Landauer and Dumais 1997). Our analyses demonstrate how SimLex-999 can be applied to uncover substantial differences in the ability of models to represent concepts of different types. Despite these differences, the models we consider each share the characteristic of being better able to capture association than similarity. We show that the difficulty of estimating similarity is driven primarily by those strongly associ</context>
<context position="23935" citStr="Collobert and Weston (2008)" startWordPosition="3581" endWordPosition="3584">similarity (which also afflict other resources to a greater or lesser degree): 1. Many dissimilar word pairs receive a high rating. 2. No associated but dissimilar concepts receive low ratings. As noted in the Introduction, an arguably more serious third limitation of WS-353 is low inter-annotator agreement, and the fact that state-of-the-art models such as those 4 See, e.g., Huang et al. 2012 and Bansal, Gimpel, and Livescu 2014. 5 This fact is also noted by the data set authors. See www.cs.technion.ac.il/~gabr/resources/ data/wordsim353/. 671 Computational Linguistics Volume 41, Number 4 of Collobert and Weston (2008) and Huang et al. (2012) reach, or even surpass, the inter-annotator agreement ceiling in estimating the WS-353 scores. Huang et al. report a Spearman correlation of p = 0.713 between their model output and WS-353. This is 10 percentage points higher than inter-annotator agreement (p = 0.611) when defined as the average pairwise correlation between two annotators, as is common in NLP work (Pad´o, Pad´o, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014). It could be argued that a different comparison is more appropriate: Because the model is compared to the gold-standard avera</context>
<context position="58565" citStr="Collobert and Weston (2008)" startWordPosition="9318" endWordPosition="9321">the main classes of representation learning architectures (Baroni, Dinu, and Kruszewski 2014): Vector space co-occurrence (counting) models and NLMs (Bengio et al. 2003). We first show that SimLex-999 is notably more difficult for state-of-the-art models to estimate than existing gold standards. We then conduct more focused analyses on the various concept subsets defined in SimLex-999, exploring possible causes for the comparatively low performance of current models and, in turn, demonstrating how SimLex-999 can be applied to investigate such questions. 5.1 Semantic Models Collobert &amp; Weston. Collobert and Weston (2008) apply the architecture of an NLM to learn a word representations vw for each word w in some corpus vocabulary V. Each sentence s in the input text is represented by a matrix containing the vector representations of the words in s in order. The model then computes output scores f (s) and f (sw), where sw denotes an “incorrect” sentence created from s by replacing its last word with some other word w from V. Training involves updating the parameters of the function f and the entries of the vector representations vw such that f (s) is larger than f (sw) for any w in V, other than the correct fin</context>
<context position="60256" citStr="Collobert and Weston (2008)" startWordPosition="9612" endWordPosition="9615">) train their models on 852 million words of text from a 2007 dump of Wikipedia and the RCV1 Corpus (Lewis et al. 2004) and use their embeddings to achieve state-of-the-art results on a variety of NLP tasks. We downloaded the embeddings directly from the authors’ Web page.12 Huang et al. Huang et al. (2012) present a NLM that learns word embeddings to maximize the likelihood of predicting the last word in a sentence s based on (i) the previous words in that sentence (local context, as with Collobert and Weston [2008]) and (ii) the document d in which that word occurs (global context). As with Collobert and Weston (2008), the model represents input sentences as a matrix of word embeddings. In addition, it represents documents in the input corpus as single-vector averages over all word embeddings in that document. It can then compute scores g(s, d) and g(sw, d), whereas before sw is a sentence with an “incorrect” randomly selected last word. 12 http://ml.nec-labs.com/senna/. 683 Computational Linguistics Volume 41, Number 4 Training is again by stochastic gradient descent, and corresponds to minimizing the sum of the sentence objectives Cs,d over all of the sentences in the corpus: �Cs,d = max(0,1 − g(s, d) + </context>
<context position="67356" citStr="Collobert and Weston (2008)" startWordPosition="10743" endWordPosition="10746">e reported in Huang et al. (2012), mentioned in Section 5.1. 685 Computational Linguistics Volume 41, Number 4 Correlation ρ 0.75 0.50 0.25 0.00 0.623 0.3 Evaluation Gold Standard RN353 imLex−999 0.098 0.494 0.575 0.268 0.655 0.699 0.414 Huang et al. Collobert &amp; Weston Mikolov et al. Figure 7 Performance of NLMs on WS-353, MEN, and SimLex-999. All models are trained on Wikipedia; note that as Wikipedia is constantly growing, the Mikolov et al. (2013a) model exploited slightly more training data (:t1000M tokens) than the Huang et al. (2012) model (:t990M), which in turn exploited more than the Collobert and Weston (2008) model (:t852M). Dashed horizontal lines indicate the level of inter-annotator agreement for the three data sets. global context objective leads the Huang et al. (2012) model to focus on association rather than similarity. However, the true explanation may be less simple, since the Huang et al. (2012) model performs weakly on the association-based MEN data set. The Collobert and Weston (2008) model is more robust across WS-353 and MEN, but still does not match the performance of the Mikolov et al. (2013a) model on SimLex-999. Figure 8 compares the best performing NLM model (Mikolov et al. 2013</context>
<context position="69666" citStr="Collobert and Weston (2008)" startWordPosition="11110" endWordPosition="11113">imLex-999 compared with MEN and WS-353 is consistent with our hypothesis that modeling similarity is more difficult than modeling association. Indeed, given that many strongly associated but dissimilar pairs, such as [coffee, cup], are likely to have high co-occurrence in the training data, and that all models infer connections between concepts from linguistic co-occurrence in some form or another, 16 We conduct this comparison on the smaller RCV1 Corpus (Lewis et al. 2004) because training the VSM and SVD models is comparatively slow. 17 Training times reported by Huang et al. (2012) and for Collobert and Weston (2008) at http://ronan.collobert.com/senna/. 686 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models Figure 8 Comparison between the leading NLM, Mikolov et al., the vector space model, VSM, and the SVD model. All models were trained on the ≈150m word RCV1 Corpus (Lewis et al. 2004). it seems plausible that models may overestimate the similarity of such pairs because they are “distracted” by association. To test this hypothesis more precisely, we compared the performance of models on the whole of SimLex-999 versus its 333 most associated pairs (according to the USF free association s</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Collobert, R. and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning, ICML, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Alan Cruse</author>
</authors>
<title>Lexical semantics.</title>
<date>1986</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="53559" citStr="Cruse 1986" startWordPosition="8561" endWordPosition="8562">le subjects to distinguish similarity from association effectively. 4.3 Finer-Grained Semantic Relations We have established the validity of similarity as a notion understood by human raters and distinct from association. However, much theoretical semantics focuses on relations between words or concepts that are finer-grained than similarity and association. These include meronymy (a part to its whole, e.g., blade–knife), hypernymy (a category concept to a member of that category, e.g., animal–dog), and cohyponymy (two members of the same implicit category, e.g., the pair of animals dog–cat) (Cruse 1986). Beyond theoretical interest, these relations can have practical relevance. For instance, hypernymy can form the basis of semantic entailment and therefore textual inference: The proposition a cat is on the table entails that an animal is on the table precisely because of the hypernymy relation from animal to cat. We chose not to make these finer-grained relations the basis of our evaluation for several reasons. At present, detecting relations such as hypernymy using distributional methods is challenging, even when supported by supervised classifiers with access to labeled pairs (Levy et al. </context>
</contexts>
<marker>Cruse, 1986</marker>
<rawString>Cruse, D. Alan. 1986. Lexical semantics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamish Cunningham</author>
</authors>
<title>Information extraction, automatic. Encyclopedia of language and linguistics,</title>
<date>2005</date>
<pages>665--677</pages>
<contexts>
<context position="5716" citStr="Cunningham 2005" startWordPosition="821" endWordPosition="822">uation resources actually measure.1 Although certain smaller gold standards—those of Rubenstein and Goodenough (1965) (RG) and Agirre et al. (2009) (WS-Sim)—do focus clearly on similarity, these resources suffer from other important limitations. For instance, as we show, and as is also the case for WS-353 and MEN, state-of-the-art models have reached the average performance of a human annotator on these evaluations. It is common practice in NLP to define the upper limit for automated performance on an evaluation as the average human performance or inter-annotator agreement (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010). Based on this established principle and the current evaluations, it would therefore be reasonable to conclude that the problem of representation learning, at least for similarity modeling, is approaching resolution. However, circumstantial evidence suggests that distributional models are far from perfect. For instance, we are some way from automatically generated dictionaries, thesauri, or ontologies that can be used with the same confidence as their manually created equivalents. 1 For instance, Huang et al. (2012, pages 1, 4, 10) and Reisinger and Mooney (2010b, page 4</context>
<context position="24925" citStr="Cunningham 2005" startWordPosition="3741" endWordPosition="3742">mmon in NLP work (Pad´o, Pad´o, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014). It could be argued that a different comparison is more appropriate: Because the model is compared to the gold-standard average across all annotators, we should compare a single annotator with the (almost) gold-standard average over all other annotators. Based on this metric the average performance of an annotator on WS-353 is p = 0.756, which is still only marginally better than the best automatic method.6 Thus, at least according to the established wisdom in NLP evaluation (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010), the strength of the conclusions that can be inferred from improvements on WS-353 is limited. At the same time, however, state-of-the-art distributional models are clearly not perfect representation-learning or even similarity estimation engines, as evidenced by the fact they cannot yet be applied, for instance, to generate flawless lexical resources (Alfonseca and Manandhar 2002). WS-Sim. WS-Sim is the set of pairs in WS-353 identified by Agirre et al. (2009) as either containing similar or unrelated (neither similar nor associated) concepts. The ratings in WS-Sim are m</context>
</contexts>
<marker>Cunningham, 2005</marker>
<rawString>Cunningham, Hamish. 2005. Information extraction, automatic. Encyclopedia of language and linguistics, pages 665–677.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<date>1998</date>
<publisher>WordNet. Wiley Online Library.</publisher>
<contexts>
<context position="13185" citStr="Fellbaum 1998" startWordPosition="1931" endWordPosition="1932">verse is true is comparatively more difficult. One exception is common concepts paired with low frequency synonyms, such as camel and dromedary. Because the essence of association is co-occurrence (linguistic or otherwise [McRae, Khalkhali, and Hare 2012]), such pairs can seem, at least intuitively, to be similar but not strongly associated. To explore the interaction between the two cognitive phenomena quantitatively, we exploited perhaps the only two existing large-scale means of quantifying similarity and association. To estimate similarity, we considered proximity in the WordNet taxonomy (Fellbaum 1998). Specifically, we applied the measure of Wu and Palmer (1994) (henceforth WupSim), which approximates similarity on a [0,1] scale reflecting the minimum distance between any two synsets of two given concepts in WordNet. WupSim has been shown to correlate well with human judgments on the similarity-focused RG data set (Wu and Palmer 1994). To estimate association, we extracted ratings directly from the University of South Florida Free Association Database (USF) (Nelson, McEvoy, and Schreiber 2004). These data were generated by presenting human subjects with one of 5,000 cue concepts and asking</context>
<context position="33144" citStr="Fellbaum 1998" startWordPosition="5012" endWordPosition="5013">han 75% tendency towards one particular POS. This yielded three sets of potential pairs: [A,A] pairs (of two concepts whose majority tag was Adjective), [N,N] pairs, and [V,V] pairs. Given the likelihood that different cognitive operations are used in estimating the similarity between items of different POS-category (Section 2.2), concept pairs were presented to raters in batches defined according to POS. Unlike both WS-353 and MEN, pairs of concepts of mixed POS ([white, rabbit], [run,marathon]) were excluded. POS categories are generally considered to reflect very broad ontological classes (Fellbaum 1998). We thus felt it would be very difficult, or even counter-intuitive, for annotators to quantify the similarity of mixed POS pairs according to our instructions. Concreteness. Although a clear majority of pairs in gold standards such as MEN and RG contain concrete items, perhaps surprisingly, the vast majority of adjective, noun, and verb concepts in everyday language are in fact abstract (Hill, Reichart, and Korhonen 2014; Kiela et al. 2014).8 To facilitate the evaluation of models for both concrete and abstract concept meaning, and in light of the cognitive and computational modeling differe</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, Christiane 1998. WordNet. Wiley Online Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th International Conference on World Wide Web,</booktitle>
<pages>406--414</pages>
<location>Hong Kong.</location>
<contexts>
<context position="2147" citStr="Finkelstein et al. 2001" startWordPosition="303" endWordPosition="306">, guiding the development of the next generation of representation-learning architectures. 1. Introduction There is very little similar about coffee and cups. Coffee refers to a plant, which is a living organism or a hot brown (liquid) drink. In contrast, a cup is a man-made solid of broadly well-defined shape and size with a specific function relating to the consumption of liquids. Perhaps the only clear trait these concepts have in common is that they are concrete entities. Nevertheless, in what is currently the most popular evaluation gold standard for semantic similarity, WordSim(WS)-353 (Finkelstein et al. 2001), coffee and * Computer Laboratory University of Cambridge, UK. E-mail: Ifelix.hill, anna.korhonen}® cl.cam.ac.uk. ** Technion, Israel Institute of Technology, Haifa, Israel. E-mail: roiri®ie.technion.ac.il. Submission received: 25 July 2014; revised submission received: 10 June 2015; accepted for publication: 31 August 2015. doi:10.1162/COLI a 00237 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 4 cup are rated as more “similar” than pairs such as car and train, which share numerous common properties (function, material, dynamic behavior, wheels, </context>
<context position="22484" citStr="Finkelstein et al. 2001" startWordPosition="3362" endWordPosition="3365">articular, it must clearly distinguish between dissociable semantic relations such as association and similarity. Consistent and reliable. Untrained native speakers must be able to quantify the target property consistently, without requiring lengthy or detailed instructions. This ensures that the data reflect a meaningful cognitive or semantic phenomenon, and also enables the data set to be scaled up or transferred to other languages at minimal cost and effort. We begin our review of existing evaluation with the gold standard most commonly applied in current NLP research. WordSim-353. WS-353 (Finkelstein et al. 2001) is perhaps the most commonly used evaluation gold standard for semantic models. Despite its name, and the fact that it is often referred to as a “similarity gold standard,”4 in fact, the instructions given to annotators when producing WS-353 were ambiguous with respect to similarity and association. Subjects were asked to: Assign a numerical similarity score between 0 and 10 (0 = words totally unrelated,10 = words VERY closely related) ... when estimating similarity of antonyms, consider them “similar” (i.e., belonging to the same domain or representing features of the same concept), not “dis</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Finkelstein, Lev, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In Proceedings of the 10th International Conference on World Wide Web, pages 406–414, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>Papers in Linguistics 1934–1951.</title>
<date>1957</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="82178" citStr="Firth 1957" startWordPosition="12975" endWordPosition="12976">y computations for concrete concepts, appears to be grounded in the perceptual modalities as much as in language (Barsalou et al. 2003). Whatever the means by which the improvements are achieved, accurate conceptlevel representation is likely to constitute a necessary first step towards learning informative, language-neutral phrasal and sentential representations. Such representations would be hugely valuable for fundamental NLP applications such as language understanding tools and machine translation. Distributional semantics aims to infer the meaning of words based on the company they keep (Firth 1957). However, although words that occur together in text often have associated meanings, these meanings may be very similar or indeed very different. Thus, possibly excepting the population of Argentina, most people would agree that, strictly speaking, Maradona is not synonymous with football (despite their high rating of 8.62 in WordSim-353). The challenge for the next generation of distributional models 691 Computational Linguistics Volume 41, Number 4 may therefore be to infer what is useful from the co-occurrence signal and to overlook what is not. Perhaps only then will models capture most, </context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>Firth, J. R. 1957. Papers in Linguistics 1934–1951. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dedre Gentner</author>
</authors>
<title>On relational meaning: The acquisition of verb meaning. Child Development,</title>
<date>1978</date>
<pages>988--998</pages>
<contexts>
<context position="7163" citStr="Gentner 1978" startWordPosition="1036" endWordPosition="1037">; Li et al. 2014). 666 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models Motivated by these observations, in Section 3 we present SimLex-999, a gold standard resource for evaluating the ability of models to reflect similarity. SimLex-999 was produced by 500 paid native English speakers, recruited via Amazon Mechanical Turk,2 who were asked to rate the similarity, as opposed to association, of concepts via a simple visual interface. The choice of evaluation pairs in SimLex-999 was motivated by empirical evidence that humans represent concepts of distinct part-of-speech (POS) (Gentner 1978) and conceptual concreteness (Hill, Korhonen, and Bentz 2014) differently. Whereas existing gold standards contain only concrete noun concepts (MEN) or cover only some of these distinctions via a random selection of items (WS-353, RG), SimLex-999 contains a principled selection of adjective, verb, and noun concept pairs covering the full concreteness spectrum. This design enables more nuanced analyses of how computational models overcome the distinct challenges of representing concepts of these types. In Section 4 we present quantitative and qualitative analyses of the SimLex-999 ratings, whic</context>
</contexts>
<marker>Gentner, 1978</marker>
<rawString>Gentner, Dedre. 1978. On relational meaning: The acquisition of verb meaning. Child Development, pages 988–998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dedre Gentner</author>
</authors>
<title>Why verbs are hard to learn. Action meets word: How Children Learn Verbs,</title>
<date>2006</date>
<pages>544--564</pages>
<contexts>
<context position="19852" citStr="Gentner (2006)" startWordPosition="2969" endWordPosition="2970">for syntactic or dependency relations better reflect similarity, whereas approaches that learn from running-text or bag-of-words input better model association (Agirre et al. 2009; Levy and Goldberg 2014). • Models with larger context windows may learn representations that better capture association, whereas models with narrower windows better reflect similarity (Agirre et al. 2009; Kiela and Clark 2014). 2.2 Concepts, Part-of-Speech, and Concreteness Empirical studies have shown that the performance of both humans and distributional models depends on the POS category of the concepts learned. Gentner (2006) showed that children find verb concepts harder to learn than noun concepts, and Markman and Wisniewski (1997) present evidence that different cognitive operations are used when comparing two nouns or two verbs. Hill, Reichart, and Korhonen (2014) demonstrate differences in the ability of distributional models to acquire noun and verb semantics. Further, they show that these differences are greater for models that learn from both text and perceptual input (as with humans). In addition to POS category, differences in human and computational concept learning and representation have been attribut</context>
</contexts>
<marker>Gentner, 2006</marker>
<rawString>Gentner, Dedre. 2006. Why verbs are hard to learn. Action meets word: How Children Learn Verbs, pages 544–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Nyberg Boytsov</author>
<author>Chris Dyer</author>
</authors>
<title>Metaphor detection with cross-lingual model transfer.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Baltimore, MD.</location>
<marker>Boytsov, Dyer, 2014</marker>
<rawString>Boytsov, Eric Nyberg, and Chris Dyer. 2014. Metaphor detection with cross-lingual model transfer. In Proceedings of ACL, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gene H Golub</author>
<author>Christian Reinsch</author>
</authors>
<title>Singular value decomposition and least squares solutions.</title>
<date>1970</date>
<journal>Numerische Mathematik,</journal>
<volume>14</volume>
<issue>5</issue>
<contexts>
<context position="65120" citStr="Golub and Reinsch 1970" startWordPosition="10392" endWordPosition="10395"> list of stopwords14 as features, we populated a matrix of co-occurrence counts with a row for each of the concepts in some pair in our evaluation sets, and a column for each of the features. Co-occurrence was counted within a specified window size, although never across a sentence boundary. This resulting matrix was then weighted according to Pointwise Mutual Information (PMI) (Recchia and Jones 2009). The rows of the resulting matrix constitute the vector representations of the concepts. SVD. As proposed initially in Landauer and Dumais (1997), we also experimented with models in which SVD (Golub and Reinsch 1970) is applied to the PMI-weighted VSM matrix, reducing the dimension of each concept representation to 300 (which yielded best results after experimenting, as before, with 100–500 dimension vectors). For each model described in this section, we calculate similarity as the cosine similarity between the (vector) representations learned by that model. 5.2 Results In experimenting with different models on SimLex-999, we aimed to answer the following questions: (i) How well do the established models perform on SimLex-999 versus on existing gold standards? (ii) Are any observed differences caused by t</context>
</contexts>
<marker>Golub, Reinsch, 1970</marker>
<rawString>Golub, Gene H. and Christian Reinsch. 1970. Singular value decomposition and least squares solutions. Numerische Mathematik, 14(5):403–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>2</issue>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Griffiths, Thomas L., Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114(2):211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL 2008,</booktitle>
<location>Columbus, OH.</location>
<contexts>
<context position="16243" citStr="Haghighi et al. 2008" startWordPosition="2418" endWordPosition="2421">ese are precisely the sort of pairs that are not contained in existing evaluation gold standards. Table 1 lists the USF noun pairs with the lowest similarity scores overall, and also those with the largest additive discrepancy between association strength and similarity. 2.1.1 Association and Similarity in NLP. As noted in the Introduction, the similarity/association distinction is not only of interest to researchers in psychology or linguistics. Models of similarity are particularly applicable to various NLP tasks, such as lexical resource building, semantic parsing, and machine translation (Haghighi et al. 2008; He et al. 2008; Marton, Callison-Burch, and Resnik 2009; Beltagy, Erk, and Mooney 2014). Models of association, on the other hand, may be better suited to tasks such as wordsense disambiguation (Navigli 2009), and applications such as text classification (Phan, Nguyen, and Horiguchi 2008) in which the target classes correspond to topical domains such as agriculture or sport (Rose, Stevenson, and Whitehead 2002). Much recent research in distributional semantics does not distinguish between association and similarity in a principled way (see, e.g., Reisinger and Mooney 2010b; Huang et al. 2012</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of ACL 2008, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samer Hassan</author>
<author>Rada Mihalcea</author>
</authors>
<title>Semantic relatedness using salient semantic analysis.</title>
<date>2011</date>
<booktitle>In AAAI,</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="28030" citStr="Hassan and Mihalcea 2011" startWordPosition="4223" endWordPosition="4226">g” of each concept pair. Thus RG does appear to reflect similarity rather than association. However, although limitation (1) of WS-353 is therefore avoided, RG still suffers from limitation (2): By inspection, 6 Individual annotator responses for WS-353 were downloaded from www.cs.technion.ac.il/~gabr/ resources/data/wordsim353. 672 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models it is clear that the low similarity pairs in RG are not associated. A further limitation is that distributional models now achieve better performance on RG (correlations of up to Pearson r = 0.86 [Hassan and Mihalcea 2011]) than the reported inter-annotator agreement of r = 0.85 (Rubenstein and Goodenough 1965). Finally, the size of RG renders it an even less comprehensive evaluation than WS-Sim. The MEN Test Collection. A larger data set, MEN (Bruni et al. 2012a), is used in a handful of recent studies (Bruni et al. 2012b; Bernardi et al. 2013). As with WS-353, both terms similarity and relatedness are used by the authors when describing MEN, although the annotators were expressly asked to rate pairs according to relatedness.7 The construction of MEN differed from RG and WS-353 in that each pair was only cons</context>
</contexts>
<marker>Hassan, Mihalcea, 2011</marker>
<rawString>Hassan, Samer and Rada Mihalcea. 2011. Semantic relatedness using salient semantic analysis. In AAAI, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Melissa L Holcombe</author>
<author>Regina Barzilay</author>
<author>Min-Yen Kan</author>
<author>Kathleen McKeown</author>
</authors>
<title>Simfinder: A flexible clustering tool for summarization.</title>
<date>2001</date>
<booktitle>In NAACL Workshop on Automatic Summarization,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="3327" citStr="Hatzivassiloglou et al. 2001" startWordPosition="469" endWordPosition="472">s (function, material, dynamic behavior, wheels, windows, etc.). Such anomalies also exist in other gold standards such as the MEN data set (Bruni et al. 2012a). As a consequence, these evaluations effectively penalize models for learning the evident truth that coffee and cup are dissimilar. Although clearly different, coffee and cup are very much related. The psychological literature refers to the conceptual relationship between these concepts as association, although it has been given a range of names including relatedness (Budanitsky and Hirst 2006; Agirre et al. 2009), topical similarity (Hatzivassiloglou et al. 2001), and domain similarity (Turney 2012). Association contrasts with similarity, the relation connecting cup and mug (Tversky 1977). At its strongest, the similarity relation is exemplified by pairs of synonyms; words with identical referents. Computational models that effectively capture similarity as distinct from association have numerous applications. Such models are used for the automatic generation of dictionaries, thesauri, ontologies, and language correction tools (Biemann 2005; Cimiano, Hotho, and Staab 2005; Li et al. 2006). Machine translation systems, which aim to define mappings betw</context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Holcombe, Barzilay, Kan, McKeown, 2001</marker>
<rawString>Hatzivassiloglou, Vasileios, Judith L. Klavans, Melissa L. Holcombe, Regina Barzilay, Min-Yen Kan, and Kathleen McKeown. 2001. Simfinder: A flexible clustering tool for summarization. In NAACL Workshop on Automatic Summarization, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect-HMM-based hypothesis alignment for combining outputs from machine translation systems.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>98--107</pages>
<location>Edinburgh.</location>
<contexts>
<context position="4073" citStr="He et al. 2008" startWordPosition="573" endWordPosition="576"> its strongest, the similarity relation is exemplified by pairs of synonyms; words with identical referents. Computational models that effectively capture similarity as distinct from association have numerous applications. Such models are used for the automatic generation of dictionaries, thesauri, ontologies, and language correction tools (Biemann 2005; Cimiano, Hotho, and Staab 2005; Li et al. 2006). Machine translation systems, which aim to define mappings between fragments of different languages whose meaning is similar, but not necessarily associated, are another established application (He et al. 2008; Marton, Callison-Burch, and Resnik 2009). Moreover, since, as we establish, similarity is a cognitively complex operation that can require rich, structured conceptual knowledge to compute accurately, similarity estimation constitutes an effective proxy evaluation for general-purpose representation-learning models whose ultimate application is variable or unknown (Collobert and Weston 2008; Baroni and Lenci 2010). As we show in Section 2, the predominant gold standards for semantic evaluation in NLP do not measure the ability of models to reflect similarity. In particular, in both WS353 and M</context>
<context position="16259" citStr="He et al. 2008" startWordPosition="2422" endWordPosition="2425">sort of pairs that are not contained in existing evaluation gold standards. Table 1 lists the USF noun pairs with the lowest similarity scores overall, and also those with the largest additive discrepancy between association strength and similarity. 2.1.1 Association and Similarity in NLP. As noted in the Introduction, the similarity/association distinction is not only of interest to researchers in psychology or linguistics. Models of similarity are particularly applicable to various NLP tasks, such as lexical resource building, semantic parsing, and machine translation (Haghighi et al. 2008; He et al. 2008; Marton, Callison-Burch, and Resnik 2009; Beltagy, Erk, and Mooney 2014). Models of association, on the other hand, may be better suited to tasks such as wordsense disambiguation (Navigli 2009), and applications such as text classification (Phan, Nguyen, and Horiguchi 2008) in which the target classes correspond to topical domains such as agriculture or sport (Rose, Stevenson, and Whitehead 2002). Much recent research in distributional semantics does not distinguish between association and similarity in a principled way (see, e.g., Reisinger and Mooney 2010b; Huang et al. 2012; Luong, Socher,</context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>He, Xiaodong, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore. 2008. Indirect-HMM-based hypothesis alignment for combining outputs from machine translation systems. In Proceedings of EMNLP, pages 98–107, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Douwe Kiela</author>
<author>Anna Korhonen</author>
</authors>
<title>Concreteness and corpora: A theoretical and practical analysis.</title>
<date>2013</date>
<booktitle>CMCL 2013,</booktitle>
<pages>75</pages>
<location>Sofia.</location>
<marker>Hill, Kiela, Korhonen, 2013</marker>
<rawString>Hill, Felix, Douwe Kiela, and Anna Korhonen. 2013. Concreteness and corpora: A theoretical and practical analysis. CMCL 2013, page 75, Sofia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Anna Korhonen</author>
<author>Christian Bentz</author>
</authors>
<title>A quantitative empirical analysis of the abstract/concrete distinction.</title>
<date>2014</date>
<journal>Cognitive Science,</journal>
<volume>38</volume>
<issue>1</issue>
<marker>Hill, Korhonen, Bentz, 2014</marker>
<rawString>Hill, Felix, Anna Korhonen, and Christian Bentz. 2014. A quantitative empirical analysis of the abstract/concrete distinction. Cognitive Science, 38(1):162–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Multi-modal models for concrete and abstract concept meaning.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<pages>2--285</pages>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Hill, Felix, Roi Reichart, and Anna Korhonen. 2014. Multi-modal models for concrete and abstract concept meaning. Transactions of the Association for Computational Linguistics (TACL), 2:285–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>873--882</pages>
<location>Jeju Island.</location>
<contexts>
<context position="6259" citStr="Huang et al. (2012" startWordPosition="897" endWordPosition="900">erformance or inter-annotator agreement (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010). Based on this established principle and the current evaluations, it would therefore be reasonable to conclude that the problem of representation learning, at least for similarity modeling, is approaching resolution. However, circumstantial evidence suggests that distributional models are far from perfect. For instance, we are some way from automatically generated dictionaries, thesauri, or ontologies that can be used with the same confidence as their manually created equivalents. 1 For instance, Huang et al. (2012, pages 1, 4, 10) and Reisinger and Mooney (2010b, page 4) refer to MEN and/or WS-353 as “similarity data sets.” Others evaluate on both these association-based and genuine similarity-based gold standards with no reference to the fact that they measure different things (Medelyan et al. 2009; Li et al. 2014). 666 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models Motivated by these observations, in Section 3 we present SimLex-999, a gold standard resource for evaluating the ability of models to reflect similarity. SimLex-999 was produced by 500 paid native English speakers, rec</context>
<context position="8340" citStr="Huang et al. (2012)" startWordPosition="1202" endWordPosition="1205">e analyses of the SimLex-999 ratings, which indicate that participants found it unproblematic to quantify consistently the similarity of the full range of concepts and to distinguish it from association. Unlike existing data sets, SimLex-999 therefore contains a significant number of pairs, such as [movie, theater], which are strongly associated but receive low similarity scores. The second main contribution of this paper, presented in Section 5, is the evaluation of state-of-the-art distributional semantic models using SimLex-999. These include the well-known neural language models (NLMs) of Huang et al. (2012), Collobert and Weston (2008), and Mikolov et al. (2013a), which we compare with traditional vectorspace co-occurrence models (VSMs) (Turney and Pantel 2010) with and without dimensionality reduction (SVD) (Landauer and Dumais 1997). Our analyses demonstrate how SimLex-999 can be applied to uncover substantial differences in the ability of models to represent concepts of different types. Despite these differences, the models we consider each share the characteristic of being better able to capture association than similarity. We show that the difficulty of estimating similarity is driven prima</context>
<context position="16843" citStr="Huang et al. 2012" startWordPosition="2510" endWordPosition="2513">highi et al. 2008; He et al. 2008; Marton, Callison-Burch, and Resnik 2009; Beltagy, Erk, and Mooney 2014). Models of association, on the other hand, may be better suited to tasks such as wordsense disambiguation (Navigli 2009), and applications such as text classification (Phan, Nguyen, and Horiguchi 2008) in which the target classes correspond to topical domains such as agriculture or sport (Rose, Stevenson, and Whitehead 2002). Much recent research in distributional semantics does not distinguish between association and similarity in a principled way (see, e.g., Reisinger and Mooney 2010b; Huang et al. 2012; Luong, Socher, and Manning 2013).3 One exception is Turney (2012), who constructs two distributional models with different features and parameter settings, explicitly designed to capture either similarity or association. Using the output of these two models as input to a logistic regression classifier, Turney predicts whether two 3 Several papers that take a knowledge-based or symbolic approach to meaning do address the similarity/association issue (Budanitsky and Hirst 2006). 669 Computational Linguistics Volume 41, Number 4 concepts are associated, similar, or both, with 61% accuracy. Howe</context>
<context position="23704" citStr="Huang et al. 2012" startWordPosition="3549" endWordPosition="3552">ilar”. As we confirm analytically in Section 5.2, these instructions result in pairs being rated according to association rather than similarity.5 WS-353 consequently suffers two important limitations as an evaluation of similarity (which also afflict other resources to a greater or lesser degree): 1. Many dissimilar word pairs receive a high rating. 2. No associated but dissimilar concepts receive low ratings. As noted in the Introduction, an arguably more serious third limitation of WS-353 is low inter-annotator agreement, and the fact that state-of-the-art models such as those 4 See, e.g., Huang et al. 2012 and Bansal, Gimpel, and Livescu 2014. 5 This fact is also noted by the data set authors. See www.cs.technion.ac.il/~gabr/resources/ data/wordsim353/. 671 Computational Linguistics Volume 41, Number 4 of Collobert and Weston (2008) and Huang et al. (2012) reach, or even surpass, the inter-annotator agreement ceiling in estimating the WS-353 scores. Huang et al. report a Spearman correlation of p = 0.713 between their model output and WS-353. This is 10 percentage points higher than inter-annotator agreement (p = 0.611) when defined as the average pairwise correlation between two annotators, as</context>
<context position="38452" citStr="Huang et al. (2012)" startWordPosition="5845" endWordPosition="5848">41, Number 4 Figure 2 Instructions for SimLex-999 annotators. the next page) between the tasks of rating adjective, noun, and verb pairs. For better inter-group calibration, from the second group onwards the last pair of the previous group became the first pair of the present group, and participants were asked to re-assign the rating previously attributed to the first pair before rating the remaining new items. 3.3 Context-Free Rating As with MEN, WS-353, and RG, SimLex-999 consists of pairs of concept words together with a numerical rating. Thus, unlike in the small evaluation constructed by Huang et al. (2012), words are not rated in a phrasal or sentential context. Such meaning-in-context evaluations are motivated by a desire to disambiguate words that otherwise might be considered to have multiple senses. We did not attempt to construct an evaluation based on meaning-in-context for several reasons. First, determining the set of senses for a given word, and then the set of contexts that represent those senses, introduces a high degree of subjectivity into the design process. Second, ensuring that a model has learned a high quality representation of a given concept would have required evaluating th</context>
<context position="59937" citStr="Huang et al. (2012)" startWordPosition="9557" endWordPosition="9560">a (mini-batch) stochastic gradient descent: �Cs = max(0,1 − f (s) + f (sw)) w∈V The relatively low-dimension, dense (vector) representations learned by this model and the other NLMs introduced in this section are sometimes referred to as embeddings (Turian, Ratinov, and Bengio 2010). Collobert and Weston (2008) train their models on 852 million words of text from a 2007 dump of Wikipedia and the RCV1 Corpus (Lewis et al. 2004) and use their embeddings to achieve state-of-the-art results on a variety of NLP tasks. We downloaded the embeddings directly from the authors’ Web page.12 Huang et al. Huang et al. (2012) present a NLM that learns word embeddings to maximize the likelihood of predicting the last word in a sentence s based on (i) the previous words in that sentence (local context, as with Collobert and Weston [2008]) and (ii) the document d in which that word occurs (global context). As with Collobert and Weston (2008), the model represents input sentences as a matrix of word embeddings. In addition, it represents documents in the input corpus as single-vector averages over all word embeddings in that document. It can then compute scores g(s, d) and g(sw, d), whereas before sw is a sentence wit</context>
<context position="66321" citStr="Huang et al. (2012)" startWordPosition="10573" endWordPosition="10576">rences caused by the potential of different models to measure similarity vs. association? (iii) Are there interesting differences in ability of models to capture similarity between adjectives vs. nouns vs. verbs? (iv) In this case, are the observed differences driven by concreteness, and its interaction with POS, or are other factors also relevant? Overall Performance on SimLex-999. Figure 7 shows the performance of the NLMs on SimLex-999 versus on comparable data sets, measured by Spearman’s ρ correlation. All models estimate the ratings of MEN and WS-353 more accurately than SimLex-999. The Huang et al. (2012) model performs well on WS-353,15 but is not very robust to changes in evaluation gold standard, and performs worst of all the models on SimLex-999. Given the focus of the WS-353 ratings, it is tempting to explain this by concluding that the 14 Taken from the Python Natural Language Toolkit (Bird 2006). 15 This score, based on embeddings downloaded from the authors’ webpage, is notably lower than the score reported in Huang et al. (2012), mentioned in Section 5.1. 685 Computational Linguistics Volume 41, Number 4 Correlation ρ 0.75 0.50 0.25 0.00 0.623 0.3 Evaluation Gold Standard RN353 imLex−</context>
<context position="67658" citStr="Huang et al. (2012)" startWordPosition="10790" endWordPosition="10793">LMs on WS-353, MEN, and SimLex-999. All models are trained on Wikipedia; note that as Wikipedia is constantly growing, the Mikolov et al. (2013a) model exploited slightly more training data (:t1000M tokens) than the Huang et al. (2012) model (:t990M), which in turn exploited more than the Collobert and Weston (2008) model (:t852M). Dashed horizontal lines indicate the level of inter-annotator agreement for the three data sets. global context objective leads the Huang et al. (2012) model to focus on association rather than similarity. However, the true explanation may be less simple, since the Huang et al. (2012) model performs weakly on the association-based MEN data set. The Collobert and Weston (2008) model is more robust across WS-353 and MEN, but still does not match the performance of the Mikolov et al. (2013a) model on SimLex-999. Figure 8 compares the best performing NLM model (Mikolov et al. 2013a) with the VSM and SVD models.16 In contrast to recent results that emphasize the superiority of NLMs over alternatives (Baroni, Dinu, and Kruszewski 2014), we observed no clear advantage for the NLM over the VSM or SVD when considering the associationbased gold standards WS-353 and MEN together. Whi</context>
<context position="69630" citStr="Huang et al. (2012)" startWordPosition="11104" endWordPosition="11107">LM, VSM, and SVD models on SimLex-999 compared with MEN and WS-353 is consistent with our hypothesis that modeling similarity is more difficult than modeling association. Indeed, given that many strongly associated but dissimilar pairs, such as [coffee, cup], are likely to have high co-occurrence in the training data, and that all models infer connections between concepts from linguistic co-occurrence in some form or another, 16 We conduct this comparison on the smaller RCV1 Corpus (Lewis et al. 2004) because training the VSM and SVD models is comparatively slow. 17 Training times reported by Huang et al. (2012) and for Collobert and Weston (2008) at http://ronan.collobert.com/senna/. 686 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models Figure 8 Comparison between the leading NLM, Mikolov et al., the vector space model, VSM, and the SVD model. All models were trained on the ≈150m word RCV1 Corpus (Lewis et al. 2004). it seems plausible that models may overestimate the similarity of such pairs because they are “distracted” by association. To test this hypothesis more precisely, we compared the performance of models on the whole of SimLex-999 versus its 333 most associated pairs (acc</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Huang, Eric H., Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of ACL, pages 873–882, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>Stephen Clark</author>
</authors>
<title>A systematic study of semantic vector space model parameters.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)@ EACL,</booktitle>
<pages>21--30</pages>
<contexts>
<context position="10116" citStr="Kiela and Clark 2014" startWordPosition="1468" endWordPosition="1471">so, we evaluate the models on the SimLex999 subsets of adjectives, nouns, and verbs, as well as on abstract and concrete subsets and subsets of more and less strongly associated pairs (Sections 5.2.2–5.2.4). As part of these analyses, we confirm the hypothesis (Agirre et al. 2009; Levy and Goldberg 2014) that models learning from input informed by dependency parsing, rather than simple running-text input, yield improved similarity estimation and, specifically, clearer distinction between similarity and association. In contrast, we find no evidence for a related hypothesis (Agirre et al. 2009; Kiela and Clark 2014) that smaller context windows improve the ability of models to capture similarity. We do, however, observe clear differences in model performance on the distinct concept types included in SimLex-999. Taken together, these experiments demonstrate the benefit of the diversity of concepts 2 www.mturk.com/. 667 Computational Linguistics Volume 41, Number 4 included in SimLex-999; it would not have been possible to derive similar insights by evaluating based on existing gold standards. We conclude by discussing how observations such as these can guide future research into distributional semantic mo</context>
<context position="18963" citStr="Kiela and Clark 2014" startWordPosition="2839" endWordPosition="2842">ls perform relatively well on WS-Rel, whereas others perform comparatively better on WS-Sim. However, as shown in the following section, a model need not be an exemplary model of similarity in order to perform well on WS-Sim, because an important class of concept pair (associated but not similar entities) is not represented in this data set. Therefore the insights that can be drawn from the results of the Agirre et al. study are limited. Several other authors touch on the similarity/association distinction in inspecting the output of distributional models (Andrews, Vigliocco, and Vinson 2009; Kiela and Clark 2014; Levy and Goldberg 2014). Although the strength of the conclusions that can be drawn from such qualitative analyses is clearly limited, there appear to be two broad areas of consensus concerning similarity and distributional models: • Models that learn from input annotated for syntactic or dependency relations better reflect similarity, whereas approaches that learn from running-text or bag-of-words input better model association (Agirre et al. 2009; Levy and Goldberg 2014). • Models with larger context windows may learn representations that better capture association, whereas models with nar</context>
<context position="64404" citStr="Kiela and Clark (2014)" startWordPosition="10275" endWordPosition="10278"> the context-embeddings of its collocates. In the target-embedding space, this results in embeddings of concept words that regularly occur in similar contexts moving closer together. We use the author’s Word2vec software in order to train their model and use the target embeddings in our evaluations. We experimented with embeddings of dimension 100, 200, 300, 400, and 500 and found that 200 gave the best performance on both WS-353 and SimLex-999. Vector Space Model (VSM). As an alternative to NLMs, we constructed a vector space model following the guidelines for optimal performance outlined by Kiela and Clark (2014). After extracting the 2,000 most frequent word tokens in the corpus that are not in a common list of stopwords14 as features, we populated a matrix of co-occurrence counts with a row for each of the concepts in some pair in our evaluation sets, and a column for each of the features. Co-occurrence was counted within a specified window size, although never across a sentence boundary. This resulting matrix was then weighted according to Pointwise Mutual Information (PMI) (Recchia and Jones 2009). The rows of the resulting matrix constitute the vector representations of the concepts. SVD. As prop</context>
</contexts>
<marker>Kiela, Clark, 2014</marker>
<rawString>Kiela, Douwe and Stephen Clark. 2014. A systematic study of semantic vector space model parameters. In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)@ EACL, pages 21–30, Gothenburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>Felix Hill</author>
<author>Anna Korhonen</author>
<author>Stephen Clark</author>
</authors>
<title>Improving multi-modal representations using image dispersion: Why less is sometimes more.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Baltimore, MD.</location>
<contexts>
<context position="20901" citStr="Kiela et al. 2014" startWordPosition="3122" endWordPosition="3125">th text and perceptual input (as with humans). In addition to POS category, differences in human and computational concept learning and representation have been attributed to the effects of concreteness, the extent to which a concept has a directly perceptible physical referent. On the cognitive side, these “concreteness effects” are well established, even if the causes are still debated (Paivio 1991; Hill, Korhonen, and Bentz 2014). Concreteness has also been associated with differential performance in computational text-based (Hill, Kiela, and Korhonen 2013) and multi-modal semantic models (Kiela et al. 2014). 670 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models 2.3 Existing Gold Standards and Evaluation Resources For brevity, we do not exhaustively review all methods that have been used to evaluate semantic models, but instead focus on the similarity or association-based gold standards that are most commonly applied in recent work in NLP. In each case, we consider how well the data set satisfies one of the three following criteria: Representative. The resource should cover the full range of concepts that occur in natural language. In particular, it should include cases represen</context>
<context position="33590" citStr="Kiela et al. 2014" startWordPosition="5080" endWordPosition="5083"> of concepts of mixed POS ([white, rabbit], [run,marathon]) were excluded. POS categories are generally considered to reflect very broad ontological classes (Fellbaum 1998). We thus felt it would be very difficult, or even counter-intuitive, for annotators to quantify the similarity of mixed POS pairs according to our instructions. Concreteness. Although a clear majority of pairs in gold standards such as MEN and RG contain concrete items, perhaps surprisingly, the vast majority of adjective, noun, and verb concepts in everyday language are in fact abstract (Hill, Reichart, and Korhonen 2014; Kiela et al. 2014).8 To facilitate the evaluation of models for both concrete and abstract concept meaning, and in light of the cognitive and computational modeling differences between abstract and concrete concepts noted in Section 2.2, we aimed to include both concept types in SimLex-999. Unlike the POS distinction, concreteness is generally considered to be a gradual phenomenon. One benefit of sampling pairs for SimLex-999 from the USF data set is that most items have been rated according to concreteness on a scale of 1–7 by at least 10 human subjects. As Figure 1 demonstrates, concreteness (as the average o</context>
</contexts>
<marker>Kiela, Hill, Korhonen, Clark, 2014</marker>
<rawString>Kiela, Douwe, Felix Hill, Anna Korhonen, and Stephen Clark. 2014. Improving multi-modal representations using image dispersion: Why less is sometimes more. In Proceedings of ACL, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="8572" citStr="Landauer and Dumais 1997" startWordPosition="1236" endWordPosition="1239">ets, SimLex-999 therefore contains a significant number of pairs, such as [movie, theater], which are strongly associated but receive low similarity scores. The second main contribution of this paper, presented in Section 5, is the evaluation of state-of-the-art distributional semantic models using SimLex-999. These include the well-known neural language models (NLMs) of Huang et al. (2012), Collobert and Weston (2008), and Mikolov et al. (2013a), which we compare with traditional vectorspace co-occurrence models (VSMs) (Turney and Pantel 2010) with and without dimensionality reduction (SVD) (Landauer and Dumais 1997). Our analyses demonstrate how SimLex-999 can be applied to uncover substantial differences in the ability of models to represent concepts of different types. Despite these differences, the models we consider each share the characteristic of being better able to capture association than similarity. We show that the difficulty of estimating similarity is driven primarily by those strongly associated pairs with a high (association) rating in gold standards such as WS-353 and MEN, but a low similarity rating in SimLex-999. As a result of including these challenging cases, together with a wider di</context>
<context position="29347" citStr="Landauer and Dumais 1997" startWordPosition="4439" endWordPosition="4442">t. An overall score out of 50 was then attributed to each pair corresponding to how many times it was ranked as more related than an alternative. However, because these rankings are based on relatedness, with respect to evaluating similarity MEN necessarily suffers from both of the limitations (1) and (2) that apply to WS-353. Further, there is a strong bias towards concrete concepts in MEN because the concepts were originally selected from those identified in an image-bank (Bruni et al. 2012a). Synonym Detection Sets. Multiple-choice synonym detection tasks, such as the TOEFL test questions (Landauer and Dumais 1997), are an alternative means of evaluating distributional models. A question in the TOEFL task consists of a cue word and four possible answer words, only one of which is a true synonym. Models are scored on the number of true synonyms identified out of 80 questions. The questions were designed by linguists to evaluate synonymy, so, unlike the evaluations considered thus far, TOEFL-style tests effectively discriminate between similarity and association. However, because they require a zero-one classification of pairs as synonymous or not, they do not test how well models discern pairs of medium </context>
<context position="65048" citStr="Landauer and Dumais (1997)" startWordPosition="10380" endWordPosition="10383"> the 2,000 most frequent word tokens in the corpus that are not in a common list of stopwords14 as features, we populated a matrix of co-occurrence counts with a row for each of the concepts in some pair in our evaluation sets, and a column for each of the features. Co-occurrence was counted within a specified window size, although never across a sentence boundary. This resulting matrix was then weighted according to Pointwise Mutual Information (PMI) (Recchia and Jones 2009). The rows of the resulting matrix constitute the vector representations of the concepts. SVD. As proposed initially in Landauer and Dumais (1997), we also experimented with models in which SVD (Golub and Reinsch 1970) is applied to the PMI-weighted VSM matrix, reducing the dimension of each concept representation to 300 (which yielded best results after experimenting, as before, with 100–500 dimension vectors). For each model described in this section, we calculate similarity as the cosine similarity between the (vector) representations learned by that model. 5.2 Results In experimenting with different models on SimLex-999, we aimed to answer the following questions: (i) How well do the established models perform on SimLex-999 versus o</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, Thomas K. and Susan T. Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
<author>Michael Bryant</author>
</authors>
<title>Claws4: The tagging of the British National Corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>622--628</pages>
<marker>Leech, Garside, Bryant, 1994</marker>
<rawString>Leech, Geoffrey, Roger Garside, and Michael Bryant. 1994. Claws4: The tagging of the British National Corpus. In Proceedings of COLING, pages 622–628, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependency-based word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<volume>2</volume>
<contexts>
<context position="9800" citStr="Levy and Goldberg 2014" startWordPosition="1424" endWordPosition="1427">ty of lexical concepts in general, current models achieve notably lower scores on SimLex-999 than on existing gold standard evaluations, and well below the SimLex-999 inter-human agreement ceiling. Finally, we explore ways in which distributional models might improve on this performance in similarity modeling. To do so, we evaluate the models on the SimLex999 subsets of adjectives, nouns, and verbs, as well as on abstract and concrete subsets and subsets of more and less strongly associated pairs (Sections 5.2.2–5.2.4). As part of these analyses, we confirm the hypothesis (Agirre et al. 2009; Levy and Goldberg 2014) that models learning from input informed by dependency parsing, rather than simple running-text input, yield improved similarity estimation and, specifically, clearer distinction between similarity and association. In contrast, we find no evidence for a related hypothesis (Agirre et al. 2009; Kiela and Clark 2014) that smaller context windows improve the ability of models to capture similarity. We do, however, observe clear differences in model performance on the distinct concept types included in SimLex-999. Taken together, these experiments demonstrate the benefit of the diversity of concep</context>
<context position="18988" citStr="Levy and Goldberg 2014" startWordPosition="2843" endWordPosition="2846">well on WS-Rel, whereas others perform comparatively better on WS-Sim. However, as shown in the following section, a model need not be an exemplary model of similarity in order to perform well on WS-Sim, because an important class of concept pair (associated but not similar entities) is not represented in this data set. Therefore the insights that can be drawn from the results of the Agirre et al. study are limited. Several other authors touch on the similarity/association distinction in inspecting the output of distributional models (Andrews, Vigliocco, and Vinson 2009; Kiela and Clark 2014; Levy and Goldberg 2014). Although the strength of the conclusions that can be drawn from such qualitative analyses is clearly limited, there appear to be two broad areas of consensus concerning similarity and distributional models: • Models that learn from input annotated for syntactic or dependency relations better reflect similarity, whereas approaches that learn from running-text or bag-of-words input better model association (Agirre et al. 2009; Levy and Goldberg 2014). • Models with larger context windows may learn representations that better capture association, whereas models with narrower windows better refl</context>
<context position="72407" citStr="Levy and Goldberg (2014)" startWordPosition="11528" endWordPosition="11531">ay bars), suggesting that the improvement is driven at least in part by an increased ability to “distinguish” similarity from association. To understand better how the architecture of models captures information pertinent to similarity modeling, we performed two additional experiments using SimLex-999. These comparisons were also motivated by the hypotheses, made in previous studies and outlined in Section 2.1.2, that both dependency-informed input and smaller context windows encourage models to capture similarity rather than association. We tested the first hypothesis using the embeddings of Levy and Goldberg (2014), whose model extends the Mikolov et al. (2013a) model so that target-context training instances are extracted based on dependency-parsed rather than simple running text. As illustrated in Figure 9, the dependency-based embeddings outperform the original (running text) embeddings trained on the same corpus. Moreover, the comparatively large increase in the red bar compared to the gray bar suggests that an important part of the improvement of the dependency-based model derives from a greater ability to discern similarity from association. Our comparisons provided less support for the second (wi</context>
<context position="74919" citStr="Levy and Goldberg (2014)" startWordPosition="11916" endWordPosition="11919">ls. 688 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models Figure 11 Performance of models on POS-based subsets of SimLex-999. The window size for each model is indicated in parentheses. Inter-annotator agreement for each POS is indicated by the dashed horizontal line. than noun similarity by humans, but models estimate these ratings more accurately for nouns than for verbs. To better understand the linguistic information exploited by models when acquiring concepts of different POS, we also computed performance on the POS subsets of SimLex-999 of the dependency-based model of Levy and Goldberg (2014) and the standard skipgram model, in which linguistic contexts are encoded as simple bagsof-words (BOW) (Mikolov et al. (2013a) [trained on the same Wikipedia text]). As shown in Figure 12, dependency-aware contexts yield the largest improvements for capturing verb similarity. This aligns with the cognitive theory of verbs as relational concepts (Markman and Wisniewski 1997) whose meanings rely on their interaction with (or dependency on) other words or concepts. It is also consistent with research on the automatic acquisition of verb semantics, in which syntactic features have proven particul</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Levy, Omer and Yoav Goldberg. 2014. Dependency-based word embeddings. In Proceedings of ACL, volume 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Steffen Remus</author>
<author>Chris Biemann</author>
<author>Idol Dagan</author>
</authors>
<title>Do supervised distributional methods really learn lexical inference relations?</title>
<date>2015</date>
<booktitle>Proceedings of NAACL,</booktitle>
<location>Denver, CO.</location>
<contexts>
<context position="54164" citStr="Levy et al. 2015" startWordPosition="8651" endWordPosition="8654">(Cruse 1986). Beyond theoretical interest, these relations can have practical relevance. For instance, hypernymy can form the basis of semantic entailment and therefore textual inference: The proposition a cat is on the table entails that an animal is on the table precisely because of the hypernymy relation from animal to cat. We chose not to make these finer-grained relations the basis of our evaluation for several reasons. At present, detecting relations such as hypernymy using distributional methods is challenging, even when supported by supervised classifiers with access to labeled pairs (Levy et al. 2015). Such a designation can seem to require specific 681 Computational Linguistics Volume 41, Number 4 world-knowledge (is a snale a reptile?), can be gradual, as evidenced by typicality effects (Rosch, Simpson, and Miller 1976), or simply highly subjective. Moreover, a fine-grained relation R will only be attested (to any degree) between a small subset of all possible word pairs, whereas similarity can in theory be quantified for any two words chosen at random. We thus considered a focus on fine-grained semantic relations to be less appropriate for a general-purpose evaluation of representation </context>
</contexts>
<marker>Levy, Remus, Biemann, Dagan, 2015</marker>
<rawString>Levy, Omer, Steffen Remus, Chris Biemann, and Idol Dagan. 2015. Do supervised distributional methods really learn lexical inference relations? Proceedings of NAACL, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Yiming Yang</author>
<author>Tony G Rose</author>
<author>Fan Li</author>
</authors>
<title>Rcv1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>5--361</pages>
<contexts>
<context position="59748" citStr="Lewis et al. 2004" startWordPosition="9526" endWordPosition="9529"> w in V, other than the correct final word of s. This corresponds to minimizing the sum of the following sentence objectives Cs over all sentences in the input corpus, which is achieved via (mini-batch) stochastic gradient descent: �Cs = max(0,1 − f (s) + f (sw)) w∈V The relatively low-dimension, dense (vector) representations learned by this model and the other NLMs introduced in this section are sometimes referred to as embeddings (Turian, Ratinov, and Bengio 2010). Collobert and Weston (2008) train their models on 852 million words of text from a 2007 dump of Wikipedia and the RCV1 Corpus (Lewis et al. 2004) and use their embeddings to achieve state-of-the-art results on a variety of NLP tasks. We downloaded the embeddings directly from the authors’ Web page.12 Huang et al. Huang et al. (2012) present a NLM that learns word embeddings to maximize the likelihood of predicting the last word in a sentence s based on (i) the previous words in that sentence (local context, as with Collobert and Weston [2008]) and (ii) the document d in which that word occurs (global context). As with Collobert and Weston (2008), the model represents input sentences as a matrix of word embeddings. In addition, it repre</context>
<context position="69517" citStr="Lewis et al. 2004" startWordPosition="11085" endWordPosition="11088"> the development of improved models. Modeling Similarity vs. Association. The comparatively low performance of NLM, VSM, and SVD models on SimLex-999 compared with MEN and WS-353 is consistent with our hypothesis that modeling similarity is more difficult than modeling association. Indeed, given that many strongly associated but dissimilar pairs, such as [coffee, cup], are likely to have high co-occurrence in the training data, and that all models infer connections between concepts from linguistic co-occurrence in some form or another, 16 We conduct this comparison on the smaller RCV1 Corpus (Lewis et al. 2004) because training the VSM and SVD models is comparatively slow. 17 Training times reported by Huang et al. (2012) and for Collobert and Weston (2008) at http://ronan.collobert.com/senna/. 686 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models Figure 8 Comparison between the leading NLM, Mikolov et al., the vector space model, VSM, and the SVD model. All models were trained on the ≈150m word RCV1 Corpus (Lewis et al. 2004). it seems plausible that models may overestimate the similarity of such pairs because they are “distracted” by association. To test this hypothesis more prec</context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>Lewis, David D., Yiming Yang, Tony G. Rose, and Fan Li. 2004. Rcv1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changliang Li</author>
<author>Bo Xu</author>
</authors>
<title>Gaowei Wu, Xiuying Wang, Wendong Ge, and Yan Li.</title>
<date>2014</date>
<booktitle>Computational Linguistics and Intelligent Text Processing.</booktitle>
<pages>128--137</pages>
<editor>In A. Gelbukh, editor,</editor>
<publisher>Springer,</publisher>
<marker>Li, Xu, 2014</marker>
<rawString>Li, Changliang, Bo Xu, Gaowei Wu, Xiuying Wang, Wendong Ge, and Yan Li. 2014. Obtaining better word representations via language transfer. In A. Gelbukh, editor, Computational Linguistics and Intelligent Text Processing. Springer, pages 128–137.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mu Li</author>
</authors>
<location>Yang Zhang, Muhua Zhu, and</location>
<marker>Li, </marker>
<rawString>Li, Mu, Yang Zhang, Muhua Zhu, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming Zhou</author>
</authors>
<title>Exploring distributional similarity based models for query spelling correction.</title>
<date>2006</date>
<booktitle>In Proceedings of ALC,</booktitle>
<pages>1025--1032</pages>
<marker>Zhou, 2006</marker>
<rawString>Ming Zhou. 2006. Exploring distributional similarity based models for query spelling correction. In Proceedings of ALC, pages 1025–1032.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology.</title>
<date>2013</date>
<booktitle>CoNLL-2013,</booktitle>
<pages>104</pages>
<location>Sofia.</location>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Luong, Minh-Thang, Richard Socher, and Christopher D. Manning. 2013. Better word representations with recursive neural networks for morphology. CoNLL-2013, page 104, Sofia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur B Markman</author>
<author>Edward J Wisniewski</author>
</authors>
<title>Similar and different: The differentiation of basic-level categories.</title>
<date>1997</date>
<journal>Journal of Experimental Psychology: Learning, Memory, and Cognition,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="19962" citStr="Markman and Wisniewski (1997)" startWordPosition="2984" endWordPosition="2987">from running-text or bag-of-words input better model association (Agirre et al. 2009; Levy and Goldberg 2014). • Models with larger context windows may learn representations that better capture association, whereas models with narrower windows better reflect similarity (Agirre et al. 2009; Kiela and Clark 2014). 2.2 Concepts, Part-of-Speech, and Concreteness Empirical studies have shown that the performance of both humans and distributional models depends on the POS category of the concepts learned. Gentner (2006) showed that children find verb concepts harder to learn than noun concepts, and Markman and Wisniewski (1997) present evidence that different cognitive operations are used when comparing two nouns or two verbs. Hill, Reichart, and Korhonen (2014) demonstrate differences in the ability of distributional models to acquire noun and verb semantics. Further, they show that these differences are greater for models that learn from both text and perceptual input (as with humans). In addition to POS category, differences in human and computational concept learning and representation have been attributed to the effects of concreteness, the extent to which a concept has a directly perceptible physical referent.</context>
<context position="75296" citStr="Markman and Wisniewski 1997" startWordPosition="11972" endWordPosition="11975">nouns than for verbs. To better understand the linguistic information exploited by models when acquiring concepts of different POS, we also computed performance on the POS subsets of SimLex-999 of the dependency-based model of Levy and Goldberg (2014) and the standard skipgram model, in which linguistic contexts are encoded as simple bagsof-words (BOW) (Mikolov et al. (2013a) [trained on the same Wikipedia text]). As shown in Figure 12, dependency-aware contexts yield the largest improvements for capturing verb similarity. This aligns with the cognitive theory of verbs as relational concepts (Markman and Wisniewski 1997) whose meanings rely on their interaction with (or dependency on) other words or concepts. It is also consistent with research on the automatic acquisition of verb semantics, in which syntactic features have proven particularly important (Sun, Korhonen, and Krymolowski 2008). Although a deeper exploration of these effects is beyond the scope of this work, this preliminary analysis adjectives verbs nouns Figure 12 The importance of dependency-focused contexts (in the Levy &amp; Goldberg model) for capturing concepts of different POS, when compared to a standard Skipgram (BOW) model trained on the s</context>
</contexts>
<marker>Markman, Wisniewski, 1997</marker>
<rawString>Markman, Arthur B. and Edward J. Wisniewski. 1997. Similar and different: The differentiation of basic-level categories. Journal of Experimental Psychology: Learning, Memory, and Cognition, 23(1):54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Chris Callison-Burch</author>
<author>Philip Resnik</author>
</authors>
<title>Improved statistical machine translation using monolinguallyderived paraphrases.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>381--390</pages>
<location>Edinburgh.</location>
<marker>Marton, Callison-Burch, Resnik, 2009</marker>
<rawString>Marton, Yuval, Chris Callison-Burch, and Philip Resnik. 2009. Improved statistical machine translation using monolinguallyderived paraphrases. In Proceedings of EMNLP, pages 381–390, Edinburgh.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ken McRae</author>
</authors>
<title>Saman Khalkhali,</title>
<location>and</location>
<marker>McRae, </marker>
<rawString>McRae, Ken, Saman Khalkhali, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Hare</author>
</authors>
<title>Semantic and associative relations in adolescents and young adults: Examining a tenuous dichotomy.</title>
<date>2012</date>
<booktitle>The Adolescent Brain: Learning, Reasoning, and Decision Making.</booktitle>
<pages>39--66</pages>
<editor>In Valerie F. Reyna, Sandra B. Chapman, Michael R. Dougherty, and Jere Ed Confrey, editors,</editor>
<publisher>American Psychological Association,</publisher>
<contexts>
<context position="12152" citStr="Hare 2012" startWordPosition="1776" endWordPosition="1777">ilarity is exemplified by the concept pairs [car, bike] and [car, petrol]. Car is said to be (semantically) similar to bike and associated with (but not similar to) petrol. Intuitively, car and bike can be understood as similar because of their common physical features (e.g., wheels), their common function (transport), or because they fall within a clearly definable category (modes of transport). In contrast, car and petrol are associated because they frequently occur together in space and language, in this case as a result of a clear functional relationship (Plaut 1995; McRae, Khalkhali, and Hare 2012). Association and similarity are neither mutually exclusive nor independent. Bike and car, for instance, are related to some degree by both relations. Because it is common in both the physical world and in language for distinct entities to interact, it is relatively easy to conceive of concept pairs, such as car and petrol, that are strongly associated but not similar. Identifying pairs of concepts for which the converse is true is comparatively more difficult. One exception is common concepts paired with low frequency synonyms, such as camel and dromedary. Because the essence of association i</context>
</contexts>
<marker>Hare, 2012</marker>
<rawString>Mary Hare. 2012. Semantic and associative relations in adolescents and young adults: Examining a tenuous dichotomy. In Valerie F. Reyna, Sandra B. Chapman, Michael R. Dougherty, and Jere Ed Confrey, editors, The Adolescent Brain: Learning, Reasoning, and Decision Making. American Psychological Association, pages 39–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olena Medelyan</author>
<author>David Milne</author>
<author>Catherine Legg</author>
<author>Ian H Witten</author>
</authors>
<title>Mining meaning from Wikipedia.</title>
<date>2009</date>
<journal>International Journal of Human-Computer Studies,</journal>
<volume>67</volume>
<issue>9</issue>
<contexts>
<context position="6550" citStr="Medelyan et al. 2009" startWordPosition="943" endWordPosition="946">s approaching resolution. However, circumstantial evidence suggests that distributional models are far from perfect. For instance, we are some way from automatically generated dictionaries, thesauri, or ontologies that can be used with the same confidence as their manually created equivalents. 1 For instance, Huang et al. (2012, pages 1, 4, 10) and Reisinger and Mooney (2010b, page 4) refer to MEN and/or WS-353 as “similarity data sets.” Others evaluate on both these association-based and genuine similarity-based gold standards with no reference to the fact that they measure different things (Medelyan et al. 2009; Li et al. 2014). 666 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models Motivated by these observations, in Section 3 we present SimLex-999, a gold standard resource for evaluating the ability of models to reflect similarity. SimLex-999 was produced by 500 paid native English speakers, recruited via Amazon Mechanical Turk,2 who were asked to rate the similarity, as opposed to association, of concepts via a simple visual interface. The choice of evaluation pairs in SimLex-999 was motivated by empirical evidence that humans represent concepts of distinct part-of-speech (POS) (</context>
</contexts>
<marker>Medelyan, Milne, Legg, Witten, 2009</marker>
<rawString>Medelyan, Olena, David Milne, Catherine Legg, and Ian H. Witten. 2009. Mining meaning from Wikipedia. International Journal of Human-Computer Studies, 67(9):716–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of International Conference of Learning Representations,</booktitle>
<location>Scottsdale, AZ.</location>
<contexts>
<context position="8395" citStr="Mikolov et al. (2013" startWordPosition="1211" endWordPosition="1214">hat participants found it unproblematic to quantify consistently the similarity of the full range of concepts and to distinguish it from association. Unlike existing data sets, SimLex-999 therefore contains a significant number of pairs, such as [movie, theater], which are strongly associated but receive low similarity scores. The second main contribution of this paper, presented in Section 5, is the evaluation of state-of-the-art distributional semantic models using SimLex-999. These include the well-known neural language models (NLMs) of Huang et al. (2012), Collobert and Weston (2008), and Mikolov et al. (2013a), which we compare with traditional vectorspace co-occurrence models (VSMs) (Turney and Pantel 2010) with and without dimensionality reduction (SVD) (Landauer and Dumais 1997). Our analyses demonstrate how SimLex-999 can be applied to uncover substantial differences in the ability of models to represent concepts of different types. Despite these differences, the models we consider each share the characteristic of being better able to capture association than similarity. We show that the difficulty of estimating similarity is driven primarily by those strongly associated pairs with a high (as</context>
<context position="61434" citStr="Mikolov et al. (2013" startWordPosition="9804" endWordPosition="9807">n the corpus: �Cs,d = max(0,1 − g(s, d) + g(sw, d)) wEV The combination of local and global contexts in the objective encourages the final word embeddings to reflect aspects of both the meaning of nearby words and of the documents in which those words appear. When learning from 990M words of Wikipedia text, Huang et al. report a Spearman correlation of p = 71.3 between the cosine similarity of their model embeddings and the WS-353 scores, which constitutes state-of-the-art performance for a NLM model on that data set. We downloaded these embeddings from the authors’ Web page.13 Mikolov et al. Mikolov et al. (2013a) present an architecture that learns word embeddings similar to those of standard NLMs but with no nonlinear hidden layer (resulting in a simpler scoring function). This enables faster representation learning for large vocabularies. Despite this simplification, the embeddings achieve state-of-theart performance on several semantic tasks including sentence completion and analogy modeling (Mikolov et al. 2013a, 2013b). For each word type w in the vocabulary V, the model learns both a “targetembedding” rw E Rd and a “context-embedding” ˆrw E Rd such that, given a target word, its ability to pre</context>
<context position="67182" citStr="Mikolov et al. (2013" startWordPosition="10716" endWordPosition="10719"> 14 Taken from the Python Natural Language Toolkit (Bird 2006). 15 This score, based on embeddings downloaded from the authors’ webpage, is notably lower than the score reported in Huang et al. (2012), mentioned in Section 5.1. 685 Computational Linguistics Volume 41, Number 4 Correlation ρ 0.75 0.50 0.25 0.00 0.623 0.3 Evaluation Gold Standard RN353 imLex−999 0.098 0.494 0.575 0.268 0.655 0.699 0.414 Huang et al. Collobert &amp; Weston Mikolov et al. Figure 7 Performance of NLMs on WS-353, MEN, and SimLex-999. All models are trained on Wikipedia; note that as Wikipedia is constantly growing, the Mikolov et al. (2013a) model exploited slightly more training data (:t1000M tokens) than the Huang et al. (2012) model (:t990M), which in turn exploited more than the Collobert and Weston (2008) model (:t852M). Dashed horizontal lines indicate the level of inter-annotator agreement for the three data sets. global context objective leads the Huang et al. (2012) model to focus on association rather than similarity. However, the true explanation may be less simple, since the Huang et al. (2012) model performs weakly on the association-based MEN data set. The Collobert and Weston (2008) model is more robust across WS</context>
<context position="72453" citStr="Mikolov et al. (2013" startWordPosition="11536" endWordPosition="11539">n at least in part by an increased ability to “distinguish” similarity from association. To understand better how the architecture of models captures information pertinent to similarity modeling, we performed two additional experiments using SimLex-999. These comparisons were also motivated by the hypotheses, made in previous studies and outlined in Section 2.1.2, that both dependency-informed input and smaller context windows encourage models to capture similarity rather than association. We tested the first hypothesis using the embeddings of Levy and Goldberg (2014), whose model extends the Mikolov et al. (2013a) model so that target-context training instances are extracted based on dependency-parsed rather than simple running text. As illustrated in Figure 9, the dependency-based embeddings outperform the original (running text) embeddings trained on the same corpus. Moreover, the comparatively large increase in the red bar compared to the gray bar suggests that an important part of the improvement of the dependency-based model derives from a greater ability to discern similarity from association. Our comparisons provided less support for the second (window size) hypothesis. As shown in Figure 10, </context>
<context position="75044" citStr="Mikolov et al. (2013" startWordPosition="11936" endWordPosition="11939">of SimLex-999. The window size for each model is indicated in parentheses. Inter-annotator agreement for each POS is indicated by the dashed horizontal line. than noun similarity by humans, but models estimate these ratings more accurately for nouns than for verbs. To better understand the linguistic information exploited by models when acquiring concepts of different POS, we also computed performance on the POS subsets of SimLex-999 of the dependency-based model of Levy and Goldberg (2014) and the standard skipgram model, in which linguistic contexts are encoded as simple bagsof-words (BOW) (Mikolov et al. (2013a) [trained on the same Wikipedia text]). As shown in Figure 12, dependency-aware contexts yield the largest improvements for capturing verb similarity. This aligns with the cognitive theory of verbs as relational concepts (Markman and Wisniewski 1997) whose meanings rely on their interaction with (or dependency on) other words or concepts. It is also consistent with research on the automatic acquisition of verb semantics, in which syntactic features have proven particularly important (Sun, Korhonen, and Krymolowski 2008). Although a deeper exploration of these effects is beyond the scope of t</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of International Conference of Learning Representations, Scottsdale, AZ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<location>Lake Tahoe, NV.</location>
<contexts>
<context position="8395" citStr="Mikolov et al. (2013" startWordPosition="1211" endWordPosition="1214">hat participants found it unproblematic to quantify consistently the similarity of the full range of concepts and to distinguish it from association. Unlike existing data sets, SimLex-999 therefore contains a significant number of pairs, such as [movie, theater], which are strongly associated but receive low similarity scores. The second main contribution of this paper, presented in Section 5, is the evaluation of state-of-the-art distributional semantic models using SimLex-999. These include the well-known neural language models (NLMs) of Huang et al. (2012), Collobert and Weston (2008), and Mikolov et al. (2013a), which we compare with traditional vectorspace co-occurrence models (VSMs) (Turney and Pantel 2010) with and without dimensionality reduction (SVD) (Landauer and Dumais 1997). Our analyses demonstrate how SimLex-999 can be applied to uncover substantial differences in the ability of models to represent concepts of different types. Despite these differences, the models we consider each share the characteristic of being better able to capture association than similarity. We show that the difficulty of estimating similarity is driven primarily by those strongly associated pairs with a high (as</context>
<context position="61434" citStr="Mikolov et al. (2013" startWordPosition="9804" endWordPosition="9807">n the corpus: �Cs,d = max(0,1 − g(s, d) + g(sw, d)) wEV The combination of local and global contexts in the objective encourages the final word embeddings to reflect aspects of both the meaning of nearby words and of the documents in which those words appear. When learning from 990M words of Wikipedia text, Huang et al. report a Spearman correlation of p = 71.3 between the cosine similarity of their model embeddings and the WS-353 scores, which constitutes state-of-the-art performance for a NLM model on that data set. We downloaded these embeddings from the authors’ Web page.13 Mikolov et al. Mikolov et al. (2013a) present an architecture that learns word embeddings similar to those of standard NLMs but with no nonlinear hidden layer (resulting in a simpler scoring function). This enables faster representation learning for large vocabularies. Despite this simplification, the embeddings achieve state-of-theart performance on several semantic tasks including sentence completion and analogy modeling (Mikolov et al. 2013a, 2013b). For each word type w in the vocabulary V, the model learns both a “targetembedding” rw E Rd and a “context-embedding” ˆrw E Rd such that, given a target word, its ability to pre</context>
<context position="67182" citStr="Mikolov et al. (2013" startWordPosition="10716" endWordPosition="10719"> 14 Taken from the Python Natural Language Toolkit (Bird 2006). 15 This score, based on embeddings downloaded from the authors’ webpage, is notably lower than the score reported in Huang et al. (2012), mentioned in Section 5.1. 685 Computational Linguistics Volume 41, Number 4 Correlation ρ 0.75 0.50 0.25 0.00 0.623 0.3 Evaluation Gold Standard RN353 imLex−999 0.098 0.494 0.575 0.268 0.655 0.699 0.414 Huang et al. Collobert &amp; Weston Mikolov et al. Figure 7 Performance of NLMs on WS-353, MEN, and SimLex-999. All models are trained on Wikipedia; note that as Wikipedia is constantly growing, the Mikolov et al. (2013a) model exploited slightly more training data (:t1000M tokens) than the Huang et al. (2012) model (:t990M), which in turn exploited more than the Collobert and Weston (2008) model (:t852M). Dashed horizontal lines indicate the level of inter-annotator agreement for the three data sets. global context objective leads the Huang et al. (2012) model to focus on association rather than similarity. However, the true explanation may be less simple, since the Huang et al. (2012) model performs weakly on the association-based MEN data set. The Collobert and Weston (2008) model is more robust across WS</context>
<context position="72453" citStr="Mikolov et al. (2013" startWordPosition="11536" endWordPosition="11539">n at least in part by an increased ability to “distinguish” similarity from association. To understand better how the architecture of models captures information pertinent to similarity modeling, we performed two additional experiments using SimLex-999. These comparisons were also motivated by the hypotheses, made in previous studies and outlined in Section 2.1.2, that both dependency-informed input and smaller context windows encourage models to capture similarity rather than association. We tested the first hypothesis using the embeddings of Levy and Goldberg (2014), whose model extends the Mikolov et al. (2013a) model so that target-context training instances are extracted based on dependency-parsed rather than simple running text. As illustrated in Figure 9, the dependency-based embeddings outperform the original (running text) embeddings trained on the same corpus. Moreover, the comparatively large increase in the red bar compared to the gray bar suggests that an important part of the improvement of the dependency-based model derives from a greater ability to discern similarity from association. Our comparisons provided less support for the second (window size) hypothesis. As shown in Figure 10, </context>
<context position="75044" citStr="Mikolov et al. (2013" startWordPosition="11936" endWordPosition="11939">of SimLex-999. The window size for each model is indicated in parentheses. Inter-annotator agreement for each POS is indicated by the dashed horizontal line. than noun similarity by humans, but models estimate these ratings more accurately for nouns than for verbs. To better understand the linguistic information exploited by models when acquiring concepts of different POS, we also computed performance on the POS subsets of SimLex-999 of the dependency-based model of Levy and Goldberg (2014) and the standard skipgram model, in which linguistic contexts are encoded as simple bagsof-words (BOW) (Mikolov et al. (2013a) [trained on the same Wikipedia text]). As shown in Figure 12, dependency-aware contexts yield the largest improvements for capturing verb similarity. This aligns with the cognitive theory of verbs as relational concepts (Markman and Wisniewski 1997) whose meanings rely on their interaction with (or dependency on) other words or concepts. It is also consistent with research on the automatic acquisition of verb semantics, in which syntactic features have proven particularly important (Sun, Korhonen, and Krymolowski 2008). Although a deeper exploration of these effects is beyond the scope of t</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119, Lake Tahoe, NV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="16453" citStr="Navigli 2009" startWordPosition="2454" endWordPosition="2455">screpancy between association strength and similarity. 2.1.1 Association and Similarity in NLP. As noted in the Introduction, the similarity/association distinction is not only of interest to researchers in psychology or linguistics. Models of similarity are particularly applicable to various NLP tasks, such as lexical resource building, semantic parsing, and machine translation (Haghighi et al. 2008; He et al. 2008; Marton, Callison-Burch, and Resnik 2009; Beltagy, Erk, and Mooney 2014). Models of association, on the other hand, may be better suited to tasks such as wordsense disambiguation (Navigli 2009), and applications such as text classification (Phan, Nguyen, and Horiguchi 2008) in which the target classes correspond to topical domains such as agriculture or sport (Rose, Stevenson, and Whitehead 2002). Much recent research in distributional semantics does not distinguish between association and similarity in a principled way (see, e.g., Reisinger and Mooney 2010b; Huang et al. 2012; Luong, Socher, and Manning 2013).3 One exception is Turney (2012), who constructs two distributional models with different features and parameter settings, explicitly designed to capture either similarity or </context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Navigli, Roberto. 2009. Word sense disambiguation: A survey. ACM Computing Surveys (CSUR), 41(2):10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L Nelson</author>
<author>Cathy L McEvoy</author>
<author>Thomas A Schreiber</author>
</authors>
<title>The University of South Florida free association, rhyme, and word fragment norms.</title>
<date>2004</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<volume>36</volume>
<issue>3</issue>
<marker>Nelson, McEvoy, Schreiber, 2004</marker>
<rawString>Nelson, Douglas L., Cathy L. McEvoy, and Thomas A. Schreiber. 2004. The University of South Florida free association, rhyme, and word fragment norms. Behavior Research Methods, Instruments, &amp; Computers, 36(3):402–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Ulrike Pad´o</author>
<author>Katrin Erk</author>
</authors>
<title>Flexible, corpus-based modelling of human plausibility judgements.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>400--409</pages>
<location>Prague.</location>
<marker>Pad´o, Pad´o, Erk, 2007</marker>
<rawString>Pad´o, Sebastian, Ulrike Pad´o, and Katrin Erk. 2007. Flexible, corpus-based modelling of human plausibility judgements. In Proceedings of EMNLP-CoNLL, pages 400–409, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allan Paivio</author>
</authors>
<title>Dual coding theory: Retrospect and current status.</title>
<date>1991</date>
<journal>Canadian Journal of Psychology/Revue canadienne de psychologie,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="20686" citStr="Paivio 1991" startWordPosition="3095" endWordPosition="3096">art, and Korhonen (2014) demonstrate differences in the ability of distributional models to acquire noun and verb semantics. Further, they show that these differences are greater for models that learn from both text and perceptual input (as with humans). In addition to POS category, differences in human and computational concept learning and representation have been attributed to the effects of concreteness, the extent to which a concept has a directly perceptible physical referent. On the cognitive side, these “concreteness effects” are well established, even if the causes are still debated (Paivio 1991; Hill, Korhonen, and Bentz 2014). Concreteness has also been associated with differential performance in computational text-based (Hill, Kiela, and Korhonen 2013) and multi-modal semantic models (Kiela et al. 2014). 670 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models 2.3 Existing Gold Standards and Evaluation Resources For brevity, we do not exhaustively review all methods that have been used to evaluate semantic models, but instead focus on the similarity or association-based gold standards that are most commonly applied in recent work in NLP. In each case, we consider ho</context>
<context position="46670" citStr="Paivio 1991" startWordPosition="7170" endWordPosition="7171">nt suggests that participants were able to understand the (single) characterization of similarity presented in the instructions and to apply it to concepts of various types consistently. This conclusion was supported by inspection of the brief feedback offered by the majority of annotators in a final text field in the questionnaire: 78% expressed sentiment that the test was clear, easy to complete, or some similar sentiment. Interestingly, as shown in Figure 4 (left), agreement was not uniform across the concept types. Contrary to what might be expected given established concreteness effects (Paivio 1991), we observed not only higher inter-rater agreement but also less per-pair variability for abstract rather than concrete concepts.11 Strikingly, the highest inter-rater consistency and lowest per-pair variation (defined as the inverse of the standard deviation of all ratings for that pair) was observed on adjective pairs. Although we are unsure exactly what drives this effect, a possible cause 10 Reported at http://clic.cimec.unitn.it/~elia.bruni/MEN. Itis reasonable to assume that actual agreement on MEN may be somewhat lower than 0.68, given the small sample size and the expertise of the rat</context>
</contexts>
<marker>Paivio, 1991</marker>
<rawString>Paivio, Allan. 1991. Dual coding theory: Retrospect and current status. Canadian Journal of Psychology/Revue canadienne de psychologie, 45(3):255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet:: Similarity: Measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Demonstration Papers at HLT-NAACL 2004,</booktitle>
<pages>38--41</pages>
<location>New York, NY.</location>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Pedersen, Ted, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet:: Similarity: Measuring the relatedness of concepts. In Demonstration Papers at HLT-NAACL 2004, pages 38–41, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuan-Hieu Phan</author>
<author>Le-Minh Nguyen</author>
<author>Susumu Horiguchi</author>
</authors>
<title>Learning to classify short and sparse text &amp; Web with hidden topics from large-scale data collections.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th International Conference on World Wide Web,</booktitle>
<pages>91--100</pages>
<location>Beijing.</location>
<marker>Phan, Nguyen, Horiguchi, 2008</marker>
<rawString>Phan, Xuan-Hieu, Le-Minh Nguyen, and Susumu Horiguchi. 2008. Learning to classify short and sparse text &amp; Web with hidden topics from large-scale data collections. In Proceedings of the 17th International Conference on World Wide Web, pages 91–100, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David C Plaut</author>
</authors>
<title>Semantic and associative priming in a distributed attractor network.</title>
<date>1995</date>
<booktitle>In Proceedings of CogSci,</booktitle>
<volume>17</volume>
<pages>37--42</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="12118" citStr="Plaut 1995" startWordPosition="1771" endWordPosition="1772">erence between association and similarity is exemplified by the concept pairs [car, bike] and [car, petrol]. Car is said to be (semantically) similar to bike and associated with (but not similar to) petrol. Intuitively, car and bike can be understood as similar because of their common physical features (e.g., wheels), their common function (transport), or because they fall within a clearly definable category (modes of transport). In contrast, car and petrol are associated because they frequently occur together in space and language, in this case as a result of a clear functional relationship (Plaut 1995; McRae, Khalkhali, and Hare 2012). Association and similarity are neither mutually exclusive nor independent. Bike and car, for instance, are related to some degree by both relations. Because it is common in both the physical world and in language for distinct entities to interact, it is relatively easy to conceive of concept pairs, such as car and petrol, that are strongly associated but not similar. Identifying pairs of concepts for which the converse is true is comparatively more difficult. One exception is common concepts paired with low frequency synonyms, such as camel and dromedary. Be</context>
</contexts>
<marker>Plaut, 1995</marker>
<rawString>Plaut, David C. 1995. Semantic and associative priming in a distributed attractor network. In Proceedings of CogSci, volume 17, pages 37–42, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Recchia</author>
<author>Michael N Jones</author>
</authors>
<title>More data trumps smarter algorithms: Comparing pointwise mutual information with latent semantic analysis.</title>
<date>2009</date>
<journal>Behavior Research Methods,</journal>
<volume>41</volume>
<issue>3</issue>
<contexts>
<context position="64902" citStr="Recchia and Jones 2009" startWordPosition="10358" endWordPosition="10361">NLMs, we constructed a vector space model following the guidelines for optimal performance outlined by Kiela and Clark (2014). After extracting the 2,000 most frequent word tokens in the corpus that are not in a common list of stopwords14 as features, we populated a matrix of co-occurrence counts with a row for each of the concepts in some pair in our evaluation sets, and a column for each of the features. Co-occurrence was counted within a specified window size, although never across a sentence boundary. This resulting matrix was then weighted according to Pointwise Mutual Information (PMI) (Recchia and Jones 2009). The rows of the resulting matrix constitute the vector representations of the concepts. SVD. As proposed initially in Landauer and Dumais (1997), we also experimented with models in which SVD (Golub and Reinsch 1970) is applied to the PMI-weighted VSM matrix, reducing the dimension of each concept representation to 300 (which yielded best results after experimenting, as before, with 100–500 dimension vectors). For each model described in this section, we calculate similarity as the cosine similarity between the (vector) representations learned by that model. 5.2 Results In experimenting with</context>
</contexts>
<marker>Recchia, Jones, 2009</marker>
<rawString>Recchia, Gabriel and Michael N. Jones. 2009. More data trumps smarter algorithms: Comparing pointwise mutual information with latent semantic analysis. Behavior Research Methods, 41(3):647–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond Mooney</author>
</authors>
<title>A mixture model with sharing for lexical semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1173--1182</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="6307" citStr="Reisinger and Mooney (2010" startWordPosition="906" endWordPosition="909">(Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010). Based on this established principle and the current evaluations, it would therefore be reasonable to conclude that the problem of representation learning, at least for similarity modeling, is approaching resolution. However, circumstantial evidence suggests that distributional models are far from perfect. For instance, we are some way from automatically generated dictionaries, thesauri, or ontologies that can be used with the same confidence as their manually created equivalents. 1 For instance, Huang et al. (2012, pages 1, 4, 10) and Reisinger and Mooney (2010b, page 4) refer to MEN and/or WS-353 as “similarity data sets.” Others evaluate on both these association-based and genuine similarity-based gold standards with no reference to the fact that they measure different things (Medelyan et al. 2009; Li et al. 2014). 666 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models Motivated by these observations, in Section 3 we present SimLex-999, a gold standard resource for evaluating the ability of models to reflect similarity. SimLex-999 was produced by 500 paid native English speakers, recruited via Amazon Mechanical Turk,2 who were ask</context>
<context position="16823" citStr="Reisinger and Mooney 2010" startWordPosition="2506" endWordPosition="2509">and machine translation (Haghighi et al. 2008; He et al. 2008; Marton, Callison-Burch, and Resnik 2009; Beltagy, Erk, and Mooney 2014). Models of association, on the other hand, may be better suited to tasks such as wordsense disambiguation (Navigli 2009), and applications such as text classification (Phan, Nguyen, and Horiguchi 2008) in which the target classes correspond to topical domains such as agriculture or sport (Rose, Stevenson, and Whitehead 2002). Much recent research in distributional semantics does not distinguish between association and similarity in a principled way (see, e.g., Reisinger and Mooney 2010b; Huang et al. 2012; Luong, Socher, and Manning 2013).3 One exception is Turney (2012), who constructs two distributional models with different features and parameter settings, explicitly designed to capture either similarity or association. Using the output of these two models as input to a logistic regression classifier, Turney predicts whether two 3 Several papers that take a knowledge-based or symbolic approach to meaning do address the similarity/association issue (Budanitsky and Hirst 2006). 669 Computational Linguistics Volume 41, Number 4 concepts are associated, similar, or both, wit</context>
<context position="24381" citStr="Reisinger and Mooney 2010" startWordPosition="3653" endWordPosition="3656">s also noted by the data set authors. See www.cs.technion.ac.il/~gabr/resources/ data/wordsim353/. 671 Computational Linguistics Volume 41, Number 4 of Collobert and Weston (2008) and Huang et al. (2012) reach, or even surpass, the inter-annotator agreement ceiling in estimating the WS-353 scores. Huang et al. report a Spearman correlation of p = 0.713 between their model output and WS-353. This is 10 percentage points higher than inter-annotator agreement (p = 0.611) when defined as the average pairwise correlation between two annotators, as is common in NLP work (Pad´o, Pad´o, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014). It could be argued that a different comparison is more appropriate: Because the model is compared to the gold-standard average across all annotators, we should compare a single annotator with the (almost) gold-standard average over all other annotators. Based on this metric the average performance of an annotator on WS-353 is p = 0.756, which is still only marginally better than the best automatic method.6 Thus, at least according to the established wisdom in NLP evaluation (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010), the strength of the conclusions </context>
<context position="45517" citStr="Reisinger and Mooney 2010" startWordPosition="6989" endWordPosition="6992">et All Concrete Abstract Adjective Noun Verb Figure 4 Left: Inter-annotator agreement, measured by average pairwise Spearman p correlation, for ratings of concept types in SimLex-999. Right: Response consistency, reflecting the standard deviation of annotator ratings for each pair, averaged over all pairs in the concept category. valid notion of similarity was understood by the annotators, in that they were able to accurately separate similarity from association. 4.1 Inter-Annotator Agreement As in previous annotation or data collection for computational semantics (Pad´o, Pad´o, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014) we computed the inter-rater agreement as the average of pairwise Spearman p correlations between the ratings of all respondents. Overall agreement was p = 0.67. This compares favorably with the agreement on WS-353 (p = 0.61 using the same method). The design of the MEN rating system precludes a conventional calculation of inter-rater agreement (Bruni et al. 2012b). However, two of the creators of MEN who independently rated the data set achieved an agreement of p = 0.68.10 The SimLex-999 inter-rater agreement suggests that participants were able to understand the (</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Reisinger, Joseph and Raymond Mooney. 2010a. A mixture model with sharing for lexical semantics. In Proceedings of EMNLP, pages 1173–1182, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>109--117</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="6307" citStr="Reisinger and Mooney (2010" startWordPosition="906" endWordPosition="909">(Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010). Based on this established principle and the current evaluations, it would therefore be reasonable to conclude that the problem of representation learning, at least for similarity modeling, is approaching resolution. However, circumstantial evidence suggests that distributional models are far from perfect. For instance, we are some way from automatically generated dictionaries, thesauri, or ontologies that can be used with the same confidence as their manually created equivalents. 1 For instance, Huang et al. (2012, pages 1, 4, 10) and Reisinger and Mooney (2010b, page 4) refer to MEN and/or WS-353 as “similarity data sets.” Others evaluate on both these association-based and genuine similarity-based gold standards with no reference to the fact that they measure different things (Medelyan et al. 2009; Li et al. 2014). 666 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models Motivated by these observations, in Section 3 we present SimLex-999, a gold standard resource for evaluating the ability of models to reflect similarity. SimLex-999 was produced by 500 paid native English speakers, recruited via Amazon Mechanical Turk,2 who were ask</context>
<context position="16823" citStr="Reisinger and Mooney 2010" startWordPosition="2506" endWordPosition="2509">and machine translation (Haghighi et al. 2008; He et al. 2008; Marton, Callison-Burch, and Resnik 2009; Beltagy, Erk, and Mooney 2014). Models of association, on the other hand, may be better suited to tasks such as wordsense disambiguation (Navigli 2009), and applications such as text classification (Phan, Nguyen, and Horiguchi 2008) in which the target classes correspond to topical domains such as agriculture or sport (Rose, Stevenson, and Whitehead 2002). Much recent research in distributional semantics does not distinguish between association and similarity in a principled way (see, e.g., Reisinger and Mooney 2010b; Huang et al. 2012; Luong, Socher, and Manning 2013).3 One exception is Turney (2012), who constructs two distributional models with different features and parameter settings, explicitly designed to capture either similarity or association. Using the output of these two models as input to a logistic regression classifier, Turney predicts whether two 3 Several papers that take a knowledge-based or symbolic approach to meaning do address the similarity/association issue (Budanitsky and Hirst 2006). 669 Computational Linguistics Volume 41, Number 4 concepts are associated, similar, or both, wit</context>
<context position="24381" citStr="Reisinger and Mooney 2010" startWordPosition="3653" endWordPosition="3656">s also noted by the data set authors. See www.cs.technion.ac.il/~gabr/resources/ data/wordsim353/. 671 Computational Linguistics Volume 41, Number 4 of Collobert and Weston (2008) and Huang et al. (2012) reach, or even surpass, the inter-annotator agreement ceiling in estimating the WS-353 scores. Huang et al. report a Spearman correlation of p = 0.713 between their model output and WS-353. This is 10 percentage points higher than inter-annotator agreement (p = 0.611) when defined as the average pairwise correlation between two annotators, as is common in NLP work (Pad´o, Pad´o, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014). It could be argued that a different comparison is more appropriate: Because the model is compared to the gold-standard average across all annotators, we should compare a single annotator with the (almost) gold-standard average over all other annotators. Based on this metric the average performance of an annotator on WS-353 is p = 0.756, which is still only marginally better than the best automatic method.6 Thus, at least according to the established wisdom in NLP evaluation (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010), the strength of the conclusions </context>
<context position="45517" citStr="Reisinger and Mooney 2010" startWordPosition="6989" endWordPosition="6992">et All Concrete Abstract Adjective Noun Verb Figure 4 Left: Inter-annotator agreement, measured by average pairwise Spearman p correlation, for ratings of concept types in SimLex-999. Right: Response consistency, reflecting the standard deviation of annotator ratings for each pair, averaged over all pairs in the concept category. valid notion of similarity was understood by the annotators, in that they were able to accurately separate similarity from association. 4.1 Inter-Annotator Agreement As in previous annotation or data collection for computational semantics (Pad´o, Pad´o, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014) we computed the inter-rater agreement as the average of pairwise Spearman p correlations between the ratings of all respondents. Overall agreement was p = 0.67. This compares favorably with the agreement on WS-353 (p = 0.61 using the same method). The design of the MEN rating system precludes a conventional calculation of inter-rater agreement (Bruni et al. 2012b). However, two of the creators of MEN who independently rated the data set achieved an agreement of p = 0.68.10 The SimLex-999 inter-rater agreement suggests that participants were able to understand the (</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Reisinger, Joseph and Raymond J. Mooney. 2010b. Multi-prototype vector-space models of word meaning. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 109–117, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="39595" citStr="Resnik 1995" startWordPosition="6029" endWordPosition="6030"> representation of a given concept would have required evaluating that concept in each of its given contexts, necessitating many more cases and a far greater annotation effort. Third, in the (infrequent) case that some concept c1 in an evaluation pair (c1,c2) is genuinely (etymologically) polysemous, c2 can provide sufficient context to disambiguate c1.9 9 This is supported by the fact that the WordNet-based methods that perform best at modeling human ratings model the similarity between concepts c1 and c2 as the minimum of all pairwise distances between the senses of c1 and the senses of c2 (Resnik 1995; Pedersen, Patwardhan, and Michelizzi 2004). 676 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models Figure 3 A group of noun pairs to be rated by moving the sliders. The rating slider was initially at position 0, and it was possible to attribute a rating of 0, although it was necessary to have actively moved the slider to that position to proceed to the next page. Finally, the POS grouping of pairs in the survey can also serve to disambiguate in the case that the conflicting senses of the polysemous concept are of differing POS categories. 3.4 Questionnaire Structure Each par</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Resnik, Philip. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Jimmy Lin</author>
</authors>
<title>11 evaluations of NLP systems. The handbook of computational linguistics and natural language processing,</title>
<date>2010</date>
<pages>57--271</pages>
<contexts>
<context position="5738" citStr="Resnik and Lin 2010" startWordPosition="823" endWordPosition="826">actually measure.1 Although certain smaller gold standards—those of Rubenstein and Goodenough (1965) (RG) and Agirre et al. (2009) (WS-Sim)—do focus clearly on similarity, these resources suffer from other important limitations. For instance, as we show, and as is also the case for WS-353 and MEN, state-of-the-art models have reached the average performance of a human annotator on these evaluations. It is common practice in NLP to define the upper limit for automated performance on an evaluation as the average human performance or inter-annotator agreement (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010). Based on this established principle and the current evaluations, it would therefore be reasonable to conclude that the problem of representation learning, at least for similarity modeling, is approaching resolution. However, circumstantial evidence suggests that distributional models are far from perfect. For instance, we are some way from automatically generated dictionaries, thesauri, or ontologies that can be used with the same confidence as their manually created equivalents. 1 For instance, Huang et al. (2012, pages 1, 4, 10) and Reisinger and Mooney (2010b, page 4) refer to MEN and/or </context>
<context position="24947" citStr="Resnik and Lin 2010" startWordPosition="3743" endWordPosition="3746">(Pad´o, Pad´o, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014). It could be argued that a different comparison is more appropriate: Because the model is compared to the gold-standard average across all annotators, we should compare a single annotator with the (almost) gold-standard average over all other annotators. Based on this metric the average performance of an annotator on WS-353 is p = 0.756, which is still only marginally better than the best automatic method.6 Thus, at least according to the established wisdom in NLP evaluation (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010), the strength of the conclusions that can be inferred from improvements on WS-353 is limited. At the same time, however, state-of-the-art distributional models are clearly not perfect representation-learning or even similarity estimation engines, as evidenced by the fact they cannot yet be applied, for instance, to generate flawless lexical resources (Alfonseca and Manandhar 2002). WS-Sim. WS-Sim is the set of pairs in WS-353 identified by Agirre et al. (2009) as either containing similar or unrelated (neither similar nor associated) concepts. The ratings in WS-Sim are mapped directly from WS</context>
</contexts>
<marker>Resnik, Lin, 2010</marker>
<rawString>Resnik, Philip and Jimmy Lin. 2010. 11 evaluations of NLP systems. The handbook of computational linguistics and natural language processing, 57:271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleanor Rosch</author>
<author>Carol Simpson</author>
<author>R Scott Miller</author>
</authors>
<title>Structural bases of typicality effects.</title>
<date>1976</date>
<journal>Journal of Experimental Psychology: Human Perception and Performance,</journal>
<volume>2</volume>
<issue>4</issue>
<marker>Rosch, Simpson, Miller, 1976</marker>
<rawString>Rosch, Eleanor, Carol Simpson, and R. Scott Miller. 1976. Structural bases of typicality effects. Journal of Experimental Psychology: Human Perception and Performance, 2(4):491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Rose</author>
<author>Mark Stevenson</author>
<author>Miles Whitehead</author>
</authors>
<title>The Reuters corpus volume 1—from yesterday’s news to tomorrow’s language resources.</title>
<date>2002</date>
<booktitle>In LREC,</booktitle>
<volume>2</volume>
<pages>827--832</pages>
<location>Las Palmas.</location>
<marker>Rose, Stevenson, Whitehead, 2002</marker>
<rawString>Rose, Tony, Mark Stevenson, and Miles Whitehead. 2002. The Reuters corpus volume 1—from yesterday’s news to tomorrow’s language resources. In LREC, volume 2, pages 827–832, Las Palmas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="5218" citStr="Rubenstein and Goodenough (1965)" startWordPosition="740" endWordPosition="743">measure the ability of models to reflect similarity. In particular, in both WS353 and MEN, pairs of words with associated meaning, such as coffee and cup (rating = 6.810), telephone and communication (7.510), or movie and theater (7.710), receive a high rating regardless of whether or not their constituents are similar. Thus, the utility of such resources to the development and application of similarity models is limited, a problem exacerbated by the fact that many researchers appear unaware of what their evaluation resources actually measure.1 Although certain smaller gold standards—those of Rubenstein and Goodenough (1965) (RG) and Agirre et al. (2009) (WS-Sim)—do focus clearly on similarity, these resources suffer from other important limitations. For instance, as we show, and as is also the case for WS-353 and MEN, state-of-the-art models have reached the average performance of a human annotator on these evaluations. It is common practice in NLP to define the upper limit for automated performance on an evaluation as the average human performance or inter-annotator agreement (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010). Based on this established principle and the current evaluations, it would ther</context>
<context position="28121" citStr="Rubenstein and Goodenough 1965" startWordPosition="4237" endWordPosition="4240">iation. However, although limitation (1) of WS-353 is therefore avoided, RG still suffers from limitation (2): By inspection, 6 Individual annotator responses for WS-353 were downloaded from www.cs.technion.ac.il/~gabr/ resources/data/wordsim353. 672 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models it is clear that the low similarity pairs in RG are not associated. A further limitation is that distributional models now achieve better performance on RG (correlations of up to Pearson r = 0.86 [Hassan and Mihalcea 2011]) than the reported inter-annotator agreement of r = 0.85 (Rubenstein and Goodenough 1965). Finally, the size of RG renders it an even less comprehensive evaluation than WS-Sim. The MEN Test Collection. A larger data set, MEN (Bruni et al. 2012a), is used in a handful of recent studies (Bruni et al. 2012b; Bernardi et al. 2013). As with WS-353, both terms similarity and relatedness are used by the authors when describing MEN, although the annotators were expressly asked to rate pairs according to relatedness.7 The construction of MEN differed from RG and WS-353 in that each pair was only considered by one rater, who ranked it for relatedness relative to 50 other pairs in the data s</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Rubenstein, Herbert and John B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning grounded meaning representations with autoencoders.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Sofia.</location>
<contexts>
<context position="24409" citStr="Silberer and Lapata 2014" startWordPosition="3657" endWordPosition="3660"> authors. See www.cs.technion.ac.il/~gabr/resources/ data/wordsim353/. 671 Computational Linguistics Volume 41, Number 4 of Collobert and Weston (2008) and Huang et al. (2012) reach, or even surpass, the inter-annotator agreement ceiling in estimating the WS-353 scores. Huang et al. report a Spearman correlation of p = 0.713 between their model output and WS-353. This is 10 percentage points higher than inter-annotator agreement (p = 0.611) when defined as the average pairwise correlation between two annotators, as is common in NLP work (Pad´o, Pad´o, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014). It could be argued that a different comparison is more appropriate: Because the model is compared to the gold-standard average across all annotators, we should compare a single annotator with the (almost) gold-standard average over all other annotators. Based on this metric the average performance of an annotator on WS-353 is p = 0.756, which is still only marginally better than the best automatic method.6 Thus, at least according to the established wisdom in NLP evaluation (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010), the strength of the conclusions that can be inferred from im</context>
<context position="45545" citStr="Silberer and Lapata 2014" startWordPosition="6993" endWordPosition="6996">ective Noun Verb Figure 4 Left: Inter-annotator agreement, measured by average pairwise Spearman p correlation, for ratings of concept types in SimLex-999. Right: Response consistency, reflecting the standard deviation of annotator ratings for each pair, averaged over all pairs in the concept category. valid notion of similarity was understood by the annotators, in that they were able to accurately separate similarity from association. 4.1 Inter-Annotator Agreement As in previous annotation or data collection for computational semantics (Pad´o, Pad´o, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014) we computed the inter-rater agreement as the average of pairwise Spearman p correlations between the ratings of all respondents. Overall agreement was p = 0.67. This compares favorably with the agreement on WS-353 (p = 0.61 using the same method). The design of the MEN rating system precludes a conventional calculation of inter-rater agreement (Bruni et al. 2012b). However, two of the creators of MEN who independently rated the data set achieved an agreement of p = 0.68.10 The SimLex-999 inter-rater agreement suggests that participants were able to understand the (single) characterization of </context>
</contexts>
<marker>Silberer, Lapata, 2014</marker>
<rawString>Silberer, Carina and Mirella Lapata. 2014. Learning grounded meaning representations with autoencoders. In Proceedings of ACL, Sofia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
<author>Yuval Krymolowski</author>
</authors>
<title>Verb class discovery from rich syntactic data.</title>
<date>2008</date>
<booktitle>Computational Linguistics and Intelligent Text processing.</booktitle>
<pages>16--27</pages>
<editor>In A. Gelbukh, editor,</editor>
<publisher>Springer,</publisher>
<marker>Sun, Korhonen, Krymolowski, 2008</marker>
<rawString>Sun, Lin, Anna Korhonen, and Yuval Krymolowski. 2008. Verb class discovery from rich syntactic data. In A. Gelbukh, editor, Computational Linguistics and Intelligent Text processing. Springer, pages 16–27.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Joseph Turian</author>
</authors>
<location>Lev Ratinov, and</location>
<marker>Turian, </marker>
<rawString>Turian, Joseph, Lev Ratinov, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>384--394</pages>
<location>Uppsala.</location>
<contexts>
<context position="59601" citStr="Bengio 2010" startWordPosition="9501" endWordPosition="9502">lves updating the parameters of the function f and the entries of the vector representations vw such that f (s) is larger than f (sw) for any w in V, other than the correct final word of s. This corresponds to minimizing the sum of the following sentence objectives Cs over all sentences in the input corpus, which is achieved via (mini-batch) stochastic gradient descent: �Cs = max(0,1 − f (s) + f (sw)) w∈V The relatively low-dimension, dense (vector) representations learned by this model and the other NLMs introduced in this section are sometimes referred to as embeddings (Turian, Ratinov, and Bengio 2010). Collobert and Weston (2008) train their models on 852 million words of text from a 2007 dump of Wikipedia and the RCV1 Corpus (Lewis et al. 2004) and use their embeddings to achieve state-of-the-art results on a variety of NLP tasks. We downloaded the embeddings directly from the authors’ Web page.12 Huang et al. Huang et al. (2012) present a NLM that learns word embeddings to maximize the likelihood of predicting the last word in a sentence s based on (i) the previous words in that sentence (local context, as with Collobert and Weston [2008]) and (ii) the document d in which that word occur</context>
</contexts>
<marker>Bengio, 2010</marker>
<rawString>Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of ACL, pages 384–394, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Domain and function: A dual-space model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal</journal>
<volume>179</volume>
<issue>44</issue>
<contexts>
<context position="3364" citStr="Turney 2012" startWordPosition="476" endWordPosition="477">ws, etc.). Such anomalies also exist in other gold standards such as the MEN data set (Bruni et al. 2012a). As a consequence, these evaluations effectively penalize models for learning the evident truth that coffee and cup are dissimilar. Although clearly different, coffee and cup are very much related. The psychological literature refers to the conceptual relationship between these concepts as association, although it has been given a range of names including relatedness (Budanitsky and Hirst 2006; Agirre et al. 2009), topical similarity (Hatzivassiloglou et al. 2001), and domain similarity (Turney 2012). Association contrasts with similarity, the relation connecting cup and mug (Tversky 1977). At its strongest, the similarity relation is exemplified by pairs of synonyms; words with identical referents. Computational models that effectively capture similarity as distinct from association have numerous applications. Such models are used for the automatic generation of dictionaries, thesauri, ontologies, and language correction tools (Biemann 2005; Cimiano, Hotho, and Staab 2005; Li et al. 2006). Machine translation systems, which aim to define mappings between fragments of different languages </context>
<context position="16910" citStr="Turney (2012)" startWordPosition="2522" endWordPosition="2523">009; Beltagy, Erk, and Mooney 2014). Models of association, on the other hand, may be better suited to tasks such as wordsense disambiguation (Navigli 2009), and applications such as text classification (Phan, Nguyen, and Horiguchi 2008) in which the target classes correspond to topical domains such as agriculture or sport (Rose, Stevenson, and Whitehead 2002). Much recent research in distributional semantics does not distinguish between association and similarity in a principled way (see, e.g., Reisinger and Mooney 2010b; Huang et al. 2012; Luong, Socher, and Manning 2013).3 One exception is Turney (2012), who constructs two distributional models with different features and parameter settings, explicitly designed to capture either similarity or association. Using the output of these two models as input to a logistic regression classifier, Turney predicts whether two 3 Several papers that take a knowledge-based or symbolic approach to meaning do address the similarity/association issue (Budanitsky and Hirst 2006). 669 Computational Linguistics Volume 41, Number 4 concepts are associated, similar, or both, with 61% accuracy. However, in the absence of a gold standard covering the full range of s</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>Turney, Peter D. 2012. Domain and function: A dual-space model of semantic relations and compositions. Journal 179(44):533–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="8497" citStr="Turney and Pantel 2010" startWordPosition="1225" endWordPosition="1228">f concepts and to distinguish it from association. Unlike existing data sets, SimLex-999 therefore contains a significant number of pairs, such as [movie, theater], which are strongly associated but receive low similarity scores. The second main contribution of this paper, presented in Section 5, is the evaluation of state-of-the-art distributional semantic models using SimLex-999. These include the well-known neural language models (NLMs) of Huang et al. (2012), Collobert and Weston (2008), and Mikolov et al. (2013a), which we compare with traditional vectorspace co-occurrence models (VSMs) (Turney and Pantel 2010) with and without dimensionality reduction (SVD) (Landauer and Dumais 1997). Our analyses demonstrate how SimLex-999 can be applied to uncover substantial differences in the ability of models to represent concepts of different types. Despite these differences, the models we consider each share the characteristic of being better able to capture association than similarity. We show that the difficulty of estimating similarity is driven primarily by those strongly associated pairs with a high (association) rating in gold standards such as WS-353 and MEN, but a low similarity rating in SimLex-999.</context>
<context position="30159" citStr="Turney and Pantel 2010" startWordPosition="4563" endWordPosition="4566">ls are scored on the number of true synonyms identified out of 80 questions. The questions were designed by linguists to evaluate synonymy, so, unlike the evaluations considered thus far, TOEFL-style tests effectively discriminate between similarity and association. However, because they require a zero-one classification of pairs as synonymous or not, they do not test how well models discern pairs of medium or low similarity. More generally, in opposition to the fuzzy, statistical approaches to meaning predominant in both cognitive psychology (Griffiths, Steyvers, and Tenenbaum 2007) and NLP (Turney and Pantel 2010), they do not require similarity to be measured on a continuous scale. 3. The SimLex-999 Data Set Having considered the limitations of existing gold standards, in this section we describe the design of SimLex-999 in detail. 3.1 Choice of Concepts Separating similarity from association. To create a test of the ability of models to capture similarity as opposed to association, we started with the ≈ 72,000 pairs of concepts in the USF data set. As the output of a free-association experiment, each of these pairs is associated to a greater or lesser extent. Importantly, inspecting the pairs reveale</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Turney, Peter D. and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amos Tversky</author>
</authors>
<title>Features of similarity.</title>
<date>1977</date>
<journal>Psychological Review,</journal>
<volume>84</volume>
<issue>4</issue>
<contexts>
<context position="3455" citStr="Tversky 1977" startWordPosition="488" endWordPosition="489">ni et al. 2012a). As a consequence, these evaluations effectively penalize models for learning the evident truth that coffee and cup are dissimilar. Although clearly different, coffee and cup are very much related. The psychological literature refers to the conceptual relationship between these concepts as association, although it has been given a range of names including relatedness (Budanitsky and Hirst 2006; Agirre et al. 2009), topical similarity (Hatzivassiloglou et al. 2001), and domain similarity (Turney 2012). Association contrasts with similarity, the relation connecting cup and mug (Tversky 1977). At its strongest, the similarity relation is exemplified by pairs of synonyms; words with identical referents. Computational models that effectively capture similarity as distinct from association have numerous applications. Such models are used for the automatic generation of dictionaries, thesauri, ontologies, and language correction tools (Biemann 2005; Cimiano, Hotho, and Staab 2005; Li et al. 2006). Machine translation systems, which aim to define mappings between fragments of different languages whose meaning is similar, but not necessarily associated, are another established applicati</context>
</contexts>
<marker>Tversky, 1977</marker>
<rawString>Tversky, Amos. 1977. Features of similarity. Psychological Review, 84(4):327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In AAAI/IAAI,</booktitle>
<pages>735--740</pages>
<location>Austin, TX.</location>
<contexts>
<context position="48434" citStr="Wiebe 2000" startWordPosition="7439" endWordPosition="7440">py scales in the same way, possibly because concepts of these POS categories come to be associated via a more diverse range of relations. It seems plausible that humans are able to estimate the similarity of scale-based concepts more consistently than pairs of concepts related in a less uni-dimensional fashion. Regardless of cause, however, the high agreement on adjectives is a satisfactory property of SimLex-999. Adjectives exhibit various aspects of lexical semantics that have proved challenging for computational models, including antonymy, polarity (Williams and Anand 2009), and sentiment (Wiebe 2000). To approach the high level of human confidence on the adjective pairs in SimLex-999, it may be necessary to focus particularly on developing automatic ways to capture these phenomena. 4.2 Response Validity: Similarity not Association Inspection of the SimLex-999 ratings indicated that pairs were indeed evaluated according to similarity rather than association. Table 2 includes examples that demonstrate a clear dissociation between the two semantic relations. To verify this effect quantitatively, we recruited 100 additional participants to rate the WS-353 pairs, but following the SimLex-999 i</context>
</contexts>
<marker>Wiebe, 2000</marker>
<rawString>Wiebe, Janyce. 2000. Learning subjective adjectives from corpora. In AAAI/IAAI, pages 735–740, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gbolahan K Williams</author>
<author>Sarabjot Singh Anand</author>
</authors>
<title>Predicting the polarity strength of adjectives using Wordnet. In ICWSM,</title>
<date>2009</date>
<location>San Jose, CA.</location>
<contexts>
<context position="48406" citStr="Williams and Anand 2009" startWordPosition="7433" endWordPosition="7436">verbs in SimLex-999 do not appear to occupy scales in the same way, possibly because concepts of these POS categories come to be associated via a more diverse range of relations. It seems plausible that humans are able to estimate the similarity of scale-based concepts more consistently than pairs of concepts related in a less uni-dimensional fashion. Regardless of cause, however, the high agreement on adjectives is a satisfactory property of SimLex-999. Adjectives exhibit various aspects of lexical semantics that have proved challenging for computational models, including antonymy, polarity (Williams and Anand 2009), and sentiment (Wiebe 2000). To approach the high level of human confidence on the adjective pairs in SimLex-999, it may be necessary to focus particularly on developing automatic ways to capture these phenomena. 4.2 Response Validity: Similarity not Association Inspection of the SimLex-999 ratings indicated that pairs were indeed evaluated according to similarity rather than association. Table 2 includes examples that demonstrate a clear dissociation between the two semantic relations. To verify this effect quantitatively, we recruited 100 additional participants to rate the WS-353 pairs, bu</context>
</contexts>
<marker>Williams, Anand, 2009</marker>
<rawString>Williams, Gbolahan K. and Sarabjot Singh Anand. 2009. Predicting the polarity strength of adjectives using Wordnet. In ICWSM, San Jose, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs, semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>133--138</pages>
<location>Las Cruces, NM.</location>
<contexts>
<context position="13247" citStr="Wu and Palmer (1994)" startWordPosition="1939" endWordPosition="1942">tion is common concepts paired with low frequency synonyms, such as camel and dromedary. Because the essence of association is co-occurrence (linguistic or otherwise [McRae, Khalkhali, and Hare 2012]), such pairs can seem, at least intuitively, to be similar but not strongly associated. To explore the interaction between the two cognitive phenomena quantitatively, we exploited perhaps the only two existing large-scale means of quantifying similarity and association. To estimate similarity, we considered proximity in the WordNet taxonomy (Fellbaum 1998). Specifically, we applied the measure of Wu and Palmer (1994) (henceforth WupSim), which approximates similarity on a [0,1] scale reflecting the minimum distance between any two synsets of two given concepts in WordNet. WupSim has been shown to correlate well with human judgments on the similarity-focused RG data set (Wu and Palmer 1994). To estimate association, we extracted ratings directly from the University of South Florida Free Association Database (USF) (Nelson, McEvoy, and Schreiber 2004). These data were generated by presenting human subjects with one of 5,000 cue concepts and asking them to write the first word that comes into their head that </context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Wu, Zhibiao and Martha Palmer. 1994. Verbs, semantics and lexical selection. In Proceedings of ACL, pages 133–138, Las Cruces, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung Yong</author>
<author>Shou King Foo</author>
</authors>
<title>A case study on inter-annotator agreement for word sense disambiguation.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL SIGLEX Workshop on Standardizing Lexical Resources (SIGLEX99),</booktitle>
<location>College Park, MD.</location>
<contexts>
<context position="5699" citStr="Yong and Foo 1999" startWordPosition="817" endWordPosition="820"> of what their evaluation resources actually measure.1 Although certain smaller gold standards—those of Rubenstein and Goodenough (1965) (RG) and Agirre et al. (2009) (WS-Sim)—do focus clearly on similarity, these resources suffer from other important limitations. For instance, as we show, and as is also the case for WS-353 and MEN, state-of-the-art models have reached the average performance of a human annotator on these evaluations. It is common practice in NLP to define the upper limit for automated performance on an evaluation as the average human performance or inter-annotator agreement (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010). Based on this established principle and the current evaluations, it would therefore be reasonable to conclude that the problem of representation learning, at least for similarity modeling, is approaching resolution. However, circumstantial evidence suggests that distributional models are far from perfect. For instance, we are some way from automatically generated dictionaries, thesauri, or ontologies that can be used with the same confidence as their manually created equivalents. 1 For instance, Huang et al. (2012, pages 1, 4, 10) and Reisinger and Moon</context>
<context position="24908" citStr="Yong and Foo 1999" startWordPosition="3737" endWordPosition="3740">nnotators, as is common in NLP work (Pad´o, Pad´o, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014). It could be argued that a different comparison is more appropriate: Because the model is compared to the gold-standard average across all annotators, we should compare a single annotator with the (almost) gold-standard average over all other annotators. Based on this metric the average performance of an annotator on WS-353 is p = 0.756, which is still only marginally better than the best automatic method.6 Thus, at least according to the established wisdom in NLP evaluation (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010), the strength of the conclusions that can be inferred from improvements on WS-353 is limited. At the same time, however, state-of-the-art distributional models are clearly not perfect representation-learning or even similarity estimation engines, as evidenced by the fact they cannot yet be applied, for instance, to generate flawless lexical resources (Alfonseca and Manandhar 2002). WS-Sim. WS-Sim is the set of pairs in WS-353 identified by Agirre et al. (2009) as either containing similar or unrelated (neither similar nor associated) concepts. The rating</context>
</contexts>
<marker>Yong, Foo, 1999</marker>
<rawString>Yong, Chung and Shou King Foo. 1999. A case study on inter-annotator agreement for word sense disambiguation. In Proceedings of the ACL SIGLEX Workshop on Standardizing Lexical Resources (SIGLEX99), College Park, MD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>