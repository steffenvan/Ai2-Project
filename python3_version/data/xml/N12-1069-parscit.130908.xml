<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005854">
<title confidence="0.984343">
Concavity and Initialization for Unsupervised Dependency Parsing
</title>
<author confidence="0.998039">
Kevin Gimpel and Noah A. Smith
</author>
<affiliation confidence="0.890312333333333">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998866">
{kgimpel,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999457714285714">
We investigate models for unsupervised learn-
ing with concave log-likelihood functions. We
begin with the most well-known example,
IBM Model 1 for word alignment (Brown
et al., 1993) and analyze its properties, dis-
cussing why other models for unsupervised
learning are so seldom concave. We then
present concave models for dependency gram-
mar induction and validate them experimen-
tally. We find our concave models to be effec-
tive initializers for the dependency model of
Klein and Manning (2004) and show that we
can encode linguistic knowledge in them for
improved performance.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9985125">
In NLP, unsupervised learning typically implies op-
timization of a “bumpy” objective function riddled
with local maxima. However, one exception is IBM
Model 1 (Brown et al., 1993) for word alignment,
which is the only model commonly used for unsu-
pervised learning in NLP that has a concave log-
likelihood function.1 For other models, such as
those used in unsupervised part-of-speech tagging
and grammar induction, and indeed for more sophis-
ticated word alignment models, the log-likelihood
function maximized by EM is non-concave. As a
result, researchers are obligated to consider initial-
ization in addition to model design (Klein and Man-
ning, 2004; Goldberg et al., 2008).
For example, consider the dependency grammar
induction results shown in Table 1 when training the
</bodyText>
<footnote confidence="0.966495">
1It is not strictly concave (Toutanova and Galley, 2011).
</footnote>
<bodyText confidence="0.999629409090909">
widely used dependency model with valence (DMV;
Klein and Manning, 2004). Using uniform distri-
butions for initialization (UNIF) results in an accu-
racy of 17.6% on the test set, well below the base-
line of attaching each word to its right neighbor
(ATTACHRIGHT, 31.7%). Furthermore, when using
a set of 50 random initializers (RAND), the standard
deviation of the accuracy is an alarming 8.3%.
In light of this sensitivity to initialization, it is
compelling to consider unsupervised models with
concave log-likelihood functions, which may pro-
vide stable, data-supported initializers for more
complex models. In this paper, we explore the issues
involved with such an expedition and elucidate the
limitations of such models for unsupervised NLP.
We then present simple concave models for depen-
dency grammar induction that are easy to implement
and offer efficient optimization. We also show how
linguistic knowledge can be encoded without sacri-
ficing concavity. Using our models to initialize the
DMV, we find that they lead to an improvement in
average accuracy across 18 languages.
</bodyText>
<sectionHeader confidence="0.996365" genericHeader="method">
2 IBM Model 1 and Concavity
</sectionHeader>
<bodyText confidence="0.999827">
IBM Model 1 is a conditional model of a target-
language sentence a of length m and an alignment
a given a source-language sentence f of length l.
The generation of m is assumed to occur with some
(inconsequential) uniform probability c. The align-
ment vector a, a hidden variable, has an entry for
each element of a that contains the index in f of
the aligned word. These entries are used to define
which translation parameters t(ej  |fad) are active.
Model 1 assumes that the probability of the ith ele-
</bodyText>
<page confidence="0.899193333333333">
577
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 577–581,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.9996278">
ment in a, denoted a(i  |j,l, m), is simply a uni-
form distribution over all l source words plus the
null word. These assumptions result in the follow-
ing log-likelihood for a sentence pair (f, e) under
Model 1 (marginalizing a):
</bodyText>
<equation confidence="0.996084333333333">
logp(e  |f) = log �
(l+1)�+Pm j=1 log Pli=0 t(ej  |fi)
(1)
</equation>
<bodyText confidence="0.9967605">
The only parameters to be learned in the model are
t = {t(e  |f)}e,f. Since a parameter is concave in
itself, the sum of concave functions is concave, and
the log of a concave function is concave, Eq. 1 is
concave in t (Brown et al., 1993).
IBM Model 2 involves a slight change to Model
1 in which the probability of a word link depends
on the word positions. However, this change renders
it no longer concave. Consider the log-likelihood
function for Model 2:
</bodyText>
<equation confidence="0.806249">
log E+Pmj=1 log Pli=0 t(ej  |fi)-a(i  |j,l, m) (2)
</equation>
<bodyText confidence="0.968578235294118">
Eq. 2 is not concave in the parameters t(ej  |fi) and
a(i  |j,l, m) because a product is neither convex nor
concave in its vector of operands. This can be shown
by computing the Hessian matrix of f(x, y) = xy
and showing that it is indefinite.
In general, concavity is lost when the log-
likelihood function contains a product of model pa-
rameters enclosed within a log P. If the sum is not
present, the log can be used to separate the prod-
uct of parameters, making the function concave. It
can also be shown that a “featurized” version (Berg-
Kirkpatrick et al., 2010) of Model 1 is not con-
cave. More generally, any non-concave function en-
closed within log P will cause the log-likelihood
function to be non-concave, though there are few
other non-concave functions with a probabilistic se-
mantics than those just discussed.
</bodyText>
<sectionHeader confidence="0.986842" genericHeader="method">
3 Concave, Unsupervised Models
</sectionHeader>
<bodyText confidence="0.9999175">
Nearly every other model used for unsupervised
learning in NLP has a non-concave log-likelihood
function. We now proceed to describe the conditions
necessary to develop concave models for two tasks.
</bodyText>
<subsectionHeader confidence="0.999317">
3.1 Part-of-Speech Tagging
</subsectionHeader>
<bodyText confidence="0.931146944444445">
Consider a standard first-order hidden Markov
model for POS tagging. Letting y denote the tag
sequence for a sentence e with m tokens, the single-
example log-likelihood is:
log Py p(stop  |ym) Qmj=1 p(yj  |yj−1) - p(ej  |yj)
(3)
where y0 is a designated “start” symbol. Unlike IBM
Models 1 and 2, we cannot reverse the order of the
summation and product here because the transition
parameters p(yj  |yj−1) cause each tag decision to
affect its neighbors. Therefore, Eq. 3 is non-concave
due to the presence of a product within a log P.
However, if the tag transition probabilities p(yj |
yj−1) are all constants and also do not depend on
the previous tag yj−1, then we can rewrite Eq. 3 as
the following concave log-likelihood function (using
C(y) to denote a constant function of tag y, e.g., a
fixed tag prior distribution):
</bodyText>
<equation confidence="0.992064">
log C(stop) + log Qm Py; C(yj) - p(ej  |yj)
j=1
</equation>
<bodyText confidence="0.999912125">
Lacking any transition modeling power, this model
appears weak for POS tagging. However, we note
that we can add additional conditioning information
to the p(ej  |yj) distributions and retain concavity,
such as nearby words and tag dictionary informa-
tion. We speculate that such a model might learn
useful patterns about local contexts and provide an
initializer for unsupervised part-of-speech tagging.
</bodyText>
<subsectionHeader confidence="0.999824">
3.2 Dependency Grammar Induction
</subsectionHeader>
<bodyText confidence="0.9288465">
To develop dependency grammar induction models,
we begin with a version of Model 1 in which a sen-
tence e is generated from a copy of itself (denoted
e0): log p(e  |e0)
</bodyText>
<equation confidence="0.996219">
= log � i) (4)
(m+1)� + Pm j=1 log Pm i=0,i6=j c(ej  |e0
</equation>
<bodyText confidence="0.999816769230769">
If a word ej is “aligned” to e00, ej is a root. This
is a simple child-generation model with no tree con-
straint. In order to preserve concavity, we are forbid-
den from conditioning on other parent-child assign-
ments or including any sort of larger constraints.
However, we can condition the child distributions
on additional information about e0 since it is fully
observed. This conditioning information may in-
clude the direction of the edge, its distance, and
any properties about the words in the sentence. We
found that conditioning on direction improved per-
formance: we rewrite the c distributions as c(ej |
e0i, sign(j − i)) and denote this model by CCV1.
</bodyText>
<page confidence="0.989604">
578
</page>
<bodyText confidence="0.999963388888889">
We note that we can also include constraints in the
sum over possible parents and still preserve concav-
ity. Naseem et al. (2010) found that adding parent-
child constraints to a grammar induction system can
improve performance dramatically. We employ one
simple rule: roots are likely to be verbs.2 We mod-
ify CCV1 to restrict the summation over parents to
exclude e&apos; if the child word is not a verb.3 We only
employ this restriction during EM learning for sen-
tences containing at least one verb. For sentences
without verbs, we allow all words to be the root. We
denote this model by CCV2.
In related work, Brody (2010) also developed
grammar induction models based on the IBM word
alignment models. However, while our goal is to
develop concave models, Brody employed Bayesian
nonparametrics in his version of Model 1, which
makes the model non-concave.
</bodyText>
<sectionHeader confidence="0.999885" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999992545454545">
We ran experiments to determine how well our con-
cave grammar induction models CCV1 and CCV2 can
perform on their own and when used as initializers
for the DMV (Klein and Manning, 2004). The DMV
is a generative model of POS tag sequences and pro-
jective dependency trees over them. It is the foun-
dation of most state-of-the-art unsupervised gram-
mar induction models (several of which are listed in
Tab. 1). The model includes multinomial distribu-
tions for generating each POS tag given its parent
and the direction of generation: where ei is the par-
ent POS tag and ej the child tag, these distributions
take the form c(ej  |ei7 sign(j − i)), analogous to
the distributions used in our concave models. The
DMV also has multinomial distributions for decid-
ing whether to stop or continue generating children
in each direction considering whether any children
have already been generated in that direction.
The majority of researchers use the original ini-
tializer from Klein and Manning (2004), denoted
here K&amp;M. K&amp;M is a deterministic harmonic initial-
izer that sets parent-child token affinities inversely
</bodyText>
<footnote confidence="0.9951235">
2This is similar to the rule used by Mareˇcek and ˇZabokrtsk´y
(2011) with empirical success.
3As verbs, we take all tags that map to V in the universal tag
mappings from Petrov et al. (2012). Thus, to apply this con-
straint to a new language, one would have to produce a similar
tag mapping or identify verb tags through manual inspection.
</footnote>
<table confidence="0.999785176470588">
Model Init. Train &lt; 10 Train &lt; 20
&lt;10 Test Test &lt;00
&lt;00 &lt;10
ATTRIGHT N/A 38.4 31.7 38.4 31.7
CCV1 UNIF 31.4 25.6 31.0 23.7
CCV2 UNIF 43.1 28.6 43.9 27.1
UNIF 21.3 17.6 21.3 16.4
RAND* 41.0 31.8 - -
DMV K&amp;M 44.1 32.9 51.9 37.8
CCV1 45.3 30.9 53.9 36.7
CCV2 54.3 43.0 64.3 53.1
Shared LN K&amp;M 61.3 41.4
L-EVG RAND† 68.8 -
Feature DMV K&amp;M 63.0 -
LexTSG-DMV K&amp;M 67.7 55.7
Posterior Reg. K&amp;M 64.3 53.3
Punc/UTags K&amp;M&apos; - 59.1#
</table>
<tableCaption confidence="0.775616333333333">
Table 1: English attachment accuracies on Section 23, for
short sentences (&lt;10 words) and all (&lt;00). We include
selected results on this same test set: Shared LN = Cohen
</tableCaption>
<figureCaption confidence="0.749667">
and Smith (2009), L-EVG = Headden III et al. (2009),
Feature DMV = Berg-Kirkpatrick et al. (2010), LexTSG-
DMV = Blunsom and Cohn (2010), Posterior Reg. =
Gillenwater et al. (2010), Punc/UTags = Spitkovsky et
al. (2011a). K&amp;M&apos; is from Spitkovsky et al. (2011b).
*Accuracies are averages over 50 random initializers;
u = 10.9 for test sentences &lt; 10 and 8.3 for all. †Used
many random initializers with unsupervised run selec-
tion. $Used staged training with sentences &lt; 45 words.
</figureCaption>
<bodyText confidence="0.99991745">
proportional to their distances, then normalizes to
obtain probability distributions. K&amp;M is often de-
scribed as corresponding to an initial E step for an
unspecified model that favors short attachments.
Procedure We run EM for our concave models for
100 iterations. We evaluate the learned models di-
rectly as parsers on the test data and also use them
to initialize the DMV. When using them directly as
parsers, we use dynamic programming to ensure that
a valid tree is recovered. When using the concave
models as initializers for the DMV, we copy the c
parameters over directly since they appear in both
models. We do not have the stop/continue parame-
ters in our concave models, so we simply initialize
them uniformly for the DMV. We train each DMV
for 200 iterations and use minimum Bayes risk de-
coding with the final model on the test data. We use
several initializers for training the DMV, including
the uniform initializer (UNIF), K&amp;M, and our trained
concave models CCV1 and CCV2.
</bodyText>
<page confidence="0.995196">
579
</page>
<table confidence="0.9981958">
Init. eu bg ca zh cs da nl en de el hu
UNIF 24/21 32/26 27/29 44/40 32/30 24/19 21/21 21/18 31/24 37/32 23/18
K&amp;M 32/26 48/40 24/25 38/33 31/29 34/23 39/33 44/33 47/37 50/41 23/20
CCV1 22/21 34/27 44/51 46/45 33/31 19/14 24/24 45/31 46/31 51/45 32/28
CCV2 26/25 34/26 29/35 46/44 50/40 29/18 50/43 54/43 49/33 50/45 60/46
it ja pt sl es sv tr avg. accuracy avg. log-likelihood
UNIF 31/24 35/30 49/36 20/20 29/24 26/22 33/30 29.8 / 25.7 -15.05
K&amp;M 32/24 39/31 44/28 33/27 19/11 46/33 39/36 36.7 / 29.4 -14.84
CCV1 34/25 42/27 50/38 30/25 41/33 45/33 37/29 37.5 / 30.9 -14.93
CCV2 55/48 49/31 50/38 22/21 57/50 46/32 31/22 43.7 / 35.5 -14.45
</table>
<tableCaption confidence="0.987373">
Table 2: Test set attachment accuracies for 18 languages; first number in each cell is accuracy for sentences &lt; 10
words and second is for all sentences. For training, sentences &lt; 10 words from each treebank were used. In order,
languages are Basque, Bulgarian, Catalan, Chinese, Czech, Danish, Dutch, English, German, Greek, Hungarian,
Italian, Japanese, Portuguese, Slovenian, Spanish, Swedish, and Turkish.
</tableCaption>
<bodyText confidence="0.990503178571429">
Data We use data prepared for the CoNLL
2006/07 shared tasks (Buchholz and Marsi, 2006;
Nivre et al., 2007).4 We follow standard practice
in removing punctuation and using short sentences
(&lt; 10 or &lt; 20 words) for training. For all experi-
ments, we train on separate data from that used for
testing and use gold POS tags for both training and
testing. We report accuracy on (i) test set sentences
&lt;10 words and (ii) all sentences from the test set.
Results Results for English are shown in Tab. 1.
We train on §2–21 and test on §23 in the Penn Tree-
bank. The constraint on sentence roots helps a great
deal, as CCV2 by itself is competitive with the DMV
when testing on short sentences. The true benefit of
the concave models, however, appears when using
them as initializers. The DMV initialized with CCV2
achieves a substantial improvement over all others.
When training on sentences of length &lt; 20 words
(bold), the performance even rivals that of several
more sophisticated models shown in the table, de-
spite only using the DMV with a different initializer.
Tab. 2 shows results for 18 languages. On av-
erage, CCV2 performs best and CCV1 does at least
as well as K&amp;M. This shows that a simple, concave
model can be as effective as a state-of-the-art hand-
designed initializer (K&amp;M), and that concave mod-
els can encode linguistic knowledge to further im-
prove performance.
</bodyText>
<footnote confidence="0.879692">
4In some cases, we did not use official CoNLL test sets but
instead took the training data and reserved the first 80% of the
sentences for training, the next 10% for development, and the
final 10% as our test set; dataset details are omitted for space
but are the same as those given by Cohen (2011).
</footnote>
<bodyText confidence="0.997391666666667">
Average log-likelihoods (micro-averaged across
sentences) achieved by EM training are shown in the
final column of Tab. 2. CCV2 leads to substantially-
higher likelihoods than the other initializers, sug-
gesting that the verb-root constraint is helping EM
to find better local optima.5
</bodyText>
<sectionHeader confidence="0.999609" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999952583333333">
Staged training has been shown to help unsupervised
learning in the past, from early work in grammar in-
duction (Lari and Young, 1990) and word alignment
(Brown et al., 1993) to more recent work in depen-
dency grammar induction (Spitkovsky et al., 2010).
While we do not yet offer a generic procedure for
extracting a concave approximation from any model
for unsupervised learning, our results contribute evi-
dence in favor of the general methodology of staged
training in unsupervised learning, and provide a sim-
ple and powerful initialization method for depen-
dency grammar induction.
</bodyText>
<sectionHeader confidence="0.997042" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.872965">
We thank Shay Cohen, Dipanjan Das, Val Spitkovsky,
and members of the ARK research group for helpful com-
ments that improved this paper. This research was sup-
ported in part by the NSF through grant IIS-0915187, the
U. S. Army Research Laboratory and the U. S. Army Re-
search Office under contract/grant number W911NF-10-
1-0533, and Sandia National Laboratories (fellowship to
K. Gimpel).
5However, while CCV1 leads to a higher average accuracy
than K&amp;M, the latter reaches slightly higher likelihood, sug-
gesting that the success of the concave initializers is only par-
tially due to reaching high training likelihood.
</bodyText>
<page confidence="0.994579">
580
</page>
<sectionHeader confidence="0.996395" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999972984615385">
T. Berg-Kirkpatrick, A. Bouchard-Cˆot´e, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In Proc. of NAACL.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
In Proc. of EMNLP.
S. Brody. 2010. It depends on the translation: Unsu-
pervised dependency parsing via word alignment. In
Proc. of EMNLP.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
S. Cohen and N. A. Smith. 2009. Shared logistic normal
distributions for soft parameter tying in unsupervised
grammar induction. In Proc. of NAACL.
S. Cohen. 2011. Computational Learning of Probabilis-
tic Grammars in the Unsupervised Setting. Ph.D. the-
sis, Carnegie Mellon University.
J. Gillenwater, K. Ganchev, J. Grac¸a, F. Pereira, , and
B. Taskar. 2010. Posterior sparsity in unsupervised
dependency parsing. Journal of Machine Learning
Research.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM
can find pretty good HMM POS-taggers (when given
a good start). In Proc. of ACL.
W. Headden III, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proc. of NAACL.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. of ACL.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language, 4:35–56.
D. Mareˇcek and Z. ˇZabokrtsk´y. 2011. Gibbs sampling
with treeness constraint in unsupervised dependency
parsing. In Proc. of Workshop on Robust Unsuper-
vised and Semisupervised Methods in Natural Lan-
guage Processing.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010.
Using universal linguistic knowledge to guide gram-
mar induction. In Proc. of EMNLP.
J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc. of
CoNLL.
S. Petrov, D. Das, and R. McDonald. 2012. A universal
part-of-speech tagset. In Proc. of LREC.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010.
From Baby Steps to Leapfrog: How “Less is More”
in unsupervised dependency parsing. In Proc. of
NAACL-HLT.
V. I. Spitkovsky, H. Alshawi, A. X. Chang, and D. Juraf-
sky. 2011a. Unsupervised dependency parsing with-
out gold part-of-speech tags. In Proc. of EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b.
Punctuation: Making a point in unsupervised depen-
dency parsing. In Proc. of CoNLL.
K. Toutanova and M. Galley. 2011. Why initialization
matters for IBM Model 1: Multiple optima and non-
strict convexity. In Proc. of ACL.
</reference>
<page confidence="0.998256">
581
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.391010">
<title confidence="0.998382">Concavity and Initialization for Unsupervised Dependency Parsing</title>
<author confidence="0.515507">A Gimpel</author>
<affiliation confidence="0.7221125">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.991942">Pittsburgh, PA 15213,</address>
<abstract confidence="0.9968202">We investigate models for unsupervised learning with concave log-likelihood functions. We begin with the most well-known example, IBM Model 1 for word alignment (Brown et al., 1993) and analyze its properties, discussing why other models for unsupervised learning are so seldom concave. We then present concave models for dependency grammar induction and validate them experimentally. We find our concave models to be effective initializers for the dependency model of Klein and Manning (2004) and show that we can encode linguistic knowledge in them for improved performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Berg-Kirkpatrick</author>
<author>A Bouchard-Cˆot´e</author>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>T. Berg-Kirkpatrick, A. Bouchard-Cˆot´e, J. DeNero, and D. Klein. 2010. Painless unsupervised learning with features. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
</authors>
<title>Unsupervised induction of tree substitution grammars for dependency parsing.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="10569" citStr="Blunsom and Cohn (2010)" startWordPosition="1774" endWordPosition="1777">F 31.4 25.6 31.0 23.7 CCV2 UNIF 43.1 28.6 43.9 27.1 UNIF 21.3 17.6 21.3 16.4 RAND* 41.0 31.8 - - DMV K&amp;M 44.1 32.9 51.9 37.8 CCV1 45.3 30.9 53.9 36.7 CCV2 54.3 43.0 64.3 53.1 Shared LN K&amp;M 61.3 41.4 L-EVG RAND† 68.8 - Feature DMV K&amp;M 63.0 - LexTSG-DMV K&amp;M 67.7 55.7 Posterior Reg. K&amp;M 64.3 53.3 Punc/UTags K&amp;M&apos; - 59.1# Table 1: English attachment accuracies on Section 23, for short sentences (&lt;10 words) and all (&lt;00). We include selected results on this same test set: Shared LN = Cohen and Smith (2009), L-EVG = Headden III et al. (2009), Feature DMV = Berg-Kirkpatrick et al. (2010), LexTSGDMV = Blunsom and Cohn (2010), Posterior Reg. = Gillenwater et al. (2010), Punc/UTags = Spitkovsky et al. (2011a). K&amp;M&apos; is from Spitkovsky et al. (2011b). *Accuracies are averages over 50 random initializers; u = 10.9 for test sentences &lt; 10 and 8.3 for all. †Used many random initializers with unsupervised run selection. $Used staged training with sentences &lt; 45 words. proportional to their distances, then normalizes to obtain probability distributions. K&amp;M is often described as corresponding to an initial E step for an unspecified model that favors short attachments. Procedure We run EM for our concave models for 100 ite</context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>P. Blunsom and T. Cohn. 2010. Unsupervised induction of tree substitution grammars for dependency parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brody</author>
</authors>
<title>It depends on the translation: Unsupervised dependency parsing via word alignment.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="8155" citStr="Brody (2010)" startWordPosition="1357" endWordPosition="1358">te that we can also include constraints in the sum over possible parents and still preserve concavity. Naseem et al. (2010) found that adding parentchild constraints to a grammar induction system can improve performance dramatically. We employ one simple rule: roots are likely to be verbs.2 We modify CCV1 to restrict the summation over parents to exclude e&apos; if the child word is not a verb.3 We only employ this restriction during EM learning for sentences containing at least one verb. For sentences without verbs, we allow all words to be the root. We denote this model by CCV2. In related work, Brody (2010) also developed grammar induction models based on the IBM word alignment models. However, while our goal is to develop concave models, Brody employed Bayesian nonparametrics in his version of Model 1, which makes the model non-concave. 4 Experiments We ran experiments to determine how well our concave grammar induction models CCV1 and CCV2 can perform on their own and when used as initializers for the DMV (Klein and Manning, 2004). The DMV is a generative model of POS tag sequences and projective dependency trees over them. It is the foundation of most state-of-the-art unsupervised grammar ind</context>
</contexts>
<marker>Brody, 2010</marker>
<rawString>S. Brody. 2010. It depends on the translation: Unsupervised dependency parsing via word alignment. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="989" citStr="Brown et al., 1993" startWordPosition="142" endWordPosition="145">alignment (Brown et al., 1993) and analyze its properties, discussing why other models for unsupervised learning are so seldom concave. We then present concave models for dependency grammar induction and validate them experimentally. We find our concave models to be effective initializers for the dependency model of Klein and Manning (2004) and show that we can encode linguistic knowledge in them for improved performance. 1 Introduction In NLP, unsupervised learning typically implies optimization of a “bumpy” objective function riddled with local maxima. However, one exception is IBM Model 1 (Brown et al., 1993) for word alignment, which is the only model commonly used for unsupervised learning in NLP that has a concave loglikelihood function.1 For other models, such as those used in unsupervised part-of-speech tagging and grammar induction, and indeed for more sophisticated word alignment models, the log-likelihood function maximized by EM is non-concave. As a result, researchers are obligated to consider initialization in addition to model design (Klein and Manning, 2004; Goldberg et al., 2008). For example, consider the dependency grammar induction results shown in Table 1 when training the 1It is</context>
<context position="4001" citStr="Brown et al., 1993" startWordPosition="639" endWordPosition="642">ges 577–581, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics ment in a, denoted a(i |j,l, m), is simply a uniform distribution over all l source words plus the null word. These assumptions result in the following log-likelihood for a sentence pair (f, e) under Model 1 (marginalizing a): logp(e |f) = log � (l+1)�+Pm j=1 log Pli=0 t(ej |fi) (1) The only parameters to be learned in the model are t = {t(e |f)}e,f. Since a parameter is concave in itself, the sum of concave functions is concave, and the log of a concave function is concave, Eq. 1 is concave in t (Brown et al., 1993). IBM Model 2 involves a slight change to Model 1 in which the probability of a word link depends on the word positions. However, this change renders it no longer concave. Consider the log-likelihood function for Model 2: log E+Pmj=1 log Pli=0 t(ej |fi)-a(i |j,l, m) (2) Eq. 2 is not concave in the parameters t(ej |fi) and a(i |j,l, m) because a product is neither convex nor concave in its vector of operands. This can be shown by computing the Hessian matrix of f(x, y) = xy and showing that it is indefinite. In general, concavity is lost when the loglikelihood function contains a product of mod</context>
<context position="15093" citStr="Brown et al., 1993" startWordPosition="2548" endWordPosition="2551">e next 10% for development, and the final 10% as our test set; dataset details are omitted for space but are the same as those given by Cohen (2011). Average log-likelihoods (micro-averaged across sentences) achieved by EM training are shown in the final column of Tab. 2. CCV2 leads to substantiallyhigher likelihoods than the other initializers, suggesting that the verb-root constraint is helping EM to find better local optima.5 5 Discussion Staged training has been shown to help unsupervised learning in the past, from early work in grammar induction (Lari and Young, 1990) and word alignment (Brown et al., 1993) to more recent work in dependency grammar induction (Spitkovsky et al., 2010). While we do not yet offer a generic procedure for extracting a concave approximation from any model for unsupervised learning, our results contribute evidence in favor of the general methodology of staged training in unsupervised learning, and provide a simple and powerful initialization method for dependency grammar induction. Acknowledgments We thank Shay Cohen, Dipanjan Das, Val Spitkovsky, and members of the ARK research group for helpful comments that improved this paper. This research was supported in part by</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="13039" citStr="Buchholz and Marsi, 2006" startWordPosition="2194" endWordPosition="2197"> 34/25 42/27 50/38 30/25 41/33 45/33 37/29 37.5 / 30.9 -14.93 CCV2 55/48 49/31 50/38 22/21 57/50 46/32 31/22 43.7 / 35.5 -14.45 Table 2: Test set attachment accuracies for 18 languages; first number in each cell is accuracy for sentences &lt; 10 words and second is for all sentences. For training, sentences &lt; 10 words from each treebank were used. In order, languages are Basque, Bulgarian, Catalan, Chinese, Czech, Danish, Dutch, English, German, Greek, Hungarian, Italian, Japanese, Portuguese, Slovenian, Spanish, Swedish, and Turkish. Data We use data prepared for the CoNLL 2006/07 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007).4 We follow standard practice in removing punctuation and using short sentences (&lt; 10 or &lt; 20 words) for training. For all experiments, we train on separate data from that used for testing and use gold POS tags for both training and testing. We report accuracy on (i) test set sentences &lt;10 words and (ii) all sentences from the test set. Results Results for English are shown in Tab. 1. We train on §2–21 and test on §23 in the Penn Treebank. The constraint on sentence roots helps a great deal, as CCV2 by itself is competitive with the DMV when testing on short sentences. Th</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cohen</author>
<author>N A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="10451" citStr="Cohen and Smith (2009)" startWordPosition="1753" endWordPosition="1756">ual inspection. Model Init. Train &lt; 10 Train &lt; 20 &lt;10 Test Test &lt;00 &lt;00 &lt;10 ATTRIGHT N/A 38.4 31.7 38.4 31.7 CCV1 UNIF 31.4 25.6 31.0 23.7 CCV2 UNIF 43.1 28.6 43.9 27.1 UNIF 21.3 17.6 21.3 16.4 RAND* 41.0 31.8 - - DMV K&amp;M 44.1 32.9 51.9 37.8 CCV1 45.3 30.9 53.9 36.7 CCV2 54.3 43.0 64.3 53.1 Shared LN K&amp;M 61.3 41.4 L-EVG RAND† 68.8 - Feature DMV K&amp;M 63.0 - LexTSG-DMV K&amp;M 67.7 55.7 Posterior Reg. K&amp;M 64.3 53.3 Punc/UTags K&amp;M&apos; - 59.1# Table 1: English attachment accuracies on Section 23, for short sentences (&lt;10 words) and all (&lt;00). We include selected results on this same test set: Shared LN = Cohen and Smith (2009), L-EVG = Headden III et al. (2009), Feature DMV = Berg-Kirkpatrick et al. (2010), LexTSGDMV = Blunsom and Cohn (2010), Posterior Reg. = Gillenwater et al. (2010), Punc/UTags = Spitkovsky et al. (2011a). K&amp;M&apos; is from Spitkovsky et al. (2011b). *Accuracies are averages over 50 random initializers; u = 10.9 for test sentences &lt; 10 and 8.3 for all. †Used many random initializers with unsupervised run selection. $Used staged training with sentences &lt; 45 words. proportional to their distances, then normalizes to obtain probability distributions. K&amp;M is often described as corresponding to an initial</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>S. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cohen</author>
</authors>
<date>2011</date>
<booktitle>Computational Learning of Probabilistic Grammars in the Unsupervised Setting. Ph.D. thesis,</booktitle>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="14622" citStr="Cohen (2011)" startWordPosition="2476" endWordPosition="2477">. Tab. 2 shows results for 18 languages. On average, CCV2 performs best and CCV1 does at least as well as K&amp;M. This shows that a simple, concave model can be as effective as a state-of-the-art handdesigned initializer (K&amp;M), and that concave models can encode linguistic knowledge to further improve performance. 4In some cases, we did not use official CoNLL test sets but instead took the training data and reserved the first 80% of the sentences for training, the next 10% for development, and the final 10% as our test set; dataset details are omitted for space but are the same as those given by Cohen (2011). Average log-likelihoods (micro-averaged across sentences) achieved by EM training are shown in the final column of Tab. 2. CCV2 leads to substantiallyhigher likelihoods than the other initializers, suggesting that the verb-root constraint is helping EM to find better local optima.5 5 Discussion Staged training has been shown to help unsupervised learning in the past, from early work in grammar induction (Lari and Young, 1990) and word alignment (Brown et al., 1993) to more recent work in dependency grammar induction (Spitkovsky et al., 2010). While we do not yet offer a generic procedure for</context>
</contexts>
<marker>Cohen, 2011</marker>
<rawString>S. Cohen. 2011. Computational Learning of Probabilistic Grammars in the Unsupervised Setting. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gillenwater</author>
<author>K Ganchev</author>
<author>J Grac¸a</author>
<author>F Pereira</author>
</authors>
<title>Posterior sparsity in unsupervised dependency parsing.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research.</journal>
<marker>Gillenwater, Ganchev, Grac¸a, Pereira, 2010</marker>
<rawString>J. Gillenwater, K. Ganchev, J. Grac¸a, F. Pereira, , and B. Taskar. 2010. Posterior sparsity in unsupervised dependency parsing. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Goldberg</author>
<author>M Adler</author>
<author>M Elhadad</author>
</authors>
<title>EM can find pretty good HMM POS-taggers (when given a good start).</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1483" citStr="Goldberg et al., 2008" startWordPosition="220" endWordPosition="223">timization of a “bumpy” objective function riddled with local maxima. However, one exception is IBM Model 1 (Brown et al., 1993) for word alignment, which is the only model commonly used for unsupervised learning in NLP that has a concave loglikelihood function.1 For other models, such as those used in unsupervised part-of-speech tagging and grammar induction, and indeed for more sophisticated word alignment models, the log-likelihood function maximized by EM is non-concave. As a result, researchers are obligated to consider initialization in addition to model design (Klein and Manning, 2004; Goldberg et al., 2008). For example, consider the dependency grammar induction results shown in Table 1 when training the 1It is not strictly concave (Toutanova and Galley, 2011). widely used dependency model with valence (DMV; Klein and Manning, 2004). Using uniform distributions for initialization (UNIF) results in an accuracy of 17.6% on the test set, well below the baseline of attaching each word to its right neighbor (ATTACHRIGHT, 31.7%). Furthermore, when using a set of 50 random initializers (RAND), the standard deviation of the accuracy is an alarming 8.3%. In light of this sensitivity to initialization, it</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM can find pretty good HMM POS-taggers (when given a good start). In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Headden M Johnson</author>
<author>D McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL.</booktitle>
<marker>Johnson, McClosky, 2009</marker>
<rawString>W. Headden III, M. Johnson, and D. McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="712" citStr="Klein and Manning (2004)" startWordPosition="99" endWordPosition="102">. Smith Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {kgimpel,nasmith}@cs.cmu.edu Abstract We investigate models for unsupervised learning with concave log-likelihood functions. We begin with the most well-known example, IBM Model 1 for word alignment (Brown et al., 1993) and analyze its properties, discussing why other models for unsupervised learning are so seldom concave. We then present concave models for dependency grammar induction and validate them experimentally. We find our concave models to be effective initializers for the dependency model of Klein and Manning (2004) and show that we can encode linguistic knowledge in them for improved performance. 1 Introduction In NLP, unsupervised learning typically implies optimization of a “bumpy” objective function riddled with local maxima. However, one exception is IBM Model 1 (Brown et al., 1993) for word alignment, which is the only model commonly used for unsupervised learning in NLP that has a concave loglikelihood function.1 For other models, such as those used in unsupervised part-of-speech tagging and grammar induction, and indeed for more sophisticated word alignment models, the log-likelihood function max</context>
<context position="8589" citStr="Klein and Manning, 2004" startWordPosition="1426" endWordPosition="1429">ion during EM learning for sentences containing at least one verb. For sentences without verbs, we allow all words to be the root. We denote this model by CCV2. In related work, Brody (2010) also developed grammar induction models based on the IBM word alignment models. However, while our goal is to develop concave models, Brody employed Bayesian nonparametrics in his version of Model 1, which makes the model non-concave. 4 Experiments We ran experiments to determine how well our concave grammar induction models CCV1 and CCV2 can perform on their own and when used as initializers for the DMV (Klein and Manning, 2004). The DMV is a generative model of POS tag sequences and projective dependency trees over them. It is the foundation of most state-of-the-art unsupervised grammar induction models (several of which are listed in Tab. 1). The model includes multinomial distributions for generating each POS tag given its parent and the direction of generation: where ei is the parent POS tag and ej the child tag, these distributions take the form c(ej |ei7 sign(j − i)), analogous to the distributions used in our concave models. The DMV also has multinomial distributions for deciding whether to stop or continue ge</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language,</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context position="15053" citStr="Lari and Young, 1990" startWordPosition="2541" endWordPosition="2544">irst 80% of the sentences for training, the next 10% for development, and the final 10% as our test set; dataset details are omitted for space but are the same as those given by Cohen (2011). Average log-likelihoods (micro-averaged across sentences) achieved by EM training are shown in the final column of Tab. 2. CCV2 leads to substantiallyhigher likelihoods than the other initializers, suggesting that the verb-root constraint is helping EM to find better local optima.5 5 Discussion Staged training has been shown to help unsupervised learning in the past, from early work in grammar induction (Lari and Young, 1990) and word alignment (Brown et al., 1993) to more recent work in dependency grammar induction (Spitkovsky et al., 2010). While we do not yet offer a generic procedure for extracting a concave approximation from any model for unsupervised learning, our results contribute evidence in favor of the general methodology of staged training in unsupervised learning, and provide a simple and powerful initialization method for dependency grammar induction. Acknowledgments We thank Shay Cohen, Dipanjan Das, Val Spitkovsky, and members of the ARK research group for helpful comments that improved this paper</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S. J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mareˇcek</author>
<author>Z ˇZabokrtsk´y</author>
</authors>
<title>Gibbs sampling with treeness constraint in unsupervised dependency parsing.</title>
<date>2011</date>
<booktitle>In Proc. of Workshop on Robust Unsupervised and Semisupervised Methods in Natural Language Processing.</booktitle>
<marker>Mareˇcek, ˇZabokrtsk´y, 2011</marker>
<rawString>D. Mareˇcek and Z. ˇZabokrtsk´y. 2011. Gibbs sampling with treeness constraint in unsupervised dependency parsing. In Proc. of Workshop on Robust Unsupervised and Semisupervised Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Naseem</author>
<author>H Chen</author>
<author>R Barzilay</author>
<author>M Johnson</author>
</authors>
<title>Using universal linguistic knowledge to guide grammar induction.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="7666" citStr="Naseem et al. (2010)" startWordPosition="1269" endWordPosition="1272">itioning on other parent-child assignments or including any sort of larger constraints. However, we can condition the child distributions on additional information about e0 since it is fully observed. This conditioning information may include the direction of the edge, its distance, and any properties about the words in the sentence. We found that conditioning on direction improved performance: we rewrite the c distributions as c(ej | e0i, sign(j − i)) and denote this model by CCV1. 578 We note that we can also include constraints in the sum over possible parents and still preserve concavity. Naseem et al. (2010) found that adding parentchild constraints to a grammar induction system can improve performance dramatically. We employ one simple rule: roots are likely to be verbs.2 We modify CCV1 to restrict the summation over parents to exclude e&apos; if the child word is not a verb.3 We only employ this restriction during EM learning for sentences containing at least one verb. For sentences without verbs, we allow all words to be the root. We denote this model by CCV2. In related work, Brody (2010) also developed grammar induction models based on the IBM word alignment models. However, while our goal is to </context>
</contexts>
<marker>Naseem, Chen, Barzilay, Johnson, 2010</marker>
<rawString>T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010. Using universal linguistic knowledge to guide grammar induction. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S K¨ubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>shared task on dependency parsing.</title>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>In Proc. of CoNLL.</booktitle>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Das</author>
<author>R McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="9696" citStr="Petrov et al. (2012)" startWordPosition="1610" endWordPosition="1613">sed in our concave models. The DMV also has multinomial distributions for deciding whether to stop or continue generating children in each direction considering whether any children have already been generated in that direction. The majority of researchers use the original initializer from Klein and Manning (2004), denoted here K&amp;M. K&amp;M is a deterministic harmonic initializer that sets parent-child token affinities inversely 2This is similar to the rule used by Mareˇcek and ˇZabokrtsk´y (2011) with empirical success. 3As verbs, we take all tags that map to V in the universal tag mappings from Petrov et al. (2012). Thus, to apply this constraint to a new language, one would have to produce a similar tag mapping or identify verb tags through manual inspection. Model Init. Train &lt; 10 Train &lt; 20 &lt;10 Test Test &lt;00 &lt;00 &lt;10 ATTRIGHT N/A 38.4 31.7 38.4 31.7 CCV1 UNIF 31.4 25.6 31.0 23.7 CCV2 UNIF 43.1 28.6 43.9 27.1 UNIF 21.3 17.6 21.3 16.4 RAND* 41.0 31.8 - - DMV K&amp;M 44.1 32.9 51.9 37.8 CCV1 45.3 30.9 53.9 36.7 CCV2 54.3 43.0 64.3 53.1 Shared LN K&amp;M 61.3 41.4 L-EVG RAND† 68.8 - Feature DMV K&amp;M 63.0 - LexTSG-DMV K&amp;M 67.7 55.7 Posterior Reg. K&amp;M 64.3 53.3 Punc/UTags K&amp;M&apos; - 59.1# Table 1: English attachment acc</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>S. Petrov, D. Das, and R. McDonald. 2012. A universal part-of-speech tagset. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>From Baby Steps to Leapfrog: How “Less is More” in unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="15171" citStr="Spitkovsky et al., 2010" startWordPosition="2561" endWordPosition="2564">tails are omitted for space but are the same as those given by Cohen (2011). Average log-likelihoods (micro-averaged across sentences) achieved by EM training are shown in the final column of Tab. 2. CCV2 leads to substantiallyhigher likelihoods than the other initializers, suggesting that the verb-root constraint is helping EM to find better local optima.5 5 Discussion Staged training has been shown to help unsupervised learning in the past, from early work in grammar induction (Lari and Young, 1990) and word alignment (Brown et al., 1993) to more recent work in dependency grammar induction (Spitkovsky et al., 2010). While we do not yet offer a generic procedure for extracting a concave approximation from any model for unsupervised learning, our results contribute evidence in favor of the general methodology of staged training in unsupervised learning, and provide a simple and powerful initialization method for dependency grammar induction. Acknowledgments We thank Shay Cohen, Dipanjan Das, Val Spitkovsky, and members of the ARK research group for helpful comments that improved this paper. This research was supported in part by the NSF through grant IIS-0915187, the U. S. Army Research Laboratory and the</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2010</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010. From Baby Steps to Leapfrog: How “Less is More” in unsupervised dependency parsing. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>A X Chang</author>
<author>D Jurafsky</author>
</authors>
<title>Unsupervised dependency parsing without gold part-of-speech tags.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="10651" citStr="Spitkovsky et al. (2011" startWordPosition="1787" endWordPosition="1790"> 41.0 31.8 - - DMV K&amp;M 44.1 32.9 51.9 37.8 CCV1 45.3 30.9 53.9 36.7 CCV2 54.3 43.0 64.3 53.1 Shared LN K&amp;M 61.3 41.4 L-EVG RAND† 68.8 - Feature DMV K&amp;M 63.0 - LexTSG-DMV K&amp;M 67.7 55.7 Posterior Reg. K&amp;M 64.3 53.3 Punc/UTags K&amp;M&apos; - 59.1# Table 1: English attachment accuracies on Section 23, for short sentences (&lt;10 words) and all (&lt;00). We include selected results on this same test set: Shared LN = Cohen and Smith (2009), L-EVG = Headden III et al. (2009), Feature DMV = Berg-Kirkpatrick et al. (2010), LexTSGDMV = Blunsom and Cohn (2010), Posterior Reg. = Gillenwater et al. (2010), Punc/UTags = Spitkovsky et al. (2011a). K&amp;M&apos; is from Spitkovsky et al. (2011b). *Accuracies are averages over 50 random initializers; u = 10.9 for test sentences &lt; 10 and 8.3 for all. †Used many random initializers with unsupervised run selection. $Used staged training with sentences &lt; 45 words. proportional to their distances, then normalizes to obtain probability distributions. K&amp;M is often described as corresponding to an initial E step for an unspecified model that favors short attachments. Procedure We run EM for our concave models for 100 iterations. We evaluate the learned models directly as parsers on the test data and a</context>
</contexts>
<marker>Spitkovsky, Alshawi, Chang, Jurafsky, 2011</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, A. X. Chang, and D. Jurafsky. 2011a. Unsupervised dependency parsing without gold part-of-speech tags. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Punctuation: Making a point in unsupervised dependency parsing.</title>
<date>2011</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="10651" citStr="Spitkovsky et al. (2011" startWordPosition="1787" endWordPosition="1790"> 41.0 31.8 - - DMV K&amp;M 44.1 32.9 51.9 37.8 CCV1 45.3 30.9 53.9 36.7 CCV2 54.3 43.0 64.3 53.1 Shared LN K&amp;M 61.3 41.4 L-EVG RAND† 68.8 - Feature DMV K&amp;M 63.0 - LexTSG-DMV K&amp;M 67.7 55.7 Posterior Reg. K&amp;M 64.3 53.3 Punc/UTags K&amp;M&apos; - 59.1# Table 1: English attachment accuracies on Section 23, for short sentences (&lt;10 words) and all (&lt;00). We include selected results on this same test set: Shared LN = Cohen and Smith (2009), L-EVG = Headden III et al. (2009), Feature DMV = Berg-Kirkpatrick et al. (2010), LexTSGDMV = Blunsom and Cohn (2010), Posterior Reg. = Gillenwater et al. (2010), Punc/UTags = Spitkovsky et al. (2011a). K&amp;M&apos; is from Spitkovsky et al. (2011b). *Accuracies are averages over 50 random initializers; u = 10.9 for test sentences &lt; 10 and 8.3 for all. †Used many random initializers with unsupervised run selection. $Used staged training with sentences &lt; 45 words. proportional to their distances, then normalizes to obtain probability distributions. K&amp;M is often described as corresponding to an initial E step for an unspecified model that favors short attachments. Procedure We run EM for our concave models for 100 iterations. We evaluate the learned models directly as parsers on the test data and a</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2011</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctuation: Making a point in unsupervised dependency parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>M Galley</author>
</authors>
<title>Why initialization matters for IBM Model 1: Multiple optima and nonstrict convexity.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1639" citStr="Toutanova and Galley, 2011" startWordPosition="244" endWordPosition="247">ich is the only model commonly used for unsupervised learning in NLP that has a concave loglikelihood function.1 For other models, such as those used in unsupervised part-of-speech tagging and grammar induction, and indeed for more sophisticated word alignment models, the log-likelihood function maximized by EM is non-concave. As a result, researchers are obligated to consider initialization in addition to model design (Klein and Manning, 2004; Goldberg et al., 2008). For example, consider the dependency grammar induction results shown in Table 1 when training the 1It is not strictly concave (Toutanova and Galley, 2011). widely used dependency model with valence (DMV; Klein and Manning, 2004). Using uniform distributions for initialization (UNIF) results in an accuracy of 17.6% on the test set, well below the baseline of attaching each word to its right neighbor (ATTACHRIGHT, 31.7%). Furthermore, when using a set of 50 random initializers (RAND), the standard deviation of the accuracy is an alarming 8.3%. In light of this sensitivity to initialization, it is compelling to consider unsupervised models with concave log-likelihood functions, which may provide stable, data-supported initializers for more complex</context>
</contexts>
<marker>Toutanova, Galley, 2011</marker>
<rawString>K. Toutanova and M. Galley. 2011. Why initialization matters for IBM Model 1: Multiple optima and nonstrict convexity. In Proc. of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>