<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000029">
<title confidence="0.7361605">
Hierarchical Phrase-Based Translation Representations
Gonzalo Iglesias⋆ Cyril Allauzent William Byrne⋆
</title>
<author confidence="0.914665">
Adri`a de Gispert⋆ Michael Rileyt
</author>
<affiliation confidence="0.999147">
⋆Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K.
</affiliation>
<email confidence="0.856899">
{gi212,wjb31,ad465}@eng.cam.ac.uk
</email>
<address confidence="0.468505">
t Google Research, 76 Ninth Avenue, New York, NY 10011
</address>
<email confidence="0.99382">
{allauzen,riley}@google.com
</email>
<sectionHeader confidence="0.997322" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999214789473684">
This paper compares several translation rep-
resentations for a synchronous context-free
grammar parse including CFGs/hypergraphs,
finite-state automata (FSA), and pushdown
automata (PDA). The representation choice is
shown to determine the form and complex-
ity of target LM intersection and shortest-path
algorithms that follow. Intersection, shortest
path, FSA expansion and RTN replacement al-
gorithms are presented for PDAs. Chinese-to-
English translation experiments using HiFST
and HiPDT, FSA and PDA-based decoders,
are presented using admissible (or exact)
search, possible for HiFST with compact
SCFG rulesets and HiPDT with compact LMs.
For large rulesets with large LMs, we intro-
duce a two-pass search strategy which we then
analyze in terms of search errors and transla-
tion performance.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.971524588235294">
Hierarchical phrase-based translation, using a syn-
chronous context-free translation grammar (SCFG)
together with an n-gram target language model
(LM), is a popular approach in machine transla-
tion (Chiang, 2007). Given a SCFG G and an n-
gram language model M, this paper focuses on how
to decode with them, i.e. how to apply them to the
source text to generate a target translation. Decod-
ing has three basic steps, which we first describe
in terms of the formal languages and relations in-
volved, with data representations and algorithms to
follow.
1. Translating the source sentence s with G
to give target translations: T = {s} o G,
a (weighted) context-free language resulting
from the composition of a finite language and
the algebraic relation G for SCFG G.
</bodyText>
<listItem confidence="0.9775445">
2. Applying the language model to these target
translations: L=T nM, a (weighted) context-
free language resulting from the intersection
of a context-free language and the regular lan-
guage M for M.
3. Searching for the translation and language
model combination with the highest-probablity
path: ˆL=argmaxl∈LL
</listItem>
<bodyText confidence="0.999733148148148">
Of course, decoding requires explicit data represen-
tations and algorithms for combining and searching
them. In common to the approaches we will con-
sider here, s is applied to G by using the CYK algo-
rithm in Step 1 and M is represented by a finite au-
tomaton in Step 2. The choice of the representation
of T in many ways determines the remaining de-
coder representations and algorithms needed. Since
{s} is a finite language and we assume through-
out that G does not allow unbounded insertions,
T and L are, in fact, regular languages. As such,
T and L have finite automaton representations Tf
and Lf. In this case, weighted finite-state intersec-
tion and single-source shortest path algorithms (us-
ing negative log probabilities) can be used to solve
Steps 2 and 3 (Mohri, 2009). This is the approach
taken in (Iglesias et al., 2009a; de Gispert et al.,
2010). Instead T and L can be represented by hy-
pergraphs Th and Lh (or very similarly context-free
rules, and-or trees, or deductive systems). In this
case, hypergraph intersection with a finite automa-
ton and hypergraph shortest path algorithms can be
used to solve Steps 2 and 3 (Huang, 2008). This
is the approach taken by Chiang (2007). In this
paper, we will consider another representation for
context-free languages T and L as well, pushdown
automata (PDA) Tp and Lp, familiar from formal
</bodyText>
<page confidence="0.893995">
1373
</page>
<note confidence="0.9577665">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1373–1383,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999762042553192">
language theory (Aho and Ullman, 1972). We will
describe PDA intersection with a finite automaton
and PDA shortest-path algorithms in Section 2 that
can be used to solve Steps 2 and 3. It cannot be
over-emphasized that the CFG, hypergraph and PDA
representations of T are used for their compactness
rather than for expressing non-regular languages.
As presented so far, the search performed in Step
3 is admissible (or exact) – the true shortest path
is found. However, the search space in MT can be
quite large. Many systems employ aggressive prun-
ing during the shortest-path computation with little
theoretical or empirical guarantees of correctness.
Further, such pruning can greatly complicate any
complexity analysis of the underlying representa-
tions and algorithms. In this paper, we will exclude
any inadmissible pruning in the shortest-path algo-
rithm itself. This allows us in Section 3 to compare
the computational complexity of using these differ-
ent representations. We show that the PDA represen-
tation is particularly suited for decoding with large
SCFGs and compact LMs.
We present Chinese-English translation results
under the FSA and PDA translation representations.
We describe a two-pass translation strategy which
we have developed to allow use of the PDA repre-
sentation in large-scale translation. In the first pass,
translation is done using a lattice-generating version
of the shortest path algorithm. The full translation
grammar is used but with a compact, entropy-pruned
version (Stolcke, 1998) of the full language model.
This first-step uses admissible pruning and lattice
generation under the compact language model. In
the second pass, the original, unpruned LM is simply
applied to the lattices produced in the first pass. We
find that entropy-pruning and first-pass translation
can be done so as to introduce very few search errors
in the overall process; we can identify search errors
in this experiment by comparison to exact transla-
tion under the full translation grammar and language
model using the FSA representation. We then inves-
tigate a translation grammar which is large enough
that exact translation under the FSA representation
is not possible. We find that translation is possible
using the two-pass strategy with the PDA translation
representation and that gains in BLEU score result
from using the larger translation grammar.
</bodyText>
<sectionHeader confidence="0.667701" genericHeader="introduction">
1.1 Related Work
</sectionHeader>
<bodyText confidence="0.999980641025641">
There is extensive prior work on computational ef-
ficiency and algorithmic complexity in hierarchical
phrase-based translation. The challenge is to find al-
gorithms that can be made to work with large trans-
lation grammars and large language models.
Following the original algorithms and analysis of
Chiang (2007), Huang and Chiang (2007) devel-
oped the cube-growing algorithm, and more recently
Huang and Mi (2010) developed an incremental de-
coding approach that exploits left-to-right nature of
the language models.
Search errors in hierarchical translation, and in
translation more generally, have not been as exten-
sively studied; this is undoubtedly due to the diffi-
culties inherent in finding exact translations for use
in comparison. Using a relatively simple phrase-
based translation grammar, Iglesias et al. (2009b)
compared search via cube-pruning to an exact FST
implementation (Kumar et al., 2006) and found that
cube-pruning suffered significant search errors. For
Hiero translation, an extensive comparison of search
errors between the cube pruning and FSA imple-
mentation was presented by Iglesias et al. (2009a)
and de Gispert et al. (2010). Relaxation techniques
have also recently been shown to finding exact so-
lutions in parsing (Koo et al., 2010) and in SMT
with tree-to-string translation grammars and trigram
language models (Rush and Collins, 2011), much
smaller models compared to the work presented in
this paper.
Although entropy-pruned language models have
been used to produce real-time translation sys-
tems (Prasad et al., 2007), we believe our use of
entropy-pruned language models in two-pass trans-
lation to be novel. This is an approach that is widely-
used in automatic speech recognition (Ljolje et al.,
1999) and we note that it relies on efficient represen-
tation of very large search spaces T for subsequent
rescoring, as is possible with FSAs and PDAs.
</bodyText>
<sectionHeader confidence="0.974343" genericHeader="method">
2 Pushdown Automata
</sectionHeader>
<bodyText confidence="0.997573">
In this section, we formally define pushdown au-
tomata and give intersection, shortest-path and re-
lated algorithms that will be needed later.
Informally, pushdown automata are finite au-
tomata that have been augmented with a stack. Typ-
</bodyText>
<page confidence="0.993184">
1374
</page>
<figure confidence="0.999015666666667">
a
(a) (b)
(c) (d)
</figure>
<figureCaption confidence="0.9978795">
Figure 1: PDA Examples: (a) Non-regular PDA accept-
ing {anbn|n ∈ N}. (b) Regular (but not bounded-stack)
PDA accepting a*b*. (c) Bounded-stack PDA accepting
a*b* and (d) its expansion as an FSA.
</figureCaption>
<bodyText confidence="0.99983965">
ically this is done by adding a stack alphabet and la-
beling each transition with a stack operation (a stack
symbol to be pushed onto, popped or read from the
stack) in additon to the usual input label (Aho and
Ullman, 1972; Berstel, 1979) and weight (Kuich
and Salomaa, 1986; Petre and Salomaa, 2009). Our
equivalent representation allows a transition to be la-
beled by a stack operation or a regular input symbol
but not both. Stack operations are represented by
pairs of open and close parentheses (pushing a sym-
bol on and popping it from the stack). The advantage
of this representation is that is identical to the finite
automaton representation except that certain sym-
bols (the parentheses) have special semantics. As
such, several finite-state algorithms either immedi-
ately generalize to this PDA representation or do so
with minimal changes. The algorithms described in
this section have been implemented in the PDT ex-
tension (Allauzen and Riley, 2011) of the OpenFst
library (Allauzen et al., 2007).
</bodyText>
<subsectionHeader confidence="0.950829">
2.1 Definitions
</subsectionHeader>
<bodyText confidence="0.961680545454546">
A (restricted) Dyck language consist of “well-
formed” or “balanced” strings over a finite num-
ber of pairs of parentheses. Thus the string
( [ ( ) ( ) ] { } [ ] ) ( ) is in the Dyck language over 3
pairs of parentheses.
More formally, let A and A be two finite alpha-
bets such that there exists a bijection f from A to
A. Intuitively, f maps an open parenthesis to its cor-
responding close parenthesis. Let a¯ denote f(a) if
a ∈ A and f`(a) if a_∈ A. The Dyck language
DA over the alphabet A = A ∪ A is then the lan-
guage defined by the following context-free gram-
mar: 5 → ǫ, 5 → 55 and 5 → a5¯a for all a ∈ A.
We define the mapping cA :
cA(x) is the string obtained by iteratively deleting
from x all factors of the form a¯a with a ∈ A. Ob-
serve that DA =c�1
A (ǫ).
Let A and B be two finite alphabets such that
B ⊆ A, we define the mapping rB : A* → B*
by rB(x1 ... xn) = y1 ... yn with yi = xi if xi ∈ B
and yi =ǫ otherwise.
A weighted pushdown automaton (PDA) T over
the tropical semiring (R ∪ {∞}, min, +, ∞, 0) is
a 9-tuple (E, H, H, Q, E, I, F, ρ) where E is the fi-
nite input alphabet, H and H are the finite open and
close parenthesis alphabets, Q is a finite set of states,
I ∈ Q the initial state, F ⊆ Q the set of final states,
E ⊆ Q × (E ∪ H_∪ {ǫ}) × (R ∪ {∞}) × Q a fi-
nite set of transitions, and ρ : F → R ∪ {∞} the
final weight function. Let e = (p[e], i[e], w[e], n[e])
denote a transition in E.
A path 7r is a sequence of transitions 7r =e1 ... en
such that n[ei]=p[ei+1] for 1 ≤ i &lt; n. We then de-
fine p[7r] = p[e1], n[7r] = n[en], i[7r] = i[e1] ··· i[en],
and w[7r]=w[e1] + ... + w[en].
A path 7r is accepting if p[7r] = I and n[7r] ∈ F.
A path 7r is balanced if rn(i[7r]) ∈ Dn. A balanced
path 7r accepts the string x ∈ E* if it is a balanced
accepting path such that rE(i[7r])=x.
The weight associated by T to a string x ∈ E*
is T(x) = minrEP(x) w[7r] + ρ(n[7r]) where P(x)
denotes the set of balanced paths accepting x. A
weighted language is recognizable by a weighted
pushdown automaton iff it is context-free. We de-
fine the size of T as |T |=|Q|+|E|.
A PDA T has a bounded stack if there exists K ∈
N such that for any sub-path 7r of any balanced path
in T: |cn(rn(i[7r])) |≤ K. If T has a bounded stack,
then it represents a regular language. Figure 1 shows
non-regular, regular and bounded-stack PDAs.
A weightedfinite automaton (FSA) can be viewed
as a PDA where the open and close parentheses al-
phabets are empty, see (Mohri, 2009) for a stand-
alone definition.
</bodyText>
<figure confidence="0.998095891304348">
0
(
E
E
2
1
)
E
b
3
a
1
0
(
E
2
) 3
b
0
E
(
)
3
1
a
(
2
4
)
b
5
0,E
E
E
3,E
1,(
E
E
a
2,(
4,(
E
b
5,(
_A* →
A* as follow.
</figure>
<page confidence="0.862047">
1375
</page>
<subsectionHeader confidence="0.989809">
2.2 Expansion Algorithm
</subsectionHeader>
<bodyText confidence="0.999031705882353">
Given a bounded-stack PDA T, the expansion of T
is the FSA T′ equivalent to T defined as follows.
A state in T′ is a pair (q, z) where q is a state in T
and z ∈ H∗. A transition (q, a, w, q′) in T results in
a transition ((q, z), a′, w, (q′, z′)) in T′ only when:
(a) a∈E ∪ {c}, z′ =z and a′ =a, (b) a∈H, z′ =za
and a′ = c, or (c) a ∈ H, z′ is such that z = z′a
and a′ = c. The initial state of T′ is I′ = (I, c). A
state (q, z) in T′ is final if q is final in T and z = c
(p′((q, c))=p(q)). The set of states of T′ is the set of
pairs (q, z) that can be reached from an initial state
by transitions defined as above. The condition that
T has a bounded stack ensures that this set is finite
(since it implies that for any (q, z), |z |≤ K).
The complexity of the algorithm is linear in
O(|T′|) = O(e|T|). Figure 1d show the result of the
algorithm when applied to the PDA of Figure 1c.
</bodyText>
<subsectionHeader confidence="0.995523">
2.3 Intersection Algorithm
</subsectionHeader>
<bodyText confidence="0.970272037037037">
The class of weighted pushdown automata is closed
under intersection with weighted finite automata
(Bar-Hillel et al., 1964; Nederhof and Satta, 2003).
Considering a pair (T1, T2) where one element is an
FSA and the other element a PDA, then there exists
a PDA T1 ∩T2, the intersection of T1 and T2, such
that for all x ∈ E∗: (T1 ∩T2)(x) = T1(x)+T2(x).
We assume in the following that T2 is an FSA. We
also assume that T2 has no input-c transitions. When
T2 has input-c transitions, an epsilon filter (Mohri,
2009; Allauzen et al., 2011) generalized to handle
parentheses can be used.
A state in T =T1∩T2 is a pair (q1, q2) where q1 is
a state of T1 and q2 a state of T2. The initial state is
I =(I1, I2). Given a transition e1 =(q1, a, w1, q′1) in
T1, transitions out of (q1, q2) in T are obtained using
the following rules.
If a ∈ E, then e1 can be matched with a tran-
sition (q2, a, w2, q′2) in T2 resulting a transition
((q1, q2), a, w1+w2, (q′1, q′2)) in T.
If a = c, then e1 is matched with staying in q2
resulting in a transition ((q1, q2), c, w1, (q′1, q2)).
Finally, if a ∈ H, e1 is also matched
with staying in q2, resulting in a transition
((q1, q2), a, w1, (q′1, q2)) in T.
A state (q1, q2) in T is final when both q1 and q2
are final, and then p((q1, q2))=p1(q1)+p2(q2).
</bodyText>
<equation confidence="0.885044384615385">
SHORTESTDISTANCE(T)
1 for each q E Q and a E H do
2 B[q, a] +- 0
3 GETDISTANCE(T, I)
4 return d[f, I]
RELAX(q, s, w, S)
1 if d[q, s] &gt; w then
2 d[q, s] +- w
3 if q E� S then
4 ENQUEUE(S, q)
GETDISTANCE(T,s)
1 for each q E Q do
2 d[q, s] +- oo
</equation>
<figure confidence="0.98066725">
3 d[s, s] +- 0
4 Ss +- s
5 while Ss =/0 do
6 q +- HEAD(Ss)
7 DEQUEUE(Ss)
8 for each e E E[q] do
9 if i[e] E E U {ǫ} then
10 RELAX(n[e], s, d[q, s] + w[e], Ss)
11 elseif i[e] E H then
12 B[s, i[e]] +- B[s, i[e]] U {e}
13 elseif i[e] E H then
14 if d[n[e], n[e]] is undefined then
15 GETDISTANCE(T, n[e])
16 for each e′ E B[n[e], i[e]] do
17 w +- d[q, s] + w[e] + d[p[e′], n[e]] + w[e′]
18 RELAX(n[e′], s, w, Ss)
</figure>
<figureCaption confidence="0.9973865">
Figure 2: PDA shortest distance algorithm. We assume
that F ={f} and p(f)=0 to simplify the presentation.
</figureCaption>
<bodyText confidence="0.969409">
The complexity of the algorithm is in O(|T1||T2|).
</bodyText>
<subsectionHeader confidence="0.998468">
2.4 Shortest Distance and Path Algorithms
</subsectionHeader>
<bodyText confidence="0.999947133333333">
A shortest path in a PDA T is a balanced accepting
path with minimal weight and the shortest distance
in T is the weight of such a path. We show that when
T has a bounded stack, shortest distance and short-
est path can be computed in O(|T|3 log |T|) time
(assuming T has no negative weights) and O(|T |2)
space.
Given a state s in T with at least one incoming
open parenthesis transition, we denote by C3 the set
of states that can be reached from s by a balanced
path. If s has several incoming open parenthesis
transitions, a naive implementation might lead to the
states in C3 to be visited up to exponentially many
times. The basic idea of the algorithm is to memo-
ize the shortest distance from s to states in C3. The
</bodyText>
<page confidence="0.958006">
1376
</page>
<bodyText confidence="0.999781735849057">
pseudo-code is given in Figure 2.
GETDISTANCE(T, s) starts a new instance of the
shortest-distance algorithm from s using the queue
S3, initially containing s. While the queue is not
empty, a state is dequeued and its outgoing transi-
tions examined (line 5-9). Transitions labeled by
non-parenthesis are treated as in Mohri (2009) (line
9-10). When the considered transition e is labeled by
a close parenthesis, it is remembered that it balances
all incoming open parentheses in s labeled by i[e]
by adding e to B[s, i[e]] (line 11-12). Finally, when
e is labeled with an open parenthesis, if its destina-
tion has not already been visited, a new instance is
started from n[e] (line 14-15). The destination states
of all transitions balancing e are then relaxed (line
16-18).
The space complexity of the algorithm is
quadratic for two reasons. First, the number of
non-infinity d[q, s] is |Q|2. Second, the space re-
quired for storing B is at most in O(|E|2) since
for each open parenthesis transition e, the size of
|B[n[e],i[e]] |is O(|E|) in the worst case. This
last observation also implies that the cumulated
number of transitions examined at line 16 is in
O(N|Q ||E|2) in the worst case, where N denotes
the maximal number of times a state is inserted in
the queue for a given call of GETDISTANCE. As-
suming the cost of a queue operation is F(n) for a
queue containing n elements, the worst-case time
complexity of the algorithm can then be expressed
as O(N|T |3 F(|T |)). When T contains no negative
weights, using a shortest-first queue discipline leads
to a time complexity in O(|T |3 log |T|). When all
the C3’s are acyclic, using a topological order queue
discipline leads to a O(|T |3) time complexity.
In effect, we are solving a k-sources shortest-
path problem with k single-source solutions. A po-
tentially better approach might be to solve the k-
sources or k-pairs problem directly (Hershberger et
al., 2003).
When T has been obtained by converting an RTN
or an hypergraph into a PDA (Section 2.5), the poly-
nomial dependency in |T |becomes a linear depen-
dency both for the time and space complexities. In-
deed, for each q in T, there exists a unique s such
that d[q, s] is non-infinity. Moreover, for each close
parenthesis transistion e, there exists a unique open
parenthesis transition e′ such that e E B[n[e′], i[e′]].
When each component of the RTN is acyclic, the
complexity of the algorithm is hence in O(|T|) in
time and space.
The algorithm can be modified to compute the
shortest path by keeping track of parent pointers.
</bodyText>
<subsectionHeader confidence="0.977458">
2.5 Replacement Algorithm
</subsectionHeader>
<bodyText confidence="0.997508590909091">
A recursive transition network (RTN) can be speci-
fied by (N, E, (T„)„CN, S) where N is an alphabet
of nonterminals, E is the input alphabet, (T„)„CN is
a family of FSAs with input alphabet E U N, and
S EN is the root nonterminal.
A string x E E* is accepted by R if there exists
an accepting path 7r in TS such that recursively re-
placing any transition with input label v E N by an
accepting path in T„ leads to a path 7r* with input x.
The weight associated by R is the minimum over all
such 7r* of w[7r*]+ρS(n[7r*]).
Given an RTN R, the replacement of R is the
PDA T equivalent to R defined by the 9-tuple
(E, n, n, Q, E, I,F,σ,ρ) with n = Q = U„CN Q„,
I = IS, F = FS, ρ = ρS, and E = U„CN UeCEν Ee
where Ee = {e} if i[e] E� N and Ee =
{(p[e],n[e],w[e],Iµ),(f,n[e],ρµ(f),n[e])|f EFµ}
with µ=i[e]EN otherwise.
The complexity of the construction is in O(|T |).
If |F„ |= 1, then |T |= O(E„CN |T„|) = O(|R|).
Creating a superfinal state for each T„ would lead to
a T whose size is always linear in the size of R.
</bodyText>
<sectionHeader confidence="0.99854" genericHeader="method">
3 Hierarchical Phrase-Based Translation
Representation
</sectionHeader>
<bodyText confidence="0.986715466666667">
In this section, we compare several different repre-
sentations for the target translations T of the source
sentence s by synchronous CFG G prior to language
model M application. As discussed in the introduc-
tion, T is a context-free language. For example, sup-
pose it corresponds to:
S-+abXdg, S-+acXfg, and X-+bc.
Figure 3 shows several alternative representations of
T: Figure 3a shows the hypergraph representation of
this grammar; there is a 1:1 correspondence between
each production in the CFG and each hyperedge in
the hypergraph. Figure 3b shows the RTN represen-
tation of this grammar with a 1:1 correspondence be-
tween each production in the CFG and each path in
the RTN; this is the translation representation pro-
</bodyText>
<page confidence="0.951395">
1377
</page>
<figure confidence="0.999667104651163">
0
a
a
c X
6 7 8 f 9 g b c
10 11 12 13
1
b X d 4 g 5
2 3
2
b
1
3
X
3
S X
(b) RTN
2
c
2
(a) Hypergraph
1
1
S
a
4
d
4
5
f
5
g
a
0 1
a
0 1
a
0 1
b
c
b
8
c [
2
3
2
3
b
c
X
X
S X
(a) RTN
(
b 9
4
5
2
3
d
f 6 g 7
b
b
(b) PDA
4
5
c
c
c
10
6
7
)
]
d
f 8 g 9
4
5
b c
0 1 2
d
f
6
g
7
(d) FSA
</figure>
<figureCaption confidence="0.99992">
Figure 3: Alternative translation representations
</figureCaption>
<bodyText confidence="0.999832608695652">
duced by the HiFST decoder (Iglesias et al., 2009a;
de Gispert et al., 2010). Figure 3c shows the push-
down automaton representation generated from the
RTN with the replacement algorithm of Section 2.5.
Since s is a finite language and G does not allow
unbounded insertion, Tp has a bounded stack and
T is, in fact, a regular language. Figure 3d shows
the finite-state automaton representation of T gen-
erated by the PDA using the expansion algorithm
of Section 2.2. The HiFST decoder converts its
RTN translation representation immediately into the
finite-state representation using an algorithm equiv-
alent to converting the RTN into a PDA followed by
PDA expansion.
As shown in Figure 4, an advantage of the RTN,
PDA, and FSA representations is that they can bene-
fit from FSA epsilon removal, determinization and
minimization algorithms applied to their compo-
nents (for RTNs and PDAs) or their entirety (for
FSAs). For the complexity discussion below, how-
ever, we disregard these optimizations. Instead we
focus on the complexity of each MT step described
in the introduction:
</bodyText>
<listItem confidence="0.767987875">
1. SCFG Translation: Assuming that the parsing
of the input is performed by a CYK parse, then
the CFG, hypergraph, RTN and PDA represen-
tations can be generated in O(|s|3|G|) time and
space (Aho and Ullman, 1972). The FSA rep-
resentation can require an additional O(e|s|3|G|)
time and space since the PDA expansion can be
exponential.
2. Intersection: The intersection of a CFG Th
with a finite automaton M can be performed by
the classical Bar-Hillel algorithm (Bar-Hillel
et al., 1964) with time and space complex-
ity O(|Th||M|3).1 The PDA intersection algo-
rithm from Section 2.3 has time and space com-
plexity O(|Tp||M|). Finally, the FSA intersec-
tion algorithm has time and space complexity
O(|Tf ||M|) (Mohri, 2009).
3. Shortest Path: The shortest path algorithm on
the hypergraph, RTN, and FSA representations
requires linear time and space (given the under-
lying acyclicity) (Huang, 2008; Mohri, 2009).
As presented in Section 2.4, the PDA rep-
resentation can require time cubic and space
quadratic in |M|.2
</listItem>
<bodyText confidence="0.99945425">
Table 1 summarizes the complexity results. Note
the PDA representation is equivalent in time and su-
perior in space to the CFG/hypergraph representa-
tion, in general, and it can be superior in both space
</bodyText>
<footnote confidence="0.959182555555556">
1The modified Bar-Hillel construction described by Chi-
ang (2007) has time and space complexity O(|Th||M|4); the
modifications were introduced presumably to benefit the subse-
quent pruning method employed (but see Huang et al. (2005)).
2The time (resp. space) complexity is not cubic (resp.
quadratic) in |Tp||M|. Given a state q in Tp, there exists a
unique sq such that q belongs to Csq. Given a state (q1, q2)
in Tp n M, (q1, q2) E C(s1,s2) only if s1 = sq1, and hence
(q1, q2) belongs to at most |M |components.
</footnote>
<figure confidence="0.998715142857143">
0,e
0
a
a
a
a
1,e
6,e
6
1
b
c
b
c
2,e
7,e
2
7
(
[
e
e
11,(
11,[
11
b
b
b
(c) PDA
12,(
12,[
12
c
c
c
13,(
13,[
13
e
e
)
]
3,e
8,e
g
3 d 4 5
8 f g
9 10
d
f
4,e
9,e
g 5,e
g
10,e
(c) FSA
</figure>
<figureCaption confidence="0.997734">
Figure 4: Optimized translation representations
</figureCaption>
<page confidence="0.821801">
1378
</page>
<table confidence="0.99964875">
Representation Time Complexity Space Complexity
CFG/hypergraph O(|s|3 |G ||M|3) O(|s|3 |G ||M|3)
PDA O(|s|3 |G ||M|3) O(|s|3 |G ||M|2)
FSA O(e|3|3|G ||M|) O(e|3|3|G ||M|)
</table>
<tableCaption confidence="0.982239">
Table 1: Complexity using various target translation rep-
resentations.
</tableCaption>
<bodyText confidence="0.999968444444444">
and time to the FSA representation depending on the
relative SCFG and LM sizes. The FSA representa-
tion favors smaller target translation sets and larger
language models. Should a better complexity PDA
shortest path algorithm be found, this conclusion
could change. In practice, the PDA and FSA rep-
resentations benefit hugely from the optimizations
mentioned above, these optimizations improve the
time and space usage by one order of magnitude.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="method">
4 Experimental Framework
</sectionHeader>
<bodyText confidence="0.999993153846154">
We use two hierarchical phrase-based SMT de-
coders. The first one is a lattice-based decoder im-
plemented with weighted finite-state transducers (de
Gispert et al., 2010) and described in Section 3. The
second decoder is a modified version using PDAs
as described in Section 2. In order to distinguish
both decoders we call them HiFST and HiPDT, re-
spectively. The principal difference between the two
decoders is where the finite-state expansion step is
done. In HiFST, the RTN representation is immedi-
ately expanded to an FSA. In HiPDT, this expansion
is delayed as late as possible - in the output of the
shortest path algorithm. Another possible configu-
ration is to expand after the LM intersection step but
before the shortest path algorithm; in practice this is
quite similar to HiFST.
In the following sections we report experiments
in Chinese-to-English translation. For translation
model training, we use a subset of the GALE 2008
evaluation parallel text;3 this is 2.1M sentences and
approximately 45M words per language. We re-
port translation results on a development set tune-nw
(1,755 sentences) and a test set test-nw (1,671 sen-
tences). These contain translations produced by the
GALE program and portions of the newswire sec-
tions of MT02 through MT06. In tuning the sys-
</bodyText>
<footnote confidence="0.994439666666667">
3See http://projects.ldc.upenn.edu/gale/data/catalog.html.
We excluded the UN material and the LDC2002E18,
LDC2004T08, LDC2007E08 and CUDonga collections.
</footnote>
<table confidence="0.632306">
0 7.5 × 10−9 7.5 × 10−8 7.5 × 10−7
207.5 20.2 4.1 0.9
</table>
<tableCaption confidence="0.9980325">
Table 2: Number of ngrams (in millions) in the 1st pass 4-gram
language models obtained with different θ values (top row).
</tableCaption>
<bodyText confidence="0.999344">
tems, standard MERT (Och, 2003) iterative param-
eter estimation under IBM BLEU4 is performed on
the development set.
The parallel corpus is aligned using MTTK (Deng
and Byrne, 2008) in both source-to-target and
target-to-source directions. We then follow stan-
dard heuristics (Chiang, 2007) and filtering strate-
gies (Iglesias et al., 2009b) to extract hierarchical
phrases from the union of the directional word align-
ments. We call a translation grammar the set of rules
extracted from this process. We extract two transla-
tion grammars:
</bodyText>
<listItem confidence="0.998888">
• A restricted grammar where we apply the fol-
lowing additional constraint: rules are only
considered if they have a forward translation
probability p &gt; 0.01. We call this G1. As will
be discussed later, the interest of this grammar
is that decoding under it can be exact, that is,
without any pruning in search.
• An unrestricted one without the previous con-
</listItem>
<bodyText confidence="0.962176052631579">
straint. We call this G2. This is a superset of
the previous grammar, and exact search under
it is not feasible for HiFST: pruning is required
in search.
The initial English language model is a Kneser-
Ney 4-gram estimated over the target side of the par-
allel text and the AFP and Xinhua portions of mono-
lingual data from the English Gigaword Fourth Edi-
tion (LDC2009T13). This is a total of 1.3B words.
We will call this language model M1. For large lan-
guage model rescoring we also use the LM M2 ob-
tained by interpolating M1 with a zero-cutoff stupid-
backoff (Brants et al., 2007) 5-gram estimated using
6.6B words of English newswire text.
We next describe how we build translation sys-
tems using entropy-pruned language models.
1. We build a baseline HiFST system that uses M1
and a hierarchical grammar G, parameters be-
ing optimized with MERT under BLEU.
</bodyText>
<footnote confidence="0.982341">
4See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl
</footnote>
<page confidence="0.994182">
1379
</page>
<bodyText confidence="0.622509333333333">
2. We then use entropy-based pruning of the lan-
guage model (Stolcke, 1998) under a relative
perplexity threshold of θ to reduce the size of
M1. We will call the resulting language model
as MB. Table 2 shows the number of n-grams
(in millions) obtained for different θ values.
</bodyText>
<listItem confidence="0.980809736842105">
3. We translate with MB using the same param-
eters obtained in MERT in step 1, except for
the word penalty, tuned over the lattices under
BLEU performance. This produces a transla-
tion lattice in the topmost cell that contains hy-
potheses with exact scores under the translation
grammar and MB.
4. Translation lattices in the topmost cell are
pruned with a likelihood-based beam width 0.
5. We remove the MB scores from the pruned
translation lattices and reapply M1, moving the
word penalty back to the original value ob-
tained in MERT. These operations can be car-
ried out efficiently via standard FSA opera-
tions.
6. Additionally, we can rescore the translation lat-
tices obtained in steps 1 or 5 with the larger
language model M2. Again, this can be done
via standard FSA operations.
</listItem>
<bodyText confidence="0.9999318">
Note that if 0 =∞ or if θ =0, the translation lattices
obtained in step 1 should be identical to the ones of
step 5. While the goal is to increase θ to reduce the
size of the language model used at Step 3, 0 will
have to increase accordingly so as to avoid pruning
away desirable hypotheses in Step 4. If 0 defines
a sufficiently wide beam to contain the hypotheses
which would be favoured by M1, faster decoding
with MB would be possible without incurring search
errors M1. This is investigated next.
</bodyText>
<sectionHeader confidence="0.978557" genericHeader="method">
5 Entropy-Pruned LM in Rescoring
</sectionHeader>
<bodyText confidence="0.999984046511628">
In Table 3 we show translation performance under
grammar G1 for different values of θ. Performance
is reported after first-pass decoding with MB (see
step 3 in Section 4), after rescoring with M1 (see
step 5) and after rescoring with M2 (see step 6). The
baseline (experiment number 1) uses θ = 0 (that is,
M1) for decoding.
Under translation grammar G1, HiFST is able to
generate an FSA with the entire space of possible
candidate hypotheses. Therefore, any degradation
in performance is only due to the MB involved in
decoding and the 0 applied prior to rescoring.
As shown in row number 2, for θ ≤ 10−9 the
system provides the same performance to the base-
line when 0 &gt; 8, while decoding time is reduced
by roughly 40%. This is because MB is 10% of the
size of the original language model M1, as shown
in Table 2. As MB is further reduced by increas-
ing θ (see rows number 3 and 4), decoding time is
also reduced. However, the beam width 0 required
in order to recover the good hypotheses in rescoring
increases, reaching 12 for experiment 3 and 15 for
experiment 4.
Regarding rescoring with the larger M2 (step 6
in Section 4), the system is also able to match the
baseline performance as long as 0 is wide enough,
given the particular MB used in first-pass decoding.
Interestingly, results show that a similar 0 value is
needed when rescoring either with M1 or M2.
The usage of entropy-pruned language models in-
crements speed at the risk of search errors. For in-
stance, comparing the outputs of systems 1 and 2
with 0 =10 in Table 3 we find 45 different 1-best hy-
potheses, even though the BLEU score is identical.
In other words, we have 45 cases in which system 2
is not able to recover the baseline output because the
1st-pass likelihood beam 0 is not wide enough. Sim-
ilarly, system 3 fails in 101 cases (0 = 12) and sys-
tem 4 fails in 95 cases. Interestingly, some of these
sentences would require impractically huge beams.
This might be due to the Kneser-Ney smoothing,
which interacts badly with entropy pruning (Chelba
et al., 2010).
</bodyText>
<sectionHeader confidence="0.964481" genericHeader="method">
6 Hiero with PDAs and FSAs
</sectionHeader>
<bodyText confidence="0.999970416666667">
In this section we contrast HiFST with HiPDT under
the same translation grammar and entropy-pruned
language models. Under the constrained grammar
G1 their performance is identical as both decoders
can generate the entire search space which can then
be rescored with M1 or M2 as shown in the previous
section.
Therefore, we now focus on the unconstrained
grammar G2, where exact search is not feasible for
HiFST. In order to evaluate this problem, we run
both decoders over tune-nw, restricting memory us-
age to 10 gigabytes. If this limit is reached in decod-
</bodyText>
<page confidence="0.952935">
1380
</page>
<table confidence="0.999677727272727">
HiFST (G1 + Mθ1 ) +M1 +M2
# 0 tune-nw test-nw time a tune-nw test-nw tune-nw test-nw
1 0 (M1) 34.3 34.5 0.68 - - - 34.8 35.6
2 7.5 x 10−9 32.0 32.8 0.38 10 34.8 35.6
9 34.3 34.5 34.9 35.5
8
3 7.5 x 10−8 29.5 30.0 0.28 12 34.2 34.5 34.7 35.6
9 34.3 34.4 34.8 35.2
8 34.2 35.1
4 7.5 x 10−7 26.0 26.4 0.20 15 34.2 34.5 34.7 35.6
12 34.4 35.5
</table>
<tableCaption confidence="0.942551666666667">
Table 3: Results (lowercase IBM BLEU scores) under G1 with various Mθ1 as obtained with several values of 0.
Performance in subsequent rescoring with M1 and M2 after likelihood-based pruning of the translation lattices for
various a is also reported. Decoding time, in seconds/word over test-nw, refers strictly to first-pass decoding.
</tableCaption>
<table confidence="0.998367142857143">
Exact search for G2 + Mθ1 with memory usage under 10 GB
# 0 HiFST HiPDT
Success Failure Success Failure
Expand Compose Compose Expand
2 7.5 x 10−9 12 51 37 40 8 52
3 7.5 x 10−8 16 53 31 76 1 23
4 7.5 x 10−7 18 53 29 99.8 0 0.2
</table>
<tableCaption confidence="0.94301175">
Table 4: Percentage of success in producing the 1-best translation under G2 with various Mθ1 when applying a hard
memory limitation of 10 GB, as measured over tune-nw (1755 sentences). If decoder fails, we report what step was
being done when the limit was reached. HiFST could be expanding into an FSA or composing the FSA with Mθ1;
HiPDT could be PDA composing with Mθ1 or PDA expanding into an FSA.
</tableCaption>
<table confidence="0.820590333333333">
HiPDT (G2 + Mθ1 ) +M1 +M2
0 tune-nw test-nw a tune-nw test-nw tune-nw test-nw
7.5 x 10−7 25.7 26.3 15 34.6 34.8 35.2 36.1
</table>
<tableCaption confidence="0.9947695">
Table 5: HiPDT performance on grammar G2 with 0 = 7.5 x 10−7. Exact search with HiFST is not possible under
these conditions: pruning during search would be required.
</tableCaption>
<bodyText confidence="0.98787688">
ing, the process is killed5. We report what internal
decoding operation caused the system to crash. For
HiFST, these include expansion into an FSA (Ex-
pand) and subsequent intersection with the language
model (Compose). For HiPDT, these include PDA
intersection with the language model (Compose) and
subsequent expansion into an FSA (Expand), using
algorithms described in Section 2.
Table 4 shows the number of times each decoder
succeeds in finding a hypothesis given the memory
limit, and the operations being carried out when they
fail to do so, when decoding with various Mθ1. With
0 =7.5 x 10−9 (row 2), HiFST can only decode 218
sentences, while HiPDT succeeds in 703 cases. The
5We used ulimit command. The experiment was carried out
over machines with different configurations and load. There-
fore, these numbers must be considered as approximate values.
differences between both decoders increase as the
Mθ1 is more reduced, and for 0 =7.5x10−7 (row 4),
HiPDT is able to perform exact search over all but
three sentences.
Table 5 shows performance using the latter con-
figuration (Table 4, row 4). After large language
model rescoring, HiPDT improves 0.5 BLEU over
baseline with G1 (Table 3, row 1).
</bodyText>
<sectionHeader confidence="0.998178" genericHeader="discussions">
7 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999811166666667">
HiFST fails to decode mainly because the expansion
into an FST leads to far too big search spaces (e.g.
fails 938 times under 0 = 7.5 x 10−8). If it suc-
ceeds in expanding the search space into an FST,
the decoder still has to compose with the language
model, which is also critical in terms of memory us-
</bodyText>
<page confidence="0.972576">
1381
</page>
<bodyText confidence="0.999959257142857">
age (fails 536 times). In contrast, HiPDT creates a
PDA, which is a more compact representation of the
search space and allows efficient intersection with
the language model before expansion into an FST.
Therefore, the memory usage is considerably lower.
Nevertheless, the complexity of the language model
is critical for the PDA intersection and very specially
the PDA expansion into an FST (fails 403 times for
θ =7.5 × 10−8).
With the algorithms presented in this paper, de-
coding with PDAs is possible for any translation
grammar as long as an entropy pruned LM is used.
While this allows exact decoding, it comes at the
cost of making decisions based on less complex
LMs, although this has been shown to be an ad-
equate strategy when applying compact CFG rule-
sets. On the other hand, HiFST cannot decode under
large translation grammars, thus requiring pruning
during lattice construction, but it can apply an un-
pruned LM in this process. We find that with care-
fully designed pruning strategies, HiFST can match
the performance of HiPDT reported in Table 5. But
without pruning in search, expansion directly into an
FST would lead to an explosion in terms of memory
usage. Of course, without memory constraints both
strategies would reach the same performance.
Overall, these results suggest that HiPDT is more
robust than HiFST when using complex hierarchi-
cal grammars. Conversely, FSTs might be more
efficient for search spaces described by more con-
strained hierarchical grammars. This suggests that
a hybrid solution could be effective: we could use
PDAs or FSTs e.g. depending on the number of
states of the FST representing the expanded search
space, or other conditions.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.998592125">
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7-ICT-2009-4) under grant
agreement number 247762, and was supported in
part by the GALE program of the Defense Advanced
Research Projects Agency, Contract No.HR0011-
06-C-0022, and a Google Faculty Research Award,
May 2010.
</bodyText>
<sectionHeader confidence="0.998215" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99986868">
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation and Compiling, volume 1-2.
Prentice-Hall.
Cyril Allauzen and Michael Riley, 2011. Pushdown
Transducers. http://pdt.openfst.org.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proceedings of CIAA, pages 11–23.
http://www.openfst.org.
Cyril Allauzen, Michael Riley, and Johan Schalkwyk.
2011. Filters for efficient composition of weighted
finite-state transducers. In Proceedings of CIAA, vol-
ume 6482 ofLNCS, pages 28–38. Springer.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Y. Bar-Hillel, editor, Language and Information: Se-
lected Essays on their Theory and Application, pages
116–150. Addison-Wesley.
Jean Berstel. 1979. Transductions and Context-Free
Languages. Teubner.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP-ACL,
pages 858–867.
Ciprian Chelba, Thorsten Brants, Will Neveitt, and Peng
Xu. 2010. Study on interaction between entropy prun-
ing and kneser-ney smoothing. In Proceedings of In-
terspeech, pages 2242–2245.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Adri`a de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hier-
archical phrase-based translation with weighted finite
state transducers and shallow-n grammars. Computa-
tional Linguistics, 36(3).
Yonggang Deng and William Byrne. 2008. HMM word
and phrase alignment for statistical machine transla-
tion. IEEE Transactions on Audio, Speech, and Lan-
guage Processing, 16(3):494–507.
Manfred Drosde, Werner Kuick, and Heiko Vogler, ed-
itors. 2009. Handbook of Weighted Automata.
Springer.
John Hershberger, Subhash Suri, and Amit Bhosle. 2003.
On the difficulty of some shortest path problems. In
Proceedings of STACS, volume 2607 of LNCS, pages
343–354. Springer.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings ofACL, pages 144–151.
</reference>
<page confidence="0.860792">
1382
</page>
<reference confidence="0.99950406779661">
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of EMNLP, pages 273–283.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005.
Machine translation as lexicalized parsing with hooks.
In Proceedings of the Ninth International Workshop
on Parsing Technology, Parsing ’05, pages 65–73,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Liang Huang. 2008. Advanced dynamic programming in
semiring and hypergraph frameworks. In Proceedings
of COLING, pages 1–18.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Hierarchical phrase-based
translation with weighted finite state transducers. In
Proceedings ofNAACL-HLT, pages 433–441.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
EACL, pages 380–388.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proceedings of EMNLP, pages 1288–1298.
Werner Kuich and Arto Salomaa. 1986. Semirings, au-
tomata, languages. Springer.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering, 12(1):35–75.
Andrej Ljolje, Fernando Pereira, and Michael Riley.
1999. Efficient general lattice generation and rescor-
ing. In Proceedings ofEurospeech, pages 1251–1254.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Drosde et al. (Drosde et al., 2009), chapter 6, pages
213–254.
Mark-Jan Nederhof and Giorgio Satta. 2003. Probabilis-
tic parsing as intersection. In Proceedings of 8th In-
ternational Workshop on Parsing Technologies, pages
137–148.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160–167.
Ion Petre and Arto Salomaa. 2009. Algebraic systems
and pushdown automata. In Drosde et al. (Drosde et
al., 2009), chapter 7, pages 257–289.
R. Prasad, K. Krstovski, F. Choi, S. Saleem, P. Natarajan,
M. Decerbo, and D. Stallard. 2007. Real-time speech-
to-speech translation for pdas. In Proceedings ofIEEE
International Conference on Portable Information De-
vices, pages 1 –5.
Alexander M. Rush and Michael Collins. 2011. Ex-
act decoding of syntactic translation models through
lagrangian relaxation. In Proceedings of ACL-HLT,
pages 72–82.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270–274.
</reference>
<page confidence="0.978383">
1383
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.378672">
<title confidence="0.999621">Hierarchical Phrase-Based Translation Representations</title>
<author confidence="0.734716">Cyril de_Michael</author>
<affiliation confidence="0.751754">of Engineering, University of Cambridge, Cambridge, CB2 1PZ,</affiliation>
<address confidence="0.899505">Research, 76 Ninth Avenue, New York, NY</address>
<abstract confidence="0.9987403">This paper compares several translation representations for a synchronous context-free grammar parse including CFGs/hypergraphs, finite-state automata (FSA), and pushdown automata (PDA). The representation choice is shown to determine the form and complexity of target LM intersection and shortest-path algorithms that follow. Intersection, shortest path, FSA expansion and RTN replacement algorithms are presented for PDAs. Chinese-to- English translation experiments using HiFST and HiPDT, FSA and PDA-based decoders, are presented using admissible (or exact) search, possible for HiFST with compact SCFG rulesets and HiPDT with compact LMs. For large rulesets with large LMs, we introduce a two-pass search strategy which we then analyze in terms of search errors and translation performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation and Compiling,</booktitle>
<volume>volume</volume>
<pages>1--2</pages>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="3802" citStr="Aho and Ullman, 1972" startWordPosition="588" endWordPosition="591">rees, or deductive systems). In this case, hypergraph intersection with a finite automaton and hypergraph shortest path algorithms can be used to solve Steps 2 and 3 (Huang, 2008). This is the approach taken by Chiang (2007). In this paper, we will consider another representation for context-free languages T and L as well, pushdown automata (PDA) Tp and Lp, familiar from formal 1373 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1373–1383, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics language theory (Aho and Ullman, 1972). We will describe PDA intersection with a finite automaton and PDA shortest-path algorithms in Section 2 that can be used to solve Steps 2 and 3. It cannot be over-emphasized that the CFG, hypergraph and PDA representations of T are used for their compactness rather than for expressing non-regular languages. As presented so far, the search performed in Step 3 is admissible (or exact) – the true shortest path is found. However, the search space in MT can be quite large. Many systems employ aggressive pruning during the shortest-path computation with little theoretical or empirical guarantees o</context>
<context position="8723" citStr="Aho and Ullman, 1972" startWordPosition="1365" endWordPosition="1368">a and give intersection, shortest-path and related algorithms that will be needed later. Informally, pushdown automata are finite automata that have been augmented with a stack. Typ1374 a (a) (b) (c) (d) Figure 1: PDA Examples: (a) Non-regular PDA accepting {anbn|n ∈ N}. (b) Regular (but not bounded-stack) PDA accepting a*b*. (c) Bounded-stack PDA accepting a*b* and (d) its expansion as an FSA. ically this is done by adding a stack alphabet and labeling each transition with a stack operation (a stack symbol to be pushed onto, popped or read from the stack) in additon to the usual input label (Aho and Ullman, 1972; Berstel, 1979) and weight (Kuich and Salomaa, 1986; Petre and Salomaa, 2009). Our equivalent representation allows a transition to be labeled by a stack operation or a regular input symbol but not both. Stack operations are represented by pairs of open and close parentheses (pushing a symbol on and popping it from the stack). The advantage of this representation is that is identical to the finite automaton representation except that certain symbols (the parentheses) have special semantics. As such, several finite-state algorithms either immediately generalize to this PDA representation or do</context>
<context position="21907" citStr="Aho and Ullman, 1972" startWordPosition="3984" endWordPosition="3987">s shown in Figure 4, an advantage of the RTN, PDA, and FSA representations is that they can benefit from FSA epsilon removal, determinization and minimization algorithms applied to their components (for RTNs and PDAs) or their entirety (for FSAs). For the complexity discussion below, however, we disregard these optimizations. Instead we focus on the complexity of each MT step described in the introduction: 1. SCFG Translation: Assuming that the parsing of the input is performed by a CYK parse, then the CFG, hypergraph, RTN and PDA representations can be generated in O(|s|3|G|) time and space (Aho and Ullman, 1972). The FSA representation can require an additional O(e|s|3|G|) time and space since the PDA expansion can be exponential. 2. Intersection: The intersection of a CFG Th with a finite automaton M can be performed by the classical Bar-Hillel algorithm (Bar-Hillel et al., 1964) with time and space complexity O(|Th||M|3).1 The PDA intersection algorithm from Section 2.3 has time and space complexity O(|Tp||M|). Finally, the FSA intersection algorithm has time and space complexity O(|Tf ||M|) (Mohri, 2009). 3. Shortest Path: The shortest path algorithm on the hypergraph, RTN, and FSA representations</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation and Compiling, volume 1-2. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
</authors>
<date>2011</date>
<note>Pushdown Transducers. http://pdt.openfst.org.</note>
<contexts>
<context position="9459" citStr="Allauzen and Riley, 2011" startWordPosition="1482" endWordPosition="1485">llows a transition to be labeled by a stack operation or a regular input symbol but not both. Stack operations are represented by pairs of open and close parentheses (pushing a symbol on and popping it from the stack). The advantage of this representation is that is identical to the finite automaton representation except that certain symbols (the parentheses) have special semantics. As such, several finite-state algorithms either immediately generalize to this PDA representation or do so with minimal changes. The algorithms described in this section have been implemented in the PDT extension (Allauzen and Riley, 2011) of the OpenFst library (Allauzen et al., 2007). 2.1 Definitions A (restricted) Dyck language consist of “wellformed” or “balanced” strings over a finite number of pairs of parentheses. Thus the string ( [ ( ) ( ) ] { } [ ] ) ( ) is in the Dyck language over 3 pairs of parentheses. More formally, let A and A be two finite alphabets such that there exists a bijection f from A to A. Intuitively, f maps an open parenthesis to its corresponding close parenthesis. Let a¯ denote f(a) if a ∈ A and f`(a) if a_∈ A. The Dyck language DA over the alphabet A = A ∪ A is then the language defined by the fol</context>
</contexts>
<marker>Allauzen, Riley, 2011</marker>
<rawString>Cyril Allauzen and Michael Riley, 2011. Pushdown Transducers. http://pdt.openfst.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>OpenFst: A general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In Proceedings of CIAA,</booktitle>
<pages>11--23</pages>
<contexts>
<context position="9506" citStr="Allauzen et al., 2007" startWordPosition="1490" endWordPosition="1493">ion or a regular input symbol but not both. Stack operations are represented by pairs of open and close parentheses (pushing a symbol on and popping it from the stack). The advantage of this representation is that is identical to the finite automaton representation except that certain symbols (the parentheses) have special semantics. As such, several finite-state algorithms either immediately generalize to this PDA representation or do so with minimal changes. The algorithms described in this section have been implemented in the PDT extension (Allauzen and Riley, 2011) of the OpenFst library (Allauzen et al., 2007). 2.1 Definitions A (restricted) Dyck language consist of “wellformed” or “balanced” strings over a finite number of pairs of parentheses. Thus the string ( [ ( ) ( ) ] { } [ ] ) ( ) is in the Dyck language over 3 pairs of parentheses. More formally, let A and A be two finite alphabets such that there exists a bijection f from A to A. Intuitively, f maps an open parenthesis to its corresponding close parenthesis. Let a¯ denote f(a) if a ∈ A and f`(a) if a_∈ A. The Dyck language DA over the alphabet A = A ∪ A is then the language defined by the following context-free grammar: 5 → ǫ, 5 → 55 and </context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A general and efficient weighted finite-state transducer library. In Proceedings of CIAA, pages 11–23. http://www.openfst.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
</authors>
<title>Filters for efficient composition of weighted finite-state transducers.</title>
<date>2011</date>
<booktitle>In Proceedings of CIAA,</booktitle>
<volume>6482</volume>
<pages>28--38</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="13607" citStr="Allauzen et al., 2011" startWordPosition="2379" endWordPosition="2382">lt of the algorithm when applied to the PDA of Figure 1c. 2.3 Intersection Algorithm The class of weighted pushdown automata is closed under intersection with weighted finite automata (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). Considering a pair (T1, T2) where one element is an FSA and the other element a PDA, then there exists a PDA T1 ∩T2, the intersection of T1 and T2, such that for all x ∈ E∗: (T1 ∩T2)(x) = T1(x)+T2(x). We assume in the following that T2 is an FSA. We also assume that T2 has no input-c transitions. When T2 has input-c transitions, an epsilon filter (Mohri, 2009; Allauzen et al., 2011) generalized to handle parentheses can be used. A state in T =T1∩T2 is a pair (q1, q2) where q1 is a state of T1 and q2 a state of T2. The initial state is I =(I1, I2). Given a transition e1 =(q1, a, w1, q′1) in T1, transitions out of (q1, q2) in T are obtained using the following rules. If a ∈ E, then e1 can be matched with a transition (q2, a, w2, q′2) in T2 resulting a transition ((q1, q2), a, w1+w2, (q′1, q′2)) in T. If a = c, then e1 is matched with staying in q2 resulting in a transition ((q1, q2), c, w1, (q′1, q2)). Finally, if a ∈ H, e1 is also matched with staying in q2, resulting in </context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, 2011</marker>
<rawString>Cyril Allauzen, Michael Riley, and Johan Schalkwyk. 2011. Filters for efficient composition of weighted finite-state transducers. In Proceedings of CIAA, volume 6482 ofLNCS, pages 28–38. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bar-Hillel</author>
<author>M Perles</author>
<author>E Shamir</author>
</authors>
<title>On formal properties of simple phrase structure grammars.</title>
<date>1964</date>
<booktitle>Language and Information: Selected Essays on their Theory and Application,</booktitle>
<pages>116--150</pages>
<editor>In Y. Bar-Hillel, editor,</editor>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="13193" citStr="Bar-Hillel et al., 1964" startWordPosition="2301" endWordPosition="2304">. A state (q, z) in T′ is final if q is final in T and z = c (p′((q, c))=p(q)). The set of states of T′ is the set of pairs (q, z) that can be reached from an initial state by transitions defined as above. The condition that T has a bounded stack ensures that this set is finite (since it implies that for any (q, z), |z |≤ K). The complexity of the algorithm is linear in O(|T′|) = O(e|T|). Figure 1d show the result of the algorithm when applied to the PDA of Figure 1c. 2.3 Intersection Algorithm The class of weighted pushdown automata is closed under intersection with weighted finite automata (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). Considering a pair (T1, T2) where one element is an FSA and the other element a PDA, then there exists a PDA T1 ∩T2, the intersection of T1 and T2, such that for all x ∈ E∗: (T1 ∩T2)(x) = T1(x)+T2(x). We assume in the following that T2 is an FSA. We also assume that T2 has no input-c transitions. When T2 has input-c transitions, an epsilon filter (Mohri, 2009; Allauzen et al., 2011) generalized to handle parentheses can be used. A state in T =T1∩T2 is a pair (q1, q2) where q1 is a state of T1 and q2 a state of T2. The initial state is I =(I1, I2). Given a transitio</context>
<context position="22181" citStr="Bar-Hillel et al., 1964" startWordPosition="4028" endWordPosition="4031">ussion below, however, we disregard these optimizations. Instead we focus on the complexity of each MT step described in the introduction: 1. SCFG Translation: Assuming that the parsing of the input is performed by a CYK parse, then the CFG, hypergraph, RTN and PDA representations can be generated in O(|s|3|G|) time and space (Aho and Ullman, 1972). The FSA representation can require an additional O(e|s|3|G|) time and space since the PDA expansion can be exponential. 2. Intersection: The intersection of a CFG Th with a finite automaton M can be performed by the classical Bar-Hillel algorithm (Bar-Hillel et al., 1964) with time and space complexity O(|Th||M|3).1 The PDA intersection algorithm from Section 2.3 has time and space complexity O(|Tp||M|). Finally, the FSA intersection algorithm has time and space complexity O(|Tf ||M|) (Mohri, 2009). 3. Shortest Path: The shortest path algorithm on the hypergraph, RTN, and FSA representations requires linear time and space (given the underlying acyclicity) (Huang, 2008; Mohri, 2009). As presented in Section 2.4, the PDA representation can require time cubic and space quadratic in |M|.2 Table 1 summarizes the complexity results. Note the PDA representation is eq</context>
</contexts>
<marker>Bar-Hillel, Perles, Shamir, 1964</marker>
<rawString>Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal properties of simple phrase structure grammars. In Y. Bar-Hillel, editor, Language and Information: Selected Essays on their Theory and Application, pages 116–150. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Berstel</author>
</authors>
<date>1979</date>
<journal>Transductions and Context-Free Languages. Teubner.</journal>
<contexts>
<context position="8739" citStr="Berstel, 1979" startWordPosition="1369" endWordPosition="1370">n, shortest-path and related algorithms that will be needed later. Informally, pushdown automata are finite automata that have been augmented with a stack. Typ1374 a (a) (b) (c) (d) Figure 1: PDA Examples: (a) Non-regular PDA accepting {anbn|n ∈ N}. (b) Regular (but not bounded-stack) PDA accepting a*b*. (c) Bounded-stack PDA accepting a*b* and (d) its expansion as an FSA. ically this is done by adding a stack alphabet and labeling each transition with a stack operation (a stack symbol to be pushed onto, popped or read from the stack) in additon to the usual input label (Aho and Ullman, 1972; Berstel, 1979) and weight (Kuich and Salomaa, 1986; Petre and Salomaa, 2009). Our equivalent representation allows a transition to be labeled by a stack operation or a regular input symbol but not both. Stack operations are represented by pairs of open and close parentheses (pushing a symbol on and popping it from the stack). The advantage of this representation is that is identical to the finite automaton representation except that certain symbols (the parentheses) have special semantics. As such, several finite-state algorithms either immediately generalize to this PDA representation or do so with minimal</context>
</contexts>
<marker>Berstel, 1979</marker>
<rawString>Jean Berstel. 1979. Transductions and Context-Free Languages. Teubner.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-ACL,</booktitle>
<pages>858--867</pages>
<contexts>
<context position="27452" citStr="Brants et al., 2007" startWordPosition="4923" endWordPosition="4926"> unrestricted one without the previous constraint. We call this G2. This is a superset of the previous grammar, and exact search under it is not feasible for HiFST: pruning is required in search. The initial English language model is a KneserNey 4-gram estimated over the target side of the parallel text and the AFP and Xinhua portions of monolingual data from the English Gigaword Fourth Edition (LDC2009T13). This is a total of 1.3B words. We will call this language model M1. For large language model rescoring we also use the LM M2 obtained by interpolating M1 with a zero-cutoff stupidbackoff (Brants et al., 2007) 5-gram estimated using 6.6B words of English newswire text. We next describe how we build translation systems using entropy-pruned language models. 1. We build a baseline HiFST system that uses M1 and a hierarchical grammar G, parameters being optimized with MERT under BLEU. 4See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl 1379 2. We then use entropy-based pruning of the language model (Stolcke, 1998) under a relative perplexity threshold of θ to reduce the size of M1. We will call the resulting language model as MB. Table 2 shows the number of n-grams (in millions) obtained for dif</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of EMNLP-ACL, pages 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Thorsten Brants</author>
<author>Will Neveitt</author>
<author>Peng Xu</author>
</authors>
<title>Study on interaction between entropy pruning and kneser-ney smoothing.</title>
<date>2010</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<pages>2242--2245</pages>
<contexts>
<context position="31425" citStr="Chelba et al., 2010" startWordPosition="5626" endWordPosition="5629">ts speed at the risk of search errors. For instance, comparing the outputs of systems 1 and 2 with 0 =10 in Table 3 we find 45 different 1-best hypotheses, even though the BLEU score is identical. In other words, we have 45 cases in which system 2 is not able to recover the baseline output because the 1st-pass likelihood beam 0 is not wide enough. Similarly, system 3 fails in 101 cases (0 = 12) and system 4 fails in 95 cases. Interestingly, some of these sentences would require impractically huge beams. This might be due to the Kneser-Ney smoothing, which interacts badly with entropy pruning (Chelba et al., 2010). 6 Hiero with PDAs and FSAs In this section we contrast HiFST with HiPDT under the same translation grammar and entropy-pruned language models. Under the constrained grammar G1 their performance is identical as both decoders can generate the entire search space which can then be rescored with M1 or M2 as shown in the previous section. Therefore, we now focus on the unconstrained grammar G2, where exact search is not feasible for HiFST. In order to evaluate this problem, we run both decoders over tune-nw, restricting memory usage to 10 gigabytes. If this limit is reached in decod1380 HiFST (G1</context>
</contexts>
<marker>Chelba, Brants, Neveitt, Xu, 2010</marker>
<rawString>Ciprian Chelba, Thorsten Brants, Will Neveitt, and Peng Xu. 2010. Study on interaction between entropy pruning and kneser-ney smoothing. In Proceedings of Interspeech, pages 2242–2245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1360" citStr="Chiang, 2007" startWordPosition="181" endWordPosition="182">resented for PDAs. Chinese-toEnglish translation experiments using HiFST and HiPDT, FSA and PDA-based decoders, are presented using admissible (or exact) search, possible for HiFST with compact SCFG rulesets and HiPDT with compact LMs. For large rulesets with large LMs, we introduce a two-pass search strategy which we then analyze in terms of search errors and translation performance. 1 Introduction Hierarchical phrase-based translation, using a synchronous context-free translation grammar (SCFG) together with an n-gram target language model (LM), is a popular approach in machine translation (Chiang, 2007). Given a SCFG G and an ngram language model M, this paper focuses on how to decode with them, i.e. how to apply them to the source text to generate a target translation. Decoding has three basic steps, which we first describe in terms of the formal languages and relations involved, with data representations and algorithms to follow. 1. Translating the source sentence s with G to give target translations: T = {s} o G, a (weighted) context-free language resulting from the composition of a finite language and the algebraic relation G for SCFG G. 2. Applying the language model to these target tra</context>
<context position="3405" citStr="Chiang (2007)" startWordPosition="533" endWordPosition="534">resentations Tf and Lf. In this case, weighted finite-state intersection and single-source shortest path algorithms (using negative log probabilities) can be used to solve Steps 2 and 3 (Mohri, 2009). This is the approach taken in (Iglesias et al., 2009a; de Gispert et al., 2010). Instead T and L can be represented by hypergraphs Th and Lh (or very similarly context-free rules, and-or trees, or deductive systems). In this case, hypergraph intersection with a finite automaton and hypergraph shortest path algorithms can be used to solve Steps 2 and 3 (Huang, 2008). This is the approach taken by Chiang (2007). In this paper, we will consider another representation for context-free languages T and L as well, pushdown automata (PDA) Tp and Lp, familiar from formal 1373 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1373–1383, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics language theory (Aho and Ullman, 1972). We will describe PDA intersection with a finite automaton and PDA shortest-path algorithms in Section 2 that can be used to solve Steps 2 and 3. It cannot be over-emphasized that the CFG, hypergraph an</context>
<context position="6461" citStr="Chiang (2007)" startWordPosition="1002" endWordPosition="1003">on grammar which is large enough that exact translation under the FSA representation is not possible. We find that translation is possible using the two-pass strategy with the PDA translation representation and that gains in BLEU score result from using the larger translation grammar. 1.1 Related Work There is extensive prior work on computational efficiency and algorithmic complexity in hierarchical phrase-based translation. The challenge is to find algorithms that can be made to work with large translation grammars and large language models. Following the original algorithms and analysis of Chiang (2007), Huang and Chiang (2007) developed the cube-growing algorithm, and more recently Huang and Mi (2010) developed an incremental decoding approach that exploits left-to-right nature of the language models. Search errors in hierarchical translation, and in translation more generally, have not been as extensively studied; this is undoubtedly due to the difficulties inherent in finding exact translations for use in comparison. Using a relatively simple phrasebased translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar et al., 2006) and fo</context>
<context position="22971" citStr="Chiang (2007)" startWordPosition="4155" endWordPosition="4157">as time and space complexity O(|Tf ||M|) (Mohri, 2009). 3. Shortest Path: The shortest path algorithm on the hypergraph, RTN, and FSA representations requires linear time and space (given the underlying acyclicity) (Huang, 2008; Mohri, 2009). As presented in Section 2.4, the PDA representation can require time cubic and space quadratic in |M|.2 Table 1 summarizes the complexity results. Note the PDA representation is equivalent in time and superior in space to the CFG/hypergraph representation, in general, and it can be superior in both space 1The modified Bar-Hillel construction described by Chiang (2007) has time and space complexity O(|Th||M|4); the modifications were introduced presumably to benefit the subsequent pruning method employed (but see Huang et al. (2005)). 2The time (resp. space) complexity is not cubic (resp. quadratic) in |Tp||M|. Given a state q in Tp, there exists a unique sq such that q belongs to Csq. Given a state (q1, q2) in Tp n M, (q1, q2) E C(s1,s2) only if s1 = sq1, and hence (q1, q2) belongs to at most |M |components. 0,e 0 a a a a 1,e 6,e 6 1 b c b c 2,e 7,e 2 7 ( [ e e 11,( 11,[ 11 b b b (c) PDA 12,( 12,[ 12 c c c 13,( 13,[ 13 e e ) ] 3,e 8,e g 3 d 4 5 8 f g 9 10 </context>
<context position="26269" citStr="Chiang, 2007" startWordPosition="4716" endWordPosition="4717">tp://projects.ldc.upenn.edu/gale/data/catalog.html. We excluded the UN material and the LDC2002E18, LDC2004T08, LDC2007E08 and CUDonga collections. 0 7.5 × 10−9 7.5 × 10−8 7.5 × 10−7 207.5 20.2 4.1 0.9 Table 2: Number of ngrams (in millions) in the 1st pass 4-gram language models obtained with different θ values (top row). tems, standard MERT (Och, 2003) iterative parameter estimation under IBM BLEU4 is performed on the development set. The parallel corpus is aligned using MTTK (Deng and Byrne, 2008) in both source-to-target and target-to-source directions. We then follow standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009b) to extract hierarchical phrases from the union of the directional word alignments. We call a translation grammar the set of rules extracted from this process. We extract two translation grammars: • A restricted grammar where we apply the following additional constraint: rules are only considered if they have a forward translation probability p &gt; 0.01. We call this G1. As will be discussed later, the interest of this grammar is that decoding under it can be exact, that is, without any pruning in search. • An unrestricted one without the previou</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
<author>Gonzalo Iglesias</author>
<author>Graeme Blackwood</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Hierarchical phrase-based translation with weighted finite state transducers and shallow-n grammars.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<marker>de Gispert, Iglesias, Blackwood, Banga, Byrne, 2010</marker>
<rawString>Adri`a de Gispert, Gonzalo Iglesias, Graeme Blackwood, Eduardo R. Banga, and William Byrne. 2010. Hierarchical phrase-based translation with weighted finite state transducers and shallow-n grammars. Computational Linguistics, 36(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>HMM word and phrase alignment for statistical machine translation.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="26161" citStr="Deng and Byrne, 2008" startWordPosition="4700" endWordPosition="4703">ns produced by the GALE program and portions of the newswire sections of MT02 through MT06. In tuning the sys3See http://projects.ldc.upenn.edu/gale/data/catalog.html. We excluded the UN material and the LDC2002E18, LDC2004T08, LDC2007E08 and CUDonga collections. 0 7.5 × 10−9 7.5 × 10−8 7.5 × 10−7 207.5 20.2 4.1 0.9 Table 2: Number of ngrams (in millions) in the 1st pass 4-gram language models obtained with different θ values (top row). tems, standard MERT (Och, 2003) iterative parameter estimation under IBM BLEU4 is performed on the development set. The parallel corpus is aligned using MTTK (Deng and Byrne, 2008) in both source-to-target and target-to-source directions. We then follow standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009b) to extract hierarchical phrases from the union of the directional word alignments. We call a translation grammar the set of rules extracted from this process. We extract two translation grammars: • A restricted grammar where we apply the following additional constraint: rules are only considered if they have a forward translation probability p &gt; 0.01. We call this G1. As will be discussed later, the interest of this grammar is that decod</context>
</contexts>
<marker>Deng, Byrne, 2008</marker>
<rawString>Yonggang Deng and William Byrne. 2008. HMM word and phrase alignment for statistical machine translation. IEEE Transactions on Audio, Speech, and Language Processing, 16(3):494–507.</rawString>
</citation>
<citation valid="true">
<date>2009</date>
<booktitle>Handbook of Weighted Automata.</booktitle>
<editor>Manfred Drosde, Werner Kuick, and Heiko Vogler, editors.</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="16261" citStr="(2009)" startWordPosition="2922" endWordPosition="2922">by a balanced path. If s has several incoming open parenthesis transitions, a naive implementation might lead to the states in C3 to be visited up to exponentially many times. The basic idea of the algorithm is to memoize the shortest distance from s to states in C3. The 1376 pseudo-code is given in Figure 2. GETDISTANCE(T, s) starts a new instance of the shortest-distance algorithm from s using the queue S3, initially containing s. While the queue is not empty, a state is dequeued and its outgoing transitions examined (line 5-9). Transitions labeled by non-parenthesis are treated as in Mohri (2009) (line 9-10). When the considered transition e is labeled by a close parenthesis, it is remembered that it balances all incoming open parentheses in s labeled by i[e] by adding e to B[s, i[e]] (line 11-12). Finally, when e is labeled with an open parenthesis, if its destination has not already been visited, a new instance is started from n[e] (line 14-15). The destination states of all transitions balancing e are then relaxed (line 16-18). The space complexity of the algorithm is quadratic for two reasons. First, the number of non-infinity d[q, s] is |Q|2. Second, the space required for storin</context>
</contexts>
<marker>2009</marker>
<rawString>Manfred Drosde, Werner Kuick, and Heiko Vogler, editors. 2009. Handbook of Weighted Automata. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hershberger</author>
<author>Subhash Suri</author>
<author>Amit Bhosle</author>
</authors>
<title>On the difficulty of some shortest path problems.</title>
<date>2003</date>
<booktitle>In Proceedings of STACS,</booktitle>
<volume>2607</volume>
<pages>343--354</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="17849" citStr="Hershberger et al., 2003" startWordPosition="3195" endWordPosition="3198">ISTANCE. Assuming the cost of a queue operation is F(n) for a queue containing n elements, the worst-case time complexity of the algorithm can then be expressed as O(N|T |3 F(|T |)). When T contains no negative weights, using a shortest-first queue discipline leads to a time complexity in O(|T |3 log |T|). When all the C3’s are acyclic, using a topological order queue discipline leads to a O(|T |3) time complexity. In effect, we are solving a k-sources shortestpath problem with k single-source solutions. A potentially better approach might be to solve the ksources or k-pairs problem directly (Hershberger et al., 2003). When T has been obtained by converting an RTN or an hypergraph into a PDA (Section 2.5), the polynomial dependency in |T |becomes a linear dependency both for the time and space complexities. Indeed, for each q in T, there exists a unique s such that d[q, s] is non-infinity. Moreover, for each close parenthesis transistion e, there exists a unique open parenthesis transition e′ such that e E B[n[e′], i[e′]]. When each component of the RTN is acyclic, the complexity of the algorithm is hence in O(|T|) in time and space. The algorithm can be modified to compute the shortest path by keeping tra</context>
</contexts>
<marker>Hershberger, Suri, Bhosle, 2003</marker>
<rawString>John Hershberger, Subhash Suri, and Amit Bhosle. 2003. On the difficulty of some shortest path problems. In Proceedings of STACS, volume 2607 of LNCS, pages 343–354. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="6486" citStr="Huang and Chiang (2007)" startWordPosition="1004" endWordPosition="1007">h is large enough that exact translation under the FSA representation is not possible. We find that translation is possible using the two-pass strategy with the PDA translation representation and that gains in BLEU score result from using the larger translation grammar. 1.1 Related Work There is extensive prior work on computational efficiency and algorithmic complexity in hierarchical phrase-based translation. The challenge is to find algorithms that can be made to work with large translation grammars and large language models. Following the original algorithms and analysis of Chiang (2007), Huang and Chiang (2007) developed the cube-growing algorithm, and more recently Huang and Mi (2010) developed an incremental decoding approach that exploits left-to-right nature of the language models. Search errors in hierarchical translation, and in translation more generally, have not been as extensively studied; this is undoubtedly due to the difficulties inherent in finding exact translations for use in comparison. Using a relatively simple phrasebased translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar et al., 2006) and found that cube-pruning suf</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings ofACL, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Haitao Mi</author>
</authors>
<title>Efficient incremental decoding for tree-to-string translation.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>273--283</pages>
<contexts>
<context position="6562" citStr="Huang and Mi (2010)" startWordPosition="1016" endWordPosition="1019">sible. We find that translation is possible using the two-pass strategy with the PDA translation representation and that gains in BLEU score result from using the larger translation grammar. 1.1 Related Work There is extensive prior work on computational efficiency and algorithmic complexity in hierarchical phrase-based translation. The challenge is to find algorithms that can be made to work with large translation grammars and large language models. Following the original algorithms and analysis of Chiang (2007), Huang and Chiang (2007) developed the cube-growing algorithm, and more recently Huang and Mi (2010) developed an incremental decoding approach that exploits left-to-right nature of the language models. Search errors in hierarchical translation, and in translation more generally, have not been as extensively studied; this is undoubtedly due to the difficulties inherent in finding exact translations for use in comparison. Using a relatively simple phrasebased translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar et al., 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an extensive compari</context>
</contexts>
<marker>Huang, Mi, 2010</marker>
<rawString>Liang Huang and Haitao Mi. 2010. Efficient incremental decoding for tree-to-string translation. In Proceedings of EMNLP, pages 273–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Machine translation as lexicalized parsing with hooks.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology, Parsing ’05,</booktitle>
<pages>65--73</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="23138" citStr="Huang et al. (2005)" startWordPosition="4179" endWordPosition="4182">near time and space (given the underlying acyclicity) (Huang, 2008; Mohri, 2009). As presented in Section 2.4, the PDA representation can require time cubic and space quadratic in |M|.2 Table 1 summarizes the complexity results. Note the PDA representation is equivalent in time and superior in space to the CFG/hypergraph representation, in general, and it can be superior in both space 1The modified Bar-Hillel construction described by Chiang (2007) has time and space complexity O(|Th||M|4); the modifications were introduced presumably to benefit the subsequent pruning method employed (but see Huang et al. (2005)). 2The time (resp. space) complexity is not cubic (resp. quadratic) in |Tp||M|. Given a state q in Tp, there exists a unique sq such that q belongs to Csq. Given a state (q1, q2) in Tp n M, (q1, q2) E C(s1,s2) only if s1 = sq1, and hence (q1, q2) belongs to at most |M |components. 0,e 0 a a a a 1,e 6,e 6 1 b c b c 2,e 7,e 2 7 ( [ e e 11,( 11,[ 11 b b b (c) PDA 12,( 12,[ 12 c c c 13,( 13,[ 13 e e ) ] 3,e 8,e g 3 d 4 5 8 f g 9 10 d f 4,e 9,e g 5,e g 10,e (c) FSA Figure 4: Optimized translation representations 1378 Representation Time Complexity Space Complexity CFG/hypergraph O(|s|3 |G ||M|3) O</context>
</contexts>
<marker>Huang, Zhang, Gildea, 2005</marker>
<rawString>Liang Huang, Hao Zhang, and Daniel Gildea. 2005. Machine translation as lexicalized parsing with hooks. In Proceedings of the Ninth International Workshop on Parsing Technology, Parsing ’05, pages 65–73, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Advanced dynamic programming in semiring and hypergraph frameworks.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1--18</pages>
<contexts>
<context position="3360" citStr="Huang, 2008" startWordPosition="525" endWordPosition="526">. As such, T and L have finite automaton representations Tf and Lf. In this case, weighted finite-state intersection and single-source shortest path algorithms (using negative log probabilities) can be used to solve Steps 2 and 3 (Mohri, 2009). This is the approach taken in (Iglesias et al., 2009a; de Gispert et al., 2010). Instead T and L can be represented by hypergraphs Th and Lh (or very similarly context-free rules, and-or trees, or deductive systems). In this case, hypergraph intersection with a finite automaton and hypergraph shortest path algorithms can be used to solve Steps 2 and 3 (Huang, 2008). This is the approach taken by Chiang (2007). In this paper, we will consider another representation for context-free languages T and L as well, pushdown automata (PDA) Tp and Lp, familiar from formal 1373 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1373–1383, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics language theory (Aho and Ullman, 1972). We will describe PDA intersection with a finite automaton and PDA shortest-path algorithms in Section 2 that can be used to solve Steps 2 and 3. It cannot b</context>
<context position="22585" citStr="Huang, 2008" startWordPosition="4093" endWordPosition="4094"> and space since the PDA expansion can be exponential. 2. Intersection: The intersection of a CFG Th with a finite automaton M can be performed by the classical Bar-Hillel algorithm (Bar-Hillel et al., 1964) with time and space complexity O(|Th||M|3).1 The PDA intersection algorithm from Section 2.3 has time and space complexity O(|Tp||M|). Finally, the FSA intersection algorithm has time and space complexity O(|Tf ||M|) (Mohri, 2009). 3. Shortest Path: The shortest path algorithm on the hypergraph, RTN, and FSA representations requires linear time and space (given the underlying acyclicity) (Huang, 2008; Mohri, 2009). As presented in Section 2.4, the PDA representation can require time cubic and space quadratic in |M|.2 Table 1 summarizes the complexity results. Note the PDA representation is equivalent in time and superior in space to the CFG/hypergraph representation, in general, and it can be superior in both space 1The modified Bar-Hillel construction described by Chiang (2007) has time and space complexity O(|Th||M|4); the modifications were introduced presumably to benefit the subsequent pruning method employed (but see Huang et al. (2005)). 2The time (resp. space) complexity is not cu</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Advanced dynamic programming in semiring and hypergraph frameworks. In Proceedings of COLING, pages 1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Iglesias</author>
<author>Adri`a de Gispert</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Hierarchical phrase-based translation with weighted finite state transducers.</title>
<date>2009</date>
<booktitle>In Proceedings ofNAACL-HLT,</booktitle>
<pages>433--441</pages>
<marker>Iglesias, de Gispert, Banga, Byrne, 2009</marker>
<rawString>Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga, and William Byrne. 2009a. Hierarchical phrase-based translation with weighted finite state transducers. In Proceedings ofNAACL-HLT, pages 433–441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Iglesias</author>
<author>Adri`a de Gispert</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Rule filtering by pattern for efficient hierarchical translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>380--388</pages>
<marker>Iglesias, de Gispert, Banga, Byrne, 2009</marker>
<rawString>Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga, and William Byrne. 2009b. Rule filtering by pattern for efficient hierarchical translation. In Proceedings of EACL, pages 380–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1288--1298</pages>
<contexts>
<context position="7410" citStr="Koo et al., 2010" startWordPosition="1145" endWordPosition="1148">ubtedly due to the difficulties inherent in finding exact translations for use in comparison. Using a relatively simple phrasebased translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar et al., 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010). Relaxation techniques have also recently been shown to finding exact solutions in parsing (Koo et al., 2010) and in SMT with tree-to-string translation grammars and trigram language models (Rush and Collins, 2011), much smaller models compared to the work presented in this paper. Although entropy-pruned language models have been used to produce real-time translation systems (Prasad et al., 2007), we believe our use of entropy-pruned language models in two-pass translation to be novel. This is an approach that is widelyused in automatic speech recognition (Ljolje et al., 1999) and we note that it relies on efficient representation of very large search spaces T for subsequent rescoring, as is possible</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proceedings of EMNLP, pages 1288–1298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Werner Kuich</author>
<author>Arto Salomaa</author>
</authors>
<date>1986</date>
<publisher>Springer.</publisher>
<note>Semirings, automata, languages.</note>
<contexts>
<context position="8775" citStr="Kuich and Salomaa, 1986" startWordPosition="1373" endWordPosition="1376">d algorithms that will be needed later. Informally, pushdown automata are finite automata that have been augmented with a stack. Typ1374 a (a) (b) (c) (d) Figure 1: PDA Examples: (a) Non-regular PDA accepting {anbn|n ∈ N}. (b) Regular (but not bounded-stack) PDA accepting a*b*. (c) Bounded-stack PDA accepting a*b* and (d) its expansion as an FSA. ically this is done by adding a stack alphabet and labeling each transition with a stack operation (a stack symbol to be pushed onto, popped or read from the stack) in additon to the usual input label (Aho and Ullman, 1972; Berstel, 1979) and weight (Kuich and Salomaa, 1986; Petre and Salomaa, 2009). Our equivalent representation allows a transition to be labeled by a stack operation or a regular input symbol but not both. Stack operations are represented by pairs of open and close parentheses (pushing a symbol on and popping it from the stack). The advantage of this representation is that is identical to the finite automaton representation except that certain symbols (the parentheses) have special semantics. As such, several finite-state algorithms either immediately generalize to this PDA representation or do so with minimal changes. The algorithms described i</context>
</contexts>
<marker>Kuich, Salomaa, 1986</marker>
<rawString>Werner Kuich and Arto Salomaa. 1986. Semirings, automata, languages. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>A weighted finite state transducer translation template model for statistical machine translation.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="7054" citStr="Kumar et al., 2006" startWordPosition="1089" endWordPosition="1092"> analysis of Chiang (2007), Huang and Chiang (2007) developed the cube-growing algorithm, and more recently Huang and Mi (2010) developed an incremental decoding approach that exploits left-to-right nature of the language models. Search errors in hierarchical translation, and in translation more generally, have not been as extensively studied; this is undoubtedly due to the difficulties inherent in finding exact translations for use in comparison. Using a relatively simple phrasebased translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar et al., 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010). Relaxation techniques have also recently been shown to finding exact solutions in parsing (Koo et al., 2010) and in SMT with tree-to-string translation grammars and trigram language models (Rush and Collins, 2011), much smaller models compared to the work presented in this paper. Although entropy-pruned language models have been used to produce real-</context>
</contexts>
<marker>Kumar, Deng, Byrne, 2006</marker>
<rawString>Shankar Kumar, Yonggang Deng, and William Byrne. 2006. A weighted finite state transducer translation template model for statistical machine translation. Natural Language Engineering, 12(1):35–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrej Ljolje</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Efficient general lattice generation and rescoring.</title>
<date>1999</date>
<booktitle>In Proceedings ofEurospeech,</booktitle>
<pages>1251--1254</pages>
<contexts>
<context position="7884" citStr="Ljolje et al., 1999" startWordPosition="1219" endWordPosition="1222">(2009a) and de Gispert et al. (2010). Relaxation techniques have also recently been shown to finding exact solutions in parsing (Koo et al., 2010) and in SMT with tree-to-string translation grammars and trigram language models (Rush and Collins, 2011), much smaller models compared to the work presented in this paper. Although entropy-pruned language models have been used to produce real-time translation systems (Prasad et al., 2007), we believe our use of entropy-pruned language models in two-pass translation to be novel. This is an approach that is widelyused in automatic speech recognition (Ljolje et al., 1999) and we note that it relies on efficient representation of very large search spaces T for subsequent rescoring, as is possible with FSAs and PDAs. 2 Pushdown Automata In this section, we formally define pushdown automata and give intersection, shortest-path and related algorithms that will be needed later. Informally, pushdown automata are finite automata that have been augmented with a stack. Typ1374 a (a) (b) (c) (d) Figure 1: PDA Examples: (a) Non-regular PDA accepting {anbn|n ∈ N}. (b) Regular (but not bounded-stack) PDA accepting a*b*. (c) Bounded-stack PDA accepting a*b* and (d) its expa</context>
</contexts>
<marker>Ljolje, Pereira, Riley, 1999</marker>
<rawString>Andrej Ljolje, Fernando Pereira, and Michael Riley. 1999. Efficient general lattice generation and rescoring. In Proceedings ofEurospeech, pages 1251–1254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Weighted automata algorithms.</title>
<date>2009</date>
<booktitle>In Drosde et al. (Drosde et al., 2009), chapter 6,</booktitle>
<pages>213--254</pages>
<contexts>
<context position="2991" citStr="Mohri, 2009" startWordPosition="461" endWordPosition="462">is applied to G by using the CYK algorithm in Step 1 and M is represented by a finite automaton in Step 2. The choice of the representation of T in many ways determines the remaining decoder representations and algorithms needed. Since {s} is a finite language and we assume throughout that G does not allow unbounded insertions, T and L are, in fact, regular languages. As such, T and L have finite automaton representations Tf and Lf. In this case, weighted finite-state intersection and single-source shortest path algorithms (using negative log probabilities) can be used to solve Steps 2 and 3 (Mohri, 2009). This is the approach taken in (Iglesias et al., 2009a; de Gispert et al., 2010). Instead T and L can be represented by hypergraphs Th and Lh (or very similarly context-free rules, and-or trees, or deductive systems). In this case, hypergraph intersection with a finite automaton and hypergraph shortest path algorithms can be used to solve Steps 2 and 3 (Huang, 2008). This is the approach taken by Chiang (2007). In this paper, we will consider another representation for context-free languages T and L as well, pushdown automata (PDA) Tp and Lp, familiar from formal 1373 Proceedings of the 2011 </context>
<context position="11977" citStr="Mohri, 2009" startWordPosition="2021" endWordPosition="2022">s T(x) = minrEP(x) w[7r] + ρ(n[7r]) where P(x) denotes the set of balanced paths accepting x. A weighted language is recognizable by a weighted pushdown automaton iff it is context-free. We define the size of T as |T |=|Q|+|E|. A PDA T has a bounded stack if there exists K ∈ N such that for any sub-path 7r of any balanced path in T: |cn(rn(i[7r])) |≤ K. If T has a bounded stack, then it represents a regular language. Figure 1 shows non-regular, regular and bounded-stack PDAs. A weightedfinite automaton (FSA) can be viewed as a PDA where the open and close parentheses alphabets are empty, see (Mohri, 2009) for a standalone definition. 0 ( E E 2 1 ) E b 3 a 1 0 ( E 2 ) 3 b 0 E ( ) 3 1 a ( 2 4 ) b 5 0,E E E 3,E 1,( E E a 2,( 4,( E b 5,( _A* → A* as follow. 1375 2.2 Expansion Algorithm Given a bounded-stack PDA T, the expansion of T is the FSA T′ equivalent to T defined as follows. A state in T′ is a pair (q, z) where q is a state in T and z ∈ H∗. A transition (q, a, w, q′) in T results in a transition ((q, z), a′, w, (q′, z′)) in T′ only when: (a) a∈E ∪ {c}, z′ =z and a′ =a, (b) a∈H, z′ =za and a′ = c, or (c) a ∈ H, z′ is such that z = z′a and a′ = c. The initial state of T′ is I′ = (I, c). A sta</context>
<context position="13583" citStr="Mohri, 2009" startWordPosition="2377" endWordPosition="2378">show the result of the algorithm when applied to the PDA of Figure 1c. 2.3 Intersection Algorithm The class of weighted pushdown automata is closed under intersection with weighted finite automata (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). Considering a pair (T1, T2) where one element is an FSA and the other element a PDA, then there exists a PDA T1 ∩T2, the intersection of T1 and T2, such that for all x ∈ E∗: (T1 ∩T2)(x) = T1(x)+T2(x). We assume in the following that T2 is an FSA. We also assume that T2 has no input-c transitions. When T2 has input-c transitions, an epsilon filter (Mohri, 2009; Allauzen et al., 2011) generalized to handle parentheses can be used. A state in T =T1∩T2 is a pair (q1, q2) where q1 is a state of T1 and q2 a state of T2. The initial state is I =(I1, I2). Given a transition e1 =(q1, a, w1, q′1) in T1, transitions out of (q1, q2) in T are obtained using the following rules. If a ∈ E, then e1 can be matched with a transition (q2, a, w2, q′2) in T2 resulting a transition ((q1, q2), a, w1+w2, (q′1, q′2)) in T. If a = c, then e1 is matched with staying in q2 resulting in a transition ((q1, q2), c, w1, (q′1, q2)). Finally, if a ∈ H, e1 is also matched with stay</context>
<context position="16261" citStr="Mohri (2009)" startWordPosition="2921" endWordPosition="2922">rom s by a balanced path. If s has several incoming open parenthesis transitions, a naive implementation might lead to the states in C3 to be visited up to exponentially many times. The basic idea of the algorithm is to memoize the shortest distance from s to states in C3. The 1376 pseudo-code is given in Figure 2. GETDISTANCE(T, s) starts a new instance of the shortest-distance algorithm from s using the queue S3, initially containing s. While the queue is not empty, a state is dequeued and its outgoing transitions examined (line 5-9). Transitions labeled by non-parenthesis are treated as in Mohri (2009) (line 9-10). When the considered transition e is labeled by a close parenthesis, it is remembered that it balances all incoming open parentheses in s labeled by i[e] by adding e to B[s, i[e]] (line 11-12). Finally, when e is labeled with an open parenthesis, if its destination has not already been visited, a new instance is started from n[e] (line 14-15). The destination states of all transitions balancing e are then relaxed (line 16-18). The space complexity of the algorithm is quadratic for two reasons. First, the number of non-infinity d[q, s] is |Q|2. Second, the space required for storin</context>
<context position="22412" citStr="Mohri, 2009" startWordPosition="4067" endWordPosition="4068">ypergraph, RTN and PDA representations can be generated in O(|s|3|G|) time and space (Aho and Ullman, 1972). The FSA representation can require an additional O(e|s|3|G|) time and space since the PDA expansion can be exponential. 2. Intersection: The intersection of a CFG Th with a finite automaton M can be performed by the classical Bar-Hillel algorithm (Bar-Hillel et al., 1964) with time and space complexity O(|Th||M|3).1 The PDA intersection algorithm from Section 2.3 has time and space complexity O(|Tp||M|). Finally, the FSA intersection algorithm has time and space complexity O(|Tf ||M|) (Mohri, 2009). 3. Shortest Path: The shortest path algorithm on the hypergraph, RTN, and FSA representations requires linear time and space (given the underlying acyclicity) (Huang, 2008; Mohri, 2009). As presented in Section 2.4, the PDA representation can require time cubic and space quadratic in |M|.2 Table 1 summarizes the complexity results. Note the PDA representation is equivalent in time and superior in space to the CFG/hypergraph representation, in general, and it can be superior in both space 1The modified Bar-Hillel construction described by Chiang (2007) has time and space complexity O(|Th||M|4</context>
</contexts>
<marker>Mohri, 2009</marker>
<rawString>Mehryar Mohri. 2009. Weighted automata algorithms. In Drosde et al. (Drosde et al., 2009), chapter 6, pages 213–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
<author>Giorgio Satta</author>
</authors>
<title>Probabilistic parsing as intersection.</title>
<date>2003</date>
<booktitle>In Proceedings of 8th International Workshop on Parsing Technologies,</booktitle>
<pages>137--148</pages>
<contexts>
<context position="13220" citStr="Nederhof and Satta, 2003" startWordPosition="2305" endWordPosition="2308"> final if q is final in T and z = c (p′((q, c))=p(q)). The set of states of T′ is the set of pairs (q, z) that can be reached from an initial state by transitions defined as above. The condition that T has a bounded stack ensures that this set is finite (since it implies that for any (q, z), |z |≤ K). The complexity of the algorithm is linear in O(|T′|) = O(e|T|). Figure 1d show the result of the algorithm when applied to the PDA of Figure 1c. 2.3 Intersection Algorithm The class of weighted pushdown automata is closed under intersection with weighted finite automata (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). Considering a pair (T1, T2) where one element is an FSA and the other element a PDA, then there exists a PDA T1 ∩T2, the intersection of T1 and T2, such that for all x ∈ E∗: (T1 ∩T2)(x) = T1(x)+T2(x). We assume in the following that T2 is an FSA. We also assume that T2 has no input-c transitions. When T2 has input-c transitions, an epsilon filter (Mohri, 2009; Allauzen et al., 2011) generalized to handle parentheses can be used. A state in T =T1∩T2 is a pair (q1, q2) where q1 is a state of T1 and q2 a state of T2. The initial state is I =(I1, I2). Given a transition e1 =(q1, a, w1, q′1) in T</context>
</contexts>
<marker>Nederhof, Satta, 2003</marker>
<rawString>Mark-Jan Nederhof and Giorgio Satta. 2003. Probabilistic parsing as intersection. In Proceedings of 8th International Workshop on Parsing Technologies, pages 137–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="26012" citStr="Och, 2003" startWordPosition="4678" endWordPosition="4679">port translation results on a development set tune-nw (1,755 sentences) and a test set test-nw (1,671 sentences). These contain translations produced by the GALE program and portions of the newswire sections of MT02 through MT06. In tuning the sys3See http://projects.ldc.upenn.edu/gale/data/catalog.html. We excluded the UN material and the LDC2002E18, LDC2004T08, LDC2007E08 and CUDonga collections. 0 7.5 × 10−9 7.5 × 10−8 7.5 × 10−7 207.5 20.2 4.1 0.9 Table 2: Number of ngrams (in millions) in the 1st pass 4-gram language models obtained with different θ values (top row). tems, standard MERT (Och, 2003) iterative parameter estimation under IBM BLEU4 is performed on the development set. The parallel corpus is aligned using MTTK (Deng and Byrne, 2008) in both source-to-target and target-to-source directions. We then follow standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009b) to extract hierarchical phrases from the union of the directional word alignments. We call a translation grammar the set of rules extracted from this process. We extract two translation grammars: • A restricted grammar where we apply the following additional constraint: rules are only consid</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ion Petre</author>
<author>Arto Salomaa</author>
</authors>
<title>Algebraic systems and pushdown automata.</title>
<date>2009</date>
<booktitle>In Drosde et al. (Drosde</booktitle>
<pages>257--289</pages>
<note>chapter 7,</note>
<contexts>
<context position="8801" citStr="Petre and Salomaa, 2009" startWordPosition="1377" endWordPosition="1380"> needed later. Informally, pushdown automata are finite automata that have been augmented with a stack. Typ1374 a (a) (b) (c) (d) Figure 1: PDA Examples: (a) Non-regular PDA accepting {anbn|n ∈ N}. (b) Regular (but not bounded-stack) PDA accepting a*b*. (c) Bounded-stack PDA accepting a*b* and (d) its expansion as an FSA. ically this is done by adding a stack alphabet and labeling each transition with a stack operation (a stack symbol to be pushed onto, popped or read from the stack) in additon to the usual input label (Aho and Ullman, 1972; Berstel, 1979) and weight (Kuich and Salomaa, 1986; Petre and Salomaa, 2009). Our equivalent representation allows a transition to be labeled by a stack operation or a regular input symbol but not both. Stack operations are represented by pairs of open and close parentheses (pushing a symbol on and popping it from the stack). The advantage of this representation is that is identical to the finite automaton representation except that certain symbols (the parentheses) have special semantics. As such, several finite-state algorithms either immediately generalize to this PDA representation or do so with minimal changes. The algorithms described in this section have been i</context>
</contexts>
<marker>Petre, Salomaa, 2009</marker>
<rawString>Ion Petre and Arto Salomaa. 2009. Algebraic systems and pushdown automata. In Drosde et al. (Drosde et al., 2009), chapter 7, pages 257–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>K Krstovski</author>
<author>F Choi</author>
<author>S Saleem</author>
<author>P Natarajan</author>
<author>M Decerbo</author>
<author>D Stallard</author>
</authors>
<title>Real-time speechto-speech translation for pdas.</title>
<date>2007</date>
<booktitle>In Proceedings ofIEEE International Conference on Portable Information Devices,</booktitle>
<pages>1--5</pages>
<contexts>
<context position="7700" citStr="Prasad et al., 2007" startWordPosition="1188" endWordPosition="1191">ng suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010). Relaxation techniques have also recently been shown to finding exact solutions in parsing (Koo et al., 2010) and in SMT with tree-to-string translation grammars and trigram language models (Rush and Collins, 2011), much smaller models compared to the work presented in this paper. Although entropy-pruned language models have been used to produce real-time translation systems (Prasad et al., 2007), we believe our use of entropy-pruned language models in two-pass translation to be novel. This is an approach that is widelyused in automatic speech recognition (Ljolje et al., 1999) and we note that it relies on efficient representation of very large search spaces T for subsequent rescoring, as is possible with FSAs and PDAs. 2 Pushdown Automata In this section, we formally define pushdown automata and give intersection, shortest-path and related algorithms that will be needed later. Informally, pushdown automata are finite automata that have been augmented with a stack. Typ1374 a (a) (b) (</context>
</contexts>
<marker>Prasad, Krstovski, Choi, Saleem, Natarajan, Decerbo, Stallard, 2007</marker>
<rawString>R. Prasad, K. Krstovski, F. Choi, S. Saleem, P. Natarajan, M. Decerbo, and D. Stallard. 2007. Real-time speechto-speech translation for pdas. In Proceedings ofIEEE International Conference on Portable Information Devices, pages 1 –5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
</authors>
<title>Exact decoding of syntactic translation models through lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>72--82</pages>
<contexts>
<context position="7515" citStr="Rush and Collins, 2011" startWordPosition="1160" endWordPosition="1163"> a relatively simple phrasebased translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar et al., 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010). Relaxation techniques have also recently been shown to finding exact solutions in parsing (Koo et al., 2010) and in SMT with tree-to-string translation grammars and trigram language models (Rush and Collins, 2011), much smaller models compared to the work presented in this paper. Although entropy-pruned language models have been used to produce real-time translation systems (Prasad et al., 2007), we believe our use of entropy-pruned language models in two-pass translation to be novel. This is an approach that is widelyused in automatic speech recognition (Ljolje et al., 1999) and we note that it relies on efficient representation of very large search spaces T for subsequent rescoring, as is possible with FSAs and PDAs. 2 Pushdown Automata In this section, we formally define pushdown automata and give i</context>
</contexts>
<marker>Rush, Collins, 2011</marker>
<rawString>Alexander M. Rush and Michael Collins. 2011. Exact decoding of syntactic translation models through lagrangian relaxation. In Proceedings of ACL-HLT, pages 72–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>1998</date>
<booktitle>In Proceedings of DARPA Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>270--274</pages>
<contexts>
<context position="5281" citStr="Stolcke, 1998" startWordPosition="818" endWordPosition="819">re the computational complexity of using these different representations. We show that the PDA representation is particularly suited for decoding with large SCFGs and compact LMs. We present Chinese-English translation results under the FSA and PDA translation representations. We describe a two-pass translation strategy which we have developed to allow use of the PDA representation in large-scale translation. In the first pass, translation is done using a lattice-generating version of the shortest path algorithm. The full translation grammar is used but with a compact, entropy-pruned version (Stolcke, 1998) of the full language model. This first-step uses admissible pruning and lattice generation under the compact language model. In the second pass, the original, unpruned LM is simply applied to the lattices produced in the first pass. We find that entropy-pruning and first-pass translation can be done so as to introduce very few search errors in the overall process; we can identify search errors in this experiment by comparison to exact translation under the full translation grammar and language model using the FSA representation. We then investigate a translation grammar which is large enough </context>
<context position="27867" citStr="Stolcke, 1998" startWordPosition="4986" endWordPosition="4987"> total of 1.3B words. We will call this language model M1. For large language model rescoring we also use the LM M2 obtained by interpolating M1 with a zero-cutoff stupidbackoff (Brants et al., 2007) 5-gram estimated using 6.6B words of English newswire text. We next describe how we build translation systems using entropy-pruned language models. 1. We build a baseline HiFST system that uses M1 and a hierarchical grammar G, parameters being optimized with MERT under BLEU. 4See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl 1379 2. We then use entropy-based pruning of the language model (Stolcke, 1998) under a relative perplexity threshold of θ to reduce the size of M1. We will call the resulting language model as MB. Table 2 shows the number of n-grams (in millions) obtained for different θ values. 3. We translate with MB using the same parameters obtained in MERT in step 1, except for the word penalty, tuned over the lattices under BLEU performance. This produces a translation lattice in the topmost cell that contains hypotheses with exact scores under the translation grammar and MB. 4. Translation lattices in the topmost cell are pruned with a likelihood-based beam width 0. 5. We remove </context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Andreas Stolcke. 1998. Entropy-based pruning of backoff language models. In Proceedings of DARPA Broadcast News Transcription and Understanding Workshop, pages 270–274.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>