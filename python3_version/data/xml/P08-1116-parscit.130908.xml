<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.994222">
Combining Multiple Resources to Improve SMT-based Paraphrasing Model*
</title>
<author confidence="0.999552">
Shiqi Zhao1, Cheng Niu2, Ming Zhou2, Ting Liu1, Sheng Li1
</author>
<affiliation confidence="0.998974">
1Harbin Institute of Technology, Harbin, China
</affiliation>
<email confidence="0.939744">
{zhaosq,tliu,lisheng}@ir.hit.edu.cn
</email>
<affiliation confidence="0.892664">
2Microsoft Research Asia, Beijing, China
</affiliation>
<email confidence="0.998712">
{chengniu,mingzhou}@microsoft.com
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999221875">
This paper proposes a novel method that ex-
ploits multiple resources to improve statisti-
cal machine translation (SMT) based para-
phrasing. In detail, a phrasal paraphrase ta-
ble and a feature function are derived from
each resource, which are then combined in a
log-linear SMT model for sentence-level para-
phrase generation. Experimental results show
that the SMT-based paraphrasing model can
be enhanced using multiple resources. The
phrase-level and sentence-level precision of
the generated paraphrases are above 60% and
55%, respectively. In addition, the contribu-
tion of each resource is evaluated, which indi-
cates that all the exploited resources are useful
for generating paraphrases of high quality.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.972577734693878">
Paraphrases are alternative ways of conveying the
same meaning. Paraphrases are important in many
natural language processing (NLP) applications,
such as machine translation (MT), question an-
swering (QA), information extraction (IE), multi-
document summarization (MDS), and natural lan-
guage generation (NLG).
This paper addresses the problem of sentence-
level paraphrase generation, which aims at generat-
ing paraphrases for input sentences. An example of
sentence-level paraphrases can be seen below:
S1: The table was set up in the carriage shed.
S2: The table was laid under the cart-shed.
*This research was finished while the first author worked as
an intern in Microsoft Research Asia.
Paraphrase generation can be viewed as monolin-
gual machine translation (Quirk et al., 2004), which
typically includes a translation model and a lan-
guage model. The translation model can be trained
using monolingual parallel corpora. However, ac-
quiring such corpora is not easy. Hence, data sparse-
ness is a key problem for the SMT-based paraphras-
ing. On the other hand, various methods have been
presented to extract phrasal paraphrases from dif-
ferent resources, which include thesauri, monolin-
gual corpora, bilingual corpora, and the web. How-
ever, little work has been focused on using the ex-
tracted phrasal paraphrases in sentence-level para-
phrase generation.
In this paper, we exploit multiple resources to
improve the SMT-based paraphrase generation. In
detail, six kinds of resources are utilized, includ-
ing: (1) an automatically constructed thesaurus, (2)
a monolingual parallel corpus from novels, (3) a
monolingual comparable corpus from news articles,
(4) a bilingual phrase table, (5) word definitions
from Encarta dictionary, and (6) a corpus of simi-
lar user queries. Among the resources, (1), (2), (3),
and (4) have been investigated by other researchers,
while (5) and (6) are first used in this paper. From
those resources, six phrasal paraphrase tables are ex-
tracted, which are then used in a log-linear SMT-
based paraphrasing model.
Both phrase-level and sentence-level evaluations
were carried out in the experiments. In the former
one, phrase substitutes occurring in the paraphrase
sentences were evaluated. While in the latter one,
the acceptability of the paraphrase sentences was
evaluated. Experimental results show that: (1) The
</bodyText>
<page confidence="0.960796">
1021
</page>
<note confidence="0.714102">
Proceedings of ACL-08: HLT, pages 1021–1029,
</note>
<page confidence="0.536985">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.99971775">
SMT-based paraphrasing is enhanced using multiple
resources. The phrase-level and sentence-level pre-
cision of the generated paraphrases exceed 60% and
55%, respectively. (2) Although the contributions of
the resources differ a lot, all the resources are useful.
(3) The performance of the method varies greatly on
different test sets and it performs best on the test set
of news sentences, which are from the same source
as most of the training data.
The rest of the paper is organized as follows: Sec-
tion 2 reviews related work. Section 3 introduces the
log-linear model for paraphrase generation. Section
4 describes the phrasal paraphrase extraction from
different resources. Section 5 presents the parameter
estimation method. Section 6 shows the experiments
and results. Section 7 draws the conclusion.
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999846987012987">
Paraphrases have been used in many NLP applica-
tions. In MT, Callison-Burch et al. (2006) utilized
paraphrases of unseen source phrases to alleviate
data sparseness. Kauchak and Barzilay (2006) used
paraphrases of the reference translations to improve
automatic MT evaluation. In QA, Lin and Pantel
(2001) and Ravichandran and Hovy (2002) para-
phrased the answer patterns to enhance the recall of
answer extraction. In IE, Shinyama et al. (2002)
automatically learned paraphrases of IE patterns to
reduce the cost of creating IE patterns by hand. In
MDS, McKeown et al. (2002) identified paraphrase
sentences across documents before generating sum-
marizations. In NLG, Iordanskaja et al. (1991) used
paraphrases to generate more varied and fluent texts.
Previous work has examined various resources for
acquiring paraphrases, including thesauri, monolin-
gual corpora, bilingual corpora, and the web. The-
sauri, such as WordNet, have been widely used
for extracting paraphrases. Some researchers ex-
tract synonyms as paraphrases (Kauchak and Barzi-
lay, 2006), while some others use looser defini-
tions, such as hypernyms and holonyms (Barzilay
and Elhadad, 1997). Besides, the automatically
constructed thesauri can also be used. Lin (1998)
constructed a thesaurus by automatically clustering
words based on context similarity.
Barzilay and McKeown (2001) used monolingual
parallel corpora for identifying paraphrases. They
exploited a corpus of multiple English translations
of the same source text written in a foreign language,
from which phrases in aligned sentences that appear
in similar contexts were extracted as paraphrases. In
addition, Finch et al. (2005) applied MT evalua-
tion methods (BLEU, NIST, WER and PER) to build
classifiers for paraphrase identification.
Monolingual parallel corpora are difficult to find,
especially in non-literature domains. Alternatively,
some researchers utilized monolingual compara-
ble corpora for paraphrase extraction. Different
news articles reporting on the same event are com-
monly used as monolingual comparable corpora,
from which both paraphrase patterns and phrasal
paraphrases can be derived (Shinyama et al., 2002;
Barzilay and Lee, 2003; Quirk et al., 2004).
Lin and Pantel (2001) learned paraphrases from
a parsed monolingual corpus based on an extended
distributional hypothesis, where if two paths in de-
pendency trees tend to occur in similar contexts it is
hypothesized that the meanings of the paths are simi-
lar. The monolingual corpus used in their work is not
necessarily parallel or comparable. Thus it is easy
to obtain. However, since this resource is used to
extract paraphrase patterns other than phrasal para-
phrases, we do not use it in this paper.
Bannard and Callison-Burch (2005) learned
phrasal paraphrases using bilingual parallel cor-
pora. The basic idea is that if two phrases are
aligned to the same translation in a foreign language,
they may be paraphrases. This method has been
demonstrated effective in extracting large volume of
phrasal paraphrases. Besides, Wu and Zhou (2003)
exploited bilingual corpora and translation informa-
tion in learning synonymous collocations.
In addition, some researchers extracted para-
phrases from the web. For example, Ravichandran
and Hovy (2002) retrieved paraphrase patterns from
the web using hand-crafted queries. Pasca and Di-
enes (2005) extracted sentence fragments occurring
in identical contexts as paraphrases from one bil-
lion web documents. Since web mining is rather
time consuming, we do not exploit the web to ex-
tract paraphrases in this paper.
So far, two kinds of methods have been pro-
posed for sentence-level paraphrase generation, i.e.,
the pattern-based and SMT-based methods. Auto-
matically learned patterns have been used in para-
</bodyText>
<page confidence="0.993293">
1022
</page>
<bodyText confidence="0.999901260869565">
phrase generation. For example, Barzilay and Lee
(2003) applied multiple-sequence alignment (MSA)
to parallel news sentences and induced paraphras-
ing patterns for generating new sentences. Pang et
al. (2003) built finite state automata (FSA) from se-
mantically equivalent translation sets based on syn-
tactic alignment and used the FSAs in paraphrase
generation. The pattern-based methods can generate
complex paraphrases that usually involve syntactic
variation. However, the methods were demonstrated
to be of limited generality (Quirk et al., 2004).
Quirk et al. (2004) first recast paraphrase gener-
ation as monolingual SMT. They generated para-
phrases using a SMT system trained on parallel sen-
tences extracted from clustered news articles. In
addition, Madnani et al. (2007) also generated
sentence-level paraphrases based on a SMT model.
The advantage of the SMT-based method is that
it achieves better coverage than the pattern-based
method. The main difference between their methods
and ours is that they only used bilingual parallel cor-
pora as paraphrase resource, while we exploit and
combine multiple resources.
</bodyText>
<sectionHeader confidence="0.993063" genericHeader="method">
3 SMT-based Paraphrasing Model
</sectionHeader>
<bodyText confidence="0.999981">
The SMT-based paraphrasing model used by Quirk
et al. (2004) was the noisy channel model of Brown
et al. (1993), which identified the optimal paraphrase
T* of a sentence S by finding:
In contrast, we adopt a log-linear model (Och
and Ney, 2002) in this work, since multiple para-
phrase tables can be easily combined in the log-
linear model. Specifically, feature functions are de-
rived from each paraphrase resource and then com-
bined with the language model feature1:
</bodyText>
<equation confidence="0.875712">
ATM ihTM i(T, S)+
ALMhLM(T, S)} (2)
</equation>
<bodyText confidence="0.921024666666667">
where N is the number of paraphrase tables.
hTM i(T, S) is the feature function based on the i-
th paraphrase table PTi. hLM(T, S) is the language
1The reordering model is not considered in our model.
model feature. ATM i and ALM are the weights of
the feature functions. hTM i(T, S) is defined as:
</bodyText>
<equation confidence="0.996706">
K;
hTM i(T, S) = log H Scorei(Tk, Sk) (3)
k=1
</equation>
<bodyText confidence="0.9995354">
where Ki is the number of phrase substitutes from
S to T based on PTi. Tk in T and Sk in S are
phrasal paraphrases in PTi. Scorei(Tk, Sk) is the
paraphrase likelihood according to PTi2. A 5-gram
language model is used, therefore:
</bodyText>
<equation confidence="0.9977175">
hLM(T, S) = log �� p(tj|tj−4, ..., tj−1) (4)
j=1
</equation>
<bodyText confidence="0.975051">
where J is the length of T, tj is the j-th word of T.
</bodyText>
<sectionHeader confidence="0.99488" genericHeader="method">
4 Exploiting Multiple Resources
</sectionHeader>
<bodyText confidence="0.999054416666667">
This section describes the extraction of phrasal
paraphrases using various resources. Similar to
Pharaoh (Koehn, 2004), our decoder3 uses top 20
paraphrase options for each input phrase in the de-
fault setting. Therefore, we keep at most 20 para-
phrases for a phrase when extracting phrasal para-
phrases using each resource.
1 - Thesaurus: The thesaurus4 used in this work
was automatically constructed by Lin (1998). The
similarity of two words es and e2 was calculated
through the surrounding context words that have de-
pendency relations with the investigated words:
</bodyText>
<equation confidence="0.998381666666667">
Sim(e1, e2)
= E(r,e)ETr(e1)nTr(e2)(I(e1, r, e) + I(e2, r, e))
E(r,e)ETr(e1) I(e1, r, e) + E(r,e)ETr(e2) I(e2, r, e) (5)
</equation>
<bodyText confidence="0.99995775">
where Tr(ei) denotes the set of words that have de-
pendency relation r with word ei. I(ei, r, e) is the
mutual information between ei, r and e.
For each word, we keep 20 most similar words as
paraphrases. In this way, we extract 502,305 pairs of
paraphrases. The paraphrasing score Scores(ps, p2)
used in Equation (3) is defined as the similarity
based on Equation (5).
</bodyText>
<footnote confidence="0.9124674">
2If none of the phrase substitutes from S to T is from PTi
(i.e., Ki = 0), we cannot compute hTM i(T, S) as in Equation
(3). In this case, we assign hTM i(T, S) a minimum value.
3The decoder used here is a re-implementation of Pharaoh.
4http://www.cs.ualberta.ca/ lindek/downloads.htm.
</footnote>
<equation confidence="0.999245444444445">
T* = arg max{P(T|S)}
T
= arg max
T
{P(S|T)P(T)} (1)
T* = arg max {
T
N
i=1
</equation>
<page confidence="0.771781">
1023
</page>
<bodyText confidence="0.9983615">
2 - Monolingual parallel corpus: Following Barzi-
lay and McKeown (2001), we exploit a corpus
of multiple English translations of foreign nov-
els, which contains 25,804 parallel sentence pairs.
We find that most paraphrases extracted using the
method of Barzilay and McKeown (2001) are quite
short. Thus we employ a new approach for para-
phrase extraction. Specifically, we parse the sen-
tences with CollinsParser5 and extract the chunks
from the parsing results. Let S1 and S2 be a pair
of parallel sentences, p1 and p2 two chunks from S1
and S2, we compute the similarity of p1 and p2 as:
</bodyText>
<equation confidence="0.999943">
Sim(p1,p2) = αSimeontent(p1,p2)+
(1 − α)Simeontext(p1,p2) (6)
</equation>
<bodyText confidence="0.989075607142857">
where, Sim.ntent(p1, p2) is the content similarity,
which is the word overlapping rate of p1 and p2.
Sim.ntext(p1, p2) is the context similarity, which is
the word overlapping rate of the contexts of p1 and
p26. If the similarity of p1 and p2 exceeds a thresh-
old Th1, they are identified as paraphrases. We ex-
tract 18,698 pairs of phrasal paraphrases from this
resource. The paraphrasing score Score2(p1, p2) is
defined as the similarity in Equation (6). For the
paraphrases occurring more than once, we use their
maximum similarity as the paraphrasing score.
3 - Monolingual comparable corpus: Similar to
the methods in (Shinyama et al., 2002; Barzilay and
Lee, 2003), we construct a corpus of comparable
documents from a large corpus D of news articles.
The corpus D contains 612,549 news articles. Given
articles d1 and d2 from D, if their publication date
interval is less than 2 days and their similarity7 ex-
ceeds a threshold Th2, they are recognized as com-
parable documents. In this way, a corpus containing
5,672,864 pairs of comparable documents is con-
structed. From the comparable corpus, parallel sen-
tences are extracted. Let s1 and s2 be two sentences
from comparable documents d1 and d2, if their sim-
ilarity based on word overlapping rate is above a
threshold Th3, s1 and s2 are identified as parallel
sentences. In this way, 872,330 parallel sentence
pairs are extracted.
</bodyText>
<footnote confidence="0.9769112">
5http://people.csail.mit.edu/mcollins/code.html
6The context of a chunk is made up of 6 words around the
chunk, 3 to the left and 3 to the right.
7The similarity of two documents is computed using the vec-
tor space model and the word weights are based on tf·idf.
</footnote>
<bodyText confidence="0.999748375">
We run Giza++ (Och and Ney, 2000) on the paral-
lel sentences and then extract aligned phrases as de-
scribed in (Koehn, 2004). The generated paraphrase
table is pruned by keeping the top 20 paraphrases for
each phrase. After pruning, 100,621 pairs of para-
phrases are extracted. Given phrase p1 and its para-
phrase p2, we compute Score3(p1, p2) by relative
frequency (Koehn et al., 2003):
</bodyText>
<equation confidence="0.9986835">
Score3(p1, p2) = p(p2Jp1) = count(p2, p1)
� count(p�, p1) (7)
</equation>
<bodyText confidence="0.993867235294117">
People may wonder why we do not use the same
method on the monolingual parallel and comparable
corpora. This is mainly because the volumes of the
two corpora differ a lot. In detail, the monolingual
parallel corpus is fairly small, thus automatical word
alignment tool like Giza++ may not work well on
it. In contrast, the monolingual comparable corpus
is quite large, hence we cannot conduct the time-
consuming syntactic parsing on it as we do on the
monolingual parallel corpus.
4 - Bilingual phrase table: We first construct
a bilingual phrase table that contains 15,352,469
phrase pairs from an English-Chinese parallel cor-
pus. We extract paraphrases from the bilingual
phrase table and compute the paraphrasing score
of phrases p1 and p2 as in (Bannard and Callison-
Burch, 2005):
</bodyText>
<equation confidence="0.9534975">
�Score4(p1, p2) = p(f|p1)p(p2|f) (8)
f
</equation>
<bodyText confidence="0.983267214285714">
where f denotes a Chinese translation of both p1 and
p2. p(f|p1) and p(p2|f) are the translation probabil-
ities provided by the bilingual phrase table. For each
phrase, the top 20 paraphrases are kept according
to the score in Equation (8). As a result, 3,177,600
pairs of phrasal paraphrases are extracted.
5 - Encarta dictionary definitions: Words and their
definitions can be regarded as paraphrases. Here
are some examples from Encarta dictionary: “hur-
ricane: severe storm”, “clever: intelligent”, “travel:
go on journey”. In this work, we extract words’ def-
initions from Encarta dictionary web pages8. If a
word has more than one definition, all of them are
extracted. Note that the words and definitions in the
</bodyText>
<footnote confidence="0.911457">
8http://encarta.msn.com/encnet/features/dictionary/diction-
aryhome.aspx
</footnote>
<page confidence="0.993398">
1024
</page>
<bodyText confidence="0.9999775">
dictionary are lemmatized, but words in sentences
are usually inflected. Hence, we expand the word
- definition pairs by providing the inflected forms.
Here we use an inflection list and some rules for in-
flection. After expanding, 159,456 pairs of phrasal
paraphrases are extracted. Let &lt; p1, p2 &gt; be a word
- definition pair, the paraphrasing score is defined
according to the rank of p2 in all of p1’s definitions:
</bodyText>
<equation confidence="0.999406">
Score5(p1,p2) = γi−1 (9)
</equation>
<bodyText confidence="0.997107">
where γ is a constant (we empirically set γ = 0.9)
and i is the rank of p2 in p1’s definitions.
</bodyText>
<sectionHeader confidence="0.301979" genericHeader="method">
6 - Similar user queries: Clusters of similar user
</sectionHeader>
<bodyText confidence="0.733255">
queries have been used for query expansion and sug-
gestion (Gao et al., 2007). Since most queries are at
the phrase level, we exploit similar user queries as
phrasal paraphrases. In our experiment, we use the
corpus of clustered similar MSN queries constructed
by Gao et al. (2007). The similarity of two queries
p1 and p2 is computed as:
</bodyText>
<equation confidence="0.9999105">
Sim(p1,p2) = βSimcontent(p1,p2)+
(1 − β)Simc�ic*−through(p1,p2) (10)
</equation>
<bodyText confidence="0.999034909090909">
where Simcontent(p1,p2) is the content similarity,
which is computed as the word overlapping rate of
p1 and p2. Simclick−through(p1,p2) is the click
through similarity, which is the overlapping rate of
the user clicked documents for p1 and p2. For each
query q, we keep the top 20 similar queries, whose
similarity with q exceeds a threshold Tho. As a re-
sult, 395,284 pairs of paraphrases are extracted. The
score Score6(p1, p2) is defined as the similarity in
Equation (10).
7 - Self-paraphrase: In addition to the six resources
introduced above, a special paraphrase table is used,
which is made up of pairs of identical words. The
reason why this paraphrase table is necessary is that
a word should be allowed to keep unchanged in para-
phrasing. This is a difference between paraphras-
ing and MT, since all words should be translated in
MT. In our experiments, all the words that occur in
the six paraphrase table extracted above are gath-
ered to form the self-paraphrase table, which con-
tains 110,403 word pairs. The score Score7(p1, p2)
is set 1 for each identical word pair.
</bodyText>
<sectionHeader confidence="0.991634" genericHeader="method">
5 Parameter Estimation
</sectionHeader>
<bodyText confidence="0.999949333333333">
The weights of the feature functions, namely λTM i
(i = 1, 2,..., 7) and λLM, need estimation9. In MT,
the max-BLEU algorithm is widely used to estimate
parameters. However, it may not work in our case,
since it is more difficult to create a reference set of
paraphrases.
We propose a new technique to estimate parame-
ters in paraphrasing. The assumption is that, since a
SMT-based paraphrase is generated through phrase
substitution, we can measure the quality of a gener-
ated paraphrase by measuring its phrase substitutes.
Generally, the paraphrases containing more correct
phrase substitutes are judged as better paraphrases10.
We therefore present the phrase substitution error
rate (PSER) to score a generated paraphrase T:
</bodyText>
<equation confidence="0.996428">
PSER(T) = kPSo(T)k/kPS(T)k (11)
</equation>
<bodyText confidence="0.9990548">
where PS(T) is the set of phrase substitutes in T
and PSo(T) is the set of incorrect substitutes.
In practice, we keep top n paraphrases for each
sentence S. Thus we calculate the PSER for each
source sentence S as:
</bodyText>
<equation confidence="0.978344">
PS(Ti)k (12)
</equation>
<bodyText confidence="0.999971333333333">
where Ti is the i-th generated paraphrase of S.
Suppose there are N sentences in the develop-
ment set, the overall PSER is computed as:
</bodyText>
<equation confidence="0.993875333333333">
N
PSER = E PSER(Sj) (13)
j=1
</equation>
<bodyText confidence="0.998939888888889">
where Sj is the j-th sentence in the development set.
Our development set contains 75 sentences (de-
scribed in detail in Section 6). For each sentence,
all possible phrase substitutes are extracted from the
six paraphrase tables above. The extracted phrase
substitutes are then manually labeled as “correct” or
“incorrect”. A phrase substitute is considered as cor-
rect only if the two phrases have the same meaning
in the given sentence and the sentence generated by
</bodyText>
<footnote confidence="0.481778375">
9Note that, we also use some other parameters when extract-
ing phrasal paraphrases from different resources, such as the
thresholds Th1, The, Thi, Tho, as well as α and ,3 in Equa-
tion (6) and (10). These parameters are estimated using differ-
ent development sets from the investigated resources. We do
not describe the estimation of them due to space limitation.
10Paraphrasing a word to itself (based on the 7-th paraphrase
table above) is not regarded as a substitute.
</footnote>
<equation confidence="0.998060375">
n
U
i=1
PSER(S) = k
PSo(Ti)k/k
n
U
i=1
</equation>
<page confidence="0.959255">
1025
</page>
<bodyText confidence="0.9999474">
substituting the source phrase with the target phrase
remains grammatical. In decoding, the phrase sub-
stitutes are printed out and then the PSER is com-
puted based on the labeled data.
Using each set of parameters, we generate para-
phrases for the sentences in the development set
based on Equation (2). PSER is then computed as
in Equation (13). We use the gradient descent algo-
rithm (Press et al., 1992) to minimize PSER on the
development set and get the optimal parameters.
</bodyText>
<sectionHeader confidence="0.999627" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999939875">
To evaluate the performance of the method on dif-
ferent types of test data, we used three kinds of sen-
tences for testing, which were randomly extracted
from Google news, free online novels, and forums,
respectively. For each type, 50 sentences were ex-
tracted as test data and another 25 were extracted as
development data. For each test sentence, top 10 of
the generated paraphrases were kept for evaluation.
</bodyText>
<subsectionHeader confidence="0.983622">
6.1 Phrase-level Evaluation
</subsectionHeader>
<bodyText confidence="0.999982533333333">
The phrase-level evaluation was carried out to in-
vestigate the contributions of the paraphrase tables.
For each test sentence, all possible phrase substitutes
were first extracted from the paraphrase tables and
manually labeled as “correct” or “incorrect”. Here,
the criterion for identifying paraphrases is the same
as that described in Section 5. Then, in the stage
of decoding, the phrase substitutes were printed out
and evaluated using the labeled data.
Two metrics were used here. The first is the
number of distinct correct substitutes (#DCS). Ob-
viously, the more distinct correct phrase substitutes
a paraphrase table can provide, the more valuable it
is. The second is the accuracy of the phrase substi-
tutes, which is computed as:
</bodyText>
<figure confidence="0.275593666666667">
#correct phrase substitutes
Accuracy � (14)
#all phrase substitutes
</figure>
<bodyText confidence="0.9988245">
To evaluate the PTs learned from different re-
sources, we first used each PT (from 1 to 6) along
with PT-7 in decoding. The results are shown in Ta-
ble 1. It can be seen that PT-4 is the most useful, as
it provides the most correct substitutes and the ac-
curacy is the highest. We believe that it is because
PT-4 is much larger than the other PTs. Compared
with PT-4, the accuracies of the other PTs are fairly
</bodyText>
<table confidence="0.970432">
PT combination #DCS Accuracy
1+7 178 14.61%
2+7 94 25.06%
3+7 202 18.35%
4+7 553 56.93%
5+7 231 20.48%
6+7 21 14.42%
</table>
<tableCaption confidence="0.852027">
Table 1: Contributions of the paraphrase tables.
PT-1: from the thesaurus; PT-2: from the monolingual
parallel corpus; PT-3: from the monolingual comparable
corpus; PT-4: from the bilingual parallel corpus; PT-5:
from the Encarta dictionary definitions; PT-6: from the
similar MSN user queries; PT-7: self-paraphrases.
</tableCaption>
<bodyText confidence="0.999692303030303">
low. This is because those PTs are smaller, thus they
can provide fewer correct phrase substitutes. As a
result, plenty of incorrect substitutes were included
in the top 10 generated paraphrases.
PT-6 provides the least correct phrase substitutes
and the accuracy is the lowest. There are several
reasons. First, many phrases in PT-6 are not real
phrases but only sets of keywords (e.g., “lottery re-
sults ny”), which may not appear in sentences. Sec-
ond, many words in this table have spelling mis-
takes (e.g., “widows vista”). Third, some phrase
pairs in PT-6 are not paraphrases but only “related
queries” (e.g., “back tattoo” vs. “butterfly tattoo”).
Fourth, many phrases of PT-6 contain proper names
or out-of-vocabulary words, which are difficult to be
matched. The accuracy based on PT-1 is also quite
low. We found that it is mainly because the phrase
pairs in PT-1 are automatically clustered, many of
which are merely “similar” words rather than syn-
onyms (e.g., “borrow” vs. “buy”).
Next, we try to find out whether it is necessary to
combine all PTs. Thus we conducted several runs,
each of which added the most useful PT from the
left ones. The results are shown in Table 2. We can
see that all the PTs are useful, as each PT provides
some new correct phrase substitutes and the accu-
racy increases when adding each PT except PT-1.
Since the PTs are extracted from different re-
sources, they have different contributions. Here we
only discuss the contributions of PT-5 and PT-6,
which are first used in paraphrasing in this paper.
PT-5 is useful for paraphrasing uncommon concepts
since it can “explain” concepts with their definitions.
</bodyText>
<page confidence="0.969364">
1026
</page>
<table confidence="0.999692285714286">
PT combination #DCS Accuracy
4+7 553 56.93%
4+5 +7 581 58.97%
4+5 +3 +7 638 59.42%
4+5 +3 +2 +7 649 60.15%
4+5 +3 +2 +1 +7 699 60.14%
4+5 +3 +2 +1 +6+7 711 60.16%
</table>
<tableCaption confidence="0.9886865">
Table 2: Performances of different combinations of para-
phrase tables.
</tableCaption>
<bodyText confidence="0.998058">
For instance, in the following test sentence S1, the
word “amnesia” is a relatively uncommon word, es-
pecially for the people using English as the second
language. Based on PT-5, S1 can be paraphrased
into T1, which is much easier to understand.
</bodyText>
<footnote confidence="0.586876">
S1: I was suffering from amnesia.
T1: I was suffering from memory loss.
</footnote>
<bodyText confidence="0.985256294117647">
The disadvantage of PT-5 is that substituting
words with the definitions sometimes leads to gram-
matical errors. For instance, substituting “heat
shield” in the sentence S2 with “protective barrier
against heat” keeps the meaning unchanged. How-
ever, the paraphrased sentence T2 is ungrammatical.
S2: The U.S. space agency has been cautious
about heat shield damage.
T2: The U.S. space administration has been
cautious about protective barrier against heat
damage.
As previously mentioned, PT-6 is less effective
compared with the other PTs. However, it is use-
ful for paraphrasing some special phrases, such as
digital products, computer software, etc, since these
phrases often appear in user queries. For example,
S3 below can be paraphrased into T3 using PT-6.
</bodyText>
<footnote confidence="0.709100125">
S3: I have a canon powershot S230 that uses
CF memory cards.
T3: I have a canon digital camera S230 that
uses CF memory cards.
The phrase “canon powershot” can hardly be
paraphrased using the other PTs. It suggests that PT-
6 is useful for paraphrasing new emerging concepts
and expressions.
</footnote>
<table confidence="0.999893">
Test sentences Top-1 Top-5 Top-10
All 150 55.33% 45.20% 39.28%
50 from news 70.00% 62.00% 57.03%
50 from novel 56.00% 46.00% 37.42%
50 from forum 40.00% 27.60% 23.34%
</table>
<tableCaption confidence="0.999746">
Table 3: Top-n accuracy on different test sentences.
</tableCaption>
<subsectionHeader confidence="0.999323">
6.2 Sentence-level Evaluation
</subsectionHeader>
<bodyText confidence="0.827274772727273">
In this section, we evaluated the sentence-level qual-
ity of the generated paraphrases11. In detail, each
generated paraphrase was manually labeled as “ac-
ceptable” or “unacceptable”. Here, the criterion for
counting a sentence Tas an acceptable paraphrase of
sentence S is that T is understandable and its mean-
ing is not evidently changed compared with S. For
example, for the sentence S4, T4 is an acceptable
paraphrase generated using our method.
S4: The strain on USforces of fighting in Iraq
and Afghanistan was exposed yesterday when
the Pentagon published a report showing that
the number of suicides among US troops is at
its highest level since the 1991 Gulf war.
T4: The pressure on US troops of fighting in
Iraq and Afghanistan was revealed yesterday
when the Pentagon released a report showing
that the amount of suicides among US forces
is at its top since the 1991 Gulf conflict.
We carried out sentence-level evaluation using the
top-1, top-5, and top-10 results of each test sentence.
The accuracy of the top-n results was computed as:
</bodyText>
<equation confidence="0.981374">
N
Accuracytop−n = N X nz (15)
</equation>
<bodyText confidence="0.928517538461538">
where N is the number of test sentences. ni is the
number of acceptable paraphrases in the top-n para-
phrases of the i-th test sentence.
We computed the accuracy on the whole test set
(150 sentences) as well as on the three subsets, i.e.,
the 50 news sentences, 50 novel sentences, and 50
forum sentences. The results are shown in table 3.
It can be seen that the accuracy varies greatly on
different test sets. The accuracy on the news sen-
tences is the highest, while that on the forum sen-
tences is the lowest. There are several reasons. First,
11The evaluation was based on the paraphrasing results using
the combination of all seven PTs.
</bodyText>
<page confidence="0.991167">
1027
</page>
<bodyText confidence="0.99998282051282">
the largest PT used in the experiments is extracted
using the bilingual parallel data, which are mostly
from news documents. Thus, the test set of news
sentences is more similar to the training data.
Second, the news sentences are formal while the
novel and forum sentences are less formal. Espe-
cially, some of the forum sentences contain spelling
mistakes and grammar mistakes.
Third, we find in the results that, most phrases
paraphrased in the novel and forum sentences are
commonly used phrases or words, such as “food”,
“good”, “find”, etc. These phrases are more dif-
ficult to paraphrase than the less common phrases,
since they usually have much more paraphrases in
the PTs. Therefore, it is more difficult to choose the
right paraphrase from all the candidates when con-
ducting sentence-level paraphrase generation.
Fourth, the forum sentences contain plenty of
words such as “board (means computer board)”,
“site (means web site)”, “mouse (means computer
mouse)”, etc. These words are polysemous and have
particular meanings in the domains of computer sci-
ence and internet. Our method performs poor when
paraphrasing these words since the domain of a con-
text sentence is hard to identify.
After observing the results, we find that there are
three types of errors: (1) syntactic errors: the gener-
ated sentences are ungrammatical. About 32% of the
unacceptable results are due to syntactic errors. (2)
semantic errors: the generated sentences are incom-
prehensible. Nearly 60% of the unacceptable para-
phrases have semantic errors. (3) non-paraphrase:
the generated sentences are well formed and com-
prehensible but are not paraphrases of the input sen-
tences. 8% of the unacceptable results are of this
type. We believe that many of the errors above can
be avoided by applying syntactic constraints and by
making better use of context information in decod-
ing, which is left as our future work.
</bodyText>
<sectionHeader confidence="0.999286" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999993454545454">
This paper proposes a method that improves the
SMT-based sentence-level paraphrase generation
using phrasal paraphrases automatically extracted
from different resources. Our contribution is that
we combine multiple resources in the framework of
SMT for paraphrase generation, in which the dic-
tionary definitions and similar user queries are first
used as phrasal paraphrases. In addition, we analyze
and compare the contributions of different resources.
Experimental results indicate that although the
contributions of the exploited resources differ a lot,
they are all useful to sentence-level paraphrase gen-
eration. Especially, the dictionary definitions and
similar user queries are effective for paraphrasing
some certain types of phrases.
In the future work, we will try to use syntactic
and context constraints in paraphrase generation to
enhance the acceptability of the paraphrases. In ad-
dition, we will extract paraphrase patterns that con-
tain more structural variation and try to combine the
SMT-based and pattern-based systems for sentence-
level paraphrase generation.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999911">
We would like to thank Mu Li for providing us with
the SMT decoder. We are also grateful to Dongdong
Zhang for his help in the experiments.
</bodyText>
<sectionHeader confidence="0.999458" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999932807692308">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, pages 597-604.
Regina Barzilay and Michael Elhadad. 1997. Using Lex-
ical Chains for Text Summarization. In Proceedings of
the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10-17.
Regina Barzilay and Lillian Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Multiple-
Sequence Alignment. In Proceedings of HLT-NAACL,
pages 16-23.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting Paraphrases from a Parallel Corpus. In Pro-
ceedings of ACL, pages 50-57.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. In Computational Linguistics 19(2): 263-311.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.
Andrew Finch, Young-Sook Hwang, and Eiichiro
Sumita. 2005. Using Machine Translation Evalua-
tion Techniques to Determine Sentence-level Semantic
Equivalence. In Proceedings of IWP, pages 17-24.
</reference>
<page confidence="0.812189">
1028
</page>
<reference confidence="0.999876617647059">
Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian Hu,
Kam-Fai Wong, and Hsiao-Wuen Hon. 2007. Cross-
Lingual Query Suggestion Using Query Logs of Dif-
ferent Languages. In Proceedings of SIGIR, pages
463-470.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgu`ere. 1991. Lexical Selection and Paraphrase in
a Meaning-Text Generation Model. In Natural Lan-
guage Generation in Artificial Intelligence and Com-
putational Linguistics, pages 293-312.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of HLT-
NAACL, pages 455-462.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models: User Manual and Description for Version
1.2.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT-NAACL, pages 127-133.
De-Kang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of COLING/ACL,
pages 768-774.
De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natural
Language Engineering 7(4): 343-360.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using Paraphrases for Parame-
ter Tuning in Statistical Machine Translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 120-127.
Kathleen R. Mckeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and Summarizing News on
a Daily Basis with Columbia’s Newsblaster. In Pro-
ceedings of HLT, pages 280-285.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of ACL,
pages 440-447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive Training and Maximum Entropy Models for Sta-
tistical Machine Translation. In Proceedings of ACL,
pages 295-302.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences.
In Proceedings of HLT-NAACL, pages 102-109.
Marius Pasca and P´eter Dienes. 2005. Aligning Nee-
dles in a Haystack: Paraphrase Acquisition Across the
Web. In Proceedings of IJCNLP, pages 119-130.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 1992. Numerical Recipes
in C: The Art of Scientific Computing. Cambridge
University Press, Cambridge, U.K., 1992, 412-420.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering
System. In Proceedings of ACL, pages 41-47.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic Paraphrase Acquisition from News
Articles. In Proceedings of HLT, pages 40-46.
Hua Wu and Ming Zhou. 2003. Synonymous Collo-
cation Extraction Using Translation Information. In
Proceedings ofACL, pages 120-127.
</reference>
<page confidence="0.99623">
1029
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.305563">
<title confidence="0.999914">Multiple Resources to Improve SMT-based Paraphrasing</title>
<author confidence="0.983811">Cheng Ming Ting Sheng</author>
<affiliation confidence="0.989728">Institute of Technology, Harbin, China</affiliation>
<email confidence="0.698129">zhaosq@ir.hit.edu.cn</email>
<email confidence="0.698129">tliu@ir.hit.edu.cn</email>
<email confidence="0.698129">lisheng@ir.hit.edu.cn</email>
<note confidence="0.363247">Research Asia, Beijing, China</note>
<abstract confidence="0.998379882352941">This paper proposes a novel method that exploits multiple resources to improve statistical machine translation (SMT) based paraphrasing. In detail, a phrasal paraphrase table and a feature function are derived from each resource, which are then combined in a log-linear SMT model for sentence-level paraphrase generation. Experimental results show that the SMT-based paraphrasing model can be enhanced using multiple resources. The phrase-level and sentence-level precision of the generated paraphrases are above 60% and 55%, respectively. In addition, the contribution of each resource is evaluated, which indicates that all the exploited resources are useful for generating paraphrases of high quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with Bilingual Parallel Corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>597--604</pages>
<contexts>
<context position="7049" citStr="Bannard and Callison-Burch (2005)" startWordPosition="1050" endWordPosition="1053"> paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora. The basic idea is that if two phrases are aligned to the same translation in a foreign language, they may be paraphrases. This method has been demonstrated effective in extracting large volume of phrasal paraphrases. Besides, Wu and Zhou (2003) exploited bilingual corpora and translation information in learning synonymous collocations. In addition, some researchers extracted paraphrases from the web. For example, Ravichandran and Hovy (2002) retrieved paraphrase patterns from the web using hand-crafted queries. Pasca and Dienes (20</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. In Proceedings of ACL, pages 597-604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Using Lexical Chains for Text Summarization.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>10--17</pages>
<contexts>
<context position="5462" citStr="Barzilay and Elhadad, 1997" startWordPosition="812" endWordPosition="815"> patterns by hand. In MDS, McKeown et al. (2002) identified paraphrase sentences across documents before generating summarizations. In NLG, Iordanskaja et al. (1991) used paraphrases to generate more varied and fluent texts. Previous work has examined various resources for acquiring paraphrases, including thesauri, monolingual corpora, bilingual corpora, and the web. Thesauri, such as WordNet, have been widely used for extracting paraphrases. Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). Besides, the automatically constructed thesauri can also be used. Lin (1998) constructed a thesaurus by automatically clustering words based on context similarity. Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. They exploited a corpus of multiple English translations of the same source text written in a foreign language, from which phrases in aligned sentences that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase ide</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>Regina Barzilay and Michael Elhadad. 1997. Using Lexical Chains for Text Summarization. In Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization, pages 10-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to Paraphrase: An Unsupervised Approach Using MultipleSequence Alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="6490" citStr="Barzilay and Lee, 2003" startWordPosition="957" endWordPosition="960">es that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphrase extraction. Different news articles reporting on the same event are commonly used as monolingual comparable corpora, from which both paraphrase patterns and phrasal paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using biling</context>
<context position="8118" citStr="Barzilay and Lee (2003)" startWordPosition="1212" endWordPosition="1215">hrases from the web. For example, Ravichandran and Hovy (2002) retrieved paraphrase patterns from the web using hand-crafted queries. Pasca and Dienes (2005) extracted sentence fragments occurring in identical contexts as paraphrases from one billion web documents. Since web mining is rather time consuming, we do not exploit the web to extract paraphrases in this paper. So far, two kinds of methods have been proposed for sentence-level paraphrase generation, i.e., the pattern-based and SMT-based methods. Automatically learned patterns have been used in para1022 phrase generation. For example, Barzilay and Lee (2003) applied multiple-sequence alignment (MSA) to parallel news sentences and induced paraphrasing patterns for generating new sentences. Pang et al. (2003) built finite state automata (FSA) from semantically equivalent translation sets based on syntactic alignment and used the FSAs in paraphrase generation. The pattern-based methods can generate complex paraphrases that usually involve syntactic variation. However, the methods were demonstrated to be of limited generality (Quirk et al., 2004). Quirk et al. (2004) first recast paraphrase generation as monolingual SMT. They generated paraphrases us</context>
<context position="13160" citStr="Barzilay and Lee, 2003" startWordPosition="2061" endWordPosition="2064">arity, which is the word overlapping rate of p1 and p2. Sim.ntext(p1, p2) is the context similarity, which is the word overlapping rate of the contexts of p1 and p26. If the similarity of p1 and p2 exceeds a threshold Th1, they are identified as paraphrases. We extract 18,698 pairs of phrasal paraphrases from this resource. The paraphrasing score Score2(p1, p2) is defined as the similarity in Equation (6). For the paraphrases occurring more than once, we use their maximum similarity as the paraphrasing score. 3 - Monolingual comparable corpus: Similar to the methods in (Shinyama et al., 2002; Barzilay and Lee, 2003), we construct a corpus of comparable documents from a large corpus D of news articles. The corpus D contains 612,549 news articles. Given articles d1 and d2 from D, if their publication date interval is less than 2 days and their similarity7 exceeds a threshold Th2, they are recognized as comparable documents. In this way, a corpus containing 5,672,864 pairs of comparable documents is constructed. From the comparable corpus, parallel sentences are extracted. Let s1 and s2 be two sentences from comparable documents d1 and d2, if their similarity based on word overlapping rate is above a thresh</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to Paraphrase: An Unsupervised Approach Using MultipleSequence Alignment. In Proceedings of HLT-NAACL, pages 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting Paraphrases from a Parallel Corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="5655" citStr="Barzilay and McKeown (2001)" startWordPosition="838" endWordPosition="841">e more varied and fluent texts. Previous work has examined various resources for acquiring paraphrases, including thesauri, monolingual corpora, bilingual corpora, and the web. Thesauri, such as WordNet, have been widely used for extracting paraphrases. Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). Besides, the automatically constructed thesauri can also be used. Lin (1998) constructed a thesaurus by automatically clustering words based on context similarity. Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. They exploited a corpus of multiple English translations of the same source text written in a foreign language, from which phrases in aligned sentences that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphrase ext</context>
<context position="11914" citStr="Barzilay and McKeown (2001)" startWordPosition="1853" endWordPosition="1857">lar words as paraphrases. In this way, we extract 502,305 pairs of paraphrases. The paraphrasing score Scores(ps, p2) used in Equation (3) is defined as the similarity based on Equation (5). 2If none of the phrase substitutes from S to T is from PTi (i.e., Ki = 0), we cannot compute hTM i(T, S) as in Equation (3). In this case, we assign hTM i(T, S) a minimum value. 3The decoder used here is a re-implementation of Pharaoh. 4http://www.cs.ualberta.ca/ lindek/downloads.htm. T* = arg max{P(T|S)} T = arg max T {P(S|T)P(T)} (1) T* = arg max { T N i=1 1023 2 - Monolingual parallel corpus: Following Barzilay and McKeown (2001), we exploit a corpus of multiple English translations of foreign novels, which contains 25,804 parallel sentence pairs. We find that most paraphrases extracted using the method of Barzilay and McKeown (2001) are quite short. Thus we employ a new approach for paraphrase extraction. Specifically, we parse the sentences with CollinsParser5 and extract the chunks from the parsing results. Let S1 and S2 be a pair of parallel sentences, p1 and p2 two chunks from S1 and S2, we compute the similarity of p1 and p2 as: Sim(p1,p2) = αSimeontent(p1,p2)+ (1 − α)Simeontext(p1,p2) (6) where, Sim.ntent(p1, p</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2001. Extracting Paraphrases from a Parallel Corpus. In Proceedings of ACL, pages 50-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>In Computational Linguistics</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="9325" citStr="Brown et al. (1993)" startWordPosition="1394" endWordPosition="1397">raphrases using a SMT system trained on parallel sentences extracted from clustered news articles. In addition, Madnani et al. (2007) also generated sentence-level paraphrases based on a SMT model. The advantage of the SMT-based method is that it achieves better coverage than the pattern-based method. The main difference between their methods and ours is that they only used bilingual parallel corpora as paraphrase resource, while we exploit and combine multiple resources. 3 SMT-based Paraphrasing Model The SMT-based paraphrasing model used by Quirk et al. (2004) was the noisy channel model of Brown et al. (1993), which identified the optimal paraphrase T* of a sentence S by finding: In contrast, we adopt a log-linear model (Och and Ney, 2002) in this work, since multiple paraphrase tables can be easily combined in the loglinear model. Specifically, feature functions are derived from each paraphrase resource and then combined with the language model feature1: ATM ihTM i(T, S)+ ALMhLM(T, S)} (2) where N is the number of paraphrase tables. hTM i(T, S) is the feature function based on the ith paraphrase table PTi. hLM(T, S) is the language 1The reordering model is not considered in our model. model featu</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. In Computational Linguistics 19(2): 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved Statistical Machine Translation Using Paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="4397" citStr="Callison-Burch et al. (2006)" startWordPosition="653" endWordPosition="656">f the method varies greatly on different test sets and it performs best on the test set of news sentences, which are from the same source as most of the training data. The rest of the paper is organized as follows: Section 2 reviews related work. Section 3 introduces the log-linear model for paraphrase generation. Section 4 describes the phrasal paraphrase extraction from different resources. Section 5 presents the parameter estimation method. Section 6 shows the experiments and results. Section 7 draws the conclusion. 2 Related Work Paraphrases have been used in many NLP applications. In MT, Callison-Burch et al. (2006) utilized paraphrases of unseen source phrases to alleviate data sparseness. Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. In QA, Lin and Pantel (2001) and Ravichandran and Hovy (2002) paraphrased the answer patterns to enhance the recall of answer extraction. In IE, Shinyama et al. (2002) automatically learned paraphrases of IE patterns to reduce the cost of creating IE patterns by hand. In MDS, McKeown et al. (2002) identified paraphrase sentences across documents before generating summarizations. In NLG, Iordanskaja et al. (19</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved Statistical Machine Translation Using Paraphrases. In Proceedings of HLTNAACL, pages 17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Young-Sook Hwang</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Using Machine Translation Evaluation Techniques to Determine Sentence-level Semantic Equivalence.</title>
<date>2005</date>
<booktitle>In Proceedings of IWP,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="5966" citStr="Finch et al. (2005)" startWordPosition="884" endWordPosition="887">zilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). Besides, the automatically constructed thesauri can also be used. Lin (1998) constructed a thesaurus by automatically clustering words based on context similarity. Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. They exploited a corpus of multiple English translations of the same source text written in a foreign language, from which phrases in aligned sentences that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphrase extraction. Different news articles reporting on the same event are commonly used as monolingual comparable corpora, from which both paraphrase patterns and phrasal paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a pars</context>
</contexts>
<marker>Finch, Hwang, Sumita, 2005</marker>
<rawString>Andrew Finch, Young-Sook Hwang, and Eiichiro Sumita. 2005. Using Machine Translation Evaluation Techniques to Determine Sentence-level Semantic Equivalence. In Proceedings of IWP, pages 17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Gao</author>
<author>Cheng Niu</author>
<author>Jian-Yun Nie</author>
<author>Ming Zhou</author>
<author>Jian Hu</author>
<author>Kam-Fai Wong</author>
<author>Hsiao-Wuen Hon</author>
</authors>
<title>CrossLingual Query Suggestion Using Query Logs of Different Languages.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>463--470</pages>
<contexts>
<context position="16864" citStr="Gao et al., 2007" startWordPosition="2677" endWordPosition="2680">s are usually inflected. Hence, we expand the word - definition pairs by providing the inflected forms. Here we use an inflection list and some rules for inflection. After expanding, 159,456 pairs of phrasal paraphrases are extracted. Let &lt; p1, p2 &gt; be a word - definition pair, the paraphrasing score is defined according to the rank of p2 in all of p1’s definitions: Score5(p1,p2) = γi−1 (9) where γ is a constant (we empirically set γ = 0.9) and i is the rank of p2 in p1’s definitions. 6 - Similar user queries: Clusters of similar user queries have been used for query expansion and suggestion (Gao et al., 2007). Since most queries are at the phrase level, we exploit similar user queries as phrasal paraphrases. In our experiment, we use the corpus of clustered similar MSN queries constructed by Gao et al. (2007). The similarity of two queries p1 and p2 is computed as: Sim(p1,p2) = βSimcontent(p1,p2)+ (1 − β)Simc�ic*−through(p1,p2) (10) where Simcontent(p1,p2) is the content similarity, which is computed as the word overlapping rate of p1 and p2. Simclick−through(p1,p2) is the click through similarity, which is the overlapping rate of the user clicked documents for p1 and p2. For each query q, we keep</context>
</contexts>
<marker>Gao, Niu, Nie, Zhou, Hu, Wong, Hon, 2007</marker>
<rawString>Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian Hu, Kam-Fai Wong, and Hsiao-Wuen Hon. 2007. CrossLingual Query Suggestion Using Query Logs of Different Languages. In Proceedings of SIGIR, pages 463-470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidija Iordanskaja</author>
<author>Richard Kittredge</author>
<author>Alain Polgu`ere</author>
</authors>
<title>Lexical Selection and Paraphrase in a Meaning-Text Generation Model.</title>
<date>1991</date>
<booktitle>In Natural Language Generation in Artificial Intelligence and Computational Linguistics,</booktitle>
<pages>293--312</pages>
<marker>Iordanskaja, Kittredge, Polgu`ere, 1991</marker>
<rawString>Lidija Iordanskaja, Richard Kittredge, and Alain Polgu`ere. 1991. Lexical Selection and Paraphrase in a Meaning-Text Generation Model. In Natural Language Generation in Artificial Intelligence and Computational Linguistics, pages 293-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for Automatic Evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<pages>455--462</pages>
<contexts>
<context position="4501" citStr="Kauchak and Barzilay (2006)" startWordPosition="667" endWordPosition="670">, which are from the same source as most of the training data. The rest of the paper is organized as follows: Section 2 reviews related work. Section 3 introduces the log-linear model for paraphrase generation. Section 4 describes the phrasal paraphrase extraction from different resources. Section 5 presents the parameter estimation method. Section 6 shows the experiments and results. Section 7 draws the conclusion. 2 Related Work Paraphrases have been used in many NLP applications. In MT, Callison-Burch et al. (2006) utilized paraphrases of unseen source phrases to alleviate data sparseness. Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. In QA, Lin and Pantel (2001) and Ravichandran and Hovy (2002) paraphrased the answer patterns to enhance the recall of answer extraction. In IE, Shinyama et al. (2002) automatically learned paraphrases of IE patterns to reduce the cost of creating IE patterns by hand. In MDS, McKeown et al. (2002) identified paraphrase sentences across documents before generating summarizations. In NLG, Iordanskaja et al. (1991) used paraphrases to generate more varied and fluent texts. Previous work has examined various resour</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for Automatic Evaluation. In Proceedings of HLTNAACL, pages 455-462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models: User Manual and Description for Version 1.2.</title>
<date>2004</date>
<contexts>
<context position="10541" citStr="Koehn, 2004" startWordPosition="1616" endWordPosition="1617">ATM i and ALM are the weights of the feature functions. hTM i(T, S) is defined as: K; hTM i(T, S) = log H Scorei(Tk, Sk) (3) k=1 where Ki is the number of phrase substitutes from S to T based on PTi. Tk in T and Sk in S are phrasal paraphrases in PTi. Scorei(Tk, Sk) is the paraphrase likelihood according to PTi2. A 5-gram language model is used, therefore: hLM(T, S) = log �� p(tj|tj−4, ..., tj−1) (4) j=1 where J is the length of T, tj is the j-th word of T. 4 Exploiting Multiple Resources This section describes the extraction of phrasal paraphrases using various resources. Similar to Pharaoh (Koehn, 2004), our decoder3 uses top 20 paraphrase options for each input phrase in the default setting. Therefore, we keep at most 20 paraphrases for a phrase when extracting phrasal paraphrases using each resource. 1 - Thesaurus: The thesaurus4 used in this work was automatically constructed by Lin (1998). The similarity of two words es and e2 was calculated through the surrounding context words that have dependency relations with the investigated words: Sim(e1, e2) = E(r,e)ETr(e1)nTr(e2)(I(e1, r, e) + I(e2, r, e)) E(r,e)ETr(e1) I(e1, r, e) + E(r,e)ETr(e2) I(e2, r, e) (5) where Tr(ei) denotes the set of </context>
<context position="14261" citStr="Koehn, 2004" startWordPosition="2252" endWordPosition="2253">ences from comparable documents d1 and d2, if their similarity based on word overlapping rate is above a threshold Th3, s1 and s2 are identified as parallel sentences. In this way, 872,330 parallel sentence pairs are extracted. 5http://people.csail.mit.edu/mcollins/code.html 6The context of a chunk is made up of 6 words around the chunk, 3 to the left and 3 to the right. 7The similarity of two documents is computed using the vector space model and the word weights are based on tf·idf. We run Giza++ (Och and Ney, 2000) on the parallel sentences and then extract aligned phrases as described in (Koehn, 2004). The generated paraphrase table is pruned by keeping the top 20 paraphrases for each phrase. After pruning, 100,621 pairs of paraphrases are extracted. Given phrase p1 and its paraphrase p2, we compute Score3(p1, p2) by relative frequency (Koehn et al., 2003): Score3(p1, p2) = p(p2Jp1) = count(p2, p1) � count(p�, p1) (7) People may wonder why we do not use the same method on the monolingual parallel and comparable corpora. This is mainly because the volumes of the two corpora differ a lot. In detail, the monolingual parallel corpus is fairly small, thus automatical word alignment tool like Gi</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models: User Manual and Description for Version 1.2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="14521" citStr="Koehn et al., 2003" startWordPosition="2293" endWordPosition="2296">edu/mcollins/code.html 6The context of a chunk is made up of 6 words around the chunk, 3 to the left and 3 to the right. 7The similarity of two documents is computed using the vector space model and the word weights are based on tf·idf. We run Giza++ (Och and Ney, 2000) on the parallel sentences and then extract aligned phrases as described in (Koehn, 2004). The generated paraphrase table is pruned by keeping the top 20 paraphrases for each phrase. After pruning, 100,621 pairs of paraphrases are extracted. Given phrase p1 and its paraphrase p2, we compute Score3(p1, p2) by relative frequency (Koehn et al., 2003): Score3(p1, p2) = p(p2Jp1) = count(p2, p1) � count(p�, p1) (7) People may wonder why we do not use the same method on the monolingual parallel and comparable corpora. This is mainly because the volumes of the two corpora differ a lot. In detail, the monolingual parallel corpus is fairly small, thus automatical word alignment tool like Giza++ may not work well on it. In contrast, the monolingual comparable corpus is quite large, hence we cannot conduct the timeconsuming syntactic parsing on it as we do on the monolingual parallel corpus. 4 - Bilingual phrase table: We first construct a bilingu</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of HLT-NAACL, pages 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>De-Kang Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="5540" citStr="Lin (1998)" startWordPosition="825" endWordPosition="826">ts before generating summarizations. In NLG, Iordanskaja et al. (1991) used paraphrases to generate more varied and fluent texts. Previous work has examined various resources for acquiring paraphrases, including thesauri, monolingual corpora, bilingual corpora, and the web. Thesauri, such as WordNet, have been widely used for extracting paraphrases. Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). Besides, the automatically constructed thesauri can also be used. Lin (1998) constructed a thesaurus by automatically clustering words based on context similarity. Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. They exploited a corpus of multiple English translations of the same source text written in a foreign language, from which phrases in aligned sentences that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in</context>
<context position="10836" citStr="Lin (1998)" startWordPosition="1666" endWordPosition="1667">ccording to PTi2. A 5-gram language model is used, therefore: hLM(T, S) = log �� p(tj|tj−4, ..., tj−1) (4) j=1 where J is the length of T, tj is the j-th word of T. 4 Exploiting Multiple Resources This section describes the extraction of phrasal paraphrases using various resources. Similar to Pharaoh (Koehn, 2004), our decoder3 uses top 20 paraphrase options for each input phrase in the default setting. Therefore, we keep at most 20 paraphrases for a phrase when extracting phrasal paraphrases using each resource. 1 - Thesaurus: The thesaurus4 used in this work was automatically constructed by Lin (1998). The similarity of two words es and e2 was calculated through the surrounding context words that have dependency relations with the investigated words: Sim(e1, e2) = E(r,e)ETr(e1)nTr(e2)(I(e1, r, e) + I(e2, r, e)) E(r,e)ETr(e1) I(e1, r, e) + E(r,e)ETr(e2) I(e2, r, e) (5) where Tr(ei) denotes the set of words that have dependency relation r with word ei. I(ei, r, e) is the mutual information between ei, r and e. For each word, we keep 20 most similar words as paraphrases. In this way, we extract 502,305 pairs of paraphrases. The paraphrasing score Scores(ps, p2) used in Equation (3) is defined</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>De-Kang Lin. 1998. Automatic Retrieval and Clustering of Similar Words. In Proceedings of COLING/ACL, pages 768-774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>De-Kang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of Inference Rules for Question Answering.</title>
<date>2001</date>
<journal>In Natural Language Engineering</journal>
<volume>7</volume>
<issue>4</issue>
<pages>343--360</pages>
<contexts>
<context position="4613" citStr="Lin and Pantel (2001)" startWordPosition="684" endWordPosition="687">2 reviews related work. Section 3 introduces the log-linear model for paraphrase generation. Section 4 describes the phrasal paraphrase extraction from different resources. Section 5 presents the parameter estimation method. Section 6 shows the experiments and results. Section 7 draws the conclusion. 2 Related Work Paraphrases have been used in many NLP applications. In MT, Callison-Burch et al. (2006) utilized paraphrases of unseen source phrases to alleviate data sparseness. Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. In QA, Lin and Pantel (2001) and Ravichandran and Hovy (2002) paraphrased the answer patterns to enhance the recall of answer extraction. In IE, Shinyama et al. (2002) automatically learned paraphrases of IE patterns to reduce the cost of creating IE patterns by hand. In MDS, McKeown et al. (2002) identified paraphrase sentences across documents before generating summarizations. In NLG, Iordanskaja et al. (1991) used paraphrases to generate more varied and fluent texts. Previous work has examined various resources for acquiring paraphrases, including thesauri, monolingual corpora, bilingual corpora, and the web. Thesauri</context>
<context position="6534" citStr="Lin and Pantel (2001)" startWordPosition="965" endWordPosition="968">cted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphrase extraction. Different news articles reporting on the same event are commonly used as monolingual comparable corpora, from which both paraphrase patterns and phrasal paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora. The basic idea is that</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>De-Kang Lin and Patrick Pantel. 2001. Discovery of Inference Rules for Question Answering. In Natural Language Engineering 7(4): 343-360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Necip Fazil Ayan</author>
<author>Philip Resnik</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Using Paraphrases for Parameter Tuning in Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="8839" citStr="Madnani et al. (2007)" startWordPosition="1318" endWordPosition="1321">rns for generating new sentences. Pang et al. (2003) built finite state automata (FSA) from semantically equivalent translation sets based on syntactic alignment and used the FSAs in paraphrase generation. The pattern-based methods can generate complex paraphrases that usually involve syntactic variation. However, the methods were demonstrated to be of limited generality (Quirk et al., 2004). Quirk et al. (2004) first recast paraphrase generation as monolingual SMT. They generated paraphrases using a SMT system trained on parallel sentences extracted from clustered news articles. In addition, Madnani et al. (2007) also generated sentence-level paraphrases based on a SMT model. The advantage of the SMT-based method is that it achieves better coverage than the pattern-based method. The main difference between their methods and ours is that they only used bilingual parallel corpora as paraphrase resource, while we exploit and combine multiple resources. 3 SMT-based Paraphrasing Model The SMT-based paraphrasing model used by Quirk et al. (2004) was the noisy channel model of Brown et al. (1993), which identified the optimal paraphrase T* of a sentence S by finding: In contrast, we adopt a log-linear model </context>
</contexts>
<marker>Madnani, Ayan, Resnik, Dorr, 2007</marker>
<rawString>Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and Bonnie J. Dorr. 2007. Using Paraphrases for Parameter Tuning in Statistical Machine Translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 120-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R Mckeown</author>
<author>Regina Barzilay</author>
<author>David Evans</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Ani Nenkova</author>
<author>Carl Sable</author>
<author>Barry Schiffman</author>
<author>Sergey Sigelman</author>
</authors>
<title>Tracking and Summarizing News on a Daily Basis with Columbia’s Newsblaster.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT,</booktitle>
<pages>280--285</pages>
<marker>Mckeown, Barzilay, Evans, Hatzivassiloglou, Klavans, Nenkova, Sable, Schiffman, Sigelman, 2002</marker>
<rawString>Kathleen R. Mckeown, Regina Barzilay, David Evans, Vasileios Hatzivassiloglou, Judith L. Klavans, Ani Nenkova, Carl Sable, Barry Schiffman, and Sergey Sigelman. 2002. Tracking and Summarizing News on a Daily Basis with Columbia’s Newsblaster. In Proceedings of HLT, pages 280-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="14172" citStr="Och and Ney, 2000" startWordPosition="2234" endWordPosition="2237">ructed. From the comparable corpus, parallel sentences are extracted. Let s1 and s2 be two sentences from comparable documents d1 and d2, if their similarity based on word overlapping rate is above a threshold Th3, s1 and s2 are identified as parallel sentences. In this way, 872,330 parallel sentence pairs are extracted. 5http://people.csail.mit.edu/mcollins/code.html 6The context of a chunk is made up of 6 words around the chunk, 3 to the left and 3 to the right. 7The similarity of two documents is computed using the vector space model and the word weights are based on tf·idf. We run Giza++ (Och and Ney, 2000) on the parallel sentences and then extract aligned phrases as described in (Koehn, 2004). The generated paraphrase table is pruned by keeping the top 20 paraphrases for each phrase. After pruning, 100,621 pairs of paraphrases are extracted. Given phrase p1 and its paraphrase p2, we compute Score3(p1, p2) by relative frequency (Koehn et al., 2003): Score3(p1, p2) = p(p2Jp1) = count(p2, p1) � count(p�, p1) (7) People may wonder why we do not use the same method on the monolingual parallel and comparable corpora. This is mainly because the volumes of the two corpora differ a lot. In detail, the </context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proceedings of ACL, pages 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="9458" citStr="Och and Ney, 2002" startWordPosition="1417" endWordPosition="1420">also generated sentence-level paraphrases based on a SMT model. The advantage of the SMT-based method is that it achieves better coverage than the pattern-based method. The main difference between their methods and ours is that they only used bilingual parallel corpora as paraphrase resource, while we exploit and combine multiple resources. 3 SMT-based Paraphrasing Model The SMT-based paraphrasing model used by Quirk et al. (2004) was the noisy channel model of Brown et al. (1993), which identified the optimal paraphrase T* of a sentence S by finding: In contrast, we adopt a log-linear model (Och and Ney, 2002) in this work, since multiple paraphrase tables can be easily combined in the loglinear model. Specifically, feature functions are derived from each paraphrase resource and then combined with the language model feature1: ATM ihTM i(T, S)+ ALMhLM(T, S)} (2) where N is the number of paraphrase tables. hTM i(T, S) is the feature function based on the ith paraphrase table PTi. hLM(T, S) is the language 1The reordering model is not considered in our model. model feature. ATM i and ALM are the weights of the feature functions. hTM i(T, S) is defined as: K; hTM i(T, S) = log H Scorei(Tk, Sk) (3) k=1 </context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In Proceedings of ACL, pages 295-302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Syntax-based Alignment of Multiple Translations: Extracting Paraphrases and Generating New Sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>102--109</pages>
<contexts>
<context position="8270" citStr="Pang et al. (2003)" startWordPosition="1233" endWordPosition="1236"> extracted sentence fragments occurring in identical contexts as paraphrases from one billion web documents. Since web mining is rather time consuming, we do not exploit the web to extract paraphrases in this paper. So far, two kinds of methods have been proposed for sentence-level paraphrase generation, i.e., the pattern-based and SMT-based methods. Automatically learned patterns have been used in para1022 phrase generation. For example, Barzilay and Lee (2003) applied multiple-sequence alignment (MSA) to parallel news sentences and induced paraphrasing patterns for generating new sentences. Pang et al. (2003) built finite state automata (FSA) from semantically equivalent translation sets based on syntactic alignment and used the FSAs in paraphrase generation. The pattern-based methods can generate complex paraphrases that usually involve syntactic variation. However, the methods were demonstrated to be of limited generality (Quirk et al., 2004). Quirk et al. (2004) first recast paraphrase generation as monolingual SMT. They generated paraphrases using a SMT system trained on parallel sentences extracted from clustered news articles. In addition, Madnani et al. (2007) also generated sentence-level </context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-based Alignment of Multiple Translations: Extracting Paraphrases and Generating New Sentences. In Proceedings of HLT-NAACL, pages 102-109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
<author>P´eter Dienes</author>
</authors>
<title>Aligning Needles in a Haystack: Paraphrase Acquisition Across the Web. In</title>
<date>2005</date>
<booktitle>Proceedings of IJCNLP,</booktitle>
<pages>119--130</pages>
<contexts>
<context position="7652" citStr="Pasca and Dienes (2005)" startWordPosition="1138" endWordPosition="1142">allison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora. The basic idea is that if two phrases are aligned to the same translation in a foreign language, they may be paraphrases. This method has been demonstrated effective in extracting large volume of phrasal paraphrases. Besides, Wu and Zhou (2003) exploited bilingual corpora and translation information in learning synonymous collocations. In addition, some researchers extracted paraphrases from the web. For example, Ravichandran and Hovy (2002) retrieved paraphrase patterns from the web using hand-crafted queries. Pasca and Dienes (2005) extracted sentence fragments occurring in identical contexts as paraphrases from one billion web documents. Since web mining is rather time consuming, we do not exploit the web to extract paraphrases in this paper. So far, two kinds of methods have been proposed for sentence-level paraphrase generation, i.e., the pattern-based and SMT-based methods. Automatically learned patterns have been used in para1022 phrase generation. For example, Barzilay and Lee (2003) applied multiple-sequence alignment (MSA) to parallel news sentences and induced paraphrasing patterns for generating new sentences. </context>
</contexts>
<marker>Pasca, Dienes, 2005</marker>
<rawString>Marius Pasca and P´eter Dienes. 2005. Aligning Needles in a Haystack: Paraphrase Acquisition Across the Web. In Proceedings of IJCNLP, pages 119-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
<author>Brian P Flannery</author>
</authors>
<title>Numerical Recipes in C: The Art of Scientific Computing.</title>
<date>1992</date>
<pages>412--420</pages>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, U.K.,</location>
<contexts>
<context position="20831" citStr="Press et al., 1992" startWordPosition="3346" endWordPosition="3349">do not describe the estimation of them due to space limitation. 10Paraphrasing a word to itself (based on the 7-th paraphrase table above) is not regarded as a substitute. n U i=1 PSER(S) = k PSo(Ti)k/k n U i=1 1025 substituting the source phrase with the target phrase remains grammatical. In decoding, the phrase substitutes are printed out and then the PSER is computed based on the labeled data. Using each set of parameters, we generate paraphrases for the sentences in the development set based on Equation (2). PSER is then computed as in Equation (13). We use the gradient descent algorithm (Press et al., 1992) to minimize PSER on the development set and get the optimal parameters. 6 Experiments To evaluate the performance of the method on different types of test data, we used three kinds of sentences for testing, which were randomly extracted from Google news, free online novels, and forums, respectively. For each type, 50 sentences were extracted as test data and another 25 were extracted as development data. For each test sentence, top 10 of the generated paraphrases were kept for evaluation. 6.1 Phrase-level Evaluation The phrase-level evaluation was carried out to investigate the contributions </context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 1992</marker>
<rawString>William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 1992. Numerical Recipes in C: The Art of Scientific Computing. Cambridge University Press, Cambridge, U.K., 1992, 412-420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William Dolan</author>
</authors>
<title>Monolingual Machine Translation for Paraphrase Generation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>142--149</pages>
<contexts>
<context position="1795" citStr="Quirk et al., 2004" startWordPosition="253" endWordPosition="256"> translation (MT), question answering (QA), information extraction (IE), multidocument summarization (MDS), and natural language generation (NLG). This paper addresses the problem of sentencelevel paraphrase generation, which aims at generating paraphrases for input sentences. An example of sentence-level paraphrases can be seen below: S1: The table was set up in the carriage shed. S2: The table was laid under the cart-shed. *This research was finished while the first author worked as an intern in Microsoft Research Asia. Paraphrase generation can be viewed as monolingual machine translation (Quirk et al., 2004), which typically includes a translation model and a language model. The translation model can be trained using monolingual parallel corpora. However, acquiring such corpora is not easy. Hence, data sparseness is a key problem for the SMT-based paraphrasing. On the other hand, various methods have been presented to extract phrasal paraphrases from different resources, which include thesauri, monolingual corpora, bilingual corpora, and the web. However, little work has been focused on using the extracted phrasal paraphrases in sentence-level paraphrase generation. In this paper, we exploit mult</context>
<context position="6511" citStr="Quirk et al., 2004" startWordPosition="961" endWordPosition="964">r contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphrase extraction. Different news articles reporting on the same event are commonly used as monolingual comparable corpora, from which both paraphrase patterns and phrasal paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora.</context>
<context position="8612" citStr="Quirk et al., 2004" startWordPosition="1282" endWordPosition="1285">ed methods. Automatically learned patterns have been used in para1022 phrase generation. For example, Barzilay and Lee (2003) applied multiple-sequence alignment (MSA) to parallel news sentences and induced paraphrasing patterns for generating new sentences. Pang et al. (2003) built finite state automata (FSA) from semantically equivalent translation sets based on syntactic alignment and used the FSAs in paraphrase generation. The pattern-based methods can generate complex paraphrases that usually involve syntactic variation. However, the methods were demonstrated to be of limited generality (Quirk et al., 2004). Quirk et al. (2004) first recast paraphrase generation as monolingual SMT. They generated paraphrases using a SMT system trained on parallel sentences extracted from clustered news articles. In addition, Madnani et al. (2007) also generated sentence-level paraphrases based on a SMT model. The advantage of the SMT-based method is that it achieves better coverage than the pattern-based method. The main difference between their methods and ours is that they only used bilingual parallel corpora as paraphrase resource, while we exploit and combine multiple resources. 3 SMT-based Paraphrasing Mode</context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and William Dolan. 2004. Monolingual Machine Translation for Paraphrase Generation. In Proceedings of EMNLP, pages 142-149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning Surface Text Patterns for a Question Answering System. In</title>
<date>2002</date>
<booktitle>Proceedings of ACL,</booktitle>
<pages>41--47</pages>
<contexts>
<context position="4646" citStr="Ravichandran and Hovy (2002)" startWordPosition="689" endWordPosition="692">ction 3 introduces the log-linear model for paraphrase generation. Section 4 describes the phrasal paraphrase extraction from different resources. Section 5 presents the parameter estimation method. Section 6 shows the experiments and results. Section 7 draws the conclusion. 2 Related Work Paraphrases have been used in many NLP applications. In MT, Callison-Burch et al. (2006) utilized paraphrases of unseen source phrases to alleviate data sparseness. Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. In QA, Lin and Pantel (2001) and Ravichandran and Hovy (2002) paraphrased the answer patterns to enhance the recall of answer extraction. In IE, Shinyama et al. (2002) automatically learned paraphrases of IE patterns to reduce the cost of creating IE patterns by hand. In MDS, McKeown et al. (2002) identified paraphrase sentences across documents before generating summarizations. In NLG, Iordanskaja et al. (1991) used paraphrases to generate more varied and fluent texts. Previous work has examined various resources for acquiring paraphrases, including thesauri, monolingual corpora, bilingual corpora, and the web. Thesauri, such as WordNet, have been wide</context>
<context position="7557" citStr="Ravichandran and Hovy (2002)" startWordPosition="1125" endWordPosition="1128">ct paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora. The basic idea is that if two phrases are aligned to the same translation in a foreign language, they may be paraphrases. This method has been demonstrated effective in extracting large volume of phrasal paraphrases. Besides, Wu and Zhou (2003) exploited bilingual corpora and translation information in learning synonymous collocations. In addition, some researchers extracted paraphrases from the web. For example, Ravichandran and Hovy (2002) retrieved paraphrase patterns from the web using hand-crafted queries. Pasca and Dienes (2005) extracted sentence fragments occurring in identical contexts as paraphrases from one billion web documents. Since web mining is rather time consuming, we do not exploit the web to extract paraphrases in this paper. So far, two kinds of methods have been proposed for sentence-level paraphrase generation, i.e., the pattern-based and SMT-based methods. Automatically learned patterns have been used in para1022 phrase generation. For example, Barzilay and Lee (2003) applied multiple-sequence alignment (M</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning Surface Text Patterns for a Question Answering System. In Proceedings of ACL, pages 41-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
<author>Kiyoshi Sudo</author>
</authors>
<title>Automatic Paraphrase Acquisition from News Articles.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT,</booktitle>
<pages>40--46</pages>
<contexts>
<context position="4752" citStr="Shinyama et al. (2002)" startWordPosition="707" endWordPosition="710">traction from different resources. Section 5 presents the parameter estimation method. Section 6 shows the experiments and results. Section 7 draws the conclusion. 2 Related Work Paraphrases have been used in many NLP applications. In MT, Callison-Burch et al. (2006) utilized paraphrases of unseen source phrases to alleviate data sparseness. Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. In QA, Lin and Pantel (2001) and Ravichandran and Hovy (2002) paraphrased the answer patterns to enhance the recall of answer extraction. In IE, Shinyama et al. (2002) automatically learned paraphrases of IE patterns to reduce the cost of creating IE patterns by hand. In MDS, McKeown et al. (2002) identified paraphrase sentences across documents before generating summarizations. In NLG, Iordanskaja et al. (1991) used paraphrases to generate more varied and fluent texts. Previous work has examined various resources for acquiring paraphrases, including thesauri, monolingual corpora, bilingual corpora, and the web. Thesauri, such as WordNet, have been widely used for extracting paraphrases. Some researchers extract synonyms as paraphrases (Kauchak and Barzilay</context>
<context position="6466" citStr="Shinyama et al., 2002" startWordPosition="953" endWordPosition="956">ases in aligned sentences that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphrase extraction. Different news articles reporting on the same event are commonly used as monolingual comparable corpora, from which both paraphrase patterns and phrasal paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal </context>
<context position="13135" citStr="Shinyama et al., 2002" startWordPosition="2057" endWordPosition="2060">2) is the content similarity, which is the word overlapping rate of p1 and p2. Sim.ntext(p1, p2) is the context similarity, which is the word overlapping rate of the contexts of p1 and p26. If the similarity of p1 and p2 exceeds a threshold Th1, they are identified as paraphrases. We extract 18,698 pairs of phrasal paraphrases from this resource. The paraphrasing score Score2(p1, p2) is defined as the similarity in Equation (6). For the paraphrases occurring more than once, we use their maximum similarity as the paraphrasing score. 3 - Monolingual comparable corpus: Similar to the methods in (Shinyama et al., 2002; Barzilay and Lee, 2003), we construct a corpus of comparable documents from a large corpus D of news articles. The corpus D contains 612,549 news articles. Given articles d1 and d2 from D, if their publication date interval is less than 2 days and their similarity7 exceeds a threshold Th2, they are recognized as comparable documents. In this way, a corpus containing 5,672,864 pairs of comparable documents is constructed. From the comparable corpus, parallel sentences are extracted. Let s1 and s2 be two sentences from comparable documents d1 and d2, if their similarity based on word overlappi</context>
</contexts>
<marker>Shinyama, Sekine, Sudo, 2002</marker>
<rawString>Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo. 2002. Automatic Paraphrase Acquisition from News Articles. In Proceedings of HLT, pages 40-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Ming Zhou</author>
</authors>
<title>Synonymous Collocation Extraction Using Translation Information.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="7356" citStr="Wu and Zhou (2003)" startWordPosition="1098" endWordPosition="1101">eanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora. The basic idea is that if two phrases are aligned to the same translation in a foreign language, they may be paraphrases. This method has been demonstrated effective in extracting large volume of phrasal paraphrases. Besides, Wu and Zhou (2003) exploited bilingual corpora and translation information in learning synonymous collocations. In addition, some researchers extracted paraphrases from the web. For example, Ravichandran and Hovy (2002) retrieved paraphrase patterns from the web using hand-crafted queries. Pasca and Dienes (2005) extracted sentence fragments occurring in identical contexts as paraphrases from one billion web documents. Since web mining is rather time consuming, we do not exploit the web to extract paraphrases in this paper. So far, two kinds of methods have been proposed for sentence-level paraphrase generation</context>
</contexts>
<marker>Wu, Zhou, 2003</marker>
<rawString>Hua Wu and Ming Zhou. 2003. Synonymous Collocation Extraction Using Translation Information. In Proceedings ofACL, pages 120-127.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>