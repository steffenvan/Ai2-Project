<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9982645">
Constructing Corpora for the Development
and Evaluation of Paraphrase Systems
</title>
<author confidence="0.999742">
Trevor Cohn*
</author>
<affiliation confidence="0.996689">
University of Edinburgh
</affiliation>
<author confidence="0.96363">
Chris Callison-Burch**
</author>
<affiliation confidence="0.943663">
Johns Hopkins University
</affiliation>
<author confidence="0.976673">
Mirella Lapata†
</author>
<affiliation confidence="0.994451">
University of Edinburgh
</affiliation>
<bodyText confidence="0.832133857142857">
Automatic paraphrasing is an important component in many natural language processing tasks.
In this article we present a new parallel corpus with paraphrase annotations. We adopt a defini-
tion of paraphrase based on word alignments and show that it yields high inter-annotator agree-
ment. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is
appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed
in evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and F1)
and also in developing linguistically rich paraphrase models based on syntactic structure.
</bodyText>
<sectionHeader confidence="0.980089" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999753333333333">
The ability to paraphrase text automatically carries much practical import for many
NLP applications ranging from summarization (Barzilay 2003; Zhou et al. 2006) to
question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine
translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising
that recent years have witnessed increasing interest in the acquisition of paraphrases
from real world corpora. These are most often monolingual corpora containing parallel
translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and
Marcu 2003). Truly bilingual corpora consisting of documents and their translations have
also been used to acquire paraphrases (Bannard and Callison-Burch 2005; Callison-
Burch 2007) as well as comparable corpora such as collections of articles produced
by two different newswire agencies about the same events (Barzilay and Elhadad
2003).
Although paraphrase induction algorithms differ in many respects—for example,
the acquired paraphrases often vary in granularity as they can be lexical (fighting, battle)
or structural (last week’s fighting, the battle last week), and are represented as words or
</bodyText>
<note confidence="0.963932">
* School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: tcohn@inf.ed.ac.uk.
** Center for Speech and Language Processing, Johns Hopkins University, Baltimore, MD, 21218.
</note>
<email confidence="0.932744">
E-mail: ccb@cs.jhu.edu.
</email>
<note confidence="0.9513278">
† School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk.
Submission received: 10 September 2007; revised submission received: 8 February 2008; accepted for
publication: 26 March 2008.
© 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 4
</note>
<bodyText confidence="0.999913866666667">
syntax trees—they all rely on some form of alignment for extracting paraphrase pairs.
In its simplest form, the alignment can range over individual words, as is often done
in machine translation (Quirk, Brockett, and Dolan 2004). In other cases, the alignments
range over entire trees (Pang, Knight, and Marcu 2003) or sentence clusters (Barzilay
and Lee 2003).
The obtained paraphrases are typically evaluated via human judgments. Para-
phrase pairs are presented to judges who are asked to decide whether they are seman-
tically equivalent, that is, whether they can be generally substituted for one another in
the same context without great information loss (Barzilay and Lee 2003; Barzilay and
McKeown 2001; Pang, Knight, and Marcu 2003; Bannard and Callison-Burch 2005). In
some cases the automatically acquired paraphrases are compared against manually gen-
erated ones (Lin and Pantel 2001) or evaluated indirectly by demonstrating performance
increase for a specific application, such as machine translation (Callison-Burch, Koehn,
and Osborne 2006).
Unfortunately, manually evaluating paraphrases in this way has at least three draw-
backs. First, it is infeasible to perform frequent evaluations when assessing incremental
system changes or tuning system parameters. Second, it is difficult to replicate results
presented in previous work because there is no standard corpus, and no standard evalu-
ation methodology. Consequently comparisons across systems are few and far between.
The third drawback concerns the evaluation studies themselves, which primarily focus
on precision. Recall is almost never evaluated directly in the literature. And this is
for a good reason: There is no guarantee that participants will identify the same set
of paraphrases as each other or with a computational model. The problem relates to
the nature of the paraphrasing task, which has so far eluded formal definition (see
the discussion in Barzilay [2003]). Such a definition is not so crucial when assessing
precision, because subjects are asked to rate the paraphrases without actually having to
identify them. However, recall might be measured with respect to some set of “gold-
standard” paraphrases which will have to be collected according to some concrete
definition.
In this article we present a resource that could potentially be used to address
these problems. Specifically, we create a monolingual parallel corpus with human
paraphrase annotations. Our working definition of paraphrase is based on word and
phrase1 alignments between semantically equivalent sentences. Other definitions are
possible, for instance we could have asked our annotators to identify all constituents
that are more or less meaning preserving in our parallel corpus. We chose to work
with alignments for two reasons. First, the notion of alignment appears to be central
in paraphrasing—most existing paraphrase induction algorithms rely on alignments
either implicitly or explicitly for identifying paraphrase units. Secondly, research in
machine translation, where several gold-standard alignment corpora have been created,
shows that word alignments can be identified reliably by annotators (Melamed 1998;
Och and Ney 2000b; Mihalcea and Pedersen 2003; Martin, Mihalcea, and Pedersen 2005).
We therefore create word alignments similar to those observed in machine transla-
tion, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links
between words. Alignment blocks larger than one-to-one are used to specify phrase
correspondences.
</bodyText>
<footnote confidence="0.8357935">
1 Our definition of the term phrase follows the SMT literature. It refers to any contiguous sequence of
words, whether it is a syntactic constituent or not. See Section 2 for details.
</footnote>
<page confidence="0.98994">
598
</page>
<note confidence="0.953529">
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
</note>
<bodyText confidence="0.99969125">
In the following section we explain how our corpus was created and summarize our
annotation guidelines. Section 3 gives the details of an agreement study, demonstrating
that our annotators can identify and align paraphrases reliably. We measure agreement
using alignment overlap measures from the SMT literature, and also introduce a novel
agreement statistic for non-enumerable labeling spaces. Section 4 illustrates how the
corpus can be used in paraphrase research, for example, as a test set for evaluating
the output of automatic systems or as a training set for the development of paraphrase
systems. Discussion of our results concludes the article.
</bodyText>
<sectionHeader confidence="0.590733" genericHeader="method">
2. Corpus Creation and Annotation
</sectionHeader>
<bodyText confidence="0.999662303030303">
Our corpus was compiled from three data sources that have been previously used for
paraphrase induction (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003;
Dolan, Quirk, and Brockett 2004): the Multiple-Translation Chinese (MTC) corpus,
Jules Verne’s Twenty Thousand Leagues Under the Sea novel (Leagues), and the Microsoft
Research (MSR) paraphrase corpus. These are monolingual parallel corpora, aligned at
the sentence level. Both source and target sentences are in English, and express the same
content using different surface forms.
The MTC corpus contains news stories from three sources of journalistic Mandarin
Chinese text.2 These stories were translated into English by 11 translation agencies.
Because the majority of the translators were non-native English speakers, occasionally
translations contain syntactic or grammatical errors and are not entirely fluent. After
inspection, we identified four translators with consistently fluent English, and used
their sentences for our corpus. The Leagues corpus contains two English translations
of the French novel Twenty Thousand Leagues Under the Sea. The corpus was created
by Tagyoung Chung and manually aligned at the paragraph level.3 In order to obtain
sentence level paraphrase pairs, we sampled from the subset of one-to-one sentence
alignments. The MSR corpus was harvested automatically from online news sources.4
The obtained sentence pairs were further submitted to judges who rated them as being
semantically equivalent or not (Dolan, Quirk, and Brockett 2004). We only used seman-
tically equivalent pairs. The sentence pairs were filtered for length (&lt; 50) and length
ratio (&lt; 1 : 9 between the shorter and longer sentence). This was necessary to prune out
incorrectly aligned sentences.
We randomly sampled 300 sentence pairs from each corpus (900 in total). Of these,
300 pairs (100 per corpus) were first annotated by two coders to assess inter-annotator
agreement. The remaining 600 sentence pairs were split into two distinct sets, each
consisting of 300 sentences (100 per corpus), and were annotated by a single coder.
Each coder annotated the same amount of data. In addition, we obtained a trial set
of 50 sentences from the MTC corpus which was used for familiarizing our annotators
with the paraphrase alignment task (this set does not form part of the corpus). In sum,
we obtained paraphrase annotations for 900 sentence pairs, 300 of which are doubly
annotated.
To speed up the annotation process, the data sources were first aligned automati-
cally and then hand-corrected. We used Giza++ (Och and Ney 2003), a publicly available
</bodyText>
<footnote confidence="0.96480275">
2 The corpus is made available by the LDC, Catalog Number LDC2002T01, ISBN 1-58563-217-1.
3 The corpus can be downloaded from http://www.isi.edu/∼knight/.
4 The corpus is available at http://research.microsoft.com/research/downloads/Details/607D14D9-
20CD-47E3-85BC-A2F65CD28042/Details.aspx.
</footnote>
<page confidence="0.986713">
599
</page>
<note confidence="0.295556">
Computational Linguistics Volume 34, Number 4
</note>
<bodyText confidence="0.987938523809524">
implementation of the IBM word alignment models (Brown et al. 1993). Giza++ was
trained on the full 993-sentence MTC part1 corpus5 using all 11 translators and all pair-
ings of English translations as training instances. This resulted in 55 = 11·(11−1)
2 training
pairs per sentence and a total of 54,615 training pairs. In addition, we augmented the
training data with a word-identity lexicon, as proposed by Quirk, Brockett, and Dolan
(2004). This follows standard practice in SMT where entries from a bilingual dictionary
are added to the training set (Och and Ney 2000a), except in our case the “dictionary”
is monolingual and specifies that each word type can be paraphrased as itself. This is
necessary in order to inform Giza++ about word identity.
A common problem with automatic word alignments is that they are asymmetric:
one source word can only be aligned to one target word, whereas one target word can
be aligned to multiple source words. In SMT, word alignments are typically predicted
in both directions: source-to-target and target-to-source. These two alignments are then
merged (symmetrized) to produce the final alignment (Koehn, Och, and Marcu 2003).
Symmetrization improves the alignment quality compared to that of a single directional
model, while also allowing a greater range of alignment types (i.e., some many-to-
one, one-to-many, and many-to-many alignments can be produced). Analogously, we
obtained word alignments in both directions6 which we subsequently merged by taking
their intersection. This resulted in a high precision and low recall alignment.
Our annotators (two linguistics graduates) were given pairs of sentences and asked
to show which parts of these were in correspondence by aligning them on a word-by-
word basis.7 Our definition of alignment was fairly general (Och and Ney 2003): Given a
source string X = x1, ... , xN and a target string Y = y1, ... , yM, an alignment A between
two word strings is the subset of the Cartesian product of the word positions:
A C {(n,m) : n = 1,...,N;m = 1,...,M} (1)
We did not provide a formal definition of what constitutes a correspondence. As a
rule of thumb, annotators were told to align words or phrases x ↔ y in two sentences
(X, Y) whenever the words x could be substituted for y in Y, or vice versa. This relation-
ship should hold within the context of the sentence pair in question: the relation x ↔ y
need not hold in general contexts. Trivially this definition allowed for identical word
pairs.
Following common practice (Och, Tillmann, and Ney 1999; Och and Ney 2003;
Daum´e III and Marcu 2004), we distinguished between sure (S) and possible (P) align-
ments, where S C P. The intuition here is that sure alignments are clear-cut decisions
and typical of genuinely substitutable words or phrases, whereas possible alignments
flag a correspondence that has slightly divergent syntax or semantics. Annotators were
encouraged to produce sure alignments. They were also instructed to prefer smaller
alignments whenever possible, but were allowed to create larger block alignments.
Smaller alignments were generally used to indicate lexical correspondences, whereas
block alignments were reserved for non-compositional phrase pairs (e.g., idiomatic
expressions) or simply expressions with radically different syntax or vocabulary. In
</bodyText>
<footnote confidence="0.934345">
5 The IBM alignment models require a large amount of parallel data to yield reliable alignments. We
therefore selected the MTC for training purposes as it was the largest of our parallel corpora.
6 We used five iterations for each of Model 1, Model 2, and the HMM model.
7 The annotation was conducted using a Web-based alignment tool available at
http://demo.linearb.co.uk/paraphrases/.
</footnote>
<page confidence="0.992665">
600
</page>
<note confidence="0.899584">
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
</note>
<figureCaption confidence="0.993778">
Figure 1
</figureCaption>
<bodyText confidence="0.99553488">
Manual alignment between two sentence pairs from the MTC corpus, displayed as a grid. Black
squares represent sure alignment, gray squares represent possible alignment.
cases where information in one sentence was not present in the other, the annotators
were asked to leave this information unaligned.
Finally, annotators were given a list of heuristics to help them decide how to
make alignments in cases of ambiguity. These heuristics handled the alignment of
named entities (e.g., George Bush) and definite descriptions (e.g., the president), tenses
(e.g., had been and shall be), noun phrases with mismatching determiners (e.g., a man
and the man), verb complexes (e.g., was developed and had been developed), phrasal
verbs (e.g., take up and accept), genitives (e.g., Bush’s infrequent speeches and the infre-
quent speeches by Bush), pronouns, repetitions, typographic errors, and approximate
correspondences. For more details, we refer the interested reader to our annotation
guidelines.8
Figure 1 shows the alignment for two sentence pairs from the MTC corpus. The
first pair (Australia is concerned with the issue of carbon dioxide emissions. &lt;-4 The problem
of greenhouse gases has attracted Australia’s attention.) contains examples of word-to-
word (the &lt;-4 The; issue &lt;-4 problem; of &lt;-4 of; Australia &lt;-4 Australia) and many-to-many
alignments (carbon dioxide emissions &lt;-4 greenhouse gases). Importantly, we do not use
a large many-to-many block for Australia is concerned with and has attracted Australia’s
attention because it is possible to decompose the two phrases into smaller alignments.
The second sentence pair illustrates a possible alignment (could have very long term
effects &lt;-4 was of profound significance) indicated by the gray squares. Possible alignments
are used here because the two phrases only loosely correspond to each other. Possible
alignments are also used to mark significant changes in syntax where the words denote
a similar concept: for example, in cases where two words have the same stem but are
</bodyText>
<footnote confidence="0.989596">
8 Both the corpus and the annotation guidelines can be found at: http://homepages.inf.ed.ac.uk/
tcohn/paraphrase corpus.html.
</footnote>
<page confidence="0.981681">
601
</page>
<note confidence="0.285511">
Computational Linguistics Volume 34, Number 4
</note>
<bodyText confidence="0.930272">
expressed with different parts of speech, (e.g., co-operative ↔ cooperation) or when two
verbs are used that are not synonyms (e.g., this is also ↔ this also marks).
</bodyText>
<sectionHeader confidence="0.799446" genericHeader="method">
3. Human Agreement
</sectionHeader>
<bodyText confidence="0.988758444444444">
As mentioned in the previous section, 300 sentence pairs (100 pairs from each sub-
corpus) were doubly annotated, in order to measure inter-annotator agreement. Here,
we treat one annotator as gold-standard (reference) and measure the extent to which the
other annotator deviates from this reference.
Word-Based Measures. The standard technique for evaluating word alignments is to
represent them as a set of links (i.e., pairs of words) and compare them against gold-
standard alignments. The quality of an alignment A (defined in Equation (1)) compared
to reference alignment B can be then computed using standard recall, precision, and
F1 measures (Och and Ney 2003):
</bodyText>
<equation confidence="0.990954">
AS ∩ BP ||AP ∩ BS|= 2 · Precision · Recall
Precision = AS  |Recall = |BS  |F1 Precision + Recall (2)
</equation>
<bodyText confidence="0.979870538461538">
where the subscripts S and P denote sure and possible word alignments, respectively.
Note that both precision and recall are asymmetric in that they compare sets of possible
and sure alignments. This is designed to be maximally generous: sure predictions
which are present in the reference as possibles are not penalized in precision, and the
converse applies for recall. We adopt Fraser and Marcu (2007)’s definition of F1, an
F-measure between precision and recall over the sure and possibles. They argue that
it is a better alternative to the commonly used Alignment Error Rate (AER), which
does not sufficiently penalize unbalanced precision and recall.9 As our corpus is mono-
lingual, in order to avoid artificial score inflation, we limit the precision and recall
calculations to consider only pairs of non-identical words (and phrases, as discussed
subsequently).
To give an example, consider the sentence pairs in Figure 2, whose alignments have
been produced by the two annotators A (left) and B (right). Table 1 shows the individual
word alignments for each annotator and their type (sure or possible). In order to mea-
sure F1, we must first estimate Precision and Recall (see Equation (2)). Treating annota-
tor B as the gold standard, |AS |= 4, |BS |= 5, |AS ∩ BP |= 4, and |AP ∩ BS |= 4. This
results in a precision of 44 = 1, a recall of 45, and F1 of 2×1×0&apos;8
1+0&apos;8 = 0.89. Note that we ignore
alignments over identical words (i.e., discussed ↔ discussed, the ↔ the, and ↔ and,
. ↔ .).
Phrase-Based Measures. The given definitions are all word-based; however, our annota-
tors, and several paraphrasing models, create correspondences not only between words
but also between phrases. To take this into account, we also evaluate these measures
over larger blocks (similar to Ayan and Dorr [2006]). Specifically, we extract phrase
pairs from the alignments produced by our annotators using a modified version of the
standard SMT phrase extraction heuristic (Och, Tillmann, and Ney 1999). The heuristic
</bodyText>
<listItem confidence="0.767006">
9 Fraser and Marcu (2007) also argue for an unbalanced F-measure to bias towards recall. This is shown to
correlate better with translation quality. For paraphrasing it is not clear if such a bias would be beneficial.
</listItem>
<page confidence="0.99448">
602
</page>
<note confidence="0.904108">
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
</note>
<figureCaption confidence="0.989095">
Figure 2
</figureCaption>
<bodyText confidence="0.962094791666667">
Sample sentence pair showing the word alignments from two annotators.
extracts all phrase pairs consistent with the word alignment. These include phrase pairs
whose words are aligned to each other or nothing, but not to words outside the phrase
boundaries.10 The phrase extraction heuristic creates masses of phrase pairs, many of
which are of dubious quality. This is often due to the inclusion of unaligned words or
simply to the extraction of overly-large phrase pairs which might be better decomposed
into smaller units. For our purposes we wish to be maximally conservative in how we
process the data, and therefore we do not extract phrase pairs with unaligned words on
their boundaries.
Figure 3 illustrates the types of phrase pairs our extraction heuristic permits. Here,
the pair and reached ↔ and arrived at is consistent with the word alignment. In contrast,
the pair and reached ↔ and arrived isn’t; there is an alignment outside the hypothetical
phrase boundary which is not accounted for (reached is also aligned to at). The phrase
pair and reached an ↔ and arrived at is consistent with the word alignment; however it
has an unaligned word (i.e., an) on the phrase boundary, which we disallow.
Our phrase extraction procedure distinguishes between two types of phrase pairs:
atomic, that is, the smallest possible phrase pairs, and composite, which can be created
by combining smaller phrase pairs. For example, the phrase pair and reached ↔ and
arrived at in Figure 3 is composite, as it can be decomposed into and ↔ and and
reached ↔ arrived at. Table 2 shows the atomic and composite phrase pairs extracted
from the possible alignments produced by annotators A and B for the sentence pair in
Figure 2.
We compute recall, precision, and F1 over the phrase pairs extracted from the word
alignments as follows:
</bodyText>
<equation confidence="0.97508675">
Precision = |Apatom ∩ Bp |atom |Recall = |Ap ∩ Bp
atom |F1 = 2 · Precision · Recall
atom|
|Ap |Bp Precision + Recall (3)
</equation>
<page confidence="0.7893345">
10 The term phrase is not used here in the linguistic sense; many extracted phrases will not be constituents.
603
</page>
<table confidence="0.436242">
Computational Linguistics Volume 34, Number 4
</table>
<tableCaption confidence="0.994646">
Table 1
</tableCaption>
<bodyText confidence="0.953059823529412">
Single word pairs specified by the word alignments from Figure 2, for two annotators, A and B.
The column entries specify the alignment type for each annotator, either sure (S) or possible (P).
Dashes indicate that the word pair was not predicted by the annotator. Italics denote lexically
identical word pairs.
Word alignments A B
they H both – P
they H parties P P
discussed H discussed S S
the H the S S
aspects H specific P –
in H specific P P
detail H specific P P
aspects H issues P S
in H issues P –
detail H issues P –
and H and S S
reached H arrived S S
reached H at – S
an H a S S
extensive H general S P
agreement H consensus S S
. H . S S
Figure 3
Validity of phrase pairs according to the phrase extraction heuristic. Only the leftmost phrase
pair is valid. The others are inconsistent with the alignment or have an unaligned word on a
boundary, respectively, indicated by a cross.
where Ap and tap are the predicted and reference phrase pairs, respectively, and
the atom subscript denotes the subset of atomic phrase pairs, Apatom �Ap. As shown
in Equation (3) we measure precision and recall between atomic phrase pairs and
the full space of atomic and composite phrase pairs. This ensures that we do not
multiply reward composite phrase pair combinations,11 while also not unduly pe-
nalizing non-matching phrase pairs which are composed of atomic phrase pairs in
11 This contrasts with Ayan and Dorr (2006), who use all phrase pairs up to a given size, and therefore
might multiply count phrase pairs.
</bodyText>
<page confidence="0.997723">
604
</page>
<note confidence="0.933087">
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
</note>
<tableCaption confidence="0.986828">
Table 2
</tableCaption>
<bodyText confidence="0.946866692307692">
Phrase pairs are specified by the word alignments from Figure 2, using the possible alignments.
The entire set of atomic phrase pairs for either annotator (labeled A or B) and a selection of the
remaining 57 composite phrase pairs are shown. The italics denote lexically identical phrase pairs.
∗This phrase pair is atomic in A but composite in B.
Atomic phrase pairs A B
they H parties P –
they H both parties – P
discussed H discussed S S
the H the S S
aspects in detail H specific issues P –
in detail H specific – P
aspects H issues – S
and H and S S
</bodyText>
<equation confidence="0.605002333333333">
reached H arrived S S
reached H arrived at – S
reached an H arrived at a S P∗
</equation>
<bodyText confidence="0.901953375">
an H a S S
extensive H general S P
agreement H consensus S S
. H . S S
Composite phrase pairs A B
they discussed H both parties discussed – P
they discussed H parties discussed P –
they discussed the H both parties discussed the – P
they discussed the H parties discussed the P –
they ... reached an H both parties ... arrived at a P –
the aspects in detail H the specific issues P P
reached an extensive H arrived at a general S S
extensive agreement. H general consensus. S S
...
the reference. Returning to the example in Table 2, with annotator B as the gold
standard, |Ap atom |= 7, |Bp atom |= 8, |Apatom n Bp |= 5, and |Ap n Bpatom |= 4. Consequently,
</bodyText>
<equation confidence="0.995029">
precision= 57 = 0.71, recall= 48 = 0.50, and F1= 2×0 71×0 50
0 71+0 50 = 0.59. Again we ignore
</equation>
<bodyText confidence="0.9960976">
identical phrase pairs.
A potential caveat here concerns the quality of the atomic phrase pairs, which are
automatically induced and may not correspond to linguistic intuition. To evaluate this,
we had two annotators review a random sample of 166 atomic phrase pairs drawn from
the MTC corpus (sure), classifying each phrase pair as correct, incorrect, or uncertain
given the sentence pair as context. From this set, 73% were deemed correct, 22% un-
certain, and 5% incorrect.12 Annotators agreed in their decisions 75% of the time (using
the Kappa13 statistic, their agreement is 0.61). This confirms that the phrase-extraction
process produces reliable phrase pairs from our word-aligned data (although we cannot
claim that it is exhaustive).
</bodyText>
<footnote confidence="0.972066">
12 Taking a more conservative position by limiting the proportion of unaligned words within the phrase
pair improves these figures monotonically to 90% correct and 0% incorrect (fully aligned phrase pairs).
13 This Kappa is computed over three nominal categories (correct, incorrect, and uncertain) and should not
be confused with the agreement measure we develop in the following section for phrase pairs.
</footnote>
<page confidence="0.983483">
605
</page>
<note confidence="0.466236">
Computational Linguistics Volume 34, Number 4
</note>
<bodyText confidence="0.982263333333333">
Chance-Corrected Agreement. Besides precision and recall, inter-annotator agreement is
commonly measured using the Kappa statistic (Cohen 1960). Thus is a desirable mea-
sure because it is adjusted for agreement due purely to chance:
</bodyText>
<equation confidence="0.998689666666667">
Pr(A) − Pr(E)
κ = (4)
1 − Pr(E)
</equation>
<bodyText confidence="0.99914065625">
where Pr(A) is the proportion of times two coders14 agree, corrected by Pr(E), the
proportion of times we would expect them to agree by chance.
Kappa is a suitable agreement measure for nominal data. An example would be a
classification task, where two coders must assign n linguistic instances (e.g., sentences
or words) into one of m categories. Given this situation, it would be possible for each
coder to assign each instance to the same category. Kappa allows us to quantify whether
the coders agree with each other about the category membership of each instance. It is
relatively straightforward to estimate Pr(A)—it is the proportion of instances on which
the two coders agree. Pr(E) requires a model of what would happen if the coders were
to assign categories randomly. Under the assumption that coders r1 and r2 are indepen-
dent, the chance of them agreeing on the jth category is the product of each of them
assigning an instance to that category: Pr(Cj|r1) Pr(Cj|r2). Chance agreement is then the
sum of this product across all categories: Pr(E) = j=1 Pr(Cj|r1)Pr(Cj|r2). The literature
describes two different methods for estimating
Either a separate distribution
is estimated for each coder (Cohen 1960) or the same distribution for all coders (Scott
1955; Fleiss 1971; Siegel and Castellan 1988). We refer the interested reader to Di Eugenio
and Glass (2004) and Artstein and Poesio (2008) for a more detailed discussion.
Unfortunately, Kappa is not universally suited to every categorization task. A prime
example is structured labeling problems that allow a wide variety of output categories.
Importantly, the number and type of categories is not fixed in advance and can vary
from instance to instance. In parsing, annotators are given a sentence for which they
must specify a tree, of which there is an exponential number in the sentence length. Sim-
ilarly, in our case the space of possible alignments for a sentence pair is also exponential
in the input sentence lengths. Considering these annotations as nominal variables is
inappropriate.
Besides, alignments are only an intermediate representation that we have used to
facilitate the annotation of paraphrases. Ideally, we would like to measure agreement
over the set of phrase pairs which are specified by our annotators (via the word align-
ments), not the alignment matrices themselves.
Kupper and Hafner (1989) present an alternative measure similar to Kappa that is
especiall
</bodyText>
<figure confidence="0.959826740740741">
Pr(Cj|ri).
y designed for sets of variables:
Cˆ = πˆ − π0
1 − π0 , (5)
where �I Bi
πˆ = i=1 min(
Ai
Bi
)
and
0 = k Em
(
Ai
Bi
)
|Ai ∩
|
|
|,|
|
,
π
in
|
|,
|
i
</figure>
<footnote confidence="0.563346">
14 Kappa has been extended to more than two coders (Fleiss 1971; Bartko and Carpenter 1976). For
sake our discussion and subsequent examples involve two coders. Also note that we use the
term coder instead of the more common rater. This is because in our task the annotators must identify
(a.k.a. code) the paraphrases rather than
simplicity’s
rate them.
</footnote>
<page confidence="0.992689">
606
</page>
<note confidence="0.885212">
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
</note>
<bodyText confidence="0.999547269230769">
Here, Ai and Bi are the coders’ predictions on sentence pair i from our corpus of I
sentence pairs. Each prediction is a subset of the full space of k items. Expression (5)
measures the agreement (or concordance) between coders A and B and follows the
general form of Kappa from Equation (4), which is defined analogously with Pr(A) and
Pr(E) taking the roles of πˆ and π0, but with different definitions.
Kupper and Hafner (1989) developed their agreement measure with medical diag-
nostic tasks in mind. For example, two physicians classify subjects into k = 3 diagnostic
categories and wish to find out whether they agree in their diagnoses. Here, each coder
must decide which (possibly empty) subset from k categories best describes each subject.
The size of k is thus invariant with the instance under consideration. This is not true
in our case, where k will vary across sentence pairs as sentences of different lengths
license different numbers of phrase pairs. More critically, the formulation in Equa-
tion (5) assumes that items in the set are independent: All subsets of the same car-
dinality as k are equally likely, and no combination is impossible. This independence
assumption is inappropriate for the paraphrase annotation task. The phrase extraction
heuristic allows each contiguous span in a sentence to be aligned to either zero or one
span in the other sentence; that is, nominating a phrase pair precludes the choice of
many other possible phrase pairs. Consequently relatively few of the subsets of the
full set of possible phrase pairs are valid. Formally, an alignment can specify only
O(N2) phrase pairs from a total set of k = O(N4) possible phrase pairs. This disparity in
magnitudes leads to increasingly underestimated πˆ for larger N, namely, limN→∞ π0 =
limN→∞ O(N2)/O(N4) = 0. The end result is an overestimate of Cˆ on longer sentences.
For these reasons, we adapt the method of Kupper and Hafner (1989) to account for
our highly interdependent item sets. We use Cˆ from Equation (5) as our agreement sta-
tistic defined over sets of atomic phrase pairs, that is, A = Apatom, B = Bpatom. We redefine
π0 as follows:
</bodyText>
<equation confidence="0.932733">
π0 = 1I I E E atom ∩ Bp
i=1 Ap Bp Pr(Ap atom) Pr(Bp atom) |Ap atom |(6)
atom atom min(|Ap atom|, |Bp atom|)
</equation>
<bodyText confidence="0.998756181818182">
where Apatom and Bpatom range over the sets of atomic phrase pairs licensed by sentence
pair i, and Pr(Apatom) and Pr(Baptom) are priors over these sets for each annotator. A conse-
quence of dropping the independence assumptions is that calculating π0 is considerably
more difficult.
While it may be possible to calculate π0 analytically, this gets increasingly compli-
cated for larger phrase pairs or with an expressive prior. For the sake of flexibility we
estimate π0 using Monte Carlo sampling. Specifically, we approximate the full sum by
drawing samples from a prior distribution over sets of phrase pairs for each of our
annotators (Pr(Apatom) and Pr(Bpatom) in Equation (6)). These samples are then compared
using the intersection metric. This is repeated many times and the results are then
averaged. More formally:
</bodyText>
<equation confidence="0.9311238">
ˆπ0 = 1I I 1 J |Ap (j) ∩ Bp (j)|
i=1 J E atom atom
j=1 (7)
min(|Ap (j)|, |Bp (j)|)
atom atom
</equation>
<bodyText confidence="0.996231">
where for each sentence pair, i, we draw J samples of pairs of sets of phrase pairs,
(Apatom, Bpatom). We use J = 1, 000, which is ample to give reliable estimates. So far we have
</bodyText>
<page confidence="0.989136">
607
</page>
<note confidence="0.489796">
Computational Linguistics Volume 34, Number 4
</note>
<bodyText confidence="0.987664754716981">
not defined how we sample valid sets of phrase pairs. This is done via the word align-
ments. Recall that the annotators start out with alignments from an automatic word-
aligner. Firstly, we develop a distribution to predict how often an annotator changes a
cell from the initial alignment matrix. We model the number of changes made with a
binomial distribution, that is, each local change is assumed independent and has a fixed
probability, Pr(edit|r, Ni, Mi) where r is the coder and Ni and Mi are the sentence lengths.
This distribution is fit to each annotator’s predictions using a linear function over the
combined length of two sentences. Next we sample word alignments. Each sample
starts with the automatic alignment, and each cell is changed with probability Pr(edit).
These changes are binary, swapping alignments for non-alignments and vice versa.
Finally, the phrase-extraction heuristic is run over the alignment matrix to produce a
set of phrase pairs. This is done for each annotator, A and B, after which we have a
sample, (Apatom, Bpatom). Each sample is then fed into Equation (7). Admittedly, this is not
the most accurate prior, as annotators are not just randomly changing the alignment, but
instead are influenced by the content expressed by the sentence pair and other factors
such as syntactic complexity. However, this prior produces estimates for ˆπ0 which are
several orders of magnitude larger than those using Kupper and Hafner’s model of π0
in Equation (5).
We now illustrate the process of measuring chance-corrected agre
ˆC,
ement,
with
respect to the example in Figure 2. Here, |Apatom |= 7, |Bpatom |= 8, |Apatom ∩ Bpatom |= 4, and
therefore πˆ = 4 7 = 0.571. For this sentence our annotators edited eight and nine align-
ment cells, respectively, of the initial alignment matrix. This translates into Pr(edit|r =
A) = 8
12×13 = 5.13% and Pr(edit|r = B) = 5.77%. Given these priors, we run the Monte
Carlo sampling process from Equation (7), which results in ˆπ0 = 0.147. Combining the
agreement estimate, ˆπ, and chance correction estimate, ˆπ0, using Equation (6) results in
Cˆ = 0.571−0.147
1−0.147 = 0.497.
Now, imagine a hypothetical case where πˆ = 4 7 = 0.571 (i.e., the agreement is the
same as before), annotator B edits nine alignment cells, but annotator A chooses not
to make any edits. This leads to an increased estimate of ˆπ0 = 0.259 and a decreased
Cˆ = 0.442. If both annotators were not to make any edits, ˆπ0 = 1 and Cˆ = −∞. Interest-
ingly, at the other extreme when Pr(edit|r = A) = Pr(edit|r = B) = 1, agreement is also
perfect, ˆπ0 = 1 and Cˆ = −∞. This is because only one phrase pair can be extracted
which consists of the two full sentences.
Results. Tables 3 and 4 display agreement statistics on our three corpora using precision,
recall, F1, and ˆC. Specifically, we estimate Cˆ by aggregating πˆ and ˆπ0 into corpus-
level estimates. Table 3 shows agreement scores for individual words, whereas Table 4
shows agreement for phrase pairs. In both cases the agreement is computed over non-
identical word and phrase pairs which are more likely to correspond to paraphrases.
The agreement figures are broken down into possible (Poss) and sure alignments (Sure)
for precision and recall.
When agreement is measured over words, our annotators obtain high F1 on all
three corpora (MTC, Leagues, and News). Recall on Possibles seems worse on the
News corpus when compared to MTC or Leagues. This is to be expected because this
corpus was automatically harvested from the Web, and some of its instances may not
be representative examples of paraphrases. For example, it is common for one sentence
to provide considerably more details than the other, despite the fact that both describe
the same event. The annotators in turn have difficulty deciding whether such instances
are valid paraphrases. The Cˆ scores for the three corpora are in the same ballpark.
</bodyText>
<page confidence="0.998505">
608
</page>
<note confidence="0.961376">
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
</note>
<tableCaption confidence="0.851703">
Table 3
Inter-annotator agreement using precision, recall, F1, and ˆC; the agreement is measured over
words.
</tableCaption>
<table confidence="0.95778675">
MTC Leagues News
Measure Poss Sure Measure Poss Sure Measure Poss Sure
Prec 0.79 0.59 Prec 0.85 0.73 Prec 0.78 0.55
Rec 0.77 0.73 Rec 0.74 0.75 Rec 0.57 0.70
F1 0.76 F1 0.79 F1 0.74
Cˆ 0.85
Cˆ
Cˆ
</table>
<tableCaption confidence="0.840486666666667">
Table 4
Inter-annotator agreement using precision, recall, F1, and ˆC; the agreement is measured over
atomic phrase pairs.
</tableCaption>
<table confidence="0.536691">
0.87
0.89
MTC Leagues News
Measure Poss Sure Measure Poss Sure Measure Poss Sure
Prec 0.77 0.67 Prec 0.74 0.72 Prec 0.72 0.68
Rec 0.77 0.66 Rec 0.77 0.73 Rec 0.69 0.81
F1 0.71 F1 0.74 F1 0.76
</table>
<equation confidence="0.671029666666667">
Cˆ 0.63
Cˆ
Cˆ
</equation>
<bodyText confidence="0.997062375">
Interestingly, Cˆ is highest on the News corpus, whereas F1 is lowest. Whereas precision
and recall are normalized by the number of predictions from annotators A and B,
respectively, Cˆ is normalized by the minimum number of predictions between the two.
Therefore, when the predictions are highly divergent, Cˆ will paint a rosier picture than
F1 (which is the combination of precision and recall). This indeed seems to be the case
for the News corpus, where precision and recall have a higher spread in comparison to
the other two corpora (see the Poss column in Table 3).
Agreement scores tend to be lower when taking phrases into account (see Table 4).
This is expected because annotators are faced with a more complex task; they must
generally make more decisions: for example, determining the phrase boundaries and
how to align their constituent words. An exception to this trend is the News corpus
where the F1 is higher for phrase pairs than for individual word pairs. This is due to the
fact that there are many similar sentence pairs in this data. These have many identical
words and a few different words. The differences are often in a clump (e.g., person
names, verb phrases), rather than distributed throughout the sentence. The annotators
tend to block align these and there is a large scope for disagreement. Whereas estimating
agreement over words heavily penalizes block differences, when phrases are taken
into account in the F1 measure, these are treated more leniently. Note that Cˆ is not
so lenient, as it measures agreement over the sets of atomic phrase pairs rather than
between atomic and composite phrase pairs in the F1 measure. This means that under
ˆC, choosing different granularities of phrases will be penalized, but would not have
been under the F1 measure.
In Figure 4 we show how Cˆ varies with sentence length for our three corpora. Specif-
ically, we plot observed agreement ˆπ, chance agreement π0, and Cˆ against sentence pairs
</bodyText>
<page confidence="0.990893">
609
</page>
<figure confidence="0.992825">
0.62
0.53
Computational Linguistics Volume 34, Number 4
</figure>
<figureCaption confidence="0.984624">
Figure 4
</figureCaption>
<tableCaption confidence="0.860721166666667">
Agreement statistics plotted against sentence length for the three sub-corpora. Each group of
three columns correspond to ˆπ, ˆπ0, and ˆC, respectively. The statistics were measured over
non-identical phrase pairs using all phrase pairs, atomic and composite.
Table 5
Agreement between automatic Giza++ predicted word alignments and our manually corrected
alignments, measured over atomic phrase pairs.
</tableCaption>
<table confidence="0.9992204">
MTC Leagues News
Measure Poss Sure Measure Poss Sure Measure Poss Sure
Prec 0.58 0.55 Prec 0.63 0.60 Prec 0.63 0.65
Rec 0.42 0.49 Rec 0.39 0.47 Rec 0.50 0.64
F1 0.53 F1 0.54 F1 0.63
</table>
<bodyText confidence="0.998569789473684">
binned by (the shorter) sentence length. In all cases we observe that chance agreement
is substantially lower than observed agreement for all sentence lengths. We also see that
Cˆ tends to be higher for shorter sentences. Differences in Cˆ across sentence lengths are
mostly of small magnitude across all three corpora. This indicates that disagreements
may be due to other factors, besides sentence length.
Unfortunately, there are no comparable annotation studies that would allow us
to gauge the quality of the obtained agreements. The use of precision, recall, and F1
is widespread in SMT, but these measures evaluate automatic alignments against a
gold standard, rather than the agreement between two or more annotators (but see
Melamed [1998] for an exception). Nevertheless, we would expect the humans to agree
more with each other than with Giza++, given that the latter produces many erroneous
word alignments and is not specifically tuned to the paraphrasing task. Table 5 shows
agreement between one annotator and Giza++ for atomic phrase pairs.15 We obtained
similar results for the other annotator and with the word-based measures. As can be
seen, human–Giza++ agreement is much lower than human–human agreement on all
three corpora (compare Tables 5 and 4). Taken together the results in Tables 3–5 show
a substantial level of agreement, thus indicating that our definition of paraphrases via
word alignments can yield reliable annotations. In the following section we discuss how
our corpus can be usefully employed in the study of paraphrasing.
</bodyText>
<footnote confidence="0.871506">
15 Note that we cannot meaningfully measure Cˆ for this data because the Giza++ predictions are already
being used to estimate π0 in our formulation. Consequently, P(A) = P(B) and Cˆ is zero.
</footnote>
<page confidence="0.963691">
610
</page>
<note confidence="0.851382">
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
</note>
<sectionHeader confidence="0.956156" genericHeader="method">
4. Experiments
</sectionHeader>
<bodyText confidence="0.98191275">
Our annotated corpus can be used in a number of ways to help paraphrase research:
for example, to inform the linguistic analysis of paraphrases, as a training set for the
development of discriminative paraphrase systems, and as a test set for the automatic
evaluation of computational models. Here, we briefly demonstrate some of these uses.
Paraphrase Modeling. Much previous research has focused on lexical paraphrases (but see
Lin and Pantel [2001] and Pang, Knight, and Marcu [2003] for exceptions). We argue
that our corpus should support a richer range of structural (syntactic) paraphrases.
To demonstrate this we have extracted paraphrase rules from our annotations using
the grammar induction algorithm from Cohn and Lapata (2007). Briefly, the algorithm
extracts tree pairs from word-aligned text by choosing aligned constituents in a pair of
equivalent sentences. These pairs are then generalized by factoring out aligned subtrees,
thereby resulting in synchronous grammar rules (Aho and Ullman 1969) with variable
nodes.
We parsed the MTC corpus with Bikel’s (2002) parser and extracted synchronous
rules from the gold-standard alignments. A sample of these rules are shown in Figure 5.
Here we see three lexical paraphrases, followed by five structural paraphrases. In
example 4, also is replaced with moreover and is moved to the start of the sentence from
the pre-verbal position. Examples 5–8 show various reordering operations, where the
boxed numbers indicate correspondences between non-terminals in the two sides of the
rules.
The synchronous rules in Figure 5 provide insight into the process of paraphrasing
at the syntactic level, and also a practical means for developing algorithms for para-
phrase generation—a task which has received little attention to date. For instance, we
could envisage a paraphrase model that transforms parse trees of an input sentence
into parse trees that represent a sentential paraphrase of that sentence. Our corpus can
be used to learn this mapping using discriminative methods (Cowan, Ku˘cerov´a, and
Collins 2006; Cohn and Lapata 2007).
Evaluation Set. As mentioned in Section 1, it is currently difficult to compare competing
approaches due to the effort involved in eliciting manual judgments of paraphrase
output. Our corpus could fill the role of a gold-standard test set, allowing for automatic
evaluation techniques.
Developing measures for automatic paraphrase evaluation is outside the scope of
this article. Nevertheless, we illustrate how the corpus can be used for this purpose.
For example we could easily measure the precision and recall of an automatic system
Figure 5
Synchronous grammar rules extracted from the MTC corpus.
</bodyText>
<page confidence="0.989752">
611
</page>
<note confidence="0.590481">
Computational Linguistics Volume 34, Number 4
</note>
<bodyText confidence="0.99983853125">
against our annotations. Computing precision and recall for an individual system is not
perhaps the most meaningful test, considering the large potential for paraphrasing in
a given sentence pair. A better evaluation strategy would include a comparison across
many systems on the same corpus. We could then rank these systems without, however,
paying so much attention to the absolute precision and recall values. We expect these
comparisons to yield relatively low numbers for many reasons. First and foremost the
task is hard, as shown by our inter-annotator agreement figures in Tables 3 and 4.
Secondly, there may be valid paraphrases that the systems identify but are not listed
in our gold standard. Thirdly, systems may have different biases, for example, towards
producing more lexical or syntactic paraphrases, but our comparison would not take
this into account. Despite all these considerations, we believe that comparison against
our corpus would treat these systems on an equal footing against the same materials
while factoring out nonessential degrees of freedom inherent in human elicitation stud-
ies (e.g., attention span, task familiarity, background).
We evaluated the performance of two systems against our corpus. Our first system
is simply Giza++ trained on the 55,615 sentence pairs described in Section 4. The
second system uses a co-training-based paraphrase extraction algorithm (Barzilay and
McKeown 2001). It was also trained on the MTC part 1 corpus, on the same data set
used for Giza++, with its default parameters. For each system, we filtered the predicted
paraphrases to just those which match part of a sentence pair in the test set. These
paraphrases were then compared to the sure phrase pairs extracted from our manually
aligned corpus. Giza++’s precision is 55% and recall 49% (see Table 5). The co-training
system obtained a precision of 30% and recall of 16%. To confirm the accuracy of
the precision estimate, we performed a human evaluation on a sample of 48 of the
predicted paraphrases which were treated as errors. Of these, 63% were confirmed as
being incorrect and only 20% were acceptable (the remaining were uncertain). The inter-
annotator agreement in Table 4 can be used as an upper bound for precision and recall
(precision for Sure phrase pairs is 67% and recall 66%). These results seem to suggest
that a hypothetical paraphrase extractor based on automatic word alignments would
obtain performance superior to the co-training approach. However, we must bear in
mind that the co-training system is highly parametrized and was not specifically tuned
to our data set.
</bodyText>
<sectionHeader confidence="0.997818" genericHeader="conclusions">
5. Conclusions
</sectionHeader>
<bodyText confidence="0.999965266666667">
In this article we have presented a human-annotated paraphrase corpus and argued
that it can be usefully employed for the evaluation and modeling of paraphrases. We
have defined paraphrases as word alignments in a corpus containing pairs of equivalent
sentences and shown that these can be reliably identified by annotators. In measur-
ing agreement, we used the standard measures of precision, recall, and F1, but also
proposed a novel formulation of chance-corrected agreement for word (and phrase)
alignments. Beyond alignment, our formulation could be applied to other structured
tasks including parsing and sequence labeling.
The uses of the corpus are many and varied. It can serve as a test set for eval-
uating the precision and recall of paraphrase induction systems trained on parallel
monolingual corpora. The corpus could be further used to develop new evaluation
metrics for paraphrase acquisition or novel paraphrasing models. An exciting avenue
for future research concerns paraphrase prediction, that is, determining when and how to
paraphrase single sentence input. Because our corpus contains paraphrase annotations
at the sentence level, it could provide a natural test-bed for prediction algorithms.
</bodyText>
<page confidence="0.992421">
612
</page>
<note confidence="0.950329">
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
</note>
<sectionHeader confidence="0.991196" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997662846153846">
The authors acknowledge the support of the
EPSRC (Cohn, grant GR/T04557/01;
Lapata, grant GR/T04540/01), the National
Science Foundation (Callison-Burch, grant
IIS-071344), and the EuroMatrix project
(Callison-Burch) funded by the European
Commission (6th Framework Programme).
We are grateful to our annotators Vasilis
Karaiskos and Tom Segler. Thanks to Regina
Barzilay for providing us the output of her
system on our data and to the anonymous
referees whose feedback helped to
substantially improve the present article.
</bodyText>
<sectionHeader confidence="0.996013" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999285088235294">
Aho, A. V. and J. D. Ullman. 1969. Syntax
directed translations and the pushdown
assembler. Journal of Compute System
Sciences, 3(1):37–56.
Artstein, Ron and Massimo Poesio. 2008.
Inter-coder agreement for Computational
Linguistics. Computational Linguistics.
Ayan, Necip Fazil and Bonnie J. Dorr. 2006.
Going beyond AER: An extensive analysis
of word alignments and their impact on
MT. In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics, pages 9–16,
Sydney.
Bannard, Colin and Chris Callison-Burch.
2005. Paraphrasing with bilingual parallel
corpora. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 597–604, Ann Arbor, MI.
Bartko, John J. and William T. Carpenter.
1976. On the methods and theory of
reliability. Journal of Nervous and Mental
Disease, 163(5):307–317.
Barzilay, Regina. 2003. Information Fusion for
Multi-Document Summarization:
Paraphrasing and Generation. Ph.D. thesis,
Columbia University, New York, NY.
Barzilay, Regina and Noemie Elhadad.
2003. Sentence alignment for monolingual
comparable corpora. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing, pages 25–32,
Sapporo.
Barzilay, Regina and Lillian Lee. 2003.
Learning to paraphrase: An unsupervised
approach using multiple-sequence
alignment. In Proceedings of the Human
Language Technology Conference and the
Annual Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 16–23, Edmonton.
Barzilay, Regina and Kathy McKeown. 2001.
Extracting paraphrases from a parallel
corpus. In Proceedings of the 39th Annual
Meeting of the Association for Computational
Linguistics, pages 50–57, Toulouse.
Bikel, Daniel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing
engine. In Proceedings of the Human
Language Technology Conference,
pages 24–27, San Diego, CA.
Brown, Peter F., Stephen A. Della-Pietra,
Vincent J. Della-Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263–311.
Callison-Burch, Chris. 2007. Paraphrasing and
Translation. Ph.D. thesis, University of
Edinburgh, Edinburgh, Scotland.
Callison-Burch, Chris, Philipp Koehn, and
Miles Osborne. 2006. Improved statistical
machine translation using paraphrases. In
Proceedings of the Human Language
Technology Conference and Annual Meeting of
the North American Chapter of the Association
for Computational Linguistics, pages 17–24,
New York, NY.
Cohen, J. 1960. A coefficient of agreement for
nominal scales. Educational and
Psychological Measurement, 20:37–46.
Cohn, Trevor and Mirella Lapata. 2007. Large
margin synchronous generation and its
application to sentence compression. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing and
on Computational Natural Language
Learning, pages 73–82, Prague.
Cowan, Brooke, Ivona Ku˘cerov´a, and
Michael Collins. 2006. A discriminative
model for tree-to-tree translation. In
Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 232–241, Sydney.
Daum´e III, Hal and Daniel Marcu.
2004. A phrase-based HMM approach
to document/abstract alignment.
In Proceedings of the 2004 Conference
on Empirical Methods in Natural
Language Processing, pages 119–126,
Barcelona.
Di Eugenio, Barbara and Michael Glass.
2004. The kappa statistic: A second look.
Computational Linguistics, 30(1):95–101.
Dolan, William, Chris Quirk, and Chris
Brockett. 2004. Unsupervised construction
of large paraphrase corpora: Exploiting
massively parallel news sources. In
Proceedings of the 20th International
Conference on Computational Linguistics,
pages 350–356, Geneva.
</reference>
<page confidence="0.971566">
613
</page>
<reference confidence="0.993854868686868">
Computational Linguistics Volume 34, Number 4
Duboue, Pablo and Jennifer Chu-Carroll.
2006. Answering the question you wish
they had asked: The impact of
paraphrasing for question answering.
In Proceedings of the Human Language
Technology Conference of the NAACL,
Companion Volume: Short Papers,
pages 33–36, New York, NY.
Fleiss, Joseph L. 1971. Measuring nominal
scale agreement among many raters.
Psychological Bulletin, 76(5):378–382.
Fraser, Alexander and Daniel Marcu. 2007.
Measuring word alignment quality for
statistical machine translation.
Computational Linguistics, 33(3):293–303.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Human Language Technology
Conference and Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics, pages 48–54,
Edmonton.
Kupper, Lawrence L. and Kerry B. Hafner.
1989. On assessing interrater agreement
for multiple attribute responses. Biometrics,
45(3):957–967.
Lin, Dekang and Patrick Pantel. 2001.
Discovery of inference rules for question
answering. Natural Language Engineering,
7(4):342–360.
Martin, Joel, Rada Mihalcea, and Ted
Pedersen. 2005. Word alignment for
languages with scarce resources. In
Proceedings of the ACL Workshop on Building
and Using Parallel Texts, pages 67–74,
Ann Arbor, MI.
Melamed, I. Dan. 1998. Manual annotation
of translational equivalence: The Blinker
project. IRCS Technical Report #98-07,
University of Pennsylvania,
Philadelphia, PA.
Mihalcea, Rada and Ted Pedersen. 2003. An
evaluation exercise for word alignment. In
Proceedings of the HLT-NAACL Workshop on
Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond,
pages 1–6, Edmonton.
Och, Franz Josef and Hermann Ney. 2000a. A
comparison of alignment models for
statistical machine translation. In
Proceedings of the 18th International
Conference on Computational Linguistics,
pages 1086–1090, Saarbr¨ucken.
Och, Franz Josef and Hermann Ney.
2000b. Improved statistical alignment
models. In Proceedings of the 38th
Annual Meeting of the Association for
Computational Linguistics, pages 440–447,
Hong Kong.
Och, Franz Josef and Hermann Ney. 2003. A
systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19–52.
Och, Franz Josef, Christoph Tillmann, and
Hermann Ney. 1999. Improved alignment
models for statistical machine translation.
In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora,
pages 20–28, College Park, MD.
Pang, Bo, Kevin Knight, and Daniel Marcu.
2003. Syntax-based alignment of multiple
translations: Extracting paraphrases and
generating new sentences. In Proceedings of
the Human Language Technology Conference
and the Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics, pages 181–188,
Edmonton.
Quirk, Chris, Chris Brockett, and William
Dolan. 2004. Monolingual machine
translation for paraphrase generation. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
pages 142–149, Barcelona.
Scott, William A. 1955. Reliability of content
analysis: The case of nominal scale. Public
Opinion Quarterly, 19:127–141.
Siegel, Sidney and N. John Castellan. 1988.
Non Parametric Statistics for the Behavioral
Sciences. McGraw-Hill, New York.
Zhou, Liang, Chin-Yew Lin, Dragos Stefan
Munteanu, and Eduard Hovy. 2006.
Paraeval: Using paraphrases to
evaluate summaries automatically. In
Proceedings of the Human Language
Technology Conference, pages 447–454,
New York, NY.
</reference>
<page confidence="0.998157">
614
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.744634">
<title confidence="0.8864955">Constructing Corpora for the Development and Evaluation of Paraphrase Systems</title>
<affiliation confidence="0.998727">University of Edinburgh Johns Hopkins University University of Edinburgh</affiliation>
<abstract confidence="0.993823428571429">Automatic paraphrasing is an important component in many natural language processing tasks. In this article we present a new parallel corpus with paraphrase annotations. We adopt a definition of paraphrase based on word alignments and show that it yields high inter-annotator agreement. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed in evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and F1) and also in developing linguistically rich paraphrase models based on syntactic structure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<title>Syntax directed translations and the pushdown assembler.</title>
<date>1969</date>
<journal>Journal of Compute System Sciences,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="42452" citStr="Aho and Ullman 1969" startWordPosition="6872" endWordPosition="6875">ocused on lexical paraphrases (but see Lin and Pantel [2001] and Pang, Knight, and Marcu [2003] for exceptions). We argue that our corpus should support a richer range of structural (syntactic) paraphrases. To demonstrate this we have extracted paraphrase rules from our annotations using the grammar induction algorithm from Cohn and Lapata (2007). Briefly, the algorithm extracts tree pairs from word-aligned text by choosing aligned constituents in a pair of equivalent sentences. These pairs are then generalized by factoring out aligned subtrees, thereby resulting in synchronous grammar rules (Aho and Ullman 1969) with variable nodes. We parsed the MTC corpus with Bikel’s (2002) parser and extracted synchronous rules from the gold-standard alignments. A sample of these rules are shown in Figure 5. Here we see three lexical paraphrases, followed by five structural paraphrases. In example 4, also is replaced with moreover and is moved to the start of the sentence from the pre-verbal position. Examples 5–8 show various reordering operations, where the boxed numbers indicate correspondences between non-terminals in the two sides of the rules. The synchronous rules in Figure 5 provide insight into the proce</context>
</contexts>
<marker>Aho, Ullman, 1969</marker>
<rawString>Aho, A. V. and J. D. Ullman. 1969. Syntax directed translations and the pushdown assembler. Journal of Compute System Sciences, 3(1):37–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for Computational Linguistics. Computational Linguistics.</title>
<date>2008</date>
<contexts>
<context position="27312" citStr="Artstein and Poesio (2008)" startWordPosition="4338" endWordPosition="4341">. Under the assumption that coders r1 and r2 are independent, the chance of them agreeing on the jth category is the product of each of them assigning an instance to that category: Pr(Cj|r1) Pr(Cj|r2). Chance agreement is then the sum of this product across all categories: Pr(E) = j=1 Pr(Cj|r1)Pr(Cj|r2). The literature describes two different methods for estimating Either a separate distribution is estimated for each coder (Cohen 1960) or the same distribution for all coders (Scott 1955; Fleiss 1971; Siegel and Castellan 1988). We refer the interested reader to Di Eugenio and Glass (2004) and Artstein and Poesio (2008) for a more detailed discussion. Unfortunately, Kappa is not universally suited to every categorization task. A prime example is structured labeling problems that allow a wide variety of output categories. Importantly, the number and type of categories is not fixed in advance and can vary from instance to instance. In parsing, annotators are given a sentence for which they must specify a tree, of which there is an exponential number in the sentence length. Similarly, in our case the space of possible alignments for a sentence pair is also exponential in the input sentence lengths. Considering </context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Artstein, Ron and Massimo Poesio. 2008. Inter-coder agreement for Computational Linguistics. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Going beyond AER: An extensive analysis of word alignments and their impact on MT.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>9--16</pages>
<location>Sydney.</location>
<contexts>
<context position="22873" citStr="Ayan and Dorr (2006)" startWordPosition="3568" endWordPosition="3571">th the alignment or have an unaligned word on a boundary, respectively, indicated by a cross. where Ap and tap are the predicted and reference phrase pairs, respectively, and the atom subscript denotes the subset of atomic phrase pairs, Apatom �Ap. As shown in Equation (3) we measure precision and recall between atomic phrase pairs and the full space of atomic and composite phrase pairs. This ensures that we do not multiply reward composite phrase pair combinations,11 while also not unduly penalizing non-matching phrase pairs which are composed of atomic phrase pairs in 11 This contrasts with Ayan and Dorr (2006), who use all phrase pairs up to a given size, and therefore might multiply count phrase pairs. 604 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems Table 2 Phrase pairs are specified by the word alignments from Figure 2, using the possible alignments. The entire set of atomic phrase pairs for either annotator (labeled A or B) and a selection of the remaining 57 composite phrase pairs are shown. The italics denote lexically identical phrase pairs. ∗This phrase pair is atomic in A but composite in B. Atomic phrase pairs A B they H parties P – they H both parties – P </context>
</contexts>
<marker>Ayan, Dorr, 2006</marker>
<rawString>Ayan, Necip Fazil and Bonnie J. Dorr. 2006. Going beyond AER: An extensive analysis of word alignments and their impact on MT. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 9–16, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>597--604</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="1624" citStr="Bannard and Callison-Burch 2005" startWordPosition="226" endWordPosition="229">marization (Barzilay 2003; Zhou et al. 2006) to question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003). Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases (Bannard and Callison-Burch 2005; CallisonBurch 2007) as well as comparable corpora such as collections of articles produced by two different newswire agencies about the same events (Barzilay and Elhadad 2003). Although paraphrase induction algorithms differ in many respects—for example, the acquired paraphrases often vary in granularity as they can be lexical (fighting, battle) or structural (last week’s fighting, the battle last week), and are represented as words or * School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: tcohn@inf.ed.ac.uk. ** Center for Speech and Language Processing, Johns Hopk</context>
<context position="3379" citStr="Bannard and Callison-Burch 2005" startWordPosition="481" endWordPosition="484">ndividual words, as is often done in machine translation (Quirk, Brockett, and Dolan 2004). In other cases, the alignments range over entire trees (Pang, Knight, and Marcu 2003) or sentence clusters (Barzilay and Lee 2003). The obtained paraphrases are typically evaluated via human judgments. Paraphrase pairs are presented to judges who are asked to decide whether they are semantically equivalent, that is, whether they can be generally substituted for one another in the same context without great information loss (Barzilay and Lee 2003; Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003; Bannard and Callison-Burch 2005). In some cases the automatically acquired paraphrases are compared against manually generated ones (Lin and Pantel 2001) or evaluated indirectly by demonstrating performance increase for a specific application, such as machine translation (Callison-Burch, Koehn, and Osborne 2006). Unfortunately, manually evaluating paraphrases in this way has at least three drawbacks. First, it is infeasible to perform frequent evaluations when assessing incremental system changes or tuning system parameters. Second, it is difficult to replicate results presented in previous work because there is no standard </context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Bannard, Colin and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 597–604, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Bartko</author>
<author>William T Carpenter</author>
</authors>
<title>On the methods and theory of reliability.</title>
<date>1976</date>
<journal>Journal of Nervous and Mental Disease,</journal>
<volume>163</volume>
<issue>5</issue>
<contexts>
<context position="28611" citStr="Bartko and Carpenter 1976" startWordPosition="4567" endWordPosition="4570">nts are only an intermediate representation that we have used to facilitate the annotation of paraphrases. Ideally, we would like to measure agreement over the set of phrase pairs which are specified by our annotators (via the word alignments), not the alignment matrices themselves. Kupper and Hafner (1989) present an alternative measure similar to Kappa that is especiall Pr(Cj|ri). y designed for sets of variables: Cˆ = πˆ − π0 1 − π0 , (5) where �I Bi πˆ = i=1 min( Ai Bi ) and 0 = k Em ( Ai Bi ) |Ai ∩ | | |,| | , π in | |, | i 14 Kappa has been extended to more than two coders (Fleiss 1971; Bartko and Carpenter 1976). For sake our discussion and subsequent examples involve two coders. Also note that we use the term coder instead of the more common rater. This is because in our task the annotators must identify (a.k.a. code) the paraphrases rather than simplicity’s rate them. 606 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems Here, Ai and Bi are the coders’ predictions on sentence pair i from our corpus of I sentence pairs. Each prediction is a subset of the full space of k items. Expression (5) measures the agreement (or concordance) between coders A and B and follows the gen</context>
</contexts>
<marker>Bartko, Carpenter, 1976</marker>
<rawString>Bartko, John J. and William T. Carpenter. 1976. On the methods and theory of reliability. Journal of Nervous and Mental Disease, 163(5):307–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
</authors>
<title>Information Fusion for Multi-Document Summarization: Paraphrasing and Generation.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University,</institution>
<location>New York, NY.</location>
<contexts>
<context position="1018" citStr="Barzilay 2003" startWordPosition="141" endWordPosition="142">ase based on word alignments and show that it yields high inter-annotator agreement. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed in evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and F1) and also in developing linguistically rich paraphrase models based on syntactic structure. 1. Introduction The ability to paraphrase text automatically carries much practical import for many NLP applications ranging from summarization (Barzilay 2003; Zhou et al. 2006) to question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003). Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases (Bannard and Callison-Burc</context>
</contexts>
<marker>Barzilay, 2003</marker>
<rawString>Barzilay, Regina. 2003. Information Fusion for Multi-Document Summarization: Paraphrasing and Generation. Ph.D. thesis, Columbia University, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
</authors>
<title>Sentence alignment for monolingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>25--32</pages>
<location>Sapporo.</location>
<contexts>
<context position="1801" citStr="Barzilay and Elhadad 2003" startWordPosition="253" endWordPosition="256">). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003). Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases (Bannard and Callison-Burch 2005; CallisonBurch 2007) as well as comparable corpora such as collections of articles produced by two different newswire agencies about the same events (Barzilay and Elhadad 2003). Although paraphrase induction algorithms differ in many respects—for example, the acquired paraphrases often vary in granularity as they can be lexical (fighting, battle) or structural (last week’s fighting, the battle last week), and are represented as words or * School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: tcohn@inf.ed.ac.uk. ** Center for Speech and Language Processing, Johns Hopkins University, Baltimore, MD, 21218. E-mail: ccb@cs.jhu.edu. † School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk. Submission rec</context>
</contexts>
<marker>Barzilay, Elhadad, 2003</marker>
<rawString>Barzilay, Regina and Noemie Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 25–32, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiple-sequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<location>Edmonton.</location>
<contexts>
<context position="2969" citStr="Barzilay and Lee 2003" startWordPosition="418" endWordPosition="421">inburgh, UK. E-mail: mlap@inf.ed.ac.uk. Submission received: 10 September 2007; revised submission received: 8 February 2008; accepted for publication: 26 March 2008. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 4 syntax trees—they all rely on some form of alignment for extracting paraphrase pairs. In its simplest form, the alignment can range over individual words, as is often done in machine translation (Quirk, Brockett, and Dolan 2004). In other cases, the alignments range over entire trees (Pang, Knight, and Marcu 2003) or sentence clusters (Barzilay and Lee 2003). The obtained paraphrases are typically evaluated via human judgments. Paraphrase pairs are presented to judges who are asked to decide whether they are semantically equivalent, that is, whether they can be generally substituted for one another in the same context without great information loss (Barzilay and Lee 2003; Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003; Bannard and Callison-Burch 2005). In some cases the automatically acquired paraphrases are compared against manually generated ones (Lin and Pantel 2001) or evaluated indirectly by demonstrating performance increase for a </context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Barzilay, Regina and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiple-sequence alignment. In Proceedings of the Human Language Technology Conference and the Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 16–23, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathy McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>50--57</pages>
<location>Toulouse.</location>
<contexts>
<context position="1445" citStr="Barzilay and McKeown 2001" startWordPosition="201" endWordPosition="204">se models based on syntactic structure. 1. Introduction The ability to paraphrase text automatically carries much practical import for many NLP applications ranging from summarization (Barzilay 2003; Zhou et al. 2006) to question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003). Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases (Bannard and Callison-Burch 2005; CallisonBurch 2007) as well as comparable corpora such as collections of articles produced by two different newswire agencies about the same events (Barzilay and Elhadad 2003). Although paraphrase induction algorithms differ in many respects—for example, the acquired paraphrases often vary in granularity as they can be lexical (fighting, battle) or structural (last week’s fighting, the battle last week), and are rep</context>
<context position="3315" citStr="Barzilay and McKeown 2001" startWordPosition="472" endWordPosition="475">irs. In its simplest form, the alignment can range over individual words, as is often done in machine translation (Quirk, Brockett, and Dolan 2004). In other cases, the alignments range over entire trees (Pang, Knight, and Marcu 2003) or sentence clusters (Barzilay and Lee 2003). The obtained paraphrases are typically evaluated via human judgments. Paraphrase pairs are presented to judges who are asked to decide whether they are semantically equivalent, that is, whether they can be generally substituted for one another in the same context without great information loss (Barzilay and Lee 2003; Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003; Bannard and Callison-Burch 2005). In some cases the automatically acquired paraphrases are compared against manually generated ones (Lin and Pantel 2001) or evaluated indirectly by demonstrating performance increase for a specific application, such as machine translation (Callison-Burch, Koehn, and Osborne 2006). Unfortunately, manually evaluating paraphrases in this way has at least three drawbacks. First, it is infeasible to perform frequent evaluations when assessing incremental system changes or tuning system parameters. Second, it is difficult to replicate </context>
<context position="7222" citStr="Barzilay and McKeown 2001" startWordPosition="1047" endWordPosition="1050">entify and align paraphrases reliably. We measure agreement using alignment overlap measures from the SMT literature, and also introduce a novel agreement statistic for non-enumerable labeling spaces. Section 4 illustrates how the corpus can be used in paraphrase research, for example, as a test set for evaluating the output of automatic systems or as a training set for the development of paraphrase systems. Discussion of our results concludes the article. 2. Corpus Creation and Annotation Our corpus was compiled from three data sources that have been previously used for paraphrase induction (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003; Dolan, Quirk, and Brockett 2004): the Multiple-Translation Chinese (MTC) corpus, Jules Verne’s Twenty Thousand Leagues Under the Sea novel (Leagues), and the Microsoft Research (MSR) paraphrase corpus. These are monolingual parallel corpora, aligned at the sentence level. Both source and target sentences are in English, and express the same content using different surface forms. The MTC corpus contains news stories from three sources of journalistic Mandarin Chinese text.2 These stories were translated into English by 11 translation agencies. Because the majorit</context>
<context position="45619" citStr="Barzilay and McKeown 2001" startWordPosition="7356" endWordPosition="7359">c paraphrases, but our comparison would not take this into account. Despite all these considerations, we believe that comparison against our corpus would treat these systems on an equal footing against the same materials while factoring out nonessential degrees of freedom inherent in human elicitation studies (e.g., attention span, task familiarity, background). We evaluated the performance of two systems against our corpus. Our first system is simply Giza++ trained on the 55,615 sentence pairs described in Section 4. The second system uses a co-training-based paraphrase extraction algorithm (Barzilay and McKeown 2001). It was also trained on the MTC part 1 corpus, on the same data set used for Giza++, with its default parameters. For each system, we filtered the predicted paraphrases to just those which match part of a sentence pair in the test set. These paraphrases were then compared to the sure phrase pairs extracted from our manually aligned corpus. Giza++’s precision is 55% and recall 49% (see Table 5). The co-training system obtained a precision of 30% and recall of 16%. To confirm the accuracy of the precision estimate, we performed a human evaluation on a sample of 48 of the predicted paraphrases w</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Barzilay, Regina and Kathy McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 50–57, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Bikel</author>
</authors>
<title>Design of a multi-lingual, parallel-processing statistical parsing engine.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<pages>24--27</pages>
<location>San Diego, CA.</location>
<marker>Bikel, 2002</marker>
<rawString>Bikel, Daniel. 2002. Design of a multi-lingual, parallel-processing statistical parsing engine. In Proceedings of the Human Language Technology Conference, pages 24–27, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della-Pietra</author>
<author>Vincent J Della-Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="10120" citStr="Brown et al. 1993" startWordPosition="1477" endWordPosition="1480">airs, 300 of which are doubly annotated. To speed up the annotation process, the data sources were first aligned automatically and then hand-corrected. We used Giza++ (Och and Ney 2003), a publicly available 2 The corpus is made available by the LDC, Catalog Number LDC2002T01, ISBN 1-58563-217-1. 3 The corpus can be downloaded from http://www.isi.edu/∼knight/. 4 The corpus is available at http://research.microsoft.com/research/downloads/Details/607D14D9- 20CD-47E3-85BC-A2F65CD28042/Details.aspx. 599 Computational Linguistics Volume 34, Number 4 implementation of the IBM word alignment models (Brown et al. 1993). Giza++ was trained on the full 993-sentence MTC part1 corpus5 using all 11 translators and all pairings of English translations as training instances. This resulted in 55 = 11·(11−1) 2 training pairs per sentence and a total of 54,615 training pairs. In addition, we augmented the training data with a word-identity lexicon, as proposed by Quirk, Brockett, and Dolan (2004). This follows standard practice in SMT where entries from a bilingual dictionary are added to the training set (Och and Ney 2000a), except in our case the “dictionary” is monolingual and specifies that each word type can be </context>
</contexts>
<marker>Brown, Della-Pietra, Della-Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen A. Della-Pietra, Vincent J. Della-Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing and Translation.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh,</institution>
<location>Edinburgh, Scotland.</location>
<marker>Callison-Burch, 2007</marker>
<rawString>Callison-Burch, Chris. 2007. Paraphrasing and Translation. Ph.D. thesis, University of Edinburgh, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>17--24</pages>
<location>New York, NY.</location>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Callison-Burch, Chris, Philipp Koehn, and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In Proceedings of the Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 17–24, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="25788" citStr="Cohen 1960" startWordPosition="4085" endWordPosition="4086">haustive). 12 Taking a more conservative position by limiting the proportion of unaligned words within the phrase pair improves these figures monotonically to 90% correct and 0% incorrect (fully aligned phrase pairs). 13 This Kappa is computed over three nominal categories (correct, incorrect, and uncertain) and should not be confused with the agreement measure we develop in the following section for phrase pairs. 605 Computational Linguistics Volume 34, Number 4 Chance-Corrected Agreement. Besides precision and recall, inter-annotator agreement is commonly measured using the Kappa statistic (Cohen 1960). Thus is a desirable measure because it is adjusted for agreement due purely to chance: Pr(A) − Pr(E) κ = (4) 1 − Pr(E) where Pr(A) is the proportion of times two coders14 agree, corrected by Pr(E), the proportion of times we would expect them to agree by chance. Kappa is a suitable agreement measure for nominal data. An example would be a classification task, where two coders must assign n linguistic instances (e.g., sentences or words) into one of m categories. Given this situation, it would be possible for each coder to assign each instance to the same category. Kappa allows us to quantify</context>
<context position="27125" citStr="Cohen 1960" startWordPosition="4309" endWordPosition="4310">estimate Pr(A)—it is the proportion of instances on which the two coders agree. Pr(E) requires a model of what would happen if the coders were to assign categories randomly. Under the assumption that coders r1 and r2 are independent, the chance of them agreeing on the jth category is the product of each of them assigning an instance to that category: Pr(Cj|r1) Pr(Cj|r2). Chance agreement is then the sum of this product across all categories: Pr(E) = j=1 Pr(Cj|r1)Pr(Cj|r2). The literature describes two different methods for estimating Either a separate distribution is estimated for each coder (Cohen 1960) or the same distribution for all coders (Scott 1955; Fleiss 1971; Siegel and Castellan 1988). We refer the interested reader to Di Eugenio and Glass (2004) and Artstein and Poesio (2008) for a more detailed discussion. Unfortunately, Kappa is not universally suited to every categorization task. A prime example is structured labeling problems that allow a wide variety of output categories. Importantly, the number and type of categories is not fixed in advance and can vary from instance to instance. In parsing, annotators are given a sentence for which they must specify a tree, of which there i</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Cohen, J. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Large margin synchronous generation and its application to sentence compression.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing and on Computational Natural Language Learning,</booktitle>
<pages>73--82</pages>
<location>Prague.</location>
<contexts>
<context position="42180" citStr="Cohn and Lapata (2007)" startWordPosition="6833" endWordPosition="6836">ic analysis of paraphrases, as a training set for the development of discriminative paraphrase systems, and as a test set for the automatic evaluation of computational models. Here, we briefly demonstrate some of these uses. Paraphrase Modeling. Much previous research has focused on lexical paraphrases (but see Lin and Pantel [2001] and Pang, Knight, and Marcu [2003] for exceptions). We argue that our corpus should support a richer range of structural (syntactic) paraphrases. To demonstrate this we have extracted paraphrase rules from our annotations using the grammar induction algorithm from Cohn and Lapata (2007). Briefly, the algorithm extracts tree pairs from word-aligned text by choosing aligned constituents in a pair of equivalent sentences. These pairs are then generalized by factoring out aligned subtrees, thereby resulting in synchronous grammar rules (Aho and Ullman 1969) with variable nodes. We parsed the MTC corpus with Bikel’s (2002) parser and extracted synchronous rules from the gold-standard alignments. A sample of these rules are shown in Figure 5. Here we see three lexical paraphrases, followed by five structural paraphrases. In example 4, also is replaced with moreover and is moved to</context>
<context position="43533" citStr="Cohn and Lapata 2007" startWordPosition="7040" endWordPosition="7043">dicate correspondences between non-terminals in the two sides of the rules. The synchronous rules in Figure 5 provide insight into the process of paraphrasing at the syntactic level, and also a practical means for developing algorithms for paraphrase generation—a task which has received little attention to date. For instance, we could envisage a paraphrase model that transforms parse trees of an input sentence into parse trees that represent a sentential paraphrase of that sentence. Our corpus can be used to learn this mapping using discriminative methods (Cowan, Ku˘cerov´a, and Collins 2006; Cohn and Lapata 2007). Evaluation Set. As mentioned in Section 1, it is currently difficult to compare competing approaches due to the effort involved in eliciting manual judgments of paraphrase output. Our corpus could fill the role of a gold-standard test set, allowing for automatic evaluation techniques. Developing measures for automatic paraphrase evaluation is outside the scope of this article. Nevertheless, we illustrate how the corpus can be used for this purpose. For example we could easily measure the precision and recall of an automatic system Figure 5 Synchronous grammar rules extracted from the MTC cor</context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>Cohn, Trevor and Mirella Lapata. 2007. Large margin synchronous generation and its application to sentence compression. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and on Computational Natural Language Learning, pages 73–82, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brooke Cowan</author>
<author>Ivona Ku˘cerov´a</author>
<author>Michael Collins</author>
</authors>
<title>A discriminative model for tree-to-tree translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>232--241</pages>
<location>Sydney.</location>
<marker>Cowan, Ku˘cerov´a, Collins, 2006</marker>
<rawString>Cowan, Brooke, Ivona Ku˘cerov´a, and Michael Collins. 2006. A discriminative model for tree-to-tree translation. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 232–241, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daum´e Hal</author>
<author>Daniel Marcu</author>
</authors>
<title>A phrase-based HMM approach to document/abstract alignment.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>119--126</pages>
<location>Barcelona.</location>
<marker>Hal, Marcu, 2004</marker>
<rawString>Daum´e III, Hal and Daniel Marcu. 2004. A phrase-based HMM approach to document/abstract alignment. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 119–126, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Michael Glass</author>
</authors>
<title>The kappa statistic: A second look.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<marker>Di Eugenio, Glass, 2004</marker>
<rawString>Di Eugenio, Barbara and Michael Glass. 2004. The kappa statistic: A second look. Computational Linguistics, 30(1):95–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>350--356</pages>
<location>Geneva.</location>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Dolan, William, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th International Conference on Computational Linguistics, pages 350–356, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Duboue</author>
<author>Jennifer Chu-Carroll</author>
</authors>
<title>Answering the question you wish they had asked: The impact of paraphrasing for question answering.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>33--36</pages>
<location>New York, NY.</location>
<contexts>
<context position="1110" citStr="Duboue and Chu-Carroll 2006" startWordPosition="154" endWordPosition="157">eement. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed in evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and F1) and also in developing linguistically rich paraphrase models based on syntactic structure. 1. Introduction The ability to paraphrase text automatically carries much practical import for many NLP applications ranging from summarization (Barzilay 2003; Zhou et al. 2006) to question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003). Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases (Bannard and Callison-Burch 2005; CallisonBurch 2007) as well as comparable corpora such as collections of articles pr</context>
</contexts>
<marker>Duboue, Chu-Carroll, 2006</marker>
<rawString>Duboue, Pablo and Jennifer Chu-Carroll. 2006. Answering the question you wish they had asked: The impact of paraphrasing for question answering. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 33–36, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="27190" citStr="Fleiss 1971" startWordPosition="4320" endWordPosition="4321"> coders agree. Pr(E) requires a model of what would happen if the coders were to assign categories randomly. Under the assumption that coders r1 and r2 are independent, the chance of them agreeing on the jth category is the product of each of them assigning an instance to that category: Pr(Cj|r1) Pr(Cj|r2). Chance agreement is then the sum of this product across all categories: Pr(E) = j=1 Pr(Cj|r1)Pr(Cj|r2). The literature describes two different methods for estimating Either a separate distribution is estimated for each coder (Cohen 1960) or the same distribution for all coders (Scott 1955; Fleiss 1971; Siegel and Castellan 1988). We refer the interested reader to Di Eugenio and Glass (2004) and Artstein and Poesio (2008) for a more detailed discussion. Unfortunately, Kappa is not universally suited to every categorization task. A prime example is structured labeling problems that allow a wide variety of output categories. Importantly, the number and type of categories is not fixed in advance and can vary from instance to instance. In parsing, annotators are given a sentence for which they must specify a tree, of which there is an exponential number in the sentence length. Similarly, in our</context>
<context position="28583" citStr="Fleiss 1971" startWordPosition="4565" endWordPosition="4566">ides, alignments are only an intermediate representation that we have used to facilitate the annotation of paraphrases. Ideally, we would like to measure agreement over the set of phrase pairs which are specified by our annotators (via the word alignments), not the alignment matrices themselves. Kupper and Hafner (1989) present an alternative measure similar to Kappa that is especiall Pr(Cj|ri). y designed for sets of variables: Cˆ = πˆ − π0 1 − π0 , (5) where �I Bi πˆ = i=1 min( Ai Bi ) and 0 = k Em ( Ai Bi ) |Ai ∩ | | |,| | , π in | |, | i 14 Kappa has been extended to more than two coders (Fleiss 1971; Bartko and Carpenter 1976). For sake our discussion and subsequent examples involve two coders. Also note that we use the term coder instead of the more common rater. This is because in our task the annotators must identify (a.k.a. code) the paraphrases rather than simplicity’s rate them. 606 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems Here, Ai and Bi are the coders’ predictions on sentence pair i from our corpus of I sentence pairs. Each prediction is a subset of the full space of k items. Expression (5) measures the agreement (or concordance) between coders</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Fleiss, Joseph L. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Measuring word alignment quality for statistical machine translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="17425" citStr="Fraser and Marcu (2007)" startWordPosition="2627" endWordPosition="2630">red to reference alignment B can be then computed using standard recall, precision, and F1 measures (Och and Ney 2003): AS ∩ BP ||AP ∩ BS|= 2 · Precision · Recall Precision = AS |Recall = |BS |F1 Precision + Recall (2) where the subscripts S and P denote sure and possible word alignments, respectively. Note that both precision and recall are asymmetric in that they compare sets of possible and sure alignments. This is designed to be maximally generous: sure predictions which are present in the reference as possibles are not penalized in precision, and the converse applies for recall. We adopt Fraser and Marcu (2007)’s definition of F1, an F-measure between precision and recall over the sure and possibles. They argue that it is a better alternative to the commonly used Alignment Error Rate (AER), which does not sufficiently penalize unbalanced precision and recall.9 As our corpus is monolingual, in order to avoid artificial score inflation, we limit the precision and recall calculations to consider only pairs of non-identical words (and phrases, as discussed subsequently). To give an example, consider the sentence pairs in Figure 2, whose alignments have been produced by the two annotators A (left) and B </context>
<context position="19053" citStr="Fraser and Marcu (2007)" startWordPosition="2901" endWordPosition="2904">ments over identical words (i.e., discussed ↔ discussed, the ↔ the, and ↔ and, . ↔ .). Phrase-Based Measures. The given definitions are all word-based; however, our annotators, and several paraphrasing models, create correspondences not only between words but also between phrases. To take this into account, we also evaluate these measures over larger blocks (similar to Ayan and Dorr [2006]). Specifically, we extract phrase pairs from the alignments produced by our annotators using a modified version of the standard SMT phrase extraction heuristic (Och, Tillmann, and Ney 1999). The heuristic 9 Fraser and Marcu (2007) also argue for an unbalanced F-measure to bias towards recall. This is shown to correlate better with translation quality. For paraphrasing it is not clear if such a bias would be beneficial. 602 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems Figure 2 Sample sentence pair showing the word alignments from two annotators. extracts all phrase pairs consistent with the word alignment. These include phrase pairs whose words are aligned to each other or nothing, but not to words outside the phrase boundaries.10 The phrase extraction heuristic creates masses of phrase p</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Fraser, Alexander and Daniel Marcu. 2007. Measuring word alignment quality for statistical machine translation. Computational Linguistics, 33(3):293–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>48--54</pages>
<location>Edmonton.</location>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 48–54, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence L Kupper</author>
<author>Kerry B Hafner</author>
</authors>
<title>On assessing interrater agreement for multiple attribute responses.</title>
<date>1989</date>
<journal>Biometrics,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="28293" citStr="Kupper and Hafner (1989)" startWordPosition="4492" endWordPosition="4495">nce for which they must specify a tree, of which there is an exponential number in the sentence length. Similarly, in our case the space of possible alignments for a sentence pair is also exponential in the input sentence lengths. Considering these annotations as nominal variables is inappropriate. Besides, alignments are only an intermediate representation that we have used to facilitate the annotation of paraphrases. Ideally, we would like to measure agreement over the set of phrase pairs which are specified by our annotators (via the word alignments), not the alignment matrices themselves. Kupper and Hafner (1989) present an alternative measure similar to Kappa that is especiall Pr(Cj|ri). y designed for sets of variables: Cˆ = πˆ − π0 1 − π0 , (5) where �I Bi πˆ = i=1 min( Ai Bi ) and 0 = k Em ( Ai Bi ) |Ai ∩ | | |,| | , π in | |, | i 14 Kappa has been extended to more than two coders (Fleiss 1971; Bartko and Carpenter 1976). For sake our discussion and subsequent examples involve two coders. Also note that we use the term coder instead of the more common rater. This is because in our task the annotators must identify (a.k.a. code) the paraphrases rather than simplicity’s rate them. 606 Cohn, Callison</context>
<context position="30887" citStr="Kupper and Hafner (1989)" startWordPosition="4944" endWordPosition="4947">ntence to be aligned to either zero or one span in the other sentence; that is, nominating a phrase pair precludes the choice of many other possible phrase pairs. Consequently relatively few of the subsets of the full set of possible phrase pairs are valid. Formally, an alignment can specify only O(N2) phrase pairs from a total set of k = O(N4) possible phrase pairs. This disparity in magnitudes leads to increasingly underestimated πˆ for larger N, namely, limN→∞ π0 = limN→∞ O(N2)/O(N4) = 0. The end result is an overestimate of Cˆ on longer sentences. For these reasons, we adapt the method of Kupper and Hafner (1989) to account for our highly interdependent item sets. We use Cˆ from Equation (5) as our agreement statistic defined over sets of atomic phrase pairs, that is, A = Apatom, B = Bpatom. We redefine π0 as follows: π0 = 1I I E E atom ∩ Bp i=1 Ap Bp Pr(Ap atom) Pr(Bp atom) |Ap atom |(6) atom atom min(|Ap atom|, |Bp atom|) where Apatom and Bpatom range over the sets of atomic phrase pairs licensed by sentence pair i, and Pr(Apatom) and Pr(Baptom) are priors over these sets for each annotator. A consequence of dropping the independence assumptions is that calculating π0 is considerably more difficult.</context>
</contexts>
<marker>Kupper, Hafner, 1989</marker>
<rawString>Kupper, Lawrence L. and Kerry B. Hafner. 1989. On assessing interrater agreement for multiple attribute responses. Biometrics, 45(3):957–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="1080" citStr="Lin and Pantel 2001" startWordPosition="150" endWordPosition="153">h inter-annotator agreement. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed in evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and F1) and also in developing linguistically rich paraphrase models based on syntactic structure. 1. Introduction The ability to paraphrase text automatically carries much practical import for many NLP applications ranging from summarization (Barzilay 2003; Zhou et al. 2006) to question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003). Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases (Bannard and Callison-Burch 2005; CallisonBurch 2007) as well as comparable corpora such</context>
<context position="3500" citStr="Lin and Pantel 2001" startWordPosition="499" endWordPosition="502"> entire trees (Pang, Knight, and Marcu 2003) or sentence clusters (Barzilay and Lee 2003). The obtained paraphrases are typically evaluated via human judgments. Paraphrase pairs are presented to judges who are asked to decide whether they are semantically equivalent, that is, whether they can be generally substituted for one another in the same context without great information loss (Barzilay and Lee 2003; Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003; Bannard and Callison-Burch 2005). In some cases the automatically acquired paraphrases are compared against manually generated ones (Lin and Pantel 2001) or evaluated indirectly by demonstrating performance increase for a specific application, such as machine translation (Callison-Burch, Koehn, and Osborne 2006). Unfortunately, manually evaluating paraphrases in this way has at least three drawbacks. First, it is infeasible to perform frequent evaluations when assessing incremental system changes or tuning system parameters. Second, it is difficult to replicate results presented in previous work because there is no standard corpus, and no standard evaluation methodology. Consequently comparisons across systems are few and far between. The thir</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Lin, Dekang and Patrick Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 7(4):342–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Martin</author>
<author>Rada Mihalcea</author>
<author>Ted Pedersen</author>
</authors>
<title>Word alignment for languages with scarce resources.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>67--74</pages>
<location>Ann Arbor, MI.</location>
<marker>Martin, Mihalcea, Pedersen, 2005</marker>
<rawString>Martin, Joel, Rada Mihalcea, and Ted Pedersen. 2005. Word alignment for languages with scarce resources. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 67–74, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Manual annotation of translational equivalence: The Blinker project.</title>
<date>1998</date>
<tech>IRCS Technical Report #98-07,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="5786" citStr="Melamed 1998" startWordPosition="835" endWordPosition="836">ther definitions are possible, for instance we could have asked our annotators to identify all constituents that are more or less meaning preserving in our parallel corpus. We chose to work with alignments for two reasons. First, the notion of alignment appears to be central in paraphrasing—most existing paraphrase induction algorithms rely on alignments either implicitly or explicitly for identifying paraphrase units. Secondly, research in machine translation, where several gold-standard alignment corpora have been created, shows that word alignments can be identified reliably by annotators (Melamed 1998; Och and Ney 2000b; Mihalcea and Pedersen 2003; Martin, Mihalcea, and Pedersen 2005). We therefore create word alignments similar to those observed in machine translation, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links between words. Alignment blocks larger than one-to-one are used to specify phrase correspondences. 1 Our definition of the term phrase follows the SMT literature. It refers to any contiguous sequence of words, whether it is a syntactic constituent or not. See Section 2 for details. 598 Cohn, Callison-Burch, and Lapata Constructing Corpora for Par</context>
</contexts>
<marker>Melamed, 1998</marker>
<rawString>Melamed, I. Dan. 1998. Manual annotation of translational equivalence: The Blinker project. IRCS Technical Report #98-07, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ted Pedersen</author>
</authors>
<title>An evaluation exercise for word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,</booktitle>
<pages>1--6</pages>
<location>Edmonton.</location>
<contexts>
<context position="5833" citStr="Mihalcea and Pedersen 2003" startWordPosition="841" endWordPosition="844">r instance we could have asked our annotators to identify all constituents that are more or less meaning preserving in our parallel corpus. We chose to work with alignments for two reasons. First, the notion of alignment appears to be central in paraphrasing—most existing paraphrase induction algorithms rely on alignments either implicitly or explicitly for identifying paraphrase units. Secondly, research in machine translation, where several gold-standard alignment corpora have been created, shows that word alignments can be identified reliably by annotators (Melamed 1998; Och and Ney 2000b; Mihalcea and Pedersen 2003; Martin, Mihalcea, and Pedersen 2005). We therefore create word alignments similar to those observed in machine translation, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links between words. Alignment blocks larger than one-to-one are used to specify phrase correspondences. 1 Our definition of the term phrase follows the SMT literature. It refers to any contiguous sequence of words, whether it is a syntactic constituent or not. See Section 2 for details. 598 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems In the following section we exp</context>
</contexts>
<marker>Mihalcea, Pedersen, 2003</marker>
<rawString>Mihalcea, Rada and Ted Pedersen. 2003. An evaluation exercise for word alignment. In Proceedings of the HLT-NAACL Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond, pages 1–6, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>1086--1090</pages>
<contexts>
<context position="5804" citStr="Och and Ney 2000" startWordPosition="837" endWordPosition="840">ns are possible, for instance we could have asked our annotators to identify all constituents that are more or less meaning preserving in our parallel corpus. We chose to work with alignments for two reasons. First, the notion of alignment appears to be central in paraphrasing—most existing paraphrase induction algorithms rely on alignments either implicitly or explicitly for identifying paraphrase units. Secondly, research in machine translation, where several gold-standard alignment corpora have been created, shows that word alignments can be identified reliably by annotators (Melamed 1998; Och and Ney 2000b; Mihalcea and Pedersen 2003; Martin, Mihalcea, and Pedersen 2005). We therefore create word alignments similar to those observed in machine translation, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links between words. Alignment blocks larger than one-to-one are used to specify phrase correspondences. 1 Our definition of the term phrase follows the SMT literature. It refers to any contiguous sequence of words, whether it is a syntactic constituent or not. See Section 2 for details. 598 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems In</context>
<context position="10624" citStr="Och and Ney 2000" startWordPosition="1560" endWordPosition="1563">omputational Linguistics Volume 34, Number 4 implementation of the IBM word alignment models (Brown et al. 1993). Giza++ was trained on the full 993-sentence MTC part1 corpus5 using all 11 translators and all pairings of English translations as training instances. This resulted in 55 = 11·(11−1) 2 training pairs per sentence and a total of 54,615 training pairs. In addition, we augmented the training data with a word-identity lexicon, as proposed by Quirk, Brockett, and Dolan (2004). This follows standard practice in SMT where entries from a bilingual dictionary are added to the training set (Och and Ney 2000a), except in our case the “dictionary” is monolingual and specifies that each word type can be paraphrased as itself. This is necessary in order to inform Giza++ about word identity. A common problem with automatic word alignments is that they are asymmetric: one source word can only be aligned to one target word, whereas one target word can be aligned to multiple source words. In SMT, word alignments are typically predicted in both directions: source-to-target and target-to-source. These two alignments are then merged (symmetrized) to produce the final alignment (Koehn, Och, and Marcu 2003).</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2000a. A comparison of alignment models for statistical machine translation. In Proceedings of the 18th International Conference on Computational Linguistics, pages 1086–1090, Saarbr¨ucken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th</booktitle>
<contexts>
<context position="5804" citStr="Och and Ney 2000" startWordPosition="837" endWordPosition="840">ns are possible, for instance we could have asked our annotators to identify all constituents that are more or less meaning preserving in our parallel corpus. We chose to work with alignments for two reasons. First, the notion of alignment appears to be central in paraphrasing—most existing paraphrase induction algorithms rely on alignments either implicitly or explicitly for identifying paraphrase units. Secondly, research in machine translation, where several gold-standard alignment corpora have been created, shows that word alignments can be identified reliably by annotators (Melamed 1998; Och and Ney 2000b; Mihalcea and Pedersen 2003; Martin, Mihalcea, and Pedersen 2005). We therefore create word alignments similar to those observed in machine translation, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links between words. Alignment blocks larger than one-to-one are used to specify phrase correspondences. 1 Our definition of the term phrase follows the SMT literature. It refers to any contiguous sequence of words, whether it is a syntactic constituent or not. See Section 2 for details. 598 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems In</context>
<context position="10624" citStr="Och and Ney 2000" startWordPosition="1560" endWordPosition="1563">omputational Linguistics Volume 34, Number 4 implementation of the IBM word alignment models (Brown et al. 1993). Giza++ was trained on the full 993-sentence MTC part1 corpus5 using all 11 translators and all pairings of English translations as training instances. This resulted in 55 = 11·(11−1) 2 training pairs per sentence and a total of 54,615 training pairs. In addition, we augmented the training data with a word-identity lexicon, as proposed by Quirk, Brockett, and Dolan (2004). This follows standard practice in SMT where entries from a bilingual dictionary are added to the training set (Och and Ney 2000a), except in our case the “dictionary” is monolingual and specifies that each word type can be paraphrased as itself. This is necessary in order to inform Giza++ about word identity. A common problem with automatic word alignments is that they are asymmetric: one source word can only be aligned to one target word, whereas one target word can be aligned to multiple source words. In SMT, word alignments are typically predicted in both directions: source-to-target and target-to-source. These two alignments are then merged (symmetrized) to produce the final alignment (Koehn, Och, and Marcu 2003).</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2000b. Improved statistical alignment models. In Proceedings of the 38th</rawString>
</citation>
<citation valid="false">
<title>Annual Meeting of the Association for Computational Linguistics,</title>
<pages>440--447</pages>
<location>Hong Kong.</location>
<marker></marker>
<rawString>Annual Meeting of the Association for Computational Linguistics, pages 440–447, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="9687" citStr="Och and Ney 2003" startWordPosition="1426" endWordPosition="1429">were split into two distinct sets, each consisting of 300 sentences (100 per corpus), and were annotated by a single coder. Each coder annotated the same amount of data. In addition, we obtained a trial set of 50 sentences from the MTC corpus which was used for familiarizing our annotators with the paraphrase alignment task (this set does not form part of the corpus). In sum, we obtained paraphrase annotations for 900 sentence pairs, 300 of which are doubly annotated. To speed up the annotation process, the data sources were first aligned automatically and then hand-corrected. We used Giza++ (Och and Ney 2003), a publicly available 2 The corpus is made available by the LDC, Catalog Number LDC2002T01, ISBN 1-58563-217-1. 3 The corpus can be downloaded from http://www.isi.edu/∼knight/. 4 The corpus is available at http://research.microsoft.com/research/downloads/Details/607D14D9- 20CD-47E3-85BC-A2F65CD28042/Details.aspx. 599 Computational Linguistics Volume 34, Number 4 implementation of the IBM word alignment models (Brown et al. 1993). Giza++ was trained on the full 993-sentence MTC part1 corpus5 using all 11 translators and all pairings of English translations as training instances. This resulted </context>
<context position="11879" citStr="Och and Ney 2003" startWordPosition="1753" endWordPosition="1756"> quality compared to that of a single directional model, while also allowing a greater range of alignment types (i.e., some many-toone, one-to-many, and many-to-many alignments can be produced). Analogously, we obtained word alignments in both directions6 which we subsequently merged by taking their intersection. This resulted in a high precision and low recall alignment. Our annotators (two linguistics graduates) were given pairs of sentences and asked to show which parts of these were in correspondence by aligning them on a word-byword basis.7 Our definition of alignment was fairly general (Och and Ney 2003): Given a source string X = x1, ... , xN and a target string Y = y1, ... , yM, an alignment A between two word strings is the subset of the Cartesian product of the word positions: A C {(n,m) : n = 1,...,N;m = 1,...,M} (1) We did not provide a formal definition of what constitutes a correspondence. As a rule of thumb, annotators were told to align words or phrases x ↔ y in two sentences (X, Y) whenever the words x could be substituted for y in Y, or vice versa. This relationship should hold within the context of the sentence pair in question: the relation x ↔ y need not hold in general context</context>
<context position="16920" citStr="Och and Ney 2003" startWordPosition="2540" endWordPosition="2543">ce pairs (100 pairs from each subcorpus) were doubly annotated, in order to measure inter-annotator agreement. Here, we treat one annotator as gold-standard (reference) and measure the extent to which the other annotator deviates from this reference. Word-Based Measures. The standard technique for evaluating word alignments is to represent them as a set of links (i.e., pairs of words) and compare them against goldstandard alignments. The quality of an alignment A (defined in Equation (1)) compared to reference alignment B can be then computed using standard recall, precision, and F1 measures (Och and Ney 2003): AS ∩ BP ||AP ∩ BS|= 2 · Precision · Recall Precision = AS |Recall = |BS |F1 Precision + Recall (2) where the subscripts S and P denote sure and possible word alignments, respectively. Note that both precision and recall are asymmetric in that they compare sets of possible and sure alignments. This is designed to be maximally generous: sure predictions which are present in the reference as possibles are not penalized in precision, and the converse applies for recall. We adopt Fraser and Marcu (2007)’s definition of F1, an F-measure between precision and recall over the sure and possibles. The</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<location>College Park, MD.</location>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Och, Franz Josef, Christoph Tillmann, and Hermann Ney. 1999. Improved alignment models for statistical machine translation. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20–28, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>181--188</pages>
<location>Edmonton.</location>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>Pang, Bo, Kevin Knight, and Daniel Marcu. 2003. Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences. In Proceedings of the Human Language Technology Conference and the Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 181–188, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>142--149</pages>
<location>Barcelona.</location>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Quirk, Chris, Chris Brockett, and William Dolan. 2004. Monolingual machine translation for paraphrase generation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 142–149, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Scott</author>
</authors>
<title>Reliability of content analysis: The case of nominal scale. Public Opinion Quarterly,</title>
<date>1955</date>
<contexts>
<context position="27177" citStr="Scott 1955" startWordPosition="4318" endWordPosition="4319">hich the two coders agree. Pr(E) requires a model of what would happen if the coders were to assign categories randomly. Under the assumption that coders r1 and r2 are independent, the chance of them agreeing on the jth category is the product of each of them assigning an instance to that category: Pr(Cj|r1) Pr(Cj|r2). Chance agreement is then the sum of this product across all categories: Pr(E) = j=1 Pr(Cj|r1)Pr(Cj|r2). The literature describes two different methods for estimating Either a separate distribution is estimated for each coder (Cohen 1960) or the same distribution for all coders (Scott 1955; Fleiss 1971; Siegel and Castellan 1988). We refer the interested reader to Di Eugenio and Glass (2004) and Artstein and Poesio (2008) for a more detailed discussion. Unfortunately, Kappa is not universally suited to every categorization task. A prime example is structured labeling problems that allow a wide variety of output categories. Importantly, the number and type of categories is not fixed in advance and can vary from instance to instance. In parsing, annotators are given a sentence for which they must specify a tree, of which there is an exponential number in the sentence length. Simi</context>
</contexts>
<marker>Scott, 1955</marker>
<rawString>Scott, William A. 1955. Reliability of content analysis: The case of nominal scale. Public Opinion Quarterly, 19:127–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
<author>N John Castellan</author>
</authors>
<title>Non Parametric Statistics for the Behavioral Sciences.</title>
<date>1988</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="27218" citStr="Siegel and Castellan 1988" startWordPosition="4322" endWordPosition="4325">. Pr(E) requires a model of what would happen if the coders were to assign categories randomly. Under the assumption that coders r1 and r2 are independent, the chance of them agreeing on the jth category is the product of each of them assigning an instance to that category: Pr(Cj|r1) Pr(Cj|r2). Chance agreement is then the sum of this product across all categories: Pr(E) = j=1 Pr(Cj|r1)Pr(Cj|r2). The literature describes two different methods for estimating Either a separate distribution is estimated for each coder (Cohen 1960) or the same distribution for all coders (Scott 1955; Fleiss 1971; Siegel and Castellan 1988). We refer the interested reader to Di Eugenio and Glass (2004) and Artstein and Poesio (2008) for a more detailed discussion. Unfortunately, Kappa is not universally suited to every categorization task. A prime example is structured labeling problems that allow a wide variety of output categories. Importantly, the number and type of categories is not fixed in advance and can vary from instance to instance. In parsing, annotators are given a sentence for which they must specify a tree, of which there is an exponential number in the sentence length. Similarly, in our case the space of possible </context>
</contexts>
<marker>Siegel, Castellan, 1988</marker>
<rawString>Siegel, Sidney and N. John Castellan. 1988. Non Parametric Statistics for the Behavioral Sciences. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Chin-Yew Lin</author>
<author>Dragos Stefan Munteanu</author>
<author>Eduard Hovy</author>
</authors>
<title>Paraeval: Using paraphrases to evaluate summaries automatically.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<pages>447--454</pages>
<location>New York, NY.</location>
<contexts>
<context position="1037" citStr="Zhou et al. 2006" startWordPosition="143" endWordPosition="146">rd alignments and show that it yields high inter-annotator agreement. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed in evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and F1) and also in developing linguistically rich paraphrase models based on syntactic structure. 1. Introduction The ability to paraphrase text automatically carries much practical import for many NLP applications ranging from summarization (Barzilay 2003; Zhou et al. 2006) to question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003). Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases (Bannard and Callison-Burch 2005; CallisonBur</context>
</contexts>
<marker>Zhou, Lin, Munteanu, Hovy, 2006</marker>
<rawString>Zhou, Liang, Chin-Yew Lin, Dragos Stefan Munteanu, and Eduard Hovy. 2006. Paraeval: Using paraphrases to evaluate summaries automatically. In Proceedings of the Human Language Technology Conference, pages 447–454, New York, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>