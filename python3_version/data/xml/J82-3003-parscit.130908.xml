<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999038">
Using Semantics in Non-Context-Free Parsing
of Montague Grammarl
</title>
<author confidence="0.822461">
David Scott Warren
</author>
<affiliation confidence="0.79366">
Department of Computer Science
</affiliation>
<author confidence="0.684622">
SUNY at Stony Brook
Long Island, NY 11794
Joyce Friedman
</author>
<affiliation confidence="0.943333">
University of Michigan
</affiliation>
<author confidence="0.315502">
Ann Arbor, MI
</author>
<bodyText confidence="0.9995787">
In natural language processing, the question of the appropriate interaction of syntax
and semantics during sentence analysis has long been of interest. Montague grammar with
its fully formalized syntax and semantics provides a complete, well-defined context in which
these questions can be considered. This paper describes how semantics can be used during
parsing to reduce the combinatorial explosion of syntactic ambiguity in Montague grammar.
A parsing algorithm, called semantic equivalence parsing, is presented and examples of its
operation are given. The algorithm is applicable to general non-context-free grammars
that include a formal semantic component. The second portion of the paper places
semantic equivalence parsing in the context of the very general definition of an interpreted
language as a homomorphism between syntactic and semantic algebras (Montague 1970).
</bodyText>
<sectionHeader confidence="0.834123" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999691375">
The close interrelation between syntax and seman-
tics in Montague grammar provides a good framework
in which to consider the interaction of syntax and
semantics in sentence analysis. Several different ap-
proaches are possible in this framework and they can
be developed rigorously for comparison. In this paper
we develop an approach called semantic equivalence
parsing that introduces logical translation into the on-
going parsing process. We compare this with our ear-
lier directed process implementation in which syntactic
parsing is completed prior to translation to logical
form.
Part I of the paper gives an algorithm that parses a
class of grammars that contains both essentially
context-free rules and non-context-free rules as in
Montague&apos;s 1973 PTQ. Underlying this algorithm is a
</bodyText>
<footnote confidence="0.79880775">
1 A preliminary version of this paper was presented at the
symposium on Modelling Human Parsing Strategies at the Universi-
ty of Texas at Austin, March 24-26, 1981. The work of the first
author was supported in part by NSF grant 1ST 80-10834.
</footnote>
<bodyText confidence="0.95134444">
nondeterministic syntactic program expressed as an
ATN. The algorithm introduces equivalence parsing,
which is a general execution method for nondetermin-
istic programs that is based on a recall table, a gener-
alization of the well-formed substring table. Semantic
equivalence, based on logical equivalence of formulas
obtained as translations, is used. We discuss the con-
sequences of incorporating semantic processing into
the parser and give examples of both syntactic and
semantic parsing. In Part II the semantic parsing al-
gorithm is related to earlier tabular context-free recog-
nition methods. Relating our algorithm to its prede-
cessors gives a new way of viewing the technique.
The algorithmic description is then replaced by a de-
scription in terms of refined grammars. Finally we
suggest how this notion might be generalized to the
full class of Montague grammars.
The particular version of Montague grammar used
here is that of PTQ, with which the reader is assumed
to be conversant. The syntactic component of PTQ is
an essentially context-free grammar, augmented by
some additional rules of a different form. The non-
Copyright 1982 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted
provided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included on
the first page. To copy otherwise, or to republish, requires a fee and/or specific permission.
</bodyText>
<page confidence="0.40105">
0362-613X/82/030123-16$03.00
</page>
<note confidence="0.9197195">
American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 123
David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing
</note>
<bodyText confidence="0.999871722222222">
context-free aspects arise in the treatment of quantifi-
er scope and pronouns and their antecedents. Syntac-
tically each antecedent is regarded as substituted into
a place marked by a variable. This is not unlike the
way fillers are inserted into gaps in Gazdar&apos;s 1979
treatment. However, Montague&apos;s use of variables
allows complicated interactions between different
variable-antecedent pairs. Each substitution rule sub-
stitutes a term phrase (NP) for one or more occurrenc-
es of a free variable in a phrase (which may be a sen-
tence, common noun phrase, or intransitive verb
phrase). The first occurrence of the variable is re-
placed by the phrase; later occurrences are replaced by
appropriate pronouns. The translation of the resulting
phrase expresses the coreferentiality of the noun
phrase and the pronouns. With substitution, but with-
out pronouns, the only function of substitution is to
determine quantifier scope.
</bodyText>
<subsectionHeader confidence="0.901208">
Directed Process Approach
</subsectionHeader>
<bodyText confidence="0.999935714285714">
One computational approach to processing a sen-
tence is the directed process approach, which is a se-
quential analysis that follows the three-part presenta-
tion in PTQ. The three steps are as follows. A purely
syntactic analysis of a sentence yields a set of parse
trees, each an expression in the disambiguated lan-
guage. Each parse tree is then translated by
Montague&apos;s rules into a formula of intentional logic to
which logical reductions are immediately applied. The
reduced formulas can then be interpreted in a model.
The directed process approach is the one taken in the
system described by Friedman, Moran, and Warren
1978a,b.
Semantic equivalence parsing is motivated by the
observation that the directed process approach, in
which all of the syntactic processing is completed be-
fore any semantic processing begins, does not take
maximal advantage of the coupling of syntax and se-
mantics in Montague grammars. Compositionality and
the fact that for each syntactic rule there is a transla-
tion rule suggest that it would be possible to do a
combined syntactic-semantic parse. In this approach,
as soon as a subphrase is parsed, its logical formula is
obtained and reduced to an extensionalized normal
form. Two parses for the same phrase can then be
regarded equivalent if they have the same formula.
The approach to parsing suggested by Cooper&apos;s
1975 treatment of quantified noun phrases is like our
semantic equivalence parsing in storing translations as
one element of the tuple corresponding to a noun
phrase. Cooper&apos;s approach differs from the approach
followed here because he has an intermediate stage
that might be called an &amp;quot;autonomous syntax tree&amp;quot;.
The frontier of the tree is the sentence; the scope of
the quantifier of a noun phrase is not yet indicated.
Cooper&apos;s approach has been followed by the GPSG
system (Gawron et al. 1982) and by Rosenschein and
Shieber 1982. Neither of those systems treats pro-
nouns. In Montague&apos;s approach, which we follow
here, the trees produced by the parser are expressions
in the disambiguated language, so scope is determined,
pronoun antecedents are indicated, and each tree has a
unique (unreduced) translation. The descriptions of
the systems that use Cooper&apos;s approach seem to imply
that they use a second pass over the syntax tree to
determine the actual quantifier scopes in the final logi-
cal forms. Were these systems to use a single pass to
produce the final logical forms, the results described in
this paper would be directly applicable.
</bodyText>
<sectionHeader confidence="0.9843635" genericHeader="method">
1. Equivalence Parsing
Ambiguity
</sectionHeader>
<bodyText confidence="0.999925294117647">
Ambiguity in Montague grammar is measured by
the number of different meanings. In this view syn-
tactic structure is of no interest in its own right, but
only as a vehicle for mapping semantics. Syntactic
ambiguity does not directly correspond to semantic
ambiguity, and there may be many parses with the
same semantic interpretation. Further, sentences with
scope ambiguity, such as A man loves every woman,
require more than one parse, because the syntactic
derivation determines quantifier scope.
In PTQ there is infinite syntactic ambiguity arising
from three sources: alphabetic variants of variables,
variable for variable substitutions, and vacuous varia-
ble substitution. However, these semantically unnec-
essary constructs can be eliminated, so that the set of
syntactic sources for any sentence is finite, and a par-
ser that finds the full set is possible. (This corre-
sponds to the &amp;quot;variable principle&amp;quot; enunciated by Jans-
sen 1980 and used by Landsbergen 1980.) This ap-
proach was the basis of our earlier PTQ parser
(Friedman and Warren 1978).
However, even with these reductions the number of
remaining parses for a sentence of reasonable com-
plexity is still large compared to the number of non-
equivalent translations. In the directed process ap-
proach this is treated by first finding all the parses,
next finding for each parse a reduced translation, and
then finally obtaining the set of reduced translations.
Each reduced translation may, but does not necessari-
ly, represent a different sentence meaning. No mean-
ings are lost. Further reductions of the set of transla-
tions would be possible, but the undecidability of logi-
cal equivalence precludes algorithmic reduction to a
minimal set.
</bodyText>
<subsectionHeader confidence="0.871191">
The ATN Program
</subsectionHeader>
<bodyText confidence="0.52991325">
In the underlying parser the grammar is expressed
as an augmented transition network (ATN) (Woods
1973). Both the syntactic and the semantic parsers
use this same ATN. The main difficulty in construct-
</bodyText>
<page confidence="0.801888">
124 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982
</page>
<note confidence="0.677302">
David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing
</note>
<bodyText confidence="0.999955666666667">
ing the ATN was, as usual, the non-context-free as-
pects of the grammar, in particular the incorporation
of a treatment of substitution rules and variables. The
grammar given in PTQ generates infinitely many deriv-
ations for each sentence. All but finitely many of
these are unnecessary variations on variables and were
eliminated in the construction of the ATN. The ATN
represents only the reduced set of structures, and must
therefore be more complex.
</bodyText>
<subsectionHeader confidence="0.998197">
Equivalence Testing
</subsectionHeader>
<bodyText confidence="0.996493628571429">
In order to say what we mean by semantic equiva-
lence parsing, we use Harel&apos;s 1979 notion of execution
method for nondeterministic programs. An execution
method is a deterministic procedure for finding the
possible execution paths through a nondeterministic
program given an input. For an ATN, these execution
paths correspond to different parses. Viewing parsing
in this way, the only difference between the usual
syntactic parsing and semantic equivalence parsing is a
difference in the execution method. As will be seen,
semantic equivalence parsing uses semantic tests as
part of the execution method.
We call the execution method we use to process a
general ATN equivalence parsing (Warren 1979).
Equivalence parsing is based on a recall table. The
recall table is a set of buckets used to organize and
hold partial syntactic structures while larger ones are
constructed. Equivalence parsing can be viewed as
processing an input sentence and the ATN to define
and fill in the buckets of the recall table. The use of
the recall table reduces the amount of redundant proc-
essing in parsing a sentence. Syntactic structures
found along one execution path through the ATN need
not be reconstructed but can be directly retrieved from
the recall table and used on other paths. The recall
table is a generalization of the familiar well-formed
substring table (WFST) to arbitrary programs that
contain procedure calls. Use of the WFST in ATN
parsing is noted in Woods 1973 and Bates 1978.
Bates observes that the WFST is complicated by the
HOLDs and SENDRs in the ATN. These are the ATN
actions that correspond to parameter passing in proce-
dures and are required in the ATN for PTQ to correctly
treat the substitution rules.
In the Woods system the WFST is viewed as a pos-
sible optimization, to be turned on when it improves
parsing efficiency. In our system the recall table is an
intrinsic part of the parsing algorithm. Because any
ATN that naturally represents PTQ must contain left
recursion, the usual depth-first (or breadth-first or
best-first) ATN parsing algorithm would go into an
infinite loop when trying to find all the parses of any
sentence. The use of the recall table in equivalence
parsing handles left-recursive ATNs without special
consideration (Warren 1981). As a result there is no
need to rewrite the grammar to eliminate left-recursive
rules as is usually necessary.
In a general nondeterministic program, a bucket in
the recall table corresponds to a particular subroutine
and a set of values for the calling parameters and re-
turn parameters. For an ATN a bucket is indexed by a
triple: (1) a grammatical category, that is, a subnet to
which a PUSH is made, (2) the contents of the SENDR
registers at the PUSH and the current string, and (3)
the contents of the LIFTR registers at the POP and the
then-current string. A bucket contains the members
of an equivalence class of syntactic structures; precise-
ly what they are depends on what type of equivalence
is being used.
What makes equivalence parsing applicable to non-
context-free grammars is that its buckets are more
general than the cells in the standard tabular context-
free algorithms. In the C-K-Y algorithm (Kasami
1965), for example, a cell is indexed only by the start-
ing position and the length of the parsed segment, i.e.,
the current string at PUSH and POP. The cell contents
are nonterminals. In our case all three are part of the
bucket index, which also includes SENDR and LIFTR
register values. The bucket contents are equivalence
classes of structures.
</bodyText>
<subsectionHeader confidence="0.998718">
Sentence Recognition
</subsectionHeader>
<bodyText confidence="0.999958206896552">
For sentence recognition all parses are equivalent.
So it is enough to determine, for each bucket of the
recall table, whether or not it is empty. A sentence is
in the language if the bucket corresponding to the
sentence category (with empty SENDR registers and
full string, and empty LIFTR registers and null string)
is nonempty. The particular forms of the syntactic
structures in the bucket are irrelevant; the contents of
the buckets are only a superfluous record of the spe-
cific syntactic structures. The syntactic structure is
never tested and so does not affect the flow of con-
trol. Thus which buckets are nonempty depends only
on what other buckets are nonempty and not on what
those other buckets contain. For sentence recognition,
when the execution method constructs a new member
of a bucket that is already nonempty, it may or may
not add the new substructure, but it does not need to
use it to construct any larger syntactic structures. This
is because the earlier member has already verified this
bucket as nonempty. Therefore this fact is already
known and is already being used to determine the
nonemptiness of other buckets. To find all parses,
however, equivalence parsing does use all members of
each bucket to construct larger structures.
It would be possible first to do recognition and
determine all the nonempty buckets in the recall table,
and then to go back and take all variants of one single
parse that can be obtained by replacing any substruc-
ture by another substructure from the same bucket.
</bodyText>
<note confidence="0.368532">
American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 125
David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing
</note>
<bodyText confidence="0.99996884">
This is essentially how the context-free parsing algor-
ithms constructed from the tabular recognition me-
thods work. This is not how the equivalence parsing
algorithm works. When it obtains a substructure, it
immediately tries to use it to construct larger struc-
tures.
The difference described above between sentence
recognition and sentence parsing is a difference only
in the execution methods used to execute the ATN and
not in the ATN itself. This difference is in the test for
equivalence of bucket contents. In sentence recogni-
tion any two syntactic structures in a bucket are equiv-
alent since we only care whether or not the substring
can be parsed to the given category. At the other
extreme, in finding all parses, two entries are equiva-
lent only if they are the identical structure. For most
reasonable ATNs, including our ATN for PTQ, this
would not happen; distinct paths lead to distinct struc-
tures.
Semantic parsing is obtained by again modifying
only the equivalence test used in the execution method
to test bucket contents. For semantic parsing two
entries are equivalent if their logical translations, after
logical reduction and extensionalization, are identical
to within change of bound variable.
</bodyText>
<subsectionHeader confidence="0.978149">
Small Grammar
</subsectionHeader>
<bodyText confidence="0.9991313125">
For our examples, we introduce in Figure 1 a small
subnet of the ATN for PTQ. Arcs with fully capital-
ized labels are PUSH arcs; those with lower case labels
are CAT arcs. Structure-building operations are indi-
cated in parentheses. This net implements just three
rules of PTQ. Rule S4 forms a sentence by concaten-
ating a term phrase and an intransitive verb phrase;
S1 1 conjoins two sentences, and S14,i substitutes a
term phrase for the syntactic variable he in a sen-
tence. S4 and Sll are context-free rules; S14,i is one
of the substitution rules that make the grammar non-
context-free and is basic to the handling of quantifiers,
pronouns, and antecedents. The ATN handles the
substitution by using a LIFTR to carry the variable-
binding information. The LIFTR is not used for the
context-free rules.
</bodyText>
<equation confidence="0.984425416666667">
TE
IV (S4 TERM IVP)
POP SENT t
TS
TS
TS
coni
(S11 SENT1
SENT2)
(S14,1 TERM SENT)
i POP TERM
POP hei
</equation>
<page confidence="0.356353">
14
</page>
<figure confidence="0.796223">
bte
TE
biv POP IVP
IV
</figure>
<figureCaption confidence="0.999592">
Figure 1. Subnet of the ATN for PTO.
</figureCaption>
<page confidence="0.668163">
126 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982
</page>
<note confidence="0.774389">
David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing
</note>
<figureCaption confidence="0.429129">
Example 1: Bill walks
</figureCaption>
<bodyText confidence="0.999540304347826">
The first example is the sentence Bill walks. This
sentence has the obvious parse using only the context-
free rule S4. It also has the parse using the substitu-
tion rule. We will carry through the details of its
parse to show how this substitution rule is treated in
the parsing process.
In the trace PUSHes and POPs in the syntactic anal-
ysis of this sentence are shown. The entries are in
chronological order. The PUSHes are numbered se-
quentially for identification. The PUSH number uni-
quely determines a) the category to which the PUSH is
made, b) the remainder of the sentence being parsed
at the time of the PUSH, and c) the contents of the
SENDR registers at the time of the PUSH, called the
PUSH environment. At each POP a bucket and an
element in that bucket are returned. The bucket name
at a POP is made up of the corresponding PUSH num-
ber, the remaining input string, and the contents of the
LIFTR registers, which are called the POP environ-
ment. The element in the bucket is the tree that is
returned. For brevity we use in the trace only the first
letters of the words in the sentence; for example, Bill
walks becomes Bw.
</bodyText>
<figure confidence="0.79212325">
Trace of Bill walks
PUSH: Bucket: Contents:
# CAT Str Env from Str Env Tree
1 TS Bw null
[Parsing begins with a PUSH to the sentence category passing the entire string and an empty or null environment.]
2 TE Bw null
[In the sentence subnet we first PUSH to find a TE.]
2 w null Bill
[The TE subnet finds and POPs the term Bill to return from PUSH 2.]
3 IV w null
3 c null walk
1 c null (S4 Bill walk)
</figure>
<bodyText confidence="0.848324625">
[Now since a tree is returned to the top level, covers the whole string, and the returned environment is null, the
tree is printed. The parses are always the trees in bucket 1-E -null. The execution method now backs up; there
are no more POPs from PUSH 3; there is another from PUSH 2.]
2 w (he0 B) he0
[Continuing forward with the new environment...]
4 IV w (he0 B)
4 c null walk
[Note that this is not the same bucket as on the previous PUSH 3 because the PUSH environments differ.]
</bodyText>
<listItem confidence="0.26981925">
1 c (he0 B) (S4 he0 walk)
[The tree has been returned and covers the whole string. However, the returned environment is not null so the
parse fails, and the execution method backs up to seek another return from PUSH 1.]
1 null (S14,0 Bill (S4 he0 walk))
</listItem>
<bodyText confidence="0.9896375">
[This is another element in bucket 1-E -null; it is a successful parse so it is printed out. Execution continues but
there are no more parses of the sentence.]
</bodyText>
<sectionHeader confidence="0.947327" genericHeader="method">
Discussion
</sectionHeader>
<bodyText confidence="0.97724265">
In this trace bucket 1-E -null is the only bucket
with more than one entry. The execution method was
syntactic parsing, so each of the two entries was re-
turned and printed out. For recognition, these two
entries in the bucket would be considered the same
and the second would not have been POPped. Instead
of continuing the computation up in the subnet from
which the PUSH was made, this path would be made to
fail and the execution method would back up. For
semantic equivalence parsing, the bucket contents
throughout would not be the syntax trees, but would
instead be their reduced extensionalized logical formu-
las. (Each such logical formula represents the equiva-
lence class of the syntactic structures that correspond
to the formula.) For example, bucket 2-w-null would
contain APP{Ab} and bucket 3-c-null would contain
walk&apos;. The first entry to bucket 1-E -null would be the
formula for (S4 Bill walk), that is, walk,&apos;(b). The
entry to bucket 1-E -null on the last line of the trace
American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 127
</bodyText>
<subsectionHeader confidence="0.975126">
David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing
</subsectionHeader>
<bodyText confidence="0.9997187">
would be the formula for (S14,0 Bill (S4 he0 walk)),
which is also walk.&apos;(b). Therefore, this second entry
would not be POPped.
Buckets also serve to reduce the amount of repeat-
ed computation. Suppose we have a second PUSH to
the same category with the same string and environ-
ment as an earlier PUSH. The buckets resulting from
this new PUSH would come out to be the same as the
buckets from the earlier PUSH. Therefore the buckets
need not be recomputed; the results of the earlier
buckets can be used directly. This is called a
&amp;quot;FAKEPUSH&amp;quot; because we don&apos;t actually do the PUSH
to continue through the invoked subnet but simply do
a &amp;quot;FAKEPOP&amp;quot; using the contents of the previously
computed buckets.
Consider, as an example of FAKEPOP, the partial
trace of the syntactic parse of the sentence Bill walks
and Mary runs (or Bw&amp;Mr for short). The initial part
of this trace, through step 4, is essentially the same as
the trace above for the shorter sentence Bill walks.
</bodyText>
<table confidence="0.951441">
Trace of Bill walks and Mary runs
PUSH: Bucket: Contents:
# CAT Str Env from Str Env Tree
1 TS Bw&amp;Mr null
2 TE Bw null
3 IV w null 2 w&amp;Mr null Bill
3 &amp;Mr null walk
1 &amp;Mr null (S4 Bill walk)
</table>
<figure confidence="0.690163166666667">
[A tree has been returned to the top level, but it does not cover the whole sentence, so the path fails and the
execution method backs up.]
2 w&amp;Mr (he0 B) he0
4 IV w&amp;Mr (he0 B)
4 &amp;Mr null walk
1 &amp;Mr (he0 B (S4 he0 walk)
</figure>
<bodyText confidence="0.9621253">
[Again a tree has been returned to the top level, but it does not span the whole string, nor is the returned
environment null, so we fail.]
1 &amp;Mr null (S14,0 Bill (S4 he0 walk))
[Again we are the top level; again we do not span the whole string, so again we fail.]
5 TS Bw&amp;Mr null
[This is the second arc from the TS node of Figure 1. The PUSH to TE (2 above) has completely failed.
However, this PUSH, TS-Bw&amp;Mr-null has been done before; it is PUSH 1. We already have two buckets from
that PUSH: 1-&amp;Mr-null containing two trees, and 1-&amp;Mr-(he0 B) with one tree. There is no need to re-enter this
subnet; the buckets and their contents tell us what would happen. Therefore we FAKEPOP one subtree and its
bucket and follow that computation to the end; later we will return to FAKEPOP the next one.]
</bodyText>
<listItem confidence="0.589185">
1(5) &amp;Mr null (S4 Bill walk)
6 TS Mr null
7 TE Mr null
7 r null Mary
</listItem>
<bodyText confidence="0.90190725">
[This computation continues and parses the second half of this sentence. Two parses are produced:
(S11 (S4 Bill walk) (S4 Mary run)) and
(S11 (S4 Bill walk) (S14,0 Mary (S4 he0 run)))
After this, the execution method fails back to the FAKEPOP at PUSH 5, and another subtree from a bucket from
</bodyText>
<equation confidence="0.8449705">
PUSH 2 is FAKEPOPped.]
1 (5 ) &amp;Mr null (S14,0 Bill (he0 walk))
</equation>
<bodyText confidence="0.993345333333333">
[And the computation continues, eventually producing a total of ten parses for this sentence.]
(In the earlier example of Bill walks, these ately fail, because they are looking for a conjunction
FAKEPOPs are done, but their computations immedi- but are at the end of the sentence.)
</bodyText>
<page confidence="0.757003">
128 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982
</page>
<note confidence="0.799739">
David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing
</note>
<subsectionHeader confidence="0.971915">
Results of Parsing
</subsectionHeader>
<bodyText confidence="0.999545214285714">
The sentence Bill walks and Mary runs has ten
syntactic structures with respect to the PTQ grammar.
The rules S4, S11, and S14,i can be used in various
orders. Figure 2 shows the ten different structures in
the order they are produced by the syntactic parser.
The nodes in the trees of Figure 2 that are in italics
are the syntactic structures used for the first time.
The nodes in standard type are structures used previ-
ously, and thus either are part of an execution path in
common with an earlier parse, or are retrieved from a
bucket in the recall table to be used again. Thus the
number of italicized nodes measures in a crude way
the amount of work required to find all the parses for
this sentence.
</bodyText>
<figure confidence="0.999321050847458">
a.
C.
Si]
I
I I
S4 S4
I
I I
walk Mary
Si]
I
I I
S4 S/I0 Mary
IS4
I
I I
he0 run
b. S11,0 Mary
Si]
I
I I
SI4 I
S4
I 1 I I
Bill walk he0
d.
S14 I
S4
I I
I I I I
he0 walk Mary run
1
I
run
I
walk
run
e. S111 Mary
S14,0 Bill
I
Si]
I
I I
S4 34
I I
I I
hel0 walk hel 1 run
f. S11,0 Bill
S14,1 Mary
I
Sll
I
I I
S4 S4
I I
II he
I
walk l 1 I
run
</figure>
<figureCaption confidence="0.998031">
Figure 2. Ten parses for Bill walks and Mary runs.
</figureCaption>
<figure confidence="0.9938999">
American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 129
David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing
x.
S4
he0 walk
S1.11,1 Mary
S4
he 1 run
Si]
S14,0 Bill
S4
he0 walk
S14,0 Bill h.
S11
S4
Mary run
J.
run
hel
I. S14(1 Mary
S11
S14,0 Bill S4
S4
he0 walk
Si]
S14,0 Bill S1,(1,1 Mary
S4 S4
I I
I I I I
he0 walk he 1 run
</figure>
<figureCaption confidence="0.999929">
Figure 2. continued
</figureCaption>
<subsectionHeader confidence="0.965369">
Example of Semantic Equivalence Parsing
</subsectionHeader>
<bodyText confidence="0.999722166666667">
This sentence, Bill walks and Mary runs, is one for
which semantic parsing is substantially faster. It is
unambiguous; its only reduced extensionalized logical
translation is &amp;quot;walk&apos;(b)&amp;run&apos;(m)&amp;quot;. In the directed
process parser, all ten trees of Figure 2 are found.
They will all have the same translation. In semantic
parsing only one is found. Here the method works to
advantage because both parses of the initial string Bill
walks result in the same environment for parsing Mary
runs. These two parses go into the same bucket so
only one needs to be used to construct larger struc-
titres. We trace the example.
</bodyText>
<table confidence="0.997567923076923">
PUSH: Bucket: Contents:
# CAT Str Env from Str Env Formula
1 TS Bw&amp;Mr null 2 w&amp;Mr null XPP[Ab]
2 TE Bw null 3 &amp;Mr null walk&apos;
3 IV w null 1 &amp;Mr null walk&apos;(b)
[Fail]
4 IV w&amp;Mr (he0 B) 2 w&amp;Mr (he0 B) AFT { x01
&amp;Mr null walk&apos;
1 &amp;Mr (he0 B) walk&apos;(&amp;quot;x0)
[Fail]
130 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982
David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing
1 &amp;Mr null walk.&apos;(b)
</table>
<bodyText confidence="0.953370714285714">
[This formula is the translation of the syntactic structure using S14,0 to substitute Bill into &amp;quot;he0 walks&amp;quot;. This is
the same bucket and the same translation as obtained at the return from 1 after PUSH 3 above, so we do not POP
(indicated by the &apos;n&apos; in the final column), but instead fail back.]
5 TS Bw&amp;Mr null
[FAKEPOP, since this is a repeat PUSH to this category with these parameters. There are two buckets: 1-&amp;Mr-
null, which in syntactic parsing had two trees but now has only one translation, and bucket 1-&amp;Mr-(he0 B) with
one translation. So we FAKEPOP 1-&amp;Mr-null.]
</bodyText>
<equation confidence="0.505775705882353">
(FAKEPOP)
6 TS Mr null
7 TE Mr null
8 IV r null
1(5) &amp;Mr null
7 r null Mary
8 E null run&apos; Y
6 E null run,&apos;(m) y
1 c null walk,&apos;(b)&amp;run&apos;(m) y
[This is a successful parse. The top level prints out the translation and then the execution method fails back.]
7 (he0 M) null XPP [ x0] y
9 IV r (he0 M)
9 E null run&apos; 3&apos;
6 E null run&apos;(&amp;quot;x0) y
1 E (he0 M) walk,&apos;(b)&amp;run,,,&apos;(&amp;quot;x0) Y
[Fail because we are at the top level and the environment is not null.]
1 E null walk&apos;(b)&amp;run&apos;(m) n
</equation>
<bodyText confidence="0.9971545">
[Again we want to enter a translation into bucket 1-€ -null. This translation duplicates the one already there. So
it is not returned and we fail back.]
</bodyText>
<sectionHeader confidence="0.959655" genericHeader="method">
6 null run&apos;(m)
</sectionHeader>
<bodyText confidence="0.997488">
[This again duplicates a bucket and its contents, so we fail back to the second FAKEPOP from PUSH 5. Now we
use the other bucket: 1-&amp;Mr-(he0 B).]
</bodyText>
<table confidence="0.995679419354839">
(FAKEPOP) 1(5) &amp;Mr (he0 B) walk&apos;(&amp;quot;x0)
10 TS Mr (he0 B)
11 TE Mr (he0 B)
11 r null XP{x0} Y
121V r (he0 B)
12 c null run&apos; Y
10 E null run&apos;(m) Y
1 c (he0 B) walk.&apos;(&amp;quot;x0)&amp;run.&apos;(m) Y
[Fail at the top level since the environment is not null.]
1 E (he0 B) walk.&apos;(b)&amp;run&apos;(m) n
[This duplicates a bucket and its contents, so we do not POP it but fail back.]
11 r (hel M) AFT { x 1 } y
13 IV r (he0 B)
(hel M)
13 E null run&apos; y
10 E (hel M) run,&apos;(&amp;quot;xl) Y
1 E (he0 B) walk&apos;(&amp;quot;x0)&amp;run&apos;(&amp;quot;x1) Y
(hel M)
[Fail at top level because environment is not null.]
1 E (hel M) walk,&apos;(b)&amp;run.&apos;(&amp;quot;xl) y
[Fail at top level because environment is not null.]
1 E null walk.&apos;(b)&amp;run,;(m) n
[Duplicate bucket and translation, so fail.]
American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 131
David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing
1 E (he0 B) walk.&apos;(&amp;quot;x0)&amp;run&apos;(m) n
[Duplicate bucket and translation, so fail.] E null walk,&apos;(b)&amp;run&apos;(m) n
1
[Duplicate, so fail.] E null run:(m) n
10
[Duplicate, so fail.]
</table>
<bodyText confidence="0.82238">
This completes the trace of the semantic parse of the sentence.
</bodyText>
<subsectionHeader confidence="0.952106">
Results of Parsing
</subsectionHeader>
<bodyText confidence="0.861553333333333">
Figure 3 displays in graphical form the syntactic
structures built during the semantic parsing of Bill
walks and Mary runs traced above. A horizontal line
over a particular node in a tree indicates that the
translation of the structure duplicated a translation
already in its bucket, so no larger structures were built
</bodyText>
<figure confidence="0.789401333333333">
a. Si /
1
I I
S4 S4
I
I III
I
Bill walk Mary run
c. S14,0 Mary
S4
El-1
he0 run
</figure>
<bodyText confidence="0.983364857142857">
using it. Only parse a) is a full parse of the sentence
and thus it is the only parse returned. All the others
are aborted when they are found equivalent to earlier
partial results. These points of abortion in the compu-
tation are the points in the trace above at which a POP
fails due to the duplication of a bucket and its con-
tents.
</bodyText>
<figure confidence="0.9935650625">
b. S14,0 Mary
Si&apos;
I
I I
S4 S4
I I
I I I I
Bill walk he0
d. S14,0 Bill
Si&apos;
I I
S4 S4
I I
I I I 1
he0 walk Mary run
run
</figure>
<figureCaption confidence="0.998213">
Figure 3. Semantic parses of Bill walks and Mary runs.
</figureCaption>
<page confidence="0.523764">
132 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982
</page>
<figure confidence="0.974597285714286">
David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing
e. SI4,1 Mary
S14,0 Bill
SI 1
S4 S4
he0 walk hel run
f. S14,0 Bill
S14,1 Mary
Sll
S14 S4
he0 walk hel run
g. SIr Mary
S4
hel run
</figure>
<figureCaption confidence="0.99995">
Figure 3. continued
</figureCaption>
<bodyText confidence="0.9997813125">
Note that construction of parse c) is halted when a
translation is built that duplicates the translation of
the right S4 subtree of parse a). This corresponds to
the failure due to duplicate bucket contents in bucket
6- e -null following PUSH 9 in the trace above. Simi-
larly parse g) is aborted before the entire tree is built.
This corresponds to the failure in the final line of the
trace due to a duplicate translation in bucket
10- E -null. Semantic parses that would correspond to
syntactic parses h), i), and j) of Figure 2 are not con-
sidered at all. This is because bucket 1-&amp;Mr-null con-
tains two syntactic structures, but only one translation.
Thus in semantic equivalence parsing we only do one
FAKEPOP for this bucket for PUSH 5. In syntactic
parsing the other parses are generated by the
FAKEPOP of the other structure in this bucket.
</bodyText>
<subsectionHeader confidence="0.950114">
Reducing the Environment
</subsectionHeader>
<bodyText confidence="0.995435375">
The potential advantage of semantic equivalence
parsing derives from treating partial results as an equi-
valence class in proceeding. A partial result consists
of a structure, its extensionalized reduced translation,
and a set of parameters of the parse to that point.
These parameters are the environment for parsing the
phrase. Consider the sentence John loves Mary and its
parses:
</bodyText>
<listItem confidence="0.909411333333333">
(1) (S4 John (S5 love Mary))
(2) (S4 John (S16,0 Mary (S5 love he0)))
(3) (S14,0 John (S4 (he0 (S5 love Mary)))
(4) (S14,0 John (S4 he0 (S16,1 Mary
(S5 love he 1))))
(plus 3 more)
</listItem>
<bodyText confidence="0.9998970625">
On reaching the phrase love Mary in parse (3) the
parameters are not the same as they were at that point
in parse (1), because the pair (he0 John) is in the
environment. Thus the parser is not able to consult
the recall table and immediately return the already
parsed substructure. Instead it must reparse love Mary
in the new context.
This environment problem arises because the ATN
is designed to follow PTQ in treating pronouns by the
non-context-free substitution rules. We have also
considered, but have not to this point implemented,
alternative ways of treating variables to make partial
results equal. One way would be not to pass variable
bindings down into lower nets at all. Thus the PUSH
environment would always be null. Since these bind-
ings are used to find the antecedent for a pronoun, the
way antecedents are determined would have to be
changed. An implementation might be as follows: On
encountering a pronoun during parsing, replace it by a
new he-variable. Then pass back up the tree informa-
tion concerning both the variable number used and the
pronoun&apos;s gender. At a higher point in the tree, where
the substitution rule is to be applied, a determination
can be made as to which of the substituted terms
could be the antecedent for the pronoun. The variable
number of the pronoun can then be changed to agree
with the variable number of its antecedent term by a
variable-for-variable substitution. Finally the substitu-
tion rule can be used to substitute the term into the
phrase for all occurrences of the variable. Note that
this alternative process would construct trees that do
have substitution rules to substitute variables for varia-
</bodyText>
<note confidence="0.78623">
American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 133
David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing
</note>
<bodyText confidence="0.999790571428571">
bles, contrary to the variable principle mentioned
above. We also note that with this modification a
pronoun is not associated with its antecedent when it is
first encountered. Instead the pronoun is saved and at
some later point in the parse the association is made.
This revised treatment is related computationally to
that proposed in Cooper 1975.
</bodyText>
<subsectionHeader confidence="0.989102">
Evaluation of Semantic Equivalence Parsing
</subsectionHeader>
<bodyText confidence="0.999929927083333">
The question of the interaction of syntax and se-
mantics in parsing was introduced early in computa-
tional linguistics. Winograd 1971 argued for the in-
corporation of semantics as early as possible in the
recognition process, in order to reduce the amount of
syntactic processing that would be needed. Partial
parses that had no interpretation did not need to be
continued. The alternative position represented by
Woods&apos;s early work (Woods and Kaplan 1971) was
basically the inverse: less semantic processing would
be needed if only completed parses were interpreted.
This argument is based on the idea of eliminating un-
interpretable parses as soon as possible.
This advantage, if it is one, of integrated syntactic
and semantic procedures does not occur here because
the semantic aspect does not eliminate any logical
analyses. The translation of a structure to a formula is
always successful, so no partial parse is ever eliminat-
ed for lack of a translation. What happens instead is
that several partial parses are found to be equivalent
because they have the same translation. In this case
only a representative of the set of partial parses needs
to be carried forward.
A further expansion of equivalence parsing would
be interpretation equivalence parsing. Sentence process-
ing would take place in the context of a specified mod-
el. Two structures would be regarded as equivalent if
they had the same denotation in the model. More
partial structures would be found equivalent under the
equivalence relation than under the reduce-
extensionalize relation, and fewer structures would
need to be constructed. Further, with the interpreta-
tion equivalence relation, we might be able to use an
inconsistent denotation to eliminate an incorrect par-
tial parse. For example, consider a sentence such as
Sandy and Pat are running and she is talking to him. In
this case, since the gender of Sandy and Pat cannot be
determined syntactically, these words would have to
be marked in the lexicon with both genders. This
would result in multiple logical formulas for this sen-
tence, one for each gender assumption. However,
during interpretation equivalence parsing, the referents
for Sandy and Pat would be found in the model and
the meaning with the incorrect coreference could be
rejected.
Logical normal forms other than the reduced, ex-
tensionalized form used above lead to other reasonable
versions of equivalence parsing. For example, we
could further process the reduced, extensionalized
form to obtain a prenex normal form with the matrix
in clausal form. We would use some standard conven-
tions for naming variables, ordering sequences of the
same quantifier in the prefix, and ordering the literals
in the clauses of the matrix. This would allow the
algorithm to eliminate, for example, multiple parses
arising from various equivalent scopes and orderings of
existential quantifiers.
The semantic equivalence processor has been im-
plemented in Franz Lisp. We have applied it to the
PTO grammar and tested it on various examples. For
purposes of comparison the directed process version
includes syntactic parse, translation to logical formula
and reduction, and finally the reduction of the list of
formulas to a set of formulas. The mixed strategy
yields exactly this set of formulas, with one parse tree
for each. Experiments with the combined parser and
the directed parser show that they take approximately
the same time for reasonably simple sentences. For
more complicated sentences the mixed strategy usually
results in less processing time and, in the best cases,
results in about a 40 percent speed-up. The distin-
guishing characteristic of a string for which the me-
thod yields the greatest speed-up is that the environ-
ment resulting from parsing an initial segment is the
same for several distinct parses.
The two parsing method we have described, the
sequential process and the mixed process, were obvi-
ously not developed with psychological modeling in
mind. The directed process version of the system can
be immediately rejected as a possible psychological
model, since it involves obtaining and storing all the
structures for a sentence before beginning to interpret
any one of them. However, a reorganization of the
program• would make it possible to interpret each
structure immediately after it is obtained. This would
have the same cost in time as the first version, but
would not require storing all the parses.
Although semantic equivalence parsing was devel-
oped in the specific context of the grammar of PTQ, it
is more general in its applicability. The strict compos-
itionality of syntax and semantics in PTQ is the main
feature on which it depends. The general idea of equi-
valence parsing can be applied whenever syntactic
structure is used as an intermediate form and there is a
syntax-directed translation to an output form on which
an equivalence relation is defined.
</bodyText>
<sectionHeader confidence="0.984888" genericHeader="method">
2. Input-Refined Grammars
</sectionHeader>
<bodyText confidence="0.9999455">
We now switch our point of view and examine
equivalence parsing not in algorithmic terms but in
formal grammatical terms. This will then lead into
showing how equivalence parsing relates to Universal
</bodyText>
<page confidence="0.709482">
134 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982
</page>
<note confidence="0.909441">
David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing
</note>
<bodyText confidence="0.997824414634146">
Grammar (UG) (Montague 1970). The basic concept
to be used is an input-refined grammar. We begin by
defining this concept for context-free grammars and
using it to relate the tabular context-free recognition
algorithms of Earley 1970, Cocke-Kasami-Younger
(Kasami 1965), and Sheil 1976 to each other and
eventually to our algorithm.
Given a context-free grammar G and a string s over
the terminal symbols of G, we define from G and s a
new grammar Gs, called an input-refinement of G.
This new grammar Gs will bear a particular relation-
ship to G: L(Gs) = {s} n L(G), i.e., L(Gs) is the single-
ton set {s} if s is in L(G), and empty otherwise. Fur-
thermore, there is a direct one-to-one relationship
between the derivations of s in G and the derivations
of s in G. Thus the problem of recognizing s in G is
reduced to the problem of determining emptiness for
the grammar G. Also, the problem of parsing s with
respect to the grammar G reduces to the problem of
exhaustive generation of the derivations of Gs (there is
at most one string). Each of the tabular context-free
recognition algorithms can be viewed as implicitly
defining this grammar Gs and testing it for emptiness.
Emptiness testing is essentially done by reducing the
grammar, that is by eliminating useless symbols and
productions. The table-constructing portion of a tabu-
lar recognition algorithm, in effect, constructs and
reduces the grammar Gs, thus determining whether or
not it is empty. The tabular methods differ in the
construction and reduction algorithm used.
In each case, to turn a tabular recognition method
into a parsing algorithm, the table must first be con-
structed and then reprocessed to generate all the pars-
es. This corresponds to reprocessing the grammar Gs&apos;,
the result of reducing the grammar Gs, and using it to
exhaustively generate all derivations in G.
Rather than formally defining Gs from a context-
free grammar G and a string s in the general case, we
illustrate the definition by example. The general defi-
nition should be clear.
Let G be the following context-free grammar:
</bodyText>
<figure confidence="0.670813666666667">
Terminals: {a,b}
Nonterminals: {S]
Start Symbol: S
Productions: S-*S S a
S b
(S produces the empty string)
</figure>
<bodyText confidence="0.899018076923077">
Let s be the string bba. Gbba is defined from G and
bba:
Terminals: {a,13}
Nonterminals: fa1,a2,a3,b1,b2,b3,
(ti for t a terminal of G
and 1&lt;i&lt;length(s))
S123,S12,S1,S23,S2,S3,
(Ax for each nonterminal A of G
and each x a nonempty subse-
quence of &lt;1,2,3,...,length(s)&gt;)
S°,S1,S2,S31
(Ai for each nonterminal A of G
and i, 0&lt;i&lt;length(s))
</bodyText>
<table confidence="0.466172">
Start Symbol: S123
Productions: [from G production: S-1.S S a]
S123-*S12S2a3
S123-&apos;Se2a3
S123-+S Si2a3
S12-1-S4Sla2
</table>
<equation confidence="0.916942333333334">
S12-&apos;S S1a2
S1-S°S°art
S23-1•SiS2a3
S23-1-S S2a3
S2-•SiSia2
53-4•S2S2a3
[from G production: S-1.. b]
S1-.131
S2-.132
S3-1.3
[from G production: S-4. e ]
E
Si E
s2-■ E
S3 E
[for the terminals]
b1-.13
b2-).13
</equation>
<bodyText confidence="0.99976825">
These productions for Gs were constructed by begin-
ning with a production of G, adding a subscript or a
superscript to the nonterminal on the LHS to obtain a
nonterminal of Gs, adding single subscripts to all ter-
minals and sequence subscripts to some nonterminals
on the RHS so that the concatenation of all subscripts
on the RHS equals the subscript on the LHS. For the
RHS nonterminals without subscripts, add the appro-
priate subscript. Also, to handle the terminals, for
each ti add the production T1-1.4 where t is the ith sym-
bol in s.
It is straightforward to show inductively that if a
nonterminal symbol generates any string at all it gen-
erates exactly the substring of s that its subscript de-
termines Symbols with superscripts generate the emp-
ty string. Also a parse tree of Gs can be converted to
a parse tree of G by first deleting all terminals (each is
dominated by the same symbol with a subscript) and
then erasing all superscripts and subscripts on all sym-
bols in the tree. Conversely, any parse tree for s in G
can be converted to a parse tree of s in Gs by adding
appropriate subscripts and superscripts to all the sym-
bols of the tree and then adding the terminal symbols
at the leaves.
</bodyText>
<note confidence="0.840336">
American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 135
David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing
</note>
<bodyText confidence="0.999990673913043">
It is clear that Gs is not in general a reduced gram-
mar. Gs can be reduced to Gs&apos; by eliminating unpro-
ductive and unreachable symbols and the rules involv-
ing them. Reducing the grammar will determine
whether or not L(Gs) is empty. By the above discus-
sion, this will determine whether s is in L(G), and thus
an algorithm for constructing and reducing the refined
grammar Gs from G and s yields a recognition algor-
ithm. Also, given the reduced grammar Gs&apos;, it is
straightforward, in light of the above discussion, to
generate all parses of s in G: simply exhaustively gen-
erate the parse trees of Gs&apos; and delete subscripts and
superscripts.
The tabular context-free recognition methods of
Cocke-Kasami-Younger, Earley, and Sheil can all be
understood as variations of this general approach. The
C-K-Y recognition algorithm uses the standard bottom-
up method to determine emptiness of G. It starts
with the terminals and determines which Gs nontermi-
nals are productive, eventually finding whether or not
the start symbol is productive. The matrix it con-
structs is essentially the set of productive nonterminals
of G.
Sheil&apos;s well-formed substring table algorithm is the
most obviously and directly related. His simplest al-
gorithm constructs the refined grammar and reduces it
top-down. It uses a top-down control mechanism to
determine the productivity only of nonterminals that
are reachable from the start symbol. The well-formed
substring table again consists essentially of the reacha-
ble, productive nonterminals of G.
Earley&apos;s recognition algorithm is more complicated
because it simultaneously constructs and reduces the
refined grammar It can be viewed as manipulating
sets of subscripted nonterminals and sets of prod-
uctions of G. The items on the item lists, however,
correspond quite directly to reachable, productive
nonterminals of G.
The concept of input-refined grammar provides a
unified view of the tabular context-free recognition
methods. Equivalence parsing as described in Part I
above is also a tabular method, although it is not
context-free. It applies to context-free grammars and
also to some grammars such as PTQ that are not
context-free. We next relate it to the very general
class of grammars defined by Montague in UG.
</bodyText>
<subsectionHeader confidence="0.916317">
Universal Grammar and Equivalence Parsing
</subsectionHeader>
<bodyText confidence="0.98407688">
In the following discussion of the problem of pars-
ing in the general context of Montague&apos;s definitions of
a language (which might more naturally be called a
grammar) and an interpretation, we assume the reader
is familiar with the definitions in UG (Montague
1970). We begin with a formal definition of a refine-
ment of a general disambiguated language. A particu-
lar type of refinement, input-refinement, leads to an
equivalence parsing algorithm. This generalizes the
procedure for input-refining a grammar shown above
for the special case of a context-free grammar. We
then discuss the implications for equivalence parsing of
using the formal interpretation of the language. Final-
ly we show how the ATN for PTQ and semantic equi-
valence parsing fit into this general framework.
Recall that a disambiguated language S2 = &lt;A, F7,
X8, S,can be regarded as consisting of an
80&gt;-yET,8E A
algebra &lt;A,Fy&gt;y, r, with proper expressions A and
operations F7, basic expressions X8 for each category
index S E A, a set of syntactic rules S, and a sentence
category index So E A. A language is a pair &lt;12,R&gt;
where S2 is a disambiguated language and R is a binary
relation with domain included in A. Given a disambig-
uated language
</bodyText>
<equation confidence="0.395887">
S2 = &lt;A, F7, X8, S, 8 &apos;›
-0- Ser,
</equation>
<bodyText confidence="0.8335844">
a disambiguated language
= &lt;A, Fy,X&apos; 8,, S&apos;,
is a refinement of S2 if there is a refinement function
d:A&apos; -.A from the category indices of 12&apos; to those of S-2
such that
</bodyText>
<listItem confidence="0.9265355">
1) X&apos;s Xos,),
2) If &lt;Fy,&lt;811,821,—,8nI&gt;,61&gt; 6 SI, then
&lt;F)„&lt;d(81&apos;),d(821),...,d(Sa&apos;)&gt;, d(S9&gt; E S&apos;, and
3) d(S01) = So.
</listItem>
<bodyText confidence="0.981617103448276">
(Note that the proper expressions A, the operation
indexing set T, and the operations F7 of S2 and S2&apos; are
the same.)
The word refinement refers to the fact that the
catgories of S2 are split into finer categories. Condi-
tion 1 requires that the basic expressions of a refined
category come from the basic expressions of the cate-
gory it refines. Condition 2 requires that the new
syntactic rules be consistent with the old ones. Note
that Condition 2 is not a biconditional.
If S2&apos; is a refinement of S2 with refinement function
d, &lt;C&apos; st&gt; st A, is the family of syntactic categories of
S2&apos; and &lt;C&gt; is the family of syntactic categories
of C2, then C&apos;stgCos,)•
As a simple example of a refinement, consider an
arbitrary disambiguated language S2&apos; = &lt;A, F7, X18,,
601&gt;yEr,81,At. Now let S2 be the disambiguated lan-
guage &lt;A, F7, Xa, S, a&gt;?Er, in which the set of cate-
gory names is the singleton set { Xa = U81,A, X8,.
Let S be {&lt;F,,,, &lt;a,a,...,a&gt;, a&gt; : y E T and the number
of a&apos;s agrees with the arity of Then S2&apos; is a refine-
ment of 0, with refinement function d:&apos; -*.{a}, d(S&apos; )
= a for all 8&apos;E A&apos;. Note that the disambiguated lan-
guage S2 is completely determined by the algebra
&lt;A,Fy&gt;y,r, and is the natural disambiguated language
to associate with it. Thus in a formal sense, we can
view a disambiguated language as a refinement of its
algebra.
8o1&gt;yEr,
</bodyText>
<page confidence="0.663351">
136 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982
</page>
<note confidence="0.878475">
David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing
</note>
<bodyText confidence="0.99993806">
As a more intuitive example of refinement, consider
an English-like language with categories term (TE) and
intransitive verb phrase (IV) that both include singular
and plural forms. The language generated would then
allow subject-verb disagreement (assuming the ambig-
uating relation R does not filter them out). By refin-
ing category TE to TEsing and TEpi and category IV to
IVsing and IVo, and having syntactic rules that com-
bine category TEsing with I&apos;sing and TEpi with IVPI
only, we obtain a refined language that has subject-
verb agreement. A similar kind of refinement could
eliminate such combinations as &amp;quot;colorless green
ideas&amp;quot;, if so desired.
With this definition of refinement, we return to the
problem of parsing a language L = &lt;52, R&gt;. The
problem can now be restated: find an algorithm that,
given a string constructs a disambiguated language
that is an input-refinement of Q. That is, RE is a
refinement in which the sentence category C&apos;,, is ex-
actly the set of parses of E in L. Finding this algor-
ithm is equivalent to solving the parsing problem. For
given such an algorithm, the parsing problem reduces
to the problem of generating all members of C&apos;80,.
In the case of a general language &lt;2, R&gt;, it may
be the case that for a string, the input-refined lan-
guage 52E has finitely many categories. In this case the
reduced grammar can be computed and a recursive
parsing algorithm exists. If the reduced grammar has
infinitely many categories, then the string has infinitely
many parses and we are not, in general, interested in
trying to parse such languages. It may happen, how-
ever, that S2E has infinitely many categories, even
though its reduction has only finitely many. In this
case, we are not guaranteed a recursive parsing algor-
ithm. However, if this reduced language can be effec-
tively constructed, a recursive parsing algorithm still
exists.
The ATN for PTQ represents the disambiguated
language for PTQ in the UG sense. The categories of
this disambiguated language correspond to the set of
possible triples: PTO category name, contents of
SENDR registers at a PUSH to that subnet, contents of
the LIFTR registers at the corresponding POP. The
input-refined categories include the remainder of the
input string at the PUSH and POP. Thus the buckets
in the recall table are exactly the input-refined cate-
gories. The syntactic execution method is thus an
exhaustive generation of all expressions in the sen-
tence category of the input-refined disambiguated
language.
</bodyText>
<subsectionHeader confidence="0.925471">
Semantic Equivalence Parsing in UG
</subsectionHeader>
<bodyText confidence="0.999928661290323">
In UG, Montague inclues a theory of meaning by
providing a definition of interpretation for a language.
Let L = &lt;&lt;A,F7,X8,S,80&gt;1„r,85A,R&gt; be a language.
An interpretation 4&apos; for L is a system &lt;B,Gy,f&gt;7 cr.
such that &lt;B,G7&gt;l, cr is an algebra similar to
&lt;A,F7&gt;y,r; i.e., for each y e r, F7 and G7 have the
same number of arguments, and f is a function from
u AX8 into B. Note that the algebra &lt;13,G75.
- y E r
need not be a free algebra (even though &lt;A,Fy&gt;y
must be). B is the set of meanings of the interpreta-
tion 4&apos;; G7 is the semantic rule corresponding to syn-
tactic rule Fv • f assigns meanings to the basic expres-
sions X7. The meaning assignment for L determined
by 4, is the unique homomorphism g from &lt;A,Fy&gt;.), r
into &lt;B,Gy&gt;y, F that is an extension of f.
There are two ways to proceed in order to find all
the meanings of a sentence E in a language L = &lt;12,
R&gt; with interpretation The first method is to gen-
erate all members of the sentence category C&apos;80, of
the input-refined language S-2. As discussed above,
this is done in the algebra &lt;A,F.y&gt;y, r of SZE, using the
syntactic functions F7 to inductively construct mem-
bers of A from the basic categories of RE and members
of A constructed earlier and then applying g. The
second method is to use the fact that g is a homomor-
phism from &lt;A,F..y&gt;.y, r into &lt;B,G7&apos;-7E1&apos;
Because g
is a homomorphism, we can carry out the construction
of the image of the sentence category entirely in the
algebra &lt;B,G),&gt;ye r of the interpretation T. We may
use the G functions to construct inductively members
of B from the basic semantic categories, that is, the
images under g (and f) of the basic syntactic categor-
ies, and members of B already constructed. The ad-
vantage of carrying out the construction in the algebra
of 4, is that this algebra may not be free, i.e., some
element of B may have multiple construction se-
quences. By carrying out the construction there, such
instances can be noticed and used to advantage, thus
eliminating some redundant search. There are addi-
tional costs, however, associated with parsing in the
interpretation algebra NI,. Usually, the cost of evaluat-
ing a G function in the semantic algebra is greater
than the cost of the corresponding F function in the
syntactic algebra. Also in semantic parsing, each
member of B as it is constructed is compared to the
other members of the same refined category that were
previously constructed.
In the PTQ parsing system discussed above, the
interpretation algebra is the set of reduced transla-
tions. The semantic functions are those obtained from
the functions given in the T-rules in PTQ, and reducing
and extensionalizing their results. The directed proc-
ess version of the parser finds the meanings in this
algebra by the first method, generating all parses in
the syntactic algebra and then taking their images un-
der the interpretation homomorphism. Semantic equi-
valence parsing for PTQ uses the second method, car-
rying out the construction of the meaning entirely
within the semantic algebra. The savings in the exam-
ple sentence Bill walks and Mary runs comes about
</bodyText>
<note confidence="0.448173">
American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 137
David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing
</note>
<bodyText confidence="0.999832">
because the algebra of reduced translations is not a
free algebra, and the redundant search thus eliminated
more than made up for the increase in the cost of
translating and comparing formulas.
</bodyText>
<sectionHeader confidence="0.695151" genericHeader="conclusions">
Summary
</sectionHeader>
<bodyText confidence="0.999976291666667">
We have described a parsing algorithm for the lan-
guage of PTQ viewed as consisting of two parts, a
nondeterministic program and an execution method.
We showed how, with only a change to an equivalence
relation used in the execution method, the parser be-
comes a recognizer. We then discussed the addition of
the semantic component of PTQ to the parser. With
again only a change to the equivalence relation of the
execution method, the semantic parser is obtained.
The semantic equivalence relation is equality (to with-
in change of bound variable) of reduced extensional-
ized translations. Examples were given to compare the
two parsing methods.
In the final portion of the paper we described how
the parsing method initially presented in procedural
terms can be viewed in formal grammatical terms.
The notion of input-refinement for context-free gram-
mars was introduced by example, and the tabular
context-free recognition algorithms were described in
these terms. We then indicated how this notion of
refinement can be extended to the UG theory of lan-
guage and suggested how our semantic parser is essen-
tially parsing in the algebra of an interpretation for the
PTQ language.
</bodyText>
<sectionHeader confidence="0.995428" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999423235294117">
Bates, Madeleine 1978 The theory and practise of augmented
transition network grammars. In Bole, Ed., Natural Lanauge
Communication with Computers. New York: 191-260.
Cooper, R. 1975 Montague&apos;s semantic theory and transformation-
al syntax. Ph.D. thesis. Amherst, MA: University of Massa-
chusetts.
Earley, Jay 1970 An efficient context-free parsing algorithm.
Comm. ACM 13, 94-102.
Friedman, J., Moran, D., and Warren, D.S. 1978a Evaluating
English sentences in a logical model. Abstract 16, Information
Abstracts, 7th International Conference on Computational
Linguistics. Norway: University of Bergen (11 pp.).
Friedman, J., Moran, D., and Warren, D.S. 1978b Evaluating
English sentences in a logical model, presented to the 7th Inter-
national Conference on Computation Linguistics, University of
Bergen, Norway (August 14-18). Report N-15. Ann Arbor,
MI: University of Michigan, Computer and Communication
Sciences Department (mimeographed).
Friedman, J. and Warren, D.S. 1978 A parsing method for Mon-
tague grammars. Lingustics and Philosophy 2, 347-372.
Gawron, J.M., et al. 1982 The GPSG linguistic system. In Pro-
ceedings 20th Annual Meeting of the Association for Computational
Linguistics,74-81.
Gazdar, G. 1979 English as a context-free language University of
Sussex (mimeograph).
Harel, David 1979 On the total correctness of nondeterministic
programs. IBM Research Report RC 7691.
Hintikka, J., Moravcsik, J., and Suppes, P., Eds. 1973 Approaches
to Natural Language. Dordrecht: D. Reidel.
Janssen, T.W.V. 1978 Compositionality and the form of rules in
Montague grammar. In Groenenijk, J. and Stokhof, M., Eds.,
Proceedings of the Second Amsterdam Colloquium on Montague
Grammar and Related Topics. Amsterdam Papers in Formal
Grammar, Volume II. University of Amsterdam, 211-234.
Janssen, T.W.V. 1980 On problems concerning the quantification
rules in Montague grammar. In Roher, G., Ed., Time, Tense,
and Quantifiers. Tuebingen, Max Niemeyer Verlag.
Kasami, T. 1965 An efficient recognition and syntax-analysis
algorithm for context-free languages. Science Report AFCRL-
65-758. Bedford, MA: Air Force Cambridge Research Labora-
tory.
Landsbergen, S.P.J. 1980 Adaptation of Montague grammar to
the requirements of parsing. M.S. 11.646. Eindhoven, The
Netherlands: Philips Research Laboratories.
Montague, Richard 1970 Universal grammar (UG). Theoria 36,
373-398.
Montague, Richard 1973 The proper treatment of quantification
in ordinary English. In Hintikka, Moravcsik, and Suppes 1973.
Reprinted in Montague 1974, 247-270.
Montague, Richard 1974 Formal Philosophy: Selected Papers of
Richard Montague. Edited and with an introduction by Rich-
mond Thomason. New Haven, CT: Yale University Press.
Rosenschein, S.J. and Shieber, S.M. 1982 Translating English into
logical form. In Proceedings 20th Annual Meeting of the Associa-
tion for Computational Linguistics, 1-8.
Sheil, B.A. 1976 Observations on context-free parsing. Statistical
Methods in Linguistics 71-109.
Warren, David S. 1979 Syntax and semantics in parsing: an appli-
cation to Montague grammar. Ph.D. thesis. Ann Arbor, MI:
University of Michigan.
Winograd, T.A. 1972 Understanding Natural Language. New York:
Academic Press.
Woods, W.A. and Kaplan, R.M. 1971 The Lunar Sciences Natural
Language Information System. BBN Report No. 2265. Cam-
bridge, MA Bolt Beranek and Newman.
Woods, W.A. 1973 An experimental parsing system for transition
network grammars. In Rustin, R., Ed., Natural Language
Processing. New York: Algorithmics Press, Inc., 111-154.
</reference>
<page confidence="0.884172">
138 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.510909">
<title confidence="0.9881395">Using Semantics in Non-Context-Free of Montague Grammarl</title>
<author confidence="0.999894">David Scott Warren</author>
<affiliation confidence="0.953816">Department of Computer SUNY at Stony</affiliation>
<address confidence="0.94416">Long Island, NY 11794</address>
<author confidence="0.999025">Joyce Friedman</author>
<affiliation confidence="0.99539">University of</affiliation>
<abstract confidence="0.9627033">Ann Arbor, MI In natural language processing, the question of the appropriate interaction of syntax and semantics during sentence analysis has long been of interest. Montague grammar with its fully formalized syntax and semantics provides a complete, well-defined context in which these questions can be considered. This paper describes how semantics can be used during parsing to reduce the combinatorial explosion of syntactic ambiguity in Montague grammar. parsing algorithm, called equivalence parsing, presented and examples of its operation are given. The algorithm is applicable to general non-context-free grammars that include a formal semantic component. The second portion of the paper places semantic equivalence parsing in the context of the very general definition of an interpreted</abstract>
<note confidence="0.93242">language as a homomorphism between syntactic and semantic algebras (Montague 1970).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Madeleine Bates</author>
</authors>
<title>The theory and practise of augmented transition network grammars.</title>
<date>1978</date>
<booktitle>In Bole, Ed., Natural Lanauge Communication with Computers.</booktitle>
<pages>191--260</pages>
<location>New York:</location>
<contexts>
<context position="11342" citStr="Bates 1978" startWordPosition="1782" endWordPosition="1783">ed. Equivalence parsing can be viewed as processing an input sentence and the ATN to define and fill in the buckets of the recall table. The use of the recall table reduces the amount of redundant processing in parsing a sentence. Syntactic structures found along one execution path through the ATN need not be reconstructed but can be directly retrieved from the recall table and used on other paths. The recall table is a generalization of the familiar well-formed substring table (WFST) to arbitrary programs that contain procedure calls. Use of the WFST in ATN parsing is noted in Woods 1973 and Bates 1978. Bates observes that the WFST is complicated by the HOLDs and SENDRs in the ATN. These are the ATN actions that correspond to parameter passing in procedures and are required in the ATN for PTQ to correctly treat the substitution rules. In the Woods system the WFST is viewed as a possible optimization, to be turned on when it improves parsing efficiency. In our system the recall table is an intrinsic part of the parsing algorithm. Because any ATN that naturally represents PTQ must contain left recursion, the usual depth-first (or breadth-first or best-first) ATN parsing algorithm would go int</context>
</contexts>
<marker>Bates, 1978</marker>
<rawString>Bates, Madeleine 1978 The theory and practise of augmented transition network grammars. In Bole, Ed., Natural Lanauge Communication with Computers. New York: 191-260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cooper</author>
</authors>
<title>Montague&apos;s semantic theory and transformational syntax.</title>
<date>1975</date>
<booktitle>Ph.D. thesis.</booktitle>
<institution>University of Massachusetts.</institution>
<location>Amherst, MA:</location>
<contexts>
<context position="34127" citStr="Cooper 1975" startWordPosition="5881" endWordPosition="5882"> would construct trees that do have substitution rules to substitute variables for variaAmerican Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 133 David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing bles, contrary to the variable principle mentioned above. We also note that with this modification a pronoun is not associated with its antecedent when it is first encountered. Instead the pronoun is saved and at some later point in the parse the association is made. This revised treatment is related computationally to that proposed in Cooper 1975. Evaluation of Semantic Equivalence Parsing The question of the interaction of syntax and semantics in parsing was introduced early in computational linguistics. Winograd 1971 argued for the incorporation of semantics as early as possible in the recognition process, in order to reduce the amount of syntactic processing that would be needed. Partial parses that had no interpretation did not need to be continued. The alternative position represented by Woods&apos;s early work (Woods and Kaplan 1971) was basically the inverse: less semantic processing would be needed if only completed parses were int</context>
</contexts>
<marker>Cooper, 1975</marker>
<rawString>Cooper, R. 1975 Montague&apos;s semantic theory and transformational syntax. Ph.D. thesis. Amherst, MA: University of Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Comm. ACM</journal>
<volume>13</volume>
<pages>94--102</pages>
<contexts>
<context position="39684" citStr="Earley 1970" startWordPosition="6767" endWordPosition="6768">ars We now switch our point of view and examine equivalence parsing not in algorithmic terms but in formal grammatical terms. This will then lead into showing how equivalence parsing relates to Universal 134 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing Grammar (UG) (Montague 1970). The basic concept to be used is an input-refined grammar. We begin by defining this concept for context-free grammars and using it to relate the tabular context-free recognition algorithms of Earley 1970, Cocke-Kasami-Younger (Kasami 1965), and Sheil 1976 to each other and eventually to our algorithm. Given a context-free grammar G and a string s over the terminal symbols of G, we define from G and s a new grammar Gs, called an input-refinement of G. This new grammar Gs will bear a particular relationship to G: L(Gs) = {s} n L(G), i.e., L(Gs) is the singleton set {s} if s is in L(G), and empty otherwise. Furthermore, there is a direct one-to-one relationship between the derivations of s in G and the derivations of s in G. Thus the problem of recognizing s in G is reduced to the problem of det</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, Jay 1970 An efficient context-free parsing algorithm. Comm. ACM 13, 94-102.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Friedman</author>
<author>D Moran</author>
<author>D S Warren</author>
</authors>
<title>1978a Evaluating English sentences in a logical model. Abstract 16, Information Abstracts,</title>
<booktitle>7th International Conference on Computational Linguistics.</booktitle>
<volume>11</volume>
<pages>pp.).</pages>
<institution>Norway: University of Bergen</institution>
<marker>Friedman, Moran, Warren, </marker>
<rawString>Friedman, J., Moran, D., and Warren, D.S. 1978a Evaluating English sentences in a logical model. Abstract 16, Information Abstracts, 7th International Conference on Computational Linguistics. Norway: University of Bergen (11 pp.).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Friedman</author>
<author>D Moran</author>
<author>D S Warren</author>
</authors>
<title>1978b Evaluating English sentences in a logical model, presented to the</title>
<date></date>
<booktitle>7th International Conference on Computation Linguistics,</booktitle>
<tech>Report N-15.</tech>
<institution>University of Bergen, Norway</institution>
<location>Ann Arbor, MI:</location>
<marker>Friedman, Moran, Warren, </marker>
<rawString>Friedman, J., Moran, D., and Warren, D.S. 1978b Evaluating English sentences in a logical model, presented to the 7th International Conference on Computation Linguistics, University of Bergen, Norway (August 14-18). Report N-15. Ann Arbor, MI: University of Michigan, Computer and Communication Sciences Department (mimeographed).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Friedman</author>
<author>D S Warren</author>
</authors>
<title>A parsing method for Montague grammars.</title>
<date>1978</date>
<journal>Lingustics and Philosophy</journal>
<volume>2</volume>
<pages>347--372</pages>
<contexts>
<context position="8348" citStr="Friedman and Warren 1978" startWordPosition="1301" endWordPosition="1304">han one parse, because the syntactic derivation determines quantifier scope. In PTQ there is infinite syntactic ambiguity arising from three sources: alphabetic variants of variables, variable for variable substitutions, and vacuous variable substitution. However, these semantically unnecessary constructs can be eliminated, so that the set of syntactic sources for any sentence is finite, and a parser that finds the full set is possible. (This corresponds to the &amp;quot;variable principle&amp;quot; enunciated by Janssen 1980 and used by Landsbergen 1980.) This approach was the basis of our earlier PTQ parser (Friedman and Warren 1978). However, even with these reductions the number of remaining parses for a sentence of reasonable complexity is still large compared to the number of nonequivalent translations. In the directed process approach this is treated by first finding all the parses, next finding for each parse a reduced translation, and then finally obtaining the set of reduced translations. Each reduced translation may, but does not necessarily, represent a different sentence meaning. No meanings are lost. Further reductions of the set of translations would be possible, but the undecidability of logical equivalence </context>
</contexts>
<marker>Friedman, Warren, 1978</marker>
<rawString>Friedman, J. and Warren, D.S. 1978 A parsing method for Montague grammars. Lingustics and Philosophy 2, 347-372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Gawron</author>
</authors>
<title>The GPSG linguistic system.</title>
<date>1982</date>
<booktitle>In Proceedings 20th Annual Meeting of the Association for Computational Linguistics,74-81.</booktitle>
<marker>Gawron, 1982</marker>
<rawString>Gawron, J.M., et al. 1982 The GPSG linguistic system. In Proceedings 20th Annual Meeting of the Association for Computational Linguistics,74-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>English as a context-free language</title>
<date>1979</date>
<institution>University of Sussex (mimeograph).</institution>
<marker>Gazdar, 1979</marker>
<rawString>Gazdar, G. 1979 English as a context-free language University of Sussex (mimeograph).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Harel</author>
</authors>
<title>On the total correctness of nondeterministic programs.</title>
<date>1979</date>
<journal>IBM Research Report RC</journal>
<pages>7691</pages>
<marker>Harel, 1979</marker>
<rawString>Harel, David 1979 On the total correctness of nondeterministic programs. IBM Research Report RC 7691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hintikka</author>
<author>J Moravcsik</author>
<author>P Suppes</author>
<author>Eds</author>
</authors>
<title>Approaches to Natural Language.</title>
<date>1973</date>
<location>Dordrecht: D. Reidel.</location>
<marker>Hintikka, Moravcsik, Suppes, Eds, 1973</marker>
<rawString>Hintikka, J., Moravcsik, J., and Suppes, P., Eds. 1973 Approaches to Natural Language. Dordrecht: D. Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T W V Janssen</author>
</authors>
<title>Compositionality and the form of rules in Montague grammar. In</title>
<date>1978</date>
<booktitle>Proceedings of the Second Amsterdam Colloquium on Montague Grammar and</booktitle>
<pages>211--234</pages>
<institution>University of Amsterdam,</institution>
<marker>Janssen, 1978</marker>
<rawString>Janssen, T.W.V. 1978 Compositionality and the form of rules in Montague grammar. In Groenenijk, J. and Stokhof, M., Eds., Proceedings of the Second Amsterdam Colloquium on Montague Grammar and Related Topics. Amsterdam Papers in Formal Grammar, Volume II. University of Amsterdam, 211-234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T W V Janssen</author>
</authors>
<title>On problems concerning the quantification rules in Montague grammar. In</title>
<date>1980</date>
<institution>Max Niemeyer Verlag.</institution>
<contexts>
<context position="8236" citStr="Janssen 1980" startWordPosition="1282" endWordPosition="1284">pretation. Further, sentences with scope ambiguity, such as A man loves every woman, require more than one parse, because the syntactic derivation determines quantifier scope. In PTQ there is infinite syntactic ambiguity arising from three sources: alphabetic variants of variables, variable for variable substitutions, and vacuous variable substitution. However, these semantically unnecessary constructs can be eliminated, so that the set of syntactic sources for any sentence is finite, and a parser that finds the full set is possible. (This corresponds to the &amp;quot;variable principle&amp;quot; enunciated by Janssen 1980 and used by Landsbergen 1980.) This approach was the basis of our earlier PTQ parser (Friedman and Warren 1978). However, even with these reductions the number of remaining parses for a sentence of reasonable complexity is still large compared to the number of nonequivalent translations. In the directed process approach this is treated by first finding all the parses, next finding for each parse a reduced translation, and then finally obtaining the set of reduced translations. Each reduced translation may, but does not necessarily, represent a different sentence meaning. No meanings are lost.</context>
</contexts>
<marker>Janssen, 1980</marker>
<rawString>Janssen, T.W.V. 1980 On problems concerning the quantification rules in Montague grammar. In Roher, G., Ed., Time, Tense, and Quantifiers. Tuebingen, Max Niemeyer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kasami</author>
</authors>
<title>An efficient recognition and syntax-analysis algorithm for context-free languages. Science Report AFCRL65-758.</title>
<date>1965</date>
<institution>Air Force Cambridge Research Laboratory.</institution>
<location>Bedford, MA:</location>
<contexts>
<context position="13060" citStr="Kasami 1965" startWordPosition="2073" endWordPosition="2074">indexed by a triple: (1) a grammatical category, that is, a subnet to which a PUSH is made, (2) the contents of the SENDR registers at the PUSH and the current string, and (3) the contents of the LIFTR registers at the POP and the then-current string. A bucket contains the members of an equivalence class of syntactic structures; precisely what they are depends on what type of equivalence is being used. What makes equivalence parsing applicable to noncontext-free grammars is that its buckets are more general than the cells in the standard tabular contextfree algorithms. In the C-K-Y algorithm (Kasami 1965), for example, a cell is indexed only by the starting position and the length of the parsed segment, i.e., the current string at PUSH and POP. The cell contents are nonterminals. In our case all three are part of the bucket index, which also includes SENDR and LIFTR register values. The bucket contents are equivalence classes of structures. Sentence Recognition For sentence recognition all parses are equivalent. So it is enough to determine, for each bucket of the recall table, whether or not it is empty. A sentence is in the language if the bucket corresponding to the sentence category (with </context>
<context position="39720" citStr="Kasami 1965" startWordPosition="6770" endWordPosition="6771"> and examine equivalence parsing not in algorithmic terms but in formal grammatical terms. This will then lead into showing how equivalence parsing relates to Universal 134 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing Grammar (UG) (Montague 1970). The basic concept to be used is an input-refined grammar. We begin by defining this concept for context-free grammars and using it to relate the tabular context-free recognition algorithms of Earley 1970, Cocke-Kasami-Younger (Kasami 1965), and Sheil 1976 to each other and eventually to our algorithm. Given a context-free grammar G and a string s over the terminal symbols of G, we define from G and s a new grammar Gs, called an input-refinement of G. This new grammar Gs will bear a particular relationship to G: L(Gs) = {s} n L(G), i.e., L(Gs) is the singleton set {s} if s is in L(G), and empty otherwise. Furthermore, there is a direct one-to-one relationship between the derivations of s in G and the derivations of s in G. Thus the problem of recognizing s in G is reduced to the problem of determining emptiness for the grammar G</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>Kasami, T. 1965 An efficient recognition and syntax-analysis algorithm for context-free languages. Science Report AFCRL65-758. Bedford, MA: Air Force Cambridge Research Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P J Landsbergen</author>
</authors>
<title>Adaptation of Montague grammar to the requirements of parsing.</title>
<date>1980</date>
<journal>M.S. 11.646. Eindhoven, The Netherlands: Philips Research Laboratories.</journal>
<contexts>
<context position="8265" citStr="Landsbergen 1980" startWordPosition="1288" endWordPosition="1289">nces with scope ambiguity, such as A man loves every woman, require more than one parse, because the syntactic derivation determines quantifier scope. In PTQ there is infinite syntactic ambiguity arising from three sources: alphabetic variants of variables, variable for variable substitutions, and vacuous variable substitution. However, these semantically unnecessary constructs can be eliminated, so that the set of syntactic sources for any sentence is finite, and a parser that finds the full set is possible. (This corresponds to the &amp;quot;variable principle&amp;quot; enunciated by Janssen 1980 and used by Landsbergen 1980.) This approach was the basis of our earlier PTQ parser (Friedman and Warren 1978). However, even with these reductions the number of remaining parses for a sentence of reasonable complexity is still large compared to the number of nonequivalent translations. In the directed process approach this is treated by first finding all the parses, next finding for each parse a reduced translation, and then finally obtaining the set of reduced translations. Each reduced translation may, but does not necessarily, represent a different sentence meaning. No meanings are lost. Further reductions of the se</context>
</contexts>
<marker>Landsbergen, 1980</marker>
<rawString>Landsbergen, S.P.J. 1980 Adaptation of Montague grammar to the requirements of parsing. M.S. 11.646. Eindhoven, The Netherlands: Philips Research Laboratories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<date>1970</date>
<journal>Universal grammar (UG). Theoria</journal>
<volume>36</volume>
<pages>373--398</pages>
<contexts>
<context position="1088" citStr="Montague 1970" startWordPosition="155" endWordPosition="156">h these questions can be considered. This paper describes how semantics can be used during parsing to reduce the combinatorial explosion of syntactic ambiguity in Montague grammar. A parsing algorithm, called semantic equivalence parsing, is presented and examples of its operation are given. The algorithm is applicable to general non-context-free grammars that include a formal semantic component. The second portion of the paper places semantic equivalence parsing in the context of the very general definition of an interpreted language as a homomorphism between syntactic and semantic algebras (Montague 1970). Introduction The close interrelation between syntax and semantics in Montague grammar provides a good framework in which to consider the interaction of syntax and semantics in sentence analysis. Several different approaches are possible in this framework and they can be developed rigorously for comparison. In this paper we develop an approach called semantic equivalence parsing that introduces logical translation into the ongoing parsing process. We compare this with our earlier directed process implementation in which syntactic parsing is completed prior to translation to logical form. Part</context>
<context position="39479" citStr="Montague 1970" startWordPosition="6735" endWordPosition="6736">ing can be applied whenever syntactic structure is used as an intermediate form and there is a syntax-directed translation to an output form on which an equivalence relation is defined. 2. Input-Refined Grammars We now switch our point of view and examine equivalence parsing not in algorithmic terms but in formal grammatical terms. This will then lead into showing how equivalence parsing relates to Universal 134 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing Grammar (UG) (Montague 1970). The basic concept to be used is an input-refined grammar. We begin by defining this concept for context-free grammars and using it to relate the tabular context-free recognition algorithms of Earley 1970, Cocke-Kasami-Younger (Kasami 1965), and Sheil 1976 to each other and eventually to our algorithm. Given a context-free grammar G and a string s over the terminal symbols of G, we define from G and s a new grammar Gs, called an input-refinement of G. This new grammar Gs will bear a particular relationship to G: L(Gs) = {s} n L(G), i.e., L(Gs) is the singleton set {s} if s is in L(G), and emp</context>
<context position="46212" citStr="Montague 1970" startWordPosition="7853" endWordPosition="7854">nition methods. Equivalence parsing as described in Part I above is also a tabular method, although it is not context-free. It applies to context-free grammars and also to some grammars such as PTQ that are not context-free. We next relate it to the very general class of grammars defined by Montague in UG. Universal Grammar and Equivalence Parsing In the following discussion of the problem of parsing in the general context of Montague&apos;s definitions of a language (which might more naturally be called a grammar) and an interpretation, we assume the reader is familiar with the definitions in UG (Montague 1970). We begin with a formal definition of a refinement of a general disambiguated language. A particular type of refinement, input-refinement, leads to an equivalence parsing algorithm. This generalizes the procedure for input-refining a grammar shown above for the special case of a context-free grammar. We then discuss the implications for equivalence parsing of using the formal interpretation of the language. Finally we show how the ATN for PTQ and semantic equivalence parsing fit into this general framework. Recall that a disambiguated language S2 = &lt;A, F7, X8, S,can be regarded as consisting </context>
</contexts>
<marker>Montague, 1970</marker>
<rawString>Montague, Richard 1970 Universal grammar (UG). Theoria 36, 373-398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>The proper treatment of quantification in ordinary English.</title>
<date>1973</date>
<booktitle>In Hintikka, Moravcsik, and Suppes</booktitle>
<pages>247--270</pages>
<location>Montague</location>
<note>Reprinted in</note>
<marker>Montague, 1973</marker>
<rawString>Montague, Richard 1973 The proper treatment of quantification in ordinary English. In Hintikka, Moravcsik, and Suppes 1973. Reprinted in Montague 1974, 247-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>Formal Philosophy: Selected Papers of Richard Montague. Edited and with an introduction by Richmond Thomason.</title>
<date>1974</date>
<publisher>Yale University Press.</publisher>
<location>New Haven, CT:</location>
<marker>Montague, 1974</marker>
<rawString>Montague, Richard 1974 Formal Philosophy: Selected Papers of Richard Montague. Edited and with an introduction by Richmond Thomason. New Haven, CT: Yale University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Rosenschein</author>
<author>S M Shieber</author>
</authors>
<title>Translating English into logical form.</title>
<date>1982</date>
<booktitle>In Proceedings 20th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="6654" citStr="Rosenschein and Shieber 1982" startWordPosition="1031" endWordPosition="1034">egarded equivalent if they have the same formula. The approach to parsing suggested by Cooper&apos;s 1975 treatment of quantified noun phrases is like our semantic equivalence parsing in storing translations as one element of the tuple corresponding to a noun phrase. Cooper&apos;s approach differs from the approach followed here because he has an intermediate stage that might be called an &amp;quot;autonomous syntax tree&amp;quot;. The frontier of the tree is the sentence; the scope of the quantifier of a noun phrase is not yet indicated. Cooper&apos;s approach has been followed by the GPSG system (Gawron et al. 1982) and by Rosenschein and Shieber 1982. Neither of those systems treats pronouns. In Montague&apos;s approach, which we follow here, the trees produced by the parser are expressions in the disambiguated language, so scope is determined, pronoun antecedents are indicated, and each tree has a unique (unreduced) translation. The descriptions of the systems that use Cooper&apos;s approach seem to imply that they use a second pass over the syntax tree to determine the actual quantifier scopes in the final logical forms. Were these systems to use a single pass to produce the final logical forms, the results described in this paper would be direct</context>
</contexts>
<marker>Rosenschein, Shieber, 1982</marker>
<rawString>Rosenschein, S.J. and Shieber, S.M. 1982 Translating English into logical form. In Proceedings 20th Annual Meeting of the Association for Computational Linguistics, 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B A Sheil</author>
</authors>
<title>Observations on context-free parsing.</title>
<date>1976</date>
<booktitle>Statistical Methods in Linguistics</booktitle>
<pages>71--109</pages>
<contexts>
<context position="39736" citStr="Sheil 1976" startWordPosition="6773" endWordPosition="6774">alence parsing not in algorithmic terms but in formal grammatical terms. This will then lead into showing how equivalence parsing relates to Universal 134 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing Grammar (UG) (Montague 1970). The basic concept to be used is an input-refined grammar. We begin by defining this concept for context-free grammars and using it to relate the tabular context-free recognition algorithms of Earley 1970, Cocke-Kasami-Younger (Kasami 1965), and Sheil 1976 to each other and eventually to our algorithm. Given a context-free grammar G and a string s over the terminal symbols of G, we define from G and s a new grammar Gs, called an input-refinement of G. This new grammar Gs will bear a particular relationship to G: L(Gs) = {s} n L(G), i.e., L(Gs) is the singleton set {s} if s is in L(G), and empty otherwise. Furthermore, there is a direct one-to-one relationship between the derivations of s in G and the derivations of s in G. Thus the problem of recognizing s in G is reduced to the problem of determining emptiness for the grammar G. Also, the prob</context>
</contexts>
<marker>Sheil, 1976</marker>
<rawString>Sheil, B.A. 1976 Observations on context-free parsing. Statistical Methods in Linguistics 71-109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David S Warren</author>
</authors>
<title>Syntax and semantics in parsing: an application to Montague grammar.</title>
<date>1979</date>
<tech>Ph.D. thesis.</tech>
<institution>Ann Arbor, MI: University of Michigan.</institution>
<contexts>
<context position="10559" citStr="Warren 1979" startWordPosition="1649" endWordPosition="1650">n of execution method for nondeterministic programs. An execution method is a deterministic procedure for finding the possible execution paths through a nondeterministic program given an input. For an ATN, these execution paths correspond to different parses. Viewing parsing in this way, the only difference between the usual syntactic parsing and semantic equivalence parsing is a difference in the execution method. As will be seen, semantic equivalence parsing uses semantic tests as part of the execution method. We call the execution method we use to process a general ATN equivalence parsing (Warren 1979). Equivalence parsing is based on a recall table. The recall table is a set of buckets used to organize and hold partial syntactic structures while larger ones are constructed. Equivalence parsing can be viewed as processing an input sentence and the ATN to define and fill in the buckets of the recall table. The use of the recall table reduces the amount of redundant processing in parsing a sentence. Syntactic structures found along one execution path through the ATN need not be reconstructed but can be directly retrieved from the recall table and used on other paths. The recall table is a gen</context>
</contexts>
<marker>Warren, 1979</marker>
<rawString>Warren, David S. 1979 Syntax and semantics in parsing: an application to Montague grammar. Ph.D. thesis. Ann Arbor, MI: University of Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<marker>Winograd, 1972</marker>
<rawString>Winograd, T.A. 1972 Understanding Natural Language. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
<author>R M Kaplan</author>
</authors>
<title>The Lunar Sciences Natural Language Information System.</title>
<date>1971</date>
<tech>BBN Report No. 2265.</tech>
<location>Cambridge, MA Bolt Beranek and Newman.</location>
<contexts>
<context position="34625" citStr="Woods and Kaplan 1971" startWordPosition="5957" endWordPosition="5960">oint in the parse the association is made. This revised treatment is related computationally to that proposed in Cooper 1975. Evaluation of Semantic Equivalence Parsing The question of the interaction of syntax and semantics in parsing was introduced early in computational linguistics. Winograd 1971 argued for the incorporation of semantics as early as possible in the recognition process, in order to reduce the amount of syntactic processing that would be needed. Partial parses that had no interpretation did not need to be continued. The alternative position represented by Woods&apos;s early work (Woods and Kaplan 1971) was basically the inverse: less semantic processing would be needed if only completed parses were interpreted. This argument is based on the idea of eliminating uninterpretable parses as soon as possible. This advantage, if it is one, of integrated syntactic and semantic procedures does not occur here because the semantic aspect does not eliminate any logical analyses. The translation of a structure to a formula is always successful, so no partial parse is ever eliminated for lack of a translation. What happens instead is that several partial parses are found to be equivalent because they hav</context>
</contexts>
<marker>Woods, Kaplan, 1971</marker>
<rawString>Woods, W.A. and Kaplan, R.M. 1971 The Lunar Sciences Natural Language Information System. BBN Report No. 2265. Cambridge, MA Bolt Beranek and Newman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>An experimental parsing system for transition network grammars. In</title>
<date>1973</date>
<pages>111--154</pages>
<publisher>Algorithmics Press, Inc.,</publisher>
<location>New York:</location>
<contexts>
<context position="9117" citStr="Woods 1973" startWordPosition="1426" endWordPosition="1427">alent translations. In the directed process approach this is treated by first finding all the parses, next finding for each parse a reduced translation, and then finally obtaining the set of reduced translations. Each reduced translation may, but does not necessarily, represent a different sentence meaning. No meanings are lost. Further reductions of the set of translations would be possible, but the undecidability of logical equivalence precludes algorithmic reduction to a minimal set. The ATN Program In the underlying parser the grammar is expressed as an augmented transition network (ATN) (Woods 1973). Both the syntactic and the semantic parsers use this same ATN. The main difficulty in construct124 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing ing the ATN was, as usual, the non-context-free aspects of the grammar, in particular the incorporation of a treatment of substitution rules and variables. The grammar given in PTQ generates infinitely many derivations for each sentence. All but finitely many of these are unnecessary variations on variables and were eliminated </context>
<context position="11327" citStr="Woods 1973" startWordPosition="1779" endWordPosition="1780">s are constructed. Equivalence parsing can be viewed as processing an input sentence and the ATN to define and fill in the buckets of the recall table. The use of the recall table reduces the amount of redundant processing in parsing a sentence. Syntactic structures found along one execution path through the ATN need not be reconstructed but can be directly retrieved from the recall table and used on other paths. The recall table is a generalization of the familiar well-formed substring table (WFST) to arbitrary programs that contain procedure calls. Use of the WFST in ATN parsing is noted in Woods 1973 and Bates 1978. Bates observes that the WFST is complicated by the HOLDs and SENDRs in the ATN. These are the ATN actions that correspond to parameter passing in procedures and are required in the ATN for PTQ to correctly treat the substitution rules. In the Woods system the WFST is viewed as a possible optimization, to be turned on when it improves parsing efficiency. In our system the recall table is an intrinsic part of the parsing algorithm. Because any ATN that naturally represents PTQ must contain left recursion, the usual depth-first (or breadth-first or best-first) ATN parsing algorit</context>
</contexts>
<marker>Woods, 1973</marker>
<rawString>Woods, W.A. 1973 An experimental parsing system for transition network grammars. In Rustin, R., Ed., Natural Language Processing. New York: Algorithmics Press, Inc., 111-154.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>